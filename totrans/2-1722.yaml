- en: Python for Data Engineers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://towardsdatascience.com/python-for-data-engineers-f3d5db59b6dd](https://towardsdatascience.com/python-for-data-engineers-f3d5db59b6dd)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Advanced ETL techniques for beginners
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://mshakhomirov.medium.com/?source=post_page-----f3d5db59b6dd--------------------------------)[![ðŸ’¡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----f3d5db59b6dd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f3d5db59b6dd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f3d5db59b6dd--------------------------------)
    [ðŸ’¡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----f3d5db59b6dd--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f3d5db59b6dd--------------------------------)
    Â·17 min readÂ·Oct 21, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9c664876939298ade749c4c53cb490c8.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Boitumelo](https://unsplash.com/@writecodenow?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: In this story I will speak about advanced data engineering techniques in Python.
    No doubt, Python is the most popular programming language for data. During my
    almost twelve-year career in data engineering, I encountered various situations
    when code had issues. This story is a brief summary of how I resolved them and
    learned to write better code. I will show a few techniques that make our ETL faster
    and help to improve the performance of our code.
  prefs: []
  type: TYPE_NORMAL
- en: List comprehensions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Imagine you are looping through a list of tables. Typically, we would do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'But instead, we could use list comprehensions. Not only they are faster, they
    also reduce the code making it more concise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, looping through a super large file with data to transform (ETL)
    each row has never been easier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'List comprehensions are extremely useful for ETL processing of large data files.
    Imagine a data file we need to transform into a newline delimited format. Try
    running this example in your Python environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Output will be a **newline delimited JSON**. This is a standard format for
    data in BigQuery data warehouse and it is ready for loading into the table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Generators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If we are dealing with CSV and DAT files where data is **stored line by line**
    then our file object is already a **generator** of lines and we can use a list
    comprehension to process data **not consuming too much of our memory**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Validating records before we actually insert them into a data warehouse table
    might be useful for batch data processing pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Often we need to validate data files before we load them into a data warehouse.
    If one record fails the whole batch will fail.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'We can use it to create close to real-time analytics pipelines. This is also
    a very cost-effective way to process data compared to streaming data pipeline
    design pattern. I previously wrote about it here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----f3d5db59b6dd--------------------------------)
    [## Data pipeline design patterns'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right architecture with examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----f3d5db59b6dd--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, we could use `yield` when dealing with big data and when our
    file is not a newline delimited text. It is always a good practice as we would
    want to process the data file in a memory-efficient manner. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This will read a local file and process it in chunks of 19 bytes. Output will
    be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Itâ€™s just an example of how we can deal with **binary** data. In real life,
    it might be easier to split the file contents into segments using a **separator**,
    i.e. newline `'\nâ€™` or `'}{'` depending on how our data was structured.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that the text data is pulled from some **external** location, i.e. from
    **cloud storage**. We also can process it as a **stream**. We wouldnâ€™t want to
    load the whole data file and run `split('\n')` to process it line by line. It
    would consume a lot of memory. We can use `re.finditer` instead which acts like
    a **generator** and will read our data file in chunks so weâ€™ll be able to run
    the required ETL **not** consuming too much memory.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Python **properties for data validation**
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can use **Python properties** [2] to validate data records. If a record is
    not an instance of a class that we defined then the exception must be raised.
  prefs: []
  type: TYPE_NORMAL
- en: We can store our data as objects of the data class.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It is as simple as that. Letâ€™s imagine that we have a **streaming data pipeline**
    and we want to validate some fields in our records.
  prefs: []
  type: TYPE_NORMAL
- en: Put it simplyâ€” they must match the existing table schema.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We can use Python properties for that. Consider this example below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: If we choose to break the rules and assign some values that donâ€™t match our
    criteria, the exception will be thrown. For example, the exception will be raised
    if we try to call `ConnectionDataRecord('', 1)`
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, we can use a library called `Pydantic` Consider this cde below.
    It will throw an error if we call the function with an object that doesnâ€™t meet
    our requirements.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Decorators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Decorators were designed to make our code look leaner and to add extra functionality
    to it. We can simply pass one function as an argument into another function (decorator)
    and do some data transformation inside this wrapper. Imagine that we have many
    different ETL functions to process data but we need just one to upload the result
    into the data lake. This is how we do it:'
  prefs: []
  type: TYPE_NORMAL
- en: If some code logic repeats it is a good practice to use **a decorator**.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It helps to maintain code base easier and saves a lot of time in case we need
    to change the repetitive logic.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Decorators are used everywhere due to their effectiveness. Consider this Airflow
    DAG example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Working with APIs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a data engineer you would perform HTTP requests calling various API endpoints
    very often. Consider this example of GET request below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: It pulls some data from the **free** **NASA Asteroids API** and will return
    all asteroids close to Earth on that date. Just replace your API key in this URL
    path above or use the one I created. The **requests** library handles everything
    but there is a better way of doing it.
  prefs: []
  type: TYPE_NORMAL
- en: We can use session and process data from our API endpoint as a **stream**.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'That would ensure we wonâ€™t encounter any memory issues and process our GET
    request in a streaming manner [3]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Understanding how HTTP requests work is crucial in data engineering.
  prefs: []
  type: TYPE_NORMAL
- en: I do all sorts of things with API requests daily and I donâ€™t have to rely on
    any other frameworks or libraries.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'For example, just a couple of weeks ago I was working on the **Dataform migration
    project** and realised that the existing Google library (`from google.cloud import
    dataform_v1beta1`) canâ€™t create schedules. The workaround was to use Dataform
    API [4] and it was as simple as POST request to that particular endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The essence of this request is that we send `workflow_config` as **json** and
    add `workflowConfigId` in path parameters using this knowledge from Google documentation
    [4].
  prefs: []
  type: TYPE_NORMAL
- en: This will create a required schedule to run our data transformation scripts
    in BigQueryâ€™s Dataform.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Similarly, as we did in GET request we can stream data into our POST API endpoint
    using Python **generators** like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The idea is clear. We can process and send data in a memory-efficient manner.
  prefs: []
  type: TYPE_NORMAL
- en: Handling API rate limits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All APIs have rate limits and we would want to keep that in mind while fetching
    the data out of it. We can use decorators to handle it. Simple decoration can
    be implemented like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Using this decorator our function wonâ€™t make more than 10 API calls within 15
    minutes.
  prefs: []
  type: TYPE_NORMAL
- en: The most simple way to handle this situation is to use `time.sleep()` but Python
    ratelimit allows us to do it in this elegant manner.
  prefs: []
  type: TYPE_NORMAL
- en: Async and `await` in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Performing ETL **asynchronously** is another extremely useful feature. We can
    use the `asyncio` library to run tasks simultaneously. Let consider this simple
    synchronous example where we process tables in `for` loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Running this code we would have to wait for each table to finish the `pull_data()`
    task but with `Async`, we can process them in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider using this code instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: It will pull data from reporting APIs simultaneously and it will significantly
    improve our ETL performance.
  prefs: []
  type: TYPE_NORMAL
- en: It helps to manage ETL tasks easier while system resources are being distributed
    in the best way possible.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'For example, we can run two ETL jobs simultaneously but then we can define
    the order of execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Use Map and Filter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Map and filter work even faster than list comprehensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can transform our data line by line aplying `map` function to items in our
    dataset prrocessing it as an `iterable`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We would want to use `filter` to extract objects matching a certain criteria,
    i.e.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Process large datasets using Pandas
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Later versions of Pandas library provide a handy context manager that can be
    used like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: It will process data in batch mode assuming we donâ€™t need to load the whole
    dataset into a dataframe at one time.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'It has a wide range of applications from OLAP reporting to Machine learning
    (ML) pipelines, for instance, we might want to create a recommendation model trainer
    task and would need to prepare a dataset like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This way Pandas will ensure our application always has enough memory to process
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Use joblib for pipelining and parallel computation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`joblib.dump()` and `joblib.load()` methods allow us to pipeline large dataset
    transformations efficiently. `joblib` will store and serialise big data working
    with arbitrary Python objects such as `numpy` arrays.'
  prefs: []
  type: TYPE_NORMAL
- en: What do you think `scikit-learn` uses to save and load ML models? The correct
    answer is - `joblib`.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: First of all, why save a model? â€” Simply because we might need it later further
    down the pipeline, i.e. to make predictions using new data, etc.
  prefs: []
  type: TYPE_NORMAL
- en: We wouldnâ€™t want to retrain our ML model as it is a very time-consuming task.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Another reason is that we might want to save different versions of the same
    model to see which one performs better. `joblib` helps to do all that [5]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'These functions explicitly connect the file we save on disk and the execution
    context of the original Python object. So instead of file names `joblib` also
    accepts file objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '**AWS S3 model dump/load example:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Parallel computing with joblib
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It is very efficient because it relies on multiprocessing and will execute
    tasks concurrently using multiple Python workers on all CPU cores or across multiple
    machines. Consider this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: We can use all CPU cores we have to unlock full potential of our hardware.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Here we tell `Parallel` to use all cores (-1) and the computation runs **5 times
    faster:**
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Unit test ETL pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most important picks Iâ€™ve learned throughout my data engineering
    career is that everything must be unit-tested. That includes not only **SQL**
    but **ETL jobs** and **integrations** with other services we use on our data pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the `unittest` Pyhton library to test our code. Letâ€™s imagine we
    have a helper module that was created to check a number if the number is a prime
    number:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: How do we test the logic inside this function?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`unittest` makes it really simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now if we run this in our command line we will test the logic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This is correct because 13 is a prime number. Letâ€™s test is further. We know
    that 4 is not a prime number and, therefore, we would want our unit test for this
    particular function to return pass while asserting False:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Simple. Letâ€™s take a look at a more advanced example.
  prefs: []
  type: TYPE_NORMAL
- en: Letâ€™s imagine we have an ETL service that pulls data from some API and it takes
    a lot of time. Then our service will transform this dataset and we would like
    to test that this ETL transformation logic persists.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How do we do it?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We can use mock and patch methods from the `unittest` library. Consider this
    application file `asteroids.py`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'If we run app.py the output will have a list of asteroids that are close to
    Earth on that particular date:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Pulling data from API might take a lot of time but we would want our unit test
    to run fast. We can **mock** some fake API response into our `get_data()` function
    and then use it to test ETL logic in `save_data()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: In our unit test, we replaced (using `mock`) values returned by `asteroids.get_data`
    function and expected them to be transformed into (ETL) `[â€˜asteroid_1â€™, â€˜asteroid_2â€™]`
    which our ETL function failed to do. Unit test failed.
  prefs: []
  type: TYPE_NORMAL
- en: Unit tests are very powerful.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'It helps us to deal with human errors while deploying new features in ETL pipelines.
    Some more advanced examples can be found in one of my previous stories. I use
    it very often in CI/CD pipelines [6]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/test-data-pipelines-the-fun-and-easy-way-d0f974a93a59?source=post_page-----f3d5db59b6dd--------------------------------)
    [## Test Data Pipelines the Fun and Easy Way'
  prefs: []
  type: TYPE_NORMAL
- en: 'Beginners guide: Why unit and integration tests are so important for your data
    platform'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/test-data-pipelines-the-fun-and-easy-way-d0f974a93a59?source=post_page-----f3d5db59b6dd--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring memory usage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Often I deploy ETL microservices using serverless. Itâ€™s a very neat a cost-effective
    tool. I deploy Lambdas and Cloud Functions and wouldnâ€™t want to overprovision
    them with excessive memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'I previously wrote about it here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----f3d5db59b6dd--------------------------------)
    [## Infrastructure as Code for Beginners'
  prefs: []
  type: TYPE_NORMAL
- en: Deploy Data Pipelines like a pro with these templates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----f3d5db59b6dd--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, why would we give our Lambda 3Gb of memory and pay more when the data
    can be processed at 256Mb?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: There are various ways to monitor our ETL application memory usage. One of the
    most popular is `tracemalloc` [7] library.
  prefs: []
  type: TYPE_NORMAL
- en: 'It can trace Python memory blocks and return the results in (<current>, <peak
    memory>) format in bytes. Consider this example to extract data from asteroid
    API in one big chunk and save it to the disk:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '**We can see that peak usage is around 540Kb.**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Letâ€™s see how it can be optimised simply by using `stream` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '**We can see that peak memory usage is twice lower.**'
  prefs: []
  type: TYPE_NORMAL
- en: Working with SDKs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a data engineer, we would work with cloud service providers very often. In
    a nutshell, SDKs are collections of service libraries that allow programmatic
    access to cloud services. We would want to learn and master at least one or two
    SDKs from market leaders such as Amazon, Azure or Google.
  prefs: []
  type: TYPE_NORMAL
- en: One of the services I use very often using programmatic access is Cloud Storage.
    Indeed in data engineering, almost every data pipeline relies on data storage
    in the cloud, i.e. Google Cloud Storage or AWS S3.
  prefs: []
  type: TYPE_NORMAL
- en: The most common data pipeline design would be the one created around a data
    bucket. I described this pattern in one of my previous stories [9].
  prefs: []
  type: TYPE_NORMAL
- en: '[](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----f3d5db59b6dd--------------------------------)
    [## Data pipeline design patterns'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right architecture with examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----f3d5db59b6dd--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Objects created in cloud storage can trigger other ETL services. This becomes
    useful when orchestrating data pipelines using these events.
  prefs: []
  type: TYPE_NORMAL
- en: In this scenario we would want to be able to read and write data in cloud storage
    used as a data lake for our data platform.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/851620512cf55064bfb03114e16ea2ab.png)'
  prefs: []
  type: TYPE_IMG
- en: Typical data pipeline. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: In this diagram, we can see that we extract and save our data into the datalake
    bucket first. Then it will trigger data warehouse data ingestion and load the
    data into our table for OLAP analytics using the Business intelligence (BI) tool.
  prefs: []
  type: TYPE_NORMAL
- en: This code snippet below explains how to save data using AWS SDK as a stream.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'In your command line run this to extract asteroids data from NASA API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This story is a summary of Python code techniques I use in ETL services almost
    every day. I hope you find it useful too. It helps to keep the code clean and
    execute data pipeline transformations efficiently. The serverless application
    model is a very cost-effective framework where we can deploy ETL microservices
    that cost almost nothing. We just need to optimise the memory usage and deploy
    them in an atomic manner so they run fast. It can handle almost any type of data
    pipeline for our data platform. A good summary of these architectural types and
    design patterns can be found in one of my previous stories.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----f3d5db59b6dd--------------------------------)
    [## Data Platform Architecture Types'
  prefs: []
  type: TYPE_NORMAL
- en: How well does it answer your business needs? Dilemma of a choice.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----f3d5db59b6dd--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Understanding basic HTTP methods is crucial in data engineering and it helps
    to create robust API interactions for our data pipelines. Pipelining our functions
    and models using `joblib` helps to write fast and efficient code. Pulling data
    from APIs using streams and running ETL tasks in a memory-efficient manner prevents
    resources from overprovisioning and ensures that our data services will never
    run out of memory. Unit tests can be run continuously using CI/CD tools. It helps
    to catch mistakes and human errors early before our code changes reach production
    environments. I hope you enjoyed reading this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recommended read:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] [https://stackoverflow.com/questions/519633/lazy-method-for-reading-big-file-in-python](https://stackoverflow.com/questions/519633/lazy-method-for-reading-big-file-in-python)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [https://docs.python.org/3/library/functions.html#property](https://docs.python.org/3/library/functions.html#property)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] [https://stackoverflow.com/questions/60343944/how-does-requests-stream-true-option-streams-data-one-block-at-a-time](https://stackoverflow.com/questions/60343944/how-does-requests-stream-true-option-streams-data-one-block-at-a-time)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] [https://cloud.google.com/dataform/reference/rest/v1beta1/projects.locations.repositories.workflowConfigs/create](https://cloud.google.com/dataform/reference/rest/v1beta1/projects.locations.repositories.workflowConfigs/create)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] [https://joblib.readthedocs.io/en/stable/persistence.html#persistence](https://joblib.readthedocs.io/en/stable/persistence.html#persistence)'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] [https://medium.com/towards-data-science/test-data-pipelines-the-fun-and-easy-way-d0f974a93a59](https://medium.com/towards-data-science/test-data-pipelines-the-fun-and-easy-way-d0f974a93a59)'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] [https://docs.python.org/3/library/tracemalloc.html](https://docs.python.org/3/library/tracemalloc.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] [https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316)'
  prefs: []
  type: TYPE_NORMAL
- en: '[9] [https://medium.com/towards-data-science/data-pipeline-design-patterns-100afa4b93e3](https://medium.com/towards-data-science/data-pipeline-design-patterns-100afa4b93e3)'
  prefs: []
  type: TYPE_NORMAL
