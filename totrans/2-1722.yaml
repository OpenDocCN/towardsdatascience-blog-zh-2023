- en: Python for Data Engineers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python 数据工程师
- en: 原文：[https://towardsdatascience.com/python-for-data-engineers-f3d5db59b6dd](https://towardsdatascience.com/python-for-data-engineers-f3d5db59b6dd)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/python-for-data-engineers-f3d5db59b6dd](https://towardsdatascience.com/python-for-data-engineers-f3d5db59b6dd)
- en: Advanced ETL techniques for beginners
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初学者的高级ETL技巧
- en: '[](https://mshakhomirov.medium.com/?source=post_page-----f3d5db59b6dd--------------------------------)[![💡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----f3d5db59b6dd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f3d5db59b6dd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f3d5db59b6dd--------------------------------)
    [💡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----f3d5db59b6dd--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mshakhomirov.medium.com/?source=post_page-----f3d5db59b6dd--------------------------------)[![💡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----f3d5db59b6dd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f3d5db59b6dd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f3d5db59b6dd--------------------------------)
    [💡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----f3d5db59b6dd--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f3d5db59b6dd--------------------------------)
    ·17 min read·Oct 21, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f3d5db59b6dd--------------------------------)
    ·阅读时间17分钟·2023年10月21日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/9c664876939298ade749c4c53cb490c8.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c664876939298ade749c4c53cb490c8.png)'
- en: Photo by [Boitumelo](https://unsplash.com/@writecodenow?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Boitumelo](https://unsplash.com/@writecodenow?utm_source=medium&utm_medium=referral)
    提供，来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: In this story I will speak about advanced data engineering techniques in Python.
    No doubt, Python is the most popular programming language for data. During my
    almost twelve-year career in data engineering, I encountered various situations
    when code had issues. This story is a brief summary of how I resolved them and
    learned to write better code. I will show a few techniques that make our ETL faster
    and help to improve the performance of our code.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个故事中，我将讲述Python中的高级数据工程技术。毫无疑问，Python是最受欢迎的数据编程语言。在我近十二年的数据工程职业生涯中，我遇到过各种代码问题。这个故事简要总结了我如何解决这些问题并学会写出更好的代码。我将展示一些使我们的ETL更快并有助于提高代码性能的技术。
- en: List comprehensions
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 列表推导式
- en: 'Imagine you are looping through a list of tables. Typically, we would do this:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下你正在遍历一个表的列表。通常，我们会这样做：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'But instead, we could use list comprehensions. Not only they are faster, they
    also reduce the code making it more concise:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们可以使用列表推导式。它们不仅更快，还减少了代码，使其更简洁：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For example, looping through a super large file with data to transform (ETL)
    each row has never been easier:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，循环处理一个超大的文件以转换（ETL）每一行，从未如此简单：
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'List comprehensions are extremely useful for ETL processing of large data files.
    Imagine a data file we need to transform into a newline delimited format. Try
    running this example in your Python environment:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 列表推导式对于ETL处理大数据文件非常有用。假设我们有一个需要转换为换行符分隔格式的数据文件。在你的Python环境中尝试运行这个示例：
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Output will be a **newline delimited JSON**. This is a standard format for
    data in BigQuery data warehouse and it is ready for loading into the table:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将是**换行符分隔的JSON**。这是BigQuery数据仓库中的一种标准格式，准备好加载到表中了：
- en: '[PRE4]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Generators
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成器
- en: 'If we are dealing with CSV and DAT files where data is **stored line by line**
    then our file object is already a **generator** of lines and we can use a list
    comprehension to process data **not consuming too much of our memory**:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们处理的是**逐行存储**的CSV和DAT文件，那么我们的文件对象已经是一个**生成器**，我们可以使用列表推导式来处理数据，**不会消耗太多内存**：
- en: '[PRE5]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Validating records before we actually insert them into a data warehouse table
    might be useful for batch data processing pipelines.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们实际将记录插入数据仓库表之前验证记录，对于批量数据处理管道可能是有用的。
- en: Often we need to validate data files before we load them into a data warehouse.
    If one record fails the whole batch will fail.
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们经常需要在将数据文件加载到数据仓库之前验证它们。如果一个记录失败，那么整个批次都会失败。
- en: 'We can use it to create close to real-time analytics pipelines. This is also
    a very cost-effective way to process data compared to streaming data pipeline
    design pattern. I previously wrote about it here:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用它来创建接近实时的分析管道。这也是一种非常经济高效的方式来处理数据，相比于流数据管道设计模式。我之前在这里写过：
- en: '[](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----f3d5db59b6dd--------------------------------)
    [## Data pipeline design patterns'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----f3d5db59b6dd--------------------------------)
    [## 数据管道设计模式'
- en: Choosing the right architecture with examples
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择合适的架构和示例
- en: towardsdatascience.com](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----f3d5db59b6dd--------------------------------)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----f3d5db59b6dd--------------------------------)'
- en: 'Alternatively, we could use `yield` when dealing with big data and when our
    file is not a newline delimited text. It is always a good practice as we would
    want to process the data file in a memory-efficient manner. For example:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，在处理大数据时，如果我们的文件不是换行符分隔的文本，我们可以使用`yield`。这始终是一个好习惯，因为我们希望以内存高效的方式处理数据文件。例如：
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This will read a local file and process it in chunks of 19 bytes. Output will
    be:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这将读取一个本地文件并以19字节为块进行处理。输出将是：
- en: '[PRE7]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: It’s just an example of how we can deal with **binary** data. In real life,
    it might be easier to split the file contents into segments using a **separator**,
    i.e. newline `'\n’` or `'}{'` depending on how our data was structured.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是处理**二进制**数据的一个示例。在现实生活中，将文件内容分割成段使用**分隔符**（即换行符`'\n’`或`'}{'`）可能会更容易，这取决于我们的数据结构。
- en: Imagine that the text data is pulled from some **external** location, i.e. from
    **cloud storage**. We also can process it as a **stream**. We wouldn’t want to
    load the whole data file and run `split('\n')` to process it line by line. It
    would consume a lot of memory. We can use `re.finditer` instead which acts like
    a **generator** and will read our data file in chunks so we’ll be able to run
    the required ETL **not** consuming too much memory.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 假设文本数据来自某个**外部**位置，即**云存储**。我们也可以将其处理为**流**。我们不希望加载整个数据文件并运行`split('\n')`逐行处理，这会消耗大量内存。我们可以使用`re.finditer`，它像**生成器**一样，以块的方式读取数据文件，这样我们就可以运行所需的ETL而**不会**消耗太多内存。
- en: '[PRE8]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Output:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE9]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Python **properties for data validation**
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python **数据验证属性**
- en: We can use **Python properties** [2] to validate data records. If a record is
    not an instance of a class that we defined then the exception must be raised.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用**Python属性** [2]来验证数据记录。如果记录不是我们定义的类的实例，则必须抛出异常。
- en: We can store our data as objects of the data class.
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们可以将数据存储为数据类的对象。
- en: It is as simple as that. Let’s imagine that we have a **streaming data pipeline**
    and we want to validate some fields in our records.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这么简单。假设我们有一个**流数据管道**，我们想要验证记录中的一些字段。
- en: Put it simply— they must match the existing table schema.
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 简单来说——它们必须匹配现有的表格模式。
- en: We can use Python properties for that. Consider this example below.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用Python属性来实现。请看下面的示例。
- en: '[PRE10]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: If we choose to break the rules and assign some values that don’t match our
    criteria, the exception will be thrown. For example, the exception will be raised
    if we try to call `ConnectionDataRecord('', 1)`
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们选择打破规则并分配一些不符合我们标准的值，则会抛出异常。例如，如果我们尝试调用`ConnectionDataRecord('', 1)`，将会抛出异常。
- en: Alternatively, we can use a library called `Pydantic` Consider this cde below.
    It will throw an error if we call the function with an object that doesn’t meet
    our requirements.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以使用一个名为`Pydantic`的库。请看下面的代码。如果我们用不符合要求的对象调用该函数，它将抛出一个错误。
- en: '[PRE11]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Decorators
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 装饰器
- en: 'Decorators were designed to make our code look leaner and to add extra functionality
    to it. We can simply pass one function as an argument into another function (decorator)
    and do some data transformation inside this wrapper. Imagine that we have many
    different ETL functions to process data but we need just one to upload the result
    into the data lake. This is how we do it:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 装饰器的设计目的是使我们的代码看起来更简洁，并为其添加额外的功能。我们可以简单地将一个函数作为参数传递给另一个函数（装饰器），并在这个包装器内部进行一些数据转换。想象一下，我们有很多不同的ETL函数来处理数据，但我们只需要一个将结果上传到数据湖的函数。这就是我们如何做到的：
- en: If some code logic repeats it is a good practice to use **a decorator**.
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果一些代码逻辑重复，使用**装饰器**是一个好习惯。
- en: It helps to maintain code base easier and saves a lot of time in case we need
    to change the repetitive logic.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这有助于更容易维护代码库，并节省了我们在需要更改重复逻辑时的很多时间。
- en: '[PRE12]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Decorators are used everywhere due to their effectiveness. Consider this Airflow
    DAG example:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 装饰器因其有效性被广泛使用。考虑这个Airflow DAG示例：
- en: '[PRE13]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Working with APIs
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与API的工作
- en: As a data engineer you would perform HTTP requests calling various API endpoints
    very often. Consider this example of GET request below.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据工程师，你会经常执行HTTP请求，调用各种API端点。下面是一个GET请求的示例。
- en: '[PRE14]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: It pulls some data from the **free** **NASA Asteroids API** and will return
    all asteroids close to Earth on that date. Just replace your API key in this URL
    path above or use the one I created. The **requests** library handles everything
    but there is a better way of doing it.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 它从**免费**的**NASA小行星API**中提取一些数据，并返回所有在该日期接近地球的小行星。只需在上面的URL路径中替换你的API密钥或使用我创建的密钥。**requests**库处理所有事情，但还有更好的方法。
- en: We can use session and process data from our API endpoint as a **stream**.
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们可以使用会话并以**流**的形式处理来自我们API端点的数据。
- en: 'That would ensure we won’t encounter any memory issues and process our GET
    request in a streaming manner [3]:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这将确保我们不会遇到任何内存问题，并以流式方式处理我们的GET请求[3]：
- en: '[PRE15]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Understanding how HTTP requests work is crucial in data engineering.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 理解HTTP请求的工作原理在数据工程中至关重要。
- en: I do all sorts of things with API requests daily and I don’t have to rely on
    any other frameworks or libraries.
  id: totrans-63
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我每天处理各种API请求，不必依赖其他框架或库。
- en: 'For example, just a couple of weeks ago I was working on the **Dataform migration
    project** and realised that the existing Google library (`from google.cloud import
    dataform_v1beta1`) can’t create schedules. The workaround was to use Dataform
    API [4] and it was as simple as POST request to that particular endpoint:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，就在几周前，我在**Dataform迁移项目**上工作，意识到现有的Google库（`from google.cloud import dataform_v1beta1`）无法创建调度。解决方法是使用Dataform
    API [4]，这就像向特定端点发出POST请求一样简单：
- en: '[PRE16]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The essence of this request is that we send `workflow_config` as **json** and
    add `workflowConfigId` in path parameters using this knowledge from Google documentation
    [4].
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这个请求的核心在于我们将`workflow_config`作为**json**发送，并使用来自Google文档[4]的知识在路径参数中添加`workflowConfigId`。
- en: This will create a required schedule to run our data transformation scripts
    in BigQuery’s Dataform.
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这将创建一个必要的调度，以在BigQuery的Dataform中运行我们的数据转换脚本。
- en: 'Similarly, as we did in GET request we can stream data into our POST API endpoint
    using Python **generators** like so:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，正如我们在GET请求中所做的那样，我们可以使用Python **生成器**将数据流入我们的POST API端点：
- en: '[PRE17]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The idea is clear. We can process and send data in a memory-efficient manner.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 思路很清楚。我们可以以节省内存的方式处理和发送数据。
- en: Handling API rate limits
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理API速率限制
- en: 'All APIs have rate limits and we would want to keep that in mind while fetching
    the data out of it. We can use decorators to handle it. Simple decoration can
    be implemented like this:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 所有API都有速率限制，我们在提取数据时要记住这一点。我们可以使用装饰器来处理它。简单的装饰可以像这样实现：
- en: '[PRE18]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Using this decorator our function won’t make more than 10 API calls within 15
    minutes.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个装饰器，我们的函数在15分钟内不会发起超过10次API调用。
- en: The most simple way to handle this situation is to use `time.sleep()` but Python
    ratelimit allows us to do it in this elegant manner.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 处理这种情况的最简单方法是使用`time.sleep()`，但Python速率限制允许我们以这种优雅的方式做到这一点。
- en: Async and `await` in Python
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python中的Async和`await`
- en: 'Performing ETL **asynchronously** is another extremely useful feature. We can
    use the `asyncio` library to run tasks simultaneously. Let consider this simple
    synchronous example where we process tables in `for` loop:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 以**异步**方式执行ETL是另一个极其有用的功能。我们可以使用`asyncio`库来同时运行任务。让我们考虑这个简单的同步示例，其中我们在`for`循环中处理表：
- en: '[PRE19]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Running this code we would have to wait for each table to finish the `pull_data()`
    task but with `Async`, we can process them in parallel.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这段代码时，我们必须等待每个表完成`pull_data()`任务，但使用`Async`，我们可以并行处理它们。
- en: 'Consider using this code instead:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑使用以下代码：
- en: '[PRE20]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: It will pull data from reporting APIs simultaneously and it will significantly
    improve our ETL performance.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 它将同时从报告API中提取数据，并显著提高我们的ETL性能。
- en: It helps to manage ETL tasks easier while system resources are being distributed
    in the best way possible.
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 它帮助管理ETL任务，同时系统资源以最佳方式分配。
- en: 'For example, we can run two ETL jobs simultaneously but then we can define
    the order of execution:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以同时运行两个ETL作业，但我们可以定义执行顺序：
- en: '[PRE21]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Use Map and Filter
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Map和Filter
- en: Map and filter work even faster than list comprehensions.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 映射和过滤比列表推导式的速度更快。
- en: 'We can transform our data line by line aplying `map` function to items in our
    dataset prrocessing it as an `iterable`:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以逐行转换数据，将`map`函数应用于数据集中的项目，将其处理为`iterable`：
- en: '[PRE22]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We would want to use `filter` to extract objects matching a certain criteria,
    i.e.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望使用`filter`来提取符合特定条件的对象，即
- en: '[PRE23]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Process large datasets using Pandas
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Pandas 处理大型数据集
- en: 'Later versions of Pandas library provide a handy context manager that can be
    used like so:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 后来的 Pandas 库版本提供了一个方便的上下文管理器，可以像这样使用：
- en: '[PRE24]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: It will process data in batch mode assuming we don’t need to load the whole
    dataset into a dataframe at one time.
  id: totrans-95
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 它将以批处理模式处理数据，假设我们不需要一次性将整个数据集加载到数据框中。
- en: 'It has a wide range of applications from OLAP reporting to Machine learning
    (ML) pipelines, for instance, we might want to create a recommendation model trainer
    task and would need to prepare a dataset like so:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 它有广泛的应用，从 OLAP 报告到机器学习（ML）管道。例如，我们可能想要创建一个推荐模型训练任务，并需要像这样准备数据集：
- en: '[PRE25]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This way Pandas will ensure our application always has enough memory to process
    data.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，Pandas 将确保我们的应用程序始终有足够的内存来处理数据。
- en: Use joblib for pipelining and parallel computation
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 joblib 进行管道处理和并行计算
- en: '`joblib.dump()` and `joblib.load()` methods allow us to pipeline large dataset
    transformations efficiently. `joblib` will store and serialise big data working
    with arbitrary Python objects such as `numpy` arrays.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`joblib.dump()`和`joblib.load()`方法允许我们高效地管道大型数据集转换。`joblib`将存储和序列化大数据，处理任意 Python
    对象，如`numpy`数组。'
- en: What do you think `scikit-learn` uses to save and load ML models? The correct
    answer is - `joblib`.
  id: totrans-101
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你认为`scikit-learn`用什么来保存和加载机器学习模型？正确的答案是 - `joblib`。
- en: First of all, why save a model? — Simply because we might need it later further
    down the pipeline, i.e. to make predictions using new data, etc.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，为什么要保存模型？——简单来说，因为我们可能在管道后面需要它，即使用新数据进行预测等。
- en: We wouldn’t want to retrain our ML model as it is a very time-consuming task.
  id: totrans-103
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们不希望重新训练我们的机器学习模型，因为这是一个非常耗时的任务。
- en: 'Another reason is that we might want to save different versions of the same
    model to see which one performs better. `joblib` helps to do all that [5]:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个原因是我们可能希望保存相同模型的不同版本，以便查看哪个版本表现更好。`joblib`有助于完成所有这些工作[5]：
- en: '[PRE26]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'These functions explicitly connect the file we save on disk and the execution
    context of the original Python object. So instead of file names `joblib` also
    accepts file objects:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数明确连接了我们在磁盘上保存的文件和原始 Python 对象的执行上下文。因此，除了文件名，`joblib`还接受文件对象：
- en: '[PRE27]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '**AWS S3 model dump/load example:**'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**AWS S3 模型转储/加载示例：**'
- en: '[PRE28]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Parallel computing with joblib
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 joblib 进行并行计算
- en: 'It is very efficient because it relies on multiprocessing and will execute
    tasks concurrently using multiple Python workers on all CPU cores or across multiple
    machines. Consider this example:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常高效，因为它依赖于多进程，并且会使用多个 Python 工作者在所有 CPU 核心上或跨多台机器并发执行任务。考虑这个例子：
- en: '[PRE29]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We can use all CPU cores we have to unlock full potential of our hardware.
  id: totrans-113
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们可以利用所有 CPU 核心来释放硬件的全部潜力。
- en: Here we tell `Parallel` to use all cores (-1) and the computation runs **5 times
    faster:**
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们告诉`Parallel`使用所有核心（-1），计算速度**提高了 5 倍：**
- en: '[PRE30]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Unit test ETL pipelines
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单元测试 ETL 管道
- en: One of the most important picks I’ve learned throughout my data engineering
    career is that everything must be unit-tested. That includes not only **SQL**
    but **ETL jobs** and **integrations** with other services we use on our data pipelines.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在我整个数据工程师职业生涯中，我学到的最重要的一点是所有东西都必须进行单元测试。这不仅包括**SQL**，还包括**ETL 作业**和与我们数据管道中使用的其他服务的**集成**。
- en: 'We can use the `unittest` Pyhton library to test our code. Let’s imagine we
    have a helper module that was created to check a number if the number is a prime
    number:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`unittest` Python 库来测试我们的代码。假设我们有一个助手模块，用于检查一个数字是否是素数：
- en: '[PRE31]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: How do we test the logic inside this function?
  id: totrans-120
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们如何测试这个函数内部的逻辑？
- en: '`unittest` makes it really simple:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`unittest`使这一切变得非常简单：'
- en: '[PRE32]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now if we run this in our command line we will test the logic:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在如果我们在命令行中运行这个，我们将测试逻辑：
- en: '[PRE33]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This is correct because 13 is a prime number. Let’s test is further. We know
    that 4 is not a prime number and, therefore, we would want our unit test for this
    particular function to return pass while asserting False:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这是正确的，因为13是一个素数。让我们进一步测试一下。我们知道4不是一个素数，因此我们希望针对这个特定函数的单元测试在断言为 False 时返回通过：
- en: '[PRE34]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Simple. Let’s take a look at a more advanced example.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 很简单。让我们看一个更高级的示例。
- en: Let’s imagine we have an ETL service that pulls data from some API and it takes
    a lot of time. Then our service will transform this dataset and we would like
    to test that this ETL transformation logic persists.
  id: totrans-129
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 让我们假设我们有一个 ETL 服务，从某个 API 中提取数据，这需要很多时间。然后我们的服务将转换这个数据集，我们希望测试这个 ETL 转换逻辑是否持续存在。
- en: How do we do it?
  id: totrans-130
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们该怎么做呢？
- en: We can use mock and patch methods from the `unittest` library. Consider this
    application file `asteroids.py`
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`unittest`库中的 mock 和 patch 方法。考虑这个应用程序文件`asteroids.py`
- en: '[PRE36]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'If we run app.py the output will have a list of asteroids that are close to
    Earth on that particular date:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们运行 app.py，输出将会是列出在特定日期接近地球的小行星：
- en: '[PRE37]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Pulling data from API might take a lot of time but we would want our unit test
    to run fast. We can **mock** some fake API response into our `get_data()` function
    and then use it to test ETL logic in `save_data()` function:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 从 API 中提取数据可能需要很多时间，但我们希望我们的单元测试运行得更快。我们可以**模拟**一些假的 API 响应到我们的`get_data()`函数中，然后使用它来测试
    `save_data()`函数中的 ETL 逻辑：
- en: '[PRE38]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output will be:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将是：
- en: '[PRE39]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: In our unit test, we replaced (using `mock`) values returned by `asteroids.get_data`
    function and expected them to be transformed into (ETL) `[‘asteroid_1’, ‘asteroid_2’]`
    which our ETL function failed to do. Unit test failed.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的单元测试中，我们替换了（使用`mock`）`asteroids.get_data`函数返回的值，并期望它们被转换为（ETL）`['asteroid_1',
    'asteroid_2']`，而我们的 ETL 函数未能做到这一点。单元测试失败了。
- en: Unit tests are very powerful.
  id: totrans-140
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 单元测试是非常强大的。
- en: 'It helps us to deal with human errors while deploying new features in ETL pipelines.
    Some more advanced examples can be found in one of my previous stories. I use
    it very often in CI/CD pipelines [6]:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 它帮助我们处理在 ETL 管道中部署新功能时的人为错误。更多高级示例可以在我之前的故事中找到。我在 CI/CD 管道中非常频繁地使用它 [6]：
- en: '[](/test-data-pipelines-the-fun-and-easy-way-d0f974a93a59?source=post_page-----f3d5db59b6dd--------------------------------)
    [## Test Data Pipelines the Fun and Easy Way'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/test-data-pipelines-the-fun-and-easy-way-d0f974a93a59?source=post_page-----f3d5db59b6dd--------------------------------)
    [## 以有趣和简单的方式测试数据管道'
- en: 'Beginners guide: Why unit and integration tests are so important for your data
    platform'
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 初学者指南：为什么单元测试和集成测试对你的数据平台如此重要
- en: towardsdatascience.com](/test-data-pipelines-the-fun-and-easy-way-d0f974a93a59?source=post_page-----f3d5db59b6dd--------------------------------)
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/test-data-pipelines-the-fun-and-easy-way-d0f974a93a59?source=post_page-----f3d5db59b6dd--------------------------------)'
- en: Monitoring memory usage
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控内存使用情况
- en: Often I deploy ETL microservices using serverless. It’s a very neat a cost-effective
    tool. I deploy Lambdas and Cloud Functions and wouldn’t want to overprovision
    them with excessive memory.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我经常使用无服务器部署 ETL 微服务。这是一种非常整洁且具有成本效益的工具。我部署 Lambdas 和 Cloud Functions，不希望它们因内存过多而被过度配置。
- en: 'I previously wrote about it here:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前在这里写过：
- en: '[](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----f3d5db59b6dd--------------------------------)
    [## Infrastructure as Code for Beginners'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 初学者的基础设施即代码](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----f3d5db59b6dd--------------------------------)'
- en: Deploy Data Pipelines like a pro with these templates
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用这些模板像专业人士一样部署数据管道
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----f3d5db59b6dd--------------------------------)
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[levelup.gitconnected.com](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----f3d5db59b6dd--------------------------------)'
- en: Indeed, why would we give our Lambda 3Gb of memory and pay more when the data
    can be processed at 256Mb?
  id: totrans-151
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 确实，我们为什么要给我们的 Lambda 配置 3Gb 的内存并支付更多费用，而数据可以在 256Mb 的内存中处理？
- en: There are various ways to monitor our ETL application memory usage. One of the
    most popular is `tracemalloc` [7] library.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以监控我们的 ETL 应用程序内存使用情况。其中一种最受欢迎的方法是`tracemalloc` [7]库。
- en: 'It can trace Python memory blocks and return the results in (<current>, <peak
    memory>) format in bytes. Consider this example to extract data from asteroid
    API in one big chunk and save it to the disk:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以跟踪 Python 内存块，并以（<current>, <peak memory>）字节格式返回结果。考虑这个例子，从小行星 API 中提取数据到一个大块中并保存到磁盘：
- en: '[PRE40]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The output will be:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将是：
- en: '[PRE41]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '**We can see that peak usage is around 540Kb.**'
  id: totrans-157
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**我们可以看到峰值使用量约为 540Kb。**'
- en: 'Let’s see how it can be optimised simply by using `stream` :'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何通过使用`stream`来进行简单的优化：
- en: '[PRE42]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '**We can see that peak memory usage is twice lower.**'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们可以看到峰值内存使用量减少了一半。**'
- en: Working with SDKs
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 SDK
- en: As a data engineer, we would work with cloud service providers very often. In
    a nutshell, SDKs are collections of service libraries that allow programmatic
    access to cloud services. We would want to learn and master at least one or two
    SDKs from market leaders such as Amazon, Azure or Google.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据工程师，我们通常需要与云服务提供商频繁合作。简而言之，SDK 是一组服务库，允许以编程方式访问云服务。我们希望学习并掌握市场领导者如 Amazon、Azure
    或 Google 的一两个 SDK。
- en: One of the services I use very often using programmatic access is Cloud Storage.
    Indeed in data engineering, almost every data pipeline relies on data storage
    in the cloud, i.e. Google Cloud Storage or AWS S3.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我经常以编程方式访问的服务之一是 Cloud Storage。实际上，在数据工程中，几乎每个数据管道都依赖于云中的数据存储，即 Google Cloud
    Storage 或 AWS S3。
- en: The most common data pipeline design would be the one created around a data
    bucket. I described this pattern in one of my previous stories [9].
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的数据管道设计是围绕数据存储桶创建的。我在之前的故事中描述了这一模式 [9]。
- en: '[](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----f3d5db59b6dd--------------------------------)
    [## Data pipeline design patterns'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----f3d5db59b6dd--------------------------------)
    [## 数据管道设计模式'
- en: Choosing the right architecture with examples
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择合适的架构及示例
- en: towardsdatascience.com](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----f3d5db59b6dd--------------------------------)
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----f3d5db59b6dd--------------------------------)'
- en: Objects created in cloud storage can trigger other ETL services. This becomes
    useful when orchestrating data pipelines using these events.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在云存储中创建的对象可以触发其他 ETL 服务。这在使用这些事件编排数据管道时变得非常有用。
- en: In this scenario we would want to be able to read and write data in cloud storage
    used as a data lake for our data platform.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们希望能够在用作数据平台的数据湖的云存储中读取和写入数据。
- en: '![](../Images/851620512cf55064bfb03114e16ea2ab.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/851620512cf55064bfb03114e16ea2ab.png)'
- en: Typical data pipeline. Image by author
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的数据管道。作者提供的图像
- en: In this diagram, we can see that we extract and save our data into the datalake
    bucket first. Then it will trigger data warehouse data ingestion and load the
    data into our table for OLAP analytics using the Business intelligence (BI) tool.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图示中，我们可以看到我们首先将数据提取并保存到数据湖存储桶中。然后，它将触发数据仓库的数据摄取，并将数据加载到我们的表中，以便使用商业智能（BI）工具进行
    OLAP 分析。
- en: This code snippet below explains how to save data using AWS SDK as a stream.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码片段解释了如何使用 AWS SDK 以流的形式保存数据。
- en: '[PRE44]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'In your command line run this to extract asteroids data from NASA API:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的命令行中运行以下命令以从 NASA API 提取小行星数据：
- en: '[PRE45]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Conclusion
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: This story is a summary of Python code techniques I use in ETL services almost
    every day. I hope you find it useful too. It helps to keep the code clean and
    execute data pipeline transformations efficiently. The serverless application
    model is a very cost-effective framework where we can deploy ETL microservices
    that cost almost nothing. We just need to optimise the memory usage and deploy
    them in an atomic manner so they run fast. It can handle almost any type of data
    pipeline for our data platform. A good summary of these architectural types and
    design patterns can be found in one of my previous stories.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这个故事总结了我在 ETL 服务中几乎每天使用的 Python 代码技术。我希望你也能发现它有用。它有助于保持代码的整洁，并高效地执行数据管道转换。无服务器应用模型是一个非常具有成本效益的框架，我们可以在其中部署几乎不花费任何费用的
    ETL 微服务。我们只需要优化内存使用，并以原子方式部署它们，以便它们运行得更快。它几乎可以处理我们数据平台中的任何类型的数据管道。在我之前的故事中可以找到这些架构类型和设计模式的良好总结。
- en: '[](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----f3d5db59b6dd--------------------------------)
    [## Data Platform Architecture Types'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----f3d5db59b6dd--------------------------------)
    [## 数据平台架构类型'
- en: How well does it answer your business needs? Dilemma of a choice.
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它在多大程度上满足你的业务需求？选择的困境。
- en: towardsdatascience.com](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----f3d5db59b6dd--------------------------------)
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----f3d5db59b6dd--------------------------------)'
- en: Understanding basic HTTP methods is crucial in data engineering and it helps
    to create robust API interactions for our data pipelines. Pipelining our functions
    and models using `joblib` helps to write fast and efficient code. Pulling data
    from APIs using streams and running ETL tasks in a memory-efficient manner prevents
    resources from overprovisioning and ensures that our data services will never
    run out of memory. Unit tests can be run continuously using CI/CD tools. It helps
    to catch mistakes and human errors early before our code changes reach production
    environments. I hope you enjoyed reading this.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 理解基本的 HTTP 方法在数据工程中至关重要，它有助于为我们的数据管道创建稳健的 API 交互。使用`joblib`对我们的函数和模型进行管道化可以编写快速高效的代码。通过流的方式从
    API 拉取数据，并以内存高效的方式运行 ETL 任务，可以防止资源过度分配，并确保我们的数据服务不会耗尽内存。可以使用 CI/CD 工具持续运行单元测试，这有助于在我们的代码更改达到生产环境之前，及早发现错误和人为失误。希望你喜欢阅读这篇文章。
- en: 'Recommended read:'
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推荐阅读：
- en: '[1] [https://stackoverflow.com/questions/519633/lazy-method-for-reading-big-file-in-python](https://stackoverflow.com/questions/519633/lazy-method-for-reading-big-file-in-python)'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [https://stackoverflow.com/questions/519633/lazy-method-for-reading-big-file-in-python](https://stackoverflow.com/questions/519633/lazy-method-for-reading-big-file-in-python)'
- en: '[2] [https://docs.python.org/3/library/functions.html#property](https://docs.python.org/3/library/functions.html#property)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [https://docs.python.org/3/library/functions.html#property](https://docs.python.org/3/library/functions.html#property)'
- en: '[3] [https://stackoverflow.com/questions/60343944/how-does-requests-stream-true-option-streams-data-one-block-at-a-time](https://stackoverflow.com/questions/60343944/how-does-requests-stream-true-option-streams-data-one-block-at-a-time)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [https://stackoverflow.com/questions/60343944/how-does-requests-stream-true-option-streams-data-one-block-at-a-time](https://stackoverflow.com/questions/60343944/how-does-requests-stream-true-option-streams-data-one-block-at-a-time)'
- en: '[4] [https://cloud.google.com/dataform/reference/rest/v1beta1/projects.locations.repositories.workflowConfigs/create](https://cloud.google.com/dataform/reference/rest/v1beta1/projects.locations.repositories.workflowConfigs/create)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [https://cloud.google.com/dataform/reference/rest/v1beta1/projects.locations.repositories.workflowConfigs/create](https://cloud.google.com/dataform/reference/rest/v1beta1/projects.locations.repositories.workflowConfigs/create)'
- en: '[5] [https://joblib.readthedocs.io/en/stable/persistence.html#persistence](https://joblib.readthedocs.io/en/stable/persistence.html#persistence)'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] [https://joblib.readthedocs.io/en/stable/persistence.html#persistence](https://joblib.readthedocs.io/en/stable/persistence.html#persistence)'
- en: '[6] [https://medium.com/towards-data-science/test-data-pipelines-the-fun-and-easy-way-d0f974a93a59](https://medium.com/towards-data-science/test-data-pipelines-the-fun-and-easy-way-d0f974a93a59)'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] [https://medium.com/towards-data-science/test-data-pipelines-the-fun-and-easy-way-d0f974a93a59](https://medium.com/towards-data-science/test-data-pipelines-the-fun-and-easy-way-d0f974a93a59)'
- en: '[7] [https://docs.python.org/3/library/tracemalloc.html](https://docs.python.org/3/library/tracemalloc.html)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] [https://docs.python.org/3/library/tracemalloc.html](https://docs.python.org/3/library/tracemalloc.html)'
- en: '[8] [https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] [https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316)'
- en: '[9] [https://medium.com/towards-data-science/data-pipeline-design-patterns-100afa4b93e3](https://medium.com/towards-data-science/data-pipeline-design-patterns-100afa4b93e3)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] [https://medium.com/towards-data-science/data-pipeline-design-patterns-100afa4b93e3](https://medium.com/towards-data-science/data-pipeline-design-patterns-100afa4b93e3)'
