# 高级提示工程

> 原文：[https://towardsdatascience.com/advanced-prompt-engineering-f07f9e55fe01](https://towardsdatascience.com/advanced-prompt-engineering-f07f9e55fe01)

## 当少样本学习不足时该怎么办……

[](https://wolfecameron.medium.com/?source=post_page-----f07f9e55fe01--------------------------------)[![Cameron R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----f07f9e55fe01--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f07f9e55fe01--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f07f9e55fe01--------------------------------) [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----f07f9e55fe01--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f07f9e55fe01--------------------------------) ·阅读时间 17 分钟·2023 年 8 月 7 日

--

![](../Images/c81386dd2900922e3779e5a589170682.png)

（图片由 [Mike Tinnion](https://unsplash.com/@m15ky?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 提供，来自 [Unsplash](https://unsplash.com/photos/3ym6i13Y9LU?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)）

大型语言模型（LLMs）的普及彻底改变了我们解决问题的方式。在以前，使用计算机解决任何任务（例如，重新格式化文档或分类句子）都需要创建一个程序（即一组根据某种编程语言精确编写的命令）。有了 LLM，解决此类问题只需要一个文本提示。例如，我们可以通过类似下面所示的提示来指导 LLM 重新格式化任何文档。

![](../Images/f0e2c7c36f265458b3c1eaa3d0a17396.png)

使用提示来重新格式化 XML 文档（作者创作）

如上例所示，LLM 的通用文本到文本格式使我们能够轻松解决各种问题。我们首先通过 [GPT-3](https://cameronrwolfe.substack.com/p/language-model-scaling-laws-and-gpt) [18] 的提议看到了这一潜力，显示了足够大的语言模型可以使用 [少样本学习](https://cameronrwolfe.substack.com/i/117151147/few-shot-learning) 以惊人的准确性解决许多任务。然而，随着围绕 LLM 的研究进展，我们开始超越这些基本的（但仍然非常有效！）提示技术，如零样本/少样本学习。

[指令跟随 LLM](https://cameronrwolfe.substack.com/i/117151147/instruction-prompting)（例如，[InstructGPT](https://cameronrwolfe.substack.com/i/93578656/training-language_models_to_follow_instructions_with_human_feedback) 和 [ChatGPT](https://openai.com/blog/chatgpt)）使我们探索语言模型是否能解决真正困难的任务。我们希望将 LLM 用于不仅仅是玩具问题。为了实际有用，LLM 需要能够遵循复杂指令并进行多步骤推理，以正确回答人类提出的困难问题。不幸的是，这类问题往往不能通过基本的提示技巧来解决。为了引发 LLM 的复杂问题解决行为，我们需要更复杂的方法。

![](../Images/9a436cca569569781c24bca3ee30082f.png)

（来自 [1, 2, 4, 7]）

# 扩展可能性范围…

![](../Images/82fd27178f3ef4edbd63b3476efaa691.png)

（由作者创作）

在之前的文章中，我们学习了 LLM 的一些基本提示方法，如零/少量学习和指令提示。理解这些实际的提示技巧对掌握这里将介绍的更高级的提示程序非常重要。有关这些技术的更多细节，请查看[此处](https://cameronrwolfe.substack.com/p/practical-prompt-engineering-part)的概述！

**更好的提示 → 更好的结果。** 这些技术可以用来在 LLM 上实现许多目标（前提是正确应用）。然而，它们可能由于各种原因而有所欠缺。少量学习需要大多数 LLM 的[有限上下文窗口](https://cameronrwolfe.substack.com/i/117151147/what-is-prompt-engineering)被示例占据，如果没有设置保护措施，LLM 可能会被引导产生有害输出，而且大多数模型在解决推理任务或遵循多步骤指令方面表现较差。鉴于这些局限，*我们应该如何继续尝试用 LLM 解决困难任务？*

一种方法是创建更强大的 LLM，无论是[从头开始](https://twitter.com/cwolferesearch/status/1635693551584522256?s=20)还是通过更好的[改进程序](https://cameronrwolfe.substack.com/i/93578656/training-language-models-to-follow_instructions_with_human_feedback)。然而，这需要付出很多努力！*如果我们能让现有模型在问题解决上更出色呢？* 在这篇文章中，我们将探讨更高级的提示工程形式（例如，思维链提示、自动提示工程、信息检索等），这些方法可以提高 LLM 的性能并引发更复杂的问题解决行为。这些想法很重要，因为它们拓宽了 LLM 的应用范围。例如，通过使用这些技术，我们可以：

+   让 LLM 访问外部知识数据库。

+   使复杂的、基于推理的问题得到解决。

+   通过允许模型存储和访问对话中的先前信息，为LLM提供无限的记忆。

**提示工程正在不断发展。** 本概述将重点介绍提示工程最近进展的高层次视角。我们将集中了解可能有用的不同提示技巧，而不是深入探讨各个方法。然而，需要注意的是，提示工程这一主题既新颖又迅速发展。几乎每天都有新研究发布，许多前沿思想只是[在线分享](https://github.com/openai/openai-cookbook)而未正式出版。因此，该主题在未来几个月内可能会发生重大变化，从而扩展LLMs能够解决的问题。

## 理解LLMs

由于该概述专注于提示，因此不会解释语言模型的[历史](https://twitter.com/cwolferesearch/status/1639378997627826176?s=20)或[机制](https://twitter.com/cwolferesearch/status/1635693551584522256?s=20)。为了更好地理解语言模型（这对于深入理解提示是一个重要的前提），我已经编写了各种概述，这些概述如下所示（按重要性排序）：

+   语言建模基础（GPT和GPT-2）[[link](https://cameronrwolfe.substack.com/p/language-models-gpt-and-gpt-2)]

+   语言模型规模的重要性（GPT-3）[[link](https://cameronrwolfe.substack.com/p/language-model-scaling-laws-and-gpt)]

+   现代[[link](https://cameronrwolfe.substack.com/p/modern-llms-mt-nlg-chinchilla-gopher)]和专业[[link](https://cameronrwolfe.substack.com/p/specialized-llms-chatgpt-lamda-galactica)] LLMs

+   [PaLM](https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive)，T5（第[一部分](https://cameronrwolfe.substack.com/p/t5-text-to-text-transformers-part)和[二部分](https://cameronrwolfe.substack.com/p/t5-text-to-text-transformers-part-354)），LLaMA（第[一部分](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone)和[二部分](https://cameronrwolfe.substack.com/p/beyond-llama-the-power-of-open-llms)）

# 高级提示技巧

我们现在将讨论提示工程领域的三个影响力话题。首先，我们将了解如何利用思维链提示，包括一些显著的扩展和变体，来改善LLMs的推理能力。接下来，我们将讨论LLMs与外部数据库的集成，允许将相关、准确的信息注入到每个提示中。最后，我们将学习如何使用自动化提示工程方法从数据中发现更好的提示。

## 思维链提示及其扩展

我们在之前的帖子中覆盖了链式思维（CoT）提示的主要思想 [1] 及其几个流行变体。欲了解详细信息，请查看链接中的概述 [这里](https://cameronrwolfe.substack.com/p/chain-of-thought-prompting-for-llms)。

**什么是 CoT 提示？** CoT 提示是一种简单的技术，用于提高 LLM 在常识或符号推理等推理任务上的表现。CoT 提示通过在提示中插入几个解决推理问题的示例来利用少量学习。每个示例都配有一个思维链（或推理过程），该过程通过逐步解释如何解决问题来增强问题的答案；见下文。

![](../Images/b39b50a667079c6a74b854117c20aec4.png)

（来自 [1]）

由于其少量学习能力，LLMs 可以通过观察 CoT 提示中的示例来学习生成推理过程。以这种方式生成准确的推理过程已被证明可以提高推理性能 [10, 11]，我们在 CoT 提示的实验中也确实看到了这一效果。即，教会 LLM 输出一个相关的思维链来解释其最终答案，可以显著提高在算术、符号和常识推理等任务上的表现；见下文。

![](../Images/974e6497204cdab17cc36bd7c38b08fd.png)

（来自 [9]）

**流行的 CoT 变体。** 除了基本的 CoT 提示外，还有几个该技术的[变体](https://cameronrwolfe.substack.com/i/116166267/variants-of-cot-prompting)被探讨，如下所示：

+   *零样本 CoT 提示 [13]:* 替换所有示例推理过程，而是在提示的末尾注入“让我们一步一步思考”这一声明。

+   *自我一致性 [14]:* 使用 LLM 生成多个思维链，并将这些多个输出的多数投票作为最终答案。

+   *最少到最多提示 [15]:* 将推理问题分解为逐步解决的较小步骤，每个子问题的输出作为下一个问题的输入。

这些技术（如下面的图所示）与 CoT 提示类似，产生了可比较的结果，但它们各自具有独特的优点。例如，零样本 CoT 提示非常简单！我们只需在提示中插入一个声明，而不是手动编写或策划多个相关的思维链。另一方面，最少到最多提示比原始的 CoT 提示稍微复杂一些，但该技术也更能解决需要多个步骤的推理问题。因此，我们可以使用最少到最多提示来解决那些 CoT 提示难以应对的最困难任务。

![](../Images/a557024afdffc8dcc3983763969f49ef.png)

（来自 [13, 14, 15]）

在这些技术中，自一致性是我个人的最爱。为什么？因为它是一种简单的技术，广泛适用且非常有效。实际上，这个想法甚至不是 CoT 提示特有的！自一致性可以在许多情况下提高 LLM 应用的性能。我们生成多个输出并取它们的平均值作为最终答案，从而提高了 [可靠性](https://github.com/dair-ai/Prompt-Engineering-Guide/blob/main/guides/prompts-reliability.md) 和准确性。

这个想法让我想起了深度学习中的[模型集成](/ensemble-methods-in-machine-learning-what-are-they-and-why-use-them-68ec3f9fef5f)，在这种方法中，我们 *i)* 独立训练多个模型来解决某个任务，并且 *ii)* 在推理时取每个模型输出的平均值。尽管自一致性只使用单个模型而不是集成模型，但类似的技术已经在更广泛的深度学习文献中得到应用；例如，为了模拟集成，可以生成并平均来自包含非确定性模块（如 dropout）的神经网络的多个输出 [19, 20]。

**扩展 CoT 提示。** CoT 提示是否真的教会了 LLMs 如何“推理”仍不清楚。尽管如此，CoT 提示具有重要的实际意义，因为它可以用于解决复杂的多步骤问题。因此，最近围绕 CoT 提示的一些有趣的想法已被探讨。在 [16] 中，探索了 CoT 提示的多模态版本，其中同时使用图像和文本模态来执行不同的推理任务；见下文。

![](../Images/f9752c741a02b4ec2ec773ee41ab97a4.png)

（来自 [16]）

除了探索多种数据模态（即图像和文本）外，[16] 的作者通过将多步骤推理生成和答案推断视为解决基于推理的任务的两个不同步骤，稍微调整了 CoT 设置；见下文。

![](../Images/7a28d1abae2f949c19d33289a5699512.png)

（来自 [16]）

通过明确隔离这些组件，我们可以更容易地分析 CoT 提示中的错误来源。因此，[16] 的作者发现 *i)* 错误答案通常是由于生成的推理中的幻觉造成的，*ii)* 使用多模态数据可以生成更有效的推理。

![](../Images/a7909060f15cfb3130e49779f6c8f8fa.png)

（来自 [17]）

更进一步，[17]中的作者将CoT提示与[主动学习](https://jacobgil.github.io/deeplearning/activelearning)（即使用模型本身识别应包含在训练集中的数据）的理念结合起来。LLM首先使用CoT提示回答若干问题。从这里开始，输出“**不确定性**”（基于同一LLM生成的多个答案之间的分歧来衡量）用于识别模型理解较差的问题。然后，这些问题组中的问题将由人工标注（由人类）正确的思维链，并用作解决未来问题的示例。

我们在实践中应用CoT提示时可能遇到的最大问题之一是缺乏与我们试图解决的任务良好对齐的少量示例。也许我们可以访问多个高质量的思维链来包含在我们的提示中，*但如果我们试图解决的问题与这些示例中的问题略有不同，该怎么办？* 尽管这种问题可能导致性能下降，但[17]中提出的方法旨在应对这一问题。即，我们可以使用主动学习动态识别何时用于CoT提示的现有示例不足以解决某个问题。

## 知识增强

尽管LLMs在[预训练](https://cameronrwolfe.substack.com/i/85568430/language-modeling)过程中学到了大量信息，但用额外的相关信息增强它们的提示通常是有帮助的。这种方法可以通过在LLM的提示中提供准确的信息来源，作为生成输出时的上下文，来帮助解决诸如[幻觉](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence))（即生成不正确的事实）等问题。虽然有多种方法可以实现这一点，但我们将重点关注基于信息检索和生成知识的技术。

![](../Images/ad9878ade279b24b3048ba7b88670cef.png)

（来自 [2]）

**信息检索。** LLM社区近期对向量数据库技术（例如，[Pinecone](https://www.pinecone.io/)，[Milvus](https://milvus.io/)，[Weaviate](https://weaviate.io/)等）给予了重视，原因在于其在执行[信息检索](https://mattboegner.com/knowledge-retrieval-architecture-for-llms/)中的作用；见上文。从高层次来看，信息检索的目标是使LLMs能够访问大量文本信息（超出最大[上下文窗口](https://cameronrwolfe.substack.com/i/117151147/what-is-prompt-engineering)），通过：

1.  将文本分割成小部分。

1.  为每个文本块生成[嵌入](https://platform.openai.com/docs/guides/embeddings)。

1.  将这些嵌入存储在向量数据库中。

1.  执行[向量相似性搜索](https://www.pinecone.io/learn/what-is-similarity-search/)（基于这些嵌入）以找到相关的文本块以包含在提示中。

最终结果是，我们可以迅速找到相关的文本信息作为LLM提示中的额外上下文。这种方法甚至可以与[链式思维提示](https://cameronrwolfe.substack.com/p/chain-of-thought-prompting-for-llms)结合，指导检索过程以获取新的和有用的信息 [2]。

![](../Images/35cba62dbf07231f8160f33bd011a863.png)

（来自 [1]）

**生成的知识。** 信息检索功能强大（即，它能访问几乎无限的信息量！），但我们可能会疑问：*外部向量数据库是否完全必要？* 有趣的是，近期的研究 [1] 表明答案可能是否定的！我们可以通过提示另一个LLM生成信息来提高LLM性能；见上文。特别地，我们可以通过提示LLM以各种主题的知识生成示例，并以请求生成关于所需主题的有用上下文来使用少量学习；见下文。

![](../Images/017aa16a8f8ec032691b2b31f6e7381c.png)

（来自 [1]）

从这里，我们可以在生成预测时将生成的信息作为额外的上下文输入。尽管不依赖任何外部数据库，这种方法可以显著提高LLM在几个常识推理任务上的表现；见下文。

![](../Images/12f8679c24f4a8c694ee56f1f853fbe6.png)

（来自 [1]）

生成的知识对于假设理解世界常识的任务（如常识推理）最为有用。简单来说，只要小心使用并用于正确的任务，LLMs 是良好的信息来源。

> *“生成知识提示突显了大型语言模型作为改善常识推理的灵活外部知识来源”* — 来自 [1]

## 自动提示

提示工程的目标是调整输入以最大化模型提供正确结果的机会。考虑到这一点，我们甚至可以将提示视为一组[可训练参数](https://datascience.stackexchange.com/questions/17635/model-parameters-hyper-parameters-of-neural-network-their-tuning-in-training)，可以进行更新（例如，使用[梯度下降](https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=2)或其他数据驱动标准）以生成正确答案。基于数据自动更新提示的想法相当通用，但近期的研究成功探索了几种这样的技术。

**自动提示工程师（APE）[4]** 提出了一种简单的方法来自动生成[指令](https://cameronrwolfe.substack.com/i/117151147/instruction-prompting)。首先，使用LLM通过带有多个指令示例的少量提示来提出一组潜在的指令。探索了几种提示模板以生成指令；见下文。

![](../Images/3fad653a651c8e32e11055c02e058084.png)

(来源于 [4])

然后，我们通过评估每个指令的零样本表现（即准确性或正确结果的对数概率）来搜索这些指令“候选者”的池。换句话说，每个提示下的LLM表现被用作评估指令质量的度量标准。

![](../Images/20a693e8e51214feace589caf4c79bb2.png)

(来源于 [4])

更进一步，我们在 [4] 中看到，指令可以通过重复此过程进行迭代优化。具体而言，我们可以 i) 提出一组候选者，ii) 根据表现评估这些候选者，iii) 选择表现最好的候选者，iv) 通过提示LLM生成类似的指令（即重新采样）来生成表现最好的候选者的新变体。此过程（及相关提示）在下图中概述。

![](../Images/5fa916468f2854ea9ba135b99061e396.png)

(来源于 [4])

**基于梯度的搜索。** 除了搜索更好文本提示的技术外，还有一类有用的提示工程工作探索对提示嵌入的持续更新。首先，我们应该回顾一下语言模型中的提示嵌入是什么。给定一个文本提示，我们通常将该提示标记化（即，将其分成单词或子词），然后查找每个生成的标记的嵌入。这个过程给我们一个*标记嵌入的列表*（即，提示嵌入！），我们将其作为输入传递给语言模型；见下文。

![](../Images/0a6805ea8e47fa59523984de29257312.png)

提示和提示嵌入在语言模型中（作者创建）

一些工作探索了直接修改提示嵌入的提示工程策略（即每个标记的嵌入列表）。换句话说，这些工作并不直接修改提示的单词，而是使用类似梯度下降的规则更新提示嵌入。该领域的主要工作在下面的列表中列出：

+   **AutoPrompt [5]** 将原始提示输入与一组共享的（跨所有输入数据）“触发标记”结合，这些标记通过基于梯度的搜索选择，以提高性能。

+   **前缀调优 [6]** 在输入和隐藏层的提示嵌入中添加了几个“前缀”标记，然后使用梯度下降训练这些前缀的参数（保持模型参数固定），作为一种高效的微调策略。

+   **提示调优 [7]** 类似于前缀调优，但前缀标记仅添加到输入层。这些标记在语言模型解决的每个任务上进行微调，使得前缀标记能够为给定任务调整模型。

+   **P-Tuning [8]** 向模型的输入层添加任务特定的锚点标记，并进行微调，但允许这些标记放置在任意位置（例如，提示的中间），使得该方法比前缀调优更加灵活。

**我们应该使用哪一种？** 所有这些方法（如下所示）都探索了将“软”标记添加到语言模型中，并在目标数据集上进行监督微调。值得注意的是，这些技术不能与只能通过付费 API 访问的语言模型一起使用（例如，[OpenAI API](https://platform.openai.com/docs/api-reference)）。这是因为我们需要能够访问和修改提示嵌入，而大多数 API 仅展示模型的文本输入和输出。目前，如果我们使用的是自托管的 LLM，我们只能使用基于梯度的自动提示技术。

![](../Images/5962419a7691f7b349b34965c2913116.png)

（来自 [5, 6, 7, 8]）

在这些方法中，Prompt Tuning 是最简单的，并且带来了显著的性能提升。使用 Prompt Tuning，我们只需 *i)* 在输入中附加一些前缀标记嵌入，并 *ii)* 对这些嵌入在各个下游任务上进行参数高效的微调。[7] 中的方法通过将几种不同的任务混合到每次更新中，并为每个任务提供一个独特的学习前缀来执行多任务微调；见下图。

![](../Images/b039469709a3f3e661eb7193dc65afe2.png)

（来自 [7]）

通常，微调语言模型意味着我们必须为每个任务存储模型参数的单独副本。相比之下，Prompt Tuning 只是微调一小组前缀标记嵌入，并保持其余模型参数不变。尽管仅微调了一小部分参数，但 Prompt Tuning 的性能接近于端到端微调，如下图所示。

![](../Images/96d16323129650a1bf72e27a82b59e9e.png)

（来自 [7]）

# 收获

> “我们可以期待推理能力随着模型规模的扩大而提高多少？还有哪些其他提示方法可以扩展语言模型能够解决的任务范围？” *— 来自 [9]*

本概述的主要目的是探索可能在解决难题时对 LLMs 实际有用的不同提示技术。如果应用得当，像零-shot学习和少量-shot学习以及指令提示这样的基本技术是有用且有效的。然而，可能需要更复杂的方法来使 LLMs 能够解决基于推理的任务或遵循复杂的多步骤指令。虽然模型可能随着时间的推移质量有所提升，并且更容易处理这些难题，但本概述中介绍的技术可以用于扩展目前可用的 LLMs 的应用范围。以下是这些技术的一些基本收获。

**解决难题。** CoT 提示的分析显示，LLM能够解决复杂的多步骤问题。然而，为了实现这一点，问题需要被分解为LLM可以处理的更小部分。我们可以通过鼓励模型在回答之前生成问题解决的推理，或者通过使用从少到多的提示将问题分解为LLM逐一解决的小部分来隐性地做到这一点。不管哪种方式，我们通常会发现鼓励LLM逐步解决问题比一次性解决整个问题更有益。

**学习提示。** 如果我们听到“提示工程”这个词，大多数人可能会想到调整提示的词汇或结构，以找出最佳效果。然而，这并不是提示工程的唯一方法！我们可以采用一种自动提示方法，通过梯度下降从数据中学习最佳提示。为此，我们使提示嵌入（即，提示中每个标记的嵌入列表）可训练，并进行微调。虽然这种方法有趣且有用，但需要注意以下几点：

1.  学习到的提示嵌入不能映射回文本提示，因为模型词汇表中每个标记的嵌入是离散的。

1.  只有在我们能够访问语言模型的嵌入层时，我们才能使用这些方法。通过付费API（例如，来自OpenAI）并未提供此访问权限。

**简单却强大。** 尽管本概述集中于高级提示工程技术，但仍有许多简单的技巧可以轻松应用，以提升LLM应用的效果。例如，通过生成多个答案并取其平均值，自一致性可以提高LLM的可靠性。零-shot CoT 提示可以通过在提示的末尾附加一个单一陈述来轻松提升LLM的推理能力。最后，生成的知识可以通过简单地要求模型列出有关某个主题的有用信息来提高LLM的表现，在生成最终答案之前。在许多情况下，将简单技巧加入到我们的提示工程工具包中可以带来巨大的变化！

## 结束语

非常感谢你阅读这篇文章。我是 [Cameron R. Wolfe](https://cameronrwolfe.me/)，[Rebuy](https://www.rebuyengine.com/) 的人工智能总监。我研究深度学习的经验和理论基础。你还可以查看我在medium上的 [其他文章](https://medium.com/@wolfecameron)！如果你喜欢这篇文章，请在 [twitter](https://twitter.com/cwolferesearch) 上关注我，或订阅我的 [Deep (Learning) Focus 新闻通讯](https://cameronrwolfe.substack.com/)，我通过对热门论文的易懂概述，帮助读者深入理解AI研究中的话题。

## 参考文献

[1] 刘佳成等。“生成知识提示用于常识推理。” *arXiv预印本arXiv:2110.08387*（2021年）。

[2] Trivedi, Harsh, 等. “将检索与思维链推理交替用于知识密集型多步骤问题。” *arXiv 预印本 arXiv:2212.10509* (2022)。

[3] Wei, Jason, 等. “思维链提示引发大语言模型中的推理。” *arXiv 预印本 arXiv:2201.11903* (2022)。

[4] Zhou, Yongchao, 等. “大语言模型是人类级别的提示工程师。” *arXiv 预印本 arXiv:2211.01910* (2022)。

[5] Shin, Taylor, 等. “自动提示：通过自动生成的提示从语言模型中引出知识。” *arXiv 预印本 arXiv:2010.15980* (2020)。

[6] Li, Xiang Lisa, 和 Percy Liang. “前缀调优：优化生成的连续提示。” *arXiv 预印本 arXiv:2101.00190* (2021)。

[7] Lester, Brian, Rami Al-Rfou, 和 Noah Constant. “规模的力量对参数高效提示调优。” *arXiv 预印本 arXiv:2104.08691* (2021)。

[8] Liu, Xiao, 等. “GPT 也能理解。” *arXiv 预印本 arXiv:2103.10385* (2021)。

[9] Wei, Jason, 等. “思维链提示引发大语言模型中的推理。” *arXiv 预印本 arXiv:2201.11903* (2022)。

[10] Wang Ling, Dani Yogatama, Chris Dyer, 和 Phil Blunsom. 2017\. 通过理由生成进行程序归纳：学习解决和解释代数词题。ACL。

[11] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, 和 John Schulman. 2021\. 培训验证器解决数学词题。arXiv 预印本 arXiv:2110.14168。

[12] Kojima, Takeshi, 等. “大语言模型是零样本推理器。” *arXiv 预印本 arXiv:2205.11916* (2022)。

[13] Kojima, Takeshi, 等. “大语言模型是零样本推理器。” *arXiv 预印本 arXiv:2205.11916* (2022)。

[14] Wang, Xuezhi, 等. “自洽性提高了语言模型中的思维链推理。” *arXiv 预印本 arXiv:2203.11171* (2022)。

[15] Zhou, Denny, 等. “从最少到最多的提示使大语言模型能够进行复杂推理。” *arXiv 预印本 arXiv:2205.10625* (2022)。

[16] Zhang, Zhuosheng, 等. “语言模型中的多模态思维链推理。” *arXiv 预印本 arXiv:2302.00923* (2023)。

[17] Diao, Shizhe, 等. “使用思维链进行积极提示以优化大语言模型。” *arXiv 预印本 arXiv:2302.12246* (2023)。

[18] Brown, Tom, 等. “语言模型是少样本学习者。” *神经信息处理系统进展* 33 (2020): 1877–1901。

[19] Hara, Kazuyuki, Daisuke Saitoh, 和 Hayaru Shouno. “将 dropout 学习视为集成学习的分析。” *人工神经网络和机器学习–ICANN 2016: 第25届国际人工神经网络大会，西班牙巴塞罗那，2016年9月6–9日，会议录，第二部分 25*。施普林格国际出版公司，2016。

[20] Huang, Gao 等人。 “具有随机深度的深度网络。” *计算机视觉–ECCV 2016：第十四届欧洲会议，荷兰阿姆斯特丹，2016年10月11–14日，会议论文集，第四部分 14*。Springer International Publishing，2016年。
