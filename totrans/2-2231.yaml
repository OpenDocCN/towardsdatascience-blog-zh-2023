- en: 'Unraveling the Design Pattern of Physics-Informed Neural Networks: Part 05'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-05-67a35a984b23](https://towardsdatascience.com/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-05-67a35a984b23)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Harnessing automated hyperparameter optimization for effective PINN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://shuaiguo.medium.com/?source=post_page-----67a35a984b23--------------------------------)[![Shuai
    Guo](../Images/d673c066f8006079be5bf92757e73a59.png)](https://shuaiguo.medium.com/?source=post_page-----67a35a984b23--------------------------------)[](https://towardsdatascience.com/?source=post_page-----67a35a984b23--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----67a35a984b23--------------------------------)
    [Shuai Guo](https://shuaiguo.medium.com/?source=post_page-----67a35a984b23--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----67a35a984b23--------------------------------)
    ¬∑9 min read¬∑Jun 4, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/33b724900a8fa118517e377c2e46f3fb.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Drew Patrick Miller](https://unsplash.com/@drewpatrickmiller?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Welcome to the 5th blog of this series, where we continue our exciting journey
    of exploring ***design patterns*** of physics-informed neural networks (PINN)üôå
  prefs: []
  type: TYPE_NORMAL
- en: Have you ever wondered if the best architecture for physics-informed neural
    networks can be automatically searched? It turns out there is a way to do that,
    as indicated by the paper we will be talking about today.
  prefs: []
  type: TYPE_NORMAL
- en: Same as usual, let‚Äôs start with a discussion of the issues at hand, followed
    by the solutions brought forward, the benchmarking process, as well as the strengths
    and weaknesses of the proposed technique. We will end the blog with some potential
    future opportunities.
  prefs: []
  type: TYPE_NORMAL
- en: 'In case you are also interested in other PINN design patterns covered in this
    series, feel free to catch up here:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PINN design pattern 01: Optimizing the residual point distribution](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-series-01-8190df459527)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PINN design pattern 02: Dynamic solution interval expansion](/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-02-2156516f2791)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PINN design pattern 03: training PINN with gradient boosting](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-03-fe365ef480d9)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PINN design pattern 04: gradient-enhanced PINN learning](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-04-c778f4829dde)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PINN design pattern 06: Causal PINN training](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-06-bcb3557199e2)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PINN design pattern 07: Active learning with PINN](/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-07-4ecb543b616a)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let‚Äôs dive in!
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Paper at a glance üîç
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Title**: Auto-PINN: Understanding and Optimizing Physics-Informed Neural
    Architecture'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Authors**: Y. C. Wang, X. T. Han, C. Y. Chang, D. C. Zha, U. Braga-Neto,
    X. Hu'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Institutes**: Texas A&M University, Rice University'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Link: [arXiv](https://arxiv.org/abs/2205.13748)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2\. Design pattern üé®
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 2.1 Problem üéØ
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the application of Physics-Informed Neural Networks (PINNs), it comes as
    no surprise that the neural network hyperparameters, such as network depth, width,
    the choice of activation function, etc, all have significant impacts on the PINNs‚Äô
    efficiency and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Naturally, people would resort to **AutoML** (more specifically, neural architecture
    search) to automatically identify the optimal network hyperparameters. But before
    we can do that, there are two questions that need to be addressed:'
  prefs: []
  type: TYPE_NORMAL
- en: How to effectively navigate the vast search space?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to define a proper search objective?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This latter point is due to the fact that PINN is usually seen as an ‚Äúunsupervised‚Äù
    problem: no labeled data is needed since the training is guided by minimizing
    the ODE/PDE residuals.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b054003291a6248de678f76d6c53354e.png)'
  prefs: []
  type: TYPE_IMG
- en: PINN workflow. PINNs‚Äô performance is highly sensitive to the network structure.
    One promising way to address this issue is by leveraging AutoML for automatic
    hyperparameter tuning. (Image by this blog author)
  prefs: []
  type: TYPE_NORMAL
- en: To better understand those two issues, the authors have conducted extensive
    experiments to investigate the PINN performance‚Äôs sensitivity with respect to
    the network structure. Let‚Äôs now take a look at what they have found.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Solution üí°
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first idea proposed in the paper is that **the training loss can be used
    as the surrogate for the search objective**, as it highly correlates with the
    final prediction accuracy of the PINN. This addresses the issue of defining a
    proper optimization target for hyperparameter search.
  prefs: []
  type: TYPE_NORMAL
- en: The second idea is that **there is no need to optimize all network hyperparameters
    simultaneously**. Instead, we can adopt a **step-by-step decoupling strategy**
    to, for example, first search for the optimal activation function, then fix the
    choice of the activation function and find the optimal network width, then fix
    the previous decisions and optimize network depth, and so on. In their experiments,
    the authors demonstrated that this strategy is very effective.
  prefs: []
  type: TYPE_NORMAL
- en: With those two ideas in mind, let‚Äôs see how we can execute the search in detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, which network hyperparameters are considered? In the paper, the
    recommended search space is:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Width**: number of neurons in each hidden layer. The considered range is
    [8, 512] with a step of 4 or 8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Depth**: number of hidden layers. The considered range is [3, 10] with a
    step of 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Activation function**: Tanh, Sigmoid, ReLU, and [Swish](https://paperswithcode.com/method/swish).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Changing point**: the portion of the epochs using Adam to the total training
    epochs. The considered values are [0.1, 0.2, 0.3, 0.4, 0.5]. In PINN, it‚Äôs a common
    practice to first use Adam to train for certain epochs and then switch to L-BFGS
    to keep training for some epochs. This changing point hyperparameter determines
    the timing of the change.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning rate**: a fixed value of 1e-5, as it has a small effect on the final
    architecture search results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training epochs**: a fixed value of 10000, as it has a small effect on the
    final architecture search results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Secondly, let‚Äôs examine the proposed procedure in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: The first search target is the *activation function*. To achieve that, we sample
    the width and depth parameter space and calculate the losses for all width-depth
    samples under different activation functions. These results can give us ideas
    of which activation function is the dominant one. Once decided, we fix the activation
    function for the following steps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/e70feda925ce7e4e1ad55422943f9095.png)'
  prefs: []
  type: TYPE_IMG
- en: The first step is to identify the dominant activation function. (Image by this
    blog author)
  prefs: []
  type: TYPE_NORMAL
- en: The second search target is the *width*. More specifically, we are looking for
    a couple of width intervals where PINN performs well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/2417264fc5c89ffe09c62dba799b2279.png)'
  prefs: []
  type: TYPE_IMG
- en: The second step is to identify the promising intervals for the network width.
    (Image by this blog author)
  prefs: []
  type: TYPE_NORMAL
- en: The third search target is the *depth*. Here, we only consider width varying
    within the best-performing intervals determined from the last step, and we would
    like to find the best K width-depth combinations where PINN performs well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/f37b81dfa8baf9b6797c43c68b6e824d.png)'
  prefs: []
  type: TYPE_IMG
- en: The third step is to identify the top-K best-performing width-depth combinations.
    (Image by this blog author)
  prefs: []
  type: TYPE_NORMAL
- en: The final search target is the *changing point*. We simply search for the best
    changing point for each of the top-K configurations identified from the last step.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/b2c971e519b77efebc33fc216a95519c.png)'
  prefs: []
  type: TYPE_IMG
- en: The final step is to identify the best changing point. (Image by this blog author)
  prefs: []
  type: TYPE_NORMAL
- en: The outcome of this search procedure is **K different PINN structures**. We
    can either select the best-performing one out of those K candidates or simply
    use all of them to form a K-ensemble PINN model.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that several tuning parameters need to be specified in the above-presented
    procedure (e.g., number of width intervals, number of K, etc.), which would depend
    on the available tuning budget.
  prefs: []
  type: TYPE_NORMAL
- en: As for the specific optimization algorithms used in individual steps, off-the-shelf
    AutoML libraries can be employed to complete the task. For example, the authors
    in the paper used [Tune package](https://docs.ray.io/en/latest/tune/index.html)
    for executing the hyperparameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Why the solution might work üõ†Ô∏è
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By decoupling the search of different hyperparameters, the scale of the search
    space can be greatly decreased. This not only substantially decreases the search
    complexity, but also significantly increases the chance of locating a (near) optimal
    network architecture for the physical problems under investigation.
  prefs: []
  type: TYPE_NORMAL
- en: Also, using the training loss as the search objective is both simple to implement
    and desirable. As the training loss (mainly constituted by PDE residual loss)
    highly correlates with the PINN accuracy during inference (according to the experiments
    conducted in the paper), identifying an architecture that delivers minimum training
    loss will also likely lead to a model with high prediction accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Benchmark ‚è±Ô∏è
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The paper considered a total of 7 different benchmark problems. All problems
    are forward problems where PINN is used to solve the PDEs.
  prefs: []
  type: TYPE_NORMAL
- en: Heat equation with Dirichlet boundary condition. This type of equation describes
    the heat or temperature distribution in a given domain over
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: time.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](../Images/4baa6f65157c0e3740d7b95632c2b11a.png)'
  prefs: []
  type: TYPE_IMG
- en: Heat equation with Neumann boundary conditions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/7ceb8b43195cdc1210a823a443906a72.png)'
  prefs: []
  type: TYPE_IMG
- en: Wave equation, which describes the propagation of oscillations in a space, such
    as mechanical and electromagnetic waves. Both Dirichlet and Neumann conditions
    are considered here.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/863da3e4ee07cc436fd1d948a0c6f4e4.png)'
  prefs: []
  type: TYPE_IMG
- en: Burgers equation, which has been leveraged to model shock flows, wave propagation
    in combustion chambers, vehicular traffic movement, and more.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/ccb5818acb93b826e62f1637680b1f1b.png)'
  prefs: []
  type: TYPE_IMG
- en: Advection equation, which describes the motion of a scalar field as it is advected
    by a known velocity vector field.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/af451ca8c4d7d29e6ce8c86244de3eaa.png)'
  prefs: []
  type: TYPE_IMG
- en: Advection equation, with different boundary conditions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/966c71e45e89ed8449727b345a07ba24.png)'
  prefs: []
  type: TYPE_IMG
- en: Reaction equation, which describes chemical reactions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/97b592caefa99d168ae75a97f3ebe41d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The benchmark studies yielded that:'
  prefs: []
  type: TYPE_NORMAL
- en: The proposed Auto-PINN shows stable performance for various PDEs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For most cases, Auto-PINN is able to identify the neural network architecture
    with the smallest error values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The search trials are fewer with the Auto-PINN approach.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2.5 Strengths and Weaknesses ‚ö°
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Strengths** üí™'
  prefs: []
  type: TYPE_NORMAL
- en: Significantly reduced computational cost for performing neural architecture
    search for PINN applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improved likelihood of identifying a (near) optimal neural network architecture
    for different PDE problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weaknesses** üìâ'
  prefs: []
  type: TYPE_NORMAL
- en: The effectiveness of using the training loss value as the search objective might
    depend on the specific characteristics of the PDE problem at hand, as the benchmarks
    are performed only for a specific set of PDEs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data sampling strategy influences Auto-PINN performance. While the paper discusses
    the impact of different data sampling strategies, it does not provide a clear
    guideline on how to choose the best strategy for a given PDE problem. This could
    potentially add another layer of complexity to the use of Auto-PINN.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2.6 Alternatives üîÄ
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The conventional out-of-box AutoML algorithms can also be employed to tackle
    the problem of hyperparameter optimization in Physics-Informed Neural Networks
    (PINNs). Those algorithms include *Random Search*, *Genetic Algorithms*, *Bayesian
    optimization*, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to those alternative algorithms, the newly proposed Auto-PINN is specifically
    designed for PINN. This makes it a unique and effective solution for optimizing
    PINN hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Potential Future Improvements üåü
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are several possibilities to further improve the proposed strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating more sophisticated data sampling strategies, such as adaptive-
    and residual-based sampling methods, to improve the search accuracy and the model
    performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To learn more about how to optimize the residual points distribution, check
    out [this blog](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-series-01-8190df459527)
    in the PINN design pattern series.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: More benchmarking on the search objective, to assess if training loss value
    is indeed a good surrogate for various types of PDEs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incorporating other types of neural networks. The current version of Auto-PINN
    is designed for multilayer perceptron (MLP) architectures only. Future work could
    explore convolutional neural networks (CNNs) or recurrent neural networks (RNNs),
    which could potentially enhance the capability of PINNs in solving more complex
    PDE problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transfer learning in Auto-PINN. For instance, architectures that perform well
    on certain types of PDE problems could be used as starting points for the search
    process on similar types of PDE problems. This could potentially speed up the
    search process and improve the performance of the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4 Takeaways üìù
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this blog, we looked at how to efficiently tune PINN model hyperparameters
    with the Auto-PINN approach. Here are the highlights of the design pattern proposed
    in the paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Problem]: How to automatically tune PINNs‚Äô model hyperparameters?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Solution]: **Customized Neural Architecture Search**, where the training loss
    is used as the search objective and a step-by-step decoupling strategy is employed
    to effectively narrow the search space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Potential benefits]: 1\. More efficient search with significantly reduced
    computational cost. 2\. Improved likelihood to identify the (near) optimal neural
    network hyperparameters for different types of PDE problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As usual, I have prepared a PINN design card to summarize the takeaways:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/50de7543bac39e0495455ac5c882c868.png)'
  prefs: []
  type: TYPE_IMG
- en: PINN design pattern proposed in the paper. (Image by this blog author)
  prefs: []
  type: TYPE_NORMAL
- en: 'I hope you found this blog useful! To learn more about PINN design patterns,
    feel free to check out other posts in this series:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PINN design pattern 01: Optimizing the residual point distribution](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-series-01-8190df459527)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PINN design pattern 02: Dynamic solution interval expansion](/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-02-2156516f2791)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PINN design pattern 03: PINN training with gradient boost](/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-03-fe365ef480d9)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PINN design pattern 04: gradient-enhanced PINN learning](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-04-c778f4829dde)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PINN design pattern 06: Causal PINN training](https://medium.com/towards-data-science/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-06-bcb3557199e2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PINN design pattern 07: Active learning with PINN](/unraveling-the-design-pattern-of-physics-informed-neural-networks-part-07-4ecb543b616a)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking forward to sharing more insights with you in the upcoming blogs!
  prefs: []
  type: TYPE_NORMAL
- en: Reference üìë
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Wang et al., Auto-PINN: Understanding and Optimizing Physics-Informed Neural
    Architecture, [arXiv](https://arxiv.org/abs/2205.13748), 2022.'
  prefs: []
  type: TYPE_NORMAL
