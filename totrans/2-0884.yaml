- en: 'Feature Transformations: A Tutorial on PCA and LDA'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/feature-transformations-a-tutorial-on-pca-and-lda-1ac160088092](https://towardsdatascience.com/feature-transformations-a-tutorial-on-pca-and-lda-1ac160088092)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Reducing the dimension of a dataset using methods such as PCA
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@PadraigC?source=post_page-----1ac160088092--------------------------------)[![Pádraig
    Cunningham](../Images/9521fb3dc64c947edf6adf2ce7b80f0f.png)](https://medium.com/@PadraigC?source=post_page-----1ac160088092--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1ac160088092--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1ac160088092--------------------------------)
    [Pádraig Cunningham](https://medium.com/@PadraigC?source=post_page-----1ac160088092--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1ac160088092--------------------------------)
    ·7 min read·Jul 14, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4bb1b05b0d0163b3ecd9ab788f291fdb.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: Photo by [Nicole Cagnina](https://unsplash.com/@nicole_c?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/projection?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When dealing with high-dimension data, it is common to use methods such as Principal
    Component Analysis (PCA) to reduce the dimension of the data. This converts the
    data to a different (lower dimension) set of features. This contrasts with feature
    subset selection which selects a subset of the original features (see [[1]](https://medium.com/towards-data-science/feature-subset-selection-6de1f05822b0)
    for a turorial on feature selection).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: PCA is a linear transformation of the data to a lower dimension space. In this
    article we start off by explaining what a linear transformation is. Then we show
    with Python examples how PCA works. The article concludes with a description of
    Linear Discriminant Analysis (LDA) a *supervised* linear transformation method.
    Python code for the methods presented in that paper is available on [GitHub](https://github.com/PadraigC/FeatTransTutorial).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Linear Transformations
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Imagine that after a holiday Bill owes Mary £5 and $15 that needs to be paid
    in euro (€). The rates of exchange are; £1 = €1.15 and $1 = €0.93\. So the debt
    in € is:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c0dd42037c086d8bb90eb484a162cb76.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
- en: Here we are converting a debt in two dimensions (£,$) to one dimension (€).
    Three examples of this are illustrated in Figure 1, the original (£5, $15) debt
    and two other debts of (£15, $20) and (£20, $35). The green dots are the original
    debts and the red dots are the debts projected into a single dimension. The red
    line is this new dimension.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cbf16cd8b8dd7f74d2227ab0570161a3.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
- en: '**Figure 1\.** An illustration of how converting £,$ debts to € is a linear
    transformation. Image by author.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: On the left in the figure we can see how this can be represented as matrix multiplication.
    The original dataset is a 3 by 2 matrix (3 samples, 2 features), the rates of
    exchange form a 1D matrix of two components and the output is a 1D matrix of 3
    components. The exchange rate matrix *is* the transformation; if the exchange
    rates are changed then the transformation changes.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: We can perform this matrix multiplication in Python using the code below. The
    matrices are represented as numpy arrays; the final line calls the `dot` method
    on the `cur` matrix to perform matrix multiplication (dot product). This will
    return the matrix `[19.7, 35.85, 55.55]`.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: The general format for such data transformations is shown in Figure 2\. **Y**
    is the original dataset (*n* samples, *d* features); this is reduced to *k* featuresin
    **X’** by multiplying by the transformation matrix **P** which has dimension *d*
    by *k.*
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f5cdbb8952098a66272d10d823b4a790.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: '**Figure 2\.** If we have a dataset **Y** of *n samples described by d features,
    this can be reduced to* k *features (****X’)*** *by multiplying by the transformation
    matrix* ***P*** *of dimension d x k.* Image by author*.*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Principal Components Analysis
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In general the transformation matrix **P** determines the transformation. In
    the example in Figure 1 the details of the transformation are determined by the
    rates of exchange and these are given. If we wish to use PCA to reduce the dimension
    of a dataset, how do we decide on the nature of the transformation? Well PCA is
    driven by three principles:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Select a transformation that preserves the spread in the data, i.e. prefer dimensions
    that preserve distances between data points.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select dimensions that are orthogonal to each other (no redundancy).
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select *k* dimensions that capture most of the variance in the data (say 90%).
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These principles are illustrated in Figure 3\. We have a dataset of individuals
    described by two features, waist measurement and weight. These features are correlated
    with each other so the objective is to project the data into different, uncorrelated
    dimensions without losing the ‘spread’ in the data. These new dimensions are the
    principal components that give PCA its name. As an alternative to thinking about
    this as a projection, you can think of it as *rotating* the data cloud to align
    with the red axes in Figure 3\. Either way, the new axes are PC1, the first principal
    component and PC2 which is perpendicular to PC1\. If it were felt that PC1 captured
    enough of the variation in the data then PC2 might be dropped.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/30cb90ec4da05433963e882d8ae62a79.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: '**Figure 3\.** A 2D dataset where the features are weight and waist measurement.
    The first principle component (PC1) should be in the direction that captures most
    of the variance in the data. PC2 should be orthogonal to PC1 so that they are
    independent. Image by author.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps to perform PCA on a data matrix **Y** as shown in Figure 2 are as
    follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the means and standard deviations of the columns of **Y**.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Subtract the column means from each row of **Y** and divide by the standard
    deviation to create the *normalised centred matrix* **Z**.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the covariance matrix **C** = 1/(n-1) **Zᵀ Z** where **Zᵀ** is the
    transpose of **Z**.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the eigenvectors and eigenvalues of the covariance matrix **C**.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Examine the eigenvalues in descending order to determine the number of dimensions
    *k* to be retained — this is the number of principle components.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The top *k* eigenvectors make up the columns of the transformation matrix **P**
    which has dimension (*p* × *k*).
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The data is transformed by **X′ = ZP** where **X**′ has dimension (*n* × *k*).
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the following examples we will work with the Harry Potter TT dataset that
    is shared on [Github](https://github.com/PadraigC/FeatTransTutorial). The format
    of the data is shown below. There are 22 rows and fve descriptive features. We
    will use PCA to compress this to two dimensions.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Python code to do this is shown below. `Y_df`is a Pandas dataframe with the
    dataset. The eigenvalues and eigenvectors (`ev,evec`) are calculated in line 8\.
    If we examine the eigenvalues they tells us the amount of variance captured in
    each PC; [49%, 31%, 11%, 5%, 4%]. The first two PCs will retain 80% of (49% +
    31%) of the variance in the data so we decide to go with two PCs. `X_dash`contains
    this data projected into two dimensions. The data projected in 2D is shown in
    Figure 4\. It could be argued that PC1 dimension represents competence/incompetence
    and the PC2 dimension represents goodness. Fred & George Weasley (twins) are plotted
    at the same point as they have exactly the same feature values in the original
    dataset.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8140bd9901ba3831fbe1ba6a5950c832.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
- en: '**Figure 4\.** The Harry Potter dataset projected into two dimensions (2 PCs).
    Image by author.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 'If we use the PCA implementation in scikit learn we can do this in three lines
    of code:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: In line 4 the data is normalised, the PCA object is set up in line 5 and the
    data transformation is done in line 6\. Again, the code for this is available
    in the notebook on [Github](https://github.com/PadraigC/FeatTransTutorial).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Linear Discriminant Analysis
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It should be clear that PCA is inherently an *unsupervised* ML technique in
    that it does not take any class labels into account. In fact, PCA will not necessarily
    be effective in a supervised learning context — this should not be surprising
    given the focus on maintaining the *spread* in the data without consideration
    for class labels. In Figure 4 we see how PCA does on the penguins dataset [2].
    This is a three class dataset described by four features (also available on [GitHub](https://github.com/PadraigC/FeatTransTutorial)).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 应该明确的是，PCA 本质上是一种*无监督*的机器学习技术，因为它不考虑任何类别标签。实际上，在监督学习的背景下，PCA 不一定会有效——考虑到其重点是保持数据的*分布*而不考虑类别标签，这一点并不令人惊讶。在图
    4 中，我们可以看到 PCA 在企鹅数据集 [2] 上的表现。这是一个由四个特征描述的三类数据集（也可在[GitHub](https://github.com/PadraigC/FeatTransTutorial)上找到）。
- en: '![](../Images/e6187650d55b3a67a19b4b44252761e0.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e6187650d55b3a67a19b4b44252761e0.png)'
- en: '**Figure 5\.** These scatter plots compare the performance of PCA and LDA on
    the penguins dataset. PCA does not do so well as it does not take class label
    information into account. Image by author.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 5\.** 这些散点图比较了 PCA 和 LDA 在企鹅数据集上的表现。PCA 由于未考虑类别标签信息，其表现不如 LDA。图片由作者提供。'
- en: 'On the right in Figure 5 we see how Linear Discriminant Analysis (LDA) performs
    on the same dataset. LDA takes the class labels into account and seeks a projection
    that maximises the separation between the classes. The objective is to uncover
    a transformation that will maximise between-class separation and minimise within-class
    separation. These can be calculated in two matrices **S***ᵦ* for between-class
    separation and **S***𝓌* for within-class separation:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 5 右侧，我们可以看到线性判别分析 (LDA) 在相同数据集上的表现。LDA 考虑了类别标签，并寻求一个最大化类别之间分离的投影。目标是揭示一个能够最大化类别间分离并最小化类别内分离的变换。这些可以在两个矩阵中计算：**S***ᵦ*
    代表类别间分离，**S***𝓌* 代表类别内分离：
- en: '![](../Images/5eb6c85583a67a29b962b5f60edf9f40.png)![](../Images/64a85f1afb154f360b51171e931a292e.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5eb6c85583a67a29b962b5f60edf9f40.png)![](../Images/64a85f1afb154f360b51171e931a292e.png)'
- en: 'where *n*𝒸 is the number of objects in class *c*, *μ* is the mean of all examples
    and *μ*𝒸 is the mean of all examples in class *c*:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *n*𝒸 是类别 *c* 中对象的数量，*μ* 是所有示例的均值，*μ*𝒸 是类别 *c* 中所有示例的均值：
- en: '![](../Images/b3fdafe3b26e98a1d1681038917d8179.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3fdafe3b26e98a1d1681038917d8179.png)'
- en: 'The components within these summations *μ, μ*𝒸*, xⱼ* are vectors of dimension
    *p* so **S***ᵦ* and **S***𝓌* are matrices of dimension *p* × *p*. The objectives
    of maximising between-class separation and minimising within-class separation
    can be combined into a single maximisation called the Fisher criterion:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这些求和中的组件 *μ, μ*𝒸*, xⱼ* 是维度为 *p* 的向量，因此 **S***ᵦ* 和 **S***𝓌* 是维度为 *p* × *p* 的矩阵。最大化类别间分离和最小化类别内分离的目标可以结合成一个称为
    Fisher 判别准则的单一最大化：
- en: '![](../Images/ad7cc6716c5572688b96e8ea797df542.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad7cc6716c5572688b96e8ea797df542.png)'
- en: This formulation and the task of finding the best **W***ˡᵈᵃ*matrix is discussed
    in more detail in [3]. For now we just need to recognise that **W***ˡᵈᵃ* has the
    same role as the **P** matrix in PCA. It has dimension *p* × *k* and will project
    the data into a *k* dimension space that maximises between class separation and
    minimises within class separation. The objectives are represented by the two **S**
    matrices. We see on the right in Figure 5 that it does a pretty good job.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这种表述以及找到最佳 **W***ˡᵈᵃ* 矩阵的任务在 [3] 中有更详细的讨论。现在我们只需认识到 **W***ˡᵈᵃ* 的作用与 PCA 中的 **P**
    矩阵相同。它的维度是 *p* × *k*，将数据投影到一个 *k* 维空间中，以最大化类别间分离并最小化类别内分离。目标由两个 **S** 矩阵表示。我们可以看到图
    5 右侧，它做得相当不错。
- en: While the inner workings of LDA might seem complicated, it is very straightforward
    in scikit-learn as can be seen in the code block below. This is very similar to
    the PCA code above; the main difference is that the `y`target variable is considered
    when the LDA is fitted to the data; this is not the case with PCA.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 LDA 的内部工作机制可能看起来复杂，但在 scikit-learn 中它非常简单，如下面的代码块所示。这与上述的 PCA 代码非常相似；主要区别在于在拟合
    LDA 时会考虑 `y` 目标变量；而 PCA 并非如此。
- en: Conclusion
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: The objective for this article was to explain the general principles underlying
    data transformations, to show how PCA and LDA work in scikit-learn and to present
    some examples of these methods in operation.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的目标是解释数据转换的基本原理，展示 PCA 和 LDA 在 scikit-learn 中的工作方式，并展示这些方法的一些实际示例。
- en: The code and data for these examples is available on on [GitHub](https://github.com/PadraigC/FeatTransTutorial)).
    A more in-depth treatment of these methods is presented in this *arXiv* report
    [3].
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] P. Cunningham, “Feature Subset Selection”, Towards Data Science, 2022,
    [Online], [https://towardsdatascience.com/feature-subset-selection-6de1f05822b0](/feature-subset-selection-6de1f05822b0)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[2] A.M. Horst, A.P. Hill, K.B. Gorman KB, palmerpenguins: Palmer Archipelago
    (Antarctica) penguin data, 2020, [doi:10.5281/zenodo.3960218](https://doi.org/10.5281/zenodo.3960218),
    R package version 0.1.0, [https://allisonhorst.github.io/palmerpenguins/](https://allisonhorst.github.io/palmerpenguins/).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '[3] P. Cunningham, B. Kathirgamanathan, & S.J. Delany, Feature selection tutorial
    with python examples, 2021, [*arXiv preprint arXiv:2106.06437*.](https://arxiv.org/abs/2106.06437)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
