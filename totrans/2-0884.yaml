- en: 'Feature Transformations: A Tutorial on PCA and LDA'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/feature-transformations-a-tutorial-on-pca-and-lda-1ac160088092](https://towardsdatascience.com/feature-transformations-a-tutorial-on-pca-and-lda-1ac160088092)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Reducing the dimension of a dataset using methods such as PCA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@PadraigC?source=post_page-----1ac160088092--------------------------------)[![P√°draig
    Cunningham](../Images/9521fb3dc64c947edf6adf2ce7b80f0f.png)](https://medium.com/@PadraigC?source=post_page-----1ac160088092--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1ac160088092--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1ac160088092--------------------------------)
    [P√°draig Cunningham](https://medium.com/@PadraigC?source=post_page-----1ac160088092--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1ac160088092--------------------------------)
    ¬∑7 min read¬∑Jul 14, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4bb1b05b0d0163b3ecd9ab788f291fdb.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Nicole Cagnina](https://unsplash.com/@nicole_c?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/projection?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When dealing with high-dimension data, it is common to use methods such as Principal
    Component Analysis (PCA) to reduce the dimension of the data. This converts the
    data to a different (lower dimension) set of features. This contrasts with feature
    subset selection which selects a subset of the original features (see [[1]](https://medium.com/towards-data-science/feature-subset-selection-6de1f05822b0)
    for a turorial on feature selection).
  prefs: []
  type: TYPE_NORMAL
- en: PCA is a linear transformation of the data to a lower dimension space. In this
    article we start off by explaining what a linear transformation is. Then we show
    with Python examples how PCA works. The article concludes with a description of
    Linear Discriminant Analysis (LDA) a *supervised* linear transformation method.
    Python code for the methods presented in that paper is available on [GitHub](https://github.com/PadraigC/FeatTransTutorial).
  prefs: []
  type: TYPE_NORMAL
- en: Linear Transformations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Imagine that after a holiday Bill owes Mary ¬£5 and $15 that needs to be paid
    in euro (‚Ç¨). The rates of exchange are; ¬£1 = ‚Ç¨1.15 and $1 = ‚Ç¨0.93\. So the debt
    in ‚Ç¨ is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c0dd42037c086d8bb90eb484a162cb76.png)'
  prefs: []
  type: TYPE_IMG
- en: Here we are converting a debt in two dimensions (¬£,$) to one dimension (‚Ç¨).
    Three examples of this are illustrated in Figure 1, the original (¬£5, $15) debt
    and two other debts of (¬£15, $20) and (¬£20, $35). The green dots are the original
    debts and the red dots are the debts projected into a single dimension. The red
    line is this new dimension.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cbf16cd8b8dd7f74d2227ab0570161a3.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 1\.** An illustration of how converting ¬£,$ debts to ‚Ç¨ is a linear
    transformation. Image by author.'
  prefs: []
  type: TYPE_NORMAL
- en: On the left in the figure we can see how this can be represented as matrix multiplication.
    The original dataset is a 3 by 2 matrix (3 samples, 2 features), the rates of
    exchange form a 1D matrix of two components and the output is a 1D matrix of 3
    components. The exchange rate matrix *is* the transformation; if the exchange
    rates are changed then the transformation changes.
  prefs: []
  type: TYPE_NORMAL
- en: We can perform this matrix multiplication in Python using the code below. The
    matrices are represented as numpy arrays; the final line calls the `dot` method
    on the `cur` matrix to perform matrix multiplication (dot product). This will
    return the matrix `[19.7, 35.85, 55.55]`.
  prefs: []
  type: TYPE_NORMAL
- en: The general format for such data transformations is shown in Figure 2\. **Y**
    is the original dataset (*n* samples, *d* features); this is reduced to *k* featuresin
    **X‚Äô** by multiplying by the transformation matrix **P** which has dimension *d*
    by *k.*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f5cdbb8952098a66272d10d823b4a790.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 2\.** If we have a dataset **Y** of *n samples described by d features,
    this can be reduced to* k *features (****X‚Äô)*** *by multiplying by the transformation
    matrix* ***P*** *of dimension d x k.* Image by author*.*'
  prefs: []
  type: TYPE_NORMAL
- en: Principal Components Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In general the transformation matrix **P** determines the transformation. In
    the example in Figure 1 the details of the transformation are determined by the
    rates of exchange and these are given. If we wish to use PCA to reduce the dimension
    of a dataset, how do we decide on the nature of the transformation? Well PCA is
    driven by three principles:'
  prefs: []
  type: TYPE_NORMAL
- en: Select a transformation that preserves the spread in the data, i.e. prefer dimensions
    that preserve distances between data points.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select dimensions that are orthogonal to each other (no redundancy).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select *k* dimensions that capture most of the variance in the data (say 90%).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These principles are illustrated in Figure 3\. We have a dataset of individuals
    described by two features, waist measurement and weight. These features are correlated
    with each other so the objective is to project the data into different, uncorrelated
    dimensions without losing the ‚Äòspread‚Äô in the data. These new dimensions are the
    principal components that give PCA its name. As an alternative to thinking about
    this as a projection, you can think of it as *rotating* the data cloud to align
    with the red axes in Figure 3\. Either way, the new axes are PC1, the first principal
    component and PC2 which is perpendicular to PC1\. If it were felt that PC1 captured
    enough of the variation in the data then PC2 might be dropped.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/30cb90ec4da05433963e882d8ae62a79.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 3\.** A 2D dataset where the features are weight and waist measurement.
    The first principle component (PC1) should be in the direction that captures most
    of the variance in the data. PC2 should be orthogonal to PC1 so that they are
    independent. Image by author.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps to perform PCA on a data matrix **Y** as shown in Figure 2 are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the means and standard deviations of the columns of **Y**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Subtract the column means from each row of **Y** and divide by the standard
    deviation to create the *normalised centred matrix* **Z**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the covariance matrix **C** = 1/(n-1) **Z·µÄ Z** where **Z·µÄ** is the
    transpose of **Z**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the eigenvectors and eigenvalues of the covariance matrix **C**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Examine the eigenvalues in descending order to determine the number of dimensions
    *k* to be retained ‚Äî this is the number of principle components.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The top *k* eigenvectors make up the columns of the transformation matrix **P**
    which has dimension (*p* √ó *k*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The data is transformed by **X‚Ä≤ = ZP** where **X**‚Ä≤ has dimension (*n* √ó *k*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the following examples we will work with the Harry Potter TT dataset that
    is shared on [Github](https://github.com/PadraigC/FeatTransTutorial). The format
    of the data is shown below. There are 22 rows and fve descriptive features. We
    will use PCA to compress this to two dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Python code to do this is shown below. `Y_df`is a Pandas dataframe with the
    dataset. The eigenvalues and eigenvectors (`ev,evec`) are calculated in line 8\.
    If we examine the eigenvalues they tells us the amount of variance captured in
    each PC; [49%, 31%, 11%, 5%, 4%]. The first two PCs will retain 80% of (49% +
    31%) of the variance in the data so we decide to go with two PCs. `X_dash`contains
    this data projected into two dimensions. The data projected in 2D is shown in
    Figure 4\. It could be argued that PC1 dimension represents competence/incompetence
    and the PC2 dimension represents goodness. Fred & George Weasley (twins) are plotted
    at the same point as they have exactly the same feature values in the original
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8140bd9901ba3831fbe1ba6a5950c832.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 4\.** The Harry Potter dataset projected into two dimensions (2 PCs).
    Image by author.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we use the PCA implementation in scikit learn we can do this in three lines
    of code:'
  prefs: []
  type: TYPE_NORMAL
- en: In line 4 the data is normalised, the PCA object is set up in line 5 and the
    data transformation is done in line 6\. Again, the code for this is available
    in the notebook on [Github](https://github.com/PadraigC/FeatTransTutorial).
  prefs: []
  type: TYPE_NORMAL
- en: Linear Discriminant Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It should be clear that PCA is inherently an *unsupervised* ML technique in
    that it does not take any class labels into account. In fact, PCA will not necessarily
    be effective in a supervised learning context ‚Äî this should not be surprising
    given the focus on maintaining the *spread* in the data without consideration
    for class labels. In Figure 4 we see how PCA does on the penguins dataset [2].
    This is a three class dataset described by four features (also available on [GitHub](https://github.com/PadraigC/FeatTransTutorial)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e6187650d55b3a67a19b4b44252761e0.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 5\.** These scatter plots compare the performance of PCA and LDA on
    the penguins dataset. PCA does not do so well as it does not take class label
    information into account. Image by author.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the right in Figure 5 we see how Linear Discriminant Analysis (LDA) performs
    on the same dataset. LDA takes the class labels into account and seeks a projection
    that maximises the separation between the classes. The objective is to uncover
    a transformation that will maximise between-class separation and minimise within-class
    separation. These can be calculated in two matrices **S***·µ¶* for between-class
    separation and **S***ùìå* for within-class separation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5eb6c85583a67a29b962b5f60edf9f40.png)![](../Images/64a85f1afb154f360b51171e931a292e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where *n*ùí∏ is the number of objects in class *c*, *Œº* is the mean of all examples
    and *Œº*ùí∏ is the mean of all examples in class *c*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b3fdafe3b26e98a1d1681038917d8179.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The components within these summations *Œº, Œº*ùí∏*, x‚±º* are vectors of dimension
    *p* so **S***·µ¶* and **S***ùìå* are matrices of dimension *p* √ó *p*. The objectives
    of maximising between-class separation and minimising within-class separation
    can be combined into a single maximisation called the Fisher criterion:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ad7cc6716c5572688b96e8ea797df542.png)'
  prefs: []
  type: TYPE_IMG
- en: This formulation and the task of finding the best **W***À°·µà·µÉ*matrix is discussed
    in more detail in [3]. For now we just need to recognise that **W***À°·µà·µÉ* has the
    same role as the **P** matrix in PCA. It has dimension *p* √ó *k* and will project
    the data into a *k* dimension space that maximises between class separation and
    minimises within class separation. The objectives are represented by the two **S**
    matrices. We see on the right in Figure 5 that it does a pretty good job.
  prefs: []
  type: TYPE_NORMAL
- en: While the inner workings of LDA might seem complicated, it is very straightforward
    in scikit-learn as can be seen in the code block below. This is very similar to
    the PCA code above; the main difference is that the `y`target variable is considered
    when the LDA is fitted to the data; this is not the case with PCA.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The objective for this article was to explain the general principles underlying
    data transformations, to show how PCA and LDA work in scikit-learn and to present
    some examples of these methods in operation.
  prefs: []
  type: TYPE_NORMAL
- en: The code and data for these examples is available on on [GitHub](https://github.com/PadraigC/FeatTransTutorial)).
    A more in-depth treatment of these methods is presented in this *arXiv* report
    [3].
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] P. Cunningham, ‚ÄúFeature Subset Selection‚Äù, Towards Data Science, 2022,
    [Online], [https://towardsdatascience.com/feature-subset-selection-6de1f05822b0](/feature-subset-selection-6de1f05822b0)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] A.M. Horst, A.P. Hill, K.B. Gorman KB, palmerpenguins: Palmer Archipelago
    (Antarctica) penguin data, 2020, [doi:10.5281/zenodo.3960218](https://doi.org/10.5281/zenodo.3960218),
    R package version 0.1.0, [https://allisonhorst.github.io/palmerpenguins/](https://allisonhorst.github.io/palmerpenguins/).'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] P. Cunningham, B. Kathirgamanathan, & S.J. Delany, Feature selection tutorial
    with python examples, 2021, [*arXiv preprint arXiv:2106.06437*.](https://arxiv.org/abs/2106.06437)'
  prefs: []
  type: TYPE_NORMAL
