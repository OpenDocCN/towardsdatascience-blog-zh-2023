- en: Build More Capable LLMs with Retrieval Augmented Generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/build-more-capable-llms-with-retrieval-augmented-generation-99d5f86e9779](https://towardsdatascience.com/build-more-capable-llms-with-retrieval-augmented-generation-99d5f86e9779)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How Retrieval Augmented Generation Can Enhance Your LLMs by Integrating a Knowledge
    Base
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://johnadeojo.medium.com/?source=post_page-----99d5f86e9779--------------------------------)[![John
    Adeojo](../Images/f6460fae462b055d36dce16fefcd142c.png)](https://johnadeojo.medium.com/?source=post_page-----99d5f86e9779--------------------------------)[](https://towardsdatascience.com/?source=post_page-----99d5f86e9779--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----99d5f86e9779--------------------------------)
    [John Adeojo](https://johnadeojo.medium.com/?source=post_page-----99d5f86e9779--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----99d5f86e9779--------------------------------)
    ·12 min read·Aug 9, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/021250c8f523175e7cc88889a35f18bb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by author: Generated with Midjourney'
  prefs: []
  type: TYPE_NORMAL
- en: '**The Limitations of ChatGPT**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ChatGPT is limited for many practical business use cases outside of code generation.
    The limitation arises from the training data, and the model’s propensity to hallucinate.
    At the time of writing, if you try to ask the Chat-GPT questions about events
    occurring after September 2021, you will probably receive a response like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7979343974282a45cf26222befb3887c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: This isn’t helpful, so how can we go about rectifying it?
  prefs: []
  type: TYPE_NORMAL
- en: '**Option 1 — Train or fine-tune the model on up-to-date data.**'
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning or training a model can be impractical and expensive. Putting aside
    the costs, the effort required to prepare the data sets is enough to forgo this
    option.
  prefs: []
  type: TYPE_NORMAL
- en: '**Option 2 — Use retrieval augmented generation (RAG) methods.**'
  prefs: []
  type: TYPE_NORMAL
- en: RAG methods enable us to give the large language model access to an up-to-date
    knowledge base. This is much cheaper than training a model from scratch or fine-tuning,
    and much easier to implement. In this article, I show you how to leverage RAG
    with your OpenAI model. We will put the model to the test by conducting a short
    analysis of its ability to answer questions about the Russia-Ukraine conflict
    of 2022 from a [Wikipedia](https://en.wikipedia.org/wiki/Russian_invasion_of_Ukraine)
    knowledge base.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: This topic, although sensitive, was chosen for the obvious reason that
    the current ChatGPT model has no knowledge of it.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Libraries & Pre-requisites**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will require an OpenAI API key, you can grab one directly from their website
    or follow this [tutorial](https://www.howtogeek.com/885918/how-to-get-an-openai-api-key/).
    The framework used for RAG is [Haystack](https://haystack.deepset.ai/) by Deepset,
    which is open source. They provide APIs enabling you to build applications on
    top of large language models. We also leverage sentence transformers and the transformers
    library from [Hugging Face](https://huggingface.co/).
  prefs: []
  type: TYPE_NORMAL
- en: '**Sentence Embeddings Help Models Interpret Text**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before getting into the technical details around building, we should briefly
    cover sentence embedding. Understanding this concept is key to gaining an intuition
    into how RAG methods work.
  prefs: []
  type: TYPE_NORMAL
- en: This may be a cliché, especially for those from a data science background, but
    models don’t actually understand text, they only understand numbers. Much of language
    modelling is about formulating ways to effectively encode text into numbers, and
    currently, we do this with sentence embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Sentence embeddings are a way for us to represent sentences as a dense vector
    while preserving the semantic structure. Embeddings are learnt from a dense layer
    in a deep neural network, the structure of which can vary from network to network.
  prefs: []
  type: TYPE_NORMAL
- en: In much simpler terms, sentence embeddings can be thought of as numeric representations
    of our sentences that preserve information about their meaning. We can get our
    sentence embeddings from pre-trained models. Hugging Face provides open-source
    models for this via their sentence transformers library.
  prefs: []
  type: TYPE_NORMAL
- en: '**Pre-processing and Storage**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we can build our RAG-enabled model, we need to pre-process and store
    our documents. Let’s explore how we do this, but first take note of the architecture
    of this process to help with your understanding.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: the architectural diagram also applies to the extractive question answer
    pipeline we define in the next section.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/252d1afb85f050e37dbd6e0dff83e8ba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by author: High level architecture pre-processing, vectore store, and
    extractive question-answer pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: Pre-processing our Documents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Haystack provides us tools for easily pre-processing most types of text documents
    (.pdf, .txt, .docx included). The preprocessing steps are simple; we read in our
    knowledge base using the [convert_files_to_docs()](https://docs.haystack.deepset.ai/reference/utils-api#convert_files_to_docs:~:text=Module%20preprocessing-,convert_files_to_docs,-Python)
    function, which can automatically detect the file type and convert it to the format
    we need to work with downstream.
  prefs: []
  type: TYPE_NORMAL
- en: Haystack also provides a [PreProcessor](https://docs.haystack.deepset.ai/reference/preprocessor-api#preprocessor::text=Module%20preprocessor-,PreProcessor,-Python:~:text=Module%20preprocessor-,PreProcessor,-Python)
    class that enables us to apply preprocessing steps to our documents. The steps
    you apply will depend very much on your specific application.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: the processor will enable you to split a long document into a list of
    smaller documents, defined as sentences. For question-and-answer applications,
    a common approach is to have some overlap between sentences; I have set this at
    roughly 30%.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vector Store**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We leverage [FAISS](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/),
    a library by Meta that enables efficient similarity search over our sentence embeddings.
    The importance of this will become more obvious in the coming sections. The script
    below shows how we can set up our pre-process for our text documents and set up
    the FAISS vector store.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full pre-processing script is outlined below:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Script by author: Document Preprocessing and FAISS vector store'
  prefs: []
  type: TYPE_NORMAL
- en: '**Defining an Extractive Question-Answer Pipeline**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next step is to construct our extractive question answer pipeline. Our pipeline
    is defined by nodes which are run sequentially as a directed acyclic graph (DAG).
    In this case, the pipeline consists of two nodes, a retriever, and a reader.
  prefs: []
  type: TYPE_NORMAL
- en: Retriever
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Retrieval is the method used to find the relevant information from a knowledge
    base based on the user’s query. When defining our retriever node, we specify a
    sentence embedding model from the sentence transformers library, in this case,
    we use [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2),
    which creates 768-dimensional embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have fully defined our retrieval node, we can compute and store the
    sentence embeddings in our FAISS vector store. The same sentence embedding model
    is used to generate sentence embeddings for the user’s query.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: when selecting your sentence embedding model, there is a trade-off between
    computational efficiency and information loss. In general, sentence embedding
    models with higher dimensions capture more information but are less computationally
    efficient.*'
  prefs: []
  type: TYPE_NORMAL
- en: Remember, the ultimate purpose of the retriever node is to find the information
    that relates semantically to the query. This is done by performing a similarity
    search between the sentence embeddings from the query and the documents in our
    vector store. The top-k most relevant sentence embeddings are returned as an output
    from this node.
  prefs: []
  type: TYPE_NORMAL
- en: Reader
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The reader node is a large language model that has been fine-tuned on question-answering
    tasks. For our use case, we leverage [Roberta-base-squad2](https://huggingface.co/deepset/roberta-base-squad2)
    as our reader model. The reader operates on the output from the retriever and
    the initial query from the user and returns the relevant span of text to answer
    the query. The reader will do this for each document in the retriever output,
    allocating a confidence score in each case. The answers are ranked by their score,
    and the top-k answers are returned.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full script for the pipeline is outlined below:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Script by author: Defining our Extractive question-answer pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '**Leveraging LLM-Powered Agents**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have pre-processed our documents and defined our pipeline, we can
    construct our agent. An agent is powered by a large language model, in our case,
    OpenAI’s gpt-4 (or gpt-3.5-turbo alternatively).
  prefs: []
  type: TYPE_NORMAL
- en: The agent we are using works on the basis of Zero-shot [ReAcT](https://www.promptingguide.ai/techniques/react)
    (Reason + Act) prompting. We prompt the large language model to return verbal
    reasoning traces and actions for a task. The agent can “act” on the verbal reasoning
    traces using a set of tools we provide it access to. The agent can observe and
    reason about the output from using the tools, helping it to inform its next action.
  prefs: []
  type: TYPE_NORMAL
- en: The animation below gives a simplified view of our ReAct agent at work.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0fc3988afd5cd5d727547d73b0400245.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Gif by author: Simplified view of a our agent at work'
  prefs: []
  type: TYPE_NORMAL
- en: '*Note:* [*Research*](https://arxiv.org/pdf/2210.03629.pdf) *suggests that ReAct
    prompting has been shown to effectively reduce hallucinations by LLMs.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The script below shows how we construct our agent:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Script by author: Defining our zero-shot ReAct agent'
  prefs: []
  type: TYPE_NORMAL
- en: In this use case, the tool we have provided the agent is the extractive question-answer
    pipeline we defined earlier. Essentially the agent can interpret our query, use
    our pipeline as a tool to find the relevant responses in our knowledge base, reason
    to see if this answers the query, choose to respond, or perform the action again
    to get a better answer.
  prefs: []
  type: TYPE_NORMAL
- en: A Brief Note on Tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It’s important that we define our tools clearly. For our particular use case,
    we only have one tool, but we need to provide a description of that tool so that
    the LLM-powered agent knows how and when to use the tool. This becomes even more
    important when an agent has access to more than just one tool.
  prefs: []
  type: TYPE_NORMAL
- en: '**Putting it to the Test**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So we have defined our RAG-enabled model; let’s put it to the test by asking
    it trivia on the Russia-Ukraine conflict of 2022\. I have chosen two questions,
    each of which can be answered with information from the wikipedia knowledge base.
    As well as the responses I provide the workflow trace for each agent, you should
    inspect this carefully to gain a better insight into how the ReAct prompting works
    with the agent.
  prefs: []
  type: TYPE_NORMAL
- en: How did the international community respond to Russia’s invasion of Ukraine
    in 2022?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Response from the RAG enabled model:'
  prefs: []
  type: TYPE_NORMAL
- en: The international community responded to Russia’s invasion of Ukraine in 2022
    with condemnation and the imposition of new sanctions. These sanctions were comprehensive,
    affecting individuals, banks, businesses, and various types of economic activity
    including monetary exchanges, bank transfers, exports, and imports. Some of the
    specific measures included prison sentences of up to 15 years.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Note: The prison sentence for up to 15 years appears to be a hallucination.
    The model has mistakenly suggested this as a sanction measure.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/26c5cd8bac5577b38f7edfb479eabe62.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Gif by author: Response to the question “How did the international community
    respond to Russia’s invasion of Ukraine in 2022?”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Response from Google search:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c7fcf3156e0cc6beb2969b48263dccc7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by author: Response from Google search for “How did the international
    community respond to Russia’s invasion of Ukraine in 2022?”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the trace tracking the agent’s workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Who led the IAEA team that investigated the plant on 29 August 2022?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Response from the RAG enabled model:'
  prefs: []
  type: TYPE_NORMAL
- en: Rafael Grossi led the IAEA team that investigated the plant on 29 August 2022.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/cde1812cfea100b4e511e008fc60fed9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Gif by author: Response to the question “Who led the IAEA team that investigated
    the plant on 29 August 2022?”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Response from Google search:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/32e0d5573db5f2f843d64a8ae2a3875d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by author: Response from Google search for “Who led the IAEA team that
    investigated the plant on 29 August 2022?”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the trace tracking the agent’s workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: For your own curiosity, have a look at the [responses](https://chat.openai.com/share/af96d7f7-1408-422a-923f-ced160313828)
    from ChatGPT for the same questions. You can even try to ask the questions yourself
    to confirm the response.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusion**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Retrieval augmented generation (RAG) enables a large language model to connect
    with an existing knowledge base. RAG-enabled language models have access to up-to-date
    information, making them more useful across a variety of use cases.
  prefs: []
  type: TYPE_NORMAL
- en: The retriever and reader methods enable the models to query large corpuses of
    text, overcoming the issues faced by the limited context of large language models
    by themselves. Open-source frameworks like Haystack make it easy to build a RAG-enabled
    LLM prototype quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Some key points to note here are that the performance of this method is only
    as good as the knowledge base provided. Also, the inference time can be sped up
    dramatically by deploying the model on suitable infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: For more complex queries, agents may get into situations where they can’t respond
    in the alotted amount of steps. It would be interesting to see how the quality
    of responses varies by increasing the amount of steps, or adding a memory component
    to make the pipeline more conversational. Leveraging a more heavy duty sentence
    embedding model could also serve to improve overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: The code base is in the [GitHub](https://github.com/john-adeojo/haystack-lfqa)
    repo (including the front-end) and is available for you to experiment with.
  prefs: []
  type: TYPE_NORMAL
- en: There is also a [YouTube](https://youtu.be/-3MLqZppSCY) demo of project.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re keen to enhance your skills in artificial intelligence, join the waiting
    list for [my course](https://www.data-centric-solutions.com/course), where I will
    guide you through the process of developing large language model powered applications.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re seeking AI-transformation for your business, book a discovery call
    today.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.brainqub3.com/?source=post_page-----99d5f86e9779--------------------------------)
    [## Brainqub3 | AI software development'
  prefs: []
  type: TYPE_NORMAL
- en: At Brainqub3 we develop bespoke AI software. We create qub3s, advanced artificial
    brains, using the latest AI to…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.brainqub3.com](https://www.brainqub3.com/?source=post_page-----99d5f86e9779--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: For more insights on artificial intelligence, data science, and large language
    models you can subscribe to the [YouTube](https://www.youtube.com/channel/UCkXe-exqi25V4GnZendgEaA)
    channel.
  prefs: []
  type: TYPE_NORMAL
