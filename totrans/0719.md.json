["```py\n # Dummy data\nnp.random.seed(0)\nX = 2 * np.random.rand(100, 1)\ny = 1 + 2 * X + np.random.randn(100, 1)\n\n# Model\nclass LinearRegression(nn.Module):\n    def __init__(self):\n        super(LinearRegression, self).__init__()\n        self.linear = nn.Linear(1, 1)\n\n    def forward(self, x):\n        return self.linear(x)\n\n# Training\nfor epoch in range(num_epochs):\n    # Forward pass\n    outputs = model(X_tensor)\n    loss = criterion(outputs, y_tensor)\n\n    # Backward pass and optimization\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n```", "```py\n# save model as a torchscript model\ntorch.jit.save(torch.jit.script(model), 'model.pt')\n\n# Load the saved model\nloaded_model = torch.jit.load('model.pt')\n\n# sample inference\ntest = torch.tensor([[2.5]])\npred = loaded_model(test)\nprint(pred)\n```", "```py\nname: \"linear_regression_model\"\nplatform: \"pytorch_libtorch\"\n\ninput {\n  name: \"input\"\n  data_type: TYPE_FP32\n  dims: [ 1, 1 ]\n}\n\noutput {\n  name: \"output\"\n  data_type: TYPE_FP32\n  dims: [ 1, 1 ]\n}\n```", "```py\n- models/\n  - linear_regression_model\n    - 1\n       - model.pt\n       - model.py (optional, not included here)\n    - config.pbtxt\n  - optional incldue any other models for example if you have an ensemble\n```", "```py\nmkdir linear_regression_model\nmv config.pbtxt model.pt linear_regression_model\ncd linear_regression_model\nmkdir 1\nmv model.pt 1/\ncd ..\n```", "```py\ndocker pull nvcr.io/nvidia/tritonserver:23.08-py3\n```", "```py\ndocker run --gpus all --rm -p 8000:8000 -p 8001:8001 -p 8002:8002 \n-v /home/ec2-user/SageMaker:/models nvcr.io/nvidia/tritonserver:23.08-py3 \ntritonserver --model-repository=/models --exit-on-error=false --log-verbose=1\n```", "```py\n# sample data\ninput_data = np.array([[2.5]], dtype=np.float32)\n\n# Specify the model name and version\nmodel_name = \"linear_regression_model\" #specified in config.pbtxt\nmodel_version = \"1\"\n\n# Set the inference URL based on the Triton server's address\nurl = f\"http://localhost:8000/v2/models/{model_name}/versions/{model_version}/infer\"\n\n# payload with input params\npayload = {\n    \"inputs\": [\n        {\n            \"name\": \"input\",  # what you named input in config.pbtxt\n            \"datatype\": \"FP32\",  \n            \"shape\": input_data.shape,\n            \"data\": input_data.tolist(),\n        }\n    ]\n}\n\n# sample invoke\nresponse = requests.post(url, data=json.dumps(payload))\n```", "```py\nimport tritonclient.http as httpclient #pip install if needed\n\n# setup triton inference client\nclient = httpclient.InferenceServerClient(url=\"localhost:8000\")\n\n# triton can infer the inputs from your config values\ninputs = httpclient.InferInput(\"input\", input_data.shape, datatype=\"FP32\")\ninputs.set_data_from_numpy(input_data) #we set a numpy array in this case\n\n# output configuration\noutputs = httpclient.InferRequestedOutput(\"output\")\n\n#sample inference\nres = client.infer(model_name = \"linear_regression_model\", inputs=[inputs], outputs=[outputs],\n                  )\ninference_output = res.as_numpy('output') #serialize numpy output\n```"]