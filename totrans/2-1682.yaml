- en: Primer on Bayesian Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/primer-on-bayesian-deep-learning-d06e0601c2ae](https://towardsdatascience.com/primer-on-bayesian-deep-learning-d06e0601c2ae)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Probabilistic Deep Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@luisroque?source=post_page-----d06e0601c2ae--------------------------------)[![Luís
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----d06e0601c2ae--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d06e0601c2ae--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d06e0601c2ae--------------------------------)
    [Luís Roque](https://medium.com/@luisroque?source=post_page-----d06e0601c2ae--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d06e0601c2ae--------------------------------)
    ·8 min read·Feb 1, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article belongs to the series “Probabilistic Deep Learning”. This weekly
    series covers probabilistic approaches to deep learning. The main goal is to extend
    deep learning models to quantify uncertainty, i.e., know what they do not know.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian Deep Learning is an emerging field that combines the expressiveness
    and representational power of deep learning with the uncertainty modeling capabilities
    of Bayesian methods. The integration of these two paradigms offers a principled
    framework for addressing various challenges in deep learning, such as overfitting,
    weight uncertainty, and model comparison.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we provide a comprehensive introduction to Bayesian Deep Learning,
    covering its foundations, methodology, and recent advances. Our aim is to present
    the fundamental concepts and ideas in a clear and accessible manner, making it
    an ideal resource for researchers and practitioners who are new to the field.
  prefs: []
  type: TYPE_NORMAL
- en: 'Articles published so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Gentle Introduction to TensorFlow Probability: Distribution Objects](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-distribution-objects-1bb6165abee1)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Gentle Introduction to TensorFlow Probability: Trainable Parameters](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-trainable-parameters-5098ea4fed15)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Maximum Likelihood Estimation from scratch in TensorFlow Probability](/maximum-likelihood-estimation-from-scratch-in-tensorflow-probability-2fc0eefdbfc2)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Probabilistic Linear Regression from scratch in TensorFlow](/probabilistic-linear-regression-from-scratch-in-tensorflow-2eb633fffc00)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Probabilistic vs. Deterministic Regression with Tensorflow](https://medium.com/towards-data-science/probabilistic-vs-deterministic-regression-with-tensorflow-85ef791beeef)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Frequentist vs. Bayesian Statistics with Tensorflow](https://medium.com/towards-data-science/frequentist-vs-bayesian-statistics-with-tensorflow-fbba2c6c9ae5)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Deterministic vs. Probabilistic Deep Learning](/deterministic-vs-probabilistic-deep-learning-5325769dc758)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Naive Bayes from scratch with TensorFlow](/naive-bayes-from-scratch-with-tensorflow-6e04c5a25947)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Probabilistic Logistic Regression with TensorFlow](https://medium.com/towards-data-science/probabilistic-logistic-regression-with-tensorflow-73e18f0ddc48)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bayesian Deep Learning
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/12127b517c9ad0820110791614e60d31.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Motto for today: it is layers all the way down ([source](https://unsplash.com/photos/1fzR7q6GB6A))'
  prefs: []
  type: TYPE_NORMAL
- en: Deterministic, Probabilistic and Bayesian Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning has achieved remarkable success in a wide range of applications,
    including computer vision, natural language processing, and game playing. Despite
    its successes, traditional deep learning models are inherently deterministic and
    provide limited ability to quantify uncertainty in their predictions. To address
    this issue, Bayesian deep learning and probabilistic deep learning have emerged
    as important paradigms that allow for incorporating uncertainty into deep learning
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian deep learning and probabilistic deep learning represent important paradigms
    for incorporating uncertainty into deep learning models. These approaches offer
    several advantages over traditional deterministic deep learning, including the
    ability to provide uncertainty estimates and the ability to perform robust inference
    in the presence of out-of-distribution data.
  prefs: []
  type: TYPE_NORMAL
- en: In Bayesian deep learning, the model parameters are treated as random variables
    and a prior distribution is placed over them. This prior represents prior knowledge
    about the model parameters, such as their expected values or their distributional
    shape. The posterior distribution over the parameters is then updated through
    Bayesian inference, using the data to form a posterior that represents our updated
    beliefs about the parameters given the data. This results in a distribution over
    the model parameters and a measure of uncertainty in the model predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Probabilistic deep learning, on the other hand, models the data-generating process
    as a probabilistic function. Given an input, the model predicts a distribution
    over outputs, allowing for the quantification of uncertainty in predictions. This
    approach is particularly useful for problems where the output space is complex,
    such as image generation or speech synthesis. In these cases, modeling the data-generating
    process as a probabilistic function allows for the capture of complex patterns
    in the data and the generation of high-quality outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Both Bayesian deep learning and probabilistic deep learning are important and
    active areas of research, with recent advances in techniques such as Bayesian
    neural networks, variational inference, and deep generative models. Despite their
    potential benefits, these approaches remain challenging due to the computational
    and statistical difficulties posed by high-dimensional models and the large amounts
    of data typically used in deep learning. Nevertheless, significant progress has
    been made in this field in recent years, and there is much excitement and potential
    for future research in Bayesian deep learning and probabilistic deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bayesian Learning: A Primer'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we consider the Bayesian framework for statistical inference,
    where we represent probability density with the notation 𝑃.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bayesian methods provide a unified approach for conducting statistical inference,
    where we aim to compute the distribution of model parameters given some observed
    data. In the context of neural networks, this is particularly useful for estimating
    the uncertainty in weight parameters, as we can calculate the posterior distribution
    of these parameters given the training data. This is achieved by applying Bayes’
    theorem, which states:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1e00cd974ec578996564be058a73c5a5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: '𝐷: The observed data, represented as pairs of 𝑥 and 𝑦 values, e.g. 𝐷={(𝑥1,𝑦1),…,(𝑥𝑛,𝑦𝑛)}'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '𝑤: The value of a model weight'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '𝑃(𝑤): The prior density, representing our initial belief on the distribution
    of model weights before seeing the data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '𝑃(𝐷|𝑤): The likelihood of observing the data given the weight 𝑤'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '𝑃(𝑤|𝐷): The posterior density of the model weight, calculated by incorporating
    the observed data and prior belief through Bayes’ theorem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The normalization term ∫𝑃(𝐷|𝑤′)𝑃(𝑤′)d𝑤′=𝑃(𝐷) is independent of 𝑤 and serves
    to normalize the posterior density.
  prefs: []
  type: TYPE_NORMAL
- en: Bayes’ theorem enables us to synthesize the observed data with our prior belief
    to obtain the posterior distribution of model parameters, providing a probabilistic
    representation of their uncertainty. The implementation of Bayesian learning in
    the context of neural networks can be challenging due to the complexity of calculating
    the normalization constant. To overcome this, various approximate methods such
    as Variational Bayes are utilized. These methods aim to efficiently estimate the
    posterior distribution of the NN weights.
  prefs: []
  type: TYPE_NORMAL
- en: Variational Bayesian Inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Variational Bayes, we approximate the posterior distribution with a second
    function, known as the variational posterior. This function is parameterized by
    some set of parameters, denoted by 𝜃, which can be learned through optimization.
    The key idea is to choose a functional form for the variational posterior such
    that it can be effectively optimized to approximate the true posterior as closely
    as possible.
  prefs: []
  type: TYPE_NORMAL
- en: One common approach to measuring the quality of the approximation is through
    the use of the Kullback-Leibler (KL) divergence between the variational posterior
    and the true posterior. The KL divergence is a measure of the difference between
    two probability distributions, and its value is 0 when the two distributions are
    equal.
  prefs: []
  type: TYPE_NORMAL
- en: Given the observed data, 𝐷, the KL divergence between the variational posterior,
    𝑞(𝑤|𝜃), and the true posterior, 𝑃(𝑤|𝐷), is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c6241bd40be3a89f3550ea48e775964a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When considering the observed data as constant, we can simplify this expression
    to a function that depends only on 𝜃 and 𝐷:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0ebf632c807e607833aee3ca46b04c29.png)'
  prefs: []
  type: TYPE_IMG
- en: where the first term measures the discrepancy between the variational and prior
    distributions, and the second term is the expected negative log likelihood under
    the variational posterior.
  prefs: []
  type: TYPE_NORMAL
- en: This function, known as the variational lower bound, can then be optimized with
    respect to 𝜃 to obtain the best possible approximation of the true posterior.
    The optimization can be performed using gradient-based methods, making it possible
    to scale Variational Bayes to large, complex models.
  prefs: []
  type: TYPE_NORMAL
- en: The Backpropagation Scheme for Bayesian Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we describe a backpropagation algorithm for Bayesian neural
    networks, which allow us to incorporate weight uncertainty into our models. Our
    approach involves introducing a variational posterior with density 𝑞(𝑤|𝜃), where
    𝑤 is a weight in the network and 𝜃 is a set of trainable parameters. This posterior
    represents our approximation of the true weight posterior, which is determined
    by the observed data and our prior beliefs about the weights encoded by the prior
    density 𝑃(𝑤).
  prefs: []
  type: TYPE_NORMAL
- en: 'Our goal is to update the variational posterior so that it accurately approximates
    the true weight posterior, which we quantify by the Evidence Lower Bound (ELBO).
    To do so, we minimize the ELBO between the variational posterior and the prior,
    which can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/76fb86a007c33e8767c16d006e8a9d5f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where 𝐷 is the training data. This expression, however, involves an integral
    over the weight 𝑤 which may be computationally intractable. To circumvent this
    difficulty, we re-write the ELBO as an expectation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/95494b29915faa629278884b262d0660.png)'
  prefs: []
  type: TYPE_IMG
- en: In order to perform backpropagation, we need to take derivatives of *𝐿*(*𝜃*|*𝐷*)
    with respect to *𝜃*. However, this is challenging because the underlying distribution
    with respect to which the expectation is taken depends on *𝜃*. We overcome this
    issue through the reparameterization trick, which allows us to differentiate through
    the expectation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Reparameterization Trick: A Key Technique for Stochastic Gradient Optimization
    in Bayesian Neural Networks'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The reparameterization trick is a mathematical technique used to enable the
    optimization of models with stochastic parameters. The goal is to convert the
    dependent variable, 𝑤, into a random variable, 𝜖, with a fixed distribution independent
    of model parameters, 𝜃. This conversion allows the expectation of 𝑓(𝑤;𝜇,𝜎) with
    respect to 𝑞(𝑤|𝜇,𝜎) to be calculated independently of 𝜃, enabling efficient optimization
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, consider a Gaussian distribution 𝑞(𝑤|𝜇,𝜎) where 𝜃=(𝜇,𝜎). Using
    a change of variables 𝑤=𝜇+𝜎𝜖, where 𝜖∼𝑁(0,1), we obtain
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e57ba17221fe7b4704dc142ae6f77473.png)'
  prefs: []
  type: TYPE_IMG
- en: The derivatives with respect to 𝜇 and 𝜎 can then be easily calculated, and the
    expectations can be estimated through Monte Carlo sampling.
  prefs: []
  type: TYPE_NORMAL
- en: The reparameterization trick is a crucial tool for efficient optimization in
    Bayesian Neural Networks and has been widely adopted in modern deep learning research.
    It enables the gradient-based optimization of models with stochastic parameters,
    allowing for efficient training and inference.
  prefs: []
  type: TYPE_NORMAL
- en: The Final Connection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bayesian deep learning offers a framework for incorporating uncertainty into
    deep learning models. By treating neural network weights as random variables,
    we can capture both aleatoric and epistemic uncertainty, allowing for more robust
    and reliable predictions. Variational Bayesian inference provides a way to learn
    the parameters of the posterior distribution over network weights, which can be
    optimized using the backpropagation scheme.
  prefs: []
  type: TYPE_NORMAL
- en: The reparameterization trick, a key technique for stochastic gradient optimization
    in Bayesian neural networks, allows us to estimate gradients efficiently by transforming
    random variables into differentiable forms. This trick enables the use of standard
    gradient-based optimization algorithms to learn the parameters of the posterior
    distribution over network weights.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that while aleatoric uncertainty accounts for the inherent
    noise in the data, epistemic uncertainty captures the uncertainty in our knowledge
    of the underlying model. In Bayesian deep learning, epistemic uncertainty is modeled
    by placing a prior distribution over network weights and updating it with observed
    data through the variational inference process. This results in a more flexible
    and informative model, as the network can adapt to new data and incorporate additional
    information about the underlying relationships.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In conclusion, Bayesian Deep Learning provides a framework for incorporating
    uncertainty into the predictions of Deep Neural Networks. By treating the network
    weights as random variables and modeling their posterior distributions, we can
    obtain more robust and informative models compared to traditional deterministic
    deep learning approaches.
  prefs: []
  type: TYPE_NORMAL
- en: The reparameterization trick, combined with gradient-based optimization algorithms,
    enables scalable Bayesian inference in deep models. As a result, Bayesian Deep
    Learning has gained increasing attention and found success in various applications
    including active learning, out-of-distribution detection, and uncertainty-aware
    reinforcement learning. The field is still relatively new, and there is much room
    for further exploration and innovation.
  prefs: []
  type: TYPE_NORMAL
- en: In the future, we expect Bayesian Deep Learning to play an increasingly important
    role in the advancement of artificial intelligence and machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: About me
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Serial entrepreneur and leader in the AI space. I develop AI products for businesses
    and invest in AI-focused startups.
  prefs: []
  type: TYPE_NORMAL
- en: '[Founder @ ZAAI](http://zaai.ai) | [LinkedIn](https://www.linkedin.com/in/luisbrasroque/)
    | [X/Twitter](https://x.com/luisbrasroque)'
  prefs: []
  type: TYPE_NORMAL
- en: References and Materials
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] — [Coursera: Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] — [Coursera: TensorFlow 2 for Deep Learning](https://www.coursera.org/specializations/tensorflow2-deeplearning)
    Specialization'
  prefs: []
  type: TYPE_NORMAL
