- en: Modeling EEG Signals using Polynomial Regression in R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/modeling-eeg-signals-using-polynomial-regression-in-r-bd1b0da08251](https://towardsdatascience.com/modeling-eeg-signals-using-polynomial-regression-in-r-bd1b0da08251)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: selecting best model with polynomial regression from scratch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@maladeep.upadhaya?source=post_page-----bd1b0da08251--------------------------------)[![Mala
    Deep](../Images/9d542f8a1c2448065aa110e58e6259e6.png)](https://medium.com/@maladeep.upadhaya?source=post_page-----bd1b0da08251--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bd1b0da08251--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bd1b0da08251--------------------------------)
    [Mala Deep](https://medium.com/@maladeep.upadhaya?source=post_page-----bd1b0da08251--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bd1b0da08251--------------------------------)
    ·12 min read·Jun 15, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/97592fae65e69dc98967db098aaa0bd2.png)'
  prefs: []
  type: TYPE_IMG
- en: Timeseries plot of input EEG signals for polynomial regression in R. Image by
    the author.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to EEG signals
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: EEG stands for electroencephalogram, which is an electrical signal that measures
    the electrical activity of the brain [1]. To get the EEG result, electrodes consisting
    of small metal discs with thin wires are pasted onto the scalp. The electrodes
    detect tiny electrical charges that result from the activity of your brain cells,
    and the thus obtained charges are amplified and appear as a graph on a computer
    screen or as a recording that may be printed out on paper. The main purpose of
    EEG is to detect potential problems (encephalitis, hemorrhage, epilepsy, Parkinson’s
    disease, and others) with brain cell communication by a painless method [2].
  prefs: []
  type: TYPE_NORMAL
- en: Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our objectives are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: To assess the fit of the polynomial regression model to the EEG signals and
    estimate the parameters of the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To use a simulation-based approach to estimate the posterior distribution of
    the model parameters, allowing for uncertainty in the model and data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get access to the complete code on [GitHub](https://github.com/maladeep/Modeling-EEG-signals-using-polynomial-regression-in-R)
    or view the book on [RPubs](https://rpubs.com/mala101/eegmodeling).
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/maladeep/Modeling-EEG-signals-using-polynomial-regression-in-R?source=post_page-----bd1b0da08251--------------------------------)
    [## GitHub - maladeep/Modeling-EEG-signals-using-polynomial-regression-in-R: Modeling
    EEG signals using…'
  prefs: []
  type: TYPE_NORMAL
- en: This repository provides an implementation of modeling EEG signals using polynomial
    regression. The code and analysis…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/maladeep/Modeling-EEG-signals-using-polynomial-regression-in-R?source=post_page-----bd1b0da08251--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Data set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will be using the [dataset](https://sccn.ucsd.edu/~arno/fam2data/publicly_available_EEG_data.html)
    provided by the [Swartz Center for Computational Neuroscience](https://sccn.ucsd.edu/),
    which contains time, four input signals (x1, x2, x3, and x4), and one output signal
    (y).
  prefs: []
  type: TYPE_NORMAL
- en: 'Below is a sample of the data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e3e73ef7920b4903997a2eaad5464cd7.png)'
  prefs: []
  type: TYPE_IMG
- en: Snapshot of the dataset provided by [Swartz Center for Computational Neuroscience](https://sccn.ucsd.edu/).
    Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Challenge
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, we are going to explain the relationship between the input EEG signals
    and the output EEG signals based on the assumption that the relationship can be
    expressed as a polynomial regression model.
  prefs: []
  type: TYPE_NORMAL
- en: We are given five different nonlinear polynomial regression models, out of which
    we need to find the one that is most suitable.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4aece3c5c09e8bede206c86dec9bbf48.png)'
  prefs: []
  type: TYPE_IMG
- en: We are given five different nonlinear polynomial regression models. Image by
    the author
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve the problem, the following steps would be taken:'
  prefs: []
  type: TYPE_NORMAL
- en: Using least square, estimate the model parameter
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculating model residual errors (RSS)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculating log-likelihood functions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculating Akaike Information Criterion (AIC) and Bayesian Information Criteria
    (BIC)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Checking the distribution of model prediction errors
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Selecting the best regression model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Step 1\. Using least square, estimate the model parameter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we have no idea (unknown) about the true value of the distribution, we
    use the concept of an estimator (random variable) [3]. In other words, we are
    using the estimator variable to estimate the true value of the EEG data distribution
    that relates the input and output variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the estimator variable is represented by “θ” and can take on multiple
    values such as θ1, θ2, …, θbias. Now, the least squares method (LSM) is used to
    calculate the estimator model parameters for different candidate models of EEG
    data, and the LSM (θ̂) is used to estimate the true value of the distribution
    by minimizing the sum of the squared residuals between the predicted and actual
    values of the output variable [4], which is expressed by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/46117ddc87ebbca70f87f61dad6132b5.png)'
  prefs: []
  type: TYPE_IMG
- en: Formula for Least square estimator. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Now, to calculate the least squares, we first need to format the input data
    by binding the appropriate columns or values from the EEG data set. With the function
    cbind(). Once the input data is correctly formatted, we can then use the least
    squares formula as mentioned above, and using the built-in solve linear equations
    function called solve(), we find the θ̂. We use solve() because it’s more efficient
    and less error-prone [5].
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, as we have 5 models, for each model, we will follow the same process:
    creating cbind, calculating estimated values of the model parameters, θ̂ and printing
    the value of theta_hat.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1faf44a062e19de51759fede320f05be.png)'
  prefs: []
  type: TYPE_IMG
- en: θ̂ for each 5 models. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2\. Calculating model residual errors (RSS)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The residual sum of squares (RSS), also termed the sum of squared errors of
    prediction (SSE) or the sum of squared residuals (SSR), is a measure of discrepancy
    between the data and an estimation model. It is calculated by subtracting the
    average square of the actual values from the estimated values of the dependent
    variable based on the model parameters [6].
  prefs: []
  type: TYPE_NORMAL
- en: We usually want to minimize the error, so the smaller the error, the better
    the estimation power of the regression and the better the fit of the model. On
    the other hand, a larger RSS indicates a worse fit of the model to the data [7].
    It is worth mentioning that the RSS is never negative because the squares of the
    residuals are always non-negative [8].
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate RSS, we first have to calculate the error of every model 1–5 with
    the help of (θ̂) that we calculated in the above step, and RSS is mathematically
    presented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d111072fa864f3b384322e9fca9d5a21.png)'
  prefs: []
  type: TYPE_IMG
- en: Formula for model residual errors (RSS). Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'For calculating RSS for model 1, we have the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we will calculate the RSS value for each model.
  prefs: []
  type: TYPE_NORMAL
- en: So, we have an RSS value for each model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/75e6f0d52492c286292cc18038fec62b.png)'
  prefs: []
  type: TYPE_IMG
- en: Table showing the RSS value of each model. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Here, the lowest RSS value in the table is 2.1355, which is associated with
    model 5, followed by the second lowest RSS value of 2.1398, which is associated
    with model 2.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3\. Calculating log-likelihood functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, our objective is to identify how well the measured value fits the sample
    data of a provided model when parameters are unknown. To meet our objective, we
    are going to calculate log-likelihood functions for a linear regression model
    using the RSS that we obtained from step 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Log-likelihood is a way to measure the goodness of fit for a model [9] and
    is used to simplify the optimization problem and avoid numerical underflow or
    overflow. Mathematically presented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/099e19f5dc8ddc3210223189590bda15.png)'
  prefs: []
  type: TYPE_IMG
- en: Formula for Log-likelihood. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: In this task, our goal is to find the set of parameters that maximizes the probability
    of the observations. As the nature of the log-likelihood function is that it increases
    monotonically (non-decreasing) and has no local maxima, it is suitable for identifying
    how well the measured value fits [10]. In layman’s terms, monotonically increasing
    means that as the value of the independent variable (say x) increases, so does
    the value of the function (say y), i.e., as x increases, y can only increase and
    cannot ever decrease.
  prefs: []
  type: TYPE_NORMAL
- en: So, here, as the value of the log-likelihood increases, the likelihood of the
    data given the model parameters also increases. Therefore, finding the maximum
    of the log-likelihood function is the same as finding the maximum of the likelihood
    function, but if we go with just likelihood, then the concave nature of log is
    missing and we cannot get the one global maxima [11].
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, using the above formula, we will first calculate the variance of
    the model using RSS along with the length of the Y signal and then calculate the
    log-likelihood function.
  prefs: []
  type: TYPE_NORMAL
- en: Using the same formula, we calculate the rest of the models.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Calculating Akaike Information Criterion (AIC) and Bayesian Information
    Criteria (BIC)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now as we have RSS and log-likelihood value, we need model selection criteria
    method for which we will work with Akaike information criterion (AIC) and Bayesian
    Information Criteria (BIC). According to [12], model selection involves estimating
    the performance of various candidate models with the sole objective of choosing
    the best one.
  prefs: []
  type: TYPE_NORMAL
- en: Both of them can be used to compare different models and choose the best one.
    They are both based on the likelihood of the model given the data and the number
    of parameters in the model. However, the main difference between these two-model
    selection methods is that AIC gives less penalty to models with more parameters
    compared to BIC [7].
  prefs: []
  type: TYPE_NORMAL
- en: '**4.1 Calculating AIC for each model**'
  prefs: []
  type: TYPE_NORMAL
- en: Akaike information criterion (AIC) is a statistical method that aims to determine
    the model that truly explains the variance in the dependent variable with the
    fewest number of independent variables (parameters) [13]. With this, it helps
    to select a simpler model containing fewer parameters over a complex model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the maximum likelihood estimate (step 3), the relative information value
    of the model and the number of parameters are determined. The formula for AIC
    is expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/67dea92e4837dbe9f0d58127812a8d2f.png)'
  prefs: []
  type: TYPE_IMG
- en: Formula for Akaike information criterion (AIC). Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: The major goal of applying AIC in this situation is to eliminate the problem
    of overfitting because AIC penalizes models with more parameters and balances
    the trade-off between a model’s goodness of fit and complexity [14] [7].
  prefs: []
  type: TYPE_NORMAL
- en: According to [15], one thing worth mentioning is that the model fits the data
    better when the AIC value lower, and the AIC absolute value could be favorable
    or unfavorable.
  prefs: []
  type: TYPE_NORMAL
- en: Before going into the code for AIC, let’s see about BIC.
  prefs: []
  type: TYPE_NORMAL
- en: '**4.2 Calculating BIC for each model**'
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned earlier, BIC is similar to AIC, but BIC will give a greater penalty
    on models with more parameters [16]. Similar to AIC, lower BIC values indicate
    better model fit. The formula for BIC is expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c983cfafe3f7d861fa8f154b1bbe0a33.png)'
  prefs: []
  type: TYPE_IMG
- en: Formula for Bayesian Information Criteria (BIC). Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Using the above formula, we calculated BIC for each model, as which is similar
    to AIC.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the above formula, AIC and BIC for each model are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d8d83c2614855c5053c47328f324fd79.png)'
  prefs: []
  type: TYPE_IMG
- en: AIC and BIC value of each models. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: As we recall, AIC and BIC with lower values are the best fit, so model 2 with
    an AIC of -334.6489 and a BIC of -321.4357 is the best fit among the models listed.
  prefs: []
  type: TYPE_NORMAL
- en: Step 5\. Checking the distribution of model prediction errors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have obtained the AIC and BIC values, we are interested in viewing
    the error distribution. After all, our goal is to pick one that shows the least
    error. Before going with the decision to graph the distribution, we need to calculate
    the error of each model. Then we will use a Q-Q plot (quantile-quantile plot)
    to visualize and compare two probability distributions using the qqnorm() function,
    as our assumption is that the data are independent and identically distributed.
  prefs: []
  type: TYPE_NORMAL
- en: According to [17], a Q-Q plot is formed by plotting two quantile sets against
    each other. In the case of the same distribution, both sets of quantiles would
    form a relatively straight line; however, in practice, this does not mean a hard
    and fast rule. In the same plot, we are going to add a reference line called the
    Q-Q line, which is the line of perfect fit for a normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The qqline() function takes two arguments: the first is the data model’s prediction
    errors, and the second is the color (col), the width of the line (lw), and a dashed
    line (lty).'
  prefs: []
  type: TYPE_NORMAL
- en: The Q-Q plot for model 5 shows that most of the data follows the Q-Q line (red
    color), so we can say the data follows a normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Here, with the obtained Q-Q plot, we simply visually check if a data set follows
    a theoretical distribution or not. To formally test whether or not a data set
    follows a particular distribution, we need to go one step further.
  prefs: []
  type: TYPE_NORMAL
- en: Step 6\. Selecting the best regression model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By completing steps 1–5, we have gathered all the necessary information to select
    the best candidate model. By calculating the RSS, log-likelihood function, plotting
    normal distribution graphs, and comparing the AIC and BIC values, we have all
    the information to identify the best model for our data. The best model fit based
    on AIC and BIC would be model 2, as it has the lowest value. To verify that the
    selected model 2 is a good candidate, we will look at the Q-Q plot.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the Q-Q plot, except for model 3, all models seem to have the same
    nature; however, looking at the position of the Q-Q line, model 2 seems to be
    the most suitable one.
  prefs: []
  type: TYPE_NORMAL
- en: To get more inclination toward the decision of picking up model 2, we would
    like to plot a histogram to show the distribution of residuals. For easy viewing,
    we will plot all the histogram into 3 rows and 2 columns using par(mfrow = c(3,
    2)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/55aa9a52f595f429a4a245f9056779ee.png)'
  prefs: []
  type: TYPE_IMG
- en: '`*Distribution of prediction error using histogram. Image by the author.*`'
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the distribution of each model, models 2, 5, and 6 both seem to have
    a normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we will go one step further and see if model 2 is more suitable
    than the rest. Considering additional factors will help us determine the best
    model in this scenario. We will compare the number of parameters in each model
    based on the interpretability of the model, i.e., a simpler model with fewer parameters
    is easier to interpret and understand [18].
  prefs: []
  type: TYPE_NORMAL
- en: Looking at each length of parameter, the lowest is model 3, with 3 numbers of
    parameters, but it doesn’t follow a normal distribution and is skewed, and the
    next is model 4, with 4 parameters, but its AIC and BIC are greater than those
    of model 2.
  prefs: []
  type: TYPE_NORMAL
- en: So as a conclusion of AIC, BIC, Q-Q plot, and extra interpretability, we have
    picked up model 2 as the best fit model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/191b24dbaeb85b53e7662da6b656dfcb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Unveiling the champion: model 2 emerges as the optimal fit for EEG signal modeling
    using polynomial regression in R. Image by the author.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, by using the least squares estimating model parameters, model residual
    errors (RSS), log-likelihood functions, Akaike Information Criterion(AIC) and
    Bayesian Information Criterion(BIC), and visualizing prediction errors with Q-Q
    plots, we unveil the champion: Model 2 emerges as the optimal fit for EEG signal
    modeling using polynomial regression in R.'
  prefs: []
  type: TYPE_NORMAL
- en: This completes our polynomial regression in R.
  prefs: []
  type: TYPE_NORMAL
- en: '*My* [*GitHub repository*](https://github.com/maladeep/Modeling-EEG-signals-using-polynomial-regression-in-R)
    *has the entire working code available, or you can access* [*RPubs*](https://rpubs.com/mala101/eegmodeling)
    *to see it online.*'
  prefs: []
  type: TYPE_NORMAL
- en: '***Loved the article?*** *Unlock unlimited learning by becoming a Medium member*.
    *By using the* [*following link*](https://medium.com/@maladeep.upadhaya/subscribe)*,
    you can support me while joining, at no additional cost.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Please reach out to me on* [*LinkedIn*](https://www.linkedin.com/in/maladeep/)
    *if you have any questions about the article or are interested in collaborating.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] St, E.K., Frey, L.C., Britton, J.W., Frey, L.C., Hopp, J.L., Pearce Korb,
    Koubeissi, M.Z., Lievens, W.E., Pestana-Knight, E.M. and St, E.K. (2016). Introduction
    Electroencephalography (EEG): An Introductory Text and Atlas of Normal and Abnormal
    Findings in Adults, Children, and Infants [Internet]. [online] Nih.gov. Available
    at: [https://www.ncbi.nlm.nih.gov/books/NBK390346/](https://www.ncbi.nlm.nih.gov/books/NBK390346/).'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Healthline (2012). EEG (Electroencephalogram): Purpose, Procedure, and
    Risks. [online] Healthline. Available at: [https://www.healthline.com/health/eeg](https://www.healthline.com/health/eeg](https://www.healthline.com/health/eeg))'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Peterka, V. (1981). Chapter 8 — Bayesian Approach To System Identification.
    [online] ScienceDirect. Available at: [https://www.sciencedirect.com/science/article/pii/B9780080256832500132](https://www.sciencedirect.com/science/article/pii/B9780080256832500132](https://www.sciencedirect.com/science/article/pii/B9780080256832500132))
    [Accessed 31 Jan. 2023].'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Björck, Å. (1990). Least squares methods. [online] ScienceDirect. Available
    at: [https://www.sciencedirect.com/science/article/abs/pii/S1570865905800365](https://www.sciencedirect.com/science/article/abs/pii/S1570865905800365](https://www.sciencedirect.com/science/article/abs/pii/S1570865905800365))[Accessed
    4 Oct. 2022].'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Schork, J. (n.d.). Solve System of Equations in R (3 Examples) | Using
    solve() Function. [online] Statistics Globe. Available at: [https://statisticsglobe.com/solve-system-of-equations-in-r/](https://statisticsglobe.com/solve-system-of-equations-in-r/](https://statisticsglobe.com/solve-system-of-equations-in-r/))'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] Allen, D.M. (1971). Mean Square Error of Prediction as a Criterion for
    Selecting Variables. Technometrics, 13(3), pp.469–475\. doi:10.1080/00401706.1971.10488811'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] Brownlee, J. (2019). Probabilistic Model Selection with AIC, BIC, and MDL.
    [online] Machine Learning Mastery. Available at: [https://machinelearningmastery.com/probabilistic-model-selection-measures/](https://machinelearningmastery.com/probabilistic-model-selection-measures/](https://machinelearningmastery.com/probabilistic-model-selection-measures/))'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] Valchanov, I. (2018). Sum of Squares: SST, SSR, SSE. [online] 365 Data
    Science. Available at: [https://365datascience.com/tutorials/statistics-tutorials/sum-squares/](https://365datascience.com/tutorials/statistics-tutorials/sum-squares/](https://365datascience.com/tutorials/statistics-tutorials/sum-squares/))'
  prefs: []
  type: TYPE_NORMAL
- en: '[9] Zach (2021a). How to Interpret Log-Likelihood Values (With Examples). [online]
    Statology. Available'
  prefs: []
  type: TYPE_NORMAL
- en: '[10] Stephanie (2021). Log Likelihood Function. [online] Statistics How To.
    Available at: [https://www.statisticshowto.com/log-likelihood-function/](https://www.statisticshowto.com/log-likelihood-function/](https://www.statisticshowto.com/log-likelihood-function/))'
  prefs: []
  type: TYPE_NORMAL
- en: '[11] Music, A. (2020). Gaussian Distribution and Maximum Likelihood Estimate
    Method (Step-by-Step). [online] The Startup. Available at: [https://medium.com/swlh/gaussian-distribution-and-maximum-likelihood-estimate-method-step-by-step-e4f6014fa83e](https://medium.com/swlh/gaussian-distribution-and-maximum-likelihood-estimate-method-step-by-step-e4f6014fa83e](https://medium.com/swlh/gaussian-distribution-and-maximum-likelihood-estimate-method-step-by-step-e4f6014fa83e))
    [Accessed 31 Jan. 2023].'
  prefs: []
  type: TYPE_NORMAL
- en: '[12] Trevor Hastie, Tibshirani, R. and Friedman, J. (2009). The Elements of
    Statistical Learning. Editorial: New York, Ny Springer New York.'
  prefs: []
  type: TYPE_NORMAL
- en: '[13] Manikantan, A. (2021). Akaike Information Criterion: Model Selection.
    [online] Geek Culture. Available at: [https://medium.com/geekculture/akaike-information-criterion-model-selection-c47df96ee9a8](https://medium.com/geekculture/akaike-information-criterion-model-selection-c47df96ee9a8](https://medium.com/geekculture/akaike-information-criterion-model-selection-c47df96ee9a8))'
  prefs: []
  type: TYPE_NORMAL
- en: '[14] Bevans, R. (2020). Akaike Information Criterion | When & How to Use It
    (Example). [online] Scribbr. Available at: [https://www.scribbr.com/statistics/akaike-information-criterion/](https://www.scribbr.com/statistics/akaike-information-criterion/#](https://www.scribbr.com/statistics/akaike-information-criterion/)):~:text=It%20penalizes%20models%20which%20use
    [Accessed 31 Jan. 2023].'
  prefs: []
  type: TYPE_NORMAL
- en: '[15] Zach (2021b). How to Interpret Negative AIC Values. [online] Statology.
    Available at: [https://www.statology.org/negative-aic](https://www.statology.org/negative-aic/](https://www.statology.org/negative-aic/))'
  prefs: []
  type: TYPE_NORMAL
- en: '[16] Datacadamia (2014). Statistics — Bayesian Information Criterion (BIC).
    [online] Datacadamia — Data and Co. Available at: [https://datacadamia.com/data_mining/bic](https://datacadamia.com/data_mining/bic](https://datacadamia.com/data_mining/bic))
    [Accessed 31 Jan. 2023].'
  prefs: []
  type: TYPE_NORMAL
- en: '[17] Clay Ford (2015). Understanding Q-Q Plots | University of Virginia Library
    Research Data Services + Sciences. [online] Virginia.edu. Available at: [https://data.library.virginia.edu/understanding-q-q-plots/](https://data.library.virginia.edu/understanding-q-q-plots/](https://data.library.virginia.edu/understanding-q-q-plots/))'
  prefs: []
  type: TYPE_NORMAL
- en: '[18] De’ath, G. and Fabricius, K.E. (2000). Classification And Regression Trees:
    A Powerful Yet Simple Technique For Ecological Data Analysis. Ecology, 81(11),
    pp.3178–3192\. doi:10.1890/0012–9658(2000)081%5B3178:cartap%5D2.0.co;2'
  prefs: []
  type: TYPE_NORMAL
