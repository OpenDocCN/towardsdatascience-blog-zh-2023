["```py\n# Create and activate a conda environment\nconda create -n openai_chatbot python=3.10\nconda activate openai_chatbot\n\n# Install the necessary libraries\npip install ipykernel streamlit \"fastapi[all]\" openai\n```", "```py\n# %%writefile backend.py\nimport os\nfrom typing import Literal\n\nimport openai\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel, Field\n\napp = FastAPI()\n\n# Load your API key from an environment variable or secret management service\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\nsystem_prompt = \"You are a comic book assistant. You reply to the user's question strictly from the perspective of a comic book assistant. If the question is not related to comic books, you politely decline to answer.\"\n\nclass Conversation(BaseModel):\n    role: Literal[\"assistant\", \"user\"]\n    content: str\n\nclass ConversationHistory(BaseModel):\n    history: list[Conversation] = Field(\n        example=[\n            {\"role\": \"user\", \"content\": \"tell me a quote from DC comics about life\"},\n        ]\n    )\n\n@app.get(\"/\")\nasync def health_check():\n    return {\"status\": \"OK!\"}\n\n@app.post(\"/chat\")\nasync def llm_response(history: ConversationHistory) -> dict:\n    # Step 0: Receive the API payload as a dictionary\n    history = history.dict()\n\n    # Step 1: Initialize messages with a system prompt and conversation history\n    messages = [{\"role\": \"system\", \"content\": system_prompt}, *history[\"history\"]]\n\n    # Step 2: Generate a response\n    llm_response = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\", messages=messages\n    )\n\n    # Step 3: Return the generated response and the token usage\n    return {\n        \"message\": llm_response.choices[0][\"message\"],\n        \"token_usage\": llm_response[\"usage\"],\n    }\n```", "```py\n# %%writefile utils.py\nfrom datetime import datetime\n\nimport pandas as pd\nimport streamlit as st\n\nuser_avatar = \"ðŸ˜ƒ\"\nassistant_avatar = \"ðŸ¦¸\"\n\ndef display_conversation(conversation_history):\n    \"\"\"Display the conversation history\"\"\"\n\n    # Loop over all messages in the conversation\n    for message in conversation_history:\n        # Change avatar based on the role\n        avatar = user_avatar if message[\"role\"] == \"user\" else assistant_avatar\n\n        # Display the message content\n        with st.chat_message(message[\"role\"], avatar=avatar):\n            st.markdown(message[\"content\"])\n\n            if \"api_call_cost\" in message:\n                st.caption(f\"Cost: US${message['api_call_cost']:.5f}\")\n\ndef clear_conversation():\n    \"\"\"Clear the conversation history.\"\"\"\n    if (\n        st.button(\"ðŸ§¹ Clear conversation\", use_container_width=True)\n        or \"conversation_history\" not in st.session_state\n    ):\n        st.session_state.conversation_history = []\n        st.session_state.total_cost = 0\n\ndef download_conversation():\n    \"\"\"Download the conversation history as a CSV file.\"\"\"\n    conversation_df = pd.DataFrame(\n        st.session_state.conversation_history, columns=[\"role\", \"content\"]\n    )\n    csv = conversation_df.to_csv(index=False)\n\n    st.download_button(\n        label=\"ðŸ’¾ Download conversation\",\n        data=csv,\n        file_name=f\"conversation_{datetime.now().strftime('%Y%m%d%H%M%S')}.csv\",\n        mime=\"text/csv\",\n        use_container_width=True,\n    )\n\ndef calc_cost(token_usage):\n    # https://openai.com/pricing\n\n    return (token_usage[\"prompt_tokens\"] * 0.0015 / 1000) + (\n        token_usage[\"completion_tokens\"] * 0.002 / 1000\n    )\n```", "```py\n# %%writefile frontend.py\nimport requests\nimport streamlit as st\nimport utils\n\n# Replace with the URL of your backend\napp_url = \"http://127.0.0.1:8000/chat\"\n\n@st.cache_data(show_spinner=\"ðŸ¤” Thinking...\")\ndef openai_llm_response(user_input):\n    \"\"\"Send the user input to the LLM API and return the response.\"\"\"\n\n    # Append user question to the conversation history\n    st.session_state.conversation_history.append(\n        {\"role\": \"user\", \"content\": user_input}\n    )\n\n    # Send the entire conversation history to the backend\n    payload = {\"history\": st.session_state.conversation_history}\n    response = requests.post(app_url, json=payload).json()\n\n    # Generate the unit api call cost and add it to the response\n    api_call_cost = utils.calc_cost(response[\"token_usage\"])\n    api_call_response = response[\"message\"]\n    api_call_response[\"api_call_cost\"] = api_call_cost\n\n    # Add everything to the session state\n    st.session_state.conversation_history.append(api_call_response)\n    st.session_state.total_cost += api_call_cost\n\ndef main():\n    st.title(\"ðŸ¦¸ ChatGPT Comic Book Assistant\")\n\n    col1, col2 = st.columns(2)\n    with col1:\n        utils.clear_conversation()\n\n    # Get user input\n    if user_input := st.chat_input(\"Ask me any comic book question!\", max_chars=50):\n        openai_llm_response(user_input)\n\n    # Display the total cost\n    st.caption(f\"Total cost of this session: US${st.session_state.total_cost:.5f}\")\n\n    # Display the entire conversation on the frontend\n    utils.display_conversation(st.session_state.conversation_history)\n\n    # Download conversation code runs last to ensure the latest messages are captured\n    with col2:\n        utils.download_conversation()\n\nif __name__ == \"__main__\":\n    main()\n```"]