- en: AI’s Sentence Embeddings, Demystified
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/ais-sentence-embeddings-demystified-7c9cce145dd2](https://towardsdatascience.com/ais-sentence-embeddings-demystified-7c9cce145dd2)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Bridging the gap between computers and language: How AI Sentence Embeddings
    Revolutionize NLP'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@dataemporium?source=post_page-----7c9cce145dd2--------------------------------)[![Ajay
    Halthor](../Images/1be821c8d8ed336b9ecedcf94f960ede.png)](https://medium.com/@dataemporium?source=post_page-----7c9cce145dd2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7c9cce145dd2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7c9cce145dd2--------------------------------)
    [Ajay Halthor](https://medium.com/@dataemporium?source=post_page-----7c9cce145dd2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7c9cce145dd2--------------------------------)
    ·10 min read·Aug 1, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c42af0ad915c5feb7d8468cc3195ef2d.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Steve Johnson](https://unsplash.com/@steve_j?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: In this blog post, let’s demystify how computers understand sentences and documents.
    To kick this discussion off, we will rewind time beginning with the earliest methods
    of representing sentences using n-gram vectors and TF-IDF vectors. Later sections
    will discuss methods that aggregate word vectors from neural bag of words to the
    sentence transformers and language models we see today. There is a lot of fun
    technology to cover. Let’s begin our journey with the simple, elegant n-grams.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. N-gram vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Computers don’t understand words, but they do understand numbers. As such, we
    need to convert words and sentences into vectors when processing by a computer.
    One of the earliest representations of sentences as a vector can be traced back
    to a [1948 paper by Claude Shanon](https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf),
    father of information theory. In this seminal work, sentences were represented
    as an n-gram vector of words. *What does this mean?*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/adde47738399c0b060b5a05fc6910ffe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Generating n-gram vector from a sentence. (image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the sentence “This is a good day”. We can break this sentence down
    into the following n-grams:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unigrams**: This, is, a, good, day'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bigrams**: This is, is a, a good, good day'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trigrams**: this is a, is a good, a good day'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and much more …
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general, a sentence can be broken down into its constituent n-grams, iterating
    from 1 to n. When constructing the vector, each number in this vector represents
    whether the n-gram was present in the sentence or not. Some methods instead could
    us the count of the n-gram present in the sentence. A sample vector representation
    of a sentence is shown above in *Figure 1*.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. TF-IDF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another early, yet popular method of representing sentences and documents involved
    determining TF-IDF vector of a sentence or the “Term Frequency — Inverse Document
    Frequency” vector. In this case, we would count the number of times a word appears
    in the sentence to determine the text frequency (TF). Then we would calculate
    the number of sentences containing this word to determine the inverse Document
    Frequency (IDF). Multiply these values gives the TF-IDF product for every word.
    Repeating this computation for every single word in the sentence allows us to
    generate the TF-IDF vector for that sentence. A sample TF-IDF representation of
    a sentence is shown in *Figure 2*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a4a440ddd75570d0adc296f3c8b05636.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Generating TF-IDF vector from a sentence. (image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: Hence, each sentence (document) is represented by a vector equal to the vocabulary
    of the language. And each position wold be the TF-IDF score for every word in
    that sentence. In case you are interested, TF-IDF was introduced in this [1972
    dissertation](https://www.emerald.com/insight/content/doi/10.1108/eb026526/full/html).
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Assessing early sentence vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pros
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Simple**: Count based representations are very easy to understand and compute
    manually.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fixed length vectors**: Sentences with different number of words can be represented
    by vectors of the same length. This uniformity makes processing easier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cons
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Sparsity**: The sentence vectors generated are large, of the order of the
    vocabulary size. For the n-grams case, the vector size is the number of possible
    n-grams for growing values of *n*. This value can get exceptionally large even
    when *n* is a modest size. Sentence similarities are typically computed by taking
    the distance between their vectors. With such large and sparse vectors, the distance
    between sentences A and B can become indistinguishable from another pair of sentences
    A and C. This means we cannot determine if sentence A is more similar to B or
    C as the number of dimensions becomes too large. This is issue is popularly known
    as the **Curse of Dimensionality** and plagues many fields of statistics and AI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clearly, these sentence representations were getting out of hand. To handle
    this, we take a step back. Instead of representing sentences as vectors, *what
    happens when words themselves are represented as vectors?*
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Dense Word Representations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A [2001 paper on neural probabilistic language models](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)
    introduced the idea of representing words with dense & continuous vectors that
    are fixed length. This combated the curse of dimensionality by ensuring every
    word can be represented as a vector of tractable size (~64, 128 or 256), instead
    of being proportional to the vocabulary size (~100,000)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9db69c57c74cbf82d32ee605199e8eb9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: In 2001, this language model learned dense word representations during
    training ([image source](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 3*, *C(.)* are vectors that each represent a single word. Ideally,
    we want the computer to have some semantic understanding of these words. One way
    to measure this is to ensure the vectors learned that correspond to similar words
    should be physically closer to each other in space as shown in *Figure 4*. The
    NLP space was geared towards improving these word embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/68e681ab7694b422101565d5590bf4c2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: This is an embedding space where each dot represents the vector representation
    of a word. The vector representations are good if similar words are closer to
    each other as shown here (image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Word2Vec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once such advancement to further achieve the goal of learning word embeddings
    came with the [Word2Vec framework in 2013](https://arxiv.org/abs/1301.3781). Word
    vectors of higher quality than ever before were learned with very simple feed
    forward neural network architectures show in *Figure 5*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/31749ebf457fa5e7e39836c58c8e96b8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The 2 Word2Vec architectures that learn effective continuous dense
    representations for every word ([image source](https://arxiv.org/abs/1301.3781))'
  prefs: []
  type: TYPE_NORMAL
- en: For more information on Word2Vec and other similar architectures, [check out
    my blog on this topic where we explain the architectures](https://medium.com/towards-data-science/word2vec-glove-and-fasttext-explained-215a5cd4c06f).
    Essentially, by this point, words could efficiently be represented by fixed length
    vectors that could be processed by computers. *But what about sentences*, which
    is the topic of interest for this blog?
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Neural Bag of Words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One simple approach is to just take the average of word vectors in a sentence.
    A simple approach that is shown in *Figure 6*. A sentence is essentially treated
    like a *bag of words* with no ordering or context information considered. From
    this fixed size vector, we can add feed forward layers to perform machine learning
    tasks such as sentiment analysis or topic classification among others. This has
    the pro of ensuring the resulting vector is smaller in size, yet preserves some
    meaning of its constituent words. However, its representation lacks understanding
    of context and grammar.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6dd0ca9b6881d598c73da6a4862eb388.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The neural bag of words archtiecture simply averages word vectors
    to get the corresponding sentence vector (image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: There have been strides over the years to better preserve context and grammar
    in a sentence vector.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Convolution Neural Networks in NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Convolution neural networks (CNNs) allows sequences of different lengths to
    be represented with fixed length vectors using a combination of convolution and
    pooling operation. The convolution operation allows words in the sentence to gain
    context from surrounding words and hence encode meaning in the final vector. This
    was applied as early as [1989 in Time Delay Neural Networks for phoneme detection](https://www.cs.toronto.edu/~fritz/absps/waibelTDNN.pdf).
    The architecture of this network is shown in *Figure 7* where given an audio wave,
    we classify the phonetic sound spoken as “b”, “d” or “g”.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8332afc1d25166a60aaad89f961750c6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Initial Time Delay Neural Network architecture that used convolution
    to process sequence information ([image source](https://www.cs.toronto.edu/~fritz/absps/waibelTDNN.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Advancements of convolution in NLP were minimal after this seminal paper. However
    they made a resurgence in 2010s with improved performance and follow up research
    in the form of [Dynamic Convolution Neural Networks (DCNNs)](https://arxiv.org/pdf/1404.2188.pdf)
    for following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**More data available**: More data is needed for training larger models wit
    lots of parameters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Software advancements**: These are the advancements in learning dense word
    representations discussed in previous paragraphs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hardware advancements**: The introduction of GPUs and cUDA to speed up the
    training of models and exploit parallelization .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamic Convolution Neural Network have a dynamic architecture depending on
    the sequence length. So even long sentences and documents can be processed appropriately
    without changing the configurations of the network.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/31ca0e4b58e95e2c79454d098f517de8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: DCNN architecture that has a dynamic max-k pooling depending on the
    sentence input length ([image source](https://arxiv.org/pdf/1404.2188.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: For more information on these convolution architectures and how they structured
    the landscape of NLP, [check out this other medium blog on this topic](https://medium.com/@dataemporium/convolution-in-nlp-573d0329cc37).
    Like convolution, there have been architectures introduced that have seen more
    of the spot light in recent years. *What are they?*
  prefs: []
  type: TYPE_NORMAL
- en: 8\. Recurrent Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recurrent neural networks (RNNs) are feed forward networks rolled out over time.
    As such, they can process sequence data. While RNNs have been talked about since
    the 1980s, [this dissertation by Rumelhart being one of the first indirect mentions](https://apps.dtic.mil/dtic/tr/fulltext/u2/a164453.pdf),
    they hadn’t been used in the early years because they were difficult to train.
    This changed with [Ilya Sutskever’s 2013 PhD thesis](https://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf)
    that showed how RNNs can be trained stably. Since this time, we have seen RNNs
    burst into the scene to process sequence data.
  prefs: []
  type: TYPE_NORMAL
- en: They processed input words sequentially so every word would have context of
    the words that came before it. With the introduction of bidirectional RNNs, the
    context of the words could be understood based on the words that came bother before
    and after it’s position in the sentence.
  prefs: []
  type: TYPE_NORMAL
- en: With RNNs however, long sentence and document inputs may lead to vanishing and
    exploding gradients during the training of the network. This is where LSTM (Long
    Short Term Memory) networks shine with their ability to process long sequences.
    For more information on this topic, [check out my accompanying YouTube video](https://youtu.be/QciIcRxJvsM).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/77baf6de9426431566f7cee5eb9c18da.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: LSTM Cells for better understanding long sentences ([image source](https://colah.github.io/posts/2015-08-Understanding-LSTMs/))'
  prefs: []
  type: TYPE_NORMAL
- en: However, an issue with these recurrent neural networks is they are so slow to
    train that they use a truncated version of backpropogation to learn parameters.
    This slowness is attributed to the fact that RNNs process data sequentially, one
    word at a time. And so they don’t make use of modern GPUs very well.
  prefs: []
  type: TYPE_NORMAL
- en: 9\. Transformer Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The issue of slowness seen in RNNs was addressed with Transformer neural networks.
    In the [2017 paper “Attention is all you need”](https://arxiv.org/abs/1706.03762),
    the transformer architecture was used to solve sequence to sequence problems like
    Machine Translation, Question Answering, and more. The network consists of an
    encoder and decoder that comprises of attention units as shown in *Figure 10*.
    These attention units would allow the network to learn high quality vector representations
    of a word while accounting for long term dependencies. In addition to these benefits,
    it has an edge over RNNs as input words could now be processed faster in parallel.
    This leads to faster training than RNNs. For more information on transformers
    and how to build them yourself, [check out this playlist of videos](https://www.youtube.com/playlist?list=PLTl9hO2Oobd97qfWC40gOSU8C0iu0m2l4).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1d808e6bbe3bacb4794106514f4684e9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: The architecture of a transformer neural network ([image source](https://arxiv.org/abs/1706.03762))'
  prefs: []
  type: TYPE_NORMAL
- en: Despite these wonderful features, this transformer network has it’s limitations.
    We need to train the model from scratch every time a new problem needs to be learned.
    This requires a ton of tailored data for each task. For example, if I want to
    build a translator from English to Kannada, I might need tens of thousands of
    examples. This data can be difficult to come by for many tasks. *How do we address
    this issue?*
  prefs: []
  type: TYPE_NORMAL
- en: 10\. BERT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'BERT stands for Bidirectional Encoder Representation from Transformers. As
    such, it is a stack of transformer encoders that is designed to handle language
    tasks. Many NLP tasks have the common element of “language understanding”. So
    if we have an AI that has some fundamental understanding of language, it shouldn’t
    require too much data to fine tune this model to understand language translation,
    or question answering or any other language task. Based on this philosophy, [BERT
    was introduced in 2019](https://arxiv.org/pdf/1810.04805.pdf) with the training
    phase is split into 2 parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**pretraining**: The model learns from scratch the understanding of language'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**fine tuning**: The model learns to solve a specific NLP task with less data
    requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/4a0082a8259bf5ef50d09197a56b390c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Pretraining and fine tuning of BERT ([image source](https://arxiv.org/pdf/1810.04805.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, you can download a pretrained BERT model and fine tune it with
    the data you have, even if you don’t have much of it. Hence BERT leverages transfer
    learning to quickly train models to solve language problems. For more information
    on BERT and its training and inference, [check out this YouTube video](https://youtu.be/xI0HHN5XKDo).
  prefs: []
  type: TYPE_NORMAL
- en: However, BERT is a stack of transformer encoders. And transformer encoders internally
    learn high quality word embeddings. So coming back to the question again, *how
    do we derive sentence embeddings?*
  prefs: []
  type: TYPE_NORMAL
- en: 11\. Sentence Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The easiest way is to pass the sentence through BERT to generate high quality
    word embeddings. Then take the average of these to get a sentence. This is the
    most basic sentence transformer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2d20b1f22bbc2fc26444cb48fd4021b7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: A basic sentence transformer that takes in a sentence and generates
    a vector ([image source](https://arxiv.org/pdf/1908.10084.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sentence transformers have a straightforward flow of passing in a sentence
    through BERT and a pooling layer to get a vector representation of that sentence.
    [But this leads to bad embeddings that even words than GloVe embeddings](https://arxiv.org/pdf/1908.10084.pdf).
    Instead, we fine tune this BERT architecture on one, or both, of the following
    tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Natural Language Inference** [(NLI)](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fwww.sbert.net%2Fdocs%2Fpretrained-models%2Fnli-models.html&link_redirector=1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentence Test Similarity** [(STS)](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fwww.sbert.net%2Fdocs%2Fpretrained-models%2Fsts-models.html&link_redirector=1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Figure 13* below shows the fine tuning process for each.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/008214a995416a71ba5f85a0cb30a474.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Siamese network trained on NLI on the left and STS on the right
    ([image source](https://arxiv.org/pdf/1908.10084.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: Once one form of this architecture is trained, we can then use one of the BERT
    and pooling units for inference to convert a sentence into a vector representation
    that is of higher quality.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article we have seen the advancements in representing sentences as vectors.
    In the early days of NLP, we started with count based methods such as n-gram vectors
    and TF-IDF vectors. These methods however created sparse vectors that are difficult
    to process by computers. But with the introduction of dense vector representations
    in the early 2000s, increased data, hardware advancements, we have been able to
    train models that have a high quality understanding of words represented in vectors.
    This could also be used to create sentence vectors effectively as seen in the
    previous section on sentence transformers.
  prefs: []
  type: TYPE_NORMAL
- en: Current language models are increasingly having a better understanding of words,
    sentences and documents. And I hope you are as excited as I am to learn more about
    the AI future.
  prefs: []
  type: TYPE_NORMAL
- en: '*Happy Learning!*'
  prefs: []
  type: TYPE_NORMAL
