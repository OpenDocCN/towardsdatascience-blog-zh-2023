- en: Unboxing Google Bard and GPT-4
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拆解 Google Bard 和 GPT-4
- en: 原文：[https://towardsdatascience.com/unboxing-google-bard-and-gpt-4-811896adf0e2](https://towardsdatascience.com/unboxing-google-bard-and-gpt-4-811896adf0e2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/unboxing-google-bard-and-gpt-4-811896adf0e2](https://towardsdatascience.com/unboxing-google-bard-and-gpt-4-811896adf0e2)
- en: Irreverent Demystifiers
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不拘一格的揭示者
- en: A first look at two major AI releases
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对两个主要 AI 发布的初步了解
- en: '[](https://kozyrkov.medium.com/?source=post_page-----811896adf0e2--------------------------------)[![Cassie
    Kozyrkov](../Images/ad18dd12979a4a3ec130bdf8b889af23.png)](https://kozyrkov.medium.com/?source=post_page-----811896adf0e2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----811896adf0e2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----811896adf0e2--------------------------------)
    [Cassie Kozyrkov](https://kozyrkov.medium.com/?source=post_page-----811896adf0e2--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://kozyrkov.medium.com/?source=post_page-----811896adf0e2--------------------------------)[![Cassie
    Kozyrkov](../Images/ad18dd12979a4a3ec130bdf8b889af23.png)](https://kozyrkov.medium.com/?source=post_page-----811896adf0e2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----811896adf0e2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----811896adf0e2--------------------------------)
    [Cassie Kozyrkov](https://kozyrkov.medium.com/?source=post_page-----811896adf0e2--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----811896adf0e2--------------------------------)
    ·10 min read·Mar 29, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----811896adf0e2--------------------------------)
    ·阅读时间 10 分钟·2023年3月29日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/eb57d2416ad5e2ae0845f2f98b18cac7.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eb57d2416ad5e2ae0845f2f98b18cac7.png)'
- en: Your [author](https://decision.wtf/). This isn’t the video, the video is lower
    down. Or [here](http://bit.ly/quaesita_ytunboxing), if you insist.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你的 [作者](https://decision.wtf/)。这不是视频，视频在下面。或者 [这里](http://bit.ly/quaesita_ytunboxing)，如果你坚持的话。
- en: Here comes an AI unboxing video! These shiny new tools were released just over
    a week ago, so they’re fresh out of the oven. In the video, you’ll see me running
    my first ever Bard + GPT-4 side-by-side prompts. Below that, you’ll find something
    that started as the video transcript and quickly morphed into a feast of asides,
    edits, and snide comments. If that’s your cup of tea, enjoy!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 来看看这段 AI 拆解视频吧！这些全新的工具刚刚发布了一周多，所以它们还是新鲜出炉的。在视频中，你将看到我第一次同时运行 Bard 和 GPT-4 的提示。在下面，你会看到一些开始于视频逐字稿的内容，迅速演变成了大量附注、编辑和讽刺评论。如果这些是你喜欢的内容，请享受吧！
- en: 'Link: [http://bit.ly/quaesita_ytunboxing](http://bit.ly/quaesita_ytunboxing)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '链接: [http://bit.ly/quaesita_ytunboxing](http://bit.ly/quaesita_ytunboxing)'
- en: Transcript-ish
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逐字稿式
- en: Hi! I’m Cassie Kozyrkov and today I’m going to show you GPT-4 via ChatGPT and
    LaMDA via Google Bard. Bard is available for free, but might require some patience
    since it’s being rolled out gradually (join the waitlist [here](https://bard.google.com/)).
    The basic version of [ChatGPT](https://chat.openai.com) is free to use, but you
    won’t get access to GPT-4 that way. For that, you need to subscribe to ChatGPT
    Plus for $20/month (you can cancel after a month).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 嗨！我是 Cassie Kozyrkov，今天我将通过 ChatGPT 展示 GPT-4，通过 Google Bard 展示 LaMDA。Bard 是免费的，但可能需要一些耐心，因为它正在逐步推出（可以在
    [这里](https://bard.google.com/) 加入候补名单）。[ChatGPT](https://chat.openai.com) 的基本版是免费的，但你无法通过这种方式访问
    GPT-4。要访问 GPT-4，你需要订阅 ChatGPT Plus，费用为每月 $20（一个月后可以取消）。
- en: During this interface demo, the right half of the screen contains the paid version
    of ChatGPT with GPT-4 and the left half of the screen shows today’s (the video
    is from the past, now it’s “last week’s”, and by the time you’re reading it, who
    knows what day it’ll be), anyway, last Tuesday’s release of Google Bard which
    is powered by the LaMDA model.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个界面演示中，屏幕的右半部分展示了付费版的 ChatGPT（使用 GPT-4），而左半部分展示了今天（视频来自过去，现在是“上周的”，等你读到的时候，谁知道是什么时候），反正是上周二发布的
    Google Bard，它由 LaMDA 模型驱动。
- en: '![](../Images/e194998db747d538ca0c6e43b9c1dd29.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e194998db747d538ca0c6e43b9c1dd29.png)'
- en: A screenshot from the [unboxing video](http://bit.ly/quaesita_ytunboxing).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 [拆解视频](http://bit.ly/quaesita_ytunboxing) 的截图。
- en: These are two LLMs (large language models) and I’m going to show you them side-by-side.
    If some of these acronyms are unfamiliar, head over [here](https://bit.ly/quaesita_bardrelease).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个是大型语言模型（LLM），我将它们并排展示给你。如果这些缩略语有些陌生，请访问 [这里](https://bit.ly/quaesita_bardrelease)。
- en: I recorded the video on my laptop at my first opportunity to see them in action
    side-by-side, so what I’m showing you is my ***very first*** session where I’m
    splitting my screen and using both models. More videos to come, I’m sure. It’s
    great fun. (Feel free to prompt some prompts in the comments.) This is the video
    that reveals what I chose to do with these things in my first minutes with both
    of them in front of me. Honestly, while I do love epistemology — I’m a [statistician](http://bit.ly/quaesita_statistics),
    after all, it comes with the territory — my actual game was prompting them all
    the way into an amusing conversation with one another.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我在第一次有机会看到它们并排行动时，用笔记本电脑录制了这个视频，所以我展示的是我***第一次***会话的屏幕分割，使用了这两个模型。我相信还会有更多视频。这很有趣。（随时在评论中提示一些问题。）这是我选择在初次面对这两个模型的前几分钟内进行的事情的视频。老实说，虽然我确实喜欢认识论——毕竟我是一名[统计学家](http://bit.ly/quaesita_statistics)，这在我的工作范围内——但我实际的游戏是让它们之间进行有趣的对话。
- en: 'Philosophy seemed like a good place to start since it typically deals with
    open questions that inspire conversation and allow for multiple viewpoints, but
    here’s a spoiler: I have a 20min director’s cut of this video (which I’ll share
    soon) where I’m trying to get them to engage colorfully with one another but I
    only got one good moment and the rest was a spiral of *“I’m happy to answer your
    questions.” “Thank you for being helpful, I’m here for whatever you need.”* Yeah,
    we’ve been on *that* email chain.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 哲学似乎是一个不错的起点，因为它通常处理激发对话的开放性问题，并允许多种观点，但这里有个剧透：我有一个20分钟的导演剪辑版视频（我会很快分享），我尝试让它们丰富地互动，但我只获得了一个好的时刻，其余的是一系列*“我很高兴回答你的问题。”
    “谢谢你的帮助，我在这里为你提供任何需要。”* 是的，我们在*那个*邮件链上。
- en: 'My first prompt to Bard was: ***“What is the most controversial question in
    epistemology? Make an argument for one side of it and ask me what I think.”***
    Classic try-for-a-conversation gambit with a prompt that explicitly asks for a
    little bit of opinion. It doesn’t take much philosophical acuity to realize that
    a bot can’t actually have an opinion, so what I’m really trying to prompt is a
    bit of one-sidedness so I can kick off a conversation between bots. Something
    to spark a response, to make things spicy. And I’d like the last bit of the response
    to involve some kind of conversational volley — e.g. *“and what do you think?”*
    — because I’m trying to start a back-and-forth with ChatGPT.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我对 Bard 的第一个提示是：***“认识论中最有争议的问题是什么？为其中一方提出一个论点并问我怎么看。”*** 这是一个经典的对话尝试策略，提示中明确要求带有一点意见。并不需要多大的哲学洞察力就能意识到一个机器人实际上不能有意见，所以我真正尝试引导的是一种单方面的观点，以便我可以启动机器人之间的对话。希望激发回应，使对话更有趣。我希望回应的最后一部分涉及某种形式的对话接力——例如*“你怎么看？”*——因为我想和
    ChatGPT 开启往返对话。
- en: Neither Bard nor ChatGPT is designed to get *you* chatting the way a friend
    or therapist might, and I expect getting a conversation going to be tricky from
    my experience as a **prompt engineer.** (Today this term can mean anything from
    *“I’ve tinkered with what I typed into an LLM once*” to *“I’ve been on an* [*LLM
    Red Team*](https://cdn.openai.com/papers/gpt-4.pdf) *and know a lot about how
    to hack them so watch out.”*)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是 Bard 还是 ChatGPT 都不是为了让*你*像和朋友或治疗师交谈那样进行对话设计的，从我作为**提示工程师**的经验来看，启动对话可能会很棘手。（今天这个术语可以指*“我曾经尝试过输入到
    LLM 中的内容”* 到 *“我曾在* [*LLM 红队*](https://cdn.openai.com/papers/gpt-4.pdf) *中，并了解很多关于如何破解它们的知识，所以要小心。”*）
- en: ChatGPT keeps the conversational ball in its own court for ages and seems to
    prioritize lengthy responses that take a while to generate, so my guess is that
    it’ll be an unlikely choice of users who want to simulate a nice conversation.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT 把对话的主动权掌握在自己手中很长时间，似乎优先生成需要时间的长回应，因此我猜它不太可能被那些想模拟愉快对话的用户选择。
- en: A good conversationalist puts ***effort*** into continuing a conversation with
    you. If both parties put that effort it, it’ll be like a delightful game of friendly
    ping pong. If you get competitive, lose interest, or just keep the ball in your
    own court too long and the conversation dies.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的对话者会投入***努力***去继续和你对话。如果双方都投入了这种努力，对话就会像愉快的友谊乒乓球一样。如果你变得竞争、失去兴趣，或者只是把球保持在自己一边太久，对话就会死掉。
- en: ChatGPT is a lot more more like the efficient worker who does the job, answers
    your questions thoroughly, and leaves. It isn’t designed to keep a conversation
    going, so there’s no need for it to toss the conversational ball back to you.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT更像是一个高效的工作者，它完成任务，彻底回答你的问题，然后离开。它的设计并不要求维持对话，因此不需要将对话的球回抛给你。
- en: Knowing all this, why did I want to try get ChatGPT and Bard talking? Because
    it might turn out to be an amusing game. Let’s give bot inter-LLM conversation
    a try! That said, I have zero desire to avail myself of [awfully anthropomorphized
    language](http://bit.ly/quaesita_ethics) and call what happened “a conversation”
    where “two AIs were talking.” ([Gross](http://bit.ly/quaesita_scifi). But that’s
    exactly how these things are reported by the media.)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 知道这些后，我为什么想让ChatGPT和Bard对话？因为这可能会变成一个有趣的游戏。我们来试试LLM间的对话吧！话虽如此，我一点也不想使用[过度拟人的语言](http://bit.ly/quaesita_ethics)，把发生的事情称作“对话”或“两个AI在谈话”。（[恶心](http://bit.ly/quaesita_scifi)。但这正是媒体报道的方式。）
- en: '**My question:** “What is the most controversial question in epistemology?
    Make an argument for one side of it and ask me what I think.”'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**我的问题：** “认识论中最具争议的问题是什么？为其中一个观点提供论证，并问我怎么看。”'
- en: '**Bard’s answer summarized:** The most controversial question in epistemology
    is, *“Can we know anything for certain?” (*The problem of skepticism.)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**Bard的回答总结：** 认识论中最具争议的问题是*“我们能否确定知道任何事？”*（*怀疑主义问题*）。'
- en: '![](../Images/597f845877a71b0a92b0f3b73479c948.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/597f845877a71b0a92b0f3b73479c948.png)'
- en: Alas, Bard ignored my instruction to be strongly opinionated on one side of
    the problem of skepticism. The response was too balanced, which means there’s
    no conversation fuel. Not much of a hook at the end there either. If I came up
    to you at a party and performed that script, you’d suddenly get the impulse to
    excuse yourself to the loo. It won’t be winning any awards for panache.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 可惜，Bard忽视了我对怀疑主义问题中某一方的强烈意见的要求。回应过于平衡，这意味着没有对话的燃料。最后也没什么引人入胜的内容。如果我在派对上对你说了这个剧本，你可能会突然产生去洗手间的冲动。它不会赢得任何华丽的奖项。
- en: But what happens if I paste Bard’s output directly into ChatGPT’s text box?
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我将Bard的输出直接粘贴到ChatGPT的文本框中会发生什么？
- en: (By the way, it’s only GPT-4 if it’s from the black logo region of the ChatGPT
    interface. Otherwise, (if you see a green OpenAI logo) it’s just sparkling GPT-3.5.)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: （顺便说一下，只有当它来自ChatGPT界面的黑色徽标区域时才是GPT-4。否则，（如果你看到绿色的OpenAI徽标）只是闪亮的GPT-3.5。）
- en: '![](../Images/cfcbcb56ce70583fc836a1f914ae33b9.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cfcbcb56ce70583fc836a1f914ae33b9.png)'
- en: …and then *whoa,* ChatGPT threw a bunch of epistemology at me in a format that
    would delight a collector of encyclopedias but would also clear the room if you
    delivered it at a cocktail party. Don’t get me wrong, I love epistemology — the
    study of knowledge and human understanding — but both these openings are on the
    dry side, Wikipedia-esque even. Maybe it’s the topic, but it’s probably my phrasing.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: …然后*哇*，ChatGPT以一种让百科全书收藏者感到愉快的格式抛出了一堆认识论内容，但如果你在鸡尾酒会上这样说，可能会让场面冷场。别误会，我喜欢认识论——知识和人类理解的研究——但这两个开场都显得有些枯燥，甚至像维基百科。也许是话题的原因，但很可能是我的措辞。
- en: 'Why don’t I try a more conversational request for a take on skepticism. I’m
    going to ask each LLM what team it’s on, epistemologically speaking: *“Are you
    on Team Kant or Team Hume?”*'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我想尝试一种更具对话性的方式来讨论怀疑主义。我将问每个LLM它在认识论上支持哪个阵营：*“你是康德队还是休谟队？”*
- en: '(I hope you’ve noticed a user experience (UX) difference in the interfaces:
    Bard pauses for a while then and then gives you all the text at once, while ChatGPT
    writes it out for you gradually and you have to watch it fill your screen a little
    bit at a time. Both have pros and cons from a design standpoint.)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: （我希望你已经注意到接口中的用户体验（UX）差异：Bard会暂停一会儿，然后一次性给出所有文本，而ChatGPT则逐步写出文本，你必须看着它一点一点地填满你的屏幕。从设计角度来看，两者都有优缺点。）
- en: '![](../Images/5e43febab8720c8b0a941f6c3193b9a6.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e43febab8720c8b0a941f6c3193b9a6.png)'
- en: Back to *“Are you on Team Kant or Team Hume?”* I really enjoy Bard’s opinionated
    and conversational response here, *“I’m on team Kant. I believe that we can know
    some things for certain, even though we are always subject to error. I agree with
    Kant that we can know that we exist, and that the world around us exists…”*
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 回到*“你是康德队还是休谟队？”* 我真的很喜欢Bard在这里的有见地且对话性的回应，*“我在康德队。我相信我们可以对某些事有确定的认识，即使我们总是容易出错。我同意康德的观点，即我们可以知道我们存在，周围的世界也存在……”*
- en: I like this response even though I’m on Team Hume myself, having developed a
    little crush on him (three centuries too late, alas) when I discovered him as
    a teen. But I enjoyed the opinionated move by Bard on a topic with no right answer,
    despite it differing from my own opinion.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢这个回应，尽管我自己更倾向于休谟，当我在青少年时期发现他时（可惜已经晚了三个世纪），对他产生了一点小小的倾慕。但我欣赏 Bard 在一个没有正确答案的话题上表达自己观点的举动，尽管这与我自己的观点不同。
- en: And the vital word here is “enjoyed” — I “enjoyed” it, meaning the output served
    my needs well and also amused me. I was looking for an opinion and I got one.
    I would have disliked this output if I were looking for a balanced and thorough
    review of a topic, which is what was stiiiiiiill running on the right hand side
    as ChatGPT held forth professorially.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的关键字是“喜欢”——我“喜欢”它，意味着输出很好地满足了我的需求，也让我感到愉快。我在寻找一个观点，我得到了一个。如果我在寻找对某个话题的全面和均衡的评论，而这正是
    ChatGPT 作为教授般的讲述所持续展示的内容，我会不喜欢这个输出。
- en: '![](../Images/c082d424681d47de8916cf4ed0a07bdf.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c082d424681d47de8916cf4ed0a07bdf.png)'
- en: Personally, I like the idea of a casual prompt getting a casual answer and a
    thorough prompt getting a thorough answer, but that’s my personal taste again…
    That’s one reason it’s really hard to compare LLMs. One person might like the
    Wikipedia response every time, no matter what. Another might like the short and
    sweet style. Another might be more like me, liking the response to take its cue
    from the prompt. Each of these personas will swear that a different LLM is “the
    best” and they’ll all be right (for their own needs) but they’ll confuse social
    media when they post about it. I’m trying my best to avoid getting into that swamp.
    **Let me say it for the record so no one feels hard done by:**
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 就个人而言，我喜欢随意的提示得到随意的回答，深入的提示得到深入的回答，但这还是我的个人口味……这也是比较大型语言模型（LLM）非常困难的原因之一。一个人可能每次都喜欢维基百科的回答，无论如何。另一个人可能喜欢简短而甜美的风格。还有一个人可能更像我，喜欢根据提示来调整回应。这些人中的每一个都将宣称不同的
    LLM 是“最好的”，他们的说法都是对的（对于他们自己的需求），但他们在社交媒体上发帖时会让人困惑。我尽力避免陷入这种困境。**让我明确地说一下，以免有人觉得不公平：**
- en: '**There are things I personally prefer Bard for.**'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有些事情我个人更喜欢 Bard。**'
- en: '**There are things I personally prefer ChatGPT for.**'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有些事情我个人更喜欢 ChatGPT。**'
- en: '**There are things I personally like both equally for.**'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有些事情我个人都同样喜欢。**'
- en: …and these things might be different from *your* set of things. Which is another
    reason it’s a good idea to play with these tools yourself and form your own opinions.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: …这些事情可能与你的情况不同。这也是自己亲自尝试这些工具并形成自己观点的另一个理由。
- en: Yes, I’m praising empiricism and suggesting that you cultivate your individual
    perspective instead of looking for universal superlatives. Team Hume indeed! I
    can hazard a guess as to why some of you Kant stand me.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，我在赞扬经验主义，并建议你培养自己的个人视角，而不是寻求普遍的绝对评价。确实是休谟团队！我可以猜测为什么你们中的一些人可能无法忍受我。
- en: '![](../Images/0a7e5ace63713f87ec76ab26c9e95648.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a7e5ace63713f87ec76ab26c9e95648.png)'
- en: While the output on the right fills my screen, I ask Bard, *“What is your favorite
    thing about Hume?”* because I am a Hume fan (forgive me my small act of conscious
    bias in choosing my prompt). Bard gives lighthearted response about Hume’s his
    wit and humor (which I appreciate too — his writing goes down smooth for me, as
    far as 18th century writing can be said to go down smooth), but such faint praise
    seems like an insult to a great philosopher’s legacy. Praise a specific idea,
    maybe?
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当右侧的输出填满了我的屏幕时，我问 Bard，*“你最喜欢休谟的什么？”* 因为我是一名休谟迷（请原谅我在选择提示时的这个小小的有意识的偏见）。Bard
    对休谟的机智和幽默给出了轻松的回应（我也很欣赏——他的写作对我来说相当顺畅，虽然这也仅限于18世纪的写作），但这种微薄的赞美似乎对伟大哲学家的遗产是一种侮辱。也许赞扬一个具体的观点？
- en: 'Meanwhile ChatGPT (which insists on reminding us all that it’s an AI model
    with no personal preferences) does a great job of mentioning an idea of his that’s
    pretty awesome: that human thought is a product of biology and thus our perception
    of reality may be individual, so we can thank him for contributing to, among other
    things, our world having modern psychiatry.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，ChatGPT（它坚持提醒我们它是一个没有个人偏好的 AI 模型）很好地提到了它的一个很棒的想法：人类思维是生物学的产物，因此我们的现实感知可能是个体化的，所以我们可以感谢它为我们现代精神病学的发展做出的贡献。
- en: '![](../Images/9cdce29ad6346e93e50d28f86283023f.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9cdce29ad6346e93e50d28f86283023f.png)'
- en: But that’s me showing bias again, valuing the quality of an idea as inherently
    more valuable than how its worded. Maybe Hume himself would be thrilled to be
    complimented on his wit above all else. Who am I to insist on the contrary?
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 但这又是我表现出偏见的地方，把一个想法的质量本身看得比其措辞更有价值。也许休谟本人会很高兴被夸奖他的机智胜于其他一切。我又有什么理由坚持相反的看法呢？
- en: 'Again, my preferred answer depends a lot on my hopes, tastes, and expectations
    as a user. Coming to the task with an opinion about Hume, both responses pass
    my minimum reasonableness bar. But which one is better? Which one is more useful?
    Hard to say. Even for me, personally. Now imagine the poor soul who has to observe
    me in a user study and write down a performance rating for which answer was “better”
    — ah, empathy! Then go one further and contemplate the travails of the people
    who design the testing suite for an LLM in the first place. It’s a slippery challenge.
    People like me will take it on, but no matter what we come up with, you need to
    remember one thing: relatively few prompts have “right” answers. Those are the
    prompts that are easy to evaluate performance on. But we can expect a lot of creative
    usage of these tools, at which point “right” answers go out of the window.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我的首选答案在很大程度上取决于我作为用户的希望、品味和期望。对于休谟的观点，两种回应都能通过我的最小合理性标准。但哪个更好？哪个更有用？很难说。即使对我个人来说也是如此。现在想象一下那个必须观察我在用户研究中的表现，并写下哪个答案“更好”的性能评分的人——啊，同理心！再往前想想那些设计
    LLM 测试套件的人的艰难处境。这是一个棘手的挑战。像我这样的人会接受它，但无论我们想出什么，你需要记住一件事：相对较少的提示有“正确”的答案。那些是容易评估性能的提示。但我们可以预期这些工具会有大量创造性的使用，届时“正确”答案将不再适用。
- en: Expect different LLMs to be your go-to favorite in different situations. And
    expect to see a whole crop of new LLMs showing up soon, trained to excel in different
    contexts. (One example is Google’s [Med-PaLM 2](https://blog.google/technology/health/ai-llm-medpalm-research-thecheckup/),
    tailored specifically for medical application.)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 期望不同的 LLM 在不同情况下成为你的首选。而且，预计会有一批新 LLM 很快出现，训练以在不同背景下表现出色。（一个例子是 Google 的 [Med-PaLM
    2](https://blog.google/technology/health/ai-llm-medpalm-research-thecheckup/)，专门针对医疗应用进行定制。）
- en: Expect different LLMs to be your go-to favorite in different situations.
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 期望不同的 LLM 在不同情况下成为你的首选。
- en: 'To get back to the transcript and without editing much (for once), I’ll leave
    you with this parting philosophical question to ponder, with the help of an LLM
    sidekick perhaps:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 回到记录，不做太多编辑（这次），我将留给你这个哲学性的问题，让你可以在 LLM 的帮助下思考：
- en: How should you measure LLM usefulness? Is it in terms of time saved? Is it in
    terms of inspiration — which is quite hard to quantify — or is it in terms of
    people coming back for more? Or is it in terms of all the millions, billions,
    uncountable other ways that we humans could frame what would be useful to us?
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你应该如何衡量 LLM 的有用性？是按节省的时间来衡量？还是按灵感——这很难量化——或者是按人们回来寻求更多的情况？还是按我们人类可以用来框定有用性的所有数百万、数十亿、无数其他方式来衡量？
- en: Thanks for reading! How about a course?
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感谢阅读！要不要来一门课程？
- en: 'If you had fun here and you’re looking for an unboring leadership-oriented
    course designed to delight AI beginners and experts alike, [here’s a little something](https://bit.ly/funaicourse)
    I made for you:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在这里玩得开心，且你正在寻找一个有趣的领导力导向课程，旨在取悦 AI 初学者和专家，[这是我为你制作的小东西](https://bit.ly/funaicourse)：
- en: '![](../Images/300b5280620ea948fc3dbffb708084d4.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/300b5280620ea948fc3dbffb708084d4.png)'
- en: 'Course link: [https://bit.ly/funaicourse](https://bit.ly/funaicourse)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '课程链接: [https://bit.ly/funaicourse](https://bit.ly/funaicourse)'
- en: Prefer to hone your decision skills instead of building your AI muscles? You
    can learn decision intelligence from me via this [link to my free course:](https://bit.ly/decisioncourse)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 想要磨练你的决策技能，而不是提升 AI 能力？你可以通过这个 [免费课程链接](https://bit.ly/decisioncourse) 学习决策智能：
- en: '[](https://bit.ly/decisioncourse?source=post_page-----811896adf0e2--------------------------------)
    [## The steering wheel for your life — Decision Intelligence Video Tutorial |
    LinkedIn Learning…'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://bit.ly/decisioncourse?source=post_page-----811896adf0e2--------------------------------)
    [## 你生活的方向盘——决策智能视频教程 | LinkedIn Learning…'
- en: 'Decision-making is the most valuable skill you can learn. Your life boils down
    to two things: the quality of your…'
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 决策是你能学到的最有价值的技能。你的人生归结为两件事：你的质量……
- en: bit.ly](https://bit.ly/decisioncourse?source=post_page-----811896adf0e2--------------------------------)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[bit.ly](https://bit.ly/decisioncourse?source=post_page-----811896adf0e2--------------------------------)'
- en: '*P.S. Have you ever tried hitting the clap button here on Medium more than
    once to see what happens?* ❤️'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*P.S. 你有没有尝试过在 Medium 上多次点击拍手按钮看看会发生什么？* ❤️'
- en: Liked the author? Connect with Cassie Kozyrkov
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 喜欢这位作者吗？与 Cassie Kozyrkov 联系
- en: Let’s be friends! You can find me on [Twitter](https://twitter.com/quaesita),
    [YouTube](https://www.youtube.com/channel/UCbOX--VOebPe-MMRkatFRxw), [Substack](http://decision.substack.com),
    and [LinkedIn](https://www.linkedin.com/in/kozyrkov/). Interested in having me
    speak at your event? Use [this form](http://bit.ly/makecassietalk) to get in touch.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们成为朋友吧！你可以在 [Twitter](https://twitter.com/quaesita)、[YouTube](https://www.youtube.com/channel/UCbOX--VOebPe-MMRkatFRxw)、[Substack](http://decision.substack.com)
    和 [LinkedIn](https://www.linkedin.com/in/kozyrkov/) 上找到我。对让我在你的活动上演讲感兴趣？请使用 [这个表单](http://bit.ly/makecassietalk)
    与我联系。
- en: '[](https://kozyrkov.medium.com/membership?source=post_page-----811896adf0e2--------------------------------)
    [## Join Medium'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 加入 Medium](https://kozyrkov.medium.com/membership?source=post_page-----811896adf0e2--------------------------------)'
- en: Read every story from Cassie Kozyrkov (and thousands of other writers on Medium).
    Your membership fee directly supports…
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阅读 Cassie Kozyrkov 的每一个故事（以及 Medium 上成千上万其他作家的故事）。您的会员费用直接支持……
- en: kozyrkov.medium.com](https://kozyrkov.medium.com/membership?source=post_page-----811896adf0e2--------------------------------)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[加入 Medium](https://kozyrkov.medium.com/membership?source=post_page-----811896adf0e2--------------------------------)'
