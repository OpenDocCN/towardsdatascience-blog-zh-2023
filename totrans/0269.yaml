- en: 'Diffusion Models: How do They Diffuse?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/ai-diffusion-models-how-do-they-diffuse-5ac0fcb4426f](https://towardsdatascience.com/ai-diffusion-models-how-do-they-diffuse-5ac0fcb4426f)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Understanding the Core Processes Behind Generative AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@OnurGun?source=post_page-----5ac0fcb4426f--------------------------------)[![Onur
    Yuce Gun, PhD](../Images/aeb533625cbe7f40dd9f541e473975f6.png)](https://medium.com/@OnurGun?source=post_page-----5ac0fcb4426f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5ac0fcb4426f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5ac0fcb4426f--------------------------------)
    [Onur Yuce Gun, PhD](https://medium.com/@OnurGun?source=post_page-----5ac0fcb4426f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5ac0fcb4426f--------------------------------)
    ·7 min read·Nov 4, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Diffusion Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The name “diffusion models” in machine learning was derived from the statistical
    concept of diffusion processes.
  prefs: []
  type: TYPE_NORMAL
- en: What is that *statistical concept*?
  prefs: []
  type: TYPE_NORMAL
- en: In natural sciences, diffusion is the process by which particles spread from
    areas of high concentration to areas of low concentration over time, often described
    by the diffusion equation in physics and mathematics.
  prefs: []
  type: TYPE_NORMAL
- en: Reaction-diffusion is an excellent example of this.
  prefs: []
  type: TYPE_NORMAL
- en: Coral Growth Simulation by Karl Sims
  prefs: []
  type: TYPE_NORMAL
- en: Reaction-Diffusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Reaction-Diffusion is quite a complicated process; if you want to read the
    mathematical logic, you can visit the RD master Karl Sims’ website:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.karlsims.com/rd.html?source=post_page-----5ac0fcb4426f--------------------------------)
    [## Reaction-Diffusion Tutorial'
  prefs: []
  type: TYPE_NORMAL
- en: Illustrated tutorial of the Gray-Scott Reaction-Diffusion model.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.karlsims.com](https://www.karlsims.com/rd.html?source=post_page-----5ac0fcb4426f--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with a simple analogy:'
  prefs: []
  type: TYPE_NORMAL
- en: Reaction-diffusion systems are a way to describe how things change and move
    around, especially when you’re talking about chemicals. Imagine you have a couple
    of different paints on a piece of paper, and they start to mix and create new
    colors — that’s like the “reaction” part. The paint blots don’t just stay in one
    spot; they spread out and blend — that spreading is like the “diffusion” part.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, these systems are just a set of rules that tell us how these processes
    happen: how the chemicals react with each other to make new stuff and how they
    move around or spread out.'
  prefs: []
  type: TYPE_NORMAL
- en: This can describe a lot of different things in nature, such as how patterns
    form on animal skins, how pollution spreads in the environment, and lots of other
    situations where stuff is reacting and moving at the same time!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/54f5fc755a7329610e376bfa54c2fba6.png)'
  prefs: []
  type: TYPE_IMG
- en: The reaction-diffusion algorithm is quite skillful in generating appealing and
    functional patterns — image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Why is this useful?
  prefs: []
  type: TYPE_NORMAL
- en: Well, you can design shoes with them! ;)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/8e55db05e4d6f582465e7b0bcbb5b5e7.png)'
  prefs: []
  type: TYPE_IMG
- en: New Balance Two WXY featuring Reaction-Diffusion pattern — not my design, but
    the Computational Design Team introduced RD to designers in the office back in
    2018 — www.newbalance.com
  prefs: []
  type: TYPE_NORMAL
- en: Well, that is only one use case —
  prefs: []
  type: TYPE_NORMAL
- en: Reaction-diffusion systems can be described and simulated through a math formula,
    which in turn helps us understand the diffusion event.
  prefs: []
  type: TYPE_NORMAL
- en: These formulas mainly explain how the amounts of one or more chemicals change
    over time and move around in a space.
  prefs: []
  type: TYPE_NORMAL
- en: This involves chemical reactions, where the chemicals can turn into other chemicals,
    and diffusion, which is the process that makes these chemicals spread across an
    area.
  prefs: []
  type: TYPE_NORMAL
- en: Diffusion model in AI and ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the realm of Generative AI, diffusion models work in a somewhat analogous
    way by modeling the gradual process of adding noise to data and then learning
    to reverse this process.
  prefs: []
  type: TYPE_NORMAL
- en: However, it is essential not to confuse the patterns generated by Reaction-Diffusion
    with the noise that is added to images in ML systems!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The diffusion models take a signal (such as an image) and gradually add noise
    until the original signal is wholly obscured. Again, the noise added does not
    need to have a specific shape (As opposed to the beautiful patterns generated
    by reaction-diffusion!)
  prefs: []
  type: TYPE_NORMAL
- en: This is conceptually similar to the physical process of a substance diffusing
    through a medium.
  prefs: []
  type: TYPE_NORMAL
- en: 'The training of diffusion models involves learning the reverse process: starting
    from a noisy signal and progressively removing the noise to recover the original
    signal.'
  prefs: []
  type: TYPE_NORMAL
- en: This procedure is reminiscent of reversing the diffusion process in physics,
    where the particles’ positions are traced back from a state of equilibrium (completely
    diffused) to their original state (concentrated).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Hence, the name “diffusion models” captures the essence of this reverse process
    from noise to a clean, structured signal in the generative models of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: I have seen many diagrams explaining the diffusion models, but either I got
    lost in them, or they were oversimplified to explicate the actual process.
  prefs: []
  type: TYPE_NORMAL
- en: 'I also wanted to develop a **custom diagram for *designers***: these are people
    who understand pseudo-codes and flowcharts well but do not necessarily know the
    math that lies beneath (with exceptions, of course):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fb206fe3f5edddcb0e18cf11c14e2d16.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram showing the diffusion process — Drawn by the author
  prefs: []
  type: TYPE_NORMAL
- en: This diagram is neither super detailed not fully watered-down. I am hoping for
    the complexity to be “just right” for more people to understand the diffusion
    processes.
  prefs: []
  type: TYPE_NORMAL
- en: In this diagram, I find an urge to explain the UNET and subsequent “ADD NOISE”
    nodes as they constitute the “magic” in the diffusion processes.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start —
  prefs: []
  type: TYPE_NORMAL
- en: UNET
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The UNET is a convolutional network architecture for fast and precise segmentation
    of images.
  prefs: []
  type: TYPE_NORMAL
- en: Turns out it was first developed for biomedical image segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: The name UNET is derived from its U-shaped architecture!
  prefs: []
  type: TYPE_NORMAL
- en: 'Some visual communicators were in play in the derivation of the name “UNET”,
    I would guess!Before I make a more democratized description of UNET (that I can
    also understand (!)), here is a more line-by-lie technical description that I
    found on Medium (well done!):'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](/unet-line-by-line-explanation-9b191c76baf5?source=post_page-----5ac0fcb4426f--------------------------------)
    [## UNet Line by Line Explanation'
  prefs: []
  type: TYPE_NORMAL
- en: Example UNet Implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/unet-line-by-line-explanation-9b191c76baf5?source=post_page-----5ac0fcb4426f--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: UNET became popular in other applications, such as Geographic Information Systems
    (GIS). The technique of segmentation, in this respect, can aid in delineating
    shorelines from aerial or satellite data, allowing for precise coastal mapping.
    Similarly, it can be employed to identify and extract the outlines of large-scale
    structures, such as skyscrapers or industrial complexes, from high-resolution
    imagery.
  prefs: []
  type: TYPE_NORMAL
- en: So UNET’s utilization slowly spread to various image-to-image translation tasks
    beyond its original application, including tasks like image denoising, super-resolution,
    and finally into generative models such as stable diffusion.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the training of models for stable diffusion, which includes tasks like generating
    images from textual descriptions (text-to-image) or transforming one image into
    another (image-to-image) with certain stylistic changes, the U-Net serves a crucial
    role. Here’s a simplified explanation of its use:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Encoder-Decoder Structure**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Think of the UNET as a machine with two main parts. The first part is the “encoder,”
    which acts like a compact camera, zooming in on the most essential parts of an
    image and breaking it into smaller parts — so it’s easier to work with. The second
    part is the “decoder,” it works like an artist who takes that compacted version
    and redraws the whole picture with all the details.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. **Skip Connections**
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Typically, when you make something smaller (like in the encoder), you lose
    some details (obviously!). But the UNET has a clever trick: it uses “skip connections,”
    shortcuts that allow it to remember and bring back the details that might have
    been lost when the image was made smaller. This helps the “artist” part ensure
    the final picture is as detailed as the original.'
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. Latent Space Manipulation**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine the compact version of the image in the encoder as a recipe for baking
    a cake, which we call “latent space.” This recipe includes all the ingredients
    and steps needed to bake the cake a certain way. In stable diffusion, you can
    change this recipe to alter the final cake. Want to turn a vanilla cake into a
    chocolate one? You adjust the recipe — this is like tweaking the “latent space.”
    You’re not starting from scratch; you’re making minor changes to the ingredients
    (or image characteristics) to end up with a different but related result (And
    this can help us guess the hints for custom training). If the basic recipe is
    the sketch, the added flavors, and decorations are the colors and textures in
    a painting.
  prefs: []
  type: TYPE_NORMAL
- en: '**4\. Training with Noise**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Think of driving in a car during a heavy rainstorm. At first, the rain is pelting
    down your windshield, making it hard to see where you’re going. But your windshield
    wipers swipe away the water, giving you brief moments of clarity. Training a UNET
    with noise is like improving the quality of your windshield wipers. Initially,
    they might be flawed, and your vision is still blurry with each swipe. But as
    you “train” your wipers — adjusting their speed and ensuring they contact the
    glass correctly — they get better at clearing the rain from your view. In the
    same way, the U-Net starts by trying to see through the “rain” of noise on images.
    Over time, it gets better at swiping away the noise until it can consistently
    provide a clear “view” of the underlying image, just as effective wipers eventually
    give you a clear view of the road ahead — potentially not the best analogy, but
    you get the point…
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are not in favor of such analogies, here is a more direct description:'
  prefs: []
  type: TYPE_NORMAL
- en: ADDING AND REMOVING NOISE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: · Begin with a clean input image and introduce noise, transforming it into a
    noisy image.
  prefs: []
  type: TYPE_NORMAL
- en: · Feed the noisy image into the UNET, a denoising autoencoder, during training.
  prefs: []
  type: TYPE_NORMAL
- en: · Train the UNET to predict and identify the specific noise added to the image.
  prefs: []
  type: TYPE_NORMAL
- en: · Through iterative training, improve the U-Net’s ability to predict the noise
    accurately.
  prefs: []
  type: TYPE_NORMAL
- en: · As the UNET’s proficiency grows, it becomes more skilled at generating clean
    images from noisy data.
  prefs: []
  type: TYPE_NORMAL
- en: · Utilize the UNET’s noise handling and manipulation capabilities to enhance
    the diffusion process, which systematically adds and removes noise to craft new
    images.
  prefs: []
  type: TYPE_NORMAL
- en: This gradual diffusion process necessary to produce high-quality synthetic images
    has taken over the internet and our daily lives — but the impressive results of
    models like Midjourney, Stable Diffusion, Dall-E, and others in the domain of
    AI-driven image generation are undeniable.
  prefs: []
  type: TYPE_NORMAL
- en: There you have it. After reading this, I hope you have a better grasp of the
    diffusion models.
  prefs: []
  type: TYPE_NORMAL
- en: Get to know the author
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Want to learn more or get in touch? Here are my [LinkedIn](https://www.linkedin.com/in/onuryucegun/)
    and [YouTube](https://www.youtube.com/@computationaldesign) accounts. I share
    vibrant imagery and news over my [Instagram](https://www.instagram.com/onur.yuce.gun/)
    account, as well.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://developers.arcgis.com/python/guide/how-unet-works/?source=post_page-----5ac0fcb4426f--------------------------------)
    [## How U-net works? | ArcGIS API for Python'
  prefs: []
  type: TYPE_NORMAL
- en: ArcGIS API for Python documentation.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: developers.arcgis.com](https://developers.arcgis.com/python/guide/how-unet-works/?source=post_page-----5ac0fcb4426f--------------------------------)
    [](/convolutional-neural-networks-explained-9cc5188c4939?source=post_page-----5ac0fcb4426f--------------------------------)
    [## Convolutional Neural Networks, Explained
  prefs: []
  type: TYPE_NORMAL
- en: Let’s build your first CNN model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/convolutional-neural-networks-explained-9cc5188c4939?source=post_page-----5ac0fcb4426f--------------------------------)
  prefs: []
  type: TYPE_NORMAL
