- en: A 3-Step Approach to Evaluate a Retrieval Augmented Generation (RAG)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/a-3-step-approach-to-evaluate-a-retrieval-augmented-generation-rag-5acf2aba86de](https://towardsdatascience.com/a-3-step-approach-to-evaluate-a-retrieval-augmented-generation-rag-5acf2aba86de)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Stop selecting the parameters of your RAG randomly
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ahmedbesbes.medium.com/?source=post_page-----5acf2aba86de--------------------------------)[![Ahmed
    Besbes](../Images/93804d9291439715e578f204b79c9bdd.png)](https://ahmedbesbes.medium.com/?source=post_page-----5acf2aba86de--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5acf2aba86de--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5acf2aba86de--------------------------------)
    [Ahmed Besbes](https://ahmedbesbes.medium.com/?source=post_page-----5acf2aba86de--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5acf2aba86de--------------------------------)
    ¬∑9 min read¬∑Nov 23, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3398735846db1df7203b650519c9909b.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Adi Goldstein](https://unsplash.com/@adigold1?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 'Tuning your RAG to get optimal performance takes time, as this depends on various
    interdependent parameters: **chunk size, overlap, top K retrieved docs, embedding
    models, LLM, etc.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The best combination often depends on your data and your use case: you can‚Äôt
    simply plug in the settings you used in the last project and hope for the same
    results.'
  prefs: []
  type: TYPE_NORMAL
- en: Most people don‚Äôt address this issue properly and pick parameters almost randomly.
    While some are comfortable with this approach, I decided to tackle the problem
    numerically.
  prefs: []
  type: TYPE_NORMAL
- en: '**Here‚Äôs where evaluating your RAG comes in.**'
  prefs: []
  type: TYPE_NORMAL
- en: In ***this post, I‚Äôll show you a quick 3-step method you can follow to efficiently
    and quickly evaluate your RAGs across two tasks.***
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Retrieval**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Generation**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By mastering this evaluation pipeline, you can iterate, perform multiple experiments,
    compare them with metrics, and hopefully land on the best configuration
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs see how this works üëá.
  prefs: []
  type: TYPE_NORMAL
- en: '*PS: In each section, code snippets are provided to help you start implementing
    these ideas.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you‚Äôre interested in practical tips to increase your productivity in building
    ML systems, you feel free to subscribe to my [newsletter](https://thetechbuffet.substack.com/):
    The Tech Buffet.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I send weekly insights in programming and system design to help you ship AI
    products faster.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 1 ‚Äî Create a synthetic dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Evaluating an LLM often requires annotating a test set manually. This takes
    time, requires domain expertise, and is prone to human errors.
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully, LLMs can help us with this task.
  prefs: []
  type: TYPE_NORMAL
- en: Sample N chunks from your data. For each chunk, instruct an LLM to generate
    K tuples of questions and answers.
  prefs: []
  type: TYPE_NORMAL
- en: After the generation is complete, you will obtain a dataset of N*K tuples, each
    one having (question, answer, context).
  prefs: []
  type: TYPE_NORMAL
- en: '*Ps: The context here is the original chunk and its metadata*'
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, we‚Äôll consider a paragraph that refers to Sir Isaac
    Newton.
  prefs: []
  type: TYPE_NORMAL
- en: Isaac Newton is best know for his theory about the law of gravity, but his ‚ÄúPrincipia
    Mathematica‚Äù (1686)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: with its three laws of motion greatly influenced the Enlightenment in Europe.
    Born in 1643
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: in Woolsthorpe, England, Sir Isaac Newton began developing his theories
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: on light, calculus and celestial mechanics while on break from Cambridge University.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We‚Äôll instruct an LLM to generate three pairs of questions and answers from
    it.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/454300ccb8cb45367792c96f10991ded.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: This can be done using this prompt. (Adapted from one of my projects)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We‚Äôll learn more about the limitations of this synthetic dataset but for now,
    it‚Äôs a quick solution to evaluate our experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Code üíª
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the following section, I‚Äôll show you how you can create your synthetic evaluation
    dataset with LangChain.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first define the prompt and the LLM: I‚Äôm working on GCP, so I‚Äôm going to
    use VertexAI.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Then, we load some data from BQ and split it into chunks. Chunks will be the
    context needed for generating the questions and answers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Once the data is loaded and split, you need to sample some chunks to generate
    the pairs of questions/answers from.
  prefs: []
  type: TYPE_NORMAL
- en: This is done in the following loop.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 2 ‚Äî Run your RAG over each synthetic question
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once the synthetic dataset is built, you can use your RAG to perform predictions
    on each question.
  prefs: []
  type: TYPE_NORMAL
- en: This will generate answers grounded by a set of retrieved sources.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each question, make sure you extract the retrieved documents: these will
    be useful for running the evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/55f0058a3f8a23e00a77ee0b482ea32c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Code üíª
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the following section, we‚Äôll see how to build the RAG and make predictions
    on the evaluation data.
  prefs: []
  type: TYPE_NORMAL
- en: We first need to create a vector database to index the chunks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The default RAG implementation in LangChain uses a specific prompt template.
    In our case, we‚Äôll modify it a bit to prevent hallucinations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Then, we wrap our RAG implementation in a Python class.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the RAG is built and initialized, you can make the predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'After it‚Äôs done, here‚Äôs what you‚Äôll roughly get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e78ceb88f9a94cc94b9ff18afd317e5f.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot by the author
  prefs: []
  type: TYPE_NORMAL
- en: 3 ‚Äî Compute two evaluation metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, you can compute two evaluation metrics on the predictions you made for
    each question.
  prefs: []
  type: TYPE_NORMAL
- en: A **retrieval score** to assess the relevance of the retrieved documents
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This score can be binary (1/0 for each prediction) and tells whether the true
    source of each question is in the list of the retrieved sources. You can see this
    score as a *recall.*
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A **quality score** to evaluate the generated answer given the question and
    the ground-truth answer. Again, an LLM can be used in this task to produce such
    evaluation over 5.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here is the prompt I used to ask the LLM to rate the generated answer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: After obtaining these scores for each question, average them over the dataset
    to obtain the final two metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1149d76870704912ed16f1e5d9124ce3.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Code üíª
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we start with the`df_evaluations` dataframe and compute the
    different metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '**1 ‚Äî Retrieval scores**'
  prefs: []
  type: TYPE_NORMAL
- en: We can start by computing the top_k metrics that indicate whether the ground
    truth source is among the top k predicted sources.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Averaging over all the documents will give us the retrieval scores for each
    k.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6cc4be8b748707ba32aca1b64799e006.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot by the author
  prefs: []
  type: TYPE_NORMAL
- en: '**How to read these scores:**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Example: there is 0.37 probability that the ground truth source is among the
    top four retrieved source.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**2 ‚Äî Generation scores**'
  prefs: []
  type: TYPE_NORMAL
- en: To assess the quality of the generated answer, we will use an LLM that‚Äôll provide
    its rating and reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In this function, we instruct this LLM and fetch its rating and reasoning given:'
  prefs: []
  type: TYPE_NORMAL
- en: the query
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the predicted answer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the reference answer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we run this function over the evaluation data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'After generating the ratings, here‚Äôs the distribution we obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/06af084f3a1d30576f291d49bed50d64.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot by the author
  prefs: []
  type: TYPE_NORMAL
- en: 'Filtering on the ratings, here are some questions the RAG failed to generate
    an answer to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8d0b1d499bbf9d53c5c50c23c40d7d52.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot by the author
  prefs: []
  type: TYPE_NORMAL
- en: And here are some other questions where it succeeded.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6c4b4aee3353839f3272efc1501111f6.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot by the author
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing this data helps diagnose and address the errors the RAG makes.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of this evaluation approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I applied this evaluation method while building multiple RAGs for the pharmaceutical
    industry.
  prefs: []
  type: TYPE_NORMAL
- en: 'While it quickly provides an end-to-end pipeline and avoids manually labeling
    data, I noticed two main issues with it:'
  prefs: []
  type: TYPE_NORMAL
- en: Some of the generated questions are too specific and don‚Äôt make any sense to
    someone who didn‚Äôt read the context. Consequently, they impact the evaluation
    metrics badly since they‚Äôre *impossible* to answer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here‚Äôs an example: ‚ÄúWhat was the main issue discussed in this study?‚Äù'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Some other questions, on the other hand, are way too simple or are just a bare
    rephrasing of the original chunks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you want to learn more about this evaluation method (also called cold start),
    have a look at this [guide](https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1#cold-start).
  prefs: []
  type: TYPE_NORMAL
- en: If you‚Äôre new to RAGs and want to deepen your understanding, you can also check
    some of my previous posts to learn more.
  prefs: []
  type: TYPE_NORMAL
- en: '[Why Your RAG is Not Reliable in Production](https://thetechbuffet.substack.com/p/the-probelms-behind-rag)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Getting a RAG To Work Well Is Hard ‚Äî 5 Blog Posts To Become a RAG Master](https://thetechbuffet.substack.com/p/5-curated-rag-blog-posts)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do you know other evaluation methods that proved to be effective when building
    RAGs? Please tell me.
  prefs: []
  type: TYPE_NORMAL
- en: If you‚Äôre also dissecting RAGs for optimal performance, I‚Äôm also interested
    in discussing this topic.
  prefs: []
  type: TYPE_NORMAL
- en: That‚Äôd be all for me today. Until next time! üëã
  prefs: []
  type: TYPE_NORMAL
