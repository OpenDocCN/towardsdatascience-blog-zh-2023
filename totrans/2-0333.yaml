- en: arXiv Keyword Extraction and Analysis Pipeline with KeyBERT and Taipy
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: arXiv 关键词提取与分析管道，使用 KeyBERT 和 Taipy
- en: 原文：[https://towardsdatascience.com/arxiv-keyword-extraction-and-analysis-pipeline-with-keybert-and-taipy-2972e81d9fa4](https://towardsdatascience.com/arxiv-keyword-extraction-and-analysis-pipeline-with-keybert-and-taipy-2972e81d9fa4)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/arxiv-keyword-extraction-and-analysis-pipeline-with-keybert-and-taipy-2972e81d9fa4](https://towardsdatascience.com/arxiv-keyword-extraction-and-analysis-pipeline-with-keybert-and-taipy-2972e81d9fa4)
- en: Build a keyword analysis Python application comprising a frontend user interface
    and backend pipeline
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建一个包括前端用户界面和后端管道的关键词分析 Python 应用程序
- en: '[](https://kennethleungty.medium.com/?source=post_page-----2972e81d9fa4--------------------------------)[![Kenneth
    Leung](../Images/2514dffb34529d6d757c0c4ec5f98334.png)](https://kennethleungty.medium.com/?source=post_page-----2972e81d9fa4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2972e81d9fa4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2972e81d9fa4--------------------------------)
    [Kenneth Leung](https://kennethleungty.medium.com/?source=post_page-----2972e81d9fa4--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://kennethleungty.medium.com/?source=post_page-----2972e81d9fa4--------------------------------)[![Kenneth
    Leung](../Images/2514dffb34529d6d757c0c4ec5f98334.png)](https://kennethleungty.medium.com/?source=post_page-----2972e81d9fa4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2972e81d9fa4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2972e81d9fa4--------------------------------)
    [Kenneth Leung](https://kennethleungty.medium.com/?source=post_page-----2972e81d9fa4--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2972e81d9fa4--------------------------------)
    ·12 min read·Apr 18, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2972e81d9fa4--------------------------------)
    ·阅读时间 12 分钟·2023年4月18日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/177bb91daf175985fe9b1a3705329593.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/177bb91daf175985fe9b1a3705329593.png)'
- en: Photo by [Marylou Fortier](https://unsplash.com/@rylouma?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/heNLI144X7Y?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由 [Marylou Fortier](https://unsplash.com/@rylouma?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，来源于 [Unsplash](https://unsplash.com/photos/heNLI144X7Y?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: As the amount of textual data from sources like social media, customer reviews,
    and online platforms grows exponentially, we must be able to make sense of this
    unstructured data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随着来自社交媒体、客户评论和在线平台的文本数据量呈指数级增长，我们必须能够理解这些非结构化数据。
- en: Keyword extraction and analysis are powerful natural language processing (NLP)
    techniques that enable us to achieve that.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 关键词提取和分析是强大的自然语言处理（NLP）技术，帮助我们实现这一目标。
- en: '**Keyword extraction** involves automatically identifying and extracting the
    most relevant words from a given text, while **keyword analysis** involves analyzing
    the keywords to gain insights into the underlying patterns.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键词提取** 涉及自动识别和提取给定文本中最相关的词汇，而 **关键词分析** 则涉及分析这些关键词，以洞察潜在的模式。'
- en: In this step-by-step guide, we explore building a keyword extraction and analysis
    pipeline and web app on arXiv abstracts using the powerful tools of **KeyBERT**
    and **Taipy**.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这份逐步指南中，我们将探讨如何利用强大的工具 **KeyBERT** 和 **Taipy** 构建一个关键词提取和分析管道及网页应用程序，应用于 arXiv
    摘要。
- en: Contents
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目录
- en: '***(1)*** [*Context*](#3ed3)***(2)*** [*Tools Overview*](#e50e)***(3)*** [*Step-by-Step
    Guide*](#b70c)***(4)*** [*Wrapping it up*](#db84)'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***(1)*** [*背景*](#3ed3)***(2)*** [*工具概述*](#e50e)***(3)*** [*逐步指南*](#b70c)***(4)***
    [*总结*](#db84)'
- en: Here is the accompanying [GitHub repo](https://github.com/kennethleungty/Keyword-Analysis-with-KeyBERT-and-Taipy)
    for this article.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是本文的 [GitHub 仓库](https://github.com/kennethleungty/Keyword-Analysis-with-KeyBERT-and-Taipy)。
- en: (1) Context
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: (1) 背景
- en: Given the rapid progress in artificial intelligence (AI) and machine learning
    research, keeping track of the many papers published daily can be challenging.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 由于人工智能（AI）和机器学习研究的迅速进展，每天跟踪大量发表的论文可能具有挑战性。
- en: Regarding such research, [arXiv](https://arxiv.org/) is undoubtedly one of the
    leading sources of information. arXiv (pronounced ‘archive’) is an open-access
    archive hosting a vast collection of scientific papers covering various disciplines
    like computer science, mathematics, and more.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这类研究， [arXiv](https://arxiv.org/) 无疑是领先的信息来源之一。arXiv（发音为‘archive’）是一个开放获取的档案馆，收藏了大量涵盖计算机科学、数学等各学科的科学论文。
- en: '![](../Images/9a3babb0255822cfb690da00dd8c6e31.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9a3babb0255822cfb690da00dd8c6e31.png)'
- en: arXiv screenshot | Image used under [CC 2.0](https://ccnull.de/foto/arxivorg-logo-under-magnifying-glass/1014135)
    license
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: arXiv 截图 | 图像使用 [CC 2.0](https://ccnull.de/foto/arxivorg-logo-under-magnifying-glass/1014135)
    许可证
- en: One of the key features of arXiv is that it provides abstracts for each paper
    uploaded to its platform. These abstracts are an ideal data source as they are
    concise, rich in technical vocabulary, and contain domain-specific terminology.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: arXiv 的一个关键特性是它为每篇上传到平台上的论文提供摘要。这些摘要是理想的数据源，因为它们简洁、富含技术词汇，并包含领域特定术语。
- en: Hence, we will utilize the latest batches of arXiv abstracts as the text data
    to work on in this project.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将利用最新的 arXiv 摘要批次作为此项目中处理的文本数据。
- en: The goal is to create a web application (comprising a frontend interface and
    backend pipeline) where users can view the keywords and key phrases of arXiv abstracts
    based on specific input values.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是创建一个网络应用程序（包括前端界面和后端管道），用户可以根据特定输入值查看 arXiv 摘要的关键词和关键短语。
- en: '![](../Images/4bbf1043e0ffbbffc66e2b0d81daae13.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4bbf1043e0ffbbffc66e2b0d81daae13.png)'
- en: Screenshot of the completed application user interface | Image by author
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 完成的应用程序用户界面截图 | 作者提供的图像
- en: (2) Tools Overview
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: (2) 工具概述
- en: 'There are three main tools that we will use in this project:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们将使用三种主要工具：
- en: arXiv API Python wrapper
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: arXiv API Python 包装器
- en: KeyBERT
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KeyBERT
- en: Taipy
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Taipy
- en: (i) arXiv API Python wrapper
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: (i) arXiv API Python 包装器
- en: The arXiv website offers public API access to maximize its openness and interoperability.
    For example, to retrieve the text abstracts as part of our Python workflow, we
    can use the [**Python wrapper for the arXiv API**](https://github.com/lukasschwab/arxiv.py).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: arXiv 网站提供了公共 API 访问，以最大化其开放性和互操作性。例如，为了在我们的 Python 工作流程中检索文本摘要，我们可以使用 [**arXiv
    API 的 Python 包装器**](https://github.com/lukasschwab/arxiv.py)。
- en: The arXiv API Python wrapper provides a set of functions for searching the database
    for papers that match specific criteria, such as author, keyword, category, and
    more.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: arXiv API Python 包装器提供了一组函数，用于在数据库中搜索符合特定条件的论文，如作者、关键词、类别等。
- en: It also lets users retrieve detailed metadata about each paper, such as the
    title, abstract, authors, and publication date.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 它还允许用户检索有关每篇论文的详细元数据，如标题、摘要、作者和出版日期。
- en: (ii) KeyBERT
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: (ii) KeyBERT
- en: KeyBERT (from the terms ‘keyword’ and ‘BERT’) is a Python library that provides
    an easy-to-use interface for using [BERT](https://en.wikipedia.org/wiki/BERT_(language_model))
    embeddings and cosine similarity to extract the words in a document most representative
    of the document itself.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: KeyBERT（源自“keyword”和“BERT”）是一个 Python 库，提供了一个易于使用的界面来使用 [BERT](https://en.wikipedia.org/wiki/BERT_(language_model))
    嵌入和余弦相似度，以提取文档中最能代表文档本身的词汇。
- en: '![](../Images/9734dda5a53b0f87dec849acc0bbf8b6.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9734dda5a53b0f87dec849acc0bbf8b6.png)'
- en: Illustration of how KeyBERT works | Image used under [MIT License](https://github.com/MaartenGr/KeyBERT/blob/master/LICENSE)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: KeyBERT 工作原理的示意图 | 图像使用 [MIT 许可证](https://github.com/MaartenGr/KeyBERT/blob/master/LICENSE)
- en: The biggest strength of KeyBERT is its flexibility. It allows users to easily
    modify the underlying settings (e.g., parameters, embeddings, tokenization) to
    experiment and fine-tune the keywords obtained.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: KeyBERT 的最大优势在于其灵活性。它允许用户轻松修改基础设置（例如参数、嵌入、分词）以实验和微调获得的关键词。
- en: 'In this project, we will be tuning the following set of parameters:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们将调整以下参数集：
- en: Number of the top keywords to be returned
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 返回的顶级关键词数
- en: Word n-gram range (i.e., minimum and maximum n-gram length)
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词 n-gram 范围（即最小和最大 n-gram 长度）
- en: Diversification algorithm ([Max Sum Distance](/keyword-extraction-with-bert-724efca412ea#:~:text=Maximal%20Marginal%20Relevance-,Max%20Sum%20Similarity,-The%20maximum%20sum)
    or [Maximal Marginal Relevance](/keyword-extraction-with-bert-724efca412ea#:~:text=in%20your%20document.-,Maximal%20Marginal%20Relevance,-The%20final%20method))
    that determines how the similarity of extracted keywords is defined
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多样化算法（[最大总和距离](/keyword-extraction-with-bert-724efca412ea#:~:text=Maximal%20Marginal%20Relevance-,Max%20Sum%20Similarity,-The%20maximum%20sum)
    或 [最大边际相关性](/keyword-extraction-with-bert-724efca412ea#:~:text=in%20your%20document.-,Maximal%20Marginal%20Relevance,-The%20final%20method)），它决定了提取关键词的相似度定义方式。
- en: Number of candidates (if Max Sum Distance is set)
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 候选数（如果设置了最大总和距离）
- en: Diversity value (if Maximal Marginal Relevance is set)
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多样性值（如果设置了最大边际相关性）
- en: 'Both diversification algorithms (Max Sum Distance and Maximal Marginal Relevance)
    share the same basic idea of balancing two objectives: Retrieve results that are
    highly relevant to the query and yet are diverse in their content to avoid redundancy
    amongst each other.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 两种多样化算法（最大和距离和最大边际相关性）共享相同的基本思想，即平衡两个目标：检索与查询高度相关的结果，并且内容多样以避免彼此之间的冗余。
- en: (iii) Taipy
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: (iii) Taipy
- en: '[Taipy](https://www.taipy.io/) is an open-source Python application builder
    that quickly lets developers and data scientists turn data and machine learning
    algorithms into complete web applications.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[Taipy](https://www.taipy.io/) 是一个开源的 Python 应用程序构建工具，使开发人员和数据科学家能够迅速将数据和机器学习算法转换为完整的
    web 应用。'
- en: While designed to be a low-code library, Taipy also provides a high level of
    user customization. Therefore, it is well-suited for wide-ranging use cases, from
    simple dashboarding to production-ready industrial applications.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管设计为低代码库，Taipy 还提供了高度的用户自定义。因此，它非常适合广泛的使用案例，从简单的仪表板到生产就绪的工业应用。
- en: '![](../Images/9d3d3885aaae12d080ffe003c7a77e85.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d3d3885aaae12d080ffe003c7a77e85.png)'
- en: Taipy components | Image by author
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Taipy 组件 | 作者提供的图片
- en: 'There are two key components of Taipy: Taipy GUI and Taipy Core.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Taipy 有两个关键组件：Taipy GUI 和 Taipy Core。
- en: '**Taipy GUI**: A simple graphical user interface builder enabling us to easily
    create an interactive frontend app interface.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Taipy GUI**：一个简单的图形用户界面构建工具，使我们能够轻松创建交互式前端应用界面。'
- en: '**Taipy Core**: A modern backend framework that lets us efficiently build and
    execute pipelines and scenarios.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Taipy Core**：一个现代的后端框架，能够高效地构建和执行管道和场景。'
- en: While we can use Taipy GUI or Taipy Core independently, combining both allows
    us to build powerful applications efficiently.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们可以独立使用 Taipy GUI 或 Taipy Core，但将两者结合使用可以高效地构建强大的应用程序。
- en: (3) Step-by-Step Guide
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: (3) 分步指南
- en: As mentioned earlier in the [Context](#3ed3) section, we will build a web app
    that extracts and analyzes keywords of selected arXiv abstracts.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面在[上下文](#3ed3)部分提到的，我们将构建一个 web 应用，提取和分析选定的 arXiv 摘要的关键词。
- en: The following diagram illustrates how the data and tools are integrated.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示说明了数据和工具如何集成。
- en: '![](../Images/e3daa3594d8ca067ec33f8d2fe438e50.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e3daa3594d8ca067ec33f8d2fe438e50.png)'
- en: Overview of project | Image by author
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 项目概述 | 作者提供的图片
- en: Let us get started with the steps to create the above pipeline and web application
    in Python.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始创建上述管道和 web 应用的步骤。
- en: Step 1 — Initial Setup
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 1 步 — 初始设置
- en: 'We start by pip installing the necessary Python libraries with corresponding
    versions shown below:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用下面显示的相应版本，通过 pip 安装必要的 Python 库：
- en: '[arvix](https://pypi.org/project/arxiv/) 1.4.3'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[arvix](https://pypi.org/project/arxiv/) 1.4.3'
- en: '[keybert](https://pypi.org/project/keybert/) 0.7.0'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[keybert](https://pypi.org/project/keybert/) 0.7.0'
- en: '[pandas](https://pypi.org/project/pandas/) 1.5.3'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[pandas](https://pypi.org/project/pandas/) 1.5.3'
- en: '[taipy](https://pypi.org/project/taipy/) 2.2.0'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[taipy](https://pypi.org/project/taipy/) 2.2.0'
- en: Step 2 — Setup Configuration File
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 2 步 — 设置配置文件
- en: As numerous parameters will be used, saving them inside a separate configuration
    file is ideal. The following YAML file `config.yml` contains the initial set of
    configuration parameter values.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 由于将使用许多参数，将它们保存在单独的配置文件中是理想的。以下 YAML 文件 `config.yml` 包含初始的配置参数值。
- en: 'With the configuration file set up, we can then easily import these parameter
    values into our other Python scripts with the following code:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 配置文件设置好后，我们可以通过以下代码将这些参数值轻松导入到其他 Python 脚本中。
- en: '[PRE0]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Step 3 — Build Functions
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 3 步 — 构建函数
- en: In this step, we will create a series of Python functions that form vital components
    of the pipeline. We create a new Python file `functions.py` to store these functions.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在此步骤中，我们将创建一系列 Python 函数，这些函数构成管道的重要组件。我们创建一个新的 Python 文件 `functions.py` 来存储这些函数。
- en: (3.1) Retrieve and Save arXiv Abstracts and Metadata
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: (3.1) 检索和保存 arXiv 摘要和元数据
- en: The first function to add into `functions.py` is one for retrieving text abstracts
    from the arXiv database using the arXiv API Python wrapper.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个要添加到 `functions.py` 的函数是用于通过 arXiv API Python 包从 arXiv 数据库中检索文本摘要的函数。
- en: Next, we write a function to store the abstract texts and corresponding metadata
    in a pandas DataFrame.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们编写一个函数，将摘要文本和相应的元数据存储在 pandas DataFrame 中。
- en: (3.2) Process Data
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: (3.2) 处理数据
- en: For the data processing step, we have the following function to parse the abstract
    publication date into the appropriate format while creating new empty columns
    to store keywords.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据处理步骤，我们有以下函数来将摘要发布日期解析为适当的格式，同时创建新的空列以存储关键词。
- en: (3.3) Run KeyBERT
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: (3.3) 运行 KeyBERT
- en: We next create a function to run the `KeyBert` class from the KeyBERT library.
    The `KeyBERT` class is a minimal method for keyword extraction with BERT and is
    the easiest way for us to get started.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个函数来运行 KeyBERT 库中的 `KeyBert` 类。`KeyBert` 类是使用 BERT 进行关键词提取的最小方法，是我们入门的最简单方法。
- en: There are many different methods for generating the BERT embeddings (e.g., [Flair](https://github.com/flairNLP/),
    [Huggingface Transformers](https://github.com/huggingface/transformers), and [spaCy](https://nightly.spacy.io/)).
    In this case, we will use [sentence-transformers](https://www.sbert.net/) as recommended
    by the KeyBERT creator.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 生成 BERT 嵌入的方法有很多种（例如，[Flair](https://github.com/flairNLP/)，[Huggingface Transformers](https://github.com/huggingface/transformers)和
    [spaCy](https://nightly.spacy.io/)）。在这种情况下，我们将使用 [sentence-transformers](https://www.sbert.net/)
    ，这是 KeyBERT 创建者推荐的。
- en: In particular, we will use the default`[all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)`
    model as it provides a good balance of speed and quality.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，我们将使用默认的`[all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)`
    模型，因为它在速度和质量之间提供了很好的平衡。
- en: The following function extracts the keywords from each abstract iteratively
    and saves them in the new DataFrame columns created in the previous step.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数从每个摘要中迭代提取关键词，并将其保存到前一步创建的新 DataFrame 列中。
- en: (3.4) Get Keywords Value Counts
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: (3.4) 获取关键词值计数
- en: Finally, we create a function that generates a value count of the keywords so
    that we can plot the keyword frequencies in a chart later.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们创建一个函数生成关键词的值计数，以便稍后我们可以在图表中绘制关键词频率。
- en: 'Step 4 — Setup Taipy Core: Backend Config'
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 4 — 设置 Taipy Core：后台配置
- en: To orchestrate and link the backend pipeline flow, we will leverage the capabilities
    of Taipy Core.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了协调和连接后台管道流程，我们将利用 Taipy Core 的功能。
- en: 'Taipy Core offers an open-source framework to create, manage, and execute our
    data pipelines easily and efficiently. It has four fundamental concepts: **Data
    Nodes, Tasks, Pipelines, and Scenarios**.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Taipy Core 提供了一个开源框架，便于创建、管理和高效执行我们的数据管道。它有四个基本概念：**数据节点、任务、管道和场景**。
- en: '![](../Images/c3fc4bb3f3728edad5077f91638991d1.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c3fc4bb3f3728edad5077f91638991d1.png)'
- en: Four fundamental concepts in Taipy Core | Image by author
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Taipy Core 的四个基本概念 | 图片来自作者
- en: To set up the backend, we will use **configuration objects** (from the `Config`
    class) to model and define the characteristics and desired behavior of the abovementioned
    concepts.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了设置后台，我们将使用**配置对象**（来自`Config`类）来建模和定义上述概念的特征和期望行为。
- en: (4.1) Data Nodes
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: (4.1) 数据节点
- en: As with most data science projects, we start by handling the data. In Taipy
    Core, we use **Data Nodes** to define the data we will work with.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 与大多数数据科学项目一样，我们从处理数据开始。在 Taipy Core 中，我们使用**数据节点**来定义我们将处理的数据。
- en: We can think of Data Nodes as Taipy’s representation of data variables. However,
    instead of storing the data directly, Data Nodes contain a set of instructions
    on how to retrieve the data needed.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将数据节点视为 Taipy 对数据变量的表示。然而，数据节点不是直接存储数据，而是包含了一组如何检索所需数据的指令。
- en: Data Nodes can read and write a wide range of data types, such as Python objects
    (e.g., `str`, `int`, `list`, `dict`, `DataFrame`, etc.), Pickle files, CSVs, SQL
    databases, and more.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 数据节点可以读取和写入各种数据类型，例如 Python 对象（如`str`、`int`、`list`、`dict`、`DataFrame` 等）、Pickle
    文件、CSV 文件、SQL 数据库等。
- en: Using the `Config.configure_data_node()` function, we define the Data Nodes
    for the keyword parameters based on the values from the configuration file in
    [Step 2](#e847).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `Config.configure_data_node()` 函数，我们根据 [步骤 2](#e847) 中配置文件的值定义关键词参数的数据节点。
- en: The `id` parameter sets the name of the Data Node, while the `default_data`
    parameter defines the default values.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`id` 参数设置数据节点的名称，而 `default_data` 参数定义默认值。'
- en: 'We next include the configuration objects for the five sets of data along the
    pipeline, as illustrated below:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将配置对象包含到管道的五组数据中，如下所示：
- en: '![](../Images/ad04a4bebcbf9175e231f2a733ab274a.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad04a4bebcbf9175e231f2a733ab274a.png)'
- en: Illustration of five Data Nodes along pipeline | Image by author
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 五个数据节点沿管道的示意图 | 图片来自作者
- en: 'The following code defines the five configuration objects:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码定义了五个配置对象：
- en: (4.2) Tasks
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: (4.2) 任务
- en: Tasks in Taipy can be thought of as Python functions. We can define the configuration
    object for Tasks using the `Config.configure_task()`.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Taipy 中的任务可以被视为 Python 函数。我们可以使用 `Config.configure_task()` 定义任务的配置对象。
- en: We need to set five Task configuration objects corresponding to the five functions
    built in [Step 3](#124f).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要设置五个任务配置对象，分别对应 [步骤 3](#124f) 中构建的五个功能。
- en: '![](../Images/035776668d4c7670d9d28dedca15f3db.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/035776668d4c7670d9d28dedca15f3db.png)'
- en: Illustration of the five Tasks | Image by author
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 五个任务的示意图 | 作者提供的图片
- en: The `input` and `output` parameters refer to the input and output Data Nodes,
    respectively.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`input` 和 `output` 参数分别指输入和输出数据节点。'
- en: For example, in `task_process_data_cfg`, the input is the Data Node for the
    raw pandas DataFrame containing the arXiv search results, while the output is
    the Data Node for the DataFrame storing processed data.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在 `task_process_data_cfg` 中，输入是包含 arXiv 搜索结果的原始 pandas DataFrame 的数据节点，而输出是存储处理后数据的
    DataFrame 的数据节点。
- en: The `skippable` parameter, when set to True, indicates that the Task can be
    skipped if no changes have been made to the inputs.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`skippable` 参数设置为 True 时，表示如果输入没有更改，则可以跳过该任务。'
- en: 'Here is the flowchart of the Data Nodes and Tasks we have defined so far:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们迄今为止定义的数据节点和任务的流程图：
- en: '![](../Images/927e3dd829a3b8d3bb622dceb60d5b51.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/927e3dd829a3b8d3bb622dceb60d5b51.png)'
- en: Data Nodes and Tasks flowchart | Image by author
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 数据节点和任务流程图 | 作者提供的图片
- en: (4.3) Pipelines
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: (4.3) 管道
- en: A **Pipeline** is a series of Tasks that will be executed automatically by Taipy.
    It is a configuration object comprising a sequence of Task configuration objects.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**管道**是由 Taipy 自动执行的一系列任务。它是一个配置对象，由一系列任务配置对象组成。'
- en: 'In this case, we will allocate the five Tasks into two Pipelines (one for data
    preparation and one for keyword analysis) as illustrated below:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们将五个任务分配到两个管道中（一个用于数据准备，一个用于关键字分析），如下所示：
- en: '![](../Images/6d8dd694b913ad44a1067a4b388a8536.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d8dd694b913ad44a1067a4b388a8536.png)'
- en: Tasks within the two pipelines | Image by author
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 两个管道中的任务 | 作者提供的图片
- en: 'We use the following code to define our two Pipeline configs:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下代码来定义两个管道配置：
- en: As with all configuration objects, we assign a name to these Pipeline configurations
    using the `id` parameter.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有配置对象一样，我们使用 `id` 参数为这些管道配置分配名称。
- en: (4.4) Scenarios
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: (4.4) 场景
- en: In this project, we aim to create an application that reflects the updated set
    of keywords (and corresponding analysis) based on changes made to input parameters
    (e.g., N-gram length).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们的目标是创建一个应用程序，反映基于输入参数（例如 N-gram 长度）变化的更新关键字集合（及其相应分析）。
- en: For that to happen, we leverage the powerful concept of **Scenarios**. Taipy
    Scenarios provide the framework for running Pipelines under different conditions,
    such as when the user modifies the input parameters or data.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一目标，我们利用了强大的**场景**概念。Taipy 场景提供了一个框架，用于在不同条件下运行管道，例如用户修改输入参数或数据时。
- en: Scenarios also allow us to save the outputs from the different inputs for easy
    comparison within the same app interface.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 场景还允许我们保存来自不同输入的输出，以便在同一应用程序界面中进行轻松比较。
- en: Since we expect to do a straightforward sequential run of the Pipelines, we
    can place both Pipeline configs into the one Scenario configuration object.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们预计要对管道进行直接的顺序运行，我们可以将两个管道配置放入一个场景配置对象中。
- en: Step 5 — Setup Taipy GUI (Frontend)
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 5 步 — 设置 Taipy GUI（前端）
- en: Let us now switch gears and explore the frontend aspects of our application.
    Taipy GUI provides Python classes that make it easy to create powerful web app
    interfaces with text and graphical elements.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们转变思路，探索应用程序的前端方面。Taipy GUI 提供了 Python 类，使得创建具有文本和图形元素的强大 Web 应用程序界面变得容易。
- en: Pages are the basis for the user interface, and they hold text, images, or controls
    that display information in the application through visual elements.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 页面是用户界面的基础，它们包含文本、图像或控件，通过视觉元素在应用程序中显示信息。
- en: 'There are two pages to create: **(i)** a keyword analysis dashboard page and
    **(ii)** a data viewer page to display the keywords DataFrame.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 需要创建两个页面：**（i）** 关键字分析仪表板页面和 **（ii）** 数据查看器页面，以显示关键字 DataFrame。
- en: (5.1) Data Viewer
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: (5.1) 数据查看器
- en: Taipy GUI can be considered an **augmented** Markdown, meaning we can use the
    Markdown syntax to build our frontend interface.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Taipy GUI 可以被视为**增强版**的 Markdown，这意味着我们可以使用 Markdown 语法来构建我们的前端界面。
- en: We start with the simple frontend page displaying the DataFrame of the extracted
    arXiv abstract data. The page is set up in a Python script (named `data_viewer_md.py`)
    and storing the Markdown in a variable (called `data_page)`.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从简单的前端页面开始，显示提取的arXiv摘要数据的DataFrame。该页面设置在一个Python脚本（名为`data_viewer_md.py`）中，并将Markdown存储在一个名为`data_page`的变量中。
- en: The basic syntax for creating Taipy constructs in Markdown is using text fragments
    in the generic format of `<|...|...|>`.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在Markdown中创建Taipy构造的基本语法是使用文本片段，其通用格式为`<|...|...|>`。
- en: 'In the above Markdown, we pass our DataFrame object `df` along with `table`,
    which indicates a **table** element. With just these few lines of code, we get
    an output like the following:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述Markdown中，我们传递了我们的DataFrame对象`df`和`table`，后者表示一个**表格**元素。仅这些几行代码，我们就可以获得如下输出：
- en: '![](../Images/55985bde47e7b8cf96f7881a80dfb119.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/55985bde47e7b8cf96f7881a80dfb119.png)'
- en: Screenshot of the Data Viewer page | Image by author
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 数据查看器页面的截图 | 作者提供的图片
- en: (5.2) Keyword Analysis Dashboard
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: (5.2) 关键词分析仪表板
- en: We now move to the main dashboard page of the application, where we can make
    changes to the parameters and visualize the keywords obtained. The visual elements
    will be contained within a Python script (named `analysis_md.py`)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们转到应用程序的主要仪表板页面，在那里我们可以更改参数并可视化获得的关键词。可视化元素将包含在一个Python脚本（名为`analysis_md.py`）中
- en: This page has numerous components, so let’s take it one step at a time. First,
    we instantiate the parameter values upon the loading of the application.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 此页面包含多个组件，所以让我们一步步来。首先，在应用程序加载时，我们实例化参数值。
- en: 'Next, we define the input segment of the page where users can make changes
    to parameters and scenarios. This segment will be saved in a variable called `input_page`,
    and will eventually look like this:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义页面的输入部分，用户可以在其中更改参数和场景。此部分将保存到一个名为`input_page`的变量中，最终将如下所示：
- en: '![](../Images/a9352eb0112b9821e49a6eeca02d3be8.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9352eb0112b9821e49a6eeca02d3be8.png)'
- en: Input segment of the Keyword Analysis page | Image by author
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 关键词分析页面的输入部分 | 作者提供的图片
- en: We create a seven-column layout in the Markdown so that the input fields (e.g.,
    text input, number input, dropdown menu selector) and buttons can be organized
    neatly.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在Markdown中创建一个七列布局，以便将输入字段（例如文本输入、数字输入、下拉菜单选择器）和按钮整齐地组织起来。
- en: We will explain the callback functions in the `on_change` and `on_action` parameters
    for the elements above, so there is no need to worry about them for now.
  id: totrans-142
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们将解释`on_change`和`on_action`参数中元素的回调函数，因此现在无需担心这些内容。
- en: After that, we define the output segment, where the frequency table and chart
    of the keywords based on the input parameters will be displayed.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们定义输出部分，其中将显示基于输入参数的关键词频率表和图表。
- en: '![](../Images/7ffaf72b0ce02d879c3a4ad3980820f3.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ffaf72b0ce02d879c3a4ad3980820f3.png)'
- en: Output segment of the Keyword Analysis page | Image by author
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 关键词分析页面的输出部分 | 作者提供的图片
- en: We will define the chart properties in addition to specifying the Markdown of
    the output segment in the variable `output_page`.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 除了指定输出部分的Markdown外，我们还将定义图表属性，将其存储在`output_page`变量中。
- en: And in the last line above, we combine both input and output segments into a
    single variable called `analysis_page`.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述最后一行中，我们将输入和输出部分合并为一个名为`analysis_page`的变量。
- en: (5.3) Main Landing Page
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: (5.3) 主着陆页面
- en: One last bit before our frontend interface is complete. Now that we have both
    pages ready, we shall display them on our main landing page.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的前端界面完成之前，还有最后一部分。现在我们已经准备好两个页面，我们将在主着陆页面上显示它们。
- en: The main page is defined within `main.py`, which is the script that will be
    run when the application is launched. The aim is to create a functional menu bar
    on the main page for users to toggle between the pages.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 主页面在`main.py`中定义，这是应用程序启动时运行的脚本。目标是创建一个功能菜单栏，供用户在页面之间切换。
- en: From the above code, we can see the state functionality of Taipy in action,
    where the page is rendered based on the selected page in the session state.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述代码中，我们可以看到Taipy的状态功能正在运行，其中页面根据会话状态中选择的页面进行渲染。
- en: Step 6— Linking Backend and Frontend with Scenarios
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤6——使用场景连接后端和前端
- en: At this point, our frontend interface and backend pipeline have been set up
    successfully. However, we have yet to link both of them together.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们的前端界面和后端管道已成功设置。然而，我们还需要将它们链接在一起。
- en: More specifically, we will need to create the **Scenarios** component so that
    variations in the input parameters are processed in the pipeline, and the output
    is reflected in the dashboard.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，我们需要创建 **场景** 组件，以便管道中处理输入参数的变化，并将输出反映在仪表板中。
- en: The added benefit of Scenarios is that every input-output set can be saved so
    that users can refer back to these previous configurations.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 场景的附加好处是每个输入输出集都可以被保存，以便用户可以重新查看这些先前的配置。
- en: 'We will define four functions to set up the Scenarios component, which will
    be stored in the `analysis_md.py` script:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义四个函数来设置场景组件，这些函数将存储在 `analysis_md.py` 脚本中：
- en: (6.1) Update Chart
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: (6.1) 更新图表
- en: This function updates the keywords DataFrame, frequency count table, and corresponding
    bar chart based on the input parameters of the selected Scenario stored in the
    session state.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数根据存储在会话状态中的所选场景的输入参数更新关键词数据框、频率统计表和相应的条形图。
- en: (6.2) Submit Scenario
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: (6.2) 提交场景
- en: This function registers the updated set of input parameters the user has modified
    as a scenario and passes the values through the pipeline.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数将用户修改后的输入参数集注册为场景，并将这些值通过管道传递。
- en: (6.3) Create Scenario
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: (6.3) 创建场景
- en: This function saves a scenario that has been executed so that it can be easily
    recreated and referred to again from the dropdown menu of created Scenarios.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数保存一个已执行的场景，以便它可以轻松地从创建的场景下拉菜单中重新创建和引用。
- en: (6.4) Synchronize GUI and Core
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: (6.4) 同步 GUI 和 Core
- en: This function retrieves input parameters from a Scenario selected from the dropdown
    menu of saved Scenarios and displays the resulting output in the frontend GUI.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数从保存的场景下拉菜单中选择的场景中检索输入参数，并在前端 GUI 中显示结果输出。
- en: Step 7— Launching the Application
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 7 步—— 启动应用程序
- en: In the last step, we wrap up by completing the code in `main.py` so that the
    Taipy launches and runs correctly when the script is executed.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一步，我们通过完成 `main.py` 中的代码来结束，以确保 Taipy 启动并在脚本执行时正确运行。
- en: 'The above code does the following steps:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码完成了以下步骤：
- en: Instantiate Taipy Core
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实例化 Taipy Core
- en: Setup scenario creation and execution
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置场景创建和执行
- en: Retrieve keywords DataFrame and frequency count table
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索关键词数据框和频率统计表
- en: Launch Taipy GUI (with the specified pages)
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启动 Taipy GUI（带有指定页面）
- en: Finally, we can run `python main.py` in the Command Line, and the application
    we have built will be accessible on `localhost:8020`.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以在命令行中运行 `python main.py`，我们构建的应用程序将可以在 `localhost:8020` 上访问。
- en: '![](../Images/4bbf1043e0ffbbffc66e2b0d81daae13.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4bbf1043e0ffbbffc66e2b0d81daae13.png)'
- en: Frontend interface of completed application | Image by author
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 完成应用程序的前端界面 | 图片来源：作者
- en: (4) Wrapping it up
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: (4) 总结
- en: The keywords associated with a document offer concise and comprehensive indications
    of its subject matter, highlighting the most important themes, concepts, ideas,
    or arguments contained therein.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 与文档相关的关键词提供了该文档主题的简明而全面的指示，突出了其中包含的最重要的主题、概念、观点或论点。
- en: In this article, we explored how to extract and analyze keywords of arXiv abstracts
    using KeyBERT and Taipy. We also discovered how to deliver these capabilities
    as a web application comprising a frontend user interface and a backend pipeline.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们探讨了如何使用 KeyBERT 和 Taipy 提取和分析 arXiv 摘要中的关键词。我们还发现了如何将这些功能作为一个包含前端用户界面和后端管道的
    web 应用程序进行交付。
- en: Feel free to check out the codes in the accompanying [**GitHub repo**](https://github.com/kennethleungty/Keyword-Analysis-with-KeyBERT-and-Taipy).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 随意查看附带的 [**GitHub 仓库**](https://github.com/kennethleungty/Keyword-Analysis-with-KeyBERT-and-Taipy)
    中的代码。
- en: Before you go
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在你离开之前
- en: I welcome you to **join me on a journey of data science discovery!** Follow
    this [Medium](https://kennethleungty.medium.com/) page and visit my [GitHub](https://github.com/kennethleungty)
    to stay updated with more engaging and practical content. Meanwhile, have fun
    building your keyword extraction and analysis pipeline with KeyBERT and Taipy!
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我欢迎你 **加入我的数据科学发现之旅！** 关注这个 [Medium](https://kennethleungty.medium.com/) 页面，并访问我的
    [GitHub](https://github.com/kennethleungty)，以获取更多引人入胜和实用的内容。与此同时，祝你使用 KeyBERT
    和 Taipy 构建关键词提取和分析管道时玩得开心！
- en: '[](/when-ai-goes-astray-high-profile-machine-learning-mishaps-in-the-real-world-26bd58692195?source=post_page-----2972e81d9fa4--------------------------------)
    [## When AI Goes Astray: High-Profile Machine Learning Mishaps in the Real World'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/when-ai-goes-astray-high-profile-machine-learning-mishaps-in-the-real-world-26bd58692195?source=post_page-----2972e81d9fa4--------------------------------)
    [## 当 AI 偏离轨道：现实世界中的高调机器学习失误'
- en: A tour of infamous machine learning blunders and failures that caught the world’s
    attention
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一次关于臭名昭著的机器学习失误和失败的巡礼，这些失误和失败引起了世界的关注
- en: towardsdatascience.com](/when-ai-goes-astray-high-profile-machine-learning-mishaps-in-the-real-world-26bd58692195?source=post_page-----2972e81d9fa4--------------------------------)
    [](https://medium.datadriveninvestor.com/how-to-web-scrape-wikipedia-using-llm-agents-f0dba8400692?source=post_page-----2972e81d9fa4--------------------------------)
    [## How to Web Scrape Wikipedia with LLM Agents
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/when-ai-goes-astray-high-profile-machine-learning-mishaps-in-the-real-world-26bd58692195?source=post_page-----2972e81d9fa4--------------------------------)
    [](https://medium.datadriveninvestor.com/how-to-web-scrape-wikipedia-using-llm-agents-f0dba8400692?source=post_page-----2972e81d9fa4--------------------------------)
    [## 如何使用 LLM 代理抓取维基百科
- en: Simple guide to using LangChain Agents and Tools with OpenAI’s LLMs and Function
    Calling for web scraping of Wikipedia
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 LangChain 代理和工具与 OpenAI 的 LLM 及函数调用进行维基百科网页抓取的简单指南
- en: medium.datadriveninvestor.com](https://medium.datadriveninvestor.com/how-to-web-scrape-wikipedia-using-llm-agents-f0dba8400692?source=post_page-----2972e81d9fa4--------------------------------)
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: medium.datadriveninvestor.com](https://medium.datadriveninvestor.com/how-to-web-scrape-wikipedia-using-llm-agents-f0dba8400692?source=post_page-----2972e81d9fa4--------------------------------)
