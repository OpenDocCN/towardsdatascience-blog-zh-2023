["```py\n#for dataframe manipulation\nimport numpy as np \nimport pandas as pd\n\n#regular expressoin toolkit\nimport re\n\n#NLP toolkits\nimport nltk\nnltk.download('punkt')\nfrom nltk.tokenize import word_tokenize\n\n#for plotting expense categories later\nimport matplotlib.pyplot as plt \nplt.style.use('ggplot')\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.ticker as ticker # for formatting major units on x-y axis\n\n#for downloading BERT\n!pip install sentence_transformers\nfrom sentence_transformers import SentenceTransformer\n\n#for finding most similar text vectors\nfrom sklearn.metrics.pairwise import cosine_similarity\n```", "```py\n###############################################\n### Define a function for NLP data cleaning ###\n###############################################\n\ndef clean_text_BERT(text):\n\n    # Convert words to lower case.\n    text = text.lower()\n\n    # Remove special characters and numbers. This also removes the dates \n    # which are not important in classifying expenses\n    text = re.sub(r'[^\\w\\s]|https?://\\S+|www\\.\\S+|https?:/\\S+|[^\\x00-\\x7F]+|\\d+', '', str(text).strip())\n\n    # Tokenise \n    text_list = word_tokenize(text)\n    result = ' '.join(text_list)\n    return result\n```", "```py\ntext_raw = df_transaction_description['Description']\ntext_BERT = text_raw.apply(lambda x: clean_text_BERT(x))\n```", "```py\n######################################\n### Download pre-trained BERT model###\n######################################\n\n# This may take some time to download and run \n# depending on the size of the input\n\nbert_input = text_BERT.tolist()\nmodel = SentenceTransformer('paraphrase-mpnet-base-v2') \nembeddings = model.encode(bert_input, show_progress_bar = True)\nembedding_BERT = np.array(embeddings)\n```", "```py\n# Load texts\ntext_test_raw = df_transaction_description_test['Test']\n\n# Apply data cleaning function as for training data\ntext_test_BERT = text_test_raw.apply(lambda x: clean_text_BERT(x))\n\n# Apply BERT embedding\nbert_input_test = text_test_BERT.tolist()\n#model = SentenceTransformer('paraphrase-mpnet-base-v2') \nembeddings_test = model.encode(bert_input_test, show_progress_bar = True)\nembedding_BERT_test = np.array(embeddings_test)\n\ndf_embedding_bert_test = pd.DataFrame(embeddings_test)\n```", "```py\n # Find the most similar word embedding with unseen data in the training data\n\nsimilarity_new_data = cosine_similarity(embedding_BERT_test, embedding_BERT)\nsimilarity_df = pd.DataFrame(similarity_new_data)\n\n# Returns index for most similar embedding\n# See first column of the output dataframe below\nindex_similarity = similarity_df.idxmax(axis = 1)\n\n# Return dataframe for most similar embedding/transactions in training dataframe\ndata_inspect = df_transaction_description.iloc[index_similarity, :].reset_index(drop = True)\n\nunseen_verbatim = text_test_raw\nmatched_verbatim = data_inspect['Description']\nannotation = data_inspect['Class']\n\nd_output = {\n            'unseen_transaction': unseen_verbatim,\n            'matched_transaction': matched_verbatim, \n            'matched_class': annotation\n\n            } \n```"]