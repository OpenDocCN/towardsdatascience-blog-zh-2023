- en: Hyperparameter Optimization With Hyperopt — Intro & Implementation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**超参数优化与 Hyperopt — 介绍与实现**'
- en: 原文：[https://towardsdatascience.com/hyperparameter-optimization-with-hyperopt-intro-implementation-dfc1c54d0ba7](https://towardsdatascience.com/hyperparameter-optimization-with-hyperopt-intro-implementation-dfc1c54d0ba7)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/hyperparameter-optimization-with-hyperopt-intro-implementation-dfc1c54d0ba7](https://towardsdatascience.com/hyperparameter-optimization-with-hyperopt-intro-implementation-dfc1c54d0ba7)
- en: Improving machine learning models’ performance with hyperparameter optimization.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过超参数优化提升机器学习模型的性能。
- en: '[](https://medium.com/@fmnobar?source=post_page-----dfc1c54d0ba7--------------------------------)[![Farzad
    Mahmoodinobar](../Images/2d75209693b712300e6f0796bd2487d0.png)](https://medium.com/@fmnobar?source=post_page-----dfc1c54d0ba7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dfc1c54d0ba7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dfc1c54d0ba7--------------------------------)
    [Farzad Mahmoodinobar](https://medium.com/@fmnobar?source=post_page-----dfc1c54d0ba7--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@fmnobar?source=post_page-----dfc1c54d0ba7--------------------------------)[![Farzad
    Mahmoodinobar](../Images/2d75209693b712300e6f0796bd2487d0.png)](https://medium.com/@fmnobar?source=post_page-----dfc1c54d0ba7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dfc1c54d0ba7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dfc1c54d0ba7--------------------------------)
    [Farzad Mahmoodinobar](https://medium.com/@fmnobar?source=post_page-----dfc1c54d0ba7--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dfc1c54d0ba7--------------------------------)
    ·11 min read·Jun 5, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dfc1c54d0ba7--------------------------------)
    ·阅读时间11分钟·2023年6月5日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ba34fe1074371d14f74e66344336a40b.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba34fe1074371d14f74e66344336a40b.png)'
- en: Photo by [Te NGuyen](https://unsplash.com/@tenguyen?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/Wt7XT1R6sjU?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Te NGuyen](https://unsplash.com/@tenguyen?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，来自 [Unsplash](https://unsplash.com/photos/Wt7XT1R6sjU?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: '[Hyperopt](https://github.com/hyperopt/hyperopt) is an open-source hyperparameter
    optimization tool that I personally use to improve my machine learning projects
    and have found it to be quite easy to implement. Hyperparameter optimization,
    is the process of identifying the best combination of hyperparameters for a machine
    learning model to satisfy an objective function (this is usually defined as “minimizing”
    the objective function for consistency). To use a different analogy, each machine
    learning model comes with various knobs and levers that we can tune, until we
    get the outcome that we have been looking for from the model. The act of finding
    the right combination of hyperparameters that results in the outcome that we have
    been looking for is called hyperparameter optimization. Some examples of such
    parameters are: learning rate, architecture of a neural network (e.g. number of
    hidden layers), choice of the optimizer, etc.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[Hyperopt](https://github.com/hyperopt/hyperopt) 是一个开源的超参数优化工具，我个人使用它来提升我的机器学习项目，并发现它实现起来相当简单。超参数优化是识别最佳超参数组合的过程，以使机器学习模型满足目标函数（通常定义为“最小化”目标函数以保持一致）。换句话说，每个机器学习模型都有各种旋钮和杠杆，我们可以调节这些参数，直到获得我们所期望的结果。找到能得到我们所期望结果的正确超参数组合的过程称为超参数优化。一些这样的参数示例包括：学习率、神经网络的架构（如隐藏层数量）、优化器的选择等。'
- en: 'If you are interested in exploring other hyperparameter optimization strategies,
    such as grid search, random search and bayesian optimization, check out the post
    below:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对探索其他超参数优化策略感兴趣，如网格搜索、随机搜索和贝叶斯优化，请查看下面的帖子：
- en: '[](/hyperparameter-optimization-intro-and-implementation-of-grid-search-random-search-and-bayesian-b2f16c00578a?source=post_page-----dfc1c54d0ba7--------------------------------)
    [## Hyperparameter Optimization — Intro and Implementation of Grid Search, Random
    Search and Bayesian…'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/hyperparameter-optimization-intro-and-implementation-of-grid-search-random-search-and-bayesian-b2f16c00578a?source=post_page-----dfc1c54d0ba7--------------------------------)
    [## 超参数优化 — 网格搜索、随机搜索和贝叶斯优化的介绍与实现]'
- en: Most common hyperparameter optimization methodologies to boost machine learning
    outcomes.
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提升机器学习成果的最常见超参数优化方法。
- en: towardsdatascience.com](/hyperparameter-optimization-intro-and-implementation-of-grid-search-random-search-and-bayesian-b2f16c00578a?source=post_page-----dfc1c54d0ba7--------------------------------)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/hyperparameter-optimization-intro-and-implementation-of-grid-search-random-search-and-bayesian-b2f16c00578a?source=post_page-----dfc1c54d0ba7--------------------------------)
- en: Let’s get started!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: '[](https://medium.com/@fmnobar/membership?source=post_page-----dfc1c54d0ba7--------------------------------)
    [## Join Medium with my referral link'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@fmnobar/membership?source=post_page-----dfc1c54d0ba7--------------------------------)
    [## 使用我的推荐链接加入 Medium'
- en: Read every story from Farzad (and other writers on Medium). Your membership
    fee directly supports Farzad and other…
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阅读 Farzad（以及 Medium 上其他作者）的每一个故事。您的会员费用直接支持 Farzad 和其他人……
- en: medium.com](https://medium.com/@fmnobar/membership?source=post_page-----dfc1c54d0ba7--------------------------------)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@fmnobar/membership?source=post_page-----dfc1c54d0ba7--------------------------------)
- en: 1\. Basics
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 基础
- en: 1.1\. Concepts and Installation
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1\. 概念与安装
- en: Let’s first define some relevant concepts for using [Hyperopt](https://github.com/hyperopt/hyperopt).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 首先定义一些使用 [Hyperopt](https://github.com/hyperopt/hyperopt) 的相关概念。
- en: '**Objective Function:** This is the function that hyperparameter optimization
    tries to minimize. More specifically, objective function accepts a combination
    of hyperparameters as input and returns the error level of the model (a.k.a. loss),
    given those accepted hyperparameters. The goal of hyperparameter optimization
    is to find the combination of hyperparameters that minimize this error/loss.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标函数：** 这是超参数优化尝试最小化的函数。更具体地说，目标函数接受一组超参数作为输入，并返回模型的错误水平（即损失），给定这些接受的超参数。超参数优化的目标是找到使该错误/损失最小化的超参数组合。'
- en: '**Search Space:** The range of input values (i.e. parameters) that the Objective
    Function accepts as arguments.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**搜索空间：** 目标函数接受作为参数的输入值范围（即参数）。'
- en: '**Optimization Algorithm:** As the name suggests, this is the algorithm used
    to minimize the Objective Function. Hyperopt utilizes different search algorithms,
    such as random search and Tree of Parzen Estimators (TPE) ([documentation](http://hyperopt.github.io/hyperopt/#algorithms)).'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化算法：** 顾名思义，这是一种用于最小化目标函数的算法。Hyperopt 利用不同的搜索算法，例如随机搜索和 Parzen 估计树（TPE）（[文档](http://hyperopt.github.io/hyperopt/#algorithms)）。'
- en: 'Now that we are familiar with the concepts, let’s install [Hyperopt](https://github.com/hyperopt/hyperopt)
    by running the command below:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对这些概念已经很熟悉了，让我们通过运行以下命令来安装 [Hyperopt](https://github.com/hyperopt/hyperopt)：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now that we have the library installed, we will first walk through a very simple
    example to get a handle on how Hyperopt works. After that, we will move on to
    more interesting and complicated examples.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经安装了库，我们将首先通过一个非常简单的示例来了解 Hyperopt 是如何工作的。之后，我们将继续处理更有趣和复杂的示例。
- en: 1.2\. Simple Example
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2\. 简单示例
- en: 'Let’s start with a very simple one to help us understand how the overall process
    of hyperparameter optimization using Hyperopt works. We will start with a quadratic
    function of `f(x) = (x — 1)²`. This function’s optimized point is at `x = 1` and
    therefore we know what to expect. As it’s been a while since any of us took a
    calculus course, let’s look at the plot of the function, which helps us better
    understand how that point minimizes the function. Code block below will return
    the plot:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个非常简单的例子开始，以帮助我们理解使用 Hyperopt 进行超参数优化的整体过程。我们将从一个二次函数 `f(x) = (x — 1)²`
    开始。这个函数的优化点在 `x = 1`，因此我们知道期望是什么。由于我们已经有一段时间没上过微积分课了，让我们查看该函数的图，这有助于我们更好地理解这个点如何最小化该函数。以下代码块将返回该图：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Results:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：
- en: '![](../Images/bb99a1173530a776353c596c75cda85c.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bb99a1173530a776353c596c75cda85c.png)'
- en: Plot of f(x) = (x-1)²
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: f(x) = (x-1)² 的图
- en: As we can see, the minimum point happens where `x=1`. Let’s implement this using
    Hyperopt and see how it works.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，最小点发生在 `x=1`。让我们使用 Hyperopt 实现这一点，看看它是如何工作的。
- en: 'In order to do so, we will take the following steps:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们将采取以下步骤：
- en: Import necessary libraries and packages
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的库和包
- en: Define the objective function and the search space
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义目标函数和搜索空间
- en: Run the optimization process
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行优化过程
- en: Print the results (i.e. the optimized point that we expect to be `x = 1`)
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印结果（即我们期望的优化点是 `x = 1`）
- en: 'Code block below, follows the above steps:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块，按照上述步骤：
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Results:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：
- en: '![](../Images/fad78aa76c9fe042b4cfd6230556f462.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fad78aa76c9fe042b4cfd6230556f462.png)'
- en: “best” returns the best combination of hyperparameters that the model was able
    to find and in this case it is almost equal to `x = 1`, as we expected! The process
    to implement Hyperopt is generally the same and now that we have walked through
    a simple one, let’s move to a more advanced example.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: “best” 返回模型能够找到的最佳超参数组合，在这种情况下，它几乎等于 `x = 1`，正如我们所期望的那样！实现 Hyperopt 的过程通常是相同的，现在我们已经完成了一个简单的示例，让我们继续一个更高级的示例。
- en: 2\. Hyperopt Implementation
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. Hyperopt 实现
- en: 'We will implement two separate examples as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现以下两个独立的示例：
- en: A classification with Support Vector Machine
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 支持向量机的分类
- en: A regression with Random Forest Regressor
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用随机森林回归器的回归
- en: We will walk through the details of each of these two examples.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将详细讲解这两个示例中的每一个。
- en: 2.1\. Support Vector Machines and Iris Data Set
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1\. 支持向量机和鸢尾花数据集
- en: 'In a previous [post](/hyperparameter-optimization-intro-and-implementation-of-grid-search-random-search-and-bayesian-b2f16c00578a)
    I used Grid Search, Random Search and Bayesian Optimization for hyperparameter
    optimization using the [Iris data set provided by scikit-learn](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html).
    Iris data set includes 3 different irises petal and sepal lengths and is a commonly-used
    data set for classification exercises. In this post, we will use the same data
    set but we will use a Support Vector Machine (SVM) as a model with two parameters
    that we can optimize as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的 [文章](/hyperparameter-optimization-intro-and-implementation-of-grid-search-random-search-and-bayesian-b2f16c00578a)
    中，我使用了网格搜索、随机搜索和贝叶斯优化来进行超参数优化，数据集使用了 [scikit-learn 提供的鸢尾花数据集](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html)。鸢尾花数据集包括三种不同的鸢尾花的花瓣和萼片长度，是分类练习中常用的数据集。在本篇文章中，我们将使用相同的数据集，但我们将使用支持向量机（SVM）作为模型，并对以下两个参数进行优化：
- en: '`C`: Regularization parameter, which trades off misclassification of training
    examples against simplicity of the decision surface.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`C`：正则化参数，用于权衡训练样本的错误分类与决策面简洁性的 trade-off。'
- en: '`gamma`: Kernel coefficient, which defines how much influence a single training
    example has. The larger gamma is, the closer other examples must be to be affected.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gamma`：核系数，定义了单个训练样本的影响程度。gamma 值越大，其他样本必须越接近才能被影响。'
- en: Since the goal of this exercise is to go through the hyperparameter optimization,
    I will not go deeper into what SVMs do but if you are interested, I find [this](https://scikit-learn.org/stable/modules/svm.html)
    scikit-learn post helpful.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本练习的目标是进行超参数优化，我不会深入探讨 SVM 的具体操作，但如果你感兴趣，我发现 [这篇](https://scikit-learn.org/stable/modules/svm.html)
    scikit-learn 的文章很有帮助。
- en: 'We will generally follow the same steps that we used in the simple example
    earlier but will also visualize the process at the end:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将大致遵循之前简单示例中使用的相同步骤，但在最后也会可视化该过程：
- en: 1\. Import necessary libraries and packages
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 导入必要的库和包
- en: 2\. Define the objective function and the search space
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 定义目标函数和搜索空间
- en: 3\. Run the optimization process
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 运行优化过程
- en: 4\. Visualize the optimization
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 可视化优化过程
- en: 2.1.1\. Step 1 — Import Libraries and Packages
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1.1\. 步骤 1 — 导入库和包
- en: Let’s import the libraries and packages and then load the data set.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先导入库和包，然后加载数据集。
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 2.1.2\. Step 2 — Define Objective Function and Search Space
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1.2\. 步骤 2 — 定义目标函数和搜索空间
- en: Let’s first start with defining the objective function, which will train an
    SVM and returns the negative of the cross-validation score — that is what we want
    to minimize. Note that we are minimizing the negative of cross-validation score
    to be consistent with the general goal of “minimizing” the objective function
    (instead of “maximizing” the cross-validation score).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先从定义目标函数开始，目标函数将训练一个 SVM，并返回交叉验证得分的负值——这是我们想要最小化的。请注意，我们最小化交叉验证得分的负值，以便与“最小化”目标函数的一般目标保持一致（而不是“最大化”交叉验证得分）。
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Next we will define the search space, which consists of the values that our
    parameters of `C` and `gamma` can take. Note that we will use Hyperopt’s `hp.uniform(label,
    low, high)`, which returns a value uniformly between “low” and “high” ([source](http://hyperopt.github.io/hyperopt/getting-started/search_spaces/)).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将定义搜索空间，这些空间包括我们可以为 `C` 和 `gamma` 参数选择的值。请注意，我们将使用 Hyperopt 的 `hp.uniform(label,
    low, high)`，它返回一个在“low”和“high”之间均匀分布的值（[source](http://hyperopt.github.io/hyperopt/getting-started/search_spaces/)）。
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 2.1.3\. Run Optimization
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1.3\. 运行优化
- en: Same as the simple example earlier, we will use a TPE algorithm and store the
    results in a `Trials` object.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的简单示例相同，我们将使用TPE算法，并将结果存储在`Trials`对象中。
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Results:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：
- en: '![](../Images/bd5ea3f29056654e842c44cb30b39bb8.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd5ea3f29056654e842c44cb30b39bb8.png)'
- en: 2.1.4\. Visualize Optimization
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1.4\. 可视化优化
- en: As we remember from the simple example, “best” includes the selected set of
    hyperparameters that Hyperopt found based on the implemented optimization strategy.
    Let’s look at the results!
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们从简单示例中记得的那样，“最佳”包括Hyperopt根据实施的优化策略找到的超参数集合。让我们看看结果！
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/8e1681e9afd70df805341e84a54f927d.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8e1681e9afd70df805341e84a54f927d.png)'
- en: As expected, now we have a combination of hyperparameters that minimize the
    optimization function, using Hyperopt.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，现在我们有了一组超参数，这些超参数能够最小化优化函数，使用了Hyperopt。
- en: Let’s visually look at how the objective function values change as the hyperparameters
    change. We will start with defining a function named `plot_obj_vs_hp()` that accomplishes
    this visualization. And then use that function to visualize the results. Make
    sure to look for the red dot — that one indicates the best combination of hyperparameters,
    according to our hyperparameter optimization!
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直观地观察目标函数值如何随着超参数的变化而变化。我们将从定义一个名为`plot_obj_vs_hp()`的函数开始，该函数实现这个可视化。然后使用这个函数来可视化结果。一定要注意红点——它表示根据我们的超参数优化找到的最佳超参数组合！
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Results:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：
- en: '![](../Images/1d88ed76eee51130d11432d4d197d12e.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d88ed76eee51130d11432d4d197d12e.png)'
- en: Subplots of Loss vs. Hyperparameters of C and gamma
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 损失与C和gamma超参数的子图
- en: 'Note that since `C` and `gamma` are not really related to each other, we are
    showing them separately versus changes of the objective function. Since we want
    the objective function to be minimized, then we’re looking for the furthest bottom
    side of the plots above and based on the results of the hyperparameter optimization,
    we know that what we are looking for is where `{‘C’: 5.164418859504847, ‘gamma’:
    0.07084064498886927}`, which results in an objective function loss of around -0.986
    and is indicated by a red dot.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '请注意，由于`C`和`gamma`实际上彼此没有关系，我们将它们分别展示与目标函数变化的关系。由于我们希望目标函数最小化，所以我们寻找的是上述图表中最低的点，并且根据超参数优化的结果，我们知道我们要找的是`{''C'':
    5.164418859504847, ''gamma'': 0.07084064498886927}`，这会导致目标函数损失约为-0.986，并由红点标示。'
- en: I was also curious to look at these plots in a three-dimensional manner so I
    created the function below to accomplish that. Let’s look at the plot.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我也很好奇以三维方式查看这些图表，所以我创建了下面的函数来实现这一点。让我们看看图表。
- en: '[PRE10]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Results:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：
- en: '![](../Images/8fab4f3c2de7119c4952281873bbacaa.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8fab4f3c2de7119c4952281873bbacaa.png)'
- en: 3D Representation of Loss Function vs. Hyperparameters of C and gamma
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数与C和gamma超参数的三维表示
- en: Admittedly, this is not very easy to read but let’s give it a shot. We are looking
    for the lowest loss, which is the darkest dots on the plot (and the red dot is
    almost hidden by one of the dark dots). Visually it aligns with the two-dimensional
    plots that we had generated before.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 诚然，这不太容易阅读，但我们还是试试看。我们在寻找最低的损失，即图表上最暗的点（红点几乎被一个黑点隐藏）。从视觉上看，它与我们之前生成的二维图表一致。
- en: Next, let’s focus on a regression example.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们专注于一个回归示例。
- en: 2.2\. Random Forest and Diabetes Data Set
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2\. 随机森林与糖尿病数据集
- en: This example focuses on a regression model that attempts at measuring the progression
    of the disease, one year after baseline. This data set is also taken from [scikit-learn](https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset)
    but the difference is that the data set is mainly used for regression (instead
    of classification that we looked at with the Iris example).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例关注于一个回归模型，该模型试图测量基准期后一年内疾病的发展情况。这个数据集同样来自于[scikit-learn](https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset)，但不同之处在于这个数据集主要用于回归（而不是我们在鸢尾花示例中看到的分类）。
- en: If you are interested in learning more about the differences between regression
    and classification, check out the post below.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣了解回归和分类之间的区别，请查看下面的文章。
- en: '[](https://medium.com/@fmnobar/classification-vs-regression-in-machine-learning-which-one-should-i-use-f6b24d251c46?source=post_page-----dfc1c54d0ba7--------------------------------)
    [## Classification vs. Regression in Machine Learning — Which One Should I Use?'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 机器学习中的分类 vs. 回归 — 我该使用哪个？](https://medium.com/@fmnobar/classification-vs-regression-in-machine-learning-which-one-should-i-use-f6b24d251c46?source=post_page-----dfc1c54d0ba7--------------------------------)'
- en: Overview
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 概述
- en: medium.com](https://medium.com/@fmnobar/classification-vs-regression-in-machine-learning-which-one-should-i-use-f6b24d251c46?source=post_page-----dfc1c54d0ba7--------------------------------)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/@fmnobar/classification-vs-regression-in-machine-learning-which-one-should-i-use-f6b24d251c46?source=post_page-----dfc1c54d0ba7--------------------------------)'
- en: 'We will use a [Random Forest Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)
    model for this example and will optimize the objective function for two hyperparameters
    as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个 [随机森林回归器](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)
    模型进行本示例，并将优化以下两个超参数的目标函数：
- en: '`n_estimators`: Number of trees in the random forest'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_estimators`：随机森林中的树的数量'
- en: '`max_depth`: Maximum depth of trees in the random forest'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`：随机森林中树的最大深度'
- en: The overall process of optimization is the same as what we have done so far.
    So, let’s break it down into our four usual steps!
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 优化的整体过程与我们迄今为止所做的相同。所以，让我们将其分解为四个常规步骤！
- en: 2.2.1\. Step 1 — Import Libraries and Packages
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2.1\. 第 1 步 — 导入库和包
- en: We will start with importing the libraries and packages and then loading the
    data set.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从导入库和包开始，然后加载数据集。
- en: '[PRE12]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 2.2.2\. Step 2 — Define Objective Function and Search Space
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2.2\. 第 2 步 — 定义目标函数和搜索空间
- en: Similar to last time, let’s first start with defining the objective function,
    which will train our Random Forest Regressor and return the negative of the cross-validation
    score.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 与上次类似，我们首先定义目标函数，该函数将训练我们的随机森林回归器并返回交叉验证分数的负值。
- en: Next we will define the search space, which consists of the values that our
    parameters of `n_estimators` and `max_depth` can take. Note that we will use Hyperopt’s
    `hp.choice(label, options)`, which takes a value for the hyperparameter (i.e.
    `label`) and the possible values (i.e. `options`) for that hyperparameter ([source](http://hyperopt.github.io/hyperopt/getting-started/search_spaces/)).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义搜索空间，包括 `n_estimators` 和 `max_depth` 参数可以取的值。注意，我们将使用 Hyperopt 的 `hp.choice(label,
    options)`，它为超参数（即 `label`）提供一个值以及该超参数的可能值（即 `options`） ([source](http://hyperopt.github.io/hyperopt/getting-started/search_spaces/))。
- en: '[PRE13]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 2.2.3\. Run Optimization
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2.3\. 运行优化
- en: Same as the examples earlier, we will use a TPE algorithm and store the results
    in a `Trials` object.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的示例相同，我们将使用 TPE 算法，并将结果存储在 `Trials` 对象中。
- en: '[PRE14]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Results:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：
- en: '![](../Images/2c540458ef2e3d1459e353463455f8a8.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2c540458ef2e3d1459e353463455f8a8.png)'
- en: 2.2.4\. Visualize Optimization
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2.4\. 可视化优化
- en: As we remember from the simple example, “best” includes the selected set of
    hyperparameters that Hyperopt found based on the implemented optimization strategy.
    Let’s look at the results!
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们从简单示例中记得的，“最佳”包括 Hyperopt 基于实施的优化策略找到的超参数集合。让我们来看看结果！
- en: '[PRE15]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Results:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：
- en: '![](../Images/c88625fea1d6b8d0f5e1bd9c76ac864f.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c88625fea1d6b8d0f5e1bd9c76ac864f.png)'
- en: As expected, now we have a combination of hyperparameters that minimize the
    optimization function, using Hyperopt. Let’s visually look at how the objective
    function values change as the hyperparameters change, using the functions that
    we had defined before to create the plots in two and three dimensions.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，现在我们有了一组最小化优化函数的超参数组合，使用 Hyperopt。让我们直观地观察目标函数值如何随着超参数的变化而变化，利用我们之前定义的函数创建二维和三维图。
- en: '[PRE16]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Results:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：
- en: '![](../Images/5119e66d0173c56a30c3a4a776666070.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5119e66d0173c56a30c3a4a776666070.png)'
- en: Subplots of Loss vs. Hyperparameters of n_estimators and max_depth
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 损失 vs. `n_estimators` 和 `max_depth` 超参数的子图
- en: '[PRE17]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Results:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：
- en: '![](../Images/84eeaca7c41ae4b0d984d03e79352c4b.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84eeaca7c41ae4b0d984d03e79352c4b.png)'
- en: 3D Representation of Loss Function vs. Hyperparameters of n_estimators and max_depth
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 损失函数与 `n_estimators` 和 `max_depth` 超参数的表示
- en: Conclusion
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this post, we introduced [Hyperopt](https://github.com/hyperopt/hyperopt)
    — a powerful and open-source hyperparameter optimization tool and then walked
    through examples of implementation in the context of classification via a Support
    Vector Machine and regression via a Random Forest Regressor. Then we looked the
    best combination of hyperparameters found through these processes and visualized
    the results in two and three dimensions.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们介绍了 [Hyperopt](https://github.com/hyperopt/hyperopt)——一个强大且开源的超参数优化工具，然后通过支持向量机进行分类和通过随机森林回归器进行回归的示例来讲解了实现过程。接着，我们查看了通过这些过程找到的最佳超参数组合，并在二维和三维中可视化了结果。
- en: Thanks for Reading!
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: If you found this post helpful, please [follow me on Medium](https://medium.com/@fmnobar)
    and subscribe to receive my latest posts!
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你觉得这篇文章对你有帮助，请 [关注我的 Medium](https://medium.com/@fmnobar) 并订阅以接收我最新的文章！
- en: '*(All images, unless otherwise noted, are by the author.)*'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '*(所有图片，除非另有说明，均由作者提供。)*'
