- en: Modern Data Warehousing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现代数据仓库
- en: 原文：[https://towardsdatascience.com/modern-data-warehousing-2b1b0486ce4a](https://towardsdatascience.com/modern-data-warehousing-2b1b0486ce4a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/modern-data-warehousing-2b1b0486ce4a](https://towardsdatascience.com/modern-data-warehousing-2b1b0486ce4a)
- en: State-of-the-art data platform design
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 先进的数据平台设计
- en: '[](https://mshakhomirov.medium.com/?source=post_page-----2b1b0486ce4a--------------------------------)[![💡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----2b1b0486ce4a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2b1b0486ce4a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2b1b0486ce4a--------------------------------)
    [💡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----2b1b0486ce4a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mshakhomirov.medium.com/?source=post_page-----2b1b0486ce4a--------------------------------)[![💡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----2b1b0486ce4a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2b1b0486ce4a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2b1b0486ce4a--------------------------------)
    [💡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----2b1b0486ce4a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2b1b0486ce4a--------------------------------)
    ·12 min read·Dec 16, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2b1b0486ce4a--------------------------------)
    ·12分钟阅读·2023年12月16日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/61b0eba3203a5c89ddf3b0bd67553f9a.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/61b0eba3203a5c89ddf3b0bd67553f9a.png)'
- en: Photo by [Nubelson Fernandes](https://unsplash.com/@nublson?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Nubelson Fernandes](https://unsplash.com/@nublson?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: In this story, I will try to shed some light on the benefits of modern data
    warehouse solutions (DWH) compared to other data platform architecture types.
    I would dare to say that DWH is the most popular platform among data engineers
    at the moment. It offers invaluable benefits compared to other solution types
    but also has some well-known limitations. Want to learn data engineering? This
    story is a good place to start because it explains data engineering at its core
    — the DWH solution at the centre of the architecture diagram. We will see how
    data can be ingested and transformed in different DWHs available in the market.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个故事中，我将尝试阐明现代数据仓库解决方案（DWH）相对于其他数据平台架构类型的好处。我敢说，目前 DWH 是数据工程师中最受欢迎的平台。与其他解决方案类型相比，它提供了宝贵的好处，但也有一些众所周知的局限性。想学习数据工程吗？这个故事是一个很好的起点，因为它解释了数据工程的核心——架构图中心的
    DWH 解决方案。我们将看看市场上不同 DWH 的数据如何被摄取和转换。
- en: I’d like to open the discussion with experienced users too. It would be great
    to know your opinion and see what you have to say on this topic.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我也希望能与经验丰富的用户展开讨论。了解你的意见并听听你对此话题的看法将非常棒。
- en: Key characteristics of a data warehouse
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据仓库的关键特性
- en: A serverless, distributed SQL engine (BigQuery, Snowflake, Redshift, Microsoft
    Azure Synapse, Teradata.) is what we call a modern data warehouse (DWH). It is
    a SQL-first data architecture [1] where data is stored in a data warehouse, and
    we can use all the advantages of using denormalized star schema [2] datasets because
    most of the modern data warehouses are distributed and scale well, which means
    there is no need to worry about table keys and indices. It suits well for ad-hoc
    analytical queries on Big Data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 无服务器的分布式 SQL 引擎（BigQuery、Snowflake、Redshift、Microsoft Azure Synapse、Teradata）就是我们所称的现代数据仓库（DWH）。这是一种以
    SQL 为主的数据架构 [1]，数据存储在数据仓库中，我们可以利用非规范化星型模式 [2] 数据集的所有优点，因为大多数现代数据仓库都是分布式的，扩展性良好，这意味着无需担心表的键和索引。它非常适合对大数据进行即席分析查询。
- en: '[](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----2b1b0486ce4a--------------------------------)
    [## Data Platform Architecture Types'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----2b1b0486ce4a--------------------------------)
    [## 数据平台架构类型'
- en: How well does it answer your business needs? Dilemma of a choice.
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它在多大程度上满足了你的业务需求？选择的困境。
- en: towardsdatascience.com](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----2b1b0486ce4a--------------------------------)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----2b1b0486ce4a--------------------------------)
- en: Most of the modern data warehouse solutions can process structured and unstructured
    data and are very convenient for data analysts with good SQL skills.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现代数据仓库解决方案可以处理结构化和非结构化数据，并且对拥有良好SQL技能的数据分析师非常方便。
- en: '![](../Images/cc3e052dadb4fa0eb1d7d46cfd146875.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cc3e052dadb4fa0eb1d7d46cfd146875.png)'
- en: DWH data lifecycle. Image by author.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: DWH数据生命周期。图像由作者提供。
- en: Modern data warehouses integrate easily with business intelligence solutions
    like Looker, Tableau, Sisense, and Mode, which use ANSI-SQL to process data. In
    the diagram below I tried to map a common data transformation journey and tools
    used (not a complete list of course). We can see that DWH is in the middle.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现代数据仓库可以轻松与商业智能解决方案集成，如Looker、Tableau、Sisense和Mode，它们使用ANSI-SQL处理数据。在下面的图示中，我试图映射一个常见的数据转换过程及使用的工具（当然这不是完整的列表）。我们可以看到DWH在中间。
- en: '![](../Images/18cc03801d2c651426977bc6c4f2c68a.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/18cc03801d2c651426977bc6c4f2c68a.png)'
- en: Typical data journey and tools used. Image by author.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的数据旅程及使用的工具。图像由作者提供。
- en: Data warehouses are *not designed* to store unstructured data such as images,
    videos, or documents.
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 数据仓库*并不设计*用于存储非结构化数据，如图像、视频或文档。
- en: For this purpose, we would want to use data lakes.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们会希望使用数据湖。
- en: 'Data Warehouse vs Database: What’s the Difference?'
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据仓库与数据库：有什么区别？
- en: A data warehouse has a columnar data structure, the same as many RDSs it is
    relational. Data is organized into tables, rows, and columns. However, in RDS
    data is organized and stored by row, while data warehouse data is stored in columns.
    The latter provides better support for online analytical processing (OLAP) whereas
    RDS can only offer Online Transactional Processing (OLTP). RDS is definitely more
    transaction-oriented. Some of the modern data warehouse solutions can offer both
    approaches to data processing. For example, AWS Redshift supports both data warehouse
    and data lake approaches, enabling it to access and analyze large amounts of data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 数据仓库具有列式数据结构，与许多RDS相同，它是关系型的。数据被组织成表格、行和列。然而，在RDS中，数据是按行组织和存储的，而数据仓库中的数据则是按列存储的。后者更好地支持在线分析处理（OLAP），而RDS只能提供在线事务处理（OLTP）。RDS确实更加面向事务。一些现代的数据仓库解决方案可以同时提供这两种数据处理方法。例如，AWS
    Redshift支持数据仓库和数据湖方法，使其能够访问和分析大量数据。
- en: Relational Database (**RDS**) stores data in a row-based table with columns
    that connect related data elements. It is designed to record and optimize to fetch
    current data quickly. Popular relational databases are ***PostgreSQL, MySQL, Microsoft
    SQL Server, and Oracle.*** RDBMS is a relational database management system that
    helps to manage databases.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 关系型数据库（**RDS**）将数据存储在一个基于行的表格中，列连接相关的数据元素。它的设计和优化是为了快速提取当前数据。流行的关系型数据库有***PostgreSQL、MySQL、Microsoft
    SQL Server和Oracle。*** RDBMS是一个关系型数据库管理系统，帮助管理数据库。
- en: '**NoSQL** databases support **only simple transactions**, whereas Relational
    Database also supports complex transactions with joins. NoSQL Database is used
    to handle data coming at high velocity. Popular NoSQL databases are MongoDB and
    CouchDB (Document databases), Redis and DynamoDB (Key-value databases).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**NoSQL** 数据库支持**仅简单事务**，而关系型数据库还支持复杂事务及连接操作。NoSQL数据库用于处理高速到达的数据。流行的NoSQL数据库有MongoDB和CouchDB（文档数据库），Redis和DynamoDB（键值数据库）。'
- en: A Data warehouse is mainly designed for data analysis, including large amounts
    of historical data. Using a data warehouse requires users to create a pre-defined,
    fixed schema **upfront** which helps with data analytics. While dealing with data
    warehouses, tables must be simple (denormalized) in order to compute large amounts
    of data. RDS database tables and joins are complicated because they are normalized.
    So the primary difference between a traditional database and a data warehouse
    is that the traditional database is designed and optimized to record data, and
    the data warehouse is designed and optimized to respond to analytics.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 数据仓库主要用于数据分析，包括大量的历史数据。使用数据仓库需要用户**提前**创建一个预定义的、固定的模式，这有助于数据分析。在处理数据仓库时，表格必须简单（去规范化），以便计算大量数据。由于RDS数据库表和连接是规范化的，所以它们比较复杂。因此，传统数据库和数据仓库之间的主要区别在于，传统数据库的设计和优化是为了记录数据，而数据仓库的设计和优化则是为了响应分析需求。
- en: RDS stores the current data required to power an application. It is useful when
    running an App and when required to fetch some current data fast.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: RDS存储应用程序所需的当前数据。在运行应用程序时，当需要快速获取一些当前数据时，它非常有用。
- en: Data warehouse vs Data lake
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据仓库与数据湖
- en: A data lake is an ideal storage solution for storing large amounts of unstructured
    data, such as images, videos, and documents, along with structured data like JSON,
    CSV, PARQUET, and AVRO [3].
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖是存储大量非结构化数据（如图像、视频和文档）以及结构化数据（如 JSON、CSV、PARQUET 和 AVRO [3]）的理想存储解决方案。
- en: '[](/big-data-file-formats-explained-275876dc1fc9?source=post_page-----2b1b0486ce4a--------------------------------)
    [## Big Data File Formats, Explained'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 大数据文件格式解析'
- en: Parquet vs ORC vs AVRO vs JSON. Which one to choose and how to use them?
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Parquet vs ORC vs AVRO vs JSON。选择哪个以及如何使用它们？
- en: towardsdatascience.com](/big-data-file-formats-explained-275876dc1fc9?source=post_page-----2b1b0486ce4a--------------------------------)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/big-data-file-formats-explained-275876dc1fc9?source=post_page-----2b1b0486ce4a--------------------------------)'
- en: However, extracting insights from the data stored in a data lake **typically
    requires coding skills** as a data lake does not have built-in analytics or query
    capabilities like a data warehouse. Users would need to utilize programming languages
    such as Python, JAVA, Scala, or PySpark to access, process, and analyze the data
    stored in the data lake.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从数据湖中提取洞察**通常需要编码技能**，因为数据湖没有像数据仓库那样内置的分析或查询能力。用户需要利用编程语言如 Python、JAVA、Scala
    或 PySpark 来访问、处理和分析存储在数据湖中的数据。
- en: Amazing benefits emerge when data lake users have good coding skills
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当数据湖用户拥有良好的编码技能时，会出现惊人的好处
- en: In this case, data lake architecture can offer the highest level of flexibility
    in data processing. Users just need to know how to code in order to apply relevant
    data transformations.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，数据湖架构可以提供数据处理的最高灵活性。用户只需了解如何编码即可应用相关的数据转换。
- en: However, in general, this is not the case and SQL-first solutions become more
    useful.
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 然而，通常情况下，SQL优先的解决方案变得更有用。
- en: Top benefits of a data warehouse
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据仓库的主要优点
- en: As a centralized repository of data that is used for SQL querying and reporting
    DWH have many traits similar to conventional relational database solutions. Some
    of the top benefits of a data warehouse include better scalability (compared to
    RDS), better data governance (compared to data lake and data mesh architectures),
    enhanced business intelligence and improved data quality [4]
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 作为用于 SQL 查询和报告的集中数据仓库，数据仓库具有许多类似于传统关系数据库解决方案的特征。数据仓库的一些主要优点包括更好的可扩展性（相比于 RDS）、更好的数据治理（相比于数据湖和数据网格架构）、增强的商业智能和改善的数据质量
    [4]
- en: '[](/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0?source=post_page-----2b1b0486ce4a--------------------------------)
    [## Automated emails and data quality checks for your data'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 自动化邮件和数据质量检查'
- en: Data warehouse guide for better and cleaner data with scheduled emails
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据仓库指南，帮助通过定期邮件获得更好、更清洁的数据
- en: towardsdatascience.com](/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0?source=post_page-----2b1b0486ce4a--------------------------------)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0?source=post_page-----2b1b0486ce4a--------------------------------)'
- en: '**Better data governance:** Many DWH solutions in the market offer column-level
    access controls and row-level access controls. It means that we can define granular
    controls for users. For example in BigQuery, we can restrict access or mask any
    columns that are business or PII-critical [5]:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**更好的数据治理：**市场上的许多数据仓库解决方案提供了列级访问控制和行级访问控制。这意味着我们可以为用户定义细粒度的控制。例如，在 BigQuery
    中，我们可以限制访问或遮蔽任何对业务或个人敏感的列 [5]：'
- en: '![](../Images/0e68e73ee8db6b61e7d7920a691167f1.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e68e73ee8db6b61e7d7920a691167f1.png)'
- en: Data masking using policy tags. Image by author.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 使用策略标签的数据遮蔽。图片由作者提供。
- en: 'We can use Infrastructure as Code (IaC) to define these policies similar to
    what we would typically do when deploying infrastructure resources [6]. In this
    example below we can use platform-agnostic Terraform to define dataset access
    permissions:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用基础设施即代码（IaC）来定义这些策略，这类似于我们在部署基础设施资源时所做的 [6]。在下面的示例中，我们可以使用与平台无关的 Terraform
    来定义数据集访问权限：
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Better collaboration:** Modern DWH solutions offer collaboration features.
    Effective decision-making frequently requires input from several people inside
    an organisation — such as data analysts, marketing teams, management, and others
    — as well as multiple data sources. Indeed, **applying the agile approach** to
    any **data transformation development** is crucial. I would call it the best practice.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**更好的协作：** 现代数据仓库解决方案提供协作功能。有效的决策通常需要组织内部多个人员（如数据分析师、市场营销团队、管理层等）的输入，以及多个数据来源。确实，**将敏捷方法应用于任何**
    **数据转换开发** 是至关重要的。我称之为最佳实践。'
- en: Collaboration is the key
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 协作是关键
- en: DWH solution design makes it easier to deliver large data projects by breaking
    them into smaller pieces. From what I see, organisations tend to move away from
    the tedious waterfall approach in data project design and delivery. Now we have
    a single data integration layer provided in modern DWH solutions that helps to
    deploy incremental model updates more quickly and deliver business insights more
    frequently.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 数据仓库解决方案设计使得通过将大型数据项目拆分成更小的部分来交付这些项目变得更容易。从我所看到的，组织倾向于摆脱数据项目设计和交付中繁琐的瀑布式方法。现在我们拥有现代数据仓库解决方案提供的单一数据集成层，这有助于更快地部署增量模型更新，并更频繁地提供业务洞察。
- en: '**Scalability**: Data warehouses are designed to scale well in order to handle
    large volumes of data. It is crucial to scale data pipelines when necessary to
    meet the needs of growing businesses. It also has to be able to run concurrent
    workloads at scale in a single system. For instance, a very common DWH pain point
    is user-query concurrency. It happens when a DWH solution allows only a certain
    number of concurrent user queries (typically not more than 50). Many modern data
    warehouses can offer virtual clusters with distributed physical nodes to tackle
    this problem [11].'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**可扩展性**：数据仓库被设计为能够很好地扩展，以处理大量数据。在必要时扩展数据管道以满足不断增长的业务需求至关重要。它还必须能够在单一系统中以规模化方式运行并发工作负载。例如，一个非常常见的数据仓库痛点是用户查询的并发性。当数据仓库解决方案只允许一定数量的并发用户查询（通常不超过
    50）时，就会出现这种情况。许多现代数据仓库可以提供带有分布式物理节点的虚拟集群来解决这个问题[11]。'
- en: Indeed, having a distributed compute cluster with auto-scaling helps a lot in
    many scenarios.
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 确实，在许多场景中，具有自动扩展的分布式计算集群帮助很大。
- en: 'Consider this SQL mutli-cluster setup for Snowflake for example:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑以下 Snowflake 的 SQL 多集群设置：
- en: '![](../Images/2e8e059c087780bbd9a5f28743f45632.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e8e059c087780bbd9a5f28743f45632.png)'
- en: Multi-cluster data warehouse with Snowflake. Image by author.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Snowflake 的多集群数据仓库。图像由作者提供。
- en: '**Improved data loading:** DWH solutions vary. For some of them, data ingestion
    is a trivial task (Snowflake) while others offer greater flexibility while working
    with partitions (BigQuery). Consider this example of data loading in the Snowflake
    data warehouse below. We assume all our data files are being created in an AWS
    S3 storage bucket and look like this:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**改进的数据加载：** 数据仓库解决方案各不相同。对于其中一些，数据摄取是一个微不足道的任务（如 Snowflake），而其他的则在处理分区时提供了更大的灵活性（如
    BigQuery）。考虑下面的 Snowflake 数据仓库中的数据加载示例。我们假设所有数据文件都存储在 AWS S3 存储桶中，如下所示：'
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now we can load it using SQL!
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用 SQL 来加载它！
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This is what makes Snowflake so popular among data analysts and no-code users.
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这就是为什么 Snowflake 在数据分析师和无代码用户中如此受欢迎的原因。
- en: We can see that DWH data ingestion routines handled everything including data
    format by stripping the outer array, i.e. [{},{},{}] -> {},{},{}.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，数据仓库的数据摄取例程处理了所有内容，包括通过去除外部数组来处理数据格式，即 [{},{},{}] -> {},{},{}。
- en: 'For instance, in BigQuery we would want to create a data loader application
    that would do the same. Consider this Python code below [14]:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在 BigQuery 中，我们可能会想创建一个数据加载应用程序来做同样的事情。考虑下面的 Python 代码[14]：
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[](/python-for-data-engineers-f3d5db59b6dd?source=post_page-----2b1b0486ce4a--------------------------------)
    [## Python for Data Engineers'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/python-for-data-engineers-f3d5db59b6dd?source=post_page-----2b1b0486ce4a--------------------------------)
    [## Python for Data Engineers'
- en: Advanced ETL techniques for beginners
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 面向初学者的高级 ETL 技术
- en: towardsdatascience.com](/python-for-data-engineers-f3d5db59b6dd?source=post_page-----2b1b0486ce4a--------------------------------)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/python-for-data-engineers-f3d5db59b6dd?source=post_page-----2b1b0486ce4a--------------------------------)
- en: BigQuery looks a bit more demanding in terms of the user’s programming skills.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在用户的编程技能方面，BigQuery 看起来稍显要求较高。
- en: It is valid to assume that knowing how things work under the hood tends to be
    more cost-effective in the long run.
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 可以合理地认为，了解底层工作原理往往在长期内更具成本效益。
- en: 'Indeed, BigQuery offers more fine controls over partitioning which might lead
    to greater savings while working with data in the DWH. Consider this data loading
    example where we can define partitioning (DAY, MONTH, RANGE):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This becomes very useful when we have a data model with date/month dimensions
    and usually saves a lot of money.
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This was a bulk data loading example. **Alternatively**, we can create a data
    pipeline where data is being ingested continuously. It’s called data streaming
    [12].
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[](/streaming-in-data-engineering-2bb2b9b3b603?source=post_page-----2b1b0486ce4a--------------------------------)
    [## Streaming in Data Engineering'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Streaming data pipelines and real-time analytics
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/streaming-in-data-engineering-2bb2b9b3b603?source=post_page-----2b1b0486ce4a--------------------------------)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Snowflake has a handy Kafka connector [13] that simplifies streaming data pipelines
    and connects to the Kafka server to pull data continuously from topics.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '**Improved Business Intelligence**: Many firms collect huge volumes of data
    from a variety of sources (for example, weather, revenue, payments, customer information,
    trends, vendor information, and so on). The sheer volume of data might be useless.
    Storing this data across numerous platforms might be expensive too. So DWH as
    a single source of truth seems like a problem solver for BI pipelines where everyone
    can generate data insights with ease. Consider this Google Looker Studio integration
    below. It is available for the modern DWH solutions and all of them are pretty
    much market leaders.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/550d5f58c60aba1d313b40f1952cd9db.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
- en: Snowflake Data Connector. Image by author.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e9e59e9fe9ed618e339bc331e23b826c.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
- en: AWS Redshift Connector for Looker Studio. Image by author.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: And so on… A free community Business Intelligence (BI) tool has connectors to
    all major DWH solutions available in the market, i.e. Redshift, Snowflake, BigQuery,
    Databricks [7], Galaxy [8], etc.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/679dc8acbc096112a6f794c1fe61bbcd.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
- en: Looker Studio Data connectors. Image by author.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Data warehouses provide a platform for business intelligence tools and applications
    to access and analyze data. This enables businesses to make informed decisions
    based on data-driven insights.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '**Better product integration and DevOps lifecycle**: Some products go even
    further in terms of how data pipelines can be designed and deployed. For BI developers
    and data engineers, it is very important to have everything in Git. Having this
    enabled is crucial as it helps with continuous integration [9]. I previously wrote
    about how data pipeline resources can be deployed using IaC tools such as AWS
    CloudFormation and Terraform:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1?source=post_page-----2b1b0486ce4a--------------------------------)
    [## Continuous Integration and Deployment for Data Platforms'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: CI/CD for data engineers and ML Ops
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1?source=post_page-----2b1b0486ce4a--------------------------------)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1?source=post_page-----2b1b0486ce4a--------------------------------)
- en: Just imagine that we can deploy reports using CI/CD tools. Yes, that’s right.
    Not just data pipeline resources but also BI dashboards.
  id: totrans-90
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 想象一下我们可以使用CI/CD工具部署报告。是的，没错。不仅仅是数据管道资源，还有BI仪表板。
- en: 'Consider this AWS CloudFormation template below. It deploys AWS Quicksight
    datasets and report analysis:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅下面的AWS CloudFormation模板。它部署AWS Quicksight数据集和报告分析：
- en: '[PRE5]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Previously I wrote a tutorial [10] on how to deploy a streaming data pipeline
    using AWS CloudFormation. Adding a BI bit to this even makes it better in my opinion.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前写了一篇教程[10]，讲述了如何使用AWS CloudFormation部署流数据管道。在我看来，将BI功能添加到其中会使其更好。
- en: '[](/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2?source=post_page-----2b1b0486ce4a--------------------------------)
    [## Building a Streaming Data Pipeline with Redshift Serverless and Kinesis'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2?source=post_page-----2b1b0486ce4a--------------------------------)
    [## 使用Redshift Serverless和Kinesis构建流数据管道'
- en: An End-To-End Tutorial for Beginners
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 初学者的端到端教程
- en: towardsdatascience.com](/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2?source=post_page-----2b1b0486ce4a--------------------------------)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2?source=post_page-----2b1b0486ce4a--------------------------------)
- en: Conclusion
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: Overall, a data warehouse is one of the most popular data platforms and can
    help companies (especially on an enterprise level) gain a competitive advantage
    by providing a single source of truth for data-driven decision-making. Modern
    DWH solutions help to deliver data insights more quickly. In a rapidly changing
    business environment, companies can activate it with automation techniques to
    generate greater value for business stakeholders. Each solution offers the features
    that make it unique. However, there are a few things to consider almost in every
    case. Cost-effectiveness, data partitioning, query-user concurrency, data lake
    storage and associated costs — all these pain points are valid. Sometimes it might
    be useful to unload historical data back to a cloud storage archive to optimise
    costs [15]. In some scenarios using Apache Iceberg tables might help with user
    query concurrency issues [16].
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，数据仓库是最受欢迎的数据平台之一，可以帮助公司（特别是在企业级别）通过提供单一的真实数据源来获得竞争优势，从而进行数据驱动的决策。现代数据仓库解决方案有助于更快地提供数据洞察。在快速变化的商业环境中，公司可以通过自动化技术激活这些解决方案，为业务利益相关者创造更大的价值。每种解决方案都提供了使其独特的功能。然而，几乎每种情况下都需要考虑一些因素。成本效益、数据分区、查询用户并发、数据湖存储及相关成本——这些痛点都是有效的。有时，将历史数据卸载到云存储档案中以优化成本可能是有用的[15]。在某些情况下，使用Apache
    Iceberg表可能有助于解决用户查询并发问题[16]。
- en: '[](/introduction-to-apache-iceberg-tables-a791f1758009?source=post_page-----2b1b0486ce4a--------------------------------)
    [## Introduction to Apache Iceberg Tables'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/introduction-to-apache-iceberg-tables-a791f1758009?source=post_page-----2b1b0486ce4a--------------------------------)
    [## Apache Iceberg 表介绍'
- en: A few Compelling Reasons to Choose Apache Iceberg for Data Lakes
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择Apache Iceberg作为数据湖的几个令人信服的理由
- en: towardsdatascience.com](/introduction-to-apache-iceberg-tables-a791f1758009?source=post_page-----2b1b0486ce4a--------------------------------)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/introduction-to-apache-iceberg-tables-a791f1758009?source=post_page-----2b1b0486ce4a--------------------------------)
- en: There are a few things to consider while designing and building the perfect
    data platform. Some users might argue the best data warehouse is a lakehouse (Databricks)
    but ultimately it depends on how data is being stored and your business requirements
    for historical records. In many scenarios, a DWH solution acting as a single source
    of truth for all stakeholders might become an optimal choice. Combine it with
    the right data modelling tool and you will get the right tool for external data-to-BI
    data pipelines.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计和构建完美的数据平台时，有几个事项需要考虑。一些用户可能认为最佳的数据仓库是湖仓（Databricks），但最终这取决于数据的存储方式以及你对历史记录的业务需求。在许多情况下，作为所有利益相关者的单一真实数据源的数据仓库解决方案可能成为最佳选择。结合合适的数据建模工具，你将获得适用于外部数据到BI数据管道的正确工具。
- en: I hope you find these ideas useful. They are based on my personal experience
    and observations.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你觉得这些想法有用。它们基于我个人的经验和观察。
- en: Recommended read
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推荐阅读
- en: '[1] [https://medium.com/towards-data-science/data-platform-architecture-types-f255ac6e0b7](https://medium.com/towards-data-science/data-platform-architecture-types-f255ac6e0b7)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [https://medium.com/towards-data-science/data-platform-architecture-types-f255ac6e0b7](https://medium.com/towards-data-science/data-platform-architecture-types-f255ac6e0b7)'
- en: '[2] [https://towardsdatascience.com/data-modelling-for-data-engineers-93d058efa302](/data-modelling-for-data-engineers-93d058efa302)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [https://towardsdatascience.com/data-modelling-for-data-engineers-93d058efa302](/data-modelling-for-data-engineers-93d058efa302)'
- en: '[3] [https://medium.com/towards-data-science/big-data-file-formats-explained-275876dc1fc9](https://medium.com/towards-data-science/big-data-file-formats-explained-275876dc1fc9)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [https://medium.com/towards-data-science/big-data-file-formats-explained-275876dc1fc9](https://medium.com/towards-data-science/big-data-file-formats-explained-275876dc1fc9)'
- en: '[4] [https://towardsdatascience.com/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0](/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [https://towardsdatascience.com/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0](/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0)'
- en: '[5] [https://cloud.google.com/bigquery/docs/column-level-security-intro](https://cloud.google.com/bigquery/docs/column-level-security-intro)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] [https://cloud.google.com/bigquery/docs/column-level-security-intro](https://cloud.google.com/bigquery/docs/column-level-security-intro)'
- en: '[6] [https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/bigquery_dataset_access](https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/bigquery_dataset_access)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] [https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/bigquery_dataset_access](https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/bigquery_dataset_access)'
- en: '[7] [https://docs.databricks.com/en/partners/bi/looker.html](https://docs.databricks.com/en/partners/bi/looker.html)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] [https://docs.databricks.com/en/partners/bi/looker.html](https://docs.databricks.com/en/partners/bi/looker.html)'
- en: '[8] [https://docs.starburst.io/clients/looker.html](https://docs.starburst.io/clients/looker.html)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] [https://docs.starburst.io/clients/looker.html](https://docs.starburst.io/clients/looker.html)'
- en: '[9] [https://towardsdatascience.com/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] [https://towardsdatascience.com/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1)'
- en: '[10] [https://medium.com/towards-data-science/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2](https://medium.com/towards-data-science/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] [https://medium.com/towards-data-science/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2](https://medium.com/towards-data-science/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2)'
- en: '[11] [https://www.snowflake.com/blog/auto-scale-snowflake-major-leap-forward-massively-concurrent-enterprise-applications/](https://www.snowflake.com/blog/auto-scale-snowflake-major-leap-forward-massively-concurrent-enterprise-applications/)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] [https://www.snowflake.com/blog/auto-scale-snowflake-major-leap-forward-massively-concurrent-enterprise-applications/](https://www.snowflake.com/blog/auto-scale-snowflake-major-leap-forward-massively-concurrent-enterprise-applications/)'
- en: '[12] [https://medium.com/towards-data-science/streaming-in-data-engineering-2bb2b9b3b603](https://medium.com/towards-data-science/streaming-in-data-engineering-2bb2b9b3b603)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] [https://medium.com/towards-data-science/streaming-in-data-engineering-2bb2b9b3b603](https://medium.com/towards-data-science/streaming-in-data-engineering-2bb2b9b3b603)'
- en: '[13] [https://docs.snowflake.com/en/user-guide/kafka-connector](https://docs.snowflake.com/en/user-guide/kafka-connector)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] [https://docs.snowflake.com/en/user-guide/kafka-connector](https://docs.snowflake.com/en/user-guide/kafka-connector)'
- en: '[14] [https://towardsdatascience.com/python-for-data-engineers-f3d5db59b6dd](/python-for-data-engineers-f3d5db59b6dd)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] [https://towardsdatascience.com/python-for-data-engineers-f3d5db59b6dd](/python-for-data-engineers-f3d5db59b6dd)'
- en: '[15] [https://medium.com/towards-artificial-intelligence/supercharge-your-data-engineering-skills-with-this-machine-learning-pipeline-b69d159780b7](https://medium.com/towards-artificial-intelligence/supercharge-your-data-engineering-skills-with-this-machine-learning-pipeline-b69d159780b7)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] [https://medium.com/towards-artificial-intelligence/supercharge-your-data-engineering-skills-with-this-machine-learning-pipeline-b69d159780b7](https://medium.com/towards-artificial-intelligence/supercharge-your-data-engineering-skills-with-this-machine-learning-pipeline-b69d159780b7)'
- en: '[16] [https://medium.com/towards-data-science/introduction-to-apache-iceberg-tables-a791f1758009](https://medium.com/towards-data-science/introduction-to-apache-iceberg-tables-a791f1758009)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] [https://medium.com/towards-data-science/introduction-to-apache-iceberg-tables-a791f1758009](https://medium.com/towards-data-science/introduction-to-apache-iceberg-tables-a791f1758009)'
