- en: Modern Data Warehousing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç°ä»£æ•°æ®ä»“åº“
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/modern-data-warehousing-2b1b0486ce4a](https://towardsdatascience.com/modern-data-warehousing-2b1b0486ce4a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/modern-data-warehousing-2b1b0486ce4a](https://towardsdatascience.com/modern-data-warehousing-2b1b0486ce4a)
- en: State-of-the-art data platform design
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å…ˆè¿›çš„æ•°æ®å¹³å°è®¾è®¡
- en: '[](https://mshakhomirov.medium.com/?source=post_page-----2b1b0486ce4a--------------------------------)[![ğŸ’¡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----2b1b0486ce4a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2b1b0486ce4a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2b1b0486ce4a--------------------------------)
    [ğŸ’¡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----2b1b0486ce4a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mshakhomirov.medium.com/?source=post_page-----2b1b0486ce4a--------------------------------)[![ğŸ’¡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----2b1b0486ce4a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2b1b0486ce4a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2b1b0486ce4a--------------------------------)
    [ğŸ’¡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----2b1b0486ce4a--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2b1b0486ce4a--------------------------------)
    Â·12 min readÂ·Dec 16, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2b1b0486ce4a--------------------------------)
    Â·12åˆ†é’Ÿé˜…è¯»Â·2023å¹´12æœˆ16æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/61b0eba3203a5c89ddf3b0bd67553f9a.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/61b0eba3203a5c89ddf3b0bd67553f9a.png)'
- en: Photo by [Nubelson Fernandes](https://unsplash.com/@nublson?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”± [Nubelson Fernandes](https://unsplash.com/@nublson?utm_source=medium&utm_medium=referral)
    æä¾›ï¼Œæ¥æºäº [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: In this story, I will try to shed some light on the benefits of modern data
    warehouse solutions (DWH) compared to other data platform architecture types.
    I would dare to say that DWH is the most popular platform among data engineers
    at the moment. It offers invaluable benefits compared to other solution types
    but also has some well-known limitations. Want to learn data engineering? This
    story is a good place to start because it explains data engineering at its core
    â€” the DWH solution at the centre of the architecture diagram. We will see how
    data can be ingested and transformed in different DWHs available in the market.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªæ•…äº‹ä¸­ï¼Œæˆ‘å°†å°è¯•é˜æ˜ç°ä»£æ•°æ®ä»“åº“è§£å†³æ–¹æ¡ˆï¼ˆDWHï¼‰ç›¸å¯¹äºå…¶ä»–æ•°æ®å¹³å°æ¶æ„ç±»å‹çš„å¥½å¤„ã€‚æˆ‘æ•¢è¯´ï¼Œç›®å‰ DWH æ˜¯æ•°æ®å·¥ç¨‹å¸ˆä¸­æœ€å—æ¬¢è¿çš„å¹³å°ã€‚ä¸å…¶ä»–è§£å†³æ–¹æ¡ˆç±»å‹ç›¸æ¯”ï¼Œå®ƒæä¾›äº†å®è´µçš„å¥½å¤„ï¼Œä½†ä¹Ÿæœ‰ä¸€äº›ä¼—æ‰€å‘¨çŸ¥çš„å±€é™æ€§ã€‚æƒ³å­¦ä¹ æ•°æ®å·¥ç¨‹å—ï¼Ÿè¿™ä¸ªæ•…äº‹æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ï¼Œå› ä¸ºå®ƒè§£é‡Šäº†æ•°æ®å·¥ç¨‹çš„æ ¸å¿ƒâ€”â€”æ¶æ„å›¾ä¸­å¿ƒçš„
    DWH è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬å°†çœ‹çœ‹å¸‚åœºä¸Šä¸åŒ DWH çš„æ•°æ®å¦‚ä½•è¢«æ‘„å–å’Œè½¬æ¢ã€‚
- en: Iâ€™d like to open the discussion with experienced users too. It would be great
    to know your opinion and see what you have to say on this topic.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¹Ÿå¸Œæœ›èƒ½ä¸ç»éªŒä¸°å¯Œçš„ç”¨æˆ·å±•å¼€è®¨è®ºã€‚äº†è§£ä½ çš„æ„è§å¹¶å¬å¬ä½ å¯¹æ­¤è¯é¢˜çš„çœ‹æ³•å°†éå¸¸æ£’ã€‚
- en: Key characteristics of a data warehouse
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ•°æ®ä»“åº“çš„å…³é”®ç‰¹æ€§
- en: A serverless, distributed SQL engine (BigQuery, Snowflake, Redshift, Microsoft
    Azure Synapse, Teradata.) is what we call a modern data warehouse (DWH). It is
    a SQL-first data architecture [1] where data is stored in a data warehouse, and
    we can use all the advantages of using denormalized star schema [2] datasets because
    most of the modern data warehouses are distributed and scale well, which means
    there is no need to worry about table keys and indices. It suits well for ad-hoc
    analytical queries on Big Data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æ— æœåŠ¡å™¨çš„åˆ†å¸ƒå¼ SQL å¼•æ“ï¼ˆBigQueryã€Snowflakeã€Redshiftã€Microsoft Azure Synapseã€Teradataï¼‰å°±æ˜¯æˆ‘ä»¬æ‰€ç§°çš„ç°ä»£æ•°æ®ä»“åº“ï¼ˆDWHï¼‰ã€‚è¿™æ˜¯ä¸€ç§ä»¥
    SQL ä¸ºä¸»çš„æ•°æ®æ¶æ„ [1]ï¼Œæ•°æ®å­˜å‚¨åœ¨æ•°æ®ä»“åº“ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨éè§„èŒƒåŒ–æ˜Ÿå‹æ¨¡å¼ [2] æ•°æ®é›†çš„æ‰€æœ‰ä¼˜ç‚¹ï¼Œå› ä¸ºå¤§å¤šæ•°ç°ä»£æ•°æ®ä»“åº“éƒ½æ˜¯åˆ†å¸ƒå¼çš„ï¼Œæ‰©å±•æ€§è‰¯å¥½ï¼Œè¿™æ„å‘³ç€æ— éœ€æ‹…å¿ƒè¡¨çš„é”®å’Œç´¢å¼•ã€‚å®ƒéå¸¸é€‚åˆå¯¹å¤§æ•°æ®è¿›è¡Œå³å¸­åˆ†ææŸ¥è¯¢ã€‚
- en: '[](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----2b1b0486ce4a--------------------------------)
    [## Data Platform Architecture Types'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----2b1b0486ce4a--------------------------------)
    [## æ•°æ®å¹³å°æ¶æ„ç±»å‹'
- en: How well does it answer your business needs? Dilemma of a choice.
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å®ƒåœ¨å¤šå¤§ç¨‹åº¦ä¸Šæ»¡è¶³äº†ä½ çš„ä¸šåŠ¡éœ€æ±‚ï¼Ÿé€‰æ‹©çš„å›°å¢ƒã€‚
- en: towardsdatascience.com](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----2b1b0486ce4a--------------------------------)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----2b1b0486ce4a--------------------------------)
- en: Most of the modern data warehouse solutions can process structured and unstructured
    data and are very convenient for data analysts with good SQL skills.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å¤šæ•°ç°ä»£æ•°æ®ä»“åº“è§£å†³æ–¹æ¡ˆå¯ä»¥å¤„ç†ç»“æ„åŒ–å’Œéç»“æ„åŒ–æ•°æ®ï¼Œå¹¶ä¸”å¯¹æ‹¥æœ‰è‰¯å¥½SQLæŠ€èƒ½çš„æ•°æ®åˆ†æå¸ˆéå¸¸æ–¹ä¾¿ã€‚
- en: '![](../Images/cc3e052dadb4fa0eb1d7d46cfd146875.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cc3e052dadb4fa0eb1d7d46cfd146875.png)'
- en: DWH data lifecycle. Image by author.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: DWHæ•°æ®ç”Ÿå‘½å‘¨æœŸã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚
- en: Modern data warehouses integrate easily with business intelligence solutions
    like Looker, Tableau, Sisense, and Mode, which use ANSI-SQL to process data. In
    the diagram below I tried to map a common data transformation journey and tools
    used (not a complete list of course). We can see that DWH is in the middle.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ç°ä»£æ•°æ®ä»“åº“å¯ä»¥è½»æ¾ä¸å•†ä¸šæ™ºèƒ½è§£å†³æ–¹æ¡ˆé›†æˆï¼Œå¦‚Lookerã€Tableauã€Sisenseå’ŒModeï¼Œå®ƒä»¬ä½¿ç”¨ANSI-SQLå¤„ç†æ•°æ®ã€‚åœ¨ä¸‹é¢çš„å›¾ç¤ºä¸­ï¼Œæˆ‘è¯•å›¾æ˜ å°„ä¸€ä¸ªå¸¸è§çš„æ•°æ®è½¬æ¢è¿‡ç¨‹åŠä½¿ç”¨çš„å·¥å…·ï¼ˆå½“ç„¶è¿™ä¸æ˜¯å®Œæ•´çš„åˆ—è¡¨ï¼‰ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°DWHåœ¨ä¸­é—´ã€‚
- en: '![](../Images/18cc03801d2c651426977bc6c4f2c68a.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/18cc03801d2c651426977bc6c4f2c68a.png)'
- en: Typical data journey and tools used. Image by author.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å…¸å‹çš„æ•°æ®æ—…ç¨‹åŠä½¿ç”¨çš„å·¥å…·ã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚
- en: Data warehouses are *not designed* to store unstructured data such as images,
    videos, or documents.
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ•°æ®ä»“åº“*å¹¶ä¸è®¾è®¡*ç”¨äºå­˜å‚¨éç»“æ„åŒ–æ•°æ®ï¼Œå¦‚å›¾åƒã€è§†é¢‘æˆ–æ–‡æ¡£ã€‚
- en: For this purpose, we would want to use data lakes.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä¼šå¸Œæœ›ä½¿ç”¨æ•°æ®æ¹–ã€‚
- en: 'Data Warehouse vs Database: Whatâ€™s the Difference?'
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ•°æ®ä»“åº“ä¸æ•°æ®åº“ï¼šæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
- en: A data warehouse has a columnar data structure, the same as many RDSs it is
    relational. Data is organized into tables, rows, and columns. However, in RDS
    data is organized and stored by row, while data warehouse data is stored in columns.
    The latter provides better support for online analytical processing (OLAP) whereas
    RDS can only offer Online Transactional Processing (OLTP). RDS is definitely more
    transaction-oriented. Some of the modern data warehouse solutions can offer both
    approaches to data processing. For example, AWS Redshift supports both data warehouse
    and data lake approaches, enabling it to access and analyze large amounts of data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®ä»“åº“å…·æœ‰åˆ—å¼æ•°æ®ç»“æ„ï¼Œä¸è®¸å¤šRDSç›¸åŒï¼Œå®ƒæ˜¯å…³ç³»å‹çš„ã€‚æ•°æ®è¢«ç»„ç»‡æˆè¡¨æ ¼ã€è¡Œå’Œåˆ—ã€‚ç„¶è€Œï¼Œåœ¨RDSä¸­ï¼Œæ•°æ®æ˜¯æŒ‰è¡Œç»„ç»‡å’Œå­˜å‚¨çš„ï¼Œè€Œæ•°æ®ä»“åº“ä¸­çš„æ•°æ®åˆ™æ˜¯æŒ‰åˆ—å­˜å‚¨çš„ã€‚åè€…æ›´å¥½åœ°æ”¯æŒåœ¨çº¿åˆ†æå¤„ç†ï¼ˆOLAPï¼‰ï¼Œè€ŒRDSåªèƒ½æä¾›åœ¨çº¿äº‹åŠ¡å¤„ç†ï¼ˆOLTPï¼‰ã€‚RDSç¡®å®æ›´åŠ é¢å‘äº‹åŠ¡ã€‚ä¸€äº›ç°ä»£çš„æ•°æ®ä»“åº“è§£å†³æ–¹æ¡ˆå¯ä»¥åŒæ—¶æä¾›è¿™ä¸¤ç§æ•°æ®å¤„ç†æ–¹æ³•ã€‚ä¾‹å¦‚ï¼ŒAWS
    Redshiftæ”¯æŒæ•°æ®ä»“åº“å’Œæ•°æ®æ¹–æ–¹æ³•ï¼Œä½¿å…¶èƒ½å¤Ÿè®¿é—®å’Œåˆ†æå¤§é‡æ•°æ®ã€‚
- en: Relational Database (**RDS**) stores data in a row-based table with columns
    that connect related data elements. It is designed to record and optimize to fetch
    current data quickly. Popular relational databases are ***PostgreSQL, MySQL, Microsoft
    SQL Server, and Oracle.*** RDBMS is a relational database management system that
    helps to manage databases.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å…³ç³»å‹æ•°æ®åº“ï¼ˆ**RDS**ï¼‰å°†æ•°æ®å­˜å‚¨åœ¨ä¸€ä¸ªåŸºäºè¡Œçš„è¡¨æ ¼ä¸­ï¼Œåˆ—è¿æ¥ç›¸å…³çš„æ•°æ®å…ƒç´ ã€‚å®ƒçš„è®¾è®¡å’Œä¼˜åŒ–æ˜¯ä¸ºäº†å¿«é€Ÿæå–å½“å‰æ•°æ®ã€‚æµè¡Œçš„å…³ç³»å‹æ•°æ®åº“æœ‰***PostgreSQLã€MySQLã€Microsoft
    SQL Serverå’ŒOracleã€‚*** RDBMSæ˜¯ä¸€ä¸ªå…³ç³»å‹æ•°æ®åº“ç®¡ç†ç³»ç»Ÿï¼Œå¸®åŠ©ç®¡ç†æ•°æ®åº“ã€‚
- en: '**NoSQL** databases support **only simple transactions**, whereas Relational
    Database also supports complex transactions with joins. NoSQL Database is used
    to handle data coming at high velocity. Popular NoSQL databases are MongoDB and
    CouchDB (Document databases), Redis and DynamoDB (Key-value databases).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**NoSQL** æ•°æ®åº“æ”¯æŒ**ä»…ç®€å•äº‹åŠ¡**ï¼Œè€Œå…³ç³»å‹æ•°æ®åº“è¿˜æ”¯æŒå¤æ‚äº‹åŠ¡åŠè¿æ¥æ“ä½œã€‚NoSQLæ•°æ®åº“ç”¨äºå¤„ç†é«˜é€Ÿåˆ°è¾¾çš„æ•°æ®ã€‚æµè¡Œçš„NoSQLæ•°æ®åº“æœ‰MongoDBå’ŒCouchDBï¼ˆæ–‡æ¡£æ•°æ®åº“ï¼‰ï¼ŒRediså’ŒDynamoDBï¼ˆé”®å€¼æ•°æ®åº“ï¼‰ã€‚'
- en: A Data warehouse is mainly designed for data analysis, including large amounts
    of historical data. Using a data warehouse requires users to create a pre-defined,
    fixed schema **upfront** which helps with data analytics. While dealing with data
    warehouses, tables must be simple (denormalized) in order to compute large amounts
    of data. RDS database tables and joins are complicated because they are normalized.
    So the primary difference between a traditional database and a data warehouse
    is that the traditional database is designed and optimized to record data, and
    the data warehouse is designed and optimized to respond to analytics.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®ä»“åº“ä¸»è¦ç”¨äºæ•°æ®åˆ†æï¼ŒåŒ…æ‹¬å¤§é‡çš„å†å²æ•°æ®ã€‚ä½¿ç”¨æ•°æ®ä»“åº“éœ€è¦ç”¨æˆ·**æå‰**åˆ›å»ºä¸€ä¸ªé¢„å®šä¹‰çš„ã€å›ºå®šçš„æ¨¡å¼ï¼Œè¿™æœ‰åŠ©äºæ•°æ®åˆ†æã€‚åœ¨å¤„ç†æ•°æ®ä»“åº“æ—¶ï¼Œè¡¨æ ¼å¿…é¡»ç®€å•ï¼ˆå»è§„èŒƒåŒ–ï¼‰ï¼Œä»¥ä¾¿è®¡ç®—å¤§é‡æ•°æ®ã€‚ç”±äºRDSæ•°æ®åº“è¡¨å’Œè¿æ¥æ˜¯è§„èŒƒåŒ–çš„ï¼Œæ‰€ä»¥å®ƒä»¬æ¯”è¾ƒå¤æ‚ã€‚å› æ­¤ï¼Œä¼ ç»Ÿæ•°æ®åº“å’Œæ•°æ®ä»“åº“ä¹‹é—´çš„ä¸»è¦åŒºåˆ«åœ¨äºï¼Œä¼ ç»Ÿæ•°æ®åº“çš„è®¾è®¡å’Œä¼˜åŒ–æ˜¯ä¸ºäº†è®°å½•æ•°æ®ï¼Œè€Œæ•°æ®ä»“åº“çš„è®¾è®¡å’Œä¼˜åŒ–åˆ™æ˜¯ä¸ºäº†å“åº”åˆ†æéœ€æ±‚ã€‚
- en: RDS stores the current data required to power an application. It is useful when
    running an App and when required to fetch some current data fast.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: RDSå­˜å‚¨åº”ç”¨ç¨‹åºæ‰€éœ€çš„å½“å‰æ•°æ®ã€‚åœ¨è¿è¡Œåº”ç”¨ç¨‹åºæ—¶ï¼Œå½“éœ€è¦å¿«é€Ÿè·å–ä¸€äº›å½“å‰æ•°æ®æ—¶ï¼Œå®ƒéå¸¸æœ‰ç”¨ã€‚
- en: Data warehouse vs Data lake
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ•°æ®ä»“åº“ä¸æ•°æ®æ¹–
- en: A data lake is an ideal storage solution for storing large amounts of unstructured
    data, such as images, videos, and documents, along with structured data like JSON,
    CSV, PARQUET, and AVRO [3].
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®æ¹–æ˜¯å­˜å‚¨å¤§é‡éç»“æ„åŒ–æ•°æ®ï¼ˆå¦‚å›¾åƒã€è§†é¢‘å’Œæ–‡æ¡£ï¼‰ä»¥åŠç»“æ„åŒ–æ•°æ®ï¼ˆå¦‚ JSONã€CSVã€PARQUET å’Œ AVRO [3]ï¼‰çš„ç†æƒ³å­˜å‚¨è§£å†³æ–¹æ¡ˆã€‚
- en: '[](/big-data-file-formats-explained-275876dc1fc9?source=post_page-----2b1b0486ce4a--------------------------------)
    [## Big Data File Formats, Explained'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[## å¤§æ•°æ®æ–‡ä»¶æ ¼å¼è§£æ'
- en: Parquet vs ORC vs AVRO vs JSON. Which one to choose and how to use them?
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Parquet vs ORC vs AVRO vs JSONã€‚é€‰æ‹©å“ªä¸ªä»¥åŠå¦‚ä½•ä½¿ç”¨å®ƒä»¬ï¼Ÿ
- en: towardsdatascience.com](/big-data-file-formats-explained-275876dc1fc9?source=post_page-----2b1b0486ce4a--------------------------------)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/big-data-file-formats-explained-275876dc1fc9?source=post_page-----2b1b0486ce4a--------------------------------)'
- en: However, extracting insights from the data stored in a data lake **typically
    requires coding skills** as a data lake does not have built-in analytics or query
    capabilities like a data warehouse. Users would need to utilize programming languages
    such as Python, JAVA, Scala, or PySpark to access, process, and analyze the data
    stored in the data lake.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œä»æ•°æ®æ¹–ä¸­æå–æ´å¯Ÿ**é€šå¸¸éœ€è¦ç¼–ç æŠ€èƒ½**ï¼Œå› ä¸ºæ•°æ®æ¹–æ²¡æœ‰åƒæ•°æ®ä»“åº“é‚£æ ·å†…ç½®çš„åˆ†ææˆ–æŸ¥è¯¢èƒ½åŠ›ã€‚ç”¨æˆ·éœ€è¦åˆ©ç”¨ç¼–ç¨‹è¯­è¨€å¦‚ Pythonã€JAVAã€Scala
    æˆ– PySpark æ¥è®¿é—®ã€å¤„ç†å’Œåˆ†æå­˜å‚¨åœ¨æ•°æ®æ¹–ä¸­çš„æ•°æ®ã€‚
- en: Amazing benefits emerge when data lake users have good coding skills
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å½“æ•°æ®æ¹–ç”¨æˆ·æ‹¥æœ‰è‰¯å¥½çš„ç¼–ç æŠ€èƒ½æ—¶ï¼Œä¼šå‡ºç°æƒŠäººçš„å¥½å¤„
- en: In this case, data lake architecture can offer the highest level of flexibility
    in data processing. Users just need to know how to code in order to apply relevant
    data transformations.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ•°æ®æ¹–æ¶æ„å¯ä»¥æä¾›æ•°æ®å¤„ç†çš„æœ€é«˜çµæ´»æ€§ã€‚ç”¨æˆ·åªéœ€äº†è§£å¦‚ä½•ç¼–ç å³å¯åº”ç”¨ç›¸å…³çš„æ•°æ®è½¬æ¢ã€‚
- en: However, in general, this is not the case and SQL-first solutions become more
    useful.
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œé€šå¸¸æƒ…å†µä¸‹ï¼ŒSQLä¼˜å…ˆçš„è§£å†³æ–¹æ¡ˆå˜å¾—æ›´æœ‰ç”¨ã€‚
- en: Top benefits of a data warehouse
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ•°æ®ä»“åº“çš„ä¸»è¦ä¼˜ç‚¹
- en: As a centralized repository of data that is used for SQL querying and reporting
    DWH have many traits similar to conventional relational database solutions. Some
    of the top benefits of a data warehouse include better scalability (compared to
    RDS), better data governance (compared to data lake and data mesh architectures),
    enhanced business intelligence and improved data quality [4]
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºç”¨äº SQL æŸ¥è¯¢å’ŒæŠ¥å‘Šçš„é›†ä¸­æ•°æ®ä»“åº“ï¼Œæ•°æ®ä»“åº“å…·æœ‰è®¸å¤šç±»ä¼¼äºä¼ ç»Ÿå…³ç³»æ•°æ®åº“è§£å†³æ–¹æ¡ˆçš„ç‰¹å¾ã€‚æ•°æ®ä»“åº“çš„ä¸€äº›ä¸»è¦ä¼˜ç‚¹åŒ…æ‹¬æ›´å¥½çš„å¯æ‰©å±•æ€§ï¼ˆç›¸æ¯”äº RDSï¼‰ã€æ›´å¥½çš„æ•°æ®æ²»ç†ï¼ˆç›¸æ¯”äºæ•°æ®æ¹–å’Œæ•°æ®ç½‘æ ¼æ¶æ„ï¼‰ã€å¢å¼ºçš„å•†ä¸šæ™ºèƒ½å’Œæ”¹å–„çš„æ•°æ®è´¨é‡
    [4]
- en: '[](/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0?source=post_page-----2b1b0486ce4a--------------------------------)
    [## Automated emails and data quality checks for your data'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[## è‡ªåŠ¨åŒ–é‚®ä»¶å’Œæ•°æ®è´¨é‡æ£€æŸ¥'
- en: Data warehouse guide for better and cleaner data with scheduled emails
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ•°æ®ä»“åº“æŒ‡å—ï¼Œå¸®åŠ©é€šè¿‡å®šæœŸé‚®ä»¶è·å¾—æ›´å¥½ã€æ›´æ¸…æ´çš„æ•°æ®
- en: towardsdatascience.com](/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0?source=post_page-----2b1b0486ce4a--------------------------------)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0?source=post_page-----2b1b0486ce4a--------------------------------)'
- en: '**Better data governance:** Many DWH solutions in the market offer column-level
    access controls and row-level access controls. It means that we can define granular
    controls for users. For example in BigQuery, we can restrict access or mask any
    columns that are business or PII-critical [5]:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ›´å¥½çš„æ•°æ®æ²»ç†ï¼š**å¸‚åœºä¸Šçš„è®¸å¤šæ•°æ®ä»“åº“è§£å†³æ–¹æ¡ˆæä¾›äº†åˆ—çº§è®¿é—®æ§åˆ¶å’Œè¡Œçº§è®¿é—®æ§åˆ¶ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥ä¸ºç”¨æˆ·å®šä¹‰ç»†ç²’åº¦çš„æ§åˆ¶ã€‚ä¾‹å¦‚ï¼Œåœ¨ BigQuery
    ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é™åˆ¶è®¿é—®æˆ–é®è”½ä»»ä½•å¯¹ä¸šåŠ¡æˆ–ä¸ªäººæ•æ„Ÿçš„åˆ— [5]ï¼š'
- en: '![](../Images/0e68e73ee8db6b61e7d7920a691167f1.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e68e73ee8db6b61e7d7920a691167f1.png)'
- en: Data masking using policy tags. Image by author.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ç­–ç•¥æ ‡ç­¾çš„æ•°æ®é®è”½ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: 'We can use Infrastructure as Code (IaC) to define these policies similar to
    what we would typically do when deploying infrastructure resources [6]. In this
    example below we can use platform-agnostic Terraform to define dataset access
    permissions:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨åŸºç¡€è®¾æ–½å³ä»£ç ï¼ˆIaCï¼‰æ¥å®šä¹‰è¿™äº›ç­–ç•¥ï¼Œè¿™ç±»ä¼¼äºæˆ‘ä»¬åœ¨éƒ¨ç½²åŸºç¡€è®¾æ–½èµ„æºæ—¶æ‰€åšçš„ [6]ã€‚åœ¨ä¸‹é¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸å¹³å°æ— å…³çš„ Terraform
    æ¥å®šä¹‰æ•°æ®é›†è®¿é—®æƒé™ï¼š
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Better collaboration:** Modern DWH solutions offer collaboration features.
    Effective decision-making frequently requires input from several people inside
    an organisation â€” such as data analysts, marketing teams, management, and others
    â€” as well as multiple data sources. Indeed, **applying the agile approach** to
    any **data transformation development** is crucial. I would call it the best practice.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ›´å¥½çš„åä½œï¼š** ç°ä»£æ•°æ®ä»“åº“è§£å†³æ–¹æ¡ˆæä¾›åä½œåŠŸèƒ½ã€‚æœ‰æ•ˆçš„å†³ç­–é€šå¸¸éœ€è¦ç»„ç»‡å†…éƒ¨å¤šä¸ªäººå‘˜ï¼ˆå¦‚æ•°æ®åˆ†æå¸ˆã€å¸‚åœºè¥é”€å›¢é˜Ÿã€ç®¡ç†å±‚ç­‰ï¼‰çš„è¾“å…¥ï¼Œä»¥åŠå¤šä¸ªæ•°æ®æ¥æºã€‚ç¡®å®ï¼Œ**å°†æ•æ·æ–¹æ³•åº”ç”¨äºä»»ä½•**
    **æ•°æ®è½¬æ¢å¼€å‘** æ˜¯è‡³å…³é‡è¦çš„ã€‚æˆ‘ç§°ä¹‹ä¸ºæœ€ä½³å®è·µã€‚'
- en: Collaboration is the key
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åä½œæ˜¯å…³é”®
- en: DWH solution design makes it easier to deliver large data projects by breaking
    them into smaller pieces. From what I see, organisations tend to move away from
    the tedious waterfall approach in data project design and delivery. Now we have
    a single data integration layer provided in modern DWH solutions that helps to
    deploy incremental model updates more quickly and deliver business insights more
    frequently.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®ä»“åº“è§£å†³æ–¹æ¡ˆè®¾è®¡ä½¿å¾—é€šè¿‡å°†å¤§å‹æ•°æ®é¡¹ç›®æ‹†åˆ†æˆæ›´å°çš„éƒ¨åˆ†æ¥äº¤ä»˜è¿™äº›é¡¹ç›®å˜å¾—æ›´å®¹æ˜“ã€‚ä»æˆ‘æ‰€çœ‹åˆ°çš„ï¼Œç»„ç»‡å€¾å‘äºæ‘†è„±æ•°æ®é¡¹ç›®è®¾è®¡å’Œäº¤ä»˜ä¸­ç¹ççš„ç€‘å¸ƒå¼æ–¹æ³•ã€‚ç°åœ¨æˆ‘ä»¬æ‹¥æœ‰ç°ä»£æ•°æ®ä»“åº“è§£å†³æ–¹æ¡ˆæä¾›çš„å•ä¸€æ•°æ®é›†æˆå±‚ï¼Œè¿™æœ‰åŠ©äºæ›´å¿«åœ°éƒ¨ç½²å¢é‡æ¨¡å‹æ›´æ–°ï¼Œå¹¶æ›´é¢‘ç¹åœ°æä¾›ä¸šåŠ¡æ´å¯Ÿã€‚
- en: '**Scalability**: Data warehouses are designed to scale well in order to handle
    large volumes of data. It is crucial to scale data pipelines when necessary to
    meet the needs of growing businesses. It also has to be able to run concurrent
    workloads at scale in a single system. For instance, a very common DWH pain point
    is user-query concurrency. It happens when a DWH solution allows only a certain
    number of concurrent user queries (typically not more than 50). Many modern data
    warehouses can offer virtual clusters with distributed physical nodes to tackle
    this problem [11].'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¯æ‰©å±•æ€§**ï¼šæ•°æ®ä»“åº“è¢«è®¾è®¡ä¸ºèƒ½å¤Ÿå¾ˆå¥½åœ°æ‰©å±•ï¼Œä»¥å¤„ç†å¤§é‡æ•°æ®ã€‚åœ¨å¿…è¦æ—¶æ‰©å±•æ•°æ®ç®¡é“ä»¥æ»¡è¶³ä¸æ–­å¢é•¿çš„ä¸šåŠ¡éœ€æ±‚è‡³å…³é‡è¦ã€‚å®ƒè¿˜å¿…é¡»èƒ½å¤Ÿåœ¨å•ä¸€ç³»ç»Ÿä¸­ä»¥è§„æ¨¡åŒ–æ–¹å¼è¿è¡Œå¹¶å‘å·¥ä½œè´Ÿè½½ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªéå¸¸å¸¸è§çš„æ•°æ®ä»“åº“ç—›ç‚¹æ˜¯ç”¨æˆ·æŸ¥è¯¢çš„å¹¶å‘æ€§ã€‚å½“æ•°æ®ä»“åº“è§£å†³æ–¹æ¡ˆåªå…è®¸ä¸€å®šæ•°é‡çš„å¹¶å‘ç”¨æˆ·æŸ¥è¯¢ï¼ˆé€šå¸¸ä¸è¶…è¿‡
    50ï¼‰æ—¶ï¼Œå°±ä¼šå‡ºç°è¿™ç§æƒ…å†µã€‚è®¸å¤šç°ä»£æ•°æ®ä»“åº“å¯ä»¥æä¾›å¸¦æœ‰åˆ†å¸ƒå¼ç‰©ç†èŠ‚ç‚¹çš„è™šæ‹Ÿé›†ç¾¤æ¥è§£å†³è¿™ä¸ªé—®é¢˜[11]ã€‚'
- en: Indeed, having a distributed compute cluster with auto-scaling helps a lot in
    many scenarios.
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ç¡®å®ï¼Œåœ¨è®¸å¤šåœºæ™¯ä¸­ï¼Œå…·æœ‰è‡ªåŠ¨æ‰©å±•çš„åˆ†å¸ƒå¼è®¡ç®—é›†ç¾¤å¸®åŠ©å¾ˆå¤§ã€‚
- en: 'Consider this SQL mutli-cluster setup for Snowflake for example:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œè€ƒè™‘ä»¥ä¸‹ Snowflake çš„ SQL å¤šé›†ç¾¤è®¾ç½®ï¼š
- en: '![](../Images/2e8e059c087780bbd9a5f28743f45632.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e8e059c087780bbd9a5f28743f45632.png)'
- en: Multi-cluster data warehouse with Snowflake. Image by author.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Snowflake çš„å¤šé›†ç¾¤æ•°æ®ä»“åº“ã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚
- en: '**Improved data loading:** DWH solutions vary. For some of them, data ingestion
    is a trivial task (Snowflake) while others offer greater flexibility while working
    with partitions (BigQuery). Consider this example of data loading in the Snowflake
    data warehouse below. We assume all our data files are being created in an AWS
    S3 storage bucket and look like this:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ”¹è¿›çš„æ•°æ®åŠ è½½ï¼š** æ•°æ®ä»“åº“è§£å†³æ–¹æ¡ˆå„ä¸ç›¸åŒã€‚å¯¹äºå…¶ä¸­ä¸€äº›ï¼Œæ•°æ®æ‘„å–æ˜¯ä¸€ä¸ªå¾®ä¸è¶³é“çš„ä»»åŠ¡ï¼ˆå¦‚ Snowflakeï¼‰ï¼Œè€Œå…¶ä»–çš„åˆ™åœ¨å¤„ç†åˆ†åŒºæ—¶æä¾›äº†æ›´å¤§çš„çµæ´»æ€§ï¼ˆå¦‚
    BigQueryï¼‰ã€‚è€ƒè™‘ä¸‹é¢çš„ Snowflake æ•°æ®ä»“åº“ä¸­çš„æ•°æ®åŠ è½½ç¤ºä¾‹ã€‚æˆ‘ä»¬å‡è®¾æ‰€æœ‰æ•°æ®æ–‡ä»¶éƒ½å­˜å‚¨åœ¨ AWS S3 å­˜å‚¨æ¡¶ä¸­ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š'
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now we can load it using SQL!
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ SQL æ¥åŠ è½½å®ƒï¼
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This is what makes Snowflake so popular among data analysts and no-code users.
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ Snowflake åœ¨æ•°æ®åˆ†æå¸ˆå’Œæ— ä»£ç ç”¨æˆ·ä¸­å¦‚æ­¤å—æ¬¢è¿çš„åŸå› ã€‚
- en: We can see that DWH data ingestion routines handled everything including data
    format by stripping the outer array, i.e. [{},{},{}] -> {},{},{}.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæ•°æ®ä»“åº“çš„æ•°æ®æ‘„å–ä¾‹ç¨‹å¤„ç†äº†æ‰€æœ‰å†…å®¹ï¼ŒåŒ…æ‹¬é€šè¿‡å»é™¤å¤–éƒ¨æ•°ç»„æ¥å¤„ç†æ•°æ®æ ¼å¼ï¼Œå³ [{},{},{}] -> {},{},{}ã€‚
- en: 'For instance, in BigQuery we would want to create a data loader application
    that would do the same. Consider this Python code below [14]:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œåœ¨ BigQuery ä¸­ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šæƒ³åˆ›å»ºä¸€ä¸ªæ•°æ®åŠ è½½åº”ç”¨ç¨‹åºæ¥åšåŒæ ·çš„äº‹æƒ…ã€‚è€ƒè™‘ä¸‹é¢çš„ Python ä»£ç [14]ï¼š
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[](/python-for-data-engineers-f3d5db59b6dd?source=post_page-----2b1b0486ce4a--------------------------------)
    [## Python for Data Engineers'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/python-for-data-engineers-f3d5db59b6dd?source=post_page-----2b1b0486ce4a--------------------------------)
    [## Python for Data Engineers'
- en: Advanced ETL techniques for beginners
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é¢å‘åˆå­¦è€…çš„é«˜çº§ ETL æŠ€æœ¯
- en: towardsdatascience.com](/python-for-data-engineers-f3d5db59b6dd?source=post_page-----2b1b0486ce4a--------------------------------)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/python-for-data-engineers-f3d5db59b6dd?source=post_page-----2b1b0486ce4a--------------------------------)
- en: BigQuery looks a bit more demanding in terms of the userâ€™s programming skills.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç”¨æˆ·çš„ç¼–ç¨‹æŠ€èƒ½æ–¹é¢ï¼ŒBigQuery çœ‹èµ·æ¥ç¨æ˜¾è¦æ±‚è¾ƒé«˜ã€‚
- en: It is valid to assume that knowing how things work under the hood tends to be
    more cost-effective in the long run.
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¯ä»¥åˆç†åœ°è®¤ä¸ºï¼Œäº†è§£åº•å±‚å·¥ä½œåŸç†å¾€å¾€åœ¨é•¿æœŸå†…æ›´å…·æˆæœ¬æ•ˆç›Šã€‚
- en: 'Indeed, BigQuery offers more fine controls over partitioning which might lead
    to greater savings while working with data in the DWH. Consider this data loading
    example where we can define partitioning (DAY, MONTH, RANGE):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This becomes very useful when we have a data model with date/month dimensions
    and usually saves a lot of money.
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This was a bulk data loading example. **Alternatively**, we can create a data
    pipeline where data is being ingested continuously. Itâ€™s called data streaming
    [12].
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[](/streaming-in-data-engineering-2bb2b9b3b603?source=post_page-----2b1b0486ce4a--------------------------------)
    [## Streaming in Data Engineering'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Streaming data pipelines and real-time analytics
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/streaming-in-data-engineering-2bb2b9b3b603?source=post_page-----2b1b0486ce4a--------------------------------)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Snowflake has a handy Kafka connector [13] that simplifies streaming data pipelines
    and connects to the Kafka server to pull data continuously from topics.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '**Improved Business Intelligence**: Many firms collect huge volumes of data
    from a variety of sources (for example, weather, revenue, payments, customer information,
    trends, vendor information, and so on). The sheer volume of data might be useless.
    Storing this data across numerous platforms might be expensive too. So DWH as
    a single source of truth seems like a problem solver for BI pipelines where everyone
    can generate data insights with ease. Consider this Google Looker Studio integration
    below. It is available for the modern DWH solutions and all of them are pretty
    much market leaders.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/550d5f58c60aba1d313b40f1952cd9db.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
- en: Snowflake Data Connector. Image by author.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e9e59e9fe9ed618e339bc331e23b826c.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
- en: AWS Redshift Connector for Looker Studio. Image by author.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: And so onâ€¦ A free community Business Intelligence (BI) tool has connectors to
    all major DWH solutions available in the market, i.e. Redshift, Snowflake, BigQuery,
    Databricks [7], Galaxy [8], etc.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/679dc8acbc096112a6f794c1fe61bbcd.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
- en: Looker Studio Data connectors. Image by author.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Data warehouses provide a platform for business intelligence tools and applications
    to access and analyze data. This enables businesses to make informed decisions
    based on data-driven insights.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '**Better product integration and DevOps lifecycle**: Some products go even
    further in terms of how data pipelines can be designed and deployed. For BI developers
    and data engineers, it is very important to have everything in Git. Having this
    enabled is crucial as it helps with continuous integration [9]. I previously wrote
    about how data pipeline resources can be deployed using IaC tools such as AWS
    CloudFormation and Terraform:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1?source=post_page-----2b1b0486ce4a--------------------------------)
    [## Continuous Integration and Deployment for Data Platforms'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: CI/CD for data engineers and ML Ops
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1?source=post_page-----2b1b0486ce4a--------------------------------)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1?source=post_page-----2b1b0486ce4a--------------------------------)
- en: Just imagine that we can deploy reports using CI/CD tools. Yes, thatâ€™s right.
    Not just data pipeline resources but also BI dashboards.
  id: totrans-90
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æƒ³è±¡ä¸€ä¸‹æˆ‘ä»¬å¯ä»¥ä½¿ç”¨CI/CDå·¥å…·éƒ¨ç½²æŠ¥å‘Šã€‚æ˜¯çš„ï¼Œæ²¡é”™ã€‚ä¸ä»…ä»…æ˜¯æ•°æ®ç®¡é“èµ„æºï¼Œè¿˜æœ‰BIä»ªè¡¨æ¿ã€‚
- en: 'Consider this AWS CloudFormation template below. It deploys AWS Quicksight
    datasets and report analysis:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·å‚é˜…ä¸‹é¢çš„AWS CloudFormationæ¨¡æ¿ã€‚å®ƒéƒ¨ç½²AWS Quicksightæ•°æ®é›†å’ŒæŠ¥å‘Šåˆ†æï¼š
- en: '[PRE5]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Previously I wrote a tutorial [10] on how to deploy a streaming data pipeline
    using AWS CloudFormation. Adding a BI bit to this even makes it better in my opinion.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¹‹å‰å†™äº†ä¸€ç¯‡æ•™ç¨‹[10]ï¼Œè®²è¿°äº†å¦‚ä½•ä½¿ç”¨AWS CloudFormationéƒ¨ç½²æµæ•°æ®ç®¡é“ã€‚åœ¨æˆ‘çœ‹æ¥ï¼Œå°†BIåŠŸèƒ½æ·»åŠ åˆ°å…¶ä¸­ä¼šä½¿å…¶æ›´å¥½ã€‚
- en: '[](/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2?source=post_page-----2b1b0486ce4a--------------------------------)
    [## Building a Streaming Data Pipeline with Redshift Serverless and Kinesis'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2?source=post_page-----2b1b0486ce4a--------------------------------)
    [## ä½¿ç”¨Redshift Serverlesså’ŒKinesisæ„å»ºæµæ•°æ®ç®¡é“'
- en: An End-To-End Tutorial for Beginners
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: åˆå­¦è€…çš„ç«¯åˆ°ç«¯æ•™ç¨‹
- en: towardsdatascience.com](/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2?source=post_page-----2b1b0486ce4a--------------------------------)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2?source=post_page-----2b1b0486ce4a--------------------------------)
- en: Conclusion
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: Overall, a data warehouse is one of the most popular data platforms and can
    help companies (especially on an enterprise level) gain a competitive advantage
    by providing a single source of truth for data-driven decision-making. Modern
    DWH solutions help to deliver data insights more quickly. In a rapidly changing
    business environment, companies can activate it with automation techniques to
    generate greater value for business stakeholders. Each solution offers the features
    that make it unique. However, there are a few things to consider almost in every
    case. Cost-effectiveness, data partitioning, query-user concurrency, data lake
    storage and associated costs â€” all these pain points are valid. Sometimes it might
    be useful to unload historical data back to a cloud storage archive to optimise
    costs [15]. In some scenarios using Apache Iceberg tables might help with user
    query concurrency issues [16].
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»çš„æ¥è¯´ï¼Œæ•°æ®ä»“åº“æ˜¯æœ€å—æ¬¢è¿çš„æ•°æ®å¹³å°ä¹‹ä¸€ï¼Œå¯ä»¥å¸®åŠ©å…¬å¸ï¼ˆç‰¹åˆ«æ˜¯åœ¨ä¼ä¸šçº§åˆ«ï¼‰é€šè¿‡æä¾›å•ä¸€çš„çœŸå®æ•°æ®æºæ¥è·å¾—ç«äº‰ä¼˜åŠ¿ï¼Œä»è€Œè¿›è¡Œæ•°æ®é©±åŠ¨çš„å†³ç­–ã€‚ç°ä»£æ•°æ®ä»“åº“è§£å†³æ–¹æ¡ˆæœ‰åŠ©äºæ›´å¿«åœ°æä¾›æ•°æ®æ´å¯Ÿã€‚åœ¨å¿«é€Ÿå˜åŒ–çš„å•†ä¸šç¯å¢ƒä¸­ï¼Œå…¬å¸å¯ä»¥é€šè¿‡è‡ªåŠ¨åŒ–æŠ€æœ¯æ¿€æ´»è¿™äº›è§£å†³æ–¹æ¡ˆï¼Œä¸ºä¸šåŠ¡åˆ©ç›Šç›¸å…³è€…åˆ›é€ æ›´å¤§çš„ä»·å€¼ã€‚æ¯ç§è§£å†³æ–¹æ¡ˆéƒ½æä¾›äº†ä½¿å…¶ç‹¬ç‰¹çš„åŠŸèƒ½ã€‚ç„¶è€Œï¼Œå‡ ä¹æ¯ç§æƒ…å†µä¸‹éƒ½éœ€è¦è€ƒè™‘ä¸€äº›å› ç´ ã€‚æˆæœ¬æ•ˆç›Šã€æ•°æ®åˆ†åŒºã€æŸ¥è¯¢ç”¨æˆ·å¹¶å‘ã€æ•°æ®æ¹–å­˜å‚¨åŠç›¸å…³æˆæœ¬â€”â€”è¿™äº›ç—›ç‚¹éƒ½æ˜¯æœ‰æ•ˆçš„ã€‚æœ‰æ—¶ï¼Œå°†å†å²æ•°æ®å¸è½½åˆ°äº‘å­˜å‚¨æ¡£æ¡ˆä¸­ä»¥ä¼˜åŒ–æˆæœ¬å¯èƒ½æ˜¯æœ‰ç”¨çš„[15]ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œä½¿ç”¨Apache
    Icebergè¡¨å¯èƒ½æœ‰åŠ©äºè§£å†³ç”¨æˆ·æŸ¥è¯¢å¹¶å‘é—®é¢˜[16]ã€‚
- en: '[](/introduction-to-apache-iceberg-tables-a791f1758009?source=post_page-----2b1b0486ce4a--------------------------------)
    [## Introduction to Apache Iceberg Tables'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/introduction-to-apache-iceberg-tables-a791f1758009?source=post_page-----2b1b0486ce4a--------------------------------)
    [## Apache Iceberg è¡¨ä»‹ç»'
- en: A few Compelling Reasons to Choose Apache Iceberg for Data Lakes
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: é€‰æ‹©Apache Icebergä½œä¸ºæ•°æ®æ¹–çš„å‡ ä¸ªä»¤äººä¿¡æœçš„ç†ç”±
- en: towardsdatascience.com](/introduction-to-apache-iceberg-tables-a791f1758009?source=post_page-----2b1b0486ce4a--------------------------------)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/introduction-to-apache-iceberg-tables-a791f1758009?source=post_page-----2b1b0486ce4a--------------------------------)
- en: There are a few things to consider while designing and building the perfect
    data platform. Some users might argue the best data warehouse is a lakehouse (Databricks)
    but ultimately it depends on how data is being stored and your business requirements
    for historical records. In many scenarios, a DWH solution acting as a single source
    of truth for all stakeholders might become an optimal choice. Combine it with
    the right data modelling tool and you will get the right tool for external data-to-BI
    data pipelines.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®¾è®¡å’Œæ„å»ºå®Œç¾çš„æ•°æ®å¹³å°æ—¶ï¼Œæœ‰å‡ ä¸ªäº‹é¡¹éœ€è¦è€ƒè™‘ã€‚ä¸€äº›ç”¨æˆ·å¯èƒ½è®¤ä¸ºæœ€ä½³çš„æ•°æ®ä»“åº“æ˜¯æ¹–ä»“ï¼ˆDatabricksï¼‰ï¼Œä½†æœ€ç»ˆè¿™å–å†³äºæ•°æ®çš„å­˜å‚¨æ–¹å¼ä»¥åŠä½ å¯¹å†å²è®°å½•çš„ä¸šåŠ¡éœ€æ±‚ã€‚åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œä½œä¸ºæ‰€æœ‰åˆ©ç›Šç›¸å…³è€…çš„å•ä¸€çœŸå®æ•°æ®æºçš„æ•°æ®ä»“åº“è§£å†³æ–¹æ¡ˆå¯èƒ½æˆä¸ºæœ€ä½³é€‰æ‹©ã€‚ç»“åˆåˆé€‚çš„æ•°æ®å»ºæ¨¡å·¥å…·ï¼Œä½ å°†è·å¾—é€‚ç”¨äºå¤–éƒ¨æ•°æ®åˆ°BIæ•°æ®ç®¡é“çš„æ­£ç¡®å·¥å…·ã€‚
- en: I hope you find these ideas useful. They are based on my personal experience
    and observations.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¸Œæœ›ä½ è§‰å¾—è¿™äº›æƒ³æ³•æœ‰ç”¨ã€‚å®ƒä»¬åŸºäºæˆ‘ä¸ªäººçš„ç»éªŒå’Œè§‚å¯Ÿã€‚
- en: Recommended read
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨èé˜…è¯»
- en: '[1] [https://medium.com/towards-data-science/data-platform-architecture-types-f255ac6e0b7](https://medium.com/towards-data-science/data-platform-architecture-types-f255ac6e0b7)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [https://medium.com/towards-data-science/data-platform-architecture-types-f255ac6e0b7](https://medium.com/towards-data-science/data-platform-architecture-types-f255ac6e0b7)'
- en: '[2] [https://towardsdatascience.com/data-modelling-for-data-engineers-93d058efa302](/data-modelling-for-data-engineers-93d058efa302)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [https://towardsdatascience.com/data-modelling-for-data-engineers-93d058efa302](/data-modelling-for-data-engineers-93d058efa302)'
- en: '[3] [https://medium.com/towards-data-science/big-data-file-formats-explained-275876dc1fc9](https://medium.com/towards-data-science/big-data-file-formats-explained-275876dc1fc9)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [https://medium.com/towards-data-science/big-data-file-formats-explained-275876dc1fc9](https://medium.com/towards-data-science/big-data-file-formats-explained-275876dc1fc9)'
- en: '[4] [https://towardsdatascience.com/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0](/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [https://towardsdatascience.com/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0](/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0)'
- en: '[5] [https://cloud.google.com/bigquery/docs/column-level-security-intro](https://cloud.google.com/bigquery/docs/column-level-security-intro)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] [https://cloud.google.com/bigquery/docs/column-level-security-intro](https://cloud.google.com/bigquery/docs/column-level-security-intro)'
- en: '[6] [https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/bigquery_dataset_access](https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/bigquery_dataset_access)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] [https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/bigquery_dataset_access](https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/bigquery_dataset_access)'
- en: '[7] [https://docs.databricks.com/en/partners/bi/looker.html](https://docs.databricks.com/en/partners/bi/looker.html)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] [https://docs.databricks.com/en/partners/bi/looker.html](https://docs.databricks.com/en/partners/bi/looker.html)'
- en: '[8] [https://docs.starburst.io/clients/looker.html](https://docs.starburst.io/clients/looker.html)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] [https://docs.starburst.io/clients/looker.html](https://docs.starburst.io/clients/looker.html)'
- en: '[9] [https://towardsdatascience.com/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] [https://towardsdatascience.com/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1)'
- en: '[10] [https://medium.com/towards-data-science/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2](https://medium.com/towards-data-science/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] [https://medium.com/towards-data-science/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2](https://medium.com/towards-data-science/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2)'
- en: '[11] [https://www.snowflake.com/blog/auto-scale-snowflake-major-leap-forward-massively-concurrent-enterprise-applications/](https://www.snowflake.com/blog/auto-scale-snowflake-major-leap-forward-massively-concurrent-enterprise-applications/)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] [https://www.snowflake.com/blog/auto-scale-snowflake-major-leap-forward-massively-concurrent-enterprise-applications/](https://www.snowflake.com/blog/auto-scale-snowflake-major-leap-forward-massively-concurrent-enterprise-applications/)'
- en: '[12] [https://medium.com/towards-data-science/streaming-in-data-engineering-2bb2b9b3b603](https://medium.com/towards-data-science/streaming-in-data-engineering-2bb2b9b3b603)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] [https://medium.com/towards-data-science/streaming-in-data-engineering-2bb2b9b3b603](https://medium.com/towards-data-science/streaming-in-data-engineering-2bb2b9b3b603)'
- en: '[13] [https://docs.snowflake.com/en/user-guide/kafka-connector](https://docs.snowflake.com/en/user-guide/kafka-connector)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] [https://docs.snowflake.com/en/user-guide/kafka-connector](https://docs.snowflake.com/en/user-guide/kafka-connector)'
- en: '[14] [https://towardsdatascience.com/python-for-data-engineers-f3d5db59b6dd](/python-for-data-engineers-f3d5db59b6dd)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] [https://towardsdatascience.com/python-for-data-engineers-f3d5db59b6dd](/python-for-data-engineers-f3d5db59b6dd)'
- en: '[15] [https://medium.com/towards-artificial-intelligence/supercharge-your-data-engineering-skills-with-this-machine-learning-pipeline-b69d159780b7](https://medium.com/towards-artificial-intelligence/supercharge-your-data-engineering-skills-with-this-machine-learning-pipeline-b69d159780b7)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] [https://medium.com/towards-artificial-intelligence/supercharge-your-data-engineering-skills-with-this-machine-learning-pipeline-b69d159780b7](https://medium.com/towards-artificial-intelligence/supercharge-your-data-engineering-skills-with-this-machine-learning-pipeline-b69d159780b7)'
- en: '[16] [https://medium.com/towards-data-science/introduction-to-apache-iceberg-tables-a791f1758009](https://medium.com/towards-data-science/introduction-to-apache-iceberg-tables-a791f1758009)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] [https://medium.com/towards-data-science/introduction-to-apache-iceberg-tables-a791f1758009](https://medium.com/towards-data-science/introduction-to-apache-iceberg-tables-a791f1758009)'
