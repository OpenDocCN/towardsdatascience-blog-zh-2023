- en: 'Extracting Text from PDF Files with Python: A Comprehensive Guide'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/extracting-text-from-pdf-files-with-python-a-comprehensive-guide-9fc4003d517](https://towardsdatascience.com/extracting-text-from-pdf-files-with-python-a-comprehensive-guide-9fc4003d517)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A complete process to extract textual information from tables, images, and plain
    text from a PDF file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@george.stavrakis.1996?source=post_page-----9fc4003d517--------------------------------)[![George
    Stavrakis](../Images/50a2e9cac1e0af3e9a8402379d6a1f29.png)](https://medium.com/@george.stavrakis.1996?source=post_page-----9fc4003d517--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9fc4003d517--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9fc4003d517--------------------------------)
    [George Stavrakis](https://medium.com/@george.stavrakis.1996?source=post_page-----9fc4003d517--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9fc4003d517--------------------------------)
    ·17 min read·Sep 21, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b9b1eb9c7f8d41e7811c0bdffa0143f2.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Giorgio Trovato](https://unsplash.com/@giorgiotrovato?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the age of Large Language Models (LLMs) and [their wide-ranging applications](https://bit.ly/49i8JoP),
    from simple text summarisation and translation to predicting stock performance
    based on sentiment and financial report topics, the importance of text data has
    never been greater.
  prefs: []
  type: TYPE_NORMAL
- en: There are many types of documents that share this kind of unstructured information,
    from web articles and blog posts to handwritten letters and poems. However, a
    significant portion of this text data is stored and transferred in PDF format.
    More specifically, it has been found that over 2 billion PDFs are opened in Outlook
    each year, while 73 million new PDF files are saved in Google Drive and email
    daily (2).
  prefs: []
  type: TYPE_NORMAL
- en: Developing, therefore, a more systematic way to process these documents and
    extract information from them would give us the ability to have an automated flow
    and better understand and utilise this vast volume of textual data. And for this
    task, of course, our best friend could be none other than Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, before we start our process, we need to specify the different types
    of PDFs that are around these days, and more specifically, the three most frequently
    appearing:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Programmatically generated PDFs**: These PDFs are created on a computer using
    either W3C technologies such as HTML, CSS, and Javascript or another software
    like Adobe Acrobat. This type of file can contain various components, such as
    images, text, and links, which are all searchable and easy to edit.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Traditional scanned documents**: These PDFs are created from non-electronic
    mediums through a scanner machine or a mobile app. These files are nothing more
    than a collection of images stored together in a PDF file. Saying that, the elements
    appearing in these images, like the text, or links can’t be selected or searched.
    Essentially, the PDF serves as a container for these images.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Scanned documents with OCR:** In this case, Optical Character Recognition
    (OCR) software is employed after scanning the document to identify the text within
    each image in the file, converting it into searchable and editable text. Then
    the software adds a layer with the actual text to the image, and that way you
    can select it as a separate component when browsing the file. (3)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Even though nowadays more and more machines have OCR systems installed in them
    that identify the text from scanned documents, there are still documents that
    contain full pages in an image format. You’ve probably seen that when you read
    a great article and try to select a sentence, but instead you select the whole
    page. This can be a result of a limitation in the specific OCR machine or its
    complete absence. That way, in order not to leave this information undetected
    in this article, I tried to create a process that also considers these cases and
    takes the most out of our precious and information-rich PDFs.
  prefs: []
  type: TYPE_NORMAL
- en: The Theoretical Approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With all these different types of PDF files in mind and the various items that
    compose them, it’s important to perform an initial analysis of the layout of the
    PDF to identify the proper tool needed for each component. More specifically,
    based on the findings of this analysis, we will apply the appropriate method for
    extracting text from the PDF, whether it’s text rendered in a corpus block with
    its metadata, text within images, or structured text within tables. In the scanned
    document without OCR, the approach that identifies and extracts text from images
    will perform all the heavy lifting. The output of this process will be a Python
    dictionary containing information extracted for each page of the PDF file. Each
    key in this dictionary will present the page number of the document, and its corresponding
    value will be a list with the following 5 nested lists containing:'
  prefs: []
  type: TYPE_NORMAL
- en: The text extracted per text block of the corpus
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The format of the text in each text block in terms of font family and size
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The text extracted from the images on the page
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The text extracted from tables in a structured format
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The complete text content of the page
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/ca4c55a64e4df3b288a1f84f84c3ff01.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: That way, we can achieve a more logical separation of the extracted text per
    source component, and it can sometimes help us to more easily retrieve information
    that usually appears in the specific component (e.g., the company name in a logo
    image). In addition, the metadata extracted from the text, like the font family
    and size, can be used to easily identify text headers or highlighted text of greater
    importance that will help us further separate or post-process the text in multiple
    different chunks. Lastly, retaining the structured table information in a way
    that an LLM can understand will enhance significantly the quality of inferences
    made about relationships within the extracted data. Then these results can be
    composed as an output the all the textual information that appeared on each page.
  prefs: []
  type: TYPE_NORMAL
- en: You can see a flowchart of this approach in the images below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d9d68fa706ebff6487d0560e120a2e07.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Installation of all the necessary libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we start this project, though, we should install the necessary libraries.
    We assume that you have Python 3.10 or above installed on your machine. Otherwise,
    you can install it from [here](https://www.python.org/). Then let’s install the
    following libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '**PyPDF2**: To read the PDF file from the repository path.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Pdfminer**: To perform the layout analysis and extract text and format from
    the PDF. (the .six version of the library is the one that supports Python 3)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Pdfplumber**: To identify tables in a PDF page and extract the information
    from them.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Pdf2image**: To convert the cropped PDF image to a PNG image.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**PIL**: To read the PNG image.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Pytesseract**: To extract the text from the images using OCR technology'
  prefs: []
  type: TYPE_NORMAL
- en: This is a little trickier to install because first, you need to install [Google
    Tesseract OCR](https://github.com/tesseract-ocr/tesseract), which is an OCR machine
    based on an LSTM model to identify line recognition and character patterns.
  prefs: []
  type: TYPE_NORMAL
- en: You can install this on your machine if you are a Mac user through **Brew**
    from your terminal, and you are good to go.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'For Windows users, you can follow these steps to install the [link](https://linuxhint.com/install-tesseract-windows/).
    Then, when you download and install the software, you need to add their executable
    paths to Environment Variables on your computer. Alternatively, you can run the
    following commands to directly include their paths in the Python script using
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Then you can install the Python library
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Lastly, we will import all the libraries at the beginning of our script.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: So now we are all set. Let’s move to the fun part.
  prefs: []
  type: TYPE_NORMAL
- en: Document’s Layout Analysis with Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/b317b2163ebef139cb95efcbfa2b8ffe.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: 'For the preliminary analysis, we used the PDFMiner Python library to separate
    the text from a document object into multiple page objects and then break down
    and examine the layout of each page. PDF files inherently lack structured information,
    such as paragraphs, sentences, or words as seen by the human eye. Instead, they
    understand only the individual characters of the text along with their position
    on the page. That way, the PDFMiner tries to reconstruct the content of the page
    into its individual characters along with their position in the file. Then, by
    comparing the distances of those characters from others it composes the appropriate
    words, sentences, lines, and paragraphs of text. (4) To achieve that, the library:'
  prefs: []
  type: TYPE_NORMAL
- en: Separates the individual pages from the PDF file using the high-level function
    extract_pages() and converts them into **LTPage** objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then for each LTPage object, it iterates from each element from top to bottom
    and tries to identify the appropriate component as either:'
  prefs: []
  type: TYPE_NORMAL
- en: '**LTFigure** which represents the area of the PDF that can present figures
    or images that have been embedded as another PDF document in the page.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LTTextContainer** which represents a group of text lines in a rectangular
    area is then analysed further into a list of **LTTextLine** objects. Each one
    of them represents a list of **LTChar** objects, which store the single characters
    of text along with their metadata. (5)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LTRect** represents a 2-dimensional rectangle that can be used to frame images,
    and figures or create tables in an LTPage object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, based on this reconstruction of the page and the classification of
    its elements either into **LTFigure**, which contains the images or figures of
    the page, **LTTextContainer**, which represents the textual information of the
    page, or **LTRect**, which will be a strong indication of the presence of a table,
    we can apply the appropriate function to better extract the information.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: So now that we understand the analysis part of the process, let’s create the
    functions needed to extract the text from each component.
  prefs: []
  type: TYPE_NORMAL
- en: Define the function to extract text from PDF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From here on, extracting text from a text container is really straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: So to extract text from a text container, we simply use the **get_text**() method
    of the LTTextContainer element. This method retrieves all the characters that
    make up the words within the specific corpus box, storing the output in a list
    of text data. Each element in this list represents the raw textual information
    contained in the container.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, to identify this text’s format, we iterate through the LTTextContainer
    object to access each text line of this corpus individually. In each iteration,
    a new **LTTextLine** object is created, representing a line of text in this chunk
    of corpus. We then examine whether the nested line element contains text. If it
    does, we access each individual character element as LTChar, which contains all
    the metadata for that character. From this metadata, we extract two types of formats
    and store them in a separate list, positioned correspondingly to the examined
    text:'
  prefs: []
  type: TYPE_NORMAL
- en: The font family of the characters, including whether the character is in bold
    or italic format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The font size for the character
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generally, characters within a specific chunk of text tend to have consistent
    formatting unless some are highlighted in bold. To facilitate further analysis,
    we capture the unique values of text formatting for all characters within the
    text and store them in the appropriate list.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d29c147610491c54af199c2bcb946554.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Define the function to extract text from Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here I believe it is a more tricky part.
  prefs: []
  type: TYPE_NORMAL
- en: '*How to handle text in images found in PDF?*'
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, we need to establish here that image elements stored in PDFs are not
    in a different format from the file, such as JPEG or PNG. That way in order to
    apply OCR software on them we need first to separate them from the file and then
    convert them into an image format.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'To achieve this, we follow the following process:'
  prefs: []
  type: TYPE_NORMAL
- en: We use the metadata from the LTFigure object detected from PDFMiner to crop
    the image box, utilising its coordinates in the page layout. We then save it as
    a new PDF in our directory using the **PyPDF2** library.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then we employ the **convert_from_file**() function from the **pdf2image** library
    to convert all PDF files in the directory into a list of images, saving them in
    PNG format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, now that we have our image files we read them in our script using the
    **Image** package of the **PIL** module and implement the **image_to_string**()
    function of pytesseract to extract text from the images using the tesseract OCR
    engine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As a result, this process returns the text from the images, which we then save
    in a third list within the output dictionary. This list contains the textual information
    extracted from the images on the examined page.
  prefs: []
  type: TYPE_NORMAL
- en: Define the function to extract text from Tables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will extract a more logically structured text from tables
    on a PDF page. This is a slightly more complex task than extracting text from
    a corpus because we need to take into account the granularity of the information
    and the relationships formed between data points presented in a table.
  prefs: []
  type: TYPE_NORMAL
- en: Although there are several libraries used to extract table data from PDFs, with
    [**Tabula-py**](https://pypi.org/project/tabula-py/) being one of the most well-known,
    we have identified certain limitations in their functionality.
  prefs: []
  type: TYPE_NORMAL
- en: The most glaring one in our opinion comes from the way that the library identifies
    the different rows of the table using the line-break special character \n in the
    table’s text. This works pretty well in most of the cases but it fails to capture
    correctly when the text in a cell is wrapped into 2 or more rows, leading to the
    addition of unnecessary empty rows and losing the context of the extracted cell.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the example below when we tried to extract the data from a table
    using tabula-py:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e51d389d867366c2c04485ef945d1519.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Then, the extracted information is outputted in a Pandas DataFrame instead of
    a string. In most cases, this can be a desirable format but in the case of transformers
    that take into account text, these results need to be transformed before feeding
    into a model.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, to tackle this task we used the **pdfplumber** library for
    various reasons. Firstly, it is built on pdfminer.six which we used for our preliminary
    analysis, meaning that it contains similar objects. In addition, its approach
    to table detection is based on line elements along with their intersections that
    construct the cell that contains the text and then the table itself. That way
    after we identify a cell of a table, we can extract just the content inside the
    cell without carrying how many rows needed to be rendered. Then when we have the
    contents of a table, we will format it in a table-like string and store it in
    the appropriate list.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: To achieve that, we created two functions, **extract_table()** to extract the
    contents of the table into a list of lists, and **table_converter()** to join
    the contents of those lists in a table-like string.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the **extract_table()** function:'
  prefs: []
  type: TYPE_NORMAL
- en: We open the PDF file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We navigate to the examined page of the PDF file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the list of tables found on the page by pdfplumber, we select the desired
    one.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We extract the content of the table and output it in a list of nested lists
    representing each row of the table.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the **table_converter()** function:'
  prefs: []
  type: TYPE_NORMAL
- en: We iterate in each nested list and clean its context from any unwanted line
    breaks coming from any wrapped text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We join each element of the row by separating them using the | symbol to create
    the structure of a table’s cell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we add a line break at the end to move to the next row.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This will result in a string of text that will present the content of the table
    without losing the granularity of the data presented in it.
  prefs: []
  type: TYPE_NORMAL
- en: Adding all together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have all the components of the code ready let’s add them all up
    to a fully functional code. You can copy the code from here or you can find it
    along with the example PDF in my Github repo [here](https://github.com/g-stavrakis/PDF_Text_Extraction).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The script above will:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the necessary libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Open the PDF file using the **pyPDF2** library.
  prefs: []
  type: TYPE_NORMAL
- en: Extract each page of the PDF and iterate the following steps.
  prefs: []
  type: TYPE_NORMAL
- en: Examine if there are any tables on the page and create a list of them using
    **pdfplumner.**
  prefs: []
  type: TYPE_NORMAL
- en: Find all the elements nested in the page and sort them as they appeared in its
    layout.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then for each element:'
  prefs: []
  type: TYPE_NORMAL
- en: Examine if it is a text container, and does not appear in a table element. Then
    use the **text_extraction**() function to extract the text along with its format,
    else pass this text.
  prefs: []
  type: TYPE_NORMAL
- en: Examine if it is an image, and use the **crop_image**() function to crop the
    image component from the PDF, convert it into an image file using the **convert_to_images**(),
    and extract text from it using OCR with the **image_to_text**() function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examine if it is a rectangular element. In this case, we examine if the first
    rect is part of a page’s table and if yes, we move to the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Find the bounding box of the table in order not to extract its text again with
    the text_extraction() function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract the content of the table and convert it into a string.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then add a boolean parameter to clarify that we extract text from Table.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This process will finish after the last LTRect that falls into the bounding
    box of the table and the next element in the layout is not a rectangular object.
    (All the other objects that compose the table will be passed)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The outputs of the process will be stored in 5 lists per iteration, named:'
  prefs: []
  type: TYPE_NORMAL
- en: 'page_text: contains the text coming from text containers in the PDF (placeholder
    will be placed when the text was extracted from another element)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'line_format: contains the formats of the texts extracted above (placeholder
    will be placed when the text was extracted from another element)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'text_from_images: contains the texts extracted from images on the page'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'text_from_tables: contains the table-like string with the contents of tables'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'page_content: contains all the text rendered on the page in a list of elements'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All the lists will be stored under the key in a dictionary that will represent
    the number of the page examined each time.
  prefs: []
  type: TYPE_NORMAL
- en: Afterwards, we will close the PDF file.
  prefs: []
  type: TYPE_NORMAL
- en: Then we will delete all the additional files created during the process.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we can display the content of the page by joining the elements of the
    page_content list.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This was one approach that I believe uses the best characteristics of many libraries
    and makes the process resilient to various types of PDFs and elements that we
    can encounter, with PDFMiner however do the most of the heavy lifting. Also, the
    information regarding the format of the text can help us with the identification
    of potential titles that can separate the text into distinct logical sections
    rather than just content per page and can help us to identify the text of greater
    importance.
  prefs: []
  type: TYPE_NORMAL
- en: However, there will always be more efficient ways to do this task and even though
    I believe that this approach is more inclusive, I am really looking forward to
    discussing with you new and better ways of tackling this problem.
  prefs: []
  type: TYPE_NORMAL
- en: '📖 References:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[https://www.techopedia.com/12-practical-large-language-model-llm-applications](https://www.techopedia.com/12-practical-large-language-model-llm-applications)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://www.pdfa.org/wp-content/uploads/2018/06/1330_Johnson.pdf](https://www.pdfa.org/wp-content/uploads/2018/06/1330_Johnson.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://pdfpro.com/blog/guides/pdf-ocr-guide/#:~:text=OCR](https://pdfpro.com/blog/guides/pdf-ocr-guide/#:~:text=OCR)
    technology reads text from, a searchable and editable PDF.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://pdfminersix.readthedocs.io/en/latest/topic/converting_pdf_to_text.html#id1](https://pdfminersix.readthedocs.io/en/latest/topic/converting_pdf_to_text.html#id1)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://github.com/pdfminer/pdfminer.six](https://github.com/pdfminer/pdfminer.six)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
