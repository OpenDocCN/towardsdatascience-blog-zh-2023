# 通过选择性知识图谱条件优化检索增强生成（RAG）

> 原文：[https://towardsdatascience.com/optimizing-retrieval-augmented-generation-rag-by-selective-knowledge-graph-conditioning-97a4cf96eb69](https://towardsdatascience.com/optimizing-retrieval-augmented-generation-rag-by-selective-knowledge-graph-conditioning-97a4cf96eb69)

## 如何通过有针对性的增强来显著提高知识的相关性，同时保持语言流畅性

[](https://medium.com/@alcarazanthony1?source=post_page-----97a4cf96eb69--------------------------------)[![Anthony Alcaraz](../Images/6a71a1752677bd07c384246fb0c7f7e8.png)](https://medium.com/@alcarazanthony1?source=post_page-----97a4cf96eb69--------------------------------)[](https://towardsdatascience.com/?source=post_page-----97a4cf96eb69--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----97a4cf96eb69--------------------------------) [Anthony Alcaraz](https://medium.com/@alcarazanthony1?source=post_page-----97a4cf96eb69--------------------------------)

·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----97a4cf96eb69--------------------------------) ·7分钟阅读·2023年12月28日

--

*使用人工智能软件来增强本文文本的语法、流畅性和可读性。*

生成预训练模型在作为对话代理使用时表现出令人印象深刻的流畅性和连贯性。然而，它们面临的一个关键限制是缺乏对外部知识的支撑。如果仅依赖预训练的参数，这些模型往往会生成看似合理但实际上不正确的回答，这也被称为幻觉。

以往缓解这一问题的方法涉及将整个知识图谱与对话中提到的实体相关联，从而增强对话上下文。然而，这种对大型知识图谱的不加选择的条件带来了自身的问题：

天真的知识图谱增强的局限性：

+   很多1-hop上下文可能与对话无关，插入了不必要的噪音

+   编码整个知识子图会给序列长度限制带来压力

+   无法保证模型会使用相关事实进行生成

+   尽管有知识基础，幻觉的风险仍然存在

为了克服这一点，Kang等人2023年提出了SUbgraph Retrieval-augmented GEneration（SURGE）框架，具有三项关键创新：

[](https://openreview.net/forum?id=WhWlYzUTJfP&source=post_page-----97a4cf96eb69--------------------------------) [## 知识一致的对话生成与语言模型和...

### 知识一致的对话生成与上下文相关的子图检索、不变图编码和...

openreview.net](https://openreview.net/forum?id=WhWlYzUTJfP&source=post_page-----97a4cf96eb69--------------------------------)

1.  上下文相关子图检索器：使用图神经网络检索器从图知识库中检索与对话上下文最相关的知识图事实。

1.  高效图编码：根据关系扰动标记嵌入，同时仅编码子图实体而非所有三元组。保持排列和反演不变性。

1.  图-文本对比学习：通过对比损失确保检索到的知识图与生成响应之间的一致性。

这使得能够为对话提供准确的事实背景，而不会因无关事实或模型限制而稀释。实验表明，SURGE减少了幻觉现象，并改善了基础知识。

关键见解在于对个性化子图的选择性条件提供了聚焦的知识基础，而不会使预训练模型不堪重负。

![](../Images/355f9ec12caf572abb74a894d9836a92.png)

由 Dall-E-3 生成

# 计划：

## - 上下文相关知识检索；

## - 不变知识编码；

## - 强化知识一致性；

## - 结果；

## - 结论。

# 上下文相关知识检索：

+   使用上下文和三元组嵌入的相似性来建模检索分布

+   从图神经网络中获得的三元组嵌入用于捕捉关系结构

+   使得可以集中于最相关的事实，而不是所有知识图事实

SURGE 解决的关键挑战是仅从知识图中检索最相关的事实，而不是用所有上下文相关的实体淹没生成器。为了实现这种特定于上下文的选择，本文建议将检索建模为一个基于对话历史的知识图三元组分布。

从数学上讲，这个上下文条件的检索分布定义为：

pφ(z|x) ∝ exp(d(z)^T s(x))

其中：

+   x 是对话上下文

+   z 是一个知识图三元组

+   s(x) 为对话上下文生成稠密的嵌入

+   d(z) 为三元组生成稠密的嵌入

这里的关键见解是利用对话和三元组嵌入之间的相似性来建模相关性。

由于三元组包含作为图结构的实体和关系，普通语言模型编码器不够用。相反，图神经网络（GNNs）特别适合捕捉节点和边。GNNs 可以通过传播相邻嵌入来表示实体之间的关系依赖。

具体来说，节点嵌入是通过图卷积网络生成的：

e = GNN(e0; G)

而关系嵌入使用边超图网络：

r = GNN(r0; G∗)

其中 G* 表示对偶超图。

通过结合节点和边嵌入，完整的三元组嵌入可以嵌入语义关系和邻近性。这些三元组与编码器对话上下文向量的相似性提供了一个上下文相关的检索分布的基础。

# 不变知识编码：

+   高效地将检索到的子图编码到生成器转换器中

+   确保编码对关系的顺序和方向不变

+   唯一编码实体并根据关系扰动嵌入

在前一阶段检索到的上下文相关子图需要被编码到生成器变换器模型中，该模型将生成对话回应。然而，简单地编码符号三元组会遇到表示稳定性的问题。

具体来说，有两个期望的不变性属性：

1.  置换不变性：三元组的顺序不应改变整体意义

1.  关系反转不变性：正向和反向关系等效

当将知识图谱编码到用于对话的预训练语言模型中时，会出现几个实际问题：

1.  长序列：将每个三元组事实编码为单词会导致极长的输入序列。这会给模型的上下文容量带来压力。

1.  顺序依赖：打乱三元组的顺序会改变像GPT-3这样的模型看到的意义，因为它们非常依赖单词的顺序和位置。但三元组本质上是无序的——打乱事实不应改变整体意义。

1.  方向差异：关系可以被反转而不改变核心意义（X是Y的妻子 == Y有丈夫X）。但前置文本使这些看起来像完全不同的事实。

上述问题在编码结构化知识时对语言模型造成了不必要的压力。模型被大量标记所压倒，它们难以理解混乱或反向的三元组仍然传达相同的概念。

因此，理想情况下，我们需要一种方法以紧凑而稳定的方式编码知识。编码应该是：

+   高效的：不应导致成千上万的前置标记从而膨胀上下文空间。

+   顺序不变：打乱子图不应大幅改变意义。

+   方向不变：正向和反向关系应被等同对待。

SURGE通过唯一编码实体，然后根据通过图神经网络检测到的关系明智地扰动它们的嵌入来解决这个问题。这为解码器提供了紧凑、稳定的形式。

引入了一种两步嵌入和扰动的方法：

独特的实体嵌入：

+   从三元组中提取独特的实体集 ENT(Z)

+   使用对话编码器嵌入这些实体

+   这种嵌入+排序提供了置换不变性

使用关系进行扰动：

+   对三元组使用图神经网络

+   GNN提供关系感知的节点嵌入

+   应用变换β到实体嵌入：

β(f(a), Z) = (1 + γ) ∗ f(a) + δ

其中 γ, δ 是基于关系学习到的扰动因子。

这一步使用关系信息直接影响实体向量空间，同时保持高效的唯一实体基础编码。

优势：

+   向量空间编码符合生成器要求

+   不变性提供了稳定性和一致性

见解是通过集合和扰动生成不变性，而不是通过可变序列编码。

# 强制知识一致性：

+   知识图谱与生成回应之间的对比损失

+   将相关知识表示拉近到回应表示

+   改善回应在检索事实中的基础

即使在上下文相关的检索和高效编码之后，仍然无法保证生成器会实际利用提供的相关知识。幻觉的风险依然存在。

为了主动结合编码的子图，作者提议在图谱和回应表示之间添加一个跨模态对比损失：

Lcont = (1/2) * log (sim(ζ(z), ξ(h)) / ∑ξ(h’))

+   (1/2) * log (sim(ζ(z), ξ(h)) / ∑ζ(z’))

其中：

+   z 是编码的知识子图

+   h 是解码器的隐藏状态

+   ζ 和 ξ 是投影嵌入

直观上，这种损失将编码的知识图谱拉近到其对应的回应表示，同时将其推离其他随机回应或知识图谱。

这使得模型能够主动区分相关知识-回应对与无关的对。这种区分压力促使模型将其回应基于编码的事实。

好处：

+   改善事实一致性

+   减少不支持的断言

+   允许追踪幻觉到检索错误

关键见解是，如果没有明确的对齐目标，两种模态的向量空间可能会分开，限制事实基础。对比损失作为一致性的归纳偏置。

# 端到端训练：

目标函数：整体训练目标是最大化生成正确回应的对数似然，求和于潜在知识子图：

L = Σp(Z|x) p(y|x,Z)

其中 p(Z|x) 是基于上下文的检索分布，p(y|x,Z) 是生成器分布。

训练过程：

1.  使用编码器网络编码对话上下文 x

1.  通过相似性搜索检索 top-k 子图 Z_i ~ p(Z|x)

1.  使用 GNN + 扰动不变地编码 Z_i

1.  通过解码器最大化每个样本的 p(y|x,Z_i)

1.  另外，最小化 Z_i 和解码器状态之间的对比损失

因此，在对话批次中，通过共享参数优化检索分布和生成分布。

模型选择：

原则上，任何序列到序列的语言模型，如 T5、BART 或甚至 GPT-3，都可以作为生成模型，通过将编码的知识附加到输入上下文标记中。论文在实验中使用了 T5 模型，但可以进行替换。

好处：

+   统一的端到端训练将组件绑定在一起

+   边际似然汇总了整体视网膜性能

+   模块化架构允许模型扩展

# 结果：

+   在衡量知识相关性的指标中优于基线

+   定性示例显示了更多基于相关知识的事实回应

+   消融实验验证了每个组件的重要性

作者在 OpendialKG 和 KOMODIS 对话数据集上评估了 SURGE，这些数据集提供了配对的知识图谱。

定量改进：

+   SURGE在知识相关性指标上超越了所有基准，如提出的KQA（知识验证问答）指标，通过提取器测量事实正确性。

+   在评估语言流利度的现有自动指标如BLEU、ROUGE和F1上取得了新的最先进结果。

定性影响：

+   实例显示，SURGE生成了更具信息性和基于相关知识的事实性回应，这些回应来自于选择性检索的子图。

+   基准方法通常遗漏关键事实，甚至在有完整上下文的情况下仍会幻觉出不相关的陈述。

消融研究：

+   去除对比学习等组件会显著降低知识一致性指标，显示出每个模块的必要性。

SURGE通过针对性增强显著提高了知识相关性，同时保持了语言流利度。与知识无关和知识密集型基准相比的提升验证了选择性子图检索和基础的好处。
