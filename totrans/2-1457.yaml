- en: 'Machine Learning, Illustrated: Incremental Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习图解：增量学习
- en: 原文：[https://towardsdatascience.com/machine-learning-illustrated-incremental-machine-learning-4d73747dc60c](https://towardsdatascience.com/machine-learning-illustrated-incremental-machine-learning-4d73747dc60c)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/machine-learning-illustrated-incremental-machine-learning-4d73747dc60c](https://towardsdatascience.com/machine-learning-illustrated-incremental-machine-learning-4d73747dc60c)
- en: How models learn new information over time, maintaining and building upon previous
    knowledge
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型如何随着时间学习新信息，同时保持和建立在之前的知识基础上
- en: '[](https://medium.com/@shreya.rao?source=post_page-----4d73747dc60c--------------------------------)[![Shreya
    Rao](../Images/03f13be6f5f67783d32f0798f09a4f86.png)](https://medium.com/@shreya.rao?source=post_page-----4d73747dc60c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4d73747dc60c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4d73747dc60c--------------------------------)
    [Shreya Rao](https://medium.com/@shreya.rao?source=post_page-----4d73747dc60c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@shreya.rao?source=post_page-----4d73747dc60c--------------------------------)[![Shreya
    Rao](../Images/03f13be6f5f67783d32f0798f09a4f86.png)](https://medium.com/@shreya.rao?source=post_page-----4d73747dc60c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4d73747dc60c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4d73747dc60c--------------------------------)
    [Shreya Rao](https://medium.com/@shreya.rao?source=post_page-----4d73747dc60c--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4d73747dc60c--------------------------------)
    ·7 min read·Sep 15, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[数据科学前沿](https://towardsdatascience.com/?source=post_page-----4d73747dc60c--------------------------------)
    ·阅读时间7分钟·2023年9月15日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Welcome back to the Illustrated Machine Learning series. If you read the other
    [articles in the series](https://medium.com/@shreya.rao/list/machine-learning-illustrated-dfb4532491ff),
    you know the drill. We take a (*boring sounding*) machine learning concept and
    make it fun by illustrating it! This article will cover a concept called **Incremental
    Learning**, where machine learning models learn new information over time, maintaining
    and building upon previous knowledge. But before getting into that, let’s first
    talk about what the model building process looks like today.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎回到《图解机器学习》系列。如果你阅读了系列中的其他[文章](https://medium.com/@shreya.rao/list/machine-learning-illustrated-dfb4532491ff)，你会知道套路。我们将一个（*听起来很枯燥*）的机器学习概念通过图示化的方式变得有趣！这篇文章将介绍一个名为**增量学习**的概念，在这个概念中，机器学习模型会随着时间的推移学习新信息，同时保持和建立在之前的知识基础上。但在深入之前，让我们先讨论一下今天的模型构建过程是什么样的。
- en: We usually follow a process called **static learning** when building models.
    In this process, we train a model using the latest available data. We tweak and
    tune the model in the training process. And once we’re happy with its performance,
    we deploy it. This model is in production for a while. Then we notice that the
    model performance is getting worse over time. That’s when we throw away the existing
    model and build a new one using the latest available data. And we rinse and repeat
    this same process.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建模型时，我们通常遵循一种称为**静态学习**的过程。在这个过程中，我们使用最新可用的数据来训练模型。在训练过程中，我们会调整和优化模型。一旦对其性能感到满意，我们就会部署它。这个模型会在生产中使用一段时间。然后我们会注意到模型的性能随着时间的推移变得越来越差。这时，我们会丢弃现有的模型，并使用最新的数据重新构建一个新的模型。我们反复执行这个过程。
- en: '![](../Images/07e8722405e2926bf8c48eebe746363e.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07e8722405e2926bf8c48eebe746363e.png)'
- en: Let’s illustrate this using a concrete example. Consider this hypothetical scenario.
    We started building a fraud model at the end of January 2023\. This model detects
    whether a credit card transaction is fraudulent or not. We train our model using
    all the credit card transaction data that we had from the past one-year period
    (January 2022 to December 2022) and use transaction data from this month (January
    2023) to test the model.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个具体的例子来说明这一点。考虑以下假设场景。我们在2023年1月底开始构建一个欺诈检测模型。这个模型用于检测信用卡交易是否欺诈。我们使用过去一年（2022年1月到2022年12月）的所有信用卡交易数据来训练我们的模型，并用本月（2023年1月）的交易数据来测试模型。
- en: '![](../Images/993cd22d306048f39a9c11930a51c17e.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/993cd22d306048f39a9c11930a51c17e.png)'
- en: At the end of next month we notice that the model isn’t doing too well against
    new data. So we built another model, but this time using data from the past one-year
    period (February 2022 to January 2023) to train it and then use the current month’s
    data (February 2023) to test it. And all data outside of these training and testing
    periods is thrown out.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 到下个月底，我们发现模型在面对新数据时表现不佳。因此我们构建了另一个模型，但这次使用过去一年的数据（2022年2月到2023年1月）进行训练，然后用当前月份的数据（2023年2月）进行测试。所有超出这些训练和测试周期的数据都被丢弃。
- en: '![](../Images/5e6e414b3ffc200024eca1265fc138ef.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e6e414b3ffc200024eca1265fc138ef.png)'
- en: Next month, we again notice that the model performance isn’t holding up against
    new data. And again, we build a new model using data from the past one-year period.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 下个月，我们再次注意到模型性能在面对新数据时表现不佳。于是，我们再次使用过去一年的数据构建了一个新模型。
- en: '![](../Images/198acd7c211542e6d589e5b9792b883c.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/198acd7c211542e6d589e5b9792b883c.png)'
- en: And we keep repeating this process whenever we see a decline in model performance.
    This doesn't have to be after 1 month. It could be after 3 months, 6 months, or
    even a year.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们发现模型性能下降时，我们会重复这个过程。这不一定是在1个月后。也可能是在3个月、6个月甚至一年后。
- en: '**And why do we do this batching of data?**'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**那我们为什么要这样批量处理数据呢？**'
- en: 3 main reasons.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 3个主要原因。
- en: 'Concept drift: As time goes on, we see a phenomenon called concept drift, which
    means that what we’re trying to predict changes over time and using older data
    might sometimes be counterproductive.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 概念漂移：随着时间的推移，我们会看到一种叫做概念漂移的现象，这意味着我们试图预测的内容会随时间变化，使用较旧的数据有时可能会适得其反。
- en: 'Memory constraints: The larger our training set, the more memory it occupies.
    So we try to limit the data we input into the model.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 内存限制：我们的训练集越大，占用的内存就越多。因此，我们尝试限制输入到模型中的数据量。
- en: 'Time constraints: In the same vein as reason 2, the larger our training data,
    the longer it takes for our model to train. (Although this usually isn’t such
    a big concern for a lot of models we build. Where this might be problematic is
    in NLP models.)'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 时间限制：与第二个原因类似，我们的训练数据越大，模型训练所需的时间就越长。（尽管这通常对我们构建的大多数模型来说不是一个大问题。可能会出现问题的地方是自然语言处理模型。）
- en: '**But what if we don’t want to throw away all the old models and data?** Throwing
    away the old models means wasting all the knowledge that the old models have gathered
    so far. Ideally, we want to find a way to retain the previous knowledge, while
    gradually adding the information coming from new data. We want to preserve this
    ‘institutional knowledge’ because it is essential for adapting to slowly changing
    or recurring patterns.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**但如果我们不想丢弃所有旧模型和数据呢？** 丢弃旧模型意味着浪费了旧模型迄今为止积累的所有知识。理想情况下，我们希望找到一种方法来保留之前的知识，同时逐步添加来自新数据的信息。我们希望保留这种“制度性知识”，因为它对适应缓慢变化或重复出现的模式至关重要。'
- en: '**And this is exactly what incremental learning does.**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**这正是增量学习所做的事情。**'
- en: In incremental learning, the model learns and enhances its knowledge progressively,
    without forgetting previously acquired information. It grows with the data and
    becomes more refined over time.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在增量学习中，模型逐步学习和增强其知识，而不会忘记以前获得的信息。它随着数据的增长而变得更加完善。
- en: To illustrate this let’s go back to our fraud model example. We start the same
    way as we did in static learning. We build a model using data from the previous
    year, but when we get new data next month instead of building a new model from
    scratch, we just add our new month's data to the already existing model. And we
    repeat this process in 2 months, 3 months, and so on. So here we aren’t technically
    building new models but we are building new versions of the same model.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，我们回到我们的欺诈模型示例。我们以与静态学习相同的方式开始。我们使用去年数据构建模型，但当下个月获得新数据时，我们不是从头开始构建新模型，而是将本月的新数据添加到现有模型中。然后我们在2个月、3个月等时重复这个过程。所以这里我们实际上不是在构建新模型，而是在构建相同模型的新版本。
- en: '![](../Images/15e361925e082f4e2240e842431111e9.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15e361925e082f4e2240e842431111e9.png)'
- en: This can be a good thing because a) It's an efficient use of resources because
    less data storage = more memory saving. Each iteration uses less memory, reducing
    costs. b) This is great for dynamic data, which is most data in the real world
    because we can continuously update predictions as and when we get new data instead
    of building a new model each time. c) The training procedure is carried out on
    a smaller portion of data, so it is much faster.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是个好事，因为 a) 这是一种高效利用资源的方式，因为更少的数据存储 = 节省更多内存。每次迭代使用的内存更少，从而降低成本。 b) 这对动态数据很有用，现实世界中的大多数数据都是动态的，因为我们可以在获取新数据时持续更新预测，而不是每次都重新构建一个新模型。
    c) 训练过程在更小的数据部分上进行，因此速度更快。
- en: 'Fraud detection actually serves as a great example of how incremental learning
    can be beneficial, exemplified by Mastercard’s real-time fraud detection system.
    With each transaction, Mastercard’s system examines more than 100 variables (such
    as transaction size, location, and merchant type) to gauge the likelihood of fraud.
    (source: [DataCamp](https://www.datacamp.com/blog/what-is-incremental-learning))
    The system uses incremental learning to adjust to changing patterns of fraudulent
    activity. In dynamic environments like financial fraud, where fraudsters constantly
    adapt their methods, the challenge of concept drift is significant. Therefore,
    our models must adapt swiftly to maintain their performance and effectively combat
    fraudsters and their evolving tactics.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 欺诈检测实际上是增量学习如何有益的一个很好的例子，以万事达卡的实时欺诈检测系统为例。每次交易时，万事达卡的系统会检查100多个变量（如交易大小、位置和商户类型）以评估欺诈的可能性。（来源：[DataCamp](https://www.datacamp.com/blog/what-is-incremental-learning)）该系统使用增量学习来适应欺诈活动模式的变化。在像金融欺诈这样的动态环境中，欺诈者不断调整他们的方法，概念漂移的挑战非常显著。因此，我们的模型必须迅速适应，以保持其性能并有效打击欺诈者及其不断变化的策略。
- en: And the good news is — incremental learning has already been built into some
    of our favorite models like XGBoost and CatBoost. And it’s very easy to implement!
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是——增量学习已经被内置到我们一些最喜欢的模型中，比如XGBoost和CatBoost。实现起来非常简单！
- en: I explain the mathematical details behind [XGBoost](/xgboost-regression-explain-it-to-me-like-im-10-2cf324b0bbdb)
    and [CatBoost](/catboost-regression-break-it-down-for-me-16ed8c6c1eca) in my previous
    articles.
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我在之前的文章中解释了[XGBoost](/xgboost-regression-explain-it-to-me-like-im-10-2cf324b0bbdb)和[CatBoost](/catboost-regression-break-it-down-for-me-16ed8c6c1eca)背后的数学细节。
- en: Let's test static and incremental learning on real data to compare performance.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在真实数据上测试静态学习和增量学习，以比较性能。
- en: 'We’ll use this Credit Card Fraud [dataset](https://www.kaggle.com/datasets/kartik2112/fraud-detection?select=fraudTrain.csv)
    (CC0) to build the models. After some feature cleaning and selection, we end up
    with a dataset like this:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这个信用卡欺诈[数据集](https://www.kaggle.com/datasets/kartik2112/fraud-detection?select=fraudTrain.csv)（CC0）来构建模型。在进行一些特征清理和选择后，我们得到了如下数据集：
- en: '![](../Images/5ac4e5692ae31cf11cc5c656517d84a6.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5ac4e5692ae31cf11cc5c656517d84a6.png)'
- en: Where **is_fraud** is our target (y) column.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 其中**is_fraud**是我们的目标（y）列。
- en: Let’s start with static learning. We build the same 12 XGBoost models…
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从静态学习开始。我们构建了相同的12个XGBoost模型……
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '…over the following 12 one-year training periods:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: …在接下来的12个一年期训练周期中：
- en: '![](../Images/0d1644fb3704136114a2801e5f698206.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0d1644fb3704136114a2801e5f698206.png)'
- en: 'At the end of every iteration or training period, we are shifting the training
    dates ahead by a month. Then we test each of these models over the following one-month
    test periods, which start right when their corresponding training periods ends:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代或训练周期结束时，我们将训练日期提前一个月。然后，我们在接下来的一个月测试周期中测试每个模型，这些测试周期从其对应的训练周期结束时开始：
- en: '![](../Images/7ab05fb115abfbdbcd6971b03f36d127.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ab05fb115abfbdbcd6971b03f36d127.png)'
- en: 'And then we record the 12 AUC scores:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们记录了12个AUC分数：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We record these scores to compare them to the ones we’ll get from the incremental
    models. Remember the higher our AUC score, the better our model is performing.
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们记录这些分数，以便与我们从增量模型中获得的分数进行比较。记住，我们的AUC分数越高，模型的表现越好。
- en: Now onto our incremental learning models. **We start by training the first model
    over the same one-year period as we did in static learning. This is our base model.**
    But for the next 11 models, we feed in the new months' data to the already existing
    model incrementally.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们进入增量学习模型。**我们从在与静态学习相同的一年期间训练第一个模型开始。这是我们的基准模型。** 但对于接下来的11个模型，我们逐步将新月份的数据输入到已有的模型中。
- en: '![](../Images/78080de785cd2b992f8e2c994a8311c5.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/78080de785cd2b992f8e2c994a8311c5.png)'
- en: So we’re continuing to train the model right where we left off each time.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们每次都从上次结束的地方继续训练模型。
- en: The model looks pretty much the same as before except for one little change.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型看起来基本与之前相同，只是有一个小变化。
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here, we declare the previous model in the current xgboost model we’re building
    using the parameter`xgb_model`. By retaining the previously trained model, the
    training process becomes faster and more efficient, as the model doesn’t need
    to learn from scratch each time.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用参数`xgb_model`在当前构建的xgboost模型中声明之前的模型。通过保留之前训练的模型，训练过程变得更快、更高效，因为模型不需要每次都从头开始学习。
- en: Then we test the 12 models using the same 12 one-month test periods as we did
    in the static models…
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用与静态模型相同的12个月测试期来测试12个模型…
- en: '![](../Images/7ab05fb115abfbdbcd6971b03f36d127.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ab05fb115abfbdbcd6971b03f36d127.png)'
- en: '…and record the AUC scores:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: …并记录AUC分数：
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now for the fun part where we compare the performance of the two processes.
    Out of the 11 recorded AUC scores (because the first score is the same since we
    used the same training and testing data), **incremental learning had better AUC
    scores in 7 out of the 11 iterations**!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是有趣的部分，我们来比较这两种过程的性能。在记录的11个AUC分数中（因为第一个分数是相同的，因为我们使用了相同的训练和测试数据），**增量学习在11次迭代中的7次中取得了更好的AUC分数**！
- en: '![](../Images/d91c1cb0aac3f4ca94852593f5283e2e.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d91c1cb0aac3f4ca94852593f5283e2e.png)'
- en: 'Having said that there are a couple of caveats:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，仍然有一些警告：
- en: Overfitting is a concern with incremental learning because it relies on a continuous
    stream of data. The risk is that it might over-adjust its parameters based on
    recent data, which may not accurately represent the overall distribution. In contrast,
    state learning can consider the entire distribution.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增量学习面临过拟合的风险，因为它依赖于连续的数据流。风险在于，它可能会根据最近的数据过度调整其参数，这些数据可能无法准确代表整体分布。相比之下，状态学习可以考虑整个分布。
- en: While incremental learning can handle evolving data, abrupt changes in data
    trends can pose a challenge. Thus, it may not be suitable for data that changes
    too drastically.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管增量学习可以处理不断变化的数据，但数据趋势的突变可能会带来挑战。因此，对于变化过于剧烈的数据，增量学习可能不适用。
- en: Incremental learning faces a phenomenon called **catastrophic forgetting**,
    where old knowledge is lost as new data is learned and it’s difficult to determine
    what specific information is forgotten.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增量学习面临一种现象，称为**灾难性遗忘**，即在学习新数据时旧知识会丧失，并且很难确定具体丢失了什么信息。
- en: Although there are some considerations to keep in mind, it is beneficial to
    explore the integration of this approach into models by optimizing each version
    of the model. We can achieve this by fine-tuning parameters or refining our feature
    selection, in order to enhance the results even further.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有一些需要考虑的因素，但通过优化模型的每个版本，将这种方法集成到模型中是有益的。我们可以通过调整参数或改进特征选择来进一步提升结果。
- en: And that is all on incremental learning! As always, let me know if you have
    any comments/questions/concerns, and feel free to connect with me on [LinkedIn](https://www.linkedin.com/in/shreyarao24/)
    or email me at *shreya.statistics@gmail.com*.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 以上就是关于增量学习的所有内容！一如既往，如果你有任何评论/问题/顾虑，请告诉我，也可以通过[LinkedIn](https://www.linkedin.com/in/shreyarao24/)与我联系，或通过*shreya.statistics@gmail.com*给我发邮件。
- en: Unless specified, all images are by the author.
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图片均由作者提供。
