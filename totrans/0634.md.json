["```py\nimport torch\nfrom transformers import DefaultDataCollator\n\ntexts = [\"Hello world\", \"How are you?\"]\n\n# Tokenize\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\ntokens = [tokenizer(t) for t in texts]\n\n# Default collate function \ncollate_fn = DefaultDataCollator()\n\n# Pass it to dataloader\ndataloader = torch.utils.data.DataLoader(dataset=tokens, collate_fn=collate_fn, batch_size=2) \n\n# this will end in error\nfor batch in dataloader:\n    print(batch)\n    break\n```", "```py\n[{'input_ids': [101, 7592, 2088, 102], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]},\n {'input_ids': [101, 2129, 2024, 2017, 1029, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}]\n```", "```py\ntexts = [\"Hello world\", \"How are\"]\n```", "```py\n{'input_ids': tensor([[ 101, 7592, 2088,  102],\n                      [ 101, 2129, 2024,  102]]), \n'token_type_ids': tensor([[0, 0, 0, 0],\n                          [0, 0, 0, 0]]), \n'attention_mask': tensor([[1, 1, 1, 1],\n                          [1, 1, 1, 1]])}\n```", "```py\nimport torch\nfrom transformers import DataCollatorWithPadding\n\ntexts = [\"Hello world\", \"How are you?\"]\n\n# Tokenize\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\ntokens = [tokenizer(t) for t in texts]\n\n# Default collate function \ncollate_fn = DataCollatorWithPadding(tokenizer, padding=True) #padding=True, 'max_length'\n\ndataloader = torch.utils.data.DataLoader(dataset=tokens, collate_fn=collate_fn, batch_size=2) \n\nfor batch in dataloader:\n    print(batch)\n    break\n```", "```py\n{'input_ids': tensor([[ 101, 7592, 2088,  102,    0,    0],\n                      [ 101, 2129, 2024, 2017, 1029,  102]]), \n'token_type_ids': tensor([[0, 0, 0, 0, 0, 0],\n                          [0, 0, 0, 0, 0, 0]]), \n'attention_mask': tensor([[1, 1, 1, 1, 0, 0],\n                          [1, 1, 1, 1, 1, 1]])}\n```", "```py\nprint(tokenizer.special_tokens_map)\nprint()\n```", "```py\n{'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n```", "```py\nprint(tokenizer.convert_tokens_to_ids('[PAD]'))\n```", "```py\ncollate_fn = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n```", "```py\ncollate_fn = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n```", "```py\nfrom transformers import DataCollatorForLanguageModeling\nfrom transformers import AutoTokenizer\nimport torch\n\n## input text\ntexts = [\n  \"The quick brown fox jumps over the lazy dog.\",\n  \"I am learning about NLP and AI today\"  \n]\n\n## tokenize\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n\ndata = [tokenizer(t) for t in texts] \n\n## MLM collator\ncollate_fn = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n\n## pass collator to dataloader\ndataloader = torch.utils.data.DataLoader(data, collate_fn=collate_fn, batch_size=2)\n\n## let's look at one sample\nfor batch in dataloader:\n    print(batch)\n    break\n```", "```py\n{'input_ids': tensor([[  101,  1996,  4248,  2829,  4419, 14523,  2058,  1996, 13971,  3899,\n          1012,   102],\n                      [  101,  1045,  2572,   103,  2055, 17953,  2361,  1998,  9932,  2651,\n           102,     0]]), \n'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), \n'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n                          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]), \n'labels': tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n                  [-100, -100, -100, 4083, -100, -100, -100, -100, -100, -100, -100, -100]])}\n```", "```py\nprint( tokenizer.decode(101), tokenizer.decode(102))\n## prints [CLS] and [SEP]\n```", "```py\nfrom transformers import DataCollatorForLanguageModeling\nfrom transformers import AutoTokenizer\n\ntexts = [\n  \"The quick brown fox jumps over the lazy dog.\",\n  \"I am learning about NLP and AI today\"  \n]\n\n# Tokenize\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\ntokens = [tokenizer(t) for t in texts]\n\ncollate_fn = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\ndataloader = torch.utils.data.DataLoader(data, collate_fn=collate_fn, batch_size=2)\n\nfor batch in dataloader:\n    print(batch)\n```", "```py\n{'input_ids': tensor([[  101,  1996,  4248,  2829,  4419, 14523,  2058,  1996, 13971,  3899,\n          1012,   102],\n                      [  101,  1045,  2572,  4083,  2055, 17953,  2361,  1998,  9932,  2651,\n           102,     0]]), \n'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), \n'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n                           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]), \n'labels': tensor([[  101,  1996,  4248,  2829,  4419, 14523,  2058,  1996, 13971,  3899,\n          1012,   102],\n                  [  101,  1045,  2572,  4083,  2055, 17953,  2361,  1998,  9932,  2651,\n           102,  -100]])}\n```", "```py\nfrom typing import Any, Dict, List, Tuple, Union\nimport numpy as np\n\nfrom transformers import AutoModelForCausalLM\nfrom transformers import AutoTokenizer\nfrom transformers import TextDataset, DataCollatorForLanguageModeling\nfrom transformers import Pipeline, PreTrainedTokenizer\n\nRESPONSE_KEY = f\"### Response:\\n\"\n\nclass DataCollatorForCompletionLM(DataCollatorForLanguageModeling):    \n    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n\n        # The torch_call method overrides the same method in the base class and \n        # takes a list of examples as input.  \n        batch = super().torch_call(examples)\n\n        labels = batch[\"labels\"].clone()\n\n        # The code then encodes a special token, RESPONSE_KEY_NL, \n        # representing the end of the prompt followed by a newline. \n        # It searches for this token in the sequence of tokens (labels) \n        # and finds its index.\n        response_token_ids = self.tokenizer.encode(RESPONSE_KEY)\n\n        for i in range(len(examples)):\n\n            response_token_ids_start_idx = None\n            for idx in np.where(batch[\"labels\"][i] == response_token_ids[0])[0]:\n                response_token_ids_start_idx = idx\n                break\n\n            if response_token_ids_start_idx is None:\n                # If the response token is not found in the sequence, it raises a RuntimeError. \n                # Otherwise, it determines the end index of the response token.\n                raise RuntimeError(\n                    f'Could not find response key {response_token_ids} in token IDs \\\n                    {batch[\"labels\"][i]}'\n                )\n\n            response_token_ids_end_idx = response_token_ids_start_idx + 1\n\n            # To train the model to predict only the response and ignore the prompt tokens, \n            # it sets the label values before the response token to -100\\. \n            # This ensures that those tokens are ignored by the PyTorch loss function during training.\n            labels[i, :response_token_ids_end_idx] = -100\n\n        batch[\"labels\"] = labels\n\n        return batch\n```", "```py\ndata_collator = DataCollatorForCompletionLM(\n        tokenizer=tokenizer, mlm=False, return_tensors=\"pt\"\n)\n```"]