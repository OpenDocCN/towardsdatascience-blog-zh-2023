- en: 'Unlocking MLOps using Airflow: A Comprehensive Guide to ML System Orchestration'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://towardsdatascience.com/unlocking-mlops-using-airflow-a-comprehensive-guide-to-ml-system-orchestration-880aa9be8cff](https://towardsdatascience.com/unlocking-mlops-using-airflow-a-comprehensive-guide-to-ml-system-orchestration-880aa9be8cff)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[THE FULL STACK 7-STEPS MLOPS FRAMEWORK](https://towardsdatascience.com/tagged/full-stack-mlops)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Lesson 4: Private PyPi Server. Orchestrate Everything with Airflow.'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://pauliusztin.medium.com/?source=post_page-----880aa9be8cff--------------------------------)[![Paul
    Iusztin](../Images/d07551a78fa87940220b49d9358f3166.png)](https://pauliusztin.medium.com/?source=post_page-----880aa9be8cff--------------------------------)[](https://towardsdatascience.com/?source=post_page-----880aa9be8cff--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----880aa9be8cff--------------------------------)
    [Paul Iusztin](https://pauliusztin.medium.com/?source=post_page-----880aa9be8cff--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----880aa9be8cff--------------------------------)
    Â·17 min readÂ·May 23, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3c440d7e58f12e1041fcde014eea4fda.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Hassan Pasha](https://unsplash.com/@hpzworkz?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial represents **lesson 4 out of a 7-lesson course** that will walk
    you step-by-step through how to **design, implement, and deploy an ML system**
    using **MLOps good practices**. During the course, you will build a production-ready
    model to forecast energy consumption levels for the next 24 hours across multiple
    consumer types from Denmark.
  prefs: []
  type: TYPE_NORMAL
- en: '*By the end of this course, you will understand all the fundamentals of designing,
    coding and deploying an ML system using a batch-serving architecture.*'
  prefs: []
  type: TYPE_NORMAL
- en: This course *targets mid/advanced machine learning engineers* who want to level
    up their skills by building their own end-to-end projects.
  prefs: []
  type: TYPE_NORMAL
- en: '*Nowadays, certificates are everywhere. Building advanced end-to-end projects
    that you can later show off is the best way to get recognition as a professional
    engineer.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Table of Contents:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Course Introduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Course Lessons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data Source
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lesson 4: Private PyPi Server. Orchestrate Everything with Airflow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lesson 4: Code'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Course Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '***At the end of this 7 lessons course, you will know how to:***'
  prefs: []
  type: TYPE_NORMAL
- en: design a batch-serving architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use Hopsworks as a feature store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: design a feature engineering pipeline that reads data from an API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: build a training pipeline with hyper-parameter tunning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use W&B as an ML Platform to track your experiments, models, and metadata
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: implement a batch prediction pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use Poetry to build your own Python packages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: deploy your own private PyPi server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: orchestrate everything with Airflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use the predictions to code a web app using FastAPI and Streamlit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use Docker to containerize your code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use Great Expectations to ensure data validation and integrity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: monitor the performance of the predictions over time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: deploy everything to GCP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: build a CI/CD pipeline using GitHub Actions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If that sounds like a lot, don't worry. After you cover this course, you will
    understand everything I said before. Most importantly, you will know WHY I used
    all these tools and how they work together as a system.
  prefs: []
  type: TYPE_NORMAL
- en: '**If you want to get the most out of this course,** [**I suggest you access
    the GitHub repository**](https://github.com/iusztinpaul/energy-forecasting) **containing
    all the lessons'' code. This course is designed to read and replicate the code
    along the articles quickly.**'
  prefs: []
  type: TYPE_NORMAL
- en: By the end of the course, you will know how to implement the diagram below.
    Don't worry if something doesn't make sense to you. I will explain everything
    in detail.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b5c3b0b8e2162ea8fd268ca745199ec.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram of the architecture you will build during the course [Image by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: By the **end of Lesson 4**, you will know how to host your PiPy repository and
    orchestrate the three pipelines using Airflow. You will learn how to schedule
    the pipelines to create hourly forecasts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Course Lessons:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Batch Serving. Feature Stores. Feature Engineering Pipelines.](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Training Pipelines. ML Platforms. Hyperparameter Tuning.](https://medium.com/towards-data-science/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Batch Prediction Pipeline. Package Python Modules with Poetry.](https://medium.com/towards-data-science/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Private PyPi Server. Orchestrate Everything with Airflow.**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Data Validation for Quality and Integrity using GE. Model Performance Continuous
    Monitoring.](/ensuring-trustworthy-ml-systems-with-data-validation-and-real-time-monitoring-89ab079f4360)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Consume and Visualize your Modelâ€™s Predictions using FastAPI and Streamlit.
    Dockerize Everything.](https://medium.com/towards-data-science/fastapi-and-streamlit-the-python-duo-you-must-know-about-72825def1243)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Deploy All the ML Components to GCP. Build a CI/CD Pipeline Using Github Actions.](https://medium.com/towards-data-science/seamless-ci-cd-pipelines-with-github-actions-on-gcp-your-tools-for-effective-mlops-96f676f72012)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[[Bonus] Behind the Scenes of an â€˜Imperfectâ€™ ML Project â€” Lessons and Insights](https://medium.com/towards-data-science/imperfections-unveiled-the-intriguing-reality-behind-our-mlops-course-creation-6ff7d52ecb7e)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you want to grasp this lesson fully, we recommend you check out [Lesson
    1](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f),
    [Lesson 2](https://medium.com/towards-data-science/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee),
    and [Lesson 3](https://medium.com/towards-data-science/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489),
    which explain in detail the implementation of the pipelines that you will orchestrate
    in this article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Feature Engineering Pipeline](/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Training Pipeline](/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Batch Prediction Pipeline](/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data Source
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We used a free & open API that provides hourly energy consumption values for
    all the energy consumer types within Denmark [1].
  prefs: []
  type: TYPE_NORMAL
- en: They provide an intuitive interface where you can easily query and visualize
    the data. [You can access the data here](https://www.energidataservice.dk/tso-electricity/ConsumptionDE35Hour)
    [1].
  prefs: []
  type: TYPE_NORMAL
- en: 'The data has 4 main attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hour UTC:** the UTC datetime when the data point was observed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Price Area:** Denmark is divided into two price areas: DK1 and DK2 â€” divided
    by the Great Belt. DK1 is west of the Great Belt, and DK2 is east of the Great
    Belt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consumer Type:** The consumer type is the Industry Code DE35, owned and maintained
    by Danish Energy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Total Consumption:** Total electricity consumption in kWh'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Note:** The observations have a lag of 15 days! But for our demo use case,
    that is not a problem, as we can simulate the same steps as it would in real-time.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e0bc098121320b6b981889d8d712952d.png)'
  prefs: []
  type: TYPE_IMG
- en: A screenshot from our web app showing how we forecasted the energy consumption
    for area = 1 and consumer_type = 212 [Image by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: 'The data points have an hourly resolution. For example: "2023â€“04â€“15 21:00Z",
    "2023â€“04â€“15 20:00Z", "2023â€“04â€“15 19:00Z", etc.'
  prefs: []
  type: TYPE_NORMAL
- en: We will model the data as multiple time series. Each unique **price area** and
    **consumer type tuple represents its** unique time series.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we will build a model that independently forecasts the energy consumption
    for the next 24 hours for every time series.
  prefs: []
  type: TYPE_NORMAL
- en: '*Check out the video below to better understand what the data looks like* ðŸ‘‡'
  prefs: []
  type: TYPE_NORMAL
- en: Course & data source overview [Video by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: 'Lesson 4: **Private PyPi Server. Orchestrate Everything with Airflow.**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of Lesson 4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This lesson will teach you how to use Airflow to orchestrate the three pipelines
    you have implemented so far.
  prefs: []
  type: TYPE_NORMAL
- en: Also, to run the code inside Airflow, you will learn to host your PiPy repository
    and deploy the pipelines as 3 different Python modules. Later you will install
    your modules inside Airflow directly from your PiPy repository.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0867f47c56b09ac9cb2dddc9a885283f.png)'
  prefs: []
  type: TYPE_IMG
- en: Diagram of the final architecture with the Lesson 4 components highlighted in
    blue [Image by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: By orchestrating everything using Airflow, you will automate your entire process.
    Instead of running manually 10 different scripts, you will hit once a "Run" button
    to run the whole code.
  prefs: []
  type: TYPE_NORMAL
- en: Also, connecting all the steps together in a programmatic way is less prone
    to bugs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Why?**'
  prefs: []
  type: TYPE_NORMAL
- en: Because every script needs its configurations. For example, the batch prediction
    pipeline needs the feature view version (data version) and the model version as
    input.
  prefs: []
  type: TYPE_NORMAL
- en: This information is generated as metadata from previous scripts. When you run
    everything manually, you can easily copy the wrong version. But when you wrap
    up everything inside a single DAG, you have to build it once, and afterward, it
    will work all the time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, by using Airflow, you can:'
  prefs: []
  type: TYPE_NORMAL
- en: schedule the pipeline to run periodically (you will run it hourly);
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: configure your entire process using Airflow Variables;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: monitor the logs of every task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here is an overview of what you will build in Airflow ðŸ‘‡
  prefs: []
  type: TYPE_NORMAL
- en: Theoretical Concepts & Tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Airflow:** Airflow is oneof the most popular orchestration tools out there.
    The project was developed at Airbnb but is now open source under the Apache License.
    That means that you can modify and host it yourself for free. Airflow lets you
    build, schedule and monitor DAGs.'
  prefs: []
  type: TYPE_NORMAL
- en: '**DAG (Directed Acyclic Graph):** A DAG is a graph with no loops, meaning the
    logic flow can go only one way.'
  prefs: []
  type: TYPE_NORMAL
- en: '**PyPi Registry:** A PiPy registry is a server where you can host various Python
    modules. When you run "**pip install <your_package>**", pip knows how to look
    at the official PyPi repository for your package and install it. Hosting your
    own PyPi registry will behave precisely the same, but you must configure pip to
    know how to access it. Only people with access to your PyPi server can install
    packages from it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Lesson 4: Code'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[You can access the GitHub repository here.](https://github.com/iusztinpaul/energy-forecasting)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** All the installation instructions are in the READMEs of the repository.
    Here you will jump straight to the code.'
  prefs: []
  type: TYPE_NORMAL
- en: '*All the code within Lesson 4 is located under the* [***airflow***](https://github.com/iusztinpaul/energy-forecasting/tree/main/airflow)
    *folder.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The files under the [**airflow**](https://github.com/iusztinpaul/energy-forecasting/tree/main/airflow)
    folderare structured as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/626fcaf985c055b03a8e886cca969cbc.png)'
  prefs: []
  type: TYPE_IMG
- en: A screenshot that shows the structure of the airflow folder [Image by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: All the code is located under the [**dags**](https://github.com/iusztinpaul/energy-forecasting/tree/main/airflow/dags)
    directory**.** Every DAG will have its own Python file.
  prefs: []
  type: TYPE_NORMAL
- en: The Docker files will help you quickly host Airflow and the PiPy repository.
    I will explain them in detail later.
  prefs: []
  type: TYPE_NORMAL
- en: Directly storing credentials in your git repository is a huge security risk.
    That is why you will inject sensitive information using a **.env** file.
  prefs: []
  type: TYPE_NORMAL
- en: The **.env.default** is an example of all the variables you must configure.
    It is also helpful to store default values for attributes that are not sensitive
    (e.g., project name).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5e1d209fa47b2b18eea853fa1a9848b8.png)'
  prefs: []
  type: TYPE_IMG
- en: A screenshot of the .env.default file [Image by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: Prepare Credentials
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As Lesson 4 talks about orchestrating the code from all the other lessons, if
    you want to reproduce the code, you need to check how to set up the 3 pipelines
    from [Lesson 1](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f),
    [Lesson 2](https://medium.com/towards-data-science/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee),
    and [Lesson 3](https://medium.com/towards-data-science/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489).
  prefs: []
  type: TYPE_NORMAL
- en: These three lessons will show you how to set up all the necessary tools and
    services. Also, it will show you how to create and complete the required .env
    file that contains all the credentials.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5e1d209fa47b2b18eea853fa1a9848b8.png)'
  prefs: []
  type: TYPE_IMG
- en: A screenshot of the .env.default file [Image by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: '*The only thing to be careful of is* ðŸ‘‡'
  prefs: []
  type: TYPE_NORMAL
- en: This time you have to place the **.env** that contains your credentials under
    the [**airflow/dags**](https://github.com/iusztinpaul/energy-forecasting/tree/main/airflow/dags)folder**.**
  prefs: []
  type: TYPE_NORMAL
- en: We have set up a default value of **/opt/airflow/dags** for the **ML_PIPELINE_ROOT_DIR**
    environment variable inside the docker-compose.yaml file. Thus, when running the
    pipelines inside Airflow, it will know to load the **.env** file from **/opt/airflow/dags**
    by default.
  prefs: []
  type: TYPE_NORMAL
- en: Also, note that there is another **.env** file under the /[**airflow**](https://github.com/iusztinpaul/energy-forecasting/tree/main/airflow)folder.
    This one doesn't contain your custom credentials, but Airflow needs some custom
    configurations. This is what it looks like ðŸ‘‡
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/262b1e1f5c8889636694e184cec858b4.png)'
  prefs: []
  type: TYPE_IMG
- en: A screenshot of the .env file from the /airflow folder [Image by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: I explained how to complete this **.env** file in the [README.md](https://github.com/iusztinpaul/energy-forecasting/tree/main#usage)
    of the repository. But as a side note, **AIRFLOW_UID** represents your computer's
    USER ID, and you know what **ML_PIPELINE_ROOT_DIR** is.
  prefs: []
  type: TYPE_NORMAL
- en: I just wanted to show you that you can override the default value for **ML_PIPELINE_ROOT_DIR**
    here. Note that this path will be used inside the Docker container, hence the
    path that starts with **/opt/**.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Setup Private PyPi Server
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can easily host a PiPy server using [this repository](https://github.com/pypiserver/pypiserver).
    But let me explain how we did it in our setup.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to create a set of credentials that you will need to connect
    to the PyPi server.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The PyPi repository will know to load the credentials from the **~/.htpasswd/htpasswd.txt**
    file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, you will add the new private PyPi repository to Poetry. To configure Poetry,
    you need to specify the URL of the server, the name of the server and the username
    & password to use to authenticate (which are the ones you configured one step
    before):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '**name of the server:** my-pypy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**URL:** [http://localhost](http://localhost)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**username:** energy-forecasting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**password:** <password>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Check if your credentials are set correctly in your Poetry **auth.toml** file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: So, you finished preparing the username and password that will be loaded by
    your PyPi repository to authenticate. Also, you configured Poetry to be aware
    of your PyPi server.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's see how to run the PyPi server.
  prefs: []
  type: TYPE_NORMAL
- en: The [pyserver code](https://github.com/pypiserver/pypiserver) you will be using
    is already dockerized.
  prefs: []
  type: TYPE_NORMAL
- en: To simplify things, we added the PyPi server as an additional service to the
    docker-compose.yaml file that runs the Airflow application.
  prefs: []
  type: TYPE_NORMAL
- en: To better understand the docker-compose.yaml file check [Airflow's official
    documentation](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html)
    [2] and [our README.md](https://github.com/iusztinpaul/energy-forecasting/tree/main#the-pipeline).
    But be careful to use the docker-compose.yaml file from our repository as we modified
    the original one, as you will see below.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scroll at the bottom of the [**airflow/docker-compose.yaml**](https://github.com/iusztinpaul/energy-forecasting/blob/main/airflow/docker-compose.yaml)
    file, and you will see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This code uses the PyPi server''slatest image, exposes the server under the
    **80 port**, loads the **~/.htpasswd** folder that contains your credentials as
    a volume and runs the server with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '"-P .htpasswd/htpasswd.txt" explicitly tells the server what credentials to
    use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '"â€” overwrite" states that if a new module with the same version is deployed,
    it will overwrite the last one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thats it! When you run the Airflow application, you automatically start the
    PyPi server.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** In a production environment, you will likely host the PyPi server
    on a different server than Airflow. The steps are identical except for adding
    everything in a single docker-compose.yaml file. In this tutorial, we wanted to
    make everything easy to run.'
  prefs: []
  type: TYPE_NORMAL
- en: Customize Airflow Docker File
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Because you have to run all the code in Python 3.9, you have to inherit the
    default **apache/airflow:2.5.2** Airflow Docker image and add some extra dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is what is going on in the Docker file below:'
  prefs: []
  type: TYPE_NORMAL
- en: inherit **apache/airflow:2.5.2**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: switch to the root user to install system dependencies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: install Python 3.9 dependencies needed to install packages from the private
    PyPi server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: switch back to the default user
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Because we switched:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'To:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Docker will know to use your custom image instead of **apache/airflow:2.5.2
    when running docker-compose**.
  prefs: []
  type: TYPE_NORMAL
- en: Run Airflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that you understand how to prepare the credentials and how the Docker files
    work, go to the [./airflow](https://github.com/iusztinpaul/energy-forecasting/tree/main/airflow)
    directory and run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[Check out the ***Usage*** *section* of the GitHub repository for more info.](https://github.com/iusztinpaul/energy-forecasting/tree/main#-usage-)'
  prefs: []
  type: TYPE_NORMAL
- en: 'After you finish your Airflow setup, you can access Airflow at **127.0.0.1:8080**
    with the default credentials:'
  prefs: []
  type: TYPE_NORMAL
- en: 'username: airflow'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'password: airflow'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/76a9df4ab40a4c9b99fb00b950b8b489.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot of the Airflow login page [Image by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: Deploy Modules to Private PyPi Server
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Remember that you added to Poetry your new PyPi server using the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Now, using **my-pypi** as an identifier, you can quickly push new packages to
    your PyPi repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the [deploy/ml-pipeline.sh](https://github.com/iusztinpaul/energy-forecasting/blob/main/deploy/ml-pipeline.sh)
    shell script, you can build & deploy all the 3 pipelines using solely Poetry:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, we iteratively go to the folders of the 3 pipelines and run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Poetry uses these two commands to look for the **pyproject.toml** and **poetry.lock**
    files inside the folders and knows how to build the package.
  prefs: []
  type: TYPE_NORMAL
- en: Afterward, based on the generated **wheel** file, running **"poetry publish
    -r my-pypi",** you push itto your **my-pipy** repository.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that you labeled your PyPi server as **my-pipy**.
  prefs: []
  type: TYPE_NORMAL
- en: You are done. You have your own PyPi repository.
  prefs: []
  type: TYPE_NORMAL
- en: In future sections, I will show you how to install packages from your private
    PyPi repository.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** You used Poetry just to build & deploy the modules. Airflow will
    use pip to install them from your PiPy repository.'
  prefs: []
  type: TYPE_NORMAL
- en: Define the DAG Object
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Your dag is defined under the [**airflow/dags/ml_pipeline_dag.py**](https://github.com/iusztinpaul/energy-forecasting/blob/main/airflow/dags/ml_pipeline_dag.py)
    file.
  prefs: []
  type: TYPE_NORMAL
- en: Using the ***API of Airflow 2.0***, you can define a DAG using the **dag()**
    Python decorator.
  prefs: []
  type: TYPE_NORMAL
- en: Your dag will be defined inside the **ml_pipeline()** function, which is called
    at the end of the file. Also, Airflow knows to load all the DAGs defined under
    the [airflow/dags](https://github.com/iusztinpaul/energy-forecasting/tree/main/airflow/dags)
    directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'The DAG has the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '**dag_id**: the ID of the DAG'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**schedule:** itdefines how often the DAG runs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**start_date:** when should the DAG start running based on the given schedule'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**catchup:** automatically backfill between [start_date, present]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**tags:** tags ðŸ˜„'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**max_active_runs:** how many instances of this DAG can run in parallel'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define the Tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The code below might look long, but you can easily read it once you understand
    the main ideas.
  prefs: []
  type: TYPE_NORMAL
- en: Inside a DAG, you have defined multiple tasks. A task is a single logic unit/step
    that performs a specific operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The tasks are defined similarly to the DAG: a function + a decorator. Every
    task has its function and decorator.'
  prefs: []
  type: TYPE_NORMAL
- en: '***Note:*** *This a simple reminder that we used the API for Airflow 2.0, not
    1.0.*'
  prefs: []
  type: TYPE_NORMAL
- en: In our case, a task will represent a main pipeline script. For example, the
    feature engineering pipeline will run inside a single task.
  prefs: []
  type: TYPE_NORMAL
- en: You will use a DAG to glue all your scripts under a single "program", where
    every script has a 1:1 representation with a task.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/832932dc0905d7afa066c5026aa25207.png)'
  prefs: []
  type: TYPE_IMG
- en: Visual representation of the "ml_pipeline" (see the Youtube video for a better
    view) [Image by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: As you can see inside every task, you just import and call the function from
    its own moduleâ€¦ and maybe add some additional logs.
  prefs: []
  type: TYPE_NORMAL
- en: The key step in defining a task is in the arguments of the **task.virtualenv()**
    Python decorator.
  prefs: []
  type: TYPE_NORMAL
- en: For every task, this specific decorator will create a different Python virtual
    environment inside which it will install all the given requirements.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** **172.17.0.1** is the IP address of your private PyPi repository.
    Remember that you host your PyPi repository using docker-compose under the same
    network as Airflow. **172.17.0.1** is the bridge IP address accessible by every
    Docker container inside the **default Docker** network. Thus, the Airflow container
    can access the PyPi server container using the bridge IP address.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the **requirements** argument, we defined the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '"**â€” trusted-host 172.17.0.1**": As the PyPi server is not secured with HTTPS,
    you must explicitly say that you trust this source.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '"**â€” extra-index-url http://172.17.0.1**": Tell Pip to also look at this PyPi
    repository when searching for a Python package. Note that Pip will still look
    in the official PyPi repository in addition to yours.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '"**<your_python_packages>**": After the two lines described above, you can
    add any Python package. But note that you installed **feature_pipeline**, **training_pipeline**,
    and **batch_prediction_pipeline** asPython packages you built and deployed using
    Poetry.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The other arguments aren''t that interesting, but let me explain them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**task_id=" <task_id>":** The unique ID of a task.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**python_version=" 3.9"**: When I was writing this course, Hopsworks worked
    only with Python 3.9, so we had to enforce this version of Python.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**multiple_outputs=True**: The task returns a Python dictionary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**system_site_packages=True:** Install default system packages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Important***'
  prefs: []
  type: TYPE_NORMAL
- en: 'Notethat almost every task returns a dictionary of metadata that contains information
    such as:'
  prefs: []
  type: TYPE_NORMAL
- en: the date range when the data was extracted,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the version of the feature group, feature view, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the version of the sweep,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the version of the model, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This information is essential to be passed between tasks. For example, the **create_feature_view**
    task needs to know what version of the **feature_group** to use to create the
    next feature view. Also, when running **batch_predict**, you have to know the
    version of the feature view and model to use to generate the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: One interesting task is **task.branch(task_id=" if_run_hyperparameter_tuning_branching")**,
    which defines an if-else logic between whether to run the hyperparameter tuning
    logic or not.
  prefs: []
  type: TYPE_NORMAL
- en: This special type of task returns a list of **task_ids** that will be executed
    next. For example, if it returns **["branch_run_hyperparameter_tuning"],** it
    will run only the task with the **task_id =** **branch_run_hyperparameter_tuning**.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see below, two empty operators (tasks) are defined with the task_ids
    used inside the **task.branch()** logic. It is a common pattern suggested by Airflow
    to use a set of empty operators (no operation) when choosing between multiple
    branches.
  prefs: []
  type: TYPE_NORMAL
- en: Connect the Tasks into a DAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you defined all the tasks, the final step is to connect them into a
    DAG. You have to perform this step so Airflow knows in what order to run every
    task.
  prefs: []
  type: TYPE_NORMAL
- en: Basically, here you will define the logic graph.
  prefs: []
  type: TYPE_NORMAL
- en: '**#1.** The first step is to ***determine the set of variables*** that you
    will use to configure the DAG, such as **days_delay, days_export, feature_group_version,
    etc.** You can access these variables from the â€œAdmin -> Variablesâ€ panel of Airflow.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that you have to add them using the blue plus button explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c016988f56a6918bcf17af862524385d.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot of the Variables Airflow panel [Image by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: '**#2.** The second step is to ***call the tasks with the right parameters***.
    As you can see, because of the Airflow 2.0 API, this step is just like calling
    a bunch of Python functions in a specific order.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** A dependency in the graph is automatically created if the output
    of a function is added as an input to another function.'
  prefs: []
  type: TYPE_NORMAL
- en: It is essential to highlight how we passed the metadata of every pipeline element
    to the next ones. In doing so, we enforce the following scripts to use the correct
    data and model versions.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'I also want to emphasize the following piece of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '**â€œ{{ dag_run.logical_date }}"** is a template variable injected by Airflow
    that reflects the logical date when the DAG is run. Not the current date. By doing
    so, using the Airflow backfill features, you can easily use this as a datetime
    reference to backfill in a given time window. Now you can easily manipulate your
    extraction window''s starting and ending points.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you want to run the DAG to backfill the energy consumption predictions
    between 10 and 11 May 2023, you will run the Airflow backfill logic with the "10
    May 2023 00:00 am date".
  prefs: []
  type: TYPE_NORMAL
- en: '**#3\.** The last step is to enforce a specific ***DAG structure*** using the
    "***>>"*** operator.'
  prefs: []
  type: TYPE_NORMAL
- en: '"**A >> B >> C**" means run A, then B, then C.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The only trickier piece of code is this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: ', where based on the **branch** operator, the DAG will either run the **branch_run_hyperparameter_tuning_operator**
    or **branch_skip_hyperparameter_tuning_operator** branches of the DAG.'
  prefs: []
  type: TYPE_NORMAL
- en: Read more about branching in Airflow [here](https://docs.astronomer.io/learn/airflow-branch-operator)
    [3].
  prefs: []
  type: TYPE_NORMAL
- en: In English, it will run hyper optimization tunning or skip it, as shown in the
    image below â€” I know the image is quite small. Check the video for a better view.
    ðŸ‘‡
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/832932dc0905d7afa066c5026aa25207.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot of the ml_pipeline DAG [Image by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: That is it! You orchestrated all the 3 pipelines using Airflow. Congrats!
  prefs: []
  type: TYPE_NORMAL
- en: Run the ML Pipeline DAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This step is easy.
  prefs: []
  type: TYPE_NORMAL
- en: Just go to your **ml_pipeline** DAG and click the play button.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8797258b1819ae7d4eab3b52f7bb272e.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot of the ml_pipeline DAG view [Image by the Author].
  prefs: []
  type: TYPE_NORMAL
- en: Backfill using Airflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Find your **airflow-webserver** docker container ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Start a shell inside the **airflow-webserver** container and run **airflow
    dags backfill** as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to clear the tasks and rerun them, run these commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations! You finished the **fourth lesson** from the **Full Stack 7-Steps
    MLOps Framework** course.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have reached this far, you know how to:'
  prefs: []
  type: TYPE_NORMAL
- en: Host your own PyPi server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build & deploy your Python modules using Poetry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Orchestrate multiple pipelines using Airflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that you understand the power of using an orchestrator such as Airflow,
    you can build robust production-ready pipelines that you can quickly schedule,
    configure, and monitor.
  prefs: []
  type: TYPE_NORMAL
- en: Check out [Lesson 5](/ensuring-trustworthy-ml-systems-with-data-validation-and-real-time-monitoring-89ab079f4360)
    to learn how to use Great Expectations to validate the integrity and quality of
    your data. Also, you will understand how to implement a monitoring component on
    top of your ML system.
  prefs: []
  type: TYPE_NORMAL
- en: '**Also,** [**you can access the GitHub repository here**](https://github.com/iusztinpaul/energy-forecasting)**.**'
  prefs: []
  type: TYPE_NORMAL
- en: ðŸ’¡ My goal is to help machine learning engineers level up in designing and productionizing
    ML systems. Follow me on [LinkedIn](https://www.linkedin.com/in/pauliusztin/)
    or subscribe to my [weekly newsletter](https://pauliusztin.substack.com/) for
    more insights!
  prefs: []
  type: TYPE_NORMAL
- en: ðŸ”¥ If you enjoy reading articles like this and wish to support my writing, consider
    [becoming a Medium member](https://pauliusztin.medium.com/membership). By using
    [my referral link](https://pauliusztin.medium.com/membership), you can support
    me without any extra cost while enjoying limitless access to Mediumâ€™s rich collection
    of stories.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://pauliusztin.medium.com/membership?source=post_page-----880aa9be8cff--------------------------------)
    [## Join Medium with my referral link - Paul Iusztin'
  prefs: []
  type: TYPE_NORMAL
- en: ðŸ¤– Join to get exclusive content about designing and building production-ready
    ML systems ðŸš€ Unlock full access toâ€¦
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: pauliusztin.medium.com](https://pauliusztin.medium.com/membership?source=post_page-----880aa9be8cff--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] [Energy Consumption per DE35 Industry Code from Denmark API](https://www.energidataservice.dk/tso-electricity/ConsumptionDE35Hour),
    [Denmark Energy Data Service](https://www.energidataservice.dk/about/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [Running Airflow in Docker](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html),
    Airflow Documentation'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] [Branching in Airflow](https://docs.astronomer.io/learn/airflow-branch-operator),
    Airflow Documentation on Astronomer'
  prefs: []
  type: TYPE_NORMAL
