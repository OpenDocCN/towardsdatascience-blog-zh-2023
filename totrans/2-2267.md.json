["```py\nimport pandas as pd\nfrom pandas import DataFrame\nimport numpy as np\n\nfrom functools import reduce\nfrom enum import Enum\n\nFILE_PATH = \"/Updated_sales.csv\"\nCHUNK_SIZE = 1000\n\ndef read_raw_data(file_path: str, chunk_size: int=1000) -> DataFrame:\n    csv_reader = pd.read_csv(file_path, chunksize=chunk_size)\n    processed_chunks = []\n\n    # append the processed chunk to the list\n    for chunk in csv_reader:\n        chunk = chunk.loc[chunk[\"Order ID\"] != \"Order ID\"].dropna()\n        processed_chunks.append(chunk)\n\n    # concatenate the processed chunks into a single DataFrame\n    return pd.concat(processed_chunks, axis=0)\n\ndf = read_raw_data(file_path=FILE_PATH)\n```", "```py\ndef split_purchase_address(df_to_process: DataFrame) -> DataFrame:\n    df_address_split = df_to_process[\"Purchase Address\"].str.split(\",\", n=3, expand=True)\n    df_address_split.columns = [\"Street Name\", \"City\", \"State and Postal Code\"]\n\n    df_state_postal_split = (\n        df_address_split[\"State and Postal Code\"]\n        .str.strip()\n        .str.split(\" \", n=2, expand=True)\n    )\n    df_state_postal_split.columns = [\"State Code\", \"Postal Code\"]\n\n    return pd.concat([df_to_process, df_address_split, df_state_postal_split], axis=1)\n```", "```py\nprocessed_df = df.pipe(split_purchase_address)\n```", "```py\n# groupby normally\ngrouped_df = (\n    processed_df\n    .groupby(\n        [\"Product\", \"Quantity Ordered\", \"Street Name\", \"City\", \"State Code\", \"Postal Code\"]\n    )\n    [\"Order ID\"]\n    .count()\n    .reset_index()\n    .sort_values(\"Order ID\", ascending=False)\n    .rename({\"Order ID\": \"Count of Order IDs\"}, axis=1)\n)\n```", "```py\nclass SalesGroupByColumns(Enum):\n    PRODUCT = \"Product\"\n    QUANTITY_ORDERED = \"Quantity Ordered\"\n    STREET_NAME = \"Street Name\"\n    CITY = \"City\"\n    STATE_CODE = \"State Code\"\n    POSTAL_CODE = \"Postal Code\"\n```", "```py\nSalesGroupByColumns.PRODUCT\n```", "```py\nSalesGroupByColumns.PRODUCT.value\n```", "```py\n[column.value for column in SalesGroupByColumns]\n```", "```py\n# groupby adjusted\ngroupby_columns = [column.value for column in SalesGroupByColumns]\n\ngrouped_df = (\n    processed_df\n    .groupby(groupby_columns)\n    [\"Order ID\"]\n    .count()\n    .reset_index()\n    .sort_values(\"Order ID\", ascending=False)\n    .rename({\"Order ID\": \"Count of Order IDs\"}, axis=1)\n)\n\ngrouped_df.head()\n```", "```py\n# what's the benefit? adding new columns!\n\nclass SalesGroupByColumns(Enum):\n    PRODUCT = \"Product\"\n    QUANTITY_ORDERED = \"Quantity Ordered\"\n    STREET_NAME = \"Street Name\"\n    CITY = \"City\"\n    STATE_CODE = \"State Code\"\n    POSTAL_CODE = \"Postal Code\"\n    HOUSE_NUMBER = \"House Number\"\n    PRODUCT_CATEGORY = \"Prouct Category\"\n\n# then you run the code same as before and it would still work\n```", "```py\ndef convert_numerical_column_types(df_to_process: DataFrame) -> DataFrame:\n    df_to_process[\"Quantity Ordered\"] = df_to_process[\"Quantity Ordered\"].astype(int)\n    df_to_process[\"Price Each\"] = df_to_process[\"Price Each\"].astype(float)\n    df_to_process[\"Order ID\"] = df_to_process[\"Order ID\"].astype(int)\n\n    return df_to_process\n\ndef calculate_total_order_cost(df_to_process: DataFrame) -> DataFrame:\n    df_to_process[\"Total Cost\"] = df_to_process[\"Quantity Ordered\"] * df_to_process[\"Price Each\"]\n    return df_to_process\n\nprocessed_df = (\n    df\n    .pipe(split_purchase_address)\n    .pipe(convert_numerical_column_types)\n    .pipe(calculate_total_order_cost)\n)\n```", "```py\n# let's say we have a file now \"SalesColumns.py\"\n# we can add to it\n\nimport numpy as np\n\nclass AddressColumns(Enum):\n    STREET_NAME = \"Street Name\"\n    CITY = \"City\"\n    STATE_CODE = \"State Code\"\n    POSTAL_CODE = \"Postal Code\"\n\nclass SalesMeasureColumns(Enum):\n    TOTAL_COST = \"Total Cost\"\n    QUANTITY_ORDERED = \"Quantity Ordered\"    \n\n# then separately we can do the groupby\ngroupby_columns = [column.value for column in AddressColumns]\n\ngrouped_df = (\n    processed_df\n    .groupby(groupby_columns)\n    .agg(\n        Total_Cost=(SalesMeasureColumns.TOTAL_COST.value, np.sum),\n        Total_Quantity_Ordered=(SalesMeasureColumns.QUANTITY_ORDERED.value, np.sum)\n    )\n    .reset_index()\n    .sort_values(\"Total_Cost\", ascending=False)\n)\n```", "```py\ngrouped_df.loc[grouped_df[\"Street Name\"].str.contains(\"North\")]\n```", "```py\ngrouped_df.loc[\n    (grouped_df[\"Street Name\"].str.contains(\"North\")) &\n    (grouped_df[\"Postal Code\"].str.contains(\"940\")) &\n    (grouped_df[\"Total_Cost\"] < 1000)\n]\n```", "```py\nfilter_conditions = [\n    grouped_df[\"Street Name\"].str.contains(\"North\"),\n    grouped_df[\"Postal Code\"].str.contains(\"940\"),\n    grouped_df[\"Total_Cost\"] < 1000\n]\n```", "```py\n# doesn't work -> you can't just pass a list into loc\ndf.loc[FILTER_CONDITIONS]\n\n# format should look like this\ndf.loc[condition_1 & condition_2 & condition_3]\n```", "```py\n# functools reduce\nreduce(lambda x, y: f\"{x} & {y}\", [\"condition_1\", \"condition_2\", \"condition_3\"])\n```", "```py\ngrouped_df.loc[reduce(lambda x, y: x & y, filter_conditions)]\n```"]