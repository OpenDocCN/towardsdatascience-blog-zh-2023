- en: 'ReLoRa: Pre-train a Large Language Model on Your GPU'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/relora-pre-train-a-large-language-model-on-your-gpu-d104756f9ddf](https://towardsdatascience.com/relora-pre-train-a-large-language-model-on-your-gpu-d104756f9ddf)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: LoRa but with multiple resets in a row
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----d104756f9ddf--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----d104756f9ddf--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d104756f9ddf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d104756f9ddf--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----d104756f9ddf--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d104756f9ddf--------------------------------)
    ·8 min read·Jul 20, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a84220fa206495cacbda94863cf4dfe4.png)'
  prefs: []
  type: TYPE_IMG
- en: The ReLoRa framework — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: In 2021, [Hu et al.](https://arxiv.org/abs/2106.09685) proposed low-rank adapters
    (LoRa) for LLMs. This method significantly reduces the cost of fine-tuning large
    language models (LLMs) by only training a few added parameters (low-rank networks)
    while keeping the LLM’s original parameters (high-rank networks) frozen.
  prefs: []
  type: TYPE_NORMAL
- en: With LoRa, we still need an existing pre-trained model to fine-tune, i.e., it
    can’t pre-train a good LLM from scratch due to the low-rank restrictions. It leaves
    pre-training unaffordable for most individuals and organizations.
  prefs: []
  type: TYPE_NORMAL
- en: To reduce this cost, [Lialin et al. (2023)](https://arxiv.org/pdf/2307.05695.pdf)
    propose ReLoRa. This is a modification of LoRa that allows pre-training LLMs from
    scratch.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I first explain how ReLoRa works. Then, I analyze and comment
    on the results presented in the scientific paper describing ReLoRa. In the last
    section, I show how to set up and run ReLoRa on your computer.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note about the licenses:* [*The scientific paper published on arXiv*](https://arxiv.org/abs/2307.05695)
    *and describing ReLoRa is distributed under a CC BY 4.0 license.* [*The source
    code of ReLoRa is published on GitHub*](https://github.com/guitaricet/peft_pretraining)
    *and distributed under an Apache 2.0 license allowing commercial use.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'ReLoRa: from low-rank to high-rank networks'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand how ReLoRa works, we must first have a closer look at LoRa.
  prefs: []
  type: TYPE_NORMAL
- en: LoRA works by adding two different sets of new trainable parameters, *A* and
    *B*, which are merged back after training into the original frozen high-rank network
    of the pre-trained model.
  prefs: []
  type: TYPE_NORMAL
- en: 'It may seem obvious but it is important to understand that the rank of the
    sum of A and B is higher than the sum of their individual ranks. This can be formalized
    as followed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c1a26cfaf96c26dd1430b0388951656f.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation (1) by [Lialin et al. (2023)](https://arxiv.org/pdf/2307.05695.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: LoRa only trained these two sets of parameters. However, if we could reset,
    train, and merge them back to the original high-rank network multiple times in
    a row we would be able to increase over time the total rank of the network. In
    other words, we would obtain a larger model.
  prefs: []
  type: TYPE_NORMAL
- en: '*Why LoRa does not perform these resets?*'
  prefs: []
  type: TYPE_NORMAL
- en: Because there are several significant obstacles to overcome to make these resets
    useful. Standard LLM training uses the Adam optimizer which has its own states.
    Resetting LoRa’s trainable parameters without changing the states of Adam would
    make the new LoRa parameters similar to the LoRa parameters of the previous iteration.
    The model wouldn’t learn anything.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the main ideas proposed in ReLoRa is to also partially reset the Adam
    optimizer’s states in combination with “a jagged scheduler to stabilize training
    and warm starts”. This jagged scheduler resets the learning rate to 0 and performs
    a new warm-up for a few training steps. The following effect on the learning rate
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d2bf3b9cf69192706a9e1e60f88e05d.png)'
  prefs: []
  type: TYPE_IMG
- en: Illustration by [Lialin et al. (2023)](https://arxiv.org/pdf/2307.05695.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: 'More formally, the full algorithm running ReLoRa can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d6e9cc39f8d3c95b14ecffaa09c32b9.png)'
  prefs: []
  type: TYPE_IMG
- en: Algorithm by [Lialin et al. (2023)](https://arxiv.org/pdf/2307.05695.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: 'Results: ReLoRa yields a lower perplexity than LoRa'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In practice, ReLoRa seems to yield results similar to standard pre-training,
    for a much lower cost, but only above a minimum number of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e7b154048bf49bad3f39d1aada246bbc.png)'
  prefs: []
  type: TYPE_IMG
- en: Table by [Lialin et al. (2023)](https://arxiv.org/pdf/2307.05695.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in the table above, ReLoRa roughly performs as well as “full training”
    (without LoRa and without freezing any parameters) for models with 250 million
    parameters or more.
  prefs: []
  type: TYPE_NORMAL
- en: They also included results for pre-training with the standard LoRa. It poorly
    performs and illustrates very well how critical are the resets in ReLoRa.
  prefs: []
  type: TYPE_NORMAL
- en: For these experiments, they used a neural network architecture similar to Meta’s
    LLaMa models. The training for most of these models took only 1 day using 8 RTX
    4090 GPUs (which are consumer GPUs).
  prefs: []
  type: TYPE_NORMAL
- en: They didn’t experiment with more parameters than 350M (approximately the size
    of BERT large) due to the computational cost.
  prefs: []
  type: TYPE_NORMAL
- en: In an ablation study, the authors also demonstrate that LoRa’s restarts and
    scheduler new warm-ups are essential to achieve a lower perplexity. They also
    show that removing the jagged scheduler may lead to training divergence.
  prefs: []
  type: TYPE_NORMAL
- en: The computational efficiency of ReLoRa
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ReLoRa iteratively trains and adds new parameters to the model, while keeping
    the parameters from previous iterations frozen.
  prefs: []
  type: TYPE_NORMAL
- en: 'ReLoRa is thus as efficient as LoRa in terms of memory usage. Moreover, the
    frozen parameters can be quantized at a low precision to further reduce memory
    usage. We can do this for instance with QLoRa as I described here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/qlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b?source=post_page-----d104756f9ddf--------------------------------)
    [## QLoRa: Fine-Tune a Large Language Model on Your GPU'
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning models with billions of parameters is now possible on consumer hardware
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/qlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b?source=post_page-----d104756f9ddf--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Running ReLoRa on your computer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lialin et al. (2023) released their implementation of [ReLoRa on GitHub](https://github.com/Guitaricet/peft_pretraining/tree/main).
  prefs: []
  type: TYPE_NORMAL
- en: Since ReLoRa works with models of 250 million parameters or more, we can run
    it on consumer hardware, e.g., with a GPU with more than 6 GB of VRAM, or on a
    free instance of Google Colab.
  prefs: []
  type: TYPE_NORMAL
- en: You can easily reproduce their experiments and pre-train your own 250-million-parameter
    LLM as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: If you want to directly test it without coding anything, I created a
    Google Colab notebook on The Kaitchup (my substack newsletter).* [*Search for
    notebook #3*](https://kaitchup.substack.com/p/notebooks)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, clone the repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'And then install all the requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Since training takes a lot of time, I recommend first trying the framework with
    hyperparameters that will early stop the training and shorten the validation (which
    can take hours on consumer hardware).
  prefs: []
  type: TYPE_NORMAL
- en: 'The framework doesn’t have the option to shorten the validation, so we will
    have to do it manually. Open the file torchrun_main.py, and replace the line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '*Note: At the time of writing this article, this is line 129.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'by:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: For better performance, the authors of the framework recommend running it in
    two steps.
  prefs: []
  type: TYPE_NORMAL
- en: The first step just initializes and trains the network for a few steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the hyperparameters I used to test that everything is working:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '*Note: I set “— nproc-per-node” to 1 since I have only 1 GPU. You should change
    it to your number of GPUs.*'
  prefs: []
  type: TYPE_NORMAL
- en: In the configs directory, you will see several llama files. They contain the
    configurations for different models, similar to the architecture of the LLaMa
    model, with different sizes. Here I chose the size 250M.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is very verbose since the framework logs everything with wandb.
    It will also require you to make a choice here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: I entered “3” since I don’t have an account.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, in the second step, we rerun the framework with ReLoRa’s hyperparameters
    and PEFT:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: With “ — continue_from”, we give the model saved at the first step. Your model
    will have a different name, so you will have to change it. You can find it in
    the “checkpoints” directory.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have confirmed that everything is working, i.e., without errors and
    with a decreasing perplexity, you can relaunch everything but with reasonable
    hyperparameters so that the model will be much better trained.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: Don’t forget to remove the change we made in torchrun_main.py to shorten
    validation. You should at least increase the total number of batches to 100 to
    have a meaningful validation perplexity.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '*Note: Increase the batch size if you have GPUs with a lot of VRAM.*'
  prefs: []
  type: TYPE_NORMAL
- en: By default (it’s hardcoded), the framework used the C4 dataset (English only)
    for pre-training. This dataset was also used to pre-train the T5 models by Google.
    It covers many tasks, domains, and genres.
  prefs: []
  type: TYPE_NORMAL
- en: Once pre-training is finished, you will still have to fine-tune the resulting
    model for downstream tasks or domains of your choice. [You can do that very efficiently
    with QLoRa](/qlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b).
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To sum up, ReLoRa is a new pre-training method exploiting low-rank networks.
    It’s like performing LoRa multiple times in a row but with:'
  prefs: []
  type: TYPE_NORMAL
- en: A partial reset of the optimizer’s states
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A jagged learning rate schedule
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A short warmup after each reset of LoRa
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thanks to ReLoRa, we can now pre-train LLM on consumer hardware.
  prefs: []
  type: TYPE_NORMAL
- en: It remains to explore whether this approach is also competitive for very large
    language models (with more than 1 billion parameters).
  prefs: []
  type: TYPE_NORMAL
- en: According to ReLoRa’s authors, ReLoRa works better as the model gets larger.
    Nonetheless, it should be empirically verified by future work.
  prefs: []
  type: TYPE_NORMAL
- en: '*If you like this article and would be interested to read the next ones, the
    best way to support my work is to become a Medium member using this link:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie/membership?source=post_page-----d104756f9ddf--------------------------------)
    [## Join Medium with my referral link - Benjamin Marie'
  prefs: []
  type: TYPE_NORMAL
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@bnjmn_marie/membership?source=post_page-----d104756f9ddf--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*If you are already a member and want to support this work,* [*just follow
    me on Medium*](https://medium.com/@bnjmn_marie)*.*'
  prefs: []
  type: TYPE_NORMAL
