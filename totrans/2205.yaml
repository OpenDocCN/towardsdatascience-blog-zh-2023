- en: Unit testing PySpark code using Pytest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/unit-testing-pyspark-code-using-pytest-b5ab2fd54415](https://towardsdatascience.com/unit-testing-pyspark-code-using-pytest-b5ab2fd54415)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://julianwest155.medium.com/?source=post_page-----b5ab2fd54415--------------------------------)[![Julian
    West](../Images/c7740cd407ed17e84af2c49a37dbbe92.png)](https://julianwest155.medium.com/?source=post_page-----b5ab2fd54415--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b5ab2fd54415--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b5ab2fd54415--------------------------------)
    [Julian West](https://julianwest155.medium.com/?source=post_page-----b5ab2fd54415--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b5ab2fd54415--------------------------------)
    ¬∑10 min read¬∑Jan 16, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/314b8ebf1cb2596498d48e1f09861a50.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Jez Timms](https://unsplash.com/@jeztimms?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/_Ch_onWf38o?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: '**I am a big fan of unit-testing.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reading two books ‚Äî** [**The Pragmatic Programmer**](https://engineeringfordatascience.com/book-notes/pragmatic-programmer/)
    **and** [**Refactoring**](https://engineeringfordatascience.com/book-notes/refactoring/)
    **‚Äî completely changed the way I viewed unit-testing.**'
  prefs: []
  type: TYPE_NORMAL
- en: ‚ÄúTesting is not about finding bugs.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We believe that the major benefits of testing happen when you think about and
    write the tests, not when you run them.‚Äù
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*‚Äî The Pragmatic Programmer, David Thomas and Andrew Hunt*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Instead of seeing testing as a chore to complete after I have finished my data
    pipelines, I see it as a powerful tool to improve the design of my code, reduce
    coupling, iterate more quickly and build trust with others in my work.
  prefs: []
  type: TYPE_NORMAL
- en: However, writing good tests for data applications can be difficult.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike traditional software applications with relatively well defined inputs,
    data applications in production depend on large and constantly changing input
    data.
  prefs: []
  type: TYPE_NORMAL
- en: It can be very challenging to accurately represent this data in a test environment
    in enough detail to cover all edge cases. Some people argue there is little point
    in unit-testing data pipelines, and focus on data validation techniques instead.
  prefs: []
  type: TYPE_NORMAL
- en: I strongly believe in implementing both unit-testing and data validation in
    your data pipelines. **Unit-testing isn‚Äôt just about finding bugs, it is about
    creating better designed code and building trust with colleagues and end users.**
  prefs: []
  type: TYPE_NORMAL
- en: If you can get in the habit of writing tests, you will write better designed
    code, save time in the long run and reduce the pain of pipelines failing or giving
    incorrect results in production.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges with Unit testing PySpark code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A good unit-test should have the following characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Focused**. Each test should test a single behaviour/functionality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fast**. Allowing you to iterate and gain feedback quickly'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Isolated**. Each test should be responsible for testing a specific functionality
    and not depend on external factors in order to run successfully'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Concise**. Creating a test shouldn‚Äôt include lots of boiler-plate code to
    mock/create complex objects in order for the test to run'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**When it comes to writing unit-tests specifically for PySpark pipelines, writing
    focussed, fast, isolated and concise tests can be a challenge.**'
  prefs: []
  type: TYPE_NORMAL
- en: Here are some of the hurdles you might come across‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: Writing testable code in the first place
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PySpark pipelines tend to be written as one giant function responsible for multiple
    transformations.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: It is logical to think about transformations in this way, and in many ways is
    easier to reason about and read.
  prefs: []
  type: TYPE_NORMAL
- en: But, when you start trying to write a test for this function you quickly realise
    it is very difficult to write a test to cover all functionality.
  prefs: []
  type: TYPE_NORMAL
- en: This is because the function is highly coupled and there many different paths
    that the function can take.
  prefs: []
  type: TYPE_NORMAL
- en: Even if you did write a test that verified the input data and output data were
    as expected. If the test failed for any reason it would be very difficult to understand
    which part of the very long function was at fault.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, you should break your transformations into blocks of reusable functions
    which are responsible for a single task. You can then write a unit-test for each
    individual function (task) which. When each of these unit-tests pass, you can
    be more confident in the output of the final pipeline when you compose all the
    functions together.
  prefs: []
  type: TYPE_NORMAL
- en: Writing tests is a good practice and forces you to think about design principles.
    If it is difficult to test your code then you probably need to rethink the design
    of your code.
  prefs: []
  type: TYPE_NORMAL
- en: Speed
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spark is optimised to work on very large data and the compute is optimised to
    be distributed across many machines.
  prefs: []
  type: TYPE_NORMAL
- en: This works great for a large cluster of machines but actually really hurts performance
    on a single machine that you might use for your unit-testing.
  prefs: []
  type: TYPE_NORMAL
- en: When you run a PySpark pipeline, spark will evaluate the entire pipeline and
    calculate an optimised ‚Äòplan‚Äô to perform the computation across a distributed
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The planning comes with significant overhead. This makes sense when you are
    processing terabytes of data on a distributed cluster of machines. But when working
    on a single machine on a small dataset it can be surprisingly slow. Especially
    compared with what you might have experienced with Pandas.
  prefs: []
  type: TYPE_NORMAL
- en: Without optimising your SparkSession configuration parameters your unit-tests
    will take an agonizingly long time to run.
  prefs: []
  type: TYPE_NORMAL
- en: Dependency on a Spark Session
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To run PySpark code in your unit-test, you need a SparkSession.
  prefs: []
  type: TYPE_NORMAL
- en: As stated above, ideally each test should be isolated from others and not require
    complex external objects. Unfortunately, there is no escaping the requirement
    to initiate a spark session for your unit-tests.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a spark session is the first hurdle to overcome when writing a unit-test
    for your PySpark pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: How should you create a SparkSession for your tests?
  prefs: []
  type: TYPE_NORMAL
- en: Initiating a new spark session for each test would dramatically increase the
    time to run the tests and introduce a ton of boiler-plate code to your tests.
  prefs: []
  type: TYPE_NORMAL
- en: Efficiently, creating and sharing a SparkSession across your tests is vital
    to keep the performance of your tests at an acceptable level.
  prefs: []
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Your tests will require input data.
  prefs: []
  type: TYPE_NORMAL
- en: There are two main problems with creating example data for big data pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: The first is size. Obviously, you cannot run your tests against the full dataset
    that will be used in production. You have to use a much smaller subset.
  prefs: []
  type: TYPE_NORMAL
- en: But, by using a small dataset, you run into the second problem which is providing
    enough test data to cover all the edge cases you want to handle.
  prefs: []
  type: TYPE_NORMAL
- en: It is *really* hard to mock realistic data for testing. There is not much you
    can do about this, however, you can use smaller, targeted datasets for your tests.
  prefs: []
  type: TYPE_NORMAL
- en: Steps to unit-test your PySpark code with Pytest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let‚Äôs work through an example of writing unit-tests for a PySpark pipline using
    [PyTest](https://docs.pytest.org/en/7.2.x/) .
  prefs: []
  type: TYPE_NORMAL
- en: '*üíª The full code is available in this* [*GitHub repository*](https://github.com/julian-west/e4ds-snippets/tree/master/pyspark/pyspark_unit_testing)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Example code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here is an example PySpark pipeline to process some bank transactions. In this
    scenario we want to take the raw transactions and classify them as debit account
    or credit account transactions by joining them to some reference data.
  prefs: []
  type: TYPE_NORMAL
- en: Each transaction record comes with an account ID. We will use this account ID
    to join to account information table which has information on whether this account
    ID is from a debit or credit account.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'There are few issues with this example pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: Difficult to read. Lots of complex logic in one place. For example, regex replacements,
    joining on substrings etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Difficult to test. Single function responsible for multiple actions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Difficult to reuse. The debit/credit classification is business logic which
    cannot easily be reused across the project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 1: Refactor into smaller logical units'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs first refactor the code into individual functions, then compose the functions
    together for the main `classify_debit_credit_transactions` function.
  prefs: []
  type: TYPE_NORMAL
- en: We can then write an test for each individual function to ensure it is behaving
    as expected.
  prefs: []
  type: TYPE_NORMAL
- en: While this increases the overall number of lines of code, it is easier to test
    and we can now reuse the functions across other parts of the project.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 3\. Creating a resuable SparkSession using Fixtures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before writing our unit-tests, we need to create a SparkSession which we can
    reuse across all our tests.
  prefs: []
  type: TYPE_NORMAL
- en: To do this, we create a [PyTest fixture](https://docs.pytest.org/en/6.2.x/fixture.html)
    in a `conftest.py` file.
  prefs: []
  type: TYPE_NORMAL
- en: Pytest fixtures are objects which are created once and then reused across multiple
    tests. This is particularly useful for complex objects like the SparkSession which
    have a significant overhead to create.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'It is important to set a number of configuration parameters in order to optimise
    the SparkSession for processing small data on a single machine for testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '`master = local[1]` ‚Äì specifies that spark is running on a local machine with
    one thread'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spark.executor.cores = 1` ‚Äì set number of cores to one'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spark.executor.instances = 1` - set executors to one'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spark.sql.shuffle.partitions = 1` - set the maximum number of partitions to
    1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spark.driver.bindAddress = 127.0.0.1` ‚Äì (optional) Explicitly specify the
    driver bind address. Useful if your machine has also has a live connection to
    a remote cluster'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These config parameters essentially tell spark that you are processing on a
    single machine and spark should not try to distribute the computation. This will
    save a significant amount of time in both the planning of the pipeline execution
    and the computation itself.
  prefs: []
  type: TYPE_NORMAL
- en: Note, it is recommended to `yield` the spark session instead of using `return`.
    Read the [PyTest documentation](https://docs.pytest.org/en/7.1.x/how-to/fixtures.html#yield-fixtures-recommended)
    for more information. Using `yield` also allows you to perform any clean up actions
    after your tests have run (e.g. deleting any local temp directories, databases
    or tables etc.).
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Creating unit-tests for the code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now let‚Äôs write some tests for our code.
  prefs: []
  type: TYPE_NORMAL
- en: 'I find it most efficient to organise my PySpark unit tests with the following
    structure:'
  prefs: []
  type: TYPE_NORMAL
- en: Create the input dataframe
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create the output dataframe using the function we want to test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specify the expected output values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compare the results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I also try to ensure the test covers positive test cases and at least one negative
    test case.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We now have unit-test for each component in the PySpark pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: As each test reuses the same SparkSession the overhead of running multiple tests
    is significantly reduced.
  prefs: []
  type: TYPE_NORMAL
- en: Further tips for unit testing PySpark code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Create test dataframes with the minimum required information
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When creating dataframes with test data, only create columns relevant to the
    transformation.
  prefs: []
  type: TYPE_NORMAL
- en: You only really need to create data with columns required for the function.
    You don‚Äôt need all the other columns which might be present in the production
    data.
  prefs: []
  type: TYPE_NORMAL
- en: This helps write concise functions and is more readable as it is clear which
    columns are required and impacted by the function. If you find you need a big
    dataframe with many columns in order to carry out a transformation you are probably
    trying to do too much at once.
  prefs: []
  type: TYPE_NORMAL
- en: This is just a guideline, your own usecase might require more complicated test
    data, but if possible keep it small, concise and localised to the test.
  prefs: []
  type: TYPE_NORMAL
- en: Remember to call an ‚Äòaction‚Äô in order to trigger the PySpark computation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PySpark uses lazy evaluation. You need to call an ‚Äòaction‚Äô (e.g. `collect`,
    `count` etc.) during your test in order to compute a result that you can compare
    to the expected output.
  prefs: []
  type: TYPE_NORMAL
- en: Don‚Äôt run all PySpark tests if you don‚Äôt need to
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PySpark tests generally take longer than normal unit tests to run as there is
    overhead to calculate a computation plan and then execute it.
  prefs: []
  type: TYPE_NORMAL
- en: During development, make use of some of Pytest‚Äôs features such as the `-k` flag
    to run single tests or just run tests in a single file. Then only run the full
    test suite before committing your code.
  prefs: []
  type: TYPE_NORMAL
- en: Keep the unit-tests isolated
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Be careful not to modify your spark session during a test (e.g. creating a table,
    but not deleting it afterwards).
  prefs: []
  type: TYPE_NORMAL
- en: The table will be persisted across all tests which may interfere with their
    expected behaviour.
  prefs: []
  type: TYPE_NORMAL
- en: Try and keep the creation of data close to where it is used.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You could use Pytest fixtures to share dataframes or even load test data from
    csv files etc. across multiple tests.
  prefs: []
  type: TYPE_NORMAL
- en: However, in my experience, it is easier and more readable to create data as
    required for each individual test.
  prefs: []
  type: TYPE_NORMAL
- en: Test positive and negative outcomes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For example, when testing a joining condition you should include data which
    should not satisfy the join condition. This helps ensure you are excluding the
    right data as well as including the right data.
  prefs: []
  type: TYPE_NORMAL
- en: This blog was originally published on [engineeringfordatascience.com](https://engineeringfordatascience.com/posts/pyspark_unit_testing_with_pytest/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'If you use Pytest for your unit testing, check out my other article with some
    useful tips for using Pytest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/13-tips-for-using-pytest-5341e3366d2d?source=post_page-----b5ab2fd54415--------------------------------)
    [## 13 Tips for using PyTest'
  prefs: []
  type: TYPE_NORMAL
- en: Unit-testing is a really important skill for software development. There are
    some great Python libraries to help us‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/13-tips-for-using-pytest-5341e3366d2d?source=post_page-----b5ab2fd54415--------------------------------)
  prefs: []
  type: TYPE_NORMAL
