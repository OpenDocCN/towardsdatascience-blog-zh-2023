- en: 'ONNX: The Standard for Interoperable Deep Learning Models'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ONNX：用于可互操作深度学习模型的标准
- en: 原文：[https://towardsdatascience.com/onnx-the-standard-for-interoperable-deep-learning-models-a47dfbdf9a09](https://towardsdatascience.com/onnx-the-standard-for-interoperable-deep-learning-models-a47dfbdf9a09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/onnx-the-standard-for-interoperable-deep-learning-models-a47dfbdf9a09](https://towardsdatascience.com/onnx-the-standard-for-interoperable-deep-learning-models-a47dfbdf9a09)
- en: '![](../Images/889af96423dbf877991bb0f406706a90.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/889af96423dbf877991bb0f406706a90.png)'
- en: Photo by [Jonny Caspari](https://unsplash.com/@jonnyuiux?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Jonny Caspari](https://unsplash.com/@jonnyuiux?utm_source=medium&utm_medium=referral)在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)上提供
- en: Learn about the benefits of using the ONNX standard for deploying models across
    frameworks and hardware platforms
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解使用ONNX标准在框架和硬件平台之间部署模型的好处
- en: '[](https://medium.com/@marcellopoliti?source=post_page-----a47dfbdf9a09--------------------------------)[![Marcello
    Politi](../Images/484e44571bd2e75acfe5fef3146ab3c2.png)](https://medium.com/@marcellopoliti?source=post_page-----a47dfbdf9a09--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a47dfbdf9a09--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a47dfbdf9a09--------------------------------)
    [Marcello Politi](https://medium.com/@marcellopoliti?source=post_page-----a47dfbdf9a09--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@marcellopoliti?source=post_page-----a47dfbdf9a09--------------------------------)[![Marcello
    Politi](../Images/484e44571bd2e75acfe5fef3146ab3c2.png)](https://medium.com/@marcellopoliti?source=post_page-----a47dfbdf9a09--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a47dfbdf9a09--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a47dfbdf9a09--------------------------------)
    [Marcello Politi](https://medium.com/@marcellopoliti?source=post_page-----a47dfbdf9a09--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a47dfbdf9a09--------------------------------)
    ·5 min read·Jan 24, 2023
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----a47dfbdf9a09--------------------------------)
    ·5分钟阅读·2023年1月24日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: The first time I heard about ONNX was during my internship at INRIA. I was working
    to develop Neural Network Pruning algorithms in the Julia language. There weren’t
    many pre-trained models yet that I could use, so utilizing ONNX to import models
    developed with other languages and frameworks might have been a solution.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我第一次听说ONNX是在INRIA实习期间。我当时在用Julia语言开发神经网络剪枝算法。那时还没有很多预训练模型可以使用，因此利用ONNX导入其他语言和框架开发的模型可能是一个解决方案。
- en: In this article, I want to introduce ONNX and explain its enormous potential
    by also seeing a practical example.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我想介绍ONNX，并通过一个实际示例来解释其巨大的潜力。
- en: What is ONNX?
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ONNX是什么？
- en: ONNX, or Open Neural Network Exchange, is an open-source standard for representing
    deep learning models. It was developed by Facebook and Microsoft in order to make
    it easier for researchers and engineers to move models between different deep-learning
    frameworks and hardware platforms.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ONNX，即开放神经网络交换，是一个用于表示深度学习模型的开源标准。它由Facebook和Microsoft开发，旨在使研究人员和工程师能够更轻松地在不同的深度学习框架和硬件平台之间迁移模型。
- en: One of the main advantages of ONNX is that it allows models to be easily **exported
    from one framework, such as PyTorch, and imported into another framework, such
    as TensorFlow**. This can be especially useful for researchers who want to try
    out different frameworks for training and deploying their models, or for engineers
    who need to deploy models on different hardware platforms.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ONNX的一个主要优势是它允许模型轻松地**从一个框架（如PyTorch）导出，并导入到另一个框架（如TensorFlow）**。这对于那些想尝试不同框架来训练和部署模型的研究人员，或者需要在不同硬件平台上部署模型的工程师尤其有用。
- en: '![](../Images/c9e402a5fd921c63034240ae17e04d24.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c9e402a5fd921c63034240ae17e04d24.png)'
- en: Frameworks Interoperability (Image By Author)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 框架互操作性（图片由作者提供）
- en: ONNX also provides a set of tools for optimizing and quantizing models, which
    can help to reduce the memory and computational requirements of the model. This
    can be especially useful for deploying models on edge devices and other resource-constrained
    environments.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ONNX还提供了一套工具，用于优化和量化模型，这有助于减少模型的内存和计算需求。这对于在边缘设备和其他资源受限环境中部署模型尤其有用。
- en: Another important feature of ONNX is that it is supported by a wide range of
    companies and organizations. This includes not only Facebook and Microsoft, but
    also companies like Amazon, NVIDIA, and Intel. This wide range of support ensures
    that ONNX will continue to be actively developed and maintained, making it a robust
    and stable standard for representing deep learning models.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个ONNX的重要特点是它得到了广泛的公司和组织的支持。这不仅包括Facebook和Microsoft，还包括像Amazon、NVIDIA和Intel这样的公司。这种广泛的支持确保了ONNX将继续得到积极开发和维护，使其成为一个稳健和稳定的深度学习模型表示标准。
- en: ONNX Runtime
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ONNX Runtime
- en: '**ONNX Runtime is an open-source inference engine for executing ONNX** (Open
    Neural Network Exchange) **models**. It is designed to be **high-performance**
    and lightweight, making it well-suited for **deployment on a wide range of hardware
    platforms**, including edge devices, servers, and cloud services.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**ONNX Runtime是一个开源推断引擎，用于执行ONNX**（开放神经网络交换）**模型**。它被设计为**高性能**且轻量级，使其非常适合**在各种硬件平台上部署**，包括边缘设备、服务器和云服务。'
- en: The ONNX Runtime provides a C++ API, a C# API, and a Python API for executing
    ONNX models. It also provides support for multiple backends, including CUDA and
    OpenCL, which allows it to run on a wide range of hardware platforms, such as
    NVIDIA GPUs and Intel CPUs.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ONNX Runtime提供了C++ API、C# API和Python API来执行ONNX模型。它还支持多种后端，包括CUDA和OpenCL，这使得它可以在各种硬件平台上运行，如NVIDIA
    GPUs和Intel CPUs。
- en: ONNX Runtime can be very useful since you can use models in inference with a
    single framework no matter what hardware you are going to use. So without having
    to actually rewrite the code depending on whether we want to use a CPU, GPU, FPGA
    or whatever!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ONNX Runtime非常有用，因为你可以在任何硬件上使用模型进行推断，无论你使用的是CPU、GPU、FPGA还是其他设备，而无需实际重写代码！
- en: '![](../Images/28c0d0696ea2096db955d905348c868c.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/28c0d0696ea2096db955d905348c868c.png)'
- en: ONNX Runtime (Image By Author)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ONNX Runtime（图片来源于作者）
- en: One of the main advantages of ONNX Runtime is its performance. It uses various
    techniques such as Just-In-Time (JIT) compilation, kernel fusion and subgraph
    partitioning to optimize the performance of the model. It also supports thread
    pooling and inter-node communication for distributed deployment which makes it
    a suitable choice for large-scale deployment.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ONNX Runtime的主要优点之一是其性能。它使用多种技术，如即时编译（JIT）、内核融合和子图分区来优化模型性能。它还支持线程池和节点间通信进行分布式部署，使其成为大规模部署的合适选择。
- en: I will explain all these advanced features in future articles!
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在未来的文章中解释所有这些高级功能！
- en: ONNX Runtime also provides support for a wide range of models, including both
    traditional machine learning models and deep learning models. This makes it a
    versatile inference engine that can be used in a wide range of applications, from
    computer vision and natural language processing to speech recognition and autonomous
    vehicles.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ONNX Runtime还支持多种模型，包括传统的机器学习模型和深度学习模型。这使得它成为一个多功能的推断引擎，可以用于从计算机视觉和自然语言处理到语音识别和自动驾驶等各种应用。
- en: Let’s code!
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我们开始编码吧！
- en: Let’s look at an example now, where we **create a Machine Learning model using**
    the classic **scikit-learn,** and then **convert** this model **to ONNX** format
    so that we can **use it with ONNX Runtime**.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一个示例，我们将**使用**经典的**scikit-learn**来**创建一个机器学习模型**，然后**将**这个模型**转换**为ONNX格式，以便我们可以**与ONNX
    Runtime一起使用**。
- en: First, we import the necessary libraries, pull a model into sklearn and export
    to the classic pickle format. We will use the iris dataset.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入必要的库，将模型拉入sklearn并导出为经典的pickle格式。我们将使用鸢尾花数据集。
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now that we have trained and saved the model, we can reimport it and convert
    it to an ONNX model. **Each framework will have its own conversion library**.
    So you will have to use another one if you developed your model in PyTorch or
    TensorFlow for example. In this case the library is called **skl2onnx**.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练并保存了模型，我们可以重新导入它并将其转换为ONNX模型。**每个框架都有其自己的转换库**。因此，如果你是在PyTorch或TensorFlow中开发的模型，你需要使用其他库。在这种情况下，库叫做**skl2onnx**。
- en: So we import the necessary libraries.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们导入了必要的库。
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now we can finally convert. We should specify the inital_type though, and then
    we could create a file called *model.onnx* where we will save the onnx model.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们终于可以进行转换了。我们应该指定`inital_type`，然后可以创建一个名为*model.onnx*的文件，用于保存onnx模型。
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now that we have the model in ONNX format we can import it and use it on some
    data to make inferences.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了ONNX格式的模型，我们可以导入它，并在一些数据上使用它进行推理。
- en: We then install ONNX Runtime.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们安装ONNX Runtime。
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now we create data, and import the model, thus creating a session. We specify
    the input and output name (label), and run the session on the data!
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们创建数据，并导入模型，从而创建一个会话。我们指定输入和输出名称（标签），然后在数据上运行会话！
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Well, you got your results by leveraging ONNX Runtime. It only took a few simple
    commands!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，你通过利用ONNX Runtime得到了结果。这只需要几个简单的命令！
- en: This is just an introduction to ONNX you can certainly do much more, but I hope
    you found this example useful.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是对ONNX的一个介绍，你当然可以做更多，但我希望你发现这个例子有用。
- en: Final Thoughts
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最后的想法
- en: ONNX is an open-source standard that makes it easy to move deep learning models
    between different frameworks and hardware platforms. It provides a set of tools
    for optimizing and quantizing models, and it is supported by a wide range of companies
    and organizations. As a result, ONNX is becoming an important standard for deep
    learning, making it easy to share models and deploy them across different platforms.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ONNX 是一个开源标准，它使得在不同框架和硬件平台之间移动深度学习模型变得容易。它提供了一套优化和量化模型的工具，并且得到了众多公司和组织的支持。因此，ONNX
    正在成为深度学习的重要标准，使得共享模型和跨平台部署变得简单。
- en: The End
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结束
- en: '*Marcello Politi*'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*Marcello Politi*'
- en: '[Linkedin](https://www.linkedin.com/in/marcello-politi/), [Twitter](https://twitter.com/_March08_),
    [CV](https://march-08.github.io/digital-cv/)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[Linkedin](https://www.linkedin.com/in/marcello-politi/)，[Twitter](https://twitter.com/_March08_)，[CV](https://march-08.github.io/digital-cv/)'
