["```py\nregion = ['APAC', 'EU', 'NORTHAM', 'MENA', 'AFRICA']\nuser_type = ['premium', 'free']\n\nordinal_list = ['region', 'user_type']\n```", "```py\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n\ndef var_encoding(X, cols, ordinal_list, encoding):\n\n    #Function to encode ordinal variables\n    if encoding == 'ordinal_ordered': \n        encoder = OrdinalEncoder(categories=ordinal_list) \n        encoder.fit(X.loc[:, cols])\n        X.loc[:, cols] = encoder.transform(X.loc[:, cols])\n\n    #Function to encode categorical variables    \n    elif encoding == 'ordinal_unordered':\n        encoder = OrdinalEncoder()\n        encoder.fit(X.loc[:, cols])\n        X.loc[:, cols] = encoder.transform(X.loc[:, cols])\n\n    else:     \n        encoder = OneHotEncoder(handle_unknown='ignore')\n        encoder.fit(df.loc[:, cols])\n        df.loc[:, cols] = encoder.transform(df.loc[:, cols])\n\n    return X \n```", "```py\ndef encoding_vars(X, ordinal_cols, ordinal_list, preprocessing_categoricals=False):\n\n    #Encode ordinal variables\n    df = var_encoding(df, ordinal_cols, ordinal_list, 'ordinal_ordered')\n\n    #Encode categorical variables\n    if preprocessing_categoricals: \n       df = var_encoding(df, categorical_cols, 'ordinal_unordered')    \n\n    #Else set your categorical variables as 'category' if needed\n    else:\n        for cat in categorical_cols: \n            X[cat] = X[cat].astype('category')\n\n    #Rename your variables as such if needed to keep track of the order\n    #An encoded feature such as region will no longer show female or male, but 0 or 1\n    df.rename(columns={'user_type': 'free_0_premium_1'},   \n    df.reset_index(drop=True, inplace=True)   \n\n    return df\n```", "```py\ndef split_df(df, ordinal_cols, ordinal_list, target):\n\n    #Splitting train and test\n    splitter = GroupShuffleSplit(test_size=.13, n_splits=2, random_state=7)\n\n    #If you're dealing with many rows belonging to the same id then make sure to split based on the same id\n    split = splitter.split(df, groups=df['user_id'])\n    train_inds, test_inds = next(split)\n\n    train = df.iloc[train_inds]\n    test = df.iloc[test_inds]\n\n    #Splitting val and test data\n    splitter2 = GroupShuffleSplit(test_size=.5, n_splits=2, random_state=7)\n    split = splitter2.split(test, groups=test['user_id'])\n    val_inds, test_inds = next(split)\n\n    val = test.iloc[val_inds]\n    test = test.iloc[test_inds]\n\n    #Defining your X (predictive features) and y (target_feature)\n    X_train = train.drop(['target_feature'], axis=1)\n    y_train = train.target_feature\n\n    X_val = val.drop(['target_feature'], axis=1)\n    y_val = val.target_feature\n\n    X_test = test.drop(['target_feature'], axis=1)\n    y_test = test.target_feature\n\n    #Encoding the variables in the sets\n    X_train = encoding_vars(X_train, ordinal_cols, ordinal_list)\n    X_val = encoding_vars(X_val, ordinal_cols, ordinal_list)\n    X_test = encoding_vars(X_test, ordinal_cols, ordinal_list)\n\n    return X_train, y_train, X_val, y_val, X_test, y_test\n```", "```py\nX_train, y_train, X_val, y_val, X_test, y_test = split_df(df, ordinal_cols, ordinal_list, target='target_feature')\n```", "```py\nimport lightgbm as lgb\n#Build your model\nclf =  lgb.LGBMClassifier(objective='binary', max_depth=-1, random_state=314, metric='roc_auc', n_estimators=5000, num_threads=16, verbose=-1,\n                           **best_hyperparameters)\n\n#Fit your model\nclf.fit(X_train, y_train, eval_set=(X_val, y_val), eval_metric='roc_auc')\n\n#Make the predictions\nroc_auc_score(y_test, clf.predict(X_test)) \n```", "```py\ndef shap_viz(model, X_val, title, target):\n\n    #Prepare the features you want to evaluate\n    my_features= [**List of features you want to evaluate**]\n    my_features_idx = list(np.flatnonzero(X_val.columns.isin(my_features)))\n\n    #Define your metric\n    class_names = ['happy_:)', 'not_happy_:(']\n\n    #Load the SHAP explainer, since LightGBM is a decision tree, I used TreeExplainer\n    explainer = shap.TreeExplainer(model)\n    #We train the SHAP values on the features based on the validation set\n    shap_values = explainer.shap_values(X_val) \n\n    plt.figure(figsize=(18, 6))\n    plt.subplot(1,2,1)\n    shap.summary_plot(shap_values, X_val.values, plot_type='bar', class_names=class_names, feature_names=X_val.columns, max_display=20, show=False, plot_size=None)\n    plt.title(f'{title} – Weight of the impact of each feature')\n\n    plt.subplot(1,2,2)\n    #shap_values[n] evaluates the impact of features for predicting satisfaction\n    #n = 0 -- not happy, n = 1 -- happy\n    shap.summary_plot(shap_values[1], X_val.values, feature_names=X_val.columns, max_display=20, show=False,  plot_size=None)\n    plt.title(f'{title} – Directional impact of each feature')\n    plt.tight_layout()\n\n    #To save a picture of your SHAP viz\n    plt.savefig(f'{title}.png', bbox_inches='tight', dpi=100)\n\n    plt.show()\n\n    #To print a list of the top 10 most impactful features\n    vals = np.abs(shap_values[0]).mean(0)\n    feature_importance = pd.DataFrame(list(zip(X_val.columns, vals)), columns=['col_name', 'feature_importance_vals'])\n    feature_importance.sort_values(by=['feature_importance_vals'], ascending=False, inplace=True)\n    for i in list(feature_importance.col_name.head(10)):\n            print(i)\n    return shap_values\n```", "```py\nyour_shap = shap_viz(clf, X_val, title='insert_title', target='target_feature')\n```"]