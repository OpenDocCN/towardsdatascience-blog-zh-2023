- en: Training of Robotic Manipulators on the Obstacle Avoidance task through Reinforcement
    Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/training-of-robotic-manipulators-on-the-obstacle-avoidance-task-through-reinforcement-learning-ea2a3404883f](https://towardsdatascience.com/training-of-robotic-manipulators-on-the-obstacle-avoidance-task-through-reinforcement-learning-ea2a3404883f)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Easily train a Robotic Manipulator to avoid obstacles by using the **robotic-maninpulator-rloa**
    framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@JavierMtz5?source=post_page-----ea2a3404883f--------------------------------)[![Javier
    Martínez Ojeda](../Images/5b5df4220fa64c13232c29de9b4177af.png)](https://medium.com/@JavierMtz5?source=post_page-----ea2a3404883f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ea2a3404883f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ea2a3404883f--------------------------------)
    [Javier Martínez Ojeda](https://medium.com/@JavierMtz5?source=post_page-----ea2a3404883f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ea2a3404883f--------------------------------)
    ·7 min read·Mar 15, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d857dfac9d0b252ac6b3ded3ae4c6d7.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [David Levêque](https://unsplash.com/ko/@davidleveque?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: If you want to read this article without a Premium Medium account, you can do
    it from this friend link :)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[https://www.learnml.wiki/training-of-robotic-manipulators-on-the-obstacle-avoidance-task-through-reinforcement-learning/](https://www.learnml.wiki/training-of-robotic-manipulators-on-the-obstacle-avoidance-task-through-reinforcement-learning/)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The purpose of this article, beyond explaining how to train a manipulator in
    the obstacle avoidance task, is to introduce the open-source framework **robotic-manipulator-rloa**
    and to explain how it works and how to use it.
  prefs: []
  type: TYPE_NORMAL
- en: In short, the **robotic-manipulator-rloa** framework allows the user to load
    a URDF or SDF model of a Robotic Manipulator (URDF or SDF files are XML file formats
    used to describe all elements of a robot) in a simulation environment, which will
    be used to train the manipulator to avoid hitting an obstacle while trying to
    reach a specific point in space. In order to make the training customizable, the
    user can configure the basic components of the environment, as well as the hyperparameters
    of the NAF algorithm, which is the algorithm used for training.
  prefs: []
  type: TYPE_NORMAL
- en: 'The NAF algorithm was previously explained at a theoretical level in this article:
    [Applied Reinforcement Learning V: Normalized Advantage Function (NAF) for Continuous
    Control](https://medium.com/towards-data-science/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095?source=post_page-----ea2a3404883f--------------------------------)
    [## Applied Reinforcement Learning V: Normalized Advantage Function (NAF) for
    Continuous Control'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction and explanation of the NAF algorithm, widely used in continuous
    control tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095?source=post_page-----ea2a3404883f--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Install the robotic-manipulator-rloa framework via PyPI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The installation of the PyPi package will install the following packages as
    dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**torch:** PyTorch is used to contruct the neural networks required for the
    NAF algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**numpy:** NumPy is used to work with numbers and arrays in an optimized way.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**pybullet:** PyBullet is used to create the simulation of the training environment,
    and to interact with and obtain information from it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**matplotlib:** Matplotlib is used to visualize the rewards obtained by the
    agent along the training process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the framework has been installed, a training and evaluation of the agent
    can be configured and executed in **5** **steps**, which are presented in the
    following sections.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Initialize ManipulatorFramework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Create an object of the **ManipulatorFramework** class.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The constructor of the ManipulatorFramework class initializes the hyperparameters
    of the NAF algorithm with the default values:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Buffer Size** defaults to 100000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch Size** defaults to 128.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gamma** defaults to 0.99.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tau** defaults to 0.001,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning Rate** defaults to 0.001,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Update Frequency** defaults to 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of Updates** defaults to 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In case you want to configure any of the hyperparameters of the NAF algorithm,
    the ManipulatorFramework class has a **set_hyperparameter()** method, which receives
    as parameters the name of the hyperparameter to be modified and the new value
    to be set, as shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 2\. Initialize the Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **Environment** needs some information from the user to configure the training
    simulation. More specifically, the fields required in the initialization of the
    Environment class are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**manipulator_file**: path to the manipulator’s URDF or SDF file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**endeffector_index**: index of the manipulator’s end-effector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**fixed_joints**: list containing the indices of every joint not involved in
    the training (joints that will be static during training).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**involved_joints**: list containing the indices of every joint involved in
    the training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**target_position**: list containing the position of the target object, as
    3D Cartesian coordinates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**obstacle_position**: list containing the position of the obstacle, as 3D
    Cartesian coordinates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**initial_joint_positions**: list containing as many items as the number of
    joints of the manipulator. Each item in the list corresponds to the initial position
    wanted for the joint with that same index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**initial_positions_variation_range**: list containing as many items as the
    number of joints of the manipulator. Each item in the list corresponds to the
    variation range (initial_pos-range, initial_pos+range) wanted for the joint with
    that same index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**max_force**:maximum force to be applied on the joints.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**visualize**: boolean indicating whether or not to visualize the simulated
    environment during training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Taking into account these parameters, the initialization of the environment
    would be carried out by calling the **initialize_environment()** method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 3\. Initialize NAF Agent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The NAF Agent is initialized in a very simple way, since most of the parameters
    it needs are taken from the previously configured Environment and from the hyperparameters
    initialized and configured in **Step 1**.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, the only parameter received by the **initialize_naf_agent()** method,
    which is used to initialize the NAF Agent, is **checkpoint_frequency**, which
    determines how often the training status will be saved. This parameter defaults
    to 500.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 4\. Run training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once the **Environment** and **NAF Agent** have been configured and initialized,
    training can proceed. This step is carried out by means of the **run_training()**
    method, which receives three parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**episodes**: maximum number of episodes to execute for the training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**frames**: maximum number of timesteps to execute per episode.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**verbose**: boolean indicating whether the verbose mode is activated or not.
    If verbose mode in on, information of each timestep will be printed on terminal.
    It is recommended to use this mode only in debug and development phases, as in
    the context of a normal training it adds too much information and prevents to
    see the training status in a clear way.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 5\. Run testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once the training has finished, it will be possible to test the agent to evaluate
    if it has managed to learn the task. To do this, use is made of the **test_trained_model()**
    method, which receives 2 parameters as for the **run_training()** method: the
    number of episodes and timesteps per episode that you want to run in the test.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The test process can also be carried out without having previously executed
    a training session, by loading the trained weights from a previous training session.
    These trained weights can be obtained from the checkpoints that the framework
    generates throughout the training, or from the *‘model.p’* file that is generated
    at the end of a training session. To execute a test on weights trained in a previous
    training, it will be also necessary to initialize the Environment and the NAF
    Agent in the same way that it was done when those weights were trained.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: It is important to note that when the trained weights are loaded from one of
    the checkpoints, it will be necessary to pass as a parameter the episode from
    which the weights are to be obtained. As an example, the code above gets the weights
    of the checkpoint generated for episode 2000.
  prefs: []
  type: TYPE_NORMAL
- en: Training Execution Information
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section explains how the information about the training is displayed during
    the execution of a training session.
  prefs: []
  type: TYPE_NORMAL
- en: If **visualize** is set to True when initializing the Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If the **visualize** parameter is set to True during the initialization of the
    **Environment**, then during the execution of the training the manipulator will
    be visualized, as shown in *Figure 1*. While this mode allows you to see that
    the manipulator movements are good and that the training is proceeding as expected,
    visualization slows down the execution of the program, so it should be used in
    development and not in the execution of a training with many episodes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fc4265da15bf81a0644df1a7ab80ec79.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 1**. Visualization of the training. Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: If verbose is set to True when initializing the Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If the **verbose** parameter is set to True during the **Environment** initialization,
    then information about each of the timesteps of each episode will be displayed,
    as shown in *Figure 2*. Considering that a normal training can last more than
    1000 episodes, and that each episode can consist of 500 timesteps, displaying
    the information for each timestep floods the terminal stdout, which can make it
    difficult to access certain information, and can be counterproductive in the context
    of a long training.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1e50d880ca03796cb76527ec0e6fd143.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 2**. Terminal logs of a training when verbose is True. Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: If verbose is set to False when initializing the Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If the **verbose** parameter is set to False during the **Environment** initialization,
    then only information about each episode will be displayed, as shown in *Figure
    3*. This type of execution is much more recommendable for a long training, since
    it allows to easily obtain the information of each one of the episodes completed
    during the training.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f8f893eaf26d43f7808c1343b5fddbef.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 3**. Terminal logs of a training when verbose is False. Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: '**Demo Training and Testing**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **robotic-manipulator-rloa** package allows running training and testing
    demos for [*KUKA IIWA*](https://www.kuka.com/es-es/productos-servicios/sistemas-de-robot/robot-industrial/lbr-iiwa)
    and [*Xarm6*](https://www.ufactory.cc/product-page/ufactory-xarm-6) robot manipulators,
    in order to show the user the necessary steps to run and evaluate a training with
    two different environment configurations. These trainings and tests are executed
    as shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: More information about the package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: More information about the **robotic-manipulator-rloa** package can be found
    in the documentation, or from the project page on PyPI. As for the code, it can
    be found in my GitHub repository, so if anyone has any improvements and proposals
    for the project, feel free to let me know.
  prefs: []
  type: TYPE_NORMAL
- en: '**Documentation**: [https://javiermtz5.github.io/robotic_manipulator_rloa/](https://javiermtz5.github.io/robotic_manipulator_rloa/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PyPI page**: [https://pypi.org/project/robotic-manipulator-rloa/](https://pypi.org/project/robotic-manipulator-rloa/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GitHub repository**: [https://github.com/JavierMtz5/robotic_manipulator_rloa](https://github.com/JavierMtz5/robotic_manipulator_rloa)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I hope that the framework is helpful, easy to use and that it manages to train
    with good results the different manipulator robots that are going to be loaded.
    If this has been the case, or if anyone runs into any problems or unexpected behavior,
    I would really appreciate it if you would let me know on the [GitHub repository](https://github.com/JavierMtz5/robotic_manipulator_rloa),
    either by starring the project or by opening an issue with the problem found.
  prefs: []
  type: TYPE_NORMAL
