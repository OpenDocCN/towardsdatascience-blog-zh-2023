- en: Unsupervised Learning Series —Exploring Self-Organizing Maps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/unsupervised-learning-series-exploring-self-organizing-maps-fe2efde9f7a1](https://towardsdatascience.com/unsupervised-learning-series-exploring-self-organizing-maps-fe2efde9f7a1)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learn how Self-Organizing Maps work and why they are a useful unsupervised learning
    algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ivopbernardo.medium.com/?source=post_page-----fe2efde9f7a1--------------------------------)[![Ivo
    Bernardo](../Images/39887b6f3e63a67c0545e87962ad5df0.png)](https://ivopbernardo.medium.com/?source=post_page-----fe2efde9f7a1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fe2efde9f7a1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fe2efde9f7a1--------------------------------)
    [Ivo Bernardo](https://ivopbernardo.medium.com/?source=post_page-----fe2efde9f7a1--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fe2efde9f7a1--------------------------------)
    ·16 min read·Aug 6, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5f541e88bf23d4f4e488dda83aa7536d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [teckhonc](https://unsplash.com/pt-br/@teckhonc) @Unsplash.com
  prefs: []
  type: TYPE_NORMAL
- en: '**Self-Organizing Maps (SOMs) are a type of unsupervised neural network utilized
    for clustering** and **visualization of high-dimensional data**. SOMs are trained
    using a competitive learning algorithm, in which nodes (also known as neurons)
    in the network compete for the right to represent input data.'
  prefs: []
  type: TYPE_NORMAL
- en: The SOM architecture consists of a 2D grid of nodes, where each node is associated
    with a weight vector that represents the means of the centroids in the SOM solution.
    The nodes are organized in such a way that nodes are organized around similar
    data points, producing a layer that represents the underlying data.
  prefs: []
  type: TYPE_NORMAL
- en: 'SOMs are commonly used for a wide array of tasks such as:'
  prefs: []
  type: TYPE_NORMAL
- en: data visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: anomaly detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: feature extraction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**We can also visualize SOMs as the most simple neural network version for
    unsupervised learning!**'
  prefs: []
  type: TYPE_NORMAL
- en: 'While they seem confusing at first, Self-Organizing Maps (or Kohonen Maps,
    named after their [inventor](https://en.wikipedia.org/wiki/Teuvo_Kohonen)) are
    one interesting type of algorithm that is able to map the underlying structure
    from the data. They can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: a one-layer unsupervised neural network, without backpropagation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a restricted *k-means* solution, where nodes have the ability to influence the
    movement of other nodes (in the context of k-means, the nodes are known as centroids).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this blog post, we’ll do a couple of experiments on the SOM model. Later,
    we’ll apply a Self-Organizing Map to a real use case, where we will be able to
    see the main features and shortcomings of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how SOMs Learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand how SOMs learn, let’s start by plotting a toy dataset in 2 dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll create a `numpy`array with the following dataset and plot it afterwards:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ecbedb622d04b6f6b3854fe98eb3e165.png)'
  prefs: []
  type: TYPE_IMG
- en: Toy Dataset Plotting — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: These 8 data points are arranged using *X* and *Y* axis, representing arbitrary
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: How do we start to train a Self-Organizing Map on this dataset? **The first
    important feature of SOMs is that it is an algorithm that relies on several hyper-parameters.**
    It’s very important to understand how they work and how they impact the learning
    process.
  prefs: []
  type: TYPE_NORMAL
- en: The first set of hyper-parameters defines the size of the SOM, commonly called
    *Lattice.* This value is similar to *k-means’s* number of centroids and to a neural
    networks layer *number of neurons*. It provides the number of elements that we
    are going to use to reduce our data points.
  prefs: []
  type: TYPE_NORMAL
- en: For this example, we are going to build a simple 2 by 2 *lattice*. This means
    that we will have 4 centroids (neurons) that we will use to represent our full
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning 1: The size of the lattice is one of SOM’s hyper-parameters'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Let’s plot them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/71d89512276db5bb873e70f465d48cb2.png)'
  prefs: []
  type: TYPE_IMG
- en: 2 by 2 Lattice — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'In orange, we can see our *Lattice* being represented as centroids in our data.
    We can also represent these data points in “*Hidden Layer*” format, as if we are
    speaking of a Neural Network, where we can imagine that each neuron is represented
    by a value (weight) from the *X* and *Y* features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5e9502fafcd8577e9f44c62d5b72183a.png)'
  prefs: []
  type: TYPE_IMG
- en: 2 by 2 Lattice in Network Format — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Black lines represent the *weight* (value) of the *x*-axis and the red dashed
    line represent the *weight* (value) of the *y*-axis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning 2: We can speak about the SOM’s units as nodes (neural network terminology)
    or centroids (clustering terminology)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Step 1 of SOM Training: picking a random point and calculate the euclidean
    distance to each *Neuron/Centroid:*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/da7cb684cfa18cc5091184a4c1623ed7.png)'
  prefs: []
  type: TYPE_IMG
- en: First Step of SOM Training — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: This data point will choose the neuron with the shortest distance as the **Best
    Matching Unit (BMU).**
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning 3: Best Matching Unit is the unit that our data point chooses as the
    most similar to itself'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**This unit is the neuron that is going to be dragged towards our data point.**'
  prefs: []
  type: TYPE_NORMAL
- en: But.. is it the only node that is going to move in this training iteration?
    No! All other centroids are also going to move, **given how far they are from
    the BMU**. This is a significantly different from *K-Means*, where centroids don’t
    have the ability do drag other centroids in every iteration of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The **BMU** and other nodeswill be dragged towards the data point. This movement
    will happen using a proximity function that will work as the “strength” that we
    will apply when pushing the nodes of our SOM.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most usual ways to define this “proximity function” is to create
    a *distance based gaussian approach*. We start by calculating the distance between
    the nodes and the **BMU**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1bb1ae6226ca238a2e82bc08b570a7d5.png)'
  prefs: []
  type: TYPE_IMG
- en: Distance between nodes and BMU— Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we’re using the euclidean distance, where *distance = sqrt((x2-x1)²
    + (y2-y1)²).*
  prefs: []
  type: TYPE_NORMAL
- en: Now, we need to input this distance inside a gaussian kernel to consider the
    influence that the BMU will have on the other nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'This Gaussian Approximation will be based on the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/487e561a2f61b665f59d65ff98346bbc.png)'
  prefs: []
  type: TYPE_IMG
- en: Gaussian Kernel example — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The numerator is the euclidean distance between neurons and the denominator
    multiplies the standard-deviation (*σ*) by 2 . Keep in mind that this standard
    deviation is another hyperparameter of the algorithm!
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see this visually. If we consider a smaller standard deviation, a gaussian
    kernel similar to this one will “draw” the following circle around the **BMU:**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/859fd0787990a77950388457c54ef923.png)'
  prefs: []
  type: TYPE_IMG
- en: Gaussian Kernel around BMU — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The opacity of the shade on the image above is a proxy for how strong the nodes
    will be pulled towards the data point.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we choose a higher standard deviation, the influence of our BMU is larger
    and more nodes will be affected:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bda239e4813e6c9b69d2be9cb5bd3f36.png)'
  prefs: []
  type: TYPE_IMG
- en: Gaussian Kernel around BMU, larger standard deviation — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning 4: The Standard Deviation is an Hyper-Parameter that we can configure
    on the SOM training'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'I’ll call the result of the gaussian formula *howclose —* for example, the
    *howclose* between our BMU node (remember that it was chosen by the data point)
    and the other nodes is the following (using *σ=1)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0c2d3b0822b8d34c054d7d5530e3a795.png)'
  prefs: []
  type: TYPE_IMG
- en: howclose of BMU vs. each data point — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: We now have the **strength** that we will apply on each node when pulling it
    towards the data point. Regarding neighborhing nodes, the data point where we
    will apply most strength will be the node on the bottom-right corner. On the reverse,
    we will barely move the nodes in the top-right and top-left.
  prefs: []
  type: TYPE_NORMAL
- en: 'We know the strength that we will use to update our SOM but we still need the
    direction where we will move each node. That can be achieved with the update formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0c07053c88898b27da1c8ed7677a9fce.png)'
  prefs: []
  type: TYPE_IMG
- en: Node Value Update Formula — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '*wi,k* is the current coordinates of variables *i* of the node *k*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*howclose* is the strength that we’ve calculated before.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*learningrate* is another hyper-parameter of the SOM algorithm that is similar
    to other neural networks training process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*(xi-w i,k)* is the **direction to where we will push** the node. For example,
    in our example, all nodes will be dragged down as the data point sits below every
    node (on the y-variable).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The result of the calculation above will give us the new coordinates of the
    existing nodes in the SOM!** Another important detail: we don’t have a calculation
    for the weight to apply to the BMU (only to neighborhing nodes). **A common way
    to go around this is to apply maximum strength when dragging the BMU (output of
    gaussian function = 1).**'
  prefs: []
  type: TYPE_NORMAL
- en: 'What are the new coordinates for our nodes if we apply this logic? The calculations
    are available in the table below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c1124e28f70687ae02ca570b02bf175c.png)'
  prefs: []
  type: TYPE_IMG
- en: Node Value Update Summary— Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see the example for the bottom-right node:'
  prefs: []
  type: TYPE_NORMAL
- en: On the *x-axis* (first row of the table above), **the new coordinate of this
    point will be 2.53*,* instead of 3**. This is the result of applying the strength
    times the learning rate and updating the node towards the data point on the x
    variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the *y-axis* (fourth row of the table above), the new coordinate of this
    point will be 3.377*,* instead of 4\. This is the result of applying the strength
    times the learning rate and updating the node towards the data point on the y
    variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, notice that the movement on the nodes on the top-right and left
    are negligible. Why? Because the *howclose* applies a low strength when pushing
    these nodes towards the data points.
  prefs: []
  type: TYPE_NORMAL
- en: 'How will our BMU move? Naturally, it will be dragged almost to the top of our
    data point:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/010e24eab3d39069dee9a85acc015885.png)'
  prefs: []
  type: TYPE_IMG
- en: Updating BMU— Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the coordinates we’ve obtain on the *Updated Value* table above*,*
    let’s view our “new” SOM:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c344db6460463b91ff8bc881c10591ae.png)'
  prefs: []
  type: TYPE_IMG
- en: New Nodes in the SOM — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'The purple nodes are the updated “SOM” nodes. Our self organizing map is trying
    to mimic the underlying data points by adjusting itself based on the random point
    we chose. Let’s see what happens in the 3 dimensional view:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f7e59b5cc04fd102d16be966bd50d19d.png)'
  prefs: []
  type: TYPE_IMG
- en: New Nodes in the SOM — 3D View — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Notice that our SOM moved a bit when we compare it with the original random
    network and the new black and dashed lines represent the new weights of the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5e9502fafcd8577e9f44c62d5b72183a.png)'
  prefs: []
  type: TYPE_IMG
- en: Original Random SOM — 3D View — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Our SOM will do this process for all the data points available in the dataset
    (adjust itself for each pass). Remembering each step:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Calculate the BMU* (best matching unit) using a distance metric.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Calculate the kernel around the BMU* to know how to pull other nodes in the
    SOM toward the data point.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Pull the nodes using a combination of strength, learning rate and direction.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Move to the next point and repeat.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Just like other neural networks model, the number of epochs in a SOM is another
    hyperparameter we need to define. An epoch consists of a single pass through the
    entire dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning 5: The SOM will continue to adjust itself until it reaches the number
    of epochs defined by the user.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In Python, we can use the *minisom* package to train a Self-Organizing-Map:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The parameters of the `Minisom` function are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first three numbers represent the dimensions of the SOM: number of nodes
    in the *x-axis*, number of nodes in *y-axis,* and number of variables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sigma contains the starting standard deviation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then we have the starting learning rate of the algorithm (in this case, set
    to 1).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`neighborhood_function` defines the `gaussian` kernel for our neighborhood.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`random_seed` ensures reproducibility of our SOM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After fitting and training this SOM (for 100 epochs, as specified in the `som.train`
    function) where will the nodes end? Let’s see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/5f6ead61493920e3b99ad7fc76263829.png)'
  prefs: []
  type: TYPE_IMG
- en: Trained SOM — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Notice that our SOM tried to mimic the underlying data points as best as it
    could!
  prefs: []
  type: TYPE_NORMAL
- en: 'Although you may think that this is very similar to *k-means,* let’s see the
    result from fitting a 4 centroid k-means solution on this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/79b6c08ff8c7bebaa24f2af3ed8e4d6c.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice that the node in the top right corner was drawn toward the outlier in
    this k-means solution. **The fact that the nodes are tied to each other in the
    SOM using the neighboring function, is one of the main features of the algoritm,
    preventing nodes from being drawn away by outliers.**
  prefs: []
  type: TYPE_NORMAL
- en: Ok, cool! We’ve had a nice introduction to SOMs, let’s now see an application
    of SOM with real data!
  prefs: []
  type: TYPE_NORMAL
- en: Applying SOM to real data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this part of the blog post, I’ll use a dataset with information about S&P
    500 companies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the top 5 rows of the dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cb532983b9d05b4ece3797aeaf822c86.png)'
  prefs: []
  type: TYPE_IMG
- en: financial_valuation_data top 5 rows — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'In this dataset, we can see the following columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Symbol*: Ticker of the company.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Name*: Name of the company.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Sector*: Sector of the company.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Price*: Current stock price of the company (as of 31st Dec 2014).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Price/Earnings*: Ratio between price of the stock and earning-per-share.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Earnings/Share*: Amount of earnings per each share for the company.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*52 Week Low*: Minimum stock price in the last year.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*52 Week High*: Maximum stock price in the last year.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Market Cap*: Total value of the company, equals to the price of each share
    times the number of shares.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*EBITDA*: Earnings before interest, depreciation and amortization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Price/Sales*: Price divided by the revenue per share.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Price/Book*: Financial valuation metric that divides the Price by the Book
    value of the company (*assets-liabilities*).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Additionally, I’ll also create a variable that will emulate the volatility
    of the stock price (although this is not the theoretical formula of stock price
    volatility, it’s a simple proxy):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Cool! So let’s start by subsetting a few columns to make our SOM simpler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**Can you determine how many dimensions our SOM will have based on the subset
    above?**'
  prefs: []
  type: TYPE_NORMAL
- en: '**6, the number of variables we’ve just extracted!** Our SOM from the toy example
    only had two dimensions (for visualization purposes). This new SOM will be harder
    to visualize, but luckily there are a few plots that will give us some insights
    on the trained model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another step that we didn’t go through in the toy example is the process of
    standardization. Remember that Self-Organizing Maps start by calculating distances
    between the random data point and each node? Because of that (and as we normally
    use euclidean distance), we need to standardize our data! Let’s use `sklearn''s
    StandardScaler` to achieve that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: I’ll also impute some `nans` to 0\. As we have few `nas` in our dataset, this
    won’t hurt much.
  prefs: []
  type: TYPE_NORMAL
- en: 'With our dataset ready, we can try to train our `SOM` using the `MiniSom` package.
    Let’s define the following hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: A 15 by 15 lattice (225 nodes) — this is a large network!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6 features (dimensions of the SOM).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A sigma of 0.5.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning Rate of 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gaussian approximation function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: And you may be thinking.. what’s the number of epochs that I should use on my
    SOM?
  prefs: []
  type: TYPE_NORMAL
- en: 'A cool way to visualize that is to train our SOM in batches and calculate the
    `quantization_error` , a metric that checks the average distance between each
    data point and it’s BMU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This will tell us how good our SOM is at replicating the underlying data points.
    Ideally, quantization error would be zero **(almost impossible, as the goal of
    SOM is to reduce the dimensionality of our dataset and there must be some error
    involved).**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/86db4780b1cb4b3a5d48ca995627d2f7.png)'
  prefs: []
  type: TYPE_IMG
- en: SOM Training Process — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, after ~500 epochs, our quantization error is pretty stable. This
    plot gives us an overview on how many epochs we may target for the training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be on the safe side, we’ll train our SOM for 1000 epochs, using the same
    parameters as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We’ve just trained our SOM on this data! With that taken care of, we’ll see
    how our data points relate to the map we’ve created.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first row is represented by the following array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3f2dc67ab4bf3f94698c4ba6a0b3b3f2.png)'
  prefs: []
  type: TYPE_IMG
- en: First Row Array — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'With a trained model, we can call the `som.winner()` function to retrieve the
    `BMU` (nearest SOM node):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the code above is *(2, 11)* — this means that the node that is
    closest to this data point (at the end of the SOM training) is the one that sits
    on coordinates 2,11 (remember that Python indexes start on 0):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/01607422134344cedcec28c634fd1322.png)'
  prefs: []
  type: TYPE_IMG
- en: BMU of our first Data Point — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Each node in the SOM above is characterized by an array of 6 values (the dimensions
    or number of variables). Continuing this process, let’s see what is the BMU for
    the second data point:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/e44464378736c348ab5a5b5da1490736.png)'
  prefs: []
  type: TYPE_IMG
- en: BMU of our Second Data Point — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'We can generalize this behavior for all data points in our dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'A natural follow-up question is: “can we see which nodes have more data points
    near them?” *(this is similar to k-means most representative clusters)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9ecbee38e17f6609bc2bc03e2e529974.png)'
  prefs: []
  type: TYPE_IMG
- en: Top 10 Nodes according to Data Point Representation — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Another common plot we can do with trained SOMs is the distance map. This plot
    gives us the normalized average distance between each node and it’s neighbours:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4ecc6dd863c16b908e56b37b460eab2d.png)'
  prefs: []
  type: TYPE_IMG
- en: SOM Distance Map (U-Matrix) — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'In this distance map, we understand that there are 7 or 8 nodes that are relatively
    distant (red ones) from the rest of the SOM. If those nodes have data points attached
    to them, there may be something special about them — for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eed6a270ec44aacb8aa1e6c162c85d26.png)'
  prefs: []
  type: TYPE_IMG
- en: Distant Node in the SOM — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Filtering by the companies that have this node as the BMU, we obtain the following
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/53779eb8f898ec5faab39b0f4a8ca6e0.png)'
  prefs: []
  type: TYPE_IMG
- en: Companies attached to BMU node — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The next question is— why are these companies tied together in this node? **Probably,
    because one of the 6 dimensions that we were analyzing have a very peculiar value
    on this node.**
  prefs: []
  type: TYPE_NORMAL
- en: 'We can check this by looking at the value of the weight of the variables for
    each node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fc35342d6df59069337356ad79970de5.png)'
  prefs: []
  type: TYPE_IMG
- en: Weight for Each variable by Node — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '**This is another useful plot to visualize trained SOMs as it shows the influence
    of variables in the SOM.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**For L Brands and Phillip Morris, it seems that they have a very values of
    Price to Book Ratio (> 1000) — this can be seen in the influence of Price/Book
    in the node**. This may be something that requires further investigation, particularly
    because the average book/value of our dataset is 14.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s investigate more about our weights in the SOM by looking at other variables,
    for example checking the node areas with unusual price to earnings ratio:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0f4ccfdb2cb69f3d24f90ff9624c82fc.png)'
  prefs: []
  type: TYPE_IMG
- en: PE Ratio Weights on the SOM Node
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting with the high values of the nodes in the red region:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/32660e173b86563248bb985c0b020591.png)'
  prefs: []
  type: TYPE_IMG
- en: Companies with High P/E Ratios — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, let’s see the blue shaded area, where we expect to have
    companies with small P/E ratios:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/e43a2b935f8fe525cfdedeced4c7365e.png)'
  prefs: []
  type: TYPE_IMG
- en: Companies with Low P/E Ratios — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: We only have one company associated with these nodes, with a very low P/E ratio!
    One interesting thing is that the node with this data point is that it is also
    flagged as distant from other nodes in the `distance_map` .
  prefs: []
  type: TYPE_NORMAL
- en: Having a trained SOM helps us understand the underlying structure of the data,
    particularly on how data points cluster around these centroids and what variables
    relate to them. Another common thing do with SOMs is to try to understand how
    different categories cluster around nodes. For example, are we able to capture
    any industry trend on the trained SOM?
  prefs: []
  type: TYPE_NORMAL
- en: 'We can plot the industry of every data point inside each node to assess that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/420a1f3bb2217b654e68314691ab67ee.png)'
  prefs: []
  type: TYPE_IMG
- en: Data Points by Industry in the SOM — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Each square represents a SOM node and inside the square we plot each data point
    attached to that node. With the variables that we have chosen for the network,
    can we identify a trend on any industry?
  prefs: []
  type: TYPE_NORMAL
- en: '*(Spoiler: there’s actually two industries where our SOM may be able to identify
    and cluster industries together very well).*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s subset “*Real Estate*” and “*Utilities*” companies on the plot above:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/16c1ef2433889c5e4bd9958329cc1cbe.png)'
  prefs: []
  type: TYPE_IMG
- en: Real Estate and Utilities Companies in the SOM — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Look how nicely our data points cluster together in few nodes! If we compare
    this with the “I*nformation Technology*” industry:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/58ef6f11dbc72a98ebe005d670ad1325.png)'
  prefs: []
  type: TYPE_IMG
- en: Information Technology Companies in the SOM — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: If we wanted to do some supervised model on top of this data (predicting industry),
    this SOM would give us some good insights on why we would have trouble predicting
    the “Information Technology” class with the variables that we are using.
  prefs: []
  type: TYPE_NORMAL
- en: For more examples, check the awesome [JustGlowing](https://github.com/JustGlowing)
    (creator of the minisom package) [example](https://github.com/JustGlowing/minisom/tree/master/examples)
    repo.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thank you for taking the time to read this post!
  prefs: []
  type: TYPE_NORMAL
- en: SOMs are very interesting overlooked algorithms able to detail the structure
    of our dataset. They may help in a variety of tasks, such as detecting outliers
    (in large SOMs) or building clustering algorithms that are robust to them (smaller
    SOMs). SOMs have historically been used in diverse problems such as [Image Compression](https://ieeexplore.ieee.org/abstract/document/668891?casa_token=Skb2_ne1VHEAAAAA%3AwSrP8MPLH9xMcSVYVuCzJh3-RwmUKiYj0GK1evhBKcgo_dRqiMbgubabTSUE1bz310-FP-y3bg),
    [Detection Systems](https://ieeexplore.ieee.org/abstract/document/8370435?casa_token=f_sMzz7FmjsAAAAA%3AXQvBv1DA39RMjl5jAIio1YhKU0eTI8ry0544AfpFB3CbRECrstASiXbWofxN2pyXEakXOW7TLw)
    and in problems related to [GeoSciences](https://d1wqtxts1xzle7.cloudfront.net/40037324/00463515e8c54d6911000000.pdf20151115-68247-iz3ok7-libre.pdf?1447636982=&response-content-disposition=inline%3B+filename%3DThe_self_organizing_map_the_Geo_SOM_and.pdf&Expires=1691321052&Signature=FI53U9nEQTeZtjWlnZH4ACWHUtK66MCmnjHKHr846t9BY3HbZJE%7ElC%7Ehwoj6UB1Y6iGL2RZhwK8IXjnAYcRhf%7Eh28iEc2r1heLxNT8XCZoM9eNGq0QOQTuzFfkh2YcBSwe7oZVnPVCXhwTEK6-QS9sYdB5tK7moddNxJegksJmvhGx%7EUsrCq4IPEevn80LjDJYX1PadWEAng9W4FDUslrPikl5PeKvaXndrg0ItGp8MbP-6jpwJ7svwWTkJMLvDGn1fSQtVJn%7EJjPBEFL4fZHLA40kURkejx0BtK4dZHfLeB7UUkY2MPBjKcdgzMjF2Xw1xbY3LqpZEIehmohMl8wQ__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA).
  prefs: []
  type: TYPE_NORMAL
- en: When compared with [*K-Means*](/unsupervised-learning-method-series-exploring-k-means-clustering-d129fff3ab6a?sk=0618fadb196a71ce4b9f378f9ee83107)
    and [*Hierarchical Clustering*](/unsupervised-learning-series-exploring-hierarchical-clustering-15d992467aa8?sk=87714b0d7aa252330dd7a8d3b1c620f4),
    Self-Organizing Maps tend to be more robust to outliers, particularly for the
    same number of clusters (don’t confuse this with the fact that large SOMs are
    able to identify edge cases in the data).
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, you can use them for a variety of data science tasks, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Pre-processing steps (identifying outliers using Large SOMs, understanding high
    correlation between features).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand your features ability to predict categories.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build clustering algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you enjoyed this post, make sure to read my other posts on the Unsupervised
    Learning series:'
  prefs: []
  type: TYPE_NORMAL
- en: '[K-Means](/unsupervised-learning-method-series-exploring-k-means-clustering-d129fff3ab6a?sk=0618fadb196a71ce4b9f378f9ee83107)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Hierarchical Clustering](/unsupervised-learning-series-exploring-hierarchical-clustering-15d992467aa8?sk=87714b0d7aa252330dd7a8d3b1c620f4)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The dataset we’ve used in this post is under Open Data Commons License and
    is available* [*here*](https://github.com/datasets/s-and-p-500-companies-financials/tree/main)'
  prefs: []
  type: TYPE_NORMAL
