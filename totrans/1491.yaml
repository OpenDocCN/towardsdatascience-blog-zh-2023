- en: 'Mastering NLP: In-Depth Python Coding for Deep Learning Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/mastering-nlp-in-depth-python-coding-for-deep-learning-models-a15055e989bf](https://towardsdatascience.com/mastering-nlp-in-depth-python-coding-for-deep-learning-models-a15055e989bf)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A step-by-step guide with comprehensive code explanations for text classification
    using deep learning in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://eligijus-bujokas.medium.com/?source=post_page-----a15055e989bf--------------------------------)[![Eligijus
    Bujokas](../Images/061fd30136caea2ba927140e8b3fae3c.png)](https://eligijus-bujokas.medium.com/?source=post_page-----a15055e989bf--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a15055e989bf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a15055e989bf--------------------------------)
    [Eligijus Bujokas](https://eligijus-bujokas.medium.com/?source=post_page-----a15055e989bf--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a15055e989bf--------------------------------)
    ·21 min read·Oct 13, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/240511bbc44eecab740578093612ef8b.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Waypixels](https://unsplash.com/@waypixels?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: This article came to fruition after reading numerous documentation resources
    and looking at videos on YouTube about textual data, classification, recurrent
    neural networks, and other hot subjects on how to develop a machine-learning project
    using text data. A lot of the information is not that user-friendly and some of
    the parts are obfuscated, thus, I want to save the reader a lot of time and shed
    light on the most important concepts in using textual data in any machine learning
    project.
  prefs: []
  type: TYPE_NORMAL
- en: 'The supporting code for the examples presented here can be found at: [https://github.com/Eligijus112/NLP-python](https://github.com/Eligijus112/NLP-python)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics covered in this article will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '***Converting text to sequences***'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Converting sequence indexes to embedded vectors***'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***In-depth RNN explanation***'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***The loss function for classification***'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Full NLP pipeline using Pytorch***'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NLP** stands for **N**atural **L**anguage **P**rocessing¹. This is a huge
    topic about how to use both hardware and software in tasks like:'
  prefs: []
  type: TYPE_NORMAL
- en: Translating one language to another
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text summarization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next token prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Named entity recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And much much more. In this article, I want to cover the most popular techniques
    and familiarize the reader with the concepts by simple and coded examples.
  prefs: []
  type: TYPE_NORMAL
- en: A lot of tasks in NLP start by ***tokenizing the text².***
  prefs: []
  type: TYPE_NORMAL
- en: Text tokenization is a process where we split the original text into smaller
    parts — **tokens**. The tokens can be either characters, subwords, words, or a
    mix of all three.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the string:'
  prefs: []
  type: TYPE_NORMAL
- en: '**“NLP in Python is fun and very well documented. Let’s get started!”**'
  prefs: []
  type: TYPE_NORMAL
- en: I will use word-level tokens because the same logic would apply to lower-level
    tokenization as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, let us define and apply a function that separates the punctuations
    from words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Applying the above function to text we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now, let us define a function that creates a dictionary that links **words to
    indexes** and **indexes to words.** The word that comes up the first in the text
    will have a lower smaller number index. The ordering of the indexes is completely
    arbitrary — the only rule is that **each word should have a unique index.**
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The **word2idx** dictionary is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The **idx2word** dictionary is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: From the above dictionaries, we can see that our data has in total **15 tokens**.
    Throughout the article, I will call the indexes of words as tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Now let us define two functions — one that creates a sequence of indexes and
    another that translates the sequences of indexes to words.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The token sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The word sequence, decoded from the token indexes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us create a new text, one which has not been seen by the indexer:'
  prefs: []
  type: TYPE_NORMAL
- en: '**“As I said, Python is a very good tool for NLP”**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The whole pipeline, from preprocessing to token sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Now the token sequence is such that we could come across it in the wild — not
    all the words were seen by the indexer and the token indexes are not ordered.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes the first part of this article. To recap, we have created some
    crucial functions to create the tokens from text which we will use in the upcoming
    steps.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have an index for every word we can start creating the **features**
    for a machine learning model. As it stands now, we only have a number of when
    the indexer has seen the word from the texts. This does not capture either the
    magnitude of a word or the ordinal information and we cannot in any way compare
    the words — the index is just used as a lookup value.
  prefs: []
  type: TYPE_NORMAL
- en: To go from an index to a feature, we will create a word vector, or, using a
    more popular term, word embedding.
  prefs: []
  type: TYPE_NORMAL
- en: '**A word embedding is a numerical vector representation of a word³**. It links
    text to a vector space. For example,'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The embedding dimension is fixed and defined by the user. When building an NLP
    system, each token must have the same dimension as an embedding.
  prefs: []
  type: TYPE_NORMAL
- en: The most popular technique to create word embeddings is to define the number
    of coordinates a vector has and randomly simulate each coordinate from a normal
    distribution with mean 0 and variance 1.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The above function, by default, will create a vector of length 16\. Let us
    apply the function to each of the words in our dictionary. For reading purposes,
    we will limit the embedding dimension to 6:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The **idx2embeddings** and **word2embeddings** dictionaries look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Usually in literature and in practice, the embedding matrix object is used
    when solving NLP tasks. An embedding matrix is nothing more than the idx2embeddings
    dictionary converted to an array. The first row of the matrix is the first entry
    in the idx2embeddings, the second row is the second entry, and so on. Let us create
    our very own embedding matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The general rule is that the **embedding matrix has the number of rows as the
    total amount of unique tokens in the text and the column number is equal to the
    embedding dimension.**
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have an embedding for each word, we have created essentially 6 features
    for each word. As they stand, they do not make much more sense than the word-to-index
    link.
  prefs: []
  type: TYPE_NORMAL
- en: But we should see these embeddings not as a final product, **but as the initialization
    of weights for a machine learning model**. During training, every coordinate will
    be trainable and the final embedding matrix will look much different than the
    one we have initialized.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have an embedding vector for each word, let us stop for a moment
    and revisit our tokenized ***sequence*** of words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In human language, the sequence of words that we want to say in a sentence is
    very important. Switching two words together may end up transforming the whole
    meaning of a sentence. Thus, it is very important that we create data for a machine-learning
    model that captures the essence of a sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we will create a ***tensor*** in the following format⁴:'
  prefs: []
  type: TYPE_NORMAL
- en: '**(sequence length, batch size, embedding dimension size)**'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is often hard to imagine what is this tensor of 3 dimensions. To visualize
    it, refer to this cube:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/53b0478b80b5c50f2e37037015f08bec.png)'
  prefs: []
  type: TYPE_IMG
- en: A typical sequence tensor (4 x 5 x 4); Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: 'The above tensor is created from the following sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The batch size is equal to the number of documents or data points or any other
    unit of text that we have in our data. In the above example, we have 5 pieces
    of text, thus, the **batch size is equal to 5**.
  prefs: []
  type: TYPE_NORMAL
- en: 'For simplicity''s sake, I have created sentences with 4 words. Thus, the maximum
    sequence length is equal to 4:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The embedding vector dimension is also equal to 4 (this is just a coincidence,
    the embedding size can be much larger than the sequence length). Thus, every token
    in the sentence has 4 numbers associated with it (an embedding vector). In the
    tensor graph, each token’s embedding vector is colored in a different way. Thus,
    if the embedding vector looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Then the sequence is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3e4705e4e3e893e2f22a46d1678fe15d.png)'
  prefs: []
  type: TYPE_IMG
- en: Sequence of “The frogs are gray”; Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: Please note that some authors visualize the tensor for the sequence modeling
    with different axe orientations. My point is that one should always have a handy
    cube in their head when thinking about sequence modeling with the embedding matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a picture of a sequence in our heads, what kind of machine
    learning models could we use to output something at the end of the sequence? One
    class of models that works especially well with sequences is recurrent neural
    networks or **RNN**s for short⁵.
  prefs: []
  type: TYPE_NORMAL
- en: By definition, a recurrent neural network is a neural network that contains
    loops, allowing information to be stored within the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us draw a vanilla feed-forward neural network with the inputs that we have.
    For each time step in our sequence, the token gets “split” into 4 features, taken
    from the embedding matrix and passed through the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/787d01ca98a5324aec2b447d601765a4.png)'
  prefs: []
  type: TYPE_IMG
- en: Feedforward NN; Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: Each weight w1, w2, w3, w4, and w5 get initialized randomly in the network and
    used to calculate the final output of such a forward pass through the network.
    The activation function of choice in the middle neuron is **RELU**. The output
    neuron’s activation function is **linear**.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, for the word **“The”** and with weights initialized with values
    **0.01, 1.14, -1.16, 1.75,** and **2.1,** the network would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e10ccceef65f49d3f372892a3bd2075f.png)'
  prefs: []
  type: TYPE_IMG
- en: Prediction for the token “The”; graph by author
  prefs: []
  type: TYPE_NORMAL
- en: 'To make our network a recurrent network, we need to store some information
    after each new token gets fed forward through the network. We do that by saving
    the output information right before the final output information. To this, we
    add one more weight to the network and expand our network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8c1c463ba7d8eeceb9f6a5571a2af6a7.png)'
  prefs: []
  type: TYPE_IMG
- en: Unrolled graph of RNN; graph by author
  prefs: []
  type: TYPE_NORMAL
- en: 'We have 3 sets of weights in the above graph: **Wx, Wa**, and **Wo**.'
  prefs: []
  type: TYPE_NORMAL
- en: The **Wx** and **Wo** are the same weights as in the simple feedforward NN.
  prefs: []
  type: TYPE_NORMAL
- en: The **Wa** weight is used to multiply the signal that gets fed forward to the
    output layer at each token. Then this signal gets used at the next token level
    at the ***sum and activation*** neuron place.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us initiate the same **Wx**, and **Wo** weights and additionally initialize
    the **Wa** weight to 1.12\. Then the calculations for the full sequence of tokens
    that we have will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The input for the activation function at the “frog” token level (or the second
    step in our sequence) is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1.25 * 0.01 + 0.69 * 1.14 -0.54 * -1.16 + 0.19 * 1.75 + 0.544 * 1.12 = 2.34228**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9cff425dbcf6116c83949e15f1410999.png)'
  prefs: []
  type: TYPE_IMG
- en: From first step to second; Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: 'The last two steps are presented in the bellow graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4535f6608e2b4f98199e087487677dc3.png)'
  prefs: []
  type: TYPE_IMG
- en: The end of the sequence; Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: Each step of the RNN produced the following sequence of numbers
  prefs: []
  type: TYPE_NORMAL
- en: '**[1.14, 4.91, 2.73, 2.56]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, in most of the packages (like Pytorch or Tensorflow), only the
    last step’s output is returned: **2.56**. If the parameter **return_sequences
    = True,** then the full sequence is returned.'
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind, that throughout the unrolling of the network, the **Wx, Wa,**
    and **Wo** are fixed and stay the same. These weights are updated during the backpropagation
    process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last part of the above pipeline is to connect the final output layer to
    the output of our RNN network. Depending on the task we want to solve, we can:'
  prefs: []
  type: TYPE_NORMAL
- en: Add a linear layer if we want to solve a regression problem (for example, given
    a job ad’s description, predict the salary)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add a sigmoid layer if want to solve a classification problem (is the movie
    review’s sentiment positive or negative)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other layers for a custom task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will add a sigmoid at the end of the network and the model predicts a value
    between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'A sigmoid is a function that inputs one number and outputs a number in the
    range of (0, 1). The formula is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/15f9bf96242b9fafb991d114d9af69fb.png)'
  prefs: []
  type: TYPE_IMG
- en: Sigmoid formula
  prefs: []
  type: TYPE_NORMAL
- en: 'If we define the weight Ws on the output of the RNN to be equal to 0.85, the
    full flow of a sequence through our network would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1506875c584a48c20abf45d16493428a.png)'
  prefs: []
  type: TYPE_IMG
- en: Full RNN pipeline; Graph by author
  prefs: []
  type: TYPE_NORMAL
- en: Recall that the output of the RNN is one number equal to 2.56 (hence the input
    to the sigmoid layer is 2.56 * 0.85 = 2.18).
  prefs: []
  type: TYPE_NORMAL
- en: 'During training, the weights that will be adjusted will be:'
  prefs: []
  type: TYPE_NORMAL
- en: Wx (4)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wa (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wo (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ws (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the numbers in the embedding matrix (unique tokens* 4)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: During training, all the weights will be adjusted to produce the best probability
    outputs based on a user-defined loss function.
  prefs: []
  type: TYPE_NORMAL
- en: Please note, that I have omitted the bias variables from the weight calculation.
    The logic does not change one bit, but an additional 1 parameter to Wx, Wa, Wo,
    and Ws would be added.
  prefs: []
  type: TYPE_NORMAL
- en: Before diving into the practical implementation of the above graphs, we need
    to define one last thing — the loss function which we will optimize. We will use
    a very popular loss function for binary classification⁶ called the **binary cross-entropy
    loss function:**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/776ca9456ed447dc0a20a0a2d85433ad.png)'
  prefs: []
  type: TYPE_IMG
- en: BCE loss; Formula by author
  prefs: []
  type: TYPE_NORMAL
- en: The above equation looks intimidating but let us dissect it one symbol at a
    time.
  prefs: []
  type: TYPE_NORMAL
- en: The **BCE(w)** part means that we can only change the weights in our network
    and this is the only parameter for the function.
  prefs: []
  type: TYPE_NORMAL
- en: '**The sum of i = 1 to N** means that when we calculate the BCE, we are using
    all the data.'
  prefs: []
  type: TYPE_NORMAL
- en: '**y_i and x_i** are the ith data points from our dataset. These are fixed and
    we cannot adjust them in any way.'
  prefs: []
  type: TYPE_NORMAL
- en: The **f(w, x_i)** means the output of our function (an RNN network with the
    embedding layer and a sigmoid output) given a set of weights and the ith observation
    of x.
  prefs: []
  type: TYPE_NORMAL
- en: 'To better understand the above loss, imagine that we have 2 observations in
    the form of (y, x): [1, 0.47], [0, 65.23]. Let us say that we have a set of weights
    **w** and f(**w**, 0.47) = 0.88 and f(**w**, 65.23) = 0.12.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then to calculate the BCE loss, we just plug in the values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let us say that we have another set of weights w that produces the following
    probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: f(**w**, 0.47) = 0.95 and f(**w**, 65.23) = 0.08
  prefs: []
  type: TYPE_NORMAL
- en: 'The BCE(w) is then:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The new set of weights is better because the loss function is smaller.
  prefs: []
  type: TYPE_NORMAL
- en: The intuition here is that the smaller the binary cross entropy, the more certain
    the model is to give a high probability when the true Y value is 1 and to give
    a low probability when the true Y value is 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let us put everything together using Pytorch. We will try to classify whether
    a tweet’s sentiment is positive or negative. The dataset was obtained here⁷: [https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis](https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis)'
  prefs: []
  type: TYPE_NORMAL
- en: The data is about whether a tweet about a computer game is positive or negative.
  prefs: []
  type: TYPE_NORMAL
- en: We will encode the Y variable such that 1 means a negative sentiment and 0 means
    a positive sentiment.
  prefs: []
  type: TYPE_NORMAL
- en: 'We split the dataset into train and test sets (34410 and 8603 rows respectively)
    and apply all the functions we have defined before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: There is a small adjustment to the **create_word_index()** function where we
    shift each token index by 1 because we will in a bit introduce the concept of
    **padding,** which is necessary for Pytorch.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Applying the function we get about 29k unique tokens in the text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us create sequences of tokens from our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The average amount of words in a tweet about an airline is equal to **21.8**
    words. So far, our dataset looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/40fb31f78b6e8eb72594311dc2bfdc3f.png)'
  prefs: []
  type: TYPE_IMG
- en: A snippet of data; Picture by author
  prefs: []
  type: TYPE_NORMAL
- en: 'The lengths of the **text_int** column are not equal to one another. Most of
    the machine learning frameworks expect that the dimension of X would be the same.
    Thus, we will introduce the padding function, which adds the sequences with 0
    to the right up to the desired sequence length:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us apply the function and see what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/0c451a88a739374b5bbfce5d7947d850.png)'
  prefs: []
  type: TYPE_IMG
- en: Padded text_int column; Picture by author
  prefs: []
  type: TYPE_NORMAL
- en: The last row in the snippet above visualized the effect of padding.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get an even clearer picture, a small example of padding would be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us say we have two sequences: [1, 2, 5, 9, 3] and [1, 6, 12]. If we fix
    the **pad_length to 4**, then the sequences would become:'
  prefs: []
  type: TYPE_NORMAL
- en: '**[1, 2, 5, 9, 3] -> [1, 2, 5, 9]**'
  prefs: []
  type: TYPE_NORMAL
- en: '**[1, 6, 12] -> [1, 6, 12, 0]**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We now can define our model class and data loader class in Pytorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Training and testing of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The loss vs epoch plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/d1d56352f3d3cce6cf6f0fe483064b50.png)'
  prefs: []
  type: TYPE_IMG
- en: Loss vs epoch; Picture by author
  prefs: []
  type: TYPE_NORMAL
- en: 'The accuracy in the validation set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The resulting test accuracy is **~75%** which is really not bad for so few lines
    of code and feature engineering.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, in this article, I have covered the following concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: Text to indexes (tokenization).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word embeddings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An in-depth explanation of an RNN layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The loss function for binary classification.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to wrap everything in Pytorch and create a model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Happy learning and happy coding!
  prefs: []
  type: TYPE_NORMAL
- en: '[1]'
  prefs: []
  type: TYPE_NORMAL
- en: '**Name:** Natural language processing'
  prefs: []
  type: TYPE_NORMAL
- en: '**URL**: [https://en.wikipedia.org/wiki/Neuro-linguistic_programming](https://en.wikipedia.org/wiki/Natural_language_processing)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2]'
  prefs: []
  type: TYPE_NORMAL
- en: '**Name:** NLP | How tokenizing text, sentence, words works'
  prefs: []
  type: TYPE_NORMAL
- en: '**URL:** [https://www.geeksforgeeks.org/nlp-how-tokenizing-text-sentence-words-works/](https://www.geeksforgeeks.org/nlp-how-tokenizing-text-sentence-words-works/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3]'
  prefs: []
  type: TYPE_NORMAL
- en: '**Name:** Word embedding'
  prefs: []
  type: TYPE_NORMAL
- en: '**URL:** [https://en.wikipedia.org/wiki/Word_embedding](https://en.wikipedia.org/wiki/Word_embedding)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4]'
  prefs: []
  type: TYPE_NORMAL
- en: '**Name:** Time series forecasting'
  prefs: []
  type: TYPE_NORMAL
- en: '**URL:** [https://www.tensorflow.org/tutorials/structured_data/time_series](https://www.tensorflow.org/tutorials/structured_data/time_series)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5]'
  prefs: []
  type: TYPE_NORMAL
- en: '**Name:** Recurrent neural network'
  prefs: []
  type: TYPE_NORMAL
- en: '**URL:** [https://en.wikipedia.org/wiki/Recurrent_neural_network](https://en.wikipedia.org/wiki/Recurrent_neural_network)'
  prefs: []
  type: TYPE_NORMAL
- en: '[6]'
  prefs: []
  type: TYPE_NORMAL
- en: '**Name:** Loss functions for classification'
  prefs: []
  type: TYPE_NORMAL
- en: '**URL:** [https://en.wikipedia.org/wiki/Loss_functions_for_classification](https://en.wikipedia.org/wiki/Loss_functions_for_classification)'
  prefs: []
  type: TYPE_NORMAL
- en: '[7]'
  prefs: []
  type: TYPE_NORMAL
- en: '**Name:** Twitter Sentiment Analysis'
  prefs: []
  type: TYPE_NORMAL
- en: '**URL:** [https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis](https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dataset Licence:** [https://creativecommons.org/publicdomain/zero/1.0/](https://creativecommons.org/publicdomain/zero/1.0/)'
  prefs: []
  type: TYPE_NORMAL
