["```py\nimport tweepy\n\napi_key = \"YjKdgxk...\"\napi_key_secret = \"Qa6ZnPs0vdp4X....\"\n\nauth = tweepy.OAuth2AppHandler(api_key, api_key_secret)\napi = tweepy.API(auth, wait_on_rate_limit=True)\n\nhashtag = \"#climate\"\n\ndef text_filter(s_data: str) -> str:\n    \"\"\" Remove extra characters from text \"\"\"\n    return s_data.replace(\"&amp;\", \"and\").replace(\";\", \" \").replace(\",\", \" \") \\\n                 .replace('\"', \" \").replace(\"\\n\", \" \").replace(\"  \", \" \")\n\ndef get_hashtags(tweet) -> str:\n    \"\"\" Parse retweeted data \"\"\"\n    hash_tags = \"\"\n    if 'hashtags' in tweet.entities:\n        hash_tags = ','.join(map(lambda x: x[\"text\"], tweet.entities['hashtags']))\n    return hash_tags\n\ndef get_csv_header() -> str:\n    \"\"\" CSV header \"\"\"\n    return \"id;created_at;user_name;user_location;user_followers_count;user_friends_count;retweets_count;favorites_count;retweet_orig_id;retweet_orig_user;hash_tags;full_text\"\n\ndef tweet_to_csv(tweet):\n    \"\"\" Convert a tweet data to the CSV string \"\"\"\n    if not hasattr(tweet, 'retweeted_status'):\n        full_text = text_filter(tweet.full_text)\n        hasgtags = get_hashtags(tweet)\n        retweet_orig_id = \"\"\n        retweet_orig_user = \"\"\n        favs, retweets = tweet.favorite_count, tweet.retweet_count\n    else:\n        retweet = tweet.retweeted_status\n        retweet_orig_id = retweet.id\n        retweet_orig_user = retweet.user.screen_name\n        full_text = text_filter(retweet.full_text)\n        hasgtags = get_hashtags(retweet)\n        favs, retweets = retweet.favorite_count, retweet.retweet_count\n    s_out = f\"{tweet.id};{tweet.created_at};{tweet.user.screen_name};{addr_filter(tweet.user.location)};{tweet.user.followers_count};{tweet.user.friends_count};{retweets};{favs};{retweet_orig_id};{retweet_orig_user};{hasgtags};{full_text}\"\n    return s_out\n\npages = tweepy.Cursor(api.search_tweets, q=hashtag, tweet_mode='extended',\n                      result_type=\"recent\",\n                      count=100,\n                      lang=\"en\").pages(limit)\n\nwith open(\"tweets.csv\", \"a\", encoding=\"utf-8\") as f_log:\n    f_log.write(get_csv_header() + \"\\n\")\n    for ind, page in enumerate(pages):\n        for tweet in page:\n            # Get data per tweet\n            str_line = tweet_to_csv(tweet)\n            # Save to CSV\n            f_log.write(str_line + \"\\n\")\n```", "```py\nimport re\n\noutput = re.sub(r\"https?://\\S+\", \"#url\", s_text)  # Replace links with '#url'\n```", "```py\noutput = re.sub(r'@\\w+', '', output)  # Remove mentioned user names @... \n```", "```py\nfrom nltk.tokenize import TweetTokenizer\n\ns = \"This system combines #solar with #wind turbines. #ActOnClimate now. #Capitalism #climate #economics\"\ntokens = TweetTokenizer().tokenize(s)\nprint(tokens)\n# > ['This', 'system', 'combines', '#solar', 'with', '#wind', 'turbines', '.', '#ActOnClimate', 'now', '.', '#capitalism', '#climate', '#economics']\n```", "```py\nwhile len(tokens) > 0 and tokens[-1].startswith(\"#\"):\n    tokens = tokens[:-1]\n# Convert array of tokens back to the phrase\ns = ' '.join(tokens)\n```", "```py\ntag = \"#ActOnClimate\"\nres = re.findall('[A-Z]+[^A-Z]*', tag)\ns = ' '.join(res) if len(res) > 0 else tag[1:]\nprint(s)\n# > Act On Climate\n```", "```py\nimport spacy\nnlp = spacy.load('en_core_web_sm')\n\ns = \"I saw two mice today!\"\n\nprint(\" \".join([token.lemma_ for token in nlp(s)]))\n# > I see two mouse today !\n```", "```py\nimport re\nimport pandas as pd\nfrom nltk.tokenize import TweetTokenizer\n\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words(\"english\"))\n\nimport spacy\nnlp = spacy.load('en_core_web_sm')\n\ndef remove_stopwords(text) -> str:\n    \"\"\" Remove stopwords from text \"\"\"\n    filtered_words = [word for word in text.split() if word.lower() not in stop]\n    return \" \".join(filtered_words)\n\ndef expand_hashtag(tag: str):\n    \"\"\" Convert #HashTag to separated words.\n    '#ActOnClimate' => 'Act On Climate'\n    '#climate' => 'climate' \"\"\"\n    res = re.findall('[A-Z]+[^A-Z]*', tag)\n    return ' '.join(res) if len(res) > 0 else tag[1:]\n\ndef expand_hashtags(s: str):\n    \"\"\" Convert string with hashtags.\n    '#ActOnClimate now' => 'Act On Climate now' \"\"\"\n    res = re.findall(r'#\\w+', s) \n    s_out = s\n    for tag in re.findall(r'#\\w+', s):\n        s_out = s_out.replace(tag, expand_hashtag(tag))\n    return s_out\n\ndef remove_last_hashtags(s: str):\n    \"\"\" Remove all hashtags at the end of the text except #url \"\"\"\n    # \"Change in #mind AP #News #Environment\" => \"Change in #mind AP\"\n    tokens = TweetTokenizer().tokenize(s)\n    # If the URL was added, keep it\n    url = \"#url\" if \"#url\" in tokens else None\n    # Remove hashtags\n    while len(tokens) > 0 and tokens[-1].startswith(\"#\"):\n        tokens = tokens[:-1]\n    # Restore 'url' if it was added\n    if url is not None:\n        tokens.append(url)\n    return ' '.join(tokens) \n\ndef lemmatize(sentence: str) -> str:\n    \"\"\" Convert all words in sentence to lemmatized form \"\"\"\n    return \" \".join([token.lemma_ for token in nlp(sentence)])\n\ndef text_clean(s_text: str) -> str:\n    \"\"\" Text clean \"\"\"\n    try:\n        output = re.sub(r\"https?://\\S+\", \"#url\", s_text)  # Replace hyperlinks with '#url'\n        output = re.sub(r'@\\w+', '', output)  # Remove mentioned user names @... \n        output = remove_last_hashtags(output)  # Remove hashtags from the end of a string\n        output = expand_hashtags(output)  # Expand hashtags to words\n        output = re.sub(\"[^a-zA-Z]+\", \" \", output) # Filter\n        output = re.sub(r\"\\s+\", \" \", output)  # Remove multiple spaces\n        output = remove_stopwords(output)  # Remove stopwords\n        return output.lower().strip()\n    except:\n        return \"\"\n\ndef text_len(s_text: str) -> int:\n    \"\"\" Length of the text \"\"\"\n    return len(s_text)\n\ndf = pd.read_csv(\"tweets.csv\", sep=';', dtype={'id': object, 'retweet_orig_id': object, 'full_text': str, 'hash_tags': str}, lineterminator='\\n')\ndf['text_clean'] = df['full_text'].map(text_clean)\n\ndf['text_len'] = df['text_clean'].map(text_len)\ndf = df[df['text_len'] > 32]\n\ndisplay(df)\n```", "```py\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt  \n\ndef draw_cloud(column: pd.Series, stopwords=None):\n    all_words = ' '.join([text for text in column]) \n\n    wordcloud = WordCloud(width=1600, height=1200, random_state=21, max_font_size=110, collocations=False, stopwords=stopwords).generate(all_words) \n    plt.figure(figsize=(16, 12)) \n    plt.imshow(wordcloud, interpolation=\"bilinear\") \n    plt.axis('off')\n    plt.show()\n\ndraw_cloud(df['text_clean'])\n```", "```py\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndocs = [\"climate change . information about climate important\", \n        \"my cat cute . love cat\"]\n\ntfidf = TfidfVectorizer()\nvectorized_docs = tfidf.fit_transform(docs).todense()\n\nprint(\"Shape:\", vectorized_docs.shape)\ndisplay(pd.DataFrame(vectorized_docs, columns=tfidf.get_feature_names_out()))\n```", "```py\ndocs = df[\"text_clean\"].values\n\ntfidf = TfidfVectorizer()\nvectorized_docs = np.asarray(tfidf.fit_transform(docs).todense())\n\nprint(\"Shape:\", vectorized_docs.shape)\n# > Shape: (19197, 22735)\n```", "```py\nfrom gensim.models import Word2Vec, KeyedVectors\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\n\nword_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n\nprint(\"Shape:\", word_vectors[\"climate\"].shape)\ndisplay(word_vectors[\"climate\"])\n```", "```py\nfrom nltk import word_tokenize\n\ndef word2vec_vectorize(text: str):\n    \"\"\" Convert text document to the embedding vector \"\"\"    \n    vectors = []\n    tokens = word_tokenize(text)\n    for token in tokens:\n        if token in word_vectors:\n            vectors.append(word_vectors[token])\n\n    return np.asarray(vectors).mean(axis=0) if len(vectors) > 0 else np.zeros(word_vectors.vector_size)\n```", "```py\ndocs = df[\"text_clean\"].values\n\nvectorized_docs = list(map(word2vec_vectorize, docs))\nprint(\"Shape:\", vectorized_docs.shape)\n\n# > Shape: (22535, 300)\n```", "```py\nfrom sentence_transformers import SentenceTransformer\n\ndocs = ['the influence of human activity on the warming of the climate system has evolved from theory to established fact', \n        'cats can jump 5 times their own height']\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\nvectorized_docs = model.encode(np.asarray(docs))\n\nprint(\"Shape:\", vectorized_docs.shape)\n# > Shape: (2, 384)\n```", "```py\ndef partial_clean(s_text: str) -> str:\n    \"\"\" Convert tweet to a plain text sentence \"\"\"\n    output = re.sub(r\"https?://\\S+\", \"#url\", s_text)  # Replace hyperlinks with '#url'\n    output = re.sub(r'@\\w+', '', output)  # Remove mentioned user names @... \n    output = remove_last_hashtags(output)  # Remove hashtags from the end of a string\n    output = expand_hashtags(output)  # Expand hashtags to words\n    output = re.sub(r\"\\s+\", \" \", output)  # Remove multiple spaces\n    return output\n\ndocs = df['full_text'].map(partial_clean).values\nvectorized_docs = model.encode(np.asarray(docs))\nprint(\"Shape:\", vectorized_docs.shape)\n\n# > Shape: (19197, 384)\n```", "```py\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\ndef make_clustered_dataframe(x: np.array, k: int) -> pd.DataFrame:\n    \"\"\" Create a new dataframe with original docs and assigned clusters \"\"\"\n    ids = df[\"id\"].values\n    user_names = df[\"user_name\"].values\n    docs = df[\"text_clean\"].values\n    tokenized_docs = df[\"text_clean\"].map(text_to_tokens).values\n\n    km = KMeans(n_clusters=k).fit(x)\n    s_score = silhouette_score(x, km.labels_)\n    print(f\"K={k}: Silhouette coefficient {s_score:0.2f}, inertia:{km.inertia_}\")\n\n    # Create new DataFrame\n    data_len = x.shape[0]\n    df_clusters = pd.DataFrame({\n        \"id\": ids[:data_len],\n        \"user\": user_names[:data_len],\n        \"text\": docs[:data_len],\n        \"tokens\": tokenized_docs[:data_len],\n        \"cluster\": km.labels_,\n    })\n    return df_clusters\n\ndef text_to_tokens(text: str) -> List[str]:\n    \"\"\" Generate tokens from the sentence \"\"\"\n    # \"this is text\" => ['this', 'is' 'text']\n    tokens = word_tokenize(text)  # Get tokens from text\n    tokens = [t for t in tokens if len(t) > 1]  # Remove short tokens\n    return tokens\n\n# Make clustered dataframe\nk = 30\ndf_clusters = make_clustered_dataframe(vectorized_docs, k)\nwith pd.option_context('display.max_colwidth', None):\n    display(df_clusters)\n```", "```py\nfrom sklearn.metrics import silhouette_samples\n\ndef show_clusters_info(x: np.array, k: int, cdf: pd.DataFrame):\n    \"\"\" Print clusters info and top clusters \"\"\"\n    labels = cdf[\"cluster\"].values\n    sample_silhouette_values = silhouette_samples(x, labels)\n\n    # Get silhouette values per cluster    \n    silhouette_values = []\n    for i in range(k):\n        cluster_values = sample_silhouette_values[labels == i]\n        silhouette_values.append((i, \n                                  cluster_values.shape[0], \n                                  cluster_values.mean(), \n                                  cluster_values.min(), \n                                  cluster_values.max()))\n    # Sort\n    silhouette_values = sorted(silhouette_values, \n                               key=lambda tup: tup[2], \n                               reverse=True)\n\n    # Show clusters, sorted by silhouette values\n    for s in silhouette_values:\n        print(f\"Cluster {s[0]}: Size:{s[1]}, avg:{s[2]:.2f}, min:{s[3]:.2f}, max: {s[4]:.2f}\")\n\n    # Show top 7 clusters\n    top_clusters = []\n    for cl in silhouette_values[:7]:\n        df_c = cdf[cdf['cluster'] == cl[0]]\n\n        # Show cluster\n        with pd.option_context('display.max_colwidth', None):\n            display(df_c[[\"id\", \"user\", \"text\", \"cluster\"]])\n\n        # Show words cloud\n        s_all = \"\"\n        for tokens_list in df_c['tokens'].values:\n            s_all += ' '.join([text for text in tokens_list]) + \" \"            \n        draw_cloud_from_words(s_all, stopwords=[\"url\"])\n\n        # Show most popular words\n        vocab = Counter()\n        for token in df_c[\"tokens\"].values:\n            vocab.update(token)\n        display(vocab.most_common(10))\n\ndef draw_cloud_from_words(all_words: str, stopwords=None):\n    \"\"\" Show the word cloud from the list of words \"\"\"\n    wordcloud = WordCloud(width=1600, height=1200, random_state=21, max_font_size=110, collocations=False, stopwords=stopwords).generate(all_words) \n    plt.figure(figsize=(16, 12)) \n    plt.imshow(wordcloud, interpolation=\"bilinear\") \n    plt.axis('off')\n    plt.show()\n\nshow_clusters_info(vectorized_docs, k, df_clusters)\n```", "```py\nimport matplotlib.pyplot as plt  \n%matplotlib inline\n\ndef graw_elbow_graph(x: np.array, k1: int, k2: int, k3: int):\n    k_values, inertia_values = [], []\n    for k in range(k1, k2, k3):\n        print(\"Processing:\", k)\n        km = KMeans(n_clusters=k).fit(x)\n        k_values.append(k)\n        inertia_values.append(km.inertia_)\n\n    plt.figure(figsize=(12,4))\n    plt.plot(k_values, inertia_values, 'o')\n    plt.title('Inertia for each K')\n    plt.xlabel('K')\n    plt.ylabel('Inertia')\n\ngraw_elbow_graph(vectorized_docs, 2, 50, 2)\n```", "```py\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\nfrom bokeh.io import show, output_notebook, export_png\nfrom bokeh.plotting import figure, output_file\nfrom bokeh.models import ColumnDataSource, LabelSet, Label, Whisker, FactorRange\nfrom bokeh.transform import factor_cmap, factor_mark, cumsum\nfrom bokeh.palettes import *\nfrom bokeh.layouts import row, column\noutput_notebook()\n\ndef draw_clusters_tsne(docs: List, cdf: pd.DataFrame):\n    \"\"\" Draw clusters using TSNE \"\"\"\n    cluster_labels = cdf[\"cluster\"].values\n    cluster_names = [str(c) for c in cluster_labels]\n\n    tsne = TSNE(n_components=2, verbose=1, perplexity=50, n_iter=300, \n                init='pca', learning_rate='auto')\n    tsne_results = tsne.fit_transform(vectorized_docs)\n\n    # Plot output\n    x, y = tsne_results[:, 0], tsne_results[:, 1]\n    source = ColumnDataSource(dict(x=x, \n                                   y=y, \n                                   labels=cluster_labels,\n                                   colors=cluster_names))\n    palette = (RdYlBu11 + BrBG11 + Viridis11 + Plasma11 + Cividis11 + RdGy11)[:len(cluster_names)]\n\n    p = figure(width=1600, height=900, title=\"\")\n    p.scatter(\"x\", \"y\",\n              source=source, fill_alpha=0.8, size=4,\n              legend_group='labels',\n              color=factor_cmap('colors', palette, cluster_names)\n              )\n    show(p)\n\ndraw_clusters_tsne(vectorized_docs, df_clusters)\n```", "```py\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\ninputs = model.tokenizer([\"thereisa online forum\"])\ntokens = [e.tokens for e in inputs.encodings]\n\nprint(tokens)\n# > [['[CLS]', 'there', '##isa', 'online', 'forum', '[SEP]']]\n```"]