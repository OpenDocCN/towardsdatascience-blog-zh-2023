- en: The Meaning Behind Logistic Classification, from Physics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-meaning-behind-logistic-classification-from-physics-291774fda579](https://towardsdatascience.com/the-meaning-behind-logistic-classification-from-physics-291774fda579)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In classification problems, why do we use the logistic and softmax functions?
    Thermal physics may have an answer.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://tim-lou.medium.com/?source=post_page-----291774fda579--------------------------------)[![Tim
    Lou, PhD](../Images/e4931bb6d59e27730529ceaf00a23822.png)](https://tim-lou.medium.com/?source=post_page-----291774fda579--------------------------------)[](https://towardsdatascience.com/?source=post_page-----291774fda579--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----291774fda579--------------------------------)
    [Tim Lou, PhD](https://tim-lou.medium.com/?source=post_page-----291774fda579--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----291774fda579--------------------------------)
    ·9 min read·Jan 24, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/506d63dd0efa1ff62f42cfd27cce22b1.png)'
  prefs: []
  type: TYPE_IMG
- en: Logistic regression is ubiquitous, but what’s the intuition behind why it works?
    (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression is perhaps the most popular and well-known machine learning
    model. It solves the binary classification problem — for predicting whether a
    data point belongs to a category.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key ingredient is the the *logistic* function, where an input is converted
    into a probability, written in the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/69180f3741a9fd0e59094ceb8a327de0.png)'
  prefs: []
  type: TYPE_IMG
- en: But why does the exponential make an appearance? What is the intuition behind
    it, beyond just converting a real number into a probability?
  prefs: []
  type: TYPE_NORMAL
- en: It turns out, thermal physics has an answer. But before digging into insights
    from physics, let’s understand the mathematics first.
  prefs: []
  type: TYPE_NORMAL
- en: Mathematical Quandary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before tackling the “why” behind the logistic function, let’s understand its
    properties first.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f4ca81396932f382d9d5bf1369094b35.png)'
  prefs: []
  type: TYPE_IMG
- en: Time to get the math out of the way! (Photo by [Michal Matlon](https://unsplash.com/@michalmatlon?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral))
  prefs: []
  type: TYPE_NORMAL
- en: It is helpful to understand the logistic function in a more generalized form
    — one that is applicable for multiple categories — so that the logistic function
    becomes a special two-category example.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be done by introducing multiple extra variables *zᵢ*, one for each
    category. We then arrive at the *softmax* function, where each class *i* is assigned
    a probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f33e64283cb7da68dcdb54fa61f49aa9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For linear models, the *zᵢ’s* are generally linear sums of features. This function
    has a lot of extra utilities beyond linear models:'
  prefs: []
  type: TYPE_NORMAL
- en: as the final classification layer in a neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: as the attention weights in a transformer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: as the sampling layer for choosing an action in reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'But why do we choose this functional form for parameterizing probabilities?
    The common explanation is that it is simply a conversion tool. Indeed, any sets
    of probabilities, as long as none of them are zero, can be written in this form.
    Mathematically, we can always solve for *zᵢ* by simply taking logarithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a12e51febb50d569d1cc14cdcb826748.png)'
  prefs: []
  type: TYPE_IMG
- en: 'But is this the only special property of the softmax, as a conversion tool?
    Not quite, because there are infinitely many choices for converting numbers into
    probabilities. In fact we can consider functions of other form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/311bfbac89d5f24a0af3c88acc89afdc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To produce a sensible probability function for optimization, we require the
    following criterion on *f*:'
  prefs: []
  type: TYPE_NORMAL
- en: Always positive, always increasing, ranges from 0 to ∞, and differentiable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So exp(*z*) is just one of infinitely many choices, as can be seen below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4c57e953c55ad9ace7354fe524663244.png)'
  prefs: []
  type: TYPE_IMG
- en: Alternatives to exp(z) that can also provide reasonable conversions from numbers
    to probabilities (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: So is exp(*z*) just a matter of conventions out of a sea of possibilities? Should
    exp(*z*) be preferable? While we could simply justify this choice by model performance,
    we should strive for understanding whenever possible. In this case, there is a
    theoretical motivation, and it comes from thermal physics.
  prefs: []
  type: TYPE_NORMAL
- en: A Thermal Physics Link
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It turns out, these probabilities are ubiquitous in thermal physics, they are
    called the [Boltzmann distribution](https://en.wikipedia.org/wiki/Boltzmann_distribution),
    which describes the probability of a particle (or a system more generally) to
    be in a particular energy state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/93a0186f7ebc8b4f5befd16f8441e39f.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *Eᵢ* denotes the energy of the state and *T* the temperature. When the
    temperature is non-zero, one can measure energy as multiples of temperature so
    that *T* can be conveniently set to 1.
  prefs: []
  type: TYPE_NORMAL
- en: The Boltzmann distribution is a bit confusing though, as they model systems
    that are governed by precise and deterministic equations, so how do probability
    and statistics come into the picture (ignoring quantum physics)?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7d4e959534b2553a09e2fa27a372e4a0.png)'
  prefs: []
  type: TYPE_IMG
- en: Thermal physics uses probability to manage our ignorance in the complex world
    (Photo by [Rainer Gelhot](https://unsplash.com/@rynaehr?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral))
  prefs: []
  type: TYPE_NORMAL
- en: The crux is complexity. While we have equations that describe the details of
    a system, they tend to be very complicated. Additionally, these equations often
    exhibit chaotic behaviors, leading to high unpredictability (the [Butterfly effect](https://en.wikipedia.org/wiki/Butterfly_effect)).
    So practically, these detailed deterministic equations are not so useful.
  prefs: []
  type: TYPE_NORMAL
- en: 'How do we understand these complex systems then? Luckily in real life, we rarely
    need to know about the microscopic details of a system, as we cannot measure them
    anyway. It is often sufficient for us to consider macroscopic and emergent quantities
    (like temperature, energy, or entropy). This is where probability theory comes
    in — it is a bridge between the micro and the macro:'
  prefs: []
  type: TYPE_NORMAL
- en: Microscopic details are modeled using probability distributions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Macroscopic quantities are modeled as various averages of these distributions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For an analogy, imagine we want to study all the digits of some irrational numbers,
    say √2, *π* and *e*
  prefs: []
  type: TYPE_NORMAL
- en: √2 = 1.41421356237309504880…
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*π =* 3.14159265358979323846…'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*e* = 2.71828182845904523536…'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The task seems daunting, as each number arises from a different mathematical
    concept. To get the precise digits would require specific numerical methods to
    compute them. However, if we simply look at *macroscopic* behaviors of the digits,
    we’ll easily find that each of the 10 digits appear roughly 10% of the time. The
    crux is that these digits, like many dynamical systems, tend to explore all the
    possibilities without prejudice. In more technical terms,
  prefs: []
  type: TYPE_NORMAL
- en: Chaotic systems tend to maximize all the possibilities, or in other words, entropy
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'What does this have to do with our Boltzmann distribution? Well, mathematically
    the Boltzmann distribution is a *maximum entropy* distribution, under the constraint
    that the statistics it approximates respects a key physical law — the conservation
    of energy. In other words, the Boltzmann distribution is the solution to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/584ef30d5e922bef91c11781bf03ae75.png)'
  prefs: []
  type: TYPE_IMG
- en: So the specific form of exp(−*E*) comes from energy conservation.
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, we can think of energy as a sort of budget. Chaotic systems are
    trying to explore all possibilities (to maximize entropy). If a category has a
    high energy need, it will explore that category less so that there are more opportunities
    to explore other categories. Yet, the probability doesn’t drop to zero because
    the system does still want to explore this energy-inefficient category. The exponential
    form is a result of the compromise between efficiency and exploration.
  prefs: []
  type: TYPE_NORMAL
- en: Going back to data science and classification, our data are not part of a dynamical
    system, and there is no conservation of energy, so how are these physics insights
    useful?
  prefs: []
  type: TYPE_NORMAL
- en: Energies for categories
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/97469963694fe23ca5faea520a744995.png)'
  prefs: []
  type: TYPE_IMG
- en: Energy is the basic unit of all interactions, but does it have a place in classification
    problem? (Photo by [Fré Sonneveld](https://unsplash.com/@fresonneveld?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral))
  prefs: []
  type: TYPE_NORMAL
- en: The crux is that in data science, we are not necessarily interested in literally
    how the data came to be—this is an almost impossible task. Rather, we are trying
    to construct a system that can mimic relevant behaviors in our data. From this
    viewpoint, a model becomes a sort of dynamical system that is molded by data in
    some desirable ways (this was explored in my [article](https://medium.com/towards-data-science/a-physicists-view-of-machine-learning-the-thermodynamics-of-machine-learning-6a3ab00e46f1)).
  prefs: []
  type: TYPE_NORMAL
- en: Just like in thermal physics, the Boltzmann distribution is effective in capturing
    rough microscopic details, while faithfully reproducing macroscopic physical quantities
    (temperature, pressure… etc). So it’s at least plausible that it could lend that
    superpower to data science.
  prefs: []
  type: TYPE_NORMAL
- en: 'Indeed, rather than looking for some sort of energy conservation laws in our
    data, we could just *enforce* a notion of energy conservation in our classification
    model. This way, analogous to the Boltzmann distribution in thermal physics:'
  prefs: []
  type: TYPE_NORMAL
- en: The softmax function in a model assumes a maximum entropy guess for categories,
    under the assumption that there is a sort of conserved budget for making these
    guesses
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The maximum entropy can be justified as a sort of maximal likelihood estimate
    (see [my article on entropy](https://medium.com/towards-data-science/what-does-entropy-measure-an-intuitive-explanation-a7f7e5d16421)).
    The remaining question is, why does it make sense to create an artificially fixed
    energy budget? Here are some reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Central Limit Theorem: the energies are often linear sums of features, so they
    have well-defined mean. So it’s not much of a leap to enforce the average of these
    energies over categories to be constant.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Regularization: this forces the extremes of the probabilities to be limited,
    as a 1 or 0 probability would require some energies to be infinite.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Variance reduction: by imposing a reasonable constraint, we introduce bias
    while reducing variance in our model (bias/variance trade-off)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Point 1 and 3 are particularly salient when using softmax in deep neural networks.
    Since network layers often have some sort of normalization already, it makes even
    more sense to enforce a fixed energy to ensure good statistical behaviors downstream.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, how do these insights help us understand our models? Well, we can use them
    to explain anecdotal facts regarding models that utilize softmax (i.e. logistic
    regression):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Issues with imbalance**: this is because softmax assumes maximum entropy.
    It is intentionally trying to get the model to explore all possible categories
    without prejudice.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Issues with large # of categories**: softmax attempts to assign all possible
    categories, even ones that are impossible (i.e. assign cats as vehicles). For
    data with cleanly separated categories, clustering, nearest-neighbors, [support-vector-machines](https://en.wikipedia.org/wiki/Support_vector_machine)
    and [random forest](https://en.wikipedia.org/wiki/Random_forest) models could
    perform better.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Beyond just the model structure, our thermal physics analogy also helps us understand
    the training paradigm, which we turn to next.
  prefs: []
  type: TYPE_NORMAL
- en: Putting on the Training Pressure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/de6e9ad36e97c0c2950d25e74211641d.png)'
  prefs: []
  type: TYPE_IMG
- en: Boltzmann distribution allows us to compute things like pressure, what’s the
    analogy in classification? (Photo by [NOAA](https://unsplash.com/@noaa?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral))
  prefs: []
  type: TYPE_NORMAL
- en: 'The utility of the Boltzmann distribution goes beyond simply categorizing states
    of a system: it enables us to compute other useful emergent quantities — things
    like pressure. This pressure can be the physical pressure we experience, or more
    abstract thermodynamic forces like magnetic fields and chemical potentials.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, the equation for pressure is defined by the change in the Boltzmann
    factor per change in volume:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/74698591107419637b4fc0cd6d3019ba.png)'
  prefs: []
  type: TYPE_IMG
- en: More generally, thermodynamic pressures are defined as some sort of change in
    our statistical distributions with respect to some variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Jumping back to data science, what are some quantities that look like “pressure”
    in classification? Well, one related quantity is the [Cross Entropy](https://en.wikipedia.org/wiki/Cross_entropy),
    which is the loss function that is typically minimized when training classification
    models. The cross entropy is often estimated via sampling:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2854f2235033e82f7c95b1bfa85829ea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where *l* indicates the proper category a particular data point is in. To optimize
    this we can perform gradient descent: taking the derivative and updating until
    the derivative is zero.'
  prefs: []
  type: TYPE_NORMAL
- en: Using the physics analogy, we could then view the derivative/gradient as a sort
    of thermodynamic pressure!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d768ee68ab9c81e0cc992617625f1b16.png)'
  prefs: []
  type: TYPE_IMG
- en: What this means then, for model training, is that we are imposing pressure on
    our system (our model) until it equalizes. The model is trained when this internal
    “model pressure” reaches zero—some sort of thermal equilibrium.
  prefs: []
  type: TYPE_NORMAL
- en: 'While the analogy isn’t 100% exact, it gives us an intuition on how classification
    models work (more specifically, models that utilize softmax and are optimized
    through something like cross-entropy). So we conclude that:'
  prefs: []
  type: TYPE_NORMAL
- en: Classification models are mock thermodynamic system driven to equilibrium to
    mimic categories in our data
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This viewpoint can perhaps explain why simple logistic regressions are effective,
    even when assumptions regarding linear models are often violated in real life.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hopefully I’ve shown you some intriguing connections between simple logistic
    regressions and thermal physics.
  prefs: []
  type: TYPE_NORMAL
- en: Data science is a very broad discipline. Like thermal physics, we can think
    of data science as a high level way of understanding our macroscopic world, while
    properly handling microscopic details of which we may be ignorant. Perhaps it
    is not surprising that many conceptual and mathematical tools in data science
    can be linked or even traced back to physics, as they both share the common objective
    of modelling our world.
  prefs: []
  type: TYPE_NORMAL
- en: If you like this article, please do leave a comment. Also let me know if you
    would like to see other abstract concepts demystified and properly explained.
    Happy reading 👋.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you like my article, you may be interested in my other insight pieces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/what-does-entropy-measure-an-intuitive-explanation-a7f7e5d16421?source=post_page-----291774fda579--------------------------------)
    [## What does Entropy Measure? An Intuitive Explanation'
  prefs: []
  type: TYPE_NORMAL
- en: Entropy can be thought of as the probability of seeing certain patterns in data.
    Here’s how it works.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/what-does-entropy-measure-an-intuitive-explanation-a7f7e5d16421?source=post_page-----291774fda579--------------------------------)
    [](/a-physicists-view-of-machine-learning-the-thermodynamics-of-machine-learning-6a3ab00e46f1?source=post_page-----291774fda579--------------------------------)
    [## A Physicist’s View of Machine Learning: The Thermodynamics of Machine Learning'
  prefs: []
  type: TYPE_NORMAL
- en: Complex systems in nature can be successfully studied using thermodynamics.
    What about Machine Learning?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/a-physicists-view-of-machine-learning-the-thermodynamics-of-machine-learning-6a3ab00e46f1?source=post_page-----291774fda579--------------------------------)
    [](https://medium.com/swlh/entropy-is-not-disorder-a-physicists-perspective-c0dccfea67f1?source=post_page-----291774fda579--------------------------------)
    [## Entropy Is Not Disorder: A Physicist’s Perspective'
  prefs: []
  type: TYPE_NORMAL
- en: Entropy is often treated as synonymous to chaos. But what is it really? In this
    article, we explore how entropy is more…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/swlh/entropy-is-not-disorder-a-physicists-perspective-c0dccfea67f1?source=post_page-----291774fda579--------------------------------)
    [](https://medium.com/physicist-musings/why-we-dont-live-in-a-simulation-a-physicist-s-perspective-1811d65f502d?source=post_page-----291774fda579--------------------------------)
    [## Why We Don’t Live in a Simulation
  prefs: []
  type: TYPE_NORMAL
- en: Describing reality as a simulation vastly understates the complexities of our
    world. Here’s why the simulation…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/physicist-musings/why-we-dont-live-in-a-simulation-a-physicist-s-perspective-1811d65f502d?source=post_page-----291774fda579--------------------------------)
  prefs: []
  type: TYPE_NORMAL
