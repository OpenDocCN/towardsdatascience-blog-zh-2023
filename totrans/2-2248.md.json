["```py\nfrom bs4 import BeautifulSoup\nimport requests\nimport re\n\nurl = \"https://en.wikipedia.org/wiki/List_of_Middle-earth_characters\"\n# Get the contents of the webpage in text format\ndata  = requests.get(url).text\n\nsoup = BeautifulSoup(data,\"html.parser\")\n\n# Parse the HTML code to isolate rows containing character names\nnames = soup.find_all('li')\n\n# Apply a second filter to isolate actual names\nnames_2 = []\nfor i in names:\n    if \"\"\"<li><a href=\"/wiki/\"\"\" in str(i):\n        names_2.append(str(i))\n\n# Retrieve final character names applying cleaning conditions\nlotr_names = []\nfor i in names_2:\n    candidate_name = re.findall(r'\"(.*?)\"', i)[1]\n    conditions = 'Category:' not in candidate_name and 'Middle-earth ' not in candidate_name and 'Tolkien' not in candidate_name and 'Lord of the Rings' not in candidate_name and 'The Hobbit' not in candidate_name\n    if conditions:\n        lotr_names.append(candidate_name.replace(' (Middle-earth)','')) \n```", "```py\nreplace_dict = {'a':['â','ä','á'],\n                'e':['ê','ë','é'],\n                'i':['î','í'],\n                'o':['ô','ö','ó'],\n                'u':['ú','û',],\n                ' ':['-']\n}\nfor new_char in replace_dict.keys():\n    for old_char in replace_dict[new_char]:\n        lotr_names = lotr_names.replace(old_char, new_char)\n```", "```py\ndef RNN_forward_prop_step(parameters, a_prev, x):\n    W_aa = parameters['W_aa']\n    W_ax = parameters['W_ax']\n    W_ya = parameters['W_ya']\n    b_y = parameters['b_y']\n    b = parameters['b']\n\n    # Compute hidden state\n    a_next = np.tanh(np.dot(W_ax, x) + np.dot(W_aa, a_prev) + b)\n\n    # Compute log probabilities for next character\n    p_t = softmax(np.dot(W_ya, a_next) + b_y)\n\n    return a_next, p_t\n```", "```py\ndef RNN_forward_prop(X, Y, a0, parameters, vocab_size):\n\n    x = {}\n    a = {}\n    y_hat = {}\n    a[-1] = np.copy(a0)\n    loss= 0\n\n    # Iterate for the T timesteps\n    for t in range(len(X)):\n\n        # One-hot representation of the t-th character\n        x[t] = np.zeros((vocab_size, 1))\n        if X[t] != None:\n            x[t][X[t]] = 1\n\n        # Run one timestep of forward prop\n        a[t], y_hat[t] = RNN_forward_prop_step(parameters, a[t-1], x[t])\n\n        # Update loss function\n        loss -= np.log(y_hat[t][Y[t],0])\n\n    cache = (y_hat, a, x)\n\n    return loss, cache\n```", "```py\ndef RNN_back_prop_step(d_y, grads, parameters, x, a, a_prev):\n\n    grads['dW_ya'] += np.dot(d_y, a.T)\n    grads['db_y'] += d_y\n    da = np.dot(parameters['W_ya'].T, d_y) + grads['da_next']\n    da_raw = (1 - a * a) * da\n    grads['db'] += da_raw\n    grads['dW_ax'] += np.dot(daraw, x.T)\n    grads['dW_aa'] += np.dot(daraw, a_prev.T)\n    grads['da_next'] = np.dot(parameters['W_aa'].T, daraw)\n    return grads\n```", "```py\ndef RNN_back_prop(X, Y, parameters, cache):\n    # Initialize gradients as an empty dictionary\n    grads = {}\n\n    # Retrieve from cache and parameters\n    (y_hat, a, x) = cache\n    W_aa = parameters['W_aa']\n    W_ax = parameters['W_ax']\n    W_ya = parameters['W_ya']\n    b_y = parameters['b_y']\n    b = parameters['b']\n\n    # Initialize gradients\n    grads['dW_ax'], grads['dW_aa'], grads['dW_ya'] = np.zeros_like(W_ax), np.zeros_like(W_aa), np.zeros_like(W_ya)\n    grads['db'], grads['db_y'] = np.zeros_like(b), np.zeros_like(b_y)\n    grads['da_next'] = np.zeros_like(a[0])\n\n    # Backpropagate through timesteps\n    for t in reversed(range(len(X))):\n        dy = np.copy(y_hat[t])\n        dy[Y[t]] -= 1\n        grads = RNN_back_prop_step(dy, grads, parameters, x[t], a[t], a[t-1])\n\n    return grads, a\n```", "```py\ndef RNN_optimization(X, Y, a_prev, parameters, alpha, vocab_size):\n\n    # 1\\. Forward propagation\n    loss_now, cache = RNN_forward_prop(X, Y, a_prev, parameters, vocab_size)\n\n    # 2\\. Backward propagation\n    grads, a = RNN_back_prop(X, Y, parameters, cache)\n\n    # 3\\. Clip gradients\n    grads = clip_grads(grads, 10)\n\n    # 4\\. Update parameters\n    parameters = update_parameters(parameters, grads, alpha)\n\n    return loss_now, parameters, a[len(X)-1]\n```", "```py\ndef train_model(data, n_a=50, max_iter = 100000):\n    # Get the list of characters\n    chars = list(set(data))\n    # Get the dictionary size (number of characters)\n    vocab_size = len(chars)\n\n    # Get encoding and decoding dictionaries\n    chars_to_encoding = encode_chars(chars)\n    encoding_to_chars = decode_chars(chars)\n\n    # Get dataset as a list of names and strip, then shuffle the dataset\n    data = data.split('\\n')\n    data = [x.strip() for x in data]   \n    np.random.shuffle(data) \n\n    # Define n_x, n_y parameters\n    n_x, n_y = vocab_size, vocab_size\n\n    # Initialize the hidden state\n    # a_prev = initialize_hidden_state(n_a)\n    a_prev = np.zeros((n_a, 1))\n\n    # Initialize the parameters\n    parameters = initialize_parameters(n_a, n_x, n_y)\n    # for k in parameters.keys():\n    #     print('{}: tipo {}, Datatype {}'.format(k, type(parameters[k]), parameters[k].dtype))\n    # Get current loss function value\n    loss_now = get_initial_loss(vocab_size, len(data))\n\n    # Perform max_iter iteration to train the model's parameters\n    for iter in range(max_iter):\n        # print(iter)\n        # Get the index of the name to pick\n        name_idx = iter % len(data)\n        example = data[name_idx]\n\n        # Convert encoded and decoded example into a list\n        example_chars = [char for char in example]\n        example_encoded = [chars_to_encoding[char] for char in example]\n\n        # Create training input X. The value None is used to consider the first input character\n        # as a vector of zeros\n        X = [None] + example_encoded\n\n        # Create the label vector Y by appending the '\\n' encoding to the end of the vector\n        Y = example_encoded + [chars_to_encoding['\\n']]\n\n        # Perform one step of the optimization cycle:\n        # 1\\. Forward propagation\n        # 2\\. Backward propagation\n        # 3\\. Gradient clipping\n        # 4\\. Parameters update\n\n        loss_tmp, parameters, a_prev = RNN_optimization(X, Y, a_prev, parameters, alpha=0.01, vocab_size=vocab_size)\n        # for k in parameters.keys():\n        #     print('{}: tipo {}, Datatype {}'.format(k, type(parameters[k]), parameters[k].dtype))\n        loss_now = smooth(loss_now, loss_tmp)\n\n    return parameters\n```", "```py\ndef sample(parameters, chars_to_encoding):\n    W_aa = parameters['W_aa']\n    W_ax = parameters['W_ax']\n    W_ya = parameters['W_ya']\n    b_y = parameters['b_y']\n    b = parameters['b']\n    vocab_size = b_y.shape[0]\n    n_a = W_aa.shape[1]\n\n    x = np.zeros((vocab_size,))\n    a_prev = np.zeros((n_a,))\n\n    indices = []\n    idx = -1 \n\n    counter = 0\n    newline_character = chars_to_encoding['\\n']\n\n    while (idx != newline_character and counter != 50):\n        a = np.tanh(np.dot(W_ax,x)+np.dot(W_aa,a_prev)+np.ravel(b))\n        z = np.dot(W_ya,a) + np.ravel(b_y)\n        y = softmax(z)\n\n        idx = np.random.choice(list(chars_to_encoding.values()), p=np.ravel(y))\n        indices.append(idx)    \n\n        x = np.zeros((vocab_size,))\n        x[idx] = 1\n\n        a_prev = a\n\n        counter +=1\n        if (counter == 50):\n            indices.append(chars_to_encoding['\\n'])\n    return indices\n```", "```py\ndef get_sample(sample_ix, encoding_to_chars):\n    txt = ''.join(encoding_to_chars[ix] for ix in sample_ix)\n    txt = txt[0].upper() + txt[1:]\n    return txt\n```"]