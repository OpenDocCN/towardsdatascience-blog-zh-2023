# 进入 HuggingFace 的切入点

> 原文：[https://towardsdatascience.com/an-entry-point-into-huggingface-2f3d1e60ad5a](https://towardsdatascience.com/an-entry-point-into-huggingface-2f3d1e60ad5a)

## 为初学者提供的基础知识的逐步指南

[](https://medium.com/@mina.ghashami?source=post_page-----2f3d1e60ad5a--------------------------------)[![Mina Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page-----2f3d1e60ad5a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2f3d1e60ad5a--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2f3d1e60ad5a--------------------------------) [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page-----2f3d1e60ad5a--------------------------------)

·发布在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2f3d1e60ad5a--------------------------------) ·15 分钟阅读·2023 年 11 月 26 日

--

![](../Images/a6112804c83872912af719222f58fd92.png)

图片来自 [unsplash](https://unsplash.com/photos/person-holding-ac-receiver-_J3oTl6acVg)

如果你不知道从哪里开始学习，HuggingFace 可能会显得复杂和困难。进入 HuggingFace 仓库的一个切入点是 [run_mlm.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py) 和 [run_clm.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py) 脚本。

在这篇文章中，我们将通过 [run_mlm.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py) 脚本进行讲解。这个脚本从 HuggingFace 中选择一个掩码语言模型，并在数据集上微调它（或者从头开始训练）。如果你是初学者，对 HuggingFace 代码了解甚少，这篇文章将帮助你理解基础知识。

> 我们将选择一个掩码语言模型，从 HuggingFace 加载数据集，并在数据集上微调模型。最后，我们将评估模型。这一切都是为了理解代码结构，因此我们的重点不在于任何特定的应用案例。

让我们开始吧。

# 关于微调的几句话

微调是深度学习中的一种常见技术，用于调整预训练的神经网络模型，以更好地适应新的数据集或任务。

当你的数据集不足以从头训练一个深度模型时，微调效果很好！因此，你可以从一个已经训练好的基础模型开始。

在微调中，你会采用一个在大数据源上预训练的模型（例如，图像的 ImageNet 或 NLP 的 BooksCorpus），然后在你的数据集上继续训练，以使模型适应你的任务。这比从随机权重开始训练需要的额外数据和周期要少得多。

# 微调在 HuggingFace 中

HuggingFace (HF) 提供了许多内置功能，让我们可以用少量代码微调预训练模型。主要步骤如下：

1.  加载预训练模型

1.  加载预训练的 tokenizer

1.  加载你想用于微调的数据集

1.  使用 tokenizer 对上述数据集进行分词

1.  使用 Trainer 对象在分词数据集上训练预训练模型

让我们看看代码中的每个步骤。我们将故意省略许多细节，以便给出整体结构的概览。

## 1) HF：加载预训练模型

例如，要加载`bert-base`模型，请编写以下内容：

[PRE0]

访问模型名称的完整列表请见[*https://huggingface.co/models*](https://huggingface.co/models)。

## 2) HF：加载预训练的 tokenizer

通常，tokenizer 的名称与模型名称相同：

[PRE1]

## 3) HF：加载用于微调的数据集

这里，我们以非流式方式加载 squad 数据集。

[PRE2]

## 4) HF：对数据集进行分词

我们定义了一个分词函数，并将样本批量传递给它。

[PRE3]

## 5) HF：训练模型的 trainer

最后但同样重要的是负责训练模型的 Trainer 对象。

[PRE4]

> 将这五个步骤结合起来，我们可以从 HuggingFace 微调模型。

在此之前，我们从一个被忽略的细节开始：**输入参数**。如上所示，代码中涉及许多超参数，例如 `model_name`、`dataset_name`、`training_args` 等。这些超参数是我们在进行上述任何步骤之前应指定的输入参数。让我们看看 HuggingFace 中有哪些参数。

## HuggingFace 中的参数组

通常有三到四个不同的参数组：

1.  `ModelArguments`：与我们将微调或从头训练的模型/配置/tokenizer 相关的参数。

1.  `DataTrainingArguments`：与训练数据和评估数据相关的参数。

1.  `TrainingArguments`：与训练超参数和配置相关的参数。

1.  `PEFTArguments`：与模型的参数高效训练相关的参数。这是可选的，你可以选择不以参数高效模式训练模型，这样你将没有这个参数组。

你可能之前见过每一个参数都被定义为 `dataclass`。例如，`ModelArguments` 如下：（有点长，但只是所有与建模相关的超参数）

[PRE5]

为了初始化这些并解析它们，我们使用 `HFArgumentParser`。

## HfArgumentParser

`HfArgumentParser` 是**HuggingFace 参数解析器**。你可以看到它的定义如下：

[PRE6]

**让我们看看如何传递参数**：你可以通过三种方式传递参数：

**1) 通过命令行**：打开终端并在命令行中传递参数。以下是一个示例：

[PRE7]

然后按照以下方式读取：

[PRE8]

解析器将自动将这些参数分配到正确的组。

**2) 通过传递 json 文件**：JSON 文件应包含与参数名称对应的键。以下是一个 json 文件的示例：

[PRE9]

注意不要在最后一行后面加`,`，否则会出错。按如下方式调用`train.py`脚本：

[PRE10]

并在`train.py`中接收参数如下：

[PRE11]

**3) 通过字典传递：**

当然，如果你可以通过json文件传递，你也可以通过字典传递参数：

[PRE12]

要点是`HfArgumentParser`允许灵活传递参数。

## 整体代码

将以上五个步骤综合起来：

首先导入所有必要的库。

[PRE13]

其次，我们定义了三组参数。注意我们已经导入了`TrainingArguments`。所以不需要再定义它。首先，我们定义`ModelArguments`。

[PRE14]

以及`DataTrainingArguments`类：

[PRE15]

然后调用`HfArgumentParser`将输入参数解析到各个类中：

[PRE16]

但为了我们的使用场景，我们将它们定义为一个字典并传递：

[PRE17]

现在，让我们打印`model_args, data_args, training_args`：

[PRE18]

[PRE19]

`training_args`是一个长属性列表。我在这里部分打印出来：

[PRE20]

**接下来，我们加载一个数据集。** 在这里，我们加载的是`glue — cola`数据集，即“语言接受性语料库”。它包含了从语言学理论的书籍和期刊文章中提取的英文接受性判断。每个示例是一个带有语法标注的单词序列[1]。

[PRE21]

数据集有三个分割：train — validation — test。注意我们在训练集中有8551个数据点。同时注意这是一个监督数据集，因为它有一个标签；但在这篇文章中我们不会使用标签，我们将以*自监督的方式*（掩蔽标记）使用此数据集来微调模型。

![](../Images/e73f842838f7952533da5c1018f0415d.png)

图片由作者提供

让我们看一个训练中的例子：

[PRE22]

![](../Images/8d58fba321e1cfd86daadab0a57266b9.png)

图片由作者提供

我们看到`label = 1`因为`sentence`在语法上是正确的。现在让我们看另一个`label = 0`的例子。

![](../Images/170694c84b5d4e9179647c24e434d9e9.png)

图片由作者提供

这句话“They drank the pub”在语法上是不正确的，所以`label = 0`。

**好了，数据部分讲解完了。让我们加载分词器**：

[PRE23]

打印分词器显示如下：

[PRE24]

**接下来，加载模型**并检查模型的`embedding_size`；这显示了模型的嵌入维度，并且如果它与分词器的词汇表大小匹配。如果不匹配，我们将调整模型的嵌入矩阵。

[PRE25]

在我们的情况下，由于我们没有向分词器添加任何特殊标记，它们匹配且都是*30522*。所以没问题。

**接下来，设置上下文长度并告知分词器从数据中读取哪一列：**

[PRE26]

**接下来，编写分词函数：**

[PRE27]

让我们看看分词后的数据：

![](../Images/3e407dd03e294dd9ac86a4463812430b.png)

图片由作者提供

第一个数据点如下所示：

![](../Images/ef0c44611f8b6bb92babbd2d6fc01925.png)

图片由作者提供

第10个数据点如下所示：

![](../Images/5d15b1454036edde4f534f77f1cee1f4.png)

图片由作者提供

请注意，不同的数据点自然具有不同的大小（input_ids在不同数据点中的长度不同）。稍后，数据收集器会对其进行填充或截断，以确保它们的长度相同。

**以及打包函数**：

[PRE28]

打包后的数据如下所示：

![](../Images/b07031fd286ce472bd50e55d7e518127.png)

作者提供的图片

你可以看到数据的大小从8551行减少到185行。这是因为打包形成了长度为`context_length=512`的序列。因此，在`group_texts`函数之后，我们在训练集中有185个数据点，每个数据点的长度为`512`个tokens。

现在数据已经被标记化并打包成上下文长度的批次，让我们来看看我们的训练集和评估集：

[PRE29]

[PRE30]

最后，我们定义了数据收集器和训练器对象。如果你不熟悉数据收集器，可以查看[这篇文章](https://medium.com/towards-data-science/data-collators-in-huggingface-a0c76db798d2)。

[](/data-collators-in-huggingface-a0c76db798d2?source=post_page-----2f3d1e60ad5a--------------------------------) [## HuggingFace中的数据收集器

### 它们是什么以及它们的作用

towardsdatascience.com](/data-collators-in-huggingface-a0c76db798d2?source=post_page-----2f3d1e60ad5a--------------------------------)

只需知道数据收集器负责确保批次中的所有序列具有相同的长度，因此它会进行截断和填充。

[PRE31]

让我们训练模型并查看结果：

[PRE32]

![](../Images/e17c3e68c92fc39b615b6234b33392aa.png)

作者提供的图片

你可以看到我们每100步都有验证结果和指标（我们选择的是准确率），这是因为在`data_args`中我们设置了以下内容：

[PRE33]

# 结论

在这篇文章中，我们简要介绍了[run_mlm.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py)脚本，这个脚本是进入HuggingFace的入口点。我们看到了在HuggingFace上微调（或从头开始训练）模型的基本步骤。如果你对HuggingFace了解甚少，这篇文章可以帮助你了解基本步骤。

# 参考文献

1.  [https://huggingface.co/datasets/glue](https://huggingface.co/datasets/glue)
