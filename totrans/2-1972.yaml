- en: Testing the Massively Multilingual Speech (MMS) Model that Supports 1162 Languages
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试支持 1162 种语言的大规模多语言语音（MMS）模型
- en: 原文：[https://towardsdatascience.com/testing-the-massively-multilingual-speech-mms-model-that-supports-1162-languages-5db957ee1602](https://towardsdatascience.com/testing-the-massively-multilingual-speech-mms-model-that-supports-1162-languages-5db957ee1602)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/testing-the-massively-multilingual-speech-mms-model-that-supports-1162-languages-5db957ee1602](https://towardsdatascience.com/testing-the-massively-multilingual-speech-mms-model-that-supports-1162-languages-5db957ee1602)
- en: Explore the cutting-edge multilingual features of Meta’s latest automatic speech
    recognition (ASR) model
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索 Meta 最新的自动语音识别（ASR）模型的前沿多语言功能
- en: '[](https://medium.com/@luisroque?source=post_page-----5db957ee1602--------------------------------)[![Luís
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----5db957ee1602--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5db957ee1602--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5db957ee1602--------------------------------)
    [Luís Roque](https://medium.com/@luisroque?source=post_page-----5db957ee1602--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@luisroque?source=post_page-----5db957ee1602--------------------------------)[![Luís
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----5db957ee1602--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5db957ee1602--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5db957ee1602--------------------------------)
    [Luís Roque](https://medium.com/@luisroque?source=post_page-----5db957ee1602--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5db957ee1602--------------------------------)
    ·8 min read·May 26, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5db957ee1602--------------------------------)
    ·阅读时间 8 分钟·2023 年 5 月 26 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Introduction
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Massively Multilingual Speech (MMS)¹ is the latest release by Meta AI (just
    a few days ago). It pushes the boundaries of speech technology by expanding its
    reach from about 100 languages to over 1,000\. This was achieved by building a
    single multilingual speech recognition model. The model can also identify over
    4,000 languages, representing a 40-fold increase over previous capabilities.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模多语言语音（MMS）¹ 是 Meta AI 最近发布的产品（就在几天前）。它通过将其覆盖范围从大约 100 种语言扩展到超过 1,000 种语言，推动了语音技术的边界。这是通过构建一个单一的多语言语音识别模型实现的。该模型还可以识别超过
    4,000 种语言，比以前的能力提高了 40 倍。
- en: The MMS project aims to make it easier for people to access information and
    use devices in their preferred language. It expands text-to-speech and speech-to-text
    technology to underserved languages, continuing to reduce language barriers in
    our global world. Existing applications can now include a wider variety of languages,
    such as virtual assistants or voice-activated devices. At the same time, new use
    cases emerge in cross-cultural communication, for example, in messaging services
    or virtual and augmented reality.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: MMS 项目旨在让人们更容易以他们首选的语言获取信息和使用设备。它将文本转语音和语音转文本技术扩展到服务不足的语言，继续减少我们全球世界中的语言障碍。现有的应用程序现在可以包括更多种类的语言，例如虚拟助手或语音激活设备。与此同时，跨文化沟通中出现了新的应用场景，例如在消息服务或虚拟与增强现实中。
- en: In this article, we will walk through the use of MMS for ASR in English and
    Portuguese and provide a step-by-step guide on setting up the environment to run
    the model.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将介绍 MMS 在英语和葡萄牙语的 ASR 应用，并提供逐步指南，帮助设置环境以运行该模型。
- en: '![](../Images/a2b17ee13d06f5681126543d65c4f142.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a2b17ee13d06f5681126543d65c4f142.png)'
- en: 'Figure 1: Massively Multilingual Speech (MMS) is capable of identifying over
    4,000 languages and supports 1162 ([source](https://unsplash.com/photos/ZzWsHbu2y80))'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：大规模多语言语音（MMS）能够识别超过 4,000 种语言，并支持 1162 种语言（[source](https://unsplash.com/photos/ZzWsHbu2y80)）
- en: 'This article belongs to “Large Language Models Chronicles: Navigating the NLP
    Frontier”, a new weekly series of articles that will explore how to leverage the
    power of large models for various NLP tasks. By diving into these cutting-edge
    technologies, we aim to empower developers, researchers, and enthusiasts to harness
    the potential of NLP and unlock new possibilities.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本文属于“**大语言模型编年史：导航 NLP 前沿**”系列文章中的一篇，这是一个新的每周系列文章，旨在探索如何利用大型模型的力量来完成各种 NLP 任务。通过深入了解这些前沿技术，我们旨在赋能开发者、研究人员和爱好者，利用
    NLP 的潜力并开启新的可能性。
- en: 'Articles published so far:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止发布的文章：
- en: '[Summarizing the latest Spotify releases with ChatGPT](https://medium.com/towards-data-science/summarizing-the-latest-spotify-releases-with-chatgpt-553245a6df88)'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[使用 ChatGPT 总结最新的 Spotify 发布内容](https://medium.com/towards-data-science/summarizing-the-latest-spotify-releases-with-chatgpt-553245a6df88)'
- en: '[Master Semantic Search at Scale: Index Millions of Documents with Lightning-Fast
    Inference Times using FAISS and Sentence Transformers](https://medium.com/towards-data-science/master-semantic-search-at-scale-index-millions-of-documents-with-lightning-fast-inference-times-fa395e4efd88)'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[掌握大规模语义搜索：使用 FAISS 和 Sentence Transformers 索引数百万文档，实现超快推理时间](https://medium.com/towards-data-science/master-semantic-search-at-scale-index-millions-of-documents-with-lightning-fast-inference-times-fa395e4efd88)'
- en: '[Unlock the Power of Audio Data: Advanced Transcription and Diarization with
    Whisper, WhisperX, and PyAnnotate](https://medium.com/towards-data-science/unlock-the-power-of-audio-data-advanced-transcription-and-diarization-with-whisper-whisperx-and-ed9424307281)'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[解锁音频数据的力量：使用 Whisper、WhisperX 和 PyAnnotate 进行高级转录和分段](https://medium.com/towards-data-science/unlock-the-power-of-audio-data-advanced-transcription-and-diarization-with-whisper-whisperx-and-ed9424307281)'
- en: '[Whisper JAX vs PyTorch: Uncovering the Truth about ASR Performance on GPUs](https://medium.com/towards-data-science/whisper-jax-vs-pytorch-uncovering-the-truth-about-asr-performance-on-gpus-8794ba7a42f5)'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Whisper JAX 与 PyTorch：揭示 ASR 在 GPU 上的性能真相](https://medium.com/towards-data-science/whisper-jax-vs-pytorch-uncovering-the-truth-about-asr-performance-on-gpus-8794ba7a42f5)'
- en: '[Vosk for Efficient Enterprise-Grade Speech Recognition: An Evaluation and
    Implementation Guide](https://medium.com/towards-data-science/vosk-for-efficient-enterprise-grade-speech-recognition-an-evaluation-and-implementation-guide-87a599217a6c)'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Vosk：高效企业级语音识别的评估和实施指南](https://medium.com/towards-data-science/vosk-for-efficient-enterprise-grade-speech-recognition-an-evaluation-and-implementation-guide-87a599217a6c)'
- en: As always, the code is available on my [Github](https://github.com/luisroque/large_laguage_models).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 与往常一样，代码可以在我的 [Github](https://github.com/luisroque/large_laguage_models) 上找到。
- en: The Approach to build the Massively Multilingual Speech Model
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建大规模多语言语音模型的方法
- en: 'Meta used religious texts, such as the Bible, to build a model covering this
    wide range of languages. These texts have several interesting components: first,
    they are translated into many languages, and second, there are publicly available
    audio recordings of people reading these texts in different languages. Thus, the
    main dataset where this model was trained was the New Testament, which the research
    team was able to collect for over 1,100 languages and provided more than 32h of
    data per language. They went further to make it recognize 4,000 languages. This
    was done by using unlabeled recordings of various other Christian religious readings.
    From the experiments results, even though the data is from a specific domain,
    it can generalize well.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Meta 使用了宗教文本，例如《圣经》，来构建一个覆盖广泛语言的模型。这些文本有几个有趣的特点：首先，它们被翻译成多种语言，其次，有公开的音频录音，记录了不同语言的人朗读这些文本。因此，训练该模型的主要数据集是《新约》，研究团队能够收集到超过
    1,100 种语言的文本，并提供了每种语言超过 32 小时的数据。他们进一步拓展了识别 4,000 种语言的能力。这是通过使用各种其他基督教宗教阅读的未标注录音实现的。从实验结果来看，尽管数据来自特定领域，但它具有良好的泛化能力。
- en: These are not the only contributions of the work. They created a new preprocessing
    and alignment model that can handle long recordings. This was used to process
    the audio, and misaligned data was removed using a final cross-validation filtering
    step. Recall from one of our previous articles that we saw that one of the challenges
    of Whisper was the incapacity to align the transcription properly. Another important
    step of the approach was the usage of wav2vec 2.0, a self-supervised learning
    model, to train their system on a massive amount of speech data (about 500,000
    hours) in over 1,400 languages. The labeled dataset we discussed previously is
    not enough to train a model of the size of MMS, so wav2vec 2.0 was used to reduce
    the need for labeled data. Finally, the resulting models were then fine-tuned
    for a specific speech task, such as multilingual speech recognition or language
    identification.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这些并不是工作的唯一贡献。他们创建了一个新的预处理和对齐模型，可以处理长时间的录音。这被用来处理音频，错位的数据通过最终的交叉验证过滤步骤被移除。回忆一下我们之前的文章，我们看到
    Whisper 的一个挑战是无法正确对齐转录。该方法的另一个重要步骤是使用 wav2vec 2.0，一个自监督学习模型，来在超过 1,400 种语言的海量语音数据（约
    500,000 小时）上训练他们的系统。我们之前讨论的标注数据集不足以训练 MMS 这样规模的模型，因此使用了 wav2vec 2.0 来减少对标注数据的需求。最后，结果模型会被微调以执行特定的语音任务，例如多语言语音识别或语言识别。
- en: The MMS models were open-sourced by Meta a few days ago and were made available
    in the Fairseq repository. In the next section, we cover what Fairseq is and how
    we can test these new models from Meta.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: MMS 模型在几天前由 Meta 开源，并在 Fairseq 仓库中提供。在下一节中，我们将深入了解 Fairseq 及如何测试这些来自 Meta 的新模型。
- en: 'Overview of the Fairseq Repository: A Powerful Toolkit for Sequence-to-Sequence
    Learning'
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Fairseq 仓库概览：一个强大的序列到序列学习工具包
- en: Fairseq is an open-source sequence-to-sequence toolkit developed by Facebook
    AI Research, also known as FAIR. It provides reference implementations of various
    sequence modeling algorithms, including convolutional and recurrent neural networks,
    transformers, and other architectures.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Fairseq 是一个开源的序列到序列工具包，由 Facebook AI Research（也称为 FAIR）开发。它提供了各种序列建模算法的参考实现，包括卷积神经网络、递归神经网络、变压器及其他架构。
- en: The Fairseq repository is based on PyTorch, another open-source project initially
    developed by the Meta and now under the umbrella of the Linux Foundation. It is
    a very powerful machine learning framework that offers high flexibility and speed,
    particularly when it comes to deep learning.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Fairseq 仓库基于 PyTorch，这是另一个开源项目，最初由 Meta 开发，现在归 Linux 基金会旗下。它是一个非常强大的机器学习框架，提供了高灵活性和速度，特别是在深度学习方面。
- en: The Fairseq implementations are designed for researchers and developers to train
    custom models and it supports tasks such as translation, summarization, language
    modeling, and other text generation tasks. One of the key features of Fairseq
    is that it supports distributed training, meaning it can efficiently utilize multiple
    GPUs either on a single machine or across multiple machines. This makes it well-suited
    for large-scale machine learning tasks.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Fairseq 实现旨在帮助研究人员和开发者训练自定义模型，支持翻译、摘要、语言建模和其他文本生成任务。Fairseq 的一个关键特性是它支持分布式训练，这意味着它可以有效利用多个
    GPU，无论是在单台机器上还是跨多台机器。这使其非常适合大规模机器学习任务。
- en: Massively Multilingual Speech Models
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大规模多语言语音模型
- en: 'Fairseq provides two pre-trained models for download: MMS-300M and MMS-1B.
    You also have access to fine-tuned models available for different languages and
    datasets. For our purpose, we test the MMS-1B model fine-tuned for 102 languages
    in the FLEURS dataset and also the MMS-1B-all, which is fine-tuned to handle 1162
    languages (!), fine-tuned using several different datasets.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Fairseq 提供了两个可下载的预训练模型：MMS-300M 和 MMS-1B。你还可以访问针对不同语言和数据集的微调模型。为了我们的目的，我们测试了在
    FLEURS 数据集中针对 102 种语言微调的 MMS-1B 模型，以及针对 1162 种语言（！）微调的 MMS-1B-all，微调使用了多个不同的数据集。
- en: Implementing Automatic Speech Recognition with Massively Multilingual Speech
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现自动语音识别与大规模多语言语音
- en: Remember that these models are still in research phase, making testing a bit
    more challenging. There are additional steps that you would not find with production-ready
    software.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这些模型仍处于研究阶段，使得测试变得更具挑战性。与生产就绪的软件相比，还需要额外的步骤。
- en: 'First, you need to set up a `.env` file in your project root to configure your
    environment variables. It should look something like this:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要在项目根目录下设置一个`.env`文件，以配置你的环境变量。它应如下所示：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next, you need to configure the YAML file located at `fairseq/examples/mms/asr/config/infer_common.yaml`.
    This file contains important settings and parameters used by the script.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你需要配置位于`fairseq/examples/mms/asr/config/infer_common.yaml`的YAML文件。该文件包含脚本使用的重要设置和参数。
- en: 'In the YAML file, use a full path for the `checkpoint` field like this (unless
    you are using a containerized application to run the script):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在 YAML 文件中，为`checkpoint`字段使用完整路径，如下所示（除非你使用的是容器化应用程序来运行脚本）：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This full path is necessary to avoid potential permission issues unless you
    are running the application in a container.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个完整路径是必要的，以避免潜在的权限问题，除非你在容器中运行应用程序。
- en: 'If you plan on using a CPU for computation instead of a GPU, you will need
    to add the following directive to the top level of the YAML file:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你计划使用 CPU 进行计算而不是 GPU，则需要在 YAML 文件的顶部添加以下指令：
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This setting directs the script to use the CPU for computations.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 此设置指示脚本使用 CPU 进行计算。
- en: We use the `dotevn` python library to load these environment variables in our
    Python script. Since we are overwriting some system variables, we will need to
    use a trick to make sure that we get the right variables loaded. We use the`dotevn_values`method
    and store the output in a variable. This ensures that we get the variables stored
    in our `.env`file and not random system variables even if they have the same name.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`dotevn` Python库在Python脚本中加载这些环境变量。由于我们正在覆盖一些系统变量，我们需要使用一个技巧以确保加载正确的变量。我们使用`dotevn_values`方法并将输出存储在一个变量中。这确保了我们获取的是存储在`.env`文件中的变量，而不是即使名称相同的随机系统变量。
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Then, we can clone the fairseq GitHub repository and install it in our machine.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以克隆fairseq GitHub仓库并在我们的机器上安装它。
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We already discussed the models that we use in this article, so let’s download
    them to our local environment.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了本文中使用的模型，因此让我们将它们下载到本地环境中。
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'There is one additional restriction related to the input of the MMS model,
    the sampling rate of the audio data needs to be 16000 Hz. In our case, we defined
    two ways to generate these files: one that converts video to audio and another
    that resamples audio files for the correct sampling rate.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 关于MMS模型的输入，还有一个额外的限制，即音频数据的采样率需要为16000 Hz。在我们的情况下，我们定义了两种生成这些文件的方法：一种是将视频转换为音频，另一种是重新采样音频文件以获得正确的采样率。
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We are now ready to run the inference process using our MMS-1B-all model, which
    supports 1162 languages.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备使用支持1162种语言的MMS-1B-all模型运行推理过程。
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Automatic Speech Recognition Results with Fairseq
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Fairseq的自动语音识别结果
- en: In this section, we describe our experimentation setup and discuss the results.
    We performed ASR using two different models from Fairseq, MMS-1B-all and MMS-1B-FL102,
    in both English and Portuguese. You can find the audio files in my GitHub repo.
    These are files that I generated myself just for testing purposes.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们描述了我们的实验设置并讨论了结果。我们使用Fairseq的两种不同模型（MMS-1B-all和MMS-1B-FL102）在英语和葡萄牙语中进行了ASR。你可以在我的GitHub库中找到音频文件。这些文件是我自己生成的，仅用于测试目的。
- en: 'Let’s start with the MMS-1B-all model. Here is the input and output for the
    English and Portuguese audio samples:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从MMS-1B-all模型开始。以下是英语和葡萄牙语音频样本的输入和输出：
- en: '**Eng**: just requiring a small clip to understand if the new facebook research
    model really performs on'
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**Eng**：只需一个小片段即可了解新的Facebook Research模型在语音识别任务中的实际表现'
- en: ''
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Por**: ora bem só agravar aqui um exemplo pa tentar perceber se de facto
    om novo modelo da facebook research realmente funciona ou não vamos estar'
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**Por**：现在只需在这里提供一个示例，以便尝试了解Facebook Research的新模型是否真正有效，我们将进行测试。'
- en: 'With the MMS-1B-FL102, the generated speech was significantly worse. Let’s
    see the same example for English:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用MMS-1B-FL102时，生成的语音质量显著下降。我们来看一下英语的相同示例：
- en: '**Eng**: just recarding a small ho clip to understand if the new facebuok research
    model really performs on speed recognition tasks lets see'
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**Eng**：只需回顾一下小片段，以了解新的Facebook Research模型在速度识别任务中的实际表现，让我们看看'
- en: While the speech generated is not super impressive for the standard of models
    we have today, we need to address these results from the perspective that these
    models open up ASR to a much wider range of the global population.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管生成的语音对于我们今天拥有的模型标准来说并不特别令人印象深刻，但我们需要从这些模型可以使ASR服务于更广泛全球人群的角度来考虑这些结果。
- en: Conclusion
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The Massively Multilingual Speech model, developed by Meta, represents one more
    step to foster global communication and broaden the reach of language technology
    using AI. Its ability to understand over 4,000 languages and function effectively
    across 1,162 of them increases accessibility for numerous languages that have
    been traditionally underserved.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Meta开发的Massively Multilingual Speech模型代表了推动全球沟通和扩大语言技术应用范围的又一步。它能够理解超过4000种语言，并在其中1162种语言中有效运行，提高了许多传统上被忽视语言的可及性。
- en: Our testing of the MMS models showcased the possibilities and limitations of
    the technology at its current stage. Although the speech generated by the MMS-1B-FL102
    model was not as impressive as expected, the MMS-1B-all model provided promising
    results, demonstrating its capacity to transcribe speech in both English and Portuguese.
    Portuguese has been one of those underserved languages, specially when we consider
    Portuguese from Portugal.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对 MMS 模型的测试展示了该技术在当前阶段的可能性和局限性。尽管 MMS-1B-FL102 模型生成的语音不如预期令人印象深刻，但 MMS-1B-all
    模型提供了有希望的结果，展示了其在英语和葡萄牙语中的转录能力。葡萄牙语一直是那些服务不足的语言之一，特别是当我们考虑到来自葡萄牙的葡萄牙语时。
- en: Feel free to try it out in your preferred language and to share the transcription
    and feedback in the comment section.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎尝试你喜欢的语言，并在评论区分享转录结果和反馈。
- en: 'Keep in touch: [LinkedIn](https://www.linkedin.com/in/luisbrasroque/)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 保持联系：[LinkedIn](https://www.linkedin.com/in/luisbrasroque/)
- en: References
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] — [Pratap, V., Tjandra, A., Shi, B., Tomasello, P., Babu, A., Kundu, S.,
    Elkahky, A., Ni, Z., Vyas, A., Fazel-Zarandi, M., Baevski, A., Adi, Y., Zhang,
    X., Hsu, W.-N., Conneau, A., & Auli, M. (2023). Scaling Speech Technology to 1,000+
    Languages. arXiv.](https://scontent.fopo6-1.fna.fbcdn.net/v/t39.8562-6/348827959_6967534189927933_6819186233244071998_n.pdf?_nc_cat=104&ccb=1-7&_nc_sid=ad8a9d&_nc_ohc=nLkMCxk5EFMAX_VcRa0&_nc_ht=scontent.fopo6-1.fna&oh=00_AfDVqBOzIgYWtX65XMtxu0wXy8DPWJWl-Z_xwk0eZxUDsw&oe=64764402)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] — [Pratap, V., Tjandra, A., Shi, B., Tomasello, P., Babu, A., Kundu, S.,
    Elkahky, A., Ni, Z., Vyas, A., Fazel-Zarandi, M., Baevski, A., Adi, Y., Zhang,
    X., Hsu, W.-N., Conneau, A., & Auli, M. (2023). 将语音技术扩展到 1,000 多种语言。arXiv.](https://scontent.fopo6-1.fna.fbcdn.net/v/t39.8562-6/348827959_6967534189927933_6819186233244071998_n.pdf?_nc_cat=104&ccb=1-7&_nc_sid=ad8a9d&_nc_ohc=nLkMCxk5EFMAX_VcRa0&_nc_ht=scontent.fopo6-1.fna&oh=00_AfDVqBOzIgYWtX65XMtxu0wXy8DPWJWl-Z_xwk0eZxUDsw&oe=64764402)'
