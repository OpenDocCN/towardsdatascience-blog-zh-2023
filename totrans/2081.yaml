- en: The Unstructured Data Funnel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-unstructured-data-funnel-245f72925176](https://towardsdatascience.com/the-unstructured-data-funnel-245f72925176)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/70a9cff10a032a2a0f68964b01824643.png)'
  prefs: []
  type: TYPE_IMG
- en: How far down you go determines how much you pay. Photo by [Ricardo Gomez Angel](https://unsplash.com/@rgaleriacom?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/black-and-white-round-tunnel-JB3mBjGt94Y?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  prefs: []
  type: TYPE_NORMAL
- en: Why a funnel is the centre of the war between data’s heaviest hitters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@hugolu87?source=post_page-----245f72925176--------------------------------)[![Hugo
    Lu](../Images/045de11463bb16ea70a816ba89118a9e.png)](https://medium.com/@hugolu87?source=post_page-----245f72925176--------------------------------)[](https://towardsdatascience.com/?source=post_page-----245f72925176--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----245f72925176--------------------------------)
    [Hugo Lu](https://medium.com/@hugolu87?source=post_page-----245f72925176--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----245f72925176--------------------------------)
    ·9 min read·Dec 15, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '*If you’re not a Medium member, you can read here for* [*free*](https://www.getorchestra.io/blog/the-unstructured-data-funnel)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unstructured data takes varying forms. It’s typically text-heavy, but may contain
    data such as dates, numbers, and dictionaries as well. Data Engineers commonly
    encounter unstructured data in the form of deeply-[nested json](https://www.ibm.com/docs/en/db2/11.5?topic=documents-json-nested-objects)s.
    However the term “unstructured” data really refers to anything non-tabular; in
    fact, over 80% of the [world’s data is unstructured](https://www.unleash.so/a/answers/database-management/how-much-data-in-the-world-is-unstructured).
  prefs: []
  type: TYPE_NORMAL
- en: 'While unstructured data may seem innocuous to us data practitioners, it’s making
    huge waves at a macro-level. Indeed, GPT Models [are all trained](https://en.wikipedia.org/wiki/ChatGPT)
    on unstructured data. This was correctly observed by Tomasz Tunguz in a [recent
    article](https://www.linkedin.com/pulse/snow-angels-come-early-data-snowflakes-strength-spells-tomasz-tunguz-bdggc/)
    on Snowflake’s Earnings Call:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a8e4d62926abb2208c99028959ff279e.png)'
  prefs: []
  type: TYPE_IMG
- en: Taken from “Snow Angels” by Tomasz Tunguz
  prefs: []
  type: TYPE_NORMAL
- en: It might seem odd to view unstructured data in this financial and macroeconomic
    context. My first job was in Investment Banking, so I’m nostalgic when it comes
    to reading stuff like this. “Unstructured data is the growth engine” could make
    sense to me — it sounds like a really big market [tailwind](https://www.peakframeworks.com/post/headwinds-vs-tailwinds)!
  prefs: []
  type: TYPE_NORMAL
- en: But it’s been a while since I’ve been aligning Powerpoint boxes. Conceptually,
    unstructured data is now a deeply-nested json waiting to be processed. But it’s
    clear from the [earnings call](https://seekingalpha.com/article/4655043-snowflake-inc-snow-q3-2024-earnings-call-transcript)
    unstructured data isn’t now just JSONs (was it ever?) but text, documents, [videos](https://benn.substack.com/p/avg-text),
    and the like.
  prefs: []
  type: TYPE_NORMAL
- en: 'What’s emerged is that this data powers some of the most soon-to-be critical
    use-cases, and where it’s processed is of paramount importance to the two heavy-hitting
    companies in the data world: Databricks and Snowflake. Let’s dive into why.'
  prefs: []
  type: TYPE_NORMAL
- en: Why is unstructured data important?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[GPT](https://en.wikipedia.org/wiki/GPT-3) Models feed on data. Specifically,
    they [feed off unstructured](https://www.forbes.com/sites/forbestechcouncil/2023/07/24/unleashing-the-power-of-unstructured-data-the-rise-of-large-ai-models/?sh=76deafdd40f8)
    data. These are things like text documents, html files, and code snippets. As
    companies increasingly look to implement [LLMs](https://cloud.google.com/ai/llms?hl=en)
    in production, the value of processing this data increases because its demand
    increases. Therefore, its value to vendors like Snowflake and Databricks increases.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However there is a second element to processing a specific type of unstructured
    data. Take Nested JSON as an example. Nested JSONs get *unnested* or *cleaned*
    when they are processed. This means you might go from this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Processing the second JSON requires less compute compared to the initial clean
    of the data, which processes the first much larger object. This means where the
    first “clean” happens within the data pipeline significantly affects the compute
    used.
  prefs: []
  type: TYPE_NORMAL
- en: All unstructured data follows this pattern. Snowflake’s [Document AI](https://www.youtube.com/watch?v=dN_IZp2W148)
    takes documents like pdfs and extracts data into tabular form. This means the
    heavy part of the processing happens once, and the resultant data is a lot cleaner
    and easier to process.
  prefs: []
  type: TYPE_NORMAL
- en: The Unstructured data funnel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The focus on *where* data processing happens is important to cloud vendors like
    Snowflake and Databricks because they charge a mark-up based on cloud [compute](https://www.databricks.com/product/pricing).
    This means the more computational power you need, the more you pay them. We saw
    in the previous section that the unstructured data is of increasing importance
    due to LLMs, but also that the compute required to work with unstructured data
    decreases it gets processed further and further through a data pipeline. This
    is intuitive, because as data progresses through data pipelines it gets cleaner
    and more aggregated.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can visualise this by imagining what our data pipeline infrastructure looks
    like. Most of us typically have an architecture which is a subset of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/61498b68ecfae2be2c4ad77e7c6f5ea7.png)'
  prefs: []
  type: TYPE_IMG
- en: The unstructured data funnel. The width of the funnel is proportional to the
    data quantity that needs to be processed. The compute required at each stage as
    data flows through the funnel is non-linear, since operations such as unstructured
    data cleaning, joining data sources, and running analytical workflows are influenced
    by factors other than data quantity. Image the author’s.
  prefs: []
  type: TYPE_NORMAL
- en: Data movement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first section of the funnel is where data teams first come into contact
    with unstructured data. It’s a data movement layer, that may be either batch or
    streaming in architecture. There is no storage element associated with this layer,
    but vendors such as Fivetran, Portable or Striim are invested in doing *some*
    transformation (“ETL” or “ET L” rather than “ELT” or “EL T”), which uses compute
    and reduces the size of the data moving to the next layer.
  prefs: []
  type: TYPE_NORMAL
- en: These tools are restricted in terms of the processing they can do, because they
    do not have a full data history so cannot do complex operations such as backfills
    or slowly-changing dimensions. However, they are suitable for simple transformations
    like joining streams or de-nesting unstructured data. Most of these vendors don’t
    work with unstructured data like text files anyway. You can use cloud-native services
    to do these services e.g. Azure EventHub, BigQuery PubSub, so these logos infringe
    on this part of the funnel too.
  prefs: []
  type: TYPE_NORMAL
- en: '**Compute up for grabs**: Medium'
  prefs: []
  type: TYPE_NORMAL
- en: Data Lake / Object Storage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The second layer refers to data in object storage such as Google Object Storage,
    AWS S3 or Azure ADLS Gen-2\. These are storage solutions from big three cloud
    providers that store data in any file format. This layer of the funnel is the
    first where all the data is centralised, and where compute in all its forms is
    easily available, either for direct lease from cloud providers or via vendors
    like Databricks for Spark. This layer is a good candidate for complex processes,
    and an especially good one for dimensional and complexity reduction. This means
    the compute at stake is exceptionally high.
  prefs: []
  type: TYPE_NORMAL
- en: In my view, this layer is where processing unstructured data makes the most
    sense. You can store anything here. You have enormous amounts of compatible cloud
    infrastructure (everything can interact with S3). You have a ready-made storage
    layer for your processed data. It makes a lot of sense to do this kind of processing
    here because it’s essentially more flexible than say, a data warehouse. Data Lakes
    were built to house data in any format — isn’t that exactly what unstructured
    data *is*?
  prefs: []
  type: TYPE_NORMAL
- en: '**Compute up for grabs**: Very high'
  prefs: []
  type: TYPE_NORMAL
- en: Data Warehouse / SQL Layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This layer of the funnel refers to data stored in a specific format or sets
    of specific formats, and is generally done so it can be easily queried and manipulated
    using SQL-like statements. It is no secret that Snowflake have their own file
    format which facilitates this, and is the reason you pay an “[ingress charge](http://www.getorchestra.io/blog/understanding-snowflakes-pricing-model-ingress-and-egress-charges)”
    to get data “[into Snowflake](http://www.getorchestra.io/blog/understanding-snowflakes-pricing-model-ingress-and-egress-charges)”.
    On the Databricks side, they have [.delta which is actually an abstraction over
    .parquet](http://www.getorchestra.io/blog/exploring-databricks-delta-and-parquet-file-formats).
    It’s also open-source. There are others such as .iceberg which are “cool” and
    facilitate external tables in Snowflake (which brings Snowflake up a level of
    the funnel).
  prefs: []
  type: TYPE_NORMAL
- en: This area is suitable for processing structured data or tabular data. However
    operations such as flattening JSON can also be performed here (see for example
    in [BigQuery](https://stackoverflow.com/questions/55244750/flatten-nested-json-string-to-different-columns-in-google-bigquery)
    or [Snowflake](https://www.google.com/search?q=snowflake+flatten&rlz=1C1RXQR_enGB1078GB1078&oq=snowflake+flatten&gs_lcrp=EgZjaHJvbWUqBwgAEAAYgAQyBwgAEAAYgAQyBwgBEAAYgAQyBwgCEAAYgAQyBwgDEAAYgAQyBwgEEAAYgAQyBwgFEAAYgAQyBwgGEAAYgAQyBwgHEAAYgAQyBwgIEAAYgAQyBwgJEAAYgATSAQgxNDQ0ajBqNKgCALACAA&sourceid=chrome&ie=UTF-8)).
    There is a general perception among data engineers this is not the best use of
    SQL-based compute. The main objection is that it is challenging and slow to perform
    such operations incrementally, and therefore expensive. However the real cost
    has already been incurred in *moving* the data. You’ve already paid to move data
    *through* the funnel. It would be cheaper if it was transformed or de-structured
    higher up the funnel, and then the volume *moving* through the funnel is minimised.
  prefs: []
  type: TYPE_NORMAL
- en: It’s interesting this was the area that got attention in the Snowflake earnings
    call. Given most engineers I speak to prefer to do data manipulation as far up
    the funnel as possible to save on cost, it’s likely that the mantra of “just get
    the data into Snowflake” is potentially striking a chord with enterprise data
    executives and CIOs. It’s also possible that features such as DocumentAI and SnowPark
    Container Services essentially mean the lines between SQL Warehouse and “Data
    Lakehouse” are blurring — the same compute can be used across data in different
    parts of the funnel.
  prefs: []
  type: TYPE_NORMAL
- en: Something to be wary of is the double-mark-up you would pay on compute processing
    unstructured data in a data warehouse. If you were to use a service like Databricks,
    you’d process text files sitting in object storage using compute power from your
    cloud provider, using Databricks as the middleman, so you pay a single mark-up
    for compute only. If you use a data warehouse, you pay a mark-up on storage, and
    on compute. You also don’t know what *exactly they do* with that compute in order
    to analyse documents, so you may end up paying a bit more there too. You can avoid
    the former if the warehouse supports [external tables](https://docs.snowflake.com/en/user-guide/tables-external-intro).
  prefs: []
  type: TYPE_NORMAL
- en: To round off this layer — running SQL workloads efficiently requires data to
    be in a different, specific format. This created a market for data warehouses,
    that convert this data into queryable and transformable formats such as .delta.
    Unstructured data is in some ways, the opposite of this, and seems to *belong*
    where it already *exists,* that is, in a Data Lake. To this end, it doesn’t really
    make sense to use compute from the Warehouse layer to do unstructured data processing
    in the context of LLMs. Indeed, some popular Snowflake use-cases such as Snowpark
    Container Services and External Tables already resemble a compute / storage model
    more similar to a Lakehouse.
  prefs: []
  type: TYPE_NORMAL
- en: '**Compute up for grabs**: High'
  prefs: []
  type: TYPE_NORMAL
- en: Data Activation — the “Last mile” of analytics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A final mention to the end of the funnel — data activation. This is the “Last
    mile” of analytics, and typically involves doing small checks to ensure processes
    can kick off, and then moving cleaned data to systems of operation. It refers
    to applications that interact with cleaned and aggregated data.
  prefs: []
  type: TYPE_NORMAL
- en: These could be the classic reverse ETL use case, dashboards, or automated slack
    / email alerts.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, all your data (structured and unstructured) is likely to have
    been cleaned sufficiently. Your data engineering, analytics engineering, and product
    teams have probably all had a good sniff at it, tested it, cleaned it, aggregated
    it.
  prefs: []
  type: TYPE_NORMAL
- en: This means from the compute side, there’s really nothing left to do.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore the part of the unstructured data funnel that is *least interesting*
    and offers the *least opportunity* for charging data teams on compute. Not only
    is all the data clean, flat, and perfect. It’s also a lot smaller.
  prefs: []
  type: TYPE_NORMAL
- en: '**Compute up for grabs**: Low'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article offers data practitioners a new way to think about data processing
    in the context of unstructured data. Unstructured data is unique in the sense
    that text documents, html files, deeply nested JSON and everything in between
    require a first, heavy computational step to extract value from the data. Where
    this step happens is important for large data cloud vendors because it closely
    correlates with usage and therefore revenue. It is particularly interesting at
    the moment, since the deployment of LLMs and GPT models have created a demand
    for this data that was hitherto fairly useless and uninteresting to most data
    practitioners.
  prefs: []
  type: TYPE_NORMAL
- en: Through the funnel illustration, we see the longer data teams “delay” this cumbersome
    processing step, the greater the cost, since the total data moved is greater.
    Furthermore, given the variety of file formats and types of operation required,
    it seems to make most sense to focus on processing data at the object-storage
    level. This is well illustrated by Snowflake apparently “moving up the funnel”
    to facilitate unstructured data processing.
  prefs: []
  type: TYPE_NORMAL
- en: It also begs the question if cloud vendors will continue moving up the funnel.
    Parsing an email from gmail could be simple enough. Suppose a tool existed that
    specialised in moving data from gmail to object storage — why shouldn’t that tool
    also do the “T”; in this case, parse out the helpful information (using AI, presumably)
    and dump a flat table of records directly into S3? Does the fact that most helpful
    unstructured live in cloud databases that belong to Google and Microsoft (who
    are neither Databricks or Snowflake, and have their *own* suite of data products)?
  prefs: []
  type: TYPE_NORMAL
- en: I honestly have no idea what the answer to this question is. However I’m certain
    there is a huge economic incentive to process as much of the data as early as
    possible. How far up, is up for debate, as shown by Snowflake’s apparent success
    in convincing companies to move data into Snowflake before doing any transformation.
    The days of BI tools charging hundreds of thousands of dollars are over. The battle
    for S3 has begun 🔥
  prefs: []
  type: TYPE_NORMAL
