- en: Matrix Approximation in Data Streams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/matrix-approximation-in-data-streams-7585720e8671](https://towardsdatascience.com/matrix-approximation-in-data-streams-7585720e8671)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Approximate a matrix without having all of its rows
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mina.ghashami?source=post_page-----7585720e8671--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page-----7585720e8671--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7585720e8671--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7585720e8671--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page-----7585720e8671--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7585720e8671--------------------------------)
    ·13 min read·Sep 17, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dc927665ca6a15dbc7eedcdf011561fe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image credit: unsplash.com'
  prefs: []
  type: TYPE_NORMAL
- en: Matrix approximation is a heavily studied sub-field in data mining and machine
    learning. A large set of data analysis tasks rely on obtaining a *low rank approximation*of
    matrices. Examples are dimensionality reduction, anomaly detection, data de-noising,
    clustering, and recommendation systems. In this article, we look into the problem
    of matrix approximation, and how to compute it when the whole data is not available
    at hand!
  prefs: []
  type: TYPE_NORMAL
- en: The content of this article is partly taken from my [lecture](https://web.stanford.edu/class/cs246/slides/17-matrix_sketching.pdf)
    at [Stanford -CS246 course](https://web.stanford.edu/class/cs246/). I hope you
    find it useful. Please find the full content [here](https://web.stanford.edu/class/cs246/slides/17-matrix_sketching.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Data as a matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most data generated on web can be represented as a matrix, where each row of
    the matrix is a data point. For example, in routers every packet sent across the
    network is a data point that can be represented as a row in a matrix of all data
    points. In retail, every purchase made is a row in the matrix of all transactions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c2174d5fadd7df8c1421ee119c9792c1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Data as a matrix — Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, almost all data generated over web is of a *streaming nature;*
    meaning the data is generated by an external source at a rapid rate which we have
    no control over. Think of all searches users make on Google search engine in a
    every second. We call this data the *streaming data*; because just like a stream
    it pours in.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some examples of typical streaming web-scale data are as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e077ae79ff78e06cf0eeb9950c667107.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Size of typical streaming web-scale data — Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: Think of streaming data as a matrix *A* containing *n* rows in *d*-dimensional
    space, where typically *n >> d.* Often *n* is in order of billions and increasing.
  prefs: []
  type: TYPE_NORMAL
- en: Data streaming model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In streaming model, data arrives at high speed, one row at a time, and algorithms
    must process the items fast, or they are lost forever.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/10a27202839bd9b4ebaadd879632b4aa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Data streaming model — Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: In data streaming model, algorithms can only make one pass over the data and
    they work with a small memory to do their processing.
  prefs: []
  type: TYPE_NORMAL
- en: Rank-k approximation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Rank-k approximation* of a matrix *A* is a smaller matrix *B* of rank *k*
    such that *B* approximates *A accurately.* Figure 2 demonstrates this notion.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/53dc66610adbfedd4d744da8ee67bd13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Getting a much smaller sketch B from A — Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: '*B* is often called a sketch of *A.* Note, in data streaming model, B will
    be much smaller than A such that it fits in memory. In addition, rank(B) << rank(A).
    For example, if A is a term-document matrix with 10 billion documents and 1 million
    words then B would probably be a 1000 by million matrix; i.e. 10 million times
    less rows!'
  prefs: []
  type: TYPE_NORMAL
- en: 'The rank-k approximation has to approximate A “*accurately*”. While accurately
    is a vague notion, we can quantify it via various error definitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '1️⃣ **Covariance error**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The covariance error is the Frobenius norm or L2 norm of differences between
    covariance of A and covariance of B. This error is mathematically defined as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/658c13473308fa953a1b50cc5155ef34.png)'
  prefs: []
  type: TYPE_IMG
- en: covariance error definition — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: '2️⃣ **Projection error**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The projection error is the norm of the residual when data points in A are
    projected onto the subspace of B. This residual norm is measured as either L2
    norm or Frobenius norm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/310d422bb40a8df456d5b33e90b82a46.png)'
  prefs: []
  type: TYPE_IMG
- en: projection error definition — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: These errors assess the quality of the approximation; the smaller they are the
    better the approximation is. But how small can they be?
  prefs: []
  type: TYPE_NORMAL
- en: When we compute these errors, we must have a baseline to compare them against.
    The baseline, everyone uses in field of matrix sketching is the rank-k approximation
    created by *Singular Value Decomposition (SVD)*! SVD computes the best rank-k
    approximation! Meaning it causes the smallest error on both “*covariance error”*
    and “*projection error*”.
  prefs: []
  type: TYPE_NORMAL
- en: 'The best rank-k approximation to A is denoted as Aₖ. Therefore, the least error
    caused by SVD is :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/037bdb52e3a227e51d760b95a27eef7a.png)'
  prefs: []
  type: TYPE_IMG
- en: least rank k approximation error — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: 'SVD, decomposes a matrix *A* into three matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: The left singular matrix *U*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The singular values matrix S
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The right singular matrix V
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: U and V are orthonormal, meaning that their columns are of unit norm and they
    are orthogonal; i.e. the dot product of every two columns in U (similarly in V)
    is zero. The matrix S is diagonal; only entries on the diagonal are non-zeros
    and they are sorted in descending order.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4e10bfbec83481719adb175f24178a92.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Singular value decomposition — Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: 'SVD computes the best rank-k approximation by taking the first k columns of
    U and V and the first k entries of S:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/674b0fccc2d859376d7e9d4bb285847c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Rank k approximation by SVD — Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned, the Aₖ computed in this way has the lowest approximation error
    among any matrices B with rank k or lower. However, SVD is a very time-consuming
    method, it requires runtime O(nd²) if A is n-by-d, and it is not applicable to
    matrices in data streaming fashion. In addition, SVD is not efficient for sparse
    matrices; it does not utilize sparsity of the matrix in computing approximation.
  prefs: []
  type: TYPE_NORMAL
- en: ❓Now the question is how do we compute matrix approximation in streaming fashion
    ?
  prefs: []
  type: TYPE_NORMAL
- en: '**There are three main family of methods in streaming matrix approximations:**'
  prefs: []
  type: TYPE_NORMAL
- en: 1️⃣ Row sampling based
  prefs: []
  type: TYPE_NORMAL
- en: 2️⃣ Random projection based
  prefs: []
  type: TYPE_NORMAL
- en: 3️⃣ Iterative sketching
  prefs: []
  type: TYPE_NORMAL
- en: '**Row sampling based methods**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'These methods sample a subset of “important” rows with respect to a well-defined
    probability distribution. These methods differ is in how they define the notion
    of “importance”. The generic framework is that they construct the sketch B as
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: They first assign a probability to each row in the streaming matrix *A*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then they sample *l* rows from *A* (often with replacement) to construct *B*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lastly, they rescale *B* appropriately to make it an unbiased estimate of *A*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/abb1b4fd0514a3f347059642385d0014.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Row sampling with replacement to build sketch B — Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: Note, the probability assigned to rows in step 1, is in fact the “importance”
    of the row. Think of “importance” of an item as the weight associated to the item
    e.g. for file records, the weight can be size of the files. Or for IP addresses,
    the weight can be the number of times the IP address makes a request.
  prefs: []
  type: TYPE_NORMAL
- en: 'In matrices, each item is a row vector, and its weight is the squared of its
    norm; also called *L2 norm*. There is a row sampling algorithm that samples with
    respect to L2 norm of rows in a one pass over data. This algorithm is called “*L2-norm
    row sampling*”, and its pseudocode is as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c4351950a8a133bb15b461a8547faac1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: L2-norm row sampling algorithm — Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: 'This algorithm samples *l = O(k/ε²)* rows with replacement and achieves the
    following error bound:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c4f00ca30599e08831b10004abfa0dee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Error guarantee of L2-norm row sampling — Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: Note, this is a weak error bound, as it is bounded by total Frobenius norm of
    A, which can be a large number! There is an extension of this algorithm that performs
    better; let’s take a look at it.
  prefs: []
  type: TYPE_NORMAL
- en: '**Extension**: There is a variation of this algorithm that samples both rows
    and columns! It is called “*CUR*” algorithm and it performs better than “*L2-norm
    row sampling”* method. The *CUR* method creates three matrices C, U and R by sampling
    rows and columns of A. Here is how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: *CUR* first samples few columns of *A*, each with the probability proportional
    to the norm of the columns. This makes the matrix *C*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f850303fc206dc110d8bea962fbe599b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: CUR algorithm step 1— Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Then *CUR* samples few rows of *A*, each with probability proportional
    to the norm of the rows. This forms matrix *R*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: The CUR then computes the pseudo-inverse of the intersection of C and
    R. This is called matrix U.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2d65f393f1a9bdf84c008271097e31b6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: CUR algorithm step 2,3— Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the product of these three matrices, i.e. *C.U.R* approximates *A*,
    and provides a low-rank approximation. This algorithm achieves the following error
    bound sampling *l = O(k log k/ε²)* rows and columns.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1e4dcb2aafc66e61401b8c4ec1791d95.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: CUR Error guarantee — Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: Note, how this is a much tighter bound as compared to *L2-norm row sampling.*
  prefs: []
  type: TYPE_NORMAL
- en: '**Summary:** The row sampling family of methods (CUR included) samples rows
    (and columns) to form a low-rank approximation; therefore they are very intuitive
    and form interpretable approximations.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we see another family of methods that are data oblivious.
  prefs: []
  type: TYPE_NORMAL
- en: '**Random projection based methods**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The key idea of these group of methods is that if points in a vector space are
    projected onto a randomly selected subspace of suitably high dimension, then the
    distances between points are approximately preserved*.*
  prefs: []
  type: TYPE_NORMAL
- en: '**Johnson-Lindenstrauss Transform (JLT)** specifies this nicely as following:
    ***d*** datapoints in any dimension (e.g. n-dimensional space for n≫d) can get
    embedded into roughly ***log d*** dimensional space, such that their pair-wise
    distances are preserved to some extent.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A more precise and mathematical definition of JLT is as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fa42af817b3ba58390a124621bb9dd9e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: JLT definition — Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many ways to construct a matrix S that preserve pair-wise distances.
    All such matrices are called to have the *JLT property*. The image below, shows
    few well-known ways of creating such matrix S:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/40fec39e26e49d953b0503b0d5d96494.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: 'One simple construction of *S,* as shown above,is to pick entries of *S* to
    be independent random variables drawn from *N(0,1),* and rescale S by √(1/r):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0eb53e2113540584d8817b5bcf163dbc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: JLT matrix — Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: 'This matrix has the *JLT property* [6]*,* and we use it to design a random
    projection method as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9ee91d359bb577156591f1798af06386.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Random projection method — Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note the second step, projects data points from a high n-dimensional space
    into a lower r-dimensional space. It’s easy to show [6]that this method produces
    an unbiased sketch:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bb25673959486234e80ad3ab294d0d4a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: Random projection provides an unbiased approximation— Image by the
    author'
  prefs: []
  type: TYPE_NORMAL
- en: The random projection method achieves the following error guarantee if it sets
    *r = O(k/ε + k log k).* Note that they achieve better bound than row sampling
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0b86a215b239461b2af2ae74e967a33c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: Random projection error bound— Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: There is a similar line of work to random projection that achieves better time
    bound. It is called *Hashing techniques* [5]. This method take a matrix S which
    has only one non-zero entries per column and that entry is either 1 or -1\. They
    compute the approximation as *B = SA.*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b5950efa90c38aa8d34de9166328aaf4.png)'
  prefs: []
  type: TYPE_IMG
- en: Hashing technique — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: '**Summary**: Random projection methods are computationally efficient, and are
    data oblivious as their computation involves only a random matrix S. Compare it
    to row sampling methods that need to access data to form a sketch.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Iterative Sketching**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These methods work over a stream A=<a1,a2,…> where each item is read once, get
    processed quickly and not read again. Upon reading each item, they update the
    sketch B.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4ec8205e7236541afe7681edcecf06b3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: Iterative sketching methods — Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: State of the art method in this group is called “*Frequent Directions*”, and
    it is based on *Misra-Gries algorithm* for finding frequent items in a data stream.
    In what follows, we first see how Misra-Gries algorithm for finding frequent items
    work, then we extend it to matrices.
  prefs: []
  type: TYPE_NORMAL
- en: Misra-Gries algorithm for finding frequent items
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose there is a stream of items, and we want to find frequency *f(i)* of
    each item.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/54d29ccd557f9d13039452fd568f5703.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19: frequent item counting over streams — Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: If we keep ***d*** counters, we can count frequency of every item. But it’s
    not good enough as for certain domains such IP addresses, queries, etc. the number
    of unique items are way too many.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/206173be287481bc8955b9da0bedef23.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20: d counters for item frequency estimation — Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: 'So let’s Let’s keep *l* counters where *l≪d.* If a new item arrives in the
    stream that is in the counters, we add 1 to its count:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3a394fab3373e54819d58c3b2995ae35.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21: incrementing counter of an item — image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: If the new item is not in the counters and we have space, we create a counter
    for it and set it to 1.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0708aad8dd0be90b5b604c637ae20f16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22: setting counter for a new item — Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: 'But if we don’t have space for the new item (here the new item is the brown
    box), we get the median counter i.e. counter at position *l/2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/db530dd4cd999766870474f0a0d4254e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 23: subtracting middle counter from every one.— Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: 'and subtract it from all counters. For all counters that get negative, we reset
    them to zero. So it becomes as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6cf66db80be1c254e6e4e3bf4f034900.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 24: half of counters are zero — Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: As we see, now we have space for the new item, so we continue processing the
    stream🙂.
  prefs: []
  type: TYPE_NORMAL
- en: 'At any time in the stream, the approximated counts for items are what we have
    kept so far, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/27582695b5d02ddba678f230d04bac92.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 25: estimating count of an item — Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: 'This method undercounts so for any item i, its approximated frequency is lesser
    or equal to its true frequency:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9acefc419d289c334bc6c37b25734dd8.png)'
  prefs: []
  type: TYPE_IMG
- en: At the same time, its approximated frequency is lower bounded because each time
    we decrease, we decrease by at most count of the counter at position *l/2 i.e.,*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/546c48ec031b97d17e06bd9c22111c61.png)'
  prefs: []
  type: TYPE_IMG
- en: 'At any point that we have seen ***n*** elements in stream, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ac227771f0be7275d8fb40c84e90ca74.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So the error guarantee it provides is as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ede440a1b30bc4c853a4dd087778ee65.png)'
  prefs: []
  type: TYPE_IMG
- en: Misra-Gries error bound — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: So Misra-Gries produces a non-zero approximated frequency for all items that
    their true frequency is larger than *2n/l* . As an example, to find items that
    appear more than 20% of the time we got to take *l = 10* counters and run Misra-Gries
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Frequent Directions: an extension of Misra-Gries'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, let’s extend Misra-Gries to vectors and matrices. In matrix case, the stream
    items are **row vectors** in ***d*** dimension. At any time ***n*** in the stream,
    all rows together form a tall matrix *A* of *n* rows. The goal is to find the
    **most important directions** of A. These correspond to top singular vectors of
    A. The more important a direction is the more frequent it is among data points,
    that’s why we call the next algorithm **Frequent Directions** [2,3].
  prefs: []
  type: TYPE_NORMAL
- en: 'The pseudocode for *Frequent Directions* algorithm is as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/561c183c6cd15343f8b9d982a2a4bddf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 26: FrequentDirections— Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we see, the algorithm takes input matrix *A* and the sketch size *l* as
    inputs. Then, in the first step (i.e. in the first line highlighted in blue) the
    algorithm initializes sketch *B* as an empty matrix of *l* rows. Then for every
    row in the stream *A*, the algorithm insert it into *B* until *B* is full:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a2fbac30a27748109eb50e886b82d496.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 27: when sketch B is full — Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: We then compute *SVD* of *B*; this produces left singular matrix *U*, singular
    values matrix *S* and right singular matrix *V*. Note *U*, and *V* provide rotation
    of subspaces as they are orthonormal matrices.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1175b22b2c060f147b19d1a8c06428b7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 28: SVD of B — Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: We then reduce *B*’s rank by subtracting the squared of middle singular value
    from squared of all singular values! Note, this step resembles the part in Misra-Gries
    where we subtract middle counter from all counters. Subtracting squared of middle
    singular value from squared of all singular values makes half of singular values
    go to zero.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/105b9e0fd0dcc1737c05d7b47ea2d5f4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 29: lowering rank of B — Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: We then multiply *S* (the singular value matrix) by *V* transposed (right singular
    matrix) and assign it to *B*. In other words, we reconstruct *B* by dropping the
    left singular matrix *U*. The effect of this operation is a new matrix *B* which
    has half of its rows as empty. That’s good news, as it has room for next upcoming
    rows in the stream matrix *A.*
  prefs: []
  type: TYPE_NORMAL
- en: '**Error guarantee**: Similar to the frequent items case, this method has the
    following error guarantees where *l* is the sketch size and *k* is the rank:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/26af11f02db213823a89fd08a730206a.png)'
  prefs: []
  type: TYPE_IMG
- en: FrequentDirections covariance error bound — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3e84e4752ba2fe64ba053b39926b8c0e.png)'
  prefs: []
  type: TYPE_IMG
- en: FrequentDirections projection error bound — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Compare the second error bound to that of random projection and row sampling.
    Note how this is a tighter and better error bound.
  prefs: []
  type: TYPE_NORMAL
- en: '**Experiments**: It is shown in experiments [2,4] that *Frequent Directions*
    algorithm outperforms every other streaming algorithm discussed above. Below is
    the experiments with respect to covariance error bound:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/053c83c270c62d2990bfd0ecab32cd13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 30: Experiments in covariance error — Image from [2]'
  prefs: []
  type: TYPE_NORMAL
- en: 'and this is the experiment with respect to the projection error bound:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/275ca9112d6e71ab891f32ca8da25aa4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 31: Experiments in projection error — Image from [2]'
  prefs: []
  type: TYPE_NORMAL
- en: This concludes the article. As we see, Frequent Directions not only outperform
    all other methods in approximation errors, but it uses the least amount of space.
    In other words, it is space optimal with respect to the error bound it achieves.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Low-rank matrix approximation in data streams is the problem of computing a
    low-rank approximation to a matrix whose its rows arrive in a streaming fashion.
    This entails not having access to all rows of the matrix at once, and not knowing
    the size of the matrix as. There are three main category of methods for computing
    such approximation: row sampling — random projection — iterative sketching. While
    the first group is the most intuitive set of methods as they sample the actual
    data points, the second group is the most efficient in run-time. The state-of-the-art
    (SOTA) is in the third group and it’s called Frequent Directions. This method
    is based on an old method from frequent item estimation, and it is space optimal
    with respect to the error bound it achieves.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have any questions or suggestions, feel free to reach out to me:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Email: mina.ghashami@gmail.com'
  prefs: []
  type: TYPE_NORMAL
- en: 'LinkedIn: [https://www.linkedin.com/in/minaghashami/](https://www.linkedin.com/in/minaghashami/)'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Fast monte carlo algorithms for matrices i: approximating matrix multiplication,
    p. drineas, et al, 2006](https://www.stat.berkeley.edu/~mmahoney/pubs/matrix1_SICOMP.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Frequent Directions: Simple and Deterministic Matrix Sketching](https://arxiv.org/abs/1501.01711)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://github.com/edoliberty/frequent-directions](https://github.com/edoliberty/frequent-directions)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Improved Practical Matrix Sketching with Guarantees](https://arxiv.org/abs/1501.06561)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Low Rank Approximation and Regression in Input Sparsity Time](https://arxiv.org/abs/1207.6365)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Improved Approximation Algorithms for Large Matrices via Random Projections](https://ieeexplore.ieee.org/document/4031351)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
