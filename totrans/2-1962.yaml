- en: 'Teaching is Hard: How to Train Small Models and Outperforming Large Counterparts'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/teaching-is-hard-how-to-train-small-models-and-outperforming-large-counterparts-f131f9d463e1](https://towardsdatascience.com/teaching-is-hard-how-to-train-small-models-and-outperforming-large-counterparts-f131f9d463e1)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '|MODEL DISTILLATION|AI|LARGE LANGUAGE MODELS|'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Distilling the knowledge of a large model is complex but a new method shows
    incredible performances
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://salvatore-raieli.medium.com/?source=post_page-----f131f9d463e1--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----f131f9d463e1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f131f9d463e1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f131f9d463e1--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----f131f9d463e1--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f131f9d463e1--------------------------------)
    ·12 min read·Nov 11, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a16ec7ef78aa98b058f1e94098af933b.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [JESHOOTS.COM](https://unsplash.com/@jeshoots?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '[Large language models](https://en.wikipedia.org/wiki/Large_language_model)
    (LLMs) and few-shot learning have shown we can use these models for unseen tasks.
    However, these skills have a cost: a huge number of parameters. This means you
    need also a specialized infrastructure and restrict state-of-the-art LLMs to only
    a few companies and research teams.'
  prefs: []
  type: TYPE_NORMAL
- en: Do we really need a unique model for each task?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Would it be possible to create specialized models that could replace them for
    specific applications?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can we have a small model that competes with giant LLMs for specific applications?
    Do we necessarily need a lot of data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this article, I give an answer to these questions.
  prefs: []
  type: TYPE_NORMAL
- en: “Education is the key to success in life, and teachers make a lasting impact
    in the lives of their students.” –Solomon Ortiz
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Match the champion!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/b6319bfeb892fa586fd258fd3e5904bd.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Fauzan Saari](https://unsplash.com/@fznsr_?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: The art of teaching is the art of assisting discovery. — Mark Van Doren
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Large language models (LLMs)](https://en.wikipedia.org/wiki/Large_language_model)
    have shown revolutionary capabilities. For example, researchers have been surprised
    by elusive behavior such as [in-context learning](https://ai.stanford.edu/blog/understanding-incontext/).
    This has led to an increase in the scale of models, with larger and larger models
    [searching for new capabilities](https://openai.com/research/scaling-laws-for-neural-language-models)
    that appear beyond a number of parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/all-you-need-to-know-about-in-context-learning-55bde1180610?source=post_page-----f131f9d463e1--------------------------------)
    [## All You Need to Know about In-Context Learning'
  prefs: []
  type: TYPE_NORMAL
- en: What is and how does it work what makes Large Language Models so powerful
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/all-you-need-to-know-about-in-context-learning-55bde1180610?source=post_page-----f131f9d463e1--------------------------------)
    [](/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9?source=post_page-----f131f9d463e1--------------------------------)
    [## Emergent Abilities in AI: Are We Chasing a Myth?'
  prefs: []
  type: TYPE_NORMAL
- en: Changing Perspective on Large Language Models emerging properties
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9?source=post_page-----f131f9d463e1--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: This comes at a cost, however; models such as [GPT-3](https://en.wikipedia.org/wiki/GPT-3)
    (more than 175 trillion parameters) require at least 350 GB GPU for running. This
    means you need specialized infrastructure not only to train but also to use it
    in [inference](https://cloud.google.com/bigquery/docs/inference-overview). Deploying
    such a model to make it publicly accessible requires significant challenges and
    costs (especially if you want to reduce latency). **Thus, only a few companies
    can afford to deploy models of a certain size for real-world applications.**
  prefs: []
  type: TYPE_NORMAL
- en: Models that have more than 100 B parameters have large modeling capabilities
    that however are spread over many skills. In contrast, models with less than 10
    B have reduced modeling ability but one can concentrate this ability on a single
    task. For example, reasoning is one of the abilities shown by models over 100
    B parameters but is absent in small models. [The authors of this study](https://arxiv.org/abs/2301.12726)
    show that reasoning is only one of many capabilities in a large LLM. Therefore,
    focusing the training of a small model on reasoning can yield appreciable results
    even with a model smaller than 100 B.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, specializing in a small model comes at a price: performance on other
    tasks. But often you are interested only in one task, so you can use a small model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2511991c35ab392135d6736ea8a5e38a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'previous works suggested that reasoning abruptly appears with the scale (left
    plot). The authors in this study show that by focusing a model on a reasoning
    task (specialization) you can achieve good results in reasoning. image source:
    [here](https://arxiv.org/abs/2301.12726)'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, several companies have focused on small models that show acceptable
    performance only for particular tasks. In addition, the use of [**fine-tuning**](https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning))
    has made it possible to create small, specialized models for a specific application.
    For some tasks such as classification, fine-tuning requires an annotated dataset
    of elements. Collecting these annotated datasets is expensive, so another technique
    used is [distillation](https://en.wikipedia.org/wiki/Knowledge_distillation).
  prefs: []
  type: TYPE_NORMAL
- en: '**Distillation** is a technique in which you train a small model using labels
    that are generated from a larger model. Collecting these unlabeled datasets can
    be equally expensive (for example, in the medical domain). The higher the performance
    must be, the higher these costs are. So achieving the same performance as an LLM
    with fine-tuning or distillation can be computationally expensive.'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, how can we make a small model capable of learning from an LLM in a data
    and time efficient manner?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to make an LLM an efficient teacher
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/45bb89ea853b0b0f6619552f333d2df3.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [ThisisEngineering RAEng](https://unsplash.com/@thisisengineering?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: I cannot teach anybody anything; I can only make them think. – Socrates
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: When we want to train a small model, LLMs are used either to generate labels
    for unlabeled text or for [data augmentation](https://en.wikipedia.org/wiki/Data_augmentation)
    (taking a dataset of examples generated by an LLM). Intuitively, this may not
    be enough to make model learning efficient.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if I want my small model to learn how to rank tweets (positively,
    negatively, or neutrally) I can download a large number of tweets, generate the
    labels with an LLM, and train the small model with the provided labels.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9b2f5ce4a0dbf71ce5fdbfc0b6dc6ec6.png)'
  prefs: []
  type: TYPE_IMG
- en: schematic representation of distillation. image by the author
  prefs: []
  type: TYPE_NORMAL
- en: '**However, while this can work for a simple task such as tweet classification
    it is not enough for more complex tasks.** We may download riddles from the internet
    and ask an LLM to solve them, but the solution itself does not provide us with
    any information on the solving process. A small model trained with the solutions
    would not learn how to solve a riddle.'
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, to learn how to solve difficult tasks (such as solving a riddle) you
    need more information than just the solution.
  prefs: []
  type: TYPE_NORMAL
- en: Actually, this is also true for LLMs, for reasoning tasks (arithmetic, commonsense,
    and symbolic reasoning) providing context with [chain-of-thought](https://www.promptingguide.ai/techniques/cot)
    helps the model arrive at the solution without hallucinating.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f304565b4553c321fc000579a7519ba6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2201.11903)'
  prefs: []
  type: TYPE_NORMAL
- en: Building on this intention, some [Google researchers](https://blog.research.google/2023/09/distilling-step-by-step-outperforming.html)
    have gone so far as to train small models with capabilities exceeding LLMs in
    specific tasks (770M parameters with 540B [PaLM](https://ai.google/discover/palm2/)).
    They then described this approach in a recently published paper.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://arxiv.org/abs/2305.02301?source=post_page-----f131f9d463e1--------------------------------)
    [## Distilling Step-by-Step! Outperforming Larger Language Models with Less Training
    Data and Smaller…'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying large language models (LLMs) is challenging because they are memory
    inefficient and compute-intensive for…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: arxiv.org](https://arxiv.org/abs/2305.02301?source=post_page-----f131f9d463e1--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: In short, the authors exploited the ability of an LLM to reason (beyond simply
    generating labels). **Taking an unlabeled dataset, they asked the LLM to generate
    the correct labels and rationale** (natural language explanations of why that
    is the most appropriate label for the question). After that, they used both the
    label and the rationale to train small models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/260f62b7e824bd0a1843e11651594841.png)'
  prefs: []
  type: TYPE_IMG
- en: schematic representation of the approach. image by the author
  prefs: []
  type: TYPE_NORMAL
- en: '**In this way, they provided the small model with not only the problem solution
    but also how the teacher (the LLMs) arrived at the solution.** Moreover, the rationale
    contains not only an explanation but also useful elements for understanding the
    task (elements that are not easy to infer from simple input, especially for a
    model with a limited number of parameters).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1705884062947f3167cb0b1628621e0e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2305.02301)'
  prefs: []
  type: TYPE_NORMAL
- en: Distilling step-by-step
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Going into more detail, the authors used the same prompts that had been used
    for [Chain-of-Thought (CoT)](https://arxiv.org/abs/2201.11903). A prompt that
    consists of a question, the context or rationale, and the answer to the question.
    After that, the rationale is appended to the question and the model must give
    the answer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8d47874aced51dc7f61a3787faf8ecd2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2305.02301)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The small model is trained with a **simple multitask approach**: it must predict
    the correct label and also generate the corresponding rationale. The [loss function](https://en.wikipedia.org/wiki/Loss_function)
    also takes into account whether there is an error in generating the rationale.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/22a15086a1ef2fee84b316f0fe4f05d0.png)'
  prefs: []
  type: TYPE_IMG
- en: In this way, the authors force the model to generate intermediate reasoning
    steps as well, guiding the model to the correct answer.
  prefs: []
  type: TYPE_NORMAL
- en: Metaphorically, it is like a teacher forcing the student to write down all the
    reasoning steps instead of providing the answer directly. The advantage of this
    approach is that at test time the model will no longer need the teacher model
    (LLM) but should have learned to reason.
  prefs: []
  type: TYPE_NORMAL
- en: Can we teach reasoning to a student?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/d9501579775396f5c29ba346354f5818.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Element5 Digital](https://unsplash.com/@element5digital?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Tell me and I forget. Teach me and I remember. Involve me and I learn. – Benjamin
    Franklin
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The authors used PaLM (540 B parameters) as the LLM to generate the rationales.
    They chose to use [T5](https://arxiv.org/abs/1910.10683) as a small model, using
    available pre-trained weights checkpoints. Interestingly, the authors use a very
    small model that has already been trained. **In this way, they use a model that
    already has a general knowledge of the language but can be adapted to a specific
    task.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2bb515c5763c2f132286a68a6666a01c.png)'
  prefs: []
  type: TYPE_IMG
- en: Model comparison to better understand the difference in size (circles are proportional).
    Image by the author, script to generate the image can be found [here](https://github.com/SalvatoreRa/tutorial/blob/main/other/Code_for_Teaching_is%C2%A0Hard.ipynb)
  prefs: []
  type: TYPE_NORMAL
- en: 'They chose three particular natural language processing tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Natural language inference**](https://nlpprogress.com/english/natural_language_inference.html)**.**
    they used two different datasets: e-SNLI and ANLI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Commonsense question answering**](https://arxiv.org/abs/1811.00937) **(CQA).**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Arithmetic math word problems (SVAMP).**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As can be seen, these are tasks and datasets that require the model to show
    reasoning capabilities
  prefs: []
  type: TYPE_NORMAL
- en: 'In the article, the approach is compared with the two classical approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**fine-tuning**](https://d2l.ai/chapter_computer-vision/fine-tuning.html)**.**
    where the pre-trained model is trained on annotated examples with the correct
    labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Distillation**](https://neptune.ai/blog/knowledge-distillation)**.** where
    the LLM is used to generate the ground-truth labels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The results show that the new approach (*Distilling step-by-step*) outperforms
    standard finetuning in all benchmark datasets and tasks analyzed but also requires
    far fewer examples to achieve better performance. Thus, the approach performs
    better but it is also cheaper (with only 12.5 percent of examples outperforming
    classical finetuning).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/89f5d43ffd244f5e602ddf7126606701.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2305.02301)'
  prefs: []
  type: TYPE_NORMAL
- en: And the same is true for standard distillation, the new approach is both more
    performant and requires many fewer examples.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/93ab3c85614ed66d327f10649bce775b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2305.02301)'
  prefs: []
  type: TYPE_NORMAL
- en: The authors then used the same approach with different versions of the model
    (220M, 770M, 11B) and compared it with the LLM baseline (PaLM). The result shows
    that the new approach improves performance according to scale (larger models perform
    better). In addition, step-by-step distilling for some tasks seems to outperform
    even the LLM baseline. In other words, a 770 M model manages in ANLI to outperform
    a model 700 times larger. Even more impressive is that in e-SNLI a 220M model
    outperforms a 2000 times larger model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c910902ea5c7d937496b0624ed16a6cb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2305.02301)'
  prefs: []
  type: TYPE_NORMAL
- en: In standard fine-tuning, we use human label annotated, while in distillation
    we use an unlabeled setting. Again the results are similar, showing that the model
    can learn even from data that are annotated by an LLM.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/543183a4dd5a3cf5b84665b2b0b95845.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2305.02301)'
  prefs: []
  type: TYPE_NORMAL
- en: '**These results in themselves are impressive, but it is incredible that you
    do not need the entire dataset.** Even with only 0.1 % of the dataset, the approach
    is effective. This is not the case for standard fine-tuning and task distillation
    where to see appreciable performance you need many more examples. In ANLI, for
    T5-770M 80% of the examples are enough to outperform PaLM 540B. Even with the
    full dataset, standard fine-tuning fails to reach the LLM baseline'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/160a8530e0ac01d2159ae2238c7b235b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2305.02301)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a51fd497ee7c1462d2a33983a1345dfc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2305.02301)'
  prefs: []
  type: TYPE_NORMAL
- en: As the authors note, although the approach works with other models (such as
    the 20B GPT-NeoX model) the results are inferior. This is because a PaLM provides
    higher quality and more detailed rationales.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e5999ff2b5c293d03d099e888ff1f58a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2305.02301)'
  prefs: []
  type: TYPE_NORMAL
- en: In an ablation study, they noted that multi-task training works better. In other
    words, asking the model to generate the rationale helps its learning.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4c69bf30d74f4a09174cecd2ecdc626e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2305.02301)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors released also the code to be tested by the community:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/google-research/distilling-step-by-step?source=post_page-----f131f9d463e1--------------------------------)
    [## GitHub - google-research/distilling-step-by-step'
  prefs: []
  type: TYPE_NORMAL
- en: Contribute to google-research/distilling-step-by-step development by creating
    an account on GitHub.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/google-research/distilling-step-by-step?source=post_page-----f131f9d463e1--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Parting thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/21e76ab1b41f50cfdf35f35c010f0b68.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Saif71.com](https://unsplash.com/@saif71?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Teaching is the one profession that creates all other professions. – Unknown
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**This article shows how an LLM can be used to teach smaller models how to
    solve specific tasks.** Beyond the results, this article shows how even for smaller
    models providing context allows them to arrive at the solution. Thus, the approach
    allows a user to distill a small model with fewer data and outperform large LLMs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6f31a123753708a494011954a7a8ef64.png)'
  prefs: []
  type: TYPE_IMG
- en: 'schematic representation of the article. Image source: [here](https://arxiv.org/pdf/2305.02301.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: The authors show in this paper that models up to 2000 times smaller than an
    LLM, can learn and outperform the model teacher on complex tasks such as reasoning
    tasks. Moreover, compared to classical step-by-step distilling approaches, it
    requires much less data.
  prefs: []
  type: TYPE_NORMAL
- en: In general, there has been a paradigm shift in recent times in model learning
    research, in which attempts are being made to separate memorization and actual
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://levelup.gitconnected.com/grokking-learning-is-generalization-and-not-memorization-52c43c9025e4?source=post_page-----f131f9d463e1--------------------------------)
    [## Grokking: Learning Is Generalization and Not Memorization'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how a neural network learns helps us to avoid that the model from
    forgetting what it learns
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/grokking-learning-is-generalization-and-not-memorization-52c43c9025e4?source=post_page-----f131f9d463e1--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, this article shows that to perform a specific task you do not need a
    large capacity (memorization). You can teach a small model to learn a task providing
    information on how to solve the problem (generalization).
  prefs: []
  type: TYPE_NORMAL
- en: This work is important because, with little data, a much smaller model can be
    trained to excel on a task. These models can then be deployed much more easily
    with very little cost. In addition, the approach works with any model, so a user
    can use either an open source model (such as LLaMA) or the API to a proprietary
    model (GPT-4 or PaLM) use step-by-step distilling, and create their own specialized
    model.
  prefs: []
  type: TYPE_NORMAL
- en: This work opens up as many exciting possibilities as inexpensively developing
    specialized models for many applications and with superior performance to giant
    models. These models then can be deployed not only online but also on desktop
    computers or in cell phone applications. Thus, having a small but proprietary
    dataset you can develop and deploy expert models with limited resources.
  prefs: []
  type: TYPE_NORMAL
- en: For example, you can imagine a user developing a small model specialized in
    solving riddles. You just need to create the rationale with the LLM, use *distill
    step-by-step* to train your expert model, and then you could even deploy it on
    a phone app.
  prefs: []
  type: TYPE_NORMAL
- en: TL;DR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Google unveils a new simple approach for distilling knowledge from a large model.
    Using both rationales and answers you can teach a small model (even 2000 times
    smaller) to outperform LLMs in reasoning tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The approach outperforms previous state-of-the-art
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This approach requires a small training set and a small model size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This approach enables the deployment of independent language models for specialized
    tasks. The model size is now compatible with web apps, and inference on device,
    and you do need complex infrastructure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What do you think? Let me know in the comments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you have found this interesting:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*You can look for my other articles, you can also* [***subscribe***](https://salvatore-raieli.medium.com/subscribe)
    *to get notified when I publish articles, and you can also connect or reach me
    on*[***LinkedIn***](https://www.linkedin.com/in/salvatore-raieli/)***.***'
  prefs: []
  type: TYPE_NORMAL
- en: '*Here is the link to my GitHub repository, where I am planning to collect code
    and many resources related to machine learning, artificial intelligence, and more.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/SalvatoreRa/tutorial?source=post_page-----f131f9d463e1--------------------------------)
    [## GitHub - SalvatoreRa/tutorial: Tutorials on machine learning, artificial intelligence,
    data science…'
  prefs: []
  type: TYPE_NORMAL
- en: Tutorials on machine learning, artificial intelligence, data science with math
    explanation and reusable code (in python…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/SalvatoreRa/tutorial?source=post_page-----f131f9d463e1--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*or you may be interested in one of my recent articles:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/reshaping-the-models-memory-without-the-need-for-retraining-9ade69f56296?source=post_page-----f131f9d463e1--------------------------------)
    [## Reshaping the Model’s Memory without the Need for Retraining'
  prefs: []
  type: TYPE_NORMAL
- en: Erasing any echo of problematic content a large language model has learned
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/reshaping-the-models-memory-without-the-need-for-retraining-9ade69f56296?source=post_page-----f131f9d463e1--------------------------------)
    [](https://levelup.gitconnected.com/beyond-words-unraveling-speech-from-brain-waves-with-ai-7ff81862dfff?source=post_page-----f131f9d463e1--------------------------------)
    [## Beyond Words: Unraveling Speech from Brain Waves with AI'
  prefs: []
  type: TYPE_NORMAL
- en: AI is capable of decoding speech from non-invasive brain recordings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/beyond-words-unraveling-speech-from-brain-waves-with-ai-7ff81862dfff?source=post_page-----f131f9d463e1--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Reference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here is the list of the principal references I consulted to write this article
    (only the first author name of an article is cited).
  prefs: []
  type: TYPE_NORMAL
- en: Fu, 2023, Specializing Smaller Language Models towards Multi-Step Reasoning,
    [link](https://arxiv.org/abs/2301.12726)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hinton, 2015, Distilling the Knowledge in a Neural Network, [link](https://arxiv.org/abs/1503.02531)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Howard, 2018, Universal Language Model Fine-tuning for Text Classification,
    [link](https://arxiv.org/abs/1801.06146)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kaplan, 2020, Scaling Laws for Neural Language Models, [link](https://arxiv.org/abs/2001.08361)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wei, 2022, Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,
    [link](https://arxiv.org/abs/2201.11903)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hsieh, 2023, Distilling Step-by-Step! Outperforming Larger Language Models with
    Less Training Data and Smaller Model Sizes, [link](https://arxiv.org/abs/2305.02301)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Chowdhery, 2022, PaLM: Scaling Language Modeling with Pathways, [link](https://arxiv.org/abs/2204.02311)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Raffel, 2019, Exploring the Limits of Transfer Learning with a Unified Text-to-Text
    Transformer, [link](https://arxiv.org/abs/1910.10683)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
