- en: 'Multi-Task Machine Learning: Solving Multiple Problems Simultaneously'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多任务机器学习：同时解决多个问题
- en: 原文：[https://towardsdatascience.com/multi-task-learning-4531eb32d77b](https://towardsdatascience.com/multi-task-learning-4531eb32d77b)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/multi-task-learning-4531eb32d77b](https://towardsdatascience.com/multi-task-learning-4531eb32d77b)
- en: Some supervised, some unsupervised, some self-supervised, in NLP and computer
    vision
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在自然语言处理和计算机视觉中，有些是监督的，有些是无监督的，有些是自监督的。
- en: '[](https://jagota-arun.medium.com/?source=post_page-----4531eb32d77b--------------------------------)[![Arun
    Jagota](../Images/3c3eb142f671b5fb933c2826d8ed78d9.png)](https://jagota-arun.medium.com/?source=post_page-----4531eb32d77b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4531eb32d77b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4531eb32d77b--------------------------------)
    [Arun Jagota](https://jagota-arun.medium.com/?source=post_page-----4531eb32d77b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://jagota-arun.medium.com/?source=post_page-----4531eb32d77b--------------------------------)[![Arun
    Jagota](../Images/3c3eb142f671b5fb933c2826d8ed78d9.png)](https://jagota-arun.medium.com/?source=post_page-----4531eb32d77b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4531eb32d77b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4531eb32d77b--------------------------------)
    [Arun Jagota](https://jagota-arun.medium.com/?source=post_page-----4531eb32d77b--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4531eb32d77b--------------------------------)
    ·11 min read·Apr 27, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4531eb32d77b--------------------------------)
    ·阅读时间 11 分钟·2023年4月27日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f245371498b59c216924e720580c26fd.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f245371498b59c216924e720580c26fd.png)'
- en: Image by [Gerd Altmann](https://pixabay.com/users/geralt-9301/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=3685928)
    From [Pixabay](https://pixabay.com/)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Gerd Altmann](https://pixabay.com/users/geralt-9301/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=3685928)
    提供，来源于 [Pixabay](https://pixabay.com/)
- en: '*Single-task learning* is the process of learning to predict a single outcome
    (binary, multi-class, or continuous) from a labeled data set.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*单任务学习*是从标记数据集中学习预测单一结果（二元、多类别或连续）的过程。'
- en: By contrast, *multi-task learning* is the process of jointly learning to predict
    multiple outcomes on inputs of the same modality. Such as images or text.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，*多任务学习*是指在相同模态的输入上联合学习以预测多个结果的过程。例如图像或文本。
- en: The obvious question to ask is, why learn *jointly*? Why not independently learn
    single-task models to predict the various outcomes?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 显而易见的问题是，为什么要*联合*学习？为什么不独立学习单任务模型来预测各种结果？
- en: The answer is that joint learning can learn features that generalize better
    across tasks. Those that form good predictors for multiple tasks are favored over
    those that don’t. The learned features may even generalize to new prediction tasks
    in the same domain.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是，联合学习可以学习到在多个任务之间更好地泛化的特征。那些能够作为多个任务的良好预测器的特征会比那些不能的特征更受青睐。这些学习到的特征甚至可能在相同领域的新预测任务中进行泛化。
- en: '**Adding Unsupervised Learning To The Mix**'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**将无监督学习添加到其中**'
- en: To this point, we have supposed that all the chosen tasks in the multi-task
    setting are supervised in nature. Let’s relax this to allow some of the tasks
    to be unsupervised.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们假设多任务设置中的所有选择任务都是监督性质的。让我们放宽这一假设，允许一些任务为无监督任务。
- en: Why? Because we may have a lot more data to train on. Some labeled for the various
    outcomes, and a lot unlabeled. Adding unsupervised tasks to the joint learning
    allows us to potentially learn from a much larger data set.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么？因为我们可能有更多的数据可以进行训练。一些是用于各种结果的标记数据，还有很多未标记数据。将无监督任务添加到联合学习中可以让我们从更大规模的数据集中进行学习。
- en: We can in fact empirically test this hypothesis. We can hold out a portion of
    the labeled data set and do two experiments, one in which we train on the rest
    of the labeled data, and in the second we train on the rest of the labeled data
    combined with a huge amount of unlabeled data. We can then compare the quality
    of the models learned in the two experiments on the held-out (labeled) test set.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上可以通过实证测试这个假设。我们可以保留一部分标记数据集，并进行两个实验：一个是在其余标记数据上训练，另一个是在其余标记数据和大量未标记数据的组合上训练。然后，我们可以比较这两个实验中学习到的模型在保留（标记）测试集上的质量。
- en: Well, the concept of learning from a mix of labeled and unlabeled data already
    has a name for it. It is called semi-supervised learning.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，从标记和未标记数据的混合中学习的概念已经有了一个名称，称为半监督学习。
- en: '**Refining Unsupervised Learning To Self-Supervised Learning**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**将无监督学习精细化为自监督学习**'
- en: While allowing some of the tasks to be unsupervised allows us to use a potentially
    huge amount of unlabeled data, can we leverage the unlabeled data in a better
    way? The general answer is Yes. Using self-supervision.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然允许一些任务为无监督任务使我们能够使用潜在的大量未标记数据，但我们能否更好地利用未标记数据？一般的回答是肯定的。使用自监督。
- en: In self-supervision, we define supervised tasks to predict certain characteristics
    of the data from the rest. For instance, we might predict the next word from the
    previous words seen till then.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在自监督中，我们定义监督任务，以从其余数据中预测数据的某些特征。例如，我们可能会预测从先前看到的单词中出现的下一个单词。
- en: Self-supervision is more powerful than unsupervised learning because it is supervised.
    Plus, it learns from all of the unlabeled data available as it does not require
    humans to label it. Finally, self-supervision is a powerful mechanism to get a
    large and diverse set of labeled data without humans needing to do any work. In
    the sections on computer vision and NLP, we will bring out crisp and realistic
    examples that will reveal not only how diverse of a labeled data set we can construct,
    but in what way this improves the learning ability over not doing self-supervision
    at all.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督比无监督学习更强大，因为它是有监督的。此外，它从所有未标记的数据中学习，因为它不需要人工标记。最后，自监督是获得大量多样化标记数据的强大机制，无需人工工作。在计算机视觉和自然语言处理部分，我们将提供简洁且现实的例子，揭示我们如何构建多样化的标记数据集，并且这种方法如何提高学习能力，相较于完全不进行自监督。
- en: In this post, when we say multi-task learning what we really mean is learning
    in which there are at least two supervised learning tasks, perhaps additional
    unsupervised learning tasks, and perhaps additional self-supervision tasks.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，当我们说多任务学习时，我们真正指的是至少有两个监督学习任务的学习，可能还有额外的无监督学习任务，以及可能的额外自监督任务。
- en: We depict this visually below.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在下面以视觉方式展示了这一点。
- en: '![](../Images/5f7ca4da8fb3370d1c25a86634908858.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5f7ca4da8fb3370d1c25a86634908858.png)'
- en: 'Figure 1: (By Author) A Venn Diagram Of Data Of A Certain Modality, Such As
    Text. Solid circles denote labeled subsets for various supervised tasks. Dashed
    circles denote self-labeled subsets for various self-supervised tasks.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：（作者提供）某种模态的数据的维恩图。实心圆表示用于各种监督任务的标记子集。虚线圆圈表示用于各种自监督任务的自标记子集。
- en: Note that this figure only depicts subsets of the input data as covered by various
    supervised and self-supervised tasks, not how the labels for the various tasks
    relate to each other.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，此图仅描绘了输入数据的子集，涵盖了各种监督和自监督任务，而不是不同任务标签之间的关系。
- en: Figure 1 also depicts
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图1也描绘了
- en: The subsets of data labeled for various supervised tasks tend to be small.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标记用于各种监督任务的数据子集往往较小。
- en: Adding self-supervised tasks can cover more of the data. These subsets can often
    be made larger.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加自监督任务可以覆盖更多的数据。这些子集通常可以做得更大。
- en: Subsets of supervised tasks can overlap with each other but are often not identical.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督任务的子集可以相互重叠，但通常不完全相同。
- en: '**Relaxing The Term “Joint Learning” As Used In This Post**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**放宽本文中“联合学习”一词的定义**'
- en: So far we’ve given the impression that joint learning means learning a single
    model simultaneously from all of the data available for all of the tasks.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们给人的印象是联合学习意味着从所有任务的所有可用数据中同时学习一个模型。
- en: We’d, in fact, like to relax this to permit more flexible training policies.
    This is because in practice it has been found effective to learn the self-supervised
    (and possibly unsupervised) tasks first from the unlabeled data and then tune
    these models for particular downstream supervised tasks further. One way this
    approach is more flexible is because it naturally permits new supervised tasks
    to be defined on-the-fly and fine-tune existing models as needed.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们希望放宽这一点以允许更灵活的训练策略。这是因为在实践中，发现首先从未标记数据中学习自监督（以及可能的无监督）任务，然后进一步调整这些模型以进行特定的下游监督任务是有效的。这种方法的一种更灵活的方式是，它自然允许在实时定义新的监督任务并根据需要微调现有模型。
- en: '**On Data For Multi-Task Learning**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**关于多任务学习的数据**'
- en: We need multiple labeled data sets, one for each outcome we are trying to predict.
    The inputs in these data sets are generally from the same domain. The actual inputs
    can vary across the data sets, as they are often collected from disparate sources.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要多个标记数据集，每个数据集用于预测一个结果。这些数据集中的输入通常来自相同领域。实际输入在数据集中可能会有所不同，因为它们通常来自不同来源。
- en: We may additionally have unlabeled data. We should use it for suitable unsupervised
    tasks added to the mix so long as we reasonably believe it will help. As mentioned
    in a previous section, we can empirically test our belief before deciding one
    way or the other. We should also define suitable self-supervision tasks, if appropriate,
    to leverage and learn to predict certain characteristics of the unlabeled data
    from others.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能还有未标记的数据。只要我们合理地相信这些数据会有帮助，就应该将其用于适当的无监督任务。如前一节所述，我们可以在决定之前通过实证测试我们的信念。如果适当的话，我们还应定义适合的自我监督任务，以便利用和学习从其他数据中预测未标记数据的某些特征。
- en: Multi-task learning has proven especially useful in NLP and in computer vision.
    We’ll cover each one by one.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务学习在自然语言处理和计算机视觉中尤其有用。我们将逐一讨论这两个领域。
- en: '**Multi-task Learning In Computer Vision**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算机视觉中的多任务学习**'
- en: Imagine this. We have access to labeled images of handwritten digits, of cars,
    of images containing human faces, of images containing pets, and many others.
    Let’s also suppose that some of these data sets are small.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，我们有手写数字、汽车、人脸图像、宠物图像等标签图像。还假设这些数据集中有些是小的。
- en: Building single-task models for each of these individually runs the risk that
    we don’t have sufficiently rich labeled data for some of the tasks.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为每个任务单独构建单任务模型的风险在于我们可能没有足够丰富的标记数据。
- en: Combining all the data sets and learning a single multi-task model from them
    may help alleviate the aforementioned task-specific data sparsity problems. So
    long as some of the features that get learned jointly also help with those tasks.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有数据集结合起来，学习一个单一的多任务模型，可能有助于缓解上述特定任务数据稀疏的问题，只要一些共同学习的特征也有助于这些任务。
- en: The thinking is that images are images and at some sufficiently low level, certain
    features may be predictive of the outcomes in multiple tasks. Such as edges or
    loops to name two.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 思路是图像就是图像，在某些足够低的层次上，某些特征可能对多个任务的结果具有预测性，比如边缘或循环。
- en: '**The Value Of Unsupervised Tasks In This Setting**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**在这种情况下无监督任务的价值**'
- en: Previously we mentioned that adding suitable unsupervised tasks to the mix in
    multi-task learning can be beneficial. We sharpen the intuition in the setting
    of images.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到，在多任务学习中加入适当的无监督任务是有益的。我们将在图像的设置中进一步阐明这一点。
- en: There are billions and billions of images accessible on the web. Most are unlabeled.
    Imagine we have some labeled images for a certain task. Say predicting whether
    or not there is a building in an image.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 网络上有数十亿张图像，大多数是未标记的。假设我们有一些特定任务的标记图像，比如预测图像中是否有建筑物。
- en: Intuition would suggest that doing unsupervised learning on lots of unlabeled
    images can help discover better features than using only the labeled images. Such
    as better edges and better loops.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 直觉上，使用大量未标记图像进行无监督学习可以帮助发现比仅使用标记图像更好的特征，比如更好的边缘和更好的循环。
- en: '**Self-Supervised Learning Tasks On The Unlabeled Data**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**未标记数据上的自监督学习任务**'
- en: As noted in the previous section, there is typically a huge amount of unlabeled
    image data. Billions and billions of images. In the previous section, we focused
    on semi-supervised learning. That is in addition to these unlabeled images we
    have a few labeled for a particular task. We want to learn in an unsupervised
    way from the unlabeled ones and in a supervised way from the labeled ones.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所述，通常有大量的未标记图像数据。数十亿张图像。在前一节中，我们关注了半监督学习。即除了这些未标记图像外，我们还拥有一些用于特定任务的标记图像。我们希望从未标记图像中以无监督方式学习，从标记图像中以监督方式学习。
- en: Introducing self-supervised learning into the mix can turbo-charge this approach.
    This is because appropriate self-supervised tasks force the model to learn representations
    that predict some characteristics of the images from others. And typically this
    easily produces a lot of labeled data, without the human having to put in any
    effort.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 引入自监督学习可以大大提升这种方法。这是因为合适的自监督任务迫使模型学习从其他特征预测图像的一些特征。这通常可以轻松地产生大量标记数据，而不需要人工付出任何努力。
- en: '**Some Specific Forms Of Self-Supervision**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**一些特定形式的自监督**'
- en: Below are some specific forms of self-supervision that are generally applicable
    to image data.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些通常适用于图像数据的自监督的具体形式。
- en: '**Masking off certain regions**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**掩盖某些区域**'
- en: Take an image and blur out certain entire regions. Create a new instance in
    which the blurred image is the input and the original instance is its label.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 取一张图像并模糊掉某些整个区域。创建一个新实例，其中模糊的图像是输入，原始实例是其标签。
- en: To fire our imagination of how this might help, imagine that we have images
    of people, some wearing hats some not. If we somehow mask off all faces in the
    images, we may be able to learn that below a hat is a head of a person, but not
    the other way around, i.e. not all heads have hats on them.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了激发我们对这种方法如何帮助的想象，假设我们有一些人的图像，有些人戴着帽子，有些人没有。如果我们以某种方式掩盖所有人的脸，我们可能会学到帽子下方是人的头，但反过来就不一定，即不是所有头上都有帽子。
- en: This begs the question, how do we figure out what to mask? We will look at this
    a bit differently. Instead of trying to optimize our masking, we will suggest
    considering a simple strategy of partitioning the image into rectangular grids
    of certain dimensions and creating instances with a single grid masked off in
    each instance. We can control how many corrupted instances we get from any one
    image by choosing the granularity of the gridding.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了一个问题，我们如何确定要掩盖哪些内容？我们将以不同的角度来看待这个问题。我们不尝试优化掩盖，而是建议考虑将图像分割成一定尺寸的矩形网格，并创建每个实例中掩盖一个单一网格的情况。通过选择网格的细粒度，我们可以控制从任何一个图像中获得多少被破坏的实例。
- en: We can then automate the process of discovering which corrupted images can be
    reconstructed well. We can even test the hypothesis of whether this self-supervision
    improves the accuracy of one or more downstream supervised tasks.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以自动化发现哪些被破坏的图像可以被很好地重建。我们甚至可以检验这种自监督是否提高了一个或多个下游监督任务的准确性。
- en: '**De-coloring Images**'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**去色图像**'
- en: By greying out colored images, we can force the model to learn to try to reconstruct
    the coloring from the remaining attributes. For example, it might learn that stop
    signs, which have a distinctive shape and often even the word *Stop* spelled out
    tend to be red in color.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将彩色图像变为灰色，我们可以迫使模型学习从剩余属性中尝试重建颜色。例如，它可能会学到，停牌通常是红色的，因为它们有独特的形状，甚至上面还写着*Stop*。
- en: '**Coarsening The Resolution**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**降低分辨率**'
- en: We can coarsen the resolution of an image and ask the model to try to reconstruct
    the original. This may force the model to learn to fill in fine details when possible.
    As an imagined evocative example, the model may learn to fill in certain details
    in coarsened versions of human faces such as eyelashes.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以降低图像的分辨率，并让模型尝试重建原始图像。这可能迫使模型在可能的情况下学习填补细节。作为一个想象中的生动例子，模型可能会学会填补人脸的某些细节，比如睫毛。
- en: '**Multi-task Learning In NLP**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**NLP中的多任务学习**'
- en: Consider a textual sentence. We might be interested in labeling each word with
    its part of speech, labeling certain segments as named entities, and labeling
    certain segments as being noun phrases or verb phrases.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个文本句子。我们可能会对标记每个词的词性、标记某些段落为命名实体以及标记某些段落为名词短语或动词短语感兴趣。
- en: In single-task learning, each of these would be treated independently.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在单任务学习中，每个任务都会被独立处理。
- en: In multi-task learning, we would learn a model with sharable features for all
    of these tasks simultaneously. This can yield a better model when the tasks have
    synergy. For instance, learning to predict the part of speech of each word in
    the sentence may help detect named entities, noun phrases, and verb phrases as
    well.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在多任务学习中，我们会学习一个具有共享特征的模型，同时处理所有这些任务。当任务之间有协同作用时，这可以产生更好的模型。例如，学习预测句子中每个词的词性可能有助于检测命名实体、名词短语和动词短语。
- en: '**Adding Suitable Tasks On Unlabeled Data**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**在未标记数据上添加合适的任务**'
- en: There may be lots of sentences with no labels on them. No labels meaning no
    parts of speech tagged, no named entities tagged, and no noun or verb phrases
    tagged.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 可能有很多句子没有标签。没有标签意味着没有词性标注，没有命名实体标注，也没有名词或动词短语标注。
- en: We should consider defining the self-supervised learning task of predicting
    the next word in a sentence from the words seen thus far. This task can leverage
    the unlabeled data in a powerful way. Predicting the next word can force the network
    to learn elaborate representations of the previous words.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该考虑定义自监督学习任务，即根据目前为止看到的词来预测句子中的下一个词。这个任务可以以强大的方式利用未标记的数据。预测下一个词可以促使网络学习前面词的复杂表示。
- en: More generally, it may be useful to define self-supervised learning tasks to
    predict certain masked-off words from the rest in a sentence. I say “more generally”
    because these words may be anywhere in the sentence.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地说，定义自监督学习任务来预测句子中剩余的某些被屏蔽的词可能会很有用。我之所以说“更一般地”，是因为这些词可以出现在句子的任何地方。
- en: Here is an example. Consider
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个例子。考虑
- en: exposure to sunlight causes skin cancer.
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 阳光暴露会导致皮肤癌。
- en: Three masked-off instances we may consider predicting are depicted in the two
    fill-in-the-blanks scenarios below.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能考虑预测的三个被屏蔽的实例在下面两个填空场景中展示。
- en: _____ causes skin cancer.
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: _____ 导致皮肤癌。
- en: ''
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: exposure to sunlight causes ______ .
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 阳光暴露会导致______。
- en: ''
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: _ to _ causes skin cancer
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: _ 到 _ 导致皮肤癌
- en: In the first one, we predict the left tail of the sentence from the rest. In
    the second one, we predict the right tail of the sentence from the rest. The third
    one illustrates that there can be multiple non-contiguous masked-off subsequences
    to predict.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个场景中，我们根据其余部分预测句子的左尾。在第二个场景中，我们根据其余部分预测句子的右尾。第三个场景展示了可以有多个不连续的被屏蔽子序列需要预测。
- en: '**Intuition Behind Predicting Masked-Off Words**'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**预测被屏蔽词背后的直觉**'
- en: Why go beyond predicting the next word to predicting any subsequences of masked-off
    words? The short answer is that (i) this vastly expands the labeled data set available
    for training, and (ii) with a tendency towards surfacing new types of scenarios
    in the newly labeled instances.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要从预测下一个词扩展到预测任何被屏蔽的词的子序列？简短的回答是：（i）这大大扩展了可用于训练的标记数据集，（ii）并且有助于在新标记的实例中呈现新类型的场景。
- en: To sharpen our intuition around (i) imagine that we generate masked-off labeled
    instances for all possible maskings of a sentence comprising *n* words. There
    are of the order 2^*n* maskings. By contrast, there are only of the order *n*
    ways to slice the sentence to predict words in the future.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高我们对（i）的直觉，想象一下我们为由 *n* 个词组成的句子的所有可能屏蔽生成被屏蔽的标记实例。屏蔽的数量大约是 2^*n*。相比之下，将句子切分以预测未来的词的方式大约是
    *n* 种。
- en: To elaborate on (ii), by allowing the maskings to be anywhere, we are allowing
    the model to discover predictive relationships that are potentially much more
    general than those constrained to be left to right. To imagine this, say we have
    lots of instances of *X causes Y* in our corpus. Also, suppose that only *X* causes
    *Y*. The model can learn this relationship that only *X* causes *Y* from the data
    if *X* is masked off. The left-to-right language model cannot learn that only
    *X* causes *Y*.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了详细说明（ii），通过允许屏蔽出现在任何地方，我们允许模型发现潜在地比从左到右限制的关系更一般的预测关系。为了想象这一点，假设我们在语料库中有很多
    *X 导致 Y* 的实例。同时，假设只有 *X* 导致 *Y*。如果 *X* 被屏蔽，模型可以从数据中学习到只有 *X* 导致 *Y* 的关系。而从左到右的语言模型不能学习到只有
    *X* 导致 *Y*。
- en: '**A Second, Very Simple, Example**'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**第二个非常简单的例子**'
- en: Imagine that we have a list of names of cars. Such as
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下我们有一个汽车名称的列表。例如
- en: Honda Civic, Toyota Celica, Ford Mustang, Jeep Wrangler.
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本田思域，丰田凯美瑞，福特野马，吉普牧马人。
- en: Predicting the next word will certainly help the model learn models of particular
    makes. Such as *Cherokee* and *Wrangler* (among others) for *Jeep*. However, additional
    self-supervision in the form of masking off any word will help the model also
    learn that model names tend to be strongly predictive of the make. For example,
    *Celica* is a *Toyota*, *Mustang* is a *Ford*, *Wrangler* is a *Jeep*, and so
    on.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 预测下一个词肯定会帮助模型学习特定品牌的模型。例如*Cherokee*和*Wrangler*（等）对于*Jeep*。然而，以屏蔽任何词的形式进行额外的自监督将帮助模型学习到品牌名称往往能够强烈预测品牌。例如，*Celica*
    是 *Toyota* 的，*Mustang* 是 *Ford* 的，*Wrangler* 是 *Jeep* 的，等等。
- en: '**Particular Cases**'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**特定案例**'
- en: Two particular cases of self-supervision of this type are the continuous bag-of-words
    model (CBOW) and the skip-gram model [3]. The former corresponds to masking off
    the middle word in the context of a certain number of words to the left and to
    the right of it. The latter corresponds to the opposite — masking off the left
    and the right context while keeping the word in the middle intact.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的自监督的两个特殊案例是连续词袋模型（CBOW）和跳字模型[3]。前者对应于在某个数量的左侧和右侧上下文中掩盖中间词。后者则相反——掩盖左侧和右侧的上下文，同时保持中间的词不变。
- en: Below are the two illustrated in our example.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们示例中展示的两个。
- en: '[PRE0]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Another self-supervised task found to be useful is next-sentence prediction
    [2]. Especially for question-answering systems. In [2], this task is formulated
    as a binary classification problem. The input (X1, X2) is interpreted as “sentence
    X1 is followed by sentence X2”. Positive instances are generated from pairs of
    adjacent sentences in the corpus. Negative instances are generated by pairing
    a sentence X1 in the corpus with a random sentence X2 in the corpus.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个被发现有用的自监督任务是下句预测[2]。特别是对于问答系统。在[2]中，这个任务被表述为一个二分类问题。输入（X1，X2）被解释为“句子X1后面跟随句子X2”。正实例是通过语料库中相邻句子的对生成的。负实例是通过将语料库中的一个句子X1与语料库中的随机句子X2配对生成的。
- en: In the context of the above paragraph, the term “sentence” denotes any contiguous
    sequence of text [2]. So predicting the next paragraph from the current paragraph
    would also be viewed as a next-sentence prediction problem.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述段落的背景下，“句子”一词指的是任何连续的文本序列[2]。因此，从当前段落预测下一个段落也会被视为下句预测问题。
- en: '**Leveraging Deep Architectures**'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**利用深度架构**'
- en: Multi-task learning can significantly leverage deep architectures. The intuition
    is this. Deep architectures learn low-level features in layers close to the input
    and higher-level features in layers closer to the output. Intuition suggests that
    the deeper layers, those that learn low-level features, are precisely the ones
    that can be shared across tasks.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务学习可以显著利用深度架构。直觉是这样的。深度架构在靠近输入的层中学习低级特征，而在靠近输出的层中学习高级特征。直觉上认为，学习低级特征的较深层次恰恰是那些可以在任务之间共享的层。
- en: Consider our multi-task image classification scenario. It's reasonable to speculate
    that low-level features such as edges or loops will be useful predictors in multiple
    of these tasks.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑我们的多任务图像分类场景。可以合理推测，像边缘或循环这样的低级特征将在这些任务中的多个任务中成为有用的预测因子。
- en: Below depicts a deep architecture in which the layers closer to the input are
    shared by all the supervised tasks and the layers closer to the tasks are task-specific.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 下面展示了一个深度架构，其中输入层靠近的层由所有监督任务共享，而靠近任务的层则是任务特定的。
- en: '![](../Images/e1499550cc1cb9b33d4fe242d30097fb.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e1499550cc1cb9b33d4fe242d30097fb.png)'
- en: 'Figure 2: A Multi-task Deep Architecture With Shared Layers. (By Author.)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：一个具有共享层的多任务深度架构。（作者提供。）
- en: Training can be done via the usual backpropagation algorithm. A particular instance
    in the training set may have a label from only one of the tasks. The error between
    the prediction and the target for this task gets backpropagated through the task-specific
    layers and then on to the shared layers. Consequently, if we mix in instances
    from different tasks, the shared layers learn representations that generalize
    across the tasks.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 训练可以通过常规的反向传播算法进行。训练集中的特定实例可能只有一个任务的标签。这个任务的预测与目标之间的误差通过任务特定层反向传播，然后传递到共享层。因此，如果我们混入来自不同任务的实例，共享层会学习到在任务间通用的表示。
- en: '**Summary**'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结**'
- en: In this post, we covered the topic of multi-task learning. We explained why
    multi-task learning can yield models that understand better and generalize better
    than models that learn each of the tasks separately. This approach is especially
    powerful when we add unsupervised and self-supervised tasks to the mix. These
    tasks allow us to learn a lot from unlabeled data.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们涵盖了多任务学习的话题。我们解释了为什么多任务学习可以产生理解和泛化能力比单独学习每个任务的模型更好的模型。当我们将无监督和自监督任务加入混合时，这种方法尤为强大。这些任务使我们能够从未标记的数据中学习很多。
- en: We also covered multi-task learning in computer vision and NLP. In these settings,
    we discussed specific supervised tasks, the nature of the data available, and
    specific self-supervision methods.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还涵盖了计算机视觉和自然语言处理中的多任务学习。在这些设置中，我们讨论了具体的监督任务、可用数据的性质以及具体的自监督方法。
- en: '**Further Reading**'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**进一步阅读**'
- en: '[https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/](https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/)'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/](https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/)'
- en: '[https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf)'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf)'
- en: '[https://arxiv.org/pdf/1301.3781.pdf](https://arxiv.org/pdf/1301.3781.pdf)'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/1301.3781.pdf](https://arxiv.org/pdf/1301.3781.pdf)'
- en: '[https://arxiv.org/pdf/2103.01988.pdf?fbclid=IwAR2pqhYda6MV9r2b3Afx_0eKUiZhX-Es6Pa_FbLOqH8fglQzO2kY3yKxZE8](https://arxiv.org/pdf/2103.01988.pdf?fbclid=IwAR2pqhYda6MV9r2b3Afx_0eKUiZhX-Es6Pa_FbLOqH8fglQzO2kY3yKxZE8)'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/2103.01988.pdf?fbclid=IwAR2pqhYda6MV9r2b3Afx_0eKUiZhX-Es6Pa_FbLOqH8fglQzO2kY3yKxZE8](https://arxiv.org/pdf/2103.01988.pdf?fbclid=IwAR2pqhYda6MV9r2b3Afx_0eKUiZhX-Es6Pa_FbLOqH8fglQzO2kY3yKxZE8)'
