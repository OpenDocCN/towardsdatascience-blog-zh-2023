- en: 'Multi-Task Machine Learning: Solving Multiple Problems Simultaneously'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/multi-task-learning-4531eb32d77b](https://towardsdatascience.com/multi-task-learning-4531eb32d77b)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Some supervised, some unsupervised, some self-supervised, in NLP and computer
    vision
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://jagota-arun.medium.com/?source=post_page-----4531eb32d77b--------------------------------)[![Arun
    Jagota](../Images/3c3eb142f671b5fb933c2826d8ed78d9.png)](https://jagota-arun.medium.com/?source=post_page-----4531eb32d77b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4531eb32d77b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4531eb32d77b--------------------------------)
    [Arun Jagota](https://jagota-arun.medium.com/?source=post_page-----4531eb32d77b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4531eb32d77b--------------------------------)
    ·11 min read·Apr 27, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f245371498b59c216924e720580c26fd.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: Image by [Gerd Altmann](https://pixabay.com/users/geralt-9301/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=3685928)
    From [Pixabay](https://pixabay.com/)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '*Single-task learning* is the process of learning to predict a single outcome
    (binary, multi-class, or continuous) from a labeled data set.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: By contrast, *multi-task learning* is the process of jointly learning to predict
    multiple outcomes on inputs of the same modality. Such as images or text.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: The obvious question to ask is, why learn *jointly*? Why not independently learn
    single-task models to predict the various outcomes?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: The answer is that joint learning can learn features that generalize better
    across tasks. Those that form good predictors for multiple tasks are favored over
    those that don’t. The learned features may even generalize to new prediction tasks
    in the same domain.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '**Adding Unsupervised Learning To The Mix**'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: To this point, we have supposed that all the chosen tasks in the multi-task
    setting are supervised in nature. Let’s relax this to allow some of the tasks
    to be unsupervised.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Why? Because we may have a lot more data to train on. Some labeled for the various
    outcomes, and a lot unlabeled. Adding unsupervised tasks to the joint learning
    allows us to potentially learn from a much larger data set.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: We can in fact empirically test this hypothesis. We can hold out a portion of
    the labeled data set and do two experiments, one in which we train on the rest
    of the labeled data, and in the second we train on the rest of the labeled data
    combined with a huge amount of unlabeled data. We can then compare the quality
    of the models learned in the two experiments on the held-out (labeled) test set.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Well, the concept of learning from a mix of labeled and unlabeled data already
    has a name for it. It is called semi-supervised learning.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '**Refining Unsupervised Learning To Self-Supervised Learning**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: While allowing some of the tasks to be unsupervised allows us to use a potentially
    huge amount of unlabeled data, can we leverage the unlabeled data in a better
    way? The general answer is Yes. Using self-supervision.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: In self-supervision, we define supervised tasks to predict certain characteristics
    of the data from the rest. For instance, we might predict the next word from the
    previous words seen till then.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Self-supervision is more powerful than unsupervised learning because it is supervised.
    Plus, it learns from all of the unlabeled data available as it does not require
    humans to label it. Finally, self-supervision is a powerful mechanism to get a
    large and diverse set of labeled data without humans needing to do any work. In
    the sections on computer vision and NLP, we will bring out crisp and realistic
    examples that will reveal not only how diverse of a labeled data set we can construct,
    but in what way this improves the learning ability over not doing self-supervision
    at all.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: In this post, when we say multi-task learning what we really mean is learning
    in which there are at least two supervised learning tasks, perhaps additional
    unsupervised learning tasks, and perhaps additional self-supervision tasks.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: We depict this visually below.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5f7ca4da8fb3370d1c25a86634908858.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: (By Author) A Venn Diagram Of Data Of A Certain Modality, Such As
    Text. Solid circles denote labeled subsets for various supervised tasks. Dashed
    circles denote self-labeled subsets for various self-supervised tasks.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Note that this figure only depicts subsets of the input data as covered by various
    supervised and self-supervised tasks, not how the labels for the various tasks
    relate to each other.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1 also depicts
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: The subsets of data labeled for various supervised tasks tend to be small.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding self-supervised tasks can cover more of the data. These subsets can often
    be made larger.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Subsets of supervised tasks can overlap with each other but are often not identical.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Relaxing The Term “Joint Learning” As Used In This Post**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: So far we’ve given the impression that joint learning means learning a single
    model simultaneously from all of the data available for all of the tasks.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: We’d, in fact, like to relax this to permit more flexible training policies.
    This is because in practice it has been found effective to learn the self-supervised
    (and possibly unsupervised) tasks first from the unlabeled data and then tune
    these models for particular downstream supervised tasks further. One way this
    approach is more flexible is because it naturally permits new supervised tasks
    to be defined on-the-fly and fine-tune existing models as needed.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '**On Data For Multi-Task Learning**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: We need multiple labeled data sets, one for each outcome we are trying to predict.
    The inputs in these data sets are generally from the same domain. The actual inputs
    can vary across the data sets, as they are often collected from disparate sources.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: We may additionally have unlabeled data. We should use it for suitable unsupervised
    tasks added to the mix so long as we reasonably believe it will help. As mentioned
    in a previous section, we can empirically test our belief before deciding one
    way or the other. We should also define suitable self-supervision tasks, if appropriate,
    to leverage and learn to predict certain characteristics of the unlabeled data
    from others.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Multi-task learning has proven especially useful in NLP and in computer vision.
    We’ll cover each one by one.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '**Multi-task Learning In Computer Vision**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Imagine this. We have access to labeled images of handwritten digits, of cars,
    of images containing human faces, of images containing pets, and many others.
    Let’s also suppose that some of these data sets are small.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Building single-task models for each of these individually runs the risk that
    we don’t have sufficiently rich labeled data for some of the tasks.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Combining all the data sets and learning a single multi-task model from them
    may help alleviate the aforementioned task-specific data sparsity problems. So
    long as some of the features that get learned jointly also help with those tasks.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: The thinking is that images are images and at some sufficiently low level, certain
    features may be predictive of the outcomes in multiple tasks. Such as edges or
    loops to name two.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '**The Value Of Unsupervised Tasks In This Setting**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Previously we mentioned that adding suitable unsupervised tasks to the mix in
    multi-task learning can be beneficial. We sharpen the intuition in the setting
    of images.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: There are billions and billions of images accessible on the web. Most are unlabeled.
    Imagine we have some labeled images for a certain task. Say predicting whether
    or not there is a building in an image.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Intuition would suggest that doing unsupervised learning on lots of unlabeled
    images can help discover better features than using only the labeled images. Such
    as better edges and better loops.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '**Self-Supervised Learning Tasks On The Unlabeled Data**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: As noted in the previous section, there is typically a huge amount of unlabeled
    image data. Billions and billions of images. In the previous section, we focused
    on semi-supervised learning. That is in addition to these unlabeled images we
    have a few labeled for a particular task. We want to learn in an unsupervised
    way from the unlabeled ones and in a supervised way from the labeled ones.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Introducing self-supervised learning into the mix can turbo-charge this approach.
    This is because appropriate self-supervised tasks force the model to learn representations
    that predict some characteristics of the images from others. And typically this
    easily produces a lot of labeled data, without the human having to put in any
    effort.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '**Some Specific Forms Of Self-Supervision**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Below are some specific forms of self-supervision that are generally applicable
    to image data.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '**Masking off certain regions**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Take an image and blur out certain entire regions. Create a new instance in
    which the blurred image is the input and the original instance is its label.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: To fire our imagination of how this might help, imagine that we have images
    of people, some wearing hats some not. If we somehow mask off all faces in the
    images, we may be able to learn that below a hat is a head of a person, but not
    the other way around, i.e. not all heads have hats on them.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: This begs the question, how do we figure out what to mask? We will look at this
    a bit differently. Instead of trying to optimize our masking, we will suggest
    considering a simple strategy of partitioning the image into rectangular grids
    of certain dimensions and creating instances with a single grid masked off in
    each instance. We can control how many corrupted instances we get from any one
    image by choosing the granularity of the gridding.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: We can then automate the process of discovering which corrupted images can be
    reconstructed well. We can even test the hypothesis of whether this self-supervision
    improves the accuracy of one or more downstream supervised tasks.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '**De-coloring Images**'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: By greying out colored images, we can force the model to learn to try to reconstruct
    the coloring from the remaining attributes. For example, it might learn that stop
    signs, which have a distinctive shape and often even the word *Stop* spelled out
    tend to be red in color.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '**Coarsening The Resolution**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: We can coarsen the resolution of an image and ask the model to try to reconstruct
    the original. This may force the model to learn to fill in fine details when possible.
    As an imagined evocative example, the model may learn to fill in certain details
    in coarsened versions of human faces such as eyelashes.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '**Multi-task Learning In NLP**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Consider a textual sentence. We might be interested in labeling each word with
    its part of speech, labeling certain segments as named entities, and labeling
    certain segments as being noun phrases or verb phrases.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: In single-task learning, each of these would be treated independently.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: In multi-task learning, we would learn a model with sharable features for all
    of these tasks simultaneously. This can yield a better model when the tasks have
    synergy. For instance, learning to predict the part of speech of each word in
    the sentence may help detect named entities, noun phrases, and verb phrases as
    well.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '**Adding Suitable Tasks On Unlabeled Data**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: There may be lots of sentences with no labels on them. No labels meaning no
    parts of speech tagged, no named entities tagged, and no noun or verb phrases
    tagged.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: We should consider defining the self-supervised learning task of predicting
    the next word in a sentence from the words seen thus far. This task can leverage
    the unlabeled data in a powerful way. Predicting the next word can force the network
    to learn elaborate representations of the previous words.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: More generally, it may be useful to define self-supervised learning tasks to
    predict certain masked-off words from the rest in a sentence. I say “more generally”
    because these words may be anywhere in the sentence.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Here is an example. Consider
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: exposure to sunlight causes skin cancer.
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Three masked-off instances we may consider predicting are depicted in the two
    fill-in-the-blanks scenarios below.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: _____ causes skin cancer.
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: exposure to sunlight causes ______ .
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: _ to _ causes skin cancer
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In the first one, we predict the left tail of the sentence from the rest. In
    the second one, we predict the right tail of the sentence from the rest. The third
    one illustrates that there can be multiple non-contiguous masked-off subsequences
    to predict.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '**Intuition Behind Predicting Masked-Off Words**'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Why go beyond predicting the next word to predicting any subsequences of masked-off
    words? The short answer is that (i) this vastly expands the labeled data set available
    for training, and (ii) with a tendency towards surfacing new types of scenarios
    in the newly labeled instances.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: To sharpen our intuition around (i) imagine that we generate masked-off labeled
    instances for all possible maskings of a sentence comprising *n* words. There
    are of the order 2^*n* maskings. By contrast, there are only of the order *n*
    ways to slice the sentence to predict words in the future.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: To elaborate on (ii), by allowing the maskings to be anywhere, we are allowing
    the model to discover predictive relationships that are potentially much more
    general than those constrained to be left to right. To imagine this, say we have
    lots of instances of *X causes Y* in our corpus. Also, suppose that only *X* causes
    *Y*. The model can learn this relationship that only *X* causes *Y* from the data
    if *X* is masked off. The left-to-right language model cannot learn that only
    *X* causes *Y*.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '**A Second, Very Simple, Example**'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that we have a list of names of cars. Such as
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Honda Civic, Toyota Celica, Ford Mustang, Jeep Wrangler.
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Predicting the next word will certainly help the model learn models of particular
    makes. Such as *Cherokee* and *Wrangler* (among others) for *Jeep*. However, additional
    self-supervision in the form of masking off any word will help the model also
    learn that model names tend to be strongly predictive of the make. For example,
    *Celica* is a *Toyota*, *Mustang* is a *Ford*, *Wrangler* is a *Jeep*, and so
    on.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '**Particular Cases**'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: Two particular cases of self-supervision of this type are the continuous bag-of-words
    model (CBOW) and the skip-gram model [3]. The former corresponds to masking off
    the middle word in the context of a certain number of words to the left and to
    the right of it. The latter corresponds to the opposite — masking off the left
    and the right context while keeping the word in the middle intact.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Below are the two illustrated in our example.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Another self-supervised task found to be useful is next-sentence prediction
    [2]. Especially for question-answering systems. In [2], this task is formulated
    as a binary classification problem. The input (X1, X2) is interpreted as “sentence
    X1 is followed by sentence X2”. Positive instances are generated from pairs of
    adjacent sentences in the corpus. Negative instances are generated by pairing
    a sentence X1 in the corpus with a random sentence X2 in the corpus.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: In the context of the above paragraph, the term “sentence” denotes any contiguous
    sequence of text [2]. So predicting the next paragraph from the current paragraph
    would also be viewed as a next-sentence prediction problem.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '**Leveraging Deep Architectures**'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Multi-task learning can significantly leverage deep architectures. The intuition
    is this. Deep architectures learn low-level features in layers close to the input
    and higher-level features in layers closer to the output. Intuition suggests that
    the deeper layers, those that learn low-level features, are precisely the ones
    that can be shared across tasks.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Consider our multi-task image classification scenario. It's reasonable to speculate
    that low-level features such as edges or loops will be useful predictors in multiple
    of these tasks.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Below depicts a deep architecture in which the layers closer to the input are
    shared by all the supervised tasks and the layers closer to the tasks are task-specific.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e1499550cc1cb9b33d4fe242d30097fb.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: A Multi-task Deep Architecture With Shared Layers. (By Author.)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Training can be done via the usual backpropagation algorithm. A particular instance
    in the training set may have a label from only one of the tasks. The error between
    the prediction and the target for this task gets backpropagated through the task-specific
    layers and then on to the shared layers. Consequently, if we mix in instances
    from different tasks, the shared layers learn representations that generalize
    across the tasks.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '**Summary**'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we covered the topic of multi-task learning. We explained why
    multi-task learning can yield models that understand better and generalize better
    than models that learn each of the tasks separately. This approach is especially
    powerful when we add unsupervised and self-supervised tasks to the mix. These
    tasks allow us to learn a lot from unlabeled data.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: We also covered multi-task learning in computer vision and NLP. In these settings,
    we discussed specific supervised tasks, the nature of the data available, and
    specific self-supervision methods.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '**Further Reading**'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**进一步阅读**'
- en: '[https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/](https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/)'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/](https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/)'
- en: '[https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf)'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf)'
- en: '[https://arxiv.org/pdf/1301.3781.pdf](https://arxiv.org/pdf/1301.3781.pdf)'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/1301.3781.pdf](https://arxiv.org/pdf/1301.3781.pdf)'
- en: '[https://arxiv.org/pdf/2103.01988.pdf?fbclid=IwAR2pqhYda6MV9r2b3Afx_0eKUiZhX-Es6Pa_FbLOqH8fglQzO2kY3yKxZE8](https://arxiv.org/pdf/2103.01988.pdf?fbclid=IwAR2pqhYda6MV9r2b3Afx_0eKUiZhX-Es6Pa_FbLOqH8fglQzO2kY3yKxZE8)'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/2103.01988.pdf?fbclid=IwAR2pqhYda6MV9r2b3Afx_0eKUiZhX-Es6Pa_FbLOqH8fglQzO2kY3yKxZE8](https://arxiv.org/pdf/2103.01988.pdf?fbclid=IwAR2pqhYda6MV9r2b3Afx_0eKUiZhX-Es6Pa_FbLOqH8fglQzO2kY3yKxZE8)'
