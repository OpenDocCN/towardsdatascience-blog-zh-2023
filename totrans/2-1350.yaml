- en: Introduction to Markov chain Monte Carlo (MCMC) Methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 马尔科夫链蒙特卡罗 (MCMC) 方法介绍
- en: 原文：[https://towardsdatascience.com/introduction-to-markov-chain-monte-carlo-mcmc-methods-b5bad18bc243](https://towardsdatascience.com/introduction-to-markov-chain-monte-carlo-mcmc-methods-b5bad18bc243)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/introduction-to-markov-chain-monte-carlo-mcmc-methods-b5bad18bc243](https://towardsdatascience.com/introduction-to-markov-chain-monte-carlo-mcmc-methods-b5bad18bc243)
- en: Markov chains, Metropolis-Hastings, Gibbs sampling, and how it relates to Bayesian
    inference
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 马尔科夫链、Metropolis-Hastings 算法、Gibbs 采样，以及它们与贝叶斯推断的关系
- en: '[](https://medium.com/@hrmnmichaels?source=post_page-----b5bad18bc243--------------------------------)[![Oliver
    S](../Images/b5ee0fa2d5fb115f62e2e9dfcb92afdd.png)](https://medium.com/@hrmnmichaels?source=post_page-----b5bad18bc243--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b5bad18bc243--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b5bad18bc243--------------------------------)
    [Oliver S](https://medium.com/@hrmnmichaels?source=post_page-----b5bad18bc243--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@hrmnmichaels?source=post_page-----b5bad18bc243--------------------------------)[![Oliver
    S](../Images/b5ee0fa2d5fb115f62e2e9dfcb92afdd.png)](https://medium.com/@hrmnmichaels?source=post_page-----b5bad18bc243--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b5bad18bc243--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b5bad18bc243--------------------------------)
    [Oliver S](https://medium.com/@hrmnmichaels?source=post_page-----b5bad18bc243--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b5bad18bc243--------------------------------)
    ·14 min read·Feb 21, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b5bad18bc243--------------------------------)
    ·阅读时间 14 分钟·2023年2月21日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f1fc4e2428f11d573f2bd8434e5b64c7.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1fc4e2428f11d573f2bd8434e5b64c7.png)'
- en: Photo by [Edge2Edge Media](https://unsplash.com/@edge2edgemedia?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/uKlneQRwaxY?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Edge2Edge Media](https://unsplash.com/@edge2edgemedia?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    在 [Unsplash](https://unsplash.com/photos/uKlneQRwaxY?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    上提供
- en: This post is an introduction to Markov chain Monte Carlo (MCMC) sampling methods.
    We will consider two methods in particular, namely the Metropolis-Hastings algorithm
    and Gibbs sampling. We will introduce them and prove why they work, implement
    practical examples in Python, and eventually explain how sampling is applied for
    Bayesian inference and why it is so important.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本文是对马尔科夫链蒙特卡罗 (MCMC) 采样方法的介绍。我们将特别考虑两种方法，即 Metropolis-Hastings 算法和 Gibbs 采样。我们将介绍这些方法并证明它们的有效性，提供
    Python 中的实际示例，最后解释采样如何应用于贝叶斯推断以及其重要性。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: MCMC methods are a family of sampling methods which make use of Markov chains
    to generate dependent data samples. Their basic idea is to build such Markov chains,
    which are easy to sample from, and whose stationary distribution is our target
    distribution — such that when following them, in the limit, we obtain samples
    from the target distribution.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: MCMC 方法是一类采样方法，利用马尔科夫链生成依赖的数据样本。其基本思想是构建这样一种马尔科夫链，使得采样简单，而其平稳分布即为我们的目标分布——这样当跟随这些链时，最终我们能从目标分布中获得样本。
- en: 'Why do we need this? In a previous post I introduced basic sampling methods,
    among others covering [rejection and importance sampling for complex distributions](/introduction-to-sampling-methods-c934b64b6b08).
    These generate independent data samples, whereas here we generate dependent ones,
    as mentioned — which does not answer the previous question, but is an important
    distinction. However, in the previous posts we saw that the presented methods
    suffer from severe limitations: it is hard to find suited proposal distributions,
    in particular in high-dimensional spaces, yielding to high variance and wasteful
    computations.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们需要这个？在之前的一篇文章中，我介绍了基本的采样方法，其中包括了[复杂分布的拒绝采样和重要性采样](/introduction-to-sampling-methods-c934b64b6b08)。这些方法生成独立的数据样本，而这里我们生成的是依赖的数据样本，如前所述——这并没有回答之前的问题，但这是一个重要的区别。然而，在之前的文章中，我们看到所提出的方法存在严重的限制：很难找到合适的建议分布，特别是在高维空间中，这会导致高方差和浪费计算。
- en: 'MCMC methods, i.e. following a (simple) Markov chain fare better in these circumstances,
    in particular due to less needed information about the distribution we want to
    sample from, and the fact that we only need to be able to evaluate it up to a
    fixed factor. That is: we do not need to be able to evaluate the full pdf for
    a given `x`, `p(x)`, but it suffices to be able to compute `zp(x)`. At the end
    of this article we will see why this is so powerful, by applying it to solve a
    Bayesian inference problem. In many tutorials and explanations this last bit is
    just given quite briefly as a side note — but I believe this deserves — especially
    for beginners to Bayesian inference — more spotlight.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，MCMC 方法，即遵循（简单的）马尔可夫链表现更好，特别是由于对我们要采样的分布所需的信息较少，以及我们只需要能够评估到一个固定因子的事实。也就是说：我们不需要能够评估给定`x`的完整pdf，`p(x)`，但能够计算`zp(x)`就足够了。在本文的最后，我们将通过应用它来解决一个贝叶斯推断问题，来看到它为何如此强大。在许多教程和解释中，这最后一点通常只是作为附带说明简要提及——但我认为这值得——特别是对于贝叶斯推断的初学者——更多关注。
- en: 'Naturally, there are disadvantages of MCMC methods too though: due to the samples
    being correlated, the *effective* sample size shrinks, and occasionally the methods
    might not converge or be very slow at it.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，MCMC 方法也有其缺点：由于样本之间的相关性，*有效*样本大小会减少，并且方法有时可能不收敛或非常慢。
- en: Markov Chains
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 马尔可夫链
- en: Since, as the name suggests (and we stated it multiple times so far), MCMC methods
    are based on [Marko Chains](https://en.wikipedia.org/wiki/Markov_chain), we introduce
    these first.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 由于正如名字所示（而且我们到目前为止已经多次提到过），MCMC 方法基于 [马尔可夫链](https://en.wikipedia.org/wiki/Markov_chain)，因此我们首先介绍这些方法。
- en: They are a way of modelling stochastic processes as a sequence of events. In
    this, the *Markovian property* states that the next state only depends on the
    current, and not any historic information.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是将随机过程建模为事件序列的一种方法。在这其中，*马尔可夫性质*指出下一个状态仅依赖于当前状态，而不依赖于任何历史信息。
- en: '(Small excursus: many practical ML methods require this property, such as RL.
    Requiring this one-step dependency might seem very limiting and impractical —
    however note that we can simply expand the state space to arbitrary dimension,
    in particular including past events in the current state — and thus totally circumventing
    this “limitation”.)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: （小插曲：许多实际的机器学习方法要求这一属性，例如强化学习。要求这种一步依赖可能看起来非常限制和不切实际——但请注意，我们可以简单地将状态空间扩展到任意维度，特别是将过去的事件包含在当前状态中——从而完全绕过这种“限制”。）
- en: Formally, let us consider a random variable `X`, and denote its per-timestep
    realisations with `X₀`, `X₁`, … How `X` develops over time is given by a transition
    function `P`, where
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，设我们考虑一个随机变量`X`，并用`X₀`、`X₁`等表示其每个时间步的实现。`X`随时间的发展由一个转移函数`P`给出，其中
- en: '![](../Images/02d81ab19151acffac1d93168c539508.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02d81ab19151acffac1d93168c539508.png)'
- en: denotes that the chance of `X` transitioning from state `i` to state `j` is
    `p`.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 表示`X`从状态`i`转移到状态`j`的概率是`p`。
- en: To fully specify a Markov chain, in addition we need to define an initial distribution
    for `X`, denoted by `π₀`. With this, we can follow the Markov chain, from `π₀`
    iteratively applying `P`, yielding the per-timestamp distributions `π₁`, `π₂`,
    …
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 要完全指定一个马尔可夫链，此外我们还需要定义一个初始分布`π₀`。有了这个，我们可以从`π₀`开始，迭代应用`P`，得到每个时间戳的分布`π₁`、`π₂`等。
- en: 'Let us visualise this with an example. We chose the following transition matrix:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来可视化这一点。我们选择了以下转移矩阵：
- en: '![](../Images/5082dd6763eec64021c6c7c472d6fb4b.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5082dd6763eec64021c6c7c472d6fb4b.png)'
- en: Note that in our notation index `ij` denotes the transition probabilities from
    state `j` to `i`, for convenience.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在我们的符号中，索引`ij`表示从状态`j`到`i`的转移概率，出于方便考虑。
- en: 'We now take a random initial distribution, and follow the Markov chain for
    30 steps. This can be implemented as follows in Python:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在取一个随机初始分布，并跟踪马尔可夫链的30步。可以通过以下 Python 代码实现：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'When executing this program, we will get some output similar to this:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此程序时，我们将得到类似于以下的输出：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As we can see, this Markov chain converges — for any initial distribution —
    to the distribution `[0.5, 0.1, 0.4]` — which we call the stationary distribution
    of this Markov chain.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，这个马尔可夫链会收敛——对于任何初始分布——到分布 `[0.5, 0.1, 0.4]`——我们称之为该马尔可夫链的平稳分布。
- en: 'Before moving on, we will introduce a criterion, needed in the following sections,
    to determine whether a Markov chain converges: *detailed balance*. We say a Markov
    chain satisfies the detailed balance criterion, if there exists a distribution
    `π` satisfying:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我们将介绍一个标准，在后续章节中需要用来判断马尔可夫链是否收敛：*详细平衡*。我们说一个马尔可夫链满足详细平衡标准，如果存在一个分布 `π`
    满足：
- en: '![](../Images/b39d9e9063701e593522c35345a382d7.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b39d9e9063701e593522c35345a382d7.png)'
- en: I.e., the probability of transitioning from state `j` to state `i` is the same
    as the reverse transition, considering the distribution `π`. Intuitively this
    should also make sense, as to why this yields a stationary distribution. Feel
    free to convince yourself that this criterion is satisfied for above defined Markov
    chain, and that indeed `[0.5, 0.1, 0.4]` is the distribution satisfying it.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 即，从状态 `j` 转移到状态 `i` 的概率与考虑分布 `π` 的反向转移概率相同。直观上这也应该是合理的，因为这解释了为什么会产生一个平稳分布。请随意确认上述定义的马尔可夫链是否满足该标准，并且确实
    `[0.5, 0.1, 0.4]` 是满足它的分布。
- en: Metropolis-Hastings
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Metropolis-Hastings
- en: Equipped with this knowledge, we now describe and introduce one of the most
    common and frequently used MCMC algorithms, namely the [Metropolis-Hastings algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm).
    To recap, what we are trying to do is sample values from a difficult probability
    distribution `f(x)`, the target distribution.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 凭借这些知识，我们现在描述并介绍一种最常见和最常用的 MCMC 算法，即 [Metropolis-Hastings 算法](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm)。总而言之，我们要做的是从一个难以处理的概率分布
    `f(x)`（目标分布）中采样值。
- en: 'Let’s begin with an overview over the algorithm. Essentially, it is made up
    of the following steps:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从算法概述开始。实际上，它由以下步骤组成：
- en: Select an arbitrary initial value `x₀` in the target distribution’s support
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在目标分布的支持范围内选择一个任意初始值 `x₀`
- en: Draw `y₁` using a proposal distribution `q`
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用提议分布 `q` 抽取 `y₁`
- en: Compute `p₁` (see below)
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 `p₁`（见下文）
- en: Draw `u₁` from the uniform distribution over [0, 1]
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从均匀分布 `[0, 1]` 中抽取 `u₁`
- en: Set `x₁ = y₁` if `u₁ ≤ p₁`, else set `x₁ = x₀`
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 `u₁ ≤ p₁`，则设置 `x₁ = y₁`，否则设置 `x₁ = x₀`
- en: Repeat steps 2–5
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤 2-5
- en: '`p₁` is given by:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`p₁` 由以下给出：'
- en: '![](../Images/990d62f08043f5efbfd2c2f5953c1271.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/990d62f08043f5efbfd2c2f5953c1271.png)'
- en: Example
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例
- en: 'Let’s demonstrate this using a concrete example, implemented in Python. The
    setup: the target distribution we want to sample is a Gaussian distribution. Our
    proposal distribution is another Gaussian. This naturally is no real-world practical
    example. However, I believe and hope, that this simplified settings helps understanding,
    instead of confusing the reader. Note that in this example, all values of interest
    are 1D.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个具体的例子来演示这个过程，使用 Python 实现。设置是：我们想要采样的目标分布是一个高斯分布。我们的提议分布是另一个高斯分布。这自然并不是一个现实世界的实际例子。然而，我相信并希望，这种简化的设置有助于理解，而不是使读者感到困惑。请注意，在这个例子中，所有感兴趣的值都是一维的。
- en: 'The corresponding Python code looks as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对应的 Python 代码如下：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Let’s go over this with some more details. In the beginning, we’re using scipy’s
    stats module to represent our target distribution `f` — then plot its pdf. We
    then define an initial value `x` to begin sampling with — simply generating one
    value from a uniform distribution. We then enter the sampling loop, iteratively
    generating `NUM_SAMPLES` value according to the algorithm introduced above. As
    proposal distribution we use another Gaussian `q` — which yields a new value `y`
    obtained by “jumping” away from `x` according to this Gaussian. It is probably
    worth noting, that the conditional evaluation of `q` equals `q`’s pdf with the
    given jump range — intuitively the further we jump, the less likely it becomes.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细了解一下。在开始时，我们使用 scipy 的 stats 模块来表示目标分布 `f` —— 然后绘制其概率密度函数（pdf）。接着我们定义一个初始值
    `x` 来开始采样 —— 仅仅是从均匀分布中生成一个值。然后我们进入采样循环，根据上述介绍的算法迭代生成 `NUM_SAMPLES` 个值。作为提议分布，我们使用另一个高斯分布
    `q` —— 这会生成一个新值 `y`，通过在 `x` 周围按照此高斯分布“跳跃”获得。值得注意的是，`q` 的条件评估等于给定跳跃范围的 `q` 的 pdf
    —— 直观地说，跳得越远，可能性就越小。
- en: 'Executing this program should yield a result similar to this:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此程序应产生类似以下的结果：
- en: '![](../Images/911cce25f9c50a0899befa04e3d88454.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/911cce25f9c50a0899befa04e3d88454.png)'
- en: We see that we correctly sampled from the “unknown” distribution `f`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到我们正确地从“未知”分布 `f` 中采样。
- en: Proof of Correctness
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正确性证明
- en: To prove the correctness of the Metropolis-Hastings algorithm, we need to show
    that the used Markov chain’s stationary distribution is indeed the target distribution.
    For this, we use above introduced notation of detailed balance. Remember, this
    involves showing that
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 要证明Metropolis-Hastings算法的正确性，我们需要展示所使用的马尔可夫链的平稳分布确实是目标分布。为此，我们使用了上述介绍的详细平衡符号。记住，这涉及到展示以下内容：
- en: '![](../Images/6c2fbdbe6c83267d45a41aa48d2ec5ec.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6c2fbdbe6c83267d45a41aa48d2ec5ec.png)'
- en: i.e. it does not matter whether we first visit state `(t-1)` and then transition
    to `(t)`, or vice versa.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 即无论我们是首先访问状态`(t-1)`然后转移到`(t)`，还是反之，都没有关系。
- en: 'Thus let’s evaluate the left side of this equation, and simply plug in our
    proposal distribution and the acceptance criterion:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们评估这个方程的左侧，并简单地代入我们的提议分布和接受标准：
- en: '![](../Images/3300d56bc2a80df286db4113a6c0e2bf.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3300d56bc2a80df286db4113a6c0e2bf.png)'
- en: 'A quick reformulation yields:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一个快速的重新表述为：
- en: '![](../Images/c41db989f62547d6b9fe7670594bc248.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c41db989f62547d6b9fe7670594bc248.png)'
- en: When doing this analogously for the right side of above target equation, we
    obtain the same result, concluding the proof.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当对上述目标方程的右侧进行类似操作时，我们得到相同的结果，从而完成证明。
- en: Discussion
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'We stated in the introduction that MCMC methods like Metropolis-Hastings are
    superior and more efficient than, e.g., rejection sampling. This is true, still
    we should put some effort into choosing q, as this choice will influence the speed
    of conversion. Consider again the acceptance criterion:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在引言中提到MCMC方法，如Metropolis-Hastings，比例如拒绝抽样更优越和高效。这是正确的，但我们仍应努力选择`q`，因为这一选择将影响转换的速度。再考虑一下接受标准：
- en: '![](../Images/e04627702d98f215d75281295238f8ff.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e04627702d98f215d75281295238f8ff.png)'
- en: If we defined `q` to be equal to `f`, we’d get 1 — i.e. accept all samples,
    which is the ideal case. Naturally, this is not possible, as we cannot sample
    from `f`(which is why we are doing this and sampling from `q` instead, after all).
    Still, this provides some intuition how to choose `q`. Conversely, if `q` is poorly
    chosen, we will reject many samples, obtaining several highly correlated samples,
    which is a problem (the chain is “stuck” in some region).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将`q`定义为等于`f`，我们将得到1——即接受所有样本，这是理想情况。自然，这是不可能的，因为我们无法从`f`中抽样（这也是我们要这样做并从`q`中抽样的原因）。不过，这提供了如何选择`q`的一些直觉。相反，如果`q`选择不当，我们将拒绝许多样本，得到多个高度相关的样本，这是一个问题（链条在某个区域“卡住”了）。
- en: 'These discussions are related to the terms *effective sample* size and *burn-in*.
    Since MCMC methods produce correlated, and not independent samples, when investigating
    these we have to consider this. In particular, this gives rise to the term effective
    sample size — which can be viewed as the actual sample size “cleaned” of effects
    due to correlation. Further, it is common to throw away the first `N` elements
    obtained by an MCMC algorithm (burn-in): this is mainly due to balance out “bad”
    initializations, which lie in regions of low probability and out of which the
    proposal distribution has a hard time getting out.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这些讨论与*有效样本*大小和*烧入*的术语相关。由于MCMC方法生成的是相关样本，而不是独立样本，在研究这些时我们必须考虑这一点。特别地，这产生了有效样本大小的术语——它可以被视为“清除”了相关效应的实际样本大小。此外，通常会丢弃由MCMC算法获得的前`N`个元素（烧入）：这主要是为了平衡“坏”的初始化，这些初始化位于低概率区域，并且提议分布难以摆脱这些区域。
- en: Gibbs Sampling
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Gibbs抽样
- en: As a second example of MCMC sampling methods we’ll have a look at [Gibbs sampling](https://en.wikipedia.org/wiki/Gibbs_sampling).
    Since we already introduced underlying ideas and proved correctness for one MCMC
    method, we’ll go considerably faster this time — but I still wanted to put it
    out there to reach sufficient depth of this tutorial, and would refer the reader
    to other resources for more details, or do the maths themselves.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 作为MCMC抽样方法的第二个例子，我们将看看[吉布斯抽样](https://en.wikipedia.org/wiki/Gibbs_sampling)。由于我们已经介绍了基本概念并证明了一种MCMC方法的正确性，这次我们会快得多——但我还是想把它放在这里，以达到教程的足够深度，并建议读者参考其他资源以获取更多细节，或者自己进行计算。
- en: Overview
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: Gibbs sampling is applied for sampling from distributions with multiple variables,
    where sampling from the joint distribution `p(X, Y)` is hard, but we do know how
    to sample the conditional distributions `p(X | Y)`, `p(Y | X)`. Making use of
    this, the employed Markov Chain iterates between sampling values for `X` and `Y`
    making use the updated conditional distributions. Thus, overall — pretty quick
    to introduce and implement — which we will do in the next section.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Gibbs 采样用于从具有多个变量的分布中进行采样，当从联合分布 `p(X, Y)` 采样困难时，我们可以采样条件分布 `p(X | Y)`、`p(Y
    | X)`。利用这一点，所使用的马尔科夫链在 `X` 和 `Y` 的采样值之间迭代，并利用更新后的条件分布。因此，整体上 — 引入和实现相当迅速 — 我们将在下一节中进行详细探讨。
- en: 'For the sake of introduction, we’ll consider a two-dimensional [multivariate
    normal distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution).
    This multi-dimensional normal distribution is characterised by a mean vector `μ`
    and a covariance matrix `Σ`. Conveniently, the needed conditionals again are normal
    distributions, and defined (exemplary for `x₁`) by:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了引入，我们将考虑一个二维的 [多元正态分布](https://en.wikipedia.org/wiki/Multivariate_normal_distribution)。这种多维正态分布由均值向量
    `μ` 和协方差矩阵 `Σ` 特征化。方便的是，所需的条件分布也是正态分布，且（以 `x₁` 为例）定义为：
- en: '![](../Images/000ef162d4dcfc55bee38a27770e9ebd.png)![](../Images/5a4217177bf288e1fb4b8ae58dab6651.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/000ef162d4dcfc55bee38a27770e9ebd.png)![](../Images/5a4217177bf288e1fb4b8ae58dab6651.png)'
- en: Implementation
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现
- en: 'To begin, let’s use `scipy.stats` to define and plot our target distribution:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们使用 `scipy.stats` 来定义并绘制我们的目标分布：
- en: '[PRE3]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We should get something like this:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该得到类似这样的结果：
- en: '![](../Images/c1f306082382d092fdce728e2d1332dc.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c1f306082382d092fdce728e2d1332dc.png)'
- en: 'Next, we execute the Gibbs sampling procedure as described above:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们按照上述描述执行 Gibbs 采样过程：
- en: '[PRE4]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Eventually, we convince ourselves the obtained distribution is correct by drawing
    a 3D histogram:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们通过绘制 3D 直方图来验证获得的分布是正确的：
- en: '[PRE5]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/17a334b9bbfbc20b80a0cefc39d11814.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/17a334b9bbfbc20b80a0cefc39d11814.png)'
- en: Applications in Bayesian Inference
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯推断中的应用
- en: 'To conclude this article, let’s give a real-world use case where such sampling
    becomes incredibly handy, and helps solve important problems: [Bayesian inference](https://en.wikipedia.org/wiki/Bayesian_inference).
    We’ll introduce this with more details in a future post, for now: this is solving
    for the “full” probability distribution of a given problem, and in particular
    calculating the probability distribution of the parameters given the data:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结这篇文章，让我们举一个实际的使用案例，在这个案例中，这种采样变得非常方便，并帮助解决重要问题：[贝叶斯推断](https://en.wikipedia.org/wiki/Bayesian_inference)。我们将在未来的帖子中详细介绍这一点，目前：这是解决给定问题的“完整”概率分布，特别是计算给定数据的参数的概率分布：
- en: '![](../Images/c65eadcada2478c53f78c81b1eedc2ff.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c65eadcada2478c53f78c81b1eedc2ff.png)'
- en: 'These terms are commonly known as:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这些术语通常被称为：
- en: '![](../Images/5be7854aafd96a21566940d45d750248.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5be7854aafd96a21566940d45d750248.png)'
- en: 'What makes solving this equation particularly challenging is the evidence,
    as this requires marginalising over all possible parameter values, i.e.:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 使解决这个方程特别具有挑战性的是证据，因为这需要对所有可能的参数值进行边际化，即：
- en: '![](../Images/9785de884a936b7618763b1b596592cd.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9785de884a936b7618763b1b596592cd.png)'
- en: This integral usually is hard to compute, or even intractable.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这个积分通常很难计算，甚至不可解。
- en: Thus, numerical approximations in the form of MCMC methods are a good starting
    point, and common choice for solving such problems. What is particularly useful
    in methods like Metropolis-Hastings, is their lax requirement of only needing
    to be able to evaluate distributions up to a certain normalisation constant —
    and the evidence just is such a constant! Meaning, we can formulate and work with
    the posterior distribution without the tricky denominator part.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，以 MCMC 方法的形式进行数值近似是一个很好的起点，并且是解决此类问题的常见选择。像 Metropolis-Hastings 这样的方法特别有用，因为它们只要求能够评估分布直到某个归一化常数
    — 证据恰好就是这样的常数！这意味着我们可以制定并处理后验分布，而不需要处理复杂的分母部分。
- en: 'Let’s demonstrate this with an example: we will flip an (unfair) coin `N` times,
    and are interested in finding out the probability `θ` of the coin landing heads.
    Particularly, we don’t just want to arrive at a point estimate, but instead go
    Bayesian, and model the full posterior.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子来说明：我们将抛掷一个（不公平的）硬币 `N` 次，并且有兴趣找出硬币正面朝上的概率 `θ`。特别地，我们不仅仅希望得到一个点估计，而是采用贝叶斯方法，建模完整的后验分布。
- en: 'Let’s analyse the single terms in more details: the result of throwing a coin
    follows a [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution),
    and, denoting by `Nₕ` the observed number of heads, and `Nₜ` the corresponding
    number of tails, for a given parameter value `θ` this yields the following likelihood:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地分析一下单个术语：抛硬币的结果遵循 [伯努利分布](https://en.wikipedia.org/wiki/Bernoulli_distribution)，并且，记
    `Nₕ` 为观察到的正面次数，`Nₜ` 为相应的反面次数，对于给定的参数值 `θ`，这产生以下似然函数：
- en: '![](../Images/c9df7b66ce44d61e935b375c2ccd922e.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c9df7b66ce44d61e935b375c2ccd922e.png)'
- en: 'Next, we need to find a suited prior — i.e. induce some sort of belief over
    the estimated parameter value. Since we don’t have to worry about solving the
    problem analytically (see: [conjugate priors](https://en.wikipedia.org/wiki/Conjugate_prior)),
    we are free to choose any prior. Thus, we simply pick a normal distribution with
    mean 0.5 and standard deviation 0.2 — expressing that we expect the coin to be
    around 50:50, but also cover all of [0, 1].'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要找到一个合适的先验——即对估计参数值引入某种信念。由于我们不必担心解析地解决问题（见：[共轭先验](https://en.wikipedia.org/wiki/Conjugate_prior)），我们可以自由选择任何先验。因此，我们简单地选择均值为
    0.5、标准差为 0.2 的正态分布——表示我们期望硬币大约是 50:50，但也覆盖 [0, 1] 的所有情况。
- en: 'These two terms are enough to run the Metropolis-Hastings algorithm. In it,
    we need to calculate `f(y)/f(x)` for some parameter values `y` and `x`, and the
    density function `f` we are interested in. In our case, as mentioned, this is
    `p(θ|x).` We also mentioned already, that the evidence cancels out, since this
    is a constant. What remains is the following:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个术语足以运行 Metropolis-Hastings 算法。在其中，我们需要计算某些参数值 `y` 和 `x` 的 `f(y)/f(x)`，以及我们感兴趣的密度函数
    `f`。在我们的例子中，正如前面提到的，这是 `p(θ|x)`。我们也已经提到，证据部分会被抵消，因为这是一个常数。剩下的就是以下内容：
- en: '![](../Images/8bf2a74917bfe90792a8e5bfec9d5487.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8bf2a74917bfe90792a8e5bfec9d5487.png)'
- en: 'With these assessments and above introduction to the Metropolis-Hastings algorithm,
    porting above into Python should be no hard feat:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些评估和上述对 Metropolis-Hastings 算法的介绍，将其移植到 Python 中应该不会太困难：
- en: '[PRE6]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Running this should print something like:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这段代码应该会输出类似的内容：
- en: '![](../Images/ced0a88512ca7f0f3385df82ec4131ed.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ced0a88512ca7f0f3385df82ec4131ed.png)'
- en: As we can see the algorithm correctly found the true `θ` of about 0.3, as shown
    by the mode of the posterior distribution. We can also observe some variance,
    which is good and actually expected / desired — this is one of the reasons we
    do full Bayesian inference. Throwing a coin “only” 100 times gives us a good first
    estimate of what its true flipping probability looks like, but to me more sure
    we’d like to see more examples.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，该算法正确地找到了约为 0.3 的真实`θ`，这从后验分布的模式中可以看出。我们还可以观察到一些方差，这很好，实际上也是预期中的——这就是我们进行全面贝叶斯推断的原因之一。抛硬币“仅”100次可以给我们一个对其真实翻转概率的良好初步估计，但为了更加确定，我们希望看到更多的例子。
- en: 'So let’s increase our dataset to 5000 throws, and inspect the output in this
    case:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们将数据集增加到 5000 次抛掷，并检查这种情况下的输出：
- en: '![](../Images/17592cf071b9d65e04de731537d93f7b.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/17592cf071b9d65e04de731537d93f7b.png)'
- en: Now, the posterior distribution indeed has a much lower variance, as expected.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，后验分布的方差确实比预期的要低得多。
- en: Conclusion
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'In this post we introduced Markov chain Monte-carlo (MCMC) methods, which are
    powerful methods for numerical sampling. Such methods allow us to efficiently
    sample from complex distributions without too strict requirements on tractability:
    in particular, we only need to be able to evaluate distributions of interest up
    to a fixed factor. MCMC methods work by generating a Marko chain, whose stationary
    distribution is the target distribution — we thus can follow this, and effectively
    sample the distribution we are after.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们介绍了马尔科夫链蒙特卡罗（MCMC）方法，这是一种用于数值采样的强大方法。这些方法使我们能够高效地从复杂分布中进行采样，而对可处理性的要求并不严格：特别是，我们只需要能够评估感兴趣的分布到固定因子的程度。MCMC
    方法通过生成马尔科夫链来工作，其平稳分布就是目标分布——我们可以跟踪这个分布，并有效地采样我们所需要的分布。
- en: As a first algorithm we introduced Metropolis-Hastings, and proved why it is
    correct. It works by introducing a proposal distribution, which is used to “jump”
    from the current point to a new one. This new point is accepted with a certain
    probability, which is proportional to the ratio of probability densities around
    these points.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一个算法，我们介绍了 Metropolis-Hastings，并证明了它的正确性。它通过引入一个提议分布来工作，该分布用于从当前点“跳”到一个新点。这个新点以一定的概率被接受，这个概率与这些点周围的概率密度比成正比。
- en: Next, we discussed Gibbs sampling, which is a method for sampling multi-dimensional
    distributions. Core idea is using conditional distributions and iterating through
    sampling a new value for each dimension while leaving the others fixed.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们讨论了吉布斯抽样，这是一种用于抽样多维分布的方法。核心思想是使用条件分布，并通过为每个维度抽样新值，同时保持其他维度固定来迭代。
- en: Eventually, we gave a practical example of Bayesian inference. Solving this
    problem analytically requires solving a complex integral, making it a prime example
    (and a very common one) for numerical approximation. We demonstrated how to estimate
    the posterior distribution of a Bernoulli variable simulating an unfair coin toss.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们给出了一个贝叶斯推断的实际例子。分析性地解决这个问题需要解一个复杂的积分，这使得它成为一个典型的（而且非常常见的）数值逼近示例。我们展示了如何通过模拟不公平抛硬币来估计伯努利变量的后验分布。
- en: I hope this post was informative for you and shed some light into this exciting
    field. Thanks for reading!
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这篇文章对你有帮助，并对这个令人兴奋的领域提供了一些见解。感谢阅读！
- en: '*All images, unless denoted otherwise, were generated by the author.*'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*所有图像，除非另有说明，均由作者生成。*'
- en: 'This post is Part 3 of a series about sampling. You can find the others here:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 本文是关于抽样的系列文章的第 3 部分。你可以在这里找到其他部分：
- en: 'Part 1: [Introduction to Sampling Methods](https://medium.com/towards-data-science/introduction-to-sampling-methods-c934b64b6b08)'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第 1 部分：[抽样方法简介](https://medium.com/towards-data-science/introduction-to-sampling-methods-c934b64b6b08)
- en: 'Part 2: [Variance Reduction with Importance Sampling](/variance-reduction-with-importance-sampling-4e5ca4b1c5a7)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第 2 部分：[通过重要性抽样减少方差](/variance-reduction-with-importance-sampling-4e5ca4b1c5a7)
