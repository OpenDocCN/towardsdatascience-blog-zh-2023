# GPT是一个不可靠的信息存储库

> 原文：[https://towardsdatascience.com/chatgpt-insists-i-am-dead-and-the-problem-with-language-models-db5a36c22f11](https://towardsdatascience.com/chatgpt-insists-i-am-dead-and-the-problem-with-language-models-db5a36c22f11)

## 了解大型语言模型的局限性和危险

[](https://medium.com/@nobleackerson?source=post_page-----db5a36c22f11--------------------------------)[![Noble Ackerson](../Images/e89361fc2723b2b08384ace7f081bfed.png)](https://medium.com/@nobleackerson?source=post_page-----db5a36c22f11--------------------------------)[](https://towardsdatascience.com/?source=post_page-----db5a36c22f11--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----db5a36c22f11--------------------------------) [Noble Ackerson](https://medium.com/@nobleackerson?source=post_page-----db5a36c22f11--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----db5a36c22f11--------------------------------) ·阅读时间9分钟·2023年2月21日

--

![](../Images/877f131a65aa9cec5806a78ebd5af18a.png)

“寻找意义” 作者拍摄

大型语言模型（或生成预训练变换器，GPT）需要更可靠的信息准确性检查，才适合用于搜索。

这些模型在讲故事、艺术、音乐等创造性应用以及为应用创建隐私保护的合成数据方面表现出色。

然而，这些模型由于[AI幻觉](https://www.wired.com/story/ai-has-a-hallucination-problem-thats-proving-tough-to-fix/)和ChatGPT、Bing Chat以及Google Bard中的迁移学习限制，无法保持一致的事实准确性。

首先，让我们定义一下什么是AI幻觉。有时，大型语言模型会生成基于不真实证据的信息，这些信息可能受到其变换器架构的偏见或错误解码的影响。换句话说，该模型会编造事实，这在对事实准确性要求严格的领域中可能会造成问题。

在一个准确和可靠的信息对抗虚假信息和错误信息至关重要的世界中，忽视一致的事实准确性是危险的。

搜索公司应该重新考虑“重新发明搜索”，将搜索与未经过滤的GPT驱动聊天模式混合，以避免对公共健康、政治稳定或社会凝聚力的潜在危害。

本文通过一个例子扩展了这一断言，展示了ChatGPT如何确信我已经去世四年，以及我那看起来非常真实的讣告，突显了使用GPT进行基于搜索的信息检索的风险。你可以尝试在ChatGPT中输入我的名字，然后让它相信我还活着。

几周前，我决定做一些轻度研究，因为我了解到 Google 因为 Bard——ChatGPT 的竞争者——在一次匆忙的演示中分享了一些不准确的信息，从而让其市值蒸发了 1000 亿美元。市场似乎对这种技术的[可靠性和可信度反应负面](https://www.cnbc.com/video/2023/02/16/the-new-york-times-kevin-roose-on-his-conversation-with-microsofts-ai-powered-chatbot-bing.html)，但我觉得我们并没有充分将这些担忧与媒体联系起来。

我决定在 ChatGPT 上“自我搜索”。*注：我刚发现“自我搜索”这个词*。我们都曾在 Google 上搜索过自己，但这次是用 ChatGPT 来搜索。

这个决定是故意的，因为没有比问它关于我的事情更好的事实准确性测试方式了。而且这个决定没有让人失望；我一直得到相同的结果：**我学到我已经死了。**

![](../Images/7650da69d1d2fed0fdb668108903f6fc.png)

ChatGPT 声称我在 2019 年去世

[这是整个对话的截取版本](https://gist.github.com/stigsfoot/acf985bf3a22c5c46ed787e34f638991)。

## ChatGPT 认为我死了！？

> ChatGPT 坚持说我已经死了，当我反驳时它更加固执，并创建了一个全新的角色。我现在明白了为什么大型语言模型是不可靠的信息来源，也理解了为什么微软 Bing 应该将聊天功能从其搜索体验中移除。

哦……我还了解到，如果我在[我的前一家创业公司 LynxFit](https://9to5google.com/2014/12/08/google-glass-app-lynxfit-join-apx-labs/)之后创建了其他技术企业的话，那就更奇怪了。它似乎对我和我的联合创始人在 LynxFit 建立的内容感到困惑，编造了一个故事，说我在加纳创办了一家运输公司。加纳？那也是我来自的地方。等等……**虚假与真实混合是经典的虚假信息**。到底发生了什么？

好吧，它对一个事实半对半错，其他几乎所有事实都是编造的，这让人不安。我很确定我还活着。在 Lynxfit，我开发了 AR 软件来跟踪和指导用户的穿戴式健身训练，而不是一个智能跳绳。此外，我是加纳裔，但我从未为加纳开发过运输应用。

> **一切似乎都很合理，但这位“虚伪的门德西斯”编造了整个故事。**

OpenAI 的文档明确指出，ChatGPT 具备通过用户的上下文线索或反馈承认错误的技术。因此，自然而然地，我给了它一些上下文线索和反馈，告诉它它在“梦见一个变种的 Earth-Two Noble Ackerson”，而不是来自这个现实的那一个。这并没有奏效，它更加固执，选择了更严重的错误。

![](../Images/9137bf800f8019d4d3afdc28b3822007.png)

ChatGPT 对错误的事实固执己见。验证它是否认为自己的回答是事实。

> 嗯……你确定吗？试图让聊天机器人变得更为真实，就像对着播放录音的公共广播系统大喊大叫一样。这是一件很古怪的事情，但为了“研究”，我花了一个小时与这个东西互动。毕竟，[OpenAI声称通过一些‘提示引导’可以承认错误](https://openai.com/blog/chatgpt/)。

完全浪费时间。

![](../Images/979d262ce4da3880108f8e38d4aff04f.png)

承认并尝试提供有关其获取证据的方式的信息。

一段时间后，它在我限制它、要求它承认不知道答案后，切换到了新的模式。

![](../Images/886f517192ef187babff9282dc6150dd.png)

AI坚持认为它是对的……所以我想我来自新泽西。

# **大型语言模型是不可靠的信息存储库。我们该如何解决这个问题？**

**从设计上讲，这些系统不知道它们知道或不知道什么。**

在我那个悲惨的例子中，我已经死了，而来自新泽西的我，嗯，我还活着。很难准确知道ChatGPT为什么会这样认为，也很复杂理解其原因。可能是因为我在创办初创公司期间被归入了一个大型技术CEO的类别，他们创建了一个健身初创公司，其中一个人在那段时间去世了。它混淆了主语和谓语之间的关系，以为我已经去世。

GPT是在大量文本数据上进行训练的，没有固有的能力来验证所提供信息的准确性或真实性。

过度依赖大型语言模型在搜索应用中的表现，例如Bing，或作为搜索的替代品，例如OpenAI的ChatGPT，将会导致负面和意外的伤害。

更直白地说，在其当前状态下，ChatGPT **不应**被视为搜索的进化。

## 那么我们是否应该在事实不可靠的GPT上进行构建？

是的。不过，当我们这样做时，我们必须确保添加适当的信任和安全检查，并通过我下面将要分享的技术进行实际约束。在这些基础模型之上构建时，我们可以通过适当的保护措施，使用如提示工程和上下文注入等技术来最小化不准确性。

或者，如果我们拥有更大的数据集，则可以考虑更先进的方法，如迁移学习、微调和强化学习。

## 迁移学习（特别是微调）是一种提高模型在特定领域准确性的技术，但它仍然存在不足之处。

让我们讨论迁移学习或微调，这是复制大型语言模型的一种技术。虽然这些技术可以提高模型在特定领域的准确性，但它们并不一定解决AI幻觉的问题。这意味着，即使模型基于新的数据领域正确地获取了一些信息，由于大型语言模型的架构问题，它仍可能生成不准确或虚假的信息。

大型语言模型缺乏演绎推理或认知架构，这使它们在认识到自己知道什么和已知的未知领域时存在认识上的盲点。毕竟，生成预训练变换器（即大型语言模型）是[极其复杂的文本预测引擎](https://writings.stephenwolfram.com/2023/01/wolframalpha-as-the-way-to-bring-computational-knowledge-superpowers-to-chatgpt/)，没有办法识别导致其生成事实或幻觉的模式。

微软计划将调整后的GPT集成到Bing中，这在2023年虚假信息、阴谋论和深度伪造成为常态的情况下是一个有问题且糟糕的策略。今天，最终用户需要带有来源和归属的事实以避免混乱。微软应该更明智。

然后是谷歌。我理解为什么谷歌将LaMDA的大型语言模型保密，并只在内部用于搜索和其他服务。不幸的是，他们看到Bing Chat后感到恐慌。谷歌发明了大多数这项技术；他们知道其危险性。谷歌应该更明智。

为了使大型语言模型成为搜索的一部分，我们需要了解这些大型语言模型生成的响应的来源和传承。

这样，我们可以：

+   提供来源的归属，

+   为AI生成的每个响应提供一个置信度等级，或者

目前，我们还没有达到这个目标，但我希望这些创新能尽快出现。

在这项研究中，我展示了如何使用[OpenAI文本补全模型端点](https://platform.openai.com/docs/guides/completion/prompt-design)提高事实准确性并防止幻觉。

![](../Images/d3b1c4384eca32b8f52cf774781e2243.png)

使用GPT3补全模型端点检查事实准确性

在类似的例子中，我问了GPT3模型，“谁赢得了2020年奥运会100米短跑？”

它回应道，***“2020年奥运会100米短跑由牙买加的谢利-安·弗雷泽-普赖斯赢得。”***

![](../Images/29050abde8904e004d12216c09b6c90f.png)

一个突显事实准确性的例子（即幻觉或编造）。提示：“谁赢得了2020年奥运会100米短跑？”

听起来很真实，但实际上更复杂，因为2020年奥运会由于疫情推迟了一年。对于大型语言模型的开发者来说，采取措施减少AI幻觉的可能性至关重要。对于最终用户来说，带着批判性思维使用AI结果，避免过度依赖AI的结果是必要的。

那么，作为开发者，有哪些方法可以减少AI编造事实的可能性，鉴于大型语言模型的缺陷？一种门槛较低的方法是提示工程。提示工程涉及构造提示和添加提示约束，以引导模型生成准确的响应。

## 提示工程

![](../Images/a605343c914ee3c95df38a0004976b66.png)

[提示工程技巧](https://platform.openai.com/docs/guides/completion/prompt-design)，使用提示约束来展示 API 如何承认它不知道某个事实。

## 或者，你可以通过上下文注入将其提供给你关心的领域的特定上下文。

![](../Images/1531e15e8da7f72a26189c01253b1dc6.png)

通过限制模型领域特定的提示来控制幻觉。“老鹰队在超级碗中击败爱国者队多少次？”模型正确回答为一次。

上下文摄取方法更快且更便宜，但需要领域知识和专业技能才能有效。这种方法在生成文本的准确性和相关性至关重要的领域特别有用。你应该在企业环境中看到这种方法，例如在客户服务或医疗诊断中。

另一种方法是使用嵌入（例如，用于向量或语义搜索），这涉及使用 OpenAI 嵌入模型端点来搜索已知为真的相关概念和术语。这种方法更贵，但也更可靠和准确。

![](../Images/03dd1eb6e3a7a26f37871e3a780971a3.png)

*人类可读文本与向量化文本对。*

AI 幻觉是大型语言模型中一个真实且潜在危险的问题。微调不一定能解决这个问题；然而，嵌入方法则是通过[余弦相似度](https://github.com/openai/openai-python/blob/ede0882939656ce4289cb4f61142e7658bb2dec7/openai/embeddings_utils.py)或同等方法，将用户的查询与向量数据库中最接近、最可能的真实信息进行匹配。

![](../Images/e9025cdebd7dabe452834c974a9a9fb1.png)

AI 构造了关于我的额外不正确的事实。这些都不是真的。

## 总结：跟上创新的步伐而不破坏事物。

让我们从过去中学习。为了确保事实准确性，必须意识到在 OpenAI 创新的规模下，不小心传播虚假信息的影响。开发人员应减少在事实准确的信息背景下呈现不正确信息的产品失败可能性，例如通过提示工程或向量搜索。这样，我们可以帮助确保大型语言模型提供的信息是准确和可靠的。

我欣赏 OpenAI 将这些工具交到人们手中，以在各个行业或领域中进行受控的早期反馈，但也有其局限性。

[不适用于他们的规模](https://www.wsj.com/articles/microsoft-says-it-plans-multibillion-dollar-investment-in-openai-11674483180)。

我**不**欣赏“快速行动”即使解决方案仍然是“有点破损”的态度。

**强烈不同意。**

在这种规模下，不要“快速行动并破坏事物”。

这种理念应该被从轨道上彻底摧毁，特别是在像OpenAI这样的大型初创公司控制的非确定性变革技术面前。萨姆·奥特曼应该知道更好。

对于在这个领域进行创新的初创公司来说。你们很多，听我说。

当虚假信息导致代表性伤害时，风险太高了，[**可能会面临巨额罚款**](https://www.ftc.gov/news-events/news/press-releases/2019/07/ftc-imposes-5-billion-penalty-sweeping-new-privacy-restrictions-facebook)；你不想失去客户的信任，或者更糟的是，让你的初创公司倒闭。

对于像微软这样的大公司来说，目前的风险可能很低，或者至少直到有人受到伤害，或者政府被接管为止。混合模式也是一种杂乱和令人困惑的体验。这项决定将导致产品失败的比例过高，一旦炒作退去，Bing 的采用率将会下降。这不是你提升8% Bing搜索市场份额的方式。

希望你喜欢这篇文章。希望我已经证明我非常活跃。这是关于**负责任使用人工智能**的一系列文章中的一部分。有关算法偏见和公平性的我的想法，请参见之前的文章。

[## 缓解人工智能偏见，...偏见](https://becominghuman.ai/mitigating-ai-bias-with-bias-6d237452258e?source=post_page-----db5a36c22f11--------------------------------)

### 这篇文章是我数据信任系列演讲的一部分。这些文章的目的是解构复杂但...

[成为人类](https://becominghuman.ai/mitigating-ai-bias-with-bias-6d237452258e?source=post_page-----db5a36c22f11--------------------------------)

*想要更多这样的内容？阅读更多信息并关注*，[*Twitter*](https://www.twitter.com/nobleackerson)*，观看我过去的演讲视频在* [*Youtube*](https://www.youtube.com/@stigsfoot/videos)*，或者在* [*LinkedIn*](https://linkedin.com/in/noblea)*与我联系。
