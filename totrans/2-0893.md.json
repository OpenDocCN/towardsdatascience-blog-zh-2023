["```py\nimport tweepy\n\napi_key = \"YjKdgxk...\"\napi_key_secret = \"Qa6ZnPs0vdp4X....\"\n\nauth = tweepy.OAuth2AppHandler(api_key, api_key_secret)\napi = tweepy.API(auth, wait_on_rate_limit=True)\n\nhashtag = \"#climate\"\nlanguage = \"en\"\n\ndef text_filter(s_data: str) -> str:\n    \"\"\" Remove extra characters from text \"\"\"\n    return s_data.replace(\"&amp;\", \"and\").replace(\";\", \" \").replace(\",\", \" \") \\\n                 .replace('\"', \" \").replace(\"\\n\", \" \").replace(\"  \", \" \")\n\ndef get_hashtags(tweet) -> str:\n    \"\"\" Parse retweeted data \"\"\"\n    hash_tags = \"\"\n    if 'hashtags' in tweet.entities:\n        hash_tags = ','.join(map(lambda x: x[\"text\"], tweet.entities['hashtags']))\n    return hash_tags\n\ndef get_csv_header() -> str:\n    \"\"\" CSV header \"\"\"\n    return \"id;created_at;user_name;user_location;user_followers_count;user_friends_count;retweets_count;favorites_count;retweet_orig_id;retweet_orig_user;hash_tags;full_text\"\n\ndef tweet_to_csv(tweet):\n    \"\"\" Convert a tweet data to the CSV string \"\"\"\n    if not hasattr(tweet, 'retweeted_status'):\n        full_text = text_filter(tweet.full_text)\n        hasgtags = get_hashtags(tweet)\n        retweet_orig_id = \"\"\n        retweet_orig_user = \"\"\n        favs, retweets = tweet.favorite_count, tweet.retweet_count\n    else:\n        retweet = tweet.retweeted_status\n        retweet_orig_id = retweet.id\n        retweet_orig_user = retweet.user.screen_name\n        full_text = text_filter(retweet.full_text)\n        hasgtags = get_hashtags(retweet)\n        favs, retweets = retweet.favorite_count, retweet.retweet_count\n    s_out = f\"{tweet.id};{tweet.created_at};{tweet.user.screen_name};{addr_filter(tweet.user.location)};{tweet.user.followers_count};{tweet.user.friends_count};{retweets};{favs};{retweet_orig_id};{retweet_orig_user};{hasgtags};{full_text}\"\n    return s_out\n\nif __name__ == \"__main__\":\n    pages = tweepy.Cursor(api.search_tweets, q=hashtag, tweet_mode='extended',\n                         result_type=\"recent\",\n                         count=100,\n                         lang=language).pages(limit)\n\n    with open(\"tweets.csv\", \"a\", encoding=\"utf-8\") as f_log:\n        f_log.write(get_csv_header() + \"\\n\")\n        for ind, page in enumerate(pages):\n            for tweet in page:\n                # Get data per tweet\n                str_line = tweet_to_csv(tweet)\n                # Save to CSV\n                f_log.write(str_line + \"\\n\")\n```", "```py\nimport pandas as pd\n\ndf = pd.read_csv(\"climate.csv\", sep=';', dtype={'id': object, 'retweet_orig_id': object, 'full_text': str, 'hash_tags': str}, parse_dates=[\"created_at\"], lineterminator='\\n')\ndf.drop([\"retweet_orig_id\", \"user_friends_count\", \"retweets_count\", \"favorites_count\", \"user_location\", \"hash_tags\", \"retweet_orig_user\", \"user_followers_count\"], inplace=True, axis=1)\ndf = df.drop_duplicates('id')\nwith pd.option_context('display.max_colwidth', 80):\n    display(df)\n```", "```py\ndef get_time(dt: datetime.datetime):\n    \"\"\" Get time and minute from datetime \"\"\"\n    return dt.time()\n\ndef get_date(dt: datetime.datetime):\n    \"\"\" Get date from datetime \"\"\"\n    return dt.date()\n\ndef get_hour(dt: datetime.datetime):\n    \"\"\" Get time and minute from datetime \"\"\"\n    return dt.hour\n\ndf[\"date\"] = df['created_at'].map(get_date)\ndf[\"time\"] = df['created_at'].map(get_time)\ndf[\"hour\"] = df['created_at'].map(get_hour)\n```", "```py\ndisplay(df[[\"user_name\", \"date\", \"time\", \"hour\"]])\n```", "```py\ndays_total = df['date'].unique().shape[0]\nprint(days_total)\n# > 46\n\nusers_total = df['user_name'].unique().shape[0]\nprint(users_total)\n# > 79985\n```", "```py\ngr_messages_per_user = df.groupby(['user_name'], as_index=False).size().sort_values(by=['size'], ascending=False)\ngr_messages_per_user[\"size_per_day\"] = gr_messages_per_user['size'].div(days_total)\n```", "```py\nimport numpy as np\nfrom bokeh.io import show, output_notebook, export_png\nfrom bokeh.plotting import figure, output_file\nfrom bokeh.models import ColumnDataSource, LabelSet, Whisker\nfrom bokeh.transform import factor_cmap, factor_mark, cumsum\nfrom bokeh.palettes import *\noutput_notebook()\n\nusers = gr_messages_per_user['user_name']\namount = gr_messages_per_user['size_per_day']\nhist_e, edges_e = np.histogram(amount, density=False, bins=100)\n\n# Draw\np = figure(width=1600, height=500, title=\"Messages per day distribution\")\np.quad(top=hist_e, bottom=0, left=edges_e[:-1], right=edges_e[1:], line_color=\"darkblue\")\np.x_range.start = 0\n# p.x_range.end = 150000\np.y_range.start = 0\np.xaxis[0].ticker.desired_num_ticks = 20\np.left[0].formatter.use_scientific = False\np.below[0].formatter.use_scientific = False\np.xaxis.axis_label = \"Messages per day, avg\"\np.yaxis.axis_label = \"Amount of users\"\nshow(p)\n```", "```py\ndef get_active_users_percent(df_in: pd.DataFrame, messages_per_day_threshold: int):\n    \"\"\" Get percentage of active users with a messages-per-day threshold \"\"\"\n    days_total = df_in['date'].unique().shape[0]\n    users_total = df_in['user_name'].unique().shape[0]\n    gr_messages_per_user = df_in.groupby(['user_name'], as_index=False).size()\n    gr_messages_per_user[\"size_per_day\"] = gr_messages_per_user['size'].div(days_total)\n    users_active = gr_messages_per_user[gr_messages_per_user['size_per_day'] >= messages_per_day_threshold].shape[0]\n    return 100*users_active/users_total\n```", "```py\nlabels = ['#Climate', '#Politics', '#Cats', '#Humour', '#Space', '#War']\ncounts = [get_active_users_percent(df_climate, messages_per_day_threshold=1), \n          get_active_users_percent(df_politics, messages_per_day_threshold=1), \n          get_active_users_percent(df_cats, messages_per_day_threshold=1), \n          get_active_users_percent(df_humour, messages_per_day_threshold=1), \n          get_active_users_percent(df_space, messages_per_day_threshold=1), \n          get_active_users_percent(df_war, messages_per_day_threshold=1)]\n\npalette = Spectral6\nsource = ColumnDataSource(data=dict(labels=labels, counts=counts, color=palette))\np = figure(width=1200, height=400, x_range=labels, y_range=(0,9), \n           title=\"Percentage of Twitter users posting 1 or more messages per day\",\n           toolbar_location=None, tools=\"\")\np.vbar(x='labels', top='counts', width=0.9, color='color', source=source)\np.xgrid.grid_line_color = None\np.y_range.start = 0\nshow(p)\n```", "```py\ndef get_cumulative_percents_distribution(df_in: pd.DataFrame, steps=200):\n    \"\"\" Get a distribution of total percent of messages sent by percent of users \"\"\"    \n    # Group dataframe by user name and sort by amount of messages\n    df_messages_per_user = df_in.groupby(['user_name'], as_index=False).size().sort_values(by=['size'], ascending=False)\n    users_total = df_messages_per_user.shape[0]\n    messages_total = df_messages_per_user[\"size\"].sum()\n\n    # Get cumulative messages/users ratio\n    messages = []\n    percentage = np.arange(0, 100, 0.05)\n    for perc in percentage:\n        msg_count = df_messages_per_user[:int(perc*users_total/100)][\"size\"].sum()\n        messages.append(100*msg_count/messages_total)\n\n    return percentage, messages\n```", "```py\n# Calculate \npercentage, messages1 = get_cumulative_percent(df_climate)\n_, messages2 = get_cumulative_percent(df_politics)\n_, messages3 = get_cumulative_percent(df_cats)\n_, messages4 = get_cumulative_percent(df_humour)\n_, messages5 = get_cumulative_percent(df_space)\n_, messages6 = get_cumulative_percent(df_war)\n\nlabels = ['#Climate', '#Politics', '#Cats', '#Humour', '#Space', '#War']\nmessages = [messages1, messages2, messages3, messages4, messages5, messages6]\n\n# Draw\npalette = Spectral6\np = figure(width=1200, height=400, \n           title=\"Twitter messages per user percentage ratio\", \n           x_axis_label='Percentage of users', \n           y_axis_label='Percentage of messages')\nfor ind in range(6): \n    p.line(percentage, messages[ind], line_width=2, color=palette[ind], legend_label=labels[ind])\np.x_range.end = 100\np.y_range.start = 0\np.y_range.end = 100\np.xaxis.ticker.desired_num_ticks = 10\np.legend.location = 'bottom_right'\np.toolbar_location = None\nshow(p)\n```", "```py\ndef draw_user_timeline(df_in: pd.DataFrame, user_name: str):\n    \"\"\" Draw cumulative messages time for specific user \"\"\"\n    df_u = df_in[df_in[\"user_name\"] == user_name]\n    days_total = df_u['date'].unique().shape[0]\n\n    # Group messages by time of the day\n    messages_per_day = df_u.groupby(['time'], as_index=False).size()\n    msg_time = messages_per_day['time']\n    msg_count = messages_per_day['size']\n\n    # Draw\n    p = figure(x_axis_type='datetime', width=1600, height=150, title=f\"Cumulative tweets timeline during {days_total} days: {user_name}\")\n    p.vbar(x=msg_time, top=msg_count, width=datetime.timedelta(seconds=30), line_color='black')\n    p.xaxis[0].ticker.desired_num_ticks = 30\n    p.xgrid.grid_line_color = None\n    p.toolbar_location = None\n    p.x_range.start = datetime.time(0,0,0)\n    p.x_range.end = datetime.time(23,59,0)\n    p.y_range.start = 0\n    p.y_range.end = 1\n    show(p)\n\ndraw_user_timeline(df, user_name=\"UserNameHere\")\n...\n```", "```py\ndef get_night_offset(hours: List):\n    \"\"\" Estimate the night position by calculating the rolling average minimum \"\"\"\n    night_len = 9\n    min_pos, min_avg = 0, 99999 \n    # Find the minimum position\n    data = np.array(hours + hours)\n    for p in range(24):\n        avg = np.average(data[p:p + night_len])\n        if avg <= min_avg:\n            min_avg = avg\n            min_pos = p\n\n    # Move the position right if possible (in case of long sequence of similar numbers)\n    for p in range(min_pos, len(data) - night_len):\n        avg = np.average(data[p:p + night_len])\n        if avg <= min_avg:\n            min_avg = avg\n            min_pos = p\n        else:\n            break\n\n    return min_pos % 24\n\ndef normalize(hours: List):\n    \"\"\" Move the hours array to the right, keeping the 'night' time at the left \"\"\"\n    offset = get_night_offset(hours)\n    data = hours + hours\n    return data[offset:offset+24]\n```", "```py\ndef get_vectorized_users(df_in: pd.DataFrame):\n    \"\"\" Get embedding vectors for all users \n        Embedding format: [total hours, total messages per hour-00, 01, .. 23]\n    \"\"\"\n    gr_messages_per_user = df_in.groupby(['user_name', 'hour'], as_index=True).size()\n\n    vectors = []\n    users = gr_messages_per_user.index.get_level_values('user_name').unique().values    \n    for ind, user in enumerate(users):\n        if ind % 10000 == 0:\n            print(f\"Processing {ind} of {users.shape[0]}\")\n        hours_all = [0]*24\n        for hr, value in gr_messages_per_user[user].items():\n            hours_all[hr] = value\n\n        hours_norm = normalize(hours_all)\n        vectors.append([sum(hours_norm)] + hours_norm)\n\n    return users, np.asarray(vectors)\n\nall_users, vectorized_users = get_vectorized_users(df)\n```", "```py\n[120 0 0 0 0 0 0 0 0 0 1 2 0 2 2 1 0 0 0 0 0 18 44 50 0]\n```", "```py\n[4 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0]\n```", "```py\nimport matplotlib.pyplot as plt  \n%matplotlib inline\n\ndef graw_elbow_graph(x: np.array, k1: int, k2: int, k3: int):\n    k_values, inertia_values = [], []\n    for k in range(k1, k2, k3):\n        print(\"Processing:\", k)\n        km = KMeans(n_clusters=k).fit(x)\n        k_values.append(k)\n        inertia_values.append(km.inertia_)\n\n    plt.figure(figsize=(12,4))\n    plt.plot(k_values, inertia_values, 'o')\n    plt.title('Inertia for each K')\n    plt.xlabel('K')\n    plt.ylabel('Inertia')\n\ngraw_elbow_graph(vectorized_users, 2, 20, 1)\n```", "```py\ndef get_clusters_kmeans(x, k):\n    \"\"\" Get clusters using K-Means \"\"\"\n    km = KMeans(n_clusters=k).fit(x)\n    s_score = silhouette_score(x, km.labels_)\n    print(f\"K={k}: Silhouette coefficient {s_score:0.2f}, inertia:{km.inertia_}\")\n\n    sample_silhouette_values = silhouette_samples(x, km.labels_)\n    silhouette_values = []\n    for i in range(k):\n        cluster_values = sample_silhouette_values[km.labels_ == i]\n        silhouette_values.append((i, cluster_values.shape[0], cluster_values.mean(), cluster_values.min(), cluster_values.max()))\n    silhouette_values = sorted(silhouette_values, key=lambda tup: tup[2], reverse=True)\n\n    for s in silhouette_values:\n        print(f\"Cluster {s[0]}: Size:{s[1]}, avg:{s[2]:.2f}, min:{s[3]:.2f}, max: {s[4]:.2f}\")        \n    print()\n\n    # Create new dataframe\n    data_len = x.shape[0]\n    cdf = pd.DataFrame({\n        \"id\": all_users,\n        \"vector\": [str(v) for v in vectorized_users],\n        \"cluster\": km.labels_,\n    })\n\n    # Show top clusters\n    for cl in silhouette_values[:10]:\n        df_c = cdf[cdf['cluster'] == cl[0]]\n        # Show cluster\n        print(\"Cluster:\", cl[0], cl[2])\n        with pd.option_context('display.max_colwidth', None):\n            display(df_c[[\"id\", \"vector\"]][:20])\n        # Show first users\n        for user in df_c[\"id\"].values[:10]:\n            draw_user_timeline(df, user_name=user)\n        print()\n\n    return km.labels_\n\nclusters = get_clusters_kmeans(vectorized_users, k=5)\n```"]