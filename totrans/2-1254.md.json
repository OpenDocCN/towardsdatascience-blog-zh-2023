["```py\nfrom transformers import BertConfig, BertForMaskedLM\n\nconfig = BertConfig(\n    hidden_size = 384,\n    vocab_size= tokenizer.vocab_size,\n    num_hidden_layers = 6,\n    num_attention_heads = 6,\n    intermediate_size = 1024,\n    max_position_embeddings = 256\n)\n\nmodel = BertForMaskedLM(config=config)\nprint(model.num_parameters()) #10457864\n```", "```py\nfrom tokenizers.implementations import ByteLevelBPETokenizer\nfrom tokenizers.processors import BertProcessing\nfrom transformers import AutoTokenizer\nfrom datasets import Dataset\nimport pandas as pd\n\n#load base tokenizer to train on dataset\ntokenizer_base = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n# convert pandas dataset to HF dataset\ndataset = Dataset.from_pandas(df.rename(columns={\"comment\":'text'}))\n\n# define iterator\ntraining_corpus = (\n    dataset[i : i + 1000][\"text\"]\n    for i in range(0, len(dataset), 1000)\n)\n\n#train the new tokenizer for dataset\ntokenizer = tokenizer_base.train_new_from_iterator(training_corpus, 5000)\n#test trained tokenizer for sample text\ntext = dataset['text'][123]\nprint(text)\n```", "```py\n# let's check tokenization process\ninput_ids = tokenizer(text).input_ids\nsubword_view = [tokenizer.convert_ids_to_tokens(id) for id in input_ids]\nnp.array(subword_view)\n```", "```py\ntokenizer.save_pretrained(\"tokenizer/sinhala-wordpiece-yt-comments\")\n```", "```py\nfrom transformers import DataCollatorForLanguageModeling\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n)\n```", "```py\nimport torch\nfrom torch.utils.data import Dataset\nfrom accelerate import Accelerator, DistributedType\n\nclass LineByLineTextDataset(Dataset):\n    def __init__(self, tokenizer, raw_datasets, max_length: int):\n        self.padding = \"max_length\"\n        self.text_column_name = 'text'\n        self.max_length = max_length\n        self.accelerator = Accelerator(gradient_accumulation_steps=1)\n        self.tokenizer = tokenizer\n\n        with self.accelerator.main_process_first():\n            self.tokenized_datasets = raw_datasets.map(\n                self.tokenize_function,\n                batched=True,\n                num_proc=4,\n                remove_columns=[self.text_column_name],\n                desc=\"Running tokenizer on dataset line_by_line\",\n            )\n            self.tokenized_datasets.set_format('torch',columns=['input_ids'],dtype=torch.long)\n\n    def tokenize_function(self,examples):\n        examples[self.text_column_name] = [\n            line for line in examples[self.text_column_name] if len(line[0]) > 0 and not line[0].isspace()\n        ]\n        return self.tokenizer(\n            examples[self.text_column_name],\n            padding=self.padding,\n            truncation=True,\n            max_length=self.max_length,\n            return_special_tokens_mask=True,\n        )\n    def __len__(self):\n        return len(self.tokenized_datasets)\n\n    def __getitem__(self, i):\n        return self.tokenized_datasets[i]\n```", "```py\ntokenized_dataset_train = LineByLineTextDataset(\n    tokenizer= tokenizer,\n    raw_datasets = dataset,\n    max_length=256,\n)\n```", "```py\nfrom transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./model\",\n    overwrite_output_dir=True,\n    push_to_hub=True,\n    hub_model_id=\"Ransaka/sinhala-bert-yt-comments\",\n    num_train_epochs=2,\n    per_device_train_batch_size=32,\n    save_steps=5_000,\n    logging_steps = 1000,\n    save_total_limit=2,\n    use_mps_device = True, # disable this if you're running non-mac env\n    hub_private_repo = False, # please set true if you want to save model privetly\n    save_safetensors= True,\n    learning_rate = 1e-4,\n    report_to='wandb'\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=tokenized_dataset_train\n)\n```", "```py\ntrainer.train()\n```"]