- en: Custom Memory for ChatGPT API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/custom-memory-for-chatgpt-api-artificial-intelligence-python-722d627d4d6d](https://towardsdatascience.com/custom-memory-for-chatgpt-api-artificial-intelligence-python-722d627d4d6d)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Gentle Introduction to LangChain Memory Types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@andvalenzuela?source=post_page-----722d627d4d6d--------------------------------)[![Andrea
    Valenzuela](../Images/ddfc1534af92413fd91076f826cc49b6.png)](https://medium.com/@andvalenzuela?source=post_page-----722d627d4d6d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----722d627d4d6d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----722d627d4d6d--------------------------------)
    [Andrea Valenzuela](https://medium.com/@andvalenzuela?source=post_page-----722d627d4d6d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----722d627d4d6d--------------------------------)
    ·8 min read·Aug 20, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a89558351cd6f8d41f0994f05d4caebe.png)'
  prefs: []
  type: TYPE_IMG
- en: Self-made gif.
  prefs: []
  type: TYPE_NORMAL
- en: If you have ever used the OpenAI API, I am sure you have noticed the catch.
  prefs: []
  type: TYPE_NORMAL
- en: '*Got it?*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Right!* Every time you call the ChatGPT API, the model has no memory of the
    previous requests you have made. In other words: **each API call is a standalone
    interaction**.'
  prefs: []
  type: TYPE_NORMAL
- en: And that is definitely annoying when you need to perform follow-up interactions
    with the model. A chatbot is the golden example where follow-up interactions are
    needed.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will explore how to give memory to ChatGPT when using the
    OpenAI API, so that it remembers our previous interactions.
  prefs: []
  type: TYPE_NORMAL
- en: Warm-Up!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s perform some interactions with the model so that we experience this default
    no-memory phenomenon:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'But when asked a follow-up question:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '*Right,* so in fact the model does not remember my name even though it was
    given on the first interaction.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** The method `chatgpt_call()` is just a wrapper around the OpenAI API.
    We already gave a shot on how easily call GPT models at [ChatGPT API Calls: A
    Gentle Introduction](https://medium.com/forcodesake/chatgpt-api-calls-introduction-chatgpt3-chatgpt4-ai-d19b79c49cc5)
    in case you want to check it out!'
  prefs: []
  type: TYPE_NORMAL
- en: Some people normally work around this memoryless situation by pre-feeding the
    previous conversation history to the model every time they do a new API call.
    Nevertheless, this practice is not cost-optimized and it has certainly a limit
    for long conversations.
  prefs: []
  type: TYPE_NORMAL
- en: In order to create a memory for ChatGPT so that it is aware of the previous
    interactions, we will be using the popular `langchain` framework. This framework
    allows you to easily manage the ChatGPT conversation history and optimize it by
    choosing the right memory type for your application.
  prefs: []
  type: TYPE_NORMAL
- en: '**LangChain Framework**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `langchain` framework’s **purpose is to assist developers when building
    applications powered by Large Language Models (LLMs).**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2db618c369cae1c8a41589c9bf47b91d.png)'
  prefs: []
  type: TYPE_IMG
- en: Self-made screenshot from the official LangChain [GitHub repository](https://github.com/hwchase17/langchain).
  prefs: []
  type: TYPE_NORMAL
- en: 'According to their [GitHub description](https://github.com/hwchase17/langchain):'
  prefs: []
  type: TYPE_NORMAL
- en: Large language models (LLMs) are emerging as a transformative technology, enabling
    developers to build applications that they previously could not. However, using
    these LLMs in isolation is often insufficient for creating a truly powerful app
    — the real power comes when you can combine them with other sources of computation
    or knowledge.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This library aims to assist in the development of those types of applications.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: They claim that building an application by only using LLMs may be insufficient.
    We found that too when doing follow-up interactions with the model by using the
    OpenAI API only.
  prefs: []
  type: TYPE_NORMAL
- en: Framework Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Getting the `langchain` library up and running in Python is simple. As for
    any other Python library, we can install it with `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: LangChain calls the OpenAI API behind the scenes. Therefore, it is necessary
    to set your OpenAI API key as an environment variable called `OPENAI_API_KEY`.
    Check out [A Step-by-Step Guide to Getting Your API Key](https://medium.com/forcodesake/a-step-by-step-guide-to-getting-your-api-key-2f6ee1d3e197)
    if you need some guidance for getting your OpenAI key.
  prefs: []
  type: TYPE_NORMAL
- en: 'LangChain: Basic Calls'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s start by setting up a basic API call to ChatGPT using LangChain.
  prefs: []
  type: TYPE_NORMAL
- en: 'This task is pretty straightforward since the module `langchain.llms` already
    provides an `OpenAI()` method for this purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the desired model is loaded, we need to start the so-called *conversation
    chain*. LangChain also provides a module for that purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Let’s define the conversation with `verbose=True` to observe the reasoning process
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, `langchain` provides a `.predict()` method to send your desired prompt
    to ChatGPT and get its completion back. *Let’s try it!*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '*Let’s do a follow-up interaction!*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We can see that **the model is capable of handling follow-up interactions without
    problems when using** `**langchain**`.
  prefs: []
  type: TYPE_NORMAL
- en: LangChain Memory Types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have observed, LangChain conversation chains already keep track of the
    `.predict` calls for a declared `conversation`. However, **the default conversation
    chain stores each and every interaction we have had with the model**.
  prefs: []
  type: TYPE_NORMAL
- en: As we have briefly discussed at the beginning of the article, storing all the
    interactions with the model can **quickly escalate to a considerable amount of
    tokens to process every time we prompt the model**. It is essential to bear in
    mind that ChatGPT has a token limit per interaction.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the **ChatGPT usage cost also depends on the number of tokens**.
    Processing all the conversation history in each new interaction is likely to be
    expensive over time.
  prefs: []
  type: TYPE_NORMAL
- en: To overcome these limitations, `langchain` implements different types of memories
    to use in your application.
  prefs: []
  type: TYPE_NORMAL
- en: '*Let’s explore them!*'
  prefs: []
  type: TYPE_NORMAL
- en: '#1\. Complete Interactions'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Although the default behavior of LangChain is to store all the past interactions,
    this memory type can be explicitly declared. It is the so-called `ConversationBufferMemory`,
    and it simply fills a buffer with all our previous interactions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Declaring the memory type allows us to have some additional control over the
    ChatGPT memory. For example, we can check the buffer content at any time with
    `memory.buffer` or `memory.load_memory_variables({})`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, we can add extra information to the buffer without doing a real
    interaction with the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: If you do not need to manipulate the buffer in your application, you might be
    good to go with the default memory and no explicit declaration. **Although I really
    recommend it for debugging purposes!**
  prefs: []
  type: TYPE_NORMAL
- en: '#2\. Interactions within a window'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One less costly alternative is storing only a certain amount of previous interactions
    (`k`) with the model. That is the so-called *window* of interaction.
  prefs: []
  type: TYPE_NORMAL
- en: When conversations grow big enough, it might be sufficient for your application
    that the model only remembers the most recent interactions. For those cases, the
    `ConversationBufferWindowMemory` module is available.
  prefs: []
  type: TYPE_NORMAL
- en: '*Let’s explore its behavior!*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, we need to load the `llm` model and the new type of `memory`. In this
    case, we are setting `k=1` which means that only the previous iteration will be
    kept in memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Secondly, let’s add some context to our conversation as shown in the previous
    section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Although we have stored two interactions in our conversation history, due to
    the fact that we have set `k=1`, the model will only remember the last interaction
    `{“input”: “Not much, just hanging”}, {“output”: “Cool”}`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To prove it, let’s check what the model has in memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can further prove it by asking a follow-up question setting `verbose=True`,
    so that we can observe the stored interactions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'And the verbose output is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: As we can observe, **the model only remembers the previous interaction**.
  prefs: []
  type: TYPE_NORMAL
- en: '#3\. Summary of the interactions'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I am sure you are now thinking that **completely deleting old interactions with
    the model might be a bit risky for some applications**.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s imagine a customer service chatbot that asks to the user its contract
    number in the first place. The model must not forget this information, no matter
    which interaction number it has.
  prefs: []
  type: TYPE_NORMAL
- en: For that purpose, there is a memory type that uses the model itself to generate
    a summary of the previous interactions. Therefore, the model only stores a summary
    of the conversation in memory.
  prefs: []
  type: TYPE_NORMAL
- en: This optimized memory type is the so-called `ConversationSummaryBufferMemory`.
    It also allows to store the complete most recent interactions up to a maximum
    number of tokens (given by `max_token_limit`) together with the summary of the
    previous ones.
  prefs: []
  type: TYPE_NORMAL
- en: '*Let’s observe this memory behavior in practice!*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s create a conversation with quite some content so that we can explore
    the summary capabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, when checking the memory content with `memory.load_memory_variables({})`
    , we will see the actual summary of our interactions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '*The summary sounds nice, isn’t it?*'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s perform a new interaction!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'And the verbose output looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: As we can observe from the example, **this memory type allows the model to keep
    important information, while reducing the irrelevant information** and, therefore,
    the amount of used tokens in each new interaction.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we have seen **different ways to create a memory for our GPT-powered
    application** depending on our needs.
  prefs: []
  type: TYPE_NORMAL
- en: By **using the LangChain framework instead of bare API calls to the OpenAI API**,
    we get rid of simple problems such as making the model aware of the previous interactions.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the fact that the default memory type of LangChain might be already
    enough for your application, I really encourage you to estimate the average length
    of your conversations. It is a nice exercise to compare the average number of
    tokes used — and therefore the cost! — with the usage of the summary memory. **You
    can get full model performance at a minimal cost!**
  prefs: []
  type: TYPE_NORMAL
- en: It seems to me that the LangChain framework has a lot to give us regarding GPT
    models. *Have you already discovered another handy functionality?*
  prefs: []
  type: TYPE_NORMAL
- en: That is all! Many thanks for reading!
  prefs: []
  type: TYPE_NORMAL
- en: I hope this article helps you when **building ChatGPT applications!**
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also subscribe to my [**Newsletter**](https://medium.com/@andvalenzuela/subscribe)
    to stay tuned for new content. **Especially**, **if you are interested in articles
    about ChatGPT**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/chatgpt-moderation-api-input-output-artificial-intelligence-chatgpt3-data-4754389ec9c8?source=post_page-----722d627d4d6d--------------------------------)
    [## ChatGPT Moderation API: Input/Output Control'
  prefs: []
  type: TYPE_NORMAL
- en: Using the OpenAI’s Moderation Endpoint for Responsible AI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/chatgpt-moderation-api-input-output-artificial-intelligence-chatgpt3-data-4754389ec9c8?source=post_page-----722d627d4d6d--------------------------------)
    [](/chatgpt-tokenizer-chatgpt3-chatgpt4-artificial-intelligence-python-ai-27f78906ea54?source=post_page-----722d627d4d6d--------------------------------)
    [## Unleashing the ChatGPT Tokenizer
  prefs: []
  type: TYPE_NORMAL
- en: Hands-On! How ChatGPT Manages Tokens?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/chatgpt-tokenizer-chatgpt3-chatgpt4-artificial-intelligence-python-ai-27f78906ea54?source=post_page-----722d627d4d6d--------------------------------)
    [](/chatgpt-summarization-llms-chatgpt3-chatgpt4-artificial-intelligence-16cf0e3625ce?source=post_page-----722d627d4d6d--------------------------------)
    [## Mastering ChatGPT: Effective Summarization with LLMs'
  prefs: []
  type: TYPE_NORMAL
- en: How to Prompt ChatGPT to get High-Quality Summaries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/chatgpt-summarization-llms-chatgpt3-chatgpt4-artificial-intelligence-16cf0e3625ce?source=post_page-----722d627d4d6d--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Also towards a **responsible AI**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/what-chatgpt-knows-about-you-openai-towards-data-privacy-science-ai-b0fa2376a5f6?source=post_page-----722d627d4d6d--------------------------------)
    [## What ChatGPT Knows about You: OpenAI’s Journey Towards Data Privacy'
  prefs: []
  type: TYPE_NORMAL
- en: New ways to manage personal data in ChatGPT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/what-chatgpt-knows-about-you-openai-towards-data-privacy-science-ai-b0fa2376a5f6?source=post_page-----722d627d4d6d--------------------------------)
  prefs: []
  type: TYPE_NORMAL
