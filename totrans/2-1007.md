# GPTQ 或 bitsandbytes：对于LLMs 应使用哪种量化方法 — 以 Llama 2 为例

> 原文：[https://towardsdatascience.com/gptq-or-bitsandbytes-which-quantization-method-to-use-for-llms-examples-with-llama-2-f79bc03046dc](https://towardsdatascience.com/gptq-or-bitsandbytes-which-quantization-method-to-use-for-llms-examples-with-llama-2-f79bc03046dc)

## 针对个人计算机上可承受的微调和推断进行大型语言模型量化

[](https://medium.com/@bnjmn_marie?source=post_page-----f79bc03046dc--------------------------------)[![Benjamin Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----f79bc03046dc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f79bc03046dc--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f79bc03046dc--------------------------------) [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----f79bc03046dc--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f79bc03046dc--------------------------------) ·7分钟阅读·2023年8月25日

--

![](../Images/ff5993f4f2ee78297a0c1cd107099ea9.png)

作者提供的图片 — 制作自[Pixabay](https://pixabay.com/vectors/llama-alpaca-animal-mammal-zoo-297668/)

随着大型语言模型（LLM）变得越来越大，参数也越来越多，新的技术也被提出以减少它们的内存使用。

减少内存中模型大小的最有效方法之一是**量化**。你可以将量化视为LLM的压缩技术。在实践中，量化的主要目标是降低LLM权重的精度，通常从16位降低到8位、4位，甚至3位，同时尽量减少性能下降。

有两种流行的LLM量化方法：GPTQ和bitsandbytes。

在这篇文章中，我讨论了这两种方法之间的主要差异。它们各有优缺点，使它们适用于不同的使用场景。我展示了它们在使用 Llama 2 时的内存使用情况和推断速度的比较。我还根据以往的实验讨论了它们的性能。

*注意：如果你想了解更多关于量化的内容，我推荐阅读* [*Maxime Labonne*](https://medium.com/u/dc89da634938?source=post_page-----f79bc03046dc--------------------------------)*的这篇出色的介绍：*

[](/introduction-to-weight-quantization-2494701b9c0c?source=post_page-----f79bc03046dc--------------------------------) [## 权重量化介绍

### 使用8位量化减少大型语言模型的大小

[原文链接](https://towardsdatascience.com/introduction-to-weight-quantization-2494701b9c0c?source=post_page-----f79bc03046dc--------------------------------)

# GPTQ：用于轻量存储和快速推理的后训练量化

[GPTQ (Frantar et al., 2023)](https://arxiv.org/abs/2210.17323)首次应用于准备部署的模型。换句话说，一旦模型完全微调，GPTQ将被应用于减少其大小。

GPTQ可以将权重精度降低到4-bit或3-bit。实际上，GPTQ主要用于4-bit量化。3-bit显示出非常不稳定（[Dettmers和Zettlemoyer, 2023](https://arxiv.org/abs/2212.09720)）。

它在量化时不需要将整个模型加载到内存中。相反，GPTQ逐模块加载和量化LLM。量化还需要少量数据进行校准，这可能需要超过一小时的消费者GPU时间。

在上一篇文章中，我进行了实验，使用GPTQ对Llama 2 7B进行4-bit精度量化。原始模型在硬盘上的大小为13.5 GB，但经过量化后，模型大小减少到3.9 GB（原始大小的28.9%）。

[](https://kaitchup.substack.com/p/quantization-of-llama-2-with-gtpq?source=post_page-----f79bc03046dc--------------------------------) [## 使用GTPQ对Llama 2进行量化，以便在您的计算机上快速推理

### Llama 2但减少了75%的大小

kaitchup.substack.com](https://kaitchup.substack.com/p/quantization-of-llama-2-with-gtpq?source=post_page-----f79bc03046dc--------------------------------)

一旦量化，模型可以在更小的GPU上运行。例如，原始的Llama 2 7B无法在12 GB VRAM的GPU上运行（这大约是您在免费Google Colab实例上得到的），但量化后就能轻松运行。不仅能运行，而且还能显著减少VRAM的使用量，允许更大批量的推理。

bitsandbytes的量化也针对推理进行了优化，正如我将在本文的下一部分展示的那样。

# bitsandbytes：即时量化，便于简单的微调和高效推理

bitsandbytes也支持量化，但使用不同类型的4-bit精度，即[NormalFloat (NF)](https://huggingface.co/blog/4bit-transformers-bitsandbytes)。

它与QLoRa同时提出，用于通过适配器对量化的LLM进行微调，当时GPTQ LLMs尚不支持这种方法（自2023年6月以来已可实现）。QLoRa中4-bit NF（nf4）的便捷集成是bitsandbytes相对于GPTQ的主要优势。

我在上一篇文章中展示了如何使用nf4对Llama 2 7B进行微调。

[](https://kaitchup.substack.com/p/fine-tune-llama-2-on-your-computer?source=post_page-----f79bc03046dc--------------------------------) [## 使用QLoRa和TRL在您的计算机上微调Llama 2

### 在Guanaco上，并使用正确的填充

kaitchup.substack.com](https://kaitchup.substack.com/p/fine-tune-llama-2-on-your-computer?source=post_page-----f79bc03046dc--------------------------------)

使用bitsandbytes时，模型在加载时会被透明地实时量化。如果你使用Hugging Face transformers，只需在调用“from_pretrained”方法时将“load_in_4bit”设置为“True”即可。

与GPTQ的一个显著区别是，在撰写本文时，bitsandbytes无法序列化nf4量化模型。模型必须在每次加载时进行量化。

# QLoRa和GPTQ的软件和硬件要求

## 硬件

NVIDIA在2018年为其Turing GPU引入了[4位精度支持](https://developer.nvidia.com/blog/int4-for-ai-inference/)。免费的Google Colab实例运行在基于Turing架构的T4 GPU上。我确认你可以在Google Colab上运行4位量化。

关于消费级GPU，我只能确定RTX 30xx系列GPU（我在我的RTX 3060上进行了测试）或更近期的型号支持这一点。理论上，它也应该适用于GTX 16xx和RTX 20xx系列，因为它们也利用了Turing架构，但我没有进行测试，也没有找到GPTQ或bitsandbytes nf4在这些GPU上工作的证据。*注意：如果你知道这样的工作，请在评论中留下链接，我会更新这一段。*

## 软件

[GPTQ的官方仓库在GitHub上](https://github.com/IST-DASLab/gptq)（Apache 2.0许可）。它可以直接用于量化OPT、BLOOM或LLaMa，支持4位和3位精度。然而，你会发现大多数在线可用的量化LLM，例如在Hugging Face Hub上，都是用[AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)（Apache 2.0许可）进行量化的。

AutoGPTQ用户友好，使用Hugging Face transformers中的接口。它也有很好的文档和维护。

要安装AutoGPTQ，我建议按照GitHub上提供的说明进行操作：

[PRE0]

[bitsandbytes库（MIT许可）由其仓库定义](https://github.com/TimDettmers/bitsandbytes)为“CUDA自定义函数的封装器”。实际上，它被Hugging Face transformers直接支持，非常类似于Accelerate和Datasets，尽管bitsandbytes不是官方的Hugging Face库。例如，如果你在加载模型时尝试量化，transformers会告诉你安装bitsandbytes。

你可以通过pip安装：

[PRE1]

*注意：在我将这篇文章提交给Towards Data Science的一天后，Hugging Face在transformers库中增加了对AutoGPTQ模型的支持。在transformers中使用GPTQ可能会比我在下一节中报告的性能更好。*

# GPTQ和bitsandbytes之间的比较

在本节中，我将展示使用两种量化方法的模型之间的比较。

+   它们在一些数据集上的困惑度

+   他们的GPU VRAM消耗

+   他们的推理速度

对于困惑度评估，我依赖于已经发布的数字。对于VRAM消耗，我依赖于我自己的实验，同时也得到了已经发布的数字的支持。对于推理速度，我没有轻易找到已经在线发布的结果，因此我展示了我使用Llama 2 7B获得的结果。

你可以使用我在The Kaitchup上发布的笔记本（[notebook #11](https://kaitchup.substack.com/p/notebooks)）来重现我的结果，这是我的substack通讯。

## GPTQ与bitsandbytes：困惑度

AutoGPTQ的主要作者通过计算C4数据集上的困惑度来评估了量化为GPTQ和bitsandbytes的LLaMa（第一版）。

结果发布在“GPTQ-for-LLaMA”仓库（Apache 2.0许可证）中：

![](../Images/020cc9ed138d3b49aa43648a37811609.png)

结果由 [GPTQ-for-LLaMA](https://github.com/qwopqwop200/GPTQ-for-LLaMa) 提供

这里有很多信息需要解读。每个表格展示了不同大小的LLaMA的结果。让我们关注每个表格中的最后一列c4(ppl)。

我们将比较GPTQ-128g（GPTQ 4-bit）与nf4-double_quant和nf4（这两者都是bitsandbytes量化算法）。 “nf4-double_quant”是一个变体，量化了量化常数。

对于7B版本，它们的表现相同，都为5.30 ppl。

我们看到13B和33B版本存在差异，其中GPTQ产生了较低的困惑度。结果表明，相对于nf4，GPTQ似乎在模型变大时表现更好。

GPTQ似乎在这里比bitsandbytes的nf4略有优势。

## GPTQ与bitsandbytes：VRAM使用情况

在上面的表格中，作者还报告了VRAM使用情况。我们可以看到，nf4-double_quant和GPTQ使用的内存几乎相同。

nf4在没有双重量化的情况下显著使用了比GPTQ更多的内存。LLaMA 33B的差异大于1 GB。双重量化是匹配GPTQ量化性能所必需的。

在我自己使用3种不同GPU的Llama 2 7B实验中，我也观察到GPTQ和nf4-double_quant消耗的VRAM量非常相似。*注意：我使用了T4、V100和A100 40 GB GPU，这些都可以在Google Colab PRO上使用。*

## GPTQ与bitsandbytes：推理速度

为了了解推理速度，我运行了5个不同的提示，对两个量化模型进行测试，没有进行批处理，生成了最多1,000个token的输出（[查看笔记本](https://kaitchup.substack.com/p/notebooks)）。对于每个提示，我计算了每秒生成的token数量。然后，我对5个提示的结果取了平均值。

![](../Images/2d78940a365267e6ff2635a76f6847a2.png)

作者提供的表格。

这里明显的赢家是GPTQ。它的速度是bitsandbytes nf4双重量化的两倍。

结果还显示，三种GPU几乎实现了相同的速度。如果不进行批处理，购买更昂贵的GPU不会加快推理速度。然而，即使在小批量的情况下，我也预期看到T4与A100之间的一些显著差异。

# 结论：GPTQ与bitsandbytes

总结来说，如果你追求更好的性能，GPTQ 4位比bitsandbytes nf4更好。它在相似的VRAM消耗下实现了更低的困惑度和更快的推理速度。

然而，如果你希望对量化模型进行微调，bitsandbytes是一个更好且更方便的选择，因为它得到了Hugging Face库的支持。bitsandbytes量化是QLoRA背后的算法，QLoRA允许使用适配器对量化模型进行微调。

如果你在消费级硬件上开发/部署LLM，我建议你考虑以下几点：

+   使用bitsandbytes nf4和QLoRa对LLM进行微调

+   将适配器合并到LLM中

+   用GPTQ 4位量化结果模型

+   部署

以下是两种量化方法的优缺点总结：

**bitsandbytes优点：**

+   支持QLoRa

+   实时量化

**bitsandbytes缺点：**

+   推理较慢

+   量化模型不能被序列化

**GPTQ优点：**

+   序列化

+   支持3位精度

+   快速

**GPTQ缺点：**

+   模型量化较慢

+   对GPTQ模型进行微调是可能的，但研究不足
