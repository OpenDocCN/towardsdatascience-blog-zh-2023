- en: 4 Crucial Factors for Evaluating Large Language Models in Industry Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/4-crucial-factors-for-evaluating-large-language-models-in-industry-applications-f0ec8f6d4e9e](https://towardsdatascience.com/4-crucial-factors-for-evaluating-large-language-models-in-industry-applications-f0ec8f6d4e9e)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Every use case is different — depending on customer needs, and industry-specific
    guidelines. Learn how to make the right LLM choices, using 4 key rubrics
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://skanda-vivek.medium.com/?source=post_page-----f0ec8f6d4e9e--------------------------------)[![Skanda
    Vivek](../Images/9d25bee2fb75176ca7f7ea6eff7d7ab5.png)](https://skanda-vivek.medium.com/?source=post_page-----f0ec8f6d4e9e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f0ec8f6d4e9e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f0ec8f6d4e9e--------------------------------)
    [Skanda Vivek](https://skanda-vivek.medium.com/?source=post_page-----f0ec8f6d4e9e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f0ec8f6d4e9e--------------------------------)
    ·8 min read·Aug 1, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/17667e0f8e19d8ed3fb3aa190bcf63ca.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: LLM Decision Metrics | Skanda Vivek
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Over the past few months, I’ve had the opportunity to chat with folks from the
    legal, healthcare, finance, tech, insurance industries on LLM adoption. And each
    of them comes with unique requirements and challenges. In healthcare, for example
    — privacy is king. In finance, getting the numbers right is paramount. Lawyers
    want specialized, fine-tuned models for tasks like drafting legal documents.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: In this article I’m going through the key decision factors that help you choose
    the right model for your particular case.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Response Quality
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As Satya Nadella stated in his 2023 [Keynote at Microsoft Inspire](https://www.youtube.com/watch?v=RhwVMt_XCUE&ab_channel=Microsoft),
    there are 2 main paradigm shifts Generative AI introduces:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: A more natural language computer interface
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A reasoning engine, that sits on top of all your custom documents
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Response quality is extremely important in both of these use categories. Our
    interface with computers has been getting closer and closer to natural language
    (think of how much more friendly Python is compared with C++ or how much more
    friendly C++ is, compared to machine language). However, the reliability of these
    programming languages have never really been an issue — if there is an issue,
    we call it a programming bug, and attribute it to humans making errors. However,
    the more natural interface from LLMs creates a new problem, where LLMs are known
    to hallucinate or give wrong answers, and so a new type of “AI bug” gets introduced.
    Thus, response quality, becomes extremely important.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: The same is with the 2nd use case. While we are all comfortable using Google
    search, behind the scenes Google is using vector embeddings and other matching
    techniques, to figure out which page most likely contains an answer to a question
    you ask. If the page lists wrong results — that again is a human error, due to
    humans listing incorrect information. However, LLMs again introduce the possibility
    that answers generated are more customized to your question, but possibly wrong.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种用例也是如此。虽然我们都习惯使用Google搜索，但在幕后，Google使用向量嵌入和其他匹配技术来确定哪个页面最有可能包含你所问问题的答案。如果页面列出了错误的结果——这也是人为错误，因为人们列出了不正确的信息。然而，LLM再次引入了答案可能更加定制化的可能性，但也可能是错误的。
- en: Recently, there have been many new open-source LLM advances like Llama2, Falcon,
    etc. Recently there was a challenging multi-bench benchmark introduced a method
    to use [strong LLMs as judges to evaluate other LLMs](https://arxiv.org/abs/2306.05685)
    on more open-ended questions. As you can see below, they have various types of
    performance based on the tasks. It’s worth taking a look at various models — depending
    on whether your organization is looking at customized chatbots, or extracting
    information from text, generating custom code, creative writing, etc.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，出现了许多新的开源LLM进展，如Llama2、Falcon等。最近有一个具有挑战性的多基准测试引入了一种方法，使用[强大的LLM作为评审来评估其他LLM](https://arxiv.org/abs/2306.05685)在更开放性的问题上的表现。如你所见，它们根据任务有不同类型的表现。值得查看各种模型——无论你的组织是在寻找定制聊天机器人，还是从文本中提取信息，生成自定义代码，创意写作等。
- en: '![](../Images/945e7e3966d57f2cf2596bdf8c64f867.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/945e7e3966d57f2cf2596bdf8c64f867.png)'
- en: Category-wise win rate of models | Skanda Vivek (Data from the MT-Bench [LLM-as-a-judge](https://arxiv.org/abs/2306.05685)
    paper)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 模型按类别的胜率 | Skanda Vivek（数据来自MT-Bench [LLM-as-a-judge](https://arxiv.org/abs/2306.05685)
    论文）
- en: LLM Economics
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM经济学
- en: 'How expensive various LLM solutions are is a function of the type of use-case
    you have. I go through this in detail here:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 各种LLM解决方案的费用取决于你拥有的用例类型。我在这里详细讨论了这个问题：
- en: '[](/llm-economics-chatgpt-vs-open-source-dfc29f69fec1?source=post_page-----f0ec8f6d4e9e--------------------------------)
    [## LLM Economics: ChatGPT vs Open-Source'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/llm-economics-chatgpt-vs-open-source-dfc29f69fec1?source=post_page-----f0ec8f6d4e9e--------------------------------)
    [## LLM经济学：ChatGPT与开源]'
- en: How much does it cost to deploy LLMs like ChatGPT? Are open-source LLMs cheaper
    to deploy? What are the tradeoffs?
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署像ChatGPT这样的LLM需要多少钱？开源LLM的部署是否更便宜？有什么权衡？
- en: towardsdatascience.com](/llm-economics-chatgpt-vs-open-source-dfc29f69fec1?source=post_page-----f0ec8f6d4e9e--------------------------------)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/llm-economics-chatgpt-vs-open-source-dfc29f69fec1?source=post_page-----f0ec8f6d4e9e--------------------------------)
- en: 'TL;DR: For lower usage in the 1000s of requests per day range, ChatGPT works
    out cheaper than using open-sourced LLMs deployed to AWS. For millions of requests
    per day, open-sourced models deployed in AWS work out cheaper.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: TL;DR：对于每天请求量在1000次范围内的低使用情况，ChatGPT比在AWS上部署的开源LLM更便宜。对于每天数百万次请求，部署在AWS上的开源模型更便宜。
- en: '![](../Images/27579390af31ac22aaa6602b23ff8f53.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/27579390af31ac22aaa6602b23ff8f53.png)'
- en: Cartoon schematic for comparing LLM costs | Skanda Vivek
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 比较LLM成本的卡通示意图 | Skanda Vivek
- en: Latency
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 延迟
- en: 'If you are serving real-time requests e.g. a customer facing chatbot, then
    latency is important. Maybe as much (or more) as response quality. Let’s look
    at the latency for various OpenAI models: GPT-3/3.5/4\. Multiple benchmarks have
    shown that GPT-4 has superior performance as compared to GPT-3.5 (ChatGPT), which
    in-turn was a game-changing revolution over GPT-3.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在处理实时请求，例如面向客户的聊天机器人，那么延迟是很重要的。可能和响应质量一样重要（甚至更多）。让我们来看一下不同OpenAI模型的延迟情况：GPT-3/3.5/4。多个基准测试表明，与GPT-3.5（ChatGPT）相比，GPT-4具有更优越的性能，而GPT-3.5则在GPT-3之上带来了革命性的变化。
- en: 'Let’s say as a creative agency, you would like to use LLM models to give entrepreneurs
    creative suggestions — say writing a tagline for an ice-cream shop venture. I’ve
    written some sample code below to generate this tagline from the three GPT models:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 假设作为一个创意机构，你希望使用LLM模型为企业家提供创意建议——比如为冰淇淋店新项目写一个标语。我在下面写了一些示例代码，从三个GPT模型生成这个标语：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The responses are below. Personally, I prefer the GPT-4 output, but the others
    are not too bad.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 响应如下。就个人而言，我更喜欢GPT-4的输出，但其他的也还不错。
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now, let’s look at the latency. Surprisingly, GPT-3.5 returns the fastest result
    (faster than GPT-3), and GPT-4 is the slowest, at 1.4X slower than GPT-3.5.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下延迟。令人惊讶的是，GPT-3.5返回的结果最快（比GPT-3快），而GPT-4最慢，比GPT-3.5慢1.4倍。
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If the results are viewed as similar based on industry specific metrics, and
    a 1.4X slowdown is unacceptable, then GPT-3.5 is the superior choice, compared
    with GPT-4\. But maybe latency is not a big issue — say users are emailed a custom
    report instead of seeing responses in real-time, and quality of responses is more
    important. In this scenario, GPT-4 could be the preferred option.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Privacy and Security
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many folks from the healthcare industry are super interested in LLMs and game-changing
    innovations in pin-pointing detailed medical information from troves of data.
    However, the same folks are also extremely concerned about privacy. They don’t
    like the idea of calling the same API that is used by everyone else — on private
    data. Plus, there might be severe repercussions by sharing private individual
    health data to an API like ChatGPT.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two other options — one is hosting open-source LLMs on cloud providers
    like AWS, Azure, or Google Cloud. Cloud providers have a long history of complying
    with healthcare guidelines and laws, such as the Health Insurance Portability
    and Accountability Act (HIPAA). AWS for example has a dedicated whitepaper on
    [architecting applications on AWS for HIPAA compliance](https://docs.aws.amazon.com/pdfs/whitepapers/latest/architecting-hipaa-security-and-compliance-on-aws/architecting-hipaa-security-and-compliance-on-aws.pdf).
    I’ve also written some blogs on how to host LLMs as APIs on private cloud instances:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://skanda-vivek.medium.com/deploying-open-source-llms-as-apis-ec026e2187bc?source=post_page-----f0ec8f6d4e9e--------------------------------)
    [## Deploying Open-Source LLMs As APIs'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Open-source LLMs are all the rage, along with concerns about data privacy with
    closed-source LLM APIs. This tutorial…
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: skanda-vivek.medium.com](https://skanda-vivek.medium.com/deploying-open-source-llms-as-apis-ec026e2187bc?source=post_page-----f0ec8f6d4e9e--------------------------------)
    [](https://skanda-vivek.medium.com/deploy-llms-using-azure-ml-804c40f8635e?source=post_page-----f0ec8f6d4e9e--------------------------------)
    [## Deploy LLMs Using Azure ML
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: A tutorial on how to use the Microsoft Azure ML catalog for deploying LLM endpoints
    as APIs and a comparison with AWS
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: skanda-vivek.medium.com](https://skanda-vivek.medium.com/deploy-llms-using-azure-ml-804c40f8635e?source=post_page-----f0ec8f6d4e9e--------------------------------)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: The second, more private option — is to deploy LLMs on-premises, on privately
    owned servers. Unlike hosting on the cloud or using a closed-source API like ChatGPT,
    deploying on-premises would require companies to invest in their own private data
    centers and teams to manage these data centers.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Notice in the above paragraph I mentioned that on-premises LLM hosting is the
    more private option, but I didn’t talk about security. This is because both have
    their own security strengths and weaknesses. The benefit of using the cloud, is
    that cloud providers have strict security standards. Also, cloud-based applications
    are typically more resilient due to providers having redundant data centers, at
    multiple locations. But there is the risk that malicious actors who exploit cloud
    systems could now get access to client data downstream.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到在上段中我提到本地LLM托管是更私密的选择，但没有谈到安全性。这是因为两者都有各自的安全优缺点。使用云服务的好处是云服务提供商有严格的安全标准。此外，云基础应用程序通常更具弹性，因为提供商在多个地点拥有冗余的数据中心。但也存在恶意攻击者利用云系统访问客户数据的风险。
- en: On the other hand, on-premises data centers are more physically secure — if
    close at hand to the place of work. However, these need to be managed well, and
    without the right team, security vulnerabilities may abound. Also, a single on-premises
    center represents a single point of failure, with a lack of backup.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，本地数据中心在物理安全性方面更有保障——如果离工作地点较近。然而，这些数据中心需要良好管理，如果没有合适的团队，安全漏洞可能会随处可见。此外，单一的本地数据中心代表了一个单点故障，缺乏备份。
- en: Takeaways
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 收获
- en: Choosing the “right” LLM is heavily application- and industry-driven. I’ve broken
    down this choice into 4 key factors — Quality, Price, Latency, and Privacy/Security.
    While you start incorporating LLMs into your workflows, this can be a good rubric
    to make decisions on which LLMs to get started off with. Note that however, this
    might change in the future.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 选择“正确”的LLM在很大程度上取决于应用和行业。我将这个选择分解为4个关键因素——质量、价格、延迟和隐私/安全。在你开始将LLM纳入工作流程时，这可以作为决定从哪个LLM开始的良好准则。不过，请注意，这些因素未来可能会有所变化。
- en: New LLMs may come by that offer far superior performance at a lower cost. Also,
    data requirements might change. Initially you might estimate customers asking
    ~1k requests a day, but this might change to ~10k in the future, in which case
    using a closed-source API that charges by usage might not be as economic as a
    privately hosted open-sourced API with minimal usage costs.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 新的语言模型（LLMs）可能会出现，这些模型在性能上远优于现有模型，并且成本更低。此外，数据需求也可能发生变化。最初你可能估计客户每天需要~1k次请求，但未来可能会增加到~10k次，在这种情况下，使用按使用量收费的闭源API可能不如使用低成本的私有托管开源API经济。
- en: 'But if you are facing a chicken-egg situation where you don’t want to get started
    because you don’t know which model to use, I would suggest not spending too much
    time getting stuck in analysis-paralysis. Andrej Karpathy (ex-Director of AI at
    Tesla, and co-founder at OpenAI) offers some good advice on this topic at his
    [State of GPT talk](https://www.youtube.com/watch?v=bZQun8Y4L2A&ab_channel=MicrosoftDeveloper).
    He said the best way to get started is by using an off-the-shelf API like ChatGPT
    and getting it to perform better through prompt engineering/retrieval augmented
    generation (which I have written an article about):'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果你面临一种“鸡与蛋”的情况，因为不知道使用哪个模型而不愿开始，我建议不要在分析瘫痪中花费太多时间。Andrej Karpathy（前特斯拉AI总监，OpenAI联合创始人）在他的[GPT状态演讲](https://www.youtube.com/watch?v=bZQun8Y4L2A&ab_channel=MicrosoftDeveloper)中提供了一些好的建议。他说，开始的最佳方式是使用像ChatGPT这样的现成API，并通过提示工程/检索增强生成使其表现更好（我写了一篇相关文章）：
- en: '[](/build-industry-specific-llms-using-retrieval-augmented-generation-af9e98bb6f68?source=post_page-----f0ec8f6d4e9e--------------------------------)
    [## Build Industry-Specific LLMs Using Retrieval Augmented Generation'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 使用检索增强生成构建特定行业的LLMs](https://example.org/build-industry-specific-llms-using-retrieval-augmented-generation-af9e98bb6f68?source=post_page-----f0ec8f6d4e9e--------------------------------)'
- en: Organizations are in a race to adopt Large Language Models. Let’s dive into
    how you can build industry-specific LLMs…
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 各组织正在竞相采用大型语言模型。让我们深入探讨如何构建特定行业的LLMs…
- en: towardsdatascience.com](/build-industry-specific-llms-using-retrieval-augmented-generation-af9e98bb6f68?source=post_page-----f0ec8f6d4e9e--------------------------------)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](https://example.org/build-industry-specific-llms-using-retrieval-augmented-generation-af9e98bb6f68?source=post_page-----f0ec8f6d4e9e--------------------------------)'
- en: If this doesn’t work — try fine-tuning open-source models on your data (or training
    from scratch if you have the resources and bravery!). I would echo this sentiment
    for showcasing or exploring the potential applications of LLMs in your industry.
    Even if you know that ChatGPT is probably not right for your application, you
    can test it out on representative sample data. Once you get a sense of the possibilities
    to explore, you can swap out your LLM. That way, you can get started with a proof-of-concept,
    impress stakeholders, and make a difference!
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这不起作用 — 尝试对你的数据进行开源模型微调（或者如果你有资源和勇气的话，从头开始训练！）。我会对展示或探索 LLM 在你所在行业中的潜在应用持这种观点。即使你知道
    ChatGPT 可能不适合你的应用，你也可以在代表性样本数据上进行测试。一旦你对探索的可能性感到满意，你可以更换 LLM。这样，你可以开始进行概念验证，给利益相关者留下深刻印象，并产生影响！
- en: I hope this helps on your journey of applying LLMs to create awesome products,
    and I look forward to hearing all about it in the comments!
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这对你在应用 LLM 创建出色产品的过程中有所帮助，并期待在评论中听到你的反馈！
- en: '*If you like this post, follow me — I write on Generative AI in real-world
    applications and, more generally, on the intersections between data and society.*'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你喜欢这篇文章，关注我 — 我会写关于生成式 AI 在现实世界中的应用，更多地是数据与社会交叉的内容。*'
- en: '*Feel free to connect with me on* [*LinkedIn*](https://www.linkedin.com/in/skanda-vivek-01619311b/)*!*'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*随时在* [*LinkedIn*](https://www.linkedin.com/in/skanda-vivek-01619311b/) *上与我联系！*'
- en: '*If you are not yet a Medium member and want to support writers like me, feel
    free to sign-up through my referral link:* [*https://skanda-vivek.medium.com/membership*](https://skanda-vivek.medium.com/membership)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你还不是 Medium 会员，并且想支持像我这样的作者，可以通过我的推荐链接注册：* [*https://skanda-vivek.medium.com/membership*](https://skanda-vivek.medium.com/membership)'
- en: '**Here are some related articles:**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**以下是一些相关文章：**'
- en: '[When Should You Fine-Tune LLMs?](https://medium.com/towards-data-science/when-should-you-fine-tune-llms-2dddc09a404a)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[何时应对 LLM 进行微调？](https://medium.com/towards-data-science/when-should-you-fine-tune-llms-2dddc09a404a)'
- en: '[LLM Economics: ChatGPT vs Open-Source](https://medium.com/towards-data-science/llm-economics-chatgpt-vs-open-source-dfc29f69fec1)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[LLM 经济学：ChatGPT 与开源的对比](https://medium.com/towards-data-science/llm-economics-chatgpt-vs-open-source-dfc29f69fec1)'
- en: '[Deploying Open-Source LLMs As APIs](https://skanda-vivek.medium.com/deploying-open-source-llms-as-apis-ec026e2187bc)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[将开源 LLM 部署为 API](https://skanda-vivek.medium.com/deploying-open-source-llms-as-apis-ec026e2187bc)'
- en: '[How Do You Build A ChatGPT-Powered App?](https://medium.com/geekculture/how-do-you-build-a-chatgpt-powered-app-89c83f3e2143)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[你如何构建一个 ChatGPT 驱动的应用？](https://medium.com/geekculture/how-do-you-build-a-chatgpt-powered-app-89c83f3e2143)'
- en: '[Extractive vs Generative Q&A — Which is better for your business?](https://medium.com/towards-data-science/extractive-vs-generative-q-a-which-is-better-for-your-business-5a8a1faab59a)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[提取式与生成式问答 — 哪种更适合你的业务？](https://medium.com/towards-data-science/extractive-vs-generative-q-a-which-is-better-for-your-business-5a8a1faab59a)'
- en: '[Fine-Tune Transformer Models For Question Answering On Custom Data](https://medium.com/towards-data-science/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[对定制数据进行问答的 Transformer 模型微调](https://medium.com/towards-data-science/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80)'
- en: '[Unleashing the Power of Generative AI For Your Customers](https://medium.com/geekculture/unleashing-the-power-of-generative-ai-for-your-customers-70297f1c9698)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[释放生成式 AI 对你客户的潜力](https://medium.com/geekculture/unleashing-the-power-of-generative-ai-for-your-customers-70297f1c9698)'
- en: '[Deploy LLMs Using Azure ML](https://medium.com/@skanda-vivek/deploy-llms-using-azure-ml-804c40f8635e)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[使用 Azure ML 部署 LLM](https://medium.com/@skanda-vivek/deploy-llms-using-azure-ml-804c40f8635e)'
