- en: 4 Crucial Factors for Evaluating Large Language Models in Industry Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/4-crucial-factors-for-evaluating-large-language-models-in-industry-applications-f0ec8f6d4e9e](https://towardsdatascience.com/4-crucial-factors-for-evaluating-large-language-models-in-industry-applications-f0ec8f6d4e9e)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Every use case is different — depending on customer needs, and industry-specific
    guidelines. Learn how to make the right LLM choices, using 4 key rubrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://skanda-vivek.medium.com/?source=post_page-----f0ec8f6d4e9e--------------------------------)[![Skanda
    Vivek](../Images/9d25bee2fb75176ca7f7ea6eff7d7ab5.png)](https://skanda-vivek.medium.com/?source=post_page-----f0ec8f6d4e9e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f0ec8f6d4e9e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f0ec8f6d4e9e--------------------------------)
    [Skanda Vivek](https://skanda-vivek.medium.com/?source=post_page-----f0ec8f6d4e9e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f0ec8f6d4e9e--------------------------------)
    ·8 min read·Aug 1, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/17667e0f8e19d8ed3fb3aa190bcf63ca.png)'
  prefs: []
  type: TYPE_IMG
- en: LLM Decision Metrics | Skanda Vivek
  prefs: []
  type: TYPE_NORMAL
- en: Over the past few months, I’ve had the opportunity to chat with folks from the
    legal, healthcare, finance, tech, insurance industries on LLM adoption. And each
    of them comes with unique requirements and challenges. In healthcare, for example
    — privacy is king. In finance, getting the numbers right is paramount. Lawyers
    want specialized, fine-tuned models for tasks like drafting legal documents.
  prefs: []
  type: TYPE_NORMAL
- en: In this article I’m going through the key decision factors that help you choose
    the right model for your particular case.
  prefs: []
  type: TYPE_NORMAL
- en: Response Quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As Satya Nadella stated in his 2023 [Keynote at Microsoft Inspire](https://www.youtube.com/watch?v=RhwVMt_XCUE&ab_channel=Microsoft),
    there are 2 main paradigm shifts Generative AI introduces:'
  prefs: []
  type: TYPE_NORMAL
- en: A more natural language computer interface
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A reasoning engine, that sits on top of all your custom documents
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Response quality is extremely important in both of these use categories. Our
    interface with computers has been getting closer and closer to natural language
    (think of how much more friendly Python is compared with C++ or how much more
    friendly C++ is, compared to machine language). However, the reliability of these
    programming languages have never really been an issue — if there is an issue,
    we call it a programming bug, and attribute it to humans making errors. However,
    the more natural interface from LLMs creates a new problem, where LLMs are known
    to hallucinate or give wrong answers, and so a new type of “AI bug” gets introduced.
    Thus, response quality, becomes extremely important.
  prefs: []
  type: TYPE_NORMAL
- en: The same is with the 2nd use case. While we are all comfortable using Google
    search, behind the scenes Google is using vector embeddings and other matching
    techniques, to figure out which page most likely contains an answer to a question
    you ask. If the page lists wrong results — that again is a human error, due to
    humans listing incorrect information. However, LLMs again introduce the possibility
    that answers generated are more customized to your question, but possibly wrong.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, there have been many new open-source LLM advances like Llama2, Falcon,
    etc. Recently there was a challenging multi-bench benchmark introduced a method
    to use [strong LLMs as judges to evaluate other LLMs](https://arxiv.org/abs/2306.05685)
    on more open-ended questions. As you can see below, they have various types of
    performance based on the tasks. It’s worth taking a look at various models — depending
    on whether your organization is looking at customized chatbots, or extracting
    information from text, generating custom code, creative writing, etc.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/945e7e3966d57f2cf2596bdf8c64f867.png)'
  prefs: []
  type: TYPE_IMG
- en: Category-wise win rate of models | Skanda Vivek (Data from the MT-Bench [LLM-as-a-judge](https://arxiv.org/abs/2306.05685)
    paper)
  prefs: []
  type: TYPE_NORMAL
- en: LLM Economics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'How expensive various LLM solutions are is a function of the type of use-case
    you have. I go through this in detail here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/llm-economics-chatgpt-vs-open-source-dfc29f69fec1?source=post_page-----f0ec8f6d4e9e--------------------------------)
    [## LLM Economics: ChatGPT vs Open-Source'
  prefs: []
  type: TYPE_NORMAL
- en: How much does it cost to deploy LLMs like ChatGPT? Are open-source LLMs cheaper
    to deploy? What are the tradeoffs?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/llm-economics-chatgpt-vs-open-source-dfc29f69fec1?source=post_page-----f0ec8f6d4e9e--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'TL;DR: For lower usage in the 1000s of requests per day range, ChatGPT works
    out cheaper than using open-sourced LLMs deployed to AWS. For millions of requests
    per day, open-sourced models deployed in AWS work out cheaper.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/27579390af31ac22aaa6602b23ff8f53.png)'
  prefs: []
  type: TYPE_IMG
- en: Cartoon schematic for comparing LLM costs | Skanda Vivek
  prefs: []
  type: TYPE_NORMAL
- en: Latency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you are serving real-time requests e.g. a customer facing chatbot, then
    latency is important. Maybe as much (or more) as response quality. Let’s look
    at the latency for various OpenAI models: GPT-3/3.5/4\. Multiple benchmarks have
    shown that GPT-4 has superior performance as compared to GPT-3.5 (ChatGPT), which
    in-turn was a game-changing revolution over GPT-3.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say as a creative agency, you would like to use LLM models to give entrepreneurs
    creative suggestions — say writing a tagline for an ice-cream shop venture. I’ve
    written some sample code below to generate this tagline from the three GPT models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The responses are below. Personally, I prefer the GPT-4 output, but the others
    are not too bad.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s look at the latency. Surprisingly, GPT-3.5 returns the fastest result
    (faster than GPT-3), and GPT-4 is the slowest, at 1.4X slower than GPT-3.5.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If the results are viewed as similar based on industry specific metrics, and
    a 1.4X slowdown is unacceptable, then GPT-3.5 is the superior choice, compared
    with GPT-4\. But maybe latency is not a big issue — say users are emailed a custom
    report instead of seeing responses in real-time, and quality of responses is more
    important. In this scenario, GPT-4 could be the preferred option.
  prefs: []
  type: TYPE_NORMAL
- en: Privacy and Security
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many folks from the healthcare industry are super interested in LLMs and game-changing
    innovations in pin-pointing detailed medical information from troves of data.
    However, the same folks are also extremely concerned about privacy. They don’t
    like the idea of calling the same API that is used by everyone else — on private
    data. Plus, there might be severe repercussions by sharing private individual
    health data to an API like ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two other options — one is hosting open-source LLMs on cloud providers
    like AWS, Azure, or Google Cloud. Cloud providers have a long history of complying
    with healthcare guidelines and laws, such as the Health Insurance Portability
    and Accountability Act (HIPAA). AWS for example has a dedicated whitepaper on
    [architecting applications on AWS for HIPAA compliance](https://docs.aws.amazon.com/pdfs/whitepapers/latest/architecting-hipaa-security-and-compliance-on-aws/architecting-hipaa-security-and-compliance-on-aws.pdf).
    I’ve also written some blogs on how to host LLMs as APIs on private cloud instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://skanda-vivek.medium.com/deploying-open-source-llms-as-apis-ec026e2187bc?source=post_page-----f0ec8f6d4e9e--------------------------------)
    [## Deploying Open-Source LLMs As APIs'
  prefs: []
  type: TYPE_NORMAL
- en: Open-source LLMs are all the rage, along with concerns about data privacy with
    closed-source LLM APIs. This tutorial…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: skanda-vivek.medium.com](https://skanda-vivek.medium.com/deploying-open-source-llms-as-apis-ec026e2187bc?source=post_page-----f0ec8f6d4e9e--------------------------------)
    [](https://skanda-vivek.medium.com/deploy-llms-using-azure-ml-804c40f8635e?source=post_page-----f0ec8f6d4e9e--------------------------------)
    [## Deploy LLMs Using Azure ML
  prefs: []
  type: TYPE_NORMAL
- en: A tutorial on how to use the Microsoft Azure ML catalog for deploying LLM endpoints
    as APIs and a comparison with AWS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: skanda-vivek.medium.com](https://skanda-vivek.medium.com/deploy-llms-using-azure-ml-804c40f8635e?source=post_page-----f0ec8f6d4e9e--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: The second, more private option — is to deploy LLMs on-premises, on privately
    owned servers. Unlike hosting on the cloud or using a closed-source API like ChatGPT,
    deploying on-premises would require companies to invest in their own private data
    centers and teams to manage these data centers.
  prefs: []
  type: TYPE_NORMAL
- en: Notice in the above paragraph I mentioned that on-premises LLM hosting is the
    more private option, but I didn’t talk about security. This is because both have
    their own security strengths and weaknesses. The benefit of using the cloud, is
    that cloud providers have strict security standards. Also, cloud-based applications
    are typically more resilient due to providers having redundant data centers, at
    multiple locations. But there is the risk that malicious actors who exploit cloud
    systems could now get access to client data downstream.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, on-premises data centers are more physically secure — if
    close at hand to the place of work. However, these need to be managed well, and
    without the right team, security vulnerabilities may abound. Also, a single on-premises
    center represents a single point of failure, with a lack of backup.
  prefs: []
  type: TYPE_NORMAL
- en: Takeaways
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Choosing the “right” LLM is heavily application- and industry-driven. I’ve broken
    down this choice into 4 key factors — Quality, Price, Latency, and Privacy/Security.
    While you start incorporating LLMs into your workflows, this can be a good rubric
    to make decisions on which LLMs to get started off with. Note that however, this
    might change in the future.
  prefs: []
  type: TYPE_NORMAL
- en: New LLMs may come by that offer far superior performance at a lower cost. Also,
    data requirements might change. Initially you might estimate customers asking
    ~1k requests a day, but this might change to ~10k in the future, in which case
    using a closed-source API that charges by usage might not be as economic as a
    privately hosted open-sourced API with minimal usage costs.
  prefs: []
  type: TYPE_NORMAL
- en: 'But if you are facing a chicken-egg situation where you don’t want to get started
    because you don’t know which model to use, I would suggest not spending too much
    time getting stuck in analysis-paralysis. Andrej Karpathy (ex-Director of AI at
    Tesla, and co-founder at OpenAI) offers some good advice on this topic at his
    [State of GPT talk](https://www.youtube.com/watch?v=bZQun8Y4L2A&ab_channel=MicrosoftDeveloper).
    He said the best way to get started is by using an off-the-shelf API like ChatGPT
    and getting it to perform better through prompt engineering/retrieval augmented
    generation (which I have written an article about):'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/build-industry-specific-llms-using-retrieval-augmented-generation-af9e98bb6f68?source=post_page-----f0ec8f6d4e9e--------------------------------)
    [## Build Industry-Specific LLMs Using Retrieval Augmented Generation'
  prefs: []
  type: TYPE_NORMAL
- en: Organizations are in a race to adopt Large Language Models. Let’s dive into
    how you can build industry-specific LLMs…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/build-industry-specific-llms-using-retrieval-augmented-generation-af9e98bb6f68?source=post_page-----f0ec8f6d4e9e--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: If this doesn’t work — try fine-tuning open-source models on your data (or training
    from scratch if you have the resources and bravery!). I would echo this sentiment
    for showcasing or exploring the potential applications of LLMs in your industry.
    Even if you know that ChatGPT is probably not right for your application, you
    can test it out on representative sample data. Once you get a sense of the possibilities
    to explore, you can swap out your LLM. That way, you can get started with a proof-of-concept,
    impress stakeholders, and make a difference!
  prefs: []
  type: TYPE_NORMAL
- en: I hope this helps on your journey of applying LLMs to create awesome products,
    and I look forward to hearing all about it in the comments!
  prefs: []
  type: TYPE_NORMAL
- en: '*If you like this post, follow me — I write on Generative AI in real-world
    applications and, more generally, on the intersections between data and society.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Feel free to connect with me on* [*LinkedIn*](https://www.linkedin.com/in/skanda-vivek-01619311b/)*!*'
  prefs: []
  type: TYPE_NORMAL
- en: '*If you are not yet a Medium member and want to support writers like me, feel
    free to sign-up through my referral link:* [*https://skanda-vivek.medium.com/membership*](https://skanda-vivek.medium.com/membership)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Here are some related articles:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[When Should You Fine-Tune LLMs?](https://medium.com/towards-data-science/when-should-you-fine-tune-llms-2dddc09a404a)'
  prefs: []
  type: TYPE_NORMAL
- en: '[LLM Economics: ChatGPT vs Open-Source](https://medium.com/towards-data-science/llm-economics-chatgpt-vs-open-source-dfc29f69fec1)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Deploying Open-Source LLMs As APIs](https://skanda-vivek.medium.com/deploying-open-source-llms-as-apis-ec026e2187bc)'
  prefs: []
  type: TYPE_NORMAL
- en: '[How Do You Build A ChatGPT-Powered App?](https://medium.com/geekculture/how-do-you-build-a-chatgpt-powered-app-89c83f3e2143)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Extractive vs Generative Q&A — Which is better for your business?](https://medium.com/towards-data-science/extractive-vs-generative-q-a-which-is-better-for-your-business-5a8a1faab59a)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Fine-Tune Transformer Models For Question Answering On Custom Data](https://medium.com/towards-data-science/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Unleashing the Power of Generative AI For Your Customers](https://medium.com/geekculture/unleashing-the-power-of-generative-ai-for-your-customers-70297f1c9698)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Deploy LLMs Using Azure ML](https://medium.com/@skanda-vivek/deploy-llms-using-azure-ml-804c40f8635e)'
  prefs: []
  type: TYPE_NORMAL
