- en: Improve Your Boosting Algorithms with Early Stopping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/improve-your-boosting-algorithms-with-early-stopping-99616bd15d83](https://towardsdatascience.com/improve-your-boosting-algorithms-with-early-stopping-99616bd15d83)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Overview and Implementation with Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@aashishnair?source=post_page-----99616bd15d83--------------------------------)[![Aashish
    Nair](../Images/23f4b3839e464419332b690a4098d824.png)](https://medium.com/@aashishnair?source=post_page-----99616bd15d83--------------------------------)[](https://towardsdatascience.com/?source=post_page-----99616bd15d83--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----99616bd15d83--------------------------------)
    [Aashish Nair](https://medium.com/@aashishnair?source=post_page-----99616bd15d83--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----99616bd15d83--------------------------------)
    ·6 min read·May 15, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/931b4e752e9becf6f9f8aadda8a47114.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Glenn Carstens-Peters](https://unsplash.com/@glenncarstenspeters?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Boosting algorithms are largely popular in the data science space, and rightly
    so. Models that incorporate boosting yield some of the best performances, which
    is why they are commonplace in both academia and industry.
  prefs: []
  type: TYPE_NORMAL
- en: That being said, these types of algorithms will register suboptimal results
    if they are not configured properly.
  prefs: []
  type: TYPE_NORMAL
- en: One such feature that is often underutilized is **early stopping**.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we give a high-level overview of early stopping and why it should be incorporated
    into your boosting algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Recap on Boosting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before getting into early stopping, let’s briefly discuss boosting.
  prefs: []
  type: TYPE_NORMAL
- en: In short, algorithms that leverage boosting train a series of sequential models,
    with each model aiming to address the error made by its predecessor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Boosting algorithms adhere to the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Train a weak model with initial weights
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the “error” of this first model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a new model with modified weights that address the issues with the previous
    model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the “error” of this new model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 3 or 4 until a specific criterion is met (e.g., number of iterations,
    model performance, etc.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In theory, boosting serves as a perfect solution to determining the optimal
    weights for a particular model.
  prefs: []
  type: TYPE_NORMAL
- en: After all, if the model keeps learning from its previous mistakes, performing
    more iterations should yield better results. So, why not just perform as many
    iterations as possible? With a near-infinite number of models, we could achieve
    peak performance!
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, that is usually not the case.
  prefs: []
  type: TYPE_NORMAL
- en: More Iterations ≠ Better
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After a select number of iterations, the model will modify its weights and likely
    become better at generalizing with the training data. However, if an algorithm
    goes past the ideal number of iterations, it will start capturing the noise and
    become unable to perform as well on unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, boosting algorithms that use too many iterations become prone
    to overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the Right Number of Iterations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we know that a boosting algorithm needs enough iterations to find the optimal
    weights for its model, but not too many that would make it succumb to overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key then is to find the “sweet spot”: a number of iterations that is not
    too high or too low.'
  prefs: []
  type: TYPE_NORMAL
- en: However, it can be challenging to determine the ideal number of iterations as
    this figure varies from case to case. There are a number of factors that influence
    this figure, ranging from the underlying data and the model being trained.
  prefs: []
  type: TYPE_NORMAL
- en: One way to address this is to use early stopping.
  prefs: []
  type: TYPE_NORMAL
- en: Early stopping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Early stopping entails ending the training of the boosting model prematurely
    if the individual model’s performance against the validation set does not improve
    after a given number of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, instead of training weak models a fixed number of times, we can
    configure it to continue training only if it is showing better results.
  prefs: []
  type: TYPE_NORMAL
- en: The benefits of this early stopping are obvious at a glance. With this technique,
    we can ensure that the model will stop training before it succumbs to overfitting,
    thereby improving performance. It also reduces the run time for the training process
    since it leads to fewer iterations.
  prefs: []
  type: TYPE_NORMAL
- en: Case Study
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The best way to demonstrate the early stopping is with a case study. Let’s work
    with the built-in titanic dataset from the Scikit Learn library.
  prefs: []
  type: TYPE_NORMAL
- en: The objective is to train a light gradient boosting machine (LGBM) *with and
    without early stopping* and compare their results in terms of their f1-scores
    and run time.
  prefs: []
  type: TYPE_NORMAL
- en: '**Without Early Stopping**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s create an LGBM classifier that uses 1000 iterations (specified in the
    `n_estimators` hyperparameter) and evaluate it against the testing set.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d2a881d19c821e019689a2b29159dbf7.png)'
  prefs: []
  type: TYPE_IMG
- en: F-1 Score (Created by Author)
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, lets use the `%%timeit` command to determine the run time for these operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fb3069130edaeedc2ff51db60e6a7545.png)'
  prefs: []
  type: TYPE_IMG
- en: Code Output (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: By using a boosting algorithm with 1000 iterations, the model yields an f-1
    score of about 0.85 in about 473 ms.
  prefs: []
  type: TYPE_NORMAL
- en: Not bad, but do we even need 1000 iterations in the first place?
  prefs: []
  type: TYPE_NORMAL
- en: For a clearer picture, let’s see how the f-1 score of the model changes as we
    increase the number of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/579a0fe9be6c17c78ed32086c94bc4fc.png)'
  prefs: []
  type: TYPE_IMG
- en: Code Output (Created By Author)
  prefs: []
  type: TYPE_NORMAL
- en: Shockingly enough, the model’s performance against the test set steadily decreases
    after around the first 50 iterations!
  prefs: []
  type: TYPE_NORMAL
- en: It’s clear that the boosting algorithm does not need that many iterations to
    reach peak performance for this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. With Early Stopping**'
  prefs: []
  type: TYPE_NORMAL
- en: This time let’s see how the model will perform after incorporating early stopping.
  prefs: []
  type: TYPE_NORMAL
- en: Using early stopping in an LGBM classifier requires the explicit assignment
    of two hyperparameters. The first one is called `eval_set`, which contains the
    validation set. The validation set is what the model will use to evaluate its
    performance at each iteration.
  prefs: []
  type: TYPE_NORMAL
- en: The second hyperparameter is called `early_stopping_rounds` , which contains
    the number of iterations that the model can run without yielding a greater performance
    against the validation set. If the performance does not improve within these iterations,
    the model will stop training prematurely.
  prefs: []
  type: TYPE_NORMAL
- en: For this case, the value assigned to `early_stopping_rounds` is 20\. This means
    that if the model’s f-1 score against the validation set does not improve over
    its predecessors within 20 iterations, the training process will stop, even though
    it has been configured to run for 1000 iterations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ce247c549354e33dbca97f77940d317d.png)'
  prefs: []
  type: TYPE_IMG
- en: Code Output (Created by Author)
  prefs: []
  type: TYPE_NORMAL
- en: With early stopping, the model yields an f-1 score of about 0.92, which is a
    considerable improvement compared to the model that doesn’t use early stopping!
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the model should now train in less time since it uses fewer iterations.
    We can confirm this with the `%%timeit` operation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4ce1f52c15ee1a103be4a397d45589d3.png)'
  prefs: []
  type: TYPE_IMG
- en: As expected, the model that *does* early stopping is trained in a fraction of
    the time it takes to train the model that *does not* use early stopping.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/727edf2cd6c22ef7f2a648bc2c6f4ffa.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Prateek Katyal](https://unsplash.com/@prateekkatyal?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Overall, algorithms that leverage boosting tend to be more robust, given how
    they “learn” from multiple weak models. However, maximizing the number of iterations
    used in these algorithms is not a viable solution.
  prefs: []
  type: TYPE_NORMAL
- en: Different use cases will call for a different number of iterations, which is
    why the ideal number of iterations used in an algorithm should be based on the
    individual models’ performances.
  prefs: []
  type: TYPE_NORMAL
- en: For that reason, early stopping is a technique that has immense practical value.
    It uses the validation set as an indicator of whether the algorithm should have
    more iterations or stop prematurely. As explained and demonstrated in the case
    study, early stopping enables models to achieve better performance with less training
    time.
  prefs: []
  type: TYPE_NORMAL
- en: I wish you the best of luck in your data science endeavors!
  prefs: []
  type: TYPE_NORMAL
