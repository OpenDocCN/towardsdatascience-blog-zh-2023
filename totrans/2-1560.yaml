- en: Naive Bayes Classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类
- en: 原文：[https://towardsdatascience.com/naive-bayes-classification-41d1fe802a1e](https://towardsdatascience.com/naive-bayes-classification-41d1fe802a1e)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/naive-bayes-classification-41d1fe802a1e](https://towardsdatascience.com/naive-bayes-classification-41d1fe802a1e)
- en: In-depth explanation of the Naive Bayes family of classifiers, including a text
    classification example in Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器家族的深入解释，包括一个Python中的文本分类示例
- en: '[](https://medium.com/@roiyeho?source=post_page-----41d1fe802a1e--------------------------------)[![Dr.
    Roi Yehoshua](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----41d1fe802a1e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----41d1fe802a1e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----41d1fe802a1e--------------------------------)
    [Dr. Roi Yehoshua](https://medium.com/@roiyeho?source=post_page-----41d1fe802a1e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@roiyeho?source=post_page-----41d1fe802a1e--------------------------------)[![Dr.
    Roi Yehoshua](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----41d1fe802a1e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----41d1fe802a1e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----41d1fe802a1e--------------------------------)
    [Dr. Roi Yehoshua](https://medium.com/@roiyeho?source=post_page-----41d1fe802a1e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----41d1fe802a1e--------------------------------)
    ·23 min read·Jun 1, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----41d1fe802a1e--------------------------------)
    ·阅读时间23分钟·2023年6月1日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/6bacbaca2c4e4c42e4040132ae1c3102.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6bacbaca2c4e4c42e4040132ae1c3102.png)'
- en: Photo by [Mediocre Studio](https://unsplash.com/@paucasals?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/1Gvog1VdtDA?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Mediocre Studio](https://unsplash.com/@paucasals?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，来源于 [Unsplash](https://unsplash.com/photos/1Gvog1VdtDA?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: The Naive Bayes classifiers are a family of probabilistic classifiers that are
    based on applying Bayes’ theorem with naive assumption on independence between
    the features.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器是一类基于应用贝叶斯定理并假设特征之间独立性的概率分类器。
- en: These classifiers are extremely fast both in training and prediction, and they
    are also highly scalable and interpretable. Despite their oversimplified assumptions,
    they often work well on complex real-world problems, especially in text classification
    tasks such as spam filtering and sentiment analysis, where their naive assumption
    largely holds.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这些分类器在训练和预测中都非常快速，并且具有很高的可扩展性和可解释性。尽管它们的假设过于简化，但它们在复杂的实际问题上通常表现良好，尤其是在如垃圾邮件过滤和情感分析等文本分类任务中，其朴素假设通常成立。
- en: Naive Bayes is also one of the earliest **generative models** (long before ChatGPT…),which
    learn the distribution of the inputs in each class. These models can be used not
    only for prediction but also for generating new samples (see [this article](https://medium.com/@roiyeho/generative-vs-discriminative-models-35b81f677822)
    for in-depth discussion on generative vs. discriminative models).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯也是最早的**生成模型**之一（早于ChatGPT……），它学习每个类别中的输入分布。这些模型不仅可用于预测，还可以用于生成新样本（有关生成模型与判别模型的深入讨论，请参见
    [这篇文章](https://medium.com/@roiyeho/generative-vs-discriminative-models-35b81f677822)）。
- en: In this article we will discuss the Naive Bayes model and its variants in depth,
    and then show how to use its implementation in Scikit-Learn to solve a document
    classification task.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将深入探讨朴素贝叶斯模型及其变体，然后展示如何使用Scikit-Learn中的实现来解决文档分类任务。
- en: 'Background: Bayes’ Theorem'
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景：贝叶斯定理
- en: Bayes’ theorem (or Bayes’ rule) is an important theorem in probability that
    allows us to compute the conditional probability of an event, based on prior knowledge
    of conditions that are related to that event.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理（或贝叶斯规则）是概率中的一个重要定理，它允许我们基于与事件相关的先验知识计算事件的条件概率。
- en: 'Mathematically, the theorem states that for any events *A* and *B*:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，定理指出，对于任何事件 *A* 和 *B*：
- en: '![](../Images/bac412da2798be7b3b8b7ebaaf058e10.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bac412da2798be7b3b8b7ebaaf058e10.png)'
- en: Bayes’ rule
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯规则
- en: '*P*(*A*|*B*) is the **posterior probability** of *A* given *B*, i.e., the probability
    of event *A* occurring given that *B* has occurred.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(*A*|*B*)是**后验概率**，即给定*B*的情况下事件*A*发生的概率。'
- en: '*P*(*B*|*A*) is the **likelihood** of *B* given *A*, i.e., the probability
    of event *B* occurring given that *A* has occurred.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(*B*|*A*)是**似然性**，即给定*A*的情况下事件*B*发生的概率。'
- en: '*P*(*A*) is the **prior probability** of *A*, i.e., the probability of *A*
    without any prior conditions.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(*A*)是**先验概率**，即*A*的概率，没有任何先验条件。'
- en: '*P*(*B*) is the **marginal probability** of *B*, i.e., the probability of *B*
    without any prior conditions.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(*B*)是**边际概率**，即*B*的概率，没有任何先验条件。'
- en: 'Bayes’ theorem is particularly useful for inferring causes from their effects,
    since it is often easier to discern the probability of an effect given the presence
    or absence of its cause, rather than the other way around. For example, it is
    much easier to estimate the probability that a patient with meningitis will suffer
    from a headache, than the other way around (since many other diseases can cause
    an headache). In such cases, we can apply Bayes’ rule as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理特别适用于从结果推断原因，因为通常更容易辨别在原因存在或不存在的情况下结果的概率，而不是相反。例如，估计一个患有脑膜炎的患者会出现头痛的概率比估计一个头痛的患者是否患有脑膜炎要容易得多（因为许多其他疾病也可能导致头痛）。在这种情况下，我们可以应用贝叶斯规则如下：
- en: '![](../Images/0dcf6a3460d2d5c39f6048fc0902712b.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0dcf6a3460d2d5c39f6048fc0902712b.png)'
- en: Example
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例
- en: It is known that approximately 25% of patients with lung cancer suffer from
    a chest pain. Assume that the incidence rate of lung cancer is 50 per 100,000
    individuals, and the incidence rate of chest pain is 1,500 per 100,000 individuals
    worldwide. What is the probability that a patient with chest pain has a lung cancer?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 已知大约25%的肺癌患者会出现胸痛。假设肺癌的发生率为每10万人中50人，全球胸痛的发生率为每10万人中1,500人。那么，一个有胸痛的患者患肺癌的概率是多少？
- en: 'Let’s write the given inputs in the form of probabilities. Let *L* be the event
    of having lung cancer, and *C* the event of having a chest pain. From the data
    we have we know that:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将给定的输入以概率的形式表示。设*L*为患肺癌的事件，*C*为胸痛的事件。根据我们拥有的数据，我们知道：
- en: '*P*(*C*|*L*) = 0.25'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(*C*|*L*) = 0.25'
- en: '*P*(*L*) = 50 / 100,000 = 0.0005'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(*L*) = 50 / 100,000 = 0.0005'
- en: '*P*(*C*) = 1,500 / 100,000 = 0.015'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(*C*) = 1,500 / 100,000 = 0.015'
- en: 'Using Bayes’ rule, the posterior probability of having a lung cancer given
    a chest pain is:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用贝叶斯规则，给定胸痛的情况下患肺癌的后验概率是：
- en: '![](../Images/3e34bb1769212aa2f026fb2c8d96013d.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e34bb1769212aa2f026fb2c8d96013d.png)'
- en: i.e., there is only a 0.833% chance that the patient has a lung cancer.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 即，患者患肺癌的概率只有0.833%。
- en: The Naive Bayes Model
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朴素贝叶斯模型
- en: The Naive Bayes models are **probabilistic classifiers**, i.e., they not only
    assign a class label to a given sample, but they also provide an estimate of the
    probability that it belongs to that class. For example, a Naive Bayes model can
    predict that a given email has 80% chance of being a spam and 20% chance of being
    a ham.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯模型是**概率分类器**，即它们不仅为给定样本分配一个类别标签，还提供该样本属于该类别的概率估计。例如，朴素贝叶斯模型可以预测某个电子邮件有80%的概率是垃圾邮件，20%的概率是正常邮件。
- en: 'Recall that in [supervised learning](https://medium.com/@roiyeho/introduction-to-supervised-machine-learning-313730eb5aa2)
    problems, we are given a training set of *n* labeled samples: *D* = {(**x**₁,
    *y*₁), (**x**₂, *y*₂), … , (**x***ₙ, yₙ*)}, where **x***ᵢ* is a *m*-dimensional
    vector that contains the **features** of sample *i*, and *yᵢ* represents the **label**
    of that sample. In classification problems, the label can take any one of *K*
    classes, i.e., *y* ∈ {1, …, *K*}.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下在[监督学习](https://medium.com/@roiyeho/introduction-to-supervised-machine-learning-313730eb5aa2)问题中，我们会得到一个包含*n*个标记样本的训练集：*D*
    = {(**x**₁, *y*₁), (**x**₂, *y*₂), … , (**x***ₙ, yₙ*)}，其中**x***ᵢ*是一个*m*维向量，包含样本*i*的**特征**，而*yᵢ*表示该样本的**标签**。在分类问题中，标签可以取任何一个*K*类别，即，*y*
    ∈ {1, …, *K*}。
- en: 'We distinguish between two types of classifiers:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们区分两种类型的分类器：
- en: '**Deterministic classifiers** output a **hard label** for each sample, without
    providing probability estimates for the classes. Examples for such classifiers
    include [K-nearest neighbors](https://medium.com/@roiyeho/k-nearest-neighbors-knn-a-comprehensive-guide-7add717806ad),
    [decision trees](https://medium.com/@roiyeho/decision-trees-part-1-da4e613d2369),
    and SVMs.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**确定性分类器**为每个样本输出一个**硬标签**，而不提供类别的概率估计。这类分类器的例子包括 [K-最近邻](https://medium.com/@roiyeho/k-nearest-neighbors-knn-a-comprehensive-guide-7add717806ad)、[决策树](https://medium.com/@roiyeho/decision-trees-part-1-da4e613d2369)
    和 SVM。'
- en: '**Probabilistic classifiers** output probability estimates for the *k* classes,
    and then assign a label to the sample based on these probabilities (typically
    the label of the class with the highest probability). Examples for such classifiers
    include Naive Bayes classifiers, [logistic regression](https://medium.com/towards-data-science/mastering-logistic-regression-3e502686f0ae),
    and [neural networks](https://medium.com/towards-data-science/multi-layer-perceptrons-8d76972afa2b)
    that use logistic/softmax in the output layer.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**概率分类器**输出对 *k* 类的概率估计，然后根据这些概率给样本分配标签（通常是概率最高的类别的标签）。这种分类器的例子包括朴素贝叶斯分类器，[逻辑回归](https://medium.com/towards-data-science/mastering-logistic-regression-3e502686f0ae)和使用逻辑/softmax输出层的[神经网络](https://medium.com/towards-data-science/multi-layer-perceptrons-8d76972afa2b)。'
- en: 'Given a sample (**x**, *y*), a Naive Bayes classifier computes its probability
    of belonging to class *k* (i.e., *y* = *k*) using Bayes’ rule:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个样本 (**x**, *y*)，朴素贝叶斯分类器使用贝叶斯规则计算它属于类别 *k* 的概率（即 *y* = *k*）：
- en: '![](../Images/24d3b0b60abe1036c29cbe090f9c497d.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/24d3b0b60abe1036c29cbe090f9c497d.png)'
- en: The probabilities on the right side of the equation are estimated from the training
    set.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 方程右侧的概率是从训练集估计得到的。
- en: 'First, the **class prior probability** *P*(*y* = *k*) can be estimated from
    the relative frequency of class *k* across the training samples:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，**类别先验概率** *P*(*y* = *k*) 可以通过类别 *k* 在训练样本中的相对频率来估计：
- en: '![](../Images/8319b56c7fa90b897afe04b7edb58e9a.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8319b56c7fa90b897afe04b7edb58e9a.png)'
- en: where *nₖ* is the number of samples that belong to class *k*, and *n* is the
    total number of samples in the training set.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *nₖ* 是属于类别 *k* 的样本数量，*n* 是训练集中的样本总数。
- en: 'Second, the **marginal probability** *P*(**x**) can be computed by summing
    the terms appearing in the numerator of the Bayes’ rule over all the classes:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，**边际概率** *P*(**x**) 可以通过对贝叶斯规则中所有类别的分子部分进行求和来计算：
- en: '![](../Images/583ff9d71bc6071535edd5069b7e4ee3.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/583ff9d71bc6071535edd5069b7e4ee3.png)'
- en: Since the marginal probability does not depend on the class, it is not necessary
    to compute it if we are only interested in assigning a hard label to the sample
    (without providing probability estimates).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 由于边际概率不依赖于类别，如果我们只对分配硬标签给样本感兴趣（而不提供概率估计），则不需要计算它。
- en: Lastly, we need to estimate the likelihood of the features given the class,
    i.e., *P*(**x**|*y* = *k*). The main issue with estimating these probabilities
    is that there are too many of them, and we may not have enough data in the training
    set to estimate them all.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要估计在给定类别下特征的可能性，即 *P*(**x**|*y* = *k*)。估计这些概率的主要问题是数量太多，可能在训练集中没有足够的数据来估计所有的概率。
- en: For example, imagine that **x** consists of *m* binary features, e.g., each
    feature represents if a given word appears in the text document or not. In this
    case, in order to model *P(***x**|*y*), we will have to estimate 2*ᵐ* conditional
    probabilities from the training set for every class (one for each possible combination
    of *x*₁, …, *xₘ*), hence 2*ᵐK* probabilities in total. In most cases, we will
    not have enough samples in the training set to estimate all these probabilities,
    and even if we had it would have taken us an exponential time to do it.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设 **x** 由 *m* 个二进制特征组成，例如，每个特征表示某个词是否出现在文本中。在这种情况下，为了建模 *P*(**x**|*y*)，我们需要从训练集中为每个类别估计
    2*ᵐ* 个条件概率（每个 *x*₁, …, *xₘ* 的所有可能组合），因此总共有 2*ᵐK* 个概率。在大多数情况下，我们在训练集中没有足够的样本来估计所有这些概率，即使有，也需要耗费指数时间。
- en: The Naive Bayes Assumption
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 朴素贝叶斯假设
- en: 'In order to reduce the number of parameters that need to be estimated, the
    Naive Bayes model makes the following assumption: **the features are independent
    of each other given the class variable**.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少需要估计的参数数量，朴素贝叶斯模型做出了以下假设：**特征在给定类别变量的情况下是相互独立的**。
- en: 'This assumption allows us to write the probability *P*(**x**|*y* = *k*) as
    a product of the conditional probabilities of each individual feature given the
    class:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个假设允许我们将概率 *P*(**x**|*y* = *k*) 写作每个个体特征在给定类别下的条件概率的乘积：
- en: '![](../Images/0918ebd065729e65144590fbeafb903d.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0918ebd065729e65144590fbeafb903d.png)'
- en: The naive Bayes assumption
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯假设
- en: For example, in a spam filtering task, the Naive Bayes assumption means that
    words such as “rich” and “prince” contribute independently to the prediction if
    the email is spam or not, regardless of any possible correlation between these
    words.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在垃圾邮件过滤任务中，朴素贝叶斯假设意味着诸如“rich”和“prince”这样的词独立地对预测邮件是否是垃圾邮件作出贡献，无论这些词之间是否存在任何可能的关联。
- en: The naive assumption largely holds in application domains such as text classification
    and recommendation systems, where the features are generally independent of each
    other.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素假设在应用领域如文本分类和推荐系统中大致成立，在这些领域中，特征通常彼此独立。
- en: The naive Bayes assumption reduces significantly the number of parameters that
    need to be estimated from the data. For example, in the case where the input **x**
    consists of *m* binary features, it reduces the number of parameters of the model
    from 2*ᵐ* to *m* per class.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯假设显著减少了需要从数据中估计的参数数量。例如，在输入**x**包含*m*个二元特征的情况下，它将模型的参数数量从2*ᵐ*减少到每个类别*m*。
- en: MAP (Maximum A-Posteriori)
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MAP（最大后验）
- en: 'Based on the naive Bayes assumption, we can now write the class posterior probability
    as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 基于朴素贝叶斯假设，我们现在可以将类别后验概率写作如下：
- en: '![](../Images/c29c8f9b70ec742304a23c04f89c06d4.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c29c8f9b70ec742304a23c04f89c06d4.png)'
- en: 'If we are only interested in assigning a class label to the given sample (and
    we do not care about the probabilities), we can ignore the denominator *P*(**x**),
    and use the following classification rule:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只关心将类别标签分配给给定样本（而不关心概率），我们可以忽略分母 *P*(**x**)，并使用以下分类规则：
- en: '![](../Images/4a3bccc51ed88944faaee78ec3e8f8d6.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4a3bccc51ed88944faaee78ec3e8f8d6.png)'
- en: This is called a **MAP (Maximum A-Posteriori)** decision rule, since it chooses
    the hypothesis that maximizes the posterior probability.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这称为**MAP（最大后验）**决策规则，因为它选择最大化后验概率的假设。
- en: Naive Bayes will make the correct MAP decision as long as the correct class
    is predicted as more probable than the other classes, even when the probability
    estimates are inaccurate. This provides some robustness to the model against the
    deficiencies in the underlying naive independence assumption.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 只要正确的类别被预测为比其他类别更可能，朴素贝叶斯将做出正确的MAP决策，即使概率估计不准确。这为模型提供了一些对基础朴素独立假设缺陷的鲁棒性。
- en: Note that if we assume that all the priors *P*(*y*) are equally likely (e.g.,
    when we don’t have any prior information on which hypothesis is more likely),
    then the MAP decision rule is equivalent to the MLE (maximum likelihood estimation)
    decision rule, which chooses the model that maximizes the likelihood of the data
    given the model *P*(**x**|*y*). (You can read more about maximum likelihood in
    [this article](https://medium.com/@roiyeho/maximum-likelihood-855b6df92c43).)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果我们假设所有先验 *P*(*y*) 是等可能的（例如，当我们没有关于哪个假设更可能的先验信息时），那么MAP决策规则等同于MLE（最大似然估计）决策规则，它选择最大化给定模型
    *P*(**x**|*y*) 的数据似然性的模型。（你可以在[这篇文章](https://medium.com/@roiyeho/maximum-likelihood-855b6df92c43)中了解更多关于最大似然的信息。）
- en: Parameter Estimation
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参数估计
- en: We are now left with the task of estimating the conditional probabilities *P*(*xⱼ*|*y*
    = *k*) for each feature *j* and for each class *k*. This estimation depends both
    on the type of the feature (e.g., discrete or continuous) and the probability
    distribution we assume that it has.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在剩下的任务是估计每个特征 *j* 和每个类别 *k* 的条件概率 *P*(*xⱼ*|*y* = *k*)。这个估计依赖于特征的类型（例如，离散或连续）以及我们假设其具有的概率分布。
- en: The assumptions on the distribution of the features are called **event models**.
    Each event model leads to a different type of Naive Bayes classifier. In the following
    sections we will discuss the different event models and how to estimate the model
    parameters in each one.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对特征分布的假设称为**事件模型**。每个事件模型导致不同类型的朴素贝叶斯分类器。在接下来的部分，我们将讨论不同的事件模型以及如何在每种模型中估计模型参数。
- en: Bernoulli Naive Bayes
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 伯努利朴素贝叶斯
- en: In the Bernoulli event model, the features *xⱼ* are modeled as independent binary
    variables with a Bernoulli distribution, i.e., each feature *xⱼ* has a probability
    *pⱼ* of appearing in the given sample and 1 − *pⱼ* for being absent from that
    sample.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在伯努利事件模型中，特征*xⱼ*被建模为具有伯努利分布的独立二元变量，即每个特征*xⱼ*在给定样本中出现的概率为*pⱼ*，不出现的概率为1 − *pⱼ*。
- en: For example, in a text classification task, each feature *xⱼ* may represent
    the occurrence or absence of the *j*th word from the vocabulary in the text.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在文本分类任务中，每个特征*xⱼ*可能代表文本中词汇表中第*j*个单词的出现或缺失。
- en: 'In the Bernoulli event model, the probability *P*(*xⱼ*|*y* = *k*) is estimated
    from the frequency in which feature *j* appears in samples of class *k*:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在伯努利事件模型中，概率*P*(*xⱼ*|*y* = *k*)是通过特征*j*在类别*k*的样本中出现的频率来估计的：
- en: '![](../Images/a4c564cd37802a624e990fc1a5e2ff65.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4c564cd37802a624e990fc1a5e2ff65.png)'
- en: where *nⱼₖ* is the number of samples in class *k* in which feature *xⱼ* appears,
    and *nₖ* is the total number of samples in class *k*.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*nⱼₖ*是类别*k*中包含特征*xⱼ*的样本数量，*nₖ*是类别*k*中样本的总数。
- en: Categorical Naive Bayes
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类朴素贝叶斯
- en: The categorical event model is an extension of the Bernoulli event model to
    *V* categories (instead of only two). In this model, we assume that each feature
    is a categorical (discrete) variable that can take one of *V* possible categories
    with some probability *pᵢ*, where the sum of all the probabilities is 1.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 分类事件模型是伯努利事件模型对*V*个类别（而不是仅两个类别）的扩展。在这个模型中，我们假设每个特征是一个类别（离散）变量，可以取*V*个可能的类别中的一个，其概率为*pᵢ*，其中所有概率的总和为1。
- en: 'In this event model, we need to estimate the probability *P*(*xⱼ = v*|*y* =
    *k*) for every feature *xⱼ* and every category *v*. Similar to the previous model,
    we estimate this probability as the frequency in which feature *j* gets the value
    *v* in samples of class *k*:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个事件模型中，我们需要估计每个特征*xⱼ*和每个类别*v*的概率*P*(*xⱼ = v*|*y* = *k*)。与之前的模型类似，我们通过特征*j*在类别*k*的样本中取值*v*的频率来估计这个概率：
- en: '![](../Images/145d92a803fe06a8250ebdccc5dff937.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/145d92a803fe06a8250ebdccc5dff937.png)'
- en: where *nⱼᵥₖ* is the number of samples in class *k* where feature *xⱼ* gets the
    value *v*, and *nₖ* is the total number of samples in class *k*.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*nⱼᵥₖ*是类别*k*中特征*xⱼ*取值*v*的样本数量，*nₖ*是类别*k*中样本的总数。
- en: 'Example: Customer Purchase Prediction'
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例：顾客购买预测
- en: 'Assume that we have the following table with data on the past purchases of
    customers in a store:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个包含商店顾客过去购买数据的表格：
- en: '![](../Images/95045bd40a835ada95321e511bf77a80.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/95045bd40a835ada95321e511bf77a80.png)'
- en: The training set
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集
- en: Each row in the table contains the age of the customer, whether they are a student
    or not, their level of income, their credit rating and whether or not they have
    purchased the product.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 表中的每一行包含顾客的年龄、是否是学生、收入水平、信用评级以及是否购买了产品。
- en: 'A new customer with the following properties arrives at the store:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有以下属性的新顾客到达商店：
- en: <Age = Young, Student = Yes, Income = Low, Credit = Excellent>
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: <Age = Young, Student = Yes, Income = Low, Credit = Excellent>
- en: You need to predict whether this customer will buy the product or not.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要预测这个顾客是否会购买产品。
- en: 'We first compute the class prior probabilities by counting the number of rows
    that have Buys = Yes (6 out of 10) and the number of rows that have Buys = No
    (4 out of 10):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先通过计算Buys = Yes（10行中的6行）和Buys = No（10行中的4行）的行数来计算类别先验概率。
- en: '![](../Images/b1a856f5d670609f9e562c945c861e00.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b1a856f5d670609f9e562c945c861e00.png)'
- en: 'Then, we compute the likelihood of the features in each class:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们计算每个类别中特征的可能性：
- en: '![](../Images/7f54ed4e1618be3a40877186b18cd1f1.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7f54ed4e1618be3a40877186b18cd1f1.png)'
- en: 'Therefore, the class posterior probabilities are:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，类别后验概率为：
- en: '![](../Images/4f124f7269457e4c9f6fd37e056ea04a.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f124f7269457e4c9f6fd37e056ea04a.png)'
- en: '*α* is the normalization factor (*α* = 1 / *P*(**x**)).'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*α*是归一化因子（*α* = 1 / *P*(**x**））。'
- en: Since *P*(Buys = Yes|**x**) > *P*(Buys = No|**x**), our prediction is that the
    customer will buy the product.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 由于*P*(Buys = Yes|**x**) > *P*(Buys = No|**x**)，我们的预测是顾客会购买产品。
- en: 'If we want to get the actual probability that the customer will buy the product,
    we can first find the normalization factor using the fact that the two posterior
    probabilities must sum to 1:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想得到顾客购买产品的实际概率，我们可以首先使用两个后验概率之和为1的事实来找到归一化因子：
- en: '![](../Images/ff50ea0ccac346812ac1db4ee25ecd4c.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ff50ea0ccac346812ac1db4ee25ecd4c.png)'
- en: 'Then, we can plug it in the posterior probability for Buy = Yes:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以将其代入“Buy = Yes”的后验概率中：
- en: '![](../Images/3a91d1a93c29d1ff236fd6e312cbef8e.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a91d1a93c29d1ff236fd6e312cbef8e.png)'
- en: The probability that the customer will buy the product is 54.21%.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 客户购买该产品的概率是54.21%。
- en: Multinomial Naive Bayes
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多项式朴素贝叶斯
- en: In the multinomial event model, we assume that the data set has only one categorical
    feature *x* that can take one of *m* categories, and each feature vector (*x*₁,
    …, *xₘ*) is a **histogram**, where *xⱼ* counts the number of times *x* had the
    value *j* in that particular instance.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在多项式事件模型中，我们假设数据集只有一个类别特征*x*，它可以取*m*个类别中的一个，每个特征向量（*x*₁, …, *xₘ*）是一个**直方图**，其中*xⱼ*计算了*x*在特定实例中取值*j*的次数。
- en: 'This event model is particularly useful when working with text documents, where
    *m* is the number of words in the vocabulary, and each feature *xⱼ* represents
    the number of times the *j*th word from the vocabulary appears in the document.
    This representation is called the **bag-of-words** model:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这个事件模型在处理文本文件时特别有用，其中*m*是词汇表中的词数，每个特征*xⱼ*表示词汇表中第*j*个词在文档中出现的次数。这种表示方法称为**词袋**模型：
- en: '![](../Images/09bd1ef31078dc7fe00d017c8ac9d1a0.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09bd1ef31078dc7fe00d017c8ac9d1a0.png)'
- en: The bag-of-words model
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋模型
- en: 'In this event model, we estimate the probability *P*(*x = v*|*y* = *k*) as
    the frequency in which feature *x* gets the value *v* in samples of class *k*:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个事件模型中，我们估计概率*P*(*x = v*|*y* = *k*)为特征*x*在类别*k*的样本中取得值*v*的频率：
- en: '![](../Images/cf24aae58f39d4397172b4dfedfbf55c.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf24aae58f39d4397172b4dfedfbf55c.png)'
- en: where *nᵥₖ* is the number of samples in class *k* in which *x* gets the value
    *v*, and *nₖ* is the total number of samples in class *k*.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*nᵥₖ*是类别*k*中*x*取值*v*的样本数量，*nₖ*是类别*k*中样本的总数量。
- en: 'Example: Spam Filter'
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例：垃圾邮件过滤器
- en: 'We would like to build a spam filter based on a training set with 100 emails:
    80 of them are non-spam (ham) and 20 of them are spam. The counts of the words
    in each type of email are given in the following tables:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望基于一个包含100封邮件的训练集构建一个垃圾邮件过滤器：其中80封为正常邮件，20封为垃圾邮件。每种类型邮件中的词语计数在以下表格中给出：
- en: '![](../Images/84e94749dec34d61a25165e71dcf02f4.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84e94749dec34d61a25165e71dcf02f4.png)'
- en: The counts of the words in ham and spam emails
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 正常邮件和垃圾邮件中的词语计数
- en: A new email has arrived with the text “rich friend need money”. Is this a ham
    or a spam?
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 一封新的邮件到达，内容为“rich friend need money”。这是正常邮件还是垃圾邮件？
- en: 'Let’s first compute the class prior probabilities:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 首先计算类别的先验概率：
- en: '![](../Images/7f3b072dcf27606302e7be6caf2cac9e.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7f3b072dcf27606302e7be6caf2cac9e.png)'
- en: 'Next, we estimate the likelihoods of the words in each type of email. The total
    number of words in the ham emails is: 80 + 5 + 5 + 20 + 50 + 40 = 200, and the
    total number of words in the spam emails is: 20 + 20 + 30 + 25 + 25 = 120\. Therefore,
    the likelihoods of the words are:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们估计每种类型邮件中词语的可能性。正常邮件中的词语总数为：80 + 5 + 5 + 20 + 50 + 40 = 200，而垃圾邮件中的词语总数为：20
    + 20 + 30 + 25 + 25 = 120。因此，词语的可能性为：
- en: '![](../Images/a703ed79c8daefa9d78d00a9f1b382fd.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a703ed79c8daefa9d78d00a9f1b382fd.png)'
- en: 'Consequently, the class posterior probabilities are:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，类别的后验概率是：
- en: '![](../Images/7ef9a082b4c0e8759a01de0010c85ad5.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ef9a082b4c0e8759a01de0010c85ad5.png)'
- en: Therefore, our prediction is that the email is spam.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的预测是该邮件是垃圾邮件。
- en: Laplace Smoothing
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拉普拉斯平滑
- en: If one of the features and a given class never occur together in the training
    set, then the estimate of its likelihood will be zero. Since the likelihoods of
    the features are multiplied together, this will wipe out all the information that
    we have from the other features.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如果某个特征和某个类别在训练集中从未一起出现，那么它的可能性估计将为零。由于特征的可能性是相乘的，这将抹去我们从其他特征中获得的所有信息。
- en: For example, since the word “movie” is missing from the spam emails, if we get
    an email with the word “movie“ in it, it will be automatically considered as a
    ham, even if the other words in the email are very “spammy”.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，由于垃圾邮件中缺少“movie”这个词，如果我们收到一封包含“movie”的邮件，它将被自动认为是正常邮件，即使邮件中的其他词语非常“垃圾”。
- en: 'As a concrete example, consider an email with the text “movie rich rich prince”.
    This email will be classified as a ham, although the words “rich” and “prince”
    are highly correlated with spam emails, because the posterior probability of Spam
    is zero:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个具体的例子，考虑一封内容为“movie rich rich prince”的邮件。这封邮件将被分类为正常邮件，尽管“rich”和“prince”这两个词与垃圾邮件高度相关，因为垃圾邮件的后验概率为零：
- en: '![](../Images/c7e947e67eaba2677a307f69e5dd998f.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7e947e67eaba2677a307f69e5dd998f.png)'
- en: In order to deal with this issue, we incorporate a small sample correction (called
    pseudocount) in all the probability estimates, such that no probability is never
    set to exactly zero.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理这个问题，我们在所有概率估计中加入一个小的样本修正（称为伪计数），以确保概率不会被设定为零。
- en: 'In multinomial Naive Bayes, this correction is applied as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在多项式朴素贝叶斯中，修正如下应用：
- en: '![](../Images/bf7d4e89875e03522340b6ccf1766495.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bf7d4e89875e03522340b6ccf1766495.png)'
- en: where *α* is a smoothing parameter and *n* is the total number of samples in
    the training set. Setting *α* = 1 is called **Laplace smoothing** (the most common
    one), while *α* < 1 is called Lidstone smoothing.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*α*是平滑参数，*n*是训练集中的样本总数。设置*α* = 1称为**拉普拉斯平滑**（最常见的），而*α* < 1称为利德斯通平滑。
- en: 'Similarly, in categorical Naive Bayes the correction is applied as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，在分类朴素贝叶斯中，修正如下应用：
- en: '![](../Images/4215977336ec7c9bd84098e75fec1f77.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4215977336ec7c9bd84098e75fec1f77.png)'
- en: where *nⱼ* is the number of possible categories of feature *j*.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*nⱼ*是特征*j*的可能类别数。
- en: 'Revisiting our spam filter example, let’s apply Laplace smoothing by adding
    a pseudocount of 1 to all the words:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 重新审视我们的垃圾邮件过滤器示例，让我们通过为所有词汇添加伪计数1来应用拉普拉斯平滑：
- en: '![](../Images/d588007a0ce5211e0b2e63c77ec95e8b.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d588007a0ce5211e0b2e63c77ec95e8b.png)'
- en: Applying Laplace smoothing to the word counts
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 对词频应用拉普拉斯平滑
- en: 'This time, the email with the text “movie rich rich prince” will be classified
    as a spam, since:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，包含文本“movie rich rich prince”的电子邮件将被分类为垃圾邮件，因为：
- en: '![](../Images/d240223b7c24e3aec87a4d27e6b55e0c.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d240223b7c24e3aec87a4d27e6b55e0c.png)'
- en: Gaussian Naive Bayes
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高斯朴素贝叶斯
- en: Up until now we assumed that all our features are discrete. How does the Naive
    Bayes model deal with continuous features?
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 直到现在我们假设所有特征都是离散的。朴素贝叶斯模型如何处理连续特征？
- en: 'There are three main approaches to deal with continuous features:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 处理连续特征的主要方法有三种：
- en: Discretize the feature values and obtain a Bernoulli or categorically distributed
    features.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将特征值离散化，并获得伯努利或分类分布特征。
- en: Assume that the feature is distributed according to some known probability distribution
    (usually a normal distribution) and estimate the parameters of that distribution
    from the training set (e.g., the mean and variance of the normal distribution).
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设特征按照某个已知概率分布（通常是正态分布）分布，并从训练集中估计该分布的参数（例如，正态分布的均值和方差）。
- en: Use [KDE (kernel density estimation)](https://en.wikipedia.org/wiki/Kernel_density_estimation)
    to estimate the probability density function of the feature using the given sample
    points as kernels.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用[KDE（核密度估计）](https://en.wikipedia.org/wiki/Kernel_density_estimation)来估计特征的概率密度函数，利用给定的样本点作为核。
- en: 'In Gaussian Naive Bayes, we take the second approach and assume that the likelihood
    of the features is Gaussian:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在高斯朴素贝叶斯中，我们采用第二种方法，假设特征的似然性是高斯分布的：
- en: '![](../Images/9e5c0eca909fdb51d02f8dec272d5883.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e5c0eca909fdb51d02f8dec272d5883.png)'
- en: where *μⱼₖ* is the mean of the values of *xⱼ* in all the samples that belong
    to class *k*, and *σⱼₖ* is the standard deviation of these values (these are the
    maximum likelihood estimates of the true parameters of the distribution).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*μⱼₖ*是类别*k*中所有样本的*xⱼ*值的均值，*σⱼₖ*是这些值的标准差（这些是分布真实参数的最大似然估计）。
- en: The event models described above can also be combined in case we have a heterogenous
    data set, i.e., a data set that contains different types of features (for example,
    both categorical and continuous features).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 上述事件模型也可以组合使用，以处理异质数据集，即包含不同类型特征的数据集（例如，既有分类特征也有连续特征）。
- en: Naive Bayes Classifiers in Scikit-Learn
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scikit-Learn中的朴素贝叶斯分类器
- en: 'The module sklearn.naive_bayes provides implementations for all the four Naive
    Bayes classifiers mentioned above:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 模块sklearn.naive_bayes提供了上述提到的四种朴素贝叶斯分类器的实现：
- en: '[BernoulliNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB)
    implements the Bernoulli Naive Bayes model.'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[BernoulliNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB)实现了伯努利朴素贝叶斯模型。'
- en: '[CategoricalNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.CategoricalNB.html#sklearn.naive_bayes.CategoricalNB)
    implements the categorical Naive Bayes model.'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[CategoricalNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.CategoricalNB.html#sklearn.naive_bayes.CategoricalNB)实现了分类朴素贝叶斯模型。'
- en: '[MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB)
    implements the multinomial Naive Bayes model.'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB)
    实现了多项式朴素贝叶斯模型。'
- en: '[GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB)
    implements the Gaussian Naive Bayes model.'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB)
    实现了高斯朴素贝叶斯模型。'
- en: The first three classes accept a parameter called *alpha* that defines the smoothing
    parameter (by default it is set to 1.0).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 前三种类别接受一个名为*alpha*的参数，该参数定义了平滑参数（默认为1.0）。
- en: Document Classification Example
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文档分类示例
- en: In the following demonstration we will use MultinomialNB to solve a document
    classification task. The data set we are going to use is the [20 newsgroups dataset](https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset),
    which consists of 18,846 newsgroups posts, partitioned (nearly) evenly across
    20 different topics. This data set has been widely used in research of text applications
    in machine learning, including document classification and clustering.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的演示中，我们将使用[MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB)来解决文档分类任务。我们将使用的数据集是[20
    newsgroups dataset](https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset)，该数据集包含18,846篇新闻组帖子，几乎均匀地划分为20个不同主题。这个数据集在机器学习中的文本应用研究中被广泛使用，包括文档分类和聚类。
- en: Loading the Data Set
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载数据集
- en: You can use the function [fetch_20newsgroups()](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html)
    in Scikit-Learn to download the text documents with their labels. You can either
    download all the documents as one group, or download the training set and the
    test set separately (using the *subset* parameter). The split between the training
    and the test sets is based upon messages posted before or after a specific date.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用函数[fetch_20newsgroups()](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html)在Scikit-Learn中下载带标签的文本文档。你可以选择将所有文档作为一个组下载，或者分别下载训练集和测试集（使用*subset*参数）。训练集和测试集的划分基于在特定日期之前或之后发布的消息。
- en: 'By default, the text documents contain some metadata such as headers (e.g.,
    the date of the post), footers (signatures) and quotes to other posts. Since these
    features are not relevant for the text classification task, we will strip them
    out by using the *remove* parameter:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，文本文档包含一些元数据，如标题（例如，帖子的日期）、页脚（签名）和其他帖子的引用。由于这些特征与文本分类任务无关，我们将通过使用*remove*参数将它们剥离掉：
- en: '[PRE0]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that the first time you call this function it may take a few minutes to
    download all the documents, after which they will be cached locally in the folder
    ~/scikit_learn_data*.*
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，第一次调用此函数时，可能需要几分钟时间下载所有文档，之后它们将被缓存到本地文件夹 ~/scikit_learn_data* 中。
- en: 'The output of the function is a dictionary that contains the following attributes:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的输出是一个包含以下属性的字典：
- en: '*data* — the set of documents'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*data* — 文档集合'
- en: '*target* — the target labels'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*target* — 目标标签'
- en: '*target_names* — the names of the document categories'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*target_names* — 文档类别名称'
- en: 'Let’s store the documents and their labels in proper variables:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将文档及其标签存储到适当的变量中：
- en: '[PRE1]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Data Exploration
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据探索
- en: 'Let’s do some basic exploration of the data. The number of documents we have
    in the training and the test sets is:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对数据进行一些基本的探索。我们在训练集和测试集中拥有的文档数量是：
- en: '[PRE2]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: A simple calculation shows that 60% of the documents belong to the training
    set, and 40% to the test set.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 简单计算显示，60%的文档属于训练集，40%属于测试集。
- en: 'Let’s print the list of categories:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打印类别列表：
- en: '[PRE4]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As evident, some of the categories are closely related to each other (e.g.,
    *comp.sys.mac.hardware* and *comp.sys.ibm.pc.hardware*), while others are highly
    uncorrelated (e.g., *sci.electronics* and *soc.religion.christian*).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，某些类别彼此紧密相关（例如，*comp.sys.mac.hardware* 和 *comp.sys.ibm.pc.hardware*），而其他类别则高度不相关（例如，*sci.electronics*
    和 *soc.religion.christian*）。
- en: 'Finally, let’s examine one of the documents in the training set (e.g., the
    first one):'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们查看训练集中其中一份文档（例如，第一个）：
- en: '[PRE6]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Unsurprisingly, the label of this document is:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 不出所料，该文档的标签是：
- en: '[PRE8]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Converting Text to Vectors
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将文本转换为向量
- en: In order to feed text documents into machine learning models, we first need
    to convert them into vectors of numerical values (i.e., **vectorize** the text).
    This process typically involves preprocessing and cleaning of the text, and then
    choosing a suitable numerical representation for the words in the text.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将文本文档输入到机器学习模型中，我们首先需要将它们转换为数值向量（即**向量化**文本）。这个过程通常涉及文本的预处理和清理，然后选择合适的数值表示来表示文本中的词。
- en: '**Text preprocessing** consists of various steps, amongst which the most common
    ones are:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '**文本预处理**包括多个步骤，其中最常见的包括：'
- en: Cleaning and normalizing the text. This includes removing punctuation marks
    and special characters, and converting the text into lower-case.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 清理和标准化文本。这包括去除标点符号和特殊字符，并将文本转换为小写。
- en: Text tokenization, i.e., splitting the text into individual words or terms.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文本分词，即将文本拆分成单个词或术语。
- en: Removal of stop words. Stop words are a set of commonly used words in a given
    language. For example, stop words in English include words like “the”, “a”, “is”,
    “and”. These words are usually filtered out since they do not carry useful information.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 停用词的去除。停用词是特定语言中常用的词。例如，英语中的停用词包括“the”、“a”、“is”、“and”。这些词通常被过滤掉，因为它们不携带有用的信息。
- en: Stemming or lemmatization. **Stemming** reduces the word to its lexical root
    by removing or replacing its suffix, while **lemmatization** reduces the word
    to its canonical form (lemma) and also takes into account the context of the word
    (its part-of-speech). For example, the word *computers* has the lemma *computer*,
    but its lexical root is *comput*.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 词干提取或词形还原。**词干提取**通过去除或替换词缀将词还原为其词汇根，而**词形还原**将词还原为其规范形式（词元），并考虑词的上下文（词性）。例如，词*computers*的词元是*computer*，但其词汇根是*comput*。
- en: 'The following example demonstrates these steps on a given sentence:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例演示了这些步骤在给定句子上的应用：
- en: '![](../Images/b8bdeff878e59c4f6ae26385f2338136.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b8bdeff878e59c4f6ae26385f2338136.png)'
- en: Text preprocessing example
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 文本预处理示例
- en: 'After cleaning the text, we need to choose how to vectorize it into a numerical
    vector. The most common approaches are:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 清理文本后，我们需要选择如何将其向量化为数值向量。最常见的方法有：
- en: '**Bag-of-words (BOW) model**. In this model, each document is represented by
    a word counts vector (similar to the one we have used in the spam filter example).'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**词袋（BOW）模型**。在这个模型中，每个文档通过词频向量表示（类似于我们在垃圾邮件过滤器示例中使用的）。'
- en: '[**TF-IDF**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (Term Frequency
    times Inverse Document Frequency) measures how relevant a word is to a document
    by multiplying two metrics:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[**TF-IDF**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)（词频与逆文档频率的乘积）通过乘以两个指标来衡量一个词对文档的相关性：'
- en: (a) TF (Term Frequency) — how many times the word appears in the document.
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: (a) TF（词频）——单词在文档中出现的次数。
- en: (b) IDF (Inverse Document Frequency) — the inverse of the frequency in which
    the word appears in documents across the entire corpus.
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: (b) IDF（逆文档频率）——词在整个语料库中出现的频率的倒数。
- en: The idea is to decrease the weight of words that occur frequently in the corpus,
    while increasing the weight of words that occur rarely (and thus are more indicative
    of the document’s category).
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其思想是减少在语料库中频繁出现的词的权重，同时增加稀有词的权重（从而更能指示文档的类别）。
- en: '**Word embeddings**.In this approach,words are mapped into real-valued vectors
    in such a way that words with similar meaning have close representation in the
    vector space. This model is typically used in deep learning and will be discussed
    in a future post.'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**词嵌入**。在这种方法中，词被映射到实值向量中，以便具有相似意义的词在向量空间中具有接近的表示。这个模型通常用于深度学习，将在未来的帖子中讨论。'
- en: 'Scikit-Learn provides the following two transformers, which support both text
    preprocessing and vectorization:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn提供了以下两种变换器，支持文本预处理和向量化：
- en: '[CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)
    uses the bag-of-words model.'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)使用词袋模型。'
- en: '[TfIdfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)
    uses the TF-IDF representation.'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[TfIdfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)使用TF-IDF表示。'
- en: 'Important hyperparameters of these transformers include:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这些变换器的重要超参数包括：
- en: '*lowercase* — whether to convert all the characters to lowercase before tokenizing
    (defaults to True).'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*lowercase* — 是否在标记化之前将所有字符转换为小写（默认为True）。'
- en: '*token_pattern* — the regular expression used to define what is a token (the
    default regex selects tokens of two or more alphanumeric characters).'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*token_pattern* — 用于定义什么是令牌的正则表达式（默认正则表达式选择两个或更多字母数字字符的令牌）。'
- en: '*stop_words* — if ‘english’, uses a built-in stop word list for English. If
    None (the default), no stop words will be used. You can also provide your own
    custom stop words list.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*stop_words* — 如果为‘english’，则使用内置的英语停用词列表。如果为None（默认值），则不会使用停用词。你也可以提供自己定制的停用词列表。'
- en: '*max_features* — if not None, build a vocabulary that includes only the top
    *max_features* with the highest term frequency across the training corpus. Otherwise,
    all the features are used (this is the default).'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*max_features* — 如果不为None，则构建一个仅包含在训练语料库中具有最高术语频率的前*max_features*个词汇的词汇表。否则，将使用所有特征（这是默认值）。'
- en: Note that these transformers do not provide advanced preprocessing techniques
    such as stemming or lemmatization. To apply these techniques, you will have to
    use other libraries such as [NLTK](https://www.nltk.org/) (Natural Language Toolkit)
    or [spaCy](https://spacy.io/).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这些变换器不提供诸如词干提取或词形还原之类的高级预处理技术。要应用这些技术，你将需要使用其他库，如[NLTK](https://www.nltk.org/)（自然语言工具包）或[spaCy](https://spacy.io/)。
- en: 'Since Naive Bayes models are known to work better with TF-IDF representations,
    we will use the TfidfVectorizer to convert the documents in the training set into
    TF-IDF vectors:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 由于朴素贝叶斯模型在TF-IDF表示上表现更好，我们将使用TfidfVectorizer将训练集中的文档转换为TF-IDF向量：
- en: '[PRE10]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The shape of the extracted TF-IDF vectors is:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 提取的TF-IDF向量的形状是：
- en: '[PRE11]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'That is, there are 101,322 unique tokens in the vocabulary of the corpus. We
    can examine these tokens by calling the method *get_feature_names_out*() of the
    vectorizer:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，语料库的词汇表中有101,322个独特的令牌。我们可以通过调用向量化器的*get_feature_names_out*()方法来检查这些令牌：
- en: '[PRE13]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Evidently, there was no automatic spell checker back in the 90s :)
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，在90年代没有自动拼写检查器 :)
- en: 'The TF-IDF vectors are very sparse, with an average of 67 non-zero components
    out of more than 100,000:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF向量非常稀疏，平均有67个非零组件，超过100,000个：
- en: '[PRE15]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Let’s also vectorize the documents in the test set (note that on the test set
    we call the *transform* method instead of *fit_transform*):'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也将测试集中的文档向量化（请注意，在测试集上我们调用*transform*方法而不是*fit_transform*）：
- en: '[PRE17]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Building the Model
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建模型
- en: 'Let’s now build a multinomial Naive Bayes classifier and fit it to the training
    set:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们构建一个多项式朴素贝叶斯分类器，并将其拟合到训练集上：
- en: '[PRE18]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note that we need to set the smoothing parameter *α* to a very small number,
    since the TF-IDF values are scaled to be between 0 and 1, so the default *α* =
    1 would cause a dramatic shift of the values.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们需要将平滑参数*α*设置为一个非常小的数字，因为TF-IDF值被缩放到0和1之间，因此默认的*α* = 1会导致值的剧烈变化。
- en: Evaluating the Model
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估模型
- en: Next, let’s evaluate the model on both the training and the test sets.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们在训练集和测试集上评估模型。
- en: 'The accuracy and F1 score of the model on the training set are:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在训练集上的准确率和F1得分为：
- en: '[PRE19]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'And the accuracy and F1 score on the test set are:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集上的准确率和F1得分为：
- en: '[PRE21]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The scores on the test set are relatively low compared to the training set.
    To investigate where the errors come from, let’s plot the confusion matrix of
    the test documents:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 与训练集相比，测试集上的得分相对较低。为了调查错误来源，让我们绘制测试文档的混淆矩阵：
- en: '[PRE23]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![](../Images/342b469405408ed5654389d78022b4a7.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/342b469405408ed5654389d78022b4a7.png)'
- en: The confusion matrix on the test set
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集上的混淆矩阵
- en: 'As we can see, most of the confusions occur between highly correlated topics,
    for example:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，大多数混淆发生在高度相关的主题之间，例如：
- en: 74 confusions between topic 0 (alt.atheism) and topic 15 (soc.religion.christian)
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主题0（alt.atheism）和主题15（soc.religion.christian）之间有74个混淆
- en: 92 confusions between topic 18 (talk.politics.misc) and topic 16 (talk.politics.guns)
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主题18（talk.politics.misc）和主题16（talk.politics.guns）之间有92个混淆
- en: 89 confusions between topic 19 (talk.religion.misc) and topic 15 (soc.religion.christian)
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主题19（talk.religion.misc）和主题15（soc.religion.christian）之间有89个混淆
- en: In light of these findings, it seems that the Naive Bayes classifier did a pretty
    good job. Let’s examine how it compares to other standard classification algorithms.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些发现，朴素贝叶斯分类器表现得相当不错。让我们看看它与其他标准分类算法的比较。
- en: Benchmarking
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基准测试
- en: 'We will benchmark the Naive Bayes model against four other classifiers: [logistic
    regression](https://medium.com/p/3e502686f0ae), [KNN](https://medium.com/@roiyeho/k-nearest-neighbors-knn-a-comprehensive-guide-7add717806ad),
    [random forest](https://medium.com/@roiyeho/random-forests-98892261dc49) and [AdaBoost](https://medium.com/@roiyeho/adaboost-illustrated-3084183a2086).'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把朴素贝叶斯模型与另外四个分类器进行基准测试：[逻辑回归](https://medium.com/p/3e502686f0ae)，[KNN](https://medium.com/@roiyeho/k-nearest-neighbors-knn-a-comprehensive-guide-7add717806ad)，[随机森林](https://medium.com/@roiyeho/random-forests-98892261dc49)和[AdaBoost](https://medium.com/@roiyeho/adaboost-illustrated-3084183a2086)。
- en: 'Let’s first write a function that gets a set of classifiers and evaluates them
    on the given data set and also measures their training time:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 首先编写一个函数，该函数获取一组分类器，并在给定的数据集上评估它们，同时测量它们的训练时间：
- en: '[PRE24]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We will now call this function with our five classifiers:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用这五个分类器调用这个函数：
- en: '[PRE25]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output we get is:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的输出是：
- en: '[PRE26]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Let’s plot the accuracy and F1 scores of the classifiers:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制分类器的准确率和F1分数：
- en: '[PRE27]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![](../Images/4e6303ae117bbcd728562b00aac15a5f.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4e6303ae117bbcd728562b00aac15a5f.png)'
- en: Accuracy scores on the test set
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集上的准确率
- en: '[PRE28]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![](../Images/22f76ec2b2c341d9883bd03363093827.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22f76ec2b2c341d9883bd03363093827.png)'
- en: F1 scores on the test set
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集上的F1分数
- en: Multinomial NB achieves both the highest accuracy and F1 scores. Notice that
    the classifiers have been used with their default parameters without any tuning.
    For a more fair comparison, the algorithms should be compared after fine tuning
    their hyperparameters. In addition, some algorithms such as KNN suffer from the
    [curse of dimensionality](https://medium.com/@roiyeho/what-is-the-curse-of-dimensionality-b9b4b81a25c5),
    and dimensionality reduction is required in order to make them work well.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式NB在准确率和F1分数上都表现最佳。注意，分类器使用的是默认参数，未经过任何调整。为了更公平的比较，应在微调其超参数后比较算法。此外，一些算法如KNN会遭遇[维度诅咒](https://medium.com/@roiyeho/what-is-the-curse-of-dimensionality-b9b4b81a25c5)，需要进行降维才能使其有效工作。
- en: 'Let’s also plot the training times of the classifiers:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们还绘制分类器的训练时间：
- en: '[PRE29]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '![](../Images/2cb15af2b0ab15b76fba4c0b93564ba3.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2cb15af2b0ab15b76fba4c0b93564ba3.png)'
- en: Training time of the different classifiers
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 不同分类器的训练时间
- en: The training of Multinomial NB is so fast that we cannot even see its time in
    the graph! By examining the function’s output from above, we can see that its
    training time is only 0.064 seconds. Note that the training of KNN is also very
    fast (since no model is actually built), but its prediction time (not shown) is
    very slow.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式NB的训练速度如此之快，以至于我们在图中看不到它的时间！通过查看上面的函数输出，我们可以看到其训练时间仅为0.064秒。请注意，KNN的训练速度也非常快（因为实际上没有构建模型），但其预测时间（未显示）非常慢。
- en: In conclusion, Multinomial NB has shown superiority over the other classifiers
    in all the examined criteria.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，多项式NB在所有检查的标准中显示出优越性。
- en: Finding the Most Informative Features
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查找最具信息量的特征
- en: The Naive Bayes model also allows us to get the most informative features of
    each class, i.e., the features with the highest likelihood *P*(*xⱼ*|*y*).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯模型还允许我们获取每个类别的最具信息量的特征，即具有最高可能性的特征 *P*(*xⱼ*|*y*)。
- en: The MultinomialNB class has an attribute named *feature_log_prob_*, which provides
    the log probability of the features for each class in a matrix of shape (*n_classes*,
    *n_features*).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: MultinomialNB类有一个名为 *feature_log_prob_* 的属性，它提供了每个类别的特征的对数概率，矩阵形状为 (*n_classes*,
    *n_features*)。
- en: 'Using this attribute, let’s write a function to find the 10 most informative
    features (tokens) in each category:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个属性，编写一个函数来找到每个类别中10个最有信息量的特征（词项）：
- en: '[PRE30]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output we get is:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的输出是：
- en: '[PRE32]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Most of the words seem to be strongly correlated with their corresponding category.
    However, there are a few generic words such as “just” and “does” that do not provide
    valuable information. This suggests that our model may be improved by having a
    better stop-words list. Indeed, Scikit-Learn recommends not to use its own default
    list, quoting from its documentation: “There are several known issues with ‘english’
    and you should consider an alternative”. 😲'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数词似乎与其对应的类别有很强的相关性。然而，也有一些像“just”和“does”这样的通用词不提供有价值的信息。这表明我们的模型可能通过更好的停用词列表来改进。实际上，Scikit-Learn建议不要使用其默认列表，并引用其文档：“‘english’存在一些已知问题，你应考虑其他替代方案。”
    😲
- en: Summary
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'Let’s summarize the pros and cons of Naive Bayes as compared to other classification
    models:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下朴素贝叶斯与其他分类模型的优缺点：
- en: '**Pros**:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**：'
- en: Extremely fast both in training and prediction
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和预测速度极快
- en: Provides class probability estimates
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供类别概率估计
- en: Can be used both for binary and multi-class classification problems
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用于二分类和多分类问题
- en: Requires a small amount of training data to estimate its parameters
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要少量的训练数据来估计其参数
- en: Highly interpretable
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高度可解释
- en: Highly scalable (the number of parameters is linear in the number of features)
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高度可扩展（参数数量与特征数量线性相关）
- en: Works well with high-dimensional data
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在高维数据上表现良好
- en: Robust to noise (the noisy samples are averaged out when estimating the conditional
    probabilities)
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对噪声具有鲁棒性（噪声样本在估计条件概率时被平均处理）
- en: Can deal with missing values (the missing values are ignored when computing
    the likelihoods of the features)
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以处理缺失值（计算特征的似然时忽略缺失值）
- en: No hyperparameters to tune (except for the smoothing parameter, which is rarely
    changed)
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有超参数需要调整（除了平滑参数，通常不做更改）
- en: '**Cons**:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点：**'
- en: Relies on the Naive Bayes assumption which does not hold in many real-world
    domains
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 依赖于朴素贝叶斯假设，而该假设在许多实际领域并不成立
- en: Correlation between the features can degrade the performance of the model
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征之间的相关性可能会降低模型的性能
- en: Generally outperformed by more complex models
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常被更复杂的模型超越
- en: 'The zero frequency problem: if a categorical feature has a category that was
    not observed in the training set, the model will assign a zero probability to
    its occurrence. Smoothing alleviates this problem but does not solve it completely.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 零频率问题：如果一个分类特征在训练集中未出现过，其类别将被模型赋予零概率。平滑处理可以缓解这一问题，但不能完全解决。
- en: Cannot handle continuous attributes without discretization or making assumptions
    on their distribution
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无法处理连续属性，除非进行离散化或对其分布做出假设
- en: Can be used only for classification tasks
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅能用于分类任务
- en: Final Notes
  id: totrans-294
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终说明
- en: This is the longest article I have written on Medium so far. I hope you enjoyed
    reading it at least as much as I enjoyed writing it. Let me know in the comments
    if something was not clear.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我在Medium上写的最长的一篇文章。希望你阅读时的感受至少与我写作时一样愉快。如果有任何不清楚的地方，请在评论中告知我。
- en: 'You can find the code examples of this article on my github: [https://github.com/roiyeho/medium/tree/main/naive_bayes](https://github.com/roiyeho/medium/tree/main/naive_bayes)'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在我的GitHub上找到这篇文章的代码示例： [https://github.com/roiyeho/medium/tree/main/naive_bayes](https://github.com/roiyeho/medium/tree/main/naive_bayes)
- en: All images unless otherwise noted are by the author.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，否则所有图片均由作者提供。
- en: 'The 20 newsgroups data set info:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 20个新闻组数据集的信息：
- en: '**Citation:** Mitchell, Tom (1999). Twenty Newsgroups. UCI Machine Learning
    Repository. [https://doi.org/10.24432/C5C323.](https://doi.org/10.24432/C5C323.)'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**引用：** Mitchell, Tom (1999). Twenty Newsgroups. UCI机器学习库。 [https://doi.org/10.24432/C5C323.](https://doi.org/10.24432/C5C323.)'
- en: '**License:** Creative Commons CC BY 4.0.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**许可证：** Creative Commons CC BY 4.0。'
