- en: Naive Bayes Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/naive-bayes-classification-41d1fe802a1e](https://towardsdatascience.com/naive-bayes-classification-41d1fe802a1e)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In-depth explanation of the Naive Bayes family of classifiers, including a text
    classification example in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@roiyeho?source=post_page-----41d1fe802a1e--------------------------------)[![Dr.
    Roi Yehoshua](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----41d1fe802a1e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----41d1fe802a1e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----41d1fe802a1e--------------------------------)
    [Dr. Roi Yehoshua](https://medium.com/@roiyeho?source=post_page-----41d1fe802a1e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----41d1fe802a1e--------------------------------)
    ·23 min read·Jun 1, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6bacbaca2c4e4c42e4040132ae1c3102.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Mediocre Studio](https://unsplash.com/@paucasals?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/1Gvog1VdtDA?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: The Naive Bayes classifiers are a family of probabilistic classifiers that are
    based on applying Bayes’ theorem with naive assumption on independence between
    the features.
  prefs: []
  type: TYPE_NORMAL
- en: These classifiers are extremely fast both in training and prediction, and they
    are also highly scalable and interpretable. Despite their oversimplified assumptions,
    they often work well on complex real-world problems, especially in text classification
    tasks such as spam filtering and sentiment analysis, where their naive assumption
    largely holds.
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes is also one of the earliest **generative models** (long before ChatGPT…),which
    learn the distribution of the inputs in each class. These models can be used not
    only for prediction but also for generating new samples (see [this article](https://medium.com/@roiyeho/generative-vs-discriminative-models-35b81f677822)
    for in-depth discussion on generative vs. discriminative models).
  prefs: []
  type: TYPE_NORMAL
- en: In this article we will discuss the Naive Bayes model and its variants in depth,
    and then show how to use its implementation in Scikit-Learn to solve a document
    classification task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Background: Bayes’ Theorem'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bayes’ theorem (or Bayes’ rule) is an important theorem in probability that
    allows us to compute the conditional probability of an event, based on prior knowledge
    of conditions that are related to that event.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, the theorem states that for any events *A* and *B*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bac412da2798be7b3b8b7ebaaf058e10.png)'
  prefs: []
  type: TYPE_IMG
- en: Bayes’ rule
  prefs: []
  type: TYPE_NORMAL
- en: '*P*(*A*|*B*) is the **posterior probability** of *A* given *B*, i.e., the probability
    of event *A* occurring given that *B* has occurred.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P*(*B*|*A*) is the **likelihood** of *B* given *A*, i.e., the probability
    of event *B* occurring given that *A* has occurred.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P*(*A*) is the **prior probability** of *A*, i.e., the probability of *A*
    without any prior conditions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P*(*B*) is the **marginal probability** of *B*, i.e., the probability of *B*
    without any prior conditions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bayes’ theorem is particularly useful for inferring causes from their effects,
    since it is often easier to discern the probability of an effect given the presence
    or absence of its cause, rather than the other way around. For example, it is
    much easier to estimate the probability that a patient with meningitis will suffer
    from a headache, than the other way around (since many other diseases can cause
    an headache). In such cases, we can apply Bayes’ rule as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0dcf6a3460d2d5c39f6048fc0902712b.png)'
  prefs: []
  type: TYPE_IMG
- en: Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is known that approximately 25% of patients with lung cancer suffer from
    a chest pain. Assume that the incidence rate of lung cancer is 50 per 100,000
    individuals, and the incidence rate of chest pain is 1,500 per 100,000 individuals
    worldwide. What is the probability that a patient with chest pain has a lung cancer?
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s write the given inputs in the form of probabilities. Let *L* be the event
    of having lung cancer, and *C* the event of having a chest pain. From the data
    we have we know that:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P*(*C*|*L*) = 0.25'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P*(*L*) = 50 / 100,000 = 0.0005'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P*(*C*) = 1,500 / 100,000 = 0.015'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using Bayes’ rule, the posterior probability of having a lung cancer given
    a chest pain is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3e34bb1769212aa2f026fb2c8d96013d.png)'
  prefs: []
  type: TYPE_IMG
- en: i.e., there is only a 0.833% chance that the patient has a lung cancer.
  prefs: []
  type: TYPE_NORMAL
- en: The Naive Bayes Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Naive Bayes models are **probabilistic classifiers**, i.e., they not only
    assign a class label to a given sample, but they also provide an estimate of the
    probability that it belongs to that class. For example, a Naive Bayes model can
    predict that a given email has 80% chance of being a spam and 20% chance of being
    a ham.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that in [supervised learning](https://medium.com/@roiyeho/introduction-to-supervised-machine-learning-313730eb5aa2)
    problems, we are given a training set of *n* labeled samples: *D* = {(**x**₁,
    *y*₁), (**x**₂, *y*₂), … , (**x***ₙ, yₙ*)}, where **x***ᵢ* is a *m*-dimensional
    vector that contains the **features** of sample *i*, and *yᵢ* represents the **label**
    of that sample. In classification problems, the label can take any one of *K*
    classes, i.e., *y* ∈ {1, …, *K*}.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We distinguish between two types of classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Deterministic classifiers** output a **hard label** for each sample, without
    providing probability estimates for the classes. Examples for such classifiers
    include [K-nearest neighbors](https://medium.com/@roiyeho/k-nearest-neighbors-knn-a-comprehensive-guide-7add717806ad),
    [decision trees](https://medium.com/@roiyeho/decision-trees-part-1-da4e613d2369),
    and SVMs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Probabilistic classifiers** output probability estimates for the *k* classes,
    and then assign a label to the sample based on these probabilities (typically
    the label of the class with the highest probability). Examples for such classifiers
    include Naive Bayes classifiers, [logistic regression](https://medium.com/towards-data-science/mastering-logistic-regression-3e502686f0ae),
    and [neural networks](https://medium.com/towards-data-science/multi-layer-perceptrons-8d76972afa2b)
    that use logistic/softmax in the output layer.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Given a sample (**x**, *y*), a Naive Bayes classifier computes its probability
    of belonging to class *k* (i.e., *y* = *k*) using Bayes’ rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/24d3b0b60abe1036c29cbe090f9c497d.png)'
  prefs: []
  type: TYPE_IMG
- en: The probabilities on the right side of the equation are estimated from the training
    set.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the **class prior probability** *P*(*y* = *k*) can be estimated from
    the relative frequency of class *k* across the training samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8319b56c7fa90b897afe04b7edb58e9a.png)'
  prefs: []
  type: TYPE_IMG
- en: where *nₖ* is the number of samples that belong to class *k*, and *n* is the
    total number of samples in the training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, the **marginal probability** *P*(**x**) can be computed by summing
    the terms appearing in the numerator of the Bayes’ rule over all the classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/583ff9d71bc6071535edd5069b7e4ee3.png)'
  prefs: []
  type: TYPE_IMG
- en: Since the marginal probability does not depend on the class, it is not necessary
    to compute it if we are only interested in assigning a hard label to the sample
    (without providing probability estimates).
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we need to estimate the likelihood of the features given the class,
    i.e., *P*(**x**|*y* = *k*). The main issue with estimating these probabilities
    is that there are too many of them, and we may not have enough data in the training
    set to estimate them all.
  prefs: []
  type: TYPE_NORMAL
- en: For example, imagine that **x** consists of *m* binary features, e.g., each
    feature represents if a given word appears in the text document or not. In this
    case, in order to model *P(***x**|*y*), we will have to estimate 2*ᵐ* conditional
    probabilities from the training set for every class (one for each possible combination
    of *x*₁, …, *xₘ*), hence 2*ᵐK* probabilities in total. In most cases, we will
    not have enough samples in the training set to estimate all these probabilities,
    and even if we had it would have taken us an exponential time to do it.
  prefs: []
  type: TYPE_NORMAL
- en: The Naive Bayes Assumption
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to reduce the number of parameters that need to be estimated, the
    Naive Bayes model makes the following assumption: **the features are independent
    of each other given the class variable**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This assumption allows us to write the probability *P*(**x**|*y* = *k*) as
    a product of the conditional probabilities of each individual feature given the
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0918ebd065729e65144590fbeafb903d.png)'
  prefs: []
  type: TYPE_IMG
- en: The naive Bayes assumption
  prefs: []
  type: TYPE_NORMAL
- en: For example, in a spam filtering task, the Naive Bayes assumption means that
    words such as “rich” and “prince” contribute independently to the prediction if
    the email is spam or not, regardless of any possible correlation between these
    words.
  prefs: []
  type: TYPE_NORMAL
- en: The naive assumption largely holds in application domains such as text classification
    and recommendation systems, where the features are generally independent of each
    other.
  prefs: []
  type: TYPE_NORMAL
- en: The naive Bayes assumption reduces significantly the number of parameters that
    need to be estimated from the data. For example, in the case where the input **x**
    consists of *m* binary features, it reduces the number of parameters of the model
    from 2*ᵐ* to *m* per class.
  prefs: []
  type: TYPE_NORMAL
- en: MAP (Maximum A-Posteriori)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Based on the naive Bayes assumption, we can now write the class posterior probability
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c29c8f9b70ec742304a23c04f89c06d4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we are only interested in assigning a class label to the given sample (and
    we do not care about the probabilities), we can ignore the denominator *P*(**x**),
    and use the following classification rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4a3bccc51ed88944faaee78ec3e8f8d6.png)'
  prefs: []
  type: TYPE_IMG
- en: This is called a **MAP (Maximum A-Posteriori)** decision rule, since it chooses
    the hypothesis that maximizes the posterior probability.
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes will make the correct MAP decision as long as the correct class
    is predicted as more probable than the other classes, even when the probability
    estimates are inaccurate. This provides some robustness to the model against the
    deficiencies in the underlying naive independence assumption.
  prefs: []
  type: TYPE_NORMAL
- en: Note that if we assume that all the priors *P*(*y*) are equally likely (e.g.,
    when we don’t have any prior information on which hypothesis is more likely),
    then the MAP decision rule is equivalent to the MLE (maximum likelihood estimation)
    decision rule, which chooses the model that maximizes the likelihood of the data
    given the model *P*(**x**|*y*). (You can read more about maximum likelihood in
    [this article](https://medium.com/@roiyeho/maximum-likelihood-855b6df92c43).)
  prefs: []
  type: TYPE_NORMAL
- en: Parameter Estimation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are now left with the task of estimating the conditional probabilities *P*(*xⱼ*|*y*
    = *k*) for each feature *j* and for each class *k*. This estimation depends both
    on the type of the feature (e.g., discrete or continuous) and the probability
    distribution we assume that it has.
  prefs: []
  type: TYPE_NORMAL
- en: The assumptions on the distribution of the features are called **event models**.
    Each event model leads to a different type of Naive Bayes classifier. In the following
    sections we will discuss the different event models and how to estimate the model
    parameters in each one.
  prefs: []
  type: TYPE_NORMAL
- en: Bernoulli Naive Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the Bernoulli event model, the features *xⱼ* are modeled as independent binary
    variables with a Bernoulli distribution, i.e., each feature *xⱼ* has a probability
    *pⱼ* of appearing in the given sample and 1 − *pⱼ* for being absent from that
    sample.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in a text classification task, each feature *xⱼ* may represent
    the occurrence or absence of the *j*th word from the vocabulary in the text.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Bernoulli event model, the probability *P*(*xⱼ*|*y* = *k*) is estimated
    from the frequency in which feature *j* appears in samples of class *k*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a4c564cd37802a624e990fc1a5e2ff65.png)'
  prefs: []
  type: TYPE_IMG
- en: where *nⱼₖ* is the number of samples in class *k* in which feature *xⱼ* appears,
    and *nₖ* is the total number of samples in class *k*.
  prefs: []
  type: TYPE_NORMAL
- en: Categorical Naive Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The categorical event model is an extension of the Bernoulli event model to
    *V* categories (instead of only two). In this model, we assume that each feature
    is a categorical (discrete) variable that can take one of *V* possible categories
    with some probability *pᵢ*, where the sum of all the probabilities is 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this event model, we need to estimate the probability *P*(*xⱼ = v*|*y* =
    *k*) for every feature *xⱼ* and every category *v*. Similar to the previous model,
    we estimate this probability as the frequency in which feature *j* gets the value
    *v* in samples of class *k*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/145d92a803fe06a8250ebdccc5dff937.png)'
  prefs: []
  type: TYPE_IMG
- en: where *nⱼᵥₖ* is the number of samples in class *k* where feature *xⱼ* gets the
    value *v*, and *nₖ* is the total number of samples in class *k*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Customer Purchase Prediction'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Assume that we have the following table with data on the past purchases of
    customers in a store:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/95045bd40a835ada95321e511bf77a80.png)'
  prefs: []
  type: TYPE_IMG
- en: The training set
  prefs: []
  type: TYPE_NORMAL
- en: Each row in the table contains the age of the customer, whether they are a student
    or not, their level of income, their credit rating and whether or not they have
    purchased the product.
  prefs: []
  type: TYPE_NORMAL
- en: 'A new customer with the following properties arrives at the store:'
  prefs: []
  type: TYPE_NORMAL
- en: <Age = Young, Student = Yes, Income = Low, Credit = Excellent>
  prefs: []
  type: TYPE_NORMAL
- en: You need to predict whether this customer will buy the product or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first compute the class prior probabilities by counting the number of rows
    that have Buys = Yes (6 out of 10) and the number of rows that have Buys = No
    (4 out of 10):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b1a856f5d670609f9e562c945c861e00.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, we compute the likelihood of the features in each class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7f54ed4e1618be3a40877186b18cd1f1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, the class posterior probabilities are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4f124f7269457e4c9f6fd37e056ea04a.png)'
  prefs: []
  type: TYPE_IMG
- en: '*α* is the normalization factor (*α* = 1 / *P*(**x**)).'
  prefs: []
  type: TYPE_NORMAL
- en: Since *P*(Buys = Yes|**x**) > *P*(Buys = No|**x**), our prediction is that the
    customer will buy the product.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to get the actual probability that the customer will buy the product,
    we can first find the normalization factor using the fact that the two posterior
    probabilities must sum to 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ff50ea0ccac346812ac1db4ee25ecd4c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, we can plug it in the posterior probability for Buy = Yes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3a91d1a93c29d1ff236fd6e312cbef8e.png)'
  prefs: []
  type: TYPE_IMG
- en: The probability that the customer will buy the product is 54.21%.
  prefs: []
  type: TYPE_NORMAL
- en: Multinomial Naive Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the multinomial event model, we assume that the data set has only one categorical
    feature *x* that can take one of *m* categories, and each feature vector (*x*₁,
    …, *xₘ*) is a **histogram**, where *xⱼ* counts the number of times *x* had the
    value *j* in that particular instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'This event model is particularly useful when working with text documents, where
    *m* is the number of words in the vocabulary, and each feature *xⱼ* represents
    the number of times the *j*th word from the vocabulary appears in the document.
    This representation is called the **bag-of-words** model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/09bd1ef31078dc7fe00d017c8ac9d1a0.png)'
  prefs: []
  type: TYPE_IMG
- en: The bag-of-words model
  prefs: []
  type: TYPE_NORMAL
- en: 'In this event model, we estimate the probability *P*(*x = v*|*y* = *k*) as
    the frequency in which feature *x* gets the value *v* in samples of class *k*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cf24aae58f39d4397172b4dfedfbf55c.png)'
  prefs: []
  type: TYPE_IMG
- en: where *nᵥₖ* is the number of samples in class *k* in which *x* gets the value
    *v*, and *nₖ* is the total number of samples in class *k*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Spam Filter'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We would like to build a spam filter based on a training set with 100 emails:
    80 of them are non-spam (ham) and 20 of them are spam. The counts of the words
    in each type of email are given in the following tables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/84e94749dec34d61a25165e71dcf02f4.png)'
  prefs: []
  type: TYPE_IMG
- en: The counts of the words in ham and spam emails
  prefs: []
  type: TYPE_NORMAL
- en: A new email has arrived with the text “rich friend need money”. Is this a ham
    or a spam?
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first compute the class prior probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7f3b072dcf27606302e7be6caf2cac9e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we estimate the likelihoods of the words in each type of email. The total
    number of words in the ham emails is: 80 + 5 + 5 + 20 + 50 + 40 = 200, and the
    total number of words in the spam emails is: 20 + 20 + 30 + 25 + 25 = 120\. Therefore,
    the likelihoods of the words are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a703ed79c8daefa9d78d00a9f1b382fd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Consequently, the class posterior probabilities are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7ef9a082b4c0e8759a01de0010c85ad5.png)'
  prefs: []
  type: TYPE_IMG
- en: Therefore, our prediction is that the email is spam.
  prefs: []
  type: TYPE_NORMAL
- en: Laplace Smoothing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If one of the features and a given class never occur together in the training
    set, then the estimate of its likelihood will be zero. Since the likelihoods of
    the features are multiplied together, this will wipe out all the information that
    we have from the other features.
  prefs: []
  type: TYPE_NORMAL
- en: For example, since the word “movie” is missing from the spam emails, if we get
    an email with the word “movie“ in it, it will be automatically considered as a
    ham, even if the other words in the email are very “spammy”.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a concrete example, consider an email with the text “movie rich rich prince”.
    This email will be classified as a ham, although the words “rich” and “prince”
    are highly correlated with spam emails, because the posterior probability of Spam
    is zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c7e947e67eaba2677a307f69e5dd998f.png)'
  prefs: []
  type: TYPE_IMG
- en: In order to deal with this issue, we incorporate a small sample correction (called
    pseudocount) in all the probability estimates, such that no probability is never
    set to exactly zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'In multinomial Naive Bayes, this correction is applied as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bf7d4e89875e03522340b6ccf1766495.png)'
  prefs: []
  type: TYPE_IMG
- en: where *α* is a smoothing parameter and *n* is the total number of samples in
    the training set. Setting *α* = 1 is called **Laplace smoothing** (the most common
    one), while *α* < 1 is called Lidstone smoothing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, in categorical Naive Bayes the correction is applied as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4215977336ec7c9bd84098e75fec1f77.png)'
  prefs: []
  type: TYPE_IMG
- en: where *nⱼ* is the number of possible categories of feature *j*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Revisiting our spam filter example, let’s apply Laplace smoothing by adding
    a pseudocount of 1 to all the words:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d588007a0ce5211e0b2e63c77ec95e8b.png)'
  prefs: []
  type: TYPE_IMG
- en: Applying Laplace smoothing to the word counts
  prefs: []
  type: TYPE_NORMAL
- en: 'This time, the email with the text “movie rich rich prince” will be classified
    as a spam, since:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d240223b7c24e3aec87a4d27e6b55e0c.png)'
  prefs: []
  type: TYPE_IMG
- en: Gaussian Naive Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until now we assumed that all our features are discrete. How does the Naive
    Bayes model deal with continuous features?
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three main approaches to deal with continuous features:'
  prefs: []
  type: TYPE_NORMAL
- en: Discretize the feature values and obtain a Bernoulli or categorically distributed
    features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assume that the feature is distributed according to some known probability distribution
    (usually a normal distribution) and estimate the parameters of that distribution
    from the training set (e.g., the mean and variance of the normal distribution).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use [KDE (kernel density estimation)](https://en.wikipedia.org/wiki/Kernel_density_estimation)
    to estimate the probability density function of the feature using the given sample
    points as kernels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In Gaussian Naive Bayes, we take the second approach and assume that the likelihood
    of the features is Gaussian:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9e5c0eca909fdb51d02f8dec272d5883.png)'
  prefs: []
  type: TYPE_IMG
- en: where *μⱼₖ* is the mean of the values of *xⱼ* in all the samples that belong
    to class *k*, and *σⱼₖ* is the standard deviation of these values (these are the
    maximum likelihood estimates of the true parameters of the distribution).
  prefs: []
  type: TYPE_NORMAL
- en: The event models described above can also be combined in case we have a heterogenous
    data set, i.e., a data set that contains different types of features (for example,
    both categorical and continuous features).
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes Classifiers in Scikit-Learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The module sklearn.naive_bayes provides implementations for all the four Naive
    Bayes classifiers mentioned above:'
  prefs: []
  type: TYPE_NORMAL
- en: '[BernoulliNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB)
    implements the Bernoulli Naive Bayes model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[CategoricalNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.CategoricalNB.html#sklearn.naive_bayes.CategoricalNB)
    implements the categorical Naive Bayes model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB)
    implements the multinomial Naive Bayes model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB)
    implements the Gaussian Naive Bayes model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first three classes accept a parameter called *alpha* that defines the smoothing
    parameter (by default it is set to 1.0).
  prefs: []
  type: TYPE_NORMAL
- en: Document Classification Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following demonstration we will use MultinomialNB to solve a document
    classification task. The data set we are going to use is the [20 newsgroups dataset](https://scikit-learn.org/stable/datasets/real_world.html#newsgroups-dataset),
    which consists of 18,846 newsgroups posts, partitioned (nearly) evenly across
    20 different topics. This data set has been widely used in research of text applications
    in machine learning, including document classification and clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the Data Set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can use the function [fetch_20newsgroups()](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html)
    in Scikit-Learn to download the text documents with their labels. You can either
    download all the documents as one group, or download the training set and the
    test set separately (using the *subset* parameter). The split between the training
    and the test sets is based upon messages posted before or after a specific date.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, the text documents contain some metadata such as headers (e.g.,
    the date of the post), footers (signatures) and quotes to other posts. Since these
    features are not relevant for the text classification task, we will strip them
    out by using the *remove* parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note that the first time you call this function it may take a few minutes to
    download all the documents, after which they will be cached locally in the folder
    ~/scikit_learn_data*.*
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the function is a dictionary that contains the following attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '*data* — the set of documents'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*target* — the target labels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*target_names* — the names of the document categories'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s store the documents and their labels in proper variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Data Exploration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s do some basic exploration of the data. The number of documents we have
    in the training and the test sets is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: A simple calculation shows that 60% of the documents belong to the training
    set, and 40% to the test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s print the list of categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As evident, some of the categories are closely related to each other (e.g.,
    *comp.sys.mac.hardware* and *comp.sys.ibm.pc.hardware*), while others are highly
    uncorrelated (e.g., *sci.electronics* and *soc.religion.christian*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s examine one of the documents in the training set (e.g., the
    first one):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Unsurprisingly, the label of this document is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Converting Text to Vectors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to feed text documents into machine learning models, we first need
    to convert them into vectors of numerical values (i.e., **vectorize** the text).
    This process typically involves preprocessing and cleaning of the text, and then
    choosing a suitable numerical representation for the words in the text.
  prefs: []
  type: TYPE_NORMAL
- en: '**Text preprocessing** consists of various steps, amongst which the most common
    ones are:'
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning and normalizing the text. This includes removing punctuation marks
    and special characters, and converting the text into lower-case.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Text tokenization, i.e., splitting the text into individual words or terms.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Removal of stop words. Stop words are a set of commonly used words in a given
    language. For example, stop words in English include words like “the”, “a”, “is”,
    “and”. These words are usually filtered out since they do not carry useful information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stemming or lemmatization. **Stemming** reduces the word to its lexical root
    by removing or replacing its suffix, while **lemmatization** reduces the word
    to its canonical form (lemma) and also takes into account the context of the word
    (its part-of-speech). For example, the word *computers* has the lemma *computer*,
    but its lexical root is *comput*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following example demonstrates these steps on a given sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b8bdeff878e59c4f6ae26385f2338136.png)'
  prefs: []
  type: TYPE_IMG
- en: Text preprocessing example
  prefs: []
  type: TYPE_NORMAL
- en: 'After cleaning the text, we need to choose how to vectorize it into a numerical
    vector. The most common approaches are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bag-of-words (BOW) model**. In this model, each document is represented by
    a word counts vector (similar to the one we have used in the spam filter example).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[**TF-IDF**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (Term Frequency
    times Inverse Document Frequency) measures how relevant a word is to a document
    by multiplying two metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (a) TF (Term Frequency) — how many times the word appears in the document.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (b) IDF (Inverse Document Frequency) — the inverse of the frequency in which
    the word appears in documents across the entire corpus.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The idea is to decrease the weight of words that occur frequently in the corpus,
    while increasing the weight of words that occur rarely (and thus are more indicative
    of the document’s category).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Word embeddings**.In this approach,words are mapped into real-valued vectors
    in such a way that words with similar meaning have close representation in the
    vector space. This model is typically used in deep learning and will be discussed
    in a future post.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scikit-Learn provides the following two transformers, which support both text
    preprocessing and vectorization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)
    uses the bag-of-words model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[TfIdfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)
    uses the TF-IDF representation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Important hyperparameters of these transformers include:'
  prefs: []
  type: TYPE_NORMAL
- en: '*lowercase* — whether to convert all the characters to lowercase before tokenizing
    (defaults to True).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*token_pattern* — the regular expression used to define what is a token (the
    default regex selects tokens of two or more alphanumeric characters).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*stop_words* — if ‘english’, uses a built-in stop word list for English. If
    None (the default), no stop words will be used. You can also provide your own
    custom stop words list.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*max_features* — if not None, build a vocabulary that includes only the top
    *max_features* with the highest term frequency across the training corpus. Otherwise,
    all the features are used (this is the default).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that these transformers do not provide advanced preprocessing techniques
    such as stemming or lemmatization. To apply these techniques, you will have to
    use other libraries such as [NLTK](https://www.nltk.org/) (Natural Language Toolkit)
    or [spaCy](https://spacy.io/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Since Naive Bayes models are known to work better with TF-IDF representations,
    we will use the TfidfVectorizer to convert the documents in the training set into
    TF-IDF vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The shape of the extracted TF-IDF vectors is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'That is, there are 101,322 unique tokens in the vocabulary of the corpus. We
    can examine these tokens by calling the method *get_feature_names_out*() of the
    vectorizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Evidently, there was no automatic spell checker back in the 90s :)
  prefs: []
  type: TYPE_NORMAL
- en: 'The TF-IDF vectors are very sparse, with an average of 67 non-zero components
    out of more than 100,000:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s also vectorize the documents in the test set (note that on the test set
    we call the *transform* method instead of *fit_transform*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Building the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s now build a multinomial Naive Bayes classifier and fit it to the training
    set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Note that we need to set the smoothing parameter *α* to a very small number,
    since the TF-IDF values are scaled to be between 0 and 1, so the default *α* =
    1 would cause a dramatic shift of the values.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, let’s evaluate the model on both the training and the test sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'The accuracy and F1 score of the model on the training set are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'And the accuracy and F1 score on the test set are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The scores on the test set are relatively low compared to the training set.
    To investigate where the errors come from, let’s plot the confusion matrix of
    the test documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/342b469405408ed5654389d78022b4a7.png)'
  prefs: []
  type: TYPE_IMG
- en: The confusion matrix on the test set
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, most of the confusions occur between highly correlated topics,
    for example:'
  prefs: []
  type: TYPE_NORMAL
- en: 74 confusions between topic 0 (alt.atheism) and topic 15 (soc.religion.christian)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 92 confusions between topic 18 (talk.politics.misc) and topic 16 (talk.politics.guns)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 89 confusions between topic 19 (talk.religion.misc) and topic 15 (soc.religion.christian)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In light of these findings, it seems that the Naive Bayes classifier did a pretty
    good job. Let’s examine how it compares to other standard classification algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will benchmark the Naive Bayes model against four other classifiers: [logistic
    regression](https://medium.com/p/3e502686f0ae), [KNN](https://medium.com/@roiyeho/k-nearest-neighbors-knn-a-comprehensive-guide-7add717806ad),
    [random forest](https://medium.com/@roiyeho/random-forests-98892261dc49) and [AdaBoost](https://medium.com/@roiyeho/adaboost-illustrated-3084183a2086).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first write a function that gets a set of classifiers and evaluates them
    on the given data set and also measures their training time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now call this function with our five classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The output we get is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s plot the accuracy and F1 scores of the classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/4e6303ae117bbcd728562b00aac15a5f.png)'
  prefs: []
  type: TYPE_IMG
- en: Accuracy scores on the test set
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/22f76ec2b2c341d9883bd03363093827.png)'
  prefs: []
  type: TYPE_IMG
- en: F1 scores on the test set
  prefs: []
  type: TYPE_NORMAL
- en: Multinomial NB achieves both the highest accuracy and F1 scores. Notice that
    the classifiers have been used with their default parameters without any tuning.
    For a more fair comparison, the algorithms should be compared after fine tuning
    their hyperparameters. In addition, some algorithms such as KNN suffer from the
    [curse of dimensionality](https://medium.com/@roiyeho/what-is-the-curse-of-dimensionality-b9b4b81a25c5),
    and dimensionality reduction is required in order to make them work well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s also plot the training times of the classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/2cb15af2b0ab15b76fba4c0b93564ba3.png)'
  prefs: []
  type: TYPE_IMG
- en: Training time of the different classifiers
  prefs: []
  type: TYPE_NORMAL
- en: The training of Multinomial NB is so fast that we cannot even see its time in
    the graph! By examining the function’s output from above, we can see that its
    training time is only 0.064 seconds. Note that the training of KNN is also very
    fast (since no model is actually built), but its prediction time (not shown) is
    very slow.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, Multinomial NB has shown superiority over the other classifiers
    in all the examined criteria.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the Most Informative Features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Naive Bayes model also allows us to get the most informative features of
    each class, i.e., the features with the highest likelihood *P*(*xⱼ*|*y*).
  prefs: []
  type: TYPE_NORMAL
- en: The MultinomialNB class has an attribute named *feature_log_prob_*, which provides
    the log probability of the features for each class in a matrix of shape (*n_classes*,
    *n_features*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Using this attribute, let’s write a function to find the 10 most informative
    features (tokens) in each category:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The output we get is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Most of the words seem to be strongly correlated with their corresponding category.
    However, there are a few generic words such as “just” and “does” that do not provide
    valuable information. This suggests that our model may be improved by having a
    better stop-words list. Indeed, Scikit-Learn recommends not to use its own default
    list, quoting from its documentation: “There are several known issues with ‘english’
    and you should consider an alternative”. 😲'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s summarize the pros and cons of Naive Bayes as compared to other classification
    models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pros**:'
  prefs: []
  type: TYPE_NORMAL
- en: Extremely fast both in training and prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provides class probability estimates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be used both for binary and multi-class classification problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requires a small amount of training data to estimate its parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Highly interpretable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Highly scalable (the number of parameters is linear in the number of features)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Works well with high-dimensional data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robust to noise (the noisy samples are averaged out when estimating the conditional
    probabilities)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can deal with missing values (the missing values are ignored when computing
    the likelihoods of the features)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No hyperparameters to tune (except for the smoothing parameter, which is rarely
    changed)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons**:'
  prefs: []
  type: TYPE_NORMAL
- en: Relies on the Naive Bayes assumption which does not hold in many real-world
    domains
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Correlation between the features can degrade the performance of the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generally outperformed by more complex models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The zero frequency problem: if a categorical feature has a category that was
    not observed in the training set, the model will assign a zero probability to
    its occurrence. Smoothing alleviates this problem but does not solve it completely.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cannot handle continuous attributes without discretization or making assumptions
    on their distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be used only for classification tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Final Notes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is the longest article I have written on Medium so far. I hope you enjoyed
    reading it at least as much as I enjoyed writing it. Let me know in the comments
    if something was not clear.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the code examples of this article on my github: [https://github.com/roiyeho/medium/tree/main/naive_bayes](https://github.com/roiyeho/medium/tree/main/naive_bayes)'
  prefs: []
  type: TYPE_NORMAL
- en: All images unless otherwise noted are by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'The 20 newsgroups data set info:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Citation:** Mitchell, Tom (1999). Twenty Newsgroups. UCI Machine Learning
    Repository. [https://doi.org/10.24432/C5C323.](https://doi.org/10.24432/C5C323.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**License:** Creative Commons CC BY 4.0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
