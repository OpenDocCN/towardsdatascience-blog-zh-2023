["```py\n# Create and activate a conda environment\nconda create -n hf_llm_chatbot python=3.9\nconda activate hf_llm_chatbot\n\n# Install the necessary libraries\npip install streamlit streamlit-chat \"fastapi[all]\" \"transformers[torch]\"\n```", "```py\n# %%writefile utils.py\nfrom datetime import datetime\n\nimport pandas as pd\nimport streamlit as st\nfrom streamlit_chat import message\n\ndef clear_conversation():\n    \"\"\"Clear the conversation history.\"\"\"\n    if (\n        st.button(\"ðŸ§¹ Clear conversation\", use_container_width=True)\n        or \"conversation_history\" not in st.session_state\n    ):\n        st.session_state.conversation_history = {\n            \"past_user_inputs\": [],\n            \"generated_responses\": [],\n        }\n        st.session_state.user_input = \"\"\n        st.session_state.interleaved_conversation = []\n\ndef display_conversation(conversation_history):\n    \"\"\"Display the conversation history in reverse chronology.\"\"\"\n\n    st.session_state.interleaved_conversation = []\n\n    for idx, (human_text, ai_text) in enumerate(\n        zip(\n            reversed(conversation_history[\"past_user_inputs\"]),\n            reversed(conversation_history[\"generated_responses\"]),\n        )\n    ):\n        # Display the messages on the frontend\n        message(ai_text, is_user=False, key=f\"ai_{idx}\")\n        message(human_text, is_user=True, key=f\"human_{idx}\")\n\n        # Store the messages in a list for download\n        st.session_state.interleaved_conversation.append([False, ai_text])\n        st.session_state.interleaved_conversation.append([True, human_text])\n\ndef download_conversation():\n    \"\"\"Download the conversation history as a CSV file.\"\"\"\n    conversation_df = pd.DataFrame(\n        reversed(st.session_state.interleaved_conversation), columns=[\"is_user\", \"text\"]\n    )\n    csv = conversation_df.to_csv(index=False)\n\n    st.download_button(\n        label=\"ðŸ’¾ Download conversation\",\n        data=csv,\n        file_name=f\"conversation_{datetime.now().strftime('%Y%m%d%H%M%S')}.csv\",\n        mime=\"text/csv\",\n        use_container_width=True,\n    )\n```", "```py\n# %%writefile monolith.py\nimport streamlit as st\nimport utils\nfrom transformers import Conversation, pipeline\n\n# https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/pipelines#transformers.Conversation\nchatbot = pipeline(\n    \"conversational\", model=\"facebook/blenderbot-400M-distill\", max_length=1000\n)\n\n@st.cache_data()\ndef monolith_llm_response(user_input):\n    \"\"\"Run the user input through the LLM and return the response.\"\"\"\n    # Step 1: Initialize the conversation history\n    conversation = Conversation(**st.session_state.conversation_history)\n\n    # Step 2: Add the latest user input\n    conversation.add_user_input(user_input)\n\n    # Step 3: Generate a response\n    _ = chatbot(conversation)\n\n    # User input and generated response are automatically added to the conversation history\n    # print(st.session_state.conversation_history)\n\ndef main():\n    st.title(\"Monolithic ChatBot App\")\n\n    col1, col2 = st.columns(2)\n    with col1:\n        utils.clear_conversation()\n\n    # Get user input\n    if user_input := st.text_input(\"Ask your question ðŸ‘‡\", key=\"user_input\"):\n        monolith_llm_response(user_input)\n\n    # Display the entire conversation on the frontend\n    utils.display_conversation(st.session_state.conversation_history)\n\n    # Download conversation code runs last to ensure the latest messages are captured\n    with col2:\n        utils.download_conversation()\n\nif __name__ == \"__main__\":\n    main()\n```", "```py\n# %%writefile backend.py\nfrom typing import Optional\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel, Field\nfrom transformers import Conversation, pipeline\n\napp = FastAPI()\n\n# https://huggingface.co/docs/transformers/v4.28.1/en/main_classes/pipelines#transformers.Conversation\nchatbot = pipeline(\n    \"conversational\", model=\"facebook/blenderbot-400M-distill\", max_length=1000\n)\n\nclass ConversationHistory(BaseModel):\n    past_user_inputs: Optional[list[str]] = []\n    generated_responses: Optional[list[str]] = []\n    user_input: str = Field(example=\"Hello, how are you?\")\n\n@app.get(\"/\")\nasync def health_check():\n    return {\"status\": \"OK!\"}\n\n@app.post(\"/chat\")\nasync def llm_response(history: ConversationHistory) -> str:\n    # Step 0: Receive the API payload as a dictionary\n    history = history.dict()\n\n    # Step 1: Initialize the conversation history\n    conversation = Conversation(\n        past_user_inputs=history[\"past_user_inputs\"],\n        generated_responses=history[\"generated_responses\"],\n    )\n\n    # Step 2: Add the latest user input\n    conversation.add_user_input(history[\"user_input\"])\n\n    # Step 3: Generate a response\n    _ = chatbot(conversation)\n\n    # Step 4: Return the last generated result to the frontend\n    return conversation.generated_responses[-1]\n```", "```py\n# %%writefile frontend.py\nimport requests\nimport streamlit as st\nimport utils\n\n# Replace with the URL of your backend\napp_url = \"http://127.0.0.1:8000/chat\"\n\n@st.cache_data()\ndef microservice_llm_response(user_input):\n    \"\"\"Send the user input to the LLM API and return the response.\"\"\"\n    payload = st.session_state.conversation_history\n    payload[\"user_input\"] = user_input\n\n    response = requests.post(app_url, json=payload)\n\n    # Manually add the user input and generated response to the conversation history\n    st.session_state.conversation_history[\"past_user_inputs\"].append(user_input)\n    st.session_state.conversation_history[\"generated_responses\"].append(response.json())\n\ndef main():\n    st.title(\"Microservices ChatBot App\")\n\n    col1, col2 = st.columns(2)\n    with col1:\n        utils.clear_conversation()\n\n    # Get user input\n    if user_input := st.text_input(\"Ask your question ðŸ‘‡\", key=\"user_input\"):\n        microservice_llm_response(user_input)\n\n    # Display the entire conversation on the frontend\n    utils.display_conversation(st.session_state.conversation_history)\n\n    # Download conversation code runs last to ensure the latest messages are captured\n    with col2:\n        utils.download_conversation()\n\nif __name__ == \"__main__\":\n    main()\n```"]