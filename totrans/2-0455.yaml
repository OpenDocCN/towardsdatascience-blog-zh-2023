- en: Building PCA from the Ground Up
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从基础构建PCA
- en: 原文：[https://towardsdatascience.com/building-pca-from-the-ground-up-434ac88b03ef](https://towardsdatascience.com/building-pca-from-the-ground-up-434ac88b03ef)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/building-pca-from-the-ground-up-434ac88b03ef](https://towardsdatascience.com/building-pca-from-the-ground-up-434ac88b03ef)
- en: Supercharge your understanding of Principal Component Analysis with a step-by-step
    derivation
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过一步步的推导，超级提升你对主成分分析的理解
- en: '[](https://harrisonfhoffman.medium.com/?source=post_page-----434ac88b03ef--------------------------------)[![Harrison
    Hoffman](../Images/5eaa3e2bd0507297eb6c4a7efcf06324.png)](https://harrisonfhoffman.medium.com/?source=post_page-----434ac88b03ef--------------------------------)[](https://towardsdatascience.com/?source=post_page-----434ac88b03ef--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----434ac88b03ef--------------------------------)
    [Harrison Hoffman](https://harrisonfhoffman.medium.com/?source=post_page-----434ac88b03ef--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://harrisonfhoffman.medium.com/?source=post_page-----434ac88b03ef--------------------------------)[![哈里森·霍夫曼](../Images/5eaa3e2bd0507297eb6c4a7efcf06324.png)](https://harrisonfhoffman.medium.com/?source=post_page-----434ac88b03ef--------------------------------)[](https://towardsdatascience.com/?source=post_page-----434ac88b03ef--------------------------------)[![数据科学前沿](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----434ac88b03ef--------------------------------)
    [哈里森·霍夫曼](https://harrisonfhoffman.medium.com/?source=post_page-----434ac88b03ef--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----434ac88b03ef--------------------------------)
    ·12 min read·Aug 7, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[数据科学前沿](https://towardsdatascience.com/?source=post_page-----434ac88b03ef--------------------------------)
    ·12分钟阅读·2023年8月7日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/09f3176416e889b15a24ea18524b299a.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09f3176416e889b15a24ea18524b299a.png)'
- en: Hot air balloons. Image by Author.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 热气球。图像由作者提供。
- en: Principal Component Analysis (PCA) is an [old](https://en.wikipedia.org/wiki/Principal_component_analysis#History:~:text=PCA%20was%20invented%20in%201901)
    technique commonly used for dimensionality reduction. Despite being a well-known
    topic among data scientists, the derivation of PCA is often overlooked, leaving
    behind valuable insights about the nature of data and the relationship between
    calculus, statistics, and linear algebra.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）是一种常用于降维的[旧技术](https://en.wikipedia.org/wiki/Principal_component_analysis#History:~:text=PCA%20was%20invented%20in%201901)。尽管在数据科学家中这是一个广为人知的话题，但PCA的推导通常被忽视，留下了关于数据本质和微积分、统计学以及线性代数之间关系的宝贵见解。
- en: In this article, we will derive PCA through a thought experiment, beginning
    with two dimensions and extending to arbitrary dimensions. As we progress through
    each derivation, we will see the harmonious interplay of seemingly distinct branches
    of mathematics, culminating in an elegant coordinate transformation. This derivation
    will unravel the mechanics of PCA and reveal the captivating interconnectedness
    of mathematical concepts. Let’s embark on this enlightening exploration of PCA
    and its beauty.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将通过思想实验推导PCA，从二维开始，扩展到任意维度。随着我们逐步推进每一项推导，我们将看到看似不同的数学分支之间的和谐互动，最终达到优雅的坐标变换。这一推导将揭示PCA的机制，并揭示数学概念的迷人相互联系。让我们开始这段启发性的PCA探索之旅。
- en: Warming Up in Two Dimensions
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在二维中热身
- en: As humans living in a three-dimensional world, we generally grasp two-dimensional
    concepts, and this is where we will begin in this article. Starting in two dimensions
    will simplify our first thought experiment and allow us to better understand the
    nature of the problem.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 作为生活在三维世界中的人类，我们通常理解二维概念，这也是本文的起点。从二维开始将简化我们的首次思想实验，并使我们更好地理解问题的本质。
- en: Theory
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理论
- en: 'We have a dataset that looks something like this (note that each feature should
    be scaled to have a mean of 0 and variance of 1):'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个数据集，长得像这样（请注意，每个特征应该被缩放到均值为0和方差为1）：
- en: '![](../Images/364f66e80e7272963610ae918440bb91.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/364f66e80e7272963610ae918440bb91.png)'
- en: (1) Correlated Data. Image by Author.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 相关数据。图像由作者提供。
- en: We immediately notice this data lies in a coordinate system described by ***x1***
    and ***x2***, and these variables are correlated. *Our goal is to find a new coordinate
    system informed by the covariance structure of the data.* In particular, the first
    basis vector in the coordinate system should explain the majority of the variance
    when projecting the original data onto it.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们立即注意到这些数据位于由***x1***和***x2***描述的坐标系中，这些变量是相关的。*我们的目标是找到一个由数据的协方差结构决定的新坐标系。*
    特别是，坐标系中的第一个基向量应解释将原始数据投影到其上的大部分方差。
- en: Our first order of business is to find a vector such that when we project the
    original data onto the vector, the maximum amount of variance is preserved. In
    other words, the ideal vector points in the direction of maximal variance, as
    defined by the data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的首要任务是找到一个向量，使得当我们将原始数据投影到这个向量上时，保留最大量的方差。换句话说，理想的向量指向方差最大化的方向，正如数据所定义的那样。
- en: 'This vector can be defined by the angle it makes with the x-axis in the counter-clockwise
    direction:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这个向量可以通过它与x轴逆时针方向所成的角度来定义：
- en: '![](../Images/3dfa6fba3e6d1cb3026078a984be9ac7.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3dfa6fba3e6d1cb3026078a984be9ac7.png)'
- en: (2) Searching for the direction of maximal variance by rotating a vector. Image
    by Author.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 通过旋转向量来寻找最大方差的方向。图像由作者提供。
- en: In the animation above, we define a vector by the angle it makes with the x-
    axis, and we can see the direction the vector points at each angle between 0 and
    180 degrees. Visually, we can see that a ***θ*** value near 45 degrees points
    in the direction of maximal variability in the data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的动画中，我们通过与x轴的角度来定义一个向量，我们可以看到向量在0到180度之间的每个角度指向的方向。从视觉上看，我们可以看到一个接近45度的***θ***值指向数据方差的最大方向。
- en: 'To restate our objective, we want to find the angle the causes the vector to
    point in the direction of maximal variance. Mathematically, we want to find ***θ***
    that maximizes this equation:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了重新阐述我们的目标，我们希望找到一个角度，使得向量指向方差最大化的方向。从数学上讲，我们希望找到一个最大化这个方程的***θ***：
- en: '![](../Images/803d660009c0ee2e4cdb6c686edbedab.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/803d660009c0ee2e4cdb6c686edbedab.png)'
- en: (3) The best θ will maximize this objective function. Image by Author.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 最佳的θ将最大化这个目标函数。图像由作者提供。
- en: Here, ***N*** is the number of observations in the data. We project each observation
    onto the axis defined by ***[cos(θ) sin(θ)]***, which is a unit vector defined
    by the angle ***θ***, and square the result.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，***N***是数据中的观察次数。我们将每个观察值投影到由***[cos(θ) sin(θ)]***定义的轴上，这是一个由角度***θ***定义的单位向量，并对结果进行平方。
- en: 'As we vary ***θ***, this equation gives of the variance of the data when projected
    onto the axis defined by ***θ***. Let’s compute the dot product inside the square
    and rewrite this expression to make it easier to work with:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们改变***θ***时，这个方程给出了当数据投影到由***θ***定义的轴上的方差。让我们计算方程中的点积并重写这个表达式，使其更易于处理：
- en: '![](../Images/d922b2bb66f5a070baa481bae0416302.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d922b2bb66f5a070baa481bae0416302.png)'
- en: (4) The variance equation rewritten. Image by Author.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: (4) 重写后的方差方程。图像由作者提供。
- en: 'Luckily for us, this is a convex function, and we can maximize it by computing
    it’s derivative and setting it equal to 0\. We will also need to compute the second
    derivative of the variance equation to know whether we’ve found a minima or maxima.
    The first and second derivatives of ***var(θ)*** look like this:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，这是一个凸函数，我们可以通过计算它的导数并将其设为0来最大化它。我们还需要计算方差方程的二阶导数，以确定我们是否找到了最小值或最大值。***var(θ)***的一阶和二阶导数如下所示：
- en: '![](../Images/64a2408a12fe159b516ab05bff0fd001.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/64a2408a12fe159b516ab05bff0fd001.png)'
- en: (5) The first and second derivative of var(θ). Image by Author.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: (5) var(θ)的一阶和二阶导数。图像由作者提供。
- en: 'Next, we can set the first derivative equal to 0, and rearrange the equation
    to isolate ***θ***:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以将一阶导数设为0，并重排方程以隔离***θ***：
- en: '![](../Images/28b55012957951891efdc1b5fd90a4dc.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/28b55012957951891efdc1b5fd90a4dc.png)'
- en: (6) The first derivative of var(θ) set equal to 0 and rearranged. Image by Author.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: (6) 将var(θ)的一阶导数设为0并重排。图像由作者提供。
- en: 'Finally, using a common trig identity and some algebra, we get a closed form
    solution for the ***θ*** that minimizes or maximizes ***var(θ)***:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，利用常见的三角恒等式和一些代数，我们得到了一个使***θ***最小化或最大化***var(θ)***的封闭形式解：
- en: '![](../Images/9643601112402e5343b2cdaac7ca86b5.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9643601112402e5343b2cdaac7ca86b5.png)'
- en: (7) The equation to compute θ that finds the direction of maximal or minimal
    variance. Image by Author.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: (7) 计算θ的方程，用于找到最大或最小方差的方向。图像由作者提供。
- en: 'Fascinatingly, this equation is all we need to perform PCA in two dimensions.
    The second derivative will tell us whether ***θ*** corresponds to a local minima
    or maxima. Because there is only one other principal component, it must be defined
    by shifting ***θ*** by 90 degrees. Therefore, the two principal component angles
    are given by:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，这个方程是我们在二维空间进行 PCA 所需的全部。二阶导数将告诉我们***θ***是否对应于局部最小值或最大值。由于只有另一个主成分，它必须通过将***θ***偏移
    90 度来定义。因此，两个主成分角度是：
- en: '![](../Images/ad642bb49557fa023e74a1fe79fd6874.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad642bb49557fa023e74a1fe79fd6874.png)'
- en: (8) The angles that define the principal components in two dimensions. Image
    by Author.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: (8) 定义二维主成分的角度。图像由作者提供。
- en: As mentioned before, we can use the second derivative of ***var(θ)*** to figure
    out which ***θ*** belongs to principal component 1 (the direction of maximal variance)
    and which ***θ*** belongs to principal component 2 (the direction of minimal variance).
    Alternatively, we can plug both ***θs*** into ***var(θ)*** and see which one results
    in a higher value.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们可以使用***var(θ)***的二阶导数来确定哪个***θ***属于主成分 1（最大方差的方向），哪个***θ***属于主成分 2（最小方差的方向）。另外，我们也可以将两个***θs***代入***var(θ)***中，看看哪个结果更高。
- en: 'Once we know which ***θ*** corresponds to each principal component, we plug
    each one into the trigonometric equation for a unit vector in two dimensions (***[cos(θ)
    sin(θ)])***. Explicitly, we do the following:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们知道哪个***θ***对应于每个主成分，我们就将每一个代入二维单位向量的三角函数方程（***[cos(θ) sin(θ)]***）。具体来说，我们做如下操作：
- en: '![](../Images/ca1eafaf1bf4f3dd1282647a2286fe02.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ca1eafaf1bf4f3dd1282647a2286fe02.png)'
- en: (9) Determining the two principal components from the θs that maximize or minimize
    var(θ). Image by Author.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: (9) 从最大化或最小化 var(θ) 的 θs 确定两个主成分。图像由作者提供。
- en: That’s it — ***pc1*** and ***pc2*** are the principal component vectors. By
    thinking through the objective of principal component analysis, we were able to
    derive the principal components from scratch in two dimensions. To convince ourselves
    this is correct, let’s write some Python code to implement our strategy.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样——***pc1***和***pc2***是主成分向量。通过思考主成分分析的目标，我们能够在二维空间中从零开始推导出主成分。为了验证这个结果是否正确，让我们编写一些
    Python 代码来实现我们的策略。
- en: Code
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代码
- en: 'The first function finds one of the principal angles derived from the derivative
    of the variance equation in figure (7). Because this is a closed-form equation,
    the implementation is straight forward:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个函数找到从方差方程的导数中推导出的主角度之一。由于这是一个封闭形式的方程，实现起来非常简单：
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Depending on the nature of the variance equation, `find_principal_angle()`
    recovers one of the principal angles, and the other principal angle is 90 degrees
    away. To determine which principal angle `find_principal_angle()` returns, we
    can use the second derivative, or hessian, of the variance equation:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 根据方差方程的性质，`find_principal_angle()` 恢复一个主角度，另一个主角度则相差 90 度。为了确定 `find_principal_angle()`
    返回的是哪个主角度，我们可以使用方差方程的二阶导数或 Hessian 矩阵：
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The logic for this function comes directly from figure (5). The last function
    we need determines the two principal components from `find_principal_angle()`
    and `compute_pca_cost_hessian()`:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数的逻辑直接来源于图 (5)。我们需要的最后一个函数是从 `find_principal_angle()` 和 `compute_pca_cost_hessian()`
    确定两个主成分：
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In `find_principal_components_2d()`, `theta0`is one of the principal angles
    and `theta0_hessian`is the second derivative of the variance equation. Because
    `theta0`is an extrema of the variance equation, `theta0_hessian`tells us whether
    `theta0`is a minimum or maximum. In particular, if `theta0_hessian`is positive,
    `theta0`must be a minimum and correspond to the second principal component. Otherwise,
    `theta0_hessian`is negative and `theta0`is a maximum corresponding to the first
    principal component.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `find_principal_components_2d()` 中，`theta0` 是其中一个主角度，`theta0_hessian` 是方差方程的二阶导数。因为
    `theta0` 是方差方程的极值点，`theta0_hessian` 告诉我们 `theta0` 是最小值还是最大值。特别地，如果 `theta0_hessian`
    为正，则 `theta0` 必定是最小值，对应于第二主成分。否则，如果 `theta0_hessian` 为负，`theta0` 则是最大值，对应于第一主成分。
- en: 'To verify `find_principal_components_2d()`does what we want, let’s find the
    principal components for a two dimensional dataset and compare them to the results
    of the [PCA implementation in scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html).
    Here’s the code:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证`find_principal_components_2d()`是否做了我们想要的事情，让我们找到一个二维数据集的主成分，并将其与[scikit-learn中的PCA实现](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)的结果进行比较。以下是代码：
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This code creates two-dimensional correlated data, normalizes it, and finds
    the principal components using our custom logic and the [scikit-learn](https://scikit-learn.org/stable/)
    PCA implementation. We can see that we’ve achieved the exact same results as scikit-learn.
    The resulting principal components look like this:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码创建了二维相关数据，进行了标准化，并使用我们的自定义逻辑和[scikit-learn](https://scikit-learn.org/stable/)
    PCA实现找到了主成分。我们可以看到，我们得到了与scikit-learn完全相同的结果。得到的主成分如下：
- en: '![](../Images/22c10018692acb9165469ef7a78b0667.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22c10018692acb9165469ef7a78b0667.png)'
- en: (10) Our two-dimensional PCA method and the scikit-learn implementation are
    exact matches. Image by Author.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: (10) 我们的二维PCA方法与scikit-learn实现完全匹配。图像来源：作者。
- en: As expected, the two sets of principal components perfectly overlap. However,
    the issue with our method is that it doesn’t generalize beyond two dimensions.
    In the next section, we will derive an analogous results for arbitrary dimensions.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，两组主成分完全重叠。然而，我们方法的问题是它无法推广到二维以外的情况。在下一节中，我们将推导出任意维度的类似结果。
- en: Deriving a General Result
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推导一般结果
- en: In two dimensions, we derived a closed form equation to find the principal components
    using the definition of variance, calculus, and a bit of trigonometry. Unfortunately,
    this approach will quickly become infeasible for datasets beyond two dimensions.
    For this, we will have to rely on the most powerful branch of mathematics for
    computation — linear algebra.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在二维中，我们利用方差定义、微积分和一点三角学推导出了一个封闭形式的方程来找到主成分。不幸的是，这种方法对于超过二维的数据集很快会变得不可行。为此，我们必须依赖于数学中最强大的分支——线性代数来进行计算。
- en: Theory
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理论
- en: The derivation of PCA in higher dimensions is analogous to the two-dimensional
    case in many ways. The fundamental difference is that we have to leverage some
    tools from linear algebra to help us account for all possible angles that the
    principal components can make with the origin of the data.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: PCA在更高维度的推导在许多方面类似于二维情况。根本的区别在于，我们必须利用一些线性代数工具来帮助我们考虑主成分与数据原点可能形成的所有角度。
- en: 'To start, assume our data set has ***d*** features and ***n*** observations.
    As before, each feature should be scaled to have a mean of 0 and variance of 1\.
    We will arrange the data in a matrix as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，假设我们的数据集有***d***个特征和***n***个观察值。如前所述，每个特征应缩放到均值为0且方差为1。我们将数据排列成如下矩阵：
- en: '![](../Images/2d635700dde60aea98bdce43f6c04407.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d635700dde60aea98bdce43f6c04407.png)'
- en: (11) The data matrix for PCA. Image by Author.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: (11) PCA的数据矩阵。图像来源：作者。
- en: 'As before, our goal is to find a unit vector, ***p***, that points in the direction
    of maximum variance of the data. To do this we first need to state two useful
    facts. The first is that the [covariance matrix](https://en.wikipedia.org/wiki/Covariance_matrix)
    of our data, the analog of variance in higher dimensions, can be found my a multiplying
    X with its transpose:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们的目标是找到一个单位向量，***p***，它指向数据方差最大方向。为此，我们首先需要陈述两个有用的事实。第一个是我们数据的[协方差矩阵](https://en.wikipedia.org/wiki/Covariance_matrix)，在更高维度中的方差的类似物，可以通过将X与其转置相乘找到：
- en: '![](../Images/be86b90f50486e6f8c0081705f66ba3a.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be86b90f50486e6f8c0081705f66ba3a.png)'
- en: (12) The equation to compute the covariance matrix of X. Image by Author.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: (12) 计算X的协方差矩阵的方程。图像来源：作者。
- en: 'While this is useful, we’re not interested in the variance of the data itself.
    **We want to know the variance of the data when projected onto a new axis.** To
    do this, we recall a result about the variance of a scalar quantity. Namely, when
    a scalar is multiplied by a constant, the resulting variance is that constant
    squared multiplied by the original variance:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这很有用，我们并不关心数据本身的方差。**我们想知道数据在投影到新轴上的方差。** 为此，我们回顾一个关于标量量的方差的结果。即，当标量乘以常数时，得到的方差是该常数的平方乘以原始方差：
- en: '![](../Images/9e81fad8387521e3c4a282cc49468c30.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e81fad8387521e3c4a282cc49468c30.png)'
- en: (13) The variance of a scalar multiplied by a constant. Image by Author.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: (13) 乘以常数的标量方差。图片由作者提供。
- en: 'Analogously, when we multiply a vector with our data matrix (i.e. when we project
    a vector onto the matrix), the resulting variance can be computed as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，当我们用数据矩阵乘以一个向量（即，当我们将一个向量投影到矩阵上）时，得到的方差可以按如下方式计算：
- en: '![](../Images/f88925c03278849f16cf1af146fa5b0e.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f88925c03278849f16cf1af146fa5b0e.png)'
- en: (14) The variance of a vector projected onto the data matrix. Image by Author.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: (14) 向量投影到数据矩阵上的方差。图片由作者提供。
- en: 'This gives us everything we need to define our problem for PCA. That is, we
    want to find the vector ***p*** (the principal component) that maximizes the following
    equation:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这给了我们定义 PCA 问题所需的一切。也就是说，我们想找到向量***p***（主成分），使下列方程最大化：
- en: '![](../Images/b68008d831be70a929522701e5ce312e.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b68008d831be70a929522701e5ce312e.png)'
- en: (15) The PCA problem statement in **d** dimensions is to find the vector, **p**,
    that maximizes the variance when projected onto the data. Image by Author.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: (15) 在**d**维度下，PCA 问题的陈述是找到向量**p**，使其在投影到数据时的方差最大。图片由作者提供。
- en: The PCA problem statement in ***d*** dimensions is to find the vector, ***p***,
    that maximizes the variance when projected onto the data. We also stipulate that
    ***p*** is a unit vector, making this a constrained optimization problem. Therefore,
    just as in the two-dimensional case, we need calculus.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ***d*** 维度下，PCA 问题的陈述是找到向量***p***，使其在投影到数据时的方差最大。我们还规定***p*** 是一个单位向量，这使得这是一个约束优化问题。因此，就像在二维情况下一样，我们需要使用微积分。
- en: 'To solve this, we can leverage [lagrange multipliers](https://en.wikipedia.org/wiki/Lagrange_multiplier),
    which allow us to minimize the objective while still meeting the specified constraints.
    The Lagrangian expression for this problem is given by:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们可以利用 [拉格朗日乘子](https://en.wikipedia.org/wiki/Lagrange_multiplier)，它允许我们在满足指定约束的同时最小化目标函数。这个问题的拉格朗日表达式如下：
- en: '![](../Images/5f51d128d65caaed839e0a5674ac90d2.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5f51d128d65caaed839e0a5674ac90d2.png)'
- en: (16) The PCA Lagrangian expression. Image by Author.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: (16) PCA 拉格朗日表达式。图片由作者提供。
- en: 'To optimize this, we take the derivative and set it equal to 0:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了优化这个问题，我们对其求导并将其设为 0：
- en: '![](../Images/48c13934f459ab096b90863c3e67ce64.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/48c13934f459ab096b90863c3e67ce64.png)'
- en: (17) The derivative of the PCA Lagrangian expression. Image by Author.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: (17) PCA 拉格朗日表达式的导数。图片由作者提供。
- en: 'Some rearranging gives us the following expression — a very familiar result
    from linear algebra:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一些重新排列给出了以下表达式——这是线性代数中一个非常熟悉的结果：
- en: '![](../Images/2b1ab5575ce78f5e2cdd9af1d9b772c1.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b1ab5575ce78f5e2cdd9af1d9b772c1.png)'
- en: (18) The PCA Eigen Equation. Image by Author.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: (18) PCA 特征方程。图片由作者提供。
- en: 'What does this tell us? Astonishingly, the optimal principal component (***p***),
    when multiplied by the covariance matrix of data (***S***), is simply that same
    ***p*** multiplied by a scalar (***λ***). In other words, ***p* must be an** [**eigenvector**](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors)
    **of the covariance matrix,** and we can find ***p*** by computing the eigenvalue
    decomposition of ***S.*** We also observe:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们什么？令人惊讶的是，最佳主成分 (***p***) 乘以数据的协方差矩阵 (***S***)，实际上就是那个相同的 ***p*** 乘以一个标量
    (***λ***)。换句话说，***p*** 必须是协方差矩阵的** [**特征向量**](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors)**，我们可以通过计算
    ***S*** 的特征值分解来找到 ***p***。我们还观察到：
- en: '![](../Images/a4dbf6605177b0bd6d092dedf00490c8.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4dbf6605177b0bd6d092dedf00490c8.png)'
- en: (19) The variance of the data when projected onto **p** is equal to the eigenvalue
    of **p.** Image by Author.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据投影到**p**上时，数据的方差等于**p**的特征值。图片由作者提供。
- en: 'The variance of the data when projected onto ***p*** is equal to the eigenvalue
    corresponding to ***p.*** We also know thatthere are at most ***d*** eigenvalues/eigenvectors
    of ***S,*** the eigenvectors are orthogonal, and the eigenvalues are sorted from
    largest to smallest. This means, by taking the eigenvalue decomposition of ***S,***
    we recover all of the principal components:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据投影到***p***上时，数据的方差等于对应于***p.***的特征值。我们还知道，***S*** 最多有 ***d*** 个特征值/特征向量，特征向量是正交的，特征值从大到小排序。这意味着，通过对
    ***S*** 进行特征值分解，我们可以恢复所有的主成分：
- en: '![](../Images/b62129059503c568dfef653e25879cfc.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b62129059503c568dfef653e25879cfc.png)'
- en: (20) All **d** principal components come from the eigenvalue decomposition of
    **S**. Image by Author.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: (20) 所有的 **d** 个主成分都来自于 **S** 的特征值分解。图片由作者提供。
- en: This is quite a beautiful intersection of statistics, calculus, and linear algebra,
    giving us an elegant way to find the principal components of high dimensional
    datasets. What a beautiful result!
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种统计学、微积分和线性代数的美丽交汇，给我们提供了一种优雅的方法来找到高维数据集的主成分。真是一个美丽的结果！
- en: Code
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代码
- en: 'If we use NumPy, the general form of PCA is actually easier to code than the
    two-dimensional version we derived. Here’s an example of PCA using a three-dimensional
    dataset:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用 NumPy，PCA 的一般形式实际上比我们推导出的二维版本更容易编码。以下是使用三维数据集的 PCA 示例：
- en: '[PRE4]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As we can see, the principal components given by the scikit-learn PCA implementation
    are exactly the eigenvectors of the covariance matrix.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，scikit-learn 的 PCA 实现给出的主成分正是协方差矩阵的特征向量。
- en: Final Thoughts
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最后的思考
- en: Principal Component Analysis (PCA) is a powerful technique used in data science
    and machine learning to reduce the dimensionality of high-dimensional datasets
    while preserving the most important information. In this article, we explored
    the foundational principles of PCA in two dimensions and extended it to arbitrary
    dimensions using linear algebra.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）是一种强大的技术，广泛应用于数据科学和机器学习中，用于减少高维数据集的维度，同时保留最重要的信息。本文探讨了 PCA 的基础原理，首先在二维情况下进行了分析，并使用线性代数将其扩展到任意维度。
- en: In the two-dimensional case, we derived a closed-form solution for finding the
    principal components by maximizing the variance when the data is projected onto
    a new axis. We used trigonometry and calculus to find the angles that define the
    principal components. We then implemented our strategy in Python and verified
    its correctness by comparing the results to the scikit-learn PCA implementation.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在二维情况下，我们通过最大化数据投影到新轴上的方差，推导出了一个闭式解来找到主成分。我们使用了三角学和微积分来确定定义主成分的角度。随后，我们在 Python
    中实现了我们的策略，并通过将结果与 scikit-learn 的 PCA 实现进行比较来验证其正确性。
- en: Moving to arbitrary dimensions, we leveraged linear algebra to find a generalized
    solution. By expressing PCA as an eigenvalue problem, we showed that the principal
    components are the eigenvectors of the covariance matrix. The corresponding eigenvalues
    represent the variance of the data when projected onto each principal component.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在扩展到任意维度时，我们利用线性代数找到了一种通用的解决方案。通过将 PCA 表达为特征值问题，我们展示了主成分是协方差矩阵的特征向量。相应的特征值表示数据在投影到每个主成分上的方差。
- en: As we often see in mathematics, seemingly unrelated concepts that were developed
    by different mathematicians over varying time periods end up culminating into
    a beautiful result. PCA is a prime example of this perplexing intersection, and
    there are many more to be explored!
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在数学中常常看到的，那些看似无关的概念，经过不同数学家在不同时间段的发展，最终会汇聚成一个美丽的结果。PCA 是这种复杂交汇的一个典型例子，还有许多类似的例子值得探索！
- en: '*Become a Member:* [*https://harrisonfhoffman.medium.com/membership*](https://harrisonfhoffman.medium.com/membership)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*成为会员:* [*https://harrisonfhoffman.medium.com/membership*](https://harrisonfhoffman.medium.com/membership)'
- en: '*Buy me a coffee:* [*https://www.buymeacoffee.com/HarrisonfhU*](https://www.buymeacoffee.com/HarrisonfhU)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '*请给我买杯咖啡:* [*https://www.buymeacoffee.com/HarrisonfhU*](https://www.buymeacoffee.com/HarrisonfhU)'
- en: References
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*Ali Ghodsi, Lec 1: Principal Component Analysis —* [https://www.youtube.com/watch?v=L-pQtGm3VS8&t=3129s](https://www.youtube.com/watch?v=L-pQtGm3VS8&t=3129s)'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Ali Ghodsi, Lec 1: 主成分分析 —* [https://www.youtube.com/watch?v=L-pQtGm3VS8&t=3129s](https://www.youtube.com/watch?v=L-pQtGm3VS8&t=3129s)'
- en: '*Independent Component Analysis 2* — [https://www.youtube.com/watch?v=olKgmOuAvrc](https://www.youtube.com/watch?v=olKgmOuAvrc)'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*独立成分分析 2* — [https://www.youtube.com/watch?v=olKgmOuAvrc](https://www.youtube.com/watch?v=olKgmOuAvrc)'
