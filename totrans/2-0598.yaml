- en: Creating a Gradient Descent Animation in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/creating-a-gradient-descent-animation-in-python-3c4dcd20ca51](https://towardsdatascience.com/creating-a-gradient-descent-animation-in-python-3c4dcd20ca51)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to plot the trajectory of a point over a complex surface
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@luisdamed?source=post_page-----3c4dcd20ca51--------------------------------)[![Luis
    Medina](../Images/d83d326290ae3272f0618d0bd28bd875.png)](https://medium.com/@luisdamed?source=post_page-----3c4dcd20ca51--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3c4dcd20ca51--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3c4dcd20ca51--------------------------------)
    [Luis Medina](https://medium.com/@luisdamed?source=post_page-----3c4dcd20ca51--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3c4dcd20ca51--------------------------------)
    ·10 min read·Nov 11, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7d7b4b9e511a33f93c362a2a92aaedc2.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Todd Diemer](https://unsplash.com/@todd_diemer?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let me tell you how I created an animation of gradient descent just to illustrate
    a point in a blog post. It was worth it since I learned more Python by doing it
    and unlocked a new skill: making animated plots.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/99b27b5838acc1c636dfd987cb31ace8.png)'
  prefs: []
  type: TYPE_IMG
- en: Gradient descent animation created in Python. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: I’ll walk you through the steps of the process I followed.
  prefs: []
  type: TYPE_NORMAL
- en: A bit of background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A few days ago, I published a [blog post about gradient descent](https://medium.com/@luisdamed/gradient-descent-f09f19eb35fb)
    as an optimization algorithm used for training Artificial Neural Networks.
  prefs: []
  type: TYPE_NORMAL
- en: I wanted to include an animated figure to show how choosing different initialization
    points for a gradient descent optimization can produce different results.
  prefs: []
  type: TYPE_NORMAL
- en: That’s when I stumbled upon these[**amazing animations**](https://imgur.com/a/Hqolp)created
    by [Alec Radford](https://twitter.com/alecrad) years ago, and shared on a [Reddit
    comment](https://www.reddit.com/r/MachineLearning/comments/2gopfa/comment/cklhott/?utm_source=share&utm_medium=web2x&context=3),
    illustrating the difference between some advanced gradient descent algorithms,
    like [Adagrad](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf), [Adadelta](http://arxiv.org/abs/1212.5701)
    and [RMSprop](https://class.coursera.org/neuralnets-2012-001/lecture/67).
  prefs: []
  type: TYPE_NORMAL
- en: Since I’ve been pushing myself to [replace Matlab with Python](https://medium.com/@luisdamed/replacing-matlab-with-python-part-1-322271314e4d),
    I decided to give it a go and try to code a similar animation myself, using a
    “vanilla” gradient descent algorithm to start with.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go, step by step.
  prefs: []
  type: TYPE_NORMAL
- en: Plot the surface used for optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first thing we do is import the libraries we’ll need and define the mathematical
    function we’ll want to represent.
  prefs: []
  type: TYPE_NORMAL
- en: 'I wanted to use a saddle point surface, so I defined the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/511edcc9f085acb7f1fb35b1a9b29e92.png)'
  prefs: []
  type: TYPE_IMG
- en: Saddle surface equation. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: We also create a grid of points for plotting our surface. `[np.mgrid](https://numpy.org/doc/stable/reference/generated/numpy.mgrid.html)`
    is perfect for this. The complex number `81j` passed as step length indicates
    how many points to create between the start and stop values (81 points).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The next thing to do is create a figure and axes, plot the surface, and format
    it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/db2cf917efd55177930978a8d073064a.png)'
  prefs: []
  type: TYPE_IMG
- en: Saddle surface. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Compute the Gradient Descent trajectories
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ll need to implement the gradient descent algorithm and apply it to our surface.
    That way, we will compute the evolution of the x and y coordinates throughout
    the iterations of the optimization.
  prefs: []
  type: TYPE_NORMAL
- en: As I mentioned in [my post about gradient descent](https://medium.com/@luisdamed/gradient-descent-f09f19eb35fb),
    the simplest implementation is an iterative update of the x and y values in proportion
    to the gradient, until we arrive at a local minimum.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you initialize x and y at any arbitrary point to start the optimization,
    the algorithm is based on the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Compute the gradient of the surface (partial derivatives) at the current point
    (x, y).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the coordinates in proportion to the gradient.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the surface function at the new coordinates.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is done for a predefined number of iterations or until the difference between
    one iteration and the previous one is smaller than some threshold.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve this, I created a function taking as arguments the initialization
    values, step size parameter, and the desired number of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: It returns an array with the x, y, z coordinates for each iteration.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: I tested it with two different initialization points, one over a symmetry line
    on y = 0, and the other one at an offset y value.
  prefs: []
  type: TYPE_NORMAL
- en: I used the same step size and number of iterations for both in order to compare
    them. It also makes it easier to animate later on since the two results will have
    the same number of frames.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: To verify the results, I plotted the trajectory for each coordinate separately
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a04e31c090a7d6f5821a30b08c2d397e.png)'
  prefs: []
  type: TYPE_IMG
- en: Evolution of x, y, and z coordinates during optimization. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: For the symmetry case, y and z converge to 0, while the optimization initialized
    at an offset moves down the z-axis quickly. All as expected.
  prefs: []
  type: TYPE_NORMAL
- en: Now we can visualize the trajectories on top of the surface.
  prefs: []
  type: TYPE_NORMAL
- en: Plot the trajectories of gradient descent on top of the optimization surface
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Initially, I thought this part was just a matter of plotting the results on
    the same coordinate axes we used for the surface, `ax1`. Unfortunately, that won’t
    work. [With Matplotlib we’ll have to do a bit more](https://stackoverflow.com/questions/49586376/draw-line-over-surface-plot/49601924#49601924).
  prefs: []
  type: TYPE_NORMAL
- en: I added new axes to the figure and set them to have the same view angles and
    position of `ax1` . Then I plot the trajectory lines.
  prefs: []
  type: TYPE_NORMAL
- en: The most important part here is using `set_zorder` to manually place the lines
    on top of the surface. Second, we need to use `set_axis_off` to make the new axes
    invisible.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/24af05ca68f7f58498ad331b0ebb109f.png)'
  prefs: []
  type: TYPE_IMG
- en: Static representation of gradient descent trajectory. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Great! Our plot makes sense, so we can start creating the animated version.
  prefs: []
  type: TYPE_NORMAL
- en: Create the animated figure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Remember how movies were done in the old days with sequences of static images?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8553a7eb8794914d4d14be18fde4d2e5.png)'
  prefs: []
  type: TYPE_IMG
- en: Eadweard Muybridge, [Waugsberg](https://commons.wikimedia.org/wiki/User:Waugsberg)
    — Unknown source, [available on Wikipedia](https://commons.wikimedia.org/wiki/Eadweard_Muybridge#/media/File:Muybridge_horse_gallop_animated_2.gif).
    [Public Domain](https://commons.wikimedia.org/wiki/File:Muybridge_horse_gallop_animated_2.gif?uselang=en#Licensing)
  prefs: []
  type: TYPE_NORMAL
- en: Well, they are still done the same way.
  prefs: []
  type: TYPE_NORMAL
- en: That’s why I stored the updated x, y, and z coordinates at each iteration on
    an array.
  prefs: []
  type: TYPE_NORMAL
- en: We need to find a way to plot sequentially those multiple “pictures” or frames
    of the optimization process.
  prefs: []
  type: TYPE_NORMAL
- en: I had no clue about how to do this, so I started to look for code that did similar
    things to what I wanted to accomplish.
  prefs: []
  type: TYPE_NORMAL
- en: I found a brilliant article by [Zack Fizell](https://medium.com/u/595b588fe9e6?source=post_page-----3c4dcd20ca51--------------------------------)
    here on Medium where he showed [how to animate plots in Python](/how-to-animate-plots-in-python-2512327c8263).
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, there is a dedicated class to do this in Matplotlib, `animation`.
  prefs: []
  type: TYPE_NORMAL
- en: Zack explained all the details of the code, and you can go check them out in
    his article. For me, the most important part was using
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: To plot all the steps of the trajectory sequentially.
  prefs: []
  type: TYPE_NORMAL
- en: Easy enough, right?
  prefs: []
  type: TYPE_NORMAL
- en: What `animation.FuncAnimation` does is updating the figure we pass as an input,
    using whatever function we define, in Zack’s case, `animate_func`. The function
    takes as an argument the number of `frames` we specify and is called `frames`
    times.
  prefs: []
  type: TYPE_NORMAL
- en: The sequence of images is stored as an `[TimedAnimation](https://matplotlib.org/stable/api/_as_gen/matplotlib.animation.TimedAnimation.html#matplotlib.animation.TimedAnimation)`
    object, with a time `interval` (in milliseconds) between them.
  prefs: []
  type: TYPE_NORMAL
- en: All of this is well explained in the [official matplotlib documentation](https://matplotlib.org/stable/api/_as_gen/matplotlib.animation.FuncAnimation.html).
  prefs: []
  type: TYPE_NORMAL
- en: In our case, we need to include in the function all the workaround to avoid
    the new axes overlapping with the surface every time we update the trajectory
    lines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how I defined the update function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Finally, to create the animation, I created a new figure and repeated the process
    of adding the second pair of axes. This time I only plotted the surface, since
    the trajectories will be handled by the `descent_animation` function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c226b36a8f2fd30642b025673726beb1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Final result: gradient descent animation created on Python. Image by the author.'
  prefs: []
  type: TYPE_NORMAL
- en: Saving and displaying the animation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can save the animation using a few lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The `PillowWriter` method works well for storing Gif files.
  prefs: []
  type: TYPE_NORMAL
- en: The problem with *Gif* files is that you can end up with an unnecessarily large
    file if you have many frames, and want to preserve good image quality.
  prefs: []
  type: TYPE_NORMAL
- en: To optimize the file size, we could play around with the `interval` passed to
    `FUncAnimation` and specify the FPS and DPI we want to use when creating and saving
    the figure. Otherwise, we could use [EZGif’s Optimization](https://ezgif.com/optimize)
    to reduce the file size.
  prefs: []
  type: TYPE_NORMAL
- en: Another option would be storing it as an *MP4* or , even better, *WebM* file,
    using `[FFMpegWriter](https://matplotlib.org/stable/api/_as_gen/matplotlib.animation.FFMpegWriter.html#matplotlib.animation.FFMpegWriter)`.
  prefs: []
  type: TYPE_NORMAL
- en: As I have discussed before, [using WebM instead of Gif files can produce a great
    improvement in speed if we want to use the figures for web display](https://medium.com/@luisdamed/using-webm-instead-of-gifs-to-improve-your-blogs-speed-e37f71a45d57),
    or just reduce the file size. Unfortunately, WebM is still not supported on Medium.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, if you are using Jupyter Notebooks or working on [Google Colab](https://colab.research.google.com/),
    you can display the animated figure using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Additional
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This type of animated figure is great for comparing the trajectory of different
    types of optimization algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Below, you can see a comparison of the performance of Stochastic Gradient Descent
    (SGD) — the one I’ve shown in this post- and a more advanced method called[*Adam*](https://arxiv.org/abs/1412.6980)
    (derived from Adaptive Moment Estimation), which is currently one of the industry
    standards for training Artificial Neural Networks, because of its robustness and
    speed.
  prefs: []
  type: TYPE_NORMAL
- en: Both were initialized on the point x = 0.2, y = 1e-8, to provide a challenging
    situation. There is very little offset from the exact symmetry line (y = 0) and
    the small x coordinate provides little room for building momentum when “descending”.
    You can see how Adam manages to break the symmetry after very few iterations when
    compared to SGD.
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in learning more about the differences between these two
    algorithms, check out my article about [advanced methods for gradient descent
    optimization](https://medium.com/towards-data-science/dl-notes-advanced-gradient-descent-4407d84c2515).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d771ddaf55ac0c03dd90fb487668e12.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparison of SGD vs Adam. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: So that’s all for this one.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading! I hope you could find any useful information from what
    I shared, and perhaps use it for your own projects.
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in learning more about gradient descent and neural networks,
    I recommend you read my other articles on those topics.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@luisdamed/gradient-descent-f09f19eb35fb?source=post_page-----3c4dcd20ca51--------------------------------)
    [## DL Notes: Gradient descent'
  prefs: []
  type: TYPE_NORMAL
- en: How Neural Networks “Learn”
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'medium.com](https://medium.com/@luisdamed/gradient-descent-f09f19eb35fb?source=post_page-----3c4dcd20ca51--------------------------------)
    [](https://medium.com/@luisdamed/feedforward-artificial-neural-networks-52bcf96d6ac3?source=post_page-----3c4dcd20ca51--------------------------------)
    [## DL Notes: Feedforward Artificial Neural Networks'
  prefs: []
  type: TYPE_NORMAL
- en: The basic concept, explained
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@luisdamed/feedforward-artificial-neural-networks-52bcf96d6ac3?source=post_page-----3c4dcd20ca51--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: I’m writing a series of articles about Deep Learning and Machine Learning, but
    I’ll be also publishing *meta* content about other things I learn while writing
    those articles — this post is an example. So not only the theory and applications
    of DL but also Python programming tips like this one.
  prefs: []
  type: TYPE_NORMAL
- en: Be sure to follow me and subscribe to my emails to get notified whenever I publish
    new posts!
  prefs: []
  type: TYPE_NORMAL
