["```py\nimport gymnasium as gym\nfrom stable_baselines3 import DQN\n\nenv_name = \"MountainCar-v0\"\nenv = gym.make(env_name)\n\nconfig = {\n    'batch_size': 128,\n    'buffer_size': 10000,\n    'exploration_final_eps': 0.07,\n    'exploration_fraction': 0.2,\n    'gamma': 0.98,\n    'gradient_steps': 8, # don't do a single gradient update, but 8\n    'learning_rate': 0.004,\n    'learning_starts': 1000,\n    'policy_kwargs': dict(net_arch=[256, 256]), # we train a neural network with two hidden layers of size 256 each\n    'target_update_interval': 600, # see below, the target network gets overwritten with the main network every 600 steps\n    'train_freq': 16, # don't train after every step in the environment, but after 16 steps\n}\n\nmodel = DQN(\"MlpPolicy\", env, verbose=1, **config) # MlpPolicy = train a normal feed-forward neural network\nmodel.learn(total_timesteps=2000, progress_bar=True)\n```", "```py\nfor _ in range(gradient_steps):\n  # Sample replay buffer\n  replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)  # type: ignore[union-attr]\n\n  with th.no_grad():\n      # Compute the next Q-values using the target network\n      next_q_values = self.q_net_target(replay_data.next_observations)\n      # Follow greedy policy: use the one with the highest value\n      next_q_values, _ = next_q_values.max(dim=1)\n      # Avoid potential broadcast issue\n      next_q_values = next_q_values.reshape(-1, 1)\n      # 1-step TD target\n      target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n\n  # Get current Q-values estimates\n  current_q_values = self.q_net(replay_data.observations)\n```", "```py\nfrom stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n\nenv_name = \"MountainCar-v0\"\n\n# callback to check the agent's performance every 1000 steps for 10 episodes\neval_callback = EvalCallback(\n    eval_env=env,\n    eval_freq=1000,\n    n_eval_episodes=10,\n    best_model_save_path=f\"./logs/{env_name}\", \n    log_path=f'./logs/{env_name}',\n)\n\n# callback to save the model every 10000 steps\nsave_callback = CheckpointCallback(save_freq=10000, save_path=f'./logs/{env_name}')\n```", "```py\nmodel.learn(total_timesteps=2000, progress_bar=True, callback=[eval_callback, save_callback])\n```", "```py\nimport numpy as np\nimport pandas as pd\n\ndata = np.load(f\"./logs/{env_name}/evaluations.npz\")\npd.DataFrame({\n    \"mean_reward\": data[\"results\"].mean(axis=1),\n    \"timestep\": data[\"timesteps\"]\n}).plot(\n    x=\"timestep\",\n    y=\"mean_reward\",\n)\n```", "```py\nimport gymnasium as gym\nfrom stable_baselines3 import DQN\nfrom stable_baselines3.common.atari_wrappers import AtariWrapper\nfrom stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\nfrom stable_baselines3.common.vec_env import SubprocVecEnv, VecFrameStack, VecTransposeImage\n\nif __name__ == \"__main__\":\n    env_name = \"BreakoutNoFrameskip-v4\"\n    env = SubprocVecEnv([lambda: AtariWrapper(gym.make(env_name)) for _ in range(4)]) # train 4 game environments in parallel, scale down images for faster training\n    env = VecFrameStack(env, n_stack=4) # don't only use a still image for training, but the last 4 frames\n    env = VecTransposeImage(env) # technical magic for putting the channels of the animation in the first coordinate, i.e., turning HxWxC into CxHxW since Stable-Baselines3 likes it that way\n\n    config = {\n        \"batch_size\": 32,\n        \"buffer_size\": 10000,\n        \"exploration_final_eps\": 0.02,\n        \"exploration_fraction\": 0.1,\n        \"gamma\": 0.99,\n        \"gradient_steps\": 4,\n        \"learning_rate\": 1e-4,\n        \"learning_starts\": 10000,\n        \"target_update_interval\": 1000,\n        \"train_freq\": 4,\n    }\n\n    eval_callback = EvalCallback(\n        eval_env=env,\n        eval_freq=1000,\n        n_eval_episodes=10,\n        best_model_save_path=f\"./logs/{env_name}\",\n        log_path=f\"./logs/{env_name}\",\n    )\n    save_callback = CheckpointCallback(save_freq=10000, save_path=f\"./logs/{env_name}\")\n\n    model = DQN(\"CnnPolicy\", env, verbose=0, **config) # CnnPolicy creates some default convolutional neural network for us for processing the screen pixels in a more efficient way\n    model.learn(total_timesteps=10_000_000, progress_bar=True, callback=[eval_callback, save_callback])\n```", "```py\nenv = VecFrameStack(env, n_stack=4)\n```", "```py\n...\n\nif __name__ == \"__main__\":\n    env_name = \"SpaceInvadersNoFrameskip-v4\"\n    ...\n```"]