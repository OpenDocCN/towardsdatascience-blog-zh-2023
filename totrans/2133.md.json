["```py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# for some einsum magic\nfrom einops import rearrange, einsum\n\n# use gpu if possible\ndevice = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(\"Device:\", device)\n\nembedding_dim_k = 10 \n\n# linear projection of the input x \nkey = nn.Linear(embedding_dim_k, embedding_dim_k, bias=False)\nquery = nn.Linear(embedding_dim_k, embedding_dim_k, bias=False)\nvalue = nn.Linear(embedding_dim_k, embedding_dim_k, bias=False)\n\n# creating  random vector of shape: \n# batch_size (b), sequence lenght (t), embedding dim (k) \nx = torch.randn(1, 12, embedding_dim_k) # b t k \n\nd_b, d_t, d_k = x.size()\n\n# linear projection of the input\nq = query(x) # b, t, k \nk = key(x) # b, t, k\nv = value(x) # b, t, k\nassert q.shape == (d_b, d_t, d_k)\n\n# scaled dot-product self-attention\n# dot_prod(Q, K)\nscaling_factor = 1/torch.sqrt(torch.tensor(d_k))\nscaled_dot_product = F.softmax(\n  einsum(q, k, \"b t k, b l k -> b t l\") * scaling_factor, dim=-1 )\nassert scaled_dot_product.shape == (d_b, d_t, d_t)\n\n# dot-prod(w, v) \nself_attention = torch.einsum('b i j , b j d -> b i d', scaled_dot_product, v)\n\n# remember that self-attention is a seq2seq operation:\n# the size that goes in, also goes out\nassert self_attention.shape == (d_b, d_t, d_k)\n```", "```py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# for some einsum magic\nfrom einops import rearrange, einsum\n\n# use gpu if possible\ndevice = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(\"Device:\", device)\n\n# we create a random normal tensor as a placeholder for an RGB image with shapes: \n# batch_size (b), channels (c), height (h), width (w)\nimg = torch.randn(1, 3, 28, 28) # b c h w \n\nk = 3 # spatial extend of the memory block N \n\n# we can extract memory blocks by using the pytorch unfold operation and rearranging the result\n# we pad the image first to keep our old dimensions intact\nstride = 1\npadding = 1\nmemory_blocks = F.pad(img, [padding]*4).unfold(dimension=2, size=k, \nstep=stride).unfold(dimension=3, size=k, step=stride)\nmemory_blocks = rearrange(memory_blocks, \"b c h w i j -> b h w c i j\")\nprint(memory_blocks.shape)\nprint(f\"We have {memory_blocks.shape[1]}x{memory_blocks.shape[2]} patches of shape: {memory_blocks.shape[2:]}\")\n\n# apply the self-attention for a specific ij:\ni, j = (3, 4)\nmemory_block_ij = memory_blocks[:, i, j, : , :, :]\n# we can flatten the memory blocks height and width \nx = rearrange(memory_block_ij, \"b h w c -> b (h w) c\")\n\n# our input dimension is the channel size\nd_in = x.shape[-1]\nd_out = d_in\n\n# linear transformations to embed the input x \nkey = nn.Linear(d_in, d_out, bias=False)\nquery = nn.Linear(d_in, d_out, bias=False)\nvalue = nn.Linear(d_in, d_out, bias=False)\n\nd_b, d_t, d_k = x.size()\n\n# linear projection of the input\nq = query(x) # b, t, k \nk = key(x) # b, t, k\nv = value(x) # b, t, k\n\nassert q.shape == (d_b, d_t, d_out)\n\n# scaled dot-product self-attention\n# dot_prod(Q, K)\nscaling_factor = 1/torch.sqrt(torch.tensor(d_k))\nscaled_dot_product = F.softmax(\n  einsum(q, k, \"b t k, b l k -> b t l\") * scaling_factor, dim=-1 )\nassert scaled_dot_product.shape == (d_b, d_t, d_t)\n\n# dot-prod(w, v) \nself_attention = torch.einsum('b i j , b j d -> b i d', scaled_dot_product, v)\n\n# remember that self-attention is a seq2seq operation:\n# the size that goes in, also goes out\nassert self_attention.shape == (d_b, d_t, d_out)\n```", "```py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# for some einsum magic\nfrom einops import rearrange, einsum\n\n# use gpu if possible\ndevice = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(\"Device:\", device)\n\n# number of input channels (e.g. 3 for RGB)\nin_channels = 3\n# the embedding dim of the input projection (embedding_dim)\nmid_channels = 22\n# the number of attention heads\nnum_heads = 2\n# the number of channels after projecting the heads together\nout_channels = 8\n# the maximum number of image pixels of a side(assuming squared images)\nmax_pos_embedding = 4\n\n# create embeddings. if we want to keep the 2D representation of the\n# input, we can do this by using 2D convolution\nquery = nn.Conv2d(in_channels, mid_channels * num_heads, kernel_size=1, device=device)\nkey = nn.Conv2d(in_channels, mid_channels * num_heads, kernel_size=1, device=device)\nvalue = nn.Conv2d(in_channels, mid_channels * num_heads, kernel_size=1, device=device)\nwout = nn.Conv2d(mid_channels * num_heads, out_channels, kernel_size=1, device=device)\n\n# Define positional embeddings\nrow_embedding = nn.Embedding(2 * max_pos_embedding - 1, mid_channels // 2, device=device)\ncol_embedding = nn.Embedding(2 * max_pos_embedding - 1, mid_channels // 2, device=device)\n\n# create relative indices\ndeltas = torch.arange(max_pos_embedding).view(1, -1) - torch.arange(max_pos_embedding).view(\n            -1, 1\n        )\n# -- shift the delta to [0, 2 * max_position_embeddings - 1]\nrelative_indices = (deltas + max_pos_embedding - 1).to(device)\n\n# create an example image\nx = torch.randn(4, 3, 4, 4, device=device) # b c h w\n\nb, cin, h, w = x.size()\nsqrt_normalizer = torch.sqrt(torch.tensor([cin], requires_grad=False, device=device))\n\nq = query(x)\nk = key(x)\nv = value(x)\n\n# Compute attention scores based on position\n# the relative indices are used to get the stair-case pattern corret vectors\nrow_embedding = row_embedding(\n    relative_indices[:w, :w].reshape(-1)\n).transpose(0, 1)\ncol_embedding = col_embedding(\n    relative_indices[:h, :h].reshape(-1)\n).transpose(0, 1) \n\n# unfold heads \nq = rearrange(\n    q, \"b (c heads) h w -> b c heads h w\", heads=num_heads, c=mid_channels)\nk = rearrange(\n    k, \"b (c heads) h w -> b c heads h w\", heads=num_heads, c=mid_channels)\nv = rearrange(\n    v, \"b (c heads) h w -> b c heads h w\", heads=num_heads, c=mid_channels)\n\n# now expand the rows and columns and conncatenate them\nexpand_row = row_embedding.unsqueeze(-1).expand(-1, -1, h*h)\nexpand_col = col_embedding.unsqueeze(-2).expand(-1, w*w, -1)\npositional_embedding = torch.cat((expand_row, expand_col), dim=0)\n\npositional_embedding = rearrange(\n    positional_embedding, \"c (h w) (i j) -> c h w i j\",\n    c=mid_channels, h=h, w=w, i=h, j=w)\n\n# dot-prod(q, r)\nattention_scores = einsum(q, positional_embedding, \n                          \"b c h i j, c i k j l -> b h i j k l\")\nattention_scores = attention_scores / sqrt_normalizer\n\n# Compute attention scores based on data\nattention_content_scores = einsum(q, k, \"b c h i j, b c h k l -> b h i j k l\")\nattention_content_scores = attention_content_scores / sqrt_normalizer\n\n# Combine attention scores\nattention_scores = attention_scores + attention_content_scores\n\n# Normalize to obtain probabilities.\nshape = attention_scores.shape\natt_probs = nn.Softmax(dim=-1)(attention_scores.view(*shape[:-2], -1)).view(shape)\n\n# Re-weight values via attention \nv_f = einsum(att_probs, v, \"b h i j k l, b c h k l -> b c h i j\")\n\n# linear project to output dimension\nv_f = rearrange(v_f, \"b c h i j -> b (c h) i j\")\nout = wout(v_f)\n\nout.shape\n```", "```py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# for some einsum magic\nfrom einops import rearrange, einsum\n\n# use gpu if possible\ndevice = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(\"Device:\", device)\n\nclass StandAloneSelfAttention(nn.Module):\n    def __init__(self, in_channels, mid_channels, out_channels, \n                 num_heads, max_pos_embedding):\n        \"\"\"\n        Inputs:\n            in_channels - Dimensionality of input and attention feature vectors\n            mid_channels - Embedding dim of the input projection\n            out_channels - Output dim after projecting heads together \n            num_heads - Number of heads to use in the Multi-Head Attention block\n            max_pos_embedding # The max(height, width) of size that has to be embedded\n        \"\"\"\n        super().__init__()\n\n        self.mid_channels = mid_channels\n        self.num_heads = num_heads\n        self.out_channels = out_channels\n\n        # create embeddings. if we want to keep the 2D representation of the\n        # input, we can do this by using 2D convolution\n        self.query = nn.Conv2d(in_channels, mid_channels * num_heads, kernel_size=1, device=device)\n        self.key = nn.Conv2d(in_channels, mid_channels * num_heads, kernel_size=1, device=device)\n        self.value = nn.Conv2d(in_channels, mid_channels * num_heads, kernel_size=1, device=device)\n        self.wout = nn.Conv2d(mid_channels * num_heads, out_channels, kernel_size=1, device=device)\n\n        # Define positional embeddings\n        self.row_embedding = nn.Embedding(2 * max_pos_embedding - 1, mid_channels // 2, device=device)\n        self.col_embedding = nn.Embedding(2 * max_pos_embedding - 1, mid_channels // 2, device=device)\n\n        # create relative indices\n        deltas = torch.arange(max_pos_embedding).view(1, -1) - torch.arange(max_pos_embedding).view(\n                    -1, 1\n                )\n        # -- shift the delta to [0, 2 * max_position_embeddings - 1]\n        self.relative_indices = (deltas + max_pos_embedding - 1).to(device)\n\n        self.verbose = False\n\n    def forward(self, x): \n      q = self.query(x)\n      k = self.key(x)\n      v = self.value(x)\n      if self.verbose is True:\n        print(f\"x: {x.shape}, q: {q.shape}, k: {k.shape}, v:{v.shape}\")\n\n      b, cin, h, w = x.size()\n      sqrt_normalizer = torch.sqrt(torch.tensor([cin], requires_grad=False, \n                                                device=device))\n\n      # Compute attention scores based on position\n      # the relative indices are used to get the stair-case pattern corret vectors\n      row_embedding = self.row_embedding(\n          self.relative_indices[:w, :w].reshape(-1)\n      ).transpose(0, 1)\n      col_embedding = self.col_embedding(\n          self.relative_indices[:h, :h].reshape(-1)\n      ).transpose(0, 1) \n\n      # unfold heads \n      q = rearrange(\n          q, \"b (c heads) h w -> b c heads h w\", \n          heads=self.num_heads, c=self.mid_channels)\n      k = rearrange(\n          k, \"b (c heads) h w -> b c heads h w\", \n          heads=self.num_heads, c=self.mid_channels)\n      v = rearrange(\n          v, \"b (c heads) h w -> b c heads h w\", \n          heads=self.num_heads, c=self.mid_channels)\n\n      if self.verbose is True:\n        print(f\"q: {q.shape}, k: {k.shape}, v:{v.shape}\")\n\n      # now expand the rows and columns and conncatenate them\n      expand_row = row_embedding.unsqueeze(-1).expand(-1, -1, w*w)\n      expand_col = col_embedding.unsqueeze(-2).expand(-1, h*h, -1)\n      positional_embedding = torch.cat((expand_row, expand_col), dim=0)\n\n      positional_embedding = rearrange(\n          positional_embedding, \"c (h w) (i j) -> c h w i j\",\n          c=self.mid_channels, h=h, w=w, i=h, j=w)\n\n      if self.verbose is True:\n        print(f\"row_encoding: {row_embedding.shape}, column_encoding: {col_embedding.shape}, pos_embedding: {positional_embedding.shape}\")\n\n      # dot-prod(q, r)\n      attention_scores = einsum(q, positional_embedding, \n                                \"b c h i j, c i k j l -> b h i j k l\")\n      attention_scores = attention_scores / sqrt_normalizer\n\n      # Compute attention scores based on data dot-prod(q, k)\n      attention_content_scores = einsum(q, k, \"b c h i j, b c h k l -> b h i j k l\")\n      attention_content_scores = attention_content_scores / sqrt_normalizer\n\n      # Combine attention scores\n      attention_scores = attention_scores + attention_content_scores\n\n      # Normalize to obtain probabilities.\n      shape = attention_scores.shape\n      att_probs = nn.Softmax(dim=-1)(attention_scores.view(*shape[:-2], -1)).view(shape)\n      if self.verbose is True:\n        print(f\"attention_scores: {attention_scores.shape}, shaped scores: {attention_scores.view(*shape[:-2], -1).shape} att_probs: {att_probs.shape}\")\n\n      # Re-weight values via attention and map to output dimension.\n      v_f = einsum(att_probs, v, \"b h i j k l, b c h k l -> b c h i j\")\n      v_f = rearrange(v_f, \"b c h i j -> b (c h) i j\")\n      if self.verbose is True:\n        print(f\"(qr + qk)V: {v_f.shape}\")\n      out = self.wout(v_f)\n\n      return out\n```"]