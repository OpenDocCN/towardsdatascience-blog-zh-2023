- en: An Introduction to Loading Large Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/an-introduction-to-loading-large-language-models-e7488a7352ed](https://towardsdatascience.com/an-introduction-to-loading-large-language-models-e7488a7352ed)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Mastering Megamodels: An Introductory Guide to Loading Llama2 and HuggingFace’s
    Large Language Models'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@angelamarieteng?source=post_page-----e7488a7352ed--------------------------------)[![Amber
    Teng](../Images/aa757151d39c3b92b7a65a932a32ec8d.png)](https://medium.com/@angelamarieteng?source=post_page-----e7488a7352ed--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e7488a7352ed--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e7488a7352ed--------------------------------)
    [Amber Teng](https://medium.com/@angelamarieteng?source=post_page-----e7488a7352ed--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e7488a7352ed--------------------------------)
    ·15 min read·Oct 12, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ab1b2fdbc7396ec29bedfbd8c64ed35f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: Photo by [Possessed Photography](https://unsplash.com/@possessedphotography?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/jIBMSMs4_kA?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: In the Age of AI Giants, where models trained on terabytes of data and billions
    of parameters reign supreme, the domain of natural language processing has become
    even more accessible — not just to engineers, data scientists, and machine learning
    researchers, but also to hobbyists, businessmen and students. We are at the crossroads
    of a technological revolution — powered by colossal language models.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: This is a revolution that affects not just a few of us, *but all of us*. Because
    of this, it’s becoming more and more essential to be well-versed not just in understanding
    what these large language models (LLMs) are as well as their capabilities, but
    also in the use of these LLMs. *So why is it essential for engineers to understand
    how to load these LLMs?*
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: These new LLMs are have far-reaches into almost every aspect of today’s tech
    landscape — and data scientists and natural language processing (NLP) engineers
    are increasingly called upon to integrate LLM-driven solutions into their products
    and systems, whether this be in academia or industry. It is evident that a fundamental
    understanding of LLMs is crucial for making informed decisions about what model
    would be appropriate to use, when it would be appropriate to use certain models,
    and what benefits these models can have on a given project or application. Without
    this foundational grasp on LLMs, engineers could miss out on impactful opportunities
    to build products with state-of-the-art (SOTA) LLM capabilities.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: A first step in utilizing and understanding these LLMs is loading the models.
    Practically speaking, to work with LLMs effectively, engineers must first understand
    how to load them. Why is it challenging to load LLMs?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 利用和理解这些LLMs的第一步是加载模型。实际来说，为了有效地使用LLMs，工程师必须首先了解如何加载它们。为什么加载LLMs会有挑战？
- en: The Challenge of Loading LLMs
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载LLMs的挑战
- en: It’s especially challenging to load LLMs because of their large scale as well
    as their potential hardware prerequisites and software configurations. Many NLP
    engineers unsurprisingly “get stuck” on the loading step of LLMs, which then could
    prevent them from experimenting with these models and truly harnessing their capabilities.
    Engineers who are able to master the art of loading LLMs definitely gain a competitive
    edge in utilizing these LLMs.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其庞大的规模以及潜在的硬件要求和软件配置，加载LLMs尤其具有挑战性。许多NLP工程师在LLMs的加载步骤上“不出意外地陷入困境”，这可能会阻碍他们尝试这些模型并真正发挥其能力。能够掌握加载LLMs艺术的工程师无疑在利用这些LLMs时获得了竞争优势。
- en: The sheer number of parameters in these models equates to unparalleled expressive
    power, enabling these LLMs to capture intricate language patterns and context
    effectively. The massive scale of LLMs such as Llama2 and GPT also allows for
    fine-tuning on specialized tasks and industries — which will be discussed in more
    detail later in this article, as well as in my future articles. Unsurprisingly,
    the vast parameter counts of these LLMs result in superior performance, making
    these models versatile for various language-related applications — from text generation,
    question answering, summarization, translation, zero-shot learning, and beyond.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型中庞大的参数数量意味着无与伦比的表达能力，使得这些LLMs能够有效地捕捉复杂的语言模式和上下文。像Llama2和GPT这样的LLMs的大规模也允许在专业任务和行业上进行微调——这些将在本文稍后更详细地讨论，并将在我未来的文章中继续探讨。毫不意外，这些LLMs的庞大参数数量带来了优越的性能，使这些模型在各种语言相关应用中表现出色——从文本生成、问答、摘要、翻译、零样本学习及其他。
- en: In this article, we’ll explore how to load one of the most popular models today
    — Meta’s Llama2 — across different configurations and using different sizes of
    the model. Whether you’re an NLP engineer, a researcher, or hobbyist eager to
    explore the future (and present) of AI, this article will guide you through the
    basics of loading LLMs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将探讨如何在不同配置下以及使用不同大小的模型来加载今天最受欢迎的模型之一——Meta的Llama2。无论你是NLP工程师、研究人员，还是对探索人工智能的未来（和现在）充满热情的爱好者，本文将指导你了解加载LLMs的基础知识。
- en: '![](../Images/29a535e5c06be6b564ccfb3b42238ab5.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29a535e5c06be6b564ccfb3b42238ab5.png)'
- en: Photo by [Paz Arando](https://unsplash.com/@pazarando?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/ZHmvek0kJW8?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Paz Arando](https://unsplash.com/@pazarando?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    提供，来源于 [Unsplash](https://unsplash.com/photos/ZHmvek0kJW8?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
- en: What is Llama2?
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是Llama2？
- en: Developed by Meta as an improvement to Llama1 and released in July 2023, [Llama2](https://ai.meta.com/llama/#inside-the-model)
    is a suite [“pretrained and fine-tuned large language models (LLMs) ranging in
    scale from 7 billion to 70 billion parameters”](https://scontent.fmnl25-1.fna.fbcdn.net/v/t39.2365-6/10000000_662098952474184_2584067087619170692_n.pdf?_nc_cat=105&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=TjG8FsFfH8cAX_uux2_&_nc_ht=scontent.fmnl25-1.fna&oh=00_AfBhA6S3oJWa52QjMx99BmgN3gg06l2jsH2-P2MNIxSmAw&oe=6529307F).
    This year, it has garnered attention because of its exceptional performance in
    comparison to other SOTA models (both open-source and closed-source models) as
    well as its staggering size — with models ranging from 2.7 billion parameters
    to 70 billion parameters. Llama2 has been shown to be capable of understanding,
    generating, and manipulating human language with extraordinary proficiency. Its
    customizability also motivated engineers to adapt it for a wide range of applications.
    Specifically, its flexibility in terms of fine-tuning allows for Llama2 to be
    tailerod to specific tasks, industries, and datasets — empowering engineers to
    explore its applications across industry and academia. Llama2 also comes with
    Llama2-Chat, which is optimized for dialogue use cases, and which we will explore
    later in this article. According to the Llama2 paper, Llama2-Chat utilizes iterative
    applications of alignment techniques, such as instruction tuning and reinforcement
    learning from human feedback. For more details on the model architecture and fine-tuning
    process, as well as an engaging discussion on safety, please see Meta’s official
    paper [here](https://scontent.fmnl25-1.fna.fbcdn.net/v/t39.2365-6/10000000_662098952474184_2584067087619170692_n.pdf?_nc_cat=105&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=TjG8FsFfH8cAX_uux2_&_nc_ht=scontent.fmnl25-1.fna&oh=00_AfBhA6S3oJWa52QjMx99BmgN3gg06l2jsH2-P2MNIxSmAw&oe=6529307F).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 由Meta开发，作为Llama1的改进版，并于2023年7月发布，[Llama2](https://ai.meta.com/llama/#inside-the-model)是一套[“预训练和微调的大型语言模型（LLM），规模从70亿到700亿参数不等”](https://scontent.fmnl25-1.fna.fbcdn.net/v/t39.2365-6/10000000_662098952474184_2584067087619170692_n.pdf?_nc_cat=105&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=TjG8FsFfH8cAX_uux2_&_nc_ht=scontent.fmnl25-1.fna&oh=00_AfBhA6S3oJWa52QjMx99BmgN3gg06l2jsH2-P2MNIxSmAw&oe=6529307F)。今年，它因其与其他SOTA模型（包括开源和闭源模型）相比的卓越表现以及惊人的规模——从27亿参数到700亿参数不等——而备受关注。Llama2已经显示出以非凡的熟练度理解、生成和操控人类语言的能力。其定制化能力也激励了工程师将其适应于广泛的应用领域。特别是，其在微调方面的灵活性使Llama2能够针对特定任务、行业和数据集进行调整——赋予工程师在行业和学术界探索其应用的能力。Llama2还配有Llama2-Chat，它经过优化用于对话用例，我们将在本文后续部分探讨。根据Llama2论文，Llama2-Chat利用了对齐技术的迭代应用，如指令调优和来自人类反馈的强化学习。有关模型架构和微调过程的更多细节，以及有关安全性的精彩讨论，请参见Meta的官方论文[这里](https://scontent.fmnl25-1.fna.fbcdn.net/v/t39.2365-6/10000000_662098952474184_2584067087619170692_n.pdf?_nc_cat=105&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=TjG8FsFfH8cAX_uux2_&_nc_ht=scontent.fmnl25-1.fna&oh=00_AfBhA6S3oJWa52QjMx99BmgN3gg06l2jsH2-P2MNIxSmAw&oe=6529307F)。
- en: Using the HuggingFace Library
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用HuggingFace库
- en: For this project, we’ll be using the HuggingFace library to load Llama2\. [HuggingFace](https://huggingface.co/)
    is an open-source data science and machine learning (ML) platform that democratizes
    the field of AI — it’s where engineers can find models, datasets, and applications
    (on [HuggingFace Spaces](https://huggingface.co/spaces)) of ML models.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们将使用HuggingFace库来加载Llama2。[HuggingFace](https://huggingface.co/)是一个开源数据科学和机器学习（ML）平台，致力于使AI领域民主化——这是工程师可以找到模型、数据集和应用程序（在[HuggingFace
    Spaces](https://huggingface.co/spaces)上）的地方。
- en: Specifically, we’ll use the HuggingFace (HF) Transformers library, which contains
    [different sizes of Meta’s Llama2 model](https://huggingface.co/models?library=transformers&sort=trending&search=meta).
    HF’s Transformers library was originally built as an open-source initiative, and
    today works very well with both PyTorch and TensorFlow. In my NLP journey, I’ve
    benefitted a lot from using HF’s Transformers library because it makes it so much
    easier to access SOTA NLP models, including Llama2\. Through HF’s API, which is
    what I used for this project, we’ll see how to start loading LLMs and other pre-trained
    models. In future articles, I will also share how to use HF’s API to fine-tune
    and deploy these LLMs.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，我们将使用 HuggingFace (HF) Transformers 库，该库包含 [不同尺寸的 Meta Llama2 模型](https://huggingface.co/models?library=transformers&sort=trending&search=meta)。HF
    的 Transformers 库最初作为一个开源项目构建，现在与 PyTorch 和 TensorFlow 都能很好地兼容。在我的 NLP 旅程中，我从使用
    HF 的 Transformers 库中受益匪浅，因为它使得访问最先进的 NLP 模型，包括 Llama2，变得更加容易。通过 HF 的 API（这是我在这个项目中使用的），我们将看到如何开始加载
    LLM 和其他预训练模型。在未来的文章中，我还将分享如何使用 HF 的 API 来微调和部署这些 LLM。
- en: '![](../Images/7172d6a6dfebca40f6fb00b437aa544b.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7172d6a6dfebca40f6fb00b437aa544b.png)'
- en: Photo by [Kate Levitskaya](https://unsplash.com/@katereyy?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/CtFF1YUD8vE?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[凯特·列维茨卡娅](https://unsplash.com/@katereyy?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)拍摄于
    [Unsplash](https://unsplash.com/photos/CtFF1YUD8vE?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)'
- en: Getting Started
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用
- en: In this article, we will explore how to load LLMs (specifically Llama2) in different
    sizes, which will be helpful in different compute setups. *This article is written
    as a companion and and extension of Suhas Pai’s Book* [*Designing Large Language
    Model Applications*](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fwww.oreilly.com%2Flibrary%2Fview%2Fdesigning-large-language%2F9781098150495%2F)*.*
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将探索如何加载不同尺寸的 LLM（特别是 Llama2），这对于不同的计算设置将很有帮助。*本文作为 Suhas Pai 的书籍* [*Designing
    Large Language Model Applications*](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fwww.oreilly.com%2Flibrary%2Fview%2Fdesigning-large-language%2F9781098150495%2F)*
    的附属和扩展。*
- en: For this project, I initially tested code on Google Colab using the Colab Free
    Tier, although I ran into a number of issues when loading larger models and running
    inference. So, for some of the larger models (like the 13 Billion Parameter Llama2
    Hugging Face model), assume a basic subscription of Google Colab Pro, as of the
    time of writing.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个项目，我最初在 Google Colab 上使用了 Colab 免费版进行代码测试，尽管在加载大型模型和进行推理时遇到了许多问题。因此，对于一些较大的模型（例如
    130 亿参数的 Llama2 Hugging Face 模型），假设使用的是 Google Colab Pro 的基本订阅（截至撰写时）。
- en: Resource Allocations
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源分配
- en: To get started, I had the following resource allocations on my Colab notebook
    — the full code can be viewed here.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用，我在我的 Colab 笔记本上进行了以下资源分配——完整代码可以在此查看。
- en: Python 3 Google Compute Engine backend (GPU)
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 3 Google Compute Engine 后端（GPU）
- en: 'System RAM: 51.0 GB'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统 RAM：51.0 GB
- en: 'GPU RAM: 40.0 GB'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'GPU RAM: 40.0 GB'
- en: 'Disk Space: 166.8 GB'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 磁盘空间：166.8 GB
- en: 'Hosted Runtime Type: A100'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 托管运行时类型：A100
- en: Note that to be more efficient and mindful of GPU RAM constraints and Disk Space
    limitations, we cleared the GPU memory after every model run, and we also cleared
    the Disk Space.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，为了提高效率并考虑到GPU RAM限制和磁盘空间限制，我们在每次模型运行后清空了GPU内存，同时也清理了磁盘空间。
- en: '![](../Images/4202ce23b001c3830a1af17c7b5faf11.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4202ce23b001c3830a1af17c7b5faf11.png)'
- en: Google Colab Resources. *Image from the Author.*
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Google Colab 资源。*图片来自作者。*
- en: HuggingFace Login
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HuggingFace 登录
- en: Llama2 is a gated model, which means that we would need to register for a license
    on the [Meta website](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fai.meta.com%2Fresources%2Fmodels-and-libraries%2Fllama-downloads%2F)
    before using it. This typically takes a day or two, and your registration key
    would be tied to your HuggingFace token. To create and use your own HuggingFace
    token, please see [this](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fhuggingface.co%2Fdocs%2Fhub%2Fsecurity-tokens)
    guide on the offical HuggingFace website.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Llama2 是一个受限模型，这意味着我们需要在 [Meta 网站](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fai.meta.com%2Fresources%2Fmodels-and-libraries%2Fllama-downloads%2F)
    上注册一个许可证才能使用它。这通常需要一两天时间，并且您的注册密钥将与您的 HuggingFace 令牌关联。要创建和使用自己的 HuggingFace 令牌，请参阅
    [此](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fhuggingface.co%2Fdocs%2Fhub%2Fsecurity-tokens)
    HuggingFace 官方网站上的指南。
- en: Since we are using Google Colab, we can use the following code to log in.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/0438ead04de6a2308a6f3a1c6e07ee68.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: Image by the Author
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Load Libraries
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, we’ll load the libraries we need to start loading the 2.7 Billion Parameter
    Llama2 Chat Model, the 2.7 Billion Parameter Llama2 Model, and the 13 Billion
    Parameter Llama2 Model. In a future article, we will also show how to load the
    70 Billion Parameter Llama2 Model using AWS, so please stay tuned for this.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We will use `bitsandbytes` for sharding. For more information on model quantization,
    see the HuuggingFace documentation [here](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fhuggingface.co%2Fdocs%2Ftransformers%2Fmain_classes%2Fquantization).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Loading the Llama2 2.7 Billion Parameter Chat Model via HF
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, we’ll start with the smallest Llama2 model available. We can specify the
    model name, based on the HF Transformers library and the associated [Model Card](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fhuggingface.co%2Fmeta-llama%2FLlama-2-7b-chat-hf).
    Then, we will specify the tokenizer — but for this demo, we can just use `AutoTokenizer`
    and use the associated Llama2 pretrained tokenizer. We can then use the `transformers.pipeline`
    function to specify our task, our model, and our PyTorch datatype, before setting
    model-specific parameters such as `top_k` , `do_sample` which to tells the model
    to sample from the logits, rather than using greedy decoding, `max_new_tokens`
    which specifies the number of maximum tokens to return. Note that the Llama2 Chat
    Model is better suited towards dialogues.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: For this project, we will ask different variations of the question “*Outline
    the steps to load a large language model*”.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Then, we will output the generated text.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The result is as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: 'Result: Outline the steps to load a large language model using Python.'
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Introduction: Large language models have gained significant attention in recent
    years due to their impressive performance in various natural language processing
    (NLP) tasks. These models require significant computational resources and memory,
    making it challenging to load them onto a local machine. In this article, we will
    outline the steps to load a large language model using Python.'
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Step 1: Choose a Language Model The first step is to choose a large language
    model that you want to load. There are several popular models available, including
    BERT, RoBERTa, and XLNet. You can choose one of these models or experiment with
    different variations.'
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Step 2: Download the Model Once you have chosen a language model, you need
    to download it. You can download the pre-trained model from the model’s official
    website or from a reputable data repository. Make sure to download the correct
    version of the model, which may vary depending on the task you are working on.'
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-63
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Step 3: Prepare the Model After downloading the model, you need to prepare
    it for use. This involves loading the model into memory and formatting it according
    to the required format. You can use the `torch` library in Python to load the
    model and perform other NLP tasks.'
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Step 4: Load the Model To load the model into memory, you can use the `torch.load()`
    function. This function takes the path to the model file and loads it into memory.
    Here is an example: [PRE5]'
  id: totrans-66
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Step 5: Fine-tune the Model After loading the model, you may want to fine-tune
    it for your specific task. This involves adjusting the model’s weights to improve
    its performance on your task. You can use the `torch.optim` module to optimize
    the model’s weights. Here is an example: [PRE6]'
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Step 6: Use the Model Once the model is loaded and fine-tuned, you can use
    it for your NLP task. You can use the `model()` function to make predictions or
    perform other NLP tasks. Here is an example: [PRE7]'
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Conclusion: In this article, we outlined the steps to load a large language
    model using Python. These steps involve choosing a language model, downloading
    the model, preparing the model, loading the model into memory, fine-tuning the
    model, and using the model for your NLP task. By following these steps, you can
    leverage the power of large language models for your NLP tasks.'
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/3ade20c5d7841a1728349a372a2014d3.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
- en: Photo by [Dušan veverkolog](https://unsplash.com/@veverkolog?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/Hc9G7owDs1s?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Loading the Llama2 2.7 Billion Parameter Model via HF
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, we’ll try out the Base Llama2 Model.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We also set the `bitsandbytes` configuration above. For more information on
    model quantization, see the HuuggingFace documentation [here](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fhuggingface.co%2Fdocs%2Ftransformers%2Fmain_classes%2Fquantization).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To test it out, we get:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: You are a helpful, respectful and honest assistant. Always answer as helpfully
    as possible, while being safe. Your answers should not include any harmful, unethical,
    racist, sexist, toxic, dangerous, or illegal content. Please ensure that your
    responses are socially unbiased and positive in nature. If a question does not
    make any sense, or is not factually coherent, explain why instead of answering
    something not correct. If you don’t know the answer to a question, please don’t
    share false information.
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Question:** What are the steps to load a large language model using Python?'
  id: totrans-83
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Answer:**'
  id: totrans-85
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-86
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'You can use the following steps to load a large language model using Python:'
  id: totrans-87
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-88
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '1.Download the model: You can download the model from the internet and save
    it on your computer.'
  id: totrans-89
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-90
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '2.Import the necessary libraries: You will need to import the necessary libraries
    for loading the model, such as numpy, pandas, and tensorflow.'
  id: totrans-91
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-92
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '3.Define the model: You will need to define the model by specifying the vocabulary
    and the number of dimensions.'
  id: totrans-93
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 3. 定义模型：你需要通过指定词汇表和维度数量来定义模型。
- en: ''
  id: totrans-94
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '4.Load the model: You will need to load the model by specifying the vocabulary,
    the number of dimensions, and the file path where the model is saved.'
  id: totrans-95
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 4. 加载模型：你需要通过指定词汇表、维度数量以及模型保存的文件路径来加载模型。
- en: ''
  id: totrans-96
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '5.Train the model: You will need to train the model by providing the input
    text and the desired output text.'
  id: totrans-97
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 5. 训练模型：你需要通过提供输入文本和期望输出文本来训练模型。
- en: ''
  id: totrans-98
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '6.Evaluate the model: You will need to evaluate the model by providing the
    input text and the desired output text and comparing the output to the desired
    output.'
  id: totrans-99
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 6. 评估模型：你需要通过提供输入文本和期望输出文本来评估模型，并将输出与期望输出进行比较。
- en: ''
  id: totrans-100
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '7.Save the model: You will need to save the model by specifying the vocabulary,
    the number of dimensions, and the file path where the model is saved.'
  id: totrans-101
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 7. 保存模型：你需要通过指定词汇表、维度数量以及模型保存的文件路径来保存模型。
- en: ''
  id: totrans-102
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '8.Load the model: You will need to load the model by specifying the vocabulary,
    the number of dimensions, and the file path where the model is saved.'
  id: totrans-103
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 8. 加载模型：你需要通过指定词汇表、维度数量以及模型保存的文件路径来加载模型。
- en: ''
  id: totrans-104
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '9.Evaluate the model: You will need to evaluate the model by providing the
    input text and the desired output text and comparing the output to the desired
    output.'
  id: totrans-105
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 9. 评估模型：你需要通过提供输入文本和期望输出文本来评估模型，并将输出与期望输出进行比较。
- en: ''
  id: totrans-106
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '10.Save the model: You will need to save the model by specifying the vocabulary,
    the number of dimensions, and the file path where the model is saved.'
  id: totrans-107
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 10. 保存模型：你需要通过指定词汇表、维度数量以及模型保存的文件路径来保存模型。
- en: ''
  id: totrans-108
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '11.Load the model: You will need to load the model by specifying the vocabulary,
    the number of dimensions, and the file path where the model is saved.'
  id: totrans-109
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 11. 加载模型：你需要通过指定词汇表、维度数量以及模型保存的文件路径来加载模型。
- en: ''
  id: totrans-110
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '12.Evaluate the model: You will need to evaluate the model by providing the
    input text and the desired output text and comparing the output to the desired
    output.'
  id: totrans-111
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 12. 评估模型：你需要通过提供输入文本和期望输出文本来评估模型，并将输出与期望输出进行比较。
- en: ''
  id: totrans-112
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '13.Save the model: You will need to save the model by specifying the vocabulary,
    the number of dimensions, and the file path where the model is saved.'
  id: totrans-113
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 13. 保存模型：你需要通过指定词汇表、维度数量以及模型保存的文件路径来保存模型。
- en: ''
  id: totrans-114
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '14.Load the model: You will need to load the model by specifying the vocabulary,
    the number of dimensions, and the file path where the model is saved.'
  id: totrans-115
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 14. 加载模型：你需要通过指定词汇表、维度数量以及模型保存的文件路径来加载模型。
- en: ''
  id: totrans-116
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '15.Evaluate the model: You will need to evaluate the model by providing the
    input text and the desired output text and comparing the output to the desired
    output.'
  id: totrans-117
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 15. 评估模型：你需要通过提供输入文本和期望输出文本来评估模型，并将输出与期望输出进行比较。
- en: ''
  id: totrans-118
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '16.Save the model: You will need to save the model by specifying the vocabulary,
    the number of dimensions, and the file path where the model is saved.'
  id: totrans-119
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 16. 保存模型：你需要通过指定词汇表、维度数量以及模型保存的文件路径来保存模型。
- en: ''
  id: totrans-120
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '17.Load the model: You will need to load the model by specifying the vocabulary,
    the number of dimensions, and the file path where the model is saved.'
  id: totrans-121
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 17. 加载模型：你需要通过指定词汇表、维度数量以及模型保存的文件路径来加载模型。
- en: ''
  id: totrans-122
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '18.Evaluate the model: You will need to evaluate the model by providing the
    input text and the desired output text and comparing the output to the desired
    output.'
  id: totrans-123
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 18. 评估模型：你需要通过提供输入文本和期望输出文本来评估模型，并将输出与期望输出进行比较。
- en: ''
  id: totrans-124
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '19.Save the model: You will need to save the model by specifying the vocabulary,
    the number of dimensions, and the file path where the model is saved.'
  id: totrans-125
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 19. 保存模型：你需要通过指定词汇表、维度数量以及模型保存的文件路径来保存模型。
- en: ''
  id: totrans-126
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '20.Load the model: You will need to load the model by specifying the vocabulary,
    the number of dimensions, and the file path where the model is saved.'
  id: totrans-127
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 20. 加载模型：你需要通过指定词汇表、维度数量以及模型保存的文件路径来加载模型。
- en: ''
  id: totrans-128
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '21.Evaluate the model: You will need to evaluate the model by providing the
    input text and the desired output text and comparing the output to the desired
    output.'
  id: totrans-129
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 21. 评估模型：你需要通过提供输入文本和期望输出文本来评估模型，并将输出与期望输出进行比较。
- en: Loading the Llama2 13 Billion Parameter Model via HF
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过 HF 加载 Llama2 130 亿参数模型
- en: Next, we’ll also try out an even bigger model — Llama2’s 13 Billion Parameter
    Model. For more information on this specific model, please check out the [Model
    Card](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fhuggingface.co%2Fmeta-llama%2FLlama-2-13b-hf).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As we did earlier in the 2.7 Billion Parameter Model, we'll also be using `bitsandbytes`.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Let's try it out!
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Using a similar question as earlier, we get an interesting result:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: You are a helpful, respectful and honest assistant. Always answer as helpfully
    as possible, while being safe. Your answers should not include any harmful, unethical,
    racist, sexist, toxic, dangerous, or illegal content. Please ensure that your
    responses are socially unbiased and positive in nature. If a question does not
    make any sense, or is not factually coherent, explain why instead of answering
    something not correct. If you don’t know the answer to a question, please don’t
    share false information.
  id: totrans-137
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-138
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Q:** What are the steps to load a large language model using Python? Please
    explain the answer to me in terms that a 5th grader can understand. Thank you.'
  id: totrans-139
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-140
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**A:**'
  id: totrans-141
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-142
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 1\. Download the language model.
  id: totrans-143
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-144
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 2\. Import the language model.
  id: totrans-145
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-146
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 3\. Load the language model.
  id: totrans-147
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-148
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 4\. Use the language model.
  id: totrans-149
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Comparing Results and A Brief Prompt Engineering Discussion
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we saw how three different sizes and versions of Llama2 performed
    on a simple question answering prompt. Note that we split our prompt into three
    parts. First, we started with the **instruction text**, which specifies how a
    model should answer the question and what its associated constraints are; second,
    including the actual question as the **input text**, which then allows us to ask
    the specific question we would like Llama2 to answer — this can be in the format
    including the prefix “Question” or “Q:”, to be more explicit with the model. Finally,
    we also have the **output text**, which is where the model will actually generate
    the tokens based on our question. Here, we can include “Answer” or “A:” to be
    more explicit with the model as well — in my testing, I’ve seen this to be particularly
    useful when giving Llama2 certain examples of how I want its answers to look like.
    We won’t dive too deeply into prompt engineering in this article, but for more
    information on that, feel free to read this [blog post](https://www.promptingguide.ai/).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: What are some aspects of the three results shown above that you found interesting?
    I personally really liked how Llama2 created an answer based on the prompting
    of “explain the answer to me in terms that a 5th grader can understand”. Additionally,
    it was interesting how Llama2’s result in the 2.7 billion parameter looked like
    it was printing out the steps for loading an LLM in a loop — while this makes
    sense conceptually, are there any ways that Llama2 could have answered this question
    in a more intuitive and human-like way?
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Other Considerations
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Loading large language models is just scratching the surface to understanding
    and creating useful applications of LLMs. There are a number of ways to improve
    LLM performance — such as parallelism, mixed precision training, model quantization
    (which we used here), caching, batch processing, and using distributed systems.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: All of these are exciting avenues to explore, and I hope that this blog post,
    along with its accompanying notebook, could give you a good starting point for
    your LLM journey!
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: The full code can be viewed [here](https://github.com/piesauce/llm-playbooks/blob/loading_llms/ateng/notebooks/Loading_LLMs_Basic.ipynb).
  id: totrans-156
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-157
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This article is written as a companion and extension of Suhas Pai’s Book [Designing
    Large Language Model Applications](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fwww.oreilly.com%2Flibrary%2Fview%2Fdesigning-large-language%2F9781098150495%2F),
    and was written as part of the book’s LLM repo.
  id: totrans-158
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-159
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Purchase [my book here](https://www.amazon.com/Data-Resource-Emerging-Countries-Landscape/dp/1641372524)
    and please do report any bugs or suggestions to this article via [email](http://at2507@nyu.edu).
    Please feel free to email me about collaborations as well! I’m open to new projects
    at the moment.
  id: totrans-160
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-161
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Connect with me via [LinkedIn](https://www.linkedin.com/in/angelavteng/), [Twitter](https://twitter.com/ambervteng),
    or [TikTok](https://www.tiktok.com/@angelaworkout).
  id: totrans-162
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-163
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Follow me on [Medium](https://medium.com/@angelamarieteng).
  id: totrans-164
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*References:*'
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[https://ai.meta.com/llama/](https://ai.meta.com/llama/)'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://about.fb.com/news/2023/07/llama-2/](https://about.fb.com/news/2023/07/llama-2/)'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://scontent.fmnl25-1.fna.fbcdn.net/v/t39.2365-6/10000000_662098952474184_2584067087619170692_n.pdf?_nc_cat=105&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=TjG8FsFfH8cAX_uux2_&_nc_ht=scontent.fmnl25-1.fna&oh=00_AfBhA6S3oJWa52QjMx99BmgN3gg06l2jsH2-P2MNIxSmAw&oe=6529307F](https://scontent.fmnl25-1.fna.fbcdn.net/v/t39.2365-6/10000000_662098952474184_2584067087619170692_n.pdf?_nc_cat=105&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=TjG8FsFfH8cAX_uux2_&_nc_ht=scontent.fmnl25-1.fna&oh=00_AfBhA6S3oJWa52QjMx99BmgN3gg06l2jsH2-P2MNIxSmAw&oe=6529307F)'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://huggingface.co/](https://huggingface.co/)'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://huggingface.co/transformers/v4.10.1/main_classes/pipelines.html](https://huggingface.co/transformers/v4.10.1/main_classes/pipelines.html)'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.Text2TextGenerationPipeline](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.Text2TextGenerationPipeline)'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://huggingface.co/docs/transformers/main/llm_tutorial](https://huggingface.co/docs/transformers/main/llm_tutorial)'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.promptingguide.ai/](https://www.promptingguide.ai/)'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://huggingface.co/docs/transformers/main_classes/quantization](https://huggingface.co/docs/transformers/main_classes/quantization)'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://huggingface.co/models?library=transformers&sort=trending&search=meta](https://huggingface.co/models?library=transformers&sort=trending&search=meta)'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://huggingface.co/meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://huggingface.co/blog/4bit-transformers-bitsandbytes](https://huggingface.co/blog/4bit-transformers-bitsandbytes)'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
