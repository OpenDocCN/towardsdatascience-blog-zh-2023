- en: An Introduction to Loading Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/an-introduction-to-loading-large-language-models-e7488a7352ed](https://towardsdatascience.com/an-introduction-to-loading-large-language-models-e7488a7352ed)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Mastering Megamodels: An Introductory Guide to Loading Llama2 and HuggingFace’s
    Large Language Models'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@angelamarieteng?source=post_page-----e7488a7352ed--------------------------------)[![Amber
    Teng](../Images/aa757151d39c3b92b7a65a932a32ec8d.png)](https://medium.com/@angelamarieteng?source=post_page-----e7488a7352ed--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e7488a7352ed--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e7488a7352ed--------------------------------)
    [Amber Teng](https://medium.com/@angelamarieteng?source=post_page-----e7488a7352ed--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e7488a7352ed--------------------------------)
    ·15 min read·Oct 12, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ab1b2fdbc7396ec29bedfbd8c64ed35f.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Possessed Photography](https://unsplash.com/@possessedphotography?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/jIBMSMs4_kA?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  prefs: []
  type: TYPE_NORMAL
- en: In the Age of AI Giants, where models trained on terabytes of data and billions
    of parameters reign supreme, the domain of natural language processing has become
    even more accessible — not just to engineers, data scientists, and machine learning
    researchers, but also to hobbyists, businessmen and students. We are at the crossroads
    of a technological revolution — powered by colossal language models.
  prefs: []
  type: TYPE_NORMAL
- en: This is a revolution that affects not just a few of us, *but all of us*. Because
    of this, it’s becoming more and more essential to be well-versed not just in understanding
    what these large language models (LLMs) are as well as their capabilities, but
    also in the use of these LLMs. *So why is it essential for engineers to understand
    how to load these LLMs?*
  prefs: []
  type: TYPE_NORMAL
- en: These new LLMs are have far-reaches into almost every aspect of today’s tech
    landscape — and data scientists and natural language processing (NLP) engineers
    are increasingly called upon to integrate LLM-driven solutions into their products
    and systems, whether this be in academia or industry. It is evident that a fundamental
    understanding of LLMs is crucial for making informed decisions about what model
    would be appropriate to use, when it would be appropriate to use certain models,
    and what benefits these models can have on a given project or application. Without
    this foundational grasp on LLMs, engineers could miss out on impactful opportunities
    to build products with state-of-the-art (SOTA) LLM capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: A first step in utilizing and understanding these LLMs is loading the models.
    Practically speaking, to work with LLMs effectively, engineers must first understand
    how to load them. Why is it challenging to load LLMs?
  prefs: []
  type: TYPE_NORMAL
- en: The Challenge of Loading LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s especially challenging to load LLMs because of their large scale as well
    as their potential hardware prerequisites and software configurations. Many NLP
    engineers unsurprisingly “get stuck” on the loading step of LLMs, which then could
    prevent them from experimenting with these models and truly harnessing their capabilities.
    Engineers who are able to master the art of loading LLMs definitely gain a competitive
    edge in utilizing these LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: The sheer number of parameters in these models equates to unparalleled expressive
    power, enabling these LLMs to capture intricate language patterns and context
    effectively. The massive scale of LLMs such as Llama2 and GPT also allows for
    fine-tuning on specialized tasks and industries — which will be discussed in more
    detail later in this article, as well as in my future articles. Unsurprisingly,
    the vast parameter counts of these LLMs result in superior performance, making
    these models versatile for various language-related applications — from text generation,
    question answering, summarization, translation, zero-shot learning, and beyond.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we’ll explore how to load one of the most popular models today
    — Meta’s Llama2 — across different configurations and using different sizes of
    the model. Whether you’re an NLP engineer, a researcher, or hobbyist eager to
    explore the future (and present) of AI, this article will guide you through the
    basics of loading LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/29a535e5c06be6b564ccfb3b42238ab5.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Paz Arando](https://unsplash.com/@pazarando?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/ZHmvek0kJW8?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  prefs: []
  type: TYPE_NORMAL
- en: What is Llama2?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Developed by Meta as an improvement to Llama1 and released in July 2023, [Llama2](https://ai.meta.com/llama/#inside-the-model)
    is a suite [“pretrained and fine-tuned large language models (LLMs) ranging in
    scale from 7 billion to 70 billion parameters”](https://scontent.fmnl25-1.fna.fbcdn.net/v/t39.2365-6/10000000_662098952474184_2584067087619170692_n.pdf?_nc_cat=105&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=TjG8FsFfH8cAX_uux2_&_nc_ht=scontent.fmnl25-1.fna&oh=00_AfBhA6S3oJWa52QjMx99BmgN3gg06l2jsH2-P2MNIxSmAw&oe=6529307F).
    This year, it has garnered attention because of its exceptional performance in
    comparison to other SOTA models (both open-source and closed-source models) as
    well as its staggering size — with models ranging from 2.7 billion parameters
    to 70 billion parameters. Llama2 has been shown to be capable of understanding,
    generating, and manipulating human language with extraordinary proficiency. Its
    customizability also motivated engineers to adapt it for a wide range of applications.
    Specifically, its flexibility in terms of fine-tuning allows for Llama2 to be
    tailerod to specific tasks, industries, and datasets — empowering engineers to
    explore its applications across industry and academia. Llama2 also comes with
    Llama2-Chat, which is optimized for dialogue use cases, and which we will explore
    later in this article. According to the Llama2 paper, Llama2-Chat utilizes iterative
    applications of alignment techniques, such as instruction tuning and reinforcement
    learning from human feedback. For more details on the model architecture and fine-tuning
    process, as well as an engaging discussion on safety, please see Meta’s official
    paper [here](https://scontent.fmnl25-1.fna.fbcdn.net/v/t39.2365-6/10000000_662098952474184_2584067087619170692_n.pdf?_nc_cat=105&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=TjG8FsFfH8cAX_uux2_&_nc_ht=scontent.fmnl25-1.fna&oh=00_AfBhA6S3oJWa52QjMx99BmgN3gg06l2jsH2-P2MNIxSmAw&oe=6529307F).
  prefs: []
  type: TYPE_NORMAL
- en: Using the HuggingFace Library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this project, we’ll be using the HuggingFace library to load Llama2\. [HuggingFace](https://huggingface.co/)
    is an open-source data science and machine learning (ML) platform that democratizes
    the field of AI — it’s where engineers can find models, datasets, and applications
    (on [HuggingFace Spaces](https://huggingface.co/spaces)) of ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, we’ll use the HuggingFace (HF) Transformers library, which contains
    [different sizes of Meta’s Llama2 model](https://huggingface.co/models?library=transformers&sort=trending&search=meta).
    HF’s Transformers library was originally built as an open-source initiative, and
    today works very well with both PyTorch and TensorFlow. In my NLP journey, I’ve
    benefitted a lot from using HF’s Transformers library because it makes it so much
    easier to access SOTA NLP models, including Llama2\. Through HF’s API, which is
    what I used for this project, we’ll see how to start loading LLMs and other pre-trained
    models. In future articles, I will also share how to use HF’s API to fine-tune
    and deploy these LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7172d6a6dfebca40f6fb00b437aa544b.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Kate Levitskaya](https://unsplash.com/@katereyy?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/CtFF1YUD8vE?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  prefs: []
  type: TYPE_NORMAL
- en: Getting Started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we will explore how to load LLMs (specifically Llama2) in different
    sizes, which will be helpful in different compute setups. *This article is written
    as a companion and and extension of Suhas Pai’s Book* [*Designing Large Language
    Model Applications*](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fwww.oreilly.com%2Flibrary%2Fview%2Fdesigning-large-language%2F9781098150495%2F)*.*
  prefs: []
  type: TYPE_NORMAL
- en: For this project, I initially tested code on Google Colab using the Colab Free
    Tier, although I ran into a number of issues when loading larger models and running
    inference. So, for some of the larger models (like the 13 Billion Parameter Llama2
    Hugging Face model), assume a basic subscription of Google Colab Pro, as of the
    time of writing.
  prefs: []
  type: TYPE_NORMAL
- en: Resource Allocations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To get started, I had the following resource allocations on my Colab notebook
    — the full code can be viewed here.
  prefs: []
  type: TYPE_NORMAL
- en: Python 3 Google Compute Engine backend (GPU)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'System RAM: 51.0 GB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GPU RAM: 40.0 GB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Disk Space: 166.8 GB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hosted Runtime Type: A100'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that to be more efficient and mindful of GPU RAM constraints and Disk Space
    limitations, we cleared the GPU memory after every model run, and we also cleared
    the Disk Space.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4202ce23b001c3830a1af17c7b5faf11.png)'
  prefs: []
  type: TYPE_IMG
- en: Google Colab Resources. *Image from the Author.*
  prefs: []
  type: TYPE_NORMAL
- en: HuggingFace Login
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Llama2 is a gated model, which means that we would need to register for a license
    on the [Meta website](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fai.meta.com%2Fresources%2Fmodels-and-libraries%2Fllama-downloads%2F)
    before using it. This typically takes a day or two, and your registration key
    would be tied to your HuggingFace token. To create and use your own HuggingFace
    token, please see [this](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fhuggingface.co%2Fdocs%2Fhub%2Fsecurity-tokens)
    guide on the offical HuggingFace website.
  prefs: []
  type: TYPE_NORMAL
- en: Since we are using Google Colab, we can use the following code to log in.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/0438ead04de6a2308a6f3a1c6e07ee68.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the Author
  prefs: []
  type: TYPE_NORMAL
- en: Load Libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, we’ll load the libraries we need to start loading the 2.7 Billion Parameter
    Llama2 Chat Model, the 2.7 Billion Parameter Llama2 Model, and the 13 Billion
    Parameter Llama2 Model. In a future article, we will also show how to load the
    70 Billion Parameter Llama2 Model using AWS, so please stay tuned for this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We will use `bitsandbytes` for sharding. For more information on model quantization,
    see the HuuggingFace documentation [here](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fhuggingface.co%2Fdocs%2Ftransformers%2Fmain_classes%2Fquantization).
  prefs: []
  type: TYPE_NORMAL
- en: Loading the Llama2 2.7 Billion Parameter Chat Model via HF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, we’ll start with the smallest Llama2 model available. We can specify the
    model name, based on the HF Transformers library and the associated [Model Card](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fhuggingface.co%2Fmeta-llama%2FLlama-2-7b-chat-hf).
    Then, we will specify the tokenizer — but for this demo, we can just use `AutoTokenizer`
    and use the associated Llama2 pretrained tokenizer. We can then use the `transformers.pipeline`
    function to specify our task, our model, and our PyTorch datatype, before setting
    model-specific parameters such as `top_k` , `do_sample` which to tells the model
    to sample from the logits, rather than using greedy decoding, `max_new_tokens`
    which specifies the number of maximum tokens to return. Note that the Llama2 Chat
    Model is better suited towards dialogues.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: For this project, we will ask different variations of the question “*Outline
    the steps to load a large language model*”.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Then, we will output the generated text.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Result: Outline the steps to load a large language model using Python.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Introduction: Large language models have gained significant attention in recent
    years due to their impressive performance in various natural language processing
    (NLP) tasks. These models require significant computational resources and memory,
    making it challenging to load them onto a local machine. In this article, we will
    outline the steps to load a large language model using Python.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Step 1: Choose a Language Model The first step is to choose a large language
    model that you want to load. There are several popular models available, including
    BERT, RoBERTa, and XLNet. You can choose one of these models or experiment with
    different variations.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Step 2: Download the Model Once you have chosen a language model, you need
    to download it. You can download the pre-trained model from the model’s official
    website or from a reputable data repository. Make sure to download the correct
    version of the model, which may vary depending on the task you are working on.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Step 3: Prepare the Model After downloading the model, you need to prepare
    it for use. This involves loading the model into memory and formatting it according
    to the required format. You can use the `torch` library in Python to load the
    model and perform other NLP tasks.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Step 4: Load the Model To load the model into memory, you can use the `torch.load()`
    function. This function takes the path to the model file and loads it into memory.
    Here is an example: [PRE5]'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Step 5: Fine-tune the Model After loading the model, you may want to fine-tune
    it for your specific task. This involves adjusting the model’s weights to improve
    its performance on your task. You can use the `torch.optim` module to optimize
    the model’s weights. Here is an example: [PRE6]'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Step 6: Use the Model Once the model is loaded and fine-tuned, you can use
    it for your NLP task. You can use the `model()` function to make predictions or
    perform other NLP tasks. Here is an example: [PRE7]'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Conclusion: In this article, we outlined the steps to load a large language
    model using Python. These steps involve choosing a language model, downloading
    the model, preparing the model, loading the model into memory, fine-tuning the
    model, and using the model for your NLP task. By following these steps, you can
    leverage the power of large language models for your NLP tasks.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/3ade20c5d7841a1728349a372a2014d3.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Dušan veverkolog](https://unsplash.com/@veverkolog?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/Hc9G7owDs1s?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  prefs: []
  type: TYPE_NORMAL
- en: Loading the Llama2 2.7 Billion Parameter Model via HF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, we’ll try out the Base Llama2 Model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We also set the `bitsandbytes` configuration above. For more information on
    model quantization, see the HuuggingFace documentation [here](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fhuggingface.co%2Fdocs%2Ftransformers%2Fmain_classes%2Fquantization).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'To test it out, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: You are a helpful, respectful and honest assistant. Always answer as helpfully
    as possible, while being safe. Your answers should not include any harmful, unethical,
    racist, sexist, toxic, dangerous, or illegal content. Please ensure that your
    responses are socially unbiased and positive in nature. If a question does not
    make any sense, or is not factually coherent, explain why instead of answering
    something not correct. If you don’t know the answer to a question, please don’t
    share false information.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Question:** What are the steps to load a large language model using Python?'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Answer:**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'You can use the following steps to load a large language model using Python:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '1.Download the model: You can download the model from the internet and save
    it on your computer.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '2.Import the necessary libraries: You will need to import the necessary libraries
    for loading the model, such as numpy, pandas, and tensorflow.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '3.Define the model: You will need to define the model by specifying the vocabulary
    and the number of dimensions.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '4.Load the model: You will need to load the model by specifying the vocabulary,
    the number of dimensions, and the file path where the model is saved.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '5.Train the model: You will need to train the model by providing the input
    text and the desired output text.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '6.Evaluate the model: You will need to evaluate the model by providing the
    input text and the desired output text and comparing the output to the desired
    output.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '7.Save the model: You will need to save the model by specifying the vocabulary,
    the number of dimensions, and the file path where the model is saved.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '8.Load the model: You will need to load the model by specifying the vocabulary,
    the number of dimensions, and the file path where the model is saved.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '9.Evaluate the model: You will need to evaluate the model by providing the
    input text and the desired output text and comparing the output to the desired
    output.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '10.Save the model: You will need to save the model by specifying the vocabulary,
    the number of dimensions, and the file path where the model is saved.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '11.Load the model: You will need to load the model by specifying the vocabulary,
    the number of dimensions, and the file path where the model is saved.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '12.Evaluate the model: You will need to evaluate the model by providing the
    input text and the desired output text and comparing the output to the desired
    output.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '13.Save the model: You will need to save the model by specifying the vocabulary,
    the number of dimensions, and the file path where the model is saved.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '14.Load the model: You will need to load the model by specifying the vocabulary,
    the number of dimensions, and the file path where the model is saved.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '15.Evaluate the model: You will need to evaluate the model by providing the
    input text and the desired output text and comparing the output to the desired
    output.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '16.Save the model: You will need to save the model by specifying the vocabulary,
    the number of dimensions, and the file path where the model is saved.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '17.Load the model: You will need to load the model by specifying the vocabulary,
    the number of dimensions, and the file path where the model is saved.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '18.Evaluate the model: You will need to evaluate the model by providing the
    input text and the desired output text and comparing the output to the desired
    output.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '19.Save the model: You will need to save the model by specifying the vocabulary,
    the number of dimensions, and the file path where the model is saved.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '20.Load the model: You will need to load the model by specifying the vocabulary,
    the number of dimensions, and the file path where the model is saved.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '21.Evaluate the model: You will need to evaluate the model by providing the
    input text and the desired output text and comparing the output to the desired
    output.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Loading the Llama2 13 Billion Parameter Model via HF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, we’ll also try out an even bigger model — Llama2’s 13 Billion Parameter
    Model. For more information on this specific model, please check out the [Model
    Card](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fhuggingface.co%2Fmeta-llama%2FLlama-2-13b-hf).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As we did earlier in the 2.7 Billion Parameter Model, we'll also be using `bitsandbytes`.
  prefs: []
  type: TYPE_NORMAL
- en: Let's try it out!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Using a similar question as earlier, we get an interesting result:'
  prefs: []
  type: TYPE_NORMAL
- en: You are a helpful, respectful and honest assistant. Always answer as helpfully
    as possible, while being safe. Your answers should not include any harmful, unethical,
    racist, sexist, toxic, dangerous, or illegal content. Please ensure that your
    responses are socially unbiased and positive in nature. If a question does not
    make any sense, or is not factually coherent, explain why instead of answering
    something not correct. If you don’t know the answer to a question, please don’t
    share false information.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Q:** What are the steps to load a large language model using Python? Please
    explain the answer to me in terms that a 5th grader can understand. Thank you.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**A:**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 1\. Download the language model.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 2\. Import the language model.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 3\. Load the language model.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 4\. Use the language model.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Comparing Results and A Brief Prompt Engineering Discussion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we saw how three different sizes and versions of Llama2 performed
    on a simple question answering prompt. Note that we split our prompt into three
    parts. First, we started with the **instruction text**, which specifies how a
    model should answer the question and what its associated constraints are; second,
    including the actual question as the **input text**, which then allows us to ask
    the specific question we would like Llama2 to answer — this can be in the format
    including the prefix “Question” or “Q:”, to be more explicit with the model. Finally,
    we also have the **output text**, which is where the model will actually generate
    the tokens based on our question. Here, we can include “Answer” or “A:” to be
    more explicit with the model as well — in my testing, I’ve seen this to be particularly
    useful when giving Llama2 certain examples of how I want its answers to look like.
    We won’t dive too deeply into prompt engineering in this article, but for more
    information on that, feel free to read this [blog post](https://www.promptingguide.ai/).
  prefs: []
  type: TYPE_NORMAL
- en: What are some aspects of the three results shown above that you found interesting?
    I personally really liked how Llama2 created an answer based on the prompting
    of “explain the answer to me in terms that a 5th grader can understand”. Additionally,
    it was interesting how Llama2’s result in the 2.7 billion parameter looked like
    it was printing out the steps for loading an LLM in a loop — while this makes
    sense conceptually, are there any ways that Llama2 could have answered this question
    in a more intuitive and human-like way?
  prefs: []
  type: TYPE_NORMAL
- en: Other Considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Loading large language models is just scratching the surface to understanding
    and creating useful applications of LLMs. There are a number of ways to improve
    LLM performance — such as parallelism, mixed precision training, model quantization
    (which we used here), caching, batch processing, and using distributed systems.
  prefs: []
  type: TYPE_NORMAL
- en: All of these are exciting avenues to explore, and I hope that this blog post,
    along with its accompanying notebook, could give you a good starting point for
    your LLM journey!
  prefs: []
  type: TYPE_NORMAL
- en: The full code can be viewed [here](https://github.com/piesauce/llm-playbooks/blob/loading_llms/ateng/notebooks/Loading_LLMs_Basic.ipynb).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This article is written as a companion and extension of Suhas Pai’s Book [Designing
    Large Language Model Applications](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fwww.oreilly.com%2Flibrary%2Fview%2Fdesigning-large-language%2F9781098150495%2F),
    and was written as part of the book’s LLM repo.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Purchase [my book here](https://www.amazon.com/Data-Resource-Emerging-Countries-Landscape/dp/1641372524)
    and please do report any bugs or suggestions to this article via [email](http://at2507@nyu.edu).
    Please feel free to email me about collaborations as well! I’m open to new projects
    at the moment.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Connect with me via [LinkedIn](https://www.linkedin.com/in/angelavteng/), [Twitter](https://twitter.com/ambervteng),
    or [TikTok](https://www.tiktok.com/@angelaworkout).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Follow me on [Medium](https://medium.com/@angelamarieteng).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*References:*'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[https://ai.meta.com/llama/](https://ai.meta.com/llama/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://about.fb.com/news/2023/07/llama-2/](https://about.fb.com/news/2023/07/llama-2/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://scontent.fmnl25-1.fna.fbcdn.net/v/t39.2365-6/10000000_662098952474184_2584067087619170692_n.pdf?_nc_cat=105&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=TjG8FsFfH8cAX_uux2_&_nc_ht=scontent.fmnl25-1.fna&oh=00_AfBhA6S3oJWa52QjMx99BmgN3gg06l2jsH2-P2MNIxSmAw&oe=6529307F](https://scontent.fmnl25-1.fna.fbcdn.net/v/t39.2365-6/10000000_662098952474184_2584067087619170692_n.pdf?_nc_cat=105&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=TjG8FsFfH8cAX_uux2_&_nc_ht=scontent.fmnl25-1.fna&oh=00_AfBhA6S3oJWa52QjMx99BmgN3gg06l2jsH2-P2MNIxSmAw&oe=6529307F)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://huggingface.co/](https://huggingface.co/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://huggingface.co/transformers/v4.10.1/main_classes/pipelines.html](https://huggingface.co/transformers/v4.10.1/main_classes/pipelines.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.Text2TextGenerationPipeline](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.Text2TextGenerationPipeline)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://huggingface.co/docs/transformers/main/llm_tutorial](https://huggingface.co/docs/transformers/main/llm_tutorial)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.promptingguide.ai/](https://www.promptingguide.ai/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://huggingface.co/docs/transformers/main_classes/quantization](https://huggingface.co/docs/transformers/main_classes/quantization)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://huggingface.co/models?library=transformers&sort=trending&search=meta](https://huggingface.co/models?library=transformers&sort=trending&search=meta)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://huggingface.co/meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://huggingface.co/blog/4bit-transformers-bitsandbytes](https://huggingface.co/blog/4bit-transformers-bitsandbytes)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
