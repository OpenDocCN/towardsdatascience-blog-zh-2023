- en: 'Machine Learning in Three Steps: How to Efficiently Learn It'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/machine-learning-in-three-steps-how-to-efficiently-learn-it-aefcf423a9e1](https://towardsdatascience.com/machine-learning-in-three-steps-how-to-efficiently-learn-it-aefcf423a9e1)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Prioritizing the Essentials for Predictive Modeling without Overwhelming Yourself
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@angela.shi?source=post_page-----aefcf423a9e1--------------------------------)[![Angela
    and Kezhan Shi](../Images/a89d678f2f3887c0c2ff3928f9d767b4.png)](https://medium.com/@angela.shi?source=post_page-----aefcf423a9e1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----aefcf423a9e1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----aefcf423a9e1--------------------------------)
    [Angela and Kezhan Shi](https://medium.com/@angela.shi?source=post_page-----aefcf423a9e1--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----aefcf423a9e1--------------------------------)
    ·21 min read·Mar 3, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: I have observed two extreme approaches when it comes to aspiring data scientists
    attempting to learn machine learning algorithms. The first approach involves learning
    **all the intricacies** of the algorithms and implementing them from scratch to
    gain true mastery. The second approach, on the other hand, assumes that the computer
    will “learn” on its own, rendering the need for the individual to learn the algorithms
    unnecessary. This leads some to **only** rely on tools such as the package lazypredict.
  prefs: []
  type: TYPE_NORMAL
- en: It is realistic to take an approach between the two extremes when learning machine
    learning algorithms. However, the question remains, where to start? In this article,
    I will categorize machine learning algorithms into three categories and provide
    my humble opinion on what to begin with and what can be skipped.
  prefs: []
  type: TYPE_NORMAL
- en: The complexity of machine learning algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Starting out in machine learning can be overwhelming due to the multitude of
    available algorithms. Linear regression, support vector machines (SVM), gradient
    descent, gradient boosting, decision trees, LASSO, ridge, grid search, and many
    more are some of the algorithms that come to mind when posed with the question.
  prefs: []
  type: TYPE_NORMAL
- en: In the field of supervised learning, these algorithms serve different purposes
    and have different objectives. In this article, we will only talk about supervised
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: To gain a better understanding of the various techniques, it can be helpful
    to categorize them according to their objectives and levels of complexity. By
    organizing these algorithms into different categories and levels of complexity,
    one can simplify these concepts and make them easier to understand. This approach
    can greatly enhance one’s comprehension of machine learning and help to identify
    the most appropriate techniques to use for a particular task or objective.
  prefs: []
  type: TYPE_NORMAL
- en: As students delve into the field of machine learning, they may become discouraged
    due to its complexity. However, it’s not necessary to learn or be familiar with
    all of the algorithms before putting them into practice. Different positions in
    the field of machine learning may demand diverse levels of proficiency, and it’s
    acceptable to lack knowledge of certain aspects. For instance, data scientists,
    data analysts, data engineers, and machine learning researchers have different
    requirements for their job roles.
  prefs: []
  type: TYPE_NORMAL
- en: Having a broad understanding of the overall process can enable machine learning
    practitioners to skip certain technical details if they are short on time, while
    still comprehending the process as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/57f99cc62121b59e1011c724286b6b76.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by Julio Wolf on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Breaking Down Machine Learning Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 1.1 Models, training, and tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The scope of “machine learning algorithms” is quite broad, and it can be categorized
    into three main types of algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: (1) **machine learning models** which are designed to receive input data and
    subsequently generate predictions, such as linear regression, SVM, decision tree,
    KNN, etc. In the scikit-learn library, these are also referred to as “**estimators**”.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (2) the **model training/fitting algorithms** that are used to create or optimize
    the models, aka find the parameters of the models for a specific dataset. And
    different machine learning models have their specific training algorithms. Although
    **gradient descent** is the most well-known method for training mathematical function-based
    models, other machine learning models can be trained using different techniques,
    which we will explore in more detail in the later sections of this article.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (3) **hyperparameter tuning** that consists of finding the optimal hyperparameters
    of the machine learning models. And contrary to the training process, the process
    of hyperparameter tuning usually does not depend on the machine learning models.
    **Grid search** is a popular and commonly used method for this task, although
    there are alternative approaches that we will delve into later in this article.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1.2 Three Categories of ML models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first aspect deals with the models that are capable of taking in data and
    generating predictions based on that data. These models can be categorized into
    three families:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Distance-based** models, which include K-nearest-neighbors, Linear Discriminant
    Analysis, and Quadratic Discriminant Analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decision tree-based** models, such as a single decision tree (used for classification
    or regression), Random Forest, and Gradient-Boosted Decision Trees.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mathematical functions-based** models: also known as parametric models, are
    models that assume a specific functional form for the relationship between the
    inputs and the outputs. They can be further divided into **linear models** like
    OLS regression, SVM (with a linear kernel), Ridge, and LASSO, and **non-linear
    models** like SVM with non-linear kernels and neural networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1.3 Meta-models and ensemble methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In machine learning, a meta-model is a model that combines the predictions of
    multiple individual models to make more accurate predictions. It is also known
    as a “stacked model” or “super learner”. The individual models that make up the
    meta-model can be of different types or use different algorithms, and their predictions
    are combined using a weighted average or other techniques.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of a meta-model is to improve the overall accuracy and robustness of
    predictions by reducing the variance and bias that may be present in individual
    models. It can also help to overcome the limitations of individual models by capturing
    more complex patterns in the data.
  prefs: []
  type: TYPE_NORMAL
- en: One common approach to creating a meta-model is to use ensemble methods such
    as bagging, boosting, or stacking.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging, or Bootstrap Aggregating, is a technique used in machine learning to
    reduce the variance of a model by combining multiple models trained on different
    samples of the dataset. The idea behind bagging is to generate several models,
    each with a subset of the data, and then combine them to create a more robust
    model that is less susceptible to overfitting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Boosting: Boosting is another ensemble method that combines multiple weak models
    to create a strong model. In contrast to bagging, which trains each model independently,
    boosting trains models in a sequence. Each new model is trained on the data that
    was misclassified by the previous models, and the final prediction is made by
    aggregating the predictions of all the models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stacking: Stacking, or stacked generalization, is a meta-model ensemble method
    that involves training multiple base models and using their predictions as input
    to a higher-level model. The higher-level model learns how to combine the predictions
    of the base models to make a final prediction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Random forests: it is an extension of bagging that adds an extra layer of randomness.
    In addition to randomly sampling the data, random forests also randomly select
    a subset of features for each split. This helps to reduce overfitting and increase
    the diversity of the models in the ensemble.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble methods are most commonly applied to decision trees rather than linear
    models such as linear regression. This is because decision trees are more prone
    to overfitting than linear models, and ensemble methods help to reduce overfitting
    by combining multiple models.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees have high variance and low bias, meaning that they are prone
    to overfitting the training data, resulting in poor performance on new, unseen
    data. Ensemble methods address this issue by aggregating the predictions of multiple
    decision trees, resulting in a more robust and accurate model.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, linear models such as linear regression have a low variance
    and high bias, meaning that they are less prone to overfitting but may underfit
    the training data. Ensemble methods are not as effective for linear models because
    the models already have a low variance and do not benefit as much from aggregation.
  prefs: []
  type: TYPE_NORMAL
- en: However, there are still some cases where ensemble methods can be applied to
    linear models. For example, the bootstrap aggregating technique used in bagging
    can be applied to any type of model, including linear regression. In this case,
    the bagging algorithm would sample the training data and fit multiple linear regression
    models on the bootstrapped samples, resulting in a more stable and robust model.
    However, it’s worth noting that the resulting model is still a linear regression
    model, not a meta-model.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, while ensemble methods are most commonly applied to decision trees,
    there are some cases where they can be used with linear models. However, it’s
    important to keep in mind the strengths and limitations of each type of model
    and choose the appropriate method for the problem at hand.
  prefs: []
  type: TYPE_NORMAL
- en: 1.4 Overview of Machine Learning Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The diagram below provides a summary of the various machine learning algorithms
    classified into the 3 categories. The subsequent sections of this article will
    delve deeper into each category.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/54c901c9803cad52e3611de1cc5d8263.png)'
  prefs: []
  type: TYPE_IMG
- en: Overview of machine learning algorithms — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Machine learning models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will have a closer look at the three families of machine
    learning models. here is a more detailed plan about the
  prefs: []
  type: TYPE_NORMAL
- en: (1) Distance-based models
  prefs: []
  type: TYPE_NORMAL
- en: 'Instance-based models: KNN'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bayes classifiers: LDA, QDA'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (2) Decision trees-base models
  prefs: []
  type: TYPE_NORMAL
- en: (3) Matemathetical function-based models
  prefs: []
  type: TYPE_NORMAL
- en: Linear models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kernelized models such as kernel SVM or kernel ridge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2.1 Instance-based models**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first family of machine learning models is distance-based models. These
    models use the distance between data points to make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest and most representative model is K-Nearest Neighbors (KNN). It
    calculates the distance between the new data point and all the existing data points
    in the dataset. It then selects the K-nearest neighbors and assigns the new data
    point to the class that is most common among the K neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: When examining k-nearest neighbors (KNN) algorithm, it can be noted that there
    is no explicit model built during the training phase. In KNN, the prediction for
    a new observation is made by finding the k nearest neighbors to that observation
    in the training set and taking the average or majority vote of their target values.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike other algorithms that fit a model to the data during training, KNN stores
    the entire training dataset and simply computes distances between new observations
    and the existing dataset to make predictions. Therefore, KNN can be considered
    a “lazy learning” algorithm, as it does not actively build a model during the
    training phase, but rather defers the decision-making process until inference
    time.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, the inference/testing phase may be slow. It’s important to note
    that more efficient algorithms such as k-d tree can be used.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Bayes classifiers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Linear Discriminant Analysis** (LDA) and **Quadratic Discriminant Analysis**
    (QDA) are distance-based models that use **Mahalanobis distance** to make predictions.
    Mahalanobis distance is a measure of the distance between a point and a distribution,
    taking into account the correlation between variables.'
  prefs: []
  type: TYPE_NORMAL
- en: Linear Discriminant Analysis (LDA) assumes that the variance is the same across
    different classes, whereas Quadratic Discriminant Analysis (QDA) assumes that
    the variance is different for each class. This means that LDA assumes that the
    covariance matrix is the same for all classes, while QDA allows for each class
    to have its own covariance matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Decision tree-based models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The second family of machine learning models is decision tree-based models.
    They also can be called rule-based models, this means that the model generates
    a set of rules that can be used to explain how it arrived at its decision or prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Each branch of a decision tree represents a rule or condition, which is used
    to determine which subset of data to follow next. These rules are typically in
    the form of simple if-then statements, such as “if the value of variable X is
    greater than 5, then follow the left branch, otherwise follow the right branch.”
  prefs: []
  type: TYPE_NORMAL
- en: The final leaf nodes of the decision tree represent the predicted class or value
    of the target variable based on the values of the input variables and the rules
    that led to that prediction.
  prefs: []
  type: TYPE_NORMAL
- en: One advantage of decision trees is that they are easy to interpret and understand
    since the rules can be visualized and explained in a clear and intuitive manner.
    This makes them useful for explaining the reasoning behind a prediction or decision
    to non-technical stakeholders.
  prefs: []
  type: TYPE_NORMAL
- en: However, decision trees can also be prone to overfitting, which occurs when
    the model becomes too complex and fits the training data too closely, resulting
    in poor generalization to new data. To address this issue, ensemble methods are
    commonly applied to decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Mathematical functions-based models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The third family of machine learning models is mathematical functions-based
    models. These models use mathematical functions to model the relationship between
    the input variables and the target variable. Linear models, such as Ordinary Least
    Squares (OLS) regression, and Support Vector Machines (SVM) with a linear kernel,
    Ridge, and LASSO, assume that the relationship between the input variables and
    the target variable is linear. Non-linear models, such as SVM with non-linear
    kernels and Neural Networks, can model more complex relationships between the
    input variables and the target variable.
  prefs: []
  type: TYPE_NORMAL
- en: For mathematical function-based models, such as linear regression or logistic
    regression, we have to define a loss function. The loss function measures how
    well the model’s predictions match the actual data. The goal is to minimize the
    loss function by adjusting the parameters of the model.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, for non-mathematical function-based models, such as KNN or decision
    trees, we don’t need to define a loss function. Instead, we use a different approach,
    such as finding the nearest neighbors in the case of KNN or recursively splitting
    the data based on the feature values in the case of decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: Defining an appropriate loss function is crucial in mathematical function-based
    models, as it determines the optimization problem that the model solves. Different
    loss functions can be used depending on the problem at hand, such as mean squared
    error for regression problems or cross-entropy for binary classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that all the linear models, such as Ordinary Least Squares
    (OLS), LASSO, Ridge, and Support Vector Machines (SVM) with linear kernel, can
    be written in the form of a linear equation y = wX + b. However, the difference
    between these models lies in the cost function used to estimate the optimal values
    of the model parameters w and b.
  prefs: []
  type: TYPE_NORMAL
- en: So, while it is true that all these models can be written in the form of the
    same mathematical function, it is important to note that the choice of cost function
    determines the behavior and performance of the model. Hence, it would be more
    accurate to consider them as different models with different cost functions, rather
    than the same model with different cost functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Non-linear models are powerful tools for solving complex machine learning problems
    that cannot be adequately addressed by linear models. And there are essentially
    two approaches used in practice: the kernel trick and neural networks.'
  prefs: []
  type: TYPE_NORMAL
- en: The kernel trick is a way of implementing feature mapping efficiently without
    explicitly computing the transformed features. Instead, it involves defining a
    kernel function that calculates the similarity between pairs of input samples
    in the transformed feature space. By using the kernel function, we can implicitly
    map the input data to a high-dimensional space, where it can be more easily separated
    and modeled.
  prefs: []
  type: TYPE_NORMAL
- en: In this sense, the kernel part can be seen as a form of feature engineering,
    where the model is able to create new features that are more suitable for the
    task at hand. This is in contrast to traditional feature engineering, where human
    experts manually create new features based on domain knowledge and intuition.
  prefs: []
  type: TYPE_NORMAL
- en: Another approach to creating non-linear models is through the use of neural
    networks. They consist of layers of interconnected nodes or “neurons,” each of
    which performs a simple mathematical operation on its inputs and passes the result
    to the next layer.
  prefs: []
  type: TYPE_NORMAL
- en: The key to the power of neural networks is their ability to learn complex non-linear
    relationships between inputs and outputs. This is accomplished by adjusting the
    weights of the connections between neurons during training, based on the error
    between the predicted output and the actual output.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Deep learning models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning is focused on learning representations of data through a hierarchy
    of multiple layers. It has become increasingly popular in recent years due to
    its success in a wide range of applications, including computer vision, natural
    language processing, and speech recognition. While deep learning models can be
    considered complex due to their large number of parameters and layers, it’s true
    that a significant part of deep learning is also about feature engineering.
  prefs: []
  type: TYPE_NORMAL
- en: One example of a deep learning model is a convolutional neural network (CNN).
    At its core, a CNN applies a series of filters to an input image, with each filter
    looking for a specific feature such as edges or corners. The later layers of the
    network then use these extracted features to classify the input image.
  prefs: []
  type: TYPE_NORMAL
- en: In this way, deep learning models like CNNs can be thought of as a combination
    of feature engineering and a trainable model. The feature engineering aspect of
    the model involves designing the architecture of the network to extract useful
    features from the input data, while the trainable model involves optimizing the
    network’s parameters to fit the data and make accurate predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Model training/fitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training a machine learning model is the process of teaching the model to make
    predictions or decisions by showing it a set of labeled examples. The labeled
    examples, also known as the training data, consist of pairs of input features
    and output labels.
  prefs: []
  type: TYPE_NORMAL
- en: During the training process, the machine learning model learns to recognize
    patterns in the input features and their corresponding output labels. The model
    uses a specific algorithm to learn from the training data and adjust its internal
    parameters in order to improve its ability to predict or classify new data.
  prefs: []
  type: TYPE_NORMAL
- en: Once the model has been trained on the labeled examples, it can be used to make
    predictions or decisions on new, unseen data. This process is known as inference
    or testing.
  prefs: []
  type: TYPE_NORMAL
- en: Different machine learning models have different training algorithms. Here are
    some examples of training algorithms used by different machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Distance-based models training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'K-Nearest Neighbors (KNN): KNN is a non-parametric algorithm that does not
    require explicit training. Instead, it stores the entire training dataset and
    uses it to predict the label of a new instance by finding the K closest instances
    in the training dataset based on some distance metric. The prediction is then
    based on the majority vote of the K nearest neighbors.'
  prefs: []
  type: TYPE_NORMAL
- en: Linear Discriminant Analysis (LDA) is a supervised learning algorithm used for
    classification tasks. LDA models the distribution of the input features for each
    class and uses this information to find a linear combination of the input features
    that maximizes the separation between classes. The resulting linear discriminants
    can then be used to classify new instances.
  prefs: []
  type: TYPE_NORMAL
- en: The training process for LDA involves estimating the mean and covariance matrix
    of the input features for each class. These estimates are then used to compute
    the within-class and between-class scatter matrices, which are used to derive
    the linear discriminants. The number of linear discriminants is equal to the number
    of classes minus one.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Decision tree-based models training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As for decision trees, they are trained using a different approach called recursive
    partitioning.
  prefs: []
  type: TYPE_NORMAL
- en: Recursive partitioning is a top-down process that starts with the entire dataset
    and splits it into subsets based on a set of rules or conditions. The splitting
    process is repeated recursively on each subset until a stopping criterion is met,
    typically when the subsets become too small or no further splitting improves the
    model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: The splitting rules are based on features or attributes of the dataset, and
    the algorithm selects the feature that provides the most significant improvement
    in model performance at each step. The splitting process results in a tree-like
    structure, where the internal nodes represent splitting conditions, and the leaf
    nodes represent the final predictions.
  prefs: []
  type: TYPE_NORMAL
- en: During the training process, decision trees can be evaluated using a variety
    of metrics, such as information gain or Gini impurity, to determine the best splitting
    criteria. Once the tree is trained, it can be used to make predictions on new,
    unseen data by following the path from the root node to the appropriate leaf node
    based on the input features.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Mathematical functions-based models training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Mathematical functions-based models, also known as parametric models, are models
    that assume a specific functional form for the relationship between the inputs
    and the outputs.
  prefs: []
  type: TYPE_NORMAL
- en: The most basic algorithm used for optimizing the parameters of mathematical
    functions-based models is gradient descent. Gradient descent is an iterative optimization
    algorithm that starts with an initial guess for the parameter values and then
    updates them based on the gradient of the loss function with respect to the parameters.
    This process continues until the algorithm converges to a minimum of the loss
    function.
  prefs: []
  type: TYPE_NORMAL
- en: For non-convex functions, stochastic gradient descent (SGD) is often used instead
    of gradient descent. SGD randomly samples a subset of the data at each iteration
    to compute the gradient, which can make it faster and more efficient than gradient
    descent.
  prefs: []
  type: TYPE_NORMAL
- en: In neural networks, backpropagation is used to compute the gradients of the
    loss function with respect to the parameters. Backpropagation is essentially just
    the chain rule of calculus applied to the composite function represented by the
    neural network. It allows the gradient to be efficiently computed for each layer
    of the network, which is essential for training deep neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: For deep learning models, more advanced optimization techniques are often used
    to improve performance. These include techniques such as momentum, which helps
    the algorithm to avoid getting stuck in local minima, and adaptive learning rate
    methods, which automatically adjust the learning rate during training to improve
    convergence speed and stability.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, gradient descent is the basic algorithm used for optimizing the
    parameters of mathematical functions-based models. For non-convex functions, stochastic
    gradient descent is often used instead. Backpropagation is used for computing
    gradients in neural networks, and more advanced techniques are often used for
    deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Model tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The third facet of machine learning involves optimizing the model’s hyperparameters
    through the use of grid search. Hyperparameters are the settings or configurations
    of the model that are not learned during training but instead must be manually
    specified.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of hyperparameters include the learning rate, the number of hidden
    layers in a neural network, and the regularization strength. Through the use of
    grid search, multiple combinations of hyperparameters are evaluated to determine
    the optimal configuration for the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Grid search is a popular technique used to optimize the hyperparameters of
    machine learning models. However, it is not the only approach available, and there
    are several other alternatives that can be used to fine-tune the parameters of
    a model. Some of the most popular alternatives to grid search include:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Randomized grid search: In contrast to grid search, random search involves
    randomly sampling hyperparameters from a predefined range, allowing for a more
    efficient exploration of the parameter space.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Bayesian optimization: Bayesian optimization uses probability models to find
    the optimal set of hyperparameters by iteratively evaluating the model’s performance
    and updating the probability distributions of the hyperparameters.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Genetic algorithms: Genetic algorithms mimic the process of natural selection
    to find the optimal set of hyperparameters by generating a population of potential
    solutions, evaluating their performance, and selecting the fittest individuals
    for reproduction.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Gradient-based optimization: Gradient-based optimization involves using gradients
    to iteratively adjust the hyperparameters, with the aim of maximizing the performance
    of the model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Ensemble-based optimization: Ensemble-based optimization involves combining
    multiple models with different hyperparameters to create a more robust and accurate
    final model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each of these alternatives has its own advantages and disadvantages, and the
    best approach may depend on the specific problem being addressed, the size of
    the parameter space, and the computational resources available.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Tips to Help You Learn Machine Learning Effectively
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a general understanding of the different categories of machine
    learning algorithms, let’s explore what we need to learn in order to create effective
    predictive models.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Algorithms too hard to learn?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s begin with some algorithms that may appear complex at first glance, leading
    you to believe that machine learning is a challenging field. However, by breaking
    down the process into the three phases (model, fitting, and tuning), you will
    be able to gain a clearer understanding.
  prefs: []
  type: TYPE_NORMAL
- en: For example, learning Support Vector Machines (SVM) can be daunting for aspiring
    data scientists due to the abundance of technical terms such as optimal hyperplane,
    unconstrained minimization, duality (prima and dual forms), Lagrange multipliers,
    Karush-Kuhn-Tucker conditions, quadratic programming, and more. However, it’s
    essential to understand that SVM is just a linear model, much like OLS regression,
    with the equation y = wX+b.
  prefs: []
  type: TYPE_NORMAL
- en: While the various techniques mentioned above are utilized to optimize SVM using
    different methods, it is crucial to not become bogged down in the technicalities
    and instead focus on the fundamental concepts of SVM as a linear model.
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in further exploring this viewpoint, I will be writing
    an article on the topic. Please let me know in the comments.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Understanding the Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have discussed the three types of machine learning algorithms — models, fitting
    algorithms, and tuning algorithms. In my view, it is important for aspiring data
    scientists to prioritize understanding the models over the other two steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking from this standpoint, the machine learning models that are categorized
    into three main types help to understand their functioning:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Distance-based models: In this type, KNN is not considered a proper model as
    the distance of a new observation is calculated directly. In LDA or QDA, the distance
    is calculated to a distribution.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Decision tree-based models: Decision trees follow if-else rules and form a
    set of rules that can be used for decision-making.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Mathematical functions-based models: they can be less intuitive to understand.
    However, the functions are usually simple.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once you have a solid understanding of how the models work, fitting and tuning
    can be done using pre-existing packages: for fitting, the popular scikit-learn
    library offers a `model.fit` method, while for tuning, tools like Optuna provide
    efficient study optimization techniques with `study.optimize`. By focusing on
    understanding the models themselves, aspiring data scientists can better equip
    themselves for success in the field.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For some individual models, if you adopt this approach, you can improve your
    understanding. I will write separate articles, but here are some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: polynomial regression is a linear regression, after transforming the features
    by raising them to different powers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression, ridge, LASSO, and SVR are the same model, only the underlying
    cost functions are different.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression, logistic regression, and SVM are the same model, only the
    underlying cost functions are different. And here you may notice that linear regression
    is a regressor whereas logistic regression and SVM are classifiers. Well, read
    the documentation of SGDClassifier or have a look at this article about [SGDClassifier](https://medium.com/code-like-a-girl/sgdclassifier-with-scikit-learn-untaught-lessons-you-need-to-know-7bf2f56c04dc).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These [Top 10 Most Common Yet Confusing Machine Learning Model Names](/10-most-common-yet-confusing-machine-learning-model-names-e70595eef514)
    illustrate that understanding the models is not always straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Visualizing the models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Visualizations can be an incredibly helpful tool in understanding the models.
    When working with machine learning models, creating visualizations with simple
    datasets can help illustrate how the models are created and how they work.
  prefs: []
  type: TYPE_NORMAL
- en: Here are some articles that I have written, and you can find the links below.
    These articles cover topics such as the visualization of linear regression, which
    can also be applied to ridge, lasso, and SVM, as well as neural networks. Additionally,
    there is an article about the implementation of KNN in Excel.
  prefs: []
  type: TYPE_NORMAL
- en: Another method is to implement the models in Excel, as it can provide a tangible
    way to see the data and the model’s outputs.
  prefs: []
  type: TYPE_NORMAL
- en: '[Visualization of linear regression](/linear-regression-visualized-and-better-understood-c8f7b9c69810)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Visualization of neural networks](https://medium.com/geekculture/a-visual-definition-of-artificial-neural-networks-part-1-24dc5b94958)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Visualization of Decision Tree Regressors](/decision-tree-regressor-a-visual-guide-with-scikit-learn-2aa9e01f5d7f)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Nearest Neighbors Regressors — A Visual Guide](https://medium.com/towards-data-science/nearest-neighbors-regressors-a-visual-guide-78595b78072e)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stay tuned, I will publish more!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/20698b15e54fdab9c0ce87e530a62dc0.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Visualization of linear regression](/linear-regression-visualized-and-better-understood-c8f7b9c69810)
    — image by author'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/acd5a0286b2b3d43450d3e604b5d5292.png)'
  prefs: []
  type: TYPE_IMG
- en: '[K Nearest Neighbors Regressors with different feature scales](https://medium.com/towards-data-science/nearest-neighbors-regressors-a-visual-guide-78595b78072e)
    — image by author'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Using Excel to Understand the Fitting Process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Understanding the fitting process can be daunting at first. However, if you
    want to learn, it’s important to start with a solid understanding of how the model
    works. One tool that can be particularly helpful in this regard is Microsoft Excel.
  prefs: []
  type: TYPE_NORMAL
- en: Excel is a widely used spreadsheet program that can be used to visualize and
    manipulate data. In the context of machine learning, it can be used to demonstrate
    how the fitting process works for simple models like linear regression. By using
    Excel, you can see how the algorithm is actually implemented step by step.
  prefs: []
  type: TYPE_NORMAL
- en: One important thing to keep in mind is that Excel is not the most efficient
    tool for machine learning. While it can be a useful way to gain a basic understanding
    of the fitting process with a simple dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Using Excel to understand the fitting process can be a helpful tool for beginners
    in machine learning. It provides a simple and accessible way to visualize the
    algorithms and see how they work.
  prefs: []
  type: TYPE_NORMAL
- en: I have written several articles on gradient descent for linear regression, logistic
    regression, and neural networks. You can access these articles through the following
    links.
  prefs: []
  type: TYPE_NORMAL
- en: '[**K-Nearest neighbors** in Excel](/k-nearest-neighbors-in-excel-76edd4102a5b)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Linear Regression With Gradient Descent in Excel](/linear-regression-from-scratch-in-excel-3d8192214752)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Logistic Regression With Gradient Descent in Excel](/logistic-regression-with-gradient-descent-in-excel-52a46c46f704)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Neural Network Classifier from Scratch in Excel](/neural-network-from-scratch-in-excel-4774f6131cdb)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Decision Tree Regressors in Excel](/decision-tree-regressor-in-excel-2d29d16df1db)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Implementing KNN in Excel](/k-nearest-neighbors-in-excel-76edd4102a5b)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[K-means from Scratch in Excel](/k-means-from-scratch-in-excel-bb60778d186e)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Neural Network with Backpropagation in Excel](https://pub.towardsai.net/building-a-neural-network-with-backpropagation-in-excel-b6c5d41b1c2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stay tuned more articles are coming!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Additionally, if you would like to receive the corresponding Excel/Google sheet
    files, please consider supporting me on Kofi via this link: [https://ko-fi.com/s/4ddca6dff1](https://ko-fi.com/s/4ddca6dff1).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4ab61635bd2890369cf005687b74a267.png)'
  prefs: []
  type: TYPE_IMG
- en: machine learning algorithms with Excel — image by author
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Testing with simple datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To gain a comprehensive understanding of machine learning algorithms, implementing
    them from scratch can be an effective approach. However, this method can be quite
    time-consuming and may require a high level of technical proficiency. An alternative
    approach is to use pre-existing packages or libraries to create and visualize
    the output of the model with simple datasets.
  prefs: []
  type: TYPE_NORMAL
- en: By utilizing these packages, you can easily experiment with different parameters
    and test various machine learning algorithms. This approach can help you to understand
    the inner workings of the algorithms while also enabling you to quickly assess
    their effectiveness on specific datasets.
  prefs: []
  type: TYPE_NORMAL
- en: By using such datasets, it becomes easy to visualize both the inputs and outputs
    of the model. This, in turn, allows for greater insight into how the model is
    making predictions. Moreover, by changing the hyperparameters and other aspects
    of the model, we can also visualize their impact on the model’s predictions.
  prefs: []
  type: TYPE_NORMAL
- en: This approach can help beginners to get started with machine learning and develop
    a better understanding of how different algorithms work. It is an excellent way
    to gain practical experience and experiment with different models without spending
    too much time on implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In conclusion, machine learning can be a complex field to navigate, especially
    for aspiring data scientists. However, understanding the three main types of machine
    learning algorithms — models, fitting algorithms, and tuning algorithms — and
    categorizing them based on their objectives and complexity can help provide an
    overall understanding of how they work. By prioritizing understanding the models,
    visualizing them, and implementing them in tools like Excel, we can demystify
    the fitting and tuning process.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s essential to keep learning about different aspects of machine learning,
    such as classification vs regression, handling missing values, and variable importance,
    to deepen our understanding of this field continuously. If you want to learn more,
    check out this article: [Overview of Supervised Machine Learning Algorithms](/overview-of-supervised-machine-learning-algorithms-a5107d036296).'
  prefs: []
  type: TYPE_NORMAL
- en: 'I write about machine learning and data science and I try to simplify complex
    concepts in a clear way. Please follow me with the link below and get full access
    to my articles: [https://medium.com/@kezhanshi/membership](https://medium.com/@angela.shi/membership)'
  prefs: []
  type: TYPE_NORMAL
