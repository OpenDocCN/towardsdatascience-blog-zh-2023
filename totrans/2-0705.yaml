- en: 'Democratizing AI: MosaicML’s Impact on the Open-Source LLM Movement'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/democratizing-ai-mosaicmls-impact-on-the-open-source-llm-movement-7972ff12dd92](https://towardsdatascience.com/democratizing-ai-mosaicmls-impact-on-the-open-source-llm-movement-7972ff12dd92)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How high-quality base models unlock new possibilities for an entire industry…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://wolfecameron.medium.com/?source=post_page-----7972ff12dd92--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----7972ff12dd92--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7972ff12dd92--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7972ff12dd92--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----7972ff12dd92--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7972ff12dd92--------------------------------)
    ·13 min read·Oct 15, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4fb50fe15a17b036375d4670a4939ea5.png)'
  prefs: []
  type: TYPE_IMG
- en: (Photo by [Raimond Klavins](https://unsplash.com/@raimondklavins?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/Ql6JhGdbQg0?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))
  prefs: []
  type: TYPE_NORMAL
- en: Recently, we have overviewed a lot of current research on the creation of open-source
    large language models (LLMs). Across all of this work, models are created using
    a common framework with a few simple components; see below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/56f9033518d72f0d72b0228371d7398c.png)'
  prefs: []
  type: TYPE_IMG
- en: Multi-step process for creating and refining an LLM (from [12, 13])
  prefs: []
  type: TYPE_NORMAL
- en: Although this framework has several steps, the first step is arguably the most
    important. Creating a more powerful base model via extensive, high-quality pre-training
    enables better results when the LLM is refined via supervised fine-tuning (SFT)
    and reinforcement learning from human feedback (RLHF). Then, downstream applications
    are better due to the use of an improved model. The pre-trained (base) model is
    the common starting point for any LLM application.
  prefs: []
  type: TYPE_NORMAL
- en: Until recently, open-source base models either performed poorly compared to
    their proprietary counterparts or could only be used for research. However, this
    changed with the release of MPT-7B and MPT-30B [1, 2] by MosaicML. These open-source
    base models achieve impressive levels of performance, are free for commercial
    use, and come with an entire suite of efficient software for training, fine-tuning,
    and evaluating LLMs. These open-source tools enable a wide variety of specialized
    use cases for LLMs to be explored at a significantly reduced cost, making them
    a powerful resource for practitioners in AI.
  prefs: []
  type: TYPE_NORMAL
- en: Faster LLMs and Longer Context Lengths
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The MPT-7B/30B models are based upon a typical, [decoder-only transformer](/language-models-gpt-and-gpt-2-8bdb9867c50a)
    architecture. However, a few key modifications are made, including:'
  prefs: []
  type: TYPE_NORMAL
- en: '[ALiBi](https://docs.mosaicml.com/projects/composer/en/stable/method_cards/alibi.html)
    [6] (instead of normal position embeddings)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Low precision layer norm](https://docs.mosaicml.com/projects/composer/en/latest/method_cards/low_precision_layernorm.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Flash Attention](https://github.com/HazyResearch/flash-attention) [7]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Within this section, we will learn about each of these components, how they
    work, and their impact on LLMs. To fully understand the details of this section,
    it might be useful to review the following concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: Self-Attention [[link](https://twitter.com/cwolferesearch/status/1641932082283700226?s=20)]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Causal Self-Attention (used by decoder-only LLMs) [[link](https://twitter.com/cwolferesearch/status/1644773244786941952?s=20)]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ALiBi enables context length extrapolation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/80ebdbd2cbc254be8fafdafed13cdd8d.png)'
  prefs: []
  type: TYPE_IMG
- en: Embedding a sequence of tokens within an LLM (created by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'In a vanilla transformer architecture, we create an input sequence of tokens
    by first [tokenizing](https://vaclavkosar.com/ml/Tokenization-in-Machine-Learning-Explained)
    the raw text and looking up the embedding (each token in the tokenizer’s vocabulary
    has a unique embedding) for each token. Then, we add a position embedding to each
    token embedding, thus injecting positional info into the embedding of each token
    in the sequence; see above. This is necessary because the self-attention operation
    is agnostic to the position of each token in the sequence. Although position embeddings
    work well, there’s one big problem: *they struggle to generalize to sequences
    longer than those seen during training*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2f1ed9718460486ca898dc3ca48bc9dc.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [6])
  prefs: []
  type: TYPE_NORMAL
- en: '**The solution.** Attention with Linear Biases **(**ALiBi) [6] solves this
    problem by getting rid of position embeddings altogether. Instead, positional
    information is injected into the transformer as part of the self-attention operation
    by adding an additive penalty to the key-query attention score; see above. We
    should recall that self-attention computes an attention score between each pair
    of tokens within a sequence. ALiBi operates by adding a static, non-learned bias
    (or penalty) to this score that is proportional to the distance between the pair
    of tokens; see below.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1ba2295d8ea2102c90b334b95463b5c8.png)'
  prefs: []
  type: TYPE_IMG
- en: Computing the key-query attention score for a particular pair of tokens (created
    by author)
  prefs: []
  type: TYPE_NORMAL
- en: Such an approach is impactful because it depends on the pairwise distance between
    tokens rather than the absolute positions of tokens within a sequence. This quantity
    is less dependent upon the length of the underlying sequence and allows ALiBi
    to generalize much better to sequences that are longer than those seen during
    training; see below. As we will see, MPT models (which use ALiBi) can be trained
    to support larger context lengths compared to most open-source alternatives *and
    can even extrapolate to sequences as long as 84K tokens*!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/21d505c2e4b85a7d291630146e65b4af.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [6])
  prefs: []
  type: TYPE_NORMAL
- en: Faster Inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Due to their use of low precision layer norm and FlashAttention [7], the MPT
    models have very fast training and inference speeds (i.e., `1.5-2X` faster than
    similarly-sized LLaMA models [3] using standard [HuggingFace inference pipelines](https://huggingface.co/blog/inference-update)).
    Going further, the weights of these models can be ported to optimized modules
    like [FasterTransformer](https://github.com/NVIDIA/FasterTransformer) or [ONNX](https://github.com/onnx/models)
    to enable even faster inference.
  prefs: []
  type: TYPE_NORMAL
- en: '**Low precision layer norm.** Put simply, low precision layer norm performs
    the operations from a [LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)
    module in 16-bit precision. Although such an approach can cause loss spikes in
    some cases, it improves hardware utilization and, in turn, speeds up both training
    and inference. Using low precision layer norm also has minimal impact on the model’s
    final performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Flash attention.** In its canonical form, self-attention is an `O(N^2)` operation,
    where `N` is the length of the input sequence. In order to improve the efficiency
    of this operation, many approximate attention variants have been proposed, such
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: Reformer [[link](https://arxiv.org/abs/2001.04451)]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SMYRF [[link](https://arxiv.org/abs/2010.05315)]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performer [[link](https://arxiv.org/abs/2009.14794)]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal of most of these techniques is to derive a “linear” variation of attention
    — a similar/approximate operation with a complexity of `O(N)`. Although these
    variants achieve a theoretical reduction in [FLOPs](https://stackoverflow.com/questions/58498651/what-is-flops-in-field-of-deep-learning),
    *many of them do not achieve any wall-clock speedup in practical scenarios*! Flash
    attention solves this problem by reformulating the attention operation in an IO-aware
    manner; see below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/95ca281c05794f507e75428c5cd1c621.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [7])
  prefs: []
  type: TYPE_NORMAL
- en: 'The hardware-related details of how FlashAttention is implemented are beyond
    the scope of this post. However, the resulting efficient attention implementation
    has a variety of positive benefits. For example, FlashAttention can:'
  prefs: []
  type: TYPE_NORMAL
- en: Speed up BERT-large [10] training time by 15%
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improve training speed by `3X` for GPT-2 [11]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enable longer context lengths for LLMs (due to better memory efficiency)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For more details on FlashAttention, check out the writeup [here](https://shreyansh26.github.io/post/2023-03-26_flash-attention/).
  prefs: []
  type: TYPE_NORMAL
- en: 'MPT-7B: A Commercially-Usable LLaMA-7B'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/fcb2e1d8d8a273e732c0af9d250ef5c7.png)'
  prefs: []
  type: TYPE_IMG
- en: The total compute cost of training the MPT-7B model and fine-tuning various
    derivatives of this model (from [1])
  prefs: []
  type: TYPE_NORMAL
- en: Proposed in [1], MPT-7B is an open-source, commercially-usable language foundation
    model that broadly matches the performance of similarly-sized, open-source base
    models like LLaMA-7B [3] (which is *not* commercially-usable!). Following the
    lessons of Chinchilla [4], MPT-7B is pre-trained over a large corpus — one trillion
    tokens in total — of diverse, publicly-available text. The code used to train,
    fine-tune, and evaluate MPT-7B is completely open-source, *making this model a
    great resource or starting point for practitioners looking to fine-tune their
    own specialized LLM for solving a variety of different downstream applications*!
  prefs: []
  type: TYPE_NORMAL
- en: Creating the Base Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Due to its modified architecture, MPT-7B has several desirable properties, such
    as the ability to generalize to much longer context lengths and faster inference
    speeds. Additionally, we see in [1] that this modified architecture leads to the
    elimination of loss spikes during pre-training of MPT-7B, allowing the model to
    be pre-trained without any human intervention (assuming that any hardware failures
    are handled automatically within the LLM’s training code)!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/719031a87595e7c842ff731839f2a5bb.png)'
  prefs: []
  type: TYPE_IMG
- en: MPT-7B only experiences hardware failures, which can be automatically resolved,
    during its pre-training process (from [1])
  prefs: []
  type: TYPE_NORMAL
- en: '**Training process.** Although most LLMs are trained using the [AdamW optimizer](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html),
    MPT adopts the Lion optimizer [8], which improves the stability of the training
    process. The entire training framework is based upon PyTorch’s [Fully Sharded
    Data Parallel (FSDP)](https://github.com/huggingface/blog/blob/main/pytorch-fsdp.md)
    package and uses no pipeline or tensor parallelism. To put it simply, the training
    framework for MPT-7B, which is [completely open-sourced](https://github.com/mosaicml/llm-foundry),
    uses popular/common components, but makes a few useful changes that are found
    to improve the stability of training.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b83525c36783bedc660d813bd8750ac2.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [1])
  prefs: []
  type: TYPE_NORMAL
- en: '**The data.** The textual corpus used to train MPT-7B is a custom mixture of
    publicly-available datasets (mostly English language); see above. In [1], we see
    that the amount of data used to train MPT-7B is quite large — 1T tokens in total.
    For comparison, open-sources models like [Pythia](https://huggingface.co/EleutherAI/pythia-1.4b)
    and [StableLM](https://github.com/Stability-AI/StableLM) pre-train on 300B and
    800B tokens, respectively. Interestingly, we see that the authors of [1] adopt
    a very particular tokenizer — the [GPT-NeoX-20B](https://huggingface.co/docs/transformers/model_doc/gpt_neox)
    BPE tokenizer— for their model. This tokenizer is desirable because it is trained
    on a large, diverse dataset and handles spaces more consistently than other popular
    tokenizers.'
  prefs: []
  type: TYPE_NORMAL
- en: '“This tokenizer has a number of desirable characteristics, most of which are
    relevant for tokenizing code: trained on a diverse mix of data that includes code,
    applies consistent space delimitation (unlike the GPT2 tokenizer which tokenizes
    inconsistently depending on the presence of prefix spaces), and contains tokens
    for repeated space characters.” *— from [1]*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As practitioners, we should always be aware of the tokenizer being used by our
    model. This choice — *although typically ignored or overlooked* — can drastically
    impact our results. For example, code-based language models need a tokenizer that
    handles whitespace in a particular manner, while multilingual language models
    have a variety of unique tokenization considerations.
  prefs: []
  type: TYPE_NORMAL
- en: How does it perform?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/4262851d604e3809968c0c19f5bcf82f.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [1])
  prefs: []
  type: TYPE_NORMAL
- en: MPT-7B is compared to a variety of open-source models (e.g., LLaMA, [StableLM](https://github.com/Stability-AI/StableLM),
    [Pythia](https://github.com/EleutherAI/pythia), [GPT-NeoX](https://huggingface.co/docs/transformers/model_doc/gpt_neox),
    [OPT](https://cameronrwolfe.substack.com/p/understanding-the-open-pre-trained-transformers-opt-library-193a29c14a15),
    and [GPT-J](https://huggingface.co/docs/transformers/model_doc/gptj)) on standard
    benchmarks. As shown above, LLaMA-7B achieves drastic improvements over open-source
    alternatives, while MPT-7B matches or exceed LLaMA’s performance. Recent open-source
    LLMs are much better than their predecessors! *LLaMA-7B and MPT-7B are both incredibly
    high-performing base models compared to other open-source models*. However, MPT-7B
    can be used commercially, while LLaMA can only be used for research.
  prefs: []
  type: TYPE_NORMAL
- en: Derivatives of MPT-7B
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to releasing the MPT-7B base model, authors in [1] leverage the
    open-source training code for MPT to fine-tune several different derivatives of
    the base model (outlined below). Fine-tuning is very cheap compared to pre-training
    an LLM from scratch (i.e., *10–100X reduction in time and cost, if not more*).
    As such, most of the time and effort in developing MPT-7B went into creating the
    base model, which serves as a starting point for fine-tuning the models below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/08fa6f43cdadaf4d961b5d44c3852f25.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [1])
  prefs: []
  type: TYPE_NORMAL
- en: '**MPT-StoryWriter-65K (commercial)** is a version of MPT-7B that has been fine-tuned
    on data with very long context lengths. In particular, authors in [1] leverage
    the [books3 dataset](https://huggingface.co/datasets/the_pile_books3), which contains
    excerpts from fiction books, to create a dataset for fine-tuning (i.e., just using
    the next-token prediction objective) with a 65K token context length. Due to the
    use of ALiBi [6] and FlashAttention [7], MPT-StoryWriter-65K can be feasibly trained
    over such large inputs, used to consume the entirety of The Great Gatsby (68K
    tokens) to write an epilogue (see above), and even generalized to process sequences
    lengths as long as 84K tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: “We expect LLMs to treat the input as instructions to follow. Instruction finetuning
    is the process of training LLMs to perform instruction-following in this way.
    By reducing the reliance on clever prompt engineering, instruction finetuning
    makes LLMs more accessible, intuitive, and immediately usable.” *— from [1]*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**MPT-7B-Instruct (commercial)** and **MPT-7B-Chat (non-commercial)** are instruction
    tuned versions of MPT-7B. The instruct variant is fine-tuned over data from [Dolly-15K](https://huggingface.co/datasets/databricks/databricks-dolly-15k)
    and the [Helpful and Harmless dataset](https://huggingface.co/datasets/Anthropic/hh-rlhf),
    while the chat model is trained with data from sources like [ShareGPT](https://sharegpt.com/),
    [HC3](https://huggingface.co/datasets/Hello-SimpleAI/HC3), [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca),
    and [Evol-Instruct](https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_70k).
    As outlined by the quote above, instruction tuning takes a pre-trained language
    model and modifies its style or behavior to be more intuitive and accessible,
    usually with an emphasis upon instruction following or problem solving.'
  prefs: []
  type: TYPE_NORMAL
- en: 'MPT-30B: An Open-Source GPT-3 Alternative'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/108528023f4e40073f14767ea75f97f7.png)'
  prefs: []
  type: TYPE_IMG
- en: MPT-30B improves upon MPT-7B in all performance categories (from [2])
  prefs: []
  type: TYPE_NORMAL
- en: Shortly after its proposal, the MPT-7B model gained significant recognition
    in the AI research community — *it even amassed over 3M downloads on HuggingFace*!
    The success of MPT-7B was no surprise, as it provided a commercially-usable alternative
    to the incredibly popular LLaMA-7B model. Riding this momentum, researchers at
    MosaicML followed MPT-7B with a slightly larger model, called MPT-30B [2], that
    was found to match or exceed the performance of GPT-3 [9]. As such, the proposal
    of MPT-30B continues the trend of making commercially-usable versions of powerful
    base LLMs available to anyone.
  prefs: []
  type: TYPE_NORMAL
- en: Diving Deeper into MPT-30B
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MPT-30B shares the same, modified decoder-only architecture as MPT-7B, which
    uses FlashAttention and low precision layer norm for improved efficiency. Overall,
    the models are quite similar aside from MPT-30B being larger. Interestingly, the
    size of MPT-30B was chosen very specifically. A model of this size is feasible
    to deploy on a single GPU using 8 or 16-bit precision, while alternatives like
    [Falcon-40B](https://falconllm.tii.ae/) are slightly too large to be deployed
    in this manner.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5bf95c22273ced86d9ebbdda2e35b4c5.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [2])
  prefs: []
  type: TYPE_NORMAL
- en: '**What’s different?** MPT-30B is different from MPT-7B in two main ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Pre-training data mixture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Context length
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pre-training dataset for MPT-30B is similar to that of MPT-7B, but the mixture
    of data is slightly different; see above. Additionally, MPT-30B is (partially)
    trained using a 8K context length, whereas most other open-source models (e.g.,
    LLaMA, Falcon, and MPT-7B) are trained using a shorter context length of 2K tokens.
    More specifically, we see that MPT-30B uses a training curriculum in which the
    model is first trained with a 2K context length, then switches to an 8K context
    length later in training. During this second phase, the proportion of code in
    the dataset is increased by `2.5X`, leading the resulting model to have improved
    coding abilities compared to other open-source LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Model variants.** In addition to the MPT-30B base model, authors in [2] release
    chat and instruct variants of MPT-30B. These models follow a similar training
    strategy as MPT-7B-Instruct and MPT-7B-Chat. However, the data used for instruction
    tuning is significantly expanded for both of these models. Interestingly, MPT-30B-Chat
    is found to have impressive programing skills!'
  prefs: []
  type: TYPE_NORMAL
- en: Does it perform well?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/7f9063398a8d60ba7f0ffd8a308921f9.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [2])
  prefs: []
  type: TYPE_NORMAL
- en: In addition to outperforming MPT-7B across a variety of categories, MPT-30B
    achieves comparable performance to top open-source alternatives like LLaMA-30B
    and Falcon-40B; see above. In general, we see that MPT-30B lags behind Falcon
    and LLaMA in solving text-based tasks, but tends to outperform these models on
    programming-related problems (likely due to the higher ratio of code in the pre-training
    dataset!). Notably, we see that MPT-30B outperforms GPT-3 on a variety of in-context
    learning tasks; see below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6d2fb7533c554c9ac72d5323b2c9b96f.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [2])
  prefs: []
  type: TYPE_NORMAL
- en: With this result in mind, it seems that models like MPT-30B could potentially
    lay the foundation for open-source LLM applications that rival the quality of
    proprietary systems. All we need is sufficient refinement and fine-tuning!
  prefs: []
  type: TYPE_NORMAL
- en: Final Remarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “You can train, finetune, and deploy your own private MPT models, either starting
    from one of our checkpoints or training from scratch” *— from [2]*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The foundation models provided by MosaicML are a huge step forward for the open-source
    LLM community, as they provide commercially-usable LLMs that are comparable to
    popular base models like LLaMA and GPT-3\. However, this open-source offering
    goes beyond the MPT models themselves — it includes an [open-source codebase for
    training LLMs](https://github.com/mosaicml/llm-foundry), a variety of [online
    demos](https://huggingface.co/mosaicml), and more.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d1e644e4c8b767e0c765327d15d87d49.png)'
  prefs: []
  type: TYPE_IMG
- en: (from [2])
  prefs: []
  type: TYPE_NORMAL
- en: The MPT-7B and 30B models come with an entire ecosystem of open-source tools
    that can be used to create specialized/personalized LLMs. Given that creating
    the base model is the most expensive aspect of any LLM-based system (see above),
    these tools significantly lower the barrier to entry for working with LLMs and
    provide a starting point for solving a variety of downstream applications. Remember,
    [fine-tuning is extremely effective](https://magazine.sebastianraschka.com/i/125373356/finetuning-task-specific-llms-for-your-business-needs)
    (i.e., hard to beat by just prompting a more generic LLM) when we have a particular
    task that we are trying to solve!
  prefs: []
  type: TYPE_NORMAL
- en: Connect with me!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Thanks so much for reading this article. I am [Cameron R. Wolfe](https://cameronrwolfe.me/),
    Director of AI at [Rebuy](https://www.rebuyengine.com/). I study the empirical
    and theoretical foundations of deep learning. If you liked this overview, subscribe
    to my [Deep (Learning) Focus newsletter](https://cameronrwolfe.substack.com/),
    where I help readers understand AI research via overviews of relevant topics from
    the ground up. You can also follow me on [X](https://twitter.com/cwolferesearch)
    and [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/), or
    check out my [other writings](https://medium.com/@wolfecameron) on medium!
  prefs: []
  type: TYPE_NORMAL
- en: Bibliography
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] “Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable
    Llms.” *MosaicML*, 5 May 2023, [www.mosaicml.com/blog/mpt-7b.](http://www.mosaicml.com/blog/mpt-7b.)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] “MPT-30B: Raising the Bar for Open-Source Foundation Models.” *MosaicML*,
    22 June 2023, [www.mosaicml.com/blog/mpt-30b.](http://www.mosaicml.com/blog/mpt-30b.)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Touvron, Hugo, et al. “Llama: Open and efficient foundation language models.”
    *arXiv preprint arXiv:2302.13971* (2023).'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Hoffmann, Jordan, et al. “Training compute-optimal large language models.”
    *arXiv preprint arXiv:2203.15556* (2022).'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] Zhang, Susan, et al. “OPT: Open Pre-trained Transformer Language Models.”
    *arXiv preprint arXiv:2205.01068* (2022).'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] Press, Ofir, Noah A. Smith, and Mike Lewis. “Train short, test long: Attention
    with linear biases enables input length extrapolation.” *arXiv preprint arXiv:2108.12409*
    (2021).'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] Dao, Tri, et al. “Flashattention: Fast and memory-efficient exact attention
    with io-awareness.” *Advances in Neural Information Processing Systems* 35 (2022):
    16344–16359.'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] Chen, Xiangning, et al. “Symbolic discovery of optimization algorithms.”
    *arXiv preprint arXiv:2302.06675* (2023).'
  prefs: []
  type: TYPE_NORMAL
- en: '[9] Brown, Tom, et al. “Language models are few-shot learners.” *Advances in
    neural information processing systems* 33 (2020): 1877–1901.'
  prefs: []
  type: TYPE_NORMAL
- en: '[10] Devlin, Jacob, et al. “Bert: Pre-training of deep bidirectional transformers
    for language understanding.” *arXiv preprint arXiv:1810.04805* (2018).'
  prefs: []
  type: TYPE_NORMAL
- en: '[11] Radford, Alec, et al. “Language Models are Unsupervised Multitask Learners.”'
  prefs: []
  type: TYPE_NORMAL
- en: '[12] Ouyang, Long, et al. “Training language models to follow instructions
    with human feedback.” *Advances in Neural Information Processing Systems* 35 (2022):
    27730–27744.'
  prefs: []
  type: TYPE_NORMAL
- en: '[13] Glaese, Amelia, et al. “Improving alignment of dialogue agents via targeted
    human judgements.” *arXiv preprint arXiv:2209.14375* (2022).'
  prefs: []
  type: TYPE_NORMAL
