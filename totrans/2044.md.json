["```py\npip install scikit-learn\n```", "```py\ndef __init__(\n       self,\n       *,\n       criterion=\"squared_error\",\n       splitter=\"best\",\n       max_depth=None,\n       min_samples_split=2,\n       min_samples_leaf=1,\n       min_weight_fraction_leaf=0.0,\n       max_features=None,\n       random_state=None,\n       max_leaf_nodes=None,\n       min_impurity_decrease=0.0,\n       ccp_alpha=0.0,\n\n   ):\n```", "```py\ndef load_auto_data_set():\n\n   # Load the automobile data set from UCI.edu\n   url = '<https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data>'\n   df = pd.read_csv(url, header=None)\n\n   # Name columns\n   df.columns = ['symboling', 'normalized_losses', 'make', 'fuel_type', 'aspiration', 'num_doors', 'body_style', 'drive_wheels', 'engine_location','wheel_base','length','width','height',          'curb_weight','engine_type','num_cylinders','engine_size','fuel_system','bore','stroke', 'compression_ratio','horsepower','peak_rpm','city_mpg','highway_mpg','price']\n\n   # Filter for lines where power and price are available\n   df = df[(df.horsepower != '?')]\n   df = df[(df.price != '?')]\n\n   # Filter for lines where power and price are available\n   df['horsepower'] = df['horsepower'].astype(int)\n   df['price'] = df['price'].astype(int)\n\n   # Define the last column of the data frame as y and the rest as X\n   self.y = self.df.iloc[:, -1]\n   self.X = self.df.iloc[:, :-1]\n\n   return df, X, y\n```", "```py\nclass NodePlot():\n\n   def __init__(self, X_parent, y_parent, threshold, selected_feature):\n       self.selected_feature = selected_feature\n       self.x_column = X_parent[self.selected_feature]\n       self.y_parent = y_parent\n       self.data_set = np.column_stack((self.x_column, y_parent))\n       self.threshold = threshold\n\n       # define a list with all observations of the left and right leaf\n       self.left_y = self.data_set[self.data_set[:, 0]<self.threshold][:, 1]\n       self.left_x = self.data_set[self.data_set[:, 0]<self.threshold][:, 0]\n       self.right_y = self.data_set[self.data_set[:, 0]>=self.threshold][:, 1]\n       self.right_x = self.data_set[self.data_set[:, 0]>=self.threshold][:, 0]\n\n       # calculate the mean of the observations for the left and right leaf\n       self.parent_y_mean = np.mean(self.y_parent)\n       self.left_y_mean = np.mean(self.left_y)\n       self.right_y_mean = np.mean(self.right_y)\n\n       # calculate the weighted mean squared error\n       self.parent_mse = np.mean((y_parent - self.parent_y_mean)**2)\n       mse_l = np.mean((self.left_y - self.left_y_mean)**2)\n       mse_r = np.mean((self.right_y - self.right_y_mean)**2)\n\n       # calculate the number of instances in the parent and child nodes\n       n_l = len(self.left_y)\n       n_r = len(self.right_y)\n       n = len(self.data_set)\n\n       # calculate the weighted mse for child nodes\n       self.child_mse = (n_l/n) * mse_l + (n_r/n) * mse_r\n\n   def plot_split(self):\n       plt.rcParams['font.size'] = '16'\n       sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n\n       fig = go.Figure()\n\n       fig.add_trace(\n           go.Scatter(\n               x=self.left_x,\n               y=self.left_y,\n               mode=\"markers\",\n               name=\"Data set: left node\",\n               line=dict(color=\"grey\")\n           )\n       )\n\n       fig.add_trace(\n           go.Scatter(\n               x=self.left_x,\n               y=np.linspace(self.left_y_mean, self.left_y_mean, len(self.left_x)),\n               mode=\"lines\",\n               name=\"Right node prediction\",\n               line=dict(color=\"black\")\n           )\n       )\n\n       # create go.scatter plot with black line\n       fig.add_trace(\n           go.Scatter(\n               x=self.right_x,\n               y=self.right_y,\n               mode=\"markers\",\n               name=\"Data set: right node\",\n               #line=dict(color=\"#ffe476\")\n               line=dict(color=\"black\")\n           )\n       )\n\n      fig.add_trace(\n           go.Scatter(\n               x=self.right_x,\n               y=np.linspace(self.right_y_mean, self.right_y_mean, len(self.right_x)),\n               mode=\"lines\",\n               name=\"Left node prediction\",\n               line=dict(color=\"black\", dash='dot')\n           )\n       )\n\n       fig.add_trace(\n           go.Scatter(\n               x=[self.threshold, self.threshold],\n               y=[min(self.y_parent), max(self.y_parent)],\n               mode=\"lines\",\n               name=\"MSE of parent node\",\n               line=dict(color=\"black\", dash='dashdot')\n           )\n       )\n\n       # update title in go.Figure\n       fig.update_layout(title=\"Data set\", xaxis_title=self.selected_feature, yaxis_title=self.y_parent.name)\n\n       fig.show()\n```", "```py\nselected_feature = \"horsepower\"\n\nlist_of_mse_childs = []\nlist_of_mse_parent = []\nthresholds = X.sort_values(by=[\"horsepower\"])[\"horsepower\"].unique()\n\nfor threshold in thresholds:\n\n   NodePlot = helper_functions.NodePlot(\n                                   X_parent = X,\n                                   y_parent = y,\n                                   threshold = threshold,\n                                   selected_feature = \"horsepower\"\n                                   )\n\n   list_of_mse_childs.append(NodePlot.child_mse)\n   list_of_mse_parent.append(NodePlot.parent_mse)\n\ndef plot_threshold_evaluation(thresholds, mse_parent_list, mse_list):\n    # create figure\n    fig = go.Figure()\n\n    fig.add_trace(\n        go.Scatter(\n            x=thresholds,\n            y=mse_list,\n            mode=\"lines\",\n            name=\"MSE after split\",\n            line=dict(color=\"black\")\n        )\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            x=thresholds,\n            y=mse_parent_list,\n            mode=\"lines\",\n            name=\"MSE of parent node\",\n            line=dict(color=\"black\", dash='dot')\n        )\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            x=[threshold,threshold],\n            y=[min(mse_list), max(mse_list)],\n            mode=\"lines\",\n            name=\"Chosen threshold\",\n            line=dict(color=\"black\", dash='dashdot')\n        )\n    )\n\n    # update title in go.Figure\n    fig.update_layout(title=\"Evaluate\", yaxis_title='MSE')\n\n    fig.show()\n\n    return fig\n\n# plot the just calculated MSE values for different thresholds\nplot_threshold_evaluation(\n                           thresholds = thresholds,\n                           mse_parent_list = list_of_mse_parent,\n                           mse_list = list_of_mse_childs,\n                           threshold = 100\n                       )\n```", "```py\n class Node():\n    def __init__(\n                    self, \n                    feature=None, \n                    threshold=None, \n                    left=None, \n                    right=None, \n                    value=None\n                ):\n        self.feature = feature\n        self.threshold = threshold\n        self.left = left\n        self.right = right\n        self.value = value # is it a leave node?\n\n    def is_leaf_node(self):\n        return self.value is not None\n\nclass RegressionTree():\n    def __init__(\n                    self, \n                    min_samples_split=2, \n                    max_depth=100):\n\n        self.min_samples_split = min_samples_split\n        self.max_depth = max_depth\n        self.root = None\n\n    def fit(self, X, y):\n        self.root = self._grow_tree(X, y)\n```", "```py\ndef _grow_tree(self, X, y, depth=0):\n    # check the stopping criteria\n    n_samples, n_feats = X.shape\n\n    if (depth>=self.max_depth or n_samples<self.min_samples_split):\n       leaf_value = np.mean(y)\n       return Node(value=leaf_value)\n\n    feat_idxs = np.random.choice(n_feats, n_feats, replace=False)\n\n    # find the best split\n    best_thresh, best_feature = self._best_split(X, y, feat_idxs)\n\n    # create child nodes\n    left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh)\n    left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth+1)\n    right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth+1)\n\n    return Node(best_feature, best_thresh, left, right)\n```", "```py\ndef _best_split(self, X, y, feat_idxs):\n    y_mean = np.mean(y)\n    residuals_y = (y - y_mean)**2\n    y_mse = np.mean(residuals_y)\n\n    best_feature_ixd, best_threshold = None, None\n    lowest_mse = y_mse\n\n    for feat_idx in feat_idxs:\n       # define possible thresholds for the split\n       X_column = X[:, feat_idx]\n       thresholds = np.convolve(np.sort(X_column), np.ones(2)/2, mode='valid')\n\n       for threshold in thresholds:\n           # getting the left and right nodes\n           left_idxs, right_idxs = self._split(X_column, threshold)\n\n           # calculate the weighted avg. mse of children\n           n = len(y)\n           n_l, n_r = len(left_idxs), len(right_idxs)\n           mse_l = self._squared_error(y[left_idxs])\n           mse_r = self._squared_error(y[right_idxs])\n           child_mse = (n_l/n) * mse_l + (n_r/n) * mse_r\n\n           if lowest_mse > child_mse:\n               lowest_mse = child_mse\n               best_feature_ixd = feat_idx\n               best_threshold = threshold\n\n    return best_feature_ixd, best_threshold\n```", "```py\ndef _split(self, X_column, split_thresh):\n    left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n    right_idxs = np.argwhere(X_column > split_thresh).flatten()\n    return left_idxs, right_idxs\n\ndef _squared_error(self, y):\n    # calculate the mean value for all observations\n    y_mean = np.mean(y)\n\n    # calculate the residuals to y_mean\n    mean_squared_error = np.mean((y - y_mean)**2)\n\n    return mean_squared_error\n```", "```py\ndef predict(self, X):\n    return np.array([self._traverse_tree(x) for x in X])\n\ndef _traverse_tree(self, x, node):\n    if node.is_leaf_node():\n       return node.value\n\n    if x[node.feature] <= node.threshold:\n       return self._traverse_tree(x, node.left)\n\n    return self._traverse_tree(x, node.right)\n```", "```py\nimport numpy as np\nfrom collections import Counter\nfrom sklearn.metrics import mean_squared_error\nfrom collections import Counter\n\nclass Node():\n    def __init__(\n                    self, \n                    feature=None, \n                    threshold=None, \n                    left=None, \n                    right=None, \n                    value=None\n                ):\n        self.feature = feature\n        self.threshold = threshold\n        self.left = left\n        self.right = right\n        self.value = value # is it a leave node?\n\n    def is_leaf_node(self):\n        return self.value is not None\n\nclass RegressionTree():\n    def __init__(\n                    self, \n                    min_samples_split=2, \n                    max_depth=100):\n        self.min_samples_split = min_samples_split\n        self.max_depth = max_depth\n        self.root = None\n\n    def fit(self, X, y):\n        self.root = self._grow_tree(X, y)\n\n    def _grow_tree(self, X, y, depth=0):\n        # check the stopping criteria\n        n_samples, n_feats = X.shape\n\n        if (depth>=self.max_depth or n_samples<self.min_samples_split):\n            leaf_value = np.mean(y)\n            return Node(value=leaf_value)\n\n        feat_idxs = np.random.choice(n_feats, n_feats, replace=False)\n\n        # find the best split\n        best_feature_ixd, best_threshold = self._best_split(X, y, feat_idxs)\n\n        # create child nodes\n        left_idxs, right_idxs = self._split(X[:, best_feature_ixd], best_threshold)\n\n        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth+1)\n        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth+1)\n\n        return Node(best_feature_ixd, best_threshold, left, right)\n\n    def _best_split(self, X, y, feat_idxs):\n        y_mean = np.mean(y)\n        residuals_y = (y - y_mean)**2\n        y_mse = np.mean(residuals_y)\n\n        best_feature_ixd, best_threshold = None, None\n        lowest_mse = y_mse\n\n        for feat_idx in feat_idxs:\n            # define possible thresholds for the split\n            X_column = X[:, feat_idx]\n            thresholds = np.convolve(np.sort(X_column), np.ones(2)/2, mode='valid')\n\n            for threshold in thresholds:\n                # getting the left and right nodes\n                left_idxs, right_idxs = self._split(X_column, threshold)\n\n                # calculate the weighted avg. mse of children\n                n = len(y)\n                n_l, n_r = len(left_idxs), len(right_idxs)\n                mse_l = self._squared_error(y[left_idxs]) \n                mse_r = self._squared_error(y[right_idxs])\n                child_mse = (n_l/n) * mse_l + (n_r/n) * mse_r\n\n                if lowest_mse > child_mse:\n                    lowest_mse = child_mse\n                    best_feature_ixd = feat_idx\n                    best_threshold = threshold\n\n        return best_feature_ixd, best_threshold\n\n    def _split(self, X_column, split_thresh):\n        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n        return left_idxs, right_idxs\n\n    def _squared_error(self, y):\n        # calculate the mean value for all observations\n        y_mean = np.mean(y)\n\n        # calculate the residuals to y_mean\n        mean_squared_error = np.mean((y - y_mean)**2)\n\n        return mean_squared_error\n\n    def predict(self, X):\n        return np.array([self._traverse_tree(x, self.root) for x in X])\n\n    def _traverse_tree(self, x, node):\n        if node.is_leaf_node():\n            return node.value\n\n        if x[node.feature] <= node.threshold:\n            return self._traverse_tree(x, node.left)\n\n        return self._traverse_tree(x, node.right)\n```", "```py\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\ndef load_auto_data_set():\n  # Load the automobile data set from UCI.edu\n  url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data'\n  df = pd.read_csv(url, header=None)\n\n  # Name columns\n  df.columns = [\n                 'symboling', 'normalized_losses', 'make',\n                 'fuel_type', 'aspiration', 'num_doors',\n                 'body_style', 'drive_wheels', 'engine_location',\n                 'wheel_base','length','width','height',\n                 'curb_weight','engine_type','num_cylinders',\n                 'engine_size','fuel_system','bore','stroke',\n                 'compression_ratio','horsepower','peak_rpm',\n                 'city_mpg','highway_mpg','price'\n             ]\n\n  # Filter for lines where power and price are available\n  df = df[(df.horsepower != '?')]\n  df = df[(df.price != '?')]\n  df = df.reset_index()\n\n  # Filter for lines where power and price are available\n  df['horsepower'] = df['horsepower'].astype(int)\n  df['price'] = df['price'].astype(int)\n\n  # Define the last column of the data frame as y and the rest as X\n  y = df.iloc[:, -1]\n  X = df.iloc[:, :-1]\n\n  return df, X, y\n\ndf, X, y = load_auto_data_set()\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nX_selected = X[[\"wheel_base\", \"length\", \"width\",\"height\"]].reset_index()\n\n# define and fit the OneHotEncoder\nohe = OneHotEncoder()\nohe.fit(df[['make']])\n\n# transform the \"make\" column\nmake_one_hot_sklearn = pd.DataFrame(ohe.transform(df[[\"make\"]]).toarray(), columns=ohe.categories_[0])\n\nX = X_selected.join(make_one_hot_sklearn)\nX = np.array(X)\ny = np.array(y)\n```", "```py\nimport tree_algorithms\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nregr = tree_algorithms.RegressionTree(min_samples_split=5, max_depth=20)\nregr.fit(X_train, y_train)\ny_pred = regr.predict(X_test)\n\n# calculate mean squared error\nprint(f\"Mean Squared Error: {round(mean_squared_error(y_test, y_pred), 1)}\")\n```", "```py\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nregr = DecisionTreeRegressor(min_samples_split=5, max_depth=20)\nregr.fit(X_train, y_train)\ny_pred = regr.predict(X_test)\n\n# calculate mean squared error\nprint(f\"Mean Squared Error: {round(mean_squared_error(y_test, y_pred), 1)}\")\n```", "```py\nimport numpy as np\nfrom collections import Counter\nfrom sklearn.metrics import mean_squared_error\nfrom collections import Counter\n\nclass Node():\n    def __init__(\n                    self, \n                    feature=None, \n                    threshold=None, \n                    left=None, \n                    right=None, \n                    value=None\n                ):\n        self.feature = feature\n        self.threshold = threshold\n        self.left = left\n        self.right = right\n        self.value = value # is it a leave node?\n\n    def is_leaf_node(self):\n        return self.value is not None\n\nclass RegressionTree():\n    def __init__(\n                    self, \n                    min_samples_split=2, \n                    max_depth=100):\n        self.min_samples_split = min_samples_split\n        self.max_depth = max_depth\n        self.root = None\n\n    def fit(self, X, y):\n        self.root = self._grow_tree(X, y)\n\n    def _grow_tree(self, X, y, depth=0):\n        # check the stopping criteria\n        n_samples, n_feats = X.shape\n\n        if (depth>=self.max_depth or n_samples<self.min_samples_split):\n            leaf_value = np.mean(y)\n            return Node(value=leaf_value)\n\n        feat_idxs = np.random.choice(n_feats, n_feats, replace=False)\n\n        # find the best split\n        best_feature_ixd, best_threshold = self._best_split(X, y, feat_idxs)\n\n        # create child nodes\n        left_idxs, right_idxs = self._split(X[:, best_feature_ixd], best_threshold)\n\n        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth+1)\n        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth+1)\n\n        return Node(best_feature_ixd, best_threshold, left, right)\n\n    def _best_split(self, X, y, feat_idxs):\n        y_mean = np.mean(y)\n        residuals_y = (y - y_mean)**2\n        y_mse = np.mean(residuals_y)\n\n        best_feature_ixd, best_threshold = None, None\n        lowest_mse = y_mse\n\n        for feat_idx in feat_idxs:\n            # define possible thresholds for the split\n            X_column = X[:, feat_idx]\n            thresholds = np.convolve(np.sort(X_column), np.ones(2)/2, mode='valid')\n\n            for threshold in thresholds:\n                # getting the left and right nodes\n                left_idxs, right_idxs = self._split(X_column, threshold)\n\n                # calculate the weighted avg. mse of children\n                n = len(y)\n                n_l, n_r = len(left_idxs), len(right_idxs)\n                mse_l = self._squared_error(y[left_idxs]) \n                mse_r = self._squared_error(y[right_idxs])\n                child_mse = (n_l/n) * mse_l + (n_r/n) * mse_r\n\n                if lowest_mse > child_mse:\n                    lowest_mse = child_mse\n                    best_feature_ixd = feat_idx\n                    best_threshold = threshold\n\n        return best_feature_ixd, best_threshold\n\n    def _split(self, X_column, split_thresh):\n        left_idxs = np.argwhere(X_column <= split_thresh).flatten()\n        right_idxs = np.argwhere(X_column > split_thresh).flatten()\n        return left_idxs, right_idxs\n\n    def _squared_error(self, y):\n        # calculate the mean value for all observations\n        y_mean = np.mean(y)\n\n        # calculate the residuals to y_mean\n        mean_squared_error = np.mean((y - y_mean)**2)\n\n        return mean_squared_error\n\n    def predict(self, X):\n        return np.array([self._traverse_tree(x, self.root) for x in X])\n\n    def _traverse_tree(self, x, node):\n        if node.is_leaf_node():\n            return node.value\n\n        if x[node.feature] <= node.threshold:\n            return self._traverse_tree(x, node.left)\n\n        return self._traverse_tree(x, node.right)\n```"]