- en: 'The Beginning of Information Extraction: Highlight Key Words and Obtain Frequencies'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-beginning-of-information-extraction-highlight-key-words-and-obtain-frequencies-a03da0a1ba71](https://towardsdatascience.com/the-beginning-of-information-extraction-highlight-key-words-and-obtain-frequencies-a03da0a1ba71)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A quick approach for highlighting keywords of interest within a PDF document
    and calculating their frequencies.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ben-mccloskey20.medium.com/?source=post_page-----a03da0a1ba71--------------------------------)[![Benjamin
    McCloskey](../Images/7118f5933f2affe2a7a4d3375452fa4c.png)](https://ben-mccloskey20.medium.com/?source=post_page-----a03da0a1ba71--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a03da0a1ba71--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a03da0a1ba71--------------------------------)
    [Benjamin McCloskey](https://ben-mccloskey20.medium.com/?source=post_page-----a03da0a1ba71--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a03da0a1ba71--------------------------------)
    ·10 min read·Aug 28, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/47bf25395e329662fb5f9381919d3942.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Judy Velazquez](https://unsplash.com/@roses_n_basil?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the amount of available information increasing every day, having the ability
    to quickly gather relevant statistics about said information is important for
    relationship mapping and acquiring a new perspective on otherwise redundant data.
    Today we will look at text extraction, also known as information extraction, of
    PDFs and a quick approach to formulating some facts and ideas about different
    corpora. Today’s article dives into the field of Natural Language Processing (NLP),
    which is a computer’s ability to comprehend human language.
  prefs: []
  type: TYPE_NORMAL
- en: Information Extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Information Extraction (IE)**, as defined by Jurafsky et al, is the *“process
    for turning unstructured information embedded in texts into structured data.”*
    [1]. A very quick way of information extraction is not only to search to find
    if a word is located within a body of the text but also to calculate the *frequency*
    of how many times that word is mentioned. This is supported by the assumption
    that **the more a word is mentioned within a body of text, the more important
    it is and its relation to the corpus’s theme.** It’s important to note that stopword
    removal is important for this given process. Why? Well, if you simply calculated
    all of the word frequencies within a corpus, the word *the* will be mentioned
    a lot. Does that make this word important in terms of relaying what information
    is within the text? No, and therefore you want to ensure you are looking at frequencies
    of words that contribute to the semantic meaning of your corpora.'
  prefs: []
  type: TYPE_NORMAL
- en: IE can lead to other NLP techniques being used on a document. These techniques
    go beyond the code of this article but I felt they were both interesting and important
    the share.
  prefs: []
  type: TYPE_NORMAL
- en: The first technique is **Named Entity Recognition (NER).** As detailed by Jurafsky
    et al. *“The task of named entity recognition (NER) is to find each named entity
    recognition mention of a named entity in the text and label its type.”* [1] This
    is similar to the idea of searching for the frequencies of the words within a
    body of text, but NER takes it another step further by using word location as
    well as other literary rules for finding different entities within a body of text.
  prefs: []
  type: TYPE_NORMAL
- en: Another technique that can be supported by IE is **Relation Extraction,** which
    is “*finding and classifying semantic relation extraction relations among the
    text entities*.” [1]. The goal is not only to be able to extract how frequently
    different words are in text, and the entity label for which they fall, but also
    how we can relate these different entities together to formulate either the underlying
    semantic meaning, patterns, or summary of a corpus of text.
  prefs: []
  type: TYPE_NORMAL
- en: The previous three tasks all relate to the goal of **Event Extraction**. Jurafsky
    et al. go on to state that “E*vent extraction is finding event extraction events
    in which these entities participate*.” [1]. What this says is that by finding
    and extracting different statistics and labels from a body of text, we can begin
    to articulate hypotheses on what event is occurring from these events and how,
    through these entities, different events may be related.
  prefs: []
  type: TYPE_NORMAL
- en: The Process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/8159545b1b4c139fe00c8330cdfd9684.png)'
  prefs: []
  type: TYPE_IMG
- en: Process (Image from Author)
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this process is to get a quick understanding of whether a corpus
    of text not only has information relating to what you are looking for but also
    to give the frequency for understanding if there is enough of said information
    to make comparisons between different documents. Finally, it provides a visualization
    of the queried word highlighted in the text which can help with drawing conclusions
    about relationships and what the text is about based on the surrounding words.
    The entire process was implemented in Python using Google Colab and can be easily
    transferred to any IDE of your choice. Let’s take a look at the code!
  prefs: []
  type: TYPE_NORMAL
- en: The Code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Python libraries needed for this code are [**PyMuPDF**](https://pypi.org/project/PyMuPDF/)
    and [**Counter**](https://pypi.org/project/Counter/).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will create a function called “hightlight_terms” which accepts an input
    pdf, a path to an output pdf, a path to an output text file, and the terms we
    want to be highlighted.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Once we create the function, we can set up the file paths to our different directories.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: And that’s it, easy as that! This can be super helpful for your analysis in
    getting a quick understanding of how relevant a certain word (or words) is in
    a document. In addition, when viewing the highlighted output PDF, outlining the
    location of a word can help draw your attention and make relationships to other
    words surrounding the queried word.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In today’s example, let’s look at a scholarly article from Google Scholar. What
    if, for example, I wanted to know if an article had adequate information about
    *Neural Networks?* We can query the document and then use the queried frequencies
    to make an assessment about whether the document focuses on Neural Networks. The
    first document we will look at is titled *ImageNet Classification with Deep Convolutional
    Neural Networks* [2].
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/90c65f51feba69d4bbcabeb2de382308.png)'
  prefs: []
  type: TYPE_IMG
- en: Let’s say, for example, we are interested in studying Neural Networks and want
    to see where and how many times those words appear within the text. By querying
    *Neural* and *Networks*, we will get a highlighted pdf, as shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fdf3a14fdd05475fd47ceac46e7036a8.png)'
  prefs: []
  type: TYPE_IMG
- en: Additionally, a text file will be created with the frequencies of both *Neural*
    and *Network*, which were 21 and 23, respectively. The frequency number really
    depends on the user whether a word is mentioned enough in a document to deem it
    relevant. With both words appearing over 20 times within the document, I would
    argue they are relevant to its context.
  prefs: []
  type: TYPE_NORMAL
- en: This process can easily be expanded to multiple documents where you can then
    compare the frequencies of certain words between different documents. In doing
    so, we can compare documents and come to conclusions about which documents are
    more related to the information we are seeking to find.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the approach is simple and completes the desired task at hand, there are
    a few limitations worth discussing.
  prefs: []
  type: TYPE_NORMAL
- en: The assumption that the higher frequency a word has in a corpus means it may
    support the underlying meaning more does not always hold.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Querying for the wrong words could cause you to not uncover the correct meaning
    of a body of text or find the information you are looking for.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The methodology is more of a summarization of information extraction and provides
    a starting point for a more in-depth analysis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first limitation around the assumption asserts that the more frequently
    a word is used, the more it supports the underlying meaning of the text depending
    on the author publishing the work and how they wish to express their ideas does
    not always hold true. Additionally, different words can be used to describe the
    same meaning, and by only focusing on a few different words and their high frequencies
    you may find yourself attaching the wrong meaning to a corpus of text.
  prefs: []
  type: TYPE_NORMAL
- en: The second limitation derives from the first regarding the word queries. If
    you are not querying the correct words, you may be missing the important underlying
    meeting of the corpus of text. One way to overcome this could be to convert the
    PDF to a text document using PyMuPDF or PyPDF2 and then calculate the N most frequent
    words. From there you can use those frequencies to find where those words are
    located in the body of text which can help help you find where the most important
    information sits.
  prefs: []
  type: TYPE_NORMAL
- en: 'The third limitation of today’s methodology is that it does not go more in-depth
    into information extraction. With that being said, this code does set you up nicely
    to begin your text extraction project and can be branched off into a few different
    directions. Here are some possible ways you can move out from this process:'
  prefs: []
  type: TYPE_NORMAL
- en: begin comparing documents and selecting documents where a certain word and its
    frequency are higher
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asses the highlighted regions of a document and create a script that will extract
    these regions. From there, perform sentiment analysis on each of the sentences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Same approach as just mentioned except now to perform topic modeling analysis.
    Any sentence where a word is highlighted will be used to support the different
    topics within the text.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Benefits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While I mentioned there are limitations to this process, it does still have
    a few benefits and I believe you will find some utility in its use. First, it
    can show us how relevant a word is in different bodies of text. This can be helpful
    in academic research especially when you are studying a certain topic. If I’m
    studying machine learning, and one academic article mentions it more than another,
    I may be more inclined to look at that article first. Once I pick an article,
    this process will then highlight where the words sit in that article. Instead
    of mindlessly having to sift through the literature, this process centers our
    attention right where the information lies within a body of text. Talk about time
    savings!
  prefs: []
  type: TYPE_NORMAL
- en: Another benefit of this process is you can use it to begin highlighting where
    certain sentences are which may be useful if you wish to cite information from
    documents in different pieces of work. Being able to trace back where you found
    said information is not only important for providing proof to the viewers of your
    work of where the information was accumulated from, but also it’s a great way
    to ensure you are not plagiarizing your work. I tutor many students and I have
    used this process to highlight different articles for them so they can see where
    I was getting my work from (based on the topic we were studying) and use the highlighted
    documents as a reference for studying.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, this process is a great kickstart to your text extraction project.
    It gathers preliminary statistics to begin shaping the direction of your project
    as well as offers you a visual piece for understanding a body of text and having
    evidence of innate meaning to give to your customer. A real-world example of this
    was, I was developing a project for a customer who worked primarily in the food
    industry. They wanted to know if this giant document (hundreds of pages) they
    were given mentioned any of the goods they were seeking to sell and they also
    wanted to know who those goods were associated with. I was able to query the different
    goods they sell and provide the document back with the products highlighted in
    the document for quick referencing. They really enjoyed it! My only suggestion
    would be to also begin annotating page numbers because that will cut down analysis
    time even more!
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Today we looked at how you could analyze a PDF document, and find and locate
    words of interest within the document. Additionally, we can collect the frequencies
    of the queried words to gain a better understanding of the possible meaning of
    a corpus of text and if it is related to what we desire to know more about. Information
    extraction is critical because it can help us not only find relationships between
    different entities but also provide us with insights into possible patterns of
    actions and events conducted by these related entities and across various documents.
    Try this code out and I hope it helps you in your next NLP project!
  prefs: []
  type: TYPE_NORMAL
- en: '**If you enjoyed today’s reading, PLEASE give me a follow and let me know if
    there is another topic you would like me to explore! If you do not have a Medium
    account, sign up through my link** [**here**](https://ben-mccloskey20.medium.com/membership)
    **(I receive a small commission when you do this)! Additionally, add me on** [**LinkedIn**](https://www.linkedin.com/in/benjamin-mccloskey-169975a8/),
    **or feel free to reach out! Thanks for reading!**'
  prefs: []
  type: TYPE_NORMAL
- en: Sources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[https://web.stanford.edu/~jurafsky/slp3/old_oct19/17.pdf](https://web.stanford.edu/~jurafsky/slp3/old_oct19/17.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://dl.acm.org/doi/10.5555/2999134.2999257](https://dl.acm.org/doi/10.5555/2999134.2999257)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
