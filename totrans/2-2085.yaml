- en: The Values of Actions in Reinforcement Learning using Q-learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Q-learning 的强化学习中行动的价值
- en: 原文：[https://towardsdatascience.com/the-values-of-actions-in-reinforcement-learning-using-q-learning-cb4b03be5c81](https://towardsdatascience.com/the-values-of-actions-in-reinforcement-learning-using-q-learning-cb4b03be5c81)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-values-of-actions-in-reinforcement-learning-using-q-learning-cb4b03be5c81](https://towardsdatascience.com/the-values-of-actions-in-reinforcement-learning-using-q-learning-cb4b03be5c81)
- en: The Q-learning algorithm implemented from scratch in Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Python 从头实现的 Q-learning 算法
- en: '[](https://eligijus-bujokas.medium.com/?source=post_page-----cb4b03be5c81--------------------------------)[![Eligijus
    Bujokas](../Images/061fd30136caea2ba927140e8b3fae3c.png)](https://eligijus-bujokas.medium.com/?source=post_page-----cb4b03be5c81--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cb4b03be5c81--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cb4b03be5c81--------------------------------)
    [Eligijus Bujokas](https://eligijus-bujokas.medium.com/?source=post_page-----cb4b03be5c81--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://eligijus-bujokas.medium.com/?source=post_page-----cb4b03be5c81--------------------------------)[![Eligijus
    Bujokas](../Images/061fd30136caea2ba927140e8b3fae3c.png)](https://eligijus-bujokas.medium.com/?source=post_page-----cb4b03be5c81--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cb4b03be5c81--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cb4b03be5c81--------------------------------)
    [Eligijus Bujokas](https://eligijus-bujokas.medium.com/?source=post_page-----cb4b03be5c81--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cb4b03be5c81--------------------------------)
    ·10 min read·Feb 14, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cb4b03be5c81--------------------------------)
    ·阅读时间 10 分钟·2023年2月14日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a7047cb8a1a1248a35a9aac5acfc6a95.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a7047cb8a1a1248a35a9aac5acfc6a95.png)'
- en: Agent traversing a maze; GIF by author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 代理在迷宫中穿越；GIF 由作者提供
- en: 'This article is a continuation of a series of articles about Reinforcement
    Learning (RL). Check out the other articles here:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章是关于强化学习（RL）系列文章的续集。请在这里查看其他文章：
- en: '[](/first-steps-in-the-world-of-reinforcement-learning-using-python-b843b76538e3?source=post_page-----cb4b03be5c81--------------------------------)
    [## First Steps in the World Of Reinforcement Learning using Python'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/first-steps-in-the-world-of-reinforcement-learning-using-python-b843b76538e3?source=post_page-----cb4b03be5c81--------------------------------)
    [## 使用 Python 进行强化学习的第一步'
- en: Original Python implementation of how to find the best places to be in one of
    the fundamental worlds of reinforcement…
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何在强化学习的基本世界之一中找到最佳位置的原始 Python 实现…
- en: towardsdatascience.com](/first-steps-in-the-world-of-reinforcement-learning-using-python-b843b76538e3?source=post_page-----cb4b03be5c81--------------------------------)
    [](/temporal-differences-with-python-first-sample-based-reinforcement-learning-algorithm-54c11745a0ee?source=post_page-----cb4b03be5c81--------------------------------)
    [## Temporal Differences with Python — First Sample-Based Reinforcement Learning
    Algorithm
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/first-steps-in-the-world-of-reinforcement-learning-using-python-b843b76538e3?source=post_page-----cb4b03be5c81--------------------------------)
    [](/temporal-differences-with-python-first-sample-based-reinforcement-learning-algorithm-54c11745a0ee?source=post_page-----cb4b03be5c81--------------------------------)
    [## 使用 Python 的时间差分 — 第一个基于样本的强化学习算法
- en: Coding up and understanding the TD(0) algorithm using Python
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Python 编写和理解 TD(0) 算法
- en: towardsdatascience.com](/temporal-differences-with-python-first-sample-based-reinforcement-learning-algorithm-54c11745a0ee?source=post_page-----cb4b03be5c81--------------------------------)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/temporal-differences-with-python-first-sample-based-reinforcement-learning-algorithm-54c11745a0ee?source=post_page-----cb4b03be5c81--------------------------------)
- en: 'All the codes used can be viewed here: [https://github.com/Eligijus112/rl-snake-game](https://github.com/Eligijus112/rl-snake-game)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 所有使用的代码可以在这里查看：[https://github.com/Eligijus112/rl-snake-game](https://github.com/Eligijus112/rl-snake-game)
- en: 'The notebook with all the plotting functions and agent training codes can be
    viewed here: [https://github.com/Eligijus112/rl-snake-game/blob/master/chapter-6-qlearning.ipynb](https://github.com/Eligijus112/rl-snake-game/blob/master/chapter-6-qlearning.ipynb)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 包含所有绘图函数和代理训练代码的笔记本可以在这里查看：[https://github.com/Eligijus112/rl-snake-game/blob/master/chapter-6-qlearning.ipynb](https://github.com/Eligijus112/rl-snake-game/blob/master/chapter-6-qlearning.ipynb)
- en: In this article, I will present the reader with the concept of Q-values. For
    the sake of intuition, the reader can change the ***Q*** in ***Q-values*** for
    ***Quality-values***. The q values are numeric values that assign a score for
    *each action* taken from *each state:*
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将向读者介绍 Q 值的概念。为了直观起见，读者可以将 ***Q*** 替换为 ***Quality***。q 值是数值，分配给 *每个动作*
    从 *每个状态* 中的得分：
- en: '![](../Images/2b4d4a26701bab505fe9e19612648c7b.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b4d4a26701bab505fe9e19612648c7b.png)'
- en: Q value function
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Q 值函数
- en: The ***higher*** the score for a particular action in a given state, the ***better***
    it is for the agent to take that action.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定状态下，某个特定动作的 ***得分越高***，代理执行该动作的 ***效果越好***。
- en: For example, if we can choose from state 1 to go either left or right, then
    if
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们可以从状态 1 选择左移或右移，那么如果
- en: Q(left, 1) = 3.187
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Q(left, 1) = 3.187
- en: Q(right, 1) = 6.588
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Q(right, 1) = 6.588
- en: Then the better action from state 1 which leads to more value is the “right”
    action.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 那么从状态 1 出发，能够带来更多价值的更好动作就是“正确”的动作。
- en: An object that stores the q values is the **q-table**. The **q-table** is a
    matrix where each row is a state and each column is an action. We will denote
    such matrix as **Q**.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 存储 q 值的对象是**q-table**。**q-table** 是一个矩阵，每一行是一个状态，每一列是一个动作。我们将这种矩阵称为**Q**。
- en: 'From the previous articles, let''s remember some other important tables that
    are needed in Q-learning:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的文章中，让我们回顾一下 Q 学习中需要的一些其他重要表格：
- en: '**S** — state matrix, which indexes all our states.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**S** — 状态矩阵，用于索引所有状态。'
- en: '**R** — reward matrix, which indicates what rewards to we get for transitioning
    to a given state.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**R** — 奖励矩阵，指示过渡到给定状态时获得的奖励。'
- en: The value function **V** is not needed in Q learning, because we are not just
    interested in the value of a state but the value of a state-action pair.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Q 学习中不需要价值函数 **V**，因为我们不仅关心状态的价值，还关心状态-动作对的价值。
- en: 'Imagine that we have the following maze with 48 states:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下我们有以下 48 状态的迷宫：
- en: '![](../Images/f9dfef88ec1dba05707a355349dbcc43.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f9dfef88ec1dba05707a355349dbcc43.png)'
- en: Maze; Photo by author
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 迷宫；作者拍摄
- en: The yellow state is the state where our agent starts from (state 1).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 黄色状态是我们代理的起始状态（状态 1）。
- en: The green state is the goal state (state 38).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 绿色状态是目标状态（状态 38）。
- en: The red states are the walls of the maze. If an agent chooses to go to the wall
    state, it is returned to the last state he was in with no reward. The same logic
    applies to going out of bounds.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 红色状态是迷宫的墙壁。如果代理选择去墙壁状态，它将返回到最后一个状态且不获得奖励。离开边界的逻辑也适用。
- en: The actions that our agent can take are represented by the vector ***[0, 1,
    2, 3]*** which corresponds to ***[up, down, left, right]***.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代理可以采取的动作由向量 ***[0, 1, 2, 3]*** 表示，对应于 ***[上, 下, 左, 右]***。
- en: 'The initial Q table for such an agent is the following:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 该代理的初始 Q 表如下：
- en: '![](../Images/82a9aef4116ecffbbd3ce6a7ef8489d2.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/82a9aef4116ecffbbd3ce6a7ef8489d2.png)'
- en: 48x4 matrix; Photo by author
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 48x4 矩阵；作者拍摄
- en: There are 48 rows indicating each state.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 有 48 行表示每个状态。
- en: 'There are 4 columns representing the 4 actions our agent can take at each step:
    **up, down, left or right.**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 有 4 列表示代理在每一步可以采取的 4 种动作：**上、下、左或右**。
- en: The main objective of the Q-learning algorithm is to ***fill the above matrix
    so that our agent learns the most optimal path through the maze.***
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Q 学习算法的主要目标是 ***填充上述矩阵，以便我们的代理学习迷宫中最优的路径***。
- en: 'We will use a custom Agent class to implement the Q-learning algorithm:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用自定义的 Agent 类来实现 Q 学习算法：
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The full Q-learning algorithm is as follows¹
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的 Q 学习算法如下¹
- en: '![](../Images/6f06f73ab24689b9ed6c13be1bf122fe.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6f06f73ab24689b9ed6c13be1bf122fe.png)'
- en: Q-learning algorithm; Photo by author
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Q 学习算法；作者拍摄
- en: The *epsilon-greedy strategy policy* in step 2.2.1is to take the action with
    the biggest Q value with the **probability of 1 — epsilon** and take a random
    action with the **probability of epsilon.**
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤 2.2.1 中的 *epsilon-贪婪策略* 是以 **1 - epsilon** 的概率采取 Q 值最大的动作，并以 **epsilon**
    的概率采取随机动作。
- en: 'The above policy is implemented in our agent by the following code:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 上述策略通过以下代码在我们的代理中实现：
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The Q-learning step is the 2.2.3 step. In each state, our agent takes an action.
    Then, the learning is done by updating not only the current state the agent is
    in but the state-action pair **Q(S, A)**. The most important part of the update
    rule is that we look at the state in which our agent ends up by taking action
    and then extracting the maximum value of that state from the Q table.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us examine the equation more closely:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d4c1790c349a17f6de2449ca222aeae4.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: Q value update equation
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: The Q(S, A) is the state our agent is in and what action he has taken.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: The
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c54e2ad90bcb0706f3ac62538ad70e30.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
- en: The maximum of the transition state
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: part is the maximum available Q value in the state where our agent ends up in,
    across all the actions.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: The **r** is the reward of transitioning to a given state.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Everything else is user-defined hyperparameters.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Because we are using the algorithms' estimates for the updates of the Q-values,
    Q-learning falls under the bootstrapping methods family.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '**After every move that our agent takes, the Q table is updated.**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: 'The full implementation of the 2.2.3 step in our Agent class is the following:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The above function is called upon every move by our agent:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The above code snippet should be read from bottom to top.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'At each movement, we check if are in a terminal state or not. If the agent
    steps in the terminal state, the Q-learning update equation simplifies to:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7afb9a74c054d2c27ad8a054c7fd3213.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
- en: Update at the terminal state; Photo by author
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'Lets us initiate our agent, train it for one episode and visualize the agent
    path:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/4a09adfdb8cb772670b0b578b9854876.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
- en: Agent wandering in the maze; GIF by author
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: It took our agent 94 steps to reach the goal. At every step, the agent chooses
    an action in an epsilon-greedy way. In the first iteration, all the Q values are
    0 for any transitioning state so the epsilon-greedy algorithm is the same as random
    wandering.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us inspect the Q table after one episode. The Q table is all zeroes except
    for the state 30:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The Q(30, 1) (meaning go “down” from state 30) has a value of 1\. The equation
    by which this was calculated is:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5c5ed0da49d4f2b172edcf94a326e338.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
- en: Keep in mind that the initial **Q(30, 1) = 0.**
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 'After one episode, we have only learnt one Q-value. Let us train one more episode:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now the agent wandered from the other side of the maze and learnt that going
    from state 37 to the right is the best choice.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: What we would like to see is the number of steps that our agent makes to start
    decreasing as the episodes progress.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/2f7485898a9f88a1e1be9e0370882db2.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
- en: Number of steps taken vs number of episodes; Photo by author
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: After the initial wandering, by episode 20 the agent has a stable policy to
    follow and takes around 10 steps to go from the starting position to the end position.
    The variation arises because we are still using the epsilon greedy algorithm to
    make a move and 10% of the time, a random action will be chosen.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在初步探索后，到第20集时，代理拥有一个稳定的策略，并需要大约10步从起始位置到达终点位置。变异发生是因为我们仍在使用epsilon贪婪算法进行移动，而10%的时间会选择随机动作。
- en: 'Now, the optimal greedy policy looks like this:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，最佳贪婪策略如下所示：
- en: '![](../Images/1dc927e5f10104e35e0b50e3ee964c8e.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1dc927e5f10104e35e0b50e3ee964c8e.png)'
- en: Optimal policy; Photo by author.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 最优策略；作者拍摄的照片。
- en: 'Additionally, our agent tracks how many times it was in any given state. We
    can plot that to see which states were the most popular during the training phase:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们的代理跟踪它在任何给定状态下出现的次数。我们可以绘制这些数据，以查看在训练阶段哪些状态最受欢迎：
- en: '![](../Images/0279e5ef68680c61a2cd3f49bc3960e3.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0279e5ef68680c61a2cd3f49bc3960e3.png)'
- en: State visits; Photo by author
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 状态访问；作者拍摄的照片
- en: We can see that the agent quite often wandered to the left and the right from
    the starting state. But, because we let the agent take a random action only 10%
    of the time, the main path is the greedy one, meaning, the agent takes an action
    with the maximum Q value.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，代理经常在起始状态的左右徘徊。但是，由于我们让代理只有10%的时间采取随机动作，因此主要路径是贪婪的，也就是说，代理选择具有最大Q值的动作。
- en: 'And lastly, we can plot the final agent traversal path:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以绘制最终的代理遍历路径：
- en: '![](../Images/a7047cb8a1a1248a35a9aac5acfc6a95.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a7047cb8a1a1248a35a9aac5acfc6a95.png)'
- en: Agent traversal; GIF by author
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 代理遍历；作者制作的GIF
- en: 'To summarize:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 总结：
- en: The Q-learning algorithm updates the values in the Q-table after every action
    the agent takes.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q学习算法在代理执行每个动作后更新Q表中的值。
- en: Q-learning is a bootstrap algorithm because it uses its estimates to update
    the Q values.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q学习是一种自举算法，因为它使用自己的估计来更新Q值。
- en: In Q-learning, we only need the state, reward and q tables for the full algorithm
    implementation.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Q学习中，我们只需要状态、奖励和Q表来实现整个算法。
- en: 'The main update rule in Q-learning is:'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q学习中的主要更新规则是：
- en: '![](../Images/d4c1790c349a17f6de2449ca222aeae4.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d4c1790c349a17f6de2449ca222aeae4.png)'
- en: Q value update equation
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Q值更新方程
- en: Happy coding and happy learning!
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 编程愉快，学习愉快！
- en: '[1]'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[1]'
- en: 'Author: **Richard S. Sutton, Andrew G. Barto**'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作者：**理查德·S·萨顿，安德鲁·G·巴托**
- en: 'Year: **2018**'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 年份：**2018**
- en: 'Page: **131**'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 页码：**131**
- en: 'Title: **Reinforcement Learning: An Introduction**'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标题：**强化学习：导论**
- en: URL:[**http://archive.ics.uci.edu/ml**](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网址：[**http://archive.ics.uci.edu/ml**](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)
