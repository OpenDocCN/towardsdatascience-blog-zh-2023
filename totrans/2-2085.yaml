- en: The Values of Actions in Reinforcement Learning using Q-learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-values-of-actions-in-reinforcement-learning-using-q-learning-cb4b03be5c81](https://towardsdatascience.com/the-values-of-actions-in-reinforcement-learning-using-q-learning-cb4b03be5c81)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Q-learning algorithm implemented from scratch in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://eligijus-bujokas.medium.com/?source=post_page-----cb4b03be5c81--------------------------------)[![Eligijus
    Bujokas](../Images/061fd30136caea2ba927140e8b3fae3c.png)](https://eligijus-bujokas.medium.com/?source=post_page-----cb4b03be5c81--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cb4b03be5c81--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cb4b03be5c81--------------------------------)
    [Eligijus Bujokas](https://eligijus-bujokas.medium.com/?source=post_page-----cb4b03be5c81--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cb4b03be5c81--------------------------------)
    ·10 min read·Feb 14, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a7047cb8a1a1248a35a9aac5acfc6a95.png)'
  prefs: []
  type: TYPE_IMG
- en: Agent traversing a maze; GIF by author
  prefs: []
  type: TYPE_NORMAL
- en: 'This article is a continuation of a series of articles about Reinforcement
    Learning (RL). Check out the other articles here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/first-steps-in-the-world-of-reinforcement-learning-using-python-b843b76538e3?source=post_page-----cb4b03be5c81--------------------------------)
    [## First Steps in the World Of Reinforcement Learning using Python'
  prefs: []
  type: TYPE_NORMAL
- en: Original Python implementation of how to find the best places to be in one of
    the fundamental worlds of reinforcement…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/first-steps-in-the-world-of-reinforcement-learning-using-python-b843b76538e3?source=post_page-----cb4b03be5c81--------------------------------)
    [](/temporal-differences-with-python-first-sample-based-reinforcement-learning-algorithm-54c11745a0ee?source=post_page-----cb4b03be5c81--------------------------------)
    [## Temporal Differences with Python — First Sample-Based Reinforcement Learning
    Algorithm
  prefs: []
  type: TYPE_NORMAL
- en: Coding up and understanding the TD(0) algorithm using Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/temporal-differences-with-python-first-sample-based-reinforcement-learning-algorithm-54c11745a0ee?source=post_page-----cb4b03be5c81--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'All the codes used can be viewed here: [https://github.com/Eligijus112/rl-snake-game](https://github.com/Eligijus112/rl-snake-game)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The notebook with all the plotting functions and agent training codes can be
    viewed here: [https://github.com/Eligijus112/rl-snake-game/blob/master/chapter-6-qlearning.ipynb](https://github.com/Eligijus112/rl-snake-game/blob/master/chapter-6-qlearning.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I will present the reader with the concept of Q-values. For
    the sake of intuition, the reader can change the ***Q*** in ***Q-values*** for
    ***Quality-values***. The q values are numeric values that assign a score for
    *each action* taken from *each state:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2b4d4a26701bab505fe9e19612648c7b.png)'
  prefs: []
  type: TYPE_IMG
- en: Q value function
  prefs: []
  type: TYPE_NORMAL
- en: The ***higher*** the score for a particular action in a given state, the ***better***
    it is for the agent to take that action.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we can choose from state 1 to go either left or right, then
    if
  prefs: []
  type: TYPE_NORMAL
- en: Q(left, 1) = 3.187
  prefs: []
  type: TYPE_NORMAL
- en: Q(right, 1) = 6.588
  prefs: []
  type: TYPE_NORMAL
- en: Then the better action from state 1 which leads to more value is the “right”
    action.
  prefs: []
  type: TYPE_NORMAL
- en: An object that stores the q values is the **q-table**. The **q-table** is a
    matrix where each row is a state and each column is an action. We will denote
    such matrix as **Q**.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the previous articles, let''s remember some other important tables that
    are needed in Q-learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**S** — state matrix, which indexes all our states.'
  prefs: []
  type: TYPE_NORMAL
- en: '**R** — reward matrix, which indicates what rewards to we get for transitioning
    to a given state.'
  prefs: []
  type: TYPE_NORMAL
- en: The value function **V** is not needed in Q learning, because we are not just
    interested in the value of a state but the value of a state-action pair.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine that we have the following maze with 48 states:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f9dfef88ec1dba05707a355349dbcc43.png)'
  prefs: []
  type: TYPE_IMG
- en: Maze; Photo by author
  prefs: []
  type: TYPE_NORMAL
- en: The yellow state is the state where our agent starts from (state 1).
  prefs: []
  type: TYPE_NORMAL
- en: The green state is the goal state (state 38).
  prefs: []
  type: TYPE_NORMAL
- en: The red states are the walls of the maze. If an agent chooses to go to the wall
    state, it is returned to the last state he was in with no reward. The same logic
    applies to going out of bounds.
  prefs: []
  type: TYPE_NORMAL
- en: The actions that our agent can take are represented by the vector ***[0, 1,
    2, 3]*** which corresponds to ***[up, down, left, right]***.
  prefs: []
  type: TYPE_NORMAL
- en: 'The initial Q table for such an agent is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/82a9aef4116ecffbbd3ce6a7ef8489d2.png)'
  prefs: []
  type: TYPE_IMG
- en: 48x4 matrix; Photo by author
  prefs: []
  type: TYPE_NORMAL
- en: There are 48 rows indicating each state.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are 4 columns representing the 4 actions our agent can take at each step:
    **up, down, left or right.**'
  prefs: []
  type: TYPE_NORMAL
- en: The main objective of the Q-learning algorithm is to ***fill the above matrix
    so that our agent learns the most optimal path through the maze.***
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use a custom Agent class to implement the Q-learning algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The full Q-learning algorithm is as follows¹
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6f06f73ab24689b9ed6c13be1bf122fe.png)'
  prefs: []
  type: TYPE_IMG
- en: Q-learning algorithm; Photo by author
  prefs: []
  type: TYPE_NORMAL
- en: The *epsilon-greedy strategy policy* in step 2.2.1is to take the action with
    the biggest Q value with the **probability of 1 — epsilon** and take a random
    action with the **probability of epsilon.**
  prefs: []
  type: TYPE_NORMAL
- en: 'The above policy is implemented in our agent by the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The Q-learning step is the 2.2.3 step. In each state, our agent takes an action.
    Then, the learning is done by updating not only the current state the agent is
    in but the state-action pair **Q(S, A)**. The most important part of the update
    rule is that we look at the state in which our agent ends up by taking action
    and then extracting the maximum value of that state from the Q table.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us examine the equation more closely:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d4c1790c349a17f6de2449ca222aeae4.png)'
  prefs: []
  type: TYPE_IMG
- en: Q value update equation
  prefs: []
  type: TYPE_NORMAL
- en: The Q(S, A) is the state our agent is in and what action he has taken.
  prefs: []
  type: TYPE_NORMAL
- en: The
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c54e2ad90bcb0706f3ac62538ad70e30.png)'
  prefs: []
  type: TYPE_IMG
- en: The maximum of the transition state
  prefs: []
  type: TYPE_NORMAL
- en: part is the maximum available Q value in the state where our agent ends up in,
    across all the actions.
  prefs: []
  type: TYPE_NORMAL
- en: The **r** is the reward of transitioning to a given state.
  prefs: []
  type: TYPE_NORMAL
- en: Everything else is user-defined hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Because we are using the algorithms' estimates for the updates of the Q-values,
    Q-learning falls under the bootstrapping methods family.
  prefs: []
  type: TYPE_NORMAL
- en: '**After every move that our agent takes, the Q table is updated.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The full implementation of the 2.2.3 step in our Agent class is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The above function is called upon every move by our agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The above code snippet should be read from bottom to top.
  prefs: []
  type: TYPE_NORMAL
- en: 'At each movement, we check if are in a terminal state or not. If the agent
    steps in the terminal state, the Q-learning update equation simplifies to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7afb9a74c054d2c27ad8a054c7fd3213.png)'
  prefs: []
  type: TYPE_IMG
- en: Update at the terminal state; Photo by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Lets us initiate our agent, train it for one episode and visualize the agent
    path:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/4a09adfdb8cb772670b0b578b9854876.png)'
  prefs: []
  type: TYPE_IMG
- en: Agent wandering in the maze; GIF by author
  prefs: []
  type: TYPE_NORMAL
- en: It took our agent 94 steps to reach the goal. At every step, the agent chooses
    an action in an epsilon-greedy way. In the first iteration, all the Q values are
    0 for any transitioning state so the epsilon-greedy algorithm is the same as random
    wandering.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us inspect the Q table after one episode. The Q table is all zeroes except
    for the state 30:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The Q(30, 1) (meaning go “down” from state 30) has a value of 1\. The equation
    by which this was calculated is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5c5ed0da49d4f2b172edcf94a326e338.png)'
  prefs: []
  type: TYPE_IMG
- en: Keep in mind that the initial **Q(30, 1) = 0.**
  prefs: []
  type: TYPE_NORMAL
- en: 'After one episode, we have only learnt one Q-value. Let us train one more episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now the agent wandered from the other side of the maze and learnt that going
    from state 37 to the right is the best choice.
  prefs: []
  type: TYPE_NORMAL
- en: What we would like to see is the number of steps that our agent makes to start
    decreasing as the episodes progress.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/2f7485898a9f88a1e1be9e0370882db2.png)'
  prefs: []
  type: TYPE_IMG
- en: Number of steps taken vs number of episodes; Photo by author
  prefs: []
  type: TYPE_NORMAL
- en: After the initial wandering, by episode 20 the agent has a stable policy to
    follow and takes around 10 steps to go from the starting position to the end position.
    The variation arises because we are still using the epsilon greedy algorithm to
    make a move and 10% of the time, a random action will be chosen.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the optimal greedy policy looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1dc927e5f10104e35e0b50e3ee964c8e.png)'
  prefs: []
  type: TYPE_IMG
- en: Optimal policy; Photo by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, our agent tracks how many times it was in any given state. We
    can plot that to see which states were the most popular during the training phase:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0279e5ef68680c61a2cd3f49bc3960e3.png)'
  prefs: []
  type: TYPE_IMG
- en: State visits; Photo by author
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the agent quite often wandered to the left and the right from
    the starting state. But, because we let the agent take a random action only 10%
    of the time, the main path is the greedy one, meaning, the agent takes an action
    with the maximum Q value.
  prefs: []
  type: TYPE_NORMAL
- en: 'And lastly, we can plot the final agent traversal path:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a7047cb8a1a1248a35a9aac5acfc6a95.png)'
  prefs: []
  type: TYPE_IMG
- en: Agent traversal; GIF by author
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize:'
  prefs: []
  type: TYPE_NORMAL
- en: The Q-learning algorithm updates the values in the Q-table after every action
    the agent takes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q-learning is a bootstrap algorithm because it uses its estimates to update
    the Q values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Q-learning, we only need the state, reward and q tables for the full algorithm
    implementation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The main update rule in Q-learning is:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/d4c1790c349a17f6de2449ca222aeae4.png)'
  prefs: []
  type: TYPE_IMG
- en: Q value update equation
  prefs: []
  type: TYPE_NORMAL
- en: Happy coding and happy learning!
  prefs: []
  type: TYPE_NORMAL
- en: '[1]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Author: **Richard S. Sutton, Andrew G. Barto**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Year: **2018**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Page: **131**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Title: **Reinforcement Learning: An Introduction**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL:[**http://archive.ics.uci.edu/ml**](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
