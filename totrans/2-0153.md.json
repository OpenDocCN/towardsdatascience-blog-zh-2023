["```py\nCreate exactly {num_questions} questions using the context and make \nsure each question doesn’t reference\nterms like \"this study\", \"this research\", or anything that’s \nnot available to the reader.\nEnd each question with a ‘?’ character and then in a newline\nwrite the answer to that question using only \nthe context provided.\n\nSeparate each question/answer pair by \"XXX\"\n\nEach question must start with \"question:\".\n\nEach answer must start with \"answer:\".\n\nCONTEXT = {context}\n```", "```py\nfrom random import sample\nfrom langchain.llms import VertexAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\n\nSYNTHETIC_DATASET_SIZE = 70\nNUM_QUESTIONS_TO_GENERATE = 3\nsampled_chunks = sample(splits, k=SYNTHETIC_DATASET_SIZE)\n\nprompt = \"\"\"\nCreate exactly {num_questions} questions using the context and make sure each question doesn't reference\nterms like \"this study\", \"this research\", or anything that's not available to the reader.\n\nEnd each question with a '?' character and then in a newline write the answer to that question using only \nthe context provided.\nSeparate each question/answer pair by \"XXX\"\nEach question must start with \"question:\".\nEach answer must start with \"answer:\".\nCONTEXT = {context}\n\"\"\"\n\nllm = VertexAI(\n    model_name=\"text-bison\",\n    max_output_tokens=256,\n    temperature=0,\n    top_p=0.95,\n    top_k=40,\n    verbose=True,\n)\nprompt_template = PromptTemplate(\n    template=prompt,\n    input_variables=[\n        \"num_questions\",\n        \"context\",\n    ],\n)\nchain = LLMChain(llm=llm, prompt=prompt_template)\n```", "```py\nfrom langchain.document_loaders.bigquery import BigQueryLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\ndef load_data_from_bq(query, content_cols, metadata_cols):\n    loader = BigQueryLoader(\n        query=query, page_content_columns=content_cols, metadata_columns=metadata_cols\n    )\n    return loader.load()\n\nquery = ... # the SQL query to \ncontent_cols = ... # the content columns of your table\nmetadata_cols = ... # the metadata columns of your\n\nloaded_data = load_data_from_bq(\n    query,\n    content_cols=content_cols,\n    metadata_cols=metadata,\n)\n\nCHUNK_SIZE = 700\nCHUNK_OVERLAP = 100\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    separators=[\".\"],\n    chunk_size=CHUNK_SIZE,\n    chunk_overlap=CHUNK_OVERLAP,\n)\n\nsplits = text_splitter.split_documents(loaded_data)\n```", "```py\nimport pandas as pd\n\nSYNTHETIC_DATASET_SIZE = 70\nNUM_QUESTIONS_TO_GENERATE = 3\nsampled_chunks = sample(splits, k=SYNTHETIC_DATASET_SIZE)\n\nsynthetic_dataset = []\nfor sampled_chunk in tqdm(sampled_chunks):\n    prediction = chain.invoke(\n        {\n            \"num_questions\": NUM_QUESTIONS_TO_GENERATE,\n            \"context\": sampled_chunk.page_content,\n        }\n    )\n    output = prediction[\"text\"]\n\n    try:\n        questions_and_answers = parse_output(output)\n\n        for question_and_answer in questions_and_answers:\n            synthetic_dataset.append(\n                {\n                    **question_and_answer,\n                    \"context\": sampled_chunk.page_content,\n                    \"source\": sampled_chunk.metadata[\"source\"],\n                }\n            )\n    except:\n        pass\n\nsynthetic_dataset_df = pd.DataFrame(synthetic_dataset)\n```", "```py\nfrom langchain.embeddings import VertexAIEmbeddings\nfrom langchain.vectorstores import Chroma\n\ndb = Chroma.from_documents(\n    documents=splits,\n    embedding=VertexAIEmbeddings(),\n    persist_directory=\"./db\",\n)\n```", "```py\nRAG_TEMPLATE = \"\"\"Use the following pieces of context to answer the question at the end. \nIf you don't know the answer, just say that you don't know, don't try to make up an answer. \n{context}\nQuestion: {question}\nHelpful Answer:\"\"\"\n```", "```py\nclass RAG:\n    def __init__(self, vectorstore, rag_template=RAG_TEMPLATE):\n        self.vectorstore = vectorstore\n        self.rag_prompt = PromptTemplate.from_template(rag_template)\n        self.chain = LLMChain(llm=llm, prompt=self.rag_prompt)\n\n    def _format_context(self, docs):\n        context = [doc.page_content for doc, score in docs]\n        context = \"\\n---\\n\".join(context)\n        return context\n\n    def _format_source_documents(self, docs):\n        source_documents = []\n        for doc, score in docs:\n            doc.metadata[\"score\"] = score\n            source_documents.append(doc)\n        return source_documents\n\n    def predict(self, question, k=4, score_threshold=0.6):\n        relevant_documents = self.vectorstore.similarity_search_with_relevance_scores(\n            query=question,\n            k=k,\n            score_threshold=score_threshold,\n        )\n        source_documents = self._format_source_documents(relevant_documents)\n        context = self._format_context(relevant_documents)\n        answer = self.chain.predict(question=question, context=context)\n        output = {\n            \"question\": question,\n            \"answer\": answer,\n            \"source_documents\": source_documents,\n        }\n        return output\n\n# Create the RAG:\n\nrag = RAG(vectorstore=db)\n```", "```py\nevaluations = []\n\nfor i, row in tqdm(synthetic_dataset_df.iterrows(), total=len(synthetic_dataset_df)):\n    question = row[\"question\"]\n    ground_truth_answer = row[\"answer\"]\n    ground_truth_context = row[\"context\"]\n    ground_truth_source = row[\"source\"]\n    evaluation = {\n        \"question\": question,\n        \"ground_truth_answer\": ground_truth_answer,\n        \"ground_truth_source\": ground_truth_source,\n    }\n\n    prediction = rag.predict(question)\n    predicted_answer = prediction[\"answer\"]\n    predicted_contexts = [\n        source_document.page_content\n        for source_document in prediction[\"source_documents\"]\n    ]\n    predicted_sources = [\n        source_document.metadata[\"source\"]\n        for source_document in prediction[\"source_documents\"]\n    ]\n\n    evaluation[\"predicted_answer\"] = predicted_answer\n    evaluation[\"predicted_contexts\"] = predicted_contexts\n    evaluation[\"predicted_sources\"] = predicted_sources\n    evaluation[\"relevance_scores\"] = [\n        source_document.metadata[\"score\"]\n        for source_document in prediction[\"source_documents\"]\n    ]\n\n    evaluations.append(evaluation)\n\ndf_evaluations = pd.DataFrame(evaluations)\n```", "```py\nYour job is to rate the quality of a generated answer\ngiven a query  and a reference answer.\n\nQUERY = {query}\n\nGENERATED ANSWER = {generated_answer}\n\nREFERENCE ANSWER = {reference_answer}\n\nYour score has to be between 1 and 5.\nYou must return your response in a line with only the score.\nDo not return answers in any other format.\nOn a separate line provide your reasoning for the score as well.\n```", "```py\ndf_evaluations[\"top_1\"] = df_evaluations.apply(\n    lambda row: row[\"ground_truth_source\"] in row[\"predicted_sources\"][:1], axis=1\n)\ndf_evaluations[\"top_2\"] = df_evaluations.apply(\n    lambda row: row[\"ground_truth_source\"] in row[\"predicted_sources\"][:2], axis=1\n)\ndf_evaluations[\"top_3\"] = df_evaluations.apply(\n    lambda row: row[\"ground_truth_source\"] in row[\"predicted_sources\"][:3], axis=1\n)\ndf_evaluations[\"top_4\"] = df_evaluations.apply(\n    lambda row: row[\"ground_truth_source\"] in row[\"predicted_sources\"][:4], axis=1\n)\n```", "```py\nEVAL_TEMPLATE = \"\"\"Your job is to rate the quality of a generated answer\ngiven a query  and a reference answer.\n\nQUERY = {query}\n\nGENERATED ANSWER = {generated_answer}\n\nREFERENCE ANSWER = {reference_answer}\n\nYour score has to be between 1 and 5.\nYou must return your response in a line with only the score.\nDo not return answers in any other format.\nOn a separate line provide your reasoning for the score as well.\"\"\"\n\nprompt_template_eval = PromptTemplate.from_template(EVAL_TEMPLATE)\nllm_eval_chain = LLMChain(llm=llm, prompt=prompt_template_eval)\n```", "```py\ndef evaluate_answer(query, generated_answer, reference_answer):\n    answer = llm_eval_chain.invoke(\n        {\n            \"query\": query,\n            \"generated_answer\": generated_answer,\n            \"reference_answer\": reference_answer,\n        }\n    )\n    answer = answer[\"text\"]\n\n    try:\n        rating, reasoning = answer.split(\"\\n\")\n        rating = rating.strip()\n        reasoning = reasoning.strip()\n\n        evaluation = {\n            \"rating\": rating,\n            \"reasoning\": reasoning,\n        }\n        return evaluation\n    except:\n        return {\n            \"rating\": None,\n            \"reasoning\": None,\n        }\n```", "```py\nanswer_quality_evaluations = []\n\nfor i, row in tqdm(df_evaluations.iterrows(), total=len(df_evaluations)):\n    evaluation = evaluate_answer(\n        row[\"question\"],\n        row[\"predicted_answer\"],\n        row[\"ground_truth_answer\"],\n    )\n    answer_quality_evaluations.append(evaluation)\n```"]