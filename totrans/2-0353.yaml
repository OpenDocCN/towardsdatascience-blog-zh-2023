- en: 'Back To Basics, Part Uno: Linear Regression and Cost Function'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归基础，第一部分：线性回归和成本函数
- en: 原文：[https://towardsdatascience.com/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46](https://towardsdatascience.com/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46](https://towardsdatascience.com/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46)
- en: An illustrated guide on essential machine learning concepts
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一份关于基本机器学习概念的图解指南
- en: '[](https://medium.com/@shreya.rao?source=post_page-----590dcb3eee46--------------------------------)[![Shreya
    Rao](../Images/03f13be6f5f67783d32f0798f09a4f86.png)](https://medium.com/@shreya.rao?source=post_page-----590dcb3eee46--------------------------------)[](https://towardsdatascience.com/?source=post_page-----590dcb3eee46--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----590dcb3eee46--------------------------------)
    [Shreya Rao](https://medium.com/@shreya.rao?source=post_page-----590dcb3eee46--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@shreya.rao?source=post_page-----590dcb3eee46--------------------------------)[![Shreya
    Rao](../Images/03f13be6f5f67783d32f0798f09a4f86.png)](https://medium.com/@shreya.rao?source=post_page-----590dcb3eee46--------------------------------)[](https://towardsdatascience.com/?source=post_page-----590dcb3eee46--------------------------------)[![数据科学前沿](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----590dcb3eee46--------------------------------)
    [Shreya Rao](https://medium.com/@shreya.rao?source=post_page-----590dcb3eee46--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----590dcb3eee46--------------------------------)
    ·7 min read·Feb 3, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[数据科学前沿](https://towardsdatascience.com/?source=post_page-----590dcb3eee46--------------------------------)
    ·阅读时间7分钟·2023年2月3日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'Today, we will delve into three crucial concepts in Machine Learning: Linear
    Regression, Cost Function, and Gradient Descent. These concepts form the foundation
    of many machine learning algorithms. Initially, I decided against writing an article
    on these topics because they are so widely covered. However, I have changed my
    mind because understanding these concepts is essential for understanding more
    advanced topics like Neural Networks (that I plan on tackling in the near future).
    In addition, this series will be divided into two parts to make it more manageable
    and organized for better understanding.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，我们将深入探讨机器学习中的三个关键概念：线性回归、成本函数和梯度下降。这些概念构成了许多机器学习算法的基础。最初，我决定不写关于这些主题的文章，因为它们已经被广泛讨论。然而，我改变了主意，因为理解这些概念对于理解像神经网络这样的高级主题（我计划在不久的将来讨论）是至关重要的。此外，为了更好地理解，本系列将分为两部分进行。
- en: So make yourself comfortable, grab a cup of coffee, and get ready to embark
    on a magical journey of machine learning.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，舒适一下，拿杯咖啡，准备好开始这场神奇的机器学习之旅吧。
- en: As with any machine learning problem, we begin with a specific question we want
    to answer. In this case, our friend Mark is considering selling his 2400 feet²
    house and has come to us for assistance in determining the most appropriate price
    to list it at.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何机器学习问题一样，我们从一个具体的问题开始。在这个案例中，我们的朋友马克正在考虑出售他2400平方英尺的房子，并寻求帮助来确定最合适的挂牌价格。
- en: '![](../Images/dcfbc5c2f2237a4bf6a0c9d903f45f91.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dcfbc5c2f2237a4bf6a0c9d903f45f91.png)'
- en: Intuitively, we start by looking for comparable houses in our friend’s neighborhood.
    After a little digging, we find a list of three nearby houses and see how much
    they sold for. Of course, a typical dataset would have thousands or even tens
    of thousands of data points, but we’ll keep it simple with just these three houses.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地，我们从寻找我们朋友所在邻里中可比的房子开始。经过一些挖掘，我们找到了一份附近三栋房子的销售记录。自然，一个典型的数据集会有成千上万的数据点，但我们将简单化，先从这三栋房子开始。
- en: '![](../Images/ebbf3ec6c07c6ef5a2562e4194f1baa6.png)![](../Images/807b1214411e58dc811da3a77c3e3b4e.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ebbf3ec6c07c6ef5a2562e4194f1baa6.png)![](../Images/807b1214411e58dc811da3a77c3e3b4e.png)'
- en: 'Let’s plot this data:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制这些数据：
- en: '![](../Images/517ae27d42e7079f1d41af719ad1f1ae.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/517ae27d42e7079f1d41af719ad1f1ae.png)'
- en: 'By examining the data, the price of a house appears to be related to its size
    in a linear fashion. To model this relationship, we can use an *ML technique called
    Linear Regression*. This involves drawing a line on a scatter plot that best represents
    the pattern of the data points. Our model might look like this:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查数据，我们发现房屋的价格似乎与其面积在一个线性关系中相关。为了建模这种关系，我们可以使用一种*称为线性回归的机器学习技术*。这涉及在散点图上绘制一条最佳拟合数据点模式的直线。我们的模型可能如下所示：
- en: '![](../Images/a877efd396f37cf0243528fc86e5a9af.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a877efd396f37cf0243528fc86e5a9af.png)'
- en: Now using this line, we can say that a house that’s 2400 feet² should sell for..
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在使用这条直线，我们可以说一个2400平方英尺的房子应该卖...
- en: '![](../Images/06fb0a555ca5a9aea646fcab9b6dbcc7.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06fb0a555ca5a9aea646fcab9b6dbcc7.png)'
- en: …~$260,000\. And boom. That’s the answer.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: …~$260,000。然后就可以得到答案了。
- en: 'Now the big question: how do we determine the best-fitting line for our data?'
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 现在最大的问提是：我们如何确定最适合我们数据的直线？
- en: 'I could’ve drawn a line that’s a little off like this:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我本可以画一条稍微偏离的直线，如下所示：
- en: '![](../Images/0b251a927138c2d14bf7e6f5e22a6130.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0b251a927138c2d14bf7e6f5e22a6130.png)'
- en: 'Or, even worse, like this:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，甚至更糟，如下所示：
- en: '![](../Images/3dcdae2bd21b20df3b6c663e95669924.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3dcdae2bd21b20df3b6c663e95669924.png)'
- en: And we can clearly see that they don't fit our data nearly as well as our first
    line does.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以清楚地看到，它们并没有像我们的第一条直线那样很好地拟合数据。
- en: 'To figure out the best line, the first thing we need to do is mathematically
    represent what a bad line looks like. So let’s take this “bad” line and according
    to this a 2000 feet² house should sell for ~$140,000, whereas we know it actually
    sold for $300,000:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 要找出最佳直线，首先需要数学上表示出一条糟糕的直线是什么样的。所以我们取这条“糟糕”的直线，并根据它，2000平方英尺的房子应该卖 ~$140,000，而我们知道它实际卖了
    $300,000：
- en: '![](../Images/d387107ea2f59ff3bf13698c40c2b252.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d387107ea2f59ff3bf13698c40c2b252.png)'
- en: 'It is also significantly different from all the other values:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 它也与所有其他值显著不同：
- en: '![](../Images/50401260693e21401a3cb215fa4eb6a4.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/50401260693e21401a3cb215fa4eb6a4.png)'
- en: On average, this line is ~$94,000 off ($50,000 + $160,000 + $72,000 / 3).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 平均而言，这条直线的偏差为 ~$94,000（$50,000 + $160,000 + $72,000 / 3）。
- en: 'Here’s a better line:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一条更好的直线：
- en: '![](../Images/b4908af7df5f6e9b85061043c9be421b.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b4908af7df5f6e9b85061043c9be421b.png)'
- en: This line is an average of ~$44,000 dollars off, which is much better. This
    $44,000 is called the *cost* of using this line. The *cost* is how far off the
    line is from the real data. The best line is the one that is the least off from
    the real data or with the lowest cost. To find out what line is the best line,
    we need to use a **cost function**.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这条直线的平均偏差为 ~$44,000，这要好得多。这$44,000被称为*使用此直线的成本*。*成本*是直线与实际数据的偏差。最好的直线是与实际数据偏差最小或成本最低的直线。为了找出哪条直线是最好的，我们需要使用**成本函数**。
- en: Cost Function
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 成本函数
- en: 'Above, we utilized the **Mean Absolute Error (MAE)** cost function to determine
    the deviation of the actual house prices from the predicted prices. This basically
    calculates the average of how off the actual house prices (denoted as y, as it
    represents the value on the y-axis) were from the predicted house prices (denoted
    as ŷ). We represent **MAE** mathematically like this:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 上面，我们利用**均绝对误差（MAE）**成本函数来确定实际房价与预测价格之间的偏差。这基本上计算了实际房价（表示为y，因为它表示y轴上的值）与预测房价（表示为ŷ）之间的偏差的平均值。我们将**MAE**数学上表示为：
- en: '![](../Images/4cb66d1062eb9375eb5fd2207a965dc0.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4cb66d1062eb9375eb5fd2207a965dc0.png)'
- en: 'NOTE: Absolute values are used in the calculation of MAE because they ensure
    that the difference between predicted and actual values is always positive, regardless
    of whether the prediction is high or low. This allows for a fair comparison of
    error across different predictions, as positive and negative differences would
    cancel out if not taken absolute.'
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注：在计算MAE时使用绝对值，因为它们确保预测值与实际值之间的差异始终为正，无论预测值是高还是低。这允许在不同预测之间公平比较误差，因为如果不取绝对值，正负差异会相互抵消。
- en: Depending on the ML algorithm and problem at hand, there are various types of
    cost functions that can be employed. For our problem, instead of using the **MAE**,
    we will employ a commonly used method, the **Mean Squared Error (MSE)**, which
    calculates the *average of the squares of the difference between the predicted
    house price and the actual house price.*
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 根据机器学习算法和当前问题的不同，可以使用各种类型的成本函数。对于我们的问题，我们将使用一种常用的方法，即**均方误差（MSE）**，它计算*预测房价与实际房价之间差异的平方的平均值*。
- en: '![](../Images/bac23c84cc28f247635746332ce95605.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bac23c84cc28f247635746332ce95605.png)'
- en: Ultimately, the purpose of any cost function is to *minimize its value and reduce
    the cost to the greatest extent possible.*
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*最终*，任何成本函数的目的都是*将其值最小化并尽可能降低成本*。'
- en: Equation of the Line
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 直线方程
- en: 'Before diving deeper into linear regression, let’s take a step back and review
    the basics. Here’s an example of a line: *y = 1 + 2x*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究线性回归之前，让我们退一步回顾基础知识。这里有一个直线的例子：*y = 1 + 2x*
- en: The first number, called the *intercept*, tells us how high the line should
    be at the start.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个数字，称为*截距*，告诉我们直线在开始时应该有多高。
- en: '![](../Images/ada857bbaf83b3a63b8637896c35fe59.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ada857bbaf83b3a63b8637896c35fe59.png)'
- en: 'And the second one tells us the angle (or, in technical terms, the *slope*)
    of the line:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个数字告诉我们直线的角度（或者，用技术术语说，就是*斜率*）：
- en: '![](../Images/3dd13ae28251003731c5ce35dd5858ec.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3dd13ae28251003731c5ce35dd5858ec.png)'
- en: Now that we understand how the equation works, we just need to determine the
    optimal values for these two values — the s*lope* and the i*ntercept* to get our
    best-fitting line for our linear regression problem. To make things even simpler,
    let’s assume that we somehow magically already have the value of the *slope*,
    0.069.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们了解了方程的工作原理，我们只需要确定这两个值的最佳值——*斜率*和*截距*，以获得我们线性回归问题的最佳拟合直线。为了更简单些，假设我们已经神奇地拥有了*斜率*的值，0.069。
- en: 'So the equation of our linear regression line is:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们的线性回归直线的方程是：
- en: '![](../Images/751b1a86ea781defb3d479f609834ff0.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/751b1a86ea781defb3d479f609834ff0.png)'
- en: To get the predicted price of any house of a certain size, all we need to do
    is plug in the values of the *intercept* and desired house size. For instance,
    for a house of size 1000 feet² with *intercept* 0…
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取某一特定大小房屋的预测价格，我们只需要插入*截距*和期望的房屋大小。例如，对于一个大小为1000平方英尺，*截距*为0的房子……
- en: '![](../Images/ad6ff0ff5b2957f775a722653083644e.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad6ff0ff5b2957f775a722653083644e.png)'
- en: …we get a predicted house price of $69,000\. So all we need to do now to get
    our linear regression model is to find the optimal value for the *intercept*.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: …我们得到一个预测的房价为69,000美元。所以现在我们需要做的就是找到*截距*的最佳值，以便得到我们的线性回归模型。
- en: One option (which we will soon find to be quite tedious and not very fun) is
    to use brute force, where we repeatedly guess the value of the *intercept*, draw
    a LR line, and calculate the **MSE**. Just for the sake of experimentation, let’s
    try this approach for a moment.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一种选择（我们很快会发现这种方法相当繁琐且不太有趣）是使用蛮力法，其中我们反复猜测*截距*的值，绘制一个线性回归线，并计算**MSE**。为了实验的目的，我们暂时尝试这种方法。
- en: 'Start by guessing a random value of the intercept (let’s start with 0) and
    plotting the LR line:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个随机的*截距*值开始猜测（我们从0开始），并绘制线性回归线：
- en: '![](../Images/8cc0976b29675159f2cc7d3e74a7fa45.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8cc0976b29675159f2cc7d3e74a7fa45.png)'
- en: 'Then we calculate the ***MSE*** of this line:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们计算这条线的***MSE***：
- en: '![](../Images/f3fdbe0e61112693b0ba88421788c678.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3fdbe0e61112693b0ba88421788c678.png)'
- en: 'To gain a visual understanding, let’s plot the intercept value and the corresponding**MSE**
    on a graph:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得直观的理解，我们在图上绘制*截距*值和相应的**MSE**：
- en: '![](../Images/69543fa1253e0fa4e1b33d29bbacf10f.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/69543fa1253e0fa4e1b33d29bbacf10f.png)'
- en: Next, we’ll test another value for the *intercept* (let’s say 25), plot the
    corresponding line, and calculate the **MSE**.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将测试另一个*截距*值（假设为25），绘制相应的直线，并计算**MSE**。
- en: '![](../Images/6741565ab99a215366b1b89dcee7eb55.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6741565ab99a215366b1b89dcee7eb55.png)'
- en: 'We can continue this process with different values of the *intercept* (= 0,
    25, 50, 75, 100, 125, 150, and 175) until we end up with a graph that looks like
    this:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以继续使用不同的*截距*值（= 0, 25, 50, 75, 100, 125, 150, 175）进行这个过程，直到我们得到一个如下所示的图：
- en: '![](../Images/4d3214ba1eaf5b35acfc0d0a384ff332.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d3214ba1eaf5b35acfc0d0a384ff332.png)'
- en: 'From the points plotted on the graph, we can see that the **MSE** is the lowest
    when the intercept is set to 100\. However, it is possible that there may be another
    *intercept* value between 75 and 100 that would result in an even lower **MSE**.
    A slow and painful method for finding the minimal **MSE** is to plug and chug
    a bunch more values for the intercept as shown below:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 从图上绘制的点可以看出，当*截距*设置为100时，**MSE**是最低的。然而，也有可能在75和100之间存在另一个*截距*值会产生更低的**MSE**。找到最小**MSE**的一种缓慢而痛苦的方法是像下面这样插入更多的*截距*值：
- en: '![](../Images/917bfb11f2b0a069d4b023b949c2bde5.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/917bfb11f2b0a069d4b023b949c2bde5.png)'
- en: Despite our efforts, we cannot be certain that we have found the lowest possible
    **MSE** value. The process of testing multiple intercept values is both tedious
    and inefficient. Fortunately, gradient descent can help solve this problem by
    finding the optimal solution in a more efficient and effective way. And this is
    exactly what we will explore in the [second part](https://medium.com/towards-data-science/back-to-basics-part-dos-linear-regression-cost-function-and-gradient-descent-e3d7d05c56fd)
    of this series!
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们付出了努力，但我们无法确定是否找到了最低可能的**均方误差**值。测试多个截距值的过程既繁琐又低效。幸运的是，梯度下降可以通过更高效和有效的方式找到最优解。而这正是我们将在本系列的[第二部分](https://medium.com/towards-data-science/back-to-basics-part-dos-linear-regression-cost-function-and-gradient-descent-e3d7d05c56fd)中探讨的内容！
- en: '[](/back-to-basics-part-dos-linear-regression-cost-function-and-gradient-descent-e3d7d05c56fd?source=post_page-----590dcb3eee46--------------------------------)
    [## Back To Basics, Part Dos: Gradient Descent'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/back-to-basics-part-dos-linear-regression-cost-function-and-gradient-descent-e3d7d05c56fd?source=post_page-----590dcb3eee46--------------------------------)
    [## 回到基础，第两部分：梯度下降'
- en: An accessible perspective on essential machine learning concepts
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于基本机器学习概念的易于理解的视角
- en: towardsdatascience.com](/back-to-basics-part-dos-linear-regression-cost-function-and-gradient-descent-e3d7d05c56fd?source=post_page-----590dcb3eee46--------------------------------)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/back-to-basics-part-dos-linear-regression-cost-function-and-gradient-descent-e3d7d05c56fd?source=post_page-----590dcb3eee46--------------------------------)
- en: You can connect with me on [LinkedIn](https://www.linkedin.com/in/shreyarao24/)
    or email me at *shreya.statistics@gmail.com* to send me questions and suggestions
    for any other algorithms that you want illustrated!
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过[LinkedIn](https://www.linkedin.com/in/shreyarao24/)与我联系，或通过*shreya.statistics@gmail.com*发邮件给我，提出问题和建议，尤其是对于你希望我讲解的其他算法！
