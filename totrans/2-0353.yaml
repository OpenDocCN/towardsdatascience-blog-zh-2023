- en: 'Back To Basics, Part Uno: Linear Regression and Cost Function'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46](https://towardsdatascience.com/back-to-basics-part-uno-linear-regression-cost-function-and-gradient-descent-590dcb3eee46)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An illustrated guide on essential machine learning concepts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@shreya.rao?source=post_page-----590dcb3eee46--------------------------------)[![Shreya
    Rao](../Images/03f13be6f5f67783d32f0798f09a4f86.png)](https://medium.com/@shreya.rao?source=post_page-----590dcb3eee46--------------------------------)[](https://towardsdatascience.com/?source=post_page-----590dcb3eee46--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----590dcb3eee46--------------------------------)
    [Shreya Rao](https://medium.com/@shreya.rao?source=post_page-----590dcb3eee46--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----590dcb3eee46--------------------------------)
    ·7 min read·Feb 3, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: 'Today, we will delve into three crucial concepts in Machine Learning: Linear
    Regression, Cost Function, and Gradient Descent. These concepts form the foundation
    of many machine learning algorithms. Initially, I decided against writing an article
    on these topics because they are so widely covered. However, I have changed my
    mind because understanding these concepts is essential for understanding more
    advanced topics like Neural Networks (that I plan on tackling in the near future).
    In addition, this series will be divided into two parts to make it more manageable
    and organized for better understanding.'
  prefs: []
  type: TYPE_NORMAL
- en: So make yourself comfortable, grab a cup of coffee, and get ready to embark
    on a magical journey of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: As with any machine learning problem, we begin with a specific question we want
    to answer. In this case, our friend Mark is considering selling his 2400 feet²
    house and has come to us for assistance in determining the most appropriate price
    to list it at.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dcfbc5c2f2237a4bf6a0c9d903f45f91.png)'
  prefs: []
  type: TYPE_IMG
- en: Intuitively, we start by looking for comparable houses in our friend’s neighborhood.
    After a little digging, we find a list of three nearby houses and see how much
    they sold for. Of course, a typical dataset would have thousands or even tens
    of thousands of data points, but we’ll keep it simple with just these three houses.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ebbf3ec6c07c6ef5a2562e4194f1baa6.png)![](../Images/807b1214411e58dc811da3a77c3e3b4e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s plot this data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/517ae27d42e7079f1d41af719ad1f1ae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By examining the data, the price of a house appears to be related to its size
    in a linear fashion. To model this relationship, we can use an *ML technique called
    Linear Regression*. This involves drawing a line on a scatter plot that best represents
    the pattern of the data points. Our model might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a877efd396f37cf0243528fc86e5a9af.png)'
  prefs: []
  type: TYPE_IMG
- en: Now using this line, we can say that a house that’s 2400 feet² should sell for..
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/06fb0a555ca5a9aea646fcab9b6dbcc7.png)'
  prefs: []
  type: TYPE_IMG
- en: …~$260,000\. And boom. That’s the answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now the big question: how do we determine the best-fitting line for our data?'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I could’ve drawn a line that’s a little off like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0b251a927138c2d14bf7e6f5e22a6130.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Or, even worse, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3dcdae2bd21b20df3b6c663e95669924.png)'
  prefs: []
  type: TYPE_IMG
- en: And we can clearly see that they don't fit our data nearly as well as our first
    line does.
  prefs: []
  type: TYPE_NORMAL
- en: 'To figure out the best line, the first thing we need to do is mathematically
    represent what a bad line looks like. So let’s take this “bad” line and according
    to this a 2000 feet² house should sell for ~$140,000, whereas we know it actually
    sold for $300,000:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d387107ea2f59ff3bf13698c40c2b252.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It is also significantly different from all the other values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/50401260693e21401a3cb215fa4eb6a4.png)'
  prefs: []
  type: TYPE_IMG
- en: On average, this line is ~$94,000 off ($50,000 + $160,000 + $72,000 / 3).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a better line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b4908af7df5f6e9b85061043c9be421b.png)'
  prefs: []
  type: TYPE_IMG
- en: This line is an average of ~$44,000 dollars off, which is much better. This
    $44,000 is called the *cost* of using this line. The *cost* is how far off the
    line is from the real data. The best line is the one that is the least off from
    the real data or with the lowest cost. To find out what line is the best line,
    we need to use a **cost function**.
  prefs: []
  type: TYPE_NORMAL
- en: Cost Function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Above, we utilized the **Mean Absolute Error (MAE)** cost function to determine
    the deviation of the actual house prices from the predicted prices. This basically
    calculates the average of how off the actual house prices (denoted as y, as it
    represents the value on the y-axis) were from the predicted house prices (denoted
    as ŷ). We represent **MAE** mathematically like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4cb66d1062eb9375eb5fd2207a965dc0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'NOTE: Absolute values are used in the calculation of MAE because they ensure
    that the difference between predicted and actual values is always positive, regardless
    of whether the prediction is high or low. This allows for a fair comparison of
    error across different predictions, as positive and negative differences would
    cancel out if not taken absolute.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Depending on the ML algorithm and problem at hand, there are various types of
    cost functions that can be employed. For our problem, instead of using the **MAE**,
    we will employ a commonly used method, the **Mean Squared Error (MSE)**, which
    calculates the *average of the squares of the difference between the predicted
    house price and the actual house price.*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bac23c84cc28f247635746332ce95605.png)'
  prefs: []
  type: TYPE_IMG
- en: Ultimately, the purpose of any cost function is to *minimize its value and reduce
    the cost to the greatest extent possible.*
  prefs: []
  type: TYPE_NORMAL
- en: Equation of the Line
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before diving deeper into linear regression, let’s take a step back and review
    the basics. Here’s an example of a line: *y = 1 + 2x*'
  prefs: []
  type: TYPE_NORMAL
- en: The first number, called the *intercept*, tells us how high the line should
    be at the start.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ada857bbaf83b3a63b8637896c35fe59.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And the second one tells us the angle (or, in technical terms, the *slope*)
    of the line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3dd13ae28251003731c5ce35dd5858ec.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we understand how the equation works, we just need to determine the
    optimal values for these two values — the s*lope* and the i*ntercept* to get our
    best-fitting line for our linear regression problem. To make things even simpler,
    let’s assume that we somehow magically already have the value of the *slope*,
    0.069.
  prefs: []
  type: TYPE_NORMAL
- en: 'So the equation of our linear regression line is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/751b1a86ea781defb3d479f609834ff0.png)'
  prefs: []
  type: TYPE_IMG
- en: To get the predicted price of any house of a certain size, all we need to do
    is plug in the values of the *intercept* and desired house size. For instance,
    for a house of size 1000 feet² with *intercept* 0…
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ad6ff0ff5b2957f775a722653083644e.png)'
  prefs: []
  type: TYPE_IMG
- en: …we get a predicted house price of $69,000\. So all we need to do now to get
    our linear regression model is to find the optimal value for the *intercept*.
  prefs: []
  type: TYPE_NORMAL
- en: One option (which we will soon find to be quite tedious and not very fun) is
    to use brute force, where we repeatedly guess the value of the *intercept*, draw
    a LR line, and calculate the **MSE**. Just for the sake of experimentation, let’s
    try this approach for a moment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by guessing a random value of the intercept (let’s start with 0) and
    plotting the LR line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8cc0976b29675159f2cc7d3e74a7fa45.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then we calculate the ***MSE*** of this line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f3fdbe0e61112693b0ba88421788c678.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To gain a visual understanding, let’s plot the intercept value and the corresponding**MSE**
    on a graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/69543fa1253e0fa4e1b33d29bbacf10f.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, we’ll test another value for the *intercept* (let’s say 25), plot the
    corresponding line, and calculate the **MSE**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6741565ab99a215366b1b89dcee7eb55.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can continue this process with different values of the *intercept* (= 0,
    25, 50, 75, 100, 125, 150, and 175) until we end up with a graph that looks like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4d3214ba1eaf5b35acfc0d0a384ff332.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the points plotted on the graph, we can see that the **MSE** is the lowest
    when the intercept is set to 100\. However, it is possible that there may be another
    *intercept* value between 75 and 100 that would result in an even lower **MSE**.
    A slow and painful method for finding the minimal **MSE** is to plug and chug
    a bunch more values for the intercept as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/917bfb11f2b0a069d4b023b949c2bde5.png)'
  prefs: []
  type: TYPE_IMG
- en: Despite our efforts, we cannot be certain that we have found the lowest possible
    **MSE** value. The process of testing multiple intercept values is both tedious
    and inefficient. Fortunately, gradient descent can help solve this problem by
    finding the optimal solution in a more efficient and effective way. And this is
    exactly what we will explore in the [second part](https://medium.com/towards-data-science/back-to-basics-part-dos-linear-regression-cost-function-and-gradient-descent-e3d7d05c56fd)
    of this series!
  prefs: []
  type: TYPE_NORMAL
- en: '[](/back-to-basics-part-dos-linear-regression-cost-function-and-gradient-descent-e3d7d05c56fd?source=post_page-----590dcb3eee46--------------------------------)
    [## Back To Basics, Part Dos: Gradient Descent'
  prefs: []
  type: TYPE_NORMAL
- en: An accessible perspective on essential machine learning concepts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/back-to-basics-part-dos-linear-regression-cost-function-and-gradient-descent-e3d7d05c56fd?source=post_page-----590dcb3eee46--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: You can connect with me on [LinkedIn](https://www.linkedin.com/in/shreyarao24/)
    or email me at *shreya.statistics@gmail.com* to send me questions and suggestions
    for any other algorithms that you want illustrated!
  prefs: []
  type: TYPE_NORMAL
