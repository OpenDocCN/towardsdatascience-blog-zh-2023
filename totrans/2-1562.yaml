- en: Naive Bayes from scratch with TensorFlow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»é›¶å¼€å§‹çš„æœ´ç´ è´å¶æ–¯ä¸ TensorFlow
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/naive-bayes-from-scratch-with-tensorflow-6e04c5a25947](https://towardsdatascience.com/naive-bayes-from-scratch-with-tensorflow-6e04c5a25947)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/naive-bayes-from-scratch-with-tensorflow-6e04c5a25947](https://towardsdatascience.com/naive-bayes-from-scratch-with-tensorflow-6e04c5a25947)
- en: Probabilistic Deep Learning
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¦‚ç‡æ·±åº¦å­¦ä¹ 
- en: '[](https://medium.com/@luisroque?source=post_page-----6e04c5a25947--------------------------------)[![LuÃ­s
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----6e04c5a25947--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6e04c5a25947--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6e04c5a25947--------------------------------)
    [LuÃ­s Roque](https://medium.com/@luisroque?source=post_page-----6e04c5a25947--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@luisroque?source=post_page-----6e04c5a25947--------------------------------)[![è·¯æ˜“æ–¯Â·ç½—å…‹](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----6e04c5a25947--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6e04c5a25947--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6e04c5a25947--------------------------------)
    [è·¯æ˜“æ–¯Â·ç½—å…‹](https://medium.com/@luisroque?source=post_page-----6e04c5a25947--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6e04c5a25947--------------------------------)
    Â·10 min readÂ·Jan 18, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6e04c5a25947--------------------------------)
    Â·10åˆ†é’Ÿé˜…è¯»Â·2023å¹´1æœˆ18æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Introduction
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»‹ç»
- en: This article belongs to the series â€œProbabilistic Deep Learningâ€. This weekly
    series covers probabilistic approaches to deep learning. The main goal is to extend
    deep learning models to quantify uncertainty, i.e., know what they do not know.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡å±äºâ€œæ¦‚ç‡æ·±åº¦å­¦ä¹ â€ç³»åˆ—ã€‚è¯¥ç³»åˆ—æ¯å‘¨æ¶µç›–æ·±åº¦å­¦ä¹ çš„æ¦‚ç‡æ–¹æ³•ã€‚ä¸»è¦ç›®æ ‡æ˜¯æ‰©å±•æ·±åº¦å­¦ä¹ æ¨¡å‹ä»¥é‡åŒ–ä¸ç¡®å®šæ€§ï¼Œå³äº†è§£å®ƒä»¬ä¸çŸ¥é“çš„å†…å®¹ã€‚
- en: In this article, we present an examination of the Naive Bayes algorithm for
    classification tasks using a dataset of wine samples. The Naive Bayes algorithm
    is a probabilistic machine learning technique based on Bayesâ€™ theorem, which makes
    assumptions about the independence of features given the target label. To facilitate
    visualization of the separation of classes, we limit the model to only two features.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹ä½¿ç”¨è‘¡è„é…’æ ·æœ¬æ•°æ®é›†çš„æœ´ç´ è´å¶æ–¯åˆ†ç±»ç®—æ³•è¿›è¡Œäº†è€ƒå¯Ÿã€‚æœ´ç´ è´å¶æ–¯ç®—æ³•æ˜¯ä¸€ç§åŸºäºè´å¶æ–¯å®šç†çš„æ¦‚ç‡æœºå™¨å­¦ä¹ æŠ€æœ¯ï¼Œå‡è®¾åœ¨ç»™å®šç›®æ ‡æ ‡ç­¾çš„æƒ…å†µä¸‹ç‰¹å¾ä¹‹é—´æ˜¯ç‹¬ç«‹çš„ã€‚ä¸ºäº†ä¾¿äºå¯è§†åŒ–ç±»åˆ«çš„åˆ†ç¦»ï¼Œæˆ‘ä»¬å°†æ¨¡å‹é™åˆ¶ä¸ºä»…ä½¿ç”¨ä¸¤ä¸ªç‰¹å¾ã€‚
- en: Our objective is to classify wine samples based on selected characteristics.
    To accomplish this, we begin by exploring the data and selecting features that
    effectively separate the classes. We then proceed to construct class prior distributions
    and class-conditional densities, leading to the ability to predict the class with
    the highest probability. The dataset used in this study consists of various features
    of wines, such as hue, alcohol, flavonoids, and a target class, and is obtained
    from the scikit-learn library [1].
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ç›®æ ‡æ˜¯åŸºäºé€‰å®šçš„ç‰¹å¾å¯¹è‘¡è„é…’æ ·æœ¬è¿›è¡Œåˆ†ç±»ã€‚ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬é¦–å…ˆæ¢ç´¢æ•°æ®å¹¶é€‰æ‹©æœ‰æ•ˆåŒºåˆ†å„ç±»åˆ«çš„ç‰¹å¾ã€‚ç„¶åï¼Œæˆ‘ä»¬æ„å»ºç±»åˆ«å…ˆéªŒåˆ†å¸ƒå’Œç±»åˆ«æ¡ä»¶å¯†åº¦ï¼Œä»è€Œèƒ½å¤Ÿé¢„æµ‹å…·æœ‰æœ€é«˜æ¦‚ç‡çš„ç±»åˆ«ã€‚è¯¥ç ”ç©¶ä½¿ç”¨çš„æ•°æ®é›†åŒ…å«è‘¡è„é…’çš„å„ç§ç‰¹å¾ï¼Œå¦‚è‰²è°ƒã€é…’ç²¾ã€ç±»é»„é…®ä»¥åŠä¸€ä¸ªç›®æ ‡ç±»åˆ«ï¼Œå¹¶ä¸”æ•°æ®é›†æ¥è‡ª
    scikit-learn åº“ [1]ã€‚
- en: 'Articles published so far:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: è¿„ä»Šä¸ºæ­¢å‘è¡¨çš„æ–‡ç« ï¼š
- en: '[Gentle Introduction to TensorFlow Probability: Distribution Objects](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-distribution-objects-1bb6165abee1)'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[TensorFlow Probability çš„æ¸©å’Œä»‹ç»ï¼šåˆ†å¸ƒå¯¹è±¡](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-distribution-objects-1bb6165abee1)'
- en: '[Gentle Introduction to TensorFlow Probability: Trainable Parameters](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-trainable-parameters-5098ea4fed15)'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[TensorFlow Probability çš„æ¸©å’Œä»‹ç»ï¼šå¯è®­ç»ƒå‚æ•°](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-trainable-parameters-5098ea4fed15)'
- en: '[Maximum Likelihood Estimation from scratch in TensorFlow Probability](/maximum-likelihood-estimation-from-scratch-in-tensorflow-probability-2fc0eefdbfc2)'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ä»é›¶å¼€å§‹çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼Œä½¿ç”¨ TensorFlow Probability](/maximum-likelihood-estimation-from-scratch-in-tensorflow-probability-2fc0eefdbfc2)'
- en: '[Probabilistic Linear Regression from scratch in TensorFlow](/probabilistic-linear-regression-from-scratch-in-tensorflow-2eb633fffc00)'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ä»é›¶å¼€å§‹çš„æ¦‚ç‡çº¿æ€§å›å½’ï¼Œä½¿ç”¨ TensorFlow](/probabilistic-linear-regression-from-scratch-in-tensorflow-2eb633fffc00)'
- en: '[Probabilistic vs. Deterministic Regression with Tensorflow](https://medium.com/towards-data-science/probabilistic-vs-deterministic-regression-with-tensorflow-85ef791beeef)'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Tensorflowä¸­çš„æ¦‚ç‡æ€§ä¸ç¡®å®šæ€§å›å½’](https://medium.com/towards-data-science/probabilistic-vs-deterministic-regression-with-tensorflow-85ef791beeef)'
- en: '[Frequentist vs. Bayesian Statistics with Tensorflow](https://medium.com/towards-data-science/frequentist-vs-bayesian-statistics-with-tensorflow-fbba2c6c9ae5)'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[é¢‘ç‡å­¦æ´¾ä¸è´å¶æ–¯ç»Ÿè®¡åœ¨Tensorflowä¸­çš„åº”ç”¨](https://medium.com/towards-data-science/frequentist-vs-bayesian-statistics-with-tensorflow-fbba2c6c9ae5)'
- en: '[Deterministic vs. Probabilistic Deep Learning](/deterministic-vs-probabilistic-deep-learning-5325769dc758)'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ç¡®å®šæ€§ä¸æ¦‚ç‡æ€§æ·±åº¦å­¦ä¹ ](/deterministic-vs-probabilistic-deep-learning-5325769dc758)'
- en: Naive Bayes from scratch with TensorFlow
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»å¤´å¼€å§‹ä½¿ç”¨TensorFlowå®ç°æœ´ç´ è´å¶æ–¯
- en: '![](../Images/32c7b8ce4cb661b5ede5f8501c912b4e.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/32c7b8ce4cb661b5ede5f8501c912b4e.png)'
- en: 'Figure 1: The motto for today: be naive when it comes to wine classification?
    ([source](https://unsplash.com/photos/3uJt73tr4hI))'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾1ï¼šä»Šå¤©çš„æ ¼è¨€ï¼šåœ¨è‘¡è„é…’åˆ†ç±»æ–¹é¢è¦ä¿æŒ**å¤©çœŸ**ï¼Ÿ ([source](https://unsplash.com/photos/3uJt73tr4hI))
- en: We develop our models using TensorFlow and TensorFlow Probability. TensorFlow
    Probability is a Python library built on top of TensorFlow. We will start with
    the basic objects we can find in TensorFlow Probability and understand how we
    can manipulate them. We will increase complexity incrementally over the following
    weeks and combine our probabilistic models with deep learning on modern hardware
    (e.g. GPU).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨TensorFlowå’ŒTensorFlow Probabilityå¼€å‘æˆ‘ä»¬çš„æ¨¡å‹ã€‚TensorFlow Probabilityæ˜¯ä¸€ä¸ªå»ºç«‹åœ¨TensorFlowä¹‹ä¸Šçš„Pythonåº“ã€‚æˆ‘ä»¬å°†ä»TensorFlow
    Probabilityä¸­çš„åŸºæœ¬å¯¹è±¡å¼€å§‹ï¼Œç†è§£å¦‚ä½•æ“ä½œå®ƒä»¬ã€‚æ¥ä¸‹æ¥çš„å‡ å‘¨é‡Œï¼Œæˆ‘ä»¬å°†é€æ­¥å¢åŠ å¤æ‚æ€§ï¼Œå¹¶å°†æˆ‘ä»¬çš„æ¦‚ç‡æ¨¡å‹ä¸ç°ä»£ç¡¬ä»¶ï¼ˆä¾‹å¦‚GPUï¼‰ä¸Šçš„æ·±åº¦å­¦ä¹ ç»“åˆèµ·æ¥ã€‚
- en: As usual, the code is available on my [GitHub](https://github.com/luisroque/probabilistic_deep_learning_with_TFP).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å¸¸ï¼Œä»£ç å¯ä»¥åœ¨æˆ‘çš„[GitHub](https://github.com/luisroque/probabilistic_deep_learning_with_TFP)ä¸Šæ‰¾åˆ°ã€‚
- en: Exploratory Data
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¢ç´¢æ€§æ•°æ®
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Our goal is to investigate the use of the Naive Bayes algorithm for classifying
    wine samples based on selected characteristics. To accomplish this, we begin by
    conducting an exploratory analysis of the data. Letâ€™s start by identifying two
    features that effectively separate the target variable and utilize them to predict
    the class of wine.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ç›®æ ‡æ˜¯è°ƒæŸ¥ä½¿ç”¨æœ´ç´ è´å¶æ–¯ç®—æ³•æ ¹æ®é€‰æ‹©çš„ç‰¹å¾å¯¹è‘¡è„é…’æ ·æœ¬è¿›è¡Œåˆ†ç±»ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼Œæˆ‘ä»¬é¦–å…ˆè¿›è¡Œæ•°æ®çš„æ¢ç´¢æ€§åˆ†æã€‚è®©æˆ‘ä»¬å¼€å§‹è¯†åˆ«ä¸¤ä¸ªæœ‰æ•ˆåŒºåˆ†ç›®æ ‡å˜é‡çš„ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨å®ƒä»¬é¢„æµ‹è‘¡è„é…’çš„ç±»åˆ«ã€‚
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/ce21d60bcffb8b0fc6367373d093b396.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce21d60bcffb8b0fc6367373d093b396.png)'
- en: 'Figure 2: Analyzing pairs of features from the wine dataset.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2ï¼šåˆ†æè‘¡è„é…’æ•°æ®é›†ä¸­çš„ç‰¹å¾å¯¹ã€‚
- en: Alcohol and hue are features that separate the classes quite nicely. As such,
    these are the two features that we will be using to build our Naive Bayes model.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: é…’ç²¾åº¦å’Œè‰²è°ƒæ˜¯æœ‰æ•ˆåŒºåˆ†ç±»åˆ«çš„ç‰¹å¾ã€‚å› æ­¤ï¼Œè¿™äº›å°±æ˜¯æˆ‘ä»¬å°†ç”¨äºæ„å»ºæœ´ç´ è´å¶æ–¯æ¨¡å‹çš„ä¸¤ä¸ªç‰¹å¾ã€‚
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/f6e044ba304f9de780daf52a77059f79.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f6e044ba304f9de780daf52a77059f79.png)'
- en: 'Figure 3: Distribution of the target samples by alcohol and hue.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾3ï¼šæŒ‰é…’ç²¾åº¦å’Œè‰²è°ƒåˆ†å¸ƒçš„ç›®æ ‡æ ·æœ¬ã€‚
- en: We can now split our data into a train and a test set.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å¯ä»¥å°†æ•°æ®åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Naive Bayes Classifier
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨
- en: Naive Bayes is a widely used probabilistic machine learning algorithm based
    on Bayesâ€™ theorem. It is particularly useful for classification tasks and is known
    for its simplicity and efficiency. Despite its name, the â€œnaiveâ€ assumption of
    independence among features is not always a limitation and can often yield good
    results in practice. In this article, we comprehensively review the Naive Bayes
    algorithm, its variants, and its implementation from the first principles.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æœ´ç´ è´å¶æ–¯æ˜¯ä¸€ç§å¹¿æ³›ä½¿ç”¨çš„æ¦‚ç‡æœºå™¨å­¦ä¹ ç®—æ³•ï¼ŒåŸºäºè´å¶æ–¯å®šç†ã€‚å®ƒç‰¹åˆ«é€‚ç”¨äºåˆ†ç±»ä»»åŠ¡ï¼Œå¹¶ä»¥å…¶ç®€å•æ€§å’Œé«˜æ•ˆæ€§è‘—ç§°ã€‚å°½ç®¡åå­—ä¸­æœ‰â€œæœ´ç´ â€ï¼Œä½†ç‰¹å¾ä¹‹é—´çš„â€œæœ´ç´ â€ç‹¬ç«‹æ€§å‡è®¾å¹¶ä¸æ€»æ˜¯é™åˆ¶ï¼Œå¹¶ä¸”åœ¨å®è·µä¸­é€šå¸¸èƒ½å–å¾—è‰¯å¥½ç»“æœã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†å…¨é¢å›é¡¾æœ´ç´ è´å¶æ–¯ç®—æ³•åŠå…¶å˜ä½“ï¼Œå¹¶ä»åŸºæœ¬åŸç†ä¸Šå®ç°å®ƒã€‚
- en: We begin by briefly introducing Bayesâ€™ theorem, which is the foundation of the
    Naive Bayes algorithm. Bayesâ€™ theorem states that the probability of a hypothesis
    (H) given some evidence (E) is proportional to the prior probability of the hypothesis
    multiplied by the likelihood of the evidence given the hypothesis. The Naive Bayes
    algorithm uses this theorem to classify new instances by computing the posterior
    probability for each class and then selecting the class with the highest probability.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆç®€è¦ä»‹ç»è´å¶æ–¯å®šç†ï¼Œè¿™æ˜¯æœ´ç´ è´å¶æ–¯ç®—æ³•çš„åŸºç¡€ã€‚è´å¶æ–¯å®šç†è¡¨æ˜ï¼Œåœ¨ç»™å®šä¸€äº›è¯æ®ï¼ˆEï¼‰çš„æƒ…å†µä¸‹ï¼Œå‡è®¾ï¼ˆHï¼‰çš„æ¦‚ç‡ä¸å‡è®¾çš„å…ˆéªŒæ¦‚ç‡ä¹˜ä»¥è¯æ®åœ¨ç»™å®šå‡è®¾ä¸‹çš„ä¼¼ç„¶æ€§æˆæ­£æ¯”ã€‚æœ´ç´ è´å¶æ–¯ç®—æ³•ä½¿ç”¨è¿™ä¸ªå®šç†é€šè¿‡è®¡ç®—æ¯ä¸ªç±»åˆ«çš„åéªŒæ¦‚ç‡æ¥åˆ†ç±»æ–°å®ä¾‹ï¼Œç„¶åé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„ç±»åˆ«ã€‚
- en: The basic principle of the Naive Bayes algorithm is to assume that the features
    of a given instance are conditionally independent given the class label. This
    assumption, also known as the â€œnaiveâ€ assumption, allows for a computationally
    efficient algorithm, as it reduces the number of parameters to be estimated. However,
    it can also lead to a decrease in accuracy when the features are not truly independent.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æœ´ç´ è´å¶æ–¯ç®—æ³•çš„åŸºæœ¬åŸç†æ˜¯å‡è®¾ç»™å®šå®ä¾‹çš„ç‰¹å¾åœ¨ç»™å®šç±»åˆ«æ ‡ç­¾çš„æƒ…å†µä¸‹æ˜¯æ¡ä»¶ç‹¬ç«‹çš„ã€‚è¿™ä¸€å‡è®¾ï¼Œä¹Ÿç§°ä¸ºâ€œæœ´ç´ â€å‡è®¾ï¼Œä½¿å¾—ç®—æ³•åœ¨è®¡ç®—ä¸Šæ›´ä¸ºé«˜æ•ˆï¼Œå› ä¸ºå®ƒå‡å°‘äº†éœ€è¦ä¼°è®¡çš„å‚æ•°æ•°é‡ã€‚ç„¶è€Œï¼Œå½“ç‰¹å¾å®é™…ä¸Šå¹¶éç‹¬ç«‹æ—¶ï¼Œè¿™ä¹Ÿå¯èƒ½å¯¼è‡´å‡†ç¡®ç‡ä¸‹é™ã€‚
- en: There are several variants of the Naive Bayes algorithm, each suited to different
    types of data. For example, the Gaussian Naive Bayes is used for continuous data,
    while the Multinomial Naive Bayes is used for discrete data. The Bernoulli Naive
    Bayes is used for binary data. In this case, we will be focusing on implementing
    Gaussian Naive Bayes.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æœ´ç´ è´å¶æ–¯ç®—æ³•æœ‰å‡ ç§å˜ä½“ï¼Œæ¯ç§éƒ½é€‚ç”¨äºä¸åŒç±»å‹çš„æ•°æ®ã€‚ä¾‹å¦‚ï¼Œé«˜æ–¯æœ´ç´ è´å¶æ–¯ç”¨äºè¿ç»­æ•°æ®ï¼Œè€Œå¤šé¡¹å¼æœ´ç´ è´å¶æ–¯ç”¨äºç¦»æ•£æ•°æ®ã€‚ä¼¯åŠªåˆ©æœ´ç´ è´å¶æ–¯ç”¨äºäºŒå…ƒæ•°æ®ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†ä¸“æ³¨äºå®ç°é«˜æ–¯æœ´ç´ è´å¶æ–¯ã€‚
- en: The Naive Bayes algorithm has been applied to a wide range of domains, including
    natural language processing, computer vision, and bioinformatics. In natural language
    processing, it is commonly used for text classification, such as spam detection
    and sentiment analysis. In computer vision, it is used for image classification
    and object detection. In bioinformatics, it is used for protein classification
    and gene prediction.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æœ´ç´ è´å¶æ–¯ç®—æ³•å·²è¢«åº”ç”¨äºå¹¿æ³›çš„é¢†åŸŸï¼ŒåŒ…æ‹¬è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰å’Œç”Ÿç‰©ä¿¡æ¯å­¦ã€‚åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œå®ƒé€šå¸¸ç”¨äºæ–‡æœ¬åˆ†ç±»ï¼Œä¾‹å¦‚åƒåœ¾é‚®ä»¶æ£€æµ‹å’Œæƒ…æ„Ÿåˆ†æã€‚åœ¨è®¡ç®—æœºè§†è§‰ä¸­ï¼Œå®ƒç”¨äºå›¾åƒåˆ†ç±»å’Œç›®æ ‡æ£€æµ‹ã€‚åœ¨ç”Ÿç‰©ä¿¡æ¯å­¦ä¸­ï¼Œå®ƒç”¨äºè›‹ç™½è´¨åˆ†ç±»å’ŒåŸºå› é¢„æµ‹ã€‚
- en: 'As we stated above the Naive Bayes classifier is based on Bayes rule:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬ä¸Šè¿°æ‰€è¿°ï¼Œæœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨åŸºäºè´å¶æ–¯è§„åˆ™ï¼š
- en: '![](../Images/6e4c9aa0f9a2d31017169c48f72d3a12.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6e4c9aa0f9a2d31017169c48f72d3a12.png)'
- en: where *ğ‘‹* are the input features, *ğ‘Œ* the output classes and *ğ¾* the number
    of classes. More specifically, *ğ‘ƒ*(*ğ‘Œ*) represents the class prior distribution,
    *ğ‘ƒ*(*ğ‘‹*|*ğ‘Œ*) the class-conditional distribution over the inputs and *ğ‘ƒ*(*ğ‘Œ*|*ğ‘‹*)
    the probability of getting a class given the input features.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ *ğ‘‹* æ˜¯è¾“å…¥ç‰¹å¾ï¼Œ*ğ‘Œ* æ˜¯è¾“å‡ºç±»åˆ«ï¼Œ*ğ¾* æ˜¯ç±»åˆ«çš„æ•°é‡ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œ*ğ‘ƒ*(*ğ‘Œ*) è¡¨ç¤ºç±»åˆ«å…ˆéªŒåˆ†å¸ƒï¼Œ*ğ‘ƒ*(*ğ‘‹*|*ğ‘Œ*) æ˜¯è¾“å…¥çš„ç±»åˆ«æ¡ä»¶åˆ†å¸ƒï¼Œè€Œ
    *ğ‘ƒ*(*ğ‘Œ*|*ğ‘‹*) æ˜¯ç»™å®šè¾“å…¥ç‰¹å¾çš„ç±»åˆ«æ¦‚ç‡ã€‚
- en: 'The assumption of independence simplifies the algorithm substantially as we
    do not need to estimate the full joint distribution *ğ‘ƒ*(*ğ‘‹*|*ğ‘Œ*=*ğ‘¦ğ‘˜*). Instead,
    the class-conditional distribution can be written as:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ç‹¬ç«‹æ€§å‡è®¾å¤§å¤§ç®€åŒ–äº†ç®—æ³•ï¼Œå› ä¸ºæˆ‘ä»¬ä¸éœ€è¦ä¼°è®¡å®Œæ•´çš„è”åˆåˆ†å¸ƒ *ğ‘ƒ*(*ğ‘‹*|*ğ‘Œ*=*ğ‘¦ğ‘˜*)ã€‚ç›¸åï¼Œç±»åˆ«æ¡ä»¶åˆ†å¸ƒå¯ä»¥å†™ä½œï¼š
- en: '![](../Images/b26ab4071d2731831e6640f443935d02.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b26ab4071d2731831e6640f443935d02.png)'
- en: where *ğ‘“* denotes the number of features.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ *ğ‘“* è¡¨ç¤ºç‰¹å¾çš„æ•°é‡ã€‚
- en: Prior
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å…ˆéªŒ
- en: In the Naive Bayes algorithm, the class prior distribution is a probability
    distribution that describes the probability of each class in the training data.
    It is a fundamental component of the algorithm, as it is used to compute the posterior
    probability of a class given some evidence.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ´ç´ è´å¶æ–¯ç®—æ³•ä¸­ï¼Œç±»åˆ«å…ˆéªŒåˆ†å¸ƒæ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œæè¿°äº†è®­ç»ƒæ•°æ®ä¸­æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡ã€‚å®ƒæ˜¯ç®—æ³•çš„ä¸€ä¸ªåŸºæœ¬ç»„æˆéƒ¨åˆ†ï¼Œå› ä¸ºå®ƒç”¨äºè®¡ç®—ç»™å®šä¸€äº›è¯æ®çš„ç±»åˆ«åéªŒæ¦‚ç‡ã€‚
- en: The class prior distribution is defined as the probability of a class, given
    the total number of instances in the training data. It is usually denoted as *ğ‘ƒ*(*ğ‘Œ*=*ğ‘¦ğ‘˜*),
    where *ğ‘˜* is the class label. The class prior distribution is estimated using
    the relative frequency of each class in the training data. For example, if there
    are 100 instances in the training data, and 60 of them belong to class A, then
    the class prior for class A is estimated as P(Y=A) = 0.6.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»åˆ«å…ˆéªŒåˆ†å¸ƒå®šä¹‰ä¸ºç»™å®šè®­ç»ƒæ•°æ®ä¸­å®ä¾‹æ€»æ•°çš„ç±»åˆ«æ¦‚ç‡ã€‚å®ƒé€šå¸¸è¡¨ç¤ºä¸º *ğ‘ƒ*(*ğ‘Œ*=*ğ‘¦ğ‘˜*), å…¶ä¸­ *ğ‘˜* æ˜¯ç±»åˆ«æ ‡ç­¾ã€‚ç±»åˆ«å…ˆéªŒåˆ†å¸ƒé€šè¿‡è®­ç»ƒæ•°æ®ä¸­æ¯ä¸ªç±»åˆ«çš„ç›¸å¯¹é¢‘ç‡æ¥ä¼°è®¡ã€‚ä¾‹å¦‚ï¼Œå¦‚æœè®­ç»ƒæ•°æ®ä¸­æœ‰100ä¸ªå®ä¾‹ï¼Œå…¶ä¸­60ä¸ªå±äºç±»åˆ«Aï¼Œé‚£ä¹ˆç±»åˆ«Açš„å…ˆéªŒæ¦‚ç‡ä¼°è®¡ä¸º
    P(Y=A) = 0.6ã€‚
- en: The class prior distribution plays a crucial role in the Naive Bayes algorithm,
    as it is used to compute the posterior probability of a class given some evidence.
    The posterior probability is computed as the product of the class prior and the
    likelihood of the evidence given the class, normalized by the marginal likelihood
    of the evidence. In other words, the class prior distribution acts as a weighting
    factor that adjusts the relative importance of the likelihood function.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»åˆ«å…ˆéªŒåˆ†å¸ƒåœ¨æœ´ç´ è´å¶æ–¯ç®—æ³•ä¸­èµ·ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œå› ä¸ºå®ƒç”¨äºè®¡ç®—åœ¨ç»™å®šä¸€äº›è¯æ®çš„æƒ…å†µä¸‹æŸä¸€ç±»åˆ«çš„åéªŒæ¦‚ç‡ã€‚åéªŒæ¦‚ç‡çš„è®¡ç®—æ˜¯å°†ç±»åˆ«å…ˆéªŒå’Œåœ¨ç»™å®šç±»åˆ«ä¸‹è¯æ®çš„ä¼¼ç„¶åº¦ç›¸ä¹˜ï¼Œå¹¶é€šè¿‡è¯æ®çš„è¾¹é™…ä¼¼ç„¶åº¦è¿›è¡Œå½’ä¸€åŒ–ã€‚æ¢å¥è¯è¯´ï¼Œç±»åˆ«å…ˆéªŒåˆ†å¸ƒä½œä¸ºä¸€ä¸ªæƒé‡å› å­ï¼Œè°ƒæ•´ä¼¼ç„¶å‡½æ•°çš„ç›¸å¯¹é‡è¦æ€§ã€‚
- en: However, if the class prior distribution is estimated from a biased training
    data, it may lead to poor performance of the algorithm, particularly if the test
    data is from a different distribution. This is known as the class imbalance problem,
    and it can be mitigated by using techniques such as oversampling, undersampling,
    or synthetic data generation.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå¦‚æœç±»åˆ«å…ˆéªŒåˆ†å¸ƒæ˜¯ä»æœ‰åçš„è®­ç»ƒæ•°æ®ä¸­ä¼°è®¡çš„ï¼Œå®ƒå¯èƒ½ä¼šå¯¼è‡´ç®—æ³•æ€§èƒ½ä¸ä½³ï¼Œç‰¹åˆ«æ˜¯å½“æµ‹è¯•æ•°æ®æ¥è‡ªä¸åŒçš„åˆ†å¸ƒæ—¶ã€‚è¿™è¢«ç§°ä¸ºç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œå¯ä»¥é€šè¿‡ä½¿ç”¨è¿‡é‡‡æ ·ã€æ¬ é‡‡æ ·æˆ–åˆæˆæ•°æ®ç”Ÿæˆç­‰æŠ€æœ¯æ¥ç¼“è§£ã€‚
- en: 'The class prior distribution is the proportion of data examples belonging to
    the class *ğ‘˜*. We can write it in the form:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»åˆ«å…ˆéªŒåˆ†å¸ƒæ˜¯å±äºç±»åˆ« *ğ‘˜* çš„æ•°æ®ç¤ºä¾‹çš„æ¯”ä¾‹ã€‚æˆ‘ä»¬å¯ä»¥å°†å…¶å†™æˆä»¥ä¸‹å½¢å¼ï¼š
- en: '![](../Images/28dbed717467724e2cbd6b707474dcab.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/28dbed717467724e2cbd6b707474dcab.png)'
- en: where *ğ‘›* denotes the *ğ‘›*-th dataset example, *ğ‘* is the total number of examples
    in the dataset and *ğ›¿* is the Kronecker delta function (returns 1 when the classes
    match and 0 otherwise). It returns a categorical distribution with probabilities
    corresponding to *ğ‘ƒ*(*ğ‘Œ*=*ğ‘¦ğ‘˜*).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼Œ*ğ‘›* è¡¨ç¤ºç¬¬ *ğ‘›* ä¸ªæ•°æ®é›†ç¤ºä¾‹ï¼Œ*ğ‘* æ˜¯æ•°æ®é›†ä¸­ç¤ºä¾‹çš„æ€»æ•°ï¼Œ*ğ›¿* æ˜¯å…‹ç½—å†…å…‹Î´å‡½æ•°ï¼ˆå½“ç±»åˆ«åŒ¹é…æ—¶è¿”å›1ï¼Œå¦åˆ™è¿”å›0ï¼‰ã€‚å®ƒè¿”å›ä¸€ä¸ªä¸ *ğ‘ƒ*(*ğ‘Œ*=*ğ‘¦ğ‘˜*)
    ç›¸å¯¹åº”çš„åˆ†ç±»åˆ†å¸ƒã€‚
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Letâ€™s plot the our prior distribution.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç»˜åˆ¶æˆ‘ä»¬çš„å…ˆéªŒåˆ†å¸ƒã€‚
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/91631c4fe3d458c9e623ebd6cefb3003.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/91631c4fe3d458c9e623ebd6cefb3003.png)'
- en: 'Figure 4: Prior probability for each target class.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4ï¼šæ¯ä¸ªç›®æ ‡ç±»åˆ«çš„å…ˆéªŒæ¦‚ç‡ã€‚
- en: Likelihood
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¼¼ç„¶æ€§
- en: In the Naive Bayes algorithm, the class conditional densities are probability
    distributions that describe the likelihood of each feature given the class label.
    They are used to compute the posterior probability of a class given some evidence,
    and are a fundamental component of the algorithm. The class conditional densities
    are defined as the probability density functions (pdf) of each feature given the
    class label. They are usually denoted as *ğ‘ƒ*(*ğ‘‹ğ‘–*|*ğ‘Œ*=*ğ‘¦ğ‘˜*), where *ğ‘‹ğ‘–* is a feature
    and *ğ‘˜* is the class label. The class conditional densities are estimated from
    the training data using various techniques, depending on the type of data. For
    example, for continuous data, the class conditional density can be estimated using
    the Gaussian distribution, while for discrete data, it can be estimated using
    the multinomial or Bernoulli distributions. As we stated earlier, in our case,
    we have continuous features and so we will explore the Gaussian approach.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ´ç´ è´å¶æ–¯ç®—æ³•ä¸­ï¼Œç±»åˆ«æ¡ä»¶å¯†åº¦æ˜¯æè¿°ç»™å®šç±»åˆ«æ ‡ç­¾ä¸‹æ¯ä¸ªç‰¹å¾çš„ä¼¼ç„¶æ€§çš„æ¦‚ç‡åˆ†å¸ƒã€‚å®ƒä»¬ç”¨äºè®¡ç®—åœ¨ç»™å®šä¸€äº›è¯æ®çš„æƒ…å†µä¸‹æŸä¸€ç±»åˆ«çš„åéªŒæ¦‚ç‡ï¼Œæ˜¯ç®—æ³•çš„ä¸€ä¸ªåŸºæœ¬ç»„æˆéƒ¨åˆ†ã€‚ç±»åˆ«æ¡ä»¶å¯†åº¦å®šä¹‰ä¸ºç»™å®šç±»åˆ«æ ‡ç­¾ä¸‹æ¯ä¸ªç‰¹å¾çš„æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ˆpdfï¼‰ã€‚å®ƒä»¬é€šå¸¸è¡¨ç¤ºä¸º
    *ğ‘ƒ*(*ğ‘‹ğ‘–*|*ğ‘Œ*=*ğ‘¦ğ‘˜*), å…¶ä¸­ *ğ‘‹ğ‘–* æ˜¯ä¸€ä¸ªç‰¹å¾ï¼Œ*ğ‘˜* æ˜¯ç±»åˆ«æ ‡ç­¾ã€‚ç±»åˆ«æ¡ä»¶å¯†åº¦æ˜¯é€šè¿‡å„ç§æŠ€æœ¯ä»è®­ç»ƒæ•°æ®ä¸­ä¼°è®¡å‡ºæ¥çš„ï¼Œå…·ä½“å–å†³äºæ•°æ®çš„ç±»å‹ã€‚ä¾‹å¦‚ï¼Œå¯¹äºè¿ç»­æ•°æ®ï¼Œç±»åˆ«æ¡ä»¶å¯†åº¦å¯ä»¥ä½¿ç”¨é«˜æ–¯åˆ†å¸ƒä¼°è®¡ï¼Œè€Œå¯¹äºç¦»æ•£æ•°æ®ï¼Œå¯ä»¥ä½¿ç”¨å¤šé¡¹å¼åˆ†å¸ƒæˆ–ä¼¯åŠªåˆ©åˆ†å¸ƒè¿›è¡Œä¼°è®¡ã€‚æ­£å¦‚æˆ‘ä»¬ä¹‹å‰æ‰€è¿°ï¼Œåœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬æœ‰è¿ç»­ç‰¹å¾ï¼Œå› æ­¤æˆ‘ä»¬å°†æ¢ç´¢é«˜æ–¯æ–¹æ³•ã€‚
- en: The class conditional densities play a crucial role in the Naive Bayes algorithm,
    as they are used to compute the likelihood of the evidence given the class label.
    This likelihood is computed by evaluating the class conditional densities for
    each feature of the evidence, and then multiplying them together. The class conditional
    densities act as a weighting factor that adjusts the relative importance of each
    feature for the classification task.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»åˆ«æ¡ä»¶å¯†åº¦åœ¨æœ´ç´ è´å¶æ–¯ç®—æ³•ä¸­èµ·ç€å…³é”®ä½œç”¨ï¼Œå› ä¸ºå®ƒä»¬ç”¨äºè®¡ç®—ç»™å®šç±»åˆ«æ ‡ç­¾ä¸‹è¯æ®çš„ä¼¼ç„¶æ€§ã€‚è¿™ä¸€ä¼¼ç„¶æ€§æ˜¯é€šè¿‡è¯„ä¼°è¯æ®çš„æ¯ä¸ªç‰¹å¾çš„ç±»åˆ«æ¡ä»¶å¯†åº¦æ¥è®¡ç®—çš„ï¼Œç„¶åå°†å®ƒä»¬ç›¸ä¹˜ã€‚ç±»åˆ«æ¡ä»¶å¯†åº¦ä½œä¸ºä¸€ä¸ªæƒé‡å› å­ï¼Œè°ƒæ•´æ¯ä¸ªç‰¹å¾åœ¨åˆ†ç±»ä»»åŠ¡ä¸­çš„ç›¸å¯¹é‡è¦æ€§ã€‚
- en: 'It is time to define *ğ‘ƒ*(*ğ‘‹*|*ğ‘Œ*) the class-conditional distribution over the
    inputs. In this case, we use univariate Gaussian distributions (remember the independence
    assumption):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ˜¯å®šä¹‰*ğ‘ƒ*(*ğ‘‹*|*ğ‘Œ*)â€”â€”è¾“å…¥çš„ç±»åˆ«æ¡ä»¶åˆ†å¸ƒçš„æ—¶å€™äº†ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨å•å˜é‡é«˜æ–¯åˆ†å¸ƒï¼ˆè¯·è®°ä½ç‹¬ç«‹æ€§å‡è®¾ï¼‰ï¼š
- en: '![](../Images/8a42de9e92b88c95baceb8575bd21346.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8a42de9e92b88c95baceb8575bd21346.png)'
- en: 'where *ğœ‡ğ‘–ğ‘˜* and *ğœğ‘–ğ‘˜* are the parameters to estimate. Using maximum likelihood,
    the estimates are just the mean and variance of the sample data points for each
    class:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­*ğœ‡ğ‘–ğ‘˜*å’Œ*ğœğ‘–ğ‘˜*æ˜¯éœ€è¦ä¼°è®¡çš„å‚æ•°ã€‚ä½¿ç”¨æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼Œä¼°è®¡å€¼å°±æ˜¯æ¯ä¸ªç±»åˆ«æ ·æœ¬æ•°æ®ç‚¹çš„å‡å€¼å’Œæ–¹å·®ï¼š
- en: '![](../Images/2cce666011e1a0307f40a86c6cf120e7.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2cce666011e1a0307f40a86c6cf120e7.png)'
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The contour plot below shows the class-conditional densities. Notice how the
    contours of each distribution correspond to a Gaussian distribution with diagonal
    covariance matrix, since the model assumes that each feature is independent given
    the class.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹æ–¹çš„ç­‰é«˜çº¿å›¾å±•ç¤ºäº†ç±»åˆ«æ¡ä»¶å¯†åº¦ã€‚è¯·æ³¨æ„æ¯ä¸ªåˆ†å¸ƒçš„ç­‰é«˜çº¿å¦‚ä½•å¯¹åº”äºå…·æœ‰å¯¹è§’åæ–¹å·®çŸ©é˜µçš„é«˜æ–¯åˆ†å¸ƒï¼Œå› ä¸ºæ¨¡å‹å‡è®¾åœ¨ç»™å®šç±»åˆ«çš„æƒ…å†µä¸‹æ¯ä¸ªç‰¹å¾æ˜¯ç‹¬ç«‹çš„ã€‚
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/7b525c510c0f1d96f30e2b6f52f72add.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b525c510c0f1d96f30e2b6f52f72add.png)'
- en: 'Figure 5: Training set with class-conditional density contours.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 5ï¼šå¸¦æœ‰ç±»åˆ«æ¡ä»¶å¯†åº¦ç­‰é«˜çº¿çš„è®­ç»ƒé›†ã€‚
- en: 'After performing the computations described above, the last step of the algorithm
    is to predict the class *ğ‘Œ*Ì‚ for a new data input *ğ‘‹*Ìƒ :=(*ğ‘‹*Ìƒ 1,â€¦,*ğ‘‹*Ìƒ *ğ‘“*).
    This can be done by:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‰§è¡Œä¸Šè¿°è®¡ç®—åï¼Œç®—æ³•çš„æœ€åä¸€æ­¥æ˜¯é¢„æµ‹æ–°çš„æ•°æ®è¾“å…¥*ğ‘‹*Ìƒ :=(*ğ‘‹*Ìƒ 1,â€¦,*ğ‘‹*Ìƒ *ğ‘“*)çš„ç±»åˆ«*ğ‘Œ*Ì‚ã€‚å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å®Œæˆï¼š
- en: '![](../Images/7a121752ff358a9a4e89d09cbecb08f4.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a121752ff358a9a4e89d09cbecb08f4.png)'
- en: '[PRE8]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Results
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“æœ
- en: 'In this article, we applied the Naive Bayes algorithm to classify wine samples
    based on selected characteristics. Specifically, we used two features: hue and
    alcohol, to predict the class of a wine. Our results indicate that the model achieved
    an accuracy of more than 91% for this task.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬åº”ç”¨äº†æœ´ç´ è´å¶æ–¯ç®—æ³•æ¥æ ¹æ®é€‰å®šçš„ç‰¹å¾å¯¹è‘¡è„é…’æ ·æœ¬è¿›è¡Œåˆ†ç±»ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸¤ä¸ªç‰¹å¾ï¼šè‰²è°ƒå’Œé…’ç²¾ï¼Œæ¥é¢„æµ‹è‘¡è„é…’çš„ç±»åˆ«ã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨è¿™é¡¹ä»»åŠ¡ä¸­çš„å‡†ç¡®ç‡è¶…è¿‡äº†91%ã€‚
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: To further analyze the performance of the model, we also plotted the decision
    regions of the model, i.e. the boundaries that separate the different classes.
    The decision regions help to visualize the separation of classes performed by
    the algorithm. As we can see, the model was able to separate the three classes
    in the dataset quite effectively.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¿›ä¸€æ­¥åˆ†ææ¨¡å‹çš„æ€§èƒ½ï¼Œæˆ‘ä»¬è¿˜ç»˜åˆ¶äº†æ¨¡å‹çš„å†³ç­–åŒºåŸŸï¼Œå³åˆ†éš”ä¸åŒç±»åˆ«çš„è¾¹ç•Œã€‚å†³ç­–åŒºåŸŸæœ‰åŠ©äºå¯è§†åŒ–ç®—æ³•æ‰§è¡Œçš„ç±»åˆ«åˆ†ç¦»ã€‚å¦‚æˆ‘ä»¬æ‰€è§ï¼Œæ¨¡å‹èƒ½å¤Ÿç›¸å½“æœ‰æ•ˆåœ°åˆ†éš”æ•°æ®é›†ä¸­çš„ä¸‰ä¸ªç±»åˆ«ã€‚
- en: It is worth noting that the Naive Bayes algorithm makes an assumption of independence
    among features, which might not be true in real-world scenarios. The correlation
    between features can help in improving the accuracy of the model. Therefore, incorporating
    a correlation between model features could help improve its performance. Furthermore,
    other algorithms that allow the correlation between features can be considered
    to improve the results.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæœ´ç´ è´å¶æ–¯ç®—æ³•å‡è®¾ç‰¹å¾ä¹‹é—´æ˜¯ç‹¬ç«‹çš„ï¼Œè¿™åœ¨å®é™…åœºæ™¯ä¸­å¯èƒ½ä¸æˆç«‹ã€‚ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§å¯ä»¥å¸®åŠ©æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚å› æ­¤ï¼Œè€ƒè™‘åœ¨æ¨¡å‹ç‰¹å¾ä¹‹é—´å¼•å…¥ç›¸å…³æ€§å¯èƒ½æœ‰åŠ©äºæå‡æ€§èƒ½ã€‚æ­¤å¤–ï¼Œè¿˜å¯ä»¥è€ƒè™‘å…è®¸ç‰¹å¾ä¹‹é—´ç›¸å…³æ€§çš„å…¶ä»–ç®—æ³•æ¥æ”¹å–„ç»“æœã€‚
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/24462cec3281dc09ff436f5997e1665d.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/24462cec3281dc09ff436f5997e1665d.png)'
- en: 'Figure 6: Training set decision regions.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 6ï¼šè®­ç»ƒé›†å†³ç­–åŒºåŸŸã€‚
- en: Conclusions
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: In this article, we implemented the Naive Bayes algorithm from scratch using
    TensorFlow Probability. We applied it for a classification task using a dataset
    of wine samples. We selected two features, hue and alcohol, to predict the class
    of wine, and achieved an accuracy of more than 91%. We also visualized the decision
    regions of the model, which helped to understand the separation of classes performed
    by the algorithm.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ TensorFlow Probability ä»é›¶å¼€å§‹å®ç°äº†æœ´ç´ è´å¶æ–¯ç®—æ³•ã€‚æˆ‘ä»¬å°†å…¶åº”ç”¨äºä½¿ç”¨è‘¡è„é…’æ ·æœ¬æ•°æ®é›†çš„åˆ†ç±»ä»»åŠ¡ã€‚æˆ‘ä»¬é€‰æ‹©äº†ä¸¤ä¸ªç‰¹å¾ï¼Œè‰²è°ƒå’Œé…’ç²¾ï¼Œæ¥é¢„æµ‹è‘¡è„é…’çš„ç±»åˆ«ï¼Œå¹¶ä¸”è¾¾åˆ°äº†è¶…è¿‡91%çš„å‡†ç¡®ç‡ã€‚æˆ‘ä»¬è¿˜å¯è§†åŒ–äº†æ¨¡å‹çš„å†³ç­–åŒºåŸŸï¼Œè¿™æœ‰åŠ©äºç†è§£ç®—æ³•æ‰§è¡Œçš„ç±»åˆ«åˆ†ç¦»ã€‚
- en: This simple example shows the simplicity and effectiveness of the Naive Bayes
    algorithm for classification tasks. However, the Naive Bayes algorithm makes an
    assumption of independence among features, which may not be true in real-world
    scenarios.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç®€å•çš„ä¾‹å­å±•ç¤ºäº†æœ´ç´ è´å¶æ–¯ç®—æ³•åœ¨åˆ†ç±»ä»»åŠ¡ä¸­çš„ç®€å•æ€§å’Œæœ‰æ•ˆæ€§ã€‚ç„¶è€Œï¼Œæœ´ç´ è´å¶æ–¯ç®—æ³•å‡è®¾ç‰¹å¾ä¹‹é—´æ˜¯ç‹¬ç«‹çš„ï¼Œè¿™åœ¨å®é™…åœºæ™¯ä¸­å¯èƒ½å¹¶ä¸æˆç«‹ã€‚
- en: References and Materials
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®å’Œèµ„æ–™
- en: '[1] â€” [Wine Dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] â€” [è‘¡è„é…’æ•°æ®é›†](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine)'
- en: '[2] â€” [Coursera: Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] â€” [Coursera: æ·±åº¦å­¦ä¹ ä¸“ä¸šè¯¾ç¨‹](https://www.coursera.org/specializations/deep-learning)'
- en: '[3] â€” [Coursera: TensorFlow 2 for Deep Learning](https://www.coursera.org/specializations/tensorflow2-deeplearning)
    Specialization'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] â€” [Coursera: TensorFlow 2 æ·±åº¦å­¦ä¹ ](https://www.coursera.org/specializations/tensorflow2-deeplearning)
    ä¸“ä¸šè¯¾ç¨‹'
- en: '[4] â€” [TensorFlow Probability Guides and Tutorials](https://www.tensorflow.org/probability/overview)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] â€” [TensorFlow æ¦‚ç‡æŒ‡å—ä¸æ•™ç¨‹](https://www.tensorflow.org/probability/overview)'
- en: '[5] â€” [TensorFlow Probability Posts in TensorFlow Blog](https://blog.tensorflow.org/search?label=TensorFlow+Probability&max-results=20)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] â€” [TensorFlow åšå®¢ä¸­çš„ TensorFlow æ¦‚ç‡å¸–å­](https://blog.tensorflow.org/search?label=TensorFlow+Probability&max-results=20)'
