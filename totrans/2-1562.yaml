- en: Naive Bayes from scratch with TensorFlow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零开始的朴素贝叶斯与 TensorFlow
- en: 原文：[https://towardsdatascience.com/naive-bayes-from-scratch-with-tensorflow-6e04c5a25947](https://towardsdatascience.com/naive-bayes-from-scratch-with-tensorflow-6e04c5a25947)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/naive-bayes-from-scratch-with-tensorflow-6e04c5a25947](https://towardsdatascience.com/naive-bayes-from-scratch-with-tensorflow-6e04c5a25947)
- en: Probabilistic Deep Learning
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概率深度学习
- en: '[](https://medium.com/@luisroque?source=post_page-----6e04c5a25947--------------------------------)[![Luís
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----6e04c5a25947--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6e04c5a25947--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6e04c5a25947--------------------------------)
    [Luís Roque](https://medium.com/@luisroque?source=post_page-----6e04c5a25947--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@luisroque?source=post_page-----6e04c5a25947--------------------------------)[![路易斯·罗克](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----6e04c5a25947--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6e04c5a25947--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6e04c5a25947--------------------------------)
    [路易斯·罗克](https://medium.com/@luisroque?source=post_page-----6e04c5a25947--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6e04c5a25947--------------------------------)
    ·10 min read·Jan 18, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6e04c5a25947--------------------------------)
    ·10分钟阅读·2023年1月18日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Introduction
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: This article belongs to the series “Probabilistic Deep Learning”. This weekly
    series covers probabilistic approaches to deep learning. The main goal is to extend
    deep learning models to quantify uncertainty, i.e., know what they do not know.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本文属于“概率深度学习”系列。该系列每周涵盖深度学习的概率方法。主要目标是扩展深度学习模型以量化不确定性，即了解它们不知道的内容。
- en: In this article, we present an examination of the Naive Bayes algorithm for
    classification tasks using a dataset of wine samples. The Naive Bayes algorithm
    is a probabilistic machine learning technique based on Bayes’ theorem, which makes
    assumptions about the independence of features given the target label. To facilitate
    visualization of the separation of classes, we limit the model to only two features.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们对使用葡萄酒样本数据集的朴素贝叶斯分类算法进行了考察。朴素贝叶斯算法是一种基于贝叶斯定理的概率机器学习技术，假设在给定目标标签的情况下特征之间是独立的。为了便于可视化类别的分离，我们将模型限制为仅使用两个特征。
- en: Our objective is to classify wine samples based on selected characteristics.
    To accomplish this, we begin by exploring the data and selecting features that
    effectively separate the classes. We then proceed to construct class prior distributions
    and class-conditional densities, leading to the ability to predict the class with
    the highest probability. The dataset used in this study consists of various features
    of wines, such as hue, alcohol, flavonoids, and a target class, and is obtained
    from the scikit-learn library [1].
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是基于选定的特征对葡萄酒样本进行分类。为实现这一目标，我们首先探索数据并选择有效区分各类别的特征。然后，我们构建类别先验分布和类别条件密度，从而能够预测具有最高概率的类别。该研究使用的数据集包含葡萄酒的各种特征，如色调、酒精、类黄酮以及一个目标类别，并且数据集来自
    scikit-learn 库 [1]。
- en: 'Articles published so far:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止发表的文章：
- en: '[Gentle Introduction to TensorFlow Probability: Distribution Objects](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-distribution-objects-1bb6165abee1)'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[TensorFlow Probability 的温和介绍：分布对象](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-distribution-objects-1bb6165abee1)'
- en: '[Gentle Introduction to TensorFlow Probability: Trainable Parameters](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-trainable-parameters-5098ea4fed15)'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[TensorFlow Probability 的温和介绍：可训练参数](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-trainable-parameters-5098ea4fed15)'
- en: '[Maximum Likelihood Estimation from scratch in TensorFlow Probability](/maximum-likelihood-estimation-from-scratch-in-tensorflow-probability-2fc0eefdbfc2)'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[从零开始的最大似然估计，使用 TensorFlow Probability](/maximum-likelihood-estimation-from-scratch-in-tensorflow-probability-2fc0eefdbfc2)'
- en: '[Probabilistic Linear Regression from scratch in TensorFlow](/probabilistic-linear-regression-from-scratch-in-tensorflow-2eb633fffc00)'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[从零开始的概率线性回归，使用 TensorFlow](/probabilistic-linear-regression-from-scratch-in-tensorflow-2eb633fffc00)'
- en: '[Probabilistic vs. Deterministic Regression with Tensorflow](https://medium.com/towards-data-science/probabilistic-vs-deterministic-regression-with-tensorflow-85ef791beeef)'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Tensorflow中的概率性与确定性回归](https://medium.com/towards-data-science/probabilistic-vs-deterministic-regression-with-tensorflow-85ef791beeef)'
- en: '[Frequentist vs. Bayesian Statistics with Tensorflow](https://medium.com/towards-data-science/frequentist-vs-bayesian-statistics-with-tensorflow-fbba2c6c9ae5)'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[频率学派与贝叶斯统计在Tensorflow中的应用](https://medium.com/towards-data-science/frequentist-vs-bayesian-statistics-with-tensorflow-fbba2c6c9ae5)'
- en: '[Deterministic vs. Probabilistic Deep Learning](/deterministic-vs-probabilistic-deep-learning-5325769dc758)'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[确定性与概率性深度学习](/deterministic-vs-probabilistic-deep-learning-5325769dc758)'
- en: Naive Bayes from scratch with TensorFlow
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从头开始使用TensorFlow实现朴素贝叶斯
- en: '![](../Images/32c7b8ce4cb661b5ede5f8501c912b4e.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/32c7b8ce4cb661b5ede5f8501c912b4e.png)'
- en: 'Figure 1: The motto for today: be naive when it comes to wine classification?
    ([source](https://unsplash.com/photos/3uJt73tr4hI))'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：今天的格言：在葡萄酒分类方面要保持**天真**？ ([source](https://unsplash.com/photos/3uJt73tr4hI))
- en: We develop our models using TensorFlow and TensorFlow Probability. TensorFlow
    Probability is a Python library built on top of TensorFlow. We will start with
    the basic objects we can find in TensorFlow Probability and understand how we
    can manipulate them. We will increase complexity incrementally over the following
    weeks and combine our probabilistic models with deep learning on modern hardware
    (e.g. GPU).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用TensorFlow和TensorFlow Probability开发我们的模型。TensorFlow Probability是一个建立在TensorFlow之上的Python库。我们将从TensorFlow
    Probability中的基本对象开始，理解如何操作它们。接下来的几周里，我们将逐步增加复杂性，并将我们的概率模型与现代硬件（例如GPU）上的深度学习结合起来。
- en: As usual, the code is available on my [GitHub](https://github.com/luisroque/probabilistic_deep_learning_with_TFP).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如常，代码可以在我的[GitHub](https://github.com/luisroque/probabilistic_deep_learning_with_TFP)上找到。
- en: Exploratory Data
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索性数据
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Our goal is to investigate the use of the Naive Bayes algorithm for classifying
    wine samples based on selected characteristics. To accomplish this, we begin by
    conducting an exploratory analysis of the data. Let’s start by identifying two
    features that effectively separate the target variable and utilize them to predict
    the class of wine.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是调查使用朴素贝叶斯算法根据选择的特征对葡萄酒样本进行分类。为了实现这一目标，我们首先进行数据的探索性分析。让我们开始识别两个有效区分目标变量的特征，并利用它们预测葡萄酒的类别。
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/ce21d60bcffb8b0fc6367373d093b396.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce21d60bcffb8b0fc6367373d093b396.png)'
- en: 'Figure 2: Analyzing pairs of features from the wine dataset.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：分析葡萄酒数据集中的特征对。
- en: Alcohol and hue are features that separate the classes quite nicely. As such,
    these are the two features that we will be using to build our Naive Bayes model.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 酒精度和色调是有效区分类别的特征。因此，这些就是我们将用于构建朴素贝叶斯模型的两个特征。
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/f6e044ba304f9de780daf52a77059f79.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f6e044ba304f9de780daf52a77059f79.png)'
- en: 'Figure 3: Distribution of the target samples by alcohol and hue.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：按酒精度和色调分布的目标样本。
- en: We can now split our data into a train and a test set.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将数据分成训练集和测试集。
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Naive Bayes Classifier
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器
- en: Naive Bayes is a widely used probabilistic machine learning algorithm based
    on Bayes’ theorem. It is particularly useful for classification tasks and is known
    for its simplicity and efficiency. Despite its name, the “naive” assumption of
    independence among features is not always a limitation and can often yield good
    results in practice. In this article, we comprehensively review the Naive Bayes
    algorithm, its variants, and its implementation from the first principles.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯是一种广泛使用的概率机器学习算法，基于贝叶斯定理。它特别适用于分类任务，并以其简单性和高效性著称。尽管名字中有“朴素”，但特征之间的“朴素”独立性假设并不总是限制，并且在实践中通常能取得良好结果。在这篇文章中，我们将全面回顾朴素贝叶斯算法及其变体，并从基本原理上实现它。
- en: We begin by briefly introducing Bayes’ theorem, which is the foundation of the
    Naive Bayes algorithm. Bayes’ theorem states that the probability of a hypothesis
    (H) given some evidence (E) is proportional to the prior probability of the hypothesis
    multiplied by the likelihood of the evidence given the hypothesis. The Naive Bayes
    algorithm uses this theorem to classify new instances by computing the posterior
    probability for each class and then selecting the class with the highest probability.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先简要介绍贝叶斯定理，这是朴素贝叶斯算法的基础。贝叶斯定理表明，在给定一些证据（E）的情况下，假设（H）的概率与假设的先验概率乘以证据在给定假设下的似然性成正比。朴素贝叶斯算法使用这个定理通过计算每个类别的后验概率来分类新实例，然后选择概率最高的类别。
- en: The basic principle of the Naive Bayes algorithm is to assume that the features
    of a given instance are conditionally independent given the class label. This
    assumption, also known as the “naive” assumption, allows for a computationally
    efficient algorithm, as it reduces the number of parameters to be estimated. However,
    it can also lead to a decrease in accuracy when the features are not truly independent.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯算法的基本原理是假设给定实例的特征在给定类别标签的情况下是条件独立的。这一假设，也称为“朴素”假设，使得算法在计算上更为高效，因为它减少了需要估计的参数数量。然而，当特征实际上并非独立时，这也可能导致准确率下降。
- en: There are several variants of the Naive Bayes algorithm, each suited to different
    types of data. For example, the Gaussian Naive Bayes is used for continuous data,
    while the Multinomial Naive Bayes is used for discrete data. The Bernoulli Naive
    Bayes is used for binary data. In this case, we will be focusing on implementing
    Gaussian Naive Bayes.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯算法有几种变体，每种都适用于不同类型的数据。例如，高斯朴素贝叶斯用于连续数据，而多项式朴素贝叶斯用于离散数据。伯努利朴素贝叶斯用于二元数据。在这种情况下，我们将专注于实现高斯朴素贝叶斯。
- en: The Naive Bayes algorithm has been applied to a wide range of domains, including
    natural language processing, computer vision, and bioinformatics. In natural language
    processing, it is commonly used for text classification, such as spam detection
    and sentiment analysis. In computer vision, it is used for image classification
    and object detection. In bioinformatics, it is used for protein classification
    and gene prediction.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯算法已被应用于广泛的领域，包括自然语言处理、计算机视觉和生物信息学。在自然语言处理领域，它通常用于文本分类，例如垃圾邮件检测和情感分析。在计算机视觉中，它用于图像分类和目标检测。在生物信息学中，它用于蛋白质分类和基因预测。
- en: 'As we stated above the Naive Bayes classifier is based on Bayes rule:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们上述所述，朴素贝叶斯分类器基于贝叶斯规则：
- en: '![](../Images/6e4c9aa0f9a2d31017169c48f72d3a12.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6e4c9aa0f9a2d31017169c48f72d3a12.png)'
- en: where *𝑋* are the input features, *𝑌* the output classes and *𝐾* the number
    of classes. More specifically, *𝑃*(*𝑌*) represents the class prior distribution,
    *𝑃*(*𝑋*|*𝑌*) the class-conditional distribution over the inputs and *𝑃*(*𝑌*|*𝑋*)
    the probability of getting a class given the input features.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *𝑋* 是输入特征，*𝑌* 是输出类别，*𝐾* 是类别的数量。更具体地说，*𝑃*(*𝑌*) 表示类别先验分布，*𝑃*(*𝑋*|*𝑌*) 是输入的类别条件分布，而
    *𝑃*(*𝑌*|*𝑋*) 是给定输入特征的类别概率。
- en: 'The assumption of independence simplifies the algorithm substantially as we
    do not need to estimate the full joint distribution *𝑃*(*𝑋*|*𝑌*=*𝑦𝑘*). Instead,
    the class-conditional distribution can be written as:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 独立性假设大大简化了算法，因为我们不需要估计完整的联合分布 *𝑃*(*𝑋*|*𝑌*=*𝑦𝑘*)。相反，类别条件分布可以写作：
- en: '![](../Images/b26ab4071d2731831e6640f443935d02.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b26ab4071d2731831e6640f443935d02.png)'
- en: where *𝑓* denotes the number of features.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *𝑓* 表示特征的数量。
- en: Prior
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 先验
- en: In the Naive Bayes algorithm, the class prior distribution is a probability
    distribution that describes the probability of each class in the training data.
    It is a fundamental component of the algorithm, as it is used to compute the posterior
    probability of a class given some evidence.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在朴素贝叶斯算法中，类别先验分布是一个概率分布，描述了训练数据中每个类别的概率。它是算法的一个基本组成部分，因为它用于计算给定一些证据的类别后验概率。
- en: The class prior distribution is defined as the probability of a class, given
    the total number of instances in the training data. It is usually denoted as *𝑃*(*𝑌*=*𝑦𝑘*),
    where *𝑘* is the class label. The class prior distribution is estimated using
    the relative frequency of each class in the training data. For example, if there
    are 100 instances in the training data, and 60 of them belong to class A, then
    the class prior for class A is estimated as P(Y=A) = 0.6.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 类别先验分布定义为给定训练数据中实例总数的类别概率。它通常表示为 *𝑃*(*𝑌*=*𝑦𝑘*), 其中 *𝑘* 是类别标签。类别先验分布通过训练数据中每个类别的相对频率来估计。例如，如果训练数据中有100个实例，其中60个属于类别A，那么类别A的先验概率估计为
    P(Y=A) = 0.6。
- en: The class prior distribution plays a crucial role in the Naive Bayes algorithm,
    as it is used to compute the posterior probability of a class given some evidence.
    The posterior probability is computed as the product of the class prior and the
    likelihood of the evidence given the class, normalized by the marginal likelihood
    of the evidence. In other words, the class prior distribution acts as a weighting
    factor that adjusts the relative importance of the likelihood function.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 类别先验分布在朴素贝叶斯算法中起着至关重要的作用，因为它用于计算在给定一些证据的情况下某一类别的后验概率。后验概率的计算是将类别先验和在给定类别下证据的似然度相乘，并通过证据的边际似然度进行归一化。换句话说，类别先验分布作为一个权重因子，调整似然函数的相对重要性。
- en: However, if the class prior distribution is estimated from a biased training
    data, it may lead to poor performance of the algorithm, particularly if the test
    data is from a different distribution. This is known as the class imbalance problem,
    and it can be mitigated by using techniques such as oversampling, undersampling,
    or synthetic data generation.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果类别先验分布是从有偏的训练数据中估计的，它可能会导致算法性能不佳，特别是当测试数据来自不同的分布时。这被称为类别不平衡问题，可以通过使用过采样、欠采样或合成数据生成等技术来缓解。
- en: 'The class prior distribution is the proportion of data examples belonging to
    the class *𝑘*. We can write it in the form:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 类别先验分布是属于类别 *𝑘* 的数据示例的比例。我们可以将其写成以下形式：
- en: '![](../Images/28dbed717467724e2cbd6b707474dcab.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/28dbed717467724e2cbd6b707474dcab.png)'
- en: where *𝑛* denotes the *𝑛*-th dataset example, *𝑁* is the total number of examples
    in the dataset and *𝛿* is the Kronecker delta function (returns 1 when the classes
    match and 0 otherwise). It returns a categorical distribution with probabilities
    corresponding to *𝑃*(*𝑌*=*𝑦𝑘*).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*𝑛* 表示第 *𝑛* 个数据集示例，*𝑁* 是数据集中示例的总数，*𝛿* 是克罗内克δ函数（当类别匹配时返回1，否则返回0）。它返回一个与 *𝑃*(*𝑌*=*𝑦𝑘*)
    相对应的分类分布。
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Let’s plot the our prior distribution.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制我们的先验分布。
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/91631c4fe3d458c9e623ebd6cefb3003.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/91631c4fe3d458c9e623ebd6cefb3003.png)'
- en: 'Figure 4: Prior probability for each target class.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：每个目标类别的先验概率。
- en: Likelihood
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 似然性
- en: In the Naive Bayes algorithm, the class conditional densities are probability
    distributions that describe the likelihood of each feature given the class label.
    They are used to compute the posterior probability of a class given some evidence,
    and are a fundamental component of the algorithm. The class conditional densities
    are defined as the probability density functions (pdf) of each feature given the
    class label. They are usually denoted as *𝑃*(*𝑋𝑖*|*𝑌*=*𝑦𝑘*), where *𝑋𝑖* is a feature
    and *𝑘* is the class label. The class conditional densities are estimated from
    the training data using various techniques, depending on the type of data. For
    example, for continuous data, the class conditional density can be estimated using
    the Gaussian distribution, while for discrete data, it can be estimated using
    the multinomial or Bernoulli distributions. As we stated earlier, in our case,
    we have continuous features and so we will explore the Gaussian approach.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在朴素贝叶斯算法中，类别条件密度是描述给定类别标签下每个特征的似然性的概率分布。它们用于计算在给定一些证据的情况下某一类别的后验概率，是算法的一个基本组成部分。类别条件密度定义为给定类别标签下每个特征的概率密度函数（pdf）。它们通常表示为
    *𝑃*(*𝑋𝑖*|*𝑌*=*𝑦𝑘*), 其中 *𝑋𝑖* 是一个特征，*𝑘* 是类别标签。类别条件密度是通过各种技术从训练数据中估计出来的，具体取决于数据的类型。例如，对于连续数据，类别条件密度可以使用高斯分布估计，而对于离散数据，可以使用多项式分布或伯努利分布进行估计。正如我们之前所述，在我们的案例中，我们有连续特征，因此我们将探索高斯方法。
- en: The class conditional densities play a crucial role in the Naive Bayes algorithm,
    as they are used to compute the likelihood of the evidence given the class label.
    This likelihood is computed by evaluating the class conditional densities for
    each feature of the evidence, and then multiplying them together. The class conditional
    densities act as a weighting factor that adjusts the relative importance of each
    feature for the classification task.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 类别条件密度在朴素贝叶斯算法中起着关键作用，因为它们用于计算给定类别标签下证据的似然性。这一似然性是通过评估证据的每个特征的类别条件密度来计算的，然后将它们相乘。类别条件密度作为一个权重因子，调整每个特征在分类任务中的相对重要性。
- en: 'It is time to define *𝑃*(*𝑋*|*𝑌*) the class-conditional distribution over the
    inputs. In this case, we use univariate Gaussian distributions (remember the independence
    assumption):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是定义*𝑃*(*𝑋*|*𝑌*)——输入的类别条件分布的时候了。在这种情况下，我们使用单变量高斯分布（请记住独立性假设）：
- en: '![](../Images/8a42de9e92b88c95baceb8575bd21346.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8a42de9e92b88c95baceb8575bd21346.png)'
- en: 'where *𝜇𝑖𝑘* and *𝜎𝑖𝑘* are the parameters to estimate. Using maximum likelihood,
    the estimates are just the mean and variance of the sample data points for each
    class:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*𝜇𝑖𝑘*和*𝜎𝑖𝑘*是需要估计的参数。使用最大似然估计，估计值就是每个类别样本数据点的均值和方差：
- en: '![](../Images/2cce666011e1a0307f40a86c6cf120e7.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2cce666011e1a0307f40a86c6cf120e7.png)'
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The contour plot below shows the class-conditional densities. Notice how the
    contours of each distribution correspond to a Gaussian distribution with diagonal
    covariance matrix, since the model assumes that each feature is independent given
    the class.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 下方的等高线图展示了类别条件密度。请注意每个分布的等高线如何对应于具有对角协方差矩阵的高斯分布，因为模型假设在给定类别的情况下每个特征是独立的。
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/7b525c510c0f1d96f30e2b6f52f72add.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b525c510c0f1d96f30e2b6f52f72add.png)'
- en: 'Figure 5: Training set with class-conditional density contours.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：带有类别条件密度等高线的训练集。
- en: 'After performing the computations described above, the last step of the algorithm
    is to predict the class *𝑌*̂ for a new data input *𝑋*̃ :=(*𝑋*̃ 1,…,*𝑋*̃ *𝑓*).
    This can be done by:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行上述计算后，算法的最后一步是预测新的数据输入*𝑋*̃ :=(*𝑋*̃ 1,…,*𝑋*̃ *𝑓*)的类别*𝑌*̂。可以通过以下方式完成：
- en: '![](../Images/7a121752ff358a9a4e89d09cbecb08f4.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a121752ff358a9a4e89d09cbecb08f4.png)'
- en: '[PRE8]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Results
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: 'In this article, we applied the Naive Bayes algorithm to classify wine samples
    based on selected characteristics. Specifically, we used two features: hue and
    alcohol, to predict the class of a wine. Our results indicate that the model achieved
    an accuracy of more than 91% for this task.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们应用了朴素贝叶斯算法来根据选定的特征对葡萄酒样本进行分类。具体来说，我们使用了两个特征：色调和酒精，来预测葡萄酒的类别。我们的结果表明，该模型在这项任务中的准确率超过了91%。
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: To further analyze the performance of the model, we also plotted the decision
    regions of the model, i.e. the boundaries that separate the different classes.
    The decision regions help to visualize the separation of classes performed by
    the algorithm. As we can see, the model was able to separate the three classes
    in the dataset quite effectively.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步分析模型的性能，我们还绘制了模型的决策区域，即分隔不同类别的边界。决策区域有助于可视化算法执行的类别分离。如我们所见，模型能够相当有效地分隔数据集中的三个类别。
- en: It is worth noting that the Naive Bayes algorithm makes an assumption of independence
    among features, which might not be true in real-world scenarios. The correlation
    between features can help in improving the accuracy of the model. Therefore, incorporating
    a correlation between model features could help improve its performance. Furthermore,
    other algorithms that allow the correlation between features can be considered
    to improve the results.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，朴素贝叶斯算法假设特征之间是独立的，这在实际场景中可能不成立。特征之间的相关性可以帮助提高模型的准确性。因此，考虑在模型特征之间引入相关性可能有助于提升性能。此外，还可以考虑允许特征之间相关性的其他算法来改善结果。
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/24462cec3281dc09ff436f5997e1665d.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/24462cec3281dc09ff436f5997e1665d.png)'
- en: 'Figure 6: Training set decision regions.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：训练集决策区域。
- en: Conclusions
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we implemented the Naive Bayes algorithm from scratch using
    TensorFlow Probability. We applied it for a classification task using a dataset
    of wine samples. We selected two features, hue and alcohol, to predict the class
    of wine, and achieved an accuracy of more than 91%. We also visualized the decision
    regions of the model, which helped to understand the separation of classes performed
    by the algorithm.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们使用 TensorFlow Probability 从零开始实现了朴素贝叶斯算法。我们将其应用于使用葡萄酒样本数据集的分类任务。我们选择了两个特征，色调和酒精，来预测葡萄酒的类别，并且达到了超过91%的准确率。我们还可视化了模型的决策区域，这有助于理解算法执行的类别分离。
- en: This simple example shows the simplicity and effectiveness of the Naive Bayes
    algorithm for classification tasks. However, the Naive Bayes algorithm makes an
    assumption of independence among features, which may not be true in real-world
    scenarios.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的例子展示了朴素贝叶斯算法在分类任务中的简单性和有效性。然而，朴素贝叶斯算法假设特征之间是独立的，这在实际场景中可能并不成立。
- en: References and Materials
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献和资料
- en: '[1] — [Wine Dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] — [葡萄酒数据集](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine)'
- en: '[2] — [Coursera: Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] — [Coursera: 深度学习专业课程](https://www.coursera.org/specializations/deep-learning)'
- en: '[3] — [Coursera: TensorFlow 2 for Deep Learning](https://www.coursera.org/specializations/tensorflow2-deeplearning)
    Specialization'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] — [Coursera: TensorFlow 2 深度学习](https://www.coursera.org/specializations/tensorflow2-deeplearning)
    专业课程'
- en: '[4] — [TensorFlow Probability Guides and Tutorials](https://www.tensorflow.org/probability/overview)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] — [TensorFlow 概率指南与教程](https://www.tensorflow.org/probability/overview)'
- en: '[5] — [TensorFlow Probability Posts in TensorFlow Blog](https://blog.tensorflow.org/search?label=TensorFlow+Probability&max-results=20)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] — [TensorFlow 博客中的 TensorFlow 概率帖子](https://blog.tensorflow.org/search?label=TensorFlow+Probability&max-results=20)'
