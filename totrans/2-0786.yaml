- en: Effectively Optimize Your Regression Model with Bayesian Hyperparameter Tuning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有效优化你的回归模型与贝叶斯超参数调优
- en: 原文：[https://towardsdatascience.com/effectively-optimize-your-regression-model-with-bayesian-hyperparameter-tuning-819c19f5dab3](https://towardsdatascience.com/effectively-optimize-your-regression-model-with-bayesian-hyperparameter-tuning-819c19f5dab3)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/effectively-optimize-your-regression-model-with-bayesian-hyperparameter-tuning-819c19f5dab3](https://towardsdatascience.com/effectively-optimize-your-regression-model-with-bayesian-hyperparameter-tuning-819c19f5dab3)
- en: Learn to effectively optimize hyperparameters, and prevent creating overtrained
    models for XGBoost, CatBoost, and LightBoost
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习如何有效优化超参数，防止为 XGBoost、CatBoost 和 LightBoost 创建过拟合模型
- en: '[](https://erdogant.medium.com/?source=post_page-----819c19f5dab3--------------------------------)[![Erdogan
    Taskesen](../Images/8e62cdae0502687710d8ae4bbcd8966e.png)](https://erdogant.medium.com/?source=post_page-----819c19f5dab3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----819c19f5dab3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----819c19f5dab3--------------------------------)
    [Erdogan Taskesen](https://erdogant.medium.com/?source=post_page-----819c19f5dab3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://erdogant.medium.com/?source=post_page-----819c19f5dab3--------------------------------)[![Erdogan
    Taskesen](../Images/8e62cdae0502687710d8ae4bbcd8966e.png)](https://erdogant.medium.com/?source=post_page-----819c19f5dab3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----819c19f5dab3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----819c19f5dab3--------------------------------)
    [Erdogan Taskesen](https://erdogant.medium.com/?source=post_page-----819c19f5dab3--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----819c19f5dab3--------------------------------)
    ·15 min read·Jul 17, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----819c19f5dab3--------------------------------)
    ·15分钟阅读·2023年7月17日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/85d161655226ea3f51d0055d58fa9bfe.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/85d161655226ea3f51d0055d58fa9bfe.png)'
- en: Photo by [Alexey Ruban](https://unsplash.com/@intelligenciya?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/tune?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Alexey Ruban](https://unsplash.com/@intelligenciya?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)提供，来源于[Unsplash](https://unsplash.com/s/photos/tune?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: Gradient boosting techniques such as *XGBoost, CatBoost, and LightBoost* has
    gained much popularity in recent years for both classification and regression
    tasks. An important part of the process is the tuning of hyperparameters to gain
    the best model performance. The key is to optimize the hyperparameter search space
    together with finding a model that can generalize on new unseen data. *In this
    blog, I will demonstrate 1\. how to learn a boosted decision tree* ***regression***
    *model with optimized hyperparameters using Bayesian optimization, 2\. how to
    select a model that can generalize (and is not overtrained), 3\. how to interpret
    and visually explain the optimized hyperparameter space together with the model
    performance accuracy. The* [*HGBoost*](https://erdogant.github.io/hgboost/) *library
    is ideal for this task which performs, among others a double loop cross-validation
    to protect against overtraining.*
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升技术如 *XGBoost、CatBoost 和 LightBoost* 在最近几年获得了很大的普及，适用于分类和回归任务。过程中的一个重要部分是超参数的调优，以获得最佳的模型性能。关键在于优化超参数搜索空间并找到一个能够在新未见数据上泛化的模型。*在这篇博客中，我将演示
    1\. 如何使用贝叶斯优化学习一个经过优化超参数的提升决策树* ***回归*** *模型，2\. 如何选择一个能够泛化（而非过拟合）的模型，3\. 如何解释并可视化优化后的超参数空间以及模型性能准确性。*
    [*HGBoost*](https://erdogant.github.io/hgboost/) *库非常适合这个任务，其中包括双重交叉验证来防止过拟合。*
- en: A brief introduction.
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简要介绍。
- en: Gradient boosting algorithms such as Extreme Gradient Boosting (*XGboost*),
    Light Gradient Boosting (*Lightboost*), and *CatBoost* are powerful ensemble machine
    learning algorithms for predictive modeling (*classification* and *regression
    tasks)* that can be applied to data sets in the form of *tabular*, *continuous,
    and mixed* forms *[1,2,3 ]*. ***Here I will focus on the regression task.*** In
    the following sections, we will train a boosted decision tree model using a double-loop
    cross-validation loop. We will carefully split the data set, set up the search
    space, and perform Bayesian optimization using the library [*Hyperopt*](http://hyperopt.github.io/hyperopt/).
    After training the model, we can deeper interpret the results by creating insightful
    plots.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升算法如极端梯度提升（*XGboost*）、轻量级梯度提升（*Lightboost*）和*CatBoost* 是强大的集成机器学习算法，用于预测建模（*分类*
    和 *回归任务*），适用于*表格*、*连续*和*混合*形式的数据集 *[1,2,3 ]*。***在这里，我将专注于回归任务。*** 在接下来的部分中，我们将使用双重交叉验证循环训练一个提升决策树模型。我们将仔细拆分数据集，设置搜索空间，并使用库[*Hyperopt*](http://hyperopt.github.io/hyperopt/)进行贝叶斯优化。训练模型后，我们可以通过创建有洞察力的图表来深入解读结果。
- en: 'If you need more background or are not entirely familiar with these concepts,
    I recommend reading this blog:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要更多背景知识或对这些概念不太熟悉，我建议阅读这篇博客：
- en: '[](/a-guide-to-find-the-best-boosting-model-using-bayesian-hyperparameter-tuning-but-without-c98b6a1ecac8?source=post_page-----819c19f5dab3--------------------------------)
    [## A Guide to Find the Best Boosting Model using Bayesian Hyperparameter Tuning
    but without…'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 如何通过贝叶斯超参数调整找到最佳的提升模型，但没有…](/a-guide-to-find-the-best-boosting-model-using-bayesian-hyperparameter-tuning-but-without-c98b6a1ecac8?source=post_page-----819c19f5dab3--------------------------------)'
- en: Boosted decision tree algorithms may outperform other models but overfitting
    is a real danger. Fit your model using the…
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提升决策树算法可能优于其他模型，但过拟合是一个真实的危险。使用…
- en: towardsdatascience.com](/a-guide-to-find-the-best-boosting-model-using-bayesian-hyperparameter-tuning-but-without-c98b6a1ecac8?source=post_page-----819c19f5dab3--------------------------------)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 如何通过贝叶斯超参数调整找到最佳的提升模型，但没有…](/a-guide-to-find-the-best-boosting-model-using-bayesian-hyperparameter-tuning-but-without-c98b6a1ecac8?source=post_page-----819c19f5dab3--------------------------------)'
- en: 'If you need a hands-on guide for the **classification task**, I recommend reading
    this blog:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要**分类任务**的实践指南，我推荐阅读这篇博客：
- en: '[](https://erdogant.medium.com/hands-on-guide-for-hyperparameter-tuning-with-bayesian-optimization-for-classification-models-2002224bfa3d?source=post_page-----819c19f5dab3--------------------------------)
    [## Hands-on Guide for Hyperparameter Tuning with Bayesian Optimization for Classification
    Models.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 分类模型的贝叶斯优化超参数调整实用指南。](https://erdogant.medium.com/hands-on-guide-for-hyperparameter-tuning-with-bayesian-optimization-for-classification-models-2002224bfa3d?source=post_page-----819c19f5dab3--------------------------------)'
- en: Learn how to split the data, optimize hyperparameters, prevent overtraining,
    select the best model, and create…
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习如何拆分数据，优化超参数，防止过拟合，选择最佳模型，并创建…
- en: erdogant.medium.com](https://erdogant.medium.com/hands-on-guide-for-hyperparameter-tuning-with-bayesian-optimization-for-classification-models-2002224bfa3d?source=post_page-----819c19f5dab3--------------------------------)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[erdogant.medium.com](https://erdogant.medium.com/hands-on-guide-for-hyperparameter-tuning-with-bayesian-optimization-for-classification-models-2002224bfa3d?source=post_page-----819c19f5dab3--------------------------------)'
- en: '*Before we go to the hands-on example, I will first briefly discuss the* [*HGBoost
    library [4]*](https://erdogant.github.io/hgboost/) *.*'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*在我们进入实际示例之前，我将简要讨论一下* [*HGBoost 库 [4]*](https://erdogant.github.io/hgboost/)
    *。*'
- en: The Steps Towards a Hyperoptimized Regression Model.
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通向超优化回归模型的步骤。
- en: There are multiple steps required to train a regression model with optimized
    hyperparameters and to prevent creating an overtrained model. The first three
    steps will form the basis, and be the input to the *HGboost* model. Let’s go step-by-step
    go through the steps.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个具有优化超参数的回归模型并防止创建过拟合模型需要多个步骤。前三个步骤将形成基础，并作为*HGboost*模型的输入。让我们逐步了解这些步骤。
- en: Training a model with hyperoptimized parameters is time-costly, and complexer
    compared to not hyperoptimized models. To prevent adopting overtrained models,
    it requires making (sanity) checks.
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用超优化参数训练模型既耗时又复杂，相比于未超优化的模型。为了防止采用过拟合的模型，需要进行（合理性）检查。
- en: Import and initialize the *HGBoost* library.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入并初始化*HGBoost*库。
- en: Import data set, and pre-processing.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入数据集并进行预处理。
- en: Decide and select the most appropriate evaluation metric.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定并选择最合适的评估指标。
- en: '**The hgboost library takes care of all the following steps:**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**hgboost 库处理所有以下步骤：**'
- en: Split the data into a *train set*, *test set*, and *validation* set.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据分为*训练集*、*测试集*和*验证集*。
- en: Create a double-loop cross-validation model where the inner loop is to optimize
    the hyperparameters and an outer loop for validation and scoring the models.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个双重交叉验证模型，其中内层循环用于优化超参数，外层循环用于验证和评分模型。
- en: Select the best-performing model.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择性能最佳的模型。
- en: Evaluate the best-performing model on the independent validation set.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在独立验证集上评估性能最佳的模型。
- en: Learn the final model on the entire dataset.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在整个数据集上训练最终模型。
- en: Create insightful plots for model and search space evaluation.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建有洞察力的图表用于模型和搜索空间评估。
- en: The final step is the interpretation of the results. *Let's go through each
    of the steps in the next section.*
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是结果解释。*我们将在下一节中详细介绍每一步。*
- en: Step 1\. Import and initialize.
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 1\. 导入和初始化。
- en: 'We can install *HGBoost* using pip with the command:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令通过 pip 安装*HGBoost*：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: After the installation, we can import and initialize a model for a *regression
    task*. The input parameters can be changed accordingly or the defaults (code section
    1) can be used. I will set `max_eval=1000` iterations. The next step is to read
    (or import) the data set.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，我们可以导入并初始化一个用于*回归任务*的模型。输入参数可以相应地更改，或者使用默认设置（代码部分 1）。我将设置`max_eval=1000`迭代。下一步是读取（或导入）数据集。
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Step 2\. Reading and preprocessing the data set.
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 2\. 读取和预处理数据集。
- en: For demonstration, let’s use the data science salary data set [6] that can be
    imported using the function `import_example(data=’ds_salaries’)`. This dataset
    contains 4134 samples and 11 features. I will set `salary_in_usd`as the ***target
    value***. For preprocessing, I will use the internal `.preprocessing()` function
    ***(****code section 2****)*** which utilizes the [***df2onehot library***](https://github.com/erdogant/df2onehot)[5]that
    in its turn encodes the categorical values into one-hot. Note that continuous
    values are not encoded but remain untouched. At this point, it is strongly recommended
    to do the Exploratory Data Analysis (EDA), and make sanity checks.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示，我们使用数据科学薪资数据集 [6]，可以通过函数`import_example(data=’ds_salaries’)`导入。该数据集包含 4134
    个样本和 11 个特征。我将设置`salary_in_usd`作为***目标值***。对于预处理，我将使用内部的`.preprocessing()`函数***(****代码部分
    2****)***，该函数利用了[***df2onehot library***](https://github.com/erdogant/df2onehot)[5]，将分类值编码为
    one-hot。请注意，连续值没有编码，而是保持不变。在这一点上，强烈建议进行探索性数据分析（EDA）和进行合理性检查。
- en: The largest model performance gains typically follow from pre-processing, and
    feature engineering.
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 最大的模型性能提升通常来自于预处理和特征工程。
- en: After the preprocessing step, there are 4134 rows x 198 columns. Be aware that
    the pre-processing steps in this example are based under the assumption that we
    are going to train a *XGboost* model. Different models (*Xgboost, Lightboost,
    Adaboost*) require different preprocessing steps. As an example, the one-hot encoding
    is required for *XGBoost* whereas other models can process non-numeric factors.
    *Read* [***this blog***](/a-guide-to-find-the-best-boosting-model-using-bayesian-hyperparameter-tuning-but-without-c98b6a1ecac8)
    *for more information about the (dis)advantages, and preprocessing steps.*
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在预处理步骤后，有 4134 行 x 198 列。请注意，此示例中的预处理步骤假设我们将训练一个*XGBoost*模型。不同的模型（*Xgboost、Lightboost、Adaboost*）需要不同的预处理步骤。例如，*XGBoost*需要
    one-hot 编码，而其他模型可以处理非数字因素。*阅读* [***这篇博客***](/a-guide-to-find-the-best-boosting-model-using-bayesian-hyperparameter-tuning-but-without-c98b6a1ecac8)
    *以获取有关（不）优点和预处理步骤的更多信息。*
- en: Step 3\. Set the hyper-parameter search space.
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 3\. 设置超参数搜索空间。
- en: The search space for hyperparameter optimization is defined in *HGBoost* and
    slightly differs between the models *XGBoost, LightBoost, and CatBoost* (code
    section 3). *Note that the search space (code section below) is pre-defined in
    HGboost.*
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数优化的搜索空间在*HGBoost*中定义，并且在模型*XGBoost、LightBoost和CatBoost*之间略有不同（代码部分 3）。*注意，搜索空间（下方代码部分）在HGboost中是预定义的。*
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Step 4\. Train/ Test/ Evaluation sets and Evaluation Metrics.
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 4\. 训练/测试/评估集和评估指标。
- en: For supervised machine learning tasks, it is important to split the data to
    avoid [overfitting](https://www.techtarget.com/whatis/definition/overfitting)
    when learning the model. Overfitting is when the model fits (or learns) the data
    too well and then fails to predict (new) unseen data reliably.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于监督式机器学习任务，重要的是拆分数据以避免[过拟合](https://www.techtarget.com/whatis/definition/overfitting)模型。过拟合是指模型过于准确地拟合（或学习）数据，进而无法可靠地预测（新的）未见数据。
- en: Without splitting the data carefully, you are at risk to overfit the model parameters
    and fit (or learn) the data too well. It can then easily fail to correctly predict
    new (unseen) data samples.
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果没有仔细拆分数据，你可能会过度拟合模型参数，对数据进行过度学习。这样一来，模型可能会无法正确预测新的（未见）数据样本。
- en: In the training process, the data set is divided into a *training set*, a *testing
    set,* and, an *independent validation set*. *The validation set remains untouched
    during the entire training process. It is used only once to evaluate the final
    model performance.* Splitting the dataset is performed in percentages, such as
    `test_size=0.2` and `eval_size=0.2`. The model evaluation metric can be set using
    the `eval_metric`. The default ***evaluation metric*** is set to Root Mean Squared
    Error (***RMSE***), but other flavors such as the Mean Squared Error (***MSE***)
    or Mean Absolute Error (***MAE***) can also be used.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，数据集被划分为*训练集*、*测试集*和*独立验证集*。*验证集在整个训练过程中保持不变，仅在评估最终模型性能时使用一次。* 数据集的拆分以百分比进行，如`test_size=0.2`和`eval_size=0.2`。可以使用`eval_metric`设置模型评估指标。默认的***评估指标***设为均方根误差（***RMSE***），但也可以使用其他指标，如均方误差（***MSE***）或平均绝对误差（***MAE***）。
- en: Step 5\. Fit, Optimize Hyperparameters, and Select The Best Model.
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5步：拟合、优化超参数并选择最佳模型。
- en: At this point, we preprocessed the data for *XGBoost* and decided which evaluation
    metric to use. ***We can now start fitting a model and optimizing the hyperparameters.***
    The first step in *HGBoost* for the hyperparameter optimization is setting up
    the inner loop for optimizing the hyperparameters using Bayesian optimization
    and, the outer loop to test how well the best-performing models can generalize
    using *k*-fold cross-validation. The search space depends on the available hyperparameters*.*
    The evaluation metric is set to MAE because of the good interpretability. Or in
    other words, if we would see MAE=10000 with ***salary*** as the target value,
    it depicts that on average, the predictive distance from the true value is 10000
    USD.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们对数据进行了*XGBoost*的预处理，并决定使用哪个评估指标。***现在我们可以开始拟合模型并优化超参数。*** 在*HGBoost*中，优化超参数的第一步是设置内部循环，以贝叶斯优化来优化超参数，以及外部循环，以测试最佳性能模型如何使用*k*-折交叉验证进行泛化。搜索空间取决于可用的超参数。评估指标设为MAE，因为其良好的可解释性。换句话说，如果我们看到目标值为***薪资***的MAE=10000，则表明平均预测距离真实值为10000美元。
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: By running the above code section, we iterated across the search space and created
    1000 different models (`max_eval=1000`) for which the performances are evaluated.
    Next, the models are ranked on their initial model performance. We can now evaluate
    the robustness of the top *K* best-performing models `top_cv_evals=10` using the
    5-fold cross-validation scheme (`cv=5`). In this sense, we aim to prevent finding
    an overtrained model. In this example, the best scoring model across the 1000
    iterations scored *MAE=35220*, while the average MAE based on the 5-fold cross-validation
    was *MAE=35860*. The MAE on the independent validation set is *35270* using the
    best model from the 5-fold CV. *Although we do not select the best scoring model,
    we do prevent taking a model that is subject to being overfitted.*
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行上述代码段，我们遍历了搜索空间并创建了1000个不同的模型（`max_eval=1000`），对其性能进行了评估。接下来，对这些模型按初始模型性能进行排名。我们现在可以使用5折交叉验证方案（`cv=5`）评估排名前*K*的最佳模型的鲁棒性（`top_cv_evals=10`）。在这方面，我们旨在防止找到过度训练的模型。在这个例子中，在1000次迭代中得分最高的模型的*MAE=35220*，而基于5折交叉验证的平均MAE为*MAE=35860*。在5折交叉验证的最佳模型上，独立验证集的MAE为*35270*。*虽然我们没有选择得分最高的模型，但我们确实避免了选择一个可能过拟合的模型。*
- en: The best model is not necessarily the one with the best model performance but
    the one that can generalize on (new) unseen data, providing accurate predictions.
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 最佳模型不一定是性能最好的模型，而是能够在（新的）未见数据上进行泛化、提供准确预测的模型。
- en: '***We now have a model that is ready to make predictions on new data.*** But
    before making predictions, let’s first try to understand what happened in all
    the steps above.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '***我们现在有了一个可以对新数据进行预测的模型。*** 但在进行预测之前，我们首先需要了解上述所有步骤中发生了什么。'
- en: Step 6\. Interpretation of the results.
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 6\. 结果解释。
- en: At this point, we have a trained model and briefly looked at the regression
    results based on the cross-validation, and using the independent validation set.
    All tested hyperparameters for the different models are returned, which can be
    deeper examined(code section below)*.* See also *Figure 1* where the summary of
    all model results `results['summary']` is shown in a data frame. In addition,
    we can also examine the results by making plots.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经有了一个训练好的模型，并简要查看了基于交叉验证的回归结果，并使用了独立验证集。所有测试过的超参数都返回了，可以进一步检查（见下方代码部分）*。*
    另见*图 1*，其中显示了所有模型结果的汇总`results['summary']`的数据框。此外，我们还可以通过绘制图表来检查结果。
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/af3eda5a237184f7d32555792f83707f.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/af3eda5a237184f7d32555792f83707f.png)'
- en: Figure 1\. The output of the results[‘summary’] data frame contains all model
    results. (image from the author)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 结果`['summary']`数据框的输出包含所有模型结果。（作者提供的图像）
- en: Creating plots is important to deeper examine the model performance and to investigate
    how the hyperparameters are tuned during the Bayesian optimization process.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 创建图表对于深入检查模型性能以及研究贝叶斯优化过程中超参数的调整非常重要。
- en: Gaining intuition with the model results is important to know whether the model
    parameters are choosen reliable.
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 理解模型结果的直观感受很重要，这有助于判断模型参数是否选择可靠。
- en: 'We can create the following plots using the built-in functionalities of *HGBsoost*:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用*HGBsoost*的内置功能创建以下图表：
- en: '*Plots to investigate the hyperparameter space.*'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*绘制以研究超参数空间的图表。*'
- en: '*Plot to summarize all evaluated models.*'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*绘制总结所有评估模型的图表。*'
- en: '*Plot the performances for the cross-validations.*'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*绘制交叉验证的性能图表。*'
- en: '*Plot the results of the independent validation set*.'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*绘制独立验证集的结果图表。*'
- en: '*Plot the decision tree to understand how features are used.*'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*绘制决策树以理解特征的使用方式。*'
- en: '[PRE5]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Interpretation of the Hyperparameter Tuning.
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数调整的解释。
- en: Let’s start by investigating how the hyperparameters are tuned during the Bayesian
    Optimization process*.* With the function `.plot_params()` we can create insightful
    plots as depicted in ***Figure 2***. This figure contains multiple histograms
    (or kernel density plots), where each subplot contains a single parameter that
    is optimized during the 1000 model iterations. The small bars at the bottom of
    the histogram depict the 1000 evaluations. In contrast, the black dashed vertical
    lines depict the specific parameter value that is used across the top 10 best-performing
    models. The green dashed line depicts the best-performing model ***without***
    the cross-validation approach, and the red dashed line depicts the best-performing
    model ***with*** cross-validation.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先从研究贝叶斯优化过程中的超参数调整开始*。* 使用函数`.plot_params()`，我们可以创建如***图 2***所示的有洞察力的图表。该图包含多个直方图（或核密度图），每个子图包含在1000次模型迭代中优化的单一参数。直方图底部的小条表示1000次评估。相比之下，黑色虚线垂直线表示在前10个最佳表现模型中使用的特定参数值。绿色虚线表示***不使用***交叉验证方法的最佳表现模型，而红色虚线表示***使用***交叉验证的最佳表现模型。
- en: Let’s have a look at ***Figure 2A***. In the left bottom corner of the figure,
    there is the parameter ***subsample*** for which the values range from 0.5 up
    to 1\. There is a clear peak around 0.95 which indicates that the Bayesian optimization
    explored these regions more intensively. Our best-performing model seems to use
    *subsample=0.956* (red dashed line). But here is more to look at. When we now
    look at ***Figure 2B***, we also have the *subsample* for which each dot is one
    of the 1000 models. The horizontal axes are the iterations and the vertical axis
    is the optimized values. For this parameter, there is a clear trend during the
    iterations of the optimization process. It first explored the lower bound regions
    and then moved towards the upper bound regions because models scored apparently
    better when increasing the *subsample* value.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 ***图 2A***。在图的左下角，有参数 ***subsample***，其值范围从 0.5 到 1。0.95 附近有一个明显的峰值，这表明贝叶斯优化在这些区域进行了更密集的探索。我们表现最好的模型似乎使用了
    *subsample=0.956*（红色虚线）。但还有更多内容需要查看。当我们查看 ***图 2B*** 时，我们也可以看到 *subsample*，每个点代表
    1000 个模型中的一个。横轴是迭代次数，纵轴是优化后的值。对于这个参数，优化过程中的迭代有一个明显的趋势。它首先探索了下限区域，然后移动到上限区域，因为增加
    *subsample* 值后模型的得分显然更好。
- en: '*In this manner, all the hyperparameters can be interpreted in relation to
    the different models.*'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*以这种方式，所有超参数都可以与不同模型进行解释。*'
- en: '![](../Images/a78739ed854fbadd02a6f9543e2a8dcf.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a78739ed854fbadd02a6f9543e2a8dcf.png)'
- en: Figure 2\. Tuned hyperparameters during the 1000 iterations for the regression
    model. A. The distribution of the seven parameters. B. Each dot is one iteration
    for which a particular value is selected. Note that this image is only for illustration
    and not the real results. (image from the author)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 回归模型在 1000 次迭代中的调整超参数。A. 七个参数的分布。B. 每个点代表一次迭代，其中选择了一个特定值。请注意，这张图片仅用于说明，并非实际结果。（图片来自作者）
- en: Interpretation of the model performance across all evaluated models.
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对所有评估模型的性能进行解释。
- en: With the plot function `.plot()` we create insights into the performance (MAE
    in this case) of the 1000 models (***Figure 3***). The green dashed line depicts
    the best-performing model ***without*** the cross-validation approach, and the
    red dashed line depicts the best-performing model ***with*** cross-validation.
    ***Our selected model is the one with the red dashed line.*** In general, we can
    see a trend that model performance improves during the iterations as the MAE scores
    get lower. This indicates that Bayesian optimization works very well. Furthermore,
    when we look at the top 10 scoring models (the red squared ones), their performance
    is slightly worse during the k-fold cross-validation. Or in other words, the best
    model during the 1000 iterations is depicted with the green dashed line but in
    the CV it does not perform the best. Therefore, a model that can better generalize
    from the top 10 is selected (red dashed line), aka the one with the lowest average
    MAE from the k-fold CV. In this manner, we aim to select the best-performing model
    that can also generalize.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `.plot()` 函数我们可以洞察 1000 个模型的性能（在这个例子中是 MAE）（***图 3***）。绿色虚线表示在没有交叉验证方法的情况下表现最好的模型，红色虚线表示使用了交叉验证的最佳模型。***我们选择的模型是红色虚线所示的模型。***
    一般来说，我们可以看到模型性能在迭代过程中有所提升，因为 MAE 分数在降低。这表明贝叶斯优化效果非常好。此外，当我们查看得分前 10 的模型（红色方框标记的模型）时，它们在
    k 折交叉验证中的表现略微较差。换句话说，尽管在 1000 次迭代中表现最好的模型用绿色虚线表示，但在交叉验证中它的表现并不是最好。因此，选择一个能更好泛化的模型（红色虚线），即从
    k 折交叉验证中获得的平均 MAE 最低的模型。通过这种方式，我们旨在选择最佳表现且具有良好泛化能力的模型。
- en: '![](../Images/0517b126c153fc4668dea92609ab0cd1.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0517b126c153fc4668dea92609ab0cd1.png)'
- en: Figure 3\. Model performance for the 1000 evaluations and cross-validation.
    (image from the author)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. 1000 次评估和交叉验证的模型性能。（图片来自作者）
- en: Interpretation of the model performance on the independent validation set.
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对独立验证集上的模型性能进行解释。
- en: To make sure the model can generalize on unseen data, we use the independent
    validation set. ***Figure 4***depicts the regression line and the predictions
    do not show strong outliers or a consistent over or underestimation. An underestimation
    is seen for the low salaries as the model predicted the salaries of those cases
    were predicted less than the real salary.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保模型能够对未见数据进行泛化，我们使用独立验证集。***图 4***展示了回归线，预测结果没有显示出强烈的异常值或一致的高估或低估。我们看到低薪资存在低估情况，因为模型预测这些情况的薪资低于实际薪资。
- en: '![](../Images/b15db596c5803ec89358c7171aa9c01f.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b15db596c5803ec89358c7171aa9c01f.png)'
- en: Figure 4\. Results on the independent validation set. (image from the author)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. 在独立验证集上的结果。 （图片来自作者）
- en: To deeper investigate how our final model generalizes, we can plot the results
    for the 5-fold cross-validation using the function `.plot_cv()`. This will create
    the ROC curves for the different folds as shown in ***Figure 5***. Here we can
    see that the slope of the model, across the different folds, is more or less similar.
    Thus our final model does not show irregularities on the k-fold CV.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更深入地研究我们最终模型的泛化情况，我们可以使用`.plot_cv()`函数绘制5折交叉验证的结果。这将创建不同折的ROC曲线，如***图 5***所示。在这里我们可以看到模型在不同折中的斜率大致相同。因此，我们的最终模型在k折交叉验证中没有显示出异常。
- en: '![](../Images/13ca0cd68ec4553664563b6d463269c7.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13ca0cd68ec4553664563b6d463269c7.png)'
- en: Figure 5\. Results for the 5-fold CV using the model with optimized hyperparameters.
    (image from the author)
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. 使用优化超参数的模型进行5折交叉验证的结果。 （图片来自作者）
- en: Decision Tree plot of the best model.
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最佳模型的决策树图。
- en: With the decision tree plot (***Figure 6***) we can get a better understanding
    of how the model makes the decision. It may also give some intuition whether such
    a model can generalize over other datasets. Note that the best tree is returned
    by default `num_tree=0`but many trees are created that can be returned by specifying
    the input parameter `.treeplot(num_trees=1)` . In addition, we can also plot the
    best-performing features as depicted in ***Figure 7***. The features *Remote*,
    *work year*, and *experience level* are the top 3 features that are important
    in the prediction of the salary.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 通过决策树图（***图 6***），我们可以更好地理解模型如何做出决策。这也可以提供一些直觉，说明这种模型是否可以对其他数据集进行泛化。请注意，默认情况下返回的最佳树是`num_tree=0`，但可以通过指定输入参数`.treeplot(num_trees=1)`返回许多树。此外，我们还可以绘制表现最佳的特征，如***图
    7***所示。*远程工作*、*工作年限*和*经验水平*是预测薪资时最重要的前三个特征。
- en: '![](../Images/b2e38a84963ead3515d36bff4743f45d.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2e38a84963ead3515d36bff4743f45d.png)'
- en: Figure 6\. Decision tree plot for the best model. (image from the author)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6\. 最佳模型的决策树图。 （图片来自作者）
- en: '![](../Images/8f225c804aca28b1aba41aa280d94199.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8f225c804aca28b1aba41aa280d94199.png)'
- en: Figure 7\. Feature importance. (image from the author)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7\. 特征重要性。 （图片来自作者）
- en: Make Predictions on new data
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对新数据进行预测
- en: After having the final trained model, we can now use it to make *predictions*
    on new data. Suppose that ***X*** is new data, and is similarly pre-processed
    as in the training process, then we can use the `.predict(X)` function to make
    predictions. This function returns the classification probability and the prediction
    label *(****code section 5)***.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在获得最终训练模型后，我们现在可以用它对新数据进行*预测*。假设***X***是新数据，并且经过与训练过程相似的预处理，然后我们可以使用`.predict(X)`函数进行预测。这个函数返回分类概率和预测标签*(****代码部分
    5)***。
- en: '[PRE6]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Save and load model
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保存和加载模型
- en: 'Saving and loading models can become handy. In order to accomplish this, there
    are two functions: `.save()` and function `.load()`.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 保存和加载模型非常方便。为了实现这一点，有两个函数：`.save()`和`.load()`函数。
- en: '[PRE7]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Wrapping up.
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结。
- en: 'I demonstrated how to train a **regression** model with optimized hyperparameters
    by splitting the dataset into a train, test, and independent validation set. Within
    the train-test set, there is the inner loop for optimizing the hyperparameters
    using Bayesian optimization (with hyperopt) and, the outer loop to score how well
    the top-performing models can generalize based on k-fold cross-validation. In
    this manner, we aim to select the model that can generalize with the best accuracy.
    Note that in our example, the best-scoring model seems to have (more or less)
    a similar score to a model with default parameters. This strengthens my point
    that an important part before training any model is to do the typical modeling
    workflow: *Start with the Exploratory Data Analysis (EDA), iteratively do the
    cleaning, feature engineering, and feature selection. The largest performance
    gains typically follow from these steps.*'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我演示了如何通过将数据集拆分为训练集、测试集和独立验证集来训练一个**回归**模型，并优化超参数。在训练-测试集内，使用贝叶斯优化（通过hyperopt）优化超参数，外部循环则是根据k折交叉验证评估顶级模型的泛化能力。这样，我们旨在选择能够以最佳准确性泛化的模型。请注意，在我们的例子中，最佳得分模型似乎与具有默认参数的模型得分（或多或少）相似。这强化了我之前提到的观点，即在训练任何模型之前，一个重要的部分是进行典型的建模工作流：*从探索性数据分析（EDA）开始，迭代进行清理、特征工程和特征选择。通常这些步骤会带来最大的性能提升。*
- en: The [HGBoost library](https://erdogant.github.io/hgboost/) also supports learning
    classification models, multi-class models, and even an ensemble of boosting tree
    models.For all tasks, the same double-loop cross-validation scheme is applied
    to make sure the best-performing and most robust model is selected. Furthermore,
    *HGBoost* utilizes the [HyperOpt](http://hyperopt.github.io/hyperopt/) [7, 8]
    library for the Bayesian optimization algorithm which is one of the most popular
    libraries for hyperparameter optimization.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[HGBoost库](https://erdogant.github.io/hgboost/)还支持学习分类模型、多分类模型，甚至是提升树模型的集成。对于所有任务，应用相同的双重交叉验证方案，以确保选择性能最佳、最稳健的模型。此外，*HGBoost*利用了[HyperOpt](http://hyperopt.github.io/hyperopt/)
    [7, 8]库进行贝叶斯优化算法，该库是超参数优化中最受欢迎的库之一。'
- en: '*Be safe. Stay frosty.*'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意安全，保持冷静。*'
- en: '***Cheers, E.***'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '***干杯，E.***'
- en: '*If you find this article helpful, you are welcome to* [*follow me*](http://erdogant.medium.com/)
    *because I write more about model training and optimizations. If you are thinking
    of taking a Medium membership, you can support my work a bit by using my* [*referral
    link*](https://medium.com/@erdogant/membership)*. It is the same price as a coffee
    but allows you to read unlimited articles monthly.*'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果您觉得这篇文章有帮助，欢迎* [*关注我*](http://erdogant.medium.com/) *，因为我会写更多关于模型训练和优化的内容。如果您考虑购买Medium会员，您可以通过使用我的*
    [*推荐链接*](https://medium.com/@erdogant/membership)*来支持我的工作。价格相当于一杯咖啡，但允许您每月阅读无限量的文章。*'
- en: Software
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 软件
- en: '[HGBoost Github](http://github.com/erdogant/hgboost)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[HGBoost Github](http://github.com/erdogant/hgboost)'
- en: '[HGBoost documentation pages](https://erdogant.github.io/hgboost/)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[HGBoost文档页面](https://erdogant.github.io/hgboost/)'
- en: '[Colab notebook](https://erdogant.github.io/hgboost/pages/html/Blog.html#colab-regression-notebook)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Colab笔记本](https://erdogant.github.io/hgboost/pages/html/Blog.html#colab-regression-notebook)'
- en: Other relevant links
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他相关链接
- en: '[Let’s connect on LinkedIn](https://www.linkedin.com/in/erdogant/)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[让我们在LinkedIn上联系](https://www.linkedin.com/in/erdogant/)'
- en: '[Follow me on Github](https://github.com/erdogant)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[关注我在Github上的动态](https://github.com/erdogant)'
- en: References
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Nan Zhu et al, [*XGBoost: Implementing the Winningest Kaggle Algorithm in Spark
    and Flink*](https://www.kdnuggets.com/2016/03/xgboost-implementing-winningest-kaggle-algorithm-spark-flink.html).'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Nan Zhu 等，[*XGBoost: 在Spark和Flink中实现最成功的Kaggle算法*](https://www.kdnuggets.com/2016/03/xgboost-implementing-winningest-kaggle-algorithm-spark-flink.html)。'
- en: '[Welcome to LightGBM’s documentation](https://lightgbm.readthedocs.io/)!'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[欢迎访问LightGBM的文档](https://lightgbm.readthedocs.io/)!'
- en: '[CatBoost is a high-performance open source library for gradient boosting on
    decision trees](https://catboost.ai/).'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[CatBoost是一个高性能的开源决策树梯度提升库](https://catboost.ai/)。'
- en: E. Taskesen, 2020, [*Hyperoptimized Gradient Boosting*](https://github.com/erdogant/hgboost)*.*
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: E. Taskesen, 2020, [*超优化梯度提升*](https://github.com/erdogant/hgboost)*.*
- en: 'E.Taskesen, 2019, [*df2onehot: Convert unstructured DataFrames into structured
    dataframes.*](https://github.com/erdogant/df2onehot)'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'E.Taskesen, 2019, [*df2onehot: 将非结构化DataFrame转换为结构化DataFrame*](https://github.com/erdogant/df2onehot)'
- en: Kaggle, [*Machine Learning from Disaster*](https://www.kaggle.com/c/titanic)*.*
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kaggle, [*灾难中的机器学习*](https://www.kaggle.com/c/titanic)*.*
- en: 'James Bergstra et al, 2013, [*Hyperopt: A Python Library for Optimizing the
    Hyperparameters of Machine Learning Algorithms.*](https://conference.scipy.org/proceedings/scipy2013/pdfs/bergstra_hyperopt.pdf)'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'James Bergstra 等，2013年，[*Hyperopt: 一个用于优化机器学习算法超参数的 Python 库*](https://conference.scipy.org/proceedings/scipy2013/pdfs/bergstra_hyperopt.pdf)。'
- en: Kris Wright, 2017, [*Parameter Tuning with Hyperopt*.](https://medium.com/district-data-labs/parameter-tuning-with-hyperopt-faa86acdfdce)
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kris Wright，2017年，[*使用 Hyperopt 进行参数调优*](https://medium.com/district-data-labs/parameter-tuning-with-hyperopt-faa86acdfdce)。
