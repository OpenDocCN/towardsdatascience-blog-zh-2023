# 语言模型能否自制工具？

> 原文：[https://towardsdatascience.com/can-language-models-make-their-own-tools-cbc7c3777d22](https://towardsdatascience.com/can-language-models-make-their-own-tools-cbc7c3777d22)

## LaTM、CREATOR以及其他LLM工具使用的闭环框架……

[](https://wolfecameron.medium.com/?source=post_page-----cbc7c3777d22--------------------------------)[![Cameron R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----cbc7c3777d22--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cbc7c3777d22--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cbc7c3777d22--------------------------------) [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----cbc7c3777d22--------------------------------)

·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----cbc7c3777d22--------------------------------) ·阅读时间16分钟·2023年9月17日

--

![](../Images/dcbddb8cfa90114c42e1cb68195ad874.png)

（照片由[Todd Quackenbush](https://unsplash.com/@toddquackenbush?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)提供，来源于[Unsplash](https://unsplash.com/photos/IClZBVw5W5A?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)）

在最近的综述中，我们探讨了通过外部工具增强大型语言模型（LLMs）的实用性。这些模型可以被教导以多种方式利用工具。然而，我们应该意识到，现有的工具跟随LLMs仅利用了一小部分潜在工具[3]，*而我们希望用LLMs解决的问题范围几乎是无穷无尽的！* 考虑到这一点，显而易见，这种范式是有限制的——我们总是能找到需要尚不存在的工具的情景。在本综述中，我们将探讨旨在解决这一问题的最新研究，通过赋予LLMs创造自身工具的能力来解决这一问题。这种方法与人类生活有趣地类比，因为制造工具的能力导致了重大的技术进步。现在，我们探讨类似技术对LLMs进化的影响。

> “根据人类进化里程碑的经验教训，一个关键的转折点是人类获得了制造自身工具以应对新兴挑战的能力。我们开始初步探索将这一进化概念应用于LLMs领域。” *— 引自 [1]*

![](../Images/34e0b7f7c970f62479d2f3d43fa0d46b.png)

（来源于 [1, 2]）

# 背景

在进一步了解工具制造的LLMs之前，我们需要刷新一些背景概念。我们在最近的综述中已经覆盖了许多这些概念，但我们现在会简要地再次讨论它们，以使我们对最新出版物的讨论更加全面和易于理解。

## 为什么我们应该使用工具？

![](../Images/5eedac10a8b560004f894d36dcc91c53.png)

(来自 [3, 8, 9])

在之前的概述中，我们已经了解了几种不同类型的工具，这些工具可以与 LLM 集成以改善其性能，例如：

+   基础工具（计算器、搜索引擎等） [[链接](/teaching-language-models-to-use-tools-7fd58916c66b)]

+   深度学习模型 API [[链接](/language-models-and-friends-gorilla-hugginggpt-taskmatrix-and-more-b88c1200afd3)]

通过让 LLM 访问某些工具，我们可以轻松解决这些模型存在的限制，例如缺乏最新信息、无法进行简单的算术运算、产生虚假信息或在长链推理中出错。

![](../Images/7718ca32c8d29642c95c0faadec6fcbb.png)

(来自 [3])

**工具提供上下文。** 例如，如果 LLM 被问到关于最近几周的流行文化事件的问题，由于知识截止日期，它不太可能提供准确的答案。在某些情况下，LLM 可能会产生看似可信的错误答案，并误导用户提供不正确的信息 — *这是一个重大问题，因为许多（非技术性）LLM 用户如 ChatGPT 使用这些模型就像使用搜索引擎一样*! 为了解决这个问题，我们可以提供一个工具，允许 LLM 执行搜索查询并从互联网上获取最新信息作为额外上下文；见上文。这样，LLM 可以记住更少的信息，而依赖上下文学习，通过工具提供的最新信息得出准确的最终答案。

> “通过赋能 LLM 使用工具，我们可以获得更广泛和不断变化的知识库，并完成复杂的计算任务。” *— 来自 [3]*

在这次概述中，我们将看到一个有趣的工具作为基准使用，即 Wolfram ChatGPT 插件。ChatGPT 的插件生态系统通过 API 将 LLM 与外部工具集成。基本上，我们向 ChatGPT 提供 API 的描述，模型通过提示方法学习如何使用该工具（即调用其 API）；更多详细信息 [这里](https://cameronrwolfe.substack.com/i/123558334/using-tools-is-getting-easier)。要了解更多关于 Wolfram 插件的信息（它非常有用！），请查看精彩概述 [这里](https://www.wolfram.com/wolfram-plugin-chatgpt/)。

**本概述。** 存在许多不同类型或类别的工具，但我们将重点关注一种特定类型的工具 — *那些实际上由 LLM 创建的工具*。通常，这些工具被格式化为独立的 Python 函数，完成对 LLM 有用的某些任务或子任务。工具通过直接提示支持代码的 LLM 生成所需的功能而创建。通过允许 LLM 创建自己的工具，*解决问题的系统不再受限于固定的工具集合*。我们可以随着时间的推移识别所需的功能，并使 LLM 能够自动创建任何有用的工具！

## 提示技术

![](../Images/e99bd26c9bf61aeed80a9bd70e2fd5fb.png)

语言模型以统一的格式解决许多不同的任务（来自 [10]）

语言模型的通用文本到文本结构极为强大，因为它允许我们通过 *i)* 将问题格式化为文本提示和 *ii)* 从模型返回的文本中提取相关输出信息来解决许多不同的任务。然而，使用语言模型通常没有这么简单。我们提供给模型的提示的措辞和结构可以极大地改变其效果——提示工程至关重要！

最近，我们讨论了许多实用技巧和技术，以通过提示工程充分利用 LLM。

+   实用提示工程 [[link](/practical-prompt-engineering-74e96130abc4)]

+   高级提示工程 [[link](/advanced-prompt-engineering-f07f9e55fe01)]

然而，有两种特别相关的提示技术——思维链（CoT） [6] 和思维程序（PoT） [7] 提示。这两种技术都旨在提高语言模型可靠解决复杂推理任务的能力。

![](../Images/acd0369561136554d21a89c1412a5094.png)

（来自 [6]）

**思维链。** 长期以来，LLM 因无法解决基于推理的任务而受到批评。尽管这一问题已通过最近的模型变体得到缓解，但诸如 CoT 提示等技术仍能引发这些模型更好的推理能力。*这怎么可能？* 我们只需要向 LLM 提供将基于推理的问题分解为逐步解决的示例（即问题解决的理由或“思维链”）。这些示例直接插入 LLM 的提示中。然后，模型可以利用其上下文学习能力，在解决用户提出的问题时生成类似的理由。有趣的是，生成这样的理由会大幅提高 LLM 在基于推理的任务上的表现。

![](../Images/aa769348f15efcaff10fe1d38cf24960.png)

（来自 [11, 12, 13]）

除了普通的 CoT 提示外，还有许多变体被提出；见上文。通过将难题分解成更小的部分并逐步解决，让 LLM 解决复杂问题的理念极为强大。然而，我们可以通过几种不同的方式来实现这一点（其中一些实际上比 CoT 提示更简单）——*CoT 提示并不总是我们最佳的选择*。

**思维程序。** 尽管思维CoT提示变体效果良好，但它们未能建模像迭代这样的概念，并且存在组成性差距，这意味着LLM可能正确解决了问题的每一步，但最终答案仍然可能不正确。为了缓解这些问题，最近的研究探索了程序辅助的语言模型。这些技术类似于CoT提示，但我们使用代码支持的LLM（例如，Codex [14]）生成包含代码和自然语言组件的混合问题解决方案——*基本上是带有信息性注释的程序*。然后，我们可以使用外部解释器执行LLM创建的程序，以得出最终答案！

![](../Images/1fb4fbd8cd57f15a4fcaadf1ea7ef34e.png)

（来源：[7]）

PoT提示的基本思想是某些想法和概念在程序中建模起来更容易。与其让LLM既解决子任务又从这些解决方案中生成最终答案，我们可以将部分过程委托给更可靠的系统。即，程序可以更容易地建模和解决数学方程式，执行迭代等，从而减少LLM的组成性差距。

# 工具使用的转折点

> “与其让LLM作为工具的用户，不如让它们成为工具的创造者，以更高的准确性和灵活性解决问题。” *— 来源于[2]*

此时，我们可能应该确信工具是对现有语言模型的有用补充。但是，*当我们将可用工具的范围扩展到LLM可以创建的任何工具时，会有什么可能性？* 简而言之，这种方法形成了一个闭环框架，在这个框架中，LLM被赋予了随意改进自身功能的能力。正如我们接下来会看到的，现有模型在制作自己的工具方面出乎意料地有能力，这使得它们能够随着时间的推移动态适应解决新的、困难的问题。

## 解耦工具制作和工具使用

我们知道，使用外部工具可以极大地提高LLM的解决问题能力[3]。然而，该领域的先前工作假设所需的工具已经存在并可供LLM使用。因此，这种方法依赖于人工制作和策划一套全面解决任何任务所需功能的工具。但是，*如果LLM需要一个工具，而这个工具不在它的工具箱里怎么办？* 现有的工具跟随方法对此类问题没有解决方案！

作为替代方法，[2]中的作者提出了一种“闭环”框架，该框架利用LLM自身即时构建所需工具。这个框架称为LLMs作为工具制造者（LaTM），它允许LLM不断生成解决不同复杂推理任务所需的工具；见下文。

![](../Images/c5e94edf24871fa201bd8d3406fadf60.png)

（来源：[1]）

**两阶段工具方法。** LaTM使用两阶段框架：

+   *工具制作：* 为给定任务制作工具（即 Python 函数）

+   *工具使用：* 使用现有工具来解决任务

值得注意的是，我们不必在这两个阶段使用相同的 LLM。例如，我们可以将更强大的模型（例如，[GPT-4](https://platform.openai.com/docs/models/gpt-4)）应用于工具制作，而使用轻量且成本效益更高的模型（例如，[GPT-3.5-turbo](https://platform.openai.com/docs/models/gpt-3-5)）来进行工具使用。鉴于工具制作通常需要相对于工具使用更强大的 LLM，这种方法使 LaTM 在实践中能够节省成本。我们只在工具制作阶段使用最昂贵和强大的模型，每个工具执行一次并不断重复使用于问题解决！

![](../Images/86100da32ee5a12c3488eb6a7e8cdc50.png)

（来自 [1]）

工具制作过程的目标是从少量任务解决示例中生成一个通用且可重用的工具 —— 以 Python 函数的形式实现。在 [2] 中，这一目标通过首先通过少量示例学习“提议”一个工具来实现。我们提供多个期望行为的演示，并提示 LLM 生成一个程序，该程序再现这些演示的输出。如果生成的程序没有错误，LaTM 使用 LLM 生成几个单元测试（即基于提供的演示）并执行这些测试以确认它们通过。最后，工具被“包装”，或通过展示其用法的提示提供；见上文。

尽管工具制作复杂且需要强大的 LLM 才能成功，但工具使用可以通过更具成本效益的模型来实现 —— *我们只是使用现有工具来解决任务*! 我们可以提示 LLM 通过工具制作过程中创建的包装工具来使用工具，这些工具提供将任务转换为相关函数调用的演示。在这里，LaTM 依赖 LLM 的上下文学习能力来确定每个工具的正确使用方法；见下文。

![](../Images/4770df899f2440d79cb6530a5c294274.png)

（来自 [1]）

值得注意的是，使用较小的模型进行工具使用意味着我们只在 LaTM 流程的短暂部分中使用更强大的模型 —— *工具只创建一次，解决更多问题时可以持续重复使用*。创建和使用 LaTM 工具的过程可能看起来有些复杂，但实际上相当简单。下图中提供了一个创建和使用工具来解决逻辑推理问题的端到端示例。

![](../Images/d40ae33172262ec07b7f043d66368d83.png)

（来自 [1]）

**添加调度器。** 除了工具创建和使用之外，[2]中的作者还提出了一个调度模块，用于处理 LaTM 中新工具的即时创建和使用；详见下文。简而言之，调度器是一个用于确定传入任务是否应该创建新工具或仅使用现有工具的 LLM。通过使用这个额外的模块，我们可以轻松识别现有工具无法处理的新任务，并创建所需的任何工具，使 LaTM 能够更好地处理新任务顺序到达的流式场景。有趣的是，[2]中的作者展示了一个基于 GPT-3.5 的调度器可以以 ~95% 的准确率识别正确的工具使用方式——或是否需要新工具。

![](../Images/693c702da58cf17fcd296693a25a9fc9.png)

（来源于[1]）

**这有效吗？** LaTM 在 [BigBench](https://github.com/google/BIG-bench) 提供的一小部分复杂推理任务上进行评估，其中 [GPT-4](https://openai.com/research/gpt-4) 被用作工具创建者，而 [GPT-3.5-turbo](https://platform.openai.com/docs/models/gpt-3-5) 被用作工具使用者。某种程度上，GPT-4 能够在大多数测试案例中创建出可行且有用的工具并不令人意外；详见下文。能力较弱的模型（例如 GPT-3.5-turbo）可以用于处理较简单的问题的工具制作，但在更复杂的领域中需要 GPT-4。进一步地，我们发现工具制作需要更长的上下文长度，因为 LaTM 在生成工具时使用了全部历史（即到目前为止生成的所有工具的示例），以提高[可靠性](https://github.com/dair-ai/Prompt-Engineering-Guide/blob/main/guides/prompts-reliability.md)。

![](../Images/24c9d326955e0631572964fb6266cb80.png)

（来源于[1]）

当 LaTM 的性能与像[链式思维提示](https://cameronrwolfe.substack.com/p/chain-of-thought-prompting-for-llms) [4]这样的技术进行比较时，我们发现所提出的方法使现有的 LLM 能力大大增强！通过使用生成的工具，像 GPT-3.5-turbo 这样的模型可以与 GPT-4 进行类似的表现，并且远超 CoT 提示的性能；详见下文。此外，我们还发现，使用更轻量的模型作为工具使用者更为可取，甚至在某些情况下优于使用像 GPT-4 这样的强大模型。

![](../Images/d73d4aa9437dd2df17e984710701ba4a.png)

（来源于[1]）

LaTM 是一个有趣的闭环框架，使 LLM 能够生成自己的工具。由于它仅在解决问题过程中的一小部分使用大型、昂贵的 LLM（例如 GPT-4），LaTM 是一种提高 LLM 性能的成本效益高的方法。*我们可以以较小的模型和更低的成本，几乎匹配 GPT-4 在复杂推理任务中的表现。*

## 纠正工具创建中的错误

![](../Images/fd545428ff3d36a211a57b4b94165541.png)

（来源于[2]）

在使用大型语言模型（LLMs）创建自己工具的想法的基础上，[2]的作者提出了一种新颖的框架，该框架 *i)* 通过文档和代码创建来使用LLMs创建相关工具，*ii)* 采用一种更简单的方法来规划如何使用工具解决问题，*iii)* 为工具使用过程添加了一个补充的错误处理机制，以使整个系统更具鲁棒性和准确性。 resulting technique, 被称为CREATOR [2]，与[2]中的研究并行探索。 *这两篇出版物的目标是通过使所需工具的创建成为可能，从而创建更智能和适应性强的系统以解决复杂问题。*

![](../Images/45b30fad060a1ea46fca5b03c9bd019c.png)

(来自 [2])

**问题解决方法。** CREATOR通过一个四步过程来处理工具的创建和使用（参见上面的插图）：

1.  *创建*：通过文档和代码实现来创建工具。

1.  *决策*：决定何时以及如何使用现有工具来解决问题。

1.  *执行*：执行将工具应用于解决问题的程序。

1.  *修正*：根据执行结果修改工具和决策。

之前的工作中没有修正步骤。这个组件作为一个自动化的错误处理机制，提高了系统的鲁棒性。由于[2]中的LLMs（以及相关出版物[1]中的LLMs）使用代码作为创建工具的媒介，我们可以轻松地检测和修正创建或使用工具时出现的错误（例如，通过堆栈跟踪或类似的方式）。

![](../Images/9ddf97431d668febd4fc3bf6c2d74ebe.png)

(来自 [2])

在[2]中，工具创建遵循一种上下文学习方法，该方法提供详细的说明和少量示例以指导LLM生成正确的工具。工具创建有两个主要组件：

+   *文档*：概述有关工具的相关信息（例如，功能、目的、签名等）。

+   *实现*：在代码中实现工具（参见上文）。

类似于[1]，[2]中创建的工具被封装在一个函数或方法（在Python中）中，可以由LLM调用。

![](../Images/85a9c5491f2873811d26a945e719d647.png)

(来自 [2])

在决策阶段，LLM会考虑所有工具的文档，并确定使用哪些工具以及如何使用它们来解决当前的问题。在制定了问题解决计划后，我们可以：

+   格式化每个工具的输入。

+   执行每个工具以获取相关输出。

+   对工具输出执行任何需要的操作以推导出答案。

如果工具执行导致生成任何错误，我们可以简单地记录这些信息，并再次迭代四步过程，将错误作为修正现有工具的额外输入；参见上文。否则，我们可以使用生成的信息提取用户问题的最终答案。

> “我们的研究揭示了利用 LLMs 作为工具创建者可以促进知识转移，且 LLMs 表现出不同的工具创建能力，从而使其能够灵活应对各种情况。” *— 来自 [2]*

**改进的数学推理。** 在 [2] 中提出的系统在数学（和表格）推理数据集 [MATH](https://github.com/hendrycks/math) 和 [TabMWP](https://promptpg.github.io/) [4, 5] 上进行了评估。在所有实验中，ChatGPT（即 [GPT-3.5-turbo](https://platform.openai.com/docs/models/gpt-3-5)）作为基础模型，因其代码生成和令人印象深刻的推理能力。CREATOR 与基线方法进行比较，如标准 CoT 提示 [6]、PoT 提示 [7] 和 [Wolfram alpha ChatGPT 插件](https://www.wolfram.com/wolfram-plugin-chatgpt/)。当所有方法都使用 ChatGPT 作为基础模型时，我们发现 CREATOR（结合 CoT 提示）比基线方法具有更高的整体准确性，并且成功执行率也有所提高（即系统提供了有效格式的答案）。

![](../Images/cfb8520cdd0b8481a1810c587acf8ce6.png)

（来自 [2]）

除了这些评估，作者在 [2] 中提出了一个新的 Creation 挑战数据集，尝试通过在没有现有工具或代码包的新场景中测试问题解决能力来评估 LLM 的工具创建能力。在这一基准测试中，CREATOR 稍微超越了现有的基线。然而，当 LLM 获得关于应创建工具的实用性的文本提示时，这种性能改进会更大；见下文。

![](../Images/44fe3ec0ed5d12a41546dd978c36e793.png)

（来自 [2]）

除了在总体上匹配或超越基线性能的能力之外，CREATOR 框架在问题变得越来越困难时表现得更加稳定，而基线方法往往会恶化。CREATOR 在某些问题类别上也会出现类似的恶化，但该框架似乎更加适应并能够处理复杂问题；见下文。

![](../Images/c9687ff5f312b27c035cf0efc4cdab90.png)

（来自 [2]）

# 收获

在本概述中，我们探讨了一种更具动态性和灵活性的工具使用方法，与 LLMs 结合。我们不再制定一组固定的工具供 LLM 使用，而是可以让 LLM 根据需要创建所需的工具。通过这种方法，我们不再遇到由于没有访问所需工具而导致的问题。我们可能最初会怀疑这种策略是否成功（即 LLMs 是否足够强大以创建自己的工具？），但最近的研究[1, 2]告诉我们，像 GPT-4 这样的最先进 LLMs 完全有能力创建独立的 Python 函数形式的工具，只要在工具创建中采取纠错措施。这些工具可以被使用和重复使用（即使是由较弱的 LLMs），以解决各种复杂问题。

> “工具制造使LLM能够不断生成可以应用于不同请求的工具，以便未来的请求在解决任务时可以调用相应的API。” *— 来自 [1]*

使用LLMs（大语言模型）来创建工具是很棒的，*但这与已经将LLMs与各种现有工具整合的并行努力有何关系？* 这还有待观察。然而，我个人认为，**最佳系统**将会使用现有技术的混合体。已经存在许多有用的工具，并且这些工具每天都在与流行的LLMs集成（例如，参见ChatGPT插件商店）。因此，单靠LLMs来创建自己的工具并不合适。相反，我们可以利用现有的工具，同时也给LLMs所需的技能，以创造它们所缺乏的任何工具。随着时间的推移，LLMs可用的工具套件将继续演变，使基于AI的问题解决系统变得更加有效。

## 与我联系！

非常感谢你阅读这篇文章。我是 [Cameron R. Wolfe](https://cameronrwolfe.me/)，[Rebuy](https://www.rebuyengine.com/)的AI总监。我研究深度学习的经验和理论基础。如果你喜欢这个概述，请订阅我的 [Deep (Learning) Focus newsletter](https://cameronrwolfe.substack.com/)，在这里我通过从基础到高级的相关主题概述来帮助读者理解AI研究。你也可以在 [X](https://twitter.com/cwolferesearch) 和 [LinkedIn](https://www.linkedin.com/in/cameron-r-wolfe-ph-d-04744a238/) 上关注我，或者查看我在medium上的 [其他文章](https://medium.com/@wolfecameron)！

## 参考文献

[1] 蔡天乐等. “大型语言模型作为工具制造者。” *arXiv预印本arXiv:2305.17126*（2023年）。

[2] 钱成等. “CREATOR: 通过工具创建解开大型语言模型的抽象与具体推理。” *arXiv预印本arXiv:2305.14318*（2023年）。

[3] Schick, Timo, 等. “Toolformer: 语言模型可以自学使用工具。” *arXiv预印本arXiv:2302.04761*（2023年）。

[4] Lu, Pan, 等. “通过策略梯度进行动态提示学习，以应对半结构化数学推理。” *arXiv预印本arXiv:2209.14610*（2022年）。

[5] Hendrycks, Dan, 等. “通过数学数据集衡量数学问题解决能力。” *arXiv预印本arXiv:2103.03874*（2021年）。

[6] Wei, Jason, 等. “思维链提示在大型语言模型中引发推理。” *arXiv预印本arXiv:2201.11903*（2022年）。

[7] 陈文华等. “思想提示程序：解开数值推理任务中的计算与推理。” *arXiv预印本arXiv:2211.12588*（2022年）。

[8] Shen, Yongliang, 等. “Hugginggpt: 用chatgpt和huggingface的朋友解决AI任务。” *arXiv预印本arXiv:2303.17580*（2023年）。

[9] Patil, Shishir G., 等. “Gorilla: 连接大量API的大型语言模型。” *arXiv预印本arXiv:2305.15334*（2023年）。

[10] 拉菲尔，科林 等。“通过统一的文本到文本转换器探索迁移学习的极限。” *机器学习研究杂志* 21.1 (2020): 5485–5551。

[11] 小岛武史 等。“大型语言模型是零样本推理器。” *arXiv 预印本 arXiv:2205.11916* (2022)。

[12] 王学智 等。“自我一致性提高了语言模型中的思维链推理能力。” *arXiv 预印本 arXiv:2203.11171* (2022)。

[13] 周，丹尼 等。“最少到最多提示方法在大型语言模型中的复杂推理能力。” *arXiv 预印本 arXiv:2205.10625* (2022)。

[14] 陈马克 等。“评估训练有代码的大型语言模型。” *arXiv 预印本 arXiv:2107.03374* (2021)。
