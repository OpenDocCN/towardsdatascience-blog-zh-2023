- en: The Ultimate Guide to nnU-Net
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-ultimate-guide-to-nnu-net-for-state-of-the-art-image-segmentation-6dda7f44b935](https://towardsdatascience.com/the-ultimate-guide-to-nnu-net-for-state-of-the-art-image-segmentation-6dda7f44b935)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Everything you need to know to understand the State of the Art nnU-Net, and
    how to apply it to your own dataset.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@francoisporcher?source=post_page-----6dda7f44b935--------------------------------)[![François
    Porcher](../Images/9ddb233f8cadbd69026bd79e2bd62dea.png)](https://medium.com/@francoisporcher?source=post_page-----6dda7f44b935--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6dda7f44b935--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6dda7f44b935--------------------------------)
    [François Porcher](https://medium.com/@francoisporcher?source=post_page-----6dda7f44b935--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6dda7f44b935--------------------------------)
    ·13 min read·Aug 2, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e93dc02adab7ff7d7eafaebe00a09554.png)'
  prefs: []
  type: TYPE_IMG
- en: Neuroimaging, by Milak Fakurian on Unsplash, [link](https://unsplash.com/photos/58Z17lnVS4U)
  prefs: []
  type: TYPE_NORMAL
- en: During my Research internship in Deep Learning and Neurosciences at Cambridge
    University, I used the nnU-Net a lot, which is an extremely strong baseline in
    Semantic Image Segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: However, I struggled a little to fully understand the model and how to train
    it, and did not find so much help on internet. Now that I am comfortable with
    it, I created this tutorial to help you, either in your quest to understand better
    what is behind this model, or how to use it in your own dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Throughout this guide, you will:'
  prefs: []
  type: TYPE_NORMAL
- en: Develop a concise overview of the key contributions of nnU-Net.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Learn how to apply nnU-Net to your own dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All code available on this [Google Collab notebook](https://colab.research.google.com/drive/1h6scef1i258x0abxT_FcSI_QBN9Eh_9e?usp=sharing)
  prefs: []
  type: TYPE_NORMAL
- en: This work took me a significant amount of time and effort. If you find this
    content valuable, please consider following me to increase its visibility and
    help support the creation of more such tutorials!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Brief History of nnU-Net
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recognized as a **state-of-the-art model in Image Segmentation**, the nnU-Net
    is an indomitable force when it comes to both 2D and 3D image processing. Its
    performance is so robust that it serves as a strong baseline against which new
    computer vision architectures are benchmarked. In essence, if you are venturing
    into the world of developing novel computer vision models, consider the nnU-Net
    as your **‘target to surpass’.**
  prefs: []
  type: TYPE_NORMAL
- en: 'This powerful tool is based on the U-Net model (You can find one of my tutorials
    here: [Cook your first U-Net](https://medium.com/@foporcher/cooking-your-first-u-net-for-image-segmentation-e812e37e9cd0)),
    which made its debut in 2015\. The appellation “nnU-Net” stands for “No New U-Net”,
    a nod to the fact that its **design doesn’t introduce revolutionary architectural
    alterations.** Instead, it takes the existing U-Net structure and squeezes out
    its full potential using a set of ingenious optimization strategies.'
  prefs: []
  type: TYPE_NORMAL
- en: Contrary to many modern neural networks, the nnU-Net doesn’t rely on residual
    connections, dense connections, or attention mechanisms. Its strength lies in
    its meticulous optimization strategy, which includes techniques like resampling,
    normalization, judicious choice of loss function, optimiser settings, data augmentation,
    patch-based inference, and ensembling across models. This holistic approach allows
    the nnU-Net to push the boundaries of what’s achievable with the original U-Net
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring Diverse Architectures within nnU-Net
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While it might seem like a singular entity, the nnU-Net is in fact an umbrella
    term for three distinct types of U-Nets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5d874d9c4bc9ca3cb7356906001aafea.png)'
  prefs: []
  type: TYPE_IMG
- en: 2D, 3D, and cascade, Image from [nnU-Net article](https://arxiv.org/abs/1809.10486)
  prefs: []
  type: TYPE_NORMAL
- en: '**2D U-Net:** Arguably the most well-known variant, this operates directly
    on 2D images.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**3D U-Net:** This is an extension of the 2D U-Net and is capable of handling
    3D images directly through the application of 3D convolutions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**U-Net Cascade:** This model generates low-resolution segmentations and subsequently
    refines them.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each of these architectures brings its unique strengths to the table and, inevitably,
    has certain limitations.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, employing a 2D U-Net for 3D image segmentation might seem counterintuitive,
    but in practice, it can still be highly effective. This is achieved by slicing
    the 3D volume into 2D planes.
  prefs: []
  type: TYPE_NORMAL
- en: While a 3D U-Net may seem more sophisticated, given its higher parameter count,
    it isn’t always the most efficient solution. Particularly, 3D U-Nets often struggle
    with anisotropy, which occurs when spatial resolutions differ along different
    axes (for example, 1mm along the x-axis and 1.2 mm along the z-axis).
  prefs: []
  type: TYPE_NORMAL
- en: The U-Net Cascade variant becomes particularly handy when dealing with large
    image sizes. It employs a preliminary model to condense the image, followed by
    a standard 3D U-Net that outputs low-resolution segmentations. The generated predictions
    are then upscaled, resulting in a refined, comprehensive output.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a664858b9bbc6fc6c909880bdc09e685.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [nnU-Net article](https://arxiv.org/abs/1809.10486)
  prefs: []
  type: TYPE_NORMAL
- en: Typically, the methodology involves training all three model variants within
    the nnU-Net framework. The subsequent step may be to either choose the best performer
    among the three or employ ensembling techniques. One such technique might involve
    integrating the predictions of both the 2D and 3D U-Nets.
  prefs: []
  type: TYPE_NORMAL
- en: However, it’s worth noting that this procedure can be quite time-consuming (and
    also money because you need GPU credits). If your constraints only allow for the
    training of a single model, fret not. You can choose to only train one model,
    since the ensembling model only brings very marginal gains.
  prefs: []
  type: TYPE_NORMAL
- en: 'This table illustrates the best-performing model variant in relation to specific
    datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/47f06af7a3d5b2e7a3c57dde766c77aa.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [nnU-Net article](https://arxiv.org/abs/1809.10486)
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic adaptation of network topologies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given the significant discrepancies in image size (consider the median shape
    of 482 × 512 × 512 for liver images versus 36 × 50 × 35 for hippocampus images),
    the nnU-Net intelligently adapts the input patch size and the number of pooling
    operations per axis. This essentially implies an automatic adjustment of the number
    of convolutional layers per dataset, facilitating the effective aggregation of
    spatial information. In addition to adapting to the varied image geometries, this
    model takes into account technical constraints, such as available memory.
  prefs: []
  type: TYPE_NORMAL
- en: It’s crucial to note that the model doesn’t perform segmentation directly on
    the entire image but instead on carefully extracted patches with overlapping regions.
    The predictions on these patches are subsequently averaged, leading to the final
    segmentation output.
  prefs: []
  type: TYPE_NORMAL
- en: But having a large patch means more memory usage, and the batch size also consumes
    memory. The tradeoff taken is to always prioritize the patch size (the model’s
    capacity) rather than the batch size (only useful for optimization).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the Heuristic algorithm used to compute the optimal patch size and
    batch size:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7048bc42072c03bd4ca7efa8edaac8c8.png)'
  prefs: []
  type: TYPE_IMG
- en: Heuristic Rule for Batch and Patch Size, Image from [nnU-Net article](https://arxiv.org/abs/1809.10486)
  prefs: []
  type: TYPE_NORMAL
- en: 'And this is what it looks like for different Datasets and input dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a092fe248ec185074a2cd72b71c96018.png)'
  prefs: []
  type: TYPE_IMG
- en: Architecture in function of the input image resolution, Image from [nnU-Net
    article](https://arxiv.org/abs/1809.10486)
  prefs: []
  type: TYPE_NORMAL
- en: 'Great! Now Let’s quickly go over all the techniques used in nnU-Net:'
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All models are trained from scratch and evaluated using five-fold cross-validation
    on the training set, meaning that the original training dataset is randomly divided
    into five equal parts, or ‘folds’. In this cross-validation process, four of these
    folds are used for the training of the model, and the remaining one fold is used
    for the evaluation or testing. This process is then repeated five times, with
    each of the five folds being used exactly once as the evaluation set.
  prefs: []
  type: TYPE_NORMAL
- en: For the loss, we use a combination of Dice and Cross Entropy Loss. This is a
    very frequent loss in Image Segmentation. More details on the Dice Loss in [V-Net,
    the U-Net big’s brother](https://medium.com/ai-mind-labs/v-net-u-nets-big-brother-in-image-segmentation-906e393968f7)
  prefs: []
  type: TYPE_NORMAL
- en: Data Augmentation techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The nnU-Net have a very strong Data Augmentation pipeline. The authors use random
    rotations, random scaling, random elastic deformation, gamma correction and mirroring.
  prefs: []
  type: TYPE_NORMAL
- en: 'NB: You can add your own transformations by modifying the source code'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/56fa89a9ffb590f5d2d10c2f0d621694.png)'
  prefs: []
  type: TYPE_IMG
- en: Elastic deformation, from this [article](https://www.researchgate.net/figure/Grid-distortion-and-elastic-transform-applied-to-a-medical-image_fig4_327742409)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d5d84805c71d600cd88c93a8d63a7aee.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [OpenCV library](https://www.google.com/url?sa=i&url=https%3A%2F%2Fpyimagesearch.com%2F2015%2F10%2F05%2Fopencv-gamma-correction%2F&psig=AOvVaw2LDHscTrcGOq-tR4Ki2AbS&ust=1691168921010000&cd=vfe&opi=89978449&ved=0CBAQjRxqFwoTCKi4i6j9wIADFQAAAAAdAAAAABAE)
  prefs: []
  type: TYPE_NORMAL
- en: Patch based Inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So as we said, the model does not predict directly on the full resolution image,
    it does that on extracted patches and then aggregates the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is what it looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cb25522983df31b26778b3f89f8ad728.png)'
  prefs: []
  type: TYPE_IMG
- en: Patch Based inference, Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'NB: The patches in the center of the picture are given more weight than the
    ones on the side, because they contain more information and the model performs
    better on them'
  prefs: []
  type: TYPE_NORMAL
- en: Pairwise Model Ensembling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/6da3d92b7f49ba1c22aaf9b0e0a7ad5c.png)'
  prefs: []
  type: TYPE_IMG
- en: Model Ensembling, Image by author
  prefs: []
  type: TYPE_NORMAL
- en: So if you remember well, we can train up to 3 different models, 2D, 3D, and
    cascade. But when we make inference we can only use one model at a time right?
  prefs: []
  type: TYPE_NORMAL
- en: Well turns out that no, different models have different strengths and weaknesses.
    So we can actually combine the predictions of several models so that if one model
    is very confident, we prioritize its prediction.
  prefs: []
  type: TYPE_NORMAL
- en: nnU-Net tests every combination of 2 models among the 3 available models and
    picks up the best one.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Practice, there are 2 ways to do that:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hard voting:** For each pixel, we look at all the probabilities outputted
    by the 2 models, and we take the class with the highest probability.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Soft Voting:** For each pixel, we average the probability of the models,
    and then we take the class with the maximum probability.'
  prefs: []
  type: TYPE_NORMAL
- en: Practical implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we begin, you can download the dataset [here](https://drive.google.com/drive/folders/1rQeujQZs5rPH1AeI-KoUICOAzs4u-Eie?usp=sharing)
    and follow the [Google Collab notebook](https://colab.research.google.com/drive/1h6scef1i258x0abxT_FcSI_QBN9Eh_9e?usp=sharing).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you did not understand anything about the first part, no worries, this is
    the practical part, you just need to follow me, and you are still going to get
    the best results.
  prefs: []
  type: TYPE_NORMAL
- en: You need a GPU to train the model otherwise it does not work. You can either
    do it locally, or on Google Collab, **don’t forget to change the runtime > GPU**
  prefs: []
  type: TYPE_NORMAL
- en: So, first of all, you need to have a dataset ready with input images and their
    corresponding segmentation. You can follow my tutorial by downloading this ready
    dataset for 3D Brain segmentation, and then you can replace it with your own dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First of all you should download your data and place them in the data folder,
    by naming the two folders “input” and “ground_truth” which contains the segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the rest of the tutorial I will use the MindBoggle dataset for image segmentation.
    You can download it on this [Google Drive](https://drive.google.com/drive/folders/1rQeujQZs5rPH1AeI-KoUICOAzs4u-Eie?usp=sharing):'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are given 3D MRI scans of the Brain and we want to segment the White and
    Gray matter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/105b1c3c613a1f97eca045e11b3bac52.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'It should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/08fad5b409f9fd75a6221ba5f8318385.png)'
  prefs: []
  type: TYPE_IMG
- en: Tree, Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the main directory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you run this on Google Colab, set collab = True, otherwise collab = False
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we are going to define a function that creates folders for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: And we use this function to create our “my_nnunet” folder where everything is
    going to be saved
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Library installation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we are going to install all the requirements. First let’s install the nnunet
    library. If you are in a notebook run this in a cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Otherwise you can install nnunet directly from the terminal with
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Now we are going to clone the nnUnet git repository and NVIDIA apex. This contains
    the training scripts as well as a GPU accelerator.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Creation of the folders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: nnUnet requires a very specific structure for the folders.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Originally the nnU-Net was designed for a decathlon challenge with different
    tasks. If you have different tasks just run this cell for all your tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'You should have a structure like that now:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/90cb0cd8c344fe8b30fade3ab7cdc037.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Setting the enironment variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The script needs to know where you put your raw_data, where it can find the
    preprocessed data, and where it had to save the results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Move the files in the right repositories:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We define a function that will move our images to the right repositories in
    the nnunet folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s run this function for the input and ground truth images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now we have to rename the files to be accepted by the nnUnet format, for example
    subject.nii.gz will become subject_0000.nii.gz
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Setting up the JSON file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are almost done!
  prefs: []
  type: TYPE_NORMAL
- en: 'You mostly need to modify 2 things:'
  prefs: []
  type: TYPE_NORMAL
- en: The Modality (if its CT or MRI this changes the normalization)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The labels: Enter your own classes'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Preprocess the data for nnU-Net format
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This creates the dataset for the nnU-Net format
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Train the models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are now ready to train the models!
  prefs: []
  type: TYPE_NORMAL
- en: 'To train the 3D U-Net:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'To train the 2D U-Net:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'To train the cascade model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Note: If you pause the traning and want to resume it, add a “-c” in the end
    for “continue”.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we can run the inference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Visualization of the predictions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First let’s check the training loss. This looks very healthy, and we have a
    Dice Score > 0.9 (green curve).
  prefs: []
  type: TYPE_NORMAL
- en: This is truly excellent for so little work and a 3D Neuroimaging segmentation
    task.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c211daa238ad5b5ee4c518148ee9fb4b.png)'
  prefs: []
  type: TYPE_IMG
- en: Training loss, test loss, validation Dice, Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at one sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c6a69656aceafc62f4945052409777de.png)'
  prefs: []
  type: TYPE_IMG
- en: Prediction on the MindBoggle dataset, Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The results are indeed impressive! It’s clear that the model has effectively
    learned how to segment brain images with high accuracy. While there may be minor
    imperfections, it’s important to remember that the field of image segmentation
    is advancing rapidly, and we’re making significant strides towards perfection.
  prefs: []
  type: TYPE_NORMAL
- en: In the future, there’s scope to further optimize the performance of nnU-Net,
    but that will be for an other article
  prefs: []
  type: TYPE_NORMAL
- en: 'Thanks for reading! Before you go:'
  prefs: []
  type: TYPE_NORMAL
- en: Check my [compilation of AI tutorials](https://github.com/FrancoisPorcher/awesome-ai-tutorials)
    on Github
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----6dda7f44b935--------------------------------)
    [## GitHub - FrancoisPorcher/awesome-ai-tutorials: The best collection of AI tutorials
    to make you a…'
  prefs: []
  type: TYPE_NORMAL
- en: The best collection of AI tutorials to make you a boss of Data Science! - GitHub
    …
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----6dda7f44b935--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Y*ou should get my articles in your inbox.* [***Subscribe here.***](https://medium.com/@francoisporcher/subscribe)
  prefs: []
  type: TYPE_NORMAL
- en: '*If you want to have access to premium articles on Medium, you only need a
    membership for $5 a month. If you sign up* [***with my link***](https://medium.com/@francoisporcher/membership)*,
    you support me with a part of your fee without additional costs.*'
  prefs: []
  type: TYPE_NORMAL
- en: If you found this article insightful and beneficial, please consider following
    me and leaving a clap for more in-depth content! Your support helps me continue
    producing content that aids our collective understanding.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Ronneberger, O., Fischer, P., & Brox, T. (2015). U-net: Convolutional networks
    for biomedical image segmentation. In International Conference on Medical image
    computing and computer-assisted intervention (pp. 234–241). Springer, Cham.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H.
    (2021). nnU-Net: a self-configuring method for deep learning-based biomedical
    image segmentation. Nature Methods, 18(2), 203–211.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Ioffe, S., & Szegedy, C. (2015). Batch normalization: Accelerating deep network
    training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Ulyanov, D., Vedaldi, A., & Lempitsky, V. (2016). Instance normalization: The
    missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[MindBoggle dataset](https://mindboggle.info)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
