# 使用 Q-learning 的强化学习中行动的价值

> 原文：[https://towardsdatascience.com/the-values-of-actions-in-reinforcement-learning-using-q-learning-cb4b03be5c81](https://towardsdatascience.com/the-values-of-actions-in-reinforcement-learning-using-q-learning-cb4b03be5c81)

## 使用 Python 从头实现的 Q-learning 算法

[](https://eligijus-bujokas.medium.com/?source=post_page-----cb4b03be5c81--------------------------------)[![Eligijus Bujokas](../Images/061fd30136caea2ba927140e8b3fae3c.png)](https://eligijus-bujokas.medium.com/?source=post_page-----cb4b03be5c81--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cb4b03be5c81--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cb4b03be5c81--------------------------------) [Eligijus Bujokas](https://eligijus-bujokas.medium.com/?source=post_page-----cb4b03be5c81--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cb4b03be5c81--------------------------------) ·阅读时间 10 分钟·2023年2月14日

--

![](../Images/a7047cb8a1a1248a35a9aac5acfc6a95.png)

代理在迷宫中穿越；GIF 由作者提供

这篇文章是关于强化学习（RL）系列文章的续集。请在这里查看其他文章：

[](/first-steps-in-the-world-of-reinforcement-learning-using-python-b843b76538e3?source=post_page-----cb4b03be5c81--------------------------------) [## 使用 Python 进行强化学习的第一步

### 如何在强化学习的基本世界之一中找到最佳位置的原始 Python 实现…

towardsdatascience.com](/first-steps-in-the-world-of-reinforcement-learning-using-python-b843b76538e3?source=post_page-----cb4b03be5c81--------------------------------) [](/temporal-differences-with-python-first-sample-based-reinforcement-learning-algorithm-54c11745a0ee?source=post_page-----cb4b03be5c81--------------------------------) [## 使用 Python 的时间差分 — 第一个基于样本的强化学习算法

### 使用 Python 编写和理解 TD(0) 算法

towardsdatascience.com](/temporal-differences-with-python-first-sample-based-reinforcement-learning-algorithm-54c11745a0ee?source=post_page-----cb4b03be5c81--------------------------------)

所有使用的代码可以在这里查看：[https://github.com/Eligijus112/rl-snake-game](https://github.com/Eligijus112/rl-snake-game)

包含所有绘图函数和代理训练代码的笔记本可以在这里查看：[https://github.com/Eligijus112/rl-snake-game/blob/master/chapter-6-qlearning.ipynb](https://github.com/Eligijus112/rl-snake-game/blob/master/chapter-6-qlearning.ipynb)

在这篇文章中，我将向读者介绍 Q 值的概念。为了直观起见，读者可以将 ***Q*** 替换为 ***Quality***。q 值是数值，分配给 *每个动作* 从 *每个状态* 中的得分：

![](../Images/2b4d4a26701bab505fe9e19612648c7b.png)

Q 值函数

在给定状态下，某个特定动作的 ***得分越高***，代理执行该动作的 ***效果越好***。

例如，如果我们可以从状态 1 选择左移或右移，那么如果

Q(left, 1) = 3.187

Q(right, 1) = 6.588

那么从状态 1 出发，能够带来更多价值的更好动作就是“正确”的动作。

存储 q 值的对象是**q-table**。**q-table** 是一个矩阵，每一行是一个状态，每一列是一个动作。我们将这种矩阵称为**Q**。

从之前的文章中，让我们回顾一下 Q 学习中需要的一些其他重要表格：

**S** — 状态矩阵，用于索引所有状态。

**R** — 奖励矩阵，指示过渡到给定状态时获得的奖励。

Q 学习中不需要价值函数 **V**，因为我们不仅关心状态的价值，还关心状态-动作对的价值。

想象一下我们有以下 48 状态的迷宫：

![](../Images/f9dfef88ec1dba05707a355349dbcc43.png)

迷宫；作者拍摄

黄色状态是我们代理的起始状态（状态 1）。

绿色状态是目标状态（状态 38）。

红色状态是迷宫的墙壁。如果代理选择去墙壁状态，它将返回到最后一个状态且不获得奖励。离开边界的逻辑也适用。

我们的代理可以采取的动作由向量 ***[0, 1, 2, 3]*** 表示，对应于 ***[上, 下, 左, 右]***。

该代理的初始 Q 表如下：

![](../Images/82a9aef4116ecffbbd3ce6a7ef8489d2.png)

48x4 矩阵；作者拍摄

有 48 行表示每个状态。

有 4 列表示代理在每一步可以采取的 4 种动作：**上、下、左或右**。

Q 学习算法的主要目标是 ***填充上述矩阵，以便我们的代理学习迷宫中最优的路径***。

我们将使用自定义的 Agent 类来实现 Q 学习算法：

[PRE0]

完整的 Q 学习算法如下¹

![](../Images/6f06f73ab24689b9ed6c13be1bf122fe.png)

Q 学习算法；作者拍摄

在步骤 2.2.1 中的 *epsilon-贪婪策略* 是以 **1 - epsilon** 的概率采取 Q 值最大的动作，并以 **epsilon** 的概率采取随机动作。

上述策略通过以下代码在我们的代理中实现：

[PRE1]

Q学习步骤是2.2.3步骤。在每个状态下，我们的智能体采取一个动作。然后，通过更新智能体所在的当前状态和状态-动作对**Q(S, A)**来完成学习。更新规则中最重要的部分是，我们查看智能体通过采取动作最终到达的状态，然后从Q表中提取该状态的最大值。

让我们更仔细地检查这个方程：

![](../Images/d4c1790c349a17f6de2449ca222aeae4.png)

Q值更新方程

Q(S, A)是智能体所在的状态以及他采取的动作。

这

![](../Images/c54e2ad90bcb0706f3ac62538ad70e30.png)

转换状态的最大值

部分是我们智能体在所有动作中最终到达的状态的最大可用Q值。

**r** 是过渡到给定状态的奖励。

其他所有的是用户定义的超参数。

由于我们使用算法估计值来更新Q值，因此Q学习属于自举方法家族。

**每次我们的智能体移动后，Q表都会更新。**

我们在Agent类中2.2.3步骤的完整实现如下：

[PRE2]

上述函数在每次智能体移动时都会被调用：

[PRE3]

上面的代码片段应从下往上阅读。

在每次移动时，我们检查是否处于终止状态。如果智能体进入终止状态，Q学习更新方程简化为：

![](../Images/7afb9a74c054d2c27ad8a054c7fd3213.png)

终止状态的更新；作者提供的照片

让我们启动我们的智能体，训练一个回合并可视化智能体路径：

[PRE4]

[PRE5]

![](../Images/4a09adfdb8cb772670b0b578b9854876.png)

智能体在迷宫中游荡；作者提供的GIF

智能体需要94步才能到达目标。在每一步，智能体以epsilon贪婪的方式选择一个动作。在第一次迭代中，任何转换状态的Q值都是0，因此epsilon贪婪算法与随机游荡相同。

让我们在一个回合后检查Q表。Q表中所有的值都是零，只有状态30的值例外：

[PRE6]

Q(30, 1)（意味着从状态30向“下”移动）值为1。计算这个值的方程是：

![](../Images/5c5ed0da49d4f2b172edcf94a326e338.png)

请记住，初始**Q(30, 1) = 0.**

在一个回合后，我们只学到了一个Q值。让我们再训练一个回合：

[PRE7]

现在智能体从迷宫的另一边游荡，学到从状态37向右移动是最佳选择。

我们希望看到的是，随着回合的进行，我们的智能体的步骤数开始减少。

[PRE8]

![](../Images/2f7485898a9f88a1e1be9e0370882db2.png)

步骤数与回合数的关系；作者提供的照片

在初步探索后，到第20集时，代理拥有一个稳定的策略，并需要大约10步从起始位置到达终点位置。变异发生是因为我们仍在使用epsilon贪婪算法进行移动，而10%的时间会选择随机动作。

现在，最佳贪婪策略如下所示：

![](../Images/1dc927e5f10104e35e0b50e3ee964c8e.png)

最优策略；作者拍摄的照片。

此外，我们的代理跟踪它在任何给定状态下出现的次数。我们可以绘制这些数据，以查看在训练阶段哪些状态最受欢迎：

![](../Images/0279e5ef68680c61a2cd3f49bc3960e3.png)

状态访问；作者拍摄的照片

我们可以看到，代理经常在起始状态的左右徘徊。但是，由于我们让代理只有10%的时间采取随机动作，因此主要路径是贪婪的，也就是说，代理选择具有最大Q值的动作。

最后，我们可以绘制最终的代理遍历路径：

![](../Images/a7047cb8a1a1248a35a9aac5acfc6a95.png)

代理遍历；作者制作的GIF

总结：

+   Q学习算法在代理执行每个动作后更新Q表中的值。

+   Q学习是一种自举算法，因为它使用自己的估计来更新Q值。

+   在Q学习中，我们只需要状态、奖励和Q表来实现整个算法。

+   Q学习中的主要更新规则是：

![](../Images/d4c1790c349a17f6de2449ca222aeae4.png)

Q值更新方程

编程愉快，学习愉快！

[1]

+   作者：**理查德·S·萨顿，安德鲁·G·巴托**

+   年份：**2018**

+   页码：**131**

+   标题：**强化学习：导论**

+   网址：[**http://archive.ics.uci.edu/ml**](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)
