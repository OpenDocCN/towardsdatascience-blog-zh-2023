- en: 'XGBoost: Intro, Step-by-Step Implementation, and Performance Comparison'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XGBoost：简介、逐步实现和性能比较
- en: 原文：[https://towardsdatascience.com/xgboost-intro-step-by-step-implementation-and-performance-comparison-6018dfa212f3](https://towardsdatascience.com/xgboost-intro-step-by-step-implementation-and-performance-comparison-6018dfa212f3)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/xgboost-intro-step-by-step-implementation-and-performance-comparison-6018dfa212f3](https://towardsdatascience.com/xgboost-intro-step-by-step-implementation-and-performance-comparison-6018dfa212f3)
- en: 'Extreme Gradient Boosting: A quick and reliable regressor and classifier'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 极端梯度提升：快速可靠的回归器和分类器
- en: '[](https://medium.com/@fmnobar?source=post_page-----6018dfa212f3--------------------------------)[![Farzad
    Mahmoodinobar](../Images/2d75209693b712300e6f0796bd2487d0.png)](https://medium.com/@fmnobar?source=post_page-----6018dfa212f3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6018dfa212f3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6018dfa212f3--------------------------------)
    [Farzad Mahmoodinobar](https://medium.com/@fmnobar?source=post_page-----6018dfa212f3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@fmnobar?source=post_page-----6018dfa212f3--------------------------------)[![Farzad
    Mahmoodinobar](../Images/2d75209693b712300e6f0796bd2487d0.png)](https://medium.com/@fmnobar?source=post_page-----6018dfa212f3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6018dfa212f3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6018dfa212f3--------------------------------)
    [Farzad Mahmoodinobar](https://medium.com/@fmnobar?source=post_page-----6018dfa212f3--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6018dfa212f3--------------------------------)
    ·12 min read·Sep 29, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6018dfa212f3--------------------------------)
    ·12 分钟阅读·2023年9月29日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/fcc4ec2ab542f20d005332a0413e63d0.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fcc4ec2ab542f20d005332a0413e63d0.png)'
- en: Photo by [Sam Moghadam Khamseh](https://unsplash.com/@sammoghadamkhamseh?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/baII27W6z7k?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 [Sam Moghadam Khamseh](https://unsplash.com/@sammoghadamkhamseh?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    于 [Unsplash](https://unsplash.com/photos/baII27W6z7k?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: XGBoost has become one of the most popular well-rounded regressors and/or classifiers
    for all machine learning practitioners. If you ask a data scientist what model
    they would use for an unknown task, without any other information, odds are they
    will choose XGBoost given the vast types of use cases it can be applied to — it
    is quick, reliable, versatile and very easy to implement.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 已成为所有机器学习从业者中最受欢迎的全能回归器和/或分类器之一。如果你问数据科学家在不知道其他信息的情况下，他们会选择哪个模型，很可能他们会选择
    XGBoost，因为它可以应用于各种类型的用例——它快速、可靠、多功能且非常容易实现。
- en: Today, we will conceptually review XGBoost, walk through the implementation
    of it and compare its performance to several other models for a classification
    task.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，我们将从概念上回顾 XGBoost，讲解其实现过程，并将其性能与几种其他模型在分类任务中的表现进行比较。
- en: Let’s get started!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 开始吧！
- en: 1\. XGBoost — Conceptual Overview
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. XGBoost — 概念概述
- en: XGBoost stands for Extreme Gradient Boosting. It is a gradient boosting decision
    tree type of a model, that can be used both for supervised regression and classification
    tasks. We used a few terms to define XGBoost so let’s walk through them one by
    one to better understand them. I will provide a brief overview of each concept
    and will include links to other posts with more details for those interested in
    a deeper discussion.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 代表极端梯度提升。它是一种梯度提升决策树模型，既可用于监督回归任务，也可用于分类任务。我们使用了几个术语来定义 XGBoost，因此让我们逐一了解它们，以便更好地理解。我将简要概述每个概念，并提供更多细节的链接，供那些对深入讨论感兴趣的读者参考。
- en: '**Supervised vs. Unsupervised Learning:** Supervised is when we use labled
    data to train a machine learning (ML) model. Therefore, the model can learn from
    the labels of the data that we provided. On the other hand, unsupervised learning
    is when our data does not have lables and the model learns the underlying patterns
    in the provided data, in the absence of any labels.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监督学习与非监督学习：** 监督学习是使用带标签的数据来训练机器学习（ML）模型。因此，模型可以从我们提供的数据标签中学习。另一方面，非监督学习是当我们的数据没有标签时，模型在没有任何标签的情况下学习数据中的潜在模式。'
- en: '**Regression vs. Classification:** Regression ML models are used to predict
    a continuous output, while classification models are used to predict a discrete
    output. For a more in-depth discussion, refer to [this post](https://medium.com/@fmnobar/classification-vs-regression-in-machine-learning-which-one-should-i-use-f6b24d251c46).'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归与分类：** 回归机器学习模型用于预测连续输出，而分类模型用于预测离散输出。欲了解更深入的讨论，请参考[这篇文章](https://medium.com/@fmnobar/classification-vs-regression-in-machine-learning-which-one-should-i-use-f6b24d251c46)。'
- en: '**Decision Trees:** Decision Trees are a type of supervised learning algorithm
    that can be used both for regression and classification tasks. For a more in-depth
    discussion, refer to [this post](https://medium.com/@fmnobar/decision-tree-regression-and-classification-modeling-through-14-practice-questions-notebook-46c2053dbcf9).'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**决策树：** 决策树是一种可以用于回归和分类任务的监督学习算法。欲了解更深入的讨论，请参考[这篇文章](https://medium.com/@fmnobar/decision-tree-regression-and-classification-modeling-through-14-practice-questions-notebook-46c2053dbcf9)。'
- en: '**Gradient Boosting:** The idea here is to combine a number of weak learners
    (e.g. models or algorithms), which results in a model that is much stronger collectively.
    In XGBoost, a number of weak decision trees are combined together to create an
    improved learner, collectively.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度提升：** 这里的想法是将若干个弱学习器（例如模型或算法）结合起来，从而得到一个整体上更强的模型。在 XGBoost 中，若干个弱决策树被组合在一起，形成一个改进的学习器。'
- en: Now that we are more familiar with what XGBoost is, let’s see how it works in
    practice! We will first cover some basics and then start the implementation.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对 XGBoost 有了更多的了解，接下来看看它在实践中的表现！我们将首先介绍一些基础知识，然后开始实施。
- en: 2\. Basics
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2. 基础
- en: Our goal is to implement XGBoos and also compare its performance to other algorithms.
    In order to do that, the overall process that we follow here includes training
    multiple machine learning models for a task of classification (and hence referring
    to these models as “classifiers”), followed by evaluation of the performance of
    these classifiers using a set of commonly-used classification metrics. In order
    to implement these steps, we will need to talk about the data set we will use
    for training and testing our models, classifiers (i.e. models or algorithms) that
    we will be comparing to XGBoost and the evaluation metrics used in this exercise
    (we use these metrics to compare various models’ performances), which are covered
    in this section. Once we get the basics out of the way, we will implement our
    models.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是实现 XGBoost，并将其性能与其他算法进行比较。为此，我们的整体过程包括为分类任务训练多个机器学习模型（因此称这些模型为“分类器”），然后使用一组常用的分类指标来评估这些分类器的性能。为了实现这些步骤，我们需要讨论将用于训练和测试我们模型的数据集、将与
    XGBoost 比较的分类器（即模型或算法）以及本练习中使用的评估指标（我们使用这些指标来比较各种模型的性能），这些内容将在本节中介绍。一旦基础知识掌握，我们将实现我们的模型。
- en: 2.1\. Data Set
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1. 数据集
- en: In order to implement and evaluate the performance of XGBoost relative to other
    classifiers, we will use a dataset from [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/dataset/2/adult)
    (CC BY 4.0) that is used to predict whether an individual makes over $50,000 annually.
    The file that is used for the example below can be downloaded [here](https://gist.github.com/fmnobar/992233799dcbd9418f009b0d6c4422ee).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现和评估 XGBoost 相对于其他分类器的性能，我们将使用来自[加州大学欧文分校机器学习库](https://archive.ics.uci.edu/dataset/2/adult)
    (CC BY 4.0) 的数据集，该数据集用于预测一个人是否年收入超过 50,000 美元。以下示例所用的文件可以[在这里](https://gist.github.com/fmnobar/992233799dcbd9418f009b0d6c4422ee)下载。
- en: Let’s look at the top 5 rows of the data set, just to get an overall idea of
    what the data looks like.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看数据集的前 5 行，以便对数据的整体情况有个大致了解。
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Results:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：
- en: '![](../Images/cf5b869b1ae6318c050c0bfd845ec429.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf5b869b1ae6318c050c0bfd845ec429.png)'
- en: The data includes a combination of numerical and categorical values. Let’s take
    a look at some more information about the data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 数据包括数值和分类值的组合。让我们查看更多关于数据的信息。
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Results:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：
- en: '![](../Images/75eefb6ee5d08cf98c3e0b9d6451d750.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75eefb6ee5d08cf98c3e0b9d6451d750.png)'
- en: As we saw before, the data is a combination of integers (`int64`) and categorical
    values (`object`) and there are 32,561 rows across 15 columns (note that column
    number starts at 0 so the total column count is 15). Data type is an important
    factor in determining what classifier to use during our model selection process.
    Now that we are more familiar with our dataset, let’s move on to the models we
    will be training.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所看到的，数据是整数（`int64`）和分类值（`object`）的组合，共有 32,561 行，涵盖 15 列（注意列数从 0 开始，所以总列数为
    15）。数据类型是决定在模型选择过程中使用什么分类器的重要因素。现在我们对数据集有了更多了解，让我们继续探讨将要训练的模型。
- en: 2.2\. Models
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2\. 模型
- en: 'We will use a total of 8 machine learning models in our comparison. A full
    understanding of each is not required for this exercise but I have included a
    brief description of each in this section as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在比较中使用总共 8 个机器学习模型。对于本次练习，不需要对每个模型有全面的了解，但我在本节中包含了每个模型的简要描述，如下所示：
- en: '**AdaBoost** (Adaptive Boosting) is an ensemble boosting technique, which (similar
    to other boosting techniques) improves the classification performance of weak
    classifiers. It is an adaptive one since it continuously adjusts the weights among
    the weak learners to get to a better overall performance.'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**AdaBoost**（自适应提升）是一种集成提升技术，它（类似于其他提升技术）提高了弱分类器的分类性能。它是一种自适应的技术，因为它不断调整弱学习者之间的权重，以获得更好的整体性能。'
- en: '**CatBoost** (Categorical Boosting), as the name suggests, is designed to handle
    categorical features, without the need for prior encoding. This can be particularly
    useful when dealing with a data set that has many categorical features and as
    we saw, our data set also includes some categorical variables so let’s keep an
    eye on this one in the evaluation results.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**CatBoost**（分类提升），顾名思义，旨在处理分类特征，无需预先编码。这在处理具有许多分类特征的数据集时特别有用，正如我们所看到的，我们的数据集也包括一些分类变量，因此在评估结果中请注意这一点。'
- en: '**GradientBoosting** is a somehow simplified gradient boosting classifier (since
    it has fewer hyperparameters to tune, compared to many other Gradient Boosting
    classifiers). This is a good starting point but when we need more advanced hyperparameter
    optimization, we usually go to XGBoost or LightGBM, which we will cover below.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**GradientBoosting** 是一种简化的梯度提升分类器（因为它的超参数较少，相较于许多其他梯度提升分类器）。这是一个很好的起点，但当我们需要更高级的超参数优化时，我们通常会使用
    XGBoost 或 LightGBM，我们将在下面介绍这些内容。'
- en: '**k-NN** (k-Nearest Neighbors) is simple to implement and good for low-dimensional
    data, but can be computationally expensive for large datasets.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**k-NN**（k-最近邻）简单易于实现，适用于低维数据，但对于大数据集可能计算开销较大。'
- en: '**LightGBM** (Light Gradient Boosting Machine) is another gradient boosting
    one. It is more efficient (with respect to memory usage and computational speed)
    compared to XGBoost and therefore is suitable for large datasets or when faster
    training times are needed.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**LightGBM**（轻量级梯度提升机）是另一种梯度提升模型。与 XGBoost 相比，它在内存使用和计算速度方面更高效，因此适合大数据集或需要更快训练时间的情况。'
- en: '**Random Forest** is another very popular ensemble learning method that operates
    by constructing multiple decision trees during training and outputs the mode of
    the classes for classification (it is also a capable model for regression tasks).'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**随机森林** 是另一种非常流行的集成学习方法，它通过在训练期间构建多个决策树来运作，并输出分类的模式（它也是一个能够进行回归任务的模型）。'
- en: '**SVM** (Support Vector Machine) is another model that can be used both for
    classification and regression. While it is effective in high-dimensional spaces,
    but it can be slower than some of the other Gradient Boosting approaches discussed
    here.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**SVM**（支持向量机）是另一种可以用于分类和回归的模型。虽然它在高维空间中有效，但它的速度可能比这里讨论的一些其他梯度提升方法要慢。'
- en: '**XGBoost** (Extreme Gradient Boosting), as discussed, is a gradient boosting
    library designed to be highly efficient and versatile, which makes it suitable
    for a variety of supervised tasks.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**XGBoost**（极端梯度提升），正如前面所讨论的，是一个旨在高效和多功能的梯度提升库，使其适合各种监督任务。'
- en: Now that we covered the models we will be using, let’s talk about the metrics
    we will use to compare our models.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了将要使用的模型，让我们讨论一下将用于比较模型的指标。
- en: 2.3\. Evaluation Metrics
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3\. 评价指标
- en: In order to compare the performance of our various models, we need a benchmark
    or “metric”. Since we will be using our models for a classification tasks, we
    would want to use relevant metrics to measure the performance of classifiers.
    We will evaluate our classifiers across 7 dimensions. The first 6 are usual metrics
    used in evaluating classification tasks and the 7th one is just the amount of
    training time required to train the classifier. Once we see the results, it is
    quite interesting to compare how fast some of these classifiers perform compared
    to others.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较我们各种模型的性能，我们需要一个基准或“指标”。由于我们将使用我们的模型进行分类任务，因此我们希望使用相关指标来衡量分类器的性能。我们将从7个维度来评估我们的分类器。前6个是评估分类任务中常用的指标，第7个只是训练分类器所需的时间。一旦看到结果，比较这些分类器的性能速度就会很有趣。
- en: 'A full understanding of the metrics we use here is not required for this post
    but if you want to better understand these metrics, we would need to define some
    terminology first. In a two-class target variable where the target variable can
    only be positive (or 1) and negative (or 0), there are four possible outcomes
    for a prediction:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本文来说，不需要对我们使用的指标有全面的理解，但如果你想更好地理解这些指标，我们需要首先定义一些术语。在一个二分类目标变量中，目标变量只能是正类（或1）和负类（或0），预测有四种可能的结果：
- en: '**True Positive:** A positive event was correctly predicted.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真正例：** 正事件被正确预测。'
- en: '**False Positive:** A negative event was incorrectly predicted as positive
    (a.k.a. Type I error).'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阳性：** 负事件被错误预测为正（也称为I型错误）。'
- en: '**True Negative:** A negative event was correctly predicted.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真阴性：** 负事件被正确预测。'
- en: '**False Negative:** A positive event was incorrectly predicted as negative
    (a.k.a. Type II error).'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阴性：** 正事件被错误预测为负（也称为II型错误）。'
- en: With the above terminology in mind, let’s look at the metrics.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 记住上述术语，我们来看看指标。
- en: '**Accuracy** is the ratio of correctly predicted observations to the total
    observations in the data set. It is a good metric when looking for the overall
    performance of a model but numbers can be misleading when the data set includes
    imbalanced classes. It is calculated as follows:`Accuracy = (True Positives +
    True Negatives) / Total Observations`'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**准确率**是正确预测的观察值与数据集中总观察值的比例。它是查看模型整体性能的一个好指标，但当数据集包含不平衡类别时，数字可能会具有误导性。它的计算公式如下：`Accuracy
    = (True Positives + True Negatives) / Total Observations`'
- en: '**Precision** is the ratio of correctly predicted positive observations to
    the total predicted positives (a.k.a. Positive Predictive Value). In some classification
    tasks, cost of a false positive is very high and we would opt for a high precision
    model. It is calculated as follows: `Precision = Positive Predictive Value (PPV)
    = True Positives / (True Positives + False Positives)`'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**精确度**是正确预测的正类观察值与总预测正类的比例（也称为正预测值）。在一些分类任务中，假阳性的成本非常高，我们会选择高精确度的模型。它的计算公式如下：`Precision
    = Positive Predictive Value (PPV) = True Positives / (True Positives + False Positives)`'
- en: '**Recall** (or Sensitivity) is the ratio of correctly predicted positive observations
    to all the observations in the positive class (a.k.a. True Positive Rate). Recall
    is particularly important in tasks with a high cost of false negative. It is calculated
    as follows: `Recall = Sensitivity = True Positive Rate (TPR) = True Positives
    / (True Positives + False Negatives)`'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**召回率**（或灵敏度）是正确预测的正类观察值与正类中所有观察值的比例（也称为真正率）。召回率在假阴性成本高的任务中尤为重要。它的计算公式如下：`Recall
    = Sensitivity = True Positive Rate (TPR) = True Positives / (True Positives +
    False Negatives)`'
- en: '**F1-Score** is the weighted average of Precision and Recall, taking both false
    positives and false negatives into account. In situations where we need to consider
    both precision and recall, we usually use F1-Score to consider the trade-off between
    the two. It is calculated as follows:`F1 Score = 2 * (Precision * Recall) / (Precision
    + Recall)`'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**F1-分数**是精确度和召回率的加权平均数，考虑了假阳性和假阴性。在需要考虑精确度和召回率的情况下，我们通常使用F1-分数来权衡二者之间的折衷。它的计算公式如下：`F1
    Score = 2 * (Precision * Recall) / (Precision + Recall)`'
- en: '**AUC-ROC** (Area Under the Receiver Operating Characteristic Curve) represents
    the ability of the model to distinguish between the positive and negative classes.
    This one is less intuitive to understand so let’s talk numbers! ROC curve shows
    the trade-off between True Positive Rate (or Recall that we discussed earlier)
    and False Positive Rate (which is the ratio of negatives that are incorrectly
    identified as positives). An AUC-ROC of 0.5 suggests no discrimination between
    positive and negative classes — in other words, the model is no better than random
    selection between positive and negative classes. A value larger than 0.5 suggests
    some discrimination between the two classes is happening and 1 is the perfect
    discrimination. In short, a higher AUC-ROC indicates a better model performance.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**AUC-ROC**（接收者操作特征曲线下面积）表示模型区分正类和负类的能力。这一点不那么直观，所以我们来谈谈数字！ROC曲线展示了真正率（或我们之前讨论过的召回率）和假正率（即被错误识别为正类的负类比例）之间的权衡。AUC-ROC为0.5表示正类和负类之间没有区分——换句话说，模型没有比随机选择正类和负类更好。大于0.5的值表示两个类别之间有一定的区分，1是完美的区分。简而言之，AUC-ROC越高，模型表现越好。'
- en: '**AUC-PR** (Area Under the Precision-Recall Curve) is similar to AUC-ROC but
    focuses on the performance with respect to the positive class. This becomes more
    important in a data set with imbalanced classes where we care more about the positive
    class.'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**AUC-PR**（精确度-召回率曲线下面积）类似于AUC-ROC，但专注于正类的性能。在数据集存在类别不平衡的情况下，我们更关注正类，这一点就显得尤为重要。'
- en: '**Training Time** is the time it takes to train the model, which we are using
    in this example to see how computationally efficient each of our models are. In
    this example, data set is not large and therefore computational cost of training
    is trivial. In many business examples, we deal with computational limitations
    when it comes to training models and therefore this metric becomes more meaningful.'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练时间**是训练模型所需的时间，我们在此示例中使用它来查看每个模型的计算效率。在这个例子中，数据集不大，因此训练的计算成本微不足道。在许多商业案例中，我们在训练模型时会面临计算限制，因此这个指标变得更加重要。'
- en: With the basics out of the way, let’s start the implementation part!
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 基础知识已经讲解完毕，接下来让我们开始实施部分吧！
- en: 3\. Implementation
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 实施
- en: 'Implementation will include the following steps:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 实施将包括以下步骤：
- en: Import libraries
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库
- en: Load and Preprocess Data
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据加载与预处理
- en: Train and Evaluate Classifiers
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练和评估分类器
- en: Results and discussion
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果与讨论
- en: 3.1\. Import Libraries
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1\. 导入库
- en: We will start by importing various libraries used in our exercise. Feel free
    to copy and paste the code blocks and follow along.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从导入在练习中使用的各种库开始。随意复制和粘贴代码块并跟随操作。
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 3.2\. Load and Preprocess Data
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2\. 数据加载与预处理
- en: Now that we have the required libraries imported, let’s define a function to
    read the data set that we will use for training our classification models. I have
    wrapped this part in a function named `load_and_preprocess_data`, which reads
    the file and breaks it down into features and target that will be used in training.
    I have added comments in the code so that you can more easily follow along.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经导入了所需的库，让我们定义一个函数来读取我们将用于训练分类模型的数据集。我将这一部分封装在一个名为`load_and_preprocess_data`的函数中，该函数读取文件并将其分解为用于训练的特征和目标。我在代码中添加了注释，以便你更容易跟随。
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 3.3\. Train and Evaluate Classifiers
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3\. 训练和评估分类器
- en: In this step, we will define a function to train a classifier and evaluate its
    performance, when we provide the train and test sets. We will break down our data
    into train and test sets later so don’t worry about having that right now. Similar
    to before, I have added comments to the code to help with readability.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，我们将定义一个函数来训练分类器并评估其性能，当我们提供训练集和测试集时。我们稍后会将数据拆分成训练集和测试集，所以现在不必担心。和以前一样，我在代码中添加了注释以帮助阅读。
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 3.4\. Results and Discussion
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4\. 结果与讨论
- en: 'Finally, we put everything together inside a main function, which calls the
    previous functions to read the data, train the classifiers, evaluate the trained
    models and then return the results in the form of a dataframe, as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将所有内容放在一个主函数中，该函数调用之前的函数来读取数据、训练分类器、评估训练好的模型，然后将结果以数据框的形式返回，如下所示：
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now that all the calculations are done, let’s take a look at the results!
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在所有计算都完成了，让我们来看看结果吧！
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Results:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：
- en: '![](../Images/67e916c87f40ded79681a7aff04e84bb.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67e916c87f40ded79681a7aff04e84bb.png)'
- en: As discussed in the evaluation metrics section, deciding which metric works
    best depends on the goal of project. For the sake of discussion, we are going
    to assume in this problem we value precision and recall equally and therefore
    discuss which classifier gives us a balanced measure of the two. As a result,
    I am going to rely mostly on F1-Score and AUC-ROC as the two metrics to look at
    for this exercise. As you recall, F1-Score is a harmonic mean of both precision
    and recall (and hence is a balanced measure between the two) and AUC-ROC is the
    area under the Receiver Operating Characteristic curve, which helps us understand
    how well the classifier can distinguish between positive and negative cases.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如评估指标部分所讨论的，决定哪种指标最有效取决于项目的目标。为了讨论的方便，我们在这个问题中假设我们同等重视精确度和召回率，因此讨论哪个分类器能提供这两者的平衡度量。因此，我将主要依赖F1-Score和AUC-ROC作为这个练习中需要关注的两个指标。回顾一下，F1-Score是精确度和召回率的调和均值（因此是两者的平衡度量），而AUC-ROC是接收者操作特征曲线下的面积，它帮助我们理解分类器在区分正负案例方面的表现。
- en: Given the above assumptions, let’s sort the results and look at the evaluation
    results one more time.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述假设，让我们对结果进行排序，并再次查看评估结果。
- en: '[PRE7]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Results:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：
- en: '![](../Images/eac74d2c7934a39561c886292e608719.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eac74d2c7934a39561c886292e608719.png)'
- en: Results are quite interesting. As you can see CatBoost, LightGBM and XGBoost
    are the top 3 models with very close levels of performance and all three are boosted
    models. CatBoost was our top performer, with a minor F1-Score edge over LightGBM.
    One reason could be the presence of categorical variables in the data set. XGBoost
    is a close third to the other two. Another interesting observation is the training
    time required for each. We see that LightGBM performed almost as well as CatBoost
    but at a fraction of CatBoost’s training time. In fact, LightGBM was almost 96%
    faster than CatBoost. Training times are quite trivial in our example but in larger
    data sets, which occur frequently in business and academic settings, we would
    look more closely at the trade-off between the level of performance and the time
    required for training the model, which could result in selection of LightGBM over
    CatBoost.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 结果相当有趣。正如你所见，CatBoost、LightGBM 和 XGBoost 是性能非常接近的前三名模型，这三者都是提升模型。CatBoost 是我们的最佳表现者，F1-Score
    比 LightGBM 高一点。一个原因可能是数据集中存在类别变量。XGBoost 是接近第二名的第三名。另一个有趣的观察是每个模型所需的训练时间。我们看到
    LightGBM 的表现几乎与 CatBoost 相当，但训练时间却远低于 CatBoost。实际上，LightGBM 的速度比 CatBoost 快了将近96%。在我们的示例中，训练时间相对微不足道，但在商业和学术环境中经常出现的大型数据集中，我们会更加关注性能水平与训练时间之间的权衡，这可能导致选择
    LightGBM 而非 CatBoost。
- en: 4\. Conclusion
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4. 结论
- en: In this post, we introduced XGBoost, a powerful machine learning algorithm that
    is widely used among machine learning practitioners. We then walked through other
    classifiers and classification evaluation metrics that can be used to compare
    the performance of various classifiers. Finally, we implemented XGBoost among
    other classifiers using a data set and compared and discussed their performance
    levels and trade-offs.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们介绍了 XGBoost，一种在机器学习实践者中广泛使用的强大机器学习算法。然后我们讨论了其他分类器和分类评估指标，这些指标可用于比较不同分类器的性能。最后，我们使用数据集实施了
    XGBoost 及其他分类器，并比较和讨论了它们的性能水平和权衡。
- en: Thanks for Reading!
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: If you found this post helpful, please [follow me on Medium](https://medium.com/@fmnobar)
    and subscribe to receive my latest posts!
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你觉得这篇文章有帮助，请[在 Medium 上关注我](https://medium.com/@fmnobar)并订阅以接收我最新的文章！
- en: '*(All images, unless otherwise noted, are by the author.)*'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*(所有图片，除非另有说明，均为作者提供。)*'
