- en: 'XGBoost: Intro, Step-by-Step Implementation, and Performance Comparison'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/xgboost-intro-step-by-step-implementation-and-performance-comparison-6018dfa212f3](https://towardsdatascience.com/xgboost-intro-step-by-step-implementation-and-performance-comparison-6018dfa212f3)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Extreme Gradient Boosting: A quick and reliable regressor and classifier'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@fmnobar?source=post_page-----6018dfa212f3--------------------------------)[![Farzad
    Mahmoodinobar](../Images/2d75209693b712300e6f0796bd2487d0.png)](https://medium.com/@fmnobar?source=post_page-----6018dfa212f3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6018dfa212f3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6018dfa212f3--------------------------------)
    [Farzad Mahmoodinobar](https://medium.com/@fmnobar?source=post_page-----6018dfa212f3--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6018dfa212f3--------------------------------)
    ·12 min read·Sep 29, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fcc4ec2ab542f20d005332a0413e63d0.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Sam Moghadam Khamseh](https://unsplash.com/@sammoghadamkhamseh?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/baII27W6z7k?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost has become one of the most popular well-rounded regressors and/or classifiers
    for all machine learning practitioners. If you ask a data scientist what model
    they would use for an unknown task, without any other information, odds are they
    will choose XGBoost given the vast types of use cases it can be applied to — it
    is quick, reliable, versatile and very easy to implement.
  prefs: []
  type: TYPE_NORMAL
- en: Today, we will conceptually review XGBoost, walk through the implementation
    of it and compare its performance to several other models for a classification
    task.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: 1\. XGBoost — Conceptual Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: XGBoost stands for Extreme Gradient Boosting. It is a gradient boosting decision
    tree type of a model, that can be used both for supervised regression and classification
    tasks. We used a few terms to define XGBoost so let’s walk through them one by
    one to better understand them. I will provide a brief overview of each concept
    and will include links to other posts with more details for those interested in
    a deeper discussion.
  prefs: []
  type: TYPE_NORMAL
- en: '**Supervised vs. Unsupervised Learning:** Supervised is when we use labled
    data to train a machine learning (ML) model. Therefore, the model can learn from
    the labels of the data that we provided. On the other hand, unsupervised learning
    is when our data does not have lables and the model learns the underlying patterns
    in the provided data, in the absence of any labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regression vs. Classification:** Regression ML models are used to predict
    a continuous output, while classification models are used to predict a discrete
    output. For a more in-depth discussion, refer to [this post](https://medium.com/@fmnobar/classification-vs-regression-in-machine-learning-which-one-should-i-use-f6b24d251c46).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decision Trees:** Decision Trees are a type of supervised learning algorithm
    that can be used both for regression and classification tasks. For a more in-depth
    discussion, refer to [this post](https://medium.com/@fmnobar/decision-tree-regression-and-classification-modeling-through-14-practice-questions-notebook-46c2053dbcf9).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gradient Boosting:** The idea here is to combine a number of weak learners
    (e.g. models or algorithms), which results in a model that is much stronger collectively.
    In XGBoost, a number of weak decision trees are combined together to create an
    improved learner, collectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we are more familiar with what XGBoost is, let’s see how it works in
    practice! We will first cover some basics and then start the implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our goal is to implement XGBoos and also compare its performance to other algorithms.
    In order to do that, the overall process that we follow here includes training
    multiple machine learning models for a task of classification (and hence referring
    to these models as “classifiers”), followed by evaluation of the performance of
    these classifiers using a set of commonly-used classification metrics. In order
    to implement these steps, we will need to talk about the data set we will use
    for training and testing our models, classifiers (i.e. models or algorithms) that
    we will be comparing to XGBoost and the evaluation metrics used in this exercise
    (we use these metrics to compare various models’ performances), which are covered
    in this section. Once we get the basics out of the way, we will implement our
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1\. Data Set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to implement and evaluate the performance of XGBoost relative to other
    classifiers, we will use a dataset from [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/dataset/2/adult)
    (CC BY 4.0) that is used to predict whether an individual makes over $50,000 annually.
    The file that is used for the example below can be downloaded [here](https://gist.github.com/fmnobar/992233799dcbd9418f009b0d6c4422ee).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at the top 5 rows of the data set, just to get an overall idea of
    what the data looks like.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cf5b869b1ae6318c050c0bfd845ec429.png)'
  prefs: []
  type: TYPE_IMG
- en: The data includes a combination of numerical and categorical values. Let’s take
    a look at some more information about the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/75eefb6ee5d08cf98c3e0b9d6451d750.png)'
  prefs: []
  type: TYPE_IMG
- en: As we saw before, the data is a combination of integers (`int64`) and categorical
    values (`object`) and there are 32,561 rows across 15 columns (note that column
    number starts at 0 so the total column count is 15). Data type is an important
    factor in determining what classifier to use during our model selection process.
    Now that we are more familiar with our dataset, let’s move on to the models we
    will be training.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use a total of 8 machine learning models in our comparison. A full
    understanding of each is not required for this exercise but I have included a
    brief description of each in this section as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**AdaBoost** (Adaptive Boosting) is an ensemble boosting technique, which (similar
    to other boosting techniques) improves the classification performance of weak
    classifiers. It is an adaptive one since it continuously adjusts the weights among
    the weak learners to get to a better overall performance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**CatBoost** (Categorical Boosting), as the name suggests, is designed to handle
    categorical features, without the need for prior encoding. This can be particularly
    useful when dealing with a data set that has many categorical features and as
    we saw, our data set also includes some categorical variables so let’s keep an
    eye on this one in the evaluation results.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**GradientBoosting** is a somehow simplified gradient boosting classifier (since
    it has fewer hyperparameters to tune, compared to many other Gradient Boosting
    classifiers). This is a good starting point but when we need more advanced hyperparameter
    optimization, we usually go to XGBoost or LightGBM, which we will cover below.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**k-NN** (k-Nearest Neighbors) is simple to implement and good for low-dimensional
    data, but can be computationally expensive for large datasets.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**LightGBM** (Light Gradient Boosting Machine) is another gradient boosting
    one. It is more efficient (with respect to memory usage and computational speed)
    compared to XGBoost and therefore is suitable for large datasets or when faster
    training times are needed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Random Forest** is another very popular ensemble learning method that operates
    by constructing multiple decision trees during training and outputs the mode of
    the classes for classification (it is also a capable model for regression tasks).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**SVM** (Support Vector Machine) is another model that can be used both for
    classification and regression. While it is effective in high-dimensional spaces,
    but it can be slower than some of the other Gradient Boosting approaches discussed
    here.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**XGBoost** (Extreme Gradient Boosting), as discussed, is a gradient boosting
    library designed to be highly efficient and versatile, which makes it suitable
    for a variety of supervised tasks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we covered the models we will be using, let’s talk about the metrics
    we will use to compare our models.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3\. Evaluation Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to compare the performance of our various models, we need a benchmark
    or “metric”. Since we will be using our models for a classification tasks, we
    would want to use relevant metrics to measure the performance of classifiers.
    We will evaluate our classifiers across 7 dimensions. The first 6 are usual metrics
    used in evaluating classification tasks and the 7th one is just the amount of
    training time required to train the classifier. Once we see the results, it is
    quite interesting to compare how fast some of these classifiers perform compared
    to others.
  prefs: []
  type: TYPE_NORMAL
- en: 'A full understanding of the metrics we use here is not required for this post
    but if you want to better understand these metrics, we would need to define some
    terminology first. In a two-class target variable where the target variable can
    only be positive (or 1) and negative (or 0), there are four possible outcomes
    for a prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '**True Positive:** A positive event was correctly predicted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Positive:** A negative event was incorrectly predicted as positive
    (a.k.a. Type I error).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True Negative:** A negative event was correctly predicted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Negative:** A positive event was incorrectly predicted as negative
    (a.k.a. Type II error).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the above terminology in mind, let’s look at the metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy** is the ratio of correctly predicted observations to the total
    observations in the data set. It is a good metric when looking for the overall
    performance of a model but numbers can be misleading when the data set includes
    imbalanced classes. It is calculated as follows:`Accuracy = (True Positives +
    True Negatives) / Total Observations`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Precision** is the ratio of correctly predicted positive observations to
    the total predicted positives (a.k.a. Positive Predictive Value). In some classification
    tasks, cost of a false positive is very high and we would opt for a high precision
    model. It is calculated as follows: `Precision = Positive Predictive Value (PPV)
    = True Positives / (True Positives + False Positives)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Recall** (or Sensitivity) is the ratio of correctly predicted positive observations
    to all the observations in the positive class (a.k.a. True Positive Rate). Recall
    is particularly important in tasks with a high cost of false negative. It is calculated
    as follows: `Recall = Sensitivity = True Positive Rate (TPR) = True Positives
    / (True Positives + False Negatives)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**F1-Score** is the weighted average of Precision and Recall, taking both false
    positives and false negatives into account. In situations where we need to consider
    both precision and recall, we usually use F1-Score to consider the trade-off between
    the two. It is calculated as follows:`F1 Score = 2 * (Precision * Recall) / (Precision
    + Recall)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**AUC-ROC** (Area Under the Receiver Operating Characteristic Curve) represents
    the ability of the model to distinguish between the positive and negative classes.
    This one is less intuitive to understand so let’s talk numbers! ROC curve shows
    the trade-off between True Positive Rate (or Recall that we discussed earlier)
    and False Positive Rate (which is the ratio of negatives that are incorrectly
    identified as positives). An AUC-ROC of 0.5 suggests no discrimination between
    positive and negative classes — in other words, the model is no better than random
    selection between positive and negative classes. A value larger than 0.5 suggests
    some discrimination between the two classes is happening and 1 is the perfect
    discrimination. In short, a higher AUC-ROC indicates a better model performance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**AUC-PR** (Area Under the Precision-Recall Curve) is similar to AUC-ROC but
    focuses on the performance with respect to the positive class. This becomes more
    important in a data set with imbalanced classes where we care more about the positive
    class.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Training Time** is the time it takes to train the model, which we are using
    in this example to see how computationally efficient each of our models are. In
    this example, data set is not large and therefore computational cost of training
    is trivial. In many business examples, we deal with computational limitations
    when it comes to training models and therefore this metric becomes more meaningful.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With the basics out of the way, let’s start the implementation part!
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Implementation will include the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Import libraries
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load and Preprocess Data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train and Evaluate Classifiers
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Results and discussion
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 3.1\. Import Libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will start by importing various libraries used in our exercise. Feel free
    to copy and paste the code blocks and follow along.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 3.2\. Load and Preprocess Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have the required libraries imported, let’s define a function to
    read the data set that we will use for training our classification models. I have
    wrapped this part in a function named `load_and_preprocess_data`, which reads
    the file and breaks it down into features and target that will be used in training.
    I have added comments in the code so that you can more easily follow along.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 3.3\. Train and Evaluate Classifiers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this step, we will define a function to train a classifier and evaluate its
    performance, when we provide the train and test sets. We will break down our data
    into train and test sets later so don’t worry about having that right now. Similar
    to before, I have added comments to the code to help with readability.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 3.4\. Results and Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Finally, we put everything together inside a main function, which calls the
    previous functions to read the data, train the classifiers, evaluate the trained
    models and then return the results in the form of a dataframe, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now that all the calculations are done, let’s take a look at the results!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/67e916c87f40ded79681a7aff04e84bb.png)'
  prefs: []
  type: TYPE_IMG
- en: As discussed in the evaluation metrics section, deciding which metric works
    best depends on the goal of project. For the sake of discussion, we are going
    to assume in this problem we value precision and recall equally and therefore
    discuss which classifier gives us a balanced measure of the two. As a result,
    I am going to rely mostly on F1-Score and AUC-ROC as the two metrics to look at
    for this exercise. As you recall, F1-Score is a harmonic mean of both precision
    and recall (and hence is a balanced measure between the two) and AUC-ROC is the
    area under the Receiver Operating Characteristic curve, which helps us understand
    how well the classifier can distinguish between positive and negative cases.
  prefs: []
  type: TYPE_NORMAL
- en: Given the above assumptions, let’s sort the results and look at the evaluation
    results one more time.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eac74d2c7934a39561c886292e608719.png)'
  prefs: []
  type: TYPE_IMG
- en: Results are quite interesting. As you can see CatBoost, LightGBM and XGBoost
    are the top 3 models with very close levels of performance and all three are boosted
    models. CatBoost was our top performer, with a minor F1-Score edge over LightGBM.
    One reason could be the presence of categorical variables in the data set. XGBoost
    is a close third to the other two. Another interesting observation is the training
    time required for each. We see that LightGBM performed almost as well as CatBoost
    but at a fraction of CatBoost’s training time. In fact, LightGBM was almost 96%
    faster than CatBoost. Training times are quite trivial in our example but in larger
    data sets, which occur frequently in business and academic settings, we would
    look more closely at the trade-off between the level of performance and the time
    required for training the model, which could result in selection of LightGBM over
    CatBoost.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this post, we introduced XGBoost, a powerful machine learning algorithm that
    is widely used among machine learning practitioners. We then walked through other
    classifiers and classification evaluation metrics that can be used to compare
    the performance of various classifiers. Finally, we implemented XGBoost among
    other classifiers using a data set and compared and discussed their performance
    levels and trade-offs.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for Reading!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you found this post helpful, please [follow me on Medium](https://medium.com/@fmnobar)
    and subscribe to receive my latest posts!
  prefs: []
  type: TYPE_NORMAL
- en: '*(All images, unless otherwise noted, are by the author.)*'
  prefs: []
  type: TYPE_NORMAL
