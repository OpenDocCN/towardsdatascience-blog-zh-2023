- en: Data Curation Guide for Computer Vision
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算机视觉数据整理指南
- en: 原文：[https://towardsdatascience.com/data-curation-guide-for-computer-vision-acc525f4cd7](https://towardsdatascience.com/data-curation-guide-for-computer-vision-acc525f4cd7)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/data-curation-guide-for-computer-vision-acc525f4cd7](https://towardsdatascience.com/data-curation-guide-for-computer-vision-acc525f4cd7)
- en: '**Data curation in computer vision lacks standardization, leaving many practitioners
    unsure how to do it correctly. We summarized some of the most common approaches.**'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**计算机视觉中的数据整理缺乏标准化，导致许多从业者不确定如何正确进行。我们总结了一些最常见的方法。**'
- en: '[](https://medium.com/@isusmelj?source=post_page-----acc525f4cd7--------------------------------)[![Igor
    Susmelj](../Images/d4ca387826a8f4c730657fb56424e8fd.png)](https://medium.com/@isusmelj?source=post_page-----acc525f4cd7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----acc525f4cd7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----acc525f4cd7--------------------------------)
    [Igor Susmelj](https://medium.com/@isusmelj?source=post_page-----acc525f4cd7--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@isusmelj?source=post_page-----acc525f4cd7--------------------------------)[![Igor
    Susmelj](../Images/d4ca387826a8f4c730657fb56424e8fd.png)](https://medium.com/@isusmelj?source=post_page-----acc525f4cd7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----acc525f4cd7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----acc525f4cd7--------------------------------)
    [伊戈尔·苏斯梅尔](https://medium.com/@isusmelj?source=post_page-----acc525f4cd7--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----acc525f4cd7--------------------------------)
    ·10 min read·Mar 9, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----acc525f4cd7--------------------------------)
    ·阅读时间10分钟·2023年3月9日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f27f307fd92fa83752779cc56ea32a2f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f27f307fd92fa83752779cc56ea32a2f.png)'
- en: 'Where to start with data curation? Picture of Valentin Antonucci: [https://www.pexels.com/de-de/foto/person-die-kompass-halt-691637/](https://www.pexels.com/de-de/foto/person-die-kompass-halt-691637/)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 从哪里开始数据整理？瓦伦丁·安东努奇的图片：[https://www.pexels.com/de-de/foto/person-die-kompass-halt-691637/](https://www.pexels.com/de-de/foto/person-die-kompass-halt-691637/)
- en: This post has been written with data curation for computer vision in mind. However,
    several concepts can be applied to other data domains, such as NLP, audio, or
    tabular data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本文是以计算机视觉中的数据整理为背景编写的。然而，一些概念可以应用于其他数据领域，如自然语言处理、音频或表格数据。
- en: What is Data Curation?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是数据整理？
- en: 'Data curation is a broad term widely used in the industry, especially in [data-centric
    AI](https://datacentricai.org/). It is important to understand the components
    of data curation in the machine learning context. We understand the following
    components under data curation:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 数据整理是一个广泛使用的术语，特别是在[数据驱动的人工智能](https://datacentricai.org/)领域。理解数据整理在机器学习中的组成部分非常重要。我们理解数据整理包括以下几个组成部分：
- en: '**Data cleaning and normalization** — Process of removing “broken” samples
    or trying to correct them'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据清理和归一化** — 删除“损坏”样本或尝试纠正它们的过程'
- en: '**Data selection** — Process of ranking the data based on importance for a
    particular task'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据选择** — 根据特定任务的重要性对数据进行排序的过程'
- en: Data Cleaning in Machine Learning
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习中的数据清理
- en: The easiest way to understand **data cleaning for structured data** is to think
    about tabular data. Imagine you’re working in a bank on a project where you want
    to analyze customers' spending based on their origin. Your data is in a CSV file,
    and you discover that the location information is unavailable. Some entries have
    typos, such as misspelled cities or completely missing entries. You can now either
    clean the data by removing all the “broken” entries or try to correct the missing
    entries based on other available data. Open source libraries like [fancyimpute](https://github.com/iskandr/fancyimpute)
    and [autoimpute](https://github.com/kearnz/autoimpute) that impute missing examples
    in tabular data exist.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 理解**结构化数据的数据清理**最简单的方法是考虑表格数据。假设你在一家银行从事一个项目，想要根据客户的来源分析他们的支出。你的数据在一个CSV文件中，你发现位置信息缺失。一些条目存在拼写错误，例如城市拼写错误或完全缺失的条目。你现在可以选择通过删除所有“损坏”的条目来清理数据，或者根据其他可用数据尝试纠正缺失的条目。像[fancyimpute](https://github.com/iskandr/fancyimpute)和[autoimpute](https://github.com/kearnz/autoimpute)这样的开源库可以对表格数据中的缺失示例进行插补。
- en: When working with **unstructured data** such as images, you can check for images
    where the camera fails. A hardware issue could result [in a broken video frame](https://docs.lightly.ai/docs/corruptness-check#broken-video-frames),
    or the recorded data could be of better quality (not enough light, blurry image).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理**非结构化数据**如图像时，可以检查相机故障的图像。硬件问题可能导致[视频帧损坏](https://docs.lightly.ai/docs/corruptness-check#broken-video-frames)，或者记录的数据可能质量更好（光线不足，图像模糊）。
- en: With unstructured data, you often rely on supervised learning. It would be best
    if you also considered cleaning your data from “broken” labels or trying to correct
    them at least.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非结构化数据，你通常依赖于监督学习。你也应该考虑清理“损坏”的标签，或至少尝试纠正它们。
- en: Data Selection (related to Active Learning)
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据选择（与主动学习相关）
- en: As you might have heard before, not all data is equally important for your machine
    learning model to learn from. You don’t want to spend resources on [data you don’t
    need](https://www.lightly.ai/post/the-data-you-dont-need).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能之前听说过的，并非所有数据对你的机器学习模型都同等重要。你不想在[不需要的数据上浪费资源](https://www.lightly.ai/post/the-data-you-dont-need)。
- en: When training machine learning models, you must ensure the data you use for
    training matches the data you expect once the system runs. This sounds simple,
    but in practice is a huge issue. Think about developing a perception system for
    an autonomous delivery robot. You have a prototype robot to select data from the
    city where you have your R&D. Your model will be trained on data from a single
    town but eventually need to work across all kinds of global cities. Different
    cities might have different architectures, environmental conditions, traffic signs,
    etc.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练机器学习模型时，你必须确保用于训练的数据与系统运行时预期的数据匹配。这听起来简单，但实际上是一个巨大的问题。考虑开发一个自动送货机器人感知系统。你有一个原型机器人从你有研发的城市中选择数据。你的模型将基于单一城市的数据进行训练，但最终需要在全球各种城市中工作。不同的城市可能有不同的建筑结构、环境条件、交通标志等。
- en: One could do a gradual rollout, deploy robots city by city and continuously
    collect more data to improve the perception system. But then, the initial cities
    contribute significantly more data than the last cities. How do you keep track
    of what is new, sound, or what is “redundant”?
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 可以进行逐步推出，按城市部署机器人，并持续收集更多数据以改进感知系统。但是，这样一来，初始城市的数据贡献显著高于最后的城市。如何跟踪哪些是新的、有效的，或者哪些是“冗余”的？
- en: What is a well-curated dataset?
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是良好整理的数据集？
- en: First of all, there is no general perfect dataset per se. A dataset's value
    depends on the task you want to solve and other variables such as model architecture,
    training routine, and available computing power. Nevertheless, a well-curated
    dataset can help prevent running into any of the problems outlined in this guide.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，实际上没有通用的完美数据集。数据集的价值取决于你想要解决的任务以及模型架构、训练过程和可用计算能力等其他变量。尽管如此，一个良好整理的数据集可以帮助防止遇到本指南中概述的任何问题。
- en: Your dataset is ideally well-balanced. Your test set represents your model's
    deployment domain and is independent of the training data. Hence, you know about
    the generalization capabilities. Your dataset covers the edge cases you care about,
    and the labels are correct.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，你的数据集是良好平衡的。你的测试集代表了模型的部署领域，并且独立于训练数据。因此，你了解模型的泛化能力。你的数据集涵盖了你关心的边缘情况，标签也是正确的。
- en: Data Curation Starter Guide
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据整理入门指南
- en: I have created a starter guide to help you identify problems in your machine
    learning pipeline. Use it as a reference to learn about the most common data curation
    workflows.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我创建了一个入门指南来帮助你识别机器学习管道中的问题。将其作为参考以了解最常见的数据整理工作流程。
- en: '*Note that this cheat sheet is not covering all possible issues and should
    be used as a helper in case you don’t know where to start.*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*请注意，这份备忘单并没有涵盖所有可能的问题，应作为你不知从何开始时的帮助工具。*'
- en: '**How to work with the data curation starter guide:**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**如何使用数据整理入门指南：**'
- en: Identify the model problem you're facing on the left side
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 识别你面临的模型问题，位于左侧
- en: Follow the arrows to find potential data problems
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 跟随箭头查找潜在的数据问题
- en: Pick the data curation workflow that solves the problem
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择解决问题的数据整理工作流程
- en: '![](../Images/e9e44d2759ef38effa61e5b5121de7b7.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e9e44d2759ef38effa61e5b5121de7b7.png)'
- en: Data Curation Starter Guide. [image by author]
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 数据整理入门指南。[图片由作者提供]
- en: Common Model Problems and their Solutions
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见模型问题及其解决方案
- en: Overview of the most common data problems and suggestions for resolving them.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的数据问题概述及解决建议。
- en: '1: My model has a high train set accuracy, but a low test set accuracy'
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '1: 我的模型在训练集上准确率很高，但在测试集上准确率很低'
- en: There are different reasons for this to happen. First, you should exclude common
    training procedure mistakes that could result in your model overfitting the training
    data. Adding more augmentations or [regularization methods such as L2 or weight
    norm could help to reduce the risk of overfitting, as outlined by Andre Ng in
    this video](https://www.youtube.com/watch?v=u73PU6Qwl1I).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 发生这种情况有不同的原因。首先，你应该排除可能导致模型过拟合训练数据的常见训练过程错误。添加更多的数据增强或[正则化方法，如 L2 或权重范数，可以帮助减少过拟合的风险，正如
    Andre Ng 在这个视频中所述](https://www.youtube.com/watch?v=u73PU6Qwl1I)。
- en: There could also be other reasons for overfitting.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 也可能存在其他过拟合的原因。
- en: '![](../Images/1f9afd4104e994e0d04cabfe3f102a3b.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1f9afd4104e994e0d04cabfe3f102a3b.png)'
- en: One of the most common scenarios is that your model works well on the training
    data but worse on the test set. Likely your model is overfitting. Simple training
    routines, such as adding regularization methods, can reduce the problem. But at
    one point, you need to improve the data itself. [image by author]
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的情况之一是你的模型在训练数据上表现良好，但在测试集上表现较差。这很可能是你的模型过拟合了。简单的训练方法，如添加正则化方法，可以减少这个问题。但在某个时候，你需要改进数据本身。[image
    by author]
- en: Once you know that it’s not a model training problem anymore but rather a data
    problem, you should look into the data you have for the training and the test
    set. Another possible reason is that your training data is not representative
    enough. It could be that some of the examples appear very infrequent, and the
    model cannot learn from them. In this case, a potential solution is to collect
    more data on the rare events your model is struggling with. Approaches like [active
    learning](https://www.lightly.ai/post/a-guide-for-active-learning-in-computer-vision)
    have been proposed to tackle this problem in an automated and scalable way. [BAAL](https://arxiv.org/abs/2006.09916)[1]
    is such an active learning algorithm from 2020.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你知道这不再是模型训练问题而是数据问题，你应该查看你用于训练和测试集的数据。另一个可能的原因是你的训练数据代表性不够。可能是一些示例出现频率很低，模型无法从中学习。在这种情况下，一个潜在的解决方案是收集更多关于模型难以处理的稀有事件的数据。像[主动学习](https://www.lightly.ai/post/a-guide-for-active-learning-in-computer-vision)这样的方案已经被提出，以自动化和可扩展的方式解决这个问题。[BAAL](https://arxiv.org/abs/2006.09916)[1]
    是2020年的一个主动学习算法。
- en: '2: Model fails on uncommon/rare cases'
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '2: 模型在不常见/稀有情况下失败'
- en: Typically, your model only performs well for some classes and situations. For
    example, rare classes are often neglected by common learning procedures. For specific
    applications, such as, for example, in medical imaging, a rare class could be
    more important than others. In this case, several solutions exist.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，你的模型只在某些类别和情况中表现良好。例如，稀有类别通常被常见的学习过程忽略。在特定的应用场景中，例如医学影像中，稀有类别可能比其他类别更重要。在这种情况下，存在几种解决方案。
- en: First, if your model performs bad for a specific class, also check if this class
    is underrepresented in your dataset. If this is a minority class, you should try
    to [use weighted loss functions](https://medium.com/gumgum-tech/handling-class-imbalance-by-introducing-sample-weighting-in-the-loss-function-3bdebd8203b4)
    to account for this imbalance. It’s a simple trick that often yields already promising
    results.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，如果你的模型在某个特定类别上的表现不佳，也要检查一下该类别是否在你的数据集中代表性不足。如果这是一个少数类别，你应该尝试[使用加权损失函数](https://medium.com/gumgum-tech/handling-class-imbalance-by-introducing-sample-weighting-in-the-loss-function-3bdebd8203b4)来解决这种不平衡。这是一个简单的技巧，通常会产生很有希望的结果。
- en: If the problem persists, you can start thinking about how to solve that problem
    from a data point of view. There are two options. We can improve the class balancing
    or try finding more edge cases.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果问题仍然存在，你可以开始从数据的角度思考如何解决这个问题。有两个选项。我们可以改善类别平衡或尝试找到更多边缘情况。
- en: '**Improve the Class Balancing** You want to change the ratio of the classes
    in your training datasets to make them more equally proportioned. Several methods
    exist to handle different scenarios. If you work with lots of data, you could
    start throwing out samples from the majority class ([undersampling](https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis))
    to balance the classes.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**改进类别平衡** 你需要改变训练数据集中类别的比例，使其更加均衡。有几种方法可以处理不同的情况。如果你处理大量数据，可以开始丢弃多数类的样本（[欠采样](https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis)）以平衡类别。'
- en: There is another approach if you work with small datasets or can’t afford to
    remove training. You could try to collect more data and prioritize classes that
    have been underrepresented previously. But how can you achieve the latter? You
    can use model predictions on the unlabeled data to get an idea of the data distribution.
    Whenever we find the majority classes, we put a lower priority on them. We increase
    the priority for potential predictions of rare classes. There exists an [open-source
    library to work on imblanced data](https://imbalanced-learn.org/stable/index.html)
    called imbalanced-learn.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你处理小型数据集或无法负担删除训练数据，你可以尝试收集更多数据，并优先考虑之前代表性不足的类别。但是如何实现后者呢？你可以使用模型在未标记数据上的预测来了解数据分布。每当我们发现主要类别时，我们就会降低对它们的优先级。我们会增加对稀有类别潜在预测的优先级。有一个[处理不平衡数据的开源库](https://imbalanced-learn.org/stable/index.html)叫做imbalanced-learn。
- en: '**Finding more edge cases using active learning** If you know exactly what
    you are looking for (e.g., the model is bad at detecting police cars), you can
    rely on **similarity search** methods like [SEALS](https://arxiv.org/abs/2007.00077)[2].
    You can take reference images of these rare objects and use their embeddings as
    a search vector to find similar-looking images or objects across the unlabeled
    data. Be careful when using similarity search. Suppose you only use images of
    a specific type of police car or a certain angle. In that case, you might find
    more similar police car images that don’t enhance your dataset as you add nearby
    duplicates. This can have a similar effect to just augmenting your initial dataset.
    Instead, you want similar police cars that look slightly different from the police
    cars you already have!'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**通过主动学习发现更多边界情况** 如果你确切知道你在寻找什么（例如，模型在检测警车方面表现不佳），你可以依靠**相似性搜索**方法，如[SEALS](https://arxiv.org/abs/2007.00077)[2]。你可以使用这些稀有物体的参考图像，并利用它们的嵌入作为搜索向量，在未标记数据中寻找相似的图像或物体。使用相似性搜索时要小心。如果你只使用某种特定类型的警车或某个角度的图像，那么你可能会发现更多相似的警车图像，这些图像不会增强你的数据集，反而增加了附近的重复项。这会产生类似于仅仅扩充初始数据集的效果。相反，你希望找到与现有警车略有不同的相似警车！'
- en: Another more general approach is to use a [combination of model predictions
    and embeddings for **active learning**](https://arxiv.org/abs/2004.04699)[3].
    You can add new images to the training set by finding unlabeled data where you
    have objects that are difficult to classify or semantically very different from
    the existing training data. At Lightly, we saw great success in this approach
    as it can be automated and scaled to large datasets.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种更通用的方法是使用[**主动学习**的模型预测和嵌入组合](https://arxiv.org/abs/2004.04699)[3]。你可以通过找到难以分类或在语义上与现有训练数据非常不同的未标记数据来将新图像添加到训练集中。在Lightly，我们在这种方法中取得了很大的成功，因为它可以自动化并扩展到大规模数据集。
- en: '3: Model gets worse over time'
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '3: 模型随着时间变差'
- en: You might have an increase in failure rates and a gut feeling that the model
    you deployed a few weeks back is not working as expected. This could be a “data
    drift” problem. The model needs to be updated with new training data. First, you
    should analyze the data your model sees in production and compare it to the data
    you used for training the model. Likely there is a clear difference in the distribution.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会发现失败率增加，并且有一种直觉认为你在几周前部署的模型没有按预期工作。这可能是“数据漂移”问题。模型需要用新的训练数据进行更新。首先，你应该分析模型在生产中看到的数据，并将其与用于训练模型的数据进行比较。可能会有明显的分布差异。
- en: '![](../Images/ab586e354de38d44f4e80a7f5b5cd10d.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ab586e354de38d44f4e80a7f5b5cd10d.png)'
- en: If the model performance degrades over time, you might face a data drift problem.
    In simpler words, the data the models see in deployment starts drifting away from
    the data seen during training. Without retraining the model with new data and
    redeploying, the loss in accuracy might increase until the system does not work
    reliably anymore. [image by author]
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型性能随着时间的推移而下降，你可能面临数据漂移问题。简单来说，模型在部署中看到的数据开始与训练中看到的数据逐渐偏离。如果不使用新数据重新训练模型并重新部署，准确率的损失可能会增加，直到系统不再可靠。[图片由作者提供]
- en: As a simple experiment, you could also train a simple classifier model to classify
    whether a given image is part of the existing training data or the new production
    data. If the data distribution matched fully, your classifier would not perform
    better than chance. However, if the classifier works well, the two domains differ.
    After analyzing the domain gap, you should update the training data. You could,
    for example, select a subset of the production data using diversity-based sampling
    followed by a new train/test split. You then add the respective train/ test splits
    to the existing splits you used to train your deployed model.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个简单的实验，你也可以训练一个简单的分类器模型，来分类给定的图像是否属于现有的训练数据或新的生产数据。如果数据分布完全匹配，你的分类器不会比随机猜测表现更好。然而，如果分类器表现良好，说明两个领域有所不同。分析领域差距后，你应该更新训练数据。例如，你可以选择一部分生产数据，使用基于多样性的采样，然后进行新的训练/测试拆分。然后，将相应的训练/测试拆分添加到你用来训练已部署模型的现有拆分中。
- en: 'The model would now be trained and evaluated using the following:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型将使用以下内容进行训练和评估：
- en: train (initial) + train (new production data)
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练（初始）+ 训练（新生产数据）
- en: test (initial), test (new production data)
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试（初始），测试（新生产数据）
- en: '*Note that we recommend evaluating both test sets individually to ensure you
    can measure the metrics for the different distributions. This will also allow
    you to spot if the accuracy changed for the test (initial) set.*'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意，我们建议分别评估两个测试集，以确保你可以衡量不同分布的指标。这也将帮助你发现测试（初始）集的准确率是否发生了变化。*'
- en: '4: My model has high accuracy on the train and test set but then performed
    poorly after deployment'
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '4: 我的模型在训练集和测试集上的准确率很高，但在部署后表现很差'
- en: This is a widespread problem every ML model will face. When you start training
    and evaluating a model, you assume that the available training and test set follow
    a similar distribution as your model will face in deployment. If that is not the
    case, you don’t know how well your model performs. For example, a model to detect
    traffic signs trained and evaluated solely on data from California might miserably
    fail when deployed in Europe, where traffic signs look differently.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这是每个机器学习模型都会面临的普遍问题。当你开始训练和评估模型时，你假设可用的训练集和测试集的分布与模型在部署时面对的情况相似。如果情况不是这样，你就不知道模型的表现如何。例如，一个专门在加州数据上训练和评估的交通标志检测模型，在部署到欧洲时可能会因为交通标志样式不同而表现很差。
- en: It’s crucial to ensure the model is trained and evaluated on data that matches
    its deployment environment as closely as possible. One way to prevent this issue
    is to continuously collect data and think carefully about data collection strategies
    to reduce any domain gap.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 确保模型在与其部署环境尽可能匹配的数据上进行训练和评估是至关重要的。避免这个问题的一种方法是持续收集数据，并仔细考虑数据收集策略，以减少任何领域差距。
- en: To spot potential issues around the generalization of your model earlier, you
    can [use a different train test split](https://www.lightly.ai/post/train-test-split-in-deep-learning)
    that divides training and testing data not randomly but based on cities, for example.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更早发现模型泛化的潜在问题，你可以[使用不同的训练测试拆分](https://www.lightly.ai/post/train-test-split-in-deep-learning)，例如基于城市而不是随机地划分训练和测试数据。
- en: '5: Train accuracy low and test accuracy low'
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '5: 训练准确率低，测试准确率低'
- en: 'If the train and test accuracy are low, you have a model that didn’t learn
    much. There can be various reasons why the model is not improving:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果训练和测试的准确率都很低，说明你的模型没有学到很多东西。模型没有改善的原因可能有多种：
- en: '**The task is too difficult —** based on the available training data and the
    model''s capacity, it’s too difficult to solve this problem.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务过于困难** — 根据现有的训练数据和模型的能力，解决这个问题太困难了。'
- en: '**The labels are wrong** — even if the data itself is very valuable. Without
    having correct labels, the model can’t learn anything.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签错误** — 即使数据本身非常有价值。没有正确的标签，模型无法学到任何东西。'
- en: '**Is the task too difficult for my model?**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**我的模型任务是否过于困难？**'
- en: A simple yet effective method in computer vision to determine if a task is eventually
    too difficult to solve is by doing the “could I do it myself”-test. Given a few
    training images with labels and the task, could you correctly classify images
    from the test set?
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉中，一个简单而有效的方法来确定任务是否最终太难解决，就是做“我能自己做吗”测试。给定几张带标签的训练图像和任务，你能否正确分类测试集中的图像？
- en: If yes, you know that you could solve it and that the data might be good enough.
    Check if the model has enough training data to pick up the right signals.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果是，你知道你可以解决它，并且数据可能足够好。检查模型是否有足够的训练数据来捕捉正确的信号。
- en: '*Note: This works exceptionally well in computer vision, as humans are brilliant
    in pattern matching. If you work with other data types, this simple test might
    not work.*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：这在计算机视觉中效果特别好，因为人类在模式匹配方面非常出色。如果你处理其他数据类型，这个简单的测试可能不适用。*'
- en: '**Dealing with bad labels**'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**处理坏标签**'
- en: In supervised learning, the training data consists of samples (e.g., images)
    and the corresponding labels. Even if the samples in the dataset are representative
    and balanced, bad labels can result in a model learning nothing useful. It’s like
    teaching a kid in school to do math but with formulas and examples that are randomly
    arranged. Today you learn that 1+1=3 and tomorrow, that 1+1=5\. If there is no
    consistent teaching pattern because the data is wrongly labeled, we have a problem.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，训练数据由样本（例如图像）及其对应的标签组成。即使数据集中的样本具有代表性和均衡性，不良标签仍然可能导致模型学习不到有用的东西。这就像在学校教一个孩子做数学，但使用的公式和例子是随机排列的。今天你学到
    1+1=3，明天你学到 1+1=5。如果没有一致的教学模式，因为数据标注错误，我们就会遇到问题。
- en: You can do the following if you have a label issue. Randomly pick a small set
    (e.g., 100) samples from the train set and evaluate them manually for potential
    labeling mistakes. You do the same for the test set.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有标签问题，可以做以下操作。从训练集中随机挑选一小部分（例如 100）样本，并手动检查潜在的标签错误。你可以对测试集做相同的操作。
- en: Having a few faulty labels is, unfortunately, very common. Several academic
    datasets such as ImageNet, CIFAR10 have a [label error rate of around 5%](https://arxiv.org/pdf/2103.14749.pdf)[4].
    If the errors are not systematic and the error rate is not too high, it should
    not have a significant impact. But if you face systematic errors (e.g., all cats
    are labeled as dogs), you must correct them.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一些错误标签是不幸的非常常见。一些学术数据集，如 ImageNet 和 CIFAR10 的[标签错误率约为 5%](https://arxiv.org/pdf/2103.14749.pdf)[4]。如果错误不是系统性的且错误率不是很高，它不应该有显著的影响。但如果你遇到系统性错误（例如，所有猫都标记为狗），你必须纠正它们。
- en: You might not want to relabel the whole train/ validation and test sets if you're
    working on large datasets. A more straightforward approach might be to correct
    the validation and test set. Having good evaluation datasets is very crucial.
    And for the training set, you can use methods such as [co-teaching](https://proceedings.neurips.cc/paper/2018/file/a19744e268754fb0148b017647355b7b-Paper.pdf)[5]
    to train your model on noisy labels. Nowadays, many models are also pre-trained
    using self-supervised learning methods that don’t require labels. Therefore, these
    models won’t pick up the systematic errors in the training data. But still, you
    want to evaluate these models properly and therefore need good validation and
    test sets.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在处理大型数据集时，可能不想重新标注整个训练集/验证集和测试集。一种更直接的方法可能是纠正验证集和测试集的标签。拥有好的评估数据集是非常关键的。对于训练集，你可以使用诸如[共教学](https://proceedings.neurips.cc/paper/2018/file/a19744e268754fb0148b017647355b7b-Paper.pdf)[5]之类的方法来训练你的模型，即便是有噪声的标签。如今，许多模型还通过不需要标签的自监督学习方法进行预训练。因此，这些模型不会拾取训练数据中的系统性错误。但你仍然需要对这些模型进行适当评估，因此需要好的验证集和测试集。
- en: I hope you liked this post. If you have any suggestions on how we can further
    improve this guide, please don’t hesitate to reach out or leave a comment!
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你喜欢这篇文章。如果你有任何关于如何进一步改进本指南的建议，请随时联系我们或留下评论！
- en: Igor Susmelj,
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Igor Susmelj，
- en: Co-Founder [Lightly](https://lightly.ai/)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 联合创始人 [Lightly](https://lightly.ai/)
- en: '[1] Atighehchian et al. (2020), “Bayesian active learning for production, a
    systematic study and a reusable library”'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Atighehchian 等（2020），“用于生产的贝叶斯主动学习：系统研究和可重用库”'
- en: '[2] Coleman et al. (2020), “Similarity Search for Efficient Active Learning
    and Search of Rare Concepts”'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Coleman 等（2020），“高效主动学习和稀有概念搜索的相似性搜索”'
- en: '[3] Haussmann et al. (2020), “Scalable Active Learning for Object Detection”'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Haussmann 等（2020），“面向目标检测的可扩展主动学习”'
- en: '[4] Northcutt et al. (2021), “Pervasive Label Errors in Test Sets Destabilize
    Machine Learning Benchmarks”'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Northcutt 等人 (2021)， “测试集中的普遍标签错误使机器学习基准不稳定”'
- en: '[5] Han et al. (2018), “Co-teaching: Robust Training of Deep Neural Networks
    with Extremely Noisy Labels”)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Han 等人 (2018)， “共同教学：在极度噪声标签下对深度神经网络的鲁棒训练”'
