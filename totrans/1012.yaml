- en: Gradient Descent Algorithm 101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/gradient-descent-algorithm-101-c226c69d756c](https://towardsdatascience.com/gradient-descent-algorithm-101-c226c69d756c)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Beginner-friendly guide
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Understand the optimization algorithm widely used in Machine and Deep Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://polmarin.medium.com/?source=post_page-----c226c69d756c--------------------------------)[![Pol
    Marin](../Images/a4f69a96717d453db9791f27b8f85e86.png)](https://polmarin.medium.com/?source=post_page-----c226c69d756c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c226c69d756c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c226c69d756c--------------------------------)
    [Pol Marin](https://polmarin.medium.com/?source=post_page-----c226c69d756c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c226c69d756c--------------------------------)
    ¬∑6 min read¬∑Apr 25, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a718894153ffc2de3d1b8a5ecdc19786.png)'
  prefs: []
  type: TYPE_IMG
- en: The slope of a mountain ‚Äî Photo by [Ralph (Ravi) Kayden](https://unsplash.com/fr/@ralphkayden?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you are a drop of water on top of a mountain, and your goal is to get
    to the lake situated right at the base of the mountain. That tall mountain has
    different slopes and obstacles, so going down following a straight line might
    not be the best solution. How would you approach this problem? The best solution
    would arguably be taking little steps, one at a time, always heading toward the
    direction that brings you closer to your end goal.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Descent (GD) is the algorithm that does just that, and it is essential
    for any data scientist to understand. It‚Äôs basic and rather simple but crucial,
    and anyone willing to enter the field should be able to explain what it is.
  prefs: []
  type: TYPE_NORMAL
- en: In this post, my goal is to make a complete and beginner-friendly guide to make
    everyone understand what GD is, what‚Äôs it used for, how it works, and mention
    different variations of it.
  prefs: []
  type: TYPE_NORMAL
- en: As always, you‚Äôll find the *resources* section at the end of the post.
  prefs: []
  type: TYPE_NORMAL
- en: But first things first.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using Wikipedia‚Äôs definition[1], **Gradient descent is a first-order iterative
    optimization algorithm for finding a local minimum of a differentiable function**.
    Even though it‚Äôs surely not the most effective method, it‚Äôs commonly used in Machine
    Learning and Deep Learning, especially in Neural Networks.
  prefs: []
  type: TYPE_NORMAL
- en: It‚Äôs basically used to minimize the value of a function by updating a set of
    parameters on each iteration. Mathematically speaking, it uses the derivative
    (gradient) to gradually decrease (descent) its value.
  prefs: []
  type: TYPE_NORMAL
- en: 'But there‚Äôs a catch: **not all functions are optimizable**. We require a function
    ‚Äî either uni or multivariate ‚Äî that‚Äôs **differentiable**, which means derivatives
    exist at each point in the function‚Äôs domain, and **convex** (U-shape or similar).'
  prefs: []
  type: TYPE_NORMAL
- en: Now, after this simple introduction, we can start digging a little bit deeper
    into the math behind it.
  prefs: []
  type: TYPE_NORMAL
- en: Practical Case
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Because all gets clearer when going beyond the theory, let‚Äôs use real numbers
    and values to understand what it does.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs use a common data science case in which we want to develop a regression
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Disclaimer: I have totally invented this and there‚Äôs no logical reasoning behind
    using these functions, all came randomly. The goal is to show the process itself.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The cost function or loss function in any data science problem is the function
    we want to optimize. As we‚Äôre using regression, we‚Äôre going to use this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a6a4e52524b004d07368220bd2def7af.png)'
  prefs: []
  type: TYPE_IMG
- en: Random regression function ‚Äî Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal is to find the optimal minimum of f(x,y). Let me plot what it looks
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1c52d8ed9fb74c38f1f10e7f86150157.png)'
  prefs: []
  type: TYPE_IMG
- en: f(x,y) plotted with a=1 ‚Äî Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: 'Now our goal is to get the proper values for ‚Äúx‚Äù and ‚Äúy‚Äù that let us find the
    optimal values of this cost function. We can already see it graphically:'
  prefs: []
  type: TYPE_NORMAL
- en: y=0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: x being either -1 or 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Onto the GD itself, because we want to make our machine learn to do the same.
  prefs: []
  type: TYPE_NORMAL
- en: The Algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As said, gradient descent is an iterative process in which we compute the gradient
    and move in the opposite direction. The reasoning behind this is that the gradient
    of a function is used to determine the slope of that function. As we want to move
    down, not up, then we move in the opposite way.
  prefs: []
  type: TYPE_NORMAL
- en: 'It‚Äôs a simple process in which we update x and y in each iteration, by following
    the next approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/453e12771a46cd3bfc0508f7eb728401.png)'
  prefs: []
  type: TYPE_IMG
- en: Parameter update in gradient descent ‚Äî Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: 'Explained in words, at iteration k:'
  prefs: []
  type: TYPE_NORMAL
- en: Compute the gradient using the values of x and y at that iteration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each of those variables ‚Äî x and y ‚Äî multiply its gradient times lambda (ùúÜ),
    which is a float number called the learning rate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove from x and y respectively the computed values in step 2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make x and y have the new value in the next iteration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This process is then repeated until a certain condition is met (that‚Äôs not important
    today). Once that happens, the training finishes and the optimization does too.
    We are (or should be) at a minimum (either local or global).
  prefs: []
  type: TYPE_NORMAL
- en: Now, let‚Äôs put this theory into practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing we need to do is compute the gradient of f(x,y). The gradient
    corresponds to a vector of partial derivatives:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f8467d1bff5f82c281fd7c44e1994644.png)'
  prefs: []
  type: TYPE_IMG
- en: Gradient of f(x,y) ‚Äî Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Now, using Python, all I‚Äôm going to do is create a loop that iteratively computes
    the gradient ‚Äî using the corresponding x and y ‚Äî and updates these parameters
    as specified above.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before that, I‚Äôll define two more values:'
  prefs: []
  type: TYPE_NORMAL
- en: The learning rate (ùúÜ) can be fixed or mobile. For this simple tutorial, it‚Äôll
    be 0.01.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I‚Äôll also use a value called eps (epsilon) to determine when to finish iterating.
    Once both partial derivatives are below this threshold, the gradient descent will
    stop. I‚Äôm setting it to 0.0001.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let‚Äôs do some code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of a random iteration was:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/75c600dbd1be375066b2f6553410f166.png)'
  prefs: []
  type: TYPE_IMG
- en: Sample GD output ‚Äî Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: We can see these values are pretty close to x=1 and y=0, which were indeed function
    minimums.
  prefs: []
  type: TYPE_NORMAL
- en: One thing I forgot to mention was the x and y initializations. I chose to randomly
    generate a number within random ranges. In real-world problems, using more time
    to think about this is always required. Same with the learning rate, the stopping
    condition, and many other hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: But for our case, this was more than enough.
  prefs: []
  type: TYPE_NORMAL
- en: Variations of Gradient Descent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I‚Äôm sure you now understand the basic algorithm. However, multiple versions
    of it are being used out there and I think some of those are worth being mentioned.
  prefs: []
  type: TYPE_NORMAL
- en: '**Stochastic Gradient Descent (SGD)**. SGD is the variation that randomly picks
    one data point from the whole dataset at each iteration. This reduces the number
    of computations but it obviously has its downsides like, for example, not being
    able to converge to the global minimum.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch Gradient Descent (BGD)**. BGD uses the whole dataset in each iteration.
    This isn‚Äôt fully desired for big datasets as can be computationally expensive
    and slow but, on the other hand, the convergence to the global minimum is theoretically
    guaranteed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mini-Batch Gradient Descent (MBGD)**. This can be considered a middle point
    between SGD and BGD. It does not use one data point at the time nor the whole
    dataset, but a subset of it. On each iteration, we pick a random number of samples
    (previously defined) and perform the gradient descent using only those.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Gradient Descent algorithm is widely used in machine and deep learning,
    but in other areas as well. That‚Äôs why understanding it is a must for everyone
    willing to become a data scientist.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this post clarified what it is, what it does, and how it does it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'If you‚Äôd like to support me further, consider subscribing to Medium‚Äôs Membership
    through the link you find below: it won‚Äôt cost you any extra penny but it‚Äôll help
    me through this process. Thanks a lot!'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@polmarin/membership?source=post_page-----c226c69d756c--------------------------------)
    [## Join Medium with my referral link - Pol Marin'
  prefs: []
  type: TYPE_NORMAL
- en: Read every story from Pol Marin (and thousands of other writers on Medium).
    Your membership fee directly supports Pol‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@polmarin/membership?source=post_page-----c226c69d756c--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] [Gradient descent ‚Äî Wikipedia](https://en.wikipedia.org/wiki/Gradient_descent)'
  prefs: []
  type: TYPE_NORMAL
