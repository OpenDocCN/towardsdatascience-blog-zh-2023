- en: Combine Multiple LoRA Adapters for Llama 2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/combine-multiple-lora-adapters-for-llama-2-ea0bef9025cf](https://towardsdatascience.com/combine-multiple-lora-adapters-for-llama-2-ea0bef9025cf)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Add skills to your LLM without fine-tuning new adapters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----ea0bef9025cf--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----ea0bef9025cf--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ea0bef9025cf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ea0bef9025cf--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----ea0bef9025cf--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ea0bef9025cf--------------------------------)
    ·12 min read·Nov 30, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6c4a40091827bce0f2a546522f563d4a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author — Made with an image from [Pixabay](https://pixabay.com/vectors/llama-alpaca-animal-mammal-zoo-297668/)
  prefs: []
  type: TYPE_NORMAL
- en: Fully fine-tuning a pre-trained large language model (LLM) for different tasks
    is very costly. Instead, we can freeze the parameters of the LLM while only fine-tuning
    a few million trainable parameters added through a LoRA adapter.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, we only need to fine-tune an adapter to get the model to perform
    a target task. For instance, if we want to turn a pre-trained LLM into a translation
    model, we would fine-tune an adapter for translation. We can fine-tune one adapter
    for each ask that we want the LLM to perform.
  prefs: []
  type: TYPE_NORMAL
- en: '*But can we combine several adapters to get one single multi-task adapter?*'
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if we have one adapter for translation and one adapter for summarization,
    can we combine both of them so that the LLM can do translation and summarization?
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I show how to combine multiple LoRA adapters into a single
    multi-task adapter. We will see that it is very simple and that the resulting
    adapter can be as good as the adapters used for the combination.
  prefs: []
  type: TYPE_NORMAL
- en: Using Llama 2 7B, we will see how to combine an adapter fine-tuned for translation
    with another adapter fine-tuned for chat. With the resulting adapter, we will
    be able to make a Llama 2 that can translate and chat.
  prefs: []
  type: TYPE_NORMAL
- en: 'I have also implemented a notebook that can run all the code explained in this
    article. You can find it here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Get the notebook (#30)](https://kaitchup.substack.com/p/notebooks)'
  prefs: []
  type: TYPE_NORMAL
- en: Add Multiple Adapters to Llama 2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before combining adapters, we need to add them to the base LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have to make sure that the adapter that we want to add has been fine-tuned
    for our base LLM, i.e., Llama 2 7B. You can find this information in the file
    “adapter_config.json” which is in the adapter directory. For instance, for [kaitchup/Llama-2–7B-oasstguanaco-adapter](https://huggingface.co/kaitchup/Llama-2-7B-oasstguanaco-adapter)
    (MIT license), the [adapter_config.json](https://huggingface.co/kaitchup/Llama-2-7B-oasstguanaco-adapter/blob/main/adapter_config.json)
    contains the following data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The field “base_model_name_or_path” indicates that the base model for this adapter
    is meta-llama/Llama-2–7b-hf. We can add this adapter to Llama 2 7B.
  prefs: []
  type: TYPE_NORMAL
- en: 'I fine-tuned this adapter myself on [timdettmers/open assistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco)
    following the steps explained in this article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://kaitchup.substack.com/p/fine-tune-llama-2-on-your-computer?source=post_page-----ea0bef9025cf--------------------------------)
    [## Fine-tune Llama 2 on Your Computer with QLoRa and TRL'
  prefs: []
  type: TYPE_NORMAL
- en: On Guanaco and with the correct padding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: kaitchup.substack.com](https://kaitchup.substack.com/p/fine-tune-llama-2-on-your-computer?source=post_page-----ea0bef9025cf--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'When loaded on top of Llama 2, it transforms it into a chat model answering
    prompts structured as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The base model should be loaded with the same configuration used for fine-tuning
    the adapter. For instance, if the adapter was fine-tuned with QLoRA, then we should
    load Llama 2 with the same QLoRA configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this adapter, you can find this information in the [model card](https://huggingface.co/kaitchup/Llama-2-7B-oasstguanaco-adapter):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the standard QLoRA quantization configuration. We should load Llama
    2 like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, with Hugging Face PEFT, we can easily load an adapter on top of this
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Set a meaningful “adapter_name”. We will see why this is necessary in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: 'At that point, Llama 2 is now a chat model. If we prompt it with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The model generates something like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '*Note: I provide the inference code in the notebook and the next section.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, imagine that we also want the model to perform translation tasks from
    French to English. We can use this adapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[kaitchup/Llama-2–7b-mt-French-to-English](https://huggingface.co/kaitchup/Llama-2-7b-mt-French-to-English)
    (MIT license)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'I fine-tuned following these instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://kaitchup.substack.com/p/llama-2-mt-turn-llama-2-into-a-translation?source=post_page-----ea0bef9025cf--------------------------------)
    [## Llama 2 MT: Turn Llama 2 into a Translation System with QLoRA'
  prefs: []
  type: TYPE_NORMAL
- en: How to Fine-tune a Cheap Translation Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: kaitchup.substack.com](https://kaitchup.substack.com/p/llama-2-mt-turn-llama-2-into-a-translation?source=post_page-----ea0bef9025cf--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'We can load this adapter as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We have now two adapters loaded. We can verify it by printing the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The “fren” adapter is fine-tuned for Llama 2 with the same quantization configuration,
    but with prompts using a different format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Following “>”, the model should generate the translation. Let’s try it with
    this prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'It prints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: It doesn’t look like a translation…
  prefs: []
  type: TYPE_NORMAL
- en: 'What the model structure (printed above) doesn’t tell us is that only one adapter
    is active: The first one we have loaded (“oasst”). Since the prompt doesn’t have
    the right format (with human and assistant tags), the model randomly guesses what
    it should do (here, it generates a monologue in which the “Assistant” talks to
    itself, in French…).'
  prefs: []
  type: TYPE_NORMAL
- en: The model can’t exploit both adapters. We have to combine them into one single
    adapter that can chat and translate.
  prefs: []
  type: TYPE_NORMAL
- en: Combine Multiple LoRA Adapters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With the [PEFT library](https://huggingface.co/docs/peft/v0.6.2/en/index),
    we can easily merge adapters. Three methods are currently implemented in “[add_weigthed_adapter](https://huggingface.co/docs/peft/v0.6.2/en/package_reference/tuners#peft.LoraModel.add_weighted_adapter)”:'
  prefs: []
  type: TYPE_NORMAL
- en: 'concatenation: This is the most simple one. It simply concatenates the adapters’
    parameters. It means that if you concatenate two adapters with a rank of 16, the
    resulting adapter will have a rank of 32\. This method is very fast.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'linear combination: This one is under-documented but it seems that it simply
    does a weighted sum of the adapters’ parameters. ([see the source code](https://github.com/huggingface/peft/blob/v0.6.2/src/peft/tuners/lora/model.py#L442))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SVD: It applies [singular value decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition)
    using [torch.linalg.svd](https://pytorch.org/docs/stable/generated/torch.linalg.svd.html).
    This is the default method. It has several arguments but we will not explore them
    in this article (I will leave them to default values). This method is much slower
    than the other two. If your adapters have unusually high ranks (>100), it may
    take several hours.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All these methods weigh the combination. For instance, if we combine two adapters
    X and Y, we can put more weight on one adapter, e.g., X, to make sure the resulting
    adapter will behave more closely to X than Y.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will try all concatenation and SVD to combine the two adapters presented
    in the previous section: “fren” and “oasst”.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, install the following dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '*Note: I install bitsandbytes because I use quantization. If you don’t quantize
    your model, you won’t need it.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we need to import the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can load the model (Llama 2 7B), quantize it, and load the tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '*Note: Remember that you need an access token to get Llama 2 from the Hugging
    Face hub.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need a function to generate text for testing the adapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we load our two adapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '***Important note****: I move the model to the CPU device (with “.to(‘cpu‘)”)
    otherwise the combination of the adapters won’t work. All the adapters must be
    on the same device to be combined but the active adapter is on the GPU while the
    inactive adapters are on the CPU. The only way I have found to make it work is
    to move the model to the CPU. However, if the model is quantized and on the CPU,
    it can’t do inference (the forward pass during inference will try to perform impossible
    multiplications). Once the combination is done and if you used quantization, I
    recommend saving the new adapter and then reloading and quantizing the model to
    be able to use it for inference.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'To combine the adapter, we only need to run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: It will create and load one new adapter named “fren_oasst”. Again, you can print
    the model to verify it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some explanations about the arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[“fren”, “oasst”]: This is a list of the names of all the adapters that we
    want to combine. These adapters must be loaded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[1.0,1.0]: The list of the weights to make the weighted combination. “fren”
    has a weight of 1.0 and “oasst” has a weight of 1.0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'combination_type: The method used for the combination: concatenation (cat),
    linear (linear), or SVD (svd).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'adapter_name: The resulting adapter will be loaded and have this name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then, I recommend saving the adapter. To avoid saving “fren” and “oasst”, delete
    them first, then “save_pretrained” will only save our new adapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'And, as discussed above (see “important note”), reload and quantize again the
    base model before loading this new adapter (don’t move it to the CPU):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: For this combination, I used “cat” to concatenate the adapters. It’s a very
    simple operation. I also gave the adapters the same weights during the combination.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s see what the model generates given chat and translation prompts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'It generates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'and:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: It seems to have worked very well. The new adapter can chat and translate.
  prefs: []
  type: TYPE_NORMAL
- en: '*How is it possible?*'
  prefs: []
  type: TYPE_NORMAL
- en: The new adapter recognizes the task to perform thanks to the format of the prompt.
    When it encodes “###>”, it identifies that it should translate the previous tokens.
    When it encodes “### Human:” and “### Assistant:”, it knows that it should chat.
  prefs: []
  type: TYPE_NORMAL
- en: It works very well when the adapters combined are fine-tuned with very different
    prompt formats. However, if I had fine-tuned the “oasst” adapter with a prompt
    format using “###> Assistant:” instead of “### Assistant:”, the new adapter would
    have been confused since “###>” may also indicate that a translation is expected.
  prefs: []
  type: TYPE_NORMAL
- en: To sum up, to work well, we should **only combine adapters that were fine-tuned
    with significantly different prompt formats**. Ideally, they should be fine-tuned
    with a tag at the beginning prompt to identify the task, e.g., [translate_fren]
    or [chat].
  prefs: []
  type: TYPE_NORMAL
- en: But even with different prompt formats, the new adapter may not perform as we
    want.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, this new adapter only generates short responses when chatting.
    This behavior is inherited from the translation adapter (fren) which was fine-tuned
    for sentence translation and thus learned to stop after generating one sentence.
  prefs: []
  type: TYPE_NORMAL
- en: We can tweak the behavior of the new adapter by giving more weight to an adapter
    during the combination and/or changing the combination type. If we give more weight
    to the chat adapter the resulting adapter may be able to generate longer responses
    while still being good at translation.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try it!
  prefs: []
  type: TYPE_NORMAL
- en: 'I changed the combination type to “svd” and tried different weights, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'I gave more weight to “oasst” so this new adapter will behave more like “oasst”.
    I tried it using the same examples used above. It generates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The result is much better for the chat prompt (printed after “- — — — -”) since
    the model generates a longer dialogue that looks like the data used to fine-tune
    the “oasst” adapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, I tried other weights to see how the combined adapter behaved but none
    of these other weights yielded better results. Here are some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'With [0.2,1.0]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: A weight of 0.2 is clearly too low for the translation adapter. The new adapter
    generates a lot of tokens with a translation prompt and behaves more like a chat
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'With [1.0,0.05]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: I didn’t observe much difference compared with the original [1.0,1.0].
  prefs: []
  type: TYPE_NORMAL
- en: 'With [1.0,0.2]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The “oasst” weight is too low. For the chat prompt, the new adapter tends to
    generate only very short answers (like in this example) repeating what the “Human”
    wrote.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Combining multiple adapters is easy and cheap. It’s a useful way to add skills
    to an LLM without having to fine-tune new adapters. We can find many adapters
    online. For instance, the Hugging Face hub hosts adapters as “models” with the
    tag “PEFT” which stands for “parameter-efficient fine-tuning”.
  prefs: []
  type: TYPE_NORMAL
- en: The combination of several adapters works well but only if the adapters were
    fine-tuned with very different prompts. If not, the new adapter will be confused
    and won’t know which task to perform.
  prefs: []
  type: TYPE_NORMAL
- en: I recommend trying different weights for the weighted combination. Since the
    combination is cheap, searching for better weights is quite fast.
  prefs: []
  type: TYPE_NORMAL
- en: As for the combination method, I recommend SVD since it doesn’t produce an adapter
    with a higher rank, i.e., the new adapter will not consume more memory than the
    adapters used for the combination.
  prefs: []
  type: TYPE_NORMAL
- en: 'To support my work, consider subscribing to my newsletter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://kaitchup.substack.com/?source=post_page-----ea0bef9025cf--------------------------------)
    [## The Kaitchup - AI on a Budget | Benjamin Marie, PhD | Substack'
  prefs: []
  type: TYPE_NORMAL
- en: Weekly news, tips, and tutorials on fine-tuning, running, and serving large
    language models on your computer. Each…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: kaitchup.substack.com](https://kaitchup.substack.com/?source=post_page-----ea0bef9025cf--------------------------------)
  prefs: []
  type: TYPE_NORMAL
