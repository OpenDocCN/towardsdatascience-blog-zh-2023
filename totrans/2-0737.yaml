- en: Diffusion Probabilistic Models and Text-to-Image Generation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ‰©æ•£æ¦‚ç‡æ¨¡å‹ä¸æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/diffusion-probabilistic-models-and-text-to-image-generation-9f441d0bc786](https://towardsdatascience.com/diffusion-probabilistic-models-and-text-to-image-generation-9f441d0bc786)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/diffusion-probabilistic-models-and-text-to-image-generation-9f441d0bc786](https://towardsdatascience.com/diffusion-probabilistic-models-and-text-to-image-generation-9f441d0bc786)
- en: Photorealistic Generation of Anything You Can Think of
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½ èƒ½æƒ³è±¡çš„ä»»ä½•ä¸œè¥¿çš„ç…§ç‰‡çº§ç”Ÿæˆ
- en: '[](https://taying-cheng.medium.com/?source=post_page-----9f441d0bc786--------------------------------)[![Tim
    Cheng](../Images/d15f96a7c415f1d8348ee084af39bc66.png)](https://taying-cheng.medium.com/?source=post_page-----9f441d0bc786--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9f441d0bc786--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9f441d0bc786--------------------------------)
    [Tim Cheng](https://taying-cheng.medium.com/?source=post_page-----9f441d0bc786--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://taying-cheng.medium.com/?source=post_page-----9f441d0bc786--------------------------------)[![Tim
    Cheng](../Images/d15f96a7c415f1d8348ee084af39bc66.png)](https://taying-cheng.medium.com/?source=post_page-----9f441d0bc786--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9f441d0bc786--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9f441d0bc786--------------------------------)
    [Tim Cheng](https://taying-cheng.medium.com/?source=post_page-----9f441d0bc786--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9f441d0bc786--------------------------------)
    Â·4 min readÂ·Mar 29, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9f441d0bc786--------------------------------)
    Â·é˜…è¯»æ—¶é—´ 4 åˆ†é’ŸÂ·2023å¹´3æœˆ29æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/52d4c0fb80be58d4a941361526c36696.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52d4c0fb80be58d4a941361526c36696.png)'
- en: Figure 1\. Text-to-Image Generation. Image made by author.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 1\. æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€‚å›¾ç‰‡ç”±ä½œè€…åˆ¶ä½œã€‚
- en: 'If you are an avid follower of the newest CV papers, you would be surprised
    at the stunning results of generative networks in creating images. Many of the
    previous literature were based on the groundbreaking generative adversarial network
    (GAN) idea, but thatâ€™s no longer the case for recent papers. In fact, if you look
    closely at the newest papers such as ImageN and Staple Diffusion, you will constantly
    see a unfamiliar term: diffusion probabilistic model.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æ˜¯æœ€æ–°è®¡ç®—æœºè§†è§‰è®ºæ–‡çš„å¿ å®è¿½éšè€…ï¼Œä½ ä¸€å®šä¼šå¯¹ç”Ÿæˆç½‘ç»œåœ¨å›¾åƒç”Ÿæˆä¸­çš„æƒŠäººæ•ˆæœæ„Ÿåˆ°æƒŠè®¶ã€‚è®¸å¤šä¹‹å‰çš„æ–‡çŒ®éƒ½æ˜¯åŸºäºå¼€åˆ›æ€§çš„ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰æ€æƒ³ï¼Œä½†æœ€è¿‘çš„è®ºæ–‡æƒ…å†µå·²ä¸å†å¦‚æ­¤ã€‚äº‹å®ä¸Šï¼Œå¦‚æœä½ ä»”ç»†æŸ¥çœ‹æœ€æ–°çš„è®ºæ–‡ï¼Œå¦‚ImageNå’ŒStaple
    Diffusionï¼Œä½ ä¼šä¸æ–­çœ‹åˆ°ä¸€ä¸ªé™Œç”Ÿçš„æœ¯è¯­ï¼šæ‰©æ•£æ¦‚ç‡æ¨¡å‹ã€‚
- en: This article dives in to the very basics of the newly trending model, how it
    is learnt in a brief overview, and the exciting applications that have soon followed.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡æ·±å…¥æ¢è®¨äº†æ–°å…´è¶‹åŠ¿æ¨¡å‹çš„åŸºæœ¬çŸ¥è¯†ï¼Œç®€è¦æ¦‚è¿°äº†å¦‚ä½•å­¦ä¹ ï¼Œä»¥åŠéšä¹‹è€Œæ¥çš„ä»¤äººå…´å¥‹çš„åº”ç”¨ã€‚
- en: Start by Gradually Adding Gaussian Noisesâ€¦
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»é€æ¸æ·»åŠ é«˜æ–¯å™ªå£°å¼€å§‹â€¦â€¦
- en: '![](../Images/7b4ab211af6d43025fb87095464a5d2c.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b4ab211af6d43025fb87095464a5d2c.png)'
- en: 'Figure 2\. Overview of Denoising Diffusion Probabilistic Models. Image Retrieved
    from: [https://arxiv.org/abs/2006.11239](https://arxiv.org/abs/2006.11239).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 2\. æ‰©æ•£å»å™ªæ¦‚ç‡æ¨¡å‹æ¦‚è¿°ã€‚å›¾ç‰‡æ¥æºï¼š[https://arxiv.org/abs/2006.11239](https://arxiv.org/abs/2006.11239)ã€‚
- en: Consider an image to which a small amount of Gaussian noise is added. The image
    may becomes a little noisy, but the original content can most likely still be
    recognised. Now repeat the step again and again; eventually the image would become
    almost a pure Gaussian noise. This is known asthe forward process of a diffusion
    probabilistic model.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘ä¸€å¼ æ·»åŠ äº†å°‘é‡é«˜æ–¯å™ªå£°çš„å›¾åƒã€‚å›¾åƒå¯èƒ½å˜å¾—æœ‰äº›å˜ˆæ‚ï¼Œä½†åŸå§‹å†…å®¹ä»ç„¶å¾ˆå¯èƒ½è¢«è¯†åˆ«å‡ºæ¥ã€‚ç°åœ¨é‡å¤è¿™ä¸€æ­¥éª¤ï¼›æœ€ç»ˆï¼Œå›¾åƒå°†å˜æˆå‡ ä¹çº¯ç²¹çš„é«˜æ–¯å™ªå£°ã€‚è¿™è¢«ç§°ä¸ºæ‰©æ•£æ¦‚ç‡æ¨¡å‹çš„å‰å‘è¿‡ç¨‹ã€‚
- en: 'The goal is simple: by leveraging the fact that forward process is a Markov
    chain (the process of the current timeframe is independent from the previous timeframe),
    we can actually learn a reverse process, denoising the image on the current frame
    slightly.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®æ ‡å¾ˆç®€å•ï¼šé€šè¿‡åˆ©ç”¨å‰å‘è¿‡ç¨‹æ˜¯é©¬å°”å¯å¤«é“¾çš„äº‹å®ï¼ˆå½“å‰æ—¶é—´æ¡†æ¶çš„è¿‡ç¨‹ä¸ä¹‹å‰çš„æ—¶é—´æ¡†æ¶ç‹¬ç«‹ï¼‰ï¼Œæˆ‘ä»¬å®é™…ä¸Šå¯ä»¥å­¦ä¹ ä¸€ä¸ªé€†è¿‡ç¨‹ï¼Œåœ¨å½“å‰å¸§ä¸Šç¨å¾®å»å™ªå›¾åƒã€‚
- en: Given a properly learnt reverse process and a random Gaussian noise, we can
    now repeatedly apply the noise and ultimately obtain an image that is very similar
    to the original data distribution the process is trained â€” hence a generative
    model.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šä¸€ä¸ªç»è¿‡é€‚å½“å­¦ä¹ çš„é€†è¿‡ç¨‹å’Œä¸€ä¸ªéšæœºé«˜æ–¯å™ªå£°ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥åå¤åº”ç”¨å™ªå£°ï¼Œå¹¶æœ€ç»ˆè·å¾—ä¸€ä¸ªä¸åŸå§‹æ•°æ®åˆ†å¸ƒéå¸¸ç›¸ä¼¼çš„å›¾åƒâ€”â€”å› æ­¤è¿™æ˜¯ä¸€ä¸ªç”Ÿæˆæ¨¡å‹ã€‚
- en: One advantage of diffusion models is that the training can be done by just picking
    a random timestamp in the middle for optimisation (instead of having to fully
    reconstruct the image end-to-end). The training itself is much more stable compared
    to GANs, where small hyperparameter differences could easily lead to model collapse.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰©æ•£æ¨¡å‹çš„ä¸€ä¸ªä¼˜ç‚¹æ˜¯ï¼Œè®­ç»ƒå¯ä»¥é€šè¿‡ä»…é€‰æ‹©ä¸­é—´çš„éšæœºæ—¶é—´æˆ³è¿›è¡Œä¼˜åŒ–ï¼ˆè€Œä¸æ˜¯å¿…é¡»å®Œå…¨é‡å»ºå›¾åƒï¼‰ã€‚ä¸GANç›¸æ¯”ï¼Œè®­ç»ƒæœ¬èº«æ›´åŠ ç¨³å®šï¼Œå› ä¸ºå°çš„è¶…å‚æ•°å·®å¼‚å¯èƒ½ä¼šå¯¼è‡´æ¨¡å‹å´©æºƒã€‚
- en: '*Note that this is a very high-level overview of what a denoising diffusion
    probabilistic model looks like. For the mathematical details, please refer to*
    [*here*](https://arxiv.org/abs/2006.11239) *and* [*here*](/understanding-diffusion-probabilistic-models-dpms-1940329d6048)*.*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*è¯·æ³¨æ„ï¼Œè¿™åªæ˜¯å¯¹å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹çš„ä¸€ä¸ªéå¸¸é«˜å±‚æ¬¡çš„æ¦‚è¿°ã€‚æœ‰å…³æ•°å­¦ç»†èŠ‚ï¼Œè¯·å‚è€ƒ* [*è¿™é‡Œ*](https://arxiv.org/abs/2006.11239)
    *å’Œ* [*è¿™é‡Œ*](/understanding-diffusion-probabilistic-models-dpms-1940329d6048)*ã€‚ '
- en: Amazing Results of Text-to-Image Generation
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ–‡å­—åˆ°å›¾åƒç”Ÿæˆçš„æƒŠäººç»“æœ
- en: '![](../Images/69a2dbdd4dc833066b7e5ebf9192f649.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/69a2dbdd4dc833066b7e5ebf9192f649.png)'
- en: 'Figure 3\. Results produced by ImageN. The text prompts are below the images.
    Image retrieved from: [https://arxiv.org/abs/2205.11487](https://arxiv.org/abs/2205.11487).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾3\. ImageNç”Ÿæˆçš„ç»“æœã€‚æ–‡æœ¬æç¤ºä½äºå›¾åƒä¸‹æ–¹ã€‚å›¾åƒæ¥æºäºï¼š[https://arxiv.org/abs/2205.11487](https://arxiv.org/abs/2205.11487)ã€‚
- en: The idea of denoising diffusion models for image generations was first published
    in 2020, but it was not until the recent Google Paper [ImageN](https://arxiv.org/abs/2205.11487)
    that truly blew up the field.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒç”Ÿæˆä¸­çš„å»å™ªæ‰©æ•£æ¨¡å‹çš„ç†å¿µé¦–æ¬¡å‘å¸ƒäº2020å¹´ï¼Œä½†ç›´åˆ°æœ€è¿‘çš„Googleè®ºæ–‡[ImageN](https://arxiv.org/abs/2205.11487)æ‰çœŸæ­£å¼•çˆ†äº†è¿™ä¸ªé¢†åŸŸã€‚
- en: Like GANs, diffusion models can also be conditioned on prompts such as images
    and texts. The Google Research Brain Team suggested that large-frozen language
    models are in fact great encoders for providing the text conditions for photorealistic
    generations.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼äºGANï¼Œæ‰©æ•£æ¨¡å‹ä¹Ÿå¯ä»¥æ ¹æ®å›¾åƒå’Œæ–‡æœ¬ç­‰æç¤ºè¿›è¡Œæ¡ä»¶åŒ–ã€‚Google Research Brainå›¢é˜Ÿå»ºè®®ï¼Œå†»ç»“çš„å¤§å‹è¯­è¨€æ¨¡å‹å®é™…ä¸Šæ˜¯æä¾›æ–‡æœ¬æ¡ä»¶ä»¥ç”ŸæˆçœŸå®æ„Ÿå›¾åƒçš„ç»ä½³ç¼–ç å™¨ã€‚
- en: Shifting from 2D to 3Dâ€¦
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»2Dåˆ°3Dçš„è½¬å˜â€¦â€¦
- en: '![](../Images/7e91fa6b38cba9d6dd29237a73851fb1.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e91fa6b38cba9d6dd29237a73851fb1.png)'
- en: 'Figure 4\. Overview of the DreamFusion pipeline. Image retrieved from: [https://arxiv.org/abs/2209.14988](https://arxiv.org/abs/2209.14988).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4\. DreamFusionæµç¨‹æ¦‚è¿°ã€‚å›¾åƒæ¥æºäºï¼š[https://arxiv.org/abs/2209.14988](https://arxiv.org/abs/2209.14988)ã€‚
- en: As with numerous computer vision trends, the excelling performances in the two-dimensional
    domain leads to ambitions of extending into 3D; diffusion models follow no different
    path. Recently, Poole et al. proposed [DreamFusion](https://arxiv.org/abs/2209.14988)
    a text-to-3D model building on the strong foundations of ImageN and NeRF.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ä¼—å¤šè®¡ç®—æœºè§†è§‰è¶‹åŠ¿ä¸€æ ·ï¼Œåœ¨äºŒç»´é¢†åŸŸçš„å‡ºè‰²è¡¨ç°æ¿€å‘äº†å‘3Dæ‰©å±•çš„é›„å¿ƒï¼›æ‰©æ•£æ¨¡å‹ä¹Ÿä¸ä¾‹å¤–ã€‚æœ€è¿‘ï¼ŒPooleç­‰äººæå‡ºäº†åŸºäºImageNå’ŒNeRFåšå®åŸºç¡€çš„[DreamFusion](https://arxiv.org/abs/2209.14988)æ–‡æœ¬åˆ°3Dæ¨¡å‹ã€‚
- en: '*For a brief overview of NeRF, please refer* [*here*](https://medium.com/p/db4a0d4c391b)*.*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*æœ‰å…³NeRFçš„ç®€è¦æ¦‚è¿°ï¼Œè¯·å‚é˜…* [*è¿™é‡Œ*](https://medium.com/p/db4a0d4c391b)*ã€‚ '
- en: Figure 4 refers to the pipeline of DreamFusion. The pipeline starts with a randomly
    initialised NeRF. Based on the generated density, albedo, and normals (with a
    given light source), the network outputs the shading and subsequently the colour
    of NeRF form a particular camera angle. The rendered image is combined with a
    Gaussian noise, and the goal is to utilise a frozen ImageN model to reconstruct
    the image and subsequently update the NeRF model.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4å±•ç¤ºäº†DreamFusionçš„æµç¨‹å›¾ã€‚è¯¥æµç¨‹ä»ä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„NeRFå¼€å§‹ã€‚åŸºäºç”Ÿæˆçš„å¯†åº¦ã€åå°„ç‡å’Œæ³•çº¿ï¼ˆåœ¨ç»™å®šå…‰æºä¸‹ï¼‰ï¼Œç½‘ç»œè¾“å‡ºç€è‰²ï¼Œå¹¶éšåæ ¹æ®ç‰¹å®šæ‘„åƒæœºè§’åº¦çš„NeRFå½¢æˆé¢œè‰²ã€‚æ¸²æŸ“çš„å›¾åƒä¸é«˜æ–¯å™ªå£°ç»“åˆï¼Œç›®æ ‡æ˜¯åˆ©ç”¨å†»ç»“çš„ImageNæ¨¡å‹é‡å»ºå›¾åƒï¼Œå¹¶éšåæ›´æ–°NeRFæ¨¡å‹ã€‚
- en: '![](../Images/153f2af140672369b84928d080c0d2f6.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/153f2af140672369b84928d080c0d2f6.png)'
- en: 'Figure 5\. Results of DreamFusion. Image retrieved from: [https://arxiv.org/abs/2209.14988](https://arxiv.org/abs/2209.14988).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾5\. DreamFusionçš„ç»“æœã€‚å›¾åƒæ¥æºäºï¼š[https://arxiv.org/abs/2209.14988](https://arxiv.org/abs/2209.14988)ã€‚
- en: Some of the stunning 3D results are presented in the gallery as show on Figure
    5\. With consistent colours and shapes of an object fully portrayed form a simple
    image.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€äº›ä»¤äººæƒŠå¹çš„3Dç»“æœå±•ç¤ºåœ¨å›¾5æ‰€ç¤ºçš„ç”»å»Šä¸­ã€‚é€šè¿‡ç®€å•å›¾åƒå®Œå…¨å±•ç°äº†å¯¹è±¡çš„ä¸€è‡´é¢œè‰²å’Œå½¢çŠ¶ã€‚
- en: Recent work such as [Magic3D](https://research.nvidia.com/labs/dir/magic3d/)
    further improved the pipeline by making the reconstruction faster and much more
    fine-grained.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘çš„å·¥ä½œå¦‚[Magic3D](https://research.nvidia.com/labs/dir/magic3d/)è¿›ä¸€æ­¥æ”¹è¿›äº†æµç¨‹ï¼Œä½¿é‡å»ºè¿‡ç¨‹æ›´å¿«ä¸”æ›´ç»†è‡´ã€‚
- en: '**End Note**'
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ç»“æŸå¤‡æ³¨**'
- en: And there you have it â€” an overview of the progression in diffusion models for
    image generation. When simple words transform into vivid images, it becomes much
    easier for everyone to imagine and paint their craziest thoughts.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯å¯¹å›¾åƒç”Ÿæˆæ‰©æ•£æ¨¡å‹è¿›å±•çš„æ¦‚è¿°ã€‚å½“ç®€å•çš„è¯æ±‡è½¬å˜ä¸ºç”ŸåŠ¨çš„å›¾åƒæ—¶ï¼Œå¤§å®¶æ›´å®¹æ˜“æƒ³è±¡å¹¶æç»˜ä»–ä»¬æœ€ç–¯ç‹‚çš„æƒ³æ³•ã€‚
- en: '*â€œWriting is the painting of the voiceâ€ â€” Voltaire*'
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*â€œå†™ä½œæ˜¯å£°éŸ³çš„ç»˜ç”»â€ â€” ä¼å°”æ³°*'
- en: '*Thank you for making it this far* ğŸ™*!* *I regularly write about different
    areas of computer vision/deep learning, so* [*join and subscribe*](https://taying-cheng.medium.com/membership)
    *if you are interested to know more!*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ„Ÿè°¢ä½ èƒ½çœ‹åˆ°è¿™é‡Œ* ğŸ™*!* *æˆ‘å®šæœŸæ’°å†™æœ‰å…³è®¡ç®—æœºè§†è§‰/æ·±åº¦å­¦ä¹ çš„ä¸åŒé¢†åŸŸçš„æ–‡ç« ï¼Œå¦‚æœä½ æœ‰å…´è¶£äº†è§£æ›´å¤šï¼Œ* [*è¯·åŠ å…¥å¹¶è®¢é˜…*](https://taying-cheng.medium.com/membership)
    *ï¼*'
