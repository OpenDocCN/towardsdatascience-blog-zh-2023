- en: Diffusion Probabilistic Models and Text-to-Image Generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/diffusion-probabilistic-models-and-text-to-image-generation-9f441d0bc786](https://towardsdatascience.com/diffusion-probabilistic-models-and-text-to-image-generation-9f441d0bc786)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Photorealistic Generation of Anything You Can Think of
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://taying-cheng.medium.com/?source=post_page-----9f441d0bc786--------------------------------)[![Tim
    Cheng](../Images/d15f96a7c415f1d8348ee084af39bc66.png)](https://taying-cheng.medium.com/?source=post_page-----9f441d0bc786--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9f441d0bc786--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9f441d0bc786--------------------------------)
    [Tim Cheng](https://taying-cheng.medium.com/?source=post_page-----9f441d0bc786--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9f441d0bc786--------------------------------)
    ¬∑4 min read¬∑Mar 29, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/52d4c0fb80be58d4a941361526c36696.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. Text-to-Image Generation. Image made by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are an avid follower of the newest CV papers, you would be surprised
    at the stunning results of generative networks in creating images. Many of the
    previous literature were based on the groundbreaking generative adversarial network
    (GAN) idea, but that‚Äôs no longer the case for recent papers. In fact, if you look
    closely at the newest papers such as ImageN and Staple Diffusion, you will constantly
    see a unfamiliar term: diffusion probabilistic model.'
  prefs: []
  type: TYPE_NORMAL
- en: This article dives in to the very basics of the newly trending model, how it
    is learnt in a brief overview, and the exciting applications that have soon followed.
  prefs: []
  type: TYPE_NORMAL
- en: Start by Gradually Adding Gaussian Noises‚Ä¶
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/7b4ab211af6d43025fb87095464a5d2c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2\. Overview of Denoising Diffusion Probabilistic Models. Image Retrieved
    from: [https://arxiv.org/abs/2006.11239](https://arxiv.org/abs/2006.11239).'
  prefs: []
  type: TYPE_NORMAL
- en: Consider an image to which a small amount of Gaussian noise is added. The image
    may becomes a little noisy, but the original content can most likely still be
    recognised. Now repeat the step again and again; eventually the image would become
    almost a pure Gaussian noise. This is known asthe forward process of a diffusion
    probabilistic model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal is simple: by leveraging the fact that forward process is a Markov
    chain (the process of the current timeframe is independent from the previous timeframe),
    we can actually learn a reverse process, denoising the image on the current frame
    slightly.'
  prefs: []
  type: TYPE_NORMAL
- en: Given a properly learnt reverse process and a random Gaussian noise, we can
    now repeatedly apply the noise and ultimately obtain an image that is very similar
    to the original data distribution the process is trained ‚Äî hence a generative
    model.
  prefs: []
  type: TYPE_NORMAL
- en: One advantage of diffusion models is that the training can be done by just picking
    a random timestamp in the middle for optimisation (instead of having to fully
    reconstruct the image end-to-end). The training itself is much more stable compared
    to GANs, where small hyperparameter differences could easily lead to model collapse.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note that this is a very high-level overview of what a denoising diffusion
    probabilistic model looks like. For the mathematical details, please refer to*
    [*here*](https://arxiv.org/abs/2006.11239) *and* [*here*](/understanding-diffusion-probabilistic-models-dpms-1940329d6048)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: Amazing Results of Text-to-Image Generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/69a2dbdd4dc833066b7e5ebf9192f649.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3\. Results produced by ImageN. The text prompts are below the images.
    Image retrieved from: [https://arxiv.org/abs/2205.11487](https://arxiv.org/abs/2205.11487).'
  prefs: []
  type: TYPE_NORMAL
- en: The idea of denoising diffusion models for image generations was first published
    in 2020, but it was not until the recent Google Paper [ImageN](https://arxiv.org/abs/2205.11487)
    that truly blew up the field.
  prefs: []
  type: TYPE_NORMAL
- en: Like GANs, diffusion models can also be conditioned on prompts such as images
    and texts. The Google Research Brain Team suggested that large-frozen language
    models are in fact great encoders for providing the text conditions for photorealistic
    generations.
  prefs: []
  type: TYPE_NORMAL
- en: Shifting from 2D to 3D‚Ä¶
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/7e91fa6b38cba9d6dd29237a73851fb1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4\. Overview of the DreamFusion pipeline. Image retrieved from: [https://arxiv.org/abs/2209.14988](https://arxiv.org/abs/2209.14988).'
  prefs: []
  type: TYPE_NORMAL
- en: As with numerous computer vision trends, the excelling performances in the two-dimensional
    domain leads to ambitions of extending into 3D; diffusion models follow no different
    path. Recently, Poole et al. proposed [DreamFusion](https://arxiv.org/abs/2209.14988)
    a text-to-3D model building on the strong foundations of ImageN and NeRF.
  prefs: []
  type: TYPE_NORMAL
- en: '*For a brief overview of NeRF, please refer* [*here*](https://medium.com/p/db4a0d4c391b)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4 refers to the pipeline of DreamFusion. The pipeline starts with a randomly
    initialised NeRF. Based on the generated density, albedo, and normals (with a
    given light source), the network outputs the shading and subsequently the colour
    of NeRF form a particular camera angle. The rendered image is combined with a
    Gaussian noise, and the goal is to utilise a frozen ImageN model to reconstruct
    the image and subsequently update the NeRF model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/153f2af140672369b84928d080c0d2f6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5\. Results of DreamFusion. Image retrieved from: [https://arxiv.org/abs/2209.14988](https://arxiv.org/abs/2209.14988).'
  prefs: []
  type: TYPE_NORMAL
- en: Some of the stunning 3D results are presented in the gallery as show on Figure
    5\. With consistent colours and shapes of an object fully portrayed form a simple
    image.
  prefs: []
  type: TYPE_NORMAL
- en: Recent work such as [Magic3D](https://research.nvidia.com/labs/dir/magic3d/)
    further improved the pipeline by making the reconstruction faster and much more
    fine-grained.
  prefs: []
  type: TYPE_NORMAL
- en: '**End Note**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: And there you have it ‚Äî an overview of the progression in diffusion models for
    image generation. When simple words transform into vivid images, it becomes much
    easier for everyone to imagine and paint their craziest thoughts.
  prefs: []
  type: TYPE_NORMAL
- en: '*‚ÄúWriting is the painting of the voice‚Äù ‚Äî Voltaire*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Thank you for making it this far* üôè*!* *I regularly write about different
    areas of computer vision/deep learning, so* [*join and subscribe*](https://taying-cheng.medium.com/membership)
    *if you are interested to know more!*'
  prefs: []
  type: TYPE_NORMAL
