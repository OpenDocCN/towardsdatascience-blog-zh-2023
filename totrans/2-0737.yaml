- en: Diffusion Probabilistic Models and Text-to-Image Generation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩散概率模型与文本到图像生成
- en: 原文：[https://towardsdatascience.com/diffusion-probabilistic-models-and-text-to-image-generation-9f441d0bc786](https://towardsdatascience.com/diffusion-probabilistic-models-and-text-to-image-generation-9f441d0bc786)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/diffusion-probabilistic-models-and-text-to-image-generation-9f441d0bc786](https://towardsdatascience.com/diffusion-probabilistic-models-and-text-to-image-generation-9f441d0bc786)
- en: Photorealistic Generation of Anything You Can Think of
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你能想象的任何东西的照片级生成
- en: '[](https://taying-cheng.medium.com/?source=post_page-----9f441d0bc786--------------------------------)[![Tim
    Cheng](../Images/d15f96a7c415f1d8348ee084af39bc66.png)](https://taying-cheng.medium.com/?source=post_page-----9f441d0bc786--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9f441d0bc786--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9f441d0bc786--------------------------------)
    [Tim Cheng](https://taying-cheng.medium.com/?source=post_page-----9f441d0bc786--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://taying-cheng.medium.com/?source=post_page-----9f441d0bc786--------------------------------)[![Tim
    Cheng](../Images/d15f96a7c415f1d8348ee084af39bc66.png)](https://taying-cheng.medium.com/?source=post_page-----9f441d0bc786--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9f441d0bc786--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9f441d0bc786--------------------------------)
    [Tim Cheng](https://taying-cheng.medium.com/?source=post_page-----9f441d0bc786--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9f441d0bc786--------------------------------)
    ·4 min read·Mar 29, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9f441d0bc786--------------------------------)
    ·阅读时间 4 分钟·2023年3月29日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/52d4c0fb80be58d4a941361526c36696.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52d4c0fb80be58d4a941361526c36696.png)'
- en: Figure 1\. Text-to-Image Generation. Image made by author.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 文本到图像生成。图片由作者制作。
- en: 'If you are an avid follower of the newest CV papers, you would be surprised
    at the stunning results of generative networks in creating images. Many of the
    previous literature were based on the groundbreaking generative adversarial network
    (GAN) idea, but that’s no longer the case for recent papers. In fact, if you look
    closely at the newest papers such as ImageN and Staple Diffusion, you will constantly
    see a unfamiliar term: diffusion probabilistic model.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是最新计算机视觉论文的忠实追随者，你一定会对生成网络在图像生成中的惊人效果感到惊讶。许多之前的文献都是基于开创性的生成对抗网络（GAN）思想，但最近的论文情况已不再如此。事实上，如果你仔细查看最新的论文，如ImageN和Staple
    Diffusion，你会不断看到一个陌生的术语：扩散概率模型。
- en: This article dives in to the very basics of the newly trending model, how it
    is learnt in a brief overview, and the exciting applications that have soon followed.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本文深入探讨了新兴趋势模型的基本知识，简要概述了如何学习，以及随之而来的令人兴奋的应用。
- en: Start by Gradually Adding Gaussian Noises…
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从逐渐添加高斯噪声开始……
- en: '![](../Images/7b4ab211af6d43025fb87095464a5d2c.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b4ab211af6d43025fb87095464a5d2c.png)'
- en: 'Figure 2\. Overview of Denoising Diffusion Probabilistic Models. Image Retrieved
    from: [https://arxiv.org/abs/2006.11239](https://arxiv.org/abs/2006.11239).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 扩散去噪概率模型概述。图片来源：[https://arxiv.org/abs/2006.11239](https://arxiv.org/abs/2006.11239)。
- en: Consider an image to which a small amount of Gaussian noise is added. The image
    may becomes a little noisy, but the original content can most likely still be
    recognised. Now repeat the step again and again; eventually the image would become
    almost a pure Gaussian noise. This is known asthe forward process of a diffusion
    probabilistic model.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一张添加了少量高斯噪声的图像。图像可能变得有些嘈杂，但原始内容仍然很可能被识别出来。现在重复这一步骤；最终，图像将变成几乎纯粹的高斯噪声。这被称为扩散概率模型的前向过程。
- en: 'The goal is simple: by leveraging the fact that forward process is a Markov
    chain (the process of the current timeframe is independent from the previous timeframe),
    we can actually learn a reverse process, denoising the image on the current frame
    slightly.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 目标很简单：通过利用前向过程是马尔可夫链的事实（当前时间框架的过程与之前的时间框架独立），我们实际上可以学习一个逆过程，在当前帧上稍微去噪图像。
- en: Given a properly learnt reverse process and a random Gaussian noise, we can
    now repeatedly apply the noise and ultimately obtain an image that is very similar
    to the original data distribution the process is trained — hence a generative
    model.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个经过适当学习的逆过程和一个随机高斯噪声，我们现在可以反复应用噪声，并最终获得一个与原始数据分布非常相似的图像——因此这是一个生成模型。
- en: One advantage of diffusion models is that the training can be done by just picking
    a random timestamp in the middle for optimisation (instead of having to fully
    reconstruct the image end-to-end). The training itself is much more stable compared
    to GANs, where small hyperparameter differences could easily lead to model collapse.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型的一个优点是，训练可以通过仅选择中间的随机时间戳进行优化（而不是必须完全重建图像）。与GAN相比，训练本身更加稳定，因为小的超参数差异可能会导致模型崩溃。
- en: '*Note that this is a very high-level overview of what a denoising diffusion
    probabilistic model looks like. For the mathematical details, please refer to*
    [*here*](https://arxiv.org/abs/2006.11239) *and* [*here*](/understanding-diffusion-probabilistic-models-dpms-1940329d6048)*.*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*请注意，这只是对去噪扩散概率模型的一个非常高层次的概述。有关数学细节，请参考* [*这里*](https://arxiv.org/abs/2006.11239)
    *和* [*这里*](/understanding-diffusion-probabilistic-models-dpms-1940329d6048)*。 '
- en: Amazing Results of Text-to-Image Generation
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文字到图像生成的惊人结果
- en: '![](../Images/69a2dbdd4dc833066b7e5ebf9192f649.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/69a2dbdd4dc833066b7e5ebf9192f649.png)'
- en: 'Figure 3\. Results produced by ImageN. The text prompts are below the images.
    Image retrieved from: [https://arxiv.org/abs/2205.11487](https://arxiv.org/abs/2205.11487).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. ImageN生成的结果。文本提示位于图像下方。图像来源于：[https://arxiv.org/abs/2205.11487](https://arxiv.org/abs/2205.11487)。
- en: The idea of denoising diffusion models for image generations was first published
    in 2020, but it was not until the recent Google Paper [ImageN](https://arxiv.org/abs/2205.11487)
    that truly blew up the field.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图像生成中的去噪扩散模型的理念首次发布于2020年，但直到最近的Google论文[ImageN](https://arxiv.org/abs/2205.11487)才真正引爆了这个领域。
- en: Like GANs, diffusion models can also be conditioned on prompts such as images
    and texts. The Google Research Brain Team suggested that large-frozen language
    models are in fact great encoders for providing the text conditions for photorealistic
    generations.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于GAN，扩散模型也可以根据图像和文本等提示进行条件化。Google Research Brain团队建议，冻结的大型语言模型实际上是提供文本条件以生成真实感图像的绝佳编码器。
- en: Shifting from 2D to 3D…
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从2D到3D的转变……
- en: '![](../Images/7e91fa6b38cba9d6dd29237a73851fb1.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e91fa6b38cba9d6dd29237a73851fb1.png)'
- en: 'Figure 4\. Overview of the DreamFusion pipeline. Image retrieved from: [https://arxiv.org/abs/2209.14988](https://arxiv.org/abs/2209.14988).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. DreamFusion流程概述。图像来源于：[https://arxiv.org/abs/2209.14988](https://arxiv.org/abs/2209.14988)。
- en: As with numerous computer vision trends, the excelling performances in the two-dimensional
    domain leads to ambitions of extending into 3D; diffusion models follow no different
    path. Recently, Poole et al. proposed [DreamFusion](https://arxiv.org/abs/2209.14988)
    a text-to-3D model building on the strong foundations of ImageN and NeRF.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 与众多计算机视觉趋势一样，在二维领域的出色表现激发了向3D扩展的雄心；扩散模型也不例外。最近，Poole等人提出了基于ImageN和NeRF坚实基础的[DreamFusion](https://arxiv.org/abs/2209.14988)文本到3D模型。
- en: '*For a brief overview of NeRF, please refer* [*here*](https://medium.com/p/db4a0d4c391b)*.*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*有关NeRF的简要概述，请参阅* [*这里*](https://medium.com/p/db4a0d4c391b)*。 '
- en: Figure 4 refers to the pipeline of DreamFusion. The pipeline starts with a randomly
    initialised NeRF. Based on the generated density, albedo, and normals (with a
    given light source), the network outputs the shading and subsequently the colour
    of NeRF form a particular camera angle. The rendered image is combined with a
    Gaussian noise, and the goal is to utilise a frozen ImageN model to reconstruct
    the image and subsequently update the NeRF model.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图4展示了DreamFusion的流程图。该流程从一个随机初始化的NeRF开始。基于生成的密度、反射率和法线（在给定光源下），网络输出着色，并随后根据特定摄像机角度的NeRF形成颜色。渲染的图像与高斯噪声结合，目标是利用冻结的ImageN模型重建图像，并随后更新NeRF模型。
- en: '![](../Images/153f2af140672369b84928d080c0d2f6.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/153f2af140672369b84928d080c0d2f6.png)'
- en: 'Figure 5\. Results of DreamFusion. Image retrieved from: [https://arxiv.org/abs/2209.14988](https://arxiv.org/abs/2209.14988).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图5\. DreamFusion的结果。图像来源于：[https://arxiv.org/abs/2209.14988](https://arxiv.org/abs/2209.14988)。
- en: Some of the stunning 3D results are presented in the gallery as show on Figure
    5\. With consistent colours and shapes of an object fully portrayed form a simple
    image.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一些令人惊叹的3D结果展示在图5所示的画廊中。通过简单图像完全展现了对象的一致颜色和形状。
- en: Recent work such as [Magic3D](https://research.nvidia.com/labs/dir/magic3d/)
    further improved the pipeline by making the reconstruction faster and much more
    fine-grained.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的工作如[Magic3D](https://research.nvidia.com/labs/dir/magic3d/)进一步改进了流程，使重建过程更快且更细致。
- en: '**End Note**'
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**结束备注**'
- en: And there you have it — an overview of the progression in diffusion models for
    image generation. When simple words transform into vivid images, it becomes much
    easier for everyone to imagine and paint their craziest thoughts.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是对图像生成扩散模型进展的概述。当简单的词汇转变为生动的图像时，大家更容易想象并描绘他们最疯狂的想法。
- en: '*“Writing is the painting of the voice” — Voltaire*'
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“写作是声音的绘画” — 伏尔泰*'
- en: '*Thank you for making it this far* 🙏*!* *I regularly write about different
    areas of computer vision/deep learning, so* [*join and subscribe*](https://taying-cheng.medium.com/membership)
    *if you are interested to know more!*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*感谢你能看到这里* 🙏*!* *我定期撰写有关计算机视觉/深度学习的不同领域的文章，如果你有兴趣了解更多，* [*请加入并订阅*](https://taying-cheng.medium.com/membership)
    *！*'
