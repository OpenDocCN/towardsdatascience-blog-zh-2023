- en: 'Applied Reinforcement Learning V: Normalized Advantage Function (NAF) for Continuous
    Control'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用强化学习 V：用于连续控制的归一化优势函数（NAF）
- en: 原文：[https://towardsdatascience.com/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095](https://towardsdatascience.com/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095](https://towardsdatascience.com/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095)
- en: Introduction and explanation of the NAF algorithm, widely used in continuous
    control tasks
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NAF 算法的介绍和解释，这是一种广泛用于连续控制任务的算法
- en: '[](https://medium.com/@JavierMtz5?source=post_page-----62ad143d3095--------------------------------)[![Javier
    Martínez Ojeda](../Images/5b5df4220fa64c13232c29de9b4177af.png)](https://medium.com/@JavierMtz5?source=post_page-----62ad143d3095--------------------------------)[](https://towardsdatascience.com/?source=post_page-----62ad143d3095--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----62ad143d3095--------------------------------)
    [Javier Martínez Ojeda](https://medium.com/@JavierMtz5?source=post_page-----62ad143d3095--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@JavierMtz5?source=post_page-----62ad143d3095--------------------------------)[![哈维尔·马丁内斯·奥赫达](../Images/5b5df4220fa64c13232c29de9b4177af.png)](https://medium.com/@JavierMtz5?source=post_page-----62ad143d3095--------------------------------)[](https://towardsdatascience.com/?source=post_page-----62ad143d3095--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----62ad143d3095--------------------------------)
    [哈维尔·马丁内斯·奥赫达](https://medium.com/@JavierMtz5?source=post_page-----62ad143d3095--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----62ad143d3095--------------------------------)
    ·9 min read·Jan 19, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----62ad143d3095--------------------------------)
    ·9 分钟阅读·2023 年 1 月 19 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/980806b6e676bcf2f2e28c3dca2000f0.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/980806b6e676bcf2f2e28c3dca2000f0.png)'
- en: Photo by [Sufyan](https://unsplash.com/@blenderdesigner_1688?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Sufyan](https://unsplash.com/@blenderdesigner_1688?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: If you want to read this article without a Premium Medium account, you can do
    it from this friend link :)
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你想在没有 Premium Medium 账户的情况下阅读这篇文章，你可以通过这个朋友链接来查看 :)
- en: '[https://www.learnml.wiki/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control/](https://www.learnml.wiki/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control/)'
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://www.learnml.wiki/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control/](https://www.learnml.wiki/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control/)'
- en: Previous articles in this series have introduced and explained two Reinforcement
    Learning algorithms that have been widely used since their inception:[**Q-Learning**](https://medium.com/towards-data-science/applied-reinforcement-learning-i-q-learning-d6086c1f437)
    and [**DQN**](https://medium.com/towards-data-science/applied-reinforcement-learning-iii-deep-q-networks-dqn-8f0e38196ba9).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本系列之前的文章介绍并解释了两种自其诞生以来广泛使用的强化学习算法：[**Q-Learning**](https://medium.com/towards-data-science/applied-reinforcement-learning-i-q-learning-d6086c1f437)
    和 [**DQN**](https://medium.com/towards-data-science/applied-reinforcement-learning-iii-deep-q-networks-dqn-8f0e38196ba9)。
- en: '**Q-Learning** stores the Q-Values in an action-state matrix, such that to
    obtain the action *a* with the largest Q-Value in state *s*, the largest element
    of the Q-Value matrix for row *s* must be found, which makes its application to
    continuous state or action spaces impossible since the Q-Value matrix would be
    infinite.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**Q-Learning** 将 Q 值存储在一个动作-状态矩阵中，因此，要在状态 *s* 中获得最大的 Q 值动作 *a*，必须找到 Q 值矩阵中第
    *s* 行的最大元素，这使得其在连续状态或动作空间中的应用变得不可能，因为 Q 值矩阵将是无限的。'
- en: On the other hand, **DQN** partially solves this problem by making use of a
    neural network to obtain the Q-Values associated to a state *s*, such that the
    output of the neural network are the Q-Values for each possible action of the
    agent (the equivalent to a row in the action-state matrix of Q-Learning). This
    algorithm allows training in environments with a continuous state space, but it
    is still impossible to train in an environment with a continuous action space,
    since the output of the neural network (which has as many elements as possible
    actions) would have an infinite length.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，**DQN** 通过利用神经网络来获取与状态 *s* 相关的 Q-值，从而部分解决了这个问题，使神经网络的输出为每个可能的动作的 Q-值（相当于
    Q-Learning 的动作-状态矩阵中的一行）。该算法允许在具有连续状态空间的环境中进行训练，但仍然无法在具有连续动作空间的环境中进行训练，因为神经网络的输出（具有与可能动作一样多的元素）将会是无限长度的。
- en: The **NAF algorithm** introduced by Shixiang Gu et al. in **[1]**, unlike Q-Learning
    or DQN, allows training in continuous state and action space environments, adding
    a great deal of versatility in terms of possible applications. Reinforcement learning
    algorithms for continuous environments such as NAF are commonly used in the field
    of control, especially in robotics, because they are able to train in environments
    that more closely represent reality.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**NAF 算法**由 Shixiang Gu 等人提出于**[1]**，与 Q-Learning 或 DQN 不同，它允许在连续状态和动作空间环境中进行训练，增加了许多应用的灵活性。像
    NAF 这样的连续环境中的强化学习算法通常在控制领域中使用，特别是在机器人技术中，因为它们能够在更贴近现实的环境中进行训练。'
- en: '![](../Images/95034a16b9224f41805bfa46c145eb24.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/95034a16b9224f41805bfa46c145eb24.png)'
- en: Types of RL Algorithms and their possible State/Action Spaces. Image by author
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习算法的类型及其可能的状态/动作空间。作者提供的图片
- en: Introductory Concepts
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言概念
- en: Advantage Function
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优势函数
- en: '**State-Value Function V** and **Action-Value Function** (**Q-Function**) **Q**,
    both explained in the [first article of this series](https://medium.com/towards-data-science/applied-reinforcement-learning-i-q-learning-d6086c1f437),
    determine the benefit of being in a state while following a certain policy and
    the benefit of taking an action from a given state while following certain policy,
    respectively. Both functions, as well as the definition of **V** with respect
    to **Q**, can be seen below.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**状态-价值函数 V** 和 **动作-价值函数**（**Q-函数**）**Q**，都在 [本系列的第一篇文章](https://medium.com/towards-data-science/applied-reinforcement-learning-i-q-learning-d6086c1f437)
    中进行了讲解，分别确定了在遵循某个策略时处于某状态的好处以及在遵循某个策略时从给定状态采取某个动作的好处。这两个函数，以及 **V** 相对于 **Q**
    的定义，可以在下方查看。'
- en: '![](../Images/2bc6f744389fa96fe75abca4ca0eb4df.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2bc6f744389fa96fe75abca4ca0eb4df.png)'
- en: Q-Function, Value Function and Q-V Relation. Image by author
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Q-函数、价值函数和 Q-V 关系。作者提供的图片
- en: Since **Q** returns the benefit of taking a certain action in a state, while
    **V** returns the benefit of being in a state, the difference of both returns
    information about how advantageous it is to take a certain action in a state with
    respect to the rest of actions, or the extra reward that the agent will receive
    by taking that action with respect to the rest of actions. This difference is
    called **Advantage Function**, and its equation is shown below.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 **Q** 返回在某状态下采取某个动作的好处，而 **V** 返回处于某状态的好处，因此两者之间的差异提供了关于在某状态下相对于其他动作采取特定动作的优势信息，或者代理通过采取该动作相对于其他动作所获得的额外奖励。这个差异称为
    **优势函数**，其方程式如下所示。
- en: '![](../Images/d76d848a25d01d7ee6e0d81bdf4c357f.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d76d848a25d01d7ee6e0d81bdf4c357f.png)'
- en: Advantage Function. Image by author
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 优势函数。作者提供的图片
- en: Ornstein-Uhlenbech Noise Process (OU)
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ornstein-Uhlenbech 噪声过程（OU）
- en: As seen in previous articles, in Reinforcement Learning algorithms for discrete
    environments such as Q-Learning or DQN, exploration is performed by randomly choosing
    an action and ignoring the optimal policy, as is the case for epsilon greedy policy.
    In continuous environments, however, the action is chosen following the optimal
    policy, and adding noise to this action.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前的文章所见，在离散环境下的强化学习算法如 Q-Learning 或 DQN 中，探索是通过随机选择一个动作并忽略最优策略来进行的，比如 epsilon
    贪心策略。然而，在连续环境中，动作是根据最优策略选择的，并且在这个动作上添加噪声。
- en: The problem with adding noise to the chosen action is that, if the noise is
    uncorrelated with the previous noise and has a distribution with zero mean, then
    the actions will cancel each other out, so that the agent will not be able to
    maintain a continuous movement to any point but will get stuck and therefore will
    not be able to explore. The **Ornstein-Uhlenbech Noise Process** obtains a noise
    value correlated with the previous noise value, so that the agent can have continuous
    movements towards some direction, and therefore explore successfully.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 向选择的动作中添加噪声的问题在于，如果噪声与之前的噪声无关且具有零均值的分布，则动作会相互抵消，使得代理无法保持对任何点的连续运动，而是会陷入困境，从而无法探索。**Ornstein-Uhlenbech噪声过程**获得与之前噪声值相关的噪声值，使得代理能够向某个方向进行连续运动，因此成功地进行探索。
- en: More in-depth information about the Ornstein-Uhlenbech Noise Process can be
    found in **[2]**
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Ornstein-Uhlenbech噪声过程的更深入信息可以在**[2]**中找到。
- en: Logic behind the NAF algorithm
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NAF算法背后的逻辑
- en: The NAF algorithm makes use of a neural network that obtains as separate outputs
    a value for the **State-Value Function V** and for the **Advantage Function A**.
    The neural network obtains these outputs since, as previously explained, the result
    of the **Action-Value Function Q** can be later obtained as the sum of **V** and
    **A**.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: NAF算法利用神经网络分别获得**状态值函数V**和**优势函数A**的值。由于之前的解释，神经网络获得这些输出的原因是，**动作值函数Q**的结果可以作为**V**和**A**的总和。
- en: Like most Reinforcement Learning algorithms, NAF aims to optimize the Q-Function,
    but in its application case it is particularly complicated since it uses a neural
    network as Q-Function estimator. For this reason, the NAF algorithm makes use
    of a quadratic function for the Advantage Function, whose solution is closed and
    known, so optimization with respect to the action is easier.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 像大多数强化学习算法一样，NAF旨在优化Q-函数，但在其应用案例中，特别复杂，因为它使用神经网络作为Q-函数估计器。因此，NAF算法利用了一个二次函数来表示优势函数，其解是封闭且已知的，从而使得关于动作的优化更加容易。
- en: '![](../Images/09b25af10b4e1ad8de2d42bad265dcf1.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09b25af10b4e1ad8de2d42bad265dcf1.png)'
- en: '**Figure 1**. Q-Function and Advantage Function for NAF algorithm. Image extracted
    from **[1]**'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**图1**。NAF算法的Q-函数和优势函数。图像摘自**[1]**。'
- en: More specifically, the Q-Function will always be quadratic with respect to theaction,
    so that the *argmax Q(x, u)* for the action is always **𝜇**(**x**|𝜃) **[3]**,
    as shown in *Figure 2*. Thanks to this, the problem of not being able to obtain
    the argmax of the neural network output due to working in a continuous action
    space, as was the case with DQN, is solved in an analytical way.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，Q-函数相对于动作总是二次的，因此动作的*argmax Q(x, u)*总是**𝜇**(**x**|𝜃) **[3]**，如*图2*所示。由于此原因，解决了由于在连续动作空间中工作而无法获得神经网络输出的argmax的问题，这在DQN中曾经存在。
- en: 'By looking at the different components that make up the Q-Function, it can
    be seen that the neural network will have three different outputs: one to estimate
    the Value Function, another to obtain the action that maximizes the Q-Function
    (*argmax Q(s, a)* or **𝜇**(**x**|𝜃)), and another to calculate the matrix P (see
    *Figure 1*):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看构成Q-函数的不同组件，可以看出神经网络将有三个不同的输出：一个用于估计价值函数，另一个用于获得最大化Q-函数的动作（*argmax Q(s,
    a)* 或 **𝜇**(**x**|𝜃)），还有一个用于计算矩阵P（见*图1*）：
- en: The first output of the neural network is the estimate of the State-Value Function.
    This estimate is then used to obtain the estimate of the Q-Function, as the sum
    of the State-Value Function and the Advantage Function. This output is represented
    by **V(x|𝜃)** in *Figure 1*.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络的第一个输出是对状态值函数的估计。这个估计值随后用于获得Q-函数的估计，即状态值函数和优势函数的总和。这个输出在*图1*中表示为**V(x|𝜃)**。
- en: The second output of the neural network is **𝜇(x|𝜃)**, which is the action that
    maximizes the Q-Function on the given state, or *argmax Q(s, a)*, and therefore
    acts as the policy to be followed by the agent.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络的第二个输出是**𝜇(x|𝜃)**，即在给定状态下最大化Q-函数的动作，或*argmax Q(s, a)*，因此作为代理应遵循的策略。
- en: The third output is used to later form the state-dependent, positive-definite
    square matrix **P**(**x**|𝜃). This linear output of the neural network is used
    as entry for a lower-triangular matrix **L**(**x**|𝜃), whose diagonal terms are
    exponentiated, and from which the mentioned matrix **P**(**x**|𝜃) is constructed,
    following the following formula.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三个输出用于随后形成状态依赖的正定方阵**P**(**x**|𝜃)。这种神经网络的线性输出用作下三角矩阵**L**(**x**|𝜃)的输入，其中对角项经过指数化处理，从而构建出上述的矩阵**P**(**x**|𝜃)，其计算公式如下。
- en: '![](../Images/2cffbad3d13d3b9ae8f6ef671c1de32b.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2cffbad3d13d3b9ae8f6ef671c1de32b.png)'
- en: Formula for constructing P matrix. Extracted from **[1]**
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 构造P矩阵的公式。摘自**[1]**
- en: '![](../Images/605c89d50581caf1849856a81b45878e.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/605c89d50581caf1849856a81b45878e.png)'
- en: '**Figure 2**. Image by author'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2**。作者提供的图像'
- en: The second and third outputs of the neural network are used to construct the
    estimate for the Advantage Function as shown in *Figure 1*, which is then added
    to the first output (the State-Value Function estimate V) to obtain the estimate
    for the Q-Function.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的第二和第三个输出用于构造优势函数的估计，如*图 1*所示，然后将其加到第一个输出（状态值函数估计V）中，以获得Q函数的估计。
- en: 'Regarding the rest of the NAF algorithm flow, it includes the same components
    and steps as the DQN algorithm explained in article [*Applied Reinforcement Learning
    III: Deep Q-Networks (DQN)*](https://medium.com/towards-data-science/applied-reinforcement-learning-iii-deep-q-networks-dqn-8f0e38196ba9).
    These components in common are the **Replay Buffer**, the **Main Neural Network**
    and the **Target Neural Network**. As for DQN, the Replay Buffer is used to store
    experiences to train the main neural network, and the target neural network is
    used to calculate the target values and compare them with the predictions from
    the main network, and then perform the backpropagation process.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 关于NAF算法的其余部分，它包含与文章[*应用强化学习 III：深度Q网络（DQN）*](https://medium.com/towards-data-science/applied-reinforcement-learning-iii-deep-q-networks-dqn-8f0e38196ba9)中解释的DQN算法相同的组件和步骤。这些共同的组件有**重放缓冲区**、**主神经网络**和**目标神经网络**。与DQN类似，重放缓冲区用于存储经验以训练主神经网络，目标神经网络用于计算目标值并与主网络的预测进行比较，然后执行反向传播过程。
- en: NAF Algorithm Flow
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NAF算法流程
- en: The flow of the NAF algorithm will be presented following the pseudocode below,
    extracted from **[1]**. As mentioned above, the NAF algorithm follows the same
    steps as the DQN algorithm, except that NAF trains its main neural network differently.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: NAF算法的流程将遵循下面的伪代码，摘自**[1]**。如前所述，NAF算法遵循与DQN算法相同的步骤，只是NAF以不同的方式训练其主神经网络。
- en: '![](../Images/e9e3dffcf427a42797efda0eedeea072.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e9e3dffcf427a42797efda0eedeea072.png)'
- en: NAF Algorithm pseudocode. Extracted from **[1]**
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: NAF算法伪代码。摘自**[1]**
- en: '*For each timestep in an episode, the agent performs the following steps:*'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*在每个时间步中，智能体执行以下步骤：*'
- en: 1\. From given state, select an action
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 从给定状态中选择一个动作
- en: The action selected is the one that maximises the estimate of the Q-Function,
    which is given by the term **𝜇(x|𝜃)**, as shown in *Figure 2*.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 选择的动作是最大化Q函数估计值的动作，由**𝜇(x|𝜃)**表示，如*图 2*所示。
- en: To this selected action the noise extracted from the Ornstein-Uhlenbech noise
    process (previosuly introduced) is added, in order to enhance the agent’s exploration.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所选择的动作，添加从Ornstein-Uhlenbech噪声过程（之前介绍过）中提取的噪声，以增强智能体的探索。
- en: 2\. Perform action on environment
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 对环境执行动作
- en: The action with noise obtained in the previous step is executed by the agent
    in the environment. After the execution of such an action, the agent receives
    information about how good the action taken was (via the **reward**), as well
    as about the new situation reached in the environment (which is the **next state**).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在环境中由智能体执行带噪声的动作。执行此动作后，智能体接收关于动作效果的信息（通过**奖励**），以及关于在环境中达到的新状态的信息（即**下一个状态**）。
- en: 3\. Store experience in Replay Buffer
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 将经验存储在重放缓冲区中
- en: The Replay Buffer stores experiences as ***{s, a, r, s’}***, being ***s*** and
    ***a*** the current state and action, and ***r*** and ***s’*** the reward and
    new state reached after performing the action from the current state.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 重放缓冲区将经验存储为***{s, a, r, s’}***，其中***s***和***a***是当前状态和动作，***r***和***s’***是奖励和执行当前状态动作后的新状态。
- en: '*The following steps, from 4 to 7, are repeated as many times as stated in
    the algorithm’s hyperparameter* ***I*** *per timestep, which can be seen in the
    pseudocode above.*'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*从步骤4到7的过程会根据算法超参数* ***I*** *在每个时间步上重复进行，如上述伪代码所示。*'
- en: 4\. Sample a random batch of experiences from Replay Buffer
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 从Replay Buffer中随机抽取一批经验
- en: As explained in [the DQN article](https://medium.com/towards-data-science/applied-reinforcement-learning-iii-deep-q-networks-dqn-8f0e38196ba9),
    a batch of experiences is extracted only when the Replay Buffer has enough data
    to fill a batch. Once this condition is met, *{batch_size}* elements are randomly
    taken from the Replay Buffer, giving the possibility to learn from previous experiences,
    without the need to have lived them recently.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如[《DQN文章》](https://medium.com/towards-data-science/applied-reinforcement-learning-iii-deep-q-networks-dqn-8f0e38196ba9)所述，仅当Replay
    Buffer中有足够的数据填充一批时，才会提取一批经验。一旦满足这一条件，*{batch_size}* 元素会从Replay Buffer中随机选取，从而可以利用以前的经验进行学习，而不需要最近经历这些经验。
- en: 5\. Set the target value
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 设置目标值
- en: The target value is defined as the sum of the reward and the Value function
    estimate of the Target neural network for the next state multiplied by the discount
    factor **γ**, which is an hyperparameter of the algorithm. The formula for the
    target value is shown below, and it is also available in the pseudocode above.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 目标值定义为奖励与下一状态的目标神经网络的值函数估计值之和，乘以折扣因子**γ**，这是算法的一个超参数。目标值的公式如下所示，并且也在上述伪代码中提供。
- en: '![](../Images/54f2066a2722a652de6d7abc8fbebeab.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/54f2066a2722a652de6d7abc8fbebeab.png)'
- en: Target Value Calculation. Extracted from the pseudocode in **[1]**
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 目标值计算。摘自**[1]**中的伪代码
- en: 6\. Perform Gradient Descent
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 执行梯度下降
- en: Gradient Descent is applied to the loss, which is calculated with the estimate
    of the Q-Function obtained from the main neural network (predicted value) and
    the previously calculated target value, following the equation shown below. As
    can be seen, the loss function used is the MSE, so the loss will be the difference
    between the Q-Function estimate and the target squared.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对损失进行梯度下降，这些损失是通过主神经网络（预测值）和先前计算的目标值的Q函数估计得出的，按照下面的方程计算。如所示，使用的损失函数是均方误差（MSE），因此损失将是Q函数估计值和目标值之间的平方差。
- en: '![](../Images/d8c243ae133cd32c2fec507318ec081b.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d8c243ae133cd32c2fec507318ec081b.png)'
- en: Loss Function. Extracted from **[1]**
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数。摘自**[1]**
- en: It should be remembered that the estimate of the Q-Function is obtained from
    the sum of the estimate of the Value Function V(x**|𝜃**) plus the estimate of
    the Advantage function A(x, u**|𝜃**), whose formula is shown in *Figure 1*.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 应记住，Q函数的估计是通过值函数V(x**|𝜃**)的估计加上优势函数A(x, u**|𝜃**)的估计得出的，其公式显示在*图1*中。
- en: 7\. Softly update the Target Neural Network
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 软更新目标神经网络
- en: The weights of the Target neural network are updated with the weights of the
    Main neural network in a soft manner. This soft updation is performed as a weighted
    sum of the Main network’s weights and the Targt network’s old weights, as shown
    in the following equation.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 目标神经网络的权重以一种软方式更新为主神经网络的权重。这种软更新是主网络权重和目标网络旧权重的加权和，如下方程所示。
- en: '![](../Images/8ff611f014ac51ff20d4c8f50968f97e.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8ff611f014ac51ff20d4c8f50968f97e.png)'
- en: Soft Update for the Target Network. Extracted from **[1]**
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 目标网络的软更新。摘自**[1]**
- en: The importance of the weights of each neural network in the weighted sum is
    given by the hyperparameter **τ**. If **τ** is zero, the target network will not
    update its weights, since it will load its own old weights. If **τ** is set to
    1, the target neural network will be updated by loading the weights of the main
    network, ignoring the old weights of the target network.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在加权和中，每个神经网络权重的重要性由超参数**τ**决定。如果**τ**为零，则目标网络不会更新其权重，因为它将加载自己的旧权重。如果**τ**设为1，则目标神经网络将通过加载主网络的权重来更新，忽略目标网络的旧权重。
- en: 8\. Timestep ends — Execute the following timestep
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8\. 时间步结束 — 执行下一个时间步
- en: Once the previous steps have been completed, this same process is repeated over
    and over again until the maximum number of timesteps per episode is reached or
    until the agent reaches a terminal state. When this happens, it goes to the next
    episode.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 完成上述步骤后，将不断重复这一过程，直到达到每个回合的最大时间步数或代理到达终止状态。当发生这种情况时，将进入下一个回合。
- en: Conclusion
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The NAF algorithm achieves really good results in its implementation in continuous
    environments, so it fulfills its objective satisfactorily. The results of NAF
    in comparison with the **DDPG algorithm** **[4]** are shown below, where it can
    be seen how it improves considerably on the previous work. In addition, the beauty
    of the NAF algorithm should be highlighted, as it deals with the limitations of
    DQN for continuous environments with the cuadratic functions and its easy optimization,
    a smart and creative solution.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: NAF 算法在连续环境中的实现中取得了非常好的结果，因此它圆满地完成了其目标。NAF 与 **DDPG 算法** **[4]** 的比较结果如下，其中可以看到它在前期工作中有了显著的改进。此外，NAF
    算法的美妙之处在于，它通过平方函数及其易于优化的特性解决了 DQN 在连续环境中的局限性，是一种聪明且富有创意的解决方案。
- en: '![](../Images/9c04138c0416455bca91bfdf1eb53a50.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c04138c0416455bca91bfdf1eb53a50.png)'
- en: DDPG and NAF comparison in different tasks. Extracted from **[1]**
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG 和 NAF 在不同任务中的比较。摘自**[1]**
- en: On the other hand, although NAF has shown to be an efficient and useful algorithm,
    its logic and implementation is not simple, especially when compared to previous
    algorithms for discrete environments, which makes it hard to use.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，尽管 NAF 被证明是一个高效且有用的算法，但它的逻辑和实现并不简单，尤其是与之前用于离散环境的算法相比，这使得它难以使用。
- en: REFERENCES
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '**[1]** GU, Shixiang, et al. Continuous deep q-learning with model-based acceleration.
    En *International conference on machine learning*. PMLR, 2016\. p. 2829–2838'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**[1]** GU, Shixiang 等人。基于模型加速的连续深度 q 学习。在 *国际机器学习大会* 上。PMLR，2016年。第2829–2838页'
- en: '**[2]** *Ornstein-Uhlenbech Process*[https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**[2]** *奥恩斯坦-乌伦贝克过程*[https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process)'
- en: '**[3]** *Quadratic Forms. Berkeley Math*[https://math.berkeley.edu/~limath/Su14Math54/0722.pdf](https://math.berkeley.edu/~limath/Su14Math54/0722.pdf)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**[3]** *二次型。伯克利数学*[https://math.berkeley.edu/~limath/Su14Math54/0722.pdf](https://math.berkeley.edu/~limath/Su14Math54/0722.pdf)'
- en: '**[4]** LILLICRAP, Timothy P., et al. Continuous control with deep reinforcement
    learning. *arXiv preprint arXiv:1509.02971*, 2015'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**[4]** LILLICRAP, Timothy P. 等人。通过深度强化学习进行连续控制。*arXiv 预印本 arXiv:1509.02971*，2015年'
