- en: 'Applied Reinforcement Learning V: Normalized Advantage Function (NAF) for Continuous
    Control'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åº”ç”¨å¼ºåŒ–å­¦ä¹  Vï¼šç”¨äºè¿ç»­æ§åˆ¶çš„å½’ä¸€åŒ–ä¼˜åŠ¿å‡½æ•°ï¼ˆNAFï¼‰
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095](https://towardsdatascience.com/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095](https://towardsdatascience.com/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095)
- en: Introduction and explanation of the NAF algorithm, widely used in continuous
    control tasks
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NAF ç®—æ³•çš„ä»‹ç»å’Œè§£é‡Šï¼Œè¿™æ˜¯ä¸€ç§å¹¿æ³›ç”¨äºè¿ç»­æ§åˆ¶ä»»åŠ¡çš„ç®—æ³•
- en: '[](https://medium.com/@JavierMtz5?source=post_page-----62ad143d3095--------------------------------)[![Javier
    MartÃ­nez Ojeda](../Images/5b5df4220fa64c13232c29de9b4177af.png)](https://medium.com/@JavierMtz5?source=post_page-----62ad143d3095--------------------------------)[](https://towardsdatascience.com/?source=post_page-----62ad143d3095--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----62ad143d3095--------------------------------)
    [Javier MartÃ­nez Ojeda](https://medium.com/@JavierMtz5?source=post_page-----62ad143d3095--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@JavierMtz5?source=post_page-----62ad143d3095--------------------------------)[![å“ˆç»´å°”Â·é©¬ä¸å†…æ–¯Â·å¥¥èµ«è¾¾](../Images/5b5df4220fa64c13232c29de9b4177af.png)](https://medium.com/@JavierMtz5?source=post_page-----62ad143d3095--------------------------------)[](https://towardsdatascience.com/?source=post_page-----62ad143d3095--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----62ad143d3095--------------------------------)
    [å“ˆç»´å°”Â·é©¬ä¸å†…æ–¯Â·å¥¥èµ«è¾¾](https://medium.com/@JavierMtz5?source=post_page-----62ad143d3095--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----62ad143d3095--------------------------------)
    Â·9 min readÂ·Jan 19, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----62ad143d3095--------------------------------)
    Â·9 åˆ†é’Ÿé˜…è¯»Â·2023 å¹´ 1 æœˆ 19 æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/980806b6e676bcf2f2e28c3dca2000f0.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/980806b6e676bcf2f2e28c3dca2000f0.png)'
- en: Photo by [Sufyan](https://unsplash.com/@blenderdesigner_1688?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”± [Sufyan](https://unsplash.com/@blenderdesigner_1688?utm_source=medium&utm_medium=referral)
    æä¾›ï¼Œæ¥æºäº [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: If you want to read this article without a Premium Medium account, you can do
    it from this friend link :)
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³åœ¨æ²¡æœ‰ Premium Medium è´¦æˆ·çš„æƒ…å†µä¸‹é˜…è¯»è¿™ç¯‡æ–‡ç« ï¼Œä½ å¯ä»¥é€šè¿‡è¿™ä¸ªæœ‹å‹é“¾æ¥æ¥æŸ¥çœ‹ :)
- en: '[https://www.learnml.wiki/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control/](https://www.learnml.wiki/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control/)'
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://www.learnml.wiki/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control/](https://www.learnml.wiki/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control/)'
- en: Previous articles in this series have introduced and explained two Reinforcement
    Learning algorithms that have been widely used since their inception:[**Q-Learning**](https://medium.com/towards-data-science/applied-reinforcement-learning-i-q-learning-d6086c1f437)
    and [**DQN**](https://medium.com/towards-data-science/applied-reinforcement-learning-iii-deep-q-networks-dqn-8f0e38196ba9).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç³»åˆ—ä¹‹å‰çš„æ–‡ç« ä»‹ç»å¹¶è§£é‡Šäº†ä¸¤ç§è‡ªå…¶è¯ç”Ÿä»¥æ¥å¹¿æ³›ä½¿ç”¨çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼š[**Q-Learning**](https://medium.com/towards-data-science/applied-reinforcement-learning-i-q-learning-d6086c1f437)
    å’Œ [**DQN**](https://medium.com/towards-data-science/applied-reinforcement-learning-iii-deep-q-networks-dqn-8f0e38196ba9)ã€‚
- en: '**Q-Learning** stores the Q-Values in an action-state matrix, such that to
    obtain the action *a* with the largest Q-Value in state *s*, the largest element
    of the Q-Value matrix for row *s* must be found, which makes its application to
    continuous state or action spaces impossible since the Q-Value matrix would be
    infinite.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**Q-Learning** å°† Q å€¼å­˜å‚¨åœ¨ä¸€ä¸ªåŠ¨ä½œ-çŠ¶æ€çŸ©é˜µä¸­ï¼Œå› æ­¤ï¼Œè¦åœ¨çŠ¶æ€ *s* ä¸­è·å¾—æœ€å¤§çš„ Q å€¼åŠ¨ä½œ *a*ï¼Œå¿…é¡»æ‰¾åˆ° Q å€¼çŸ©é˜µä¸­ç¬¬
    *s* è¡Œçš„æœ€å¤§å…ƒç´ ï¼Œè¿™ä½¿å¾—å…¶åœ¨è¿ç»­çŠ¶æ€æˆ–åŠ¨ä½œç©ºé—´ä¸­çš„åº”ç”¨å˜å¾—ä¸å¯èƒ½ï¼Œå› ä¸º Q å€¼çŸ©é˜µå°†æ˜¯æ— é™çš„ã€‚'
- en: On the other hand, **DQN** partially solves this problem by making use of a
    neural network to obtain the Q-Values associated to a state *s*, such that the
    output of the neural network are the Q-Values for each possible action of the
    agent (the equivalent to a row in the action-state matrix of Q-Learning). This
    algorithm allows training in environments with a continuous state space, but it
    is still impossible to train in an environment with a continuous action space,
    since the output of the neural network (which has as many elements as possible
    actions) would have an infinite length.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€æ–¹é¢ï¼Œ**DQN** é€šè¿‡åˆ©ç”¨ç¥ç»ç½‘ç»œæ¥è·å–ä¸çŠ¶æ€ *s* ç›¸å…³çš„ Q-å€¼ï¼Œä»è€Œéƒ¨åˆ†è§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼Œä½¿ç¥ç»ç½‘ç»œçš„è¾“å‡ºä¸ºæ¯ä¸ªå¯èƒ½çš„åŠ¨ä½œçš„ Q-å€¼ï¼ˆç›¸å½“äº
    Q-Learning çš„åŠ¨ä½œ-çŠ¶æ€çŸ©é˜µä¸­çš„ä¸€è¡Œï¼‰ã€‚è¯¥ç®—æ³•å…è®¸åœ¨å…·æœ‰è¿ç»­çŠ¶æ€ç©ºé—´çš„ç¯å¢ƒä¸­è¿›è¡Œè®­ç»ƒï¼Œä½†ä»ç„¶æ— æ³•åœ¨å…·æœ‰è¿ç»­åŠ¨ä½œç©ºé—´çš„ç¯å¢ƒä¸­è¿›è¡Œè®­ç»ƒï¼Œå› ä¸ºç¥ç»ç½‘ç»œçš„è¾“å‡ºï¼ˆå…·æœ‰ä¸å¯èƒ½åŠ¨ä½œä¸€æ ·å¤šçš„å…ƒç´ ï¼‰å°†ä¼šæ˜¯æ— é™é•¿åº¦çš„ã€‚
- en: The **NAF algorithm** introduced by Shixiang Gu et al. in **[1]**, unlike Q-Learning
    or DQN, allows training in continuous state and action space environments, adding
    a great deal of versatility in terms of possible applications. Reinforcement learning
    algorithms for continuous environments such as NAF are commonly used in the field
    of control, especially in robotics, because they are able to train in environments
    that more closely represent reality.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**NAF ç®—æ³•**ç”± Shixiang Gu ç­‰äººæå‡ºäº**[1]**ï¼Œä¸ Q-Learning æˆ– DQN ä¸åŒï¼Œå®ƒå…è®¸åœ¨è¿ç»­çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´ç¯å¢ƒä¸­è¿›è¡Œè®­ç»ƒï¼Œå¢åŠ äº†è®¸å¤šåº”ç”¨çš„çµæ´»æ€§ã€‚åƒ
    NAF è¿™æ ·çš„è¿ç»­ç¯å¢ƒä¸­çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•é€šå¸¸åœ¨æ§åˆ¶é¢†åŸŸä¸­ä½¿ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨æœºå™¨äººæŠ€æœ¯ä¸­ï¼Œå› ä¸ºå®ƒä»¬èƒ½å¤Ÿåœ¨æ›´è´´è¿‘ç°å®çš„ç¯å¢ƒä¸­è¿›è¡Œè®­ç»ƒã€‚'
- en: '![](../Images/95034a16b9224f41805bfa46c145eb24.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/95034a16b9224f41805bfa46c145eb24.png)'
- en: Types of RL Algorithms and their possible State/Action Spaces. Image by author
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ ç®—æ³•çš„ç±»å‹åŠå…¶å¯èƒ½çš„çŠ¶æ€/åŠ¨ä½œç©ºé—´ã€‚ä½œè€…æä¾›çš„å›¾ç‰‡
- en: Introductory Concepts
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¼•è¨€æ¦‚å¿µ
- en: Advantage Function
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¼˜åŠ¿å‡½æ•°
- en: '**State-Value Function V** and **Action-Value Function** (**Q-Function**) **Q**,
    both explained in the [first article of this series](https://medium.com/towards-data-science/applied-reinforcement-learning-i-q-learning-d6086c1f437),
    determine the benefit of being in a state while following a certain policy and
    the benefit of taking an action from a given state while following certain policy,
    respectively. Both functions, as well as the definition of **V** with respect
    to **Q**, can be seen below.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**çŠ¶æ€-ä»·å€¼å‡½æ•° V** å’Œ **åŠ¨ä½œ-ä»·å€¼å‡½æ•°**ï¼ˆ**Q-å‡½æ•°**ï¼‰**Q**ï¼Œéƒ½åœ¨ [æœ¬ç³»åˆ—çš„ç¬¬ä¸€ç¯‡æ–‡ç« ](https://medium.com/towards-data-science/applied-reinforcement-learning-i-q-learning-d6086c1f437)
    ä¸­è¿›è¡Œäº†è®²è§£ï¼Œåˆ†åˆ«ç¡®å®šäº†åœ¨éµå¾ªæŸä¸ªç­–ç•¥æ—¶å¤„äºæŸçŠ¶æ€çš„å¥½å¤„ä»¥åŠåœ¨éµå¾ªæŸä¸ªç­–ç•¥æ—¶ä»ç»™å®šçŠ¶æ€é‡‡å–æŸä¸ªåŠ¨ä½œçš„å¥½å¤„ã€‚è¿™ä¸¤ä¸ªå‡½æ•°ï¼Œä»¥åŠ **V** ç›¸å¯¹äº **Q**
    çš„å®šä¹‰ï¼Œå¯ä»¥åœ¨ä¸‹æ–¹æŸ¥çœ‹ã€‚'
- en: '![](../Images/2bc6f744389fa96fe75abca4ca0eb4df.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2bc6f744389fa96fe75abca4ca0eb4df.png)'
- en: Q-Function, Value Function and Q-V Relation. Image by author
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Q-å‡½æ•°ã€ä»·å€¼å‡½æ•°å’Œ Q-V å…³ç³»ã€‚ä½œè€…æä¾›çš„å›¾ç‰‡
- en: Since **Q** returns the benefit of taking a certain action in a state, while
    **V** returns the benefit of being in a state, the difference of both returns
    information about how advantageous it is to take a certain action in a state with
    respect to the rest of actions, or the extra reward that the agent will receive
    by taking that action with respect to the rest of actions. This difference is
    called **Advantage Function**, and its equation is shown below.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äº **Q** è¿”å›åœ¨æŸçŠ¶æ€ä¸‹é‡‡å–æŸä¸ªåŠ¨ä½œçš„å¥½å¤„ï¼Œè€Œ **V** è¿”å›å¤„äºæŸçŠ¶æ€çš„å¥½å¤„ï¼Œå› æ­¤ä¸¤è€…ä¹‹é—´çš„å·®å¼‚æä¾›äº†å…³äºåœ¨æŸçŠ¶æ€ä¸‹ç›¸å¯¹äºå…¶ä»–åŠ¨ä½œé‡‡å–ç‰¹å®šåŠ¨ä½œçš„ä¼˜åŠ¿ä¿¡æ¯ï¼Œæˆ–è€…ä»£ç†é€šè¿‡é‡‡å–è¯¥åŠ¨ä½œç›¸å¯¹äºå…¶ä»–åŠ¨ä½œæ‰€è·å¾—çš„é¢å¤–å¥–åŠ±ã€‚è¿™ä¸ªå·®å¼‚ç§°ä¸º
    **ä¼˜åŠ¿å‡½æ•°**ï¼Œå…¶æ–¹ç¨‹å¼å¦‚ä¸‹æ‰€ç¤ºã€‚
- en: '![](../Images/d76d848a25d01d7ee6e0d81bdf4c357f.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d76d848a25d01d7ee6e0d81bdf4c357f.png)'
- en: Advantage Function. Image by author
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼˜åŠ¿å‡½æ•°ã€‚ä½œè€…æä¾›çš„å›¾ç‰‡
- en: Ornstein-Uhlenbech Noise Process (OU)
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ornstein-Uhlenbech å™ªå£°è¿‡ç¨‹ï¼ˆOUï¼‰
- en: As seen in previous articles, in Reinforcement Learning algorithms for discrete
    environments such as Q-Learning or DQN, exploration is performed by randomly choosing
    an action and ignoring the optimal policy, as is the case for epsilon greedy policy.
    In continuous environments, however, the action is chosen following the optimal
    policy, and adding noise to this action.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚ä¹‹å‰çš„æ–‡ç« æ‰€è§ï¼Œåœ¨ç¦»æ•£ç¯å¢ƒä¸‹çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•å¦‚ Q-Learning æˆ– DQN ä¸­ï¼Œæ¢ç´¢æ˜¯é€šè¿‡éšæœºé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œå¹¶å¿½ç•¥æœ€ä¼˜ç­–ç•¥æ¥è¿›è¡Œçš„ï¼Œæ¯”å¦‚ epsilon
    è´ªå¿ƒç­–ç•¥ã€‚ç„¶è€Œï¼Œåœ¨è¿ç»­ç¯å¢ƒä¸­ï¼ŒåŠ¨ä½œæ˜¯æ ¹æ®æœ€ä¼˜ç­–ç•¥é€‰æ‹©çš„ï¼Œå¹¶ä¸”åœ¨è¿™ä¸ªåŠ¨ä½œä¸Šæ·»åŠ å™ªå£°ã€‚
- en: The problem with adding noise to the chosen action is that, if the noise is
    uncorrelated with the previous noise and has a distribution with zero mean, then
    the actions will cancel each other out, so that the agent will not be able to
    maintain a continuous movement to any point but will get stuck and therefore will
    not be able to explore. The **Ornstein-Uhlenbech Noise Process** obtains a noise
    value correlated with the previous noise value, so that the agent can have continuous
    movements towards some direction, and therefore explore successfully.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å‘é€‰æ‹©çš„åŠ¨ä½œä¸­æ·»åŠ å™ªå£°çš„é—®é¢˜åœ¨äºï¼Œå¦‚æœå™ªå£°ä¸ä¹‹å‰çš„å™ªå£°æ— å…³ä¸”å…·æœ‰é›¶å‡å€¼çš„åˆ†å¸ƒï¼Œåˆ™åŠ¨ä½œä¼šç›¸äº’æŠµæ¶ˆï¼Œä½¿å¾—ä»£ç†æ— æ³•ä¿æŒå¯¹ä»»ä½•ç‚¹çš„è¿ç»­è¿åŠ¨ï¼Œè€Œæ˜¯ä¼šé™·å…¥å›°å¢ƒï¼Œä»è€Œæ— æ³•æ¢ç´¢ã€‚**Ornstein-Uhlenbechå™ªå£°è¿‡ç¨‹**è·å¾—ä¸ä¹‹å‰å™ªå£°å€¼ç›¸å…³çš„å™ªå£°å€¼ï¼Œä½¿å¾—ä»£ç†èƒ½å¤Ÿå‘æŸä¸ªæ–¹å‘è¿›è¡Œè¿ç»­è¿åŠ¨ï¼Œå› æ­¤æˆåŠŸåœ°è¿›è¡Œæ¢ç´¢ã€‚
- en: More in-depth information about the Ornstein-Uhlenbech Noise Process can be
    found in **[2]**
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºOrnstein-Uhlenbechå™ªå£°è¿‡ç¨‹çš„æ›´æ·±å…¥ä¿¡æ¯å¯ä»¥åœ¨**[2]**ä¸­æ‰¾åˆ°ã€‚
- en: Logic behind the NAF algorithm
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NAFç®—æ³•èƒŒåçš„é€»è¾‘
- en: The NAF algorithm makes use of a neural network that obtains as separate outputs
    a value for the **State-Value Function V** and for the **Advantage Function A**.
    The neural network obtains these outputs since, as previously explained, the result
    of the **Action-Value Function Q** can be later obtained as the sum of **V** and
    **A**.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: NAFç®—æ³•åˆ©ç”¨ç¥ç»ç½‘ç»œåˆ†åˆ«è·å¾—**çŠ¶æ€å€¼å‡½æ•°V**å’Œ**ä¼˜åŠ¿å‡½æ•°A**çš„å€¼ã€‚ç”±äºä¹‹å‰çš„è§£é‡Šï¼Œç¥ç»ç½‘ç»œè·å¾—è¿™äº›è¾“å‡ºçš„åŸå› æ˜¯ï¼Œ**åŠ¨ä½œå€¼å‡½æ•°Q**çš„ç»“æœå¯ä»¥ä½œä¸º**V**å’Œ**A**çš„æ€»å’Œã€‚
- en: Like most Reinforcement Learning algorithms, NAF aims to optimize the Q-Function,
    but in its application case it is particularly complicated since it uses a neural
    network as Q-Function estimator. For this reason, the NAF algorithm makes use
    of a quadratic function for the Advantage Function, whose solution is closed and
    known, so optimization with respect to the action is easier.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: åƒå¤§å¤šæ•°å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¸€æ ·ï¼ŒNAFæ—¨åœ¨ä¼˜åŒ–Q-å‡½æ•°ï¼Œä½†åœ¨å…¶åº”ç”¨æ¡ˆä¾‹ä¸­ï¼Œç‰¹åˆ«å¤æ‚ï¼Œå› ä¸ºå®ƒä½¿ç”¨ç¥ç»ç½‘ç»œä½œä¸ºQ-å‡½æ•°ä¼°è®¡å™¨ã€‚å› æ­¤ï¼ŒNAFç®—æ³•åˆ©ç”¨äº†ä¸€ä¸ªäºŒæ¬¡å‡½æ•°æ¥è¡¨ç¤ºä¼˜åŠ¿å‡½æ•°ï¼Œå…¶è§£æ˜¯å°é—­ä¸”å·²çŸ¥çš„ï¼Œä»è€Œä½¿å¾—å…³äºåŠ¨ä½œçš„ä¼˜åŒ–æ›´åŠ å®¹æ˜“ã€‚
- en: '![](../Images/09b25af10b4e1ad8de2d42bad265dcf1.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09b25af10b4e1ad8de2d42bad265dcf1.png)'
- en: '**Figure 1**. Q-Function and Advantage Function for NAF algorithm. Image extracted
    from **[1]**'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾1**ã€‚NAFç®—æ³•çš„Q-å‡½æ•°å’Œä¼˜åŠ¿å‡½æ•°ã€‚å›¾åƒæ‘˜è‡ª**[1]**ã€‚'
- en: More specifically, the Q-Function will always be quadratic with respect to theaction,
    so that the *argmax Q(x, u)* for the action is always **ğœ‡**(**x**|ğœƒ) **[3]**,
    as shown in *Figure 2*. Thanks to this, the problem of not being able to obtain
    the argmax of the neural network output due to working in a continuous action
    space, as was the case with DQN, is solved in an analytical way.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´å…·ä½“åœ°è¯´ï¼ŒQ-å‡½æ•°ç›¸å¯¹äºåŠ¨ä½œæ€»æ˜¯äºŒæ¬¡çš„ï¼Œå› æ­¤åŠ¨ä½œçš„*argmax Q(x, u)*æ€»æ˜¯**ğœ‡**(**x**|ğœƒ) **[3]**ï¼Œå¦‚*å›¾2*æ‰€ç¤ºã€‚ç”±äºæ­¤åŸå› ï¼Œè§£å†³äº†ç”±äºåœ¨è¿ç»­åŠ¨ä½œç©ºé—´ä¸­å·¥ä½œè€Œæ— æ³•è·å¾—ç¥ç»ç½‘ç»œè¾“å‡ºçš„argmaxçš„é—®é¢˜ï¼Œè¿™åœ¨DQNä¸­æ›¾ç»å­˜åœ¨ã€‚
- en: 'By looking at the different components that make up the Q-Function, it can
    be seen that the neural network will have three different outputs: one to estimate
    the Value Function, another to obtain the action that maximizes the Q-Function
    (*argmax Q(s, a)* or **ğœ‡**(**x**|ğœƒ)), and another to calculate the matrix P (see
    *Figure 1*):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡æŸ¥çœ‹æ„æˆQ-å‡½æ•°çš„ä¸åŒç»„ä»¶ï¼Œå¯ä»¥çœ‹å‡ºç¥ç»ç½‘ç»œå°†æœ‰ä¸‰ä¸ªä¸åŒçš„è¾“å‡ºï¼šä¸€ä¸ªç”¨äºä¼°è®¡ä»·å€¼å‡½æ•°ï¼Œå¦ä¸€ä¸ªç”¨äºè·å¾—æœ€å¤§åŒ–Q-å‡½æ•°çš„åŠ¨ä½œï¼ˆ*argmax Q(s,
    a)* æˆ– **ğœ‡**(**x**|ğœƒ)ï¼‰ï¼Œè¿˜æœ‰ä¸€ä¸ªç”¨äºè®¡ç®—çŸ©é˜µPï¼ˆè§*å›¾1*ï¼‰ï¼š
- en: The first output of the neural network is the estimate of the State-Value Function.
    This estimate is then used to obtain the estimate of the Q-Function, as the sum
    of the State-Value Function and the Advantage Function. This output is represented
    by **V(x|ğœƒ)** in *Figure 1*.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œçš„ç¬¬ä¸€ä¸ªè¾“å‡ºæ˜¯å¯¹çŠ¶æ€å€¼å‡½æ•°çš„ä¼°è®¡ã€‚è¿™ä¸ªä¼°è®¡å€¼éšåç”¨äºè·å¾—Q-å‡½æ•°çš„ä¼°è®¡ï¼Œå³çŠ¶æ€å€¼å‡½æ•°å’Œä¼˜åŠ¿å‡½æ•°çš„æ€»å’Œã€‚è¿™ä¸ªè¾“å‡ºåœ¨*å›¾1*ä¸­è¡¨ç¤ºä¸º**V(x|ğœƒ)**ã€‚
- en: The second output of the neural network is **ğœ‡(x|ğœƒ)**, which is the action that
    maximizes the Q-Function on the given state, or *argmax Q(s, a)*, and therefore
    acts as the policy to be followed by the agent.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œçš„ç¬¬äºŒä¸ªè¾“å‡ºæ˜¯**ğœ‡(x|ğœƒ)**ï¼Œå³åœ¨ç»™å®šçŠ¶æ€ä¸‹æœ€å¤§åŒ–Q-å‡½æ•°çš„åŠ¨ä½œï¼Œæˆ–*argmax Q(s, a)*ï¼Œå› æ­¤ä½œä¸ºä»£ç†åº”éµå¾ªçš„ç­–ç•¥ã€‚
- en: The third output is used to later form the state-dependent, positive-definite
    square matrix **P**(**x**|ğœƒ). This linear output of the neural network is used
    as entry for a lower-triangular matrix **L**(**x**|ğœƒ), whose diagonal terms are
    exponentiated, and from which the mentioned matrix **P**(**x**|ğœƒ) is constructed,
    following the following formula.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰ä¸ªè¾“å‡ºç”¨äºéšåå½¢æˆçŠ¶æ€ä¾èµ–çš„æ­£å®šæ–¹é˜µ**P**(**x**|ğœƒ)ã€‚è¿™ç§ç¥ç»ç½‘ç»œçš„çº¿æ€§è¾“å‡ºç”¨ä½œä¸‹ä¸‰è§’çŸ©é˜µ**L**(**x**|ğœƒ)çš„è¾“å…¥ï¼Œå…¶ä¸­å¯¹è§’é¡¹ç»è¿‡æŒ‡æ•°åŒ–å¤„ç†ï¼Œä»è€Œæ„å»ºå‡ºä¸Šè¿°çš„çŸ©é˜µ**P**(**x**|ğœƒ)ï¼Œå…¶è®¡ç®—å…¬å¼å¦‚ä¸‹ã€‚
- en: '![](../Images/2cffbad3d13d3b9ae8f6ef671c1de32b.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2cffbad3d13d3b9ae8f6ef671c1de32b.png)'
- en: Formula for constructing P matrix. Extracted from **[1]**
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æ„é€ PçŸ©é˜µçš„å…¬å¼ã€‚æ‘˜è‡ª**[1]**
- en: '![](../Images/605c89d50581caf1849856a81b45878e.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/605c89d50581caf1849856a81b45878e.png)'
- en: '**Figure 2**. Image by author'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 2**ã€‚ä½œè€…æä¾›çš„å›¾åƒ'
- en: The second and third outputs of the neural network are used to construct the
    estimate for the Advantage Function as shown in *Figure 1*, which is then added
    to the first output (the State-Value Function estimate V) to obtain the estimate
    for the Q-Function.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œçš„ç¬¬äºŒå’Œç¬¬ä¸‰ä¸ªè¾“å‡ºç”¨äºæ„é€ ä¼˜åŠ¿å‡½æ•°çš„ä¼°è®¡ï¼Œå¦‚*å›¾ 1*æ‰€ç¤ºï¼Œç„¶åå°†å…¶åŠ åˆ°ç¬¬ä¸€ä¸ªè¾“å‡ºï¼ˆçŠ¶æ€å€¼å‡½æ•°ä¼°è®¡Vï¼‰ä¸­ï¼Œä»¥è·å¾—Qå‡½æ•°çš„ä¼°è®¡ã€‚
- en: 'Regarding the rest of the NAF algorithm flow, it includes the same components
    and steps as the DQN algorithm explained in article [*Applied Reinforcement Learning
    III: Deep Q-Networks (DQN)*](https://medium.com/towards-data-science/applied-reinforcement-learning-iii-deep-q-networks-dqn-8f0e38196ba9).
    These components in common are the **Replay Buffer**, the **Main Neural Network**
    and the **Target Neural Network**. As for DQN, the Replay Buffer is used to store
    experiences to train the main neural network, and the target neural network is
    used to calculate the target values and compare them with the predictions from
    the main network, and then perform the backpropagation process.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºNAFç®—æ³•çš„å…¶ä½™éƒ¨åˆ†ï¼Œå®ƒåŒ…å«ä¸æ–‡ç« [*åº”ç”¨å¼ºåŒ–å­¦ä¹  IIIï¼šæ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰*](https://medium.com/towards-data-science/applied-reinforcement-learning-iii-deep-q-networks-dqn-8f0e38196ba9)ä¸­è§£é‡Šçš„DQNç®—æ³•ç›¸åŒçš„ç»„ä»¶å’Œæ­¥éª¤ã€‚è¿™äº›å…±åŒçš„ç»„ä»¶æœ‰**é‡æ”¾ç¼“å†²åŒº**ã€**ä¸»ç¥ç»ç½‘ç»œ**å’Œ**ç›®æ ‡ç¥ç»ç½‘ç»œ**ã€‚ä¸DQNç±»ä¼¼ï¼Œé‡æ”¾ç¼“å†²åŒºç”¨äºå­˜å‚¨ç»éªŒä»¥è®­ç»ƒä¸»ç¥ç»ç½‘ç»œï¼Œç›®æ ‡ç¥ç»ç½‘ç»œç”¨äºè®¡ç®—ç›®æ ‡å€¼å¹¶ä¸ä¸»ç½‘ç»œçš„é¢„æµ‹è¿›è¡Œæ¯”è¾ƒï¼Œç„¶åæ‰§è¡Œåå‘ä¼ æ’­è¿‡ç¨‹ã€‚
- en: NAF Algorithm Flow
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NAFç®—æ³•æµç¨‹
- en: The flow of the NAF algorithm will be presented following the pseudocode below,
    extracted from **[1]**. As mentioned above, the NAF algorithm follows the same
    steps as the DQN algorithm, except that NAF trains its main neural network differently.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: NAFç®—æ³•çš„æµç¨‹å°†éµå¾ªä¸‹é¢çš„ä¼ªä»£ç ï¼Œæ‘˜è‡ª**[1]**ã€‚å¦‚å‰æ‰€è¿°ï¼ŒNAFç®—æ³•éµå¾ªä¸DQNç®—æ³•ç›¸åŒçš„æ­¥éª¤ï¼Œåªæ˜¯NAFä»¥ä¸åŒçš„æ–¹å¼è®­ç»ƒå…¶ä¸»ç¥ç»ç½‘ç»œã€‚
- en: '![](../Images/e9e3dffcf427a42797efda0eedeea072.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e9e3dffcf427a42797efda0eedeea072.png)'
- en: NAF Algorithm pseudocode. Extracted from **[1]**
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: NAFç®—æ³•ä¼ªä»£ç ã€‚æ‘˜è‡ª**[1]**
- en: '*For each timestep in an episode, the agent performs the following steps:*'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*åœ¨æ¯ä¸ªæ—¶é—´æ­¥ä¸­ï¼Œæ™ºèƒ½ä½“æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š*'
- en: 1\. From given state, select an action
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. ä»ç»™å®šçŠ¶æ€ä¸­é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œ
- en: The action selected is the one that maximises the estimate of the Q-Function,
    which is given by the term **ğœ‡(x|ğœƒ)**, as shown in *Figure 2*.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: é€‰æ‹©çš„åŠ¨ä½œæ˜¯æœ€å¤§åŒ–Qå‡½æ•°ä¼°è®¡å€¼çš„åŠ¨ä½œï¼Œç”±**ğœ‡(x|ğœƒ)**è¡¨ç¤ºï¼Œå¦‚*å›¾ 2*æ‰€ç¤ºã€‚
- en: To this selected action the noise extracted from the Ornstein-Uhlenbech noise
    process (previosuly introduced) is added, in order to enhance the agentâ€™s exploration.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ‰€é€‰æ‹©çš„åŠ¨ä½œï¼Œæ·»åŠ ä»Ornstein-Uhlenbechå™ªå£°è¿‡ç¨‹ï¼ˆä¹‹å‰ä»‹ç»è¿‡ï¼‰ä¸­æå–çš„å™ªå£°ï¼Œä»¥å¢å¼ºæ™ºèƒ½ä½“çš„æ¢ç´¢ã€‚
- en: 2\. Perform action on environment
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. å¯¹ç¯å¢ƒæ‰§è¡ŒåŠ¨ä½œ
- en: The action with noise obtained in the previous step is executed by the agent
    in the environment. After the execution of such an action, the agent receives
    information about how good the action taken was (via the **reward**), as well
    as about the new situation reached in the environment (which is the **next state**).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¯å¢ƒä¸­ç”±æ™ºèƒ½ä½“æ‰§è¡Œå¸¦å™ªå£°çš„åŠ¨ä½œã€‚æ‰§è¡Œæ­¤åŠ¨ä½œåï¼Œæ™ºèƒ½ä½“æ¥æ”¶å…³äºåŠ¨ä½œæ•ˆæœçš„ä¿¡æ¯ï¼ˆé€šè¿‡**å¥–åŠ±**ï¼‰ï¼Œä»¥åŠå…³äºåœ¨ç¯å¢ƒä¸­è¾¾åˆ°çš„æ–°çŠ¶æ€çš„ä¿¡æ¯ï¼ˆå³**ä¸‹ä¸€ä¸ªçŠ¶æ€**ï¼‰ã€‚
- en: 3\. Store experience in Replay Buffer
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. å°†ç»éªŒå­˜å‚¨åœ¨é‡æ”¾ç¼“å†²åŒºä¸­
- en: The Replay Buffer stores experiences as ***{s, a, r, sâ€™}***, being ***s*** and
    ***a*** the current state and action, and ***r*** and ***sâ€™*** the reward and
    new state reached after performing the action from the current state.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: é‡æ”¾ç¼“å†²åŒºå°†ç»éªŒå­˜å‚¨ä¸º***{s, a, r, sâ€™}***ï¼Œå…¶ä¸­***s***å’Œ***a***æ˜¯å½“å‰çŠ¶æ€å’ŒåŠ¨ä½œï¼Œ***r***å’Œ***sâ€™***æ˜¯å¥–åŠ±å’Œæ‰§è¡Œå½“å‰çŠ¶æ€åŠ¨ä½œåçš„æ–°çŠ¶æ€ã€‚
- en: '*The following steps, from 4 to 7, are repeated as many times as stated in
    the algorithmâ€™s hyperparameter* ***I*** *per timestep, which can be seen in the
    pseudocode above.*'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*ä»æ­¥éª¤4åˆ°7çš„è¿‡ç¨‹ä¼šæ ¹æ®ç®—æ³•è¶…å‚æ•°* ***I*** *åœ¨æ¯ä¸ªæ—¶é—´æ­¥ä¸Šé‡å¤è¿›è¡Œï¼Œå¦‚ä¸Šè¿°ä¼ªä»£ç æ‰€ç¤ºã€‚*'
- en: 4\. Sample a random batch of experiences from Replay Buffer
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. ä»Replay Bufferä¸­éšæœºæŠ½å–ä¸€æ‰¹ç»éªŒ
- en: As explained in [the DQN article](https://medium.com/towards-data-science/applied-reinforcement-learning-iii-deep-q-networks-dqn-8f0e38196ba9),
    a batch of experiences is extracted only when the Replay Buffer has enough data
    to fill a batch. Once this condition is met, *{batch_size}* elements are randomly
    taken from the Replay Buffer, giving the possibility to learn from previous experiences,
    without the need to have lived them recently.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚[ã€ŠDQNæ–‡ç« ã€‹](https://medium.com/towards-data-science/applied-reinforcement-learning-iii-deep-q-networks-dqn-8f0e38196ba9)æ‰€è¿°ï¼Œä»…å½“Replay
    Bufferä¸­æœ‰è¶³å¤Ÿçš„æ•°æ®å¡«å……ä¸€æ‰¹æ—¶ï¼Œæ‰ä¼šæå–ä¸€æ‰¹ç»éªŒã€‚ä¸€æ—¦æ»¡è¶³è¿™ä¸€æ¡ä»¶ï¼Œ*{batch_size}* å…ƒç´ ä¼šä»Replay Bufferä¸­éšæœºé€‰å–ï¼Œä»è€Œå¯ä»¥åˆ©ç”¨ä»¥å‰çš„ç»éªŒè¿›è¡Œå­¦ä¹ ï¼Œè€Œä¸éœ€è¦æœ€è¿‘ç»å†è¿™äº›ç»éªŒã€‚
- en: 5\. Set the target value
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. è®¾ç½®ç›®æ ‡å€¼
- en: The target value is defined as the sum of the reward and the Value function
    estimate of the Target neural network for the next state multiplied by the discount
    factor **Î³**, which is an hyperparameter of the algorithm. The formula for the
    target value is shown below, and it is also available in the pseudocode above.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®æ ‡å€¼å®šä¹‰ä¸ºå¥–åŠ±ä¸ä¸‹ä¸€çŠ¶æ€çš„ç›®æ ‡ç¥ç»ç½‘ç»œçš„å€¼å‡½æ•°ä¼°è®¡å€¼ä¹‹å’Œï¼Œä¹˜ä»¥æŠ˜æ‰£å› å­**Î³**ï¼Œè¿™æ˜¯ç®—æ³•çš„ä¸€ä¸ªè¶…å‚æ•°ã€‚ç›®æ ‡å€¼çš„å…¬å¼å¦‚ä¸‹æ‰€ç¤ºï¼Œå¹¶ä¸”ä¹Ÿåœ¨ä¸Šè¿°ä¼ªä»£ç ä¸­æä¾›ã€‚
- en: '![](../Images/54f2066a2722a652de6d7abc8fbebeab.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/54f2066a2722a652de6d7abc8fbebeab.png)'
- en: Target Value Calculation. Extracted from the pseudocode in **[1]**
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®æ ‡å€¼è®¡ç®—ã€‚æ‘˜è‡ª**[1]**ä¸­çš„ä¼ªä»£ç 
- en: 6\. Perform Gradient Descent
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. æ‰§è¡Œæ¢¯åº¦ä¸‹é™
- en: Gradient Descent is applied to the loss, which is calculated with the estimate
    of the Q-Function obtained from the main neural network (predicted value) and
    the previously calculated target value, following the equation shown below. As
    can be seen, the loss function used is the MSE, so the loss will be the difference
    between the Q-Function estimate and the target squared.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹æŸå¤±è¿›è¡Œæ¢¯åº¦ä¸‹é™ï¼Œè¿™äº›æŸå¤±æ˜¯é€šè¿‡ä¸»ç¥ç»ç½‘ç»œï¼ˆé¢„æµ‹å€¼ï¼‰å’Œå…ˆå‰è®¡ç®—çš„ç›®æ ‡å€¼çš„Qå‡½æ•°ä¼°è®¡å¾—å‡ºçš„ï¼ŒæŒ‰ç…§ä¸‹é¢çš„æ–¹ç¨‹è®¡ç®—ã€‚å¦‚æ‰€ç¤ºï¼Œä½¿ç”¨çš„æŸå¤±å‡½æ•°æ˜¯å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ï¼Œå› æ­¤æŸå¤±å°†æ˜¯Qå‡½æ•°ä¼°è®¡å€¼å’Œç›®æ ‡å€¼ä¹‹é—´çš„å¹³æ–¹å·®ã€‚
- en: '![](../Images/d8c243ae133cd32c2fec507318ec081b.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d8c243ae133cd32c2fec507318ec081b.png)'
- en: Loss Function. Extracted from **[1]**
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æŸå¤±å‡½æ•°ã€‚æ‘˜è‡ª**[1]**
- en: It should be remembered that the estimate of the Q-Function is obtained from
    the sum of the estimate of the Value Function V(x**|ğœƒ**) plus the estimate of
    the Advantage function A(x, u**|ğœƒ**), whose formula is shown in *Figure 1*.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: åº”è®°ä½ï¼ŒQå‡½æ•°çš„ä¼°è®¡æ˜¯é€šè¿‡å€¼å‡½æ•°V(x**|ğœƒ**)çš„ä¼°è®¡åŠ ä¸Šä¼˜åŠ¿å‡½æ•°A(x, u**|ğœƒ**)çš„ä¼°è®¡å¾—å‡ºçš„ï¼Œå…¶å…¬å¼æ˜¾ç¤ºåœ¨*å›¾1*ä¸­ã€‚
- en: 7\. Softly update the Target Neural Network
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. è½¯æ›´æ–°ç›®æ ‡ç¥ç»ç½‘ç»œ
- en: The weights of the Target neural network are updated with the weights of the
    Main neural network in a soft manner. This soft updation is performed as a weighted
    sum of the Main networkâ€™s weights and the Targt networkâ€™s old weights, as shown
    in the following equation.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®æ ‡ç¥ç»ç½‘ç»œçš„æƒé‡ä»¥ä¸€ç§è½¯æ–¹å¼æ›´æ–°ä¸ºä¸»ç¥ç»ç½‘ç»œçš„æƒé‡ã€‚è¿™ç§è½¯æ›´æ–°æ˜¯ä¸»ç½‘ç»œæƒé‡å’Œç›®æ ‡ç½‘ç»œæ—§æƒé‡çš„åŠ æƒå’Œï¼Œå¦‚ä¸‹æ–¹ç¨‹æ‰€ç¤ºã€‚
- en: '![](../Images/8ff611f014ac51ff20d4c8f50968f97e.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8ff611f014ac51ff20d4c8f50968f97e.png)'
- en: Soft Update for the Target Network. Extracted from **[1]**
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®æ ‡ç½‘ç»œçš„è½¯æ›´æ–°ã€‚æ‘˜è‡ª**[1]**
- en: The importance of the weights of each neural network in the weighted sum is
    given by the hyperparameter **Ï„**. If **Ï„** is zero, the target network will not
    update its weights, since it will load its own old weights. If **Ï„** is set to
    1, the target neural network will be updated by loading the weights of the main
    network, ignoring the old weights of the target network.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åŠ æƒå’Œä¸­ï¼Œæ¯ä¸ªç¥ç»ç½‘ç»œæƒé‡çš„é‡è¦æ€§ç”±è¶…å‚æ•°**Ï„**å†³å®šã€‚å¦‚æœ**Ï„**ä¸ºé›¶ï¼Œåˆ™ç›®æ ‡ç½‘ç»œä¸ä¼šæ›´æ–°å…¶æƒé‡ï¼Œå› ä¸ºå®ƒå°†åŠ è½½è‡ªå·±çš„æ—§æƒé‡ã€‚å¦‚æœ**Ï„**è®¾ä¸º1ï¼Œåˆ™ç›®æ ‡ç¥ç»ç½‘ç»œå°†é€šè¿‡åŠ è½½ä¸»ç½‘ç»œçš„æƒé‡æ¥æ›´æ–°ï¼Œå¿½ç•¥ç›®æ ‡ç½‘ç»œçš„æ—§æƒé‡ã€‚
- en: 8\. Timestep ends â€” Execute the following timestep
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8\. æ—¶é—´æ­¥ç»“æŸ â€” æ‰§è¡Œä¸‹ä¸€ä¸ªæ—¶é—´æ­¥
- en: Once the previous steps have been completed, this same process is repeated over
    and over again until the maximum number of timesteps per episode is reached or
    until the agent reaches a terminal state. When this happens, it goes to the next
    episode.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæˆä¸Šè¿°æ­¥éª¤åï¼Œå°†ä¸æ–­é‡å¤è¿™ä¸€è¿‡ç¨‹ï¼Œç›´åˆ°è¾¾åˆ°æ¯ä¸ªå›åˆçš„æœ€å¤§æ—¶é—´æ­¥æ•°æˆ–ä»£ç†åˆ°è¾¾ç»ˆæ­¢çŠ¶æ€ã€‚å½“å‘ç”Ÿè¿™ç§æƒ…å†µæ—¶ï¼Œå°†è¿›å…¥ä¸‹ä¸€ä¸ªå›åˆã€‚
- en: Conclusion
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: The NAF algorithm achieves really good results in its implementation in continuous
    environments, so it fulfills its objective satisfactorily. The results of NAF
    in comparison with the **DDPG algorithm** **[4]** are shown below, where it can
    be seen how it improves considerably on the previous work. In addition, the beauty
    of the NAF algorithm should be highlighted, as it deals with the limitations of
    DQN for continuous environments with the cuadratic functions and its easy optimization,
    a smart and creative solution.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: NAF ç®—æ³•åœ¨è¿ç»­ç¯å¢ƒä¸­çš„å®ç°ä¸­å–å¾—äº†éå¸¸å¥½çš„ç»“æœï¼Œå› æ­¤å®ƒåœ†æ»¡åœ°å®Œæˆäº†å…¶ç›®æ ‡ã€‚NAF ä¸ **DDPG ç®—æ³•** **[4]** çš„æ¯”è¾ƒç»“æœå¦‚ä¸‹ï¼Œå…¶ä¸­å¯ä»¥çœ‹åˆ°å®ƒåœ¨å‰æœŸå·¥ä½œä¸­æœ‰äº†æ˜¾è‘—çš„æ”¹è¿›ã€‚æ­¤å¤–ï¼ŒNAF
    ç®—æ³•çš„ç¾å¦™ä¹‹å¤„åœ¨äºï¼Œå®ƒé€šè¿‡å¹³æ–¹å‡½æ•°åŠå…¶æ˜“äºä¼˜åŒ–çš„ç‰¹æ€§è§£å†³äº† DQN åœ¨è¿ç»­ç¯å¢ƒä¸­çš„å±€é™æ€§ï¼Œæ˜¯ä¸€ç§èªæ˜ä¸”å¯Œæœ‰åˆ›æ„çš„è§£å†³æ–¹æ¡ˆã€‚
- en: '![](../Images/9c04138c0416455bca91bfdf1eb53a50.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c04138c0416455bca91bfdf1eb53a50.png)'
- en: DDPG and NAF comparison in different tasks. Extracted from **[1]**
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG å’Œ NAF åœ¨ä¸åŒä»»åŠ¡ä¸­çš„æ¯”è¾ƒã€‚æ‘˜è‡ª**[1]**
- en: On the other hand, although NAF has shown to be an efficient and useful algorithm,
    its logic and implementation is not simple, especially when compared to previous
    algorithms for discrete environments, which makes it hard to use.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€æ–¹é¢ï¼Œå°½ç®¡ NAF è¢«è¯æ˜æ˜¯ä¸€ä¸ªé«˜æ•ˆä¸”æœ‰ç”¨çš„ç®—æ³•ï¼Œä½†å®ƒçš„é€»è¾‘å’Œå®ç°å¹¶ä¸ç®€å•ï¼Œå°¤å…¶æ˜¯ä¸ä¹‹å‰ç”¨äºç¦»æ•£ç¯å¢ƒçš„ç®—æ³•ç›¸æ¯”ï¼Œè¿™ä½¿å¾—å®ƒéš¾ä»¥ä½¿ç”¨ã€‚
- en: REFERENCES
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '**[1]** GU, Shixiang, et al. Continuous deep q-learning with model-based acceleration.
    En *International conference on machine learning*. PMLR, 2016\. p. 2829â€“2838'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**[1]** GU, Shixiang ç­‰äººã€‚åŸºäºæ¨¡å‹åŠ é€Ÿçš„è¿ç»­æ·±åº¦ q å­¦ä¹ ã€‚åœ¨ *å›½é™…æœºå™¨å­¦ä¹ å¤§ä¼š* ä¸Šã€‚PMLRï¼Œ2016å¹´ã€‚ç¬¬2829â€“2838é¡µ'
- en: '**[2]** *Ornstein-Uhlenbech Process*[https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**[2]** *å¥¥æ©æ–¯å¦-ä¹Œä¼¦è´å…‹è¿‡ç¨‹*[https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process)'
- en: '**[3]** *Quadratic Forms. Berkeley Math*[https://math.berkeley.edu/~limath/Su14Math54/0722.pdf](https://math.berkeley.edu/~limath/Su14Math54/0722.pdf)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**[3]** *äºŒæ¬¡å‹ã€‚ä¼¯å…‹åˆ©æ•°å­¦*[https://math.berkeley.edu/~limath/Su14Math54/0722.pdf](https://math.berkeley.edu/~limath/Su14Math54/0722.pdf)'
- en: '**[4]** LILLICRAP, Timothy P., et al. Continuous control with deep reinforcement
    learning. *arXiv preprint arXiv:1509.02971*, 2015'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**[4]** LILLICRAP, Timothy P. ç­‰äººã€‚é€šè¿‡æ·±åº¦å¼ºåŒ–å­¦ä¹ è¿›è¡Œè¿ç»­æ§åˆ¶ã€‚*arXiv é¢„å°æœ¬ arXiv:1509.02971*ï¼Œ2015å¹´'
