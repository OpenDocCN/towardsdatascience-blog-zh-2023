# 现代推荐系统中的哈希：入门

> 原文：[https://towardsdatascience.com/hashing-in-modern-recommender-systems-a-primer-9c6b2cf4497a](https://towardsdatascience.com/hashing-in-modern-recommender-systems-a-primer-9c6b2cf4497a)

## 理解应用机器学习中最被低估的技巧

[](https://medium.com/@samuel.flender?source=post_page-----9c6b2cf4497a--------------------------------)[![Samuel Flender](../Images/390d82a673de8a8bb11cef66978269b5.png)](https://medium.com/@samuel.flender?source=post_page-----9c6b2cf4497a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9c6b2cf4497a--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9c6b2cf4497a--------------------------------) [Samuel Flender](https://medium.com/@samuel.flender?source=post_page-----9c6b2cf4497a--------------------------------)

·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----9c6b2cf4497a--------------------------------) ·阅读时间6分钟·2023年3月28日

--

![](../Images/9c2812cf751beb430b8fbe23eefabfa7.png)

(Midjourney)

哈希是工业机器学习应用中最常见的“技巧”之一，但它并没有得到应有的关注。

哈希的最大优势，特别是在现代[推荐系统](https://medium.com/towards-data-science/biases-in-recommender-systems-top-challenges-and-recent-breakthroughs-edcda59d30bf)中，是其有限内存保证：如果没有哈希，在不耗尽内存的情况下，学习数十亿视频、新闻文章、照片或网页的相关性将是极其不切实际的。

但我们这里有些超前了。本文是一个入门介绍，让我们回到一切开始的地方：著名的2009年“哈希技巧”论文。

## 开创一切的论文

使用哈希作为处理特征的一种方法，以及“哈希技巧”这一术语，首次在2009年由雅虎的研究团队提出，该团队由Kilian Weinberger领导，背景是电子邮件垃圾邮件检测。毕竟，电子邮件是一系列单词，每个单词都可以被视为一个特征。作者解释说，通过哈希，我们可以将电子邮件表示为向量（其中每个索引对应一个特定的单词），并在垃圾邮件协同过滤模型中使用这些向量。

例如，“您的处方已准备好”这一短语在20维哈希空间中可能等同于

[PRE0]

以及其他，1的位置取决于所使用的特定哈希函数。

然而，不同的词可能对不同的人具有不同的相关性。词“处方”对某些用户可能表示垃圾邮件，而对另一些用户则不然。为了考虑这种相关性的差异，作者引入了“个性化”标记，将用户ID与单词本身结合起来。对于上述短语，他们不仅哈希标记

[PRE1]

还包括

[PRE2]

等等。鉴于他们的数据集包含4000万唯一单词和40万用户，这种个性化标记化导致了总共16万亿个可能的特征。

通过将这些个性化特征哈希成2²²的哈希大小，或大约420万（减少了7个数量级！），作者实现了30%的垃圾邮件减少，相比于没有哈希个性化的基线模型，这是哈希在机器学习问题中有用性的首次明确展示之一。

## 现代推荐系统中的哈希

从2009年到今天，虽然许多机器学习技术已经发生了变化（深度神经网络在很大程度上取代了线性协同过滤器），但哈希仍然存在。

现代推荐系统通常是某种变体的[双塔神经网络](/learning-to-rank-a-primer-40d2ff9960af)，其中一个塔从用户ID学习用户嵌入，另一个塔则从视频ID学习视频嵌入（对于视频推荐系统）。在训练时，模型会从历史互动数据中获得用户/视频对，例如带有点击的观看次数作为正样本，没有点击的观看次数作为负样本，从而形成一个共享的用户和视频嵌入空间。然后，我们可以将所有用户和视频的嵌入存储在一组嵌入表中，并在服务时使用这些表进行推荐，例如使用kNN算法。

到目前为止，一切还算顺利，但哈希在这里的作用是什么？

好吧，考虑一个存储1B视频和1B用户的100维嵌入的模型：这已经占用了800GB的内存。这是一个巨大的内存开销，模型将非常不切实际（如果不是不可能的话）且昂贵。通过哈希，我们可以首先将视频和用户的“词汇表”减少到，比如说，1000万，从而使内存开销更易于管理，为8GB。

换句话说，哈希允许我们将现代推荐系统扩展到数十亿用户和数十亿项目。如果没有哈希，推荐系统的规模将被我们能够承受的内存量所根本限制。

## 朝着无冲突哈希的方向前进

哈希的缺点是哈希冲突的存在，即多个不同的ID最终会映射到相同的哈希值，导致嵌入表中多个用户或项目共享相同的嵌入。显然，这种信息的“挤压”会降低推荐的质量。因此，当前推荐系统中最重要的研究问题之一是如何使其无冲突。

一种思路是使用“[深度哈希嵌入](https://arxiv.org/abs/2010.10784)”（DHE），这是由谷歌大脑的王成康团队于2021年提出的。他们使用大量的哈希函数，而不是单个哈希函数，并将所有哈希合并为一个稠密向量，然后输入到深度神经网络中，网络学习ID的高阶表示。关键思想是，如果哈希函数的数量足够大，那么哈希冲突在统计上变得不可能。

的确，他们的方法显示出前景：使用1024个哈希函数的DHE，作者在一组公共基准数据集上看到AUC提高了0.25%。不过，这种方法的一个缺点是它不适用于多值特征，只适用于直接的ID。（例如，所提到的DHE不能编码过去30天你在Netflix上观看的电影列表，这将是一个多值特征。）

另一种有前景的无冲突哈希方法是“[布谷鸟哈希](https://www.brics.dk/RS/01/32/BRICS-RS-01-32.pdf)”，由丹麦奥胡斯大学的拉斯穆斯·帕赫和弗莱明·罗德勒于2001年首次提出。布谷鸟哈希被用于字节跳动的在线推荐系统Monolith，该系统在他们2022年的[论文](https://arxiv.org/abs/2209.07663)中介绍，由刘卓然领导。

在布谷鸟哈希中，我们不仅维护一个哈希表，而是多个哈希表：在字节跳动的论文中，他们使用了2个。当遇到新的ID时，默认情况下我们将其哈希到第一个哈希表中。如果第一个哈希表已被另一个ID占据，我们将逐出那个ID（因此算法的名称）并将其重新哈希到第二个哈希表中的一个位置。这个过程会重复直到没有更多逐出，哈希表集合稳定且没有冲突。与常规哈希（有冲突）相比，作者发现他们的无冲突哈希方法在公共基准数据集上提高了0.5% AUC。

## 代码：应用机器学习中最被低估的技巧

内存回顾：

+   哈希可以让我们将现代推荐系统扩展到数十亿用户和数十亿项目，同时提供有限内存保证。

+   在机器学习应用中使用哈希的想法可以追溯到2009年Yahoo的一篇论文，该论文展示了其在电子邮件垃圾邮件检测模型中的有效性。

+   然而，哈希会引入哈希冲突，这可能由于多个用户和项目共享相同的嵌入而降低推荐质量。

+   因此，最近推荐系统的研究集中在如何使哈希无冲突上。显著的例子有深度哈希嵌入（谷歌大脑）和布谷鸟哈希（字节跳动）。

多年来，哈希一直是应用机器学习文献中最被低估的技巧之一，大部分关注点集中在模型架构、数据选择或特征工程上。

这可能开始发生变化。正如谷歌大脑、字节跳动等最近的论文所示，优化推荐系统以减少哈希碰撞可以显著提升性能。TikTok（由字节跳动拥有）的惊人受欢迎程度，至少部分可以用更好的哈希解释。

请关注这个领域：新的突破确实在即将到来。

[](https://medium.com/@samuel.flender/subscribe?source=post_page-----9c6b2cf4497a--------------------------------) [## 不想依赖 Medium 的算法？注册一下。

### 不想依赖 Medium 的算法？注册一下。通过注册我的电子邮件，确保你不会错过我的下一篇文章…

medium.com](https://medium.com/@samuel.flender/subscribe?source=post_page-----9c6b2cf4497a--------------------------------)
