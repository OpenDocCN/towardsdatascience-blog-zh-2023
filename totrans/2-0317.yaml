- en: Apache Spark Optimization Techniques
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark优化技术
- en: 原文：[https://towardsdatascience.com/apache-spark-optimization-techniques-fa7f20a9a2cf](https://towardsdatascience.com/apache-spark-optimization-techniques-fa7f20a9a2cf)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/apache-spark-optimization-techniques-fa7f20a9a2cf](https://towardsdatascience.com/apache-spark-optimization-techniques-fa7f20a9a2cf)
- en: A review of some of the most common Spark performance problems and how to address
    them
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对一些最常见的Spark性能问题及其解决方案的回顾
- en: '[](https://pierpaoloippolito28.medium.com/?source=post_page-----fa7f20a9a2cf--------------------------------)[![Pier
    Paolo Ippolito](../Images/981abb84149adab275473b76bdbde66f.png)](https://pierpaoloippolito28.medium.com/?source=post_page-----fa7f20a9a2cf--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fa7f20a9a2cf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fa7f20a9a2cf--------------------------------)
    [Pier Paolo Ippolito](https://pierpaoloippolito28.medium.com/?source=post_page-----fa7f20a9a2cf--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pierpaoloippolito28.medium.com/?source=post_page-----fa7f20a9a2cf--------------------------------)[![Pier
    Paolo Ippolito](../Images/981abb84149adab275473b76bdbde66f.png)](https://pierpaoloippolito28.medium.com/?source=post_page-----fa7f20a9a2cf--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fa7f20a9a2cf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fa7f20a9a2cf--------------------------------)
    [Pier Paolo Ippolito](https://pierpaoloippolito28.medium.com/?source=post_page-----fa7f20a9a2cf--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fa7f20a9a2cf--------------------------------)
    ·5 min read·Jan 11, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[数据科学之路](https://towardsdatascience.com/?source=post_page-----fa7f20a9a2cf--------------------------------)
    ·阅读时间5分钟·2023年1月11日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/26df9dea1fbdfec07dfe0a6878c48e93.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26df9dea1fbdfec07dfe0a6878c48e93.png)'
- en: Photo by [Manuel Nägeli](https://unsplash.com/@gwundrig?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Manuel Nägeli](https://unsplash.com/@gwundrig?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Apache Spark is currently one of the most popular big data technologies used
    in the industry, supported by companies such as Databricks and Palantir.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark目前是业内最受欢迎的大数据技术之一，由Databricks和Palantir等公司支持。
- en: One of the key responsibilities of Data Engineers when using Spark, is to write
    highly optimized code in order to fully take advantage of Spark's distributed
    computation capabilities (Figure 1).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程师在使用Spark时的一个关键职责是编写高度优化的代码，以充分利用Spark的分布式计算能力（图1）。
- en: '![](../Images/5aeb1c52bb6752bdcec1ead3e338ef96.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5aeb1c52bb6752bdcec1ead3e338ef96.png)'
- en: 'Figure 1: Apache Spark Architecture (Image by Author).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：Apache Spark架构（图片由作者提供）。
- en: As part of this article, you are going to be introduced some of the most common
    performance problems when using Spark (e.g. the 5 Ss) and how to address them.
    If you are completely new to Apache Spark, you can find additional information
    about it in [my previous article](/getting-started-with-apache-spark-cb703e1b3ee9).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，你将了解在使用Spark时一些常见的性能问题（例如5 Ss）以及如何解决这些问题。如果你对Apache Spark完全陌生，可以在[我的上一篇文章](/getting-started-with-apache-spark-cb703e1b3ee9)中找到更多信息。
- en: The 5 Ss
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 Ss
- en: 'The 5 Ss (Spill, Skew, Shuffle, Storage, Serialization) are the 5 most common
    performance problems in Spark. Two key general approaches which can be used to
    increase Spark performance under any circumstances are:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 5 Ss（Spill、Skew、Shuffle、Storage、Serialization）是Spark中5个最常见的性能问题。提高Spark性能的两个关键通用方法是：
- en: Reducing the amount of data ingested.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少摄取的数据量。
- en: Reducing the time Spark spends reading data (e.g. using Predicate Pushdown with
    Disk Partitioning/Z Order Clustering).
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少Spark读取数据的时间（例如，使用磁盘分区/ Z Order聚类的谓词下推）。
- en: We will now dive into each of the problems associated with the 5 Ss.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将深入探讨与5 Ss相关的每一个问题。
- en: Spill
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spill
- en: Spill is caused by writing temporary files to disk when running out of memory
    (a partition is too big to fit in RAM). In this case, an RDD is first moved from
    RAM to disk and then back to RAM just to avoid Out Of Memory (OOM) errors. Disk
    reads and writes can although be quite expensive to compute and should therefore
    be avoided as much as possible.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Spill是由于内存不足时将临时文件写入磁盘造成的（一个分区过大无法容纳在RAM中）。在这种情况下，RDD首先从RAM移动到磁盘，然后再移回RAM，以避免内存溢出（OOM）错误。磁盘读写虽然可能很昂贵，因此应尽可能避免。
- en: Spill can be better understood when running Spark Jobs by examining the Spark
    UI for the **Spill (Memory)** and **Spill (Disk)** values.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查 Spark UI 中的 **溢出（内存）** 和 **溢出（磁盘）** 值，可以更好地理解溢出问题。
- en: 'Spill (Memory): the size of data in memory for spilled partition.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 溢出（内存）：溢出分区在内存中的数据大小。
- en: 'Spill (Disk): the size of data on the disk for the spilled partition.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 溢出（磁盘）：溢出分区在磁盘上的数据大小。
- en: Two possible approaches which can be used in order to mitigate spill are instantiating
    a cluster with more memory per worker or increasing the number of partitions (therefore
    making the existing partitions smaller).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解溢出，有两种可能的方法：一是为每个工作节点实例化更多内存的集群，二是增加分区数量（从而使现有分区变小）。
- en: Skew
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 偏斜
- en: When using Spark, data is commonly read in evenly distributed partitions of
    128 MB. Applying different transformations to the data can then result in some
    partitions becoming much bigger or smaller than their average.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Spark 时，数据通常以 128 MB 的均匀分布分区进行读取。对数据应用不同的变换可能导致一些分区变得比平均值大得多或小得多。
- en: Skew is the result of the imbalance in size between the different partitions.
    Small amounts of Skew can be perfectly acceptable but in some circumstances, Skew
    can result in Spill and OOM errors.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 偏斜是不同分区之间大小不均衡的结果。少量偏斜可能是可以接受的，但在某些情况下，偏斜可能导致溢出和内存溢出错误。
- en: 'Two possible approaches to reduce Skew are (Figure 2):'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 减少偏斜的两种可能方法是（见图 2）：
- en: Salting the skewed column with random numbers to redistribute partition sizes.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用随机数对偏斜的列进行盐化，以重新分配分区大小。
- en: Using Adaptive Query Execution (Spark 3).
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自适应查询执行（Spark 3）。
- en: '![](../Images/5edc2b3878a127c518c20edb49c082ef.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5edc2b3878a127c518c20edb49c082ef.png)'
- en: 'Figure 2: Partition Size Distribution Before and After Skew (Image by Author).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：偏斜前后分区大小分布（作者提供的图像）。
- en: Shuffle
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 洗牌
- en: Shuffle results from moving data between executors when performing wide transformations
    (e.g. joins, groupBy, etc…) or some actions such as count (Figure 3). Mishandling
    of shuffle problems can result in Skew.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 洗牌是指在执行宽变换（例如连接、分组等）或某些动作（如计数）时在执行器之间移动数据（见图 3）。处理不当的洗牌问题可能导致偏斜。
- en: '![](../Images/7f34a9526e0c58489f4cece8de613e3d.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7f34a9526e0c58489f4cece8de613e3d.png)'
- en: 'Figure 3: Shuffling Process (Image by Author).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：洗牌过程（作者提供的图像）。
- en: 'Some approaches which can be used in order to reduce the amount of shuffling
    are:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少洗牌量，可以使用以下方法：
- en: Instantiating fewer and larger workers (therefore reducing network IO overheads).
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实例化更少且更大的工作节点（从而减少网络 IO 开销）。
- en: Prefilter data to reduce its size before shuffling.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在洗牌前对数据进行预筛选，以减少其大小。
- en: Denormalize the datasets involved.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对涉及的数据集进行反规范化。
- en: Prefer using Solid State Drives over Hard Disk Drives for faster execution.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优先使用固态硬盘而非硬盘驱动器，以实现更快的执行速度。
- en: When working with small tables, Broadcast Hash Join the smaller table. For big
    tables use instead SortMergeJoin (Broadcast Hash Join can lead to Out Of Memory
    issues with big tables).
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在处理小表时，对较小的表进行广播哈希连接。对于大表，则使用排序合并连接（广播哈希连接在大表中可能导致内存溢出问题）。
- en: Storage
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 存储
- en: 'Storage issues arise when data is stored on disk in a non-optimal way. Issues
    related with storage can potentially cause excessive Shuffle. Three of the main
    problems associated with Storage are: Tiny Files, Scanning and Schemas.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 存储问题出现在数据以非最佳方式存储在磁盘上。与存储相关的问题可能会导致过度洗牌。与存储相关的三个主要问题是：小文件、扫描和模式。
- en: '**Tiny Files:** handling partition files less than 128 MB.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**小文件：** 处理小于 128 MB 的分区文件。'
- en: '**Scanning:** when scanning directories we could either have a long list of
    files in a single directory or in the case of highly partitioned datasets multiple
    levels folders. In order to reduce the amount of scanning, we can register it
    as a table.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**扫描：** 在扫描目录时，我们可能会遇到单个目录中有大量文件，或者在高度分区的数据集中有多个层级的文件夹。为了减少扫描量，我们可以将其注册为表。'
- en: '**Schema:** depending on the file format used there can be different schema
    issues. For example, using JSON and CSV the whole data needs to be read to infer
    data types. For Parquet instead just a single file read is needed, but the whole
    list of Parquet files needs to be read if we need to handle possible schema changes
    over time. In order to improve performances, it could then help to provide schema
    definitions in advance.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模式：** 根据所使用的文件格式，可能会有不同的模式问题。例如，使用 JSON 和 CSV 时，需要读取全部数据以推断数据类型。对于 Parquet，只需读取一个文件，但如果需要处理可能的模式变化，必须读取所有
    Parquet 文件。为了提高性能，提前提供模式定义可能会有所帮助。'
- en: Serialization
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 序列化
- en: Serialization encompasses all the problems associated with the distribution
    of code across clusters (the code is serialized, sent to the executors, and then
    deserialized).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 序列化包括所有与代码在集群中分发相关的问题（代码被序列化，发送到执行器，然后反序列化）。
- en: In the case of Python, this process can even be more complicated since the code
    has to be pickled and an instance of the Python interpreter has to be allocated
    to each executor.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 的情况下，这个过程可能会更复杂，因为代码必须被 pickle 化，并且需要为每个执行器分配一个 Python 解释器实例。
- en: Serialization issues can arise when integrating codebases with legacy systems
    (e.g. Hadoop), 3rd party libraries, and custom frameworks. One key approach we
    can take to reduce serialization issues is avoiding using UDFs or Vectorized UDFs
    (which act like a black box for the Catalyst Optimizer).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在与遗留系统（例如 Hadoop）、第三方库和自定义框架集成时，可能会出现序列化问题。我们可以采取的一个关键方法是避免使用 UDF 或 Vectorized
    UDF（它们对于 Catalyst Optimizer 来说像一个黑匣子）。
- en: Conclusion
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Even with the latest release of Apache Spark 3, Spark Optimization remains one
    of the core areas in which practitioners' expertise and domain knowledge are fundamental
    in order to successfully make the best use of Spark capabilities. As part of this
    article, have been covered some of the key problems which can be encountered in
    a Spark project, although these problems can in some circumstances be highly connected
    to each other therefore making it difficult to trace down the main root cause.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在最新的 Apache Spark 3 发布版中，Spark 优化仍然是从业人员的专业知识和领域知识至关重要的核心领域之一，以便成功地最大限度地利用
    Spark 的能力。作为本文的一部分，涵盖了一些在 Spark 项目中可能遇到的关键问题，尽管这些问题在某些情况下可能高度相关，从而使得追踪主要根本原因变得困难。
- en: Contacts
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 联系方式
- en: 'If you want to keep updated with my latest articles and projects [follow me
    on Medium](https://pierpaoloippolito28.medium.com/subscribe) and subscribe to
    my [mailing list](http://eepurl.com/gwO-Dr). These are some of my contacts details:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想随时了解我的最新文章和项目，请 [关注我在 Medium 上](https://pierpaoloippolito28.medium.com/subscribe)
    并订阅我的 [邮件列表](http://eepurl.com/gwO-Dr)。以下是我的一些联系方式：
- en: '[Linkedin](https://uk.linkedin.com/in/pier-paolo-ippolito-202917146)'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Linkedin](https://uk.linkedin.com/in/pier-paolo-ippolito-202917146)'
- en: '[Personal Website](https://pierpaolo28.github.io/)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[个人网站](https://pierpaolo28.github.io/)'
- en: '[Medium Profile](https://towardsdatascience.com/@pierpaoloippolito28)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Medium 个人资料](https://towardsdatascience.com/@pierpaoloippolito28)'
- en: '[GitHub](https://github.com/pierpaolo28)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GitHub](https://github.com/pierpaolo28)'
- en: '[Kaggle](https://www.kaggle.com/pierpaolo28)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Kaggle](https://www.kaggle.com/pierpaolo28)'
