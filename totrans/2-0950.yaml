- en: Full Explanation of MLE, MAP and Bayesian Inference
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLE、MAP 和贝叶斯推断的全面解释
- en: 原文：[https://towardsdatascience.com/full-explanation-of-mle-map-and-bayesian-inference-1db9a7fb1d2b](https://towardsdatascience.com/full-explanation-of-mle-map-and-bayesian-inference-1db9a7fb1d2b)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/full-explanation-of-mle-map-and-bayesian-inference-1db9a7fb1d2b](https://towardsdatascience.com/full-explanation-of-mle-map-and-bayesian-inference-1db9a7fb1d2b)
- en: Introducing maximum likelihood estimation, maximum a posteriori estimation and
    Bayesian Inference
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍最大似然估计、最大后验估计和贝叶斯推断
- en: '[](https://medium.com/@hrmnmichaels?source=post_page-----1db9a7fb1d2b--------------------------------)[![Oliver
    S](../Images/b5ee0fa2d5fb115f62e2e9dfcb92afdd.png)](https://medium.com/@hrmnmichaels?source=post_page-----1db9a7fb1d2b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1db9a7fb1d2b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1db9a7fb1d2b--------------------------------)
    [Oliver S](https://medium.com/@hrmnmichaels?source=post_page-----1db9a7fb1d2b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@hrmnmichaels?source=post_page-----1db9a7fb1d2b--------------------------------)[![Oliver
    S](../Images/b5ee0fa2d5fb115f62e2e9dfcb92afdd.png)](https://medium.com/@hrmnmichaels?source=post_page-----1db9a7fb1d2b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1db9a7fb1d2b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1db9a7fb1d2b--------------------------------)
    [Oliver S](https://medium.com/@hrmnmichaels?source=post_page-----1db9a7fb1d2b--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1db9a7fb1d2b--------------------------------)
    ·13 min read·Mar 6, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1db9a7fb1d2b--------------------------------)
    ·阅读时间 13 分钟·2023年3月6日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ed3fd52dd4249d2f6376d0b4e133f357.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ed3fd52dd4249d2f6376d0b4e133f357.png)'
- en: Photo by [fabio](https://unsplash.com/@fabioha?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/oyXis2kALVg?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 [fabio](https://unsplash.com/@fabioha?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    于 [Unsplash](https://unsplash.com/photos/oyXis2kALVg?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: In this post we will introduce the concepts *MLE (maximum likelihood estimation)*,
    *MAP (maximum a posteriori estimation)* and *Bayesian inference* — which are fundamental
    to statistics, data science and machine learning, to name just a few fields. We
    will explain each method using the same example of an unfair coin toss, derive
    results analytically and numerically (for Bayesian inference) and show differences.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将介绍 *MLE（最大似然估计）*、*MAP（最大后验估计）* 和 *贝叶斯推断*——这些概念对统计学、数据科学和机器学习等领域至关重要。我们将使用一个不公平的硬币抛掷的例子来解释每种方法，分析并数字推导（对于贝叶斯推断），并展示它们之间的差异。
- en: 'We will learn that MLE maximises the likelihood — i.e. chooses parameters which
    maximise the likelihood of the observed data. MAP adds a prior, inducing prior
    knowledge over the parameters — thus bridging the gap from a purely Frequentist
    concept to a Bayesian one ([link](/statistics-are-you-bayesian-or-frequentist-4943f953f21b)).
    Bayesian inference eventually gives us the most information, but also is the hardest
    to execute: it involves modelling the full posterior distribution of the parameter
    given the data — as opposed to the previous methods just yielding point estimates.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将了解到 MLE 最大化了似然——即选择最大化观察数据似然的参数。MAP 添加了先验，引入了对参数的先验知识——从而弥合了从纯粹的频率派概念到贝叶斯概念的差距
    ([link](/statistics-are-you-bayesian-or-frequentist-4943f953f21b)). 贝叶斯推断最终给我们提供了最完整的信息，但也是最难执行的：它涉及建模给定数据的参数的完整后验分布——而之前的方法只是给出点估计。
- en: 'Let’s set the stage by formalising these descriptions: essentially, for any
    kind of learning problem we want to find a model / parameters which describe the
    observed data as well as possible. We can fully describe and solve this by finding
    a mapping / a distribution from observed data to parameters:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过正式化这些描述来设定背景：本质上，对于任何类型的学习问题，我们都希望找到一个模型/参数，以尽可能好地描述观察到的数据。我们可以通过找到从观察数据到参数的映射/分布来完全描述和解决这个问题：
- en: '![](../Images/c65eadcada2478c53f78c81b1eedc2ff.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c65eadcada2478c53f78c81b1eedc2ff.png)'
- en: 'Usually, these terms are named as:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这些术语被称为：
- en: '![](../Images/5be7854aafd96a21566940d45d750248.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5be7854aafd96a21566940d45d750248.png)'
- en: The [conditional distribution](https://en.wikipedia.org/wiki/Conditional_probability_distribution)
    of the parameters given the data is known as posterior distribution — this is
    the term we are interested in. Likelihood is the conditional distribution of the
    data given a certain parameter value. This — as the name suggests — describes
    how likely the observed data given the chosen parameter value. The prior distribution
    describes our belief over the parameters — how do we expect them to look like
    before observing any data. Finally, the evidence is a normalisation constant making
    the posterior a “true” probability distribution. It describes the given data fully
    and , usually , is hard to compute or even intractable — one of the main reasons
    why Bayesian inference often is hard.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 给定数据的参数的[条件分布](https://en.wikipedia.org/wiki/Conditional_probability_distribution)称为后验分布——这是我们关注的术语。似然函数是给定特定参数值的数据的条件分布。这——正如其名称所示——描述了在给定选择的参数值下观察到数据的可能性。先验分布描述了我们对参数的信念——即我们在观察到数据之前对它们的预期。最后，证据是一个归一化常数，使得后验分布成为一个“真实”的概率分布。它完全描述了给定的数据，并且通常很难计算或甚至不可计算——这是贝叶斯推断常常困难的主要原因之一。
- en: With these insights and formulas / notation, we will introduce MLE / MAP / Bayesian
    inference in the following. But first, let us introduce the problem we will use
    for demonstration throughout.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这些见解和公式/符号，我们将介绍最大似然估计（MLE）、最大后验估计（MAP）和贝叶斯推断。但首先，让我们介绍一个我们将在整个演示中使用的问题。
- en: 'Toy Problem: Estimating Parameters of an Unfair Coin'
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 玩具问题：估计不公平硬币的参数
- en: 'To showcase these concepts, we will use the example of throwing an unfair coin:
    assume, we are given a coin which lands heads with probability `θ`, and tails
    with probability `1 — θ`. `θ` is unknown to us, and we would like to estimate
    it after experimenting with the coin. That is, we assume we have or collect a
    dataset of coin tosses, and then want to find an estimate for `θ` using MLE, MAP
    and Bayesian Inference.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示这些概念，我们将使用抛掷不公平硬币的例子：假设，我们给定一枚硬币，其正面出现的概率为`θ`，反面出现的概率为`1 — θ`。`θ`对我们来说是未知的，我们希望在实验后估计它。也就是说，我们假设我们拥有或收集了一个抛币的数据集，然后希望使用MLE、MAP和贝叶斯推断来为`θ`找到一个估计值。
- en: 'Before moving on, let us describe a quantity we’ll need for all following sections:
    the likelihood. Throwing a coin follows a [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution).
    Assume we throw the coin `N` times, then denote the number of heads with `Nₕ`
    and the number of tails with `Nₜ`.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们描述一个我们在所有后续部分中需要的量：似然函数。抛掷硬币遵循[伯努利分布](https://en.wikipedia.org/wiki/Bernoulli_distribution)。假设我们抛掷硬币`N`次，则用`Nₕ`表示正面的次数，用`Nₜ`表示反面的次数。
- en: 'One core assumption for this (and the following) is independence: we assume,
    all draws of the coin are independent and identically distributed (i.i.d) — allowing
    us to write the likelihood as a product of individual likelihoods. In particular,
    we obtain:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此（以及接下来的内容）的一个核心假设是独立性：我们假设，所有的抛币结果是相互独立且同分布的（i.i.d）——这使我们能够将似然函数表示为各个独立似然函数的乘积。特别地，我们得到：
- en: '![](../Images/f64f63d61b1391ccddc71b6b95540910.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f64f63d61b1391ccddc71b6b95540910.png)'
- en: 'Plugging in the Bernoulli distribution yields:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 代入伯努利分布得出：
- en: '![](../Images/9ab25af6471c0716c7722bb75698937b.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ab25af6471c0716c7722bb75698937b.png)'
- en: In this, we model single events (coin tosses) with a binary variable of value
    1 for heads, and 0 for tails.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们用一个值为1的二元变量表示单次事件（抛硬币）的正面，值为0表示反面。
- en: 'Since the logarithm is monotonic and it is often easier to work with sums,
    we apply this and reformulate:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 由于对数函数是单调的，并且处理求和往往更为简便，我们应用这一点并重新公式化：
- en: '![](../Images/d6c245ed915fb25eccac9b24f3770f56.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6c245ed915fb25eccac9b24f3770f56.png)'
- en: Dataset
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集
- en: Before continuing, let’s define a sample dataset we observed by throwing this
    unfair coin — which we’ll use throughout the next sections do demonstrate results
    of the different methods.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们定义一个我们通过抛掷这枚不公平硬币观察到的样本数据集——我们将在接下来的部分中使用它来展示不同方法的结果。
- en: We will assume we have flipped an unfair coin with true `θ = 0.3` 100 times,
    and observed heads 36 times, and tails 64 times.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将假设我们抛掷了一枚不公平的硬币，真实的`θ = 0.3`进行了100次实验，并观察到36次正面和64次反面。
- en: Maximum Likelihood Estimation (MLE)
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大似然估计（MLE）
- en: 'Now, let’s come to our first concept of interest: MLE. In this, we want to
    find that set of parameters, which maximises the likelihood of the observed data.
    For this, we form the likelihood function, form the derivative w.r.t. the parameters
    and solve for the root. Note that, even if you might not have known the name before,
    it is very likely you applied this concept before — this actually is the core
    of most ML algorithms. Think of a neural network to solve a regression or classification
    problem: L2-loss or cross-entropy can be interpreted as likelihood, and gradient
    descent then finds the optimum.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看我们的第一个感兴趣的概念：MLE。在这里，我们要找到使观察数据的似然最大化的参数集。为此，我们形成似然函数，对参数求导并解根。注意，即使你以前可能不知道这个名称，你很可能已经应用过这个概念——这实际上是大多数机器学习算法的核心。想象一个神经网络来解决回归或分类问题：L2损失或交叉熵可以被解释为似然，而梯度下降则找到最优解。
- en: 'Thus — let’s take above likelihood formula, and find that `θ` maximising it
    — i.e. form the derivative and set it to 0:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因此——让我们取上述似然公式，并找到使其最大化的`θ`——即求导并将其设为0：
- en: '![](../Images/e4f4e574d0166c9e5556244c01508d31.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4f4e574d0166c9e5556244c01508d31.png)'
- en: 'Solving for 0 and some smaller reformulations give us the MLE estimate for
    `θ`:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 解0和一些较小的重构给出了`θ`的MLE估计：
- en: '![](../Images/f4039511cb85dc16f027f5f8637d4f0f.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f4039511cb85dc16f027f5f8637d4f0f.png)'
- en: 'This should also make sense intuitively: the parameter `θ`, the probability
    of the coin landing heads, should — without any other prior information or constraints
    — equal the ratio of observed head results and total number of throws.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这也应该从直观上讲得通：参数`θ`，即硬币正面朝上的概率，在没有其他先验信息或约束的情况下，应该等于观察到的正面结果与总投掷次数的比例。
- en: 'Let’s apply this to our dataset introduced above. We obtain:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将此应用于上述介绍的数据集。我们得到：
- en: '![](../Images/fd2467fd2930d5b91345f6abd8e8f8b3.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd2467fd2930d5b91345f6abd8e8f8b3.png)'
- en: Maximum a Posteriori Estimation (MAP)
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大后验估计（MAP）
- en: MLE represents maximising the likelihood —finding the set of parameter values
    which explain the data best, without any prior information or further constraints.
    Now, we turn to MAP — which entails introducing a prior over the parameters —
    and is actually equivalent to finding the maximum of the posterior distribution.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: MLE代表最大化似然——找到解释数据最好的参数值集合，而无需任何先验信息或进一步的约束。现在，我们转向MAP——这涉及引入一个参数的先验——实际上等同于寻找后验分布的最大值。
- en: MLE and MAP are good examples for the discussions between Frequentist and Bayesian
    concepts. While Frequentists view probabilities simply as the results of observing
    random, repeatable events — the Bayesian view models everything, including parameters,
    as random variables — which can be updated given new evidence. While Bayesian
    concepts (in my opinion) usually give more powerful solutions, one core Frequentist
    criticism is their need of priors — which have to come from somewhere, and can
    be wrong.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: MLE和MAP是频率派与贝叶斯概念之间讨论的良好示例。频率派认为概率只是观察随机、可重复事件的结果——而贝叶斯观点则将一切，包括参数，建模为随机变量——这些变量可以根据新证据进行更新。虽然贝叶斯概念（在我看来）通常提供更强大的解决方案，但一个核心的频率派批评是其对先验的需求——这些先验必须来自某处，并且可能是错误的。
- en: Further note, that adding regularisation to ML models actually can be shown
    to equal MAP (in addition to above statement, that models without regularisation
    represent MLE concepts). But this interesting realisation would lead too far here,
    and we will dedicate a future post to it.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步说明，将正则化添加到机器学习模型中实际上可以被证明等同于最大后验估计（MAP）（除了上述声明，即没有正则化的模型代表最大似然估计（MLE）概念）。但这一有趣的认识在这里过于深入，我们将专门在未来的文章中讨论。
- en: 'Let’s revisit the introductory formula:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新审视引言公式：
- en: '![](../Images/c65eadcada2478c53f78c81b1eedc2ff.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c65eadcada2478c53f78c81b1eedc2ff.png)'
- en: 'Since `p(x)` is a constant, and independent of `θ`, when forming the derivative
    w.r.t. `θ` and solving for 0, it disappears. Thus, MAP indeed gives us the point
    estimate for `θ` maximising the posterior distribution. All we need, is a prior.
    We choose a [beta distribution](https://en.wikipedia.org/wiki/Beta_distribution),
    whose pdf for a given `θ` is characterised by:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`p(x)`是一个常数，并且独立于`θ`，在对`θ`求导并解0时，它会消失。因此，MAP确实给出了最大化后验分布的`θ`点估计。我们需要的只是一个先验。我们选择一个[贝塔分布](https://en.wikipedia.org/wiki/Beta_distribution)，其给定`θ`的概率密度函数由以下特征描述：
- en: '![](../Images/0c62cfbf8d9215d6a627c3622a76231d.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c62cfbf8d9215d6a627c3622a76231d.png)'
- en: 'Why we chose a beta distribution we will discuss in the next section (hint:
    conjugate priors).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择 beta 分布的原因将在下一节讨论（提示：共轭先验）。
- en: 'For simplicity we again apply the logarithm, resulting in the following problem:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为简化起见，我们再次应用对数运算，得到以下问题：
- en: '![](../Images/112cde29aea31a116f7f2cbd9af1b03e.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/112cde29aea31a116f7f2cbd9af1b03e.png)'
- en: 'Thus, when forming the derivative and solving for 0, we can consider each summand
    separately — and the left one we already calculated above to be:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在形成导数并解出 0 时，我们可以分别考虑每个加数——左侧的我们已经在上面计算过：
- en: '![](../Images/e4f4e574d0166c9e5556244c01508d31.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4f4e574d0166c9e5556244c01508d31.png)'
- en: 'Let us now look at the right one:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看右侧的图像：
- en: '![](../Images/db603282f037a3cd6b3892689a1a0c69.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/db603282f037a3cd6b3892689a1a0c69.png)'
- en: 'Forming the derivative yields:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 形成导数得到：
- en: '![](../Images/b8e3c150cb5a8372ace1cee33bd3e535.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b8e3c150cb5a8372ace1cee33bd3e535.png)'
- en: 'We can now combine these two terms, and solve for 0:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将这两个项结合起来，并解出 0：
- en: '![](../Images/59c982b02c48de555279b8d869d76bf2.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/59c982b02c48de555279b8d869d76bf2.png)'
- en: 'Some reformulations return the map estimate for `θ`:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一些重构返回 `θ` 的映射估计：
- en: '![](../Images/9c94ba790ec5e60d9a20acfebe93be8c.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c94ba790ec5e60d9a20acfebe93be8c.png)'
- en: To apply this formula, we first have to come up with our prior. Let’s say we
    have vaguely heard (e.g. from the coin manufacturer) that the true heads probability
    should be around 0.3, but we are not really sure.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 要应用这个公式，我们首先必须制定我们的先验。假设我们模糊地听说（例如来自硬币制造商）真实的正面概率应该在0.3左右，但我们并不确定。
- en: 'To model this belief, let’s pick a beta distribution with `α = 4` and `β =
    10`. We can plot this distribution via:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了建模这个信念，让我们选择一个 `α = 4` 和 `β = 10` 的 beta 分布。我们可以通过以下方式绘制这个分布：
- en: '[PRE0]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Resulting in the following plot:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 得到以下图形：
- en: '![](../Images/ccd6f0a79cb6d01f122d4bb454428f60.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ccd6f0a79cb6d01f122d4bb454428f60.png)'
- en: Image generated by author
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 作者生成的图像
- en: 'Using this prior, we obtain:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个先验，我们得到：
- en: '![](../Images/d17ce3327363c4e1f9c51b00384310f4.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d17ce3327363c4e1f9c51b00384310f4.png)'
- en: Bayesian Inference
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯推断
- en: 'Now, let’s come to full Bayesian inference and solve for the full posterior
    distribution instead of a point estimate for the parameters: i.e., now we will
    obtain a probability distribution over the parameter(s) conditioned on the observed
    data.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来进行完全的贝叶斯推断，求解完整的后验分布，而不是参数的点估计：也就是说，我们将获得一个以观察数据为条件的参数的概率分布。
- en: 'Let’s first examine the nominator:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 首先我们来检查分子：
- en: '![](../Images/a6663642b39188bbad3ce268d171f23c.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a6663642b39188bbad3ce268d171f23c.png)'
- en: 'Plugging in the previously introduced distributions and results we obtain:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 插入之前介绍的分布和结果，我们得到：
- en: '![](../Images/22eaa87581641ec45f3423e9e24d9ed9.png)![](../Images/4f99a0fc97dc3b769be626427f6f5769.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22eaa87581641ec45f3423e9e24d9ed9.png)![](../Images/4f99a0fc97dc3b769be626427f6f5769.png)'
- en: 'This gives rise to the above teased notion of [conjugate priors](https://en.wikipedia.org/wiki/Conjugate_prior):
    the prior is called conjugate prior for the likelihood, if the resulting distribution
    is in the same family as the prior — which also translates to the posterior being
    in the same family, which we’ll see in a bit.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了之前提到的[共轭先验](https://en.wikipedia.org/wiki/Conjugate_prior)的概念：如果结果分布与先验分布属于同一家族，那么这个先验被称为对似然的共轭先验——这也意味着后验分布也属于同一家族，我们稍后将看到。
- en: 'Let’s now come to the evidence, the denumerator. This usually is the tricky
    part and often is intractable, as it entails marginalising over all possible parameter
    values:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看证据，即分母。这通常是棘手的部分，且往往是不可解的，因为它涉及到对所有可能的参数值进行边际化：
- en: '![](../Images/2998bb7cb8f6326eb19ecc79a05648c6.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2998bb7cb8f6326eb19ecc79a05648c6.png)'
- en: This integral usually is hard to compute, or even intractable— and one of the
    main reasons (exact) Bayesian inference is hard, or intractable. However, as we
    will see later on, usually we don’t even try to solve this analytically, but resort
    to various approximation techniques, which also yield satisfactory results.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这个积分通常很难计算，甚至不可解——这是(精确)贝叶斯推断困难或不可解的主要原因之一。然而，正如我们稍后将看到的，通常我们不会尝试解析解，而是 resort
    to 各种近似技术，这些技术也能产生令人满意的结果。
- en: 'In our case, there exists an analytical, closed-form solution though. Looking
    at the integral, we spot the inside of it being the same as our above calculation:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，虽然存在一个解析的闭式解。观察积分，我们发现其内部与上述计算是相同的：
- en: '![](../Images/2f9db0feac1732945eff58253500e9f0.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f9db0feac1732945eff58253500e9f0.png)'
- en: 'Following the definition of the [Beta function](https://en.wikipedia.org/wiki/Beta_function),
    which we cheekily ignored above, we observe that this is actually the Beta function
    evaluated at `Nₕ+α-1`, `N-Nₕ-1`:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们之前巧妙忽略的[Beta函数](https://en.wikipedia.org/wiki/Beta_function)的定义，我们观察到这实际上是Beta函数在`Nₕ+α-1`，`N-Nₕ-1`的值：
- en: '![](../Images/69200c955e699f32ee6b6b358453ea70.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/69200c955e699f32ee6b6b358453ea70.png)'
- en: 'Now we can put nominator and demoninator together to obtain a closed-form version
    for the posterior:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将分子和分母放在一起，得到后验的闭式解：
- en: '![](../Images/1a708ca4c2ddbfb69414f9441335a891.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a708ca4c2ddbfb69414f9441335a891.png)'
- en: 'Let’s see how this distribution looks like, keeping our beta prior from before
    with `α = 4` and `β = 10`:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个分布的样子，保持我们之前的beta先验`α = 4`和`β = 10`：
- en: '![](../Images/8c03edefd7388239f692bb897225516a.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c03edefd7388239f692bb897225516a.png)'
- en: Image generated by author
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由作者生成
- en: 'Making use of existing knowledge about the [beta distribution](https://en.wikipedia.org/wiki/Beta_distribution),
    the mean of a random variable `X` following a beta distribution with parameters
    `α` and `β` is given by:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 利用关于[Beta分布](https://en.wikipedia.org/wiki/Beta_distribution)的现有知识，一个遵循beta分布的随机变量`X`的均值由参数`α`和`β`给出：
- en: '![](../Images/7a49ea2f1d21132c7fc811d5e8474d5c.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a49ea2f1d21132c7fc811d5e8474d5c.png)'
- en: Which, in our case evaluates to the same result as obtained by MAP, namely 0.348.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，这计算结果与MAP得到的结果相同，即0.348。
- en: 'For the variance we obtain:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于方差，我们得到：
- en: '![](../Images/9c986d91e585e09cfd41524105f2259e.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c986d91e585e09cfd41524105f2259e.png)'
- en: Discussion of Results
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果讨论
- en: In this section we’ll discuss and compare the results obtained in the previous
    sections.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论并比较前几节中获得的结果。
- en: '**MLE** estimates `θ`, the probability of the coin landing heads to be 0.36
    — which is exactly the relative frequency of observed head tosses (36 / 100).
    This makes sense. Without any other information, prior knowledge, … the parameter
    best explaining the data should be that one reflecting it — which is 0.36 in our
    case.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**MLE**估计`θ`，即硬币正面朝上的概率为0.36——这正是观察到的正面投掷的相对频率（36 / 100）。这很有道理。在没有其他信息、先验知识的情况下……最能解释数据的参数应为那个反映数据的参数——在我们的案例中为0.36。'
- en: 'Our **MAP** estimate is 0.348, which is closer to the true value of 0.3, and
    closer towards the mode of the prior. The latter point here is the one causing
    this: since our prior is centred around 0.3 with a relatively small variance,
    this is reflected in the final result.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的**MAP**估计为0.348，这更接近真实值0.3，并且更接近于先验的模式。这里的最后一点就是造成这种情况的原因：由于我们的先验围绕0.3中心，且方差相对较小，这在最终结果中得到了反映。
- en: 'To see this effect, consider a prior with a higher variance, e.g. given by
    `α = 2` and `β = 3`:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了观察这种效果，考虑一个方差更大的先验，例如由`α = 2`和`β = 3`给出：
- en: '![](../Images/b8f2f094764ce0c8ac1e713085298f86.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b8f2f094764ce0c8ac1e713085298f86.png)'
- en: Image generated by author
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由作者生成
- en: In this case, our MAP estimate becomes 0.359 — which is closer to the MLE value,
    as it’s less affected by the prior.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们的MAP估计变为0.359——这更接近MLE值，因为它受到先验的影响较小。
- en: '**Bayesian Inference** returns a full posterior distribution. Its mode is 0.348
    — i.e. the same as the MAP estimate. This is expected, as MAP is simply the point
    estimate solution for the posterior distribution. However, having the full posterior
    distribution gives us much more insights into the problem — which we’ll cover
    two sections down.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**贝叶斯推断**返回了完整的后验分布。其模式为0.348，即与MAP估计相同。这是预期之中的，因为MAP只是后验分布的点估计解。然而，拥有完整的后验分布能让我们对问题有更多的洞察——我们将在接下来的两节中深入探讨。'
- en: Numerical Approximations of the Posterior Distribution
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 后验分布的数值近似
- en: For this example, we managed to find a closed-form for the posterior and thus
    solve the Bayesian inference problem analytically. However, as stated before,
    this usually is hard or even infeasible. Luckily, in practise we don’t need to,
    or often don’t even try to solve this problem analytically — but instead resort
    to some sort of approximation.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我们成功找到后验的闭式解，从而分析性地解决了贝叶斯推断问题。然而，正如之前所述，这通常很困难甚至不可行。幸运的是，在实际中，我们不需要，也通常不会尝试分析性地解决这个问题——而是
    resort to 一些近似方法。
- en: 'There are several possibilities for this, here, we employ [numerical approximation
    using MCMC methods](https://medium.com/towards-data-science/introduction-to-markov-chain-monte-carlo-mcmc-methods-b5bad18bc243).
    In the linked post I’m introducing this at length, so I would kindly refer there
    for more details. Here, we just briefly summarise the core concepts: MCMC methods
    work by defining a Markov chain which is relatively simple to sample from, and
    whose stationary distribution is the target distribution. We then follow this
    Markov chain, generating `N` dependent data samples — which, due to the stationary
    property, equals sampling from the target distribution.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几种可能性，我们使用了[使用MCMC方法的数值近似](https://medium.com/towards-data-science/introduction-to-markov-chain-monte-carlo-mcmc-methods-b5bad18bc243)。在链接的文章中，我详细介绍了这一点，因此我在此建议参考更多细节。在这里，我们只是简要总结核心概念：MCMC方法通过定义一个相对简单的马尔可夫链来工作，从中采样的目标分布是其平稳分布。然后我们跟随这个马尔可夫链，生成`N`个相关的数据样本——由于平稳特性，这等同于从目标分布中采样。
- en: Here we now want to apply this principle, in particular the Metropolis-Hastings
    algorithm, to approximate the posterior distribution we solved analytically above.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们现在想应用这个原则，特别是Metropolis-Hastings算法，来近似我们之前解析得到的后验分布。
- en: 'We can do this with the following code snippet:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码片段来完成：
- en: '[PRE1]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The last two lines plot the sampled posterior distribution (histogram) vs the
    calculated posterior (line) — and we observe the expected overlap:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 最后两行绘制了采样后验分布（直方图）与计算后验（线）——我们观察到预期的重叠：
- en: '![](../Images/2eabc24e462943bbf41835ca2a457030.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2eabc24e462943bbf41835ca2a457030.png)'
- en: Image generated by author
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者生成的图像
- en: Why do Bayesian Inference?
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么要做贝叶斯推断？
- en: 'Having come this far, you might ask the question: all methods shown so far
    give me the same (or very similar) mode of the posterior distribution — leading
    me to always pick the same parameter in the end. Why bother then to do this complicated
    Bayesian inference?'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你可能会问这样一个问题：所有展示的方法都给我相同（或非常相似）的后验分布模式——导致我最终总是选择相同的参数。那么为什么还要做这种复杂的贝叶斯推断呢？
- en: In this final section we will answer this question. We’ll first explain what
    advantages this brings, and finish up with a practical example showcasing this.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一节中，我们将回答这个问题。我们首先解释这带来了什么好处，然后用一个实际的例子来展示这一点。
- en: 'To start: doing Bayesian inference gives us much more information. We not only
    get the mode of the posterior distribution, but the full distribution. We can
    thus inspect it, calculate other moments (such as the variance) and overall get
    a much better understanding of the problem. In particular, through this we get
    a sense of uncertainty, and can also decide to reject our hypothesis explaining
    the data. Let’s demonstrate this on an example.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 首先：做贝叶斯推断给了我们更多的信息。我们不仅获得了后验分布的模式，还获得了完整的分布。因此，我们可以检查它，计算其他矩（例如方差），并总体上更好地理解问题。特别是，通过这点我们能够感知不确定性，还可以决定是否拒绝我们解释数据的假设。让我们通过一个例子来演示这一点。
- en: 'Assume we change our coin analysis into a “game”: we are given two coins, and
    have to pick that one which is fair, i.e. lands 50:50.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们把硬币分析变成一个“游戏”：我们有两个硬币，必须选择那个公平的，即出现50:50的硬币。
- en: 'The game show hosts presents you with two coins:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏节目主持人给你呈现了两个硬币：
- en: Coin 1 was thrown 8 times and landed heads in 4 of them.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬币1被抛掷了8次，其中4次出现了正面。
- en: Coin 2 was thrown 100 times and landed heads 50 times of these.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬币2被抛掷了100次，其中50次出现了正面。
- en: On a first glance, both seem to land heads with around 50% probability. However,
    intuitively most people would surely pick coin 2 — as here we have a much larger
    sample size. Clever as you are, you quickly apply the maximum a posteriori method
    in your head. You pick a beta distribution with `α = β = 2`, giving you a nicely
    spread symmetric distribution over [0, 1] with mode 0.5.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 初看之下，两者似乎都有大约50%的概率出现正面。然而，直观上大多数人肯定会选择硬币2——因为这里我们有一个更大的样本量。聪明的你迅速在脑海中应用了最大后验法。你选择了一个`α
    = β = 2`的贝塔分布，为你提供了一个在[0, 1]区间上对称分布，模式为0.5。
- en: 'Let’s do the math and calculate the MAP estimates for `θ`:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们做数学计算并计算`θ`的MAP估计：
- en: '`θ₁` = (4+2-1)/(8+2+2–2)=0.5'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`θ₁` = (4+2-1)/(8+2+2–2)=0.5'
- en: '`θ₂`= (50+2–1)/(100+2+2–2)=0.5'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`θ₂`= (50+2–1)/(100+2+2–2)=0.5'
- en: Thus, according to the MAP method, both coins yield exactly the same result!
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，根据MAP方法，两枚硬币的结果完全相同！
- en: 'Let’s plot the full posterior distribution for both, as obtaind by Bayesian
    inference above:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制贝叶斯推断上得到的两个硬币的完整后验分布：
- en: '![](../Images/d51ac655967c1610b315ee4a5e08c33c.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
- en: Image generated by author
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Now, following our expectations, the posterior for Coin 2 has a much lower variance
    — justifying that we should pick this!
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we introduced MLE (maximum likelihood estimation), MAP (maximum
    a posteriori estimation) and Bayesian inference. We used the example of an unfair
    coin throughout for demonstration.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: MLE finds parameters which optimize the likelihood. MAP introduces a new prior
    over the parameters, returning parameters which maximize the full posterior. Thus,
    both MLE and MAP return point estimates. In contrast, Bayesian inference models
    the full posterior distribution. This usually is a complex, often even intractable
    task — but also more powerful, as we get more insights into this distribution,
    such as the variance.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: This brings us to the end of this article. Thanks for reading!
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 'Notes:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: All images, unless stated otherwise, were generated by the author.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples are calculations throughout this post were partially motivated by [this
    great tutorial](https://engineering.purdue.edu/kak/Trinity.pdf).
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
