["```py\ndef main(spark, country):\n    \"\"\"Example function for processing data received from different countries\"\"\"\n    # fetch input data\n    ...\n    # preprocess based on individual country requirements\n    if country == 'US':\n       # preprocessing for US\n    elif country == 'GB':\n       # preprocessing for UK\n    else:\n       # more preprocessing etc..\n    # join dataframes together\n    ...\n    # run some calculations/aggregate\n    ...\n    # save results\n    ...\n```", "```py\nimport pyspark.sql.functions as F\nfrom pyspark.sql import DataFrame\n\ndef classify_debit_credit_transactions(\n    transactionsDf: DataFrame, accountDf: DataFrame\n) -> DataFrame:\n    \"\"\"Join transactions with account information and classify as debit/credit\"\"\"\n    # normalise strings\n    transactionsDf = transactionsDf.withColumn(\n        \"transaction_information_cleaned\",\n        F.regexp_replace(F.col(\"transaction_information\"), r\"[^A-Z0-9]+\", \"\"),\n    )\n    # join on customer account using first 9 characters\n    transactions_accountsDf = transactionsDf.join(\n        accountDf,\n        on=F.substring(F.col(\"transaction_information_cleaned\"), 1, 9)\n        == F.col(\"account_number\"),\n        how=\"inner\",\n    )\n    # classify transactions as from debit or credit account customers\n    credit_account_ids = [\"100\", \"101\", \"102\"]\n    debit_account_ids = [\"200\", \"201\", \"202\"]\n    transactions_accountsDf = transactions_accountsDf.withColumn(\n        \"business_line\",\n        F.when(F.col(\"business_line_id\").isin(credit_account_ids), F.lit(\"credit\"))\n        .when(F.col(\"business_line_id\").isin(debit_account_ids), F.lit(\"debit\"))\n        .otherwise(F.lit(\"other\")),\n    )\n    return transactions_accountsDf \n```", "```py\nimport pyspark.sql.functions as F\nfrom pyspark.sql import DataFrame\n\ndef classify_debit_credit_transactions(\n    transactionsDf: DataFrame, accountsDf: DataFrame\n) -> DataFrame:\n    \"\"\"Join transactions with account information and classify as debit/credit\"\"\"\n    transactionsDf = normalise_transaction_information(transactionsDf)\n    transactions_accountsDf = join_transactionsDf_to_accountsDf(\n        transactionsDf, accountsDf\n    )\n    transactions_accountsDf = apply_debit_credit_business_classification(\n        transactions_accountsDf\n    )\n    return transactions_accountsDf\n```", "```py\ndef normalise_transaction_information(transactionsDf: DataFrame) -> DataFrame:\n    \"\"\"Remove special characters from transaction information\"\"\"\n    return transactionsDf.withColumn(\n        \"transaction_information_cleaned\",\n        F.regexp_replace(F.col(\"transaction_information\"), r\"[^A-Z0-9]+\", \"\"),\n    )\n```", "```py\ndef join_transactionsDf_to_accountsDf(\n    transactionsDf: DataFrame, accountsDf: DataFrame\n) -> DataFrame:\n    \"\"\"Join transactions to accounts information\"\"\"\n    return transactionsDf.join(\n        accountsDf,\n        on=F.substring(F.col(\"transaction_information_cleaned\"), 1, 9)\n        == F.col(\"account_number\"),\n        how=\"inner\",\n    )\n```", "```py\ndef apply_debit_credit_business_classification(\n    transactions_accountsDf: DataFrame,\n) -> DataFrame:\n    \"\"\"Classify transactions as coming from debit or credit account customers\"\"\"\n\n    CREDIT_ACCOUNT_IDS = [\"101\", \"102\", \"103\"]\n    DEBIT_ACCOUNT_IDS = [\"202\", \"202\", \"203\"]\n\n    return transactions_accountsDf.withColumn(\n        \"business_line\",\n        F.when(F.col(\"business_line_id\").isin(CREDIT_ACCOUNT_IDS), F.lit(\"credit\"))\n        .when(F.col(\"business_line_id\").isin(DEBIT_ACCOUNT_IDS), F.lit(\"debit\"))\n        .otherwise(F.lit(\"other\")),\n    )\n```", "```py\n# conftest.py\nfrom pyspark.sql import SparkSession\nimport pytest\n\n@pytest.fixture(scope=\"session\")\ndef spark():\n    spark = (\n        SparkSession.builder.master(\"local[1]\")\n        .appName(\"local-tests\")\n        .config(\"spark.executor.cores\", \"1\")\n        .config(\"spark.executor.instances\", \"1\")\n        .config(\"spark.sql.shuffle.partitions\", \"1\")\n        .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n        .getOrCreate()\n    )\n    yield spark\n    spark.stop()\n```", "```py\nfrom src.data_processing import (\n    apply_debit_credit_business_classification,\n    classify_debit_credit_transactions,\n    join_transactionsDf_to_accountsDf,\n    normalise_transaction_information,\n)\n```", "```py\ndef test_classify_debit_credit_transactions(spark):\n    # create input test dataframes\n    transactionsDf = spark.createDataFrame(\n        data=[\n            (\"1\", 1000.00, \"123-456-789\"),\n            (\"3\", 3000.00, \"222222222EUR\"),\n        ],\n        schema=[\"transaction_id\", \"amount\", \"transaction_information\"],\n    )\n    accountsDf = spark.createDataFrame(\n        data=[\n            (\"123456789\", \"101\"),\n            (\"222222222\", \"202\"),\n            (\"000000000\", \"302\"),\n        ],\n        schema=[\"account_number\", \"business_line_id\"],\n    )\n\n    # output dataframe after applying function\n    output = classify_debit_credit_transactions(transactionsDf, accountsDf)\n\n    # expected outputs in the target column\n    expected_classifications = [\"credit\", \"debit\"]\n\n    # assert results are as expected\n    assert output.count() == 2\n    assert [row.business_line for row in output.collect()] == expected_classifications\n```", "```py\ndef test_normalise_transaction_information(spark):\n    data = [\"123-456-789\", \"123456789\", \"123456789EUR\", \"TEXT*?WITH.*CHARACTERS\"]\n    test_df = spark.createDataFrame(data, \"string\").toDF(\"transaction_information\")\n    expected = [\"123456789\", \"123456789\", \"123456789EUR\", \"TEXTWITHCHARACTERS\"]\n    output = normalise_transaction_information(test_df)\n    assert [row.transaction_information_cleaned for row in output.collect()] == expected\n```", "```py\ndef test_join_transactionsDf_to_accountsDf(spark):\n    data = [\"123456789\", \"222222222EUR\"]\n    transactionsDf = spark.createDataFrame(data, \"string\").toDF(\n        \"transaction_information_cleaned\"\n    )\n    data = [\n        \"123456789\",  # match\n        \"222222222\",  # match\n        \"000000000\",  # no-match\n    ]\n    accountsDf = spark.createDataFrame(data, \"string\").toDF(\"account_number\")\n    output = join_transactionsDf_to_accountsDf(transactionsDf, accountsDf)\n    assert output.count() == 2\n```", "```py\ndef test_apply_debit_credit_business_classification(spark):\n    data = [\n        \"101\",  # credit\n        \"202\",  # debit\n        \"000\",  # other\n    ]\n    df = spark.createDataFrame(data, \"string\").toDF(\"business_line_id\")\n    output = apply_debit_credit_business_classification(df)\n    expected = [\"credit\", \"debit\", \"other\"]\n    assert [row.business_line for row in output.collect()] == expected\n```"]