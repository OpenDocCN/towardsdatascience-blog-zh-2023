["```py\nSELECT COUNT(DISTINCT customer_id) AS num_customers\nFROM purchases\nWHERE product_name = 'eggs'\nAND EXTRACT(MONTH FROM purchase_date) = 11;\n```", "```py\nCREATE TABLE purchases (\n    purchase_id INT PRIMARY KEY,\n    customer_id INT,\n    product_name VARCHAR(255),\n    purchase_date DATE\n);\n```", "```py\nfrom google.colab import drive\ndrive.mount('/content/drive')\n```", "```py\n!huggingface-cli login\n```", "```py\n!pip install torch torchvision torchaudio\n!pip install \"axolotl[flash-attn,deepspeed] @ git+https://github.com/OpenAccess-AI-Collective/axolotl\"\n!pip install accelerate\n!pip install datasets\n!pip install torch==2.0.1+cu118 -f https://download.pytorch.org/whl/torch_stable.html\n!pip uninstall xformers\n!pip install xformers\n!pip uninstall flash_attn -y\n!pip install flash_attn\n```", "```py\n!git clone https://github.com/OpenAccess-AI-Collective/axolotl /content/drive/MyDrive/fine_tune_llm/axolotl\n```", "```py\n!cd /content/drive/MyDrive/fine_tune_llm/axolotl && pip install packaging && pip install -e '.[flash-attn,deepspeed]'\n```", "```py\nds = datasets.load_dataset('knowrohit07/know_sql')\nds\ntrn = ds['validation']\ntrn[4500]\n\n###output###\n# {'answer': 'SELECT MAX(field_goals) FROM table_19722233_5 WHERE assists = 27',\n# 'question': 'what is the highest number of goals did the player with 27 asists score',\n# 'context': 'CREATE TABLE table_19722233_5 (field_goals INTEGER, assists VARCHAR)'}\n```", "```py\nbase_model: meta-llama/Llama-2-7b-hf\nbase_model_config: meta-llama/Llama-2-7b-hf\nmodel_type: LlamaForCausalLM\ntokenizer_type: LlamaTokenizer\nis_llama_derived_model: true\n\nload_in_8bit: false\nload_in_4bit: true\nstrict: false\n\ndatasets:\n  - path: knowrohit07/know_sql\n    type: context_qa.load_v2\n    train_on_split: validation\ndataset_prepared_path: last_run_prepared\nval_set_size: 0.01\noutput_dir: ./qlora-out\n\nadapter: qlora\nlora_model_dir:\n\nsequence_len: 2048\nsample_packing: false\npad_to_sequence_len: true\n\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_modules:\nlora_target_linear: true\nlora_fan_in_fan_out:\n\nwandb_project:\nwandb_entity:\nwandb_watch:\nwandb_run_id:\nwandb_log_model:\n\ngradient_accumulation_steps: 4\nmicro_batch_size: 2\nnum_epochs: 1\noptimizer: paged_adamw_32bit\nlr_scheduler: cosine\nlearning_rate: 0.0002\n\ntrain_on_inputs: false\ngroup_by_length: false\nbf16: false\nfp16: true\ntf32: false\n\ngradient_checkpointing: true\nearly_stopping_patience:\nresume_from_checkpoint:\nlocal_rank:\nlogging_steps: 1\nxformers_attention:\nflash_attention: true\n\nwarmup_steps: 10\neval_steps: 20\neval_table_size: 5\nsave_steps:\ndebug:\ndeepspeed:\nweight_decay: 0.0\nfsdp:\nfsdp_config:\nspecial_tokens:\n  bos_token: \"<s>\"\n  eos_token: \"</s>\"\n  unk_token: \"<unk>\"\n```", "```py\n\"\"\"Module containing the classes for Context QA Prompt Tokenization Strategies\"\"\"\nfrom typing import Tuple\nfrom axolotl.prompt_tokenizers import InstructionPromptTokenizingStrategy\nfrom axolotl.prompters import AlpacaPrompter, PromptStyle\n\n# article, unanswerable_question, question, answer\ndef load_404(tokenizer, cfg):\n    return AlpacaMissingInfoContextPromptTokenizingStrategy(\n        AlpacaContextPrompter(PromptStyle.CHAT.value),\n        tokenizer,\n        cfg.train_on_inputs,\n        cfg.sequence_len,\n    )\n\ndef load(tokenizer, cfg):\n    return AlpacaContextPromptTokenizingStrategy(\n        AlpacaContextPrompter(PromptStyle.CHAT.value),\n        tokenizer,\n        cfg.train_on_inputs,\n        cfg.sequence_len,\n    )\n\nclass AlpacaContextPrompter(AlpacaPrompter):\n    \"\"\"\n    Customized system prompted for concise QA\n    \"\"\"\n\n    system_prompt = (\n        \"Use the following contextual information to concisely answer the question.\\n\"\n    )\n    system_no_input_prompt = (\n        \"Use the following contextual information to concisely answer the question.\\n\"\n    )\n\nclass AlpacaContextPromptTokenizingStrategy(InstructionPromptTokenizingStrategy):\n    \"\"\"\n    Tokenization Strategy to combine in-context article with a question and answer\n    \"\"\"\n\n    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str]:\n        return (\n            prompt[\"context\"] + \"\\n===\\n\" + prompt[\"question\"],\n            \"\",\n            prompt[\"answer\"],\n        )\n\nclass AlpacaMissingInfoContextPromptTokenizingStrategy(\n    InstructionPromptTokenizingStrategy\n):\n    \"\"\"\n    Tokenization Strategy to combine in-context article with a question that can't be answered\n    from the context and a default response to that effect\n    \"\"\"\n\n    def parse_instruction_fields(self, prompt) -> Tuple[str, str, str]:\n        return (\n            prompt[\"context\"] + \"\\n===\\n\" + prompt[\"unanswerable_question\"],\n            \"\",\n            \"The context provided does not contain any information about your inquiry. \"\n            \"Therefore, I'm unable to answer your question based on the given context.\",\n        )\n```", "```py\n!accelerate launch -m axolotl.cli.train /content/drive/MyDrive/fine_tune_llm/sql.yml\n```", "```py\n!pip install torch==2.0.1+cu118 -f https://download.pytorch.org/whl/torch_stable.html\n!pip uninstall flash_attn -y\n!pip install flash_attn\n```", "```py\ntst = dict(**trn[4500])\ntst['question'] = 'What is the highest number of goals for any player with less than 5 assists'\ntst\n###output###\n# {'answer': 'SELECT MAX(field_goals) FROM table_19722233_5 WHERE assists = 27',\n# 'question': 'What is the highest number of goals for any player with less than 5 assists',\n# 'context': 'CREATE TABLE table_19722233_5 (field_goals INTEGER, assists VARCHAR)'}\n```", "```py\nfmt = \"\"\"SYSTEM: Use the following contextual information to concisely answer the question.\n\nUSER: {}\n===\n{}\nASSISTANT:\"\"\"\n```", "```py\ndef sql_prompt(d):\n  return fmt.format(d[\"context\"], d[\"question\"])\n```", "```py\nprint(sql_prompt(tst))\n###output###\n# SYSTEM: Use the following contextual information to concisely answer the question.\n\n# USER: CREATE TABLE table_19722233_5 (field_goals INTEGER, assists VARCHAR)\n# ===\n# What is the highest number of goals for any player with less than 5 assists\n# ASSISTANT:\n```", "```py\nimport torch\nfrom peft import PeftModel\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nfine_tuned_model = '/content/drive/MyDrive/fine_tune_llm/axolotl/qlora-out'\n\ntokr = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')\n\nmodel = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-2-7b-hf',\n                                             torch_dtype=torch.bfloat16, device_map=0)\nmodel = PeftModel.from_pretrained(model, fine_tuned_model)\nmodel = model.merge_and_unload()\nmodel.save_pretrained('sql-model')\n\ntoks = tokr(sql_prompt(tst), return_tensors=\"pt\")\n\nres = model.generate(**toks.to(\"cuda\"), max_new_tokens=250).to('cpu')\n\nprint(tokr.batch_decode(res)[0])\n###output###\n# SYSTEM: Use the following contextual information to concisely answer the question.\n\n# USER: CREATE TABLE table_19722233_5 (field_goals INTEGER, assists VARCHAR)\n# ===\n# What is the highest number of goals for any player with less than 5 assists\n# ASSISTANT:\n# SELECT MAX(field_goals) FROM table_19722233_5 WHERE assists < 5</s>\n```"]