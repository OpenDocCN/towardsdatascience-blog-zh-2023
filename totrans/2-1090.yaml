- en: How Much Forecasting Performance Do You Lose During Model Selection?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-much-forecasting-performance-do-you-lose-during-model-selection-923889e2f2dc](https://towardsdatascience.com/how-much-forecasting-performance-do-you-lose-during-model-selection-923889e2f2dc)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How often does cross-validation pick the best forecasting model? What happens
    when it doesn’t?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://vcerq.medium.com/?source=post_page-----923889e2f2dc--------------------------------)[![Vitor
    Cerqueira](../Images/9e52f462c6bc20453d3ea273eb52114b.png)](https://vcerq.medium.com/?source=post_page-----923889e2f2dc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----923889e2f2dc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----923889e2f2dc--------------------------------)
    [Vitor Cerqueira](https://vcerq.medium.com/?source=post_page-----923889e2f2dc--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----923889e2f2dc--------------------------------)
    ·5 min read·Jan 6, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/39b7e74a9c3a5f831df25ddcdefda989.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Héctor J. Rivas](https://unsplash.com/@hjrc33?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you have a forecasting problem. You need to select a model to solve
    it. [You may want to test a few alternatives with cross-validation](/4-things-to-do-when-applying-cross-validation-with-time-series-c6a5674ebf3a).
  prefs: []
  type: TYPE_NORMAL
- en: Have you ever wondered what’s the chance that cross-validation selects the best
    possible model? And, if not, how poorer is the model that is picked?
  prefs: []
  type: TYPE_NORMAL
- en: Let’s find out.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Cross-validation, for time series or otherwise, solves two problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Performance estimation.** How well is the model going to perform in new data?
    You can use these estimations to assess whether the model can be deployed;'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Selection.** Use the above estimates to rank a pool of available models.
    For example, different configurations of a learning algorithm for hyper-parameter
    tuning. In this case, you select the model with the best performance estimates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wait! Aren’t these two problems the same?
  prefs: []
  type: TYPE_NORMAL
- en: Not really. A given method (say, [TimeSeriesSplits](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html))
    may provide good performance estimates, on average. But it can be poor for ranking
    the available models, thereby poor for model selection.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let me give an example. Suppose you want to compare four models: *M1*, *M2*,
    *M3*, and *M4*. These are shown in the x-axis of Figure 1 below.'
  prefs: []
  type: TYPE_NORMAL
- en: The true test loss of these models is displayed in blue bars. Their ranking
    is *M1* > *M2* > *M3* > *M4*. So, *M1* is the best model because it shows the
    lowest error (say, the mean absolute error).
  prefs: []
  type: TYPE_NORMAL
- en: Then, two cross-validation methods (*CV1* and *CV2*) are used to estimate the
    error of each model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5cfdaa5ac8d9a69c342a8bae8172f02a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The goal of cross-validation is to approximate the true error (blue
    bars). CV1 (light green blue) provides, on average, better estimations than CV2
    (dark green). But CV2 ranks the models perfectly, unlike CV1.'
  prefs: []
  type: TYPE_NORMAL
- en: '*CV1* produces the best estimations (nearest to the true error), on average.
    But, the estimated ranking (*M2* > *M1* > *M4* > *M3*) is different than the actual
    one. It is also worse than the ranking produced by *CV2*.'
  prefs: []
  type: TYPE_NORMAL
- en: Despite providing worse performance estimates, *CV2* outputs a perfect ranking
    of the models.
  prefs: []
  type: TYPE_NORMAL
- en: This example shows that one CV technique can be better for performance estimation
    (*CV1*), but another for model selection (*CV2*).
  prefs: []
  type: TYPE_NORMAL
- en: Performance Loss During Model Selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Suppose you’re doing model selection for forecasting. Two questions may come
    to your mind:'
  prefs: []
  type: TYPE_NORMAL
- en: What’s the chance that cross-validation selects the best model? The one that
    will have the best performance in the test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What happens when it doesn’t? How poorer is the performance of the selected
    model?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Testing Different Cross-Validation Approaches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can answer these questions by simulating a realistic scenario. First, apply
    cross-validation to select a model using the training data. Then, check how this
    model does in a test set.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s do this step-by-step.
  prefs: []
  type: TYPE_NORMAL
- en: I prepared 50 different forecasting models. These include different configurations
    of linear models, and decision trees, among others. [The models are trained with
    a supervised learning approach called auto-regression.](/machine-learning-for-forecasting-transformations-and-feature-extraction-bbbea9de0ac2)
    Without going into details, the recent past values are used as explanatory variables.
    The target variables are future observations.
  prefs: []
  type: TYPE_NORMAL
- en: Then, I applied several cross-validation techniques to select the best model.
    These include [TimeSeriesSplits](/4-things-to-do-when-applying-cross-validation-with-time-series-c6a5674ebf3a)
    (a.k.a. Time Series Cross-Validation), [MonteCarloCV](https://medium.com/towards-data-science/monte-carlo-cross-validation-for-time-series-ed01c41e2995),
    or [K-fold Cross-validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html).
    You can check a description for each method [in a previous article](https://medium.com/@vcerq/9-techniques-for-cross-validating-time-series-data-7828fc3f781d).
  prefs: []
  type: TYPE_NORMAL
- en: I repeated this process for almost 3000 different time series.
  prefs: []
  type: TYPE_NORMAL
- en: Here are the results.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation Selection Accuracy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The selection accuracy is the percentage of times a cross-validation approach
    picks the best model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/80625c7c26117114b581529f09c3a7de.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Accuracy of different cross-validation methods for selecting the
    best forecasting model. The description of each method is available [in a previous
    article](https://medium.com/@vcerq/9-techniques-for-cross-validating-time-series-data-7828fc3f781d).
    Image by Author.'
  prefs: []
  type: TYPE_NORMAL
- en: The scores range from 7% to 10%.
  prefs: []
  type: TYPE_NORMAL
- en: Sounds low, right? Still, if you were to select a model at random you’d expect
    a 2% accuracy (1 over 50 possible models). So, 7% to 10% is way better than that.
  prefs: []
  type: TYPE_NORMAL
- en: Yet, all methods will probably fail to select the best model. Then, comes the
    second question.
  prefs: []
  type: TYPE_NORMAL
- en: How good is the selected model?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To answer this question, we compare the selected model with the model that should
    have been selected.
  prefs: []
  type: TYPE_NORMAL
- en: We can measure the percentage difference in error between these two. The difference
    is 0 when the best model is selected by cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a88956776f6c10350d79706e8b67b7bb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Average percentage difference in error (and respective standard deviation)
    between the selected model and the best possible model. Image by Author.'
  prefs: []
  type: TYPE_NORMAL
- en: Most estimators select a model that performs about 0.3% worse than the best
    possible model, on average. There are some differences here and there. But, by
    and large, different cross-validation methods show similar performance for model
    selection.
  prefs: []
  type: TYPE_NORMAL
- en: The exception is *Holdout*, which represents [a single split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).
    This corroborates the recommendation I put forth in [a previous article](https://medium.com/towards-data-science/4-things-to-do-when-applying-cross-validation-with-time-series-c6a5674ebf3a).
    Unless the time series is large, carry out many splits if you can.
  prefs: []
  type: TYPE_NORMAL
- en: You can check the full experiments in the [article](https://arxiv.org/pdf/2104.00584.pdf)
    in reference [1]. These can be reproduced using the code available in [my Github](https://github.com/vcerqueira/model_selection_forecasting).
  prefs: []
  type: TYPE_NORMAL
- en: Take Aways
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model selection is the process of using cross-validation for selecting a model
    from a pool of alternatives;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With 50 alternative models, cross-validation has a 7%-10% chance of picking
    the best one;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the best model is not picked, the selected model will perform about 0.3–0.35%
    worse, on average;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Several cross-validation splits are important for better model selection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thanks for reading and see you in the next story!
  prefs: []
  type: TYPE_NORMAL
- en: Related Articles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[4 Things to Do When Applying Cross-Validation with Time Series](https://medium.com/towards-data-science/4-things-to-do-when-applying-cross-validation-with-time-series-c6a5674ebf3a)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Monte Carlo Cross-Validation for Time Series](/monte-carlo-cross-validation-for-time-series-ed01c41e2995)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further Readings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Cerqueira, Vitor, Luis Torgo, and Carlos Soares. “Model Selection for Time
    Series Forecasting: Empirical Analysis of Different Estimators.” *arXiv preprint
    arXiv:2104.00584* (2021).'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Arlot, Sylvain, and Alain Celisse. “A survey of cross-validation procedures
    for model selection.” *Statistics surveys* 4 (2010): 40–79.'
  prefs: []
  type: TYPE_NORMAL
