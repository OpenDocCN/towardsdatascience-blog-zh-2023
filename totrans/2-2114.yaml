- en: 'TimesNet: The Latest Advance in Time Series Forecasting'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/timesnet-the-latest-advance-in-time-series-forecasting-745b69068c9c](https://towardsdatascience.com/timesnet-the-latest-advance-in-time-series-forecasting-745b69068c9c)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Understand the TimesNet architecture and apply it on a forecasting task with
    Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@marcopeixeiro?source=post_page-----745b69068c9c--------------------------------)[![Marco
    Peixeiro](../Images/7cf0a81d87281d35ff47f51e3026a3e9.png)](https://medium.com/@marcopeixeiro?source=post_page-----745b69068c9c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----745b69068c9c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----745b69068c9c--------------------------------)
    [Marco Peixeiro](https://medium.com/@marcopeixeiro?source=post_page-----745b69068c9c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----745b69068c9c--------------------------------)
    ·10 min read·Oct 10, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9a499aed5a781591ea06eef5f3cb4222.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Rachel Hisko](https://unsplash.com/@rachelhisko?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: In previous articles, we have explored the latest advances in state-of-the-art
    forecasting techniques, starting with [N-BEATS](https://medium.com/towards-data-science/the-easiest-way-to-forecast-time-series-using-n-beats-d778fcc2ba60)
    released in 2020, [N-HiTS](https://medium.com/towards-data-science/all-about-n-hits-the-latest-breakthrough-in-time-series-forecasting-a8ddcb27b0d5)
    in 2022, and [PatchTST](https://medium.com/towards-data-science/patchtst-a-breakthrough-in-time-series-forecasting-e02d48869ccc)
    in March 2023\. Recall that N-BEATS and N-HiTS rely on the multilayer perceptron
    architecture, while PatchTST leverages the Transformer architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'As of April 2023, a new model was published in the literature, and it achieves
    state-of-the-art results across multiple tasks in time series analysis, like forecasting,
    imputation, classification and anomaly detection: **TimesNet**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TimesNet was proposed by Wu, Hu, Liu et al in their paper: [TimesNet: Temporal
    2D-Variation Modeling For General Time Series Analysis](https://browse.arxiv.org/pdf/2210.02186.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike previous models, it uses a CNN-based architecture to achieve state-of-the-art
    results across different tasks, making it a great candidate for a foundation model
    for time series analysis.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we explore the architecture and inner workings of TimesNet.
    Then, we apply the model in a forecasting task, alongside N-BEATS and N-HiTS to
    complete our own little experiment.
  prefs: []
  type: TYPE_NORMAL
- en: As always, for more details, refer to the [original paper](https://browse.arxiv.org/pdf/2210.02186.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: '**Learn the latest time series analysis techniques with my** [**free time series
    cheat sheet**](https://www.datasciencewithmarco.com/pl/2147608294) **in Python!
    Get the implementation of statistical and deep learning techniques, all in Python
    and TensorFlow!**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: Explore TimesNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The motivation behind TimesNet comes from the realization that many real-life
    time series exhibit *mutli-periodicity*. This means that variations occur at different
    periods.
  prefs: []
  type: TYPE_NORMAL
- en: For example, temperature outside has both a daily and a yearly period. Usually,
    it is hotter during the day than at night, and hotter during summer than in winter.
  prefs: []
  type: TYPE_NORMAL
- en: Now, those multiple periods overlap and interact with each other, making it
    difficult to separate and model individually.
  prefs: []
  type: TYPE_NORMAL
- en: So, the authors of TimesNet propose to reshape the series in a 2D space to model
    *intraperiod-variation* and *interperiod-variation*.
  prefs: []
  type: TYPE_NORMAL
- en: Going back to our weather example, intraperiod-variation would be how the temperature
    varies within a day, and interperiod-variation would be how the temperature varies
    from day to day, or from year to year.
  prefs: []
  type: TYPE_NORMAL
- en: With all that in mind, let’s dive into the model’s architecture.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of TimesNet
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s take a look at the architecture of TimesNet.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fddcabd04aad86837a21d001fb1f9e41.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Architecture of TimesNet. Image by Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou,
    Jianmin Wang and Mingsheng Long from [TimesNet: Temporal 2D-Variation Modeling
    For General Time Series Analysis](https://browse.arxiv.org/pdf/2210.02186.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: From the figure above, we can see that TimesNet is a stack of multiple *TimesBlock*
    with residual connections.
  prefs: []
  type: TYPE_NORMAL
- en: In each TimesBlock, we can see that the series first goes through a fast Fourier
    transform (FTT) to find the different periods in the data. Then, it is reshaped
    as a 2D vector and sent to an Inception block where it learns and predicts a 2D
    representation of the series. Then, this deep representation must be reshaped
    back to a 1-dimensional vector using *adaptive aggregation*.
  prefs: []
  type: TYPE_NORMAL
- en: There is a lot to understand here, so let’s cover each step in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Capture multi-periodicity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To capture the variations across multiple periods in time series, the authors
    suggest to transform the 1-dimensional series into a 2-dimensional space to simultaneously
    model intraperiod and interperiod variations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0eaa094828efbb858b774d945b1e3cda.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Architecture of TimesNet. Image by Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou,
    Jianmin Wang and Mingsheng Long from [TimesNet: Temporal 2D-Variation Modeling
    For General Time Series Analysis](https://browse.arxiv.org/pdf/2210.02186.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: In the figure above, we can see the how the model represents the variations
    in a 2D space. Inside the red rectangles, we can see the intraperiod-variation,
    which is how the data changes inside a period. Then, the blue rectangle contains
    the interperiod-variation, which is how the data changes from period to period.
  prefs: []
  type: TYPE_NORMAL
- en: To better understand that, suppose that we have daily data with a weekly period.
    The interperiod-variation is how the data changes from Monday, to Tuesday, to
    Wednesday, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Then, the interperiod-variation would be how the data varies from Monday in
    week 1, to Monday in week 2, and from Tuesday in week 1, to Tuesday in week 2\.
    In other words, it is the variation of data in the same *phase* at different periods.
  prefs: []
  type: TYPE_NORMAL
- en: Those variations are then represented in a 2D space, where the interperiod-variation
    is vertical, and the intraperiod-variation is horizontal. This allows the model
    to learn a better representation of the variation in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Whereas a 1-dimensional vector shows variations between adjacent points, this
    2D representation shows variations between adjacent points and adjacent periods,
    giving a more complete picture of what is happening.
  prefs: []
  type: TYPE_NORMAL
- en: 'Yet, one question remains: how do we find the periods in our series?'
  prefs: []
  type: TYPE_NORMAL
- en: Identify the significant periods in your data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To identify the multiple periods in time series, the model applied the Fast
    Fourier Transform (FTT).
  prefs: []
  type: TYPE_NORMAL
- en: This is a mathematical operation that transforms a signal into a function of
    frequency and amplitude.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/01998c92d2637704def7ba8389662d71.png)'
  prefs: []
  type: TYPE_IMG
- en: 'How the model applies FTT to find the top *k* significant periods in time series.
    Image by Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang and Mingsheng
    Long from [TimesNet: Temporal 2D-Variation Modeling For General Time Series Analysis](https://browse.arxiv.org/pdf/2210.02186.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: In the figure above, the authors illustrate how FTT is applied. Once we have
    the frequency and amplitude of each period, the ones with the largest amplitudes
    are considered to be the most relevant.
  prefs: []
  type: TYPE_NORMAL
- en: For example, here is the result of performing FTT on the [Etth1 dataset](https://github.com/zhouhaoyi/ETDataset/blob/main/ETT-small/ETTh1.csv).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8c6261f834ae9c8a3167775f64b93cd5.png)'
  prefs: []
  type: TYPE_IMG
- en: Result of applying FTT on the Etth1 dataset. We can see that the daily and yearly
    periods are the most significant. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: In the figure above, the Fast Fourier Transform allows us to quickly identify
    a daily and yearly period in our data, since we see higher peaks of amplitude
    at those periods.
  prefs: []
  type: TYPE_NORMAL
- en: Once FTT is applied, the user can set a parameter *k* to select the top-*k*
    most important periods, which are the ones with the largest amplitude.
  prefs: []
  type: TYPE_NORMAL
- en: TimesNet then creates 2D vectors for each period, and those are sent to a 2D
    kernel to capture the temporal variations.
  prefs: []
  type: TYPE_NORMAL
- en: Inside the TimesBlock
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the series has gone through the Fourier transform and 2D tensors were created
    for the top-*k* periods, the data is sent to the Inception block, as shown in
    the figure below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fddcabd04aad86837a21d001fb1f9e41.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Architecture of TimesNet. Image by Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou,
    Jianmin Wang and Mingsheng Long from [TimesNet: Temporal 2D-Variation Modeling
    For General Time Series Analysis](https://browse.arxiv.org/pdf/2210.02186.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, note that all of the steps that we explored are carried out inside
    the TimesBlock.
  prefs: []
  type: TYPE_NORMAL
- en: Now, the 2D representation of the data is sent to the **Inception** block.
  prefs: []
  type: TYPE_NORMAL
- en: The Inception module is the building block of the computer vision model [GoogLeNet](https://openaccess.thecvf.com/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf),
    published in 2015.
  prefs: []
  type: TYPE_NORMAL
- en: The main idea of the Inception module is to have an efficient representation
    of the data by keeping it sparse. That way, we can technically increase the size
    of a neural network, while keeping it computationally efficient.
  prefs: []
  type: TYPE_NORMAL
- en: This is achieved by performing various convolutions and pooling operations,
    and then concatenating everything. In the context of TimesNet, this is how the
    Inception module looks like.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a4a7627cc60f293116401161f2e0beb8.png)'
  prefs: []
  type: TYPE_IMG
- en: The Inception module in TimesNet. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: You might wonder why the authors chose a vision model to treat time series data.
  prefs: []
  type: TYPE_NORMAL
- en: A simple answer to that is that vision models are particularly good at parsing
    2D data, like images.
  prefs: []
  type: TYPE_NORMAL
- en: The other benefit is that the vision backbone can be changed inside TimesNet.
    While the authors use the Inception block, it can be changed to other vision model
    backbones, and so TimesNet can also benefit from the advances in computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: Now, one element that separates the Inception module in TimesNet from the inception
    module in GoogLeNet is the use of **adaptive aggregation**.
  prefs: []
  type: TYPE_NORMAL
- en: Adaptive aggregation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To perform aggregation, the 2D representation must be reshaped to 1D vectors
    first.
  prefs: []
  type: TYPE_NORMAL
- en: Then, adaptive aggregation is used, because different periods had different
    amplitudes, which indicates how important they are.
  prefs: []
  type: TYPE_NORMAL
- en: This is why the output of the FTT is also sent to a softmax layer, so that the
    aggregation is done using the relative important of each period.
  prefs: []
  type: TYPE_NORMAL
- en: The aggregated data is the output of a single TimesBlock. Then, multiple TimesBlock
    are stacked with residual connections to create the TimesNet model.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand how the TimesNet model works, let’s test it on a forecasting
    task, alongside N-BEATS and N-HITS.
  prefs: []
  type: TYPE_NORMAL
- en: Forecast with TimesNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s now apply the TimesNet model on a forecasting task, and compare its performance
    against N-BEATS and N-HiTS.
  prefs: []
  type: TYPE_NORMAL
- en: For this little experiment, we use the [Etth1 dataset](https://github.com/zhouhaoyi/ETDataset)
    released under the Creative Commons Attribution license.
  prefs: []
  type: TYPE_NORMAL
- en: This is a popular benchmark for time series forecasting widely used in literature.
    It tracks the hourly oil temperature of an electricity transformer which reflects
    the condition of the equipment.
  prefs: []
  type: TYPE_NORMAL
- en: You can access the dataset and the code on [GitHub](https://github.com/marcopeix/time-series-analysis).
  prefs: []
  type: TYPE_NORMAL
- en: Import libraries and read the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We start off by importing the required libraries. Here, we use the implementation
    available in [NeuralForecast](https://nixtla.github.io/neuralforecast/) by Nixtla.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Then, we can read our CSV file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/485be680afc497d5d08c797437f1d4f4.png)'
  prefs: []
  type: TYPE_IMG
- en: First five rows of the Etth1 dataset. Note that the dataset has already the
    format expected by neuralforecast. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the figure above, note that the dataset already has the format expected
    by NeuralForecast. Basically, the package requires three columns:'
  prefs: []
  type: TYPE_NORMAL
- en: a date column labelled as *ds*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: an id column to label your series, labelled as *unique_id*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a value column labelled as *y*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, we can plot our series.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/da948d2b72d002b8f6c679986b30a4e3.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualizing the hourly oil temperature in the Etth1 dataset. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can move on to forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: Forecast
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For our experiment, we use a forecasting horizon of 96 hours, which is a common
    horizon for long-term forecasting in the literature.
  prefs: []
  type: TYPE_NORMAL
- en: We also reserve two windows of 96 time steps to evaluate our models.
  prefs: []
  type: TYPE_NORMAL
- en: First, we define a list of models that we want to use to carry out the forecasting
    task. Again, we will use N-BEATS, N-HiTS, and TimesNet.
  prefs: []
  type: TYPE_NORMAL
- en: We will keep the default parameters for all models, and limit the maximum number
    of epochs to 50\. Note that by default, TimesNet will select the top 5 most important
    periods in our data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Once this is done, we can instantiate the *NeuralForecasts* object with the
    list of our models and the frequency of our data, which is hourly.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Then, we run cross-validation so that we have the predictions and the actual
    values of our dataset. That way, we can evaluate the performance of each model.
  prefs: []
  type: TYPE_NORMAL
- en: Again, we use two windows of 96 time steps for the evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3eb99dd01226ce7a1505c8f248f2109d.png)'
  prefs: []
  type: TYPE_IMG
- en: First five rows of the predictions DataFrame. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Once the models are trained and predictions are done, we get the *DataFrame*
    above. We can see the actual values, as well as the predictions coming from each
    model that we specified earlier.
  prefs: []
  type: TYPE_NORMAL
- en: We can also easily visualize the predictions against the actual values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/bd875864d12fcffe264c54dfc50b1822.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualizing the predictions of each model on the test set. Here, all models
    fail to predict the decrease observed in the test set. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: In the figure above, it seems that all models fail to predict the decrease in
    oil temperature observed in the test set. Also, N-BEATS and N-HiTS have captured
    some cyclical pattern that is not observed in the predictions of TimesNet.
  prefs: []
  type: TYPE_NORMAL
- en: Still, we need to evaluate the models by calculating the MSE and MAE to determine
    which model is best.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, we simply compute the MAE and MSE to find out which model performed best.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/fc278927215ef99d54ee81093117097f.png)'
  prefs: []
  type: TYPE_IMG
- en: Peformance metrics of each model forthe long-term forecasting task on the Etth1
    dataset. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: From the figure above, we can see the N-HiTS achives the lowest MAE, while N-BEATS
    achieves the lowest MSE.
  prefs: []
  type: TYPE_NORMAL
- en: However, there is a difference of 0.002 in MAE, and a difference of 0.00025
    in MSE. Since the difference in MSE is so small, especially considering the error
    is squared, I would argue that N-HiTS is the champion model for this task.
  prefs: []
  type: TYPE_NORMAL
- en: So, it turns out that TimesNet did not achieve the best performance. However,
    keep in mind that we just conducted a simple experiment with no hyperparameter
    optimization, on one dataset only, and on two windows of 96 time steps.
  prefs: []
  type: TYPE_NORMAL
- en: This is really meant to show you how to work with TimesNet and NeuralForecast,
    so that you can have another tool in your toolbox to solve your next forecasting
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TimesNet is a CNN-based model that leverages the Inception module to achieve
    state-of-the-art performances on many time series analysis tasks, such as forecasting,
    classification, imputation and anomaly detection.
  prefs: []
  type: TYPE_NORMAL
- en: As always, each forecasting problem requires a unique approach and a specific
    model, so make sure to test out TimesNet as well as other models.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading! I hope that you enjoyed it and that you learned something
    new!
  prefs: []
  type: TYPE_NORMAL
- en: Looking to master time series forecasting? The check out my course [Applied
    Time Series Forecasting in Python](https://www.datasciencewithmarco.com/offers/zTAs2hi6/checkout?coupon_code=ATSFP10).
    This is the only course that uses Python to implement statistical, deep learning
    and state-of-the-art models in 16 guided hands-on projects.
  prefs: []
  type: TYPE_NORMAL
- en: Cheers 🍻
  prefs: []
  type: TYPE_NORMAL
- en: Support me
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Enjoying my work? Show your support with [Buy me a coffee](http://buymeacoffee.com/dswm),
    a simple way for you to encourage me, and I get to enjoy a cup of coffee! If you
    feel like it, just click the button below 👇
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/0b01a9e378012cb5701b0525dcf23def.png)](https://www.buymeacoffee.com/dswm)'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang and Mingsheng Long —
    [TimesNet: Temporal 2D-Variation Modeling For General Time Series Analysis](https://browse.arxiv.org/pdf/2210.02186.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,
  prefs: []
  type: TYPE_NORMAL
- en: Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich — [Going
    Deeper With Convolutions](https://openaccess.thecvf.com/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf)
  prefs: []
  type: TYPE_NORMAL
