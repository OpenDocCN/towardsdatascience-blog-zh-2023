["```py\n!pip install -q datasets\n\nfrom datasets import load_dataset \n\n# Load dataset\ndataset = load_dataset(\"Matthijs/snacks\")\nprint(dataset)\n\n# Output\n  '''\n  DatasetDict({\n      train: Dataset({\n          features: ['image', 'label'],\n          num_rows: 4838\n      })\n      test: Dataset({\n          features: ['image', 'label'],\n          num_rows: 952\n      })\n      validation: Dataset({\n          features: ['image', 'label'],\n          num_rows: 955\n      })\n  })''' \n```", "```py\nprint(dataset[\"train\"].features['label'].names)\n\n# Output\n'''\n['apple','banana','cake','candy','carrot','cookie','doughnut','grape',\n'hot dog', 'ice cream','juice','muffin','orange','pineapple','popcorn',\n'pretzel','salad','strawberry','waffle','watermelon']''' \n```", "```py\n# Mapping from label to index and vice versa\nlabels = dataset[\"train\"].features[\"label\"].names\nnum_labels = len(dataset[\"train\"].features[\"label\"].names)\nlabel2id, id2label = dict(), dict()\nfor i, label in enumerate(labels):\n    label2id[label] = i\n    id2label[i] = label\n\nprint(label2id)\nprint(id2label)\n\n# Output\n'''\n{'apple': 0, 'banana': 1, 'cake': 2, 'candy': 3, 'carrot': 4, 'cookie': 5, 'doughnut': 6, 'grape': 7, 'hot dog': 8, 'ice cream': 9, 'juice': 10, 'muffin': 11, 'orange': 12, 'pineapple': 13, 'popcorn': 14, 'pretzel': 15, 'salad': 16, 'strawberry': 17, 'waffle': 18, 'watermelon': 19}\n{0: 'apple', 1: 'banana', 2: 'cake', 3: 'candy', 4: 'carrot', 5: 'cookie', 6: 'doughnut', 7: 'grape', 8: 'hot dog', 9: 'ice cream', 10: 'juice', 11: 'muffin', 12: 'orange', 13: 'pineapple', 14: 'popcorn', 15: 'pretzel', 16: 'salad', 17: 'strawberry', 18: 'waffle', 19: 'watermelon'}\n'''\n```", "```py\nimport torch\nimport torch.nn as nn\n\n# Create toy image with dim (batch x channel x width x height)\ntoy_img = torch.rand(1, 3, 48, 48)\n\n# Define conv layer parameters\nnum_channels = 3\nhidden_size = 768 #or emb_dimension\npatch_size = 16\n\n# Conv 2D layer\nprojection = nn.Conv2d(num_channels, hidden_size, kernel_size=patch_size, \n             stride=patch_size)\n\n# Forward pass toy img\nout_projection = projection(toy_img)\n\nprint(f'Original image size: {toy_img.size()}')\nprint(f'Size after projection: {out_projection.size()}')\n\n# Output\n'''\nOriginal image size: torch.Size([1, 3, 48, 48])\nSize after projection: torch.Size([1, 768, 3, 3])\n'''\n```", "```py\n# Flatten the output after projection with Conv2D layer\n\npatch_embeddings = out_projection.flatten(2).transpose(1, 2)\nprint(f'Patch embedding size: {patch_embeddings.size()}')\n\n# Output\n'''\nPatch embedding size: torch.Size([1, 9, 768]) #[batch, no. of patches, emb_dim]\n'''\n```", "```py\n# Define [CLS] token embedding with the same emb dimension as the patches\nbatch_size = 1\ncls_token = nn.Parameter(torch.randn(1, 1, hidden_size))\ncls_tokens = cls_token.expand(batch_size, -1, -1)\n\n# Prepend [CLS] token in the beginning of patch embedding\npatch_embeddings = torch.cat((cls_tokens, patch_embeddings), dim=1)\nprint(f'Patch embedding size: {patch_embeddings.size()}')\n\n# Output\n'''\nPatch embedding size: torch.Size([1, 10, 768]) #[batch, no. of patches+1, emb_dim]\n'''\n```", "```py\n# Define position embedding with the same dimension as the patch embedding\nposition_embeddings = nn.Parameter(torch.randn(batch_size, 10, hidden_size))\n\n# Add position embedding into patch embedding\ninput_embeddings = patch_embeddings + position_embeddings\nprint(f'Input embedding size: {input_embeddings.size()}')\n\n# Output\n'''\nInput embedding size: torch.Size([1, 10, 768]) #[batch, no. of patches+1, emb_dim]\n'''\n```", "```py\n# Define parameters for ViT-base (example)\nnum_heads = 12\nnum_layers = 12\n\n# Define Transformer encoders' stack\ntransformer_encoder_layer = nn.TransformerEncoderLayer(\n           d_model=hidden_size, nhead=num_heads,\n           dim_feedforward=int(hidden_size * 4),\n           dropout=0.1)\ntransformer_encoder = nn.TransformerEncoder(\n           encoder_layer=transformer_encoder_layer,\n           num_layers=num_layers)\n\n# Forward pass\noutput_embeddings = transformer_encoder(input_embeddings)\nprint(f' Output embedding size: {output_embeddings.size()}')\n\n# Output\n'''\nOutput embedding size: torch.Size([1, 10, 768])\n'''\n```", "```py\n!pip install transformers\n\nfrom transformers import ViTModel\n\n# Load pretrained model\nmodel_checkpoint = 'google/vit-base-patch16-224-in21k'\nmodel = ViTModel.from_pretrained(model_checkpoint, add_pooling_layer=False)\n\n# Example input image\ninput_img = torch.rand(batch_size, num_channels, 224, 224)\n\n# Forward pass input image\noutput_embedding = model(input_img)\nprint(output_embedding)\nprint(f\"Ouput embedding size: {output_embedding['last_hidden_state'].size()}\")\n\n# Output\n'''\nBaseModelOutputWithPooling(last_hidden_state=tensor([[[ 0.0985, -0.2080,  0.0727,  ...,  0.2035,  0.0443, -0.3266],\n         [ 0.1899, -0.0641,  0.0996,  ..., -0.0209,  0.1514, -0.3397],\n         [ 0.0646, -0.3392,  0.0881,  ..., -0.0044,  0.2018, -0.3038],\n         ...,\n         [-0.0708, -0.2932, -0.1839,  ...,  0.1035,  0.0922, -0.3241],\n         [ 0.0070, -0.3093, -0.0217,  ...,  0.0666,  0.1672, -0.4103],\n         [ 0.1723, -0.1037,  0.0317,  ..., -0.0571,  0.0746, -0.2483]]],\n       grad_fn=<NativeLayerNormBackward0>), pooler_output=None, hidden_states=None, attentions=None)\n\nOutput embedding size: torch.Size([1, 197, 768])\n'''\n```", "```py\nnum_labels = 20\n\n# Define linear classifier layer\nclassifier = nn.Linear(hidden_size, num_labels) \n\n# Forward pass on the output embedding of [CLS] token\noutput_classification = classifier(output_embedding['last_hidden_state'][:, 0, :])\nprint(f\"Output embedding size: {output_classification.size()}\")\n\n# Output\n'''\nOutput embedding size: torch.Size([1, 20]) #[batch, no. of labels]\n'''\n```", "```py\nimport numpy as np\nimport torch\nimport cv2\nimport torch.nn as nn\nfrom transformers import ViTModel, ViTConfig\nfrom torchvision import transforms\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n#Pretrained model checkpoint\nmodel_checkpoint = 'google/vit-base-patch16-224-in21k'\n```", "```py\nclass ImageDataset(torch.utils.data.Dataset):\n\n  def __init__(self, input_data):\n\n      self.input_data = input_data\n      # Transform input data\n      self.transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Resize((224, 224), antialias=True),\n        transforms.Normalize(mean=[0.5, 0.5, 0.5], \n                             std=[0.5, 0.5, 0.5])\n        ])\n\n  def __len__(self):\n      return len(self.input_data)\n\n  def get_images(self, idx):\n      return self.transform(self.input_data[idx]['image'])\n\n  def get_labels(self, idx):\n      return self.input_data[idx]['label']\n\n  def __getitem__(self, idx):\n      # Get input data in a batch\n      train_images = self.get_images(idx)\n      train_labels = self.get_labels(idx)\n\n      return train_images, train_labels\n```", "```py\nclass ViT(nn.Module):\n\n  def __init__(self, config=ViTConfig(), num_labels=20, \n               model_checkpoint='google/vit-base-patch16-224-in21k'):\n\n        super(ViT, self).__init__()\n\n        self.vit = ViTModel.from_pretrained(model_checkpoint, add_pooling_layer=False)\n        self.classifier = (\n            nn.Linear(config.hidden_size, num_labels) \n        )\n\n  def forward(self, x):\n\n    x = self.vit(x)['last_hidden_state']\n    # Use the embedding of [CLS] token\n    output = self.classifier(x[:, 0, :])\n\n    return output\n```", "```py\ndef model_train(dataset, epochs, learning_rate, bs):\n\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    # Load nodel, loss function, and optimizer\n    model = ViT().to(device)\n    criterion = nn.CrossEntropyLoss().to(device)\n    optimizer = Adam(model.parameters(), lr=learning_rate)\n\n    # Load batch image\n    train_dataset = ImageDataset(dataset)\n    train_dataloader = DataLoader(train_dataset, num_workers=1, batch_size=bs, shuffle=True)\n\n    # Fine tuning loop\n    for i in range(epochs):\n        total_acc_train = 0\n        total_loss_train = 0.0\n\n        for train_image, train_label in tqdm(train_dataloader):\n            output = model(train_image.to(device))\n            loss = criterion(output, train_label.to(device))\n            acc = (output.argmax(dim=1) == train_label.to(device)).sum().item()\n            total_acc_train += acc\n            total_loss_train += loss.item()\n\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n        print(f'Epochs: {i + 1} | Loss: {total_loss_train / len(train_dataset): .3f} | Accuracy: {total_acc_train / len(train_dataset): .3f}')\n\n    return model\n\n# Hyperparameters\nEPOCHS = 10\nLEARNING_RATE = 1e-4\nBATCH_SIZE = 8\n\n# Train the model\ntrained_model = model_train(dataset['train'], EPOCHS, LEARNING_RATE, BATCH_SIZE)\n```", "```py\ndef predict(img):\n\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Resize((224, 224)),\n        transforms.Normalize(mean=[0.5, 0.5, 0.5], \n                             std=[0.5, 0.5, 0.5])\n        ])\n\n    img = transform(img)\n    output = trained_model(img.unsqueeze(0).to(device))\n    prediction = output.argmax(dim=1).item()\n\n    return id2label[prediction]\n```", "```py\nprint(predict(dataset['test'][900]['image']))\n# Output: waffle\n```", "```py\nprint(predict(dataset['test'][250]['image']))\n# Output: cookie\n```"]