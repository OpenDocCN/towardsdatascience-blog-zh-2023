- en: 'Perceptrons: The First Neural Network Model'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/perceptrons-the-first-neural-network-model-8b3ee4513757](https://towardsdatascience.com/perceptrons-the-first-neural-network-model-8b3ee4513757)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Overview and implementation in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@roiyeho?source=post_page-----8b3ee4513757--------------------------------)[![Dr.
    Roi Yehoshua](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----8b3ee4513757--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8b3ee4513757--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8b3ee4513757--------------------------------)
    [Dr. Roi Yehoshua](https://medium.com/@roiyeho?source=post_page-----8b3ee4513757--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8b3ee4513757--------------------------------)
    ·14 min read·Mar 28, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/878b3bf6ae98c62e8e147e6ad2892349.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Hal Gatewood](https://unsplash.com/ja/@halacious?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/OgvqXGL7XO4?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: Perceptrons are one of the earliest computational models of neural networks
    (NNs), and they form the basis for the more complex and deep networks we have
    today. Understanding the perceptron model and its theory will provide you with
    a good basis for understanding many of the key concepts in neural networks in
    general.
  prefs: []
  type: TYPE_NORMAL
- en: 'Background: Biological Neural Networks'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A biological neural network (such as the one we have in our brain) is composed
    of a large number of nerve cells called **neurons**.
  prefs: []
  type: TYPE_NORMAL
- en: Each neuron receives electrical signals (impulses) from its neighboring neurons
    via fibers called **dendrites**. When the total sum of its incoming signals exceeds
    some threshold, the neuron “fires” its own signal via long fibers called **axons**
    that are connected to the dendrites of other neurons.
  prefs: []
  type: TYPE_NORMAL
- en: The junction between two neurons is called a **synapse**. On average, each neuron
    is connected to about 7,000 synapses, which demonstrates the high connectivity
    of the network we have in our brain. When we learn new associations between two
    concepts, the synaptic strength between the neurons that represent these concepts
    is strengthened. This phenomenon is known as **Hebb’s rule** (1949) that states
    “Cells that fire together wire together”.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ba4ba1b74e63ac1caa391f0d56969023.png)'
  prefs: []
  type: TYPE_IMG
- en: Biological neuron ([public image](https://en.wikipedia.org/wiki/Neuron#/media/File:Blausen_0657_MultipolarNeuron.png)
    freely licensed under Wikimedia Commons)
  prefs: []
  type: TYPE_NORMAL
- en: The Perceptron Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The perceptron model, introduced by Frank Rosenblatt in 1957, is a simplified
    model of a biological neuron.
  prefs: []
  type: TYPE_NORMAL
- en: The perceptron has *m* binary inputs denoted by *x*₁, …, *xₘ*, which represent
    the incoming signals from its neighboring neurons, and it outputs a single binary
    value denoted by *o* that indicates if the perceptron is “firing” or not.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/043749ddc33f1bca4ca04eef5dd60caa.png)'
  prefs: []
  type: TYPE_IMG
- en: The perceptron model
  prefs: []
  type: TYPE_NORMAL
- en: Each input neuron *xᵢ* is connected to the perceptron via a link whose strength
    is represented by a weight *wᵢ*. Inputs with higher weights have a larger influence
    on the perceptron’s output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The perceptron first computes the weighted sum of its incoming signals, by
    multiplying each input by its corresponding weight. This weighted sum is often
    called **net input** and denoted by *z*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f7f2b629f0e4be2dab30d99b80d48746.png)'
  prefs: []
  type: TYPE_IMG
- en: The net input of the perceptron
  prefs: []
  type: TYPE_NORMAL
- en: 'If the net input exceeds some predefined threshold value *θ*, then the perceptron
    fires (its output is 1), otherwise it doesn’t fire (its output is 0). In other
    words, the perceptron fires if and only if:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7bf847811aec716d437c5eba51b5e35b.png)'
  prefs: []
  type: TYPE_IMG
- en: Our goal is to find the weights *w*₁, …, *wₘ* and the threshold *θ,* such that
    the perceptron will map correctly its inputs *x*₁, …, *xₘ* (representing the features
    in our data) to the desired output *y* (representing the label).
  prefs: []
  type: TYPE_NORMAL
- en: To simplify the learning process, instead of having to learn separately the
    weights and the threshold, we add a special input neuron called **bias neuron**
    that always outputs the value 1\. This neuron is typically denoted by *x*₀ and
    its connection weight is denoted by *b* or *w*₀.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a result, the net input of the perceptron becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/84fd07a2f3ead2616c2b01f358b0b5b0.png)'
  prefs: []
  type: TYPE_IMG
- en: The net input including the bias
  prefs: []
  type: TYPE_NORMAL
- en: This formulation allows us to learn the correct threshold (bias) as if it were
    one of the weights of the incoming signals.
  prefs: []
  type: TYPE_NORMAL
- en: 'In vector form, we can write *z* as the **dot product** between the input vector
    **x** = (*x*₁, …, *xₘ*)*ᵗ* and the weight vector **w** = (*w*₁, …, *wₘ*)*ᵗ* plus
    the bias:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/21d462f2bbd5c1df340311876109e3e7.png)'
  prefs: []
  type: TYPE_IMG
- en: Vector form of the net input
  prefs: []
  type: TYPE_NORMAL
- en: And the perceptron fires if and only if the net input is non-negative, i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0ba200fc4137980148b2d9262f728336.png)'
  prefs: []
  type: TYPE_IMG
- en: 'More generally, the perceptron applies an **activation function** *f*(*z*)
    on the net input that generates its output. The two most common activation functions
    used in perceptrons are:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **step function** (also known as the **heaviside function**) is a function
    whose value is 0 for negative inputs and 1 for non-negative inputs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/0864fe1dc11939ff7aea0427aa3ce731.png)'
  prefs: []
  type: TYPE_IMG
- en: The step function
  prefs: []
  type: TYPE_NORMAL
- en: '2\. The **sign function** is a function whose value is -1 for negative inputs
    and 1 for non-negative inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/77bfa4cb419bf74b935720955c8c4563.png)'
  prefs: []
  type: TYPE_IMG
- en: The sign function
  prefs: []
  type: TYPE_NORMAL
- en: Other types of activation functions are used in more complex networks, such
    as [multi-layer perceptrons](https://medium.com/@roiyeho/multi-layer-perceptrons-8d76972afa2b)
    (MLPs). For the rest of this article, I will assume that the perceptron is using
    the step function.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, the computation of the perceptron consists of two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Multiplication of the input values *x*₁, …, *xₘ* by their corresponding weights
    *w*₁, …, *wₘ*, and adding the bias *b*, which gives us the net input of the perceptron
    *z =* **w***ᵗ***x** + *b***.**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Applying an activation function *f*(*z*) on the net input that generates a binary
    output (0/1 or -1/+1).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can write this entire computation in one equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ba8a1cdc2c7301c95a677ad6bf1e1529.png)'
  prefs: []
  type: TYPE_IMG
- en: where *f* is the chosen activation function and *o* is the output of the perceptron.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Logic Gates with Perceptrons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To demonstrate how perceptrons work, let’s try to build perceptrons that compute
    the logical functions AND and OR.
  prefs: []
  type: TYPE_NORMAL
- en: As a reminder, the logical AND function has two binary inputs and returns true
    (1) if both of its inputs are true, otherwise it returns false (0).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/94ce5425ed83d8e3d79eb1e44161ed96.png)'
  prefs: []
  type: TYPE_IMG
- en: The truth table of the AND function
  prefs: []
  type: TYPE_NORMAL
- en: A perceptron that implements the AND function has two binary inputs and a bias.
    We want this perceptron to “fire” only when both of its inputs are “firing”. This
    can achieved, for example, by choosing the same weight for both inputs, e.g.,
    *w*₁ = *w*₂ = 1, and then choosing the bias to be within the range [-2, -1). This
    will make sure that when both neurons are firing, the net input 2 + *b* will be
    non-negative, but when only one of them is firing, the net input 1 + *b* will
    be negative (and when none of them is firing the net input *b* is also negative).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ab290827a4ef76a52203716bf9743d4f.png)'
  prefs: []
  type: TYPE_IMG
- en: A perceptron that computes the logical AND function
  prefs: []
  type: TYPE_NORMAL
- en: 'In a similar fashion, we can build a perceptron that computes the logical OR
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/63edd484905b306da029143caade9c83.png)'
  prefs: []
  type: TYPE_IMG
- en: A perceptron that computes the logical OR function
  prefs: []
  type: TYPE_NORMAL
- en: Verify that you understand how this perceptron works!
  prefs: []
  type: TYPE_NORMAL
- en: 'As an exercise, try to build a perceptron for the NAND function, whose truth
    table is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9e9a4ac7ef6c78105c8408baae08e5ae.png)'
  prefs: []
  type: TYPE_IMG
- en: The truth table of the NAND function
  prefs: []
  type: TYPE_NORMAL
- en: Perceptrons as Linear Classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The perceptron is a type of a **linear classifier**, since it divides the input
    space into two areas separated by the following hyperplane
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bccef6a6d09686cf3a9fc3c06fc0822e.png)'
  prefs: []
  type: TYPE_IMG
- en: The equation of the separating hyperplane
  prefs: []
  type: TYPE_NORMAL
- en: The weight vector **w** is orthogonal to this hyperplane, and thus determines
    its orientation, while the bias *b* defines its distance from the origin.
  prefs: []
  type: TYPE_NORMAL
- en: Every example above the hyperplane (**w***ᵗ***x** + *b*> 0**)** is classified
    by the perceptron as a positive example, while every example below the hyperplane
    (**w***ᵗ***x** + *b*< 0**)** is classified as a negative example.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d62f07c961837097e54a9ed3a8042ebf.png)'
  prefs: []
  type: TYPE_IMG
- en: Perceptron as a linear classifier
  prefs: []
  type: TYPE_NORMAL
- en: Other linear classifiers include logistic regression and linear SVMs (support
    vector machines).
  prefs: []
  type: TYPE_NORMAL
- en: Linear classifiers are capable of learning only **linearly separable** problems,
    i.e., problems where the decision boundary between the positive and the negative
    examples is a linear surface (a hyperplane).
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the following data set is not linearly separable, therefore a
    perceptron cannot classify correctly all the examples in this data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/11a30412fa1736f1844350d2d41065e2.png)'
  prefs: []
  type: TYPE_IMG
- en: Non-linearly separable data set
  prefs: []
  type: TYPE_NORMAL
- en: The Perceptron Learning Rule
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The perceptron has a simple learning rule that is guaranteed to find the separating
    hyperplane if the data is linearly separable.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each training sample (**x***ᵢ*, *yᵢ*) that is misclassified by the perceptron
    (i.e., *oᵢ* ≠ *yᵢ*), we apply the following update rule to the weight vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/070b24e3a53321310326907cd5fba50c.png)'
  prefs: []
  type: TYPE_IMG
- en: The perceptron learning rule
  prefs: []
  type: TYPE_NORMAL
- en: where *α* is a learning rate (0 < *α* ≤ 1) that controls the size of the weight
    adjustment in each update.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, we add to each connection weight *wⱼ* the error of the perceptron
    on this example (the difference between the true label *yᵢ* and the output *oᵢ*)
    multiplied by the value of the corresponding input *xᵢⱼ* and the learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'What this learning rule tries to do is to reduce the discrepancy between the
    perceptron’s output *oᵢ* and the true label *yᵢ*. To understand why it works,
    let’s examine the two possible cases of a misclassification by the perceptron:'
  prefs: []
  type: TYPE_NORMAL
- en: The true label is *yᵢ* = 1, but the perceptron’s prediction is *oᵢ* =0, i.e.,
    **w***ᵗ***x***ᵢ + b* < 0\. In this case, we would like to **increase** the perceptron’s
    net input so eventually it becomes positive.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To that end, we add the quantity (*yᵢ — oᵢ)***x***ᵢ* = **x***ᵢ* to the weight
    vector (multiplied by the learning rate). This increases the weights of the inputs
    with positive values (where *xᵢⱼ* > 0), while decreasing the weights of the inputs
    with negative values (where *xᵢⱼ* < 0). Consequently, the overall net input of
    the perceptron increases.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The true label is *yᵢ* = 0, but the perceptron’s prediction is *oᵢ* = 1, i.e.,
    **w***ᵗ***x***ᵢ + b >* 0\. Analogously to the previous case, here we would like
    to **decrease** the perceptron’s net input, so eventually it becomes negative.This
    is achieved by adding the quantity (*yᵢ — oᵢ)***x***ᵢ* = -**x***ᵢ* to the weight
    vector, since this decreases the weights of the inputs with positive values (where
    *xᵢⱼ* > 0) while increasing the weights of the inputs with negative values (where
    *xᵢⱼ* < 0). Consequently, the overall net input of the perceptron decreases.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This learning rule is applied to all the training samples sequentially (in an
    arbitrary order). It typically requires more than one iteration over the entire
    training set (called an **epoch**) to find the correct weight vector (i.e., the
    vector of a hyperplane that separates between the positive and the negative examples).
  prefs: []
  type: TYPE_NORMAL
- en: According to the **perceptron convergence theorem**, if the data is linearly
    separable, applying the perceptron learning rule repeatedly will eventually converge
    to the weights of the separating hyperplane (in a finite number of steps). The
    interested reader can find a formal proof of this theorem in this [paper](http://www.cs.columbia.edu/~mcollins/courses/6998-2012/notes/perc.converge.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: The Perceptron Learning Algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In practice, it may take long time for the perceptron learning process to converge
    (i.e., reach zero errors on the training set). Furthermore, the data itself may
    not be linearly separable, in which case the algorithm may never terminate. Therefore,
    we need to limit the number of training epochs by some predefined parameter. If
    the perceptron achieves zero errors on the training set before this number is
    reached, we can stop its training earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'The perceptron learning algorithm is summarized in the following pseudocode:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c4d5d412b765f71cdd637fa62ef5106e.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the weights are typically initialized to small random values in order
    to break the symmetry (if all the weights were equal, then the output of the perceptron
    would be constant for every input), while the bias is initialized to zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Learning the Majority Function'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For example, let’s see how the perceptron learning algorithm can be used to
    learn the **majority function** of three binary inputs. The majority function
    is a function that evaluates to true (1) when half or more of its inputs are true
    and to false (0) otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: 'The training set of the perceptron includes all the 8 possible binary inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/54eb3d76c88eca2dde0d65c5107accf4.png)'
  prefs: []
  type: TYPE_IMG
- en: The training set for the majority function
  prefs: []
  type: TYPE_NORMAL
- en: In this example we will assume that the initial weights and bias are 0, and
    the learning rate is *α* = 0.5.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s track the weight updates during the first epoch of training. The first
    sample presented to the perceptron is **x** = (0, 0, 0)*ᵗ*. The net input of the
    perceptron in this case is: *z* = **w***ᵗ***x** + *b* = 0 × 0 + 0 × 0 + 0 × 0
    + 0 = 0\. Therefore, its output is *o* = 1 (remember that the step function outputs
    1 whenever its input is ≥ 0). However, the target label in this case is *y* =
    0, so the error made by the perceptron is *y* - *o* = -1.'
  prefs: []
  type: TYPE_NORMAL
- en: According to the perceptron learning rule, we update each weight *wᵢ* by adding
    to it *α*(*y - o*)*xᵢ =* -0.5*xᵢ*. Since all the inputs are 0 in this case, except
    for the bias neuron (*x*₀ = 1), we only update the bias to be -0.5 instead of
    0.
  prefs: []
  type: TYPE_NORMAL
- en: 'We repeat the same process for all the other 7 training samples. The following
    table shows the weight updates after every sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2a58fd2f787931d2fc1d07daa3ea5a3f.png)'
  prefs: []
  type: TYPE_IMG
- en: The first epoch of training
  prefs: []
  type: TYPE_NORMAL
- en: During the first epoch the perceptron has made 4 errors. The weight vector after
    the first epoch is **w** = (0, 0.5, 1)*ᵗ* and the bias is 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the second training epoch, we get the following weight updates:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d71fd5516712bc06974d56d23775ac08.png)'
  prefs: []
  type: TYPE_IMG
- en: Second epoch of training
  prefs: []
  type: TYPE_NORMAL
- en: This time the perceptron has made only three errors. The weight vector after
    the second epoch is **w** = (0.5, 0.5, 1)*ᵗ* and the bias is -0.5.
  prefs: []
  type: TYPE_NORMAL
- en: 'The weight updates in the third epoch are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c065e9a729618c93fa8e78253d6bfd33.png)'
  prefs: []
  type: TYPE_IMG
- en: Third epoch of training
  prefs: []
  type: TYPE_NORMAL
- en: 'After the update of the second example in this epoch, the perceptron has converged
    to the weight vector that solves this classification problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '**w** = (0.5, 0.5, 0.5)*ᵗ* and *b* = -1\. Since all the weights are equal,
    the perceptron fires only when at least two of the inputs are 1, in which case
    their weighted sum is at least 1, i.e., greater or equal than the absolute value
    of the bias, hence the net input of the perceptron is non-negative.'
  prefs: []
  type: TYPE_NORMAL
- en: Perceptron Implementation in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s now implement the perceptron learning algorithm in Python.
  prefs: []
  type: TYPE_NORMAL
- en: We will implement it as a custom Scikit-Learn estimator by extending the [sklearn.base.BaseEstimator](https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html)
    class. This will allow us to use it as any other estimator in Scikit-Learn (e.g.,
    adding it to a [pipeline](https://medium.com/@roiyeho/pipelines-in-scikit-learn-46c61c5c60b2)).
  prefs: []
  type: TYPE_NORMAL
- en: A custom estimator needs to implement the **fit()** and **predict()** methods,
    and set all its hyperparameters in the **__init__()** method.
  prefs: []
  type: TYPE_NORMAL
- en: I will first show the complete code of this class, and then walk through it
    step-by-step.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The constructor of the class initializes the two hyperparameters of the model:
    the learning rate (*alpha*) and the number of training epochs (*n_epochs*).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The **fit()** method runs the learning algorithm on a given data set *X* with
    labels *y*. We first find out how many samples and features we have in the data
    set by interrogating the shape of *X*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '*n* is the number of training samples and *m* is the number of features.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we initialize the weight vector using the standard normal distribution
    (with mean 0 and standard deviation of 1), and the bias to 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We now run the training loop for *n_epochs* iterations. In each iteration,
    we go over all the training samples, and for each sample we check if the perceptron
    classifies it correctly by calling the **predict()** method and comparing its
    output to the true label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'If the perceptron has misclassified the sample, we apply the weight update
    rule to both the weight vector and the bias, and then increment the number of
    misclassification errors by 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'When the epoch terminates, we report the perceptron’s current accuracy on the
    training set, and if the number of errors was 0, we terminate the training loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The **predict()** method is quite straightforward. We first compute the net
    input of the perceptron as the dot product between the input vector and the weights
    plus the bias:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we use NumPy’s [heaviside()](https://numpy.org/doc/stable/reference/generated/numpy.heaviside.html)
    function to apply the step function to the net input and return the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The second parameter of np.heaviside() specifies what should be the value of
    the function for *z* = 0.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now test our implementation on a data set generated by the [make_blobs()](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html)
    function from Scikit-Learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first generate a data set with 100 random points divided into two groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We set *cluster_std* to 0.5 (instead of the default 1) in order to make sure
    that the data is linearly separable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s plot the data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/2a7752dd9f652ee1d0a99e8df9625416.png)'
  prefs: []
  type: TYPE_IMG
- en: The blobs data set
  prefs: []
  type: TYPE_NORMAL
- en: 'We now instantiate the Perceptron class and fit it to the data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output during training is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The perceptron has converged after three epochs of training.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can plot the decision boundary found by the perceptron and the two class
    areas using the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/8b3019ff338ea46ad126f23e3903a074.png)'
  prefs: []
  type: TYPE_IMG
- en: The separating hyperplane found by the perceptron
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-Learn provides its own [Perceptron](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html)
    class that implements a similar algorithm, but provides more options such as regularization
    and early stopping.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of the Perceptron Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although the perceptron model has shown some initial success, it was quickly
    realized that perceptrons cannot learn some simple functions such as the XOR function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e7015ee88d9d218589d2f6d9fa3d1279.png)'
  prefs: []
  type: TYPE_IMG
- en: The XOR problem cannot be solved by a perceptron
  prefs: []
  type: TYPE_NORMAL
- en: The XOR problem is not linearly separable, therefore linear models such as perceptrons
    cannot solve it.
  prefs: []
  type: TYPE_NORMAL
- en: This revelation has caused the field of neural networks to stagnate for many
    years (a period known as “the AI winter”), until it was realized that stacking
    multiple perceptrons in layers can solve more complex and non-linear problems
    such as the XOR problem.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-layer perceptrons (MLPs) are covered in [this article](https://medium.com/p/8d76972afa2b).
  prefs: []
  type: TYPE_NORMAL
- en: Final Notes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All images unless otherwise noted are by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the code samples of this article on my github: [https://github.com/roiyeho/medium/tree/main/perceptrons](https://github.com/roiyeho/medium/tree/main/perceptrons)'
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading!
  prefs: []
  type: TYPE_NORMAL
