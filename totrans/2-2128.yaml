- en: Topic Modeling with Llama 2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Llama 2进行主题建模
- en: 原文：[https://towardsdatascience.com/topic-modeling-with-llama-2-85177d01e174](https://towardsdatascience.com/topic-modeling-with-llama-2-85177d01e174)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/topic-modeling-with-llama-2-85177d01e174](https://towardsdatascience.com/topic-modeling-with-llama-2-85177d01e174)
- en: '![](../Images/31f5e2196198d26e5295ae70660d720c.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/31f5e2196198d26e5295ae70660d720c.png)'
- en: Create easily interpretable topics with Large Language Models
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用大型语言模型创建易于解释的主题
- en: '[](https://medium.com/@maartengrootendorst?source=post_page-----85177d01e174--------------------------------)[![Maarten
    Grootendorst](../Images/58e24b9cf7e10ff1cd5ffd75a32d1a26.png)](https://medium.com/@maartengrootendorst?source=post_page-----85177d01e174--------------------------------)[](https://towardsdatascience.com/?source=post_page-----85177d01e174--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----85177d01e174--------------------------------)
    [Maarten Grootendorst](https://medium.com/@maartengrootendorst?source=post_page-----85177d01e174--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@maartengrootendorst?source=post_page-----85177d01e174--------------------------------)[![Maarten
    Grootendorst](../Images/58e24b9cf7e10ff1cd5ffd75a32d1a26.png)](https://medium.com/@maartengrootendorst?source=post_page-----85177d01e174--------------------------------)[](https://towardsdatascience.com/?source=post_page-----85177d01e174--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----85177d01e174--------------------------------)
    [Maarten Grootendorst](https://medium.com/@maartengrootendorst?source=post_page-----85177d01e174--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----85177d01e174--------------------------------)
    ·12 min read·Aug 22, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----85177d01e174--------------------------------)
    ·12分钟阅读·2023年8月22日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: With the advent of **Llama 2**, running strong LLMs locally has become more
    and more a reality. Its accuracy approaches OpenAI’s GPT-3.5, which serves well
    for many use cases.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 随着**Llama 2**的出现，本地运行强大的LLM已变得越来越现实。其准确性接近OpenAI的GPT-3.5，适用于许多用例。
- en: In this article, we will explore how we can use Llama2 for Topic Modeling without
    the need to pass every single document to the model. Instead, we are going to
    leverage [**BERTopic**](https://github.com/MaartenGr/BERTopic), a modular topic
    modeling technique that can use any LLM for fine-tuning topic representations.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将探讨如何使用Llama2进行主题建模，而无需将每一个文档传递给模型。相反，我们将利用[**BERTopic**](https://github.com/MaartenGr/BERTopic)，这是一种模块化的主题建模技术，可以使用任何LLM来微调主题表示。
- en: 'BERTopic works rather straightforward. It consists of 5 sequential steps:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: BERTopic的工作原理非常简单。它包括5个顺序步骤：
- en: Embedding documents
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 嵌入文档
- en: Reducing the dimensionality of embeddings
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 降低嵌入的维度
- en: Cluster reduced embeddings
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 聚类减少的嵌入
- en: Tokenize documents per cluster
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按聚类对文档进行分词
- en: Extract best-representing words per cluster
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取每个聚类的最佳代表词
- en: '![](../Images/63a385b329d173ae7fc70aa9fa1b4182.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63a385b329d173ae7fc70aa9fa1b4182.png)'
- en: The 5 main steps of BERTopic.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: BERTopic的5个主要步骤。
- en: However, with the rise of LLMs like **Llama 2**, we can do much better than
    a bunch of independent words per topic. It is computationally not feasible to
    pass all documents to Llama 2 directly and have it analyze them. We can employ
    vector databases for search but we are not entirely sure which topics to search
    for.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着**Llama 2**等LLM的兴起，我们可以做得比每个主题的一堆独立单词更好。直接将所有文档传递给Llama 2并让其分析是不切实际的。我们可以使用向量数据库进行搜索，但我们不完全确定要搜索哪些主题。
- en: Instead, we will leverage the clusters and topics that were created by BERTopic
    and have Llama 2 fine-tune and distill that information into something more accurate.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们将利用由BERTopic创建的聚类和主题，并让Llama 2对这些信息进行微调和提炼，以获得更准确的结果。
- en: This is the best of both worlds, the topic creation of BERTopic together with
    the topic representation of Llama 2.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这是两全其美的最佳方案，即BERTopic的主题创建和Llama 2的主题表示。
- en: '![](../Images/66743d20879491fe94a4075743d520b5.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/66743d20879491fe94a4075743d520b5.png)'
- en: Llama 2 lets us fine-tune the topic representations generated by BERTopic.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 2使我们能够微调BERTopic生成的主题表示。
- en: Now that this intro is out of the way, let’s start the hands-on tutorial!
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在介绍部分已经完成，我们开始实际操作教程吧！
- en: 'We will start by installing a number of packages that we are going to use throughout
    this example:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开始安装我们将在整个示例中使用的几个包：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Keep in mind that you will need at least a T4 GPU in order to run this example,
    which can be used with a free Google Colab instance.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您至少需要一块T4 GPU才能运行这个示例，它可以与免费的Google Colab实例一起使用。
- en: '🔥 **TIP**: You can also follow along with the [Google Colab Notebook](https://colab.research.google.com/drive/1QCERSMUjqGetGGujdrvv_6_EeoIcd_9M?usp=sharing).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 🔥 **提示**：你也可以跟随 [Google Colab Notebook](https://colab.research.google.com/drive/1QCERSMUjqGetGGujdrvv_6_EeoIcd_9M?usp=sharing)
    一起操作。
- en: '[📄](https://emojipedia.org/page-facing-up) Data'
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[📄](https://emojipedia.org/page-facing-up) 数据'
- en: We are going to apply topic modeling on a number of ArXiv abstracts. They are
    a great source for topic modeling since they contain a wide variety of topics
    and are generally well-written.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对大量 ArXiv 摘要应用主题建模。它们是进行主题建模的绝佳来源，因为它们包含各种各样的主题，并且通常写得很好。
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To give you an idea, an abstract looks like the following:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给你一个概念，下面是一个抽象的样子：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 🤗 HuggingFace Hub Credentials
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🤗 HuggingFace Hub 凭证
- en: 'Before we can load in Llama2 using a number of tricks, we will first need to
    accept the License for using Llama2\. The steps are as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们可以使用一些技巧加载 Llama2 之前，我们首先需要接受 Llama2 的许可协议。步骤如下：
- en: Create a HuggingFace account [here](https://huggingface.co/)
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 [这里](https://huggingface.co/) 创建一个 HuggingFace 账户
- en: Apply for Llama 2 access [here](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf)
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 申请 Llama 2 访问权限 [这里](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf)
- en: Get your HuggingFace token [here](https://huggingface.co/settings/tokens)
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 [这里](https://huggingface.co/settings/tokens) 获取你的 HuggingFace 令牌
- en: After doing so, we can log in with our HuggingFace credentials so that this
    environment knows we have permission to download the Llama 2 model that we are
    interested in.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些步骤后，我们可以使用我们的 HuggingFace 凭证登录，以便这个环境知道我们有权限下载我们感兴趣的 Llama 2 模型。
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/475af2918fd29d5277c2630d8b6ea2ef.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/475af2918fd29d5277c2630d8b6ea2ef.png)'
- en: 🦙 Llama 2
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🦙 Llama 2
- en: Now comes one of the more interesting components of this tutorial, how to load
    in a Llama 2 model on a T4-GPU!
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在进入这个教程中一个更有趣的部分——如何在 T4-GPU 上加载 Llama 2 模型！
- en: We will be focusing on the `'meta-llama/Llama-2-13b-chat-hf'` variant. It is
    large enough to give interesting and useful results whilst small enough that it
    can be run on our environment.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重点关注 `'meta-llama/Llama-2-13b-chat-hf'` 变体。它足够大，能够提供有趣且有用的结果，同时又足够小，可以在我们的环境中运行。
- en: 'We start by defining our model and identifying if our GPU is correctly selected.
    We expect the output of `device` to show a Cuda device:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义我们的模型并确认 GPU 是否正确选择。我们期望 `device` 的输出显示为 Cuda 设备：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Optimization & Quantization
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化与量化
- en: In order to load our 13 billion parameter model, we will need to perform some
    optimization tricks. Since we have limited VRAM and not an A100 GPU, we will need
    to “condense” the model a bit so that we can run it.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加载我们 130 亿参数的模型，我们需要执行一些优化技巧。由于我们拥有的 VRAM 有限且没有 A100 GPU，我们需要对模型进行一些“压缩”，以便我们可以运行它。
- en: There are a number of tricks that we can use but the main principle is going
    to be 4-bit quantization.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用一些技巧，但主要原则是 4 位量化。
- en: This process reduces the 64-bit representation to only 4-bits which reduces
    the GPU memory that we will need. It is a recent technique and quite elegant at
    that for efficient LLM loading and usage. You can find more about that method
    [here](https://arxiv.org/pdf/2305.14314.pdf) in the QLoRA paper and on the amazing
    HuggingFace blog [here](https://huggingface.co/blog/4bit-transformers-bitsandbytes).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程将 64 位表示减少到仅 4 位，从而减少我们需要的 GPU 内存。这是一种最近的技术，且在高效 LLM 加载和使用方面相当优雅。你可以在 QLoRA
    论文 [这里](https://arxiv.org/pdf/2305.14314.pdf) 和令人惊叹的 HuggingFace 博客 [这里](https://huggingface.co/blog/4bit-transformers-bitsandbytes)
    中了解更多关于该方法的内容。
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'These four parameters that we just run are incredibly important and bring many
    LLM applications to consumers:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚运行的这四个参数非常重要，并将许多 LLM 应用带给消费者：
- en: '`**load_in_4bit**`Allows us to load the model in 4-bit precision compared to
    the original 32-bit precision. This gives us an incredible speed up and reduces
    memory!'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**load_in_4bit**` 允许我们以 4 位精度加载模型，相比于原始的 32 位精度，这大大加快了速度并减少了内存！'
- en: '`**bnb_4bit_quant_type**`This is the type of 4-bit precision. The paper recommends
    normalized float 4-bit, so that is what we are going to use!'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**bnb_4bit_quant_type**` 这是 4 位精度的类型。论文推荐使用标准化的浮点 4 位精度，所以这就是我们要使用的！'
- en: '`**bnb_4bit_use_double_quant**`This is a neat trick as it performs a second
    quantization after the first which further reduces the necessary bits'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**bnb_4bit_use_double_quant**` 这是一个巧妙的技巧，因为它在第一次量化后执行第二次量化，进一步减少所需的位数。'
- en: '`**bnb_4bit_compute_dtype**`The compute type used during computation, which
    further speeds up the model.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**bnb_4bit_compute_dtype**` 计算过程中使用的计算类型，这进一步加速了模型。'
- en: 'Using this configuration, we can start loading in the model as well as the
    tokenizer:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个配置，我们可以开始加载模型以及分词器：
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Using the model and tokenizer, we will generate a HuggingFace transformers
    pipeline that allows us to easily generate new text:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用模型和分词器，我们将生成一个HuggingFace transformers管道，这样我们就可以轻松生成新文本：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Prompt Engineering
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示工程
- en: To check whether our model is correctly loaded, let’s try it out with a few
    prompts.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查我们的模型是否正确加载，让我们尝试几个提示。
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Although we can directly prompt the model, there is actually a template that
    we need to follow. The template looks as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们可以直接提示模型，但实际上我们需要遵循一个模板。模板如下所示：
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This template consists of two main components, namely the `{{ System Prompt
    }}` and the `{{ User Prompt }}`:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模板由两个主要组件组成，即`{{ System Prompt }}`和`{{ User Prompt }}`：
- en: The `{{ System Prompt }}` helps us guide the model during a conversation. For
    example, we can say that it is a helpful assistant that is specialized in labeling
    topics.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`{{ System Prompt }}`帮助我们在对话过程中指导模型。例如，我们可以说它是一个专门负责标记主题的有用助手。'
- en: The `{{ User Prompt }}` is where we ask it a question.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`{{ User Prompt }}`是我们向它提问的地方。'
- en: You might have noticed the `[INST]` tags, which are used to identify the beginning
    and end of a prompt. We can use these to model the conversation history as we
    will see more in-depth later on.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能注意到`[INST]`标签，这些标签用于标识提示的开始和结束。我们可以使用这些标签来建模对话历史，稍后我们会更深入地了解。
- en: Next, let’s see how we can use this template to optimize Llama 2 for topic modeling.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何使用这个模板来优化Llama 2进行主题建模。
- en: Prompt Template
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示模板
- en: 'We are going to keep our `system prompt` simple and to the point:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将保持`system prompt`简单明了：
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We will tell the model that it is simply a helpful assistant for labeling topics
    since that is our main goal.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将告诉模型，它只是一个有用的助手，负责标记主题，因为这是我们的主要目标。
- en: In contrast, our `user prompt` is going to be a bit more involved. It will consist
    of two components, an **example** **prompt** and the **main prompt**.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，我们的`user prompt`会稍微复杂一些。它将包含两个部分，一个**示例** **提示**和**主要提示**。
- en: Let’s start with the **example prompt**. Most LLMs do a much better job of generating
    accurate responses if you give them an example to work with. We will show it an
    accurate example of the kind of output we are expecting.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从**示例提示**开始。如果你给大多数LLM一个示例，它们通常能更好地生成准确的响应。我们将展示一个准确的示例，说明我们期望的输出类型。
- en: '[PRE11]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This example, based on a number of keywords and documents primarily about the
    impact of meat, helps the model understand the kind of output it should give.
    We show the model that we were expecting only the label, which is easier for us
    to extract.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子基于一系列关键字和主要关于肉类影响的文档，帮助模型理解它应该给出什么样的输出。我们向模型展示了我们只期待标签，这样我们更容易提取。
- en: 'Next, we will create a template that we can use within BERTopic:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个可以在BERTopic中使用的模板：
- en: '[PRE12]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'There are two BERTopic-specific tags that are of interest, namely `[DOCUMENTS]`
    and `[KEYWORDS]`:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个BERTopic特定的标签值得关注，即`[DOCUMENTS]`和`[KEYWORDS]`：
- en: '`[DOCUMENTS]` contain the top 5 most relevant documents to the topic'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[DOCUMENTS]`包含与主题最相关的前5个文档'
- en: '`[KEYWORDS]` contain the top 10 most relevant keywords to the topic as generated
    through c-TF-IDF'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[KEYWORDS]`包含通过c-TF-IDF生成的与主题最相关的前10个关键字'
- en: 'This template will be filled according to each topic. And finally, we can combine
    this into our final prompt:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模板将根据每个主题进行填写。最后，我们可以将其合并为我们的最终提示：
- en: '[PRE13]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 🗨️ BERTopic
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🗨️ BERTopic
- en: 'Before we can start with topic modeling, we will first need to perform two
    steps:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始主题建模之前，我们需要先执行两个步骤：
- en: Pre-calculating Embeddings
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预计算嵌入
- en: Defining Sub-models
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义子模型
- en: Preparing Embeddings
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备嵌入
- en: By pre-calculating the embeddings for each document, we can speed up additional
    exploration steps and use the embeddings to quickly iterate over BERTopic’s hyperparameters
    if needed.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 通过为每个文档预计算嵌入，我们可以加速额外的探索步骤，并在需要时使用嵌入快速迭代BERTopic的超参数。
- en: '🔥 **TIP**: You can find a great overview of good embeddings for clustering
    on the [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard).'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 🔥 **提示**：你可以在[MTEB排行榜](https://huggingface.co/spaces/mteb/leaderboard)上找到有关聚类的优秀嵌入概述。
- en: '[PRE14]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Sub-models
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 子模型
- en: Next, we will define all sub-models in BERTopic and do some small tweaks to
    the number of clusters to be created, setting random states, etc.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义BERTopic中的所有子模型，并对要创建的聚类数量进行一些小的调整，如设置随机状态等。
- en: '[PRE15]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As a small bonus, we are going to reduce the embeddings we created before to
    2-dimensions so that we can use them for visualization purposes when we have created
    our topics.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个小额外内容，我们将把之前创建的嵌入减少到2维，以便在创建主题时用于可视化。
- en: '[PRE16]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Representation Models
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 表示模型
- en: One of the ways we are going to represent the topics is with Llama 2 which should
    give us a nice label. However, we might want to have additional representations
    to view a topic from multiple angles.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用Llama 2来表示这些主题，这样可以获得一个不错的标签。不过，我们可能还希望有额外的表示，以从多个角度查看主题。
- en: Here, we will be using c-TF-IDF as our main representation and [KeyBERT](https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#keybertinspired),
    [MMR](https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#maximalmarginalrelevance),
    and [Llama 2](https://maartengr.github.io/BERTopic/getting_started/representation/llm.html)
    as our additional representations.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用c-TF-IDF作为主要表示，并将[KeyBERT](https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#keybertinspired)、[MMR](https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#maximalmarginalrelevance)和[Llama
    2](https://maartengr.github.io/BERTopic/getting_started/representation/llm.html)作为附加表示。
- en: '[PRE17]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 🔥 Training
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🔥 训练
- en: Now that we have our models prepared, we can start training our topic model!
    We supply BERTopic with the sub-models of interest, run `.fit_transform`, and
    see what kind of topics we get.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的模型已经准备好，我们可以开始训练我们的主题模型！我们将感兴趣的子模型提供给BERTopic，运行`.fit_transform`，看看我们得到什么样的主题。
- en: '[PRE18]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now that we are done training our model, let’s see what topics were generated:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了模型训练，来看一下生成了什么主题：
- en: '[PRE19]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](../Images/d9a0326290f7988d127e851f3d452d37.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d9a0326290f7988d127e851f3d452d37.png)'
- en: '[PRE20]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](../Images/a34443869e8befd4e6af0d8622f045df.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a34443869e8befd4e6af0d8622f045df.png)'
- en: We got over 100 topics that were created and they all seem quite diverse. We
    can use the labels by Llama 2 and assign them to topics that we have created.
    Normally, the default topic representation would be c-TF-IDF, but we will focus
    on Llama 2 representations instead.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了超过100个主题，它们似乎非常多样化。我们可以使用Llama 2提供的标签，并将其分配给我们创建的主题。通常，默认的主题表示是c-TF-IDF，但我们将重点关注Llama
    2的表示。
- en: '[PRE21]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[📊](https://emojigraph.org/bar-chart/) Visualize'
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[📊](https://emojigraph.org/bar-chart/) 可视化'
- en: We can go through each topic manually, which would take a lot of work, or we
    can visualize them all in a single interactive graph.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以手动浏览每个主题，这会需要大量工作，或者我们可以在一个互动图表中可视化它们。
- en: BERTopic has a bunch of [visualization functions](https://maartengr.github.io/BERTopic/getting_started/visualization/visualize_documents.html)
    that we can use. For now, we are sticking with visualizing the documents.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: BERTopic有一系列[可视化函数](https://maartengr.github.io/BERTopic/getting_started/visualization/visualize_documents.html)供我们使用。目前，我们将继续可视化文档。
- en: '[PRE22]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](../Images/03b0559c55725b27e2ef199d98605b6e.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03b0559c55725b27e2ef199d98605b6e.png)'
- en: '[🖼️](https://emojipedia.org/framed-picture/) (BONUS): Advanced Visualization'
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[🖼️](https://emojipedia.org/framed-picture/)（额外内容）：高级可视化'
- en: Although we can use the built-in visualization features of BERTopic, we can
    also create a static visualization that might be a bit more informative.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们可以使用BERTopic的内置可视化功能，但我们也可以创建一个静态可视化，可能会提供更多的信息。
- en: 'We start by creating the necessary variables that contain our reduced embeddings
    and representations:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建必要的变量，这些变量包含我们减少的嵌入和表示：
- en: '[PRE23]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next, we will visualize the reduced embeddings with Matplotlib and process
    the labels in such a way that it is visually more pleasing:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用Matplotlib可视化减少后的嵌入，并以视觉上更令人愉悦的方式处理标签：
- en: '[PRE24]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![](../Images/8fb9e2c99f87f38e3d5f5a824b2a0d65.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8fb9e2c99f87f38e3d5f5a824b2a0d65.png)'
- en: '**Update**: I uploaded a video version to YouTube that goes more in-depth into
    how to use BERTopic with Llama 2:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**更新**：我上传了一个视频版本到YouTube，详细介绍了如何使用BERTopic与Llama 2：'
- en: Thank you for reading!
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感谢你的阅读！
- en: If you are, like me, passionate about AI and/or Psychology, please feel free
    to add me on [**LinkedIn**](https://www.linkedin.com/in/mgrootendorst/), follow
    me on [**Twitter**](https://twitter.com/MaartenGr), or subscribe to my [**Newsletter**](http://maartengrootendorst.substack.com/).
    You can also find some of my content on my [**Personal Website**](https://maartengrootendorst.com/).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你像我一样，对AI和/或心理学充满热情，请随时在[**LinkedIn**](https://www.linkedin.com/in/mgrootendorst/)上加我为好友，在[**Twitter**](https://twitter.com/MaartenGr)上关注我，或订阅我的[**Newsletter**](http://maartengrootendorst.substack.com/)。你也可以在我的[**个人网站**](https://maartengrootendorst.com/)上找到一些我的内容。
- en: '*All images without a source credit were created by the author*'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '*所有没有来源说明的图像均由作者创作*'
