- en: Topic Modeling with Llama 2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Llama 2è¿›è¡Œä¸»é¢˜å»ºæ¨¡
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/topic-modeling-with-llama-2-85177d01e174](https://towardsdatascience.com/topic-modeling-with-llama-2-85177d01e174)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/topic-modeling-with-llama-2-85177d01e174](https://towardsdatascience.com/topic-modeling-with-llama-2-85177d01e174)
- en: '![](../Images/31f5e2196198d26e5295ae70660d720c.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/31f5e2196198d26e5295ae70660d720c.png)'
- en: Create easily interpretable topics with Large Language Models
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹åˆ›å»ºæ˜“äºè§£é‡Šçš„ä¸»é¢˜
- en: '[](https://medium.com/@maartengrootendorst?source=post_page-----85177d01e174--------------------------------)[![Maarten
    Grootendorst](../Images/58e24b9cf7e10ff1cd5ffd75a32d1a26.png)](https://medium.com/@maartengrootendorst?source=post_page-----85177d01e174--------------------------------)[](https://towardsdatascience.com/?source=post_page-----85177d01e174--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----85177d01e174--------------------------------)
    [Maarten Grootendorst](https://medium.com/@maartengrootendorst?source=post_page-----85177d01e174--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@maartengrootendorst?source=post_page-----85177d01e174--------------------------------)[![Maarten
    Grootendorst](../Images/58e24b9cf7e10ff1cd5ffd75a32d1a26.png)](https://medium.com/@maartengrootendorst?source=post_page-----85177d01e174--------------------------------)[](https://towardsdatascience.com/?source=post_page-----85177d01e174--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----85177d01e174--------------------------------)
    [Maarten Grootendorst](https://medium.com/@maartengrootendorst?source=post_page-----85177d01e174--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----85177d01e174--------------------------------)
    Â·12 min readÂ·Aug 22, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page-----85177d01e174--------------------------------)
    Â·12åˆ†é’Ÿé˜…è¯»Â·2023å¹´8æœˆ22æ—¥
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: With the advent of **Llama 2**, running strong LLMs locally has become more
    and more a reality. Its accuracy approaches OpenAIâ€™s GPT-3.5, which serves well
    for many use cases.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€**Llama 2**çš„å‡ºç°ï¼Œæœ¬åœ°è¿è¡Œå¼ºå¤§çš„LLMå·²å˜å¾—è¶Šæ¥è¶Šç°å®ã€‚å…¶å‡†ç¡®æ€§æ¥è¿‘OpenAIçš„GPT-3.5ï¼Œé€‚ç”¨äºè®¸å¤šç”¨ä¾‹ã€‚
- en: In this article, we will explore how we can use Llama2 for Topic Modeling without
    the need to pass every single document to the model. Instead, we are going to
    leverage [**BERTopic**](https://github.com/MaartenGr/BERTopic), a modular topic
    modeling technique that can use any LLM for fine-tuning topic representations.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨å¦‚ä½•ä½¿ç”¨Llama2è¿›è¡Œä¸»é¢˜å»ºæ¨¡ï¼Œè€Œæ— éœ€å°†æ¯ä¸€ä¸ªæ–‡æ¡£ä¼ é€’ç»™æ¨¡å‹ã€‚ç›¸åï¼Œæˆ‘ä»¬å°†åˆ©ç”¨[**BERTopic**](https://github.com/MaartenGr/BERTopic)ï¼Œè¿™æ˜¯ä¸€ç§æ¨¡å—åŒ–çš„ä¸»é¢˜å»ºæ¨¡æŠ€æœ¯ï¼Œå¯ä»¥ä½¿ç”¨ä»»ä½•LLMæ¥å¾®è°ƒä¸»é¢˜è¡¨ç¤ºã€‚
- en: 'BERTopic works rather straightforward. It consists of 5 sequential steps:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: BERTopicçš„å·¥ä½œåŸç†éå¸¸ç®€å•ã€‚å®ƒåŒ…æ‹¬5ä¸ªé¡ºåºæ­¥éª¤ï¼š
- en: Embedding documents
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: åµŒå…¥æ–‡æ¡£
- en: Reducing the dimensionality of embeddings
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é™ä½åµŒå…¥çš„ç»´åº¦
- en: Cluster reduced embeddings
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: èšç±»å‡å°‘çš„åµŒå…¥
- en: Tokenize documents per cluster
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æŒ‰èšç±»å¯¹æ–‡æ¡£è¿›è¡Œåˆ†è¯
- en: Extract best-representing words per cluster
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æå–æ¯ä¸ªèšç±»çš„æœ€ä½³ä»£è¡¨è¯
- en: '![](../Images/63a385b329d173ae7fc70aa9fa1b4182.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63a385b329d173ae7fc70aa9fa1b4182.png)'
- en: The 5 main steps of BERTopic.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: BERTopicçš„5ä¸ªä¸»è¦æ­¥éª¤ã€‚
- en: However, with the rise of LLMs like **Llama 2**, we can do much better than
    a bunch of independent words per topic. It is computationally not feasible to
    pass all documents to Llama 2 directly and have it analyze them. We can employ
    vector databases for search but we are not entirely sure which topics to search
    for.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œéšç€**Llama 2**ç­‰LLMçš„å…´èµ·ï¼Œæˆ‘ä»¬å¯ä»¥åšå¾—æ¯”æ¯ä¸ªä¸»é¢˜çš„ä¸€å †ç‹¬ç«‹å•è¯æ›´å¥½ã€‚ç›´æ¥å°†æ‰€æœ‰æ–‡æ¡£ä¼ é€’ç»™Llama 2å¹¶è®©å…¶åˆ†ææ˜¯ä¸åˆ‡å®é™…çš„ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å‘é‡æ•°æ®åº“è¿›è¡Œæœç´¢ï¼Œä½†æˆ‘ä»¬ä¸å®Œå…¨ç¡®å®šè¦æœç´¢å“ªäº›ä¸»é¢˜ã€‚
- en: Instead, we will leverage the clusters and topics that were created by BERTopic
    and have Llama 2 fine-tune and distill that information into something more accurate.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸åï¼Œæˆ‘ä»¬å°†åˆ©ç”¨ç”±BERTopicåˆ›å»ºçš„èšç±»å’Œä¸»é¢˜ï¼Œå¹¶è®©Llama 2å¯¹è¿™äº›ä¿¡æ¯è¿›è¡Œå¾®è°ƒå’Œæç‚¼ï¼Œä»¥è·å¾—æ›´å‡†ç¡®çš„ç»“æœã€‚
- en: This is the best of both worlds, the topic creation of BERTopic together with
    the topic representation of Llama 2.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸¤å…¨å…¶ç¾çš„æœ€ä½³æ–¹æ¡ˆï¼Œå³BERTopicçš„ä¸»é¢˜åˆ›å»ºå’ŒLlama 2çš„ä¸»é¢˜è¡¨ç¤ºã€‚
- en: '![](../Images/66743d20879491fe94a4075743d520b5.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/66743d20879491fe94a4075743d520b5.png)'
- en: Llama 2 lets us fine-tune the topic representations generated by BERTopic.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 2ä½¿æˆ‘ä»¬èƒ½å¤Ÿå¾®è°ƒBERTopicç”Ÿæˆçš„ä¸»é¢˜è¡¨ç¤ºã€‚
- en: Now that this intro is out of the way, letâ€™s start the hands-on tutorial!
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä»‹ç»éƒ¨åˆ†å·²ç»å®Œæˆï¼Œæˆ‘ä»¬å¼€å§‹å®é™…æ“ä½œæ•™ç¨‹å§ï¼
- en: 'We will start by installing a number of packages that we are going to use throughout
    this example:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å¼€å§‹å®‰è£…æˆ‘ä»¬å°†åœ¨æ•´ä¸ªç¤ºä¾‹ä¸­ä½¿ç”¨çš„å‡ ä¸ªåŒ…ï¼š
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Keep in mind that you will need at least a T4 GPU in order to run this example,
    which can be used with a free Google Colab instance.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œæ‚¨è‡³å°‘éœ€è¦ä¸€å—T4 GPUæ‰èƒ½è¿è¡Œè¿™ä¸ªç¤ºä¾‹ï¼Œå®ƒå¯ä»¥ä¸å…è´¹çš„Google Colabå®ä¾‹ä¸€èµ·ä½¿ç”¨ã€‚
- en: 'ğŸ”¥ **TIP**: You can also follow along with the [Google Colab Notebook](https://colab.research.google.com/drive/1QCERSMUjqGetGGujdrvv_6_EeoIcd_9M?usp=sharing).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”¥ **æç¤º**ï¼šä½ ä¹Ÿå¯ä»¥è·Ÿéš [Google Colab Notebook](https://colab.research.google.com/drive/1QCERSMUjqGetGGujdrvv_6_EeoIcd_9M?usp=sharing)
    ä¸€èµ·æ“ä½œã€‚
- en: '[ğŸ“„](https://emojipedia.org/page-facing-up) Data'
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[ğŸ“„](https://emojipedia.org/page-facing-up) æ•°æ®'
- en: We are going to apply topic modeling on a number of ArXiv abstracts. They are
    a great source for topic modeling since they contain a wide variety of topics
    and are generally well-written.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å¯¹å¤§é‡ ArXiv æ‘˜è¦åº”ç”¨ä¸»é¢˜å»ºæ¨¡ã€‚å®ƒä»¬æ˜¯è¿›è¡Œä¸»é¢˜å»ºæ¨¡çš„ç»ä½³æ¥æºï¼Œå› ä¸ºå®ƒä»¬åŒ…å«å„ç§å„æ ·çš„ä¸»é¢˜ï¼Œå¹¶ä¸”é€šå¸¸å†™å¾—å¾ˆå¥½ã€‚
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To give you an idea, an abstract looks like the following:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç»™ä½ ä¸€ä¸ªæ¦‚å¿µï¼Œä¸‹é¢æ˜¯ä¸€ä¸ªæŠ½è±¡çš„æ ·å­ï¼š
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ğŸ¤— HuggingFace Hub Credentials
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ¤— HuggingFace Hub å‡­è¯
- en: 'Before we can load in Llama2 using a number of tricks, we will first need to
    accept the License for using Llama2\. The steps are as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€äº›æŠ€å·§åŠ è½½ Llama2 ä¹‹å‰ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦æ¥å— Llama2 çš„è®¸å¯åè®®ã€‚æ­¥éª¤å¦‚ä¸‹ï¼š
- en: Create a HuggingFace account [here](https://huggingface.co/)
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ [è¿™é‡Œ](https://huggingface.co/) åˆ›å»ºä¸€ä¸ª HuggingFace è´¦æˆ·
- en: Apply for Llama 2 access [here](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf)
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”³è¯· Llama 2 è®¿é—®æƒé™ [è¿™é‡Œ](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf)
- en: Get your HuggingFace token [here](https://huggingface.co/settings/tokens)
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ [è¿™é‡Œ](https://huggingface.co/settings/tokens) è·å–ä½ çš„ HuggingFace ä»¤ç‰Œ
- en: After doing so, we can log in with our HuggingFace credentials so that this
    environment knows we have permission to download the Llama 2 model that we are
    interested in.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæˆè¿™äº›æ­¥éª¤åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æˆ‘ä»¬çš„ HuggingFace å‡­è¯ç™»å½•ï¼Œä»¥ä¾¿è¿™ä¸ªç¯å¢ƒçŸ¥é“æˆ‘ä»¬æœ‰æƒé™ä¸‹è½½æˆ‘ä»¬æ„Ÿå…´è¶£çš„ Llama 2 æ¨¡å‹ã€‚
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/475af2918fd29d5277c2630d8b6ea2ef.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/475af2918fd29d5277c2630d8b6ea2ef.png)'
- en: ğŸ¦™ Llama 2
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ¦™ Llama 2
- en: Now comes one of the more interesting components of this tutorial, how to load
    in a Llama 2 model on a T4-GPU!
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è¿›å…¥è¿™ä¸ªæ•™ç¨‹ä¸­ä¸€ä¸ªæ›´æœ‰è¶£çš„éƒ¨åˆ†â€”â€”å¦‚ä½•åœ¨ T4-GPU ä¸ŠåŠ è½½ Llama 2 æ¨¡å‹ï¼
- en: We will be focusing on the `'meta-llama/Llama-2-13b-chat-hf'` variant. It is
    large enough to give interesting and useful results whilst small enough that it
    can be run on our environment.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†é‡ç‚¹å…³æ³¨ `'meta-llama/Llama-2-13b-chat-hf'` å˜ä½“ã€‚å®ƒè¶³å¤Ÿå¤§ï¼Œèƒ½å¤Ÿæä¾›æœ‰è¶£ä¸”æœ‰ç”¨çš„ç»“æœï¼ŒåŒæ—¶åˆè¶³å¤Ÿå°ï¼Œå¯ä»¥åœ¨æˆ‘ä»¬çš„ç¯å¢ƒä¸­è¿è¡Œã€‚
- en: 'We start by defining our model and identifying if our GPU is correctly selected.
    We expect the output of `device` to show a Cuda device:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆå®šä¹‰æˆ‘ä»¬çš„æ¨¡å‹å¹¶ç¡®è®¤ GPU æ˜¯å¦æ­£ç¡®é€‰æ‹©ã€‚æˆ‘ä»¬æœŸæœ› `device` çš„è¾“å‡ºæ˜¾ç¤ºä¸º Cuda è®¾å¤‡ï¼š
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Optimization & Quantization
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–ä¸é‡åŒ–
- en: In order to load our 13 billion parameter model, we will need to perform some
    optimization tricks. Since we have limited VRAM and not an A100 GPU, we will need
    to â€œcondenseâ€ the model a bit so that we can run it.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†åŠ è½½æˆ‘ä»¬ 130 äº¿å‚æ•°çš„æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦æ‰§è¡Œä¸€äº›ä¼˜åŒ–æŠ€å·§ã€‚ç”±äºæˆ‘ä»¬æ‹¥æœ‰çš„ VRAM æœ‰é™ä¸”æ²¡æœ‰ A100 GPUï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ¨¡å‹è¿›è¡Œä¸€äº›â€œå‹ç¼©â€ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥è¿è¡Œå®ƒã€‚
- en: There are a number of tricks that we can use but the main principle is going
    to be 4-bit quantization.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€äº›æŠ€å·§ï¼Œä½†ä¸»è¦åŸåˆ™æ˜¯ 4 ä½é‡åŒ–ã€‚
- en: This process reduces the 64-bit representation to only 4-bits which reduces
    the GPU memory that we will need. It is a recent technique and quite elegant at
    that for efficient LLM loading and usage. You can find more about that method
    [here](https://arxiv.org/pdf/2305.14314.pdf) in the QLoRA paper and on the amazing
    HuggingFace blog [here](https://huggingface.co/blog/4bit-transformers-bitsandbytes).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè¿‡ç¨‹å°† 64 ä½è¡¨ç¤ºå‡å°‘åˆ°ä»… 4 ä½ï¼Œä»è€Œå‡å°‘æˆ‘ä»¬éœ€è¦çš„ GPU å†…å­˜ã€‚è¿™æ˜¯ä¸€ç§æœ€è¿‘çš„æŠ€æœ¯ï¼Œä¸”åœ¨é«˜æ•ˆ LLM åŠ è½½å’Œä½¿ç”¨æ–¹é¢ç›¸å½“ä¼˜é›…ã€‚ä½ å¯ä»¥åœ¨ QLoRA
    è®ºæ–‡ [è¿™é‡Œ](https://arxiv.org/pdf/2305.14314.pdf) å’Œä»¤äººæƒŠå¹çš„ HuggingFace åšå®¢ [è¿™é‡Œ](https://huggingface.co/blog/4bit-transformers-bitsandbytes)
    ä¸­äº†è§£æ›´å¤šå…³äºè¯¥æ–¹æ³•çš„å†…å®¹ã€‚
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'These four parameters that we just run are incredibly important and bring many
    LLM applications to consumers:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åˆšåˆšè¿è¡Œçš„è¿™å››ä¸ªå‚æ•°éå¸¸é‡è¦ï¼Œå¹¶å°†è®¸å¤š LLM åº”ç”¨å¸¦ç»™æ¶ˆè´¹è€…ï¼š
- en: '`**load_in_4bit**`Allows us to load the model in 4-bit precision compared to
    the original 32-bit precision. This gives us an incredible speed up and reduces
    memory!'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**load_in_4bit**` å…è®¸æˆ‘ä»¬ä»¥ 4 ä½ç²¾åº¦åŠ è½½æ¨¡å‹ï¼Œç›¸æ¯”äºåŸå§‹çš„ 32 ä½ç²¾åº¦ï¼Œè¿™å¤§å¤§åŠ å¿«äº†é€Ÿåº¦å¹¶å‡å°‘äº†å†…å­˜ï¼'
- en: '`**bnb_4bit_quant_type**`This is the type of 4-bit precision. The paper recommends
    normalized float 4-bit, so that is what we are going to use!'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**bnb_4bit_quant_type**` è¿™æ˜¯ 4 ä½ç²¾åº¦çš„ç±»å‹ã€‚è®ºæ–‡æ¨èä½¿ç”¨æ ‡å‡†åŒ–çš„æµ®ç‚¹ 4 ä½ç²¾åº¦ï¼Œæ‰€ä»¥è¿™å°±æ˜¯æˆ‘ä»¬è¦ä½¿ç”¨çš„ï¼'
- en: '`**bnb_4bit_use_double_quant**`This is a neat trick as it performs a second
    quantization after the first which further reduces the necessary bits'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**bnb_4bit_use_double_quant**` è¿™æ˜¯ä¸€ä¸ªå·§å¦™çš„æŠ€å·§ï¼Œå› ä¸ºå®ƒåœ¨ç¬¬ä¸€æ¬¡é‡åŒ–åæ‰§è¡Œç¬¬äºŒæ¬¡é‡åŒ–ï¼Œè¿›ä¸€æ­¥å‡å°‘æ‰€éœ€çš„ä½æ•°ã€‚'
- en: '`**bnb_4bit_compute_dtype**`The compute type used during computation, which
    further speeds up the model.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**bnb_4bit_compute_dtype**` è®¡ç®—è¿‡ç¨‹ä¸­ä½¿ç”¨çš„è®¡ç®—ç±»å‹ï¼Œè¿™è¿›ä¸€æ­¥åŠ é€Ÿäº†æ¨¡å‹ã€‚'
- en: 'Using this configuration, we can start loading in the model as well as the
    tokenizer:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¿™ä¸ªé…ç½®ï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹åŠ è½½æ¨¡å‹ä»¥åŠåˆ†è¯å™¨ï¼š
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Using the model and tokenizer, we will generate a HuggingFace transformers
    pipeline that allows us to easily generate new text:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œæˆ‘ä»¬å°†ç”Ÿæˆä¸€ä¸ªHuggingFace transformersç®¡é“ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥è½»æ¾ç”Ÿæˆæ–°æ–‡æœ¬ï¼š
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Prompt Engineering
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æç¤ºå·¥ç¨‹
- en: To check whether our model is correctly loaded, letâ€™s try it out with a few
    prompts.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ£€æŸ¥æˆ‘ä»¬çš„æ¨¡å‹æ˜¯å¦æ­£ç¡®åŠ è½½ï¼Œè®©æˆ‘ä»¬å°è¯•å‡ ä¸ªæç¤ºã€‚
- en: '[PRE8]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Although we can directly prompt the model, there is actually a template that
    we need to follow. The template looks as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æˆ‘ä»¬å¯ä»¥ç›´æ¥æç¤ºæ¨¡å‹ï¼Œä½†å®é™…ä¸Šæˆ‘ä»¬éœ€è¦éµå¾ªä¸€ä¸ªæ¨¡æ¿ã€‚æ¨¡æ¿å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This template consists of two main components, namely the `{{ System Prompt
    }}` and the `{{ User Prompt }}`:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡æ¿ç”±ä¸¤ä¸ªä¸»è¦ç»„ä»¶ç»„æˆï¼Œå³`{{ System Prompt }}`å’Œ`{{ User Prompt }}`ï¼š
- en: The `{{ System Prompt }}` helps us guide the model during a conversation. For
    example, we can say that it is a helpful assistant that is specialized in labeling
    topics.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`{{ System Prompt }}`å¸®åŠ©æˆ‘ä»¬åœ¨å¯¹è¯è¿‡ç¨‹ä¸­æŒ‡å¯¼æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥è¯´å®ƒæ˜¯ä¸€ä¸ªä¸“é—¨è´Ÿè´£æ ‡è®°ä¸»é¢˜çš„æœ‰ç”¨åŠ©æ‰‹ã€‚'
- en: The `{{ User Prompt }}` is where we ask it a question.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`{{ User Prompt }}`æ˜¯æˆ‘ä»¬å‘å®ƒæé—®çš„åœ°æ–¹ã€‚'
- en: You might have noticed the `[INST]` tags, which are used to identify the beginning
    and end of a prompt. We can use these to model the conversation history as we
    will see more in-depth later on.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½æ³¨æ„åˆ°`[INST]`æ ‡ç­¾ï¼Œè¿™äº›æ ‡ç­¾ç”¨äºæ ‡è¯†æç¤ºçš„å¼€å§‹å’Œç»“æŸã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™äº›æ ‡ç­¾æ¥å»ºæ¨¡å¯¹è¯å†å²ï¼Œç¨åæˆ‘ä»¬ä¼šæ›´æ·±å…¥åœ°äº†è§£ã€‚
- en: Next, letâ€™s see how we can use this template to optimize Llama 2 for topic modeling.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ä½¿ç”¨è¿™ä¸ªæ¨¡æ¿æ¥ä¼˜åŒ–Llama 2è¿›è¡Œä¸»é¢˜å»ºæ¨¡ã€‚
- en: Prompt Template
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æç¤ºæ¨¡æ¿
- en: 'We are going to keep our `system prompt` simple and to the point:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä¿æŒ`system prompt`ç®€å•æ˜äº†ï¼š
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We will tell the model that it is simply a helpful assistant for labeling topics
    since that is our main goal.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å‘Šè¯‰æ¨¡å‹ï¼Œå®ƒåªæ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ï¼Œè´Ÿè´£æ ‡è®°ä¸»é¢˜ï¼Œå› ä¸ºè¿™æ˜¯æˆ‘ä»¬çš„ä¸»è¦ç›®æ ‡ã€‚
- en: In contrast, our `user prompt` is going to be a bit more involved. It will consist
    of two components, an **example** **prompt** and the **main prompt**.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬çš„`user prompt`ä¼šç¨å¾®å¤æ‚ä¸€äº›ã€‚å®ƒå°†åŒ…å«ä¸¤ä¸ªéƒ¨åˆ†ï¼Œä¸€ä¸ª**ç¤ºä¾‹** **æç¤º**å’Œ**ä¸»è¦æç¤º**ã€‚
- en: Letâ€™s start with the **example prompt**. Most LLMs do a much better job of generating
    accurate responses if you give them an example to work with. We will show it an
    accurate example of the kind of output we are expecting.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»**ç¤ºä¾‹æç¤º**å¼€å§‹ã€‚å¦‚æœä½ ç»™å¤§å¤šæ•°LLMä¸€ä¸ªç¤ºä¾‹ï¼Œå®ƒä»¬é€šå¸¸èƒ½æ›´å¥½åœ°ç”Ÿæˆå‡†ç¡®çš„å“åº”ã€‚æˆ‘ä»¬å°†å±•ç¤ºä¸€ä¸ªå‡†ç¡®çš„ç¤ºä¾‹ï¼Œè¯´æ˜æˆ‘ä»¬æœŸæœ›çš„è¾“å‡ºç±»å‹ã€‚
- en: '[PRE11]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This example, based on a number of keywords and documents primarily about the
    impact of meat, helps the model understand the kind of output it should give.
    We show the model that we were expecting only the label, which is easier for us
    to extract.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªä¾‹å­åŸºäºä¸€ç³»åˆ—å…³é”®å­—å’Œä¸»è¦å…³äºè‚‰ç±»å½±å“çš„æ–‡æ¡£ï¼Œå¸®åŠ©æ¨¡å‹ç†è§£å®ƒåº”è¯¥ç»™å‡ºä»€ä¹ˆæ ·çš„è¾“å‡ºã€‚æˆ‘ä»¬å‘æ¨¡å‹å±•ç¤ºäº†æˆ‘ä»¬åªæœŸå¾…æ ‡ç­¾ï¼Œè¿™æ ·æˆ‘ä»¬æ›´å®¹æ˜“æå–ã€‚
- en: 'Next, we will create a template that we can use within BERTopic:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªå¯ä»¥åœ¨BERTopicä¸­ä½¿ç”¨çš„æ¨¡æ¿ï¼š
- en: '[PRE12]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'There are two BERTopic-specific tags that are of interest, namely `[DOCUMENTS]`
    and `[KEYWORDS]`:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸¤ä¸ªBERTopicç‰¹å®šçš„æ ‡ç­¾å€¼å¾—å…³æ³¨ï¼Œå³`[DOCUMENTS]`å’Œ`[KEYWORDS]`ï¼š
- en: '`[DOCUMENTS]` contain the top 5 most relevant documents to the topic'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[DOCUMENTS]`åŒ…å«ä¸ä¸»é¢˜æœ€ç›¸å…³çš„å‰5ä¸ªæ–‡æ¡£'
- en: '`[KEYWORDS]` contain the top 10 most relevant keywords to the topic as generated
    through c-TF-IDF'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[KEYWORDS]`åŒ…å«é€šè¿‡c-TF-IDFç”Ÿæˆçš„ä¸ä¸»é¢˜æœ€ç›¸å…³çš„å‰10ä¸ªå…³é”®å­—'
- en: 'This template will be filled according to each topic. And finally, we can combine
    this into our final prompt:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡æ¿å°†æ ¹æ®æ¯ä¸ªä¸»é¢˜è¿›è¡Œå¡«å†™ã€‚æœ€åï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶åˆå¹¶ä¸ºæˆ‘ä»¬çš„æœ€ç»ˆæç¤ºï¼š
- en: '[PRE13]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ğŸ—¨ï¸ BERTopic
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ—¨ï¸ BERTopic
- en: 'Before we can start with topic modeling, we will first need to perform two
    steps:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹ä¸»é¢˜å»ºæ¨¡ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å…ˆæ‰§è¡Œä¸¤ä¸ªæ­¥éª¤ï¼š
- en: Pre-calculating Embeddings
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é¢„è®¡ç®—åµŒå…¥
- en: Defining Sub-models
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®šä¹‰å­æ¨¡å‹
- en: Preparing Embeddings
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‡†å¤‡åµŒå…¥
- en: By pre-calculating the embeddings for each document, we can speed up additional
    exploration steps and use the embeddings to quickly iterate over BERTopicâ€™s hyperparameters
    if needed.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ä¸ºæ¯ä¸ªæ–‡æ¡£é¢„è®¡ç®—åµŒå…¥ï¼Œæˆ‘ä»¬å¯ä»¥åŠ é€Ÿé¢å¤–çš„æ¢ç´¢æ­¥éª¤ï¼Œå¹¶åœ¨éœ€è¦æ—¶ä½¿ç”¨åµŒå…¥å¿«é€Ÿè¿­ä»£BERTopicçš„è¶…å‚æ•°ã€‚
- en: 'ğŸ”¥ **TIP**: You can find a great overview of good embeddings for clustering
    on the [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard).'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”¥ **æç¤º**ï¼šä½ å¯ä»¥åœ¨[MTEBæ’è¡Œæ¦œ](https://huggingface.co/spaces/mteb/leaderboard)ä¸Šæ‰¾åˆ°æœ‰å…³èšç±»çš„ä¼˜ç§€åµŒå…¥æ¦‚è¿°ã€‚
- en: '[PRE14]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Sub-models
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å­æ¨¡å‹
- en: Next, we will define all sub-models in BERTopic and do some small tweaks to
    the number of clusters to be created, setting random states, etc.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å®šä¹‰BERTopicä¸­çš„æ‰€æœ‰å­æ¨¡å‹ï¼Œå¹¶å¯¹è¦åˆ›å»ºçš„èšç±»æ•°é‡è¿›è¡Œä¸€äº›å°çš„è°ƒæ•´ï¼Œå¦‚è®¾ç½®éšæœºçŠ¶æ€ç­‰ã€‚
- en: '[PRE15]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As a small bonus, we are going to reduce the embeddings we created before to
    2-dimensions so that we can use them for visualization purposes when we have created
    our topics.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºä¸€ä¸ªå°é¢å¤–å†…å®¹ï¼Œæˆ‘ä»¬å°†æŠŠä¹‹å‰åˆ›å»ºçš„åµŒå…¥å‡å°‘åˆ°2ç»´ï¼Œä»¥ä¾¿åœ¨åˆ›å»ºä¸»é¢˜æ—¶ç”¨äºå¯è§†åŒ–ã€‚
- en: '[PRE16]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Representation Models
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¡¨ç¤ºæ¨¡å‹
- en: One of the ways we are going to represent the topics is with Llama 2 which should
    give us a nice label. However, we might want to have additional representations
    to view a topic from multiple angles.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ç”¨Llama 2æ¥è¡¨ç¤ºè¿™äº›ä¸»é¢˜ï¼Œè¿™æ ·å¯ä»¥è·å¾—ä¸€ä¸ªä¸é”™çš„æ ‡ç­¾ã€‚ä¸è¿‡ï¼Œæˆ‘ä»¬å¯èƒ½è¿˜å¸Œæœ›æœ‰é¢å¤–çš„è¡¨ç¤ºï¼Œä»¥ä»å¤šä¸ªè§’åº¦æŸ¥çœ‹ä¸»é¢˜ã€‚
- en: Here, we will be using c-TF-IDF as our main representation and [KeyBERT](https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#keybertinspired),
    [MMR](https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#maximalmarginalrelevance),
    and [Llama 2](https://maartengr.github.io/BERTopic/getting_started/representation/llm.html)
    as our additional representations.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†ä½¿ç”¨c-TF-IDFä½œä¸ºä¸»è¦è¡¨ç¤ºï¼Œå¹¶å°†[KeyBERT](https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#keybertinspired)ã€[MMR](https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#maximalmarginalrelevance)å’Œ[Llama
    2](https://maartengr.github.io/BERTopic/getting_started/representation/llm.html)ä½œä¸ºé™„åŠ è¡¨ç¤ºã€‚
- en: '[PRE17]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ğŸ”¥ Training
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ”¥ è®­ç»ƒ
- en: Now that we have our models prepared, we can start training our topic model!
    We supply BERTopic with the sub-models of interest, run `.fit_transform`, and
    see what kind of topics we get.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬çš„æ¨¡å‹å·²ç»å‡†å¤‡å¥½ï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹è®­ç»ƒæˆ‘ä»¬çš„ä¸»é¢˜æ¨¡å‹ï¼æˆ‘ä»¬å°†æ„Ÿå…´è¶£çš„å­æ¨¡å‹æä¾›ç»™BERTopicï¼Œè¿è¡Œ`.fit_transform`ï¼Œçœ‹çœ‹æˆ‘ä»¬å¾—åˆ°ä»€ä¹ˆæ ·çš„ä¸»é¢˜ã€‚
- en: '[PRE18]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now that we are done training our model, letâ€™s see what topics were generated:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»å®Œæˆäº†æ¨¡å‹è®­ç»ƒï¼Œæ¥çœ‹ä¸€ä¸‹ç”Ÿæˆäº†ä»€ä¹ˆä¸»é¢˜ï¼š
- en: '[PRE19]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](../Images/d9a0326290f7988d127e851f3d452d37.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d9a0326290f7988d127e851f3d452d37.png)'
- en: '[PRE20]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](../Images/a34443869e8befd4e6af0d8622f045df.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a34443869e8befd4e6af0d8622f045df.png)'
- en: We got over 100 topics that were created and they all seem quite diverse. We
    can use the labels by Llama 2 and assign them to topics that we have created.
    Normally, the default topic representation would be c-TF-IDF, but we will focus
    on Llama 2 representations instead.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åˆ›å»ºäº†è¶…è¿‡100ä¸ªä¸»é¢˜ï¼Œå®ƒä»¬ä¼¼ä¹éå¸¸å¤šæ ·åŒ–ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨Llama 2æä¾›çš„æ ‡ç­¾ï¼Œå¹¶å°†å…¶åˆ†é…ç»™æˆ‘ä»¬åˆ›å»ºçš„ä¸»é¢˜ã€‚é€šå¸¸ï¼Œé»˜è®¤çš„ä¸»é¢˜è¡¨ç¤ºæ˜¯c-TF-IDFï¼Œä½†æˆ‘ä»¬å°†é‡ç‚¹å…³æ³¨Llama
    2çš„è¡¨ç¤ºã€‚
- en: '[PRE21]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[ğŸ“Š](https://emojigraph.org/bar-chart/) Visualize'
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[ğŸ“Š](https://emojigraph.org/bar-chart/) å¯è§†åŒ–'
- en: We can go through each topic manually, which would take a lot of work, or we
    can visualize them all in a single interactive graph.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥æ‰‹åŠ¨æµè§ˆæ¯ä¸ªä¸»é¢˜ï¼Œè¿™ä¼šéœ€è¦å¤§é‡å·¥ä½œï¼Œæˆ–è€…æˆ‘ä»¬å¯ä»¥åœ¨ä¸€ä¸ªäº’åŠ¨å›¾è¡¨ä¸­å¯è§†åŒ–å®ƒä»¬ã€‚
- en: BERTopic has a bunch of [visualization functions](https://maartengr.github.io/BERTopic/getting_started/visualization/visualize_documents.html)
    that we can use. For now, we are sticking with visualizing the documents.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: BERTopicæœ‰ä¸€ç³»åˆ—[å¯è§†åŒ–å‡½æ•°](https://maartengr.github.io/BERTopic/getting_started/visualization/visualize_documents.html)ä¾›æˆ‘ä»¬ä½¿ç”¨ã€‚ç›®å‰ï¼Œæˆ‘ä»¬å°†ç»§ç»­å¯è§†åŒ–æ–‡æ¡£ã€‚
- en: '[PRE22]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](../Images/03b0559c55725b27e2ef199d98605b6e.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03b0559c55725b27e2ef199d98605b6e.png)'
- en: '[ğŸ–¼ï¸](https://emojipedia.org/framed-picture/) (BONUS): Advanced Visualization'
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[ğŸ–¼ï¸](https://emojipedia.org/framed-picture/)ï¼ˆé¢å¤–å†…å®¹ï¼‰ï¼šé«˜çº§å¯è§†åŒ–'
- en: Although we can use the built-in visualization features of BERTopic, we can
    also create a static visualization that might be a bit more informative.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æˆ‘ä»¬å¯ä»¥ä½¿ç”¨BERTopicçš„å†…ç½®å¯è§†åŒ–åŠŸèƒ½ï¼Œä½†æˆ‘ä»¬ä¹Ÿå¯ä»¥åˆ›å»ºä¸€ä¸ªé™æ€å¯è§†åŒ–ï¼Œå¯èƒ½ä¼šæä¾›æ›´å¤šçš„ä¿¡æ¯ã€‚
- en: 'We start by creating the necessary variables that contain our reduced embeddings
    and representations:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆåˆ›å»ºå¿…è¦çš„å˜é‡ï¼Œè¿™äº›å˜é‡åŒ…å«æˆ‘ä»¬å‡å°‘çš„åµŒå…¥å’Œè¡¨ç¤ºï¼š
- en: '[PRE23]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next, we will visualize the reduced embeddings with Matplotlib and process
    the labels in such a way that it is visually more pleasing:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨Matplotlibå¯è§†åŒ–å‡å°‘åçš„åµŒå…¥ï¼Œå¹¶ä»¥è§†è§‰ä¸Šæ›´ä»¤äººæ„‰æ‚¦çš„æ–¹å¼å¤„ç†æ ‡ç­¾ï¼š
- en: '[PRE24]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![](../Images/8fb9e2c99f87f38e3d5f5a824b2a0d65.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8fb9e2c99f87f38e3d5f5a824b2a0d65.png)'
- en: '**Update**: I uploaded a video version to YouTube that goes more in-depth into
    how to use BERTopic with Llama 2:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ›´æ–°**ï¼šæˆ‘ä¸Šä¼ äº†ä¸€ä¸ªè§†é¢‘ç‰ˆæœ¬åˆ°YouTubeï¼Œè¯¦ç»†ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨BERTopicä¸Llama 2ï¼š'
- en: Thank you for reading!
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢ä½ çš„é˜…è¯»ï¼
- en: If you are, like me, passionate about AI and/or Psychology, please feel free
    to add me on [**LinkedIn**](https://www.linkedin.com/in/mgrootendorst/), follow
    me on [**Twitter**](https://twitter.com/MaartenGr), or subscribe to my [**Newsletter**](http://maartengrootendorst.substack.com/).
    You can also find some of my content on my [**Personal Website**](https://maartengrootendorst.com/).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ åƒæˆ‘ä¸€æ ·ï¼Œå¯¹AIå’Œ/æˆ–å¿ƒç†å­¦å……æ»¡çƒ­æƒ…ï¼Œè¯·éšæ—¶åœ¨[**LinkedIn**](https://www.linkedin.com/in/mgrootendorst/)ä¸ŠåŠ æˆ‘ä¸ºå¥½å‹ï¼Œåœ¨[**Twitter**](https://twitter.com/MaartenGr)ä¸Šå…³æ³¨æˆ‘ï¼Œæˆ–è®¢é˜…æˆ‘çš„[**Newsletter**](http://maartengrootendorst.substack.com/)ã€‚ä½ ä¹Ÿå¯ä»¥åœ¨æˆ‘çš„[**ä¸ªäººç½‘ç«™**](https://maartengrootendorst.com/)ä¸Šæ‰¾åˆ°ä¸€äº›æˆ‘çš„å†…å®¹ã€‚
- en: '*All images without a source credit were created by the author*'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ‰€æœ‰æ²¡æœ‰æ¥æºè¯´æ˜çš„å›¾åƒå‡ç”±ä½œè€…åˆ›ä½œ*'
