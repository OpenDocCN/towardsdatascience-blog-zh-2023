- en: Explainable AI with TCAV from Google AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用谷歌AI的TCAV进行可解释AI
- en: 原文：[https://towardsdatascience.com/explainable-ai-with-tcav-from-google-ai-5408adf905e](https://towardsdatascience.com/explainable-ai-with-tcav-from-google-ai-5408adf905e)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/explainable-ai-with-tcav-from-google-ai-5408adf905e](https://towardsdatascience.com/explainable-ai-with-tcav-from-google-ai-5408adf905e)
- en: Explain deep neural networks using concept-based explanations
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用基于概念的解释解释深度神经网络
- en: '[](https://adib0073.medium.com/?source=post_page-----5408adf905e--------------------------------)[![Aditya
    Bhattacharya](../Images/d0f79ad4a85330c58327aea499b7eea0.png)](https://adib0073.medium.com/?source=post_page-----5408adf905e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5408adf905e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5408adf905e--------------------------------)
    [Aditya Bhattacharya](https://adib0073.medium.com/?source=post_page-----5408adf905e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://adib0073.medium.com/?source=post_page-----5408adf905e--------------------------------)[![Aditya
    Bhattacharya](../Images/d0f79ad4a85330c58327aea499b7eea0.png)](https://adib0073.medium.com/?source=post_page-----5408adf905e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5408adf905e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5408adf905e--------------------------------)
    [Aditya Bhattacharya](https://adib0073.medium.com/?source=post_page-----5408adf905e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5408adf905e--------------------------------)
    ·13 min read·Feb 18, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----5408adf905e--------------------------------)
    ·13分钟阅读·2023年2月18日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/08876ebee32ea4c27b44a794161d9bab.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08876ebee32ea4c27b44a794161d9bab.png)'
- en: 'Image Source: [Pixabay](https://pixabay.com/illustrations/tick-tock-tiktok-network-computer-7730760/)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Pixabay](https://pixabay.com/illustrations/tick-tock-tiktok-network-computer-7730760/)
- en: '[**Explainable AI (XAI)**](https://amzn.to/3cY4c2h) is a subfield of artificial
    intelligence (AI) that aims to develop AI systems that can provide clear and understandable
    explanations of their decision-making processes to humans. The goal of XAI is
    to make AI more transparent, trustworthy, responsible, and ethical. XAI is an
    important element in increasing AI adoption, especially for high stake domains
    such as healthcare, finance, and law enforcement. In these domains, it is crucial
    to understand how an AI system arrived at a particular decision or recommendation.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[**可解释人工智能 (XAI)**](https://amzn.to/3cY4c2h) 是人工智能 (AI) 的一个子领域，旨在开发能够向人类提供清晰且易于理解的决策过程解释的AI系统。XAI的目标是使AI变得更加透明、可信、负责任和伦理。XAI在增加AI应用方面至关重要，尤其是在医疗保健、金融和执法等高风险领域。在这些领域，理解AI系统如何得出特定决策或建议至关重要。'
- en: There are various techniques used in XAI, including model transparency, rule-based
    systems, and model-agnostic methods such as LIME and SHAP. XAI approaches can
    vary depending on the type of AI system, the application domain, and the level
    of explainability required. Overall, XAI is an essential field for developing
    AI systems that can be trusted and used ethically and effectively in real-world
    applications.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: XAI中使用了各种技术，包括模型透明性、基于规则的系统以及如LIME和SHAP等模型无关的方法。XAI方法可以根据AI系统的类型、应用领域和所需的可解释性水平有所不同。总体而言，XAI是一个至关重要的领域，用于开发可以信任并在现实世界应用中有效且伦理地使用的AI系统。
- en: 'If you want a brief introduction to XAI in a short 45 mins video, then you
    can watch one of my past sessions on XAI delivered at the **AI Accelerator Festival
    APAC, 2021**:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在短短45分钟的视频中获得XAI的简要介绍，你可以观看我在**2021年AI加速器节APAC**上发表的关于XAI的过去的一个讲座：
- en: 'Explainable AI: Making ML and DL models more interpretable (Talk by the author)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释AI：使机器学习和深度学习模型更具可解释性（作者讲座）
- en: One major limitation of popular [XAI methods](https://amzn.to/3J2QNnz) such
    as LIME and SHAP is that these methods are not extremely consistent and intuitive
    with how non-technical end users would explain an observation. For example, if
    you have an image of a glass filled with Coke and use LIME and SHAP to explain
    a black-box model used to correctly classify the image as Coke, both LIME and
    SHAP would highlight regions of the image that lead to the correct prediction
    by the trained model. But if you ask a non-technical user to describe the image,
    the user would classify the image as Coke due to the presence of a dark-colored
    carbonated liquid in a glass that resembles a Cola drink. In other words, human
    beings tend to relate any observation with known *concepts* to explain it.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 目前流行的 [XAI 方法](https://amzn.to/3J2QNnz) 如 LIME 和 SHAP 的一个主要限制是，这些方法与非技术性终端用户解释观察的方式不是非常一致和直观。例如，如果你有一张装满可乐的玻璃的图像，并使用
    LIME 和 SHAP 解释一个正确将图像分类为可乐的黑箱模型，LIME 和 SHAP 都会突出显示图像中导致模型正确预测的区域。但如果你让一个非技术用户描述这张图像，用户会因为玻璃中含有一种类似可乐饮料的深色碳酸液体而将其归类为可乐。换句话说，人类倾向于用已知的*概念*来解释任何观察结果。
- en: '[**Testing with Concept Activation Vector (TCAV)** from *Google AI*](https://arxiv.org/pdf/1711.11279.pdf)
    also follows a similar approach in terms of explaining model predictions with
    known *human concepts*. So, in this article, we will cover how TCAV can be used
    to provide concept-based human-friendly explanations. Unlike LIME and SHAP, TCAV
    works beyond *feature attribution* and refers to concepts such as *color*, *gender*,
    *race*, *shape*, *any known object*, or an *abstract idea* to explain model predictions.
    We will discuss the following topics about TCAV in this article:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[**概念激活向量（TCAV）测试**](https://arxiv.org/pdf/1711.11279.pdf) 也采用了类似的方法，用于通过已知的*人类概念*解释模型预测。因此，在这篇文章中，我们将探讨如何利用
    TCAV 提供基于概念的人性化解释。与 LIME 和 SHAP 不同，TCAV 超越了*特征归因*，参考诸如*颜色*、*性别*、*种族*、*形状*、*任何已知对象*或*抽象概念*等来解释模型预测。我们将在这篇文章中讨论有关
    TCAV 的以下主题：'
- en: Understanding TCAV intuitively
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直观理解 TCAV
- en: Differences between TCAV and other XAI frameworks
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TCAV 与其他 XAI 框架的区别
- en: Potential applications of concept-based explanations
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于概念的解释的潜在应用
- en: In this article, I will refer to some of the XAI frameworks discussed in my
    book [**Applied Machine Learning Explainability Techniques**](https://amzn.to/3cY4c2h)**.**
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将提到我书中讨论的一些 XAI 框架 [**应用机器学习可解释性技术**](https://amzn.to/3cY4c2h)**。**
- en: '[](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)
    [## Applied Machine Learning Explainability Techniques: Make ML models explainable
    and trustworthy for…'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)
    [## 应用机器学习可解释性技术：使 ML 模型在实际中可解释和可信…'
- en: 'Applied Machine Learning Explainability Techniques: Make ML models explainable
    and trustworthy for practical…'
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用机器学习可解释性技术：使 ML 模型在实际中可解释和可信…
- en: www.amazon.com](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: www.amazon.com](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)
- en: It’s time to get started now!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是开始的时候了！
- en: Introduction to TCAV
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TCAV 简介
- en: '**Testing with Concept Activation Vectors (TCAV)** is an XAI method to understand
    what signals neural network models use for prediction. TCAV shows the importance
    of high-level concepts (e.g., color, gender, race) for a prediction class, similar
    to how humans communicate! TCAV gives an explanation that is generally true for
    a class of interest, beyond one image (global explanation). For example, for a
    given class, we can show how much race or gender was important for classifications
    in InceptionV3\. Even though neither race nor gender labels were part of the training
    input!'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用概念激活向量（TCAV）**是一种XAI方法，用于了解神经网络模型在预测时使用了哪些信号。TCAV展示了高层次概念（例如，颜色、性别、种族）对预测类别的重要性，类似于人类的沟通方式！TCAV提供了一种通常适用于一个感兴趣类别的解释，而不仅仅是单张图片（全局解释）。例如，对于给定的类别，我们可以展示种族或性别对InceptionV3分类的影响程度。即使种族或性别标签也并非训练输入的一部分！'
- en: The algorithm depends on **Concept Activation Vectors (CAV)**, which provide
    an interpretation of the internal state of ML models using human-friendly concepts.
    In a more technical sense, TCAV uses **directional derivatives** to quantify the
    importance of human-friendly, high-level concepts for model predictions. For example,
    while describing hairstyles, concepts such as *curly hair*, *straight hair*, or
    *hair color* can be used by TCAV. These user-defined concepts are not the input
    features of the dataset that are used by the algorithm during the training process.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法依赖于**概念激活向量（CAV）**，它利用人类友好的概念来解释机器学习模型的内部状态。从更技术的角度来看，TCAV使用**方向导数**来量化对模型预测有重要影响的人类友好型高层次概念的重要性。例如，在描述发型时，*卷发*、*直发*或*发色*等概念可以被TCAV使用。这些用户定义的概念并不是算法在训练过程中使用的数据集的输入特征。
- en: '[](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)
    [## Applied Machine Learning Explainability Techniques: Make ML models explainable
    and trustworthy for…'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[应用机器学习可解释性技术：使机器学习模型对实际应用可解释和可信赖](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)'
- en: 'Applied Machine Learning Explainability Techniques: Make ML models explainable
    and trustworthy for practical…'
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '[应用机器学习可解释性技术：使机器学习模型对实际应用可解释和可信赖](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)'
- en: www.amazon.com](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[应用机器学习可解释性技术：使机器学习模型对实际应用可解释和可信赖](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)'
- en: 'Other popular XAI methods, such as LIME and SHAP, depend on features that are
    considered important by the model. There is no scope for adding customized user-defined
    concepts as input features for the basis of explainability. The following figure
    illustrates the key question addressed by TCAV:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 其他流行的XAI方法，如LIME和SHAP，依赖于模型认为重要的特征。没有为解释性添加自定义用户定义概念作为输入特征的空间。下图展示了TCAV解决的关键问题：
- en: '![](../Images/617dc5e692affa342609d8ea8d426f5d.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/617dc5e692affa342609d8ea8d426f5d.png)'
- en: Key question addressed by TCAV — *What is the importance of a concept for predicting
    the output?* (Image by author)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: TCAV解决的关键问题 — *一个概念在预测输出中的重要性是什么？* （图片作者）
- en: Explaining with abstract concepts
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用抽象概念进行解释
- en: By now, you may have an intuitive understanding of the method of providing explanations
    with abstract concepts. But why do you think this is an effective approach?
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你可能已经对使用抽象概念进行解释的方法有了直观的理解。但你认为这为什么是一种有效的方法？
- en: Let’s take another example. Suppose you are working on building a deep learning-based
    image classifier for detecting doctors from images. After applying TCAV, let’s
    say that you have found out that the *concept importance* of the concept *white
    male* is maximum, followed by *stethoscope* and *white coat*. The concept importance
    of a *stethoscope* and *white coat* is expected, but the high concept importance
    of *white male* indicates a biased dataset. Hence, TCAV can help to evaluate **fairness**
    in trained models.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再举一个例子。假设你正在构建一个基于深度学习的图像分类器，用于从图像中检测医生。在应用 TCAV 后，假设你发现*白人男性*的*概念重要性*最大，其次是*听诊器*和*白色大褂*。听诊器和白色大褂的概念重要性是预期中的，但*白人男性*的高概念重要性表明数据集存在偏差。因此，TCAV
    可以帮助评估训练模型的**公平性**。
- en: Essentially, the goal of CAVs is to estimate the importance of a concept (such
    as color, gender, and race) for predicting a trained model, even though the *concepts*
    were not used during the model training process. This is because TCAV learns *concepts*
    from a few example samples.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，CAV 的目标是估计一个概念（如颜色、性别和种族）对预测训练模型的重要性，即使这些*概念*在模型训练过程中没有被使用。这是因为 TCAV 从少量示例样本中学习*概念*。
- en: For example, in order to learn a *gender* concept, TCAV needs a few data instances
    that have a *male* concept and a few *non-male* examples. Hence, TCAV can quantitatively
    estimate the trained model’s sensitivity to a particular *concept* for that class.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，为了学习一个*性别*概念，TCAV 需要一些具有*男性*概念的数据实例和一些*非男性*示例。因此，TCAV 可以定量估计训练模型对该类特定*概念*的敏感性。
- en: For generating explanations, TCAV perturbs data points toward a *concept* that
    is relatable to humans, and so it is a type of **global perturbation method**.
    Next, let’s try to learn the main objectives of TCAV.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成解释时，TCAV 会将数据点扰动到一个对人类可理解的*概念*，因此这是一种**全球扰动方法**。接下来，让我们尝试了解 TCAV 的主要目标。
- en: Goals of TCAV
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TCAV 的目标
- en: 'I found the approach of TCAV to be unique compared to other explanation methods.
    One of the main reasons is that the developers of this framework established clear
    goals that resonate with my own understanding of human-friendly explanations.
    The following are the established goals of TCAV:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现 TCAV 的方法与其他解释方法相比非常独特。一个主要原因是这个框架的开发者设立了与我对人类友好解释的理解相一致的明确目标。以下是 TCAV 设立的目标：
- en: '**Accessibility**: The developers of TCAV wanted this approach to be accessible
    to any end user, irrespective of their knowledge of ML or data science.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可访问性**：TCAV 的开发者希望这种方法对任何终端用户都能可访问，无论他们是否了解机器学习或数据科学。'
- en: '**Customization**: The framework can adapt to any user-defined concept. This
    is not limited to concepts considered during the training process.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定制化**：该框架可以适应任何用户定义的概念。这不仅限于训练过程中考虑的概念。'
- en: '**Plug-in readiness**: The developers wanted this approach to work without
    the need to retrain or fine-tune trained ML models.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**插件准备**：开发者希望这种方法可以在不需要重新训练或微调已经训练好的机器学习模型的情况下进行工作。'
- en: '**Global interpretability**: TCAV can interpret the entire class or multiple
    samples of the dataset with a single quantitative measure. It is not restricted
    to the local explainability of data instances.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全局可解释性**：TCAV 可以通过单一的定量度量解释整个类别或数据集的多个样本。它不局限于数据实例的局部可解释性。'
- en: Now that we know what can be achieved using TCAV, let’s discuss the general
    approach to how TCAV works.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道使用 TCAV 可以实现什么，让我们讨论 TCAV 的一般工作方法。
- en: Approach of TCAV
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TCAV 的方法
- en: 'In this section, we will cover the workings of TCAV in more depth. The overall
    workings of this algorithm can be summarized in the following methods:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将更深入地讨论 TCAV 的工作原理。这个算法的整体工作可以通过以下方法进行总结：
- en: Applying directional derivatives to quantitatively estimate the sensitivity
    of predictions of trained ML models for various user-defined concepts.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用方向导数来定量估计训练的机器学习模型对各种用户定义概念的预测敏感性。
- en: Computing the final quantitative explanation, which is termed **TCAVq measure**,
    without any model re-training or fine-tuning. This measure is the relative importance
    of each concept to each model prediction class.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算最终的定量解释，称为**TCAVq 量度**，无需任何模型重新训练或微调。这个量度是每个概念对每个模型预测类别的相对重要性。
- en: '![](../Images/f0e58de286513f79414f96434b9a9bef.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f0e58de286513f79414f96434b9a9bef.png)'
- en: The approach used by TCAV to estimate the concept importance of stripes in a
    tiger image classifier (Image by author)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: TCAV 用于估计老虎图像分类器中条纹概念重要性的 approach（图片由作者提供）
- en: Now, I will try to further simplify the approach of TCAV without using too many
    mathematical notions. Let’s assume we have a model for identifying zebras from
    images.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我将尝试进一步简化 TCAV 的方法，而不使用太多数学概念。假设我们有一个从图像中识别斑马的模型。
- en: 'To apply TCAV, the following approach can be taken:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要应用 TCAV，可以采取以下方法：
- en: '**Defining a concept of interest**: The very first step is to consider the
    concepts of interest. For our zebra classifier, either we can have a given set
    of examples that represent the concept (such as black stripes are important in
    identifying a zebra) or we can have an independent dataset with the concepts labeled.
    The major benefit of this step is that it does not limit the algorithm from using
    features used by the model. Even non-technical users or domain experts can define
    the concepts based on their existing knowledge.'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义感兴趣的概念**：第一步是考虑感兴趣的概念。对于我们的斑马分类器，我们可以拥有一组表示该概念的示例（例如黑色条纹在识别斑马时很重要），或者我们可以拥有一个标记了概念的独立数据集。这一步的主要好处是它不限制算法使用模型所用的特征。即使是非技术用户或领域专家也可以根据现有知识定义概念。'
- en: '**Learning concept activation vectors**: The algorithm tries to learn a vector
    in the space of activation of the layers by training a linear classifier to differentiate
    between activations generated by a concept’s instances and instances present in
    any layer. So, a **CAV** is defined as the normal projection to a hyperplane that
    separates instances with a concept and instances without a concept in the model’s
    activation. For our zebra classifier, CAVs help to distinguish representations
    that denote *black stripes* and representations that do not denote *black stripes*.'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**学习概念激活向量**：算法尝试通过训练线性分类器来区分由概念实例生成的激活与存在于任何层中的实例，从而在激活层空间中学习一个向量。因此，**CAV**
    被定义为将带有概念的实例和没有概念的实例在模型激活中分开的超平面的正常投影。对于我们的斑马分类器，CAV 帮助区分表示 *黑色条纹* 的表征和不表示 *黑色条纹*
    的表征。'
- en: '**Estimating directional derivatives**: Directional derivatives are used to
    quantify the sensitivity of a model prediction toward a concept. So, for our zebra
    classifier, directional directives help us to measure the importance of the *black
    stripes* representation in predicting zebras. Unlike saliency maps, which use
    per-pixel saliency, directional derivatives are computed on the entire dataset
    or a set of inputs but for a specific concept. This helps to give a global perspective
    for the explanation.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**估计方向导数**：方向导数用于量化模型预测对某个概念的敏感性。因此，对于我们的斑马分类器，方向导数帮助我们测量 *黑色条纹* 表征在预测斑马时的重要性。与使用逐像素显著性的显著性图不同，方向导数是在整个数据集或一组输入上计算的，但针对特定概念。这有助于提供全局视角以进行解释。'
- en: '**Estimating the TCAV score**: To quantify the concept importance of a particular
    class, the TCAV score (**TCAVq**) is calculated. This metric helps to measure
    the positive or negative influence of a defined concept on a particular activation
    layer of a model.'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**估计 TCAV 分数**：为了量化特定类别的概念重要性，计算 TCAV 分数 (**TCAVq**)。这个指标有助于测量定义概念对模型特定激活层的正面或负面影响。'
- en: '**CAV validation**: CAV can be produced from randomly selected data. But unfortunately,
    this might not produce meaningful concepts. So, in order to improve the generated
    concepts, TCAV runs multiple iterations for finding concepts from different batches
    of data, instead of training CAV once, on a single batch of data. Then, a **statistical
    significance test** is performed using *two-side t-test* for selecting the statistically
    significant concepts. Necessary corrections, such as the *Bonferroni correction*,
    are also performed to control the false discovery rate.'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**CAV 验证**：CAV 可以从随机选择的数据中生成。但不幸的是，这可能不会产生有意义的概念。因此，为了改进生成的概念，TCAV 会运行多次迭代，以从不同的数据批次中寻找概念，而不是仅在单一数据批次上训练一次
    CAV。然后，使用 *双侧 t 检验* 进行 **统计显著性测试**，以选择统计上显著的概念。还会进行必要的修正，例如 *邦费罗尼修正*，以控制假发现率。'
- en: Thus, we have covered the intuitive workings of the TCAV algorithm. Next, let’s
    cover how TCAV can actually be implemented in practice.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经涵盖了 TCAV 算法的直观工作原理。接下来，让我们讨论 TCAV 如何实际应用。
- en: '[](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)
    [## Applied Machine Learning Explainability Techniques: Make ML models explainable
    and trustworthy for…'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[## Applied Machine Learning Explainability Techniques: Make ML models explainable
    and trustworthy for…](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)'
- en: 'Applied Machine Learning Explainability Techniques: Make ML models explainable
    and trustworthy for practical…'
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'Applied Machine Learning Explainability Techniques: Make ML models explainable
    and trustworthy for practical…'
- en: www.amazon.com](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[## Applied Machine Learning Explainability Techniques: Make ML models explainable
    and trustworthy for…](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)'
- en: Differences between TCAV and other XAI methods
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TCAV 与其他 XAI 方法的区别
- en: Now, let us summarize how TCAV differs from popular XAI methods such as LIME
    and SHAP.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们总结一下 TCAV 与流行的 XAI 方法如 LIME 和 SHAP 的不同之处。
- en: XAI frameworks such as LIME can generate contradicting explanations for two
    data instances for the same class. Whereas, TCAV-generated explanations are not
    only true for a single data instance but also true for the entire class. This
    is a major advantage of TCAV over LIME, which increases the user’s trust in the
    explanation method.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XAI 框架如 LIME 可能会为同一类别的两个数据实例生成相互矛盾的解释。而 TCAV 生成的解释不仅对单一数据实例是准确的，还对整个类别是准确的。这是
    TCAV 相较于 LIME 的一个主要优势，这增加了用户对解释方法的信任。
- en: Concept-based explanations are closer to how humans would explain an unknown
    observation, rather than feature-based explanations as adopted in LIME and SHAP.
    So, TCAV-generated explanations are indeed more human-friendly.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于概念的解释更接近于人类对未知观察的解释，而不是像 LIME 和 SHAP 采用的基于特征的解释。因此，TCAV 生成的解释确实更加符合人类的思维方式。
- en: Feature-based explanations are limited to the features used in the model. To
    introduce any new feature for model explainability, we would need to re-train
    the model, whereas a concept-based explanation is more flexible and is not limited
    to features used during model training. To introduce a new concept, we do not
    need to retrain the model. You would just have to make the necessary datasets
    to generate concepts.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于特征的解释局限于模型中使用的特征。要引入任何新的特征以进行模型解释，我们需要重新训练模型，而基于概念的解释则更具灵活性，不受限于模型训练期间使用的特征。要引入一个新概念，我们不需要重新训练模型。只需准备必要的数据集来生成概念即可。
- en: Model explainability is not the only benefit of TCAV. TCAV can help to detect
    issues during the training process, such as imbalanced datasets leading to bias
    in the dataset vis-à-vis the majority class. In fact, concept importance can be
    used as a metric to compare models.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型解释性并不是 TCAV 唯一的好处。TCAV 可以帮助在训练过程中发现问题，例如数据集不平衡导致对数据集的偏倚。事实上，概念重要性可以作为比较模型的一个指标。
- en: Current limitations of TCAV
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TCAV 的当前限制
- en: 'Unfortunately, like everything in this beautiful world, even TCAV is not perfect!
    Although TCAV is unique in its own way, there are certain limitations of TCAV
    which restrict its wider adoption for model explainability. Some of the prominent
    current limitations of TCAV is discussed below:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，就像这个美丽世界中的一切一样，即使是 TCAV 也并非完美无瑕！虽然 TCAV 以其独特的方式存在，但 TCAV 也有一些限制，这些限制限制了它在模型解释性方面的广泛应用。下面讨论了一些
    TCAV 目前的主要限制：
- en: Currently, the approach of concept-based explanation using TCAV is limited to
    just neural networks. In order to increase its adoption, TCAV would need an implementation
    that can work with *classical machine learning algorithms* such as *Decision Trees*,
    *Support Vector Machines*, and *Ensemble Learning algorithms*. Both LIME and SHAP
    can be applied with classical ML algorithms to solve standard ML problems and
    that is probably why LIME and SHAP have more adoption. Similarly, with text data,
    too, TCAV has very limited applications.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目前，基于概念的解释方法使用 TCAV 仅限于神经网络。为了增加其采用率，TCAV 需要一种可以与*经典机器学习算法*（如*决策树*、*支持向量机*和*集成学习算法*）一起使用的实现。LIME
    和 SHAP 可以应用于经典 ML 算法，以解决标准 ML 问题，这可能也是 LIME 和 SHAP 被更广泛采用的原因。类似地，对于文本数据，TCAV 的应用也非常有限。
- en: 'TCAV is highly prone to *data drift*, *adversarial effects*, and *other data
    quality issues*. If you are using TCAV, you would need to ensure that training
    data, inference data, and even concept data have similar statistical properties.
    Otherwise, the concepts generated can become affected due to noise or data impurity
    issues:'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TCAV 对*数据漂移*、*对抗性影响*和*其他数据质量问题*非常敏感。如果你使用 TCAV，你需要确保训练数据、推理数据，甚至概念数据具有类似的统计特性。否则，生成的概念可能会受到噪声或数据不纯问题的影响：
- en: '*Guillaume Alain* and *Yoshua Bengio*, in their paper *Understanding intermediate
    layers using linear classifier probes* ([https://arxiv.org/abs/1610.01644](https://arxiv.org/abs/1610.01644)),
    have expressed some concern about applying TCAV to shallower neural networks.
    Many similar research papers have suggested that concepts in deeper layers are
    more separable as compared to concepts in shallower networks and, hence, the use
    of TCAV is limited to mostly deep neural networks.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Guillaume Alain* 和 *Yoshua Bengio* 在他们的论文 *Understanding intermediate layers
    using linear classifier probes* ([https://arxiv.org/abs/1610.01644](https://arxiv.org/abs/1610.01644))
    中，对将 TCAV 应用于较浅的神经网络表示了一些担忧。许多类似的研究论文建议，较深层次的概念相比于较浅网络中的概念更具可分性，因此 TCAV 的使用主要限于深度神经网络。'
- en: Preparing a concept dataset can be a challenging and expensive task. Although
    you don’t need ML knowledge to prepare a concept dataset, still, in practice,
    you do not expect any common end user to spend time creating an annotated concept
    dataset for any customized user-defined concept.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备概念数据集可能是一个具有挑战性和昂贵的任务。尽管你不需要 ML 知识来准备概念数据集，但在实践中，你也不会期望任何普通终端用户花时间为任何自定义用户定义的概念创建一个注释概念数据集。
- en: I felt that the TCAV Python framework would require further improvements before
    being used in any production-level system. In my opinion, at the time of writing
    this chapter, this framework would need to mature further so that it can be used
    easily with any production-level ML system.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我认为 TCAV Python 框架在被用于任何生产级系统之前需要进一步改进。依我之见，在撰写本章节时，该框架需要进一步成熟，以便可以轻松地与任何生产级
    ML 系统配合使用。
- en: '[](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)
    [## Applied Machine Learning Explainability Techniques: Make ML models explainable
    and trustworthy for…'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)
    [## 应用机器学习可解释性技术：让 ML 模型在实践中可解释和可信赖……](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)'
- en: 'Applied Machine Learning Explainability Techniques: Make ML models explainable
    and trustworthy for practical…'
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用机器学习可解释性技术：让 ML 模型在实践中可解释和可信赖……
- en: www.amazon.com](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[www.amazon.com](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)'
- en: All these limitations can indeed be solved to make TCAV a much more robust framework
    that is widely adopted. You can also reach out to authors and developers of the
    TCAV framework and contribute to the open-source community! In the next section,
    let’s discuss some potential applications of concept-based explanations.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些限制确实可以解决，使 TCAV 成为一个更强大的框架，得到广泛采用。你也可以联系 TCAV 框架的作者和开发者，为开源社区做贡献！在下一节中，我们将讨论一些基于概念的解释的潜在应用。
- en: Potential applications of concept-based explanations
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于概念的解释的潜在应用
- en: 'I do see great potential for concept-based explanations such as TCAV! In this
    section, you will get exposure to some potential applications of concept-based
    explanations that can be important research topics for the entire AI community,
    which are as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我确实看到基于概念的解释（如 TCAV）的巨大潜力！在本节中，你将接触到一些基于概念的解释的潜在应用，这些应用可能是整个 AI 社区的重要研究主题，如下所示：
- en: '**Estimation of transparency and fairness in AI**: Most regulatory concerns
    for black-box AI models are related to concepts such as gender, color, and race.
    Concept-based explanations can actually help to estimate whether an AI algorithm
    is fair in terms of these abstract concepts. The detection of bias for AI models
    can actually improve their transparency and help to address certain regulatory
    concerns. For example, in terms of doctors using deep learning models, TCAV can
    be used to detect whether the model is biased toward a specific gender, color,
    or race as ideally, these concepts are not important as regards the model’s decision.
    High concept importance for these concepts indicates the presence of bias.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AI 中透明度和公平性的估计**：对黑箱 AI 模型的大多数监管担忧与性别、肤色和种族等概念有关。基于概念的解释实际上可以帮助估计一个 AI 算法在这些抽象概念方面是否公平。检测
    AI 模型的偏见实际上可以提高它们的透明度，并帮助解决某些监管问题。例如，在医生使用深度学习模型的情况下，TCAV 可以用于检测模型是否对特定性别、肤色或种族有偏见，因为理想情况下，这些概念对模型的决策不应重要。对这些概念的高概念重要性表示存在偏见。'
- en: '![](../Images/bc81db0624102425c9a529d6b920eb18.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc81db0624102425c9a529d6b920eb18.png)'
- en: TCAV can be used to detect model bias based on concept importance (Image by
    author)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: TCAV 可以用于根据概念重要性检测模型偏见（图片由作者提供）
- en: '**Detection of adversarial attacks with CAV**: If you go through the appendix
    of the TCAV research paper ([https://arxiv.org/pdf/1711.11279.pdf](https://arxiv.org/pdf/1711.11279.pdf)),
    the authors have mentioned that the concept importance of actual samples and adversarial
    samples are quite different. This means that if an image gets impacted by an adversarial
    attack, the concept importance would also change. So, CAVs can be a potential
    method for detecting adversarial attacks.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用 CAV 进行对抗攻击检测**：如果你查看 TCAV 研究论文的附录 ([https://arxiv.org/pdf/1711.11279.pdf](https://arxiv.org/pdf/1711.11279.pdf))，作者提到实际样本和对抗样本的概念重要性是相当不同的。这意味着如果图像受到对抗攻击的影响，概念重要性也会改变。因此，CAVs
    可以成为检测对抗攻击的潜在方法。'
- en: '**Concept-based image clustering**: Using CAVs to cluster images based on similar
    concepts can be an interesting application. Deep learning-based image search engines
    are a common application in which clustering or similarity algorithms are applied
    to feature vectors to locate similar images. However, these are feature-based
    methods. Similarly, there is a potential to apply concept-based image clustering
    using CAVs.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于概念的图像聚类**：使用 CAVs 根据相似概念对图像进行聚类可以是一个有趣的应用。基于深度学习的图像搜索引擎是一个常见的应用，其中聚类或相似性算法被应用于特征向量，以定位相似的图像。然而，这些是基于特征的方法。同样，使用
    CAVs 应用基于概念的图像聚类也是一种潜在的可能性。'
- en: '**Automated concept-based explanations (ACE)**: *Ghorbani, Amirata*, *James
    Wexler*, *James Zou*, *and Been Kim*, in their research work — *Towards automatic
    concept-based explanations*, mentioned an automated version of TCAV that goes
    through the training images and automatically discovers prominent concepts. This
    is an interesting work, as I think it can have an important application in identifying
    incorrectly labeled training data. In industrial applications, getting a perfectly
    labeled curated dataset is extremely challenging. This problem can be solved to
    a great extent using ACE.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动化基于概念的解释 (ACE)**：*Ghorbani, Amirata*、*James Wexler*、*James Zou* 和 *Been
    Kim* 在他们的研究工作 *面向自动化基于概念的解释* 中提到了一种自动化版本的 TCAV，该版本会遍历训练图像并自动发现显著的概念。这项工作很有趣，因为我认为它在识别标记错误的训练数据方面可能有重要应用。在工业应用中，获得完美标记的策划数据集是极具挑战性的。这个问题可以通过
    ACE 很大程度上解决。'
- en: '![](../Images/202b85a7ad2b7941c6704787b028d337.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/202b85a7ad2b7941c6704787b028d337.png)'
- en: Source — [*Towards Automatic Concept-based Explanations*, **Ghorbani et al.**](https://arxiv.org/abs/1902.03129)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 来源 — [*面向自动化基于概念的解释*, **Ghorbani 等**](https://arxiv.org/abs/1902.03129)
- en: '**Concept-based Counterfactual Explanation**: Another important XAI method
    is **counterfactual explanation (CFE)** which can be a mechanism for generating
    actionable insights by suggesting changes to the input features that can change
    the overall outcome. CFE provides minimum feature values required to flip the
    predicted class. However, CFE is a feature-based explanation method. It would
    be a really interesting topic of research to have a concept-based counterfactual
    explanation, which is one step closer to human-friendly explanations. There is
    no existing algorithm or framework that can help us achieve this, yet this can
    be a useful application of concept-based approaches in computer vision.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于概念的反事实解释**：另一种重要的 XAI 方法是 **反事实解释 (CFE)**，它可以通过建议对输入特征进行更改，从而改变整体结果，来生成可操作的见解。CFE
    提供了翻转预测类别所需的最小特征值。然而，CFE 是一种基于特征的解释方法。研究一个基于概念的反事实解释将是一个非常有趣的课题，它离人类友好的解释更近了一步。目前还没有现成的算法或框架能够帮助我们实现这一点，但这可能是计算机视觉中基于概念的方法的一个有用应用。'
- en: '![](../Images/a8c576c7b1f91da3d2cf2a264cf0ea1a.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8c576c7b1f91da3d2cf2a264cf0ea1a.png)'
- en: Concept-based counterfactual examples (Image by author)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 基于概念的反事实示例（作者提供的图片）
- en: I feel this is a wide-open research field and the potential to come up with
    game-changing applications using concept-based explanations is immense. I do sincerely
    hope that more and more researchers and AI developers start working on this area
    to make significant progress in the coming years! Thus, we have arrived at the
    end of this article.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我觉得这是一个广阔的研究领域，通过基于概念的解释提出颠覆性的应用的潜力巨大。我真诚地希望越来越多的研究人员和 AI 开发者开始关注这一领域，以在未来几年取得重大进展！因此，我们已到达本文的结尾。
- en: '[](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)
    [## Applied Machine Learning Explainability Techniques: Make ML models explainable
    and trustworthy for…'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)
    [## 应用机器学习可解释性技术：使 ML 模型在实践中可解释和可信…]'
- en: 'Applied Machine Learning Explainability Techniques: Make ML models explainable
    and trustworthy for practical…'
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用机器学习可解释性技术：使 ML 模型在实践中可解释和可信…
- en: www.amazon.com](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: www.amazon.com](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)
- en: Summary
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'This article covers the concepts of TCAV, a novel approach, and a framework
    developed by Google AI. You have received a conceptual understanding of TCAV,
    learned about some key advantages and limitations of TCAV, and finally, we discussed
    some interesting ideas regarding potential research problems that can be solved
    using concept-based explanations. I recommend reading this book: **“**[**Applied
    Machine Learning Explainability Techniques**](https://amzn.to/3cY4c2h)**”** and
    exploring the [GitHub repository](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques)
    for getting hands-on code examples.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 本文涵盖了 TCAV 的概念，一种新颖的方法，以及 Google AI 开发的框架。你已经获得了对 TCAV 的概念性理解，了解了 TCAV 的一些关键优势和局限性，最后，我们讨论了一些关于使用基于概念的解释解决潜在研究问题的有趣想法。我推荐阅读这本书：**“**[**应用机器学习可解释性技术**](https://amzn.to/3cY4c2h)**”**，并探索
    [GitHub 仓库](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques)
    以获取实际代码示例。
- en: 'OTHER XAI RELATED ARTICLES ON TDS BY THE AUTHOR:'
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 作者在 TDS 上的其他 XAI 相关文章：
- en: '[Essential Explainable AI Python frameworks that you should know about](/essential-explainable-ai-python-frameworks-that-you-should-know-about-84d5063b75e9)'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[你应该了解的关键可解释 AI Python 框架](/essential-explainable-ai-python-frameworks-that-you-should-know-about-84d5063b75e9)'
- en: '[Explainable Machine Learning for Models Trained on Text Data: Combining SHAP
    with Transformer Models](/explainable-machine-learning-for-models-trained-on-text-data-combining-shap-with-transformer-5095ea7f3a8)'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[解释性机器学习在文本数据训练模型中的应用：结合 SHAP 与 Transformer 模型](/explainable-machine-learning-for-models-trained-on-text-data-combining-shap-with-transformer-5095ea7f3a8)'
- en: '[EUCA — An effective XAI framework to bring artificial intelligence closer
    to end-users](/euca-an-effective-xai-framework-to-bring-artificial-intelligence-closer-to-end-users-74bb0136ffb1)'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[EUCA — 一种有效的 XAI 框架，使人工智能更接近终端用户](/euca-an-effective-xai-framework-to-bring-artificial-intelligence-closer-to-end-users-74bb0136ffb1)'
- en: '[Understand the Workings of SHAP and Shapley Values Used in Explainable AI](/understand-the-working-of-shap-based-on-shapley-values-used-in-xai-in-the-most-simple-way-d61e4947aa4e)'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[了解 SHAP 和用于可解释 AI 的 Shapley 值的工作原理](/understand-the-working-of-shap-based-on-shapley-values-used-in-xai-in-the-most-simple-way-d61e4947aa4e)'
- en: '[How to Explain Image Classifiers Using LIME](/how-to-explain-image-classifiers-using-lime-e364097335b4)'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[如何使用 LIME 解释图像分类器](/how-to-explain-image-classifiers-using-lime-e364097335b4)'
- en: '[](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)
    [## Applied Machine Learning Explainability Techniques: Make ML models explainable
    and trustworthy for…'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)
    [## 应用机器学习可解释性技术：让机器学习模型变得可解释和可信…](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)'
- en: 'Applied Machine Learning Explainability Techniques: Make ML models explainable
    and trustworthy for practical…'
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用机器学习可解释性技术：让机器学习模型在实际应用中变得可解释和可信…
- en: www.amazon.com](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[www.amazon.com](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154?_encoding=UTF8&pd_rd_w=Wr6SJ&content-id=amzn1.sym.716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_p=716a1ed9-074f-4780-9325-0019fece3c64&pf_rd_r=6P2PM599T97MRG7NZD9J&pd_rd_wg=m4qUW&pd_rd_r=6e349d93-5ba0-4bfe-9055-905c0153fe58&linkCode=li3&tag=adib0073-20&linkId=35506e1847de5c011fc57aa66c2b1d8e&language=en_US&ref_=as_li_ss_il&source=post_page-----5408adf905e--------------------------------)'
- en: REFERENCE
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[Applied Machine Learning Explainability Techniques](https://amzn.to/3cY4c2h)'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[应用机器学习解释性技术](https://amzn.to/3cY4c2h)'
- en: GitHub repo from the book Applied Machine Learning Explainability Techniques
    — [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/)
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 书籍《应用机器学习解释性技术》的 GitHub 仓库 — [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/)
- en: 'Interpretability Beyond Feature Attribution: Quantitative Testing with Concept
    Activation Vectors (TCAV): [https://arxiv.org/pdf/1711.11279.pdf](https://arxiv.org/pdf/1711.11279.pdf)'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征归因之外的可解释性：使用概念激活向量（TCAV）进行定量测试：[https://arxiv.org/pdf/1711.11279.pdf](https://arxiv.org/pdf/1711.11279.pdf)
- en: TCAV Python framework — [https://github.com/tensorflow/tcav](https://github.com/tensorflow/tcav)
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TCAV Python 框架 — [https://github.com/tensorflow/tcav](https://github.com/tensorflow/tcav)
- en: 'Koh et al. “Concept Bottleneck Models”: [https://arxiv.org/abs/2007.04612](https://arxiv.org/abs/2007.04612)'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Koh 等人，《概念瓶颈模型》：[https://arxiv.org/abs/2007.04612](https://arxiv.org/abs/2007.04612)
- en: 'Guillaume Alain and Yoshua Bengio, “Understanding intermediate layers using
    linear classifier probes”: [https://arxiv.org/abs/1610.01644](https://arxiv.org/abs/1610.01644)'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Guillaume Alain 和 Yoshua Bengio，《使用线性分类器探针理解中间层》：[https://arxiv.org/abs/1610.01644](https://arxiv.org/abs/1610.01644)
- en: 'Ghorbani, Amirata, James Wexler, James Zou and Been Kim, “Towards automatic
    concept-based explanations”: [https://arxiv.org/abs/1902.03129](https://arxiv.org/abs/1902.03129)'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ghorbani, Amirata, James Wexler, James Zou 和 Been Kim，《朝向自动化概念基础解释》：[https://arxiv.org/abs/1902.03129](https://arxiv.org/abs/1902.03129)
- en: 'Detecting Concepts, Chapter 10.3 Molnar, C. (2022). Interpretable Machine Learning:
    A Guide for Making Black Box Models Explainable (2nd ed.).: [https://christophm.github.io/interpretable-ml-book/detecting-concepts.html](https://christophm.github.io/interpretable-ml-book/detecting-concepts.html)'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 章节 10.3 检测概念，Molnar, C.（2022）。《可解释机器学习：黑箱模型可解释性的指南（第2版）》：[https://christophm.github.io/interpretable-ml-book/detecting-concepts.html](https://christophm.github.io/interpretable-ml-book/detecting-concepts.html)
