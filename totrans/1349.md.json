["```py\nfrom pyspark.sql.functions import col\nfrom pyspark.ml.feature import UnivariateFeatureSelector\nfrom pyspark.ml.feature import RFormula\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoder\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n```", "```py\n# Pulling a dataset\nfrom pyspark.sql.types import DoubleType, StringType, StructField, StructType\n\nschema = StructType([\n  StructField(\"age\", DoubleType(), False),\n  StructField(\"workclass\", StringType(), False),\n  StructField(\"fnlwgt\", DoubleType(), False),\n  StructField(\"education\", StringType(), False),\n  StructField(\"education_num\", DoubleType(), False),\n  StructField(\"marital_status\", StringType(), False),\n  StructField(\"occupation\", StringType(), False),\n  StructField(\"relationship\", StringType(), False),\n  StructField(\"race\", StringType(), False),\n  StructField(\"sex\", StringType(), False),\n  StructField(\"capital_gain\", DoubleType(), False),\n  StructField(\"capital_loss\", DoubleType(), False),\n  StructField(\"hours_per_week\", DoubleType(), False),\n  StructField(\"native_country\", StringType(), False),\n  StructField(\"income\", StringType(), False)\n])\n\nadults = spark.read.format(\"csv\").schema(schema).load(\"/databricks-datasets/adult/adult.data\")\n```", "```py\nadults = (\n  adults\n  .filter( (col('workclass') != ' ?') &\n          (col('education') != ' ?') &\n          (col('marital_status') != ' ?') &\n          (col('occupation') != ' ?') &\n          (col('relationship') != ' ?') &\n          (col('race') != ' ?') &\n          (col('sex') != ' ?') &\n          (col('native_country') != ' ?')\n          )\n)\n```", "```py\n# Adults cat only\nadults_cat = adults.select('workclass', 'education', 'marital_status', 'occupation', 'relationship','race', 'sex','native_country', 'income')\n\n# Creating a vector out of our dataset to be able to use Univariate Feature Selector\nformula=RFormula(formula= \"income ~ workclass + education + marital_status + occupation + relationship + race + sex + native_country\", \n                 featuresCol= \"features\", labelCol= \"label\")\nvector_df = formula.fit(adults_cat).transform(adults_cat)\n```", "```py\n# instance the selector\nselector = UnivariateFeatureSelector(featuresCol='features', \n                                     outputCol=\"selectedFeatures\", \n                                     labelCol= 'label')\n# Set feature and label types and define how many K variables to fetch\nselector.setFeatureType(\"categorical\").setLabelType(\"categorical\").setSelectionThreshold(4)\nmodel = selector.fit(vector_df)\n\n# selectedFeatures\nprint('Selected Features - Categorical')\nprint([name for i,name in enumerate(vector_df.columns) if i in model.selectedFeatures])\n\n-----\n[OUT]\nSelected Features - Categorical\n['workclass', 'relationship', 'race', 'sex']\n```", "```py\n# Adults numerical only\nadults_num = adults.select('age', 'fnlwgt', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week', 'income')\n\n# Creating a vector out of our dataset to be able to use Univariate Feature Selector\nformula=RFormula(formula= \"income ~ age + fnlwgt + education_num + capital_gain + capital_loss + hours_per_week\", featuresCol= \"features\", labelCol= \"label\")\nvector_df = formula.fit(adults_num).transform(adults_num)\n\n# # Using Variable selector for Feature Engineering (num)\nselector = UnivariateFeatureSelector(featuresCol='features', outputCol=\"selectedFeatures\", labelCol= 'label')\nselector.setFeatureType(\"continuous\").setLabelType(\"categorical\").setSelectionThreshold(4)\nmodel = selector.fit(vector_df)\n#model.selectedFeatures\nprint('Selected Features - Numerical')\nprint([name for i,name in enumerate(vector_df.columns) if i in model.selectedFeatures])\n\n-----\n[OUT]\nSelected Features - Numerical\n['age', 'education_num', 'capital_gain', 'capital_loss']\n```", "```py\ndf_sel = adults.select('workclass', 'occupation', 'race', 'sex', 'age', 'education_num', 'capital_gain', 'capital_loss', 'income')\n```", "```py\n# Columns to transform\ncat_cols = ['workclass', 'occupation', 'race', 'sex']\n\n# List of stages for Pipeline\nstages = []\n\nfor column in cat_cols:\n    # Instance encoding with StringIndexer\n    stringIndexer = StringIndexer(inputCol=column, outputCol=column + \"Index\")\n    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[column + \"classVec\"])\n    # Add stages.  These are not run here, but will run all at once later on.\n    stages += [stringIndexer, encoder]\n```", "```py\n# Convert label income into label indices using the StringIndexer\nlabel_encode = StringIndexer(inputCol=\"income\", outputCol=\"label\")\n\n# Add to the Pipeline stages\nstages += [label_encode]\n```", "```py\n# Transform all features into a vector using VectorAssembler\nnum_cols = ['age', 'education_num', 'capital_gain', 'capital_loss']\nassembler_cols = [c + \"classVec\" for c in cat_cols] + num_cols\nassembler = VectorAssembler(inputCols=assembler_cols, outputCol=\"features\")\nstages += [assembler]\n```", "```py\npipe = Pipeline().setStages(stages)\npipe_model = pipe.fit(df_sel)\nprepared_df = pipe_model.transform(df_sel)\n```", "```py\n# Split data into training and test sets\n(train, test) = prepared_df.randomSplit([0.7, 0.3], seed=42)\nprint(train.count())\nprint(test.count())\n```", "```py\n# Fit model to prepped data\nlrModel = LogisticRegression(labelCol= 'label', \n                             featuresCol='features',\n                             maxIter= 10).fit(train)\n```", "```py\n# predictions\npredictions = lrModel.transform(test)\npreds = predictions.select(\"label\", \"prediction\", \"probability\")\n```", "```py\n# Evaluate model\nevaluator = BinaryClassificationEvaluator()\nevaluator.evaluate(predictions)\n\nOut[189]: 0.8526180736758168\n```", "```py\n# Confusion Matrix\ndisplay(\n  preds.crosstab('label', 'prediction')\n)\n```", "```py\nprint(f'Precision: {1075/(1075+438)}')\nprint(f'Recall: {1075/(1075+1151)}')\nprint(f'Specificity: {6324/(6324+438)}')\n\nPrecision: 0.7105089226701917\nRecall: 0.4829290206648697\nSpecificity: 0.935226264418811\n```", "```py\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\nlogit = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n\n# Create ParamGrid for Cross Validation\nparamGrid = (ParamGridBuilder()\n             .addGrid(logit.regParam, [0.01, 0.5, 2.0])\n             .addGrid(logit.threshold, [0.35, 0.38])\n             .build())\n```", "```py\n# Create 5-fold CrossValidator\ncv = CrossValidator(estimator=logit, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n\n# Run cross validations\ncv_model = cv.fit(train)\n\nprint('cutoff:', cv_model.bestModel.getThreshold())\nprint('regParam:', cv_model.bestModel.getRegParam())\n\n[OUT]\ncutoff: 0.38\nregParam: 0.01\n```", "```py\nlogit = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", \n                           threshold=0.38, maxIter=100)\nlog_fit = logit.fit(train)\n\n# Make predictions on test data using the transform() method.\npreds2 = log_fit.transform(test)\n\n# Confusion Matrix\ndisplay(\n  preds2.crosstab('label', 'prediction')\n)\n```", "```py\ndisplay(log_fit, test, 'ROC')\n```"]