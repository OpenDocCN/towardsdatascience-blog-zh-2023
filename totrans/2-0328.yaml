- en: Are You Still Using the Elbow Method?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/are-you-still-using-the-elbow-method-5d271b3063bd](https://towardsdatascience.com/are-you-still-using-the-elbow-method-5d271b3063bd)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Elbow method is the most popular way to find the number of clusters for
    k-means. But there are much better alternatives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mazzanti.sam?source=post_page-----5d271b3063bd--------------------------------)[![Samuele
    Mazzanti](../Images/432477d6418a3f79bf25dec42755d364.png)](https://medium.com/@mazzanti.sam?source=post_page-----5d271b3063bd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5d271b3063bd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5d271b3063bd--------------------------------)
    [Samuele Mazzanti](https://medium.com/@mazzanti.sam?source=post_page-----5d271b3063bd--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5d271b3063bd--------------------------------)
    ·7 min read·Feb 3, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fbbd66b6178605b4c0396fa820bbdeb2.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Image by Author]'
  prefs: []
  type: TYPE_NORMAL
- en: 'I asked ChatGPT to advise me on how to choose the right number of clusters
    for *k*-means. This was the answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d2ed1bb5798d85dd0b8771d5e9c3196f.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Screenshot from ChatGPT: [https://chat.openai.com/chat](https://chat.openai.com/chat)]'
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT suggests using the so-called “Elbow method”, which is by far the most
    cited method in many online and offline sources.
  prefs: []
  type: TYPE_NORMAL
- en: However, **the popularity of the Elbow method is pretty inexplicable**! In fact,
    as we will see in this article, this method is almost always outperformed by different
    existing approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Read on if you want to know how to easily beat the Elbow method when deciding
    the best number of clusters for a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the elbow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The logic behind the Elbow method is the following.
  prefs: []
  type: TYPE_NORMAL
- en: Since we want to know what is the best number of clusters (*k*), we try different
    values for *k*, for instance, all the integer values between 1 and the square
    root of the number of observations of the dataset. At each iteration, we record
    the so-called “inertia”.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Inertia is the sum of squared distances between each point and the center of
    the cluster it belongs to. Thus, it makes sense to expect that inertia gets smaller
    as *k* increases. In fact, as there are more clusters, each cluster is smaller,
    so each point is closer to the center of its cluster. Eventually, when *k* equals
    the number of data points, inertia is necessarily zero.
  prefs: []
  type: TYPE_NORMAL
- en: The idea is that the best value for *k* is the point of maximum curvature of
    the inertia curve. This is the point where a lower value of inertia is not worth
    the additional complexity (i.e. more clusters). This point is called **the elbow
    of the curve**.
  prefs: []
  type: TYPE_NORMAL
- en: 'To locate the elbow of the inertia curve we can use a Python library called
    `kneed`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see the example of a two-dimensional dataset made of 3 clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/988d391ac64f9fc838bf1a1643b2680c.png)'
  prefs: []
  type: TYPE_IMG
- en: The elbow method on a toy dataset. [Image by Author]
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the Elbow method guessed the right number of clusters: 3\. But
    will it work also on other datasets?'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b65c0c71aea043d31c3025a9c5ad7558.png)'
  prefs: []
  type: TYPE_IMG
- en: Equally sized clusters. [Image by Author]
  prefs: []
  type: TYPE_NORMAL
- en: The Elbow method guessed the correct number of clusters in three datasets (2,
    3, and 5), but it severely underestimated it in the other two examples (10 and
    25).
  prefs: []
  type: TYPE_NORMAL
- en: Ok, the Elbow method probably didn’t work as we hoped. But is there something
    better on the market?
  prefs: []
  type: TYPE_NORMAL
- en: Beyond the Elbow method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Elbow method is based on inertia, which is a score of the goodness of fit
    of clusters. But if we want to use a different method, we will need to use a different
    score.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at the options that are directly available in Scikit-Learn.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9c3ca009425b3b4a073070bcd215e413.png)'
  prefs: []
  type: TYPE_IMG
- en: List of clustering metrics available in Scikit-Learn. [Screenshot from [Scikit-Learn
    documentation](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)]
  prefs: []
  type: TYPE_NORMAL
- en: Actually, many of these metrics won’t work out for us, because they require
    `labels_true` as an input, and we want to test these methods in the scenario in
    which we don’t have the true labels.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, out of this list, we could only use the metrics that take `X` and `labels`
    as input. Thus, only “Calinski-Harabasz”, “Davies-Bouldin” and “Silhouette” will
    work for us.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides these, I will add another score that is not implemented in Scikit-Learn.
    The score is “BIC” (Bayesian Information Criterion), and I will include it because
    [this paper](https://arxiv.org/abs/2212.12189) shows that it performs really well.
    For BIC, I will use the implementation made by Bob Hancock in [this GitHub repository](https://github.com/bobhancock/goxmeans/tree/a78e909e374c6f97ddd04a239658c7c5b7365e5c).
    Since the code is in GoLang, I have translated it to the following Python function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'To sum up, we now have five scores to compare:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Inertia (elbow method)**;'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Calinski-Harabasz**;'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Davies-Bouldin**;'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Silhouette**;'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**BIC**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The procedure is the same: for each dataset, we fit a *k*-means using *k* =
    1, *k* = 2, *k* = 3, … For each *k*, we calculate these five scores. This means
    that we end up with five curves, one for each metric.'
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, how do we decide what is the optimal *k*? For inertia we already
    know the answer: it’s the point that lies in the “elbow”. The other scores are
    even simpler to use because their behaviour is either “higher-is-better” or “lower-is-better”:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Inertia** → **elbow is better**;'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Calinski-Harabasz** → **higher is better**;'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Davies-Bouldin** → **lower is better**;'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Silhouette** → **higher is better**;'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**BIC** → **higher is better**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So let’s see how these scores would work on our five datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/df9b5d33c479479d7d64ec9d916e8433.png)'
  prefs: []
  type: TYPE_IMG
- en: Five datasets and the respective curves. [Image by Author]
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have seen, the Elbow method guessed correctly three datasets and missed
    completely the target in the remaining two. Instead, **the other methods guessed
    correctly all the datasets** (except for Davies-Bouldin on the fifth dataset,
    which anyway is very close to the real number: 24 instead of 25).'
  prefs: []
  type: TYPE_NORMAL
- en: 'But maybe these datasets were too easy. In fact, all the datasets were made
    of equally sized clusters. This is an unlikely situation: **in most real datasets,
    we expect the clusters to have a different number of observations**.'
  prefs: []
  type: TYPE_NORMAL
- en: So let’s see what happens with clusters of unequal size.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2a33502a2f71db975470ace06a54f2c4.png)'
  prefs: []
  type: TYPE_IMG
- en: Five datasets with clusters of different sizes. [Image by Author]
  prefs: []
  type: TYPE_NORMAL
- en: This time, the difference between the Elbow method and the other methods is
    even more evident than before. Indeed, whereas the Elbow method made the right
    choice only for one cluster, the other four methods guessed correctly all five
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: These examples are interesting but they still don’t tell us which method is
    preferable. In order to answer that question we should make a larger-scale comparison.
  prefs: []
  type: TYPE_NORMAL
- en: A systematic comparison
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I built 30 datasets, each one with a different number of clusters, from 1 to
    30\. Each cluster has a different number of observations.
  prefs: []
  type: TYPE_NORMAL
- en: For each dataset, I tried different values of *k* and recorded the five resulting
    scores (Inertia, Calinski-Harabasz, Davies-Bouldin, Silhouette, and BIC). Then,
    according to the rules we have seen above, I have identified the value of *k*
    recommended by each method.
  prefs: []
  type: TYPE_NORMAL
- en: To decide which method is the winner, I then plotted the true number of clusters
    of each dataset (*x*-axis) and the number of clusters suggested by each method
    (*y*-axis). Clearly, the best method is the one that gets closer to the bisector.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ead232e3735aecdbdd502528a494619d.png)'
  prefs: []
  type: TYPE_IMG
- en: True number of clusters (x-axis) and number of clusters estimated by each method
    (y-axis). [Image by Author]
  prefs: []
  type: TYPE_NORMAL
- en: From the plot, it is clear that **the Elbow method is by far the worst-performing**.
    Davies-Bouldin and Silhouette are right most of the time, or at least very close
    to the ground truth (except when the true *k* equals 1). But **the clear winners
    (on par) are Calinski-Harabasz and BIC**.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also summarize these visual insights by counting **how many times a
    method guessed the true number of clusters (accuracy) and how far off they were,
    on average, from the ground truth (average distance)**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b259717a67fcc3c495f44663a9d3d99b.png)'
  prefs: []
  type: TYPE_IMG
- en: Accuracy and average distance from the ground truth. [Image by Author]
  prefs: []
  type: TYPE_NORMAL
- en: Calinski-Harabasz and BIC were right in 97% of the cases (29 out of 30), which
    is pretty impressive. Silhouette was right 73% of the time (22 out of 30) and
    Davies-Bouldin 70% (21 out of 30). The Elbow method is pretty far from the others,
    at only 13% (4 out of 30).
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we have seen that, despite its popularity, **the Elbow method
    is pretty much the worst choice one can do when setting the number of clusters
    for a dataset**. Indeed, all the four alternatives that we tested did much better
    than the Elbow method. In particular, Calinski-Harabasz and BIC performed extremely
    well, with only one mistake out of 30 datasets.
  prefs: []
  type: TYPE_NORMAL
- en: You can find all the Python code used for this article in [this notebook](https://github.com/smazzanti/are_you_still_using_elbow_method/blob/main/are-you-still-using-elbow-method.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: '*Thank you for reading!*'
  prefs: []
  type: TYPE_NORMAL
- en: '*If you find my work useful, you can subscribe to* [***get an email every time
    that I publish a new article***](https://medium.com/@mazzanti.sam/subscribe) *(usually
    once a month).*'
  prefs: []
  type: TYPE_NORMAL
- en: '*If you want to support my work, you can* [***buy me a coffee***](https://ko-fi.com/samuelemazzanti)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*If you’d like,* [***add me on Linkedin***](https://www.linkedin.com/in/samuelemazzanti/)*!*'
  prefs: []
  type: TYPE_NORMAL
