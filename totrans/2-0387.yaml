- en: 'Beyond the Warm Embrace: A Deeper Look at Hugging Face'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/beyond-the-warm-embrace-a-deeper-look-at-hugging-face-96b3497788e7](https://towardsdatascience.com/beyond-the-warm-embrace-a-deeper-look-at-hugging-face-96b3497788e7)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Fine tuning language models for Named Entity Recognition
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@raicik.zach?source=post_page-----96b3497788e7--------------------------------)[![Zachary
    Raicik](../Images/860760b53fcc75013007067190e8ca65.png)](https://medium.com/@raicik.zach?source=post_page-----96b3497788e7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----96b3497788e7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----96b3497788e7--------------------------------)
    [Zachary Raicik](https://medium.com/@raicik.zach?source=post_page-----96b3497788e7--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----96b3497788e7--------------------------------)
    ·9 min read·Nov 3, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ea850ddee92eada7c048b2f96261e036.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: Photo by [Choong Deng Xiang](https://unsplash.com/@dengxiangs?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 'Hugging Face is a platform that offers tools and pre-trained models for various
    Natural Language Processing (NLP) and Natural Language Understanding (NLU) tasks.
    In our previous article, [A Warm Embrace: Exploring Hugging Face](https://medium.com/@raicik.zach/a-warm-embrace-exploring-hugging-face-210c7f4a9078),
    we dove into the basics of this platform and its open-source library that features
    implementations of many state-of-the-art transformer architectures. This post
    enhances the Hugging Face documentation by providing emerging data scientists
    with a single, connected view of various Hugging Face tools for a specific task.
    Specifically, this article explains how to piece together multiple Hugging Face
    capabilities to fine-tune an existing language model for named entity recognition
    (“NER”).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Relevant Background
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we briefly look at two foundational concepts essential for
    building our model. As a reminder, we covered Hugging Face basics in [A Warm Embrace:
    Exploring Hugging Face](https://medium.com/@raicik.zach/a-warm-embrace-exploring-hugging-face-210c7f4a9078).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Named Entity Recognition
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model Fine-tuning
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the sections below, it’s assumed you have some knowledge of model development
    and the associated concepts — however, if anything is unclear feel free to reach
    out!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Named Entity Recognition
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Named Entity Recognition (“NER”) is a common natural language processing task
    of identifying and categorizing relevant information, or entities, into one of
    many predefined (named) groups. NER models can be trained on a variety of entities.
    Some of the most common ones are:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Names
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Organizations
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dates
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Places & Locations
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the image below, I manually tagged a couple of different named entities in
    a sample sentence. In the context of machine learning and NLP, NER is the process
    of automating this categorization process through models.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5c1045e5fc606e6d1f3ba22cc4e0cc77.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: NER models can enable a variety of tasks including but not limited to, information
    retrieval, content summarization, content recommendation and machine translation.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Model Fine-Tuning
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the highest level, fine tuning a model refers to adjusting existing model
    weights based on a new dataset by replacing some or all of the model’s layers
    and retraining. Depending on your task and dataset, you might retrain just the
    last layer, some layers of the model, or the entire model.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/04f1b91b24dd65f050e51fabed9ee704.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
- en: Image by Author; inspired by [http://d2l.ai/chapter_computer-vision/fine-tuning.html](http://d2l.ai/chapter_computer-vision/fine-tuning.html)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: One might wonder—why not just train your own model from scratch?
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Creating a new model often demands substantial computational resources and time.
    Utilizing a pre-trained model allows us to harness the capabilities of a model
    trained on extensive data without the hefty computational and time investments.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since pre-trained models are often trained on large and comprehensive datasets,
    fine-tuning a model allows you to achieve strong performance on a smaller dataset,
    thereby minimizing the risk of overfitting and improving generalization, among
    other benefits.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing our NER Model
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dataset we will be using is called [Broad Twitter Corpus](https://huggingface.co/datasets/GateNLP/broad_twitter_corpus)
    (clear for commercial use under the *Creative Commons Attribution 4.0 International*
    license). The dataset itself is just a huge collection of tweets. Each of these
    tweets has been annotated so that they include named entity tags. More importantly,
    according to the [white paper](https://aclanthology.org/C16-1111.pdf), these tweets
    were **manually** annotated. Building an NER model on top of this dataset will
    not only enable us to automatically annotate the entities in the future, but it
    will also enable some of the downstream use cases we described in a previous section.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: A rough outline of the process we will follow is below.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Set up our environment
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load [Broad Twitter dataset](https://huggingface.co/datasets/GateNLP/broad_twitter_corpus)
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load pre-trained [BERT](https://huggingface.co/bert-base-cased) model
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Re-tokenize Broad Twitter tokens
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fine tune pre-trained BERT model
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Environment Setup
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For simplicity, I did this work in a Google Colab [notebook](https://colab.research.google.com/drive/1JfuURjpI_lB12fiR2IlZYs9NdEKIGQR9?usp=sharing).
    Google Colab provides free GPU access by going to runtime -> change runtime type
    and selecting T4 GPU. As an aside, this code can be run in many different environments
    — not just Colab.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: The first thing we need to do is install the required Hugging Face packages.
    I copied a brief description of each from the Hugging Face documentation, which
    is linked to each package name.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[**Datasets**](https://huggingface.co/docs/datasets/index)**: “**Datasets is
    a library for easily accessing and sharing datasets for Audio, Computer Vision,
    and Natural Language Processing (NLP) tasks”'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Transformers**](https://huggingface.co/docs/transformers/index)**: “**Transformers
    provides APIs and tools to easily download and train state-of-the-art pre-trained
    models”'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Evaluate**](https://huggingface.co/docs/evaluate/index)**: “**A library
    for easily evaluating machine learning models and datasets”'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These packages can be installed using PIP.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Loading Broad Twitter dataset
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hugging Face’s datasets library makes it extremely easy to load datasets using
    a two lines of Python code.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: When you load a dataset, it loads all relevant data splits contained within
    the dataset. Printing the dataset will show you the available splits, number of
    rows for each split, and the features each row has. The cell below shows the results
    of `print(twitter)`.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Loading BERT
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When loading BERT for token classification, we need to specify the number of
    labels. This dataset has been annotated with [IOB2](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging))
    labels. According to the HF dataset page, mappings between numeric tag and string
    tag are as follows.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: If you are unfamiliar with IOB2 labels, one might wonder what is the difference
    between `B-PER` and `I-PER`? What about the difference between `B-ORG` and `I-ORG`
    , or the difference between `B-LOC` and `I-LOC`? The answer lies in tokenization.
    Specifically, some entities might span over multiple tokens. We use these entity
    prefixes to tell the model where the entity begins and how far it continues. The
    `B` prefix before a label indicates that the token is the beginning of a chunk
    of that type, and the `I` prefix indicates that the token is inside a chunk. The
    `O` label means that a token is outside of any chunk.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know we have 7 labels, we can load BERT using the following lines
    of code.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Lastly, we need to tell the model how to map between ID and label.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We have BERT loaded, but do we even need to fine tune BERT for our NER task?
    When BERT was introduced by Google it was considered groundbreaking. Can’t we
    use it as-is for NER without fine-tuning? We can test its ability to classify
    tokens without any fine tuning below. We’ll use our example sentence from earlier,
    which closely resembles a tweet (short).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The results may surprise you. It turns out that without any fine tuning, BERT
    isn’t very good at our task. In the code block below, you can see that BERT labeled
    `Zachary Raicik works for Corvus and lives in San` as one entity and `Diego` as
    another.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Re-tokenize Broad Twitter tokens
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we downloaded the Broad Twitter dataset, it came with a set of predefined
    tokens. However, there is no guarantee that the tokens in the dataset will match
    the ones generated by the BERT tokenizer. BERT may split up some tokens into sub-word
    tokens. Consequently, we need to build a function to re-distribute the provided
    tags to the sub-word tokens generated by BERT.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Hugging Face expects the fields `input_ids`, `attention_mask`, `token_type_ids`
    and `labels` for training. See [here](https://huggingface.co/docs/transformers/v4.34.1/en/preprocessing#natural-language-processing)
    for more information about processing your data for use with the `Transformers`
    library.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: We can use this function to re-tokenize our datasets.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Fine tune pre-trained BERT model
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At this point, our datasets are ready to go and we are ready to begin training.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: When fine tuning a pre-trained model, you can choose to retrain as many layers
    as you want. Since this is just an illustrative exercise, we will only tune the
    last layer of BERT.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We use `TrainingArguments` to specify things like the learning rate, number
    of epochs, etc.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Loss functions are designed for optimizing model weights during training but
    not for interpretation. For that reason, we will include additional metrics to
    understand how good our model is.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: In NER tasks, class imbalance is relatively common. For that reason, metrics
    like accuracy may not be the most appropriate. In this case, the class imbalance
    isn’t awful (it’s shown in the dataset description). However, for completeness,
    we will use the weighted F1 score to evaluate our model during training. The weighted
    F1 score takes into account the number of true instances for each label when calculating
    the F1 score for that label. This means that each class contributes to the average
    proportionally to its size.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We have all the pieces required to train.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Once the training process completes, you should get a view that looks something
    like this.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1d4be0d8640699f2865348c4d9a4922c.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: In a realistic setting, you might want to increase the number of epochs and
    invest more time in choosing the right parameters for the task. Some examples
    of this may include
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '**Learning Rate Optimization:** The learning rate controls how much to update
    the model’s weights. We use `5e-05` in our example, but it’s possible a different
    learning rate is more appropriate for this task.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weight decay:** This is a regularization technique that discourages large
    weights. In general, it leads to a simpler model and helps to prevent overfitting.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Putting it All Together
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s revisit our sentence from earlier.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The results indicate that our fine tuned model is much better compared to BERT
    without any fine tuning.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In this article we covered the basics of fine tuning an existing model for a
    specific task. In our case, we used BERT to build a named entity recognition model.
    This process can be applied to any number of tasks using different datasets or
    models. Although this post provided a strong introduction to some of what Hugging
    Face can do, we barely scratched the surface. For example, we didn’t even pass
    our model to the robust evaluation library maintained by Hugging Face. In a future
    post, we will cover how to use some of these additional tools for both fine tuning
    a model for NER and additional use cases.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们介绍了如何为特定任务微调现有模型的基本知识。在我们的案例中，我们使用了 BERT 来构建一个命名实体识别模型。这个过程可以应用于任何数量的任务，使用不同的数据集或模型。虽然这篇文章为
    Hugging Face 能做的事情提供了强有力的介绍，但我们仅仅触及了表面。例如，我们甚至没有将我们的模型传递给 Hugging Face 维护的强大评估库。在未来的文章中，我们将介绍如何使用这些附加工具来微调
    NER 模型以及其他用例。
