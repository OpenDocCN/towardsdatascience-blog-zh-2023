- en: 'Beyond the Warm Embrace: A Deeper Look at Hugging Face'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超越温暖的拥抱：深入了解 Hugging Face
- en: 原文：[https://towardsdatascience.com/beyond-the-warm-embrace-a-deeper-look-at-hugging-face-96b3497788e7](https://towardsdatascience.com/beyond-the-warm-embrace-a-deeper-look-at-hugging-face-96b3497788e7)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/beyond-the-warm-embrace-a-deeper-look-at-hugging-face-96b3497788e7](https://towardsdatascience.com/beyond-the-warm-embrace-a-deeper-look-at-hugging-face-96b3497788e7)
- en: Fine tuning language models for Named Entity Recognition
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调语言模型以进行命名实体识别
- en: '[](https://medium.com/@raicik.zach?source=post_page-----96b3497788e7--------------------------------)[![Zachary
    Raicik](../Images/860760b53fcc75013007067190e8ca65.png)](https://medium.com/@raicik.zach?source=post_page-----96b3497788e7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----96b3497788e7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----96b3497788e7--------------------------------)
    [Zachary Raicik](https://medium.com/@raicik.zach?source=post_page-----96b3497788e7--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@raicik.zach?source=post_page-----96b3497788e7--------------------------------)[![Zachary
    Raicik](../Images/860760b53fcc75013007067190e8ca65.png)](https://medium.com/@raicik.zach?source=post_page-----96b3497788e7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----96b3497788e7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----96b3497788e7--------------------------------)
    [Zachary Raicik](https://medium.com/@raicik.zach?source=post_page-----96b3497788e7--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----96b3497788e7--------------------------------)
    ·9 min read·Nov 3, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----96b3497788e7--------------------------------)
    ·阅读时间 9 分钟·2023年11月3日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ea850ddee92eada7c048b2f96261e036.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea850ddee92eada7c048b2f96261e036.png)'
- en: Photo by [Choong Deng Xiang](https://unsplash.com/@dengxiangs?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Choong Deng Xiang](https://unsplash.com/@dengxiangs?utm_source=medium&utm_medium=referral)在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)提供
- en: 'Hugging Face is a platform that offers tools and pre-trained models for various
    Natural Language Processing (NLP) and Natural Language Understanding (NLU) tasks.
    In our previous article, [A Warm Embrace: Exploring Hugging Face](https://medium.com/@raicik.zach/a-warm-embrace-exploring-hugging-face-210c7f4a9078),
    we dove into the basics of this platform and its open-source library that features
    implementations of many state-of-the-art transformer architectures. This post
    enhances the Hugging Face documentation by providing emerging data scientists
    with a single, connected view of various Hugging Face tools for a specific task.
    Specifically, this article explains how to piece together multiple Hugging Face
    capabilities to fine-tune an existing language model for named entity recognition
    (“NER”).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 是一个提供各种自然语言处理（NLP）和自然语言理解（NLU）任务工具和预训练模型的平台。在我们之前的文章[温暖的拥抱：探索 Hugging
    Face](https://medium.com/@raicik.zach/a-warm-embrace-exploring-hugging-face-210c7f4a9078)中，我们深入探讨了这个平台及其开源库，该库实现了许多最先进的变换器架构。本文通过提供对特定任务的
    Hugging Face 工具的统一视图，增强了 Hugging Face 文档，帮助新兴的数据科学家更好地理解如何将多种 Hugging Face 能力结合起来，以微调现有的语言模型进行命名实体识别（“NER”）。
- en: Relevant Background
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相关背景
- en: 'In this section, we briefly look at two foundational concepts essential for
    building our model. As a reminder, we covered Hugging Face basics in [A Warm Embrace:
    Exploring Hugging Face](https://medium.com/@raicik.zach/a-warm-embrace-exploring-hugging-face-210c7f4a9078).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们简要介绍了构建模型所必需的两个基础概念。提醒一下，我们在[温暖的拥抱：探索 Hugging Face](https://medium.com/@raicik.zach/a-warm-embrace-exploring-hugging-face-210c7f4a9078)中涵盖了
    Hugging Face 的基础知识。
- en: Named Entity Recognition
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命名实体识别
- en: Model Fine-tuning
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型微调
- en: In the sections below, it’s assumed you have some knowledge of model development
    and the associated concepts — however, if anything is unclear feel free to reach
    out!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下各节中，假设你对模型开发及相关概念有一定了解——然而，如果有任何不清楚的地方，随时可以联系我们！
- en: Named Entity Recognition
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 命名实体识别
- en: 'Named Entity Recognition (“NER”) is a common natural language processing task
    of identifying and categorizing relevant information, or entities, into one of
    many predefined (named) groups. NER models can be trained on a variety of entities.
    Some of the most common ones are:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 命名实体识别（“NER”）是自然语言处理中的一个常见任务，其目的是识别和分类相关信息或实体，将其归入多个预定义（命名）类别之一。NER 模型可以针对各种实体进行训练。一些最常见的实体包括：
- en: Names
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 姓名
- en: Organizations
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组织
- en: Dates
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日期
- en: Places & Locations
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 地点与位置
- en: In the image below, I manually tagged a couple of different named entities in
    a sample sentence. In the context of machine learning and NLP, NER is the process
    of automating this categorization process through models.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图片中，我手动标记了一个示例句子中的几个不同的命名实体。在机器学习和 NLP 的上下文中，NER 是通过模型自动化这一分类过程的过程。
- en: '![](../Images/5c1045e5fc606e6d1f3ba22cc4e0cc77.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5c1045e5fc606e6d1f3ba22cc4e0cc77.png)'
- en: Image by the author
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者
- en: NER models can enable a variety of tasks including but not limited to, information
    retrieval, content summarization, content recommendation and machine translation.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: NER 模型可以支持多种任务，包括但不限于信息检索、内容摘要、内容推荐和机器翻译。
- en: Model Fine-Tuning
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型微调
- en: At the highest level, fine tuning a model refers to adjusting existing model
    weights based on a new dataset by replacing some or all of the model’s layers
    and retraining. Depending on your task and dataset, you might retrain just the
    last layer, some layers of the model, or the entire model.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在最高层次上，微调模型是指根据新的数据集调整现有模型权重，通过替换模型的一些或全部层并重新训练来实现。根据你的任务和数据集，你可能只需重新训练最后一层、模型的某些层，或整个模型。
- en: '![](../Images/04f1b91b24dd65f050e51fabed9ee704.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04f1b91b24dd65f050e51fabed9ee704.png)'
- en: Image by Author; inspired by [http://d2l.ai/chapter_computer-vision/fine-tuning.html](http://d2l.ai/chapter_computer-vision/fine-tuning.html)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者；灵感来自于 [http://d2l.ai/chapter_computer-vision/fine-tuning.html](http://d2l.ai/chapter_computer-vision/fine-tuning.html)
- en: One might wonder—why not just train your own model from scratch?
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 有人可能会问——为什么不从头开始训练自己的模型呢？
- en: Creating a new model often demands substantial computational resources and time.
    Utilizing a pre-trained model allows us to harness the capabilities of a model
    trained on extensive data without the hefty computational and time investments.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建新模型通常需要大量的计算资源和时间。利用预训练模型可以让我们利用在广泛数据上训练的模型能力，而无需进行巨大的计算和时间投入。
- en: Since pre-trained models are often trained on large and comprehensive datasets,
    fine-tuning a model allows you to achieve strong performance on a smaller dataset,
    thereby minimizing the risk of overfitting and improving generalization, among
    other benefits.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于预训练模型通常是在大型和全面的数据集上训练的，微调模型可以让你在较小的数据集上获得良好的表现，从而最小化过拟合风险并改善泛化能力等好处。
- en: Developing our NER Model
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发我们的 NER 模型
- en: The dataset we will be using is called [Broad Twitter Corpus](https://huggingface.co/datasets/GateNLP/broad_twitter_corpus)
    (clear for commercial use under the *Creative Commons Attribution 4.0 International*
    license). The dataset itself is just a huge collection of tweets. Each of these
    tweets has been annotated so that they include named entity tags. More importantly,
    according to the [white paper](https://aclanthology.org/C16-1111.pdf), these tweets
    were **manually** annotated. Building an NER model on top of this dataset will
    not only enable us to automatically annotate the entities in the future, but it
    will also enable some of the downstream use cases we described in a previous section.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的数据集称为 [Broad Twitter Corpus](https://huggingface.co/datasets/GateNLP/broad_twitter_corpus)（根据
    *Creative Commons Attribution 4.0 International* 许可证可用于商业用途）。该数据集本身只是一个巨大的推文集合。这些推文都被标注了命名实体标签。更重要的是，根据
    [白皮书](https://aclanthology.org/C16-1111.pdf)，这些推文是**手动**标注的。在这个数据集上构建 NER 模型不仅可以让我们在未来自动标注实体，还能支持我们在前面部分描述的一些下游使用场景。
- en: A rough outline of the process we will follow is below.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将遵循的流程的大致概述如下。
- en: Set up our environment
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置我们的环境
- en: Load [Broad Twitter dataset](https://huggingface.co/datasets/GateNLP/broad_twitter_corpus)
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载 [Broad Twitter 数据集](https://huggingface.co/datasets/GateNLP/broad_twitter_corpus)
- en: Load pre-trained [BERT](https://huggingface.co/bert-base-cased) model
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载预训练的 [BERT](https://huggingface.co/bert-base-cased) 模型
- en: Re-tokenize Broad Twitter tokens
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新标记 Broad Twitter 令牌
- en: Fine tune pre-trained BERT model
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 微调预训练的 BERT 模型
- en: Environment Setup
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 环境设置
- en: For simplicity, I did this work in a Google Colab [notebook](https://colab.research.google.com/drive/1JfuURjpI_lB12fiR2IlZYs9NdEKIGQR9?usp=sharing).
    Google Colab provides free GPU access by going to runtime -> change runtime type
    and selecting T4 GPU. As an aside, this code can be run in many different environments
    — not just Colab.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简便，我在 Google Colab [notebook](https://colab.research.google.com/drive/1JfuURjpI_lB12fiR2IlZYs9NdEKIGQR9?usp=sharing)
    中完成了这项工作。Google Colab 提供免费的 GPU 访问，方法是转到运行时 -> 更改运行时类型，并选择 T4 GPU。此外，这段代码可以在许多不同的环境中运行——不仅仅是
    Colab。
- en: The first thing we need to do is install the required Hugging Face packages.
    I copied a brief description of each from the Hugging Face documentation, which
    is linked to each package name.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是安装所需的 Hugging Face 包。我从 Hugging Face 文档中复制了每个包的简要描述，文档链接在每个包名旁边。
- en: '[**Datasets**](https://huggingface.co/docs/datasets/index)**: “**Datasets is
    a library for easily accessing and sharing datasets for Audio, Computer Vision,
    and Natural Language Processing (NLP) tasks”'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Datasets**](https://huggingface.co/docs/datasets/index)**: “**Datasets 是一个库，用于轻松访问和共享用于音频、计算机视觉和自然语言处理（NLP）任务的数据集**”'
- en: '[**Transformers**](https://huggingface.co/docs/transformers/index)**: “**Transformers
    provides APIs and tools to easily download and train state-of-the-art pre-trained
    models”'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Transformers**](https://huggingface.co/docs/transformers/index)**: “**Transformers
    提供了 API 和工具，以轻松下载和训练最先进的预训练模型**”'
- en: '[**Evaluate**](https://huggingface.co/docs/evaluate/index)**: “**A library
    for easily evaluating machine learning models and datasets”'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Evaluate**](https://huggingface.co/docs/evaluate/index)**: “**一个用于轻松评估机器学习模型和数据集的库**”'
- en: These packages can be installed using PIP.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这些包可以使用 PIP 安装。
- en: '[PRE0]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Loading Broad Twitter dataset
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载 Broad Twitter 数据集
- en: Hugging Face’s datasets library makes it extremely easy to load datasets using
    a two lines of Python code.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 的数据集库使得使用两行 Python 代码加载数据集变得非常简单。
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: When you load a dataset, it loads all relevant data splits contained within
    the dataset. Printing the dataset will show you the available splits, number of
    rows for each split, and the features each row has. The cell below shows the results
    of `print(twitter)`.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当你加载一个数据集时，它会加载数据集中包含的所有相关数据拆分。打印数据集将显示可用的拆分、每个拆分的行数以及每行的特征。下面的单元格展示了 `print(twitter)`
    的结果。
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Loading BERT
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载 BERT
- en: When loading BERT for token classification, we need to specify the number of
    labels. This dataset has been annotated with [IOB2](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging))
    labels. According to the HF dataset page, mappings between numeric tag and string
    tag are as follows.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载用于标记分类的 BERT 时，我们需要指定标签的数量。该数据集已经使用了[IOB2](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging))标签。根据
    HF 数据集页面，数字标签和字符串标签之间的映射如下。
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: If you are unfamiliar with IOB2 labels, one might wonder what is the difference
    between `B-PER` and `I-PER`? What about the difference between `B-ORG` and `I-ORG`
    , or the difference between `B-LOC` and `I-LOC`? The answer lies in tokenization.
    Specifically, some entities might span over multiple tokens. We use these entity
    prefixes to tell the model where the entity begins and how far it continues. The
    `B` prefix before a label indicates that the token is the beginning of a chunk
    of that type, and the `I` prefix indicates that the token is inside a chunk. The
    `O` label means that a token is outside of any chunk.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对 IOB2 标签不熟悉，可能会想知道 `B-PER` 和 `I-PER` 之间有什么区别？`B-ORG` 和 `I-ORG` 之间的区别呢？`B-LOC`
    和 `I-LOC` 之间呢？答案在于标记化。具体来说，一些实体可能跨越多个标记。我们使用这些实体前缀来告诉模型实体的开始位置以及它的延续范围。标签前的 `B`
    前缀表示该标记是该类型块的开始，`I` 前缀表示该标记在一个块内部。`O` 标签表示该标记在任何块之外。
- en: Now that we know we have 7 labels, we can load BERT using the following lines
    of code.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道我们有 7 个标签，我们可以使用以下代码加载 BERT。
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Lastly, we need to tell the model how to map between ID and label.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要告诉模型如何在 ID 和标签之间进行映射。
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We have BERT loaded, but do we even need to fine tune BERT for our NER task?
    When BERT was introduced by Google it was considered groundbreaking. Can’t we
    use it as-is for NER without fine-tuning? We can test its ability to classify
    tokens without any fine tuning below. We’ll use our example sentence from earlier,
    which closely resembles a tweet (short).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经加载了 BERT，但我们是否需要对 BERT 进行微调以适应我们的 NER 任务？当 BERT 被 Google 推出时，它被认为是具有突破性的。我们不能直接使用它进行
    NER 而无需微调吗？我们可以在下面测试它在没有任何微调的情况下对标记的分类能力。我们将使用之前的示例句子，该句子与推文（短文本）非常相似。
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The results may surprise you. It turns out that without any fine tuning, BERT
    isn’t very good at our task. In the code block below, you can see that BERT labeled
    `Zachary Raicik works for Corvus and lives in San` as one entity and `Diego` as
    another.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 结果可能会让你惊讶。事实证明，在没有任何微调的情况下，BERT 在我们的任务中表现得并不好。在下面的代码块中，你可以看到 BERT 将 `Zachary
    Raicik works for Corvus and lives in San` 标记为一个实体，而将 `Diego` 标记为另一个实体。
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Re-tokenize Broad Twitter tokens
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重新标记 Broad Twitter 标记
- en: When we downloaded the Broad Twitter dataset, it came with a set of predefined
    tokens. However, there is no guarantee that the tokens in the dataset will match
    the ones generated by the BERT tokenizer. BERT may split up some tokens into sub-word
    tokens. Consequently, we need to build a function to re-distribute the provided
    tags to the sub-word tokens generated by BERT.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们下载了Broad Twitter数据集时，它附带了一组预定义的标记。然而，数据集中的标记与BERT标记器生成的标记可能不匹配。BERT可能将某些标记拆分为子词标记。因此，我们需要构建一个函数，将提供的标签重新分配给BERT生成的子词标记。
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Hugging Face expects the fields `input_ids`, `attention_mask`, `token_type_ids`
    and `labels` for training. See [here](https://huggingface.co/docs/transformers/v4.34.1/en/preprocessing#natural-language-processing)
    for more information about processing your data for use with the `Transformers`
    library.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face期望训练时包含`input_ids`、`attention_mask`、`token_type_ids`和`labels`字段。有关如何处理数据以供`Transformers`库使用的更多信息，请参见[这里](https://huggingface.co/docs/transformers/v4.34.1/en/preprocessing#natural-language-processing)。
- en: We can use this function to re-tokenize our datasets.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个函数重新标记我们的数据集。
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Fine tune pre-trained BERT model
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调预训练BERT模型
- en: At this point, our datasets are ready to go and we are ready to begin training.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们的数据集已经准备好，我们也准备开始训练了。
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: When fine tuning a pre-trained model, you can choose to retrain as many layers
    as you want. Since this is just an illustrative exercise, we will only tune the
    last layer of BERT.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调预训练模型时，你可以选择重新训练你想要的任意多的层。由于这只是一个示例练习，我们将只微调BERT的最后一层。
- en: '[PRE11]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We use `TrainingArguments` to specify things like the learning rate, number
    of epochs, etc.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`TrainingArguments`来指定学习率、训练轮次等参数。
- en: '[PRE12]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Loss functions are designed for optimizing model weights during training but
    not for interpretation. For that reason, we will include additional metrics to
    understand how good our model is.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数旨在优化训练过程中的模型权重，而不是用于解释。因此，我们将包括额外的指标来理解模型的表现。
- en: In NER tasks, class imbalance is relatively common. For that reason, metrics
    like accuracy may not be the most appropriate. In this case, the class imbalance
    isn’t awful (it’s shown in the dataset description). However, for completeness,
    we will use the weighted F1 score to evaluate our model during training. The weighted
    F1 score takes into account the number of true instances for each label when calculating
    the F1 score for that label. This means that each class contributes to the average
    proportionally to its size.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在命名实体识别（NER）任务中，类别不平衡是比较常见的。因此，准确率等指标可能不够合适。在这种情况下，类别不平衡并不严重（它在数据集描述中有所显示）。然而，为了完整性，我们将在训练过程中使用加权F1分数来评估我们的模型。加权F1分数在计算某一标签的F1分数时，会考虑该标签的真实实例数量。这意味着每个类别的贡献按其规模成比例地影响平均值。
- en: '[PRE13]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We have all the pieces required to train.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经具备了训练所需的所有组件。
- en: '[PRE14]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Once the training process completes, you should get a view that looks something
    like this.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练过程完成，你应该能看到类似这样的视图。
- en: '![](../Images/1d4be0d8640699f2865348c4d9a4922c.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d4be0d8640699f2865348c4d9a4922c.png)'
- en: Image by Author
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: In a realistic setting, you might want to increase the number of epochs and
    invest more time in choosing the right parameters for the task. Some examples
    of this may include
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际情况下，你可能希望增加训练轮次，并花更多时间选择适合任务的参数。例如
- en: '**Learning Rate Optimization:** The learning rate controls how much to update
    the model’s weights. We use `5e-05` in our example, but it’s possible a different
    learning rate is more appropriate for this task.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习率优化：** 学习率控制模型权重的更新幅度。在我们的示例中使用了`5e-05`，但可能不同的学习率更适合这个任务。'
- en: '**Weight decay:** This is a regularization technique that discourages large
    weights. In general, it leads to a simpler model and helps to prevent overfitting.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**权重衰减：** 这是一种正则化技术，旨在抑制大的权重。通常，它会导致模型更简单，并有助于防止过拟合。'
- en: Putting it All Together
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将所有内容整合在一起
- en: Let’s revisit our sentence from earlier.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新审视一下之前的句子。
- en: '[PRE15]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The results indicate that our fine tuned model is much better compared to BERT
    without any fine tuning.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，我们微调后的模型相比未经微调的BERT要好得多。
- en: '[PRE16]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In this article we covered the basics of fine tuning an existing model for a
    specific task. In our case, we used BERT to build a named entity recognition model.
    This process can be applied to any number of tasks using different datasets or
    models. Although this post provided a strong introduction to some of what Hugging
    Face can do, we barely scratched the surface. For example, we didn’t even pass
    our model to the robust evaluation library maintained by Hugging Face. In a future
    post, we will cover how to use some of these additional tools for both fine tuning
    a model for NER and additional use cases.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们介绍了如何为特定任务微调现有模型的基本知识。在我们的案例中，我们使用了 BERT 来构建一个命名实体识别模型。这个过程可以应用于任何数量的任务，使用不同的数据集或模型。虽然这篇文章为
    Hugging Face 能做的事情提供了强有力的介绍，但我们仅仅触及了表面。例如，我们甚至没有将我们的模型传递给 Hugging Face 维护的强大评估库。在未来的文章中，我们将介绍如何使用这些附加工具来微调
    NER 模型以及其他用例。
