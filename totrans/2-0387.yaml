- en: 'Beyond the Warm Embrace: A Deeper Look at Hugging Face'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/beyond-the-warm-embrace-a-deeper-look-at-hugging-face-96b3497788e7](https://towardsdatascience.com/beyond-the-warm-embrace-a-deeper-look-at-hugging-face-96b3497788e7)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Fine tuning language models for Named Entity Recognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@raicik.zach?source=post_page-----96b3497788e7--------------------------------)[![Zachary
    Raicik](../Images/860760b53fcc75013007067190e8ca65.png)](https://medium.com/@raicik.zach?source=post_page-----96b3497788e7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----96b3497788e7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----96b3497788e7--------------------------------)
    [Zachary Raicik](https://medium.com/@raicik.zach?source=post_page-----96b3497788e7--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----96b3497788e7--------------------------------)
    ·9 min read·Nov 3, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ea850ddee92eada7c048b2f96261e036.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Choong Deng Xiang](https://unsplash.com/@dengxiangs?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 'Hugging Face is a platform that offers tools and pre-trained models for various
    Natural Language Processing (NLP) and Natural Language Understanding (NLU) tasks.
    In our previous article, [A Warm Embrace: Exploring Hugging Face](https://medium.com/@raicik.zach/a-warm-embrace-exploring-hugging-face-210c7f4a9078),
    we dove into the basics of this platform and its open-source library that features
    implementations of many state-of-the-art transformer architectures. This post
    enhances the Hugging Face documentation by providing emerging data scientists
    with a single, connected view of various Hugging Face tools for a specific task.
    Specifically, this article explains how to piece together multiple Hugging Face
    capabilities to fine-tune an existing language model for named entity recognition
    (“NER”).'
  prefs: []
  type: TYPE_NORMAL
- en: Relevant Background
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we briefly look at two foundational concepts essential for
    building our model. As a reminder, we covered Hugging Face basics in [A Warm Embrace:
    Exploring Hugging Face](https://medium.com/@raicik.zach/a-warm-embrace-exploring-hugging-face-210c7f4a9078).'
  prefs: []
  type: TYPE_NORMAL
- en: Named Entity Recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model Fine-tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the sections below, it’s assumed you have some knowledge of model development
    and the associated concepts — however, if anything is unclear feel free to reach
    out!
  prefs: []
  type: TYPE_NORMAL
- en: Named Entity Recognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Named Entity Recognition (“NER”) is a common natural language processing task
    of identifying and categorizing relevant information, or entities, into one of
    many predefined (named) groups. NER models can be trained on a variety of entities.
    Some of the most common ones are:'
  prefs: []
  type: TYPE_NORMAL
- en: Names
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Organizations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Places & Locations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the image below, I manually tagged a couple of different named entities in
    a sample sentence. In the context of machine learning and NLP, NER is the process
    of automating this categorization process through models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5c1045e5fc606e6d1f3ba22cc4e0cc77.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: NER models can enable a variety of tasks including but not limited to, information
    retrieval, content summarization, content recommendation and machine translation.
  prefs: []
  type: TYPE_NORMAL
- en: Model Fine-Tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the highest level, fine tuning a model refers to adjusting existing model
    weights based on a new dataset by replacing some or all of the model’s layers
    and retraining. Depending on your task and dataset, you might retrain just the
    last layer, some layers of the model, or the entire model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/04f1b91b24dd65f050e51fabed9ee704.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author; inspired by [http://d2l.ai/chapter_computer-vision/fine-tuning.html](http://d2l.ai/chapter_computer-vision/fine-tuning.html)
  prefs: []
  type: TYPE_NORMAL
- en: One might wonder—why not just train your own model from scratch?
  prefs: []
  type: TYPE_NORMAL
- en: Creating a new model often demands substantial computational resources and time.
    Utilizing a pre-trained model allows us to harness the capabilities of a model
    trained on extensive data without the hefty computational and time investments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since pre-trained models are often trained on large and comprehensive datasets,
    fine-tuning a model allows you to achieve strong performance on a smaller dataset,
    thereby minimizing the risk of overfitting and improving generalization, among
    other benefits.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing our NER Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dataset we will be using is called [Broad Twitter Corpus](https://huggingface.co/datasets/GateNLP/broad_twitter_corpus)
    (clear for commercial use under the *Creative Commons Attribution 4.0 International*
    license). The dataset itself is just a huge collection of tweets. Each of these
    tweets has been annotated so that they include named entity tags. More importantly,
    according to the [white paper](https://aclanthology.org/C16-1111.pdf), these tweets
    were **manually** annotated. Building an NER model on top of this dataset will
    not only enable us to automatically annotate the entities in the future, but it
    will also enable some of the downstream use cases we described in a previous section.
  prefs: []
  type: TYPE_NORMAL
- en: A rough outline of the process we will follow is below.
  prefs: []
  type: TYPE_NORMAL
- en: Set up our environment
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load [Broad Twitter dataset](https://huggingface.co/datasets/GateNLP/broad_twitter_corpus)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load pre-trained [BERT](https://huggingface.co/bert-base-cased) model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Re-tokenize Broad Twitter tokens
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fine tune pre-trained BERT model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Environment Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For simplicity, I did this work in a Google Colab [notebook](https://colab.research.google.com/drive/1JfuURjpI_lB12fiR2IlZYs9NdEKIGQR9?usp=sharing).
    Google Colab provides free GPU access by going to runtime -> change runtime type
    and selecting T4 GPU. As an aside, this code can be run in many different environments
    — not just Colab.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing we need to do is install the required Hugging Face packages.
    I copied a brief description of each from the Hugging Face documentation, which
    is linked to each package name.
  prefs: []
  type: TYPE_NORMAL
- en: '[**Datasets**](https://huggingface.co/docs/datasets/index)**: “**Datasets is
    a library for easily accessing and sharing datasets for Audio, Computer Vision,
    and Natural Language Processing (NLP) tasks”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Transformers**](https://huggingface.co/docs/transformers/index)**: “**Transformers
    provides APIs and tools to easily download and train state-of-the-art pre-trained
    models”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Evaluate**](https://huggingface.co/docs/evaluate/index)**: “**A library
    for easily evaluating machine learning models and datasets”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These packages can be installed using PIP.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Loading Broad Twitter dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hugging Face’s datasets library makes it extremely easy to load datasets using
    a two lines of Python code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: When you load a dataset, it loads all relevant data splits contained within
    the dataset. Printing the dataset will show you the available splits, number of
    rows for each split, and the features each row has. The cell below shows the results
    of `print(twitter)`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Loading BERT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When loading BERT for token classification, we need to specify the number of
    labels. This dataset has been annotated with [IOB2](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging))
    labels. According to the HF dataset page, mappings between numeric tag and string
    tag are as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: If you are unfamiliar with IOB2 labels, one might wonder what is the difference
    between `B-PER` and `I-PER`? What about the difference between `B-ORG` and `I-ORG`
    , or the difference between `B-LOC` and `I-LOC`? The answer lies in tokenization.
    Specifically, some entities might span over multiple tokens. We use these entity
    prefixes to tell the model where the entity begins and how far it continues. The
    `B` prefix before a label indicates that the token is the beginning of a chunk
    of that type, and the `I` prefix indicates that the token is inside a chunk. The
    `O` label means that a token is outside of any chunk.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know we have 7 labels, we can load BERT using the following lines
    of code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Lastly, we need to tell the model how to map between ID and label.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We have BERT loaded, but do we even need to fine tune BERT for our NER task?
    When BERT was introduced by Google it was considered groundbreaking. Can’t we
    use it as-is for NER without fine-tuning? We can test its ability to classify
    tokens without any fine tuning below. We’ll use our example sentence from earlier,
    which closely resembles a tweet (short).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The results may surprise you. It turns out that without any fine tuning, BERT
    isn’t very good at our task. In the code block below, you can see that BERT labeled
    `Zachary Raicik works for Corvus and lives in San` as one entity and `Diego` as
    another.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Re-tokenize Broad Twitter tokens
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we downloaded the Broad Twitter dataset, it came with a set of predefined
    tokens. However, there is no guarantee that the tokens in the dataset will match
    the ones generated by the BERT tokenizer. BERT may split up some tokens into sub-word
    tokens. Consequently, we need to build a function to re-distribute the provided
    tags to the sub-word tokens generated by BERT.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Hugging Face expects the fields `input_ids`, `attention_mask`, `token_type_ids`
    and `labels` for training. See [here](https://huggingface.co/docs/transformers/v4.34.1/en/preprocessing#natural-language-processing)
    for more information about processing your data for use with the `Transformers`
    library.
  prefs: []
  type: TYPE_NORMAL
- en: We can use this function to re-tokenize our datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Fine tune pre-trained BERT model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At this point, our datasets are ready to go and we are ready to begin training.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: When fine tuning a pre-trained model, you can choose to retrain as many layers
    as you want. Since this is just an illustrative exercise, we will only tune the
    last layer of BERT.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We use `TrainingArguments` to specify things like the learning rate, number
    of epochs, etc.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Loss functions are designed for optimizing model weights during training but
    not for interpretation. For that reason, we will include additional metrics to
    understand how good our model is.
  prefs: []
  type: TYPE_NORMAL
- en: In NER tasks, class imbalance is relatively common. For that reason, metrics
    like accuracy may not be the most appropriate. In this case, the class imbalance
    isn’t awful (it’s shown in the dataset description). However, for completeness,
    we will use the weighted F1 score to evaluate our model during training. The weighted
    F1 score takes into account the number of true instances for each label when calculating
    the F1 score for that label. This means that each class contributes to the average
    proportionally to its size.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We have all the pieces required to train.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Once the training process completes, you should get a view that looks something
    like this.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1d4be0d8640699f2865348c4d9a4922c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: In a realistic setting, you might want to increase the number of epochs and
    invest more time in choosing the right parameters for the task. Some examples
    of this may include
  prefs: []
  type: TYPE_NORMAL
- en: '**Learning Rate Optimization:** The learning rate controls how much to update
    the model’s weights. We use `5e-05` in our example, but it’s possible a different
    learning rate is more appropriate for this task.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weight decay:** This is a regularization technique that discourages large
    weights. In general, it leads to a simpler model and helps to prevent overfitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Putting it All Together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s revisit our sentence from earlier.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The results indicate that our fine tuned model is much better compared to BERT
    without any fine tuning.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In this article we covered the basics of fine tuning an existing model for a
    specific task. In our case, we used BERT to build a named entity recognition model.
    This process can be applied to any number of tasks using different datasets or
    models. Although this post provided a strong introduction to some of what Hugging
    Face can do, we barely scratched the surface. For example, we didn’t even pass
    our model to the robust evaluation library maintained by Hugging Face. In a future
    post, we will cover how to use some of these additional tools for both fine tuning
    a model for NER and additional use cases.
  prefs: []
  type: TYPE_NORMAL
