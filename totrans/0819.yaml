- en: Entropy based Uncertainty Prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/entropy-based-uncertainty-prediction-812cca769d7a](https://towardsdatascience.com/entropy-based-uncertainty-prediction-812cca769d7a)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This article explores how Entropy can be employed as a tool for uncertainty
    estimation in image segmentation tasks. We will walk through what Entropy is,
    and how to implement it with Python.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@francoisporcher?source=post_page-----812cca769d7a--------------------------------)[![François
    Porcher](../Images/9ddb233f8cadbd69026bd79e2bd62dea.png)](https://medium.com/@francoisporcher?source=post_page-----812cca769d7a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----812cca769d7a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----812cca769d7a--------------------------------)
    [François Porcher](https://medium.com/@francoisporcher?source=post_page-----812cca769d7a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----812cca769d7a--------------------------------)
    ·7 min read·Sep 2, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1a2be74cacdb994609be32b738dbe7bb.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Michael Dziedzic](https://unsplash.com/@lazycreekimages?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 'While working at Cambridge University as a Research Scientist in Neuroimaging
    and AI, I faced the challenge of performing image segmentation on intricate brain
    datasets using the latest Deep Learning techniques, especially the [nnU-Net](https://medium.com/towards-data-science/the-ultimate-guide-to-nnu-net-for-state-of-the-art-image-segmentation-6dda7f44b935).
    During this endeavor, I observed a significant gap: the overlooking of uncertainty
    estimation. Yet, **uncertainty is crucial for reliable decision-making.**'
  prefs: []
  type: TYPE_NORMAL
- en: Before delving into the specifics, feel free to check out my [Github repository](https://github.com/FrancoisPorcher)
    which contains all the code snippets discussed in this article.
  prefs: []
  type: TYPE_NORMAL
- en: The importance of Uncertainty in Image Segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the world of computer vision and machine learning, image segmentation is
    a central problem. Whether it’s in medical imaging, self-driving cars, or robotics,
    accurate segmentation are vital for effective decision-making. However, one often
    overlooked aspect is the **measure of uncertainty associated with these segmentations.**
  prefs: []
  type: TYPE_NORMAL
- en: Why should we care about uncertainty in image segmentation?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In many real-world applications, an incorrect segmentation could result in dire
    consequences. For example, if a self-driving car misidentifies an object or a
    medical imaging system incorrectly labels a tumor, the consequences could be catastrophic.
    Uncertainty estimation gives us a measure of how ‘sure’ the model is about its
    prediction, allowing for better-informed decisions.
  prefs: []
  type: TYPE_NORMAL
- en: We can also use Entropy as a measure of uncertainty to improve the learning
    of our neural networks. This area is knows as **‘active learning’.** This idea
    will be explored in further articles but the main idea is to identify the zones
    on which the models are the most uncertain to focus on them. For example we could
    have a CNN performing medical image segmentation on the brain, but performing
    very poorly on subjects with tumours. Then we could concentrate our efforts to
    acquire more labels of this type.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Entropy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Entropy is a concept borrowed from thermodynamics and information theory, which
    quantifies the amount of uncertainty or randomness in a system. In the context
    of machine learning, **entropy can be used to measure the uncertainty of model
    predictions.**
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, for a discrete random variable *X* with probability mass function
    *P*(*x*), the entropy *H*(*X*) is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Or in the continous case:'
  prefs: []
  type: TYPE_NORMAL
- en: The higher the entropy, the greater the uncertainty, and vice versa.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'A classic example to fully grasp the concept:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Situation 1: A biased coin'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/bb08d5bf67d9d9e7eeac8b0bf745773b.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Jizhidexiaohailang](https://unsplash.com/@jizhidexiaohailang?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a biased coin, which lands on head with a probability *p=0.9,* and tail
    with a probability *1-p=0.1.*
  prefs: []
  type: TYPE_NORMAL
- en: Its entropy is
  prefs: []
  type: TYPE_NORMAL
- en: 'Situation 2: Balanced coin'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now let’s imagine a balanced coin which lands on head and tail with probability
    *p=0.5*
  prefs: []
  type: TYPE_NORMAL
- en: 'Its entropy is:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The entropy is larger, which is coherent with what we said before: more uncertainty
    = more entropy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Actually it is interesting to note that *p=0.5* corresponds to the maximum
    entropy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e2a96f13b993644e8e39156c01cd8145.png)'
  prefs: []
  type: TYPE_IMG
- en: Entropy visualisation, Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, remember that a uniform distribution is the case with maximal entropy.
    If every outcome is equally probable, then this corresponds to the maximal uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Entropy in Image Segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To link this to image segmentation, consider that in deep learning, the final
    softmax layer usually provides the class probabilities for each pixel. One can
    easily compute the entropy for each pixel based on these softmax outputs.
  prefs: []
  type: TYPE_NORMAL
- en: But How does it work?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When a model is confident about a particular pixel belonging to a specific class,
    the softmax layer shows high probability (~1) for that class, and very small probabilities
    (~0) for the other classes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ec4bb9f2333b2ede96f5b082cb7ac70b.png)'
  prefs: []
  type: TYPE_IMG
- en: Softmax layer, confident case, Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, when the model is uncertain, the softmax output is more evenly spread
    across multiple classes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/da6ec6645085904a22464365f408233c.png)'
  prefs: []
  type: TYPE_IMG
- en: Softmax layer, uncertain case, Image by author
  prefs: []
  type: TYPE_NORMAL
- en: The probabilities are much more diffuse, close to the uniform case if you remember,
    because the model cannot decide which class is associated with the pixel.
  prefs: []
  type: TYPE_NORMAL
- en: If you have made it until now, great! You should have a great intuition of how
    entropy works.
  prefs: []
  type: TYPE_NORMAL
- en: 'Case Study: Medical Imaging'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s illustrate this with a hands-on example using medical imaging, specifically
    T1 Brain scans of fetuses. All codes and images for this case study are available
    in my [Github repository](https://github.com/FrancoisPorcher/awesome-ai-tutorials/tree/main/006%20-%20Entropy%20based%20uncertainty%20for%20Image%20Segmentation).
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Computing Entropy with Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we said before, we are working with the softmax output tensor, given by our
    Neural Network. **This approach is model-free**, it only uses the probabilities
    of each class.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s clarify something important about the dimensions of the tensors we are
    working with.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are working with 2D Images, the shape of your softmax layer should be:'
  prefs: []
  type: TYPE_NORMAL
- en: Meaning that for each pixel (or voxel), we have a vector of size *Classes,*
    which gives us the probabilities of a pixel to belong to each of the classes we
    have.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore the entropy should be computer along the first dimension:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 2\. Visualizing Entropy-based Uncertainty
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now let’s visualize the uncertainties by using a heatmap, on each slice of our
    image segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8b411a474924e5267aec28387e02b692.png)'
  prefs: []
  type: TYPE_IMG
- en: T1 scan (left), Segmentation (middle), Entropy (Right), Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at an other example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c95c8a934ca311c71d57fc93551e994e.png)'
  prefs: []
  type: TYPE_IMG
- en: T1 scan (left), Segmentation (middle), Entropy (Right), Image by author
  prefs: []
  type: TYPE_NORMAL
- en: The results look great! Indeed we can see that this is coherent because the
    zones of high entropy are at the contour of the shapes. This is normal because
    the model does not really doubt the points at the middle of each zone, but its
    rather the delimitation or contour that is difficult to spot.
  prefs: []
  type: TYPE_NORMAL
- en: Make Informed Decisions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This uncertainty can be used in plenty of different ways:'
  prefs: []
  type: TYPE_NORMAL
- en: As medical experts work more and more with AI as a tool, being aware of the
    uncertainty of the model is crucial. This mean that medical experts could spend
    more times on the zone where more fine-grained attention is required.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2\. In the context of **Active Learning** or **Semi-Supervised Learning**, we
    can leverage Entropy based Uncertainty to focus on the examples with maximal uncertainty,
    and improve the efficiency of learning (more about this in coming articles).
  prefs: []
  type: TYPE_NORMAL
- en: Main Takeaways
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Entropy is an extremely powerful concept to measure the randomness or uncertainty
    of a system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is possible to leverage Entropy in Image Segmentation. This approach is model
    free and only uses the softmax output tensor.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uncertainty estimation is overlooked, but it is crucial. Good Data Scientists
    know how to make good models. Great Data Scientists know where their model fail
    and use this to improve learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You liked this article and want to learn more? Check this out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/FrancoisPorcher?source=post_page-----812cca769d7a--------------------------------)
    [## FrancoisPorcher - Overview'
  prefs: []
  type: TYPE_NORMAL
- en: Visiting Researcher at Cambridge University in Artificial Intelligence and Neurosciences.
    Graduated from UC Berkeley in…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/FrancoisPorcher?source=post_page-----812cca769d7a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**Thanks for reading!**'
  prefs: []
  type: TYPE_NORMAL
- en: '*If you want to have access to premium articles on Medium, you only need a
    membership for $5 a month. If you sign up* [***with my link***](https://medium.com/@francoisporcher/membership)*,
    you support me with a part of your fee without additional costs.*'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bai, W., Oktay, O., Sinclair, M., Suzuki, H., Rajchl, M., Tarroni, G., Glocker,
    B., King, A., Matthews, P. M., & Rueckert, D. (2017). Semi-Supervised Learning
    for Network-Based Cardiac MR Image Segmentation. In *Proceedings of the 20th International
    Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI
    2017)* (pp. 253–260). Springer Verlag. [https://doi.org/10.1007/978-3-319-66185-8_29](https://doi.org/10.1007/978-3-319-66185-8_29)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Ta, K., Ahn, S. S., Stendahl, J. C., Sinusas, A. J., & Duncan, J. S. (2020).
    A Semi-Supervised Joint Network for Simultaneous Left Ventricular Motion Tracking
    and Segmentation in 4D Echocardiography. *Medical Image Computing and Computer-Assisted
    Intervention: MICCAI International Conference on Medical Image Computing and Computer-Assisted
    Intervention*, 12266 (October), 468–477\. [https://doi.org/10.1007/978-3-030-59725-2_45](https://doi.org/10.1007/978-3-030-59725-2_45)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Yu, L., Wang, S., Li, X., Fu, C-W., & Heng, P-A. (2019). Uncertainty-Aware Self-Ensembling
    Model for Semi-Supervised 3D Left Atrium Segmentation. *arXiv*. [http://arxiv.org/abs/1907.07034](http://arxiv.org/abs/1907.07034)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
