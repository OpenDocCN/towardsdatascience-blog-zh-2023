- en: Why Do We Have Huge Language Models and Small Vision Transformers?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/why-do-we-have-huge-language-models-and-small-vision-transformers-5d59ac36c1d6](https://towardsdatascience.com/why-do-we-have-huge-language-models-and-small-vision-transformers-5d59ac36c1d6)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Artificial intelligence | computer vision | Vision Transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Google ViT-22 paves the way for new large transformers and to revolutionize
    computer vision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://salvatore-raieli.medium.com/?source=post_page-----5d59ac36c1d6--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----5d59ac36c1d6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5d59ac36c1d6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5d59ac36c1d6--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----5d59ac36c1d6--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5d59ac36c1d6--------------------------------)
    ·9 min read·Feb 17, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e70451dc343d2e3cbc3ea2e6b5bbd3ea.png)'
  prefs: []
  type: TYPE_IMG
- en: image by [Joshua Earle](https://unsplash.com/@joshuaearle) at unsplash.com
  prefs: []
  type: TYPE_NORMAL
- en: In recent years we have seen a growth in the number of transformer parameters.
    Looking closely though, these are Language Models (LMs), up to an incredible [540
    B of parameters](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html).
    **Why not for visual models?**
  prefs: []
  type: TYPE_NORMAL
- en: As for text models, an increase in dataset size, scalable architectures, and
    new training methods have enabled this growth in the number of parameters. This
    has not only served to increase performance in particular tasks (classification
    and so on), **but as the number of parameters has grown, we have seen emerging
    capabilities.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/57b77e6e745a2ea296895080a8738106.png)'
  prefs: []
  type: TYPE_IMG
- en: '“Trend of sizes of state-of-the-art Natural Language Processing (NLP) models
    with time. The number of floating-point operations to train these models is increasing
    at an exponential rate”. source: [here](https://arxiv.org/abs/2104.04473)'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, a large model can be used as a basis for [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning)
    and fine-tuning, so there is interest in developing increasingly high-performance
    models. As much as LMs have been used successfully in a number of tasks, there
    are many other tasks where a model capable of image analysis is needed.
  prefs: []
  type: TYPE_NORMAL
- en: As of 2016, the [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    is the architecture of choice, and the use of [self-attention](https://en.wikipedia.org/wiki/Attention_(machine_learning))
    has shown obvious advantages. Therefore, several groups have trained transformer
    models capable of working with images (vision transformer, ViT). **So far, the
    widest ViT has only 15 B parameters. why?**
  prefs: []
  type: TYPE_NORMAL
- en: In a new study, Google was able to train a 22 B model of parameters and understand
    why there is difficulty in scaling ViTs.
  prefs: []
  type: TYPE_NORMAL
- en: '**In summary:**'
  prefs: []
  type: TYPE_NORMAL
- en: They explain why the traditional method of training ViTs produces instability
    during scaling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to modify the architecture for scaling, and how the model reaches the state
    of the art.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How improving fairness when scaling the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**What are vision transformers?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/0da782113c539e2c89f97d130055d58d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from Wikipedia ([source](https://en.wikipedia.org/wiki/Vision_transformer))
  prefs: []
  type: TYPE_NORMAL
- en: Transformers are of course permutation invariant, however, they cannot process
    grid-structured data (only sequences). So, we have to in order to use a Transformer
    with images we have to find a way to transform them into sequences. **How?**
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to transform the images into a sequence of patches (the image
    is divided into a series of fragments called patches). These patches are basically
    our tokens (like words in the classic transformer). These images are then flattened
    and transformed into lower-dimensional embedding (this preserves the information
    but reduces the number of dimensions). Also, as in the original transformer, we
    use positional encoding so the model knows the position of the patch in the image.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fff9926c269163af2d81f73c5de4a0ba.png)'
  prefs: []
  type: TYPE_IMG
- en: original article describing ViT ([arXiv prepint](https://arxiv.org/pdf/2010.11929.pdf))
  prefs: []
  type: TYPE_NORMAL
- en: The model then is trained in supervised learning (a huge dataset of which we
    have image labels) and then can be used for downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://pub.towardsai.net/a-visual-journey-in-what-vision-transformers-see-9db9c8ba62d4?source=post_page-----5d59ac36c1d6--------------------------------)
    [## A Visual Journey in What Vision-Transformers See'
  prefs: []
  type: TYPE_NORMAL
- en: How some of the largest models see the world
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: pub.towardsai.net](https://pub.towardsai.net/a-visual-journey-in-what-vision-transformers-see-9db9c8ba62d4?source=post_page-----5d59ac36c1d6--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Why is hard to scale them and how to solve it?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before ViTs, convolutional networks were the standard for computer vision tasks.
    In “[A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545)” the authors note
    that the issue is still open and current.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, though, we have not been able to scale up ViTs. Since in
    transformers scaling the model also leads to the emergence of behaviors that cannot
    be imagined in advance, this is a serious problem.
  prefs: []
  type: TYPE_NORMAL
- en: '**The authors noted that over 8 B of parameters**, instability emerged as divergent
    training loss after a few thousand steps. This was caused “by extremely large
    values in attention logits, which lead to (almost one-hot) attention weights with
    near-zero entropy.” **To solve this, the authors added a layer-normalization to
    the Queries and Keys before the dot-product.**'
  prefs: []
  type: TYPE_NORMAL
- en: In the figure, they show how this expedient improves training.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/46b59c54b596fea99aa96126e0fa2602.png)'
  prefs: []
  type: TYPE_IMG
- en: '(source: [here](https://arxiv.org/pdf/2302.05442.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: '**The second expedient is to modify the architecture.** In the classical transformer
    after the self-attention output is followed by a [multi-layer-perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron)
    (MLP). Instead, here the self-attention blocks are in parallel with the MLP. This
    operation does not degrade performance while speeding up the training by 15 %
    (as shown with PaLM, another large Google model, this operation is basically combining
    matrix multiplications in a single operation).'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the bias term is removed for attention projection (this also reduces
    the training time without reducing performance).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the figure, the new block after these expedients:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/72f7205f412233cfb8f97cd3fb6fb19f.png)'
  prefs: []
  type: TYPE_IMG
- en: '(source: [here](https://arxiv.org/pdf/2302.05442.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: The table compares Google’s model (ViT-22) with the previously reported largest
    ViT models, ViT-G and ViT-e.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/08b0c0b8453ad27fa65953179f7de4be.png)'
  prefs: []
  type: TYPE_IMG
- en: '(source: [here](https://arxiv.org/pdf/2302.05442.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: Training has also been optimized. Google used JAX (Google has been [focusing
    more on JAX](https://www.businessinsider.com/facebook-pytorch-beat-google-tensorflow-jax-meta-ai-2022-6)
    than TensorFlow for some time). They also used a number of tricks (asynchronous
    parallel linear operations, parameter sharding) to ensure the model was optimized
    for [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit).
  prefs: []
  type: TYPE_NORMAL
- en: The authors used a dataset of about 4 B images, which were semi-automatically
    annotated with 30,000 classes. As a gentle reminder, in a ViT the images are divided
    into several sections (called patches) which are then together with position (positional
    encoding) transformed into a sequence. Each image (224 x 224) is divided into
    14 x 14 patches. So an image is eventually represented by 256 tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Does scaling the ViT worth it?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once the model was trained, they tested it on [ImageNet](https://www.image-net.org/)
    (1 M images and 1000 classes) to test its classification ability. The authors
    show, that the frozen model (i.e., without the need for fine-tuning) has comparable
    performance to other models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/75707b112c852e613c38010d65233623.png)'
  prefs: []
  type: TYPE_IMG
- en: '(source: [here](https://arxiv.org/pdf/2302.05442.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, the model has been tested on another dataset using different image
    resolutions. **ViT-22B leads to significant accuracy improvement especially when
    the input size is small.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/03e6d9af221a76bcf308712196d3ca2c.png)'
  prefs: []
  type: TYPE_IMG
- en: '(source: [here](https://arxiv.org/pdf/2302.05442.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, one of the most frequent uses of large models is transfer
    learning. After all, people often work with small datasets and use a large model
    for fine-tuning and for a different task than the one in which it was trained.
    In the authors’ words:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Transfer learning for dense prediction is critical especially since obtaining
    pixel-level labels can be costly. In this section, we investigate the quality
    of captured geometric and spatial information by the ViT-22B model (trained using
    image-level classification objective) on semantic segmentation and monocular depth
    estimation tasks. (source: [here](https://arxiv.org/pdf/2302.05442.pdf))'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The authors tested the model with three benchmark datasets for semantic segmentation
    (ADEK20k, Pascal Context, Pascal VOC). Not only that but they tested the model
    using a limited amount of data for transfer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/739dbc1d99d1ef8afb2ec668e527b489.png)'
  prefs: []
  type: TYPE_IMG
- en: '“Fewshot semantic segmentation on ADE20k, when only a fraction of the training
    set is used. We report mean IoU for semantic segmentation on the validation set”
    (source: [here](https://arxiv.org/pdf/2302.05442.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: ViT-22 has the best performance when there is little data, which is useful because
    often getting images and their segmentation mask is expensive, so fewer examples
    are needed than the other models.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the model showed superior capabilities in monocular depth estimation
    on the Waymo Open dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/27131d454d774d155d89a525d1d54d9d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Monocular depth estimation from frozen ViT features using different decoders
    on the Waymo Open dataset. (source: [here](https://arxiv.org/pdf/2302.05442.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, by repurposing the model (but keeping the pre-trained ViT-22 as
    a component) it was possible to use it for video classification. **This demonstrates
    the plasticity of using the model for various possible tasks.**
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, the authors showed that fine-tuning is capable of improving performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1c61c29d70cf7393915f42c9642cf7fa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Video classification results. We evaluate the ViT-22B representations by freezing
    the backbone, and training a small transformer to aggregate frozen, per-frame
    representations. ViT-22B outperforms the largest previous vision backbone, ViT-e
    which contains 4 billion parameters. (source: [here](https://arxiv.org/pdf/2302.05442.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: How fair is this model?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Artificial intelligence models are susceptible to unintended bias. Many of these
    biases are present in the training dataset and the model can amplify, and learn
    spurious correlation and error disparities. **Because pre-trained models are then
    used for subsequent tasks, errors are carried along.**
  prefs: []
  type: TYPE_NORMAL
- en: The authors argue that scaling the model can serve to mitigate these biases
    and decided to test this by using demographic parity (DP) as a measure of fairness.
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors explain their approach:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We use CelebA (Liu et al., 2015) with binary gender as a sensitive attribute
    while the target is “attractive” or “smiling”. We emphasize that such experiments
    are carried out only to verify technical claims and shall by no means be interpreted
    as an endorsement of such vision-related tasks. We choose the latter attributes
    because they exhibit gender related bias as shown in Figure 15\. (source: [here](https://arxiv.org/pdf/2302.05442.pdf))'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/fccc11c2f66497bb971f1aeb4a1b8e77.png)'
  prefs: []
  type: TYPE_IMG
- en: '“DP in the model often reflects DP in the data in the absence of bias mitigation.
    In this figure, binary sex is the sensitive attribute and linear heads are trained
    to predict other attributes in CelebA using pretrained features.” (source: [here](https://arxiv.org/pdf/2302.05442.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: Scaling the model, as described in the literature, offers a more favorable tradeoff
    (“performance improves with scale subject to any prescribed level of bias constraint”).
    Second, all subgroups benefit from the improvement, and scaling reduces the disparity
    in performance across the subgroups.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/25cd25363402ccecf5c8e48343a59e54.png)'
  prefs: []
  type: TYPE_IMG
- en: '“top: Accuracy (ACC) for ViT variants after debiasing for each DP level. middle:
    Accuracy for each subgroup in CelebA prior to debiasing. bottom: y-axis is absolute
    difference in performance across the two subgroups: females and males. ViT-22B
    provides a more equitable performance, compared to smaller ViT architectures.”
    (source: [here](https://arxiv.org/pdf/2302.05442.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: What does the model see?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As described earlier, computer vision models focus primarily on texture, while
    humans rely more on shapes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Humans are at 96% shape / 4% texture bias and ViT-22B-384 achieves a previously
    unseen 87% shape bias / 13% texture bias. (source: [here](https://arxiv.org/pdf/2302.05442.pdf))'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The result is very interesting because most models have 20–30 % shape bias and
    70–80 % texture bias (and this is also true for convolutional networks). This
    bias is also one of the reasons why by varying the texture of the image even if
    the shape is recognizable, a model can be tricked to misinterpret the image and
    mislabel it.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e4c98e275488190cdbb77ce72acdbb4e.png)'
  prefs: []
  type: TYPE_IMG
- en: '“Shape bias: many vision models have a low shape / high texture bias, whereas
    ViT-22B fine-tuned on ImageNet (red, green, blue trained on 4B images as indicated
    by brackets after model names, unless trained on ImageNet only) have the highest
    shape bias recorded in a ML model to date, bringing them closer towards a human-like
    shape bias.” (source: [here](https://arxiv.org/pdf/2302.05442.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, another way to understand what the model sees is to obtain saliency
    maps (gradient-based feature attribution methods).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9cf2f5d8beeb5d76b0e1bd51ed43c6b5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Saliency before and after model cooldown (source: [here](https://arxiv.org/pdf/2302.05442.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Google unveiled a model that is more than 5 times the previous model of ViTs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We presented ViT-22B, the currently largest vision transformer model at 22
    billion parameters. We show that with small, but critical changes to the original
    architecture, we can achieve both excellent hardware utilization and training
    stability, yielding a model that advances the SOTA on several benchmarks. (source:
    [here](https://arxiv.org/pdf/2302.05442.pdf))'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Beyond the size and results in the benchmarks, this model is a starting point
    for much larger models. In fact, before now succeeding in scaling a ViT model
    was difficult because of instability during training. The authors showed that
    these problems can be solved by modifying the architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Large models can be used as pre-trained scaffolds for different tasks ( computer
    vision models can be used in many real-world tasks). In addition, [unexpected
    behaviors emerge](https://arxiv.org/pdf/2206.07682.pdf) (which are not present
    in small models and cannot be predicted by the scaling law). Moreover, as shown
    these models can then integrate into multi-modal models (and [could influence
    emergent behaviors in them](https://twitter.com/YiTayML/status/1625205983239880704?s=20&t=_W_AqpJHeJgJ_Af32Av5Jw)).
  prefs: []
  type: TYPE_NORMAL
- en: In addition, ViT-22B shows how scaling has improved in terms of fairness. This
    model is also more robust and more aligned with human vision (less dependent on
    texture and more on the shape).
  prefs: []
  type: TYPE_NORMAL
- en: '**Most likely, we will soon see larger ViTs (alone or as a component of a multi-modal
    model). What do you think about it?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have found this interesting:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can look for my other articles, you can also [**subscribe**](https://salvatore-raieli.medium.com/subscribe)
    to get notified when I publish articles, and you can also connect or reach me
    on[**LinkedIn**](https://www.linkedin.com/in/salvatore-raieli/)**.**
  prefs: []
  type: TYPE_NORMAL
- en: Here is the link to my GitHub repository, where I am planning to collect code
    and many resources related to machine learning, artificial intelligence, and more.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/SalvatoreRa/tutorial?source=post_page-----5d59ac36c1d6--------------------------------)
    [## GitHub - SalvatoreRa/tutorial: Tutorials on machine learning, artificial intelligence,
    data science…'
  prefs: []
  type: TYPE_NORMAL
- en: Tutorials on machine learning, artificial intelligence, data science with math
    explanation and reusable code (in python…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/SalvatoreRa/tutorial?source=post_page-----5d59ac36c1d6--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'or you may be interested in one of my recent articles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://levelup.gitconnected.com/microsoft-biogpt-towards-the-chatgpt-of-life-science-56e251536af6?source=post_page-----5d59ac36c1d6--------------------------------)
    [## Microsoft BioGPT: Towards the ChatGPT of life science?'
  prefs: []
  type: TYPE_NORMAL
- en: BioGPT achieves the SOTA in different biomedical NLP tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'levelup.gitconnected.com](https://levelup.gitconnected.com/microsoft-biogpt-towards-the-chatgpt-of-life-science-56e251536af6?source=post_page-----5d59ac36c1d6--------------------------------)
    [](https://levelup.gitconnected.com/sparsegpt-fewer-parameters-is-better-7b47ad60ac00?source=post_page-----5d59ac36c1d6--------------------------------)
    [## SparseGPT: fewer parameters is better?'
  prefs: []
  type: TYPE_NORMAL
- en: How to get rid of 100 billion parameters and happily infer on one GPU
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/sparsegpt-fewer-parameters-is-better-7b47ad60ac00?source=post_page-----5d59ac36c1d6--------------------------------)
    [](https://medium.com/data-driven-fiction/everything-but-everything-you-need-to-know-about-chatgpt-546af7153ee2?source=post_page-----5d59ac36c1d6--------------------------------)
    [## Everything but everything you need to know about ChatGPT
  prefs: []
  type: TYPE_NORMAL
- en: what is known, the latest news, what it is impacting, and what is changing.
    all in one article
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'medium.com](https://medium.com/data-driven-fiction/everything-but-everything-you-need-to-know-about-chatgpt-546af7153ee2?source=post_page-----5d59ac36c1d6--------------------------------)
    [](https://medium.datadriveninvestor.com/razzaie-awards-2022-what-are-the-worst-ai-of-the-year-a2596c566218?source=post_page-----5d59ac36c1d6--------------------------------)
    [## RazzAIe awards 2022: what are the worst AI of the year?'
  prefs: []
  type: TYPE_NORMAL
- en: What are the worst models of the year? What went wrong?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.datadriveninvestor.com](https://medium.datadriveninvestor.com/razzaie-awards-2022-what-are-the-worst-ai-of-the-year-a2596c566218?source=post_page-----5d59ac36c1d6--------------------------------)
  prefs: []
  type: TYPE_NORMAL
