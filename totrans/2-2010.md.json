["```py\n**Table of contents:** \nFeature #1: consistency\nFeature #2: wide range of algorithms\nFeature #3: data preprocessing and feature engineering\nFeature #4: model evaluation and validation\nFeature #5: integration with the Python data science ecosystem\nCoding examples\n```", "```py\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import StandardScaler\n\n# Generating random features with at least 50 occurrences each\nnp.random.seed(42)\n\nfeature1 = np.random.randint(0, 10, size=100)\nfeature2 = np.random.randint(0, 10, size=100)\nfeature3 = np.random.randint(0, 10, size=100)\nfeature4 = np.random.randint(0, 10, size=100)\nfeature5 = np.random.randint(0, 10, size=100)\ntarget = np.random.randint(0, 100, size=100)\n\n# Creating a Pandas data frame\ndata = {\n    'Feature1': feature1,\n    'Feature2': feature2,\n    'Feature3': feature3,\n    'Feature4': feature4,\n    'Feature5': feature5,\n    'Target': target\n}\n\ndf = pd.DataFrame(data)\n\n# Splitting the data into features and target\nX = df.drop('Target', axis=1)\ny = df['Target']\n\n# Normalizing the data using StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Splitting the normalized data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n\n# Fitting the linear regression model\nreg_model = LinearRegression()\nreg_model.fit(X_train, y_train)\n\n# Predicting the target variable for both train and test sets\ny_train_pred = reg_model.predict(X_train)\ny_test_pred = reg_model.predict(X_test)\n\n# Calculating R² scores for train and test sets\nr2_train = r2_score(y_train, y_train_pred)\nr2_test = r2_score(y_test, y_test_pred)\n\n# Printing the R² scores\nprint(\"R² score for train set:\", r2_train)\nprint(\"R² score for test set:\", r2_test)\n```", "```py\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import roc_curve, roc_auc_score\nimport matplotlib.pyplot as plt\n\n# Set a random seed for reproducibility\nnp.random.seed(42)\n\n# Generate random data\nnum_samples = 1000\nnum_features = 10\n\n# Generate features (X)\nX = np.random.randn(num_samples, num_features)\n\n# Generate labels (y)\ny = np.random.randint(2, size=num_samples)\n\n# Standardize the features\nX = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Fit the train set using random forest classifier\nrf_clf = RandomForestClassifier()\nrf_clf.fit(X_train, y_train)\n\n# Fit the train set using K-nearest neighbors (KNN) classifier\nknn_clf = KNeighborsClassifier()\nknn_clf.fit(X_train, y_train)\n\n# Predict probabilities for the positive class (class 1)\ny_train_rf_probs = rf_clf.predict_proba(X_train)[:, 1]\ny_train_knn_probs = knn_clf.predict_proba(X_train)[:, 1]\n\n# Calculate the false positive rate (FPR), true positive rate (TPR), and thresholds for the ROC curve\nrf_fpr, rf_tpr, rf_thresholds = roc_curve(y_train, y_train_rf_probs)\nknn_fpr, knn_tpr, knn_thresholds = roc_curve(y_train, y_train_knn_probs)\n\n# Calculate the AUC score for the ROC curve\nrf_auc = roc_auc_score(y_train, y_train_rf_probs)\nknn_auc = roc_auc_score(y_train, y_train_knn_probs)\n\n# Plot the ROC curve\nplt.plot(rf_fpr, rf_tpr, label=f\"Random Forest (AUC = {rf_auc:.2f})\")\nplt.plot(knn_fpr, knn_tpr, label=f\"KNN (AUC = {knn_auc:.2f})\")\nplt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random classifier\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve - Train Set')\nplt.legend(loc='lower right')\nplt.show()\n```"]