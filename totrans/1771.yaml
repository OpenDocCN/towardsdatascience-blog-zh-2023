- en: Recommender Systems From Implicit Feedback Using TensorFlow Recommenders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/recommender-systems-from-implicit-feedback-using-tensorflow-recommenders-8ba36a976c57](https://towardsdatascience.com/recommender-systems-from-implicit-feedback-using-tensorflow-recommenders-8ba36a976c57)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[RECOMMENDATION SYSTEM](https://medium.com/tag/recommendation-system)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When customers don’t explicitly tell you what they want
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://dr-robert-kuebler.medium.com/?source=post_page-----8ba36a976c57--------------------------------)[![Dr.
    Robert Kübler](../Images/3b8d8b88f76c0c43d9c305e3885e7ab9.png)](https://dr-robert-kuebler.medium.com/?source=post_page-----8ba36a976c57--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8ba36a976c57--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8ba36a976c57--------------------------------)
    [Dr. Robert Kübler](https://dr-robert-kuebler.medium.com/?source=post_page-----8ba36a976c57--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8ba36a976c57--------------------------------)
    ·11 min read·Nov 5, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/085198f24d7254f94814a1ebdec4619b.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Noom Peerapong](https://unsplash.com/@imnoom?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Making recommendations is actually not that hard. You just have to check how
    your customers rated your products, for example using 1 to 5 stars, and then train
    a **regression** model on top of it. Right?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ae0ec03fd0b1941c5147d59abe3a80c0.png)'
  prefs: []
  type: TYPE_IMG
- en: A typical dataset that you would like to have. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, we might have to deal with embeddings if we don’t have any numerical
    user or movie features, but we have seen how to do that in my earlier article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/introduction-to-embedding-based-recommender-systems-956faceb1919?source=post_page-----8ba36a976c57--------------------------------)
    [## Introduction to Embedding-Based Recommender Systems'
  prefs: []
  type: TYPE_NORMAL
- en: Learn to build a simple matrix factorization recommender in TensorFlow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/introduction-to-embedding-based-recommender-systems-956faceb1919?source=post_page-----8ba36a976c57--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: We will also need embeddings in this article, so I suggest reading the article
    above before you continue.
  prefs: []
  type: TYPE_NORMAL
- en: Implicit Feedback
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: However, sometimes we are not in the lucky position of having **explicit** user
    feedback, such as stars, thumbs up or down, or similar. This happens quite a lot
    in retail, where we know which customer bought which item, but not if they actually
    liked it. The only things we get from the customers are **implicit signals about
    their interest** in this product.
  prefs: []
  type: TYPE_NORMAL
- en: If they bought (watched, consumed, …) the product, they have showed interest
    in it. If not, they were **maybe** not interested, but maybe just didn’t know
    about it yet. We cannot tell.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This sounds like we can treat a classification problem. Interested = 1, not
    interested = 0\. However, this is the small problem that we cannot be sure if
    a 0 (not interested) is **really** a zero. It can also be that the customer just
    never had the chance to buy it, but would actually like to.
  prefs: []
  type: TYPE_NORMAL
- en: Let us go back to the movies and assume that we don’t have any ratings. We only
    know which user watched which movie.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2987a72fdbf65a3e5923851a43666eb0.png)'
  prefs: []
  type: TYPE_IMG
- en: Alice watched Gaußzilla and The Markov Chainsaw Massacre, for example. Image
    by the author.
  prefs: []
  type: TYPE_NORMAL
- en: There are at least two ways we can proceed from here.
  prefs: []
  type: TYPE_NORMAL
- en: Just treat all missing values as zero and then train a binary classifier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use a pairwise loss function to ensure that the similarity between a user and
    a movie they watched is higher than the similarity between the same user and a
    movie they did **not** watch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Treat all missing values as zero
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is the simplest solution. From the incomplete table above, you would create
    the following dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '***Note: A*** *= Alice,* ***B*** *= Bob,* ***C*** *= Charlie,* ***G*** *= Gauß,*
    ***E*** *= Euler,* ***M*** *= Markov*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/dc8285c01783fc90b2c4f4bcc62fd864.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: You can interpret the **Watched** column as a label for whether the user is
    **interested in the movie or not**. From this table, you would deduce that, for
    example, user **A** does like movie **G**, but not movie **E**,which is a bold
    statement given the data. Maybe **A** does not know about **E** yet. Or even worse,
    it’s actually on **A**’s watchlist, but did not have time to watch it.
  prefs: []
  type: TYPE_NORMAL
- en: A problem on the technical side with this approach is that the model learns
    to say 0 to almost any (user, movie) input because most **Watched** values are
    typically zero. Imagine that you have a dataset of 1,000,000 users and 100,000
    movies. How many different movies does the average user watch? Maybe 1000? Then
    you **1%** of all Watched labels being 1\. So you have a heavily imbalanced dataset,
    which is nothing bad per se. However, since we **artificially create the zeros**,
    it can lead to bad performance.
  prefs: []
  type: TYPE_NORMAL
- en: A computational problem is that this dataset gets huge. 1,000,000 users times
    100,000 movies means you have a dataset with 100,000,000,000 rows. And often,
    you have more movies and items in your database. In this case, you don’t put all
    the zero target rows into your dataset, but you **subsample, also known as negative
    sampling**. For example, if you have 1,000,000,000 rows with target 1 in your
    dataset (= transactions that happened), you could subsample 1,000,000,000 negative
    samples (= transactions that never happened) as well. Then you have a nice dataset
    you can train on.
  prefs: []
  type: TYPE_NORMAL
- en: This works, but often not optimally since you make the problem harder than it
    has to be. You don’t have to predict a Watched label perfectly. You only want
    to rank movies for each user, i.e., you want to be able to say “User **A** likes
    movie **G** more than movie **E**”. The second approach gives us just that.
  prefs: []
  type: TYPE_NORMAL
- en: Use a pairwise loss function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this approach, we do not tell the model that a user does or does not like
    a specific movie. We phrase it more carefully:'
  prefs: []
  type: TYPE_NORMAL
- en: If a user A watched a movie G, but did not watch another movie E, we only say
    that A is more interested in G than in E.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This allows us to tackle an easier target. Now, let us start with some formulas,
    so we can better understand how this intuition translates into an algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: We will train a model that works with **embeddings** again. Let us assume that
    we have embeddings for user **A**, for movie **G,** and movie **E**. If **A**
    watched **G**, but not **E,** we simply want that
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/33995c21ff7c4dc7fa4deb3b801e798c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: where the *e*’s are the embeddings and · is the dot product. This implies that
    for user **A**, movie **G** is somehow **better** than movie **E**. But it is
    less drastic than saying “**A** likes **G** but does not like **E**” as in the
    binary classification case.
  prefs: []
  type: TYPE_NORMAL
- en: Training something like this sounds way more complicated than training a binary
    classifier, but several libraries get us covered. I will show you how to do it
    with [**TensorFlow Recommenders**](https://www.tensorflow.org/recommenders) since
    this is the most flexible library that I know. Another library worth mentioning
    that is easy to use, but inflexible is [implicit](https://benfred.github.io/implicit/).
  prefs: []
  type: TYPE_NORMAL
- en: Training With TensorFlow Recommenders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now see how easy it is to put this logic described before into code.
    Just to play with open cards,I’m following the [guide from the official TFRS website](https://www.tensorflow.org/recommenders/examples/basic_retrieval).
    I just tried to make it more concise.
  prefs: []
  type: TYPE_NORMAL
- en: '*Preparations and data generation*'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, let us do a
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: and then we can load some data via
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '`ratings` is a TensorFlow dataset that is always a bit tedious to handle. For
    memory-efficient training of huge datasets, you have to use it, though. But for
    our small example, I try to stay in the friendly dataframe world as much as possible,
    hence I convert the dataset to the dataframe `ratings_df` . The data looks like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/42a19cafd1bd357457c9ce228fd02f5b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Model definition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will build a model that has two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: a user model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a movie model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These models should take a user or movie respectively, and turn it into an embedding,
    i.e., a bunch of floats.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Using these two components, we can define the complete model like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We can see how the model consists of `user_model` and `movie_model` . We will
    set this `task` attribute to a **retrieval task**, which is implementing exactly
    what we want. There is also another type of task, called a **ranking task** that
    you can use when you have explicit feedback such as ratings. We will not look
    into this further in this article.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also see that some loss is computed. The input is a dictionary named
    `features` that is supposed to look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: It contains a bunch of user IDs as well as a bunch of movie titles. In this
    example, user **A** watched **G**, user **B** watched **E**, and user **C** watched
    **M**. We only have **positive examples** here, i.e., movie sessions that happened
    in the past.
  prefs: []
  type: TYPE_NORMAL
- en: The users and movies are turned into embeddings and then some loss is computed.
    I will go into detail later, but be assured that it is doing what we want it to
    do.
  prefs: []
  type: TYPE_NORMAL
- en: 'The architecture of the model is like in my other article:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/432cf2b8372b38b95ab77cca25a82110.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can use a nice TFRS prediction class in the end, but for it to work, we need
    a unique movie list as a TensorFlow dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Using this dataset, we can define the task that I talked about before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We can now fit the model!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see something like this in the end:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4fa10014f6516a019104171a83351b54.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: It is hard to give meaning to the `loss` , but the smaller the better. I will
    go into a bit more detail soon, but let us use our model first to predict some
    movies!
  prefs: []
  type: TYPE_NORMAL
- en: Prediction time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, you have to define something called an **index**. You can then use this
    index to get predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '**Note:** You don’t use `model` anymore. You only used it to adjust the parameters
    for `user_model` and `movie_model` , and now you use these two submodels directly.
    You can basically throw the TFRS model `model` away at this point.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, you can pass a user to `index` . The index will
  prefs: []
  type: TYPE_NORMAL
- en: turn this user ID into an embedding — this is possible because you passed it
    `user_model` — and then
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: compute the embedding for each movie — this is possible because of executing
    the `index_from_dataset` function — and then
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: output the movie titles whose embeddings are closest to the user embedding.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Note:** It is doing an exact nearest neighbor search under the hood, which
    can be slow. It also supports an approximate nearest neighbor search using [ScaNN](https://github.com/google-research/google-research/tree/master/scann).
    You can use it by typing `ScaNN` instead of `BruteForce` .'
  prefs: []
  type: TYPE_NORMAL
- en: 'It works like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Nice! You are now ready to use the model.
  prefs: []
  type: TYPE_NORMAL
- en: Loose ends
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There is still something I promised I would get into: this `task` and the loss
    it outputs. It is not very well documented (yet) what is happening inside, but
    I looked at the source code to see what is going on. [You can find the source
    code I’m referring to here](https://github.com/tensorflow/recommenders/blob/7caed557b9d5194202d8323f2d4795231a5d0b1d/tensorflow_recommenders/tasks/retrieval.py#L119).'
  prefs: []
  type: TYPE_NORMAL
- en: I will explain it to you using a small example batch.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e99f92bf4583acaeb2821ea5567cd9f4.png)'
  prefs: []
  type: TYPE_IMG
- en: The batch going into the model. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: It goes into the model, and then embeddings for each user and movie are created
    using `user_model` and `movie_model` . These embeddings go into the retrieval
    task object.
  prefs: []
  type: TYPE_NORMAL
- en: In the `task` , all user embeddings are multiplied (dot product) with all movie
    embeddings. This can be done by a simple matrix multiplication, where the movie
    matrix is transposed first. Let us assume that we use two-dimensional embeddings
    to save some space.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e6949c37f37826b9474b23e3b47d341f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: The matrix product is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ee3e4eb45f9fa7a327bd476daa3cdc25.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the reasoning is as follows: From the data, we know that'
  prefs: []
  type: TYPE_NORMAL
- en: Alice watched Gauß,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bob watched Euler, and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Charlie watched Markov.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That’s why we would like the corresponding numbers in the cells (**A**, **G**),
    (**B**, **E**), (**C**, **M**) — this is the main diagonal — to have the highest
    numbers. In this small example, we are way off.
  prefs: []
  type: TYPE_NORMAL
- en: 'To quantify this, they do another step: the authors of TFRS do a row-wise softmax.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/78ec8f81d6c217d9cd20982559a039b2.png)'
  prefs: []
  type: TYPE_IMG
- en: After a row-wise softmax. Note that the sum of each row is 1\. Image by the
    author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the observation is: if the elements on the main diagonal in the previous
    matrix are way higher than the other numbers, then the “softmaxed” matrix is close
    to the **identity matrix**.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/604191623a481dc49c87dd0ceca69bce.png)'
  prefs: []
  type: TYPE_IMG
- en: The optimal identity matrix. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is because if you take the softmax of an array if one number is way higher
    than the others, this number will be close to 1\. So the other numbers must be
    close to zero. Just try it out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: So, the loss then comes from comparing the matrix from above with the identity
    matrix. To be precise, the categorical cross-entropy loss is used. But not the
    mean across the rows, but the **sum**, [see here](https://github.com/tensorflow/recommenders/blob/7caed557b9d5194202d8323f2d4795231a5d0b1d/tensorflow_recommenders/tasks/retrieval.py#L84).
    That’s why the loss numbers are always so high. The larger the batches are, the
    larger the losses will be. So don’t be confused if the losses are suddenly super
    low, just because you changed the batch size from 10,000 to 1,000 or something
    similar.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this article, we have learned how to utilize implicit feedback data to build
    a recommender system. To do this, we used TensorFlow Recommenders since it scales
    well, and is very expressive: you can take any submodel — as long as it outputs
    an embedding — and stick them together to jointly train them using the `tfrs.Model`class.'
  prefs: []
  type: TYPE_NORMAL
- en: After training, you can use a convenient class to make actual predictions. If
    you use ScaNN, this should be quite fast, but if you need a search on steroids,
    you can use dedicated vector databases like [Qdrant](https://qdrant.tech/). You
    give it the user and movie embeddings from the trained models, and it does the
    search for you.
  prefs: []
  type: TYPE_NORMAL
- en: We have also taken a glimpse into the internals of the library to understand
    where the to-be-minimized loss is coming from, so this library is no pure magic
    anymore.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to learn how to assess the quality of an implicit feedback recommender,
    please refer to my other article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/the-guide-to-recommender-metrics-c5d72193ea2b?source=post_page-----8ba36a976c57--------------------------------)
    [## The Guide to Recommender Metrics'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating a recommender system offline can be tricky
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/the-guide-to-recommender-metrics-c5d72193ea2b?source=post_page-----8ba36a976c57--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: I hope that you learned something new, interesting, and valuable today. Thanks
    for reading!
  prefs: []
  type: TYPE_NORMAL
- en: '*If you have any questions, write me on* [*LinkedIn*](https://www.linkedin.com/in/dr-robert-k%C3%BCbler-983859150/)*!*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: And if you want to dive deeper into the world of algorithms, give my new publication
    **All About Algorithms** a try! I’m still searching for writers!
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/all-about-algorithms?source=post_page-----8ba36a976c57--------------------------------)
    [## All About Algorithms'
  prefs: []
  type: TYPE_NORMAL
- en: From intuitive explanations to in-depth analysis, algorithms come to life with
    examples, code, and awesome…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/all-about-algorithms?source=post_page-----8ba36a976c57--------------------------------)
  prefs: []
  type: TYPE_NORMAL
