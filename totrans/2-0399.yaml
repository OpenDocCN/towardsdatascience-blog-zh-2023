- en: Dealing with Boosted Sample Data in Cross-Validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/boosted-sample-data-in-cross-validation-7ee589460238](https://towardsdatascience.com/boosted-sample-data-in-cross-validation-7ee589460238)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/a846b2d703659565e18e57e9ea391996.png)'
  prefs: []
  type: TYPE_IMG
- en: Cross-validation excluding boosted sample data in CV test-folds. Image by @leddebruijn.
  prefs: []
  type: TYPE_NORMAL
- en: A Python implementation of cross-validation for boosted sample data to prevent
    data leakage and overestimating your model’s performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.louisdebruijn.com/?source=post_page-----7ee589460238--------------------------------)[![Louis
    de Bruijn](../Images/68627f062ae4527063c948fe2f8651ff.png)](https://medium.louisdebruijn.com/?source=post_page-----7ee589460238--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7ee589460238--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7ee589460238--------------------------------)
    [Louis de Bruijn](https://medium.louisdebruijn.com/?source=post_page-----7ee589460238--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7ee589460238--------------------------------)
    ·6 min read·Apr 18, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: This story introduces cross-validation for boosted (upsampled) sampling strategy
    data and an implementation in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Boosted sampling is often used to target the minority class labels in (highly)
    imbalanced datasets. If left unaccounted for in cross-validation, including boosted
    sampling data in the test set, will add a bias in a machine learning model’s accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'This story helps us understand this bias and provides an implementation of
    cross-validation that accounts for boosted sample data in Python. The following
    topics are addressed:'
  prefs: []
  type: TYPE_NORMAL
- en: Imbalanced classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random and boosted sampling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data leakage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An Imbalanced Data Example
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python Implementation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Imbalanced classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An imbalanced classification problem is a classification problem where the target
    variable distribution is biased or skewed, with few instances in the minority
    class and many instances in the majority class.
  prefs: []
  type: TYPE_NORMAL
- en: Many real-world problems are imbalanced. For instance, there are significantly
    fewer internet shopping returns than purchases and fewer spammed e-mails than
    normal e-mails (even though it sometimes does not seem like that).
  prefs: []
  type: TYPE_NORMAL
- en: Imbalanced classification poses a challenge for supervised machine learning
    models because it can result in models having poor predictive performance, especially
    for the minority class. Due to a disparity of target classes, the algorithm tends
    to categorize the class with more instances, the majority class. This introduces
    a bias of a false sense of the model’s accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Random and boosted sampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One way of dealing with imbalanced datasets is by balancing the target classes
    during data collection, usually by boosting the minority class label sampling
    via a set of heuristics or business rules.
  prefs: []
  type: TYPE_NORMAL
- en: In a random sampling technique, each sample has an equal probability of being
    chosen. Boosting or upsampling is a sampling technique in which some samples are
    more likely to be chosen. It can artificially sample minority class samples to
    balance the class label in unbalanced datasets.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, sampling people under the age of 30 because they tend to return
    their shopping more often than older people. Or sampling e-mails that contain
    words such as *guarantee,* *dollar,* and *price* are examples of boosted sampling.
  prefs: []
  type: TYPE_NORMAL
- en: Data leakage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We know that upsampling before splitting the test set leads to data leakage,
    as upsampled data is now also present in our test set, making it easier for our
    model to predict data.
  prefs: []
  type: TYPE_NORMAL
- en: However, what to do when we have a sample of data for our supervised machine
    learning model containing upsampled data?
  prefs: []
  type: TYPE_NORMAL
- en: We want to use this data to train our model on a more balanced dataset to learn
    patterns (even though they are inherently biased) in the minority class samples.
    But we do not want to use this data to test our model or do our hyperparameter
    tuning. How do we train on all data but only test on the randomly sampled subset
    of our data?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a846b2d703659565e18e57e9ea391996.png)'
  prefs: []
  type: TYPE_IMG
- en: Cross-validation excluding boosted sample data in CV test-folds. Image by @leddebruijn.
  prefs: []
  type: TYPE_NORMAL
- en: '`BoostedKFold` allows training on all training data in a cross-validated fold
    but excludes boosted sample data, defined by `-1` in the `groups` parameter of
    the `split` function. As the image below shows, the training fold indices are
    drawn independently of the group indices, but the test-fold indices are drawn
    from the randomly sampled group and not from the boosted sample group.'
  prefs: []
  type: TYPE_NORMAL
- en: An Imbalanced Data Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s look at an example using `sklearn.make_classification` binary classification
    dataset of 1000 samples distributed over 95% majority class and 5% minority class.
    30% of the minority class labels are obtained via boosted sampling technique,
    and the other 70% are randomly drawn.
  prefs: []
  type: TYPE_NORMAL
- en: In the visualization below, you can see the confusion matrix and classification
    report for 5-fold cross-validation in `StratifiedKFold`. Cross-validation is performed
    on all test-labels via `cross_val_predict`. The positive minority class label
    `1` totaled 50 samples, including boosted sample data and random sample data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2b02a7e829fb0e02973f78600b7da063.png)'
  prefs: []
  type: TYPE_IMG
- en: StratifiedKFold for imbalanced datasets. Image by @leddebruijn.
  prefs: []
  type: TYPE_NORMAL
- en: Now compare this to the `BoostKFold` implementation below. Cross-validation
    is performed on all test labels via a custom `_cross_val_predict` function, as
    `cross_val_predict` cannot handle test-set size differences in true Y labels.
    The positive minority class label `1` totaled 35 samples, including randomly drawn
    sample data and excluding boosted sample data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/185a7a0745a0ed14bcbdab9b7b19dc67.png)'
  prefs: []
  type: TYPE_IMG
- en: BoostedKFold for imbalanced datasets. Image by @leddebruijn.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation metrics for the minority positive class are significantly lower than
    the previous `StratifiedKFold` implementation for the minority class, giving us
    a more realistic view of the model performance on unseen (production) data. This
    is partly due to fewer instances (35 versus 50), making the minority class even
    harder to predict.
  prefs: []
  type: TYPE_NORMAL
- en: However, the decrease in performance can not be fully explained by this. F1-score
    drops by half, whereas the minority class instances only drop by 30%. The remaining
    drop in performance can be explained by removing easy-to-predict upsampled instances,
    as the model has already seen similarly upsampled data during the training phase.
  prefs: []
  type: TYPE_NORMAL
- en: Python Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`BoostedKFold` uses `StratifiedKFold` and `PredefinedSplit` under the hood
    implemented in the `.split()` method. I also added a `.plot()` method that visualizes
    the splits in the image shown as the thumbnail of this story.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The implementation can be used in `sklearn.GridSearchCV` or `sklearn.Pipeline`
    just like any other cross-validation class in scikit-learn. For the full script,
    please visit my [GitHub page](https://github.com/LouisdeBruijn/Medium/tree/master/cross_validation).
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Upsampling data can be a good way to boost minority class labels, which can
    be very relevant in the case of imbalanced datasets. In this story, you learned
    about a potential bias in data leakage and the prevention of overestimating the
    performance of your classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Happy coding! Don’t hesitate to ask me any questions you have.
  prefs: []
  type: TYPE_NORMAL
