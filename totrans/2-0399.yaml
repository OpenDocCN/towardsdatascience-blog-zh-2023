- en: Dealing with Boosted Sample Data in Cross-Validation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理交叉验证中的增强样本数据
- en: 原文：[https://towardsdatascience.com/boosted-sample-data-in-cross-validation-7ee589460238](https://towardsdatascience.com/boosted-sample-data-in-cross-validation-7ee589460238)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/boosted-sample-data-in-cross-validation-7ee589460238](https://towardsdatascience.com/boosted-sample-data-in-cross-validation-7ee589460238)
- en: '![](../Images/a846b2d703659565e18e57e9ea391996.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a846b2d703659565e18e57e9ea391996.png)'
- en: Cross-validation excluding boosted sample data in CV test-folds. Image by @leddebruijn.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在交叉验证中排除增强样本数据的测试折叠。图像由 @leddebruijn 提供。
- en: A Python implementation of cross-validation for boosted sample data to prevent
    data leakage and overestimating your model’s performance
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 防止数据泄漏和过度估计模型性能的增强样本数据交叉验证的Python实现
- en: '[](https://medium.louisdebruijn.com/?source=post_page-----7ee589460238--------------------------------)[![Louis
    de Bruijn](../Images/68627f062ae4527063c948fe2f8651ff.png)](https://medium.louisdebruijn.com/?source=post_page-----7ee589460238--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7ee589460238--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7ee589460238--------------------------------)
    [Louis de Bruijn](https://medium.louisdebruijn.com/?source=post_page-----7ee589460238--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.louisdebruijn.com/?source=post_page-----7ee589460238--------------------------------)[![Louis
    de Bruijn](../Images/68627f062ae4527063c948fe2f8651ff.png)](https://medium.louisdebruijn.com/?source=post_page-----7ee589460238--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7ee589460238--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7ee589460238--------------------------------)
    [Louis de Bruijn](https://medium.louisdebruijn.com/?source=post_page-----7ee589460238--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7ee589460238--------------------------------)
    ·6 min read·Apr 18, 2023
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7ee589460238--------------------------------)
    ·阅读时间6分钟·2023年4月18日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: This story introduces cross-validation for boosted (upsampled) sampling strategy
    data and an implementation in Python.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了增强（上采样）采样策略数据的交叉验证及其在Python中的实现。
- en: Boosted sampling is often used to target the minority class labels in (highly)
    imbalanced datasets. If left unaccounted for in cross-validation, including boosted
    sampling data in the test set, will add a bias in a machine learning model’s accuracy.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 增强采样通常用于针对（高度）不平衡数据集中的少数类标签。如果在交叉验证中没有考虑，测试集包含增强采样数据将导致机器学习模型的准确性出现偏差。
- en: 'This story helps us understand this bias and provides an implementation of
    cross-validation that accounts for boosted sample data in Python. The following
    topics are addressed:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文帮助我们理解这种偏差，并提供了考虑增强样本数据的交叉验证的Python实现。涉及以下主题：
- en: Imbalanced classification
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不平衡分类
- en: Random and boosted sampling
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机和增强采样
- en: Data leakage
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据泄漏
- en: An Imbalanced Data Example
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个不平衡数据示例
- en: Python Implementation
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python实现
- en: Conclusion
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结论
- en: Imbalanced classification
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不平衡分类
- en: An imbalanced classification problem is a classification problem where the target
    variable distribution is biased or skewed, with few instances in the minority
    class and many instances in the majority class.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 不平衡分类问题是指目标变量分布有偏或倾斜的问题，其中少数类实例较少，而多数类实例较多。
- en: Many real-world problems are imbalanced. For instance, there are significantly
    fewer internet shopping returns than purchases and fewer spammed e-mails than
    normal e-mails (even though it sometimes does not seem like that).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现实世界的问题是不平衡的。例如，互联网购物退货远少于购买，垃圾邮件远少于正常邮件（尽管有时看起来并非如此）。
- en: Imbalanced classification poses a challenge for supervised machine learning
    models because it can result in models having poor predictive performance, especially
    for the minority class. Due to a disparity of target classes, the algorithm tends
    to categorize the class with more instances, the majority class. This introduces
    a bias of a false sense of the model’s accuracy.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 不平衡分类对监督机器学习模型构成挑战，因为这可能导致模型的预测性能较差，尤其是对少数类。由于目标类的不平衡，算法倾向于分类实例更多的类，即多数类。这引入了对模型准确性的错误感知。
- en: Random and boosted sampling
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机和增强采样
- en: One way of dealing with imbalanced datasets is by balancing the target classes
    during data collection, usually by boosting the minority class label sampling
    via a set of heuristics or business rules.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 处理不平衡数据集的一种方法是在数据收集期间平衡目标类别，通常通过一组启发式或业务规则来增强少数类标签采样。
- en: In a random sampling technique, each sample has an equal probability of being
    chosen. Boosting or upsampling is a sampling technique in which some samples are
    more likely to be chosen. It can artificially sample minority class samples to
    balance the class label in unbalanced datasets.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机抽样技术中，每个样本被选择的概率是相等的。增强或上采样是一种采样技术，其中一些样本更有可能被选择。它可以人工地采样少数类样本，以平衡不平衡数据集中的类别标签。
- en: For instance, sampling people under the age of 30 because they tend to return
    their shopping more often than older people. Or sampling e-mails that contain
    words such as *guarantee,* *dollar,* and *price* are examples of boosted sampling.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，抽样年龄在30岁以下的人，因为他们比年长的人更频繁地退货。或者抽样包含 *guarantee*、*dollar* 和 *price* 等词的电子邮件，这些都是增强抽样的例子。
- en: Data leakage
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据泄露
- en: We know that upsampling before splitting the test set leads to data leakage,
    as upsampled data is now also present in our test set, making it easier for our
    model to predict data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，在分割测试集之前进行上采样会导致数据泄露，因为上采样数据现在也出现在我们的测试集中，使得我们的模型更容易预测数据。
- en: However, what to do when we have a sample of data for our supervised machine
    learning model containing upsampled data?
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当我们有一个包含上采样数据的监督机器学习模型的数据样本时，该怎么做？
- en: We want to use this data to train our model on a more balanced dataset to learn
    patterns (even though they are inherently biased) in the minority class samples.
    But we do not want to use this data to test our model or do our hyperparameter
    tuning. How do we train on all data but only test on the randomly sampled subset
    of our data?
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望使用这些数据在更平衡的数据集上训练我们的模型，以学习少数类样本的模式（尽管它们本质上有偏）。但我们不希望使用这些数据来测试我们的模型或进行超参数调优。我们如何在所有数据上进行训练，但仅在我们数据的随机抽样子集上进行测试？
- en: '![](../Images/a846b2d703659565e18e57e9ea391996.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a846b2d703659565e18e57e9ea391996.png)'
- en: Cross-validation excluding boosted sample data in CV test-folds. Image by @leddebruijn.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在CV测试折叠中排除增强样本数据的交叉验证。图像由 @leddebruijn 提供。
- en: '`BoostedKFold` allows training on all training data in a cross-validated fold
    but excludes boosted sample data, defined by `-1` in the `groups` parameter of
    the `split` function. As the image below shows, the training fold indices are
    drawn independently of the group indices, but the test-fold indices are drawn
    from the randomly sampled group and not from the boosted sample group.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`BoostedKFold` 允许在交叉验证折叠的所有训练数据上进行训练，但排除由 `-1` 定义在 `split` 函数的 `groups` 参数中的增强样本数据。如下面的图像所示，训练折叠的索引是独立于组索引抽取的，但测试折叠的索引则来自随机抽样的组，而不是来自增强样本组。'
- en: An Imbalanced Data Example
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个不平衡数据的例子
- en: Let’s look at an example using `sklearn.make_classification` binary classification
    dataset of 1000 samples distributed over 95% majority class and 5% minority class.
    30% of the minority class labels are obtained via boosted sampling technique,
    and the other 70% are randomly drawn.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个例子，使用 `sklearn.make_classification` 二分类数据集，共 1000 个样本，95% 为多数类，5% 为少数类。30%
    的少数类标签是通过增强采样技术获得的，其他 70% 是随机抽取的。
- en: In the visualization below, you can see the confusion matrix and classification
    report for 5-fold cross-validation in `StratifiedKFold`. Cross-validation is performed
    on all test-labels via `cross_val_predict`. The positive minority class label
    `1` totaled 50 samples, including boosted sample data and random sample data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的可视化中，你可以看到 `StratifiedKFold` 的 5 折交叉验证的混淆矩阵和分类报告。交叉验证是通过 `cross_val_predict`
    在所有测试标签上执行的。正类少数标签 `1` 总共有 50 个样本，包括增强样本数据和随机样本数据。
- en: '![](../Images/2b02a7e829fb0e02973f78600b7da063.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b02a7e829fb0e02973f78600b7da063.png)'
- en: StratifiedKFold for imbalanced datasets. Image by @leddebruijn.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 用于不平衡数据集的 StratifiedKFold。图像由 @leddebruijn 提供。
- en: Now compare this to the `BoostKFold` implementation below. Cross-validation
    is performed on all test labels via a custom `_cross_val_predict` function, as
    `cross_val_predict` cannot handle test-set size differences in true Y labels.
    The positive minority class label `1` totaled 35 samples, including randomly drawn
    sample data and excluding boosted sample data.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将其与下面的`BoostKFold`实现进行比较。通过自定义`_cross_val_predict`函数在所有测试标签上进行交叉验证，因为`cross_val_predict`无法处理真实Y标签中测试集大小的差异。少数类正标签`1`共有35个样本，包括随机抽取的样本数据，排除了增强的样本数据。
- en: '![](../Images/185a7a0745a0ed14bcbdab9b7b19dc67.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/185a7a0745a0ed14bcbdab9b7b19dc67.png)'
- en: BoostedKFold for imbalanced datasets. Image by @leddebruijn.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 适用于不平衡数据集的BoostedKFold。图片来自@leddebruijn。
- en: Evaluation metrics for the minority positive class are significantly lower than
    the previous `StratifiedKFold` implementation for the minority class, giving us
    a more realistic view of the model performance on unseen (production) data. This
    is partly due to fewer instances (35 versus 50), making the minority class even
    harder to predict.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 少数类正类的评估指标显著低于以前的`StratifiedKFold`实现，提供了模型在未见（生产）数据上的更真实表现。这部分由于实例较少（35比50），使得少数类更难预测。
- en: However, the decrease in performance can not be fully explained by this. F1-score
    drops by half, whereas the minority class instances only drop by 30%. The remaining
    drop in performance can be explained by removing easy-to-predict upsampled instances,
    as the model has already seen similarly upsampled data during the training phase.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，性能的下降不能完全用这个来解释。F1分数下降了一半，而少数类实例仅下降了30%。性能的剩余下降可以通过去除容易预测的上采样实例来解释，因为模型在训练阶段已经见过类似的上采样数据。
- en: Python Implementation
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python 实现
- en: '`BoostedKFold` uses `StratifiedKFold` and `PredefinedSplit` under the hood
    implemented in the `.split()` method. I also added a `.plot()` method that visualizes
    the splits in the image shown as the thumbnail of this story.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`BoostedKFold`在内部使用了`StratifiedKFold`和`PredefinedSplit`，这些都在`.split()`方法中实现。我还添加了一个`.plot()`方法，可以可视化在此故事缩略图中显示的分裂。'
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The implementation can be used in `sklearn.GridSearchCV` or `sklearn.Pipeline`
    just like any other cross-validation class in scikit-learn. For the full script,
    please visit my [GitHub page](https://github.com/LouisdeBruijn/Medium/tree/master/cross_validation).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实现可以像scikit-learn中的其他交叉验证类一样在`sklearn.GridSearchCV`或`sklearn.Pipeline`中使用。有关完整脚本，请访问我的[GitHub页面](https://github.com/LouisdeBruijn/Medium/tree/master/cross_validation)。
- en: Conclusion
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Upsampling data can be a good way to boost minority class labels, which can
    be very relevant in the case of imbalanced datasets. In this story, you learned
    about a potential bias in data leakage and the prevention of overestimating the
    performance of your classifier.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 上采样数据可以是提升少数类标签的有效方法，这在数据集不平衡的情况下尤为重要。在这个故事中，你了解到数据泄漏的潜在偏差和避免高估分类器性能的方法。
- en: Happy coding! Don’t hesitate to ask me any questions you have.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 编程愉快！如有任何问题，请随时问我。
