["```py\nimport zipfile\nimport pandas as pd\n\nzipped_data = zipfile.ZipFile(\"anime-recommendation-ltr-dataset.zip\")\n\nanime_info_df = pd.read_csv(zipped_data.open('anime_info.csv'))\nrelavence_scores = pd.read_csv(zipped_data.open('relavence_scores.csv'))\nuser_info = pd.read_csv(zipped_data.open('user_info.csv'))\n\nanime_info_df.columns.tolist()\n\n# ['anime_id',\n#  'Genres',\n#  'is_tv',\n#  'year_aired',\n#  'is_adult',\n#  'above_five_star_users',\n#  'above_five_star_ratings',\n#  'above_five_star_ratio']\n\nuser_info.columns.tolist()\n\n# ['user_id',\n#  'review_count',\n#  'avg_score',\n#  'score_stddev',\n#  'above_five_star_count',\n#  'above_five_star_ratio']\n\nrelevance_scores.columns.tolist()\n#['anime_id', 'anime_name', 'user_id', 'relevance_score']\n```", "```py\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\npopular_genres = ['Comedy',\n 'Action',\n 'Fantasy',\n 'Adventure',\n 'Kids',\n 'Drama',\n 'Sci-Fi',\n 'Music',\n 'Shounen',\n 'Slice of Life']\n\ndef create_genre_flags(df, popular_genres):\n    df = df.dropna(subset=['Genres'])\n    df['Genres'] = df['Genres'].apply(lambda x:\",\".join(s.strip() for s in x.split(\",\")))\n    # use MultiLabelBinarizer to create a one-hot encoded dataframe of the genres\n    mlb = MultiLabelBinarizer()\n    genre_df = pd.DataFrame(mlb.fit_transform(df['Genres'].str.split(',')),\n                            columns=mlb.classes_,\n                            index=df.index)\n    # create a new dataframe with the movie id and genre columns\n    new_df = pd.concat([df['anime_id'], genre_df[popular_genres]], axis=1)\n    new_df.columns = ['anime_id'] + popular_genres\n    return new_df\n\nanime_genre_info_df = create_genre_flags(anime_info_df,popular_genres)\nanime_info_df_final = anime_info_df.merge(anime_genre_info_df,on='anime_id')\nanime_info_df_final.columns = [col if col=='anime_id' else f\"ANIME_FEATURE {col}\".upper() for col in anime_info_df_final.columns]\nuser_info.columns = [col if col=='user_id' else f\"USER_FEATURE {col}\".upper() for col in user_info.columns]\n\ntrain_interim = relavence_scores.merge(anime_info_df_final)\ntrain = train_interim.merge(user_info,how='inner')\n```", "```py\nna_counts = (train.isna().sum() * 100/len(train))\ntrain_processed = train.drop(na_counts[na_counts > 50].index,axis=1)\ntrain_processed.sort_values(by='user_id',inplace=True)\ntrain_processed.set_index(\"user_id\",inplace=True)\n\nfeatures = ['ANIME_FEATURE IS_TV',\n       'ANIME_FEATURE YEAR_AIRED', 'ANIME_FEATURE IS_ADULT',\n       'ANIME_FEATURE ABOVE_FIVE_STAR_USERS',\n       'ANIME_FEATURE ABOVE_FIVE_STAR_RATINGS',\n       'ANIME_FEATURE ABOVE_FIVE_STAR_RATIO', 'ANIME_FEATURE COMEDY',\n       'ANIME_FEATURE ACTION', 'ANIME_FEATURE FANTASY',\n       'ANIME_FEATURE ADVENTURE', 'ANIME_FEATURE KIDS', 'ANIME_FEATURE DRAMA',\n       'ANIME_FEATURE SCI-FI', 'ANIME_FEATURE MUSIC', 'ANIME_FEATURE SHOUNEN',\n       'ANIME_FEATURE SLICE OF LIFE', 'USER_FEATURE REVIEW_COUNT',\n       'USER_FEATURE AVG_SCORE', 'USER_FEATURE SCORE_STDDEV',\n       'USER_FEATURE ABOVE_FIVE_STAR_COUNT',\n       'USER_FEATURE ABOVE_FIVE_STAR_RATIO']\ntarget = 'relavence_score'\n\ntest_size = int(1e5)\nX,y = train_processed[features],train_processed[target].apply(lambda x:int(x * 10))\ntest_idx_start = len(X)-test_size\n\nxtrain,xtest,ytrain,ytest = X.iloc[0:test_idx_start],X.iloc[test_idx_start:],y.iloc[0:test_idx_start],y.iloc[test_idx_start:]\n```", "```py\nget_group_size = lambda df: df.reset_index().groupby(\"user_id\")['user_id'].count()\n\ntrain_groups = get_group_size(xtrain)\ntest_groups = get_group_size(xtest)\n\nprint(sum(train_groups) , sum(test_groups))\n#(4764372, 100000)\n\nmodel = LGBMRanker(objective=\"lambdarank\")\nmodel.fit(xtrain,ytrain,group=train_groups,eval_set=[(xtest,ytest)],eval_group=[test_groups],eval_metric=['ndcg'])\n\n#....\n# [97] valid_0's ndcg@1: 0.900624 valid_0's ndcg@2: 0.900015 valid_0's ndcg@3: 0.892648 valid_0's ndcg@4: 0.891373 valid_0's ndcg@5: 0.886585\n# [98] valid_0's ndcg@1: 0.900624 valid_0's ndcg@2: 0.900015 valid_0's ndcg@3: 0.892648 valid_0's ndcg@4: 0.891895 valid_0's ndcg@5: 0.886632\n# [99] valid_0's ndcg@1: 0.900624 valid_0's ndcg@2: 0.901216 valid_0's ndcg@3: 0.892839 valid_0's ndcg@4: 0.892053 valid_0's ndcg@5: 0.88677\n# [100] valid_0's ndcg@1: 0.900624 valid_0's ndcg@2: 0.901216 valid_0's ndcg@3: 0.892839 valid_0's ndcg@4: 0.892053 valid_0's ndcg@5: 0.886363\n```", "```py\nuser_2_anime_df = relavence_scores.groupby(\"user_id\").agg({\"anime_id\":lambda x:list(set(x))})\nuser_2_anime_map = dict(zip(user_2_anime_df.index,user_2_anime_df['anime_id']))\n\n#create candidate pool, this will be a all the animes in the database\ncandidate_pool = anime_info_df_final['anime_id'].unique().tolist()\n\n#anime_id to it's name mapping\nanime_id_2_name = relavence_scores.drop_duplicates(subset=[\"anime_id\",\"Name\"])[['anime_id',\"Name\"]]\nanime_id_2_name_map = dict(zip(anime_id_2_name['anime_id'],anime_id_2_name['Name']))\n\ndef candidate_generation(user_id:int,candidate_pool:list,user_2_anime_map:dict,N:int):\n    \"\"\"\n    Note: this a totally random generation, only for demo purpose\n    Generates a list of N anime candidates for a given user based on their previously liked animes.\n\n    Parameters:\n        user_id (int): The user's ID.\n        candidate_pool (list): A list of all possible anime candidates.\n        user_2_anime_map (dict): A dictionary that maps users to their liked animes.\n        N (int): The number of anime candidates to generate.\n\n    Returns:\n        already_interacted (list): List of animes which user already liked\n        candidates (list): A list of N anime candidates for the user.\n    \"\"\"\n\n    #get the already liked animes\n    already_interacted = user_2_anime_map[user_id]\n\n    #candidates will be rest of animes which are not exposed to user\n    candidates = list(set(candidate_pool) - set(already_interacted))\n\n    return already_interacted,np.random.choice(candidates,size=N)\n\ndef generate_predictions(user_id,user_2_anime_map,candidate_pool,feature_columns,anime_id_2_name_map,ranker,N=100):\n    \"\"\"\n    Generates predictions for anime recommendations for a given user.\n\n    Parameters:\n        user_id (int): The user's ID.\n        user_2_anime_map (dict): A dictionary that maps users to their liked animes.\n        candidate_pool (list): A list of all possible anime candidates.\n        feature_columns (list): A list of feature columns to use for generating predictions.\n        anime_id_2_name_map (dict): A dictionary that maps anime IDs to their names.\n        ranker (object): A trained model object that is used to generate predictions.\n        N (int): The number of anime predictions to generate.\n\n    Returns:\n        predictions (DataFrame): A dataframe containing the top N anime recommendations for the user.\n    \"\"\"\n    already_liked,candidates = candidate_generation(user_id,candidate_pool,user_2_anime_map,N=10000)\n\n    #Create dataframe for candidates\n    candidates_df = pd.DataFrame(data=pd.Series(candidates,name='anime_id'))\n\n    # Merge with feature dataframe\n    features = anime_info_df_final.merge(candidates_df)\n\n    #Add user id as a feature\n    features['user_id'] = user_id\n\n    # Merge with user information\n    features = features.merge(user_info)\n\n    # If number of already liked animes is less than number of candidates\n    # Extend the already liked list with -1\n    already_liked = list(already_liked)\n    if len(already_liked) < len(candidates):\n        append_list = np.full(fill_value=-1,shape=(len(candidates)-len(already_liked)))\n        already_liked.extend(list(append_list))\n\n    #Create dataframe for predictions\n    predictions = pd.DataFrame(index=candidates)\n    #Add anime names\n    predictions['name'] = np.array([anime_id_2_name_map.get(id_) for id_ in candidates])\n    #Generate predictions\n    predictions['score'] = ranker.predict(features[feature_columns])\n    predictions = predictions.sort_values(by='score',ascending=False).head(N)\n\n    predictions[f'already_liked - sample[{N}]'] = [anime_id_2_name_map.get(id_) for id_ in already_liked[0:len(predictions)]]\n    return predictions\n\n#let's generate the predictions \ngenerate_predictions(123,user_2_anime_map,candidate_pool,feature_columns=features,anime_id_2_name_map=anime_id_2_name_map,ranker=model,N=10)\n```", "```py\nimport shap\n\ndef generate_shap_plots(ranker, X_train, feature_names, N=3):\n    \"\"\"\n    Generates SHAP plots for a pre-trained LightGBM model.\n\n    Parameters:\n        ranker (lightgbm.Booster): A trained LightGBM model\n        X_train (np.ndarray): The training data used to fit the model\n        feature_names (List): list of feature names\n        N (int): The number of plots to generate\n\n    Returns:\n        None\n    \"\"\"\n    explainer = shap.Explainer(ranker, X_train, feature_names=feature_names)\n    shap_values = explainer(X_train.iloc[:N])\n\n    # Create a figure with 2 subplots\n    # fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n\n    # Plot the summary plot on the first subplot\n    plt.subplot(1, 2, 1)\n    shap.summary_plot(shap_values, feature_names=feature_names, plot_type='bar')\n\n    # Plot the feature importance plot on the second subplot\n    plt.subplot(1, 2, 2)\n    shap.summary_plot(shap_values, feature_names=feature_names, plot_type='dot')\n\n    plt.show()\n\ngenerate_shap_plots(model,xtrain,features,N=10000)\n```"]