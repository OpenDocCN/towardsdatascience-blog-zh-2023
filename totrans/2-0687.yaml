- en: Decoding Strategies in Large Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è§£ç ç­–ç•¥
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/decoding-strategies-in-large-language-models-9733a8f70539](https://towardsdatascience.com/decoding-strategies-in-large-language-models-9733a8f70539)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/decoding-strategies-in-large-language-models-9733a8f70539](https://towardsdatascience.com/decoding-strategies-in-large-language-models-9733a8f70539)
- en: A Guide to Text Generation From Beam Search to Nucleus Sampling
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»æŸæœç´¢åˆ°æ ¸é‡‡æ ·çš„æ–‡æœ¬ç”ŸæˆæŒ‡å—
- en: '[](https://medium.com/@mlabonne?source=post_page-----9733a8f70539--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----9733a8f70539--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9733a8f70539--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9733a8f70539--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----9733a8f70539--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne?source=post_page-----9733a8f70539--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----9733a8f70539--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9733a8f70539--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9733a8f70539--------------------------------)
    [é©¬å…‹è¥¿å§†Â·æ‹‰åšè®·](https://medium.com/@mlabonne?source=post_page-----9733a8f70539--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9733a8f70539--------------------------------)
    Â·15 min readÂ·Jun 4, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9733a8f70539--------------------------------)
    Â·é˜…è¯»æ—¶é—´15åˆ†é’ŸÂ·2023å¹´6æœˆ4æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/8e3084bd4e009d7fb0c6ec5d7aef4aa6.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8e3084bd4e009d7fb0c6ec5d7aef4aa6.png)'
- en: Image by author.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: In the fascinating world of large language models (LLMs), much attention is
    given to model architectures, data processing, and optimization. However, decoding
    strategies like beam search, which play a crucial role in text generation, are
    often overlooked. In this article, we will explore how LLMs generate text by delving
    into the mechanics of greedy search and beam search, as well as sampling techniques
    with top-k and nucleus sampling.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¿·äººä¸–ç•Œä¸­ï¼Œå¾ˆå¤šå…³æ³¨éƒ½é›†ä¸­åœ¨æ¨¡å‹æ¶æ„ã€æ•°æ®å¤„ç†å’Œä¼˜åŒ–ä¸Šã€‚ç„¶è€Œï¼ŒåƒæŸæœç´¢è¿™æ ·çš„è§£ç ç­–ç•¥ï¼Œåœ¨æ–‡æœ¬ç”Ÿæˆä¸­å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ï¼Œå´å¸¸å¸¸è¢«å¿½è§†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨LLMå¦‚ä½•ç”Ÿæˆæ–‡æœ¬ï¼Œé€šè¿‡æ·±å…¥äº†è§£è´ªå©ªæœç´¢å’ŒæŸæœç´¢çš„æœºåˆ¶ï¼Œä»¥åŠä½¿ç”¨top-kå’Œnucleusé‡‡æ ·çš„é‡‡æ ·æŠ€æœ¯ã€‚
- en: By the conclusion of this article, youâ€™ll not only understand these decoding
    strategies thoroughly but also be familiar with how to handle important hyperparameters
    like temperature, num_beams, top_k, and top_p.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°æœ¬æ–‡ç»“æŸæ—¶ï¼Œä½ ä¸ä»…ä¼šå½»åº•ç†è§£è¿™äº›è§£ç ç­–ç•¥ï¼Œè¿˜ä¼šç†Ÿæ‚‰å¦‚ä½•å¤„ç†é‡è¦çš„è¶…å‚æ•°ï¼Œå¦‚æ¸©åº¦ã€num_beamsã€top_kå’Œtop_pã€‚
- en: The code for this article can be found on [GitHub](https://github.com/mlabonne/llm-course/blob/main/Decoding_Strategies_in_Large_Language%C2%A0Models.ipynb)
    and [Google Colab](https://colab.research.google.com/drive/19CJlOS5lI29g-B3dziNn93Enez1yiHk2?usp=sharing)
    for reference and further exploration.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡çš„ä»£ç å¯ä»¥åœ¨ [GitHub](https://github.com/mlabonne/llm-course/blob/main/Decoding_Strategies_in_Large_Language%C2%A0Models.ipynb)
    å’Œ [Google Colab](https://colab.research.google.com/drive/19CJlOS5lI29g-B3dziNn93Enez1yiHk2?usp=sharing)
    ä¸Šæ‰¾åˆ°ï¼Œä»¥ä¾›å‚è€ƒå’Œè¿›ä¸€æ­¥æ¢ç´¢ã€‚
- en: ğŸ“š Background
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ“š èƒŒæ™¯
- en: To kick things off, letâ€™s start with an example. Weâ€™ll feed the text â€œI have
    a dreamâ€ to a GPT-2 model and ask it to generate the next five tokens (words or
    subwords).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å¼€å§‹ï¼Œè®©æˆ‘ä»¬ä»ä¸€ä¸ªä¾‹å­å¼€å§‹ã€‚æˆ‘ä»¬å°†â€œæˆ‘æœ‰ä¸€ä¸ªæ¢¦æƒ³â€è¿™ä¸ªæ–‡æœ¬è¾“å…¥åˆ°GPT-2æ¨¡å‹ä¸­ï¼Œå¹¶è¦æ±‚å®ƒç”Ÿæˆæ¥ä¸‹æ¥çš„äº”ä¸ªæ ‡è®°ï¼ˆå•è¯æˆ–å­å•è¯ï¼‰ã€‚
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The sentence â€œI have a dream of being a doctorâ€ appears to have been generated
    by GPT-2\. However, GPT-2 didnâ€™t *exactly* produce this sentence.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å¥å­â€œæˆ‘æœ‰ä¸€ä¸ªæˆä¸ºåŒ»ç”Ÿçš„æ¢¦æƒ³â€ä¼¼ä¹æ˜¯ç”±GPT-2ç”Ÿæˆçš„ã€‚ç„¶è€Œï¼ŒGPT-2å¹¶æ²¡æœ‰*å‡†ç¡®*ç”Ÿæˆè¿™ä¸ªå¥å­ã€‚
- en: 'Thereâ€™s a common misconception that LLMs like GPT-2 **directly produce text**.
    This isnâ€™t the case. Instead, LLMs calculate logits, which are scores assigned
    to every possible token in their vocabulary. To simplify, hereâ€™s an illustrative
    breakdown of the process:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸€ç§æ™®éçš„è¯¯è§£æ˜¯ï¼ŒåƒGPT-2è¿™æ ·çš„LLM **ç›´æ¥ç”Ÿæˆæ–‡æœ¬**ã€‚äº‹å®å¹¶éå¦‚æ­¤ã€‚ç›¸åï¼ŒLLMè®¡ç®—logitsï¼Œå³åˆ†é…ç»™å…¶è¯æ±‡è¡¨ä¸­æ¯ä¸ªå¯èƒ½æ ‡è®°çš„åˆ†æ•°ã€‚ä¸ºäº†ç®€åŒ–ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªè¯´æ˜æ€§è¿‡ç¨‹åˆ†è§£ï¼š
- en: '![](../Images/27050515483e3936d16fe97747ec1884.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/27050515483e3936d16fe97747ec1884.png)'
- en: Image by author.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: The tokenizer, [Byte-Pair Encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding)
    in this instance, translates each token in the input text into a corresponding
    token ID. Then, GPT-2 uses these token IDs as input and tries to predict the next
    most likely token. Finally, the model generates logits, which are converted into
    probabilities using a softmax function.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ†è¯å™¨ï¼Œ[å­—èŠ‚å¯¹ç¼–ç ](https://en.wikipedia.org/wiki/Byte_pair_encoding) åœ¨è¿™ä¸ªå®ä¾‹ä¸­ï¼Œå°†è¾“å…¥æ–‡æœ¬ä¸­çš„æ¯ä¸ªè¯ç¿»è¯‘æˆå¯¹åº”çš„è¯
    IDã€‚ç„¶åï¼ŒGPT-2 ä½¿ç”¨è¿™äº›è¯ ID ä½œä¸ºè¾“å…¥å¹¶å°è¯•é¢„æµ‹ä¸‹ä¸€ä¸ªæœ€å¯èƒ½çš„è¯ã€‚æœ€åï¼Œæ¨¡å‹ç”Ÿæˆ logitsï¼Œè¿™äº› logits é€šè¿‡ softmax å‡½æ•°è½¬æ¢ä¸ºæ¦‚ç‡ã€‚
- en: For example, the model assigns a probability of 17% to the token for â€œofâ€ being
    the next token after â€œI have a dreamâ€. This output essentially represents a ranked
    list of potential next tokens in the sequence. More formally, we denote this probability
    as *P(of | I have a dream) = 17%*.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œè¯¥æ¨¡å‹ä¸ºâ€œofâ€ä½œä¸º â€œI have a dreamâ€ ä¹‹åçš„ä¸‹ä¸€ä¸ªè¯åˆ†é…äº† 17% çš„æ¦‚ç‡ã€‚è¿™ä¸ªè¾“å‡ºæœ¬è´¨ä¸Šè¡¨ç¤ºäº†æ½œåœ¨ä¸‹ä¸€ä¸ªè¯çš„æ’ååˆ—è¡¨ã€‚æ›´æ­£å¼åœ°ï¼Œæˆ‘ä»¬å°†è¿™ä¸ªæ¦‚ç‡è¡¨ç¤ºä¸º
    *P(of | I have a dream) = 17%*ã€‚
- en: 'Autoregressive models like GPT predict the next token in a sequence based on
    the preceding tokens. Consider a sequence of tokens *w = (w*â‚*, w*â‚‚*, â€¦, w*â‚œ*)*.
    The joint probability of this sequence *P(w)* can be broken down as:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªå›å½’æ¨¡å‹å¦‚ GPT ä¼šåŸºäºå‰é¢çš„è¯é¢„æµ‹åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªè¯ã€‚è€ƒè™‘ä¸€ä¸ªè¯åºåˆ— *w = (w*â‚*, w*â‚‚*, â€¦, w*â‚œ*)*ã€‚è¿™ä¸ªåºåˆ—çš„è”åˆæ¦‚ç‡ *P(w)*
    å¯ä»¥è¢«åˆ†è§£ä¸ºï¼š
- en: '![](../Images/4b65c8dd18074c1330b34b2d9bbb8250.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b65c8dd18074c1330b34b2d9bbb8250.png)'
- en: For each token *wáµ¢* in the sequence, *P(wáµ¢ | wâ‚, wâ‚‚, â€¦, wáµ¢â‚‹â‚)* represents the
    conditional probability of *wáµ¢* given all the preceding tokens (*wâ‚, wâ‚‚, â€¦, wáµ¢â‚‹â‚*).
    GPT-2 calculates this conditional probability for each of the 50,257 tokens in
    its vocabulary.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºåºåˆ—ä¸­çš„æ¯ä¸ªè¯ *wáµ¢*ï¼Œ*P(wáµ¢ | wâ‚, wâ‚‚, â€¦, wáµ¢â‚‹â‚)* è¡¨ç¤ºåœ¨ç»™å®šæ‰€æœ‰å‰é¢çš„è¯ï¼ˆ*wâ‚, wâ‚‚, â€¦, wáµ¢â‚‹â‚*ï¼‰çš„æƒ…å†µä¸‹
    *wáµ¢* çš„æ¡ä»¶æ¦‚ç‡ã€‚GPT-2 ä¸ºå…¶è¯æ±‡è¡¨ä¸­çš„æ¯ä¸€ä¸ª 50,257 ä¸ªè¯è®¡ç®—è¿™ä¸ªæ¡ä»¶æ¦‚ç‡ã€‚
- en: 'This leads to the question: how do we use these probabilities to generate text?
    This is where decoding strategies, such as greedy search and beam search, come
    into play.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±å¼•å‡ºäº†ä¸€ä¸ªé—®é¢˜ï¼šæˆ‘ä»¬å¦‚ä½•åˆ©ç”¨è¿™äº›æ¦‚ç‡ç”Ÿæˆæ–‡æœ¬ï¼Ÿè¿™å°±æ˜¯è§£ç ç­–ç•¥ï¼ˆå¦‚è´ªå©ªæœç´¢å’ŒæŸæœç´¢ï¼‰å‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚
- en: ğŸƒâ€â™‚ï¸ Greedy Search
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸƒâ€â™‚ï¸ è´ªå©ªæœç´¢
- en: 'Greedy search is a decoding method that takes the most probable token at each
    step as the next token in the sequence. To put it simply, it only retains the
    most likely token at each stage, discarding all other potential options. Using
    our example:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è´ªå©ªæœç´¢æ˜¯ä¸€ç§è§£ç æ–¹æ³•ï¼Œå®ƒåœ¨æ¯ä¸€æ­¥éƒ½é€‰æ‹©æœ€å¯èƒ½çš„è¯ä½œä¸ºåºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªè¯ã€‚ç®€å•æ¥è¯´ï¼Œå®ƒåªä¿ç•™æ¯ä¸ªé˜¶æ®µä¸­æœ€å¯èƒ½çš„è¯ï¼Œä¸¢å¼ƒæ‰€æœ‰å…¶ä»–æ½œåœ¨çš„é€‰é¡¹ã€‚ä»¥æˆ‘ä»¬çš„ä¾‹å­ä¸ºä¾‹ï¼š
- en: '**Step 1**: Input: â€œI have a dreamâ€ â†’ Most likely token: â€œ ofâ€'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 1**ï¼šè¾“å…¥ï¼šâ€œI have a dreamâ€ â†’ æœ€å¯èƒ½çš„è¯ï¼š â€œ ofâ€'
- en: '**Step 2**: Input: â€œI have a dream ofâ€ â†’ Most likely token: â€œ beingâ€'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 2**ï¼šè¾“å…¥ï¼šâ€œI have a dream ofâ€ â†’ æœ€å¯èƒ½çš„è¯ï¼š â€œ beingâ€'
- en: '**Step 3**: Input: â€œI have a dream of beingâ€ â†’ Most likely token: â€œ aâ€'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 3**ï¼šè¾“å…¥ï¼šâ€œI have a dream of beingâ€ â†’ æœ€å¯èƒ½çš„è¯ï¼š â€œ aâ€'
- en: '**Step 4**: Input: â€œI have a dream of being aâ€ â†’ Most likely token: â€œ doctorâ€'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 4**ï¼šè¾“å…¥ï¼šâ€œI have a dream of being aâ€ â†’ æœ€å¯èƒ½çš„è¯ï¼š â€œ doctorâ€'
- en: '**Step 5**: Input: â€œI have a dream of being a doctorâ€ â†’ Most likely token:
    â€œ.â€'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 5**ï¼šè¾“å…¥ï¼šâ€œI have a dream of being a doctorâ€ â†’ æœ€å¯èƒ½çš„è¯ï¼š â€œ.â€'
- en: 'While this approach might sound intuitive, itâ€™s important to note that the
    greedy search is short-sighted: it only considers the most probable token at each
    step without considering the overall effect on the sequence. This property makes
    it fast and efficient as it doesnâ€™t need to keep track of multiple sequences,
    but it also means that it can miss out on better sequences that might have appeared
    with slightly less probable next tokens.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡è¿™ç§æ–¹æ³•å¬èµ·æ¥ç›´è§‚ï¼Œä½†éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè´ªå©ªæœç´¢å…·æœ‰çŸ­è§†æ€§ï¼šå®ƒåªè€ƒè™‘æ¯ä¸€æ­¥ä¸­æœ€å¯èƒ½çš„è¯ï¼Œè€Œä¸è€ƒè™‘å¯¹æ•´ä¸ªåºåˆ—çš„æ€»ä½“å½±å“ã€‚è¿™ä¸€ç‰¹æ€§ä½¿å¾—å®ƒå¿«é€Ÿä¸”é«˜æ•ˆï¼Œå› ä¸ºå®ƒä¸éœ€è¦è·Ÿè¸ªå¤šä¸ªåºåˆ—ï¼Œä½†ä¹Ÿæ„å‘³ç€å®ƒå¯èƒ½é”™è¿‡äº†é‚£äº›é€šè¿‡ç¨å¾®ä¸é‚£ä¹ˆå¯èƒ½çš„ä¸‹ä¸€ä¸ªè¯å¯èƒ½å‡ºç°çš„æ›´å¥½åºåˆ—ã€‚
- en: Next, letâ€™s illustrate the greedy search implementation using graphviz and networkx.
    We select the ID with the highest score, compute its log probability (we take
    the log to simplify calculations), and add it to the tree. Weâ€™ll repeat this process
    for five tokens.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨ graphviz å’Œ networkx æ¥è¯´æ˜è´ªå©ªæœç´¢çš„å®ç°ã€‚æˆ‘ä»¬é€‰æ‹©å¾—åˆ†æœ€é«˜çš„ IDï¼Œè®¡ç®—å…¶å¯¹æ•°æ¦‚ç‡ï¼ˆæˆ‘ä»¬å–å¯¹æ•°ä»¥ç®€åŒ–è®¡ç®—ï¼‰ï¼Œå¹¶å°†å…¶æ·»åŠ åˆ°æ ‘ä¸­ã€‚æˆ‘ä»¬å°†ä¸ºäº”ä¸ªè¯é‡å¤è¿™ä¸ªè¿‡ç¨‹ã€‚
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Our greedy search generates the same text as the one from the transformers
    library: â€œI have a dream of being a doctor.â€ Letâ€™s visualize the tree we created.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„è´ªå©ªæœç´¢ç”Ÿæˆçš„æ–‡æœ¬ä¸ transformers åº“ä¸­çš„æ–‡æœ¬ç›¸åŒï¼šâ€œI have a dream of being a doctorã€‚â€è®©æˆ‘ä»¬å¯è§†åŒ–æˆ‘ä»¬åˆ›å»ºçš„æ ‘ã€‚
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/dcb58f405a250824dbd57e3f478bfd51.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dcb58f405a250824dbd57e3f478bfd51.png)'
- en: Image by author.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡ã€‚
- en: In this graph, the top node stores the input token (thus with a 100% probability),
    while all other nodes represent generated tokens. Although each token in this
    sequence was the most likely at the time of prediction, â€œbeingâ€ and â€œdoctorâ€ were
    assigned relatively low probabilities of 9.68% and 2.86%, respectively. This suggests
    that â€œofâ€, our first predicted token, may not have been the most suitable choice
    as it led to â€œbeingâ€, which is quite unlikely.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ­¤å›¾ä¸­ï¼Œé¡¶çº§èŠ‚ç‚¹å­˜å‚¨è¾“å…¥ä»¤ç‰Œï¼ˆå› æ­¤æ¦‚ç‡ä¸º100%ï¼‰ï¼Œè€Œæ‰€æœ‰å…¶ä»–èŠ‚ç‚¹è¡¨ç¤ºç”Ÿæˆçš„ä»¤ç‰Œã€‚è™½ç„¶è¿™ä¸ªåºåˆ—ä¸­çš„æ¯ä¸ªä»¤ç‰Œåœ¨é¢„æµ‹æ—¶éƒ½æ˜¯æœ€å¯èƒ½çš„ï¼Œä½†â€œbeingâ€å’Œâ€œdoctorâ€è¢«åˆ†é…äº†ç›¸å¯¹è¾ƒä½çš„æ¦‚ç‡ï¼Œåˆ†åˆ«ä¸º9.68%å’Œ2.86%ã€‚è¿™è¡¨æ˜â€œofâ€ï¼Œæˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªé¢„æµ‹ä»¤ç‰Œï¼Œå¯èƒ½ä¸æ˜¯æœ€åˆé€‚çš„é€‰æ‹©ï¼Œå› ä¸ºå®ƒå¯¼è‡´äº†â€œbeingâ€ï¼Œè¿™æ˜¯ç›¸å½“ä¸å¯èƒ½çš„ã€‚
- en: In the following section, weâ€™ll explore how beam search can address this problem.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨å…‰æŸæœç´¢å¦‚ä½•è§£å†³è¿™ä¸ªé—®é¢˜ã€‚
- en: âš–ï¸ Beam Search
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: âš–ï¸ Beam Search
- en: Unlike greedy search, which only considers the next most probable token, beam
    search takes into account the *n* most likely tokens, where *n* represents the
    number of beams. This procedure is repeated until a predefined maximum length
    is reached or an end-of-sequence token appears. At this point, the sequence (or
    â€œbeamâ€) with the highest overall score is chosen as the output.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ä»…è€ƒè™‘ä¸‹ä¸€ä¸ªæœ€å¯èƒ½ä»¤ç‰Œçš„è´ªå©ªæœç´¢ä¸åŒï¼Œå…‰æŸæœç´¢è€ƒè™‘äº†*n*ä¸ªæœ€å¯èƒ½çš„ä»¤ç‰Œï¼Œå…¶ä¸­*n*ä»£è¡¨å…‰æŸçš„æ•°é‡ã€‚è¿™ä¸ªè¿‡ç¨‹ä¼šé‡å¤ï¼Œç›´åˆ°è¾¾åˆ°é¢„å®šä¹‰çš„æœ€å¤§é•¿åº¦æˆ–å‡ºç°åºåˆ—ç»“æŸä»¤ç‰Œã€‚æ­¤æ—¶ï¼Œå…·æœ‰æœ€é«˜æ€»ä½“åˆ†æ•°çš„åºåˆ—ï¼ˆæˆ–â€œå…‰æŸâ€ï¼‰è¢«é€‰ä¸ºè¾“å‡ºã€‚
- en: We can adapt the previous function to consider the *n* most probable tokens
    instead of just one. Here, weâ€™ll maintain the sequence score log *P(w)*, which
    is the cumulative sum of the log probability of every token in the beam. We normalize
    this score by the sequence length to prevent bias towards longer sequences (this
    factor can be adjusted). Once again, weâ€™ll generate five additional tokens to
    complete the sentence â€œI have a dream.â€
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥è°ƒæ•´ä¹‹å‰çš„å‡½æ•°ï¼Œä»¥è€ƒè™‘*n*ä¸ªæœ€å¯èƒ½çš„ä»¤ç‰Œï¼Œè€Œä¸ä»…ä»…æ˜¯ä¸€ä¸ªã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†ä¿æŒåºåˆ—åˆ†æ•°æ—¥å¿—*P(w)*ï¼Œå®ƒæ˜¯å…‰æŸä¸­æ¯ä¸ªä»¤ç‰Œå¯¹æ•°æ¦‚ç‡çš„ç´¯ç§¯å’Œã€‚æˆ‘ä»¬é€šè¿‡åºåˆ—é•¿åº¦æ¥å½’ä¸€åŒ–æ­¤åˆ†æ•°ï¼Œä»¥é˜²æ­¢å¯¹è¾ƒé•¿åºåˆ—çš„åå€šï¼ˆè¿™ä¸ªå› å­å¯ä»¥è°ƒæ•´ï¼‰ã€‚æˆ‘ä»¬å°†ç”Ÿæˆäº”ä¸ªé¢å¤–çš„ä»¤ç‰Œæ¥å®Œæˆå¥å­â€œI
    have a dream.â€
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The function computes the scores for 63 tokens and beams^length = 5Â² = 25 possible
    sequences. In our implementation, all the information is stored in the graph.
    Our next step is to extract the best sequence.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥å‡½æ•°è®¡ç®—63ä¸ªä»¤ç‰Œçš„åˆ†æ•°å’Œbeams^length = 5Â² = 25ä¸ªå¯èƒ½åºåˆ—ã€‚åœ¨æˆ‘ä»¬çš„å®ç°ä¸­ï¼Œæ‰€æœ‰ä¿¡æ¯éƒ½å­˜å‚¨åœ¨å›¾è¡¨ä¸­ã€‚æˆ‘ä»¬çš„ä¸‹ä¸€æ­¥æ˜¯æå–æœ€ä½³åºåˆ—ã€‚
- en: 'First, we identify the leaf node with the highest sequence score. Next, we
    find the shortest path from the root to this leaf. Every node along this path
    contains a token from the optimal sequence. Hereâ€™s how we can implement it:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬ç¡®å®šå…·æœ‰æœ€é«˜åºåˆ—åˆ†æ•°çš„å¶èŠ‚ç‚¹ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ‰¾åˆ°ä»æ ¹åˆ°è¿™ä¸ªå¶å­èŠ‚ç‚¹çš„æœ€çŸ­è·¯å¾„ã€‚æ²¿ç€è¿™æ¡è·¯å¾„çš„æ¯ä¸ªèŠ‚ç‚¹éƒ½åŒ…å«äº†æœ€ä¼˜åºåˆ—ä¸­çš„ä¸€ä¸ªä»¤ç‰Œã€‚ä»¥ä¸‹æ˜¯æˆ‘ä»¬å¦‚ä½•å®ç°å®ƒï¼š
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The best sequence seems to be â€œI have a dream. I have a dream,â€ which is a common
    response from GPT-2, even though it may be surprising. To verify this, letâ€™s plot
    the graph.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ä½³åºåˆ—ä¼¼ä¹æ˜¯â€œI have a dream. I have a dreamâ€ï¼Œè¿™æ˜¯GPT-2çš„å¸¸è§å“åº”ï¼Œå°½ç®¡è¿™å¯èƒ½ä»¤äººæƒŠè®¶ã€‚ä¸ºäº†éªŒè¯è¿™ä¸€ç‚¹ï¼Œè®©æˆ‘ä»¬ç»˜åˆ¶å›¾è¡¨ã€‚
- en: In this visualization, weâ€™ll display the sequence score for each node, which
    represents the score of the sequence up to that point. If the function get_best_sequence()
    is correct, the â€œdreamâ€ node in the sequence â€œI have a dream. I have a dreamâ€
    should have the highest score among all the leaf nodes.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªå¯è§†åŒ–ä¸­ï¼Œæˆ‘ä»¬å°†æ˜¾ç¤ºæ¯ä¸ªèŠ‚ç‚¹çš„åºåˆ—åˆ†æ•°ï¼Œè¿™ä»£è¡¨äº†åˆ°è¯¥ç‚¹ä¸ºæ­¢çš„åºåˆ—åˆ†æ•°ã€‚å¦‚æœå‡½æ•°get_best_sequence()æ˜¯æ­£ç¡®çš„ï¼Œåˆ™åºåˆ—â€œI have
    a dream. I have a dreamâ€ä¸­çš„â€œdreamâ€èŠ‚ç‚¹åº”è¯¥åœ¨æ‰€æœ‰å¶èŠ‚ç‚¹ä¸­å…·æœ‰æœ€é«˜åˆ†æ•°ã€‚
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/64c8bcbca63a9847c6a66c247c0f25ce.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/64c8bcbca63a9847c6a66c247c0f25ce.png)'
- en: Indeed, the â€œdreamâ€ token has the **highest sequence score** with a value of
    -0.69\. Interestingly, we can see the score of the greedy sequence â€œI have a dream
    of being a doctor.â€ on the left with a value of -1.16.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡®å®ï¼Œâ€œdreamâ€ä»¤ç‰Œå…·æœ‰**æœ€é«˜åºåˆ—åˆ†æ•°**ï¼Œå€¼ä¸º-0.69ã€‚ä»¤äººæ„Ÿå…´è¶£çš„æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å·¦ä¾§è´ªå©ªåºåˆ—â€œI have a dream of being
    a doctor.â€çš„åˆ†æ•°å€¼ä¸º-1.16ã€‚
- en: 'As expected, the greedy search leads to suboptimal results. But, to be honest,
    our new outcome is not particularly compelling either. To generate more varied
    sequences, weâ€™ll implement two sampling algorithms: top-k and nucleus.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œè´ªå©ªæœç´¢å¯¼è‡´äº†æ¬¡ä¼˜ç»“æœã€‚ä½†æ˜¯ï¼Œè€å®è¯´ï¼Œæˆ‘ä»¬çš„æ–°ç»“æœä¹Ÿæ²¡æœ‰ç‰¹åˆ«å¼•äººæ³¨ç›®ã€‚ä¸ºäº†ç”Ÿæˆæ›´å¤šæ ·åŒ–çš„åºåˆ—ï¼Œæˆ‘ä»¬å°†å®ç°ä¸¤ç§é‡‡æ ·ç®—æ³•ï¼štop-kå’Œnucleusã€‚
- en: ğŸ² Top-k sampling
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ² Top-ké‡‡æ ·
- en: Top-k sampling is a technique that leverages the probability distribution generated
    by the language model to **select a token randomly from the *k* most likely options**.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Top-ké‡‡æ ·æ˜¯ä¸€ç§åˆ©ç”¨è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æ¦‚ç‡åˆ†å¸ƒæ¥**ä»æœ€å¯èƒ½çš„*k*ä¸ªé€‰é¡¹ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªä»¤ç‰Œ**çš„æŠ€æœ¯ã€‚
- en: 'To illustrate, suppose we have *k = 3* and four tokens: A, B, C, and D, with
    respective probabilities: *P(A) = 30%*, *P(B) = 15%*, *P(C) = 5%*, and *P(D) =
    1%*. In top-k sampling, token D is disregarded, and the algorithm will output
    A 60% of the time, B 30% of the time, and C 10% of the time. This approach ensures
    that we prioritize the most probable tokens while introducing an element of randomness
    in the selection process.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¾ä¾‹æ¥è¯´ï¼Œå‡è®¾æˆ‘ä»¬æœ‰*k = 3*å’Œå››ä¸ªtokenï¼šAã€Bã€Cå’ŒDï¼Œåˆ†åˆ«çš„æ¦‚ç‡ä¸ºï¼š*P(A) = 30%*ã€*P(B) = 15%*ã€*P(C) = 5%*å’Œ*P(D)
    = 1%*ã€‚åœ¨top-ké‡‡æ ·ä¸­ï¼Œtoken D è¢«å¿½ç•¥ï¼Œç®—æ³•ä¼šåœ¨60%çš„æ—¶é—´å†…è¾“å‡ºAï¼Œåœ¨30%çš„æ—¶é—´å†…è¾“å‡ºBï¼Œåœ¨10%çš„æ—¶é—´å†…è¾“å‡ºCã€‚è¿™ç§æ–¹æ³•ç¡®ä¿æˆ‘ä»¬ä¼˜å…ˆè€ƒè™‘æœ€å¯èƒ½çš„tokenï¼ŒåŒæ—¶åœ¨é€‰æ‹©è¿‡ç¨‹ä¸­å¼•å…¥ä¸€å®šçš„éšæœºæ€§ã€‚
- en: 'Another way of introducing randomness is the concept of temperature. The temperature
    *T* is a parameter that ranges from 0 to 1, which affects the probabilities generated
    by the softmax function, making the most likely tokens more influential. In practice,
    it simply consists of dividing the input logits by a value we call temperature:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å¼•å…¥éšæœºæ€§çš„å¦ä¸€ç§æ–¹å¼æ˜¯æ¸©åº¦çš„æ¦‚å¿µã€‚æ¸©åº¦*T*æ˜¯ä¸€ä¸ªèŒƒå›´ä»0åˆ°1çš„å‚æ•°ï¼Œå®ƒå½±å“softmaxå‡½æ•°ç”Ÿæˆçš„æ¦‚ç‡ï¼Œä½¿æœ€å¯èƒ½çš„tokenæ›´å…·å½±å“åŠ›ã€‚åœ¨å®é™…æ“ä½œä¸­ï¼Œå®ƒç®€å•åœ°åŒ…æ‹¬å°†è¾“å…¥logitsé™¤ä»¥æˆ‘ä»¬ç§°ä¹‹ä¸ºæ¸©åº¦çš„å€¼ï¼š
- en: '![](../Images/b3e030655ba992e2bc09d381d196c363.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3e030655ba992e2bc09d381d196c363.png)'
- en: Here is a chart that demonstrates the impact of temperature on the probabilities
    generated for a given set of input logits [1.5, -1.8, 0.9, -3.2]. Weâ€™ve plotted
    three different temperature values to observe the differences.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ä¸€ä¸ªå›¾è¡¨ï¼Œå±•ç¤ºäº†æ¸©åº¦å¯¹ç»™å®šè¾“å…¥logits [1.5, -1.8, 0.9, -3.2] ç”Ÿæˆçš„æ¦‚ç‡çš„å½±å“ã€‚æˆ‘ä»¬ç»˜åˆ¶äº†ä¸‰ç§ä¸åŒçš„æ¸©åº¦å€¼æ¥è§‚å¯Ÿå·®å¼‚ã€‚
- en: '![](../Images/e34fa38ca09ebeaece4e1271c26964da.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e34fa38ca09ebeaece4e1271c26964da.png)'
- en: A temperature of 1.0 is equivalent to a default softmax with no temperature
    at all. On the other hand, a low temperature setting (0.1) significantly alters
    the probability distribution. This is commonly used in text generation to control
    the level of â€œcreativityâ€ in the generated output. By adjusting the temperature,
    we can influence the extent to which the model produces more diverse or predictable
    responses.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æ¸©åº¦ä¸º1.0ç›¸å½“äºæ²¡æœ‰æ¸©åº¦çš„é»˜è®¤softmaxã€‚å¦ä¸€æ–¹é¢ï¼Œä½æ¸©è®¾ç½®ï¼ˆ0.1ï¼‰ä¼šæ˜¾è‘—æ”¹å˜æ¦‚ç‡åˆ†å¸ƒã€‚è¿™åœ¨æ–‡æœ¬ç”Ÿæˆä¸­å¸¸ç”¨äºæ§åˆ¶ç”Ÿæˆè¾“å‡ºçš„â€œåˆ›é€ æ€§â€æ°´å¹³ã€‚é€šè¿‡è°ƒæ•´æ¸©åº¦ï¼Œæˆ‘ä»¬å¯ä»¥å½±å“æ¨¡å‹ç”Ÿæˆæ›´å…·å¤šæ ·æ€§æˆ–æ›´å¯é¢„æµ‹çš„å“åº”çš„ç¨‹åº¦ã€‚
- en: Letâ€™s now implement the top k sampling algorithm. Weâ€™ll use it in the beam_search()
    function by providing the â€œtop_kâ€ argument. To illustrate how the algorithm works,
    we will also plot the probability distributions for top_k = 20.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬å®ç°top-ké‡‡æ ·ç®—æ³•ã€‚æˆ‘ä»¬å°†åœ¨beam_search()å‡½æ•°ä¸­ä½¿ç”¨â€œtop_kâ€å‚æ•°ã€‚ä¸ºäº†è¯´æ˜ç®—æ³•çš„å·¥ä½œåŸç†ï¼Œæˆ‘ä»¬è¿˜å°†ç»˜åˆ¶top_k = 20çš„æ¦‚ç‡åˆ†å¸ƒã€‚
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/12300ff8d47224b762e26e6519b528f2.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12300ff8d47224b762e26e6519b528f2.png)'
- en: Image by author.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: These plots give a good intuition of how top-k sampling works, with all the
    potentially selected tokens on the left of the horizontal bar. While the most
    probable tokens are selected (in red) most of the time, it also allows less likely
    tokens to be chosen. This offers an interesting tradeoff that can steer a sequence
    towards a less predictable but more natural-sounding sentence. Now letâ€™s print
    the text it generated.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å›¾ç»™å‡ºäº†top-ké‡‡æ ·å¦‚ä½•å·¥ä½œçš„è‰¯å¥½ç›´è§‚å°è±¡ï¼Œæ‰€æœ‰å¯èƒ½é€‰æ‹©çš„tokenåœ¨æ°´å¹³æ¡çš„å·¦ä¾§ã€‚è™½ç„¶æœ€å¯èƒ½çš„tokenï¼ˆçº¢è‰²ï¼‰å¤§å¤šæ•°æ—¶é—´è¢«é€‰æ‹©ï¼Œä½†å®ƒä¹Ÿå…è®¸é€‰æ‹©å¯èƒ½æ€§è¾ƒä½çš„tokenã€‚è¿™æä¾›äº†ä¸€ä¸ªæœ‰è¶£çš„æƒè¡¡ï¼Œå¯ä»¥ä½¿åºåˆ—è¶‹å‘äºä¸€ä¸ªæ›´ä¸å¯é¢„æµ‹ä½†æ›´è‡ªç„¶çš„å¥å­ã€‚ç°åœ¨è®©æˆ‘ä»¬æ‰“å°å®ƒç”Ÿæˆçš„æ–‡æœ¬ã€‚
- en: '[PRE10]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The top-k sampling found a new sequence: â€œI have a dream job and I want toâ€,
    which feels significantly more natural than â€œI have a dream. I have a dreamâ€.
    Weâ€™re making progress!'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: top-ké‡‡æ ·æ‰¾åˆ°äº†ä¸€ä¸ªæ–°åºåˆ—ï¼šâ€œæˆ‘æœ‰ä¸€ä¸ªæ¢¦æƒ³å·¥ä½œï¼Œæˆ‘æƒ³è¦â€ï¼Œè¿™æ¯”â€œæˆ‘æœ‰ä¸€ä¸ªæ¢¦æƒ³ã€‚æˆ‘æœ‰ä¸€ä¸ªæ¢¦æƒ³â€æ˜¾å¾—è‡ªç„¶å¾—å¤šã€‚æˆ‘ä»¬åœ¨å–å¾—è¿›å±•ï¼
- en: Letâ€™s see how this decision tree differs from the previous one.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªå†³ç­–æ ‘ä¸ä¹‹å‰çš„æœ‰ä½•ä¸åŒã€‚
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](../Images/29cff567ceb5d77709230d8c84e85d04.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29cff567ceb5d77709230d8c84e85d04.png)'
- en: You can see how the nodes differ significantly from the previous iteration,
    making more diverse choices. Although the sequence score of this new outcome might
    not be the highest (-1.01 instead of -0.69 previously), itâ€™s important to remember
    that higher scores do not always lead to more realistic or meaningful sequences.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥çœ‹åˆ°èŠ‚ç‚¹ä¸ä¹‹å‰çš„è¿­ä»£ç›¸æ¯”æœ‰æ˜¾è‘—ä¸åŒï¼Œåšå‡ºäº†æ›´å¤šæ ·çš„é€‰æ‹©ã€‚è™½ç„¶è¿™ä¸€æ–°ç»“æœçš„åºåˆ—åˆ†æ•°å¯èƒ½ä¸æ˜¯æœ€é«˜çš„ï¼ˆ-1.01ï¼Œè€Œä¹‹å‰ä¸º-0.69ï¼‰ï¼Œä½†è¦è®°ä½ï¼Œæ›´é«˜çš„åˆ†æ•°å¹¶ä¸æ€»æ˜¯èƒ½å¯¼è‡´æ›´ç°å®æˆ–æœ‰æ„ä¹‰çš„åºåˆ—ã€‚
- en: 'Now that weâ€™ve introduced top-k sampling, we have to present the other most
    popular sampling technique: nucleus sampling.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¼•å…¥äº†top-ké‡‡æ ·ï¼Œæˆ‘ä»¬å¿…é¡»ä»‹ç»å¦ä¸€ç§æœ€å—æ¬¢è¿çš„é‡‡æ ·æŠ€æœ¯ï¼šæ ¸å¿ƒé‡‡æ ·ã€‚
- en: ğŸ”¬ Nucleus sampling
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ”¬ æ ¸å¿ƒé‡‡æ ·
- en: Nucleus sampling, also known as top-p sampling, takes a different approach from
    top-k sampling. Rather than selecting the top *k* most probable tokens, nucleus
    sampling chooses a cutoff value *p* such that the **sum of the probabilities of
    the selected tokens exceeds *p***. This forms a â€œnucleusâ€ of tokens from which
    to randomly choose the next token.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¸é‡‡æ ·ï¼Œä¹Ÿç§°ä¸ºtop-pé‡‡æ ·ï¼Œä¸top-ké‡‡æ ·é‡‡å–äº†ä¸åŒçš„æ–¹æ³•ã€‚æ ¸é‡‡æ ·ä¸æ˜¯é€‰æ‹©å‰*k*ä¸ªæœ€å¯èƒ½çš„æ ‡è®°ï¼Œè€Œæ˜¯é€‰æ‹©ä¸€ä¸ªæˆªæ­¢å€¼*p*ï¼Œä½¿å¾—**é€‰æ‹©çš„æ ‡è®°çš„æ¦‚ç‡æ€»å’Œè¶…è¿‡*p***ã€‚è¿™å½¢æˆäº†ä¸€ä¸ªä»ä¸­éšæœºé€‰æ‹©ä¸‹ä¸€ä¸ªæ ‡è®°çš„â€œæ ¸å¿ƒâ€ã€‚
- en: In other words, the model examines its top probable tokens in descending order
    and keeps adding them to the list until the total probability surpasses the threshold
    *p*. Unlike top-k sampling, the number of tokens included in the nucleus can vary
    from step to step. This variability often results in a more diverse and creative
    output, making nucleus sampling popular for tasks such as text generation.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢å¥è¯è¯´ï¼Œæ¨¡å‹æŒ‰æ¦‚ç‡ä»é«˜åˆ°ä½æ£€æŸ¥å…¶æœ€å¯èƒ½çš„æ ‡è®°ï¼Œå¹¶ä¸æ–­å°†å®ƒä»¬æ·»åŠ åˆ°åˆ—è¡¨ä¸­ï¼Œç›´åˆ°æ€»æ¦‚ç‡è¶…è¿‡é˜ˆå€¼*p*ã€‚ä¸top-ké‡‡æ ·ä¸åŒï¼Œæ ¸ä¸­åŒ…å«çš„æ ‡è®°æ•°é‡å¯èƒ½åœ¨æ¯ä¸€æ­¥å˜åŒ–ã€‚è¿™ç§å˜åŒ–é€šå¸¸å¯¼è‡´æ›´å…·å¤šæ ·æ€§å’Œåˆ›é€ æ€§çš„è¾“å‡ºï¼Œä½¿å¾—æ ¸é‡‡æ ·åœ¨æ–‡æœ¬ç”Ÿæˆç­‰ä»»åŠ¡ä¸­é¢‡å—æ¬¢è¿ã€‚
- en: To implement the nucleus sampling method, we can use the â€œnucleusâ€ parameter
    in the beam_search() function. In this example, weâ€™ll set the value of *p* to
    0.5\. To make it easier, weâ€™ll include a minimum number of tokens equal to the
    number of beams. Weâ€™ll also consider tokens with cumulative probabilities lower
    than *p*, rather than higher. Itâ€™s worth noting that while the details may differ,
    the core idea of nucleus sampling remains the same.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®ç°æ ¸é‡‡æ ·æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨`beam_search()`å‡½æ•°ä¸­ä½¿ç”¨â€œnucleusâ€å‚æ•°ã€‚åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†*p*çš„å€¼è®¾ç½®ä¸º0.5ã€‚ä¸ºäº†ç®€åŒ–æ“ä½œï¼Œæˆ‘ä»¬å°†åŒ…æ‹¬ä¸€ä¸ªæœ€å°çš„æ ‡è®°æ•°ï¼Œç­‰äºå…‰æŸçš„æ•°é‡ã€‚æˆ‘ä»¬è¿˜å°†è€ƒè™‘ç´¯è®¡æ¦‚ç‡ä½äº*p*çš„æ ‡è®°ï¼Œè€Œä¸æ˜¯é«˜äºçš„æ ‡è®°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè™½ç„¶ç»†èŠ‚å¯èƒ½æœ‰æ‰€ä¸åŒï¼Œä½†æ ¸é‡‡æ ·çš„æ ¸å¿ƒæ€æƒ³ä¿æŒä¸å˜ã€‚
- en: '[PRE13]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](../Images/85778461ebdf2816a857f2d25e65eea7.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/85778461ebdf2816a857f2d25e65eea7.png)'
- en: Image by author.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡ã€‚
- en: In this plot, you can see that the number of tokens included in the nucleus
    (left of the vertical bar) fluctuates a lot. The generated probability distributions
    vary considerably, leading to the selection of tokens that are not always among
    the most probable ones. This opens the door to the generation of unique and varied
    sequences. Now, letâ€™s observe the text it generated.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªå›¾ä¸­ï¼Œä½ å¯ä»¥çœ‹åˆ°æ ¸ä¸­çš„æ ‡è®°æ•°é‡ï¼ˆå‚ç›´æ¡çš„å·¦ä¾§ï¼‰æ³¢åŠ¨å¾ˆå¤§ã€‚ç”Ÿæˆçš„æ¦‚ç‡åˆ†å¸ƒå˜åŒ–å¾ˆå¤§ï¼Œå¯¼è‡´é€‰æ‹©çš„æ ‡è®°ä¸æ€»æ˜¯æœ€å¯èƒ½çš„ã€‚è¿™ä¸ºç”Ÿæˆç‹¬ç‰¹è€Œå¤šæ ·åŒ–çš„åºåˆ—æ‰“å¼€äº†å¤§é—¨ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬è§‚å¯Ÿå®ƒç”Ÿæˆçš„æ–‡æœ¬ã€‚
- en: '[PRE14]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The nucleus sampling algorithm produces the sequence: â€œI have a dream. Iâ€™m
    going toâ€, which shows a notable enhancement in semantic coherence compared to
    greedy sampling.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¸é‡‡æ ·ç®—æ³•ç”Ÿæˆçš„åºåˆ—æ˜¯ï¼šâ€œI have a dream. Iâ€™m going toâ€ï¼Œä¸è´ªå©ªé‡‡æ ·ç›¸æ¯”ï¼Œæ˜¾ç¤ºäº†è¯­ä¹‰ä¸€è‡´æ€§çš„æ˜¾è‘—æå‡ã€‚
- en: To compare the decision paths, letâ€™s visualize the new tree nucleus sampling
    generated.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ¯”è¾ƒå†³ç­–è·¯å¾„ï¼Œè®©æˆ‘ä»¬å¯è§†åŒ–æ–°ç”Ÿæˆçš„æ ‘æ ¸é‡‡æ ·ã€‚
- en: '[PRE16]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](../Images/23ad20ec7404c08df5cf975e2642a24b.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23ad20ec7404c08df5cf975e2642a24b.png)'
- en: As with top-k sampling, this tree is very different from the one generated with
    greedy sampling, displaying more variety. Both top-k and nucleus sampling offer
    unique advantages when generating text, enhancing diversity, and introducing creativity
    into the output. Your choice between the two methods (or even greedy search) will
    depend on the specific requirements and constraints of your project.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸top-ké‡‡æ ·ä¸€æ ·ï¼Œè¿™æ£µæ ‘ä¸è´ªå©ªé‡‡æ ·ç”Ÿæˆçš„æ ‘éå¸¸ä¸åŒï¼Œå±•ç¤ºäº†æ›´å¤šçš„å¤šæ ·æ€§ã€‚top-kå’Œæ ¸é‡‡æ ·åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶éƒ½æä¾›äº†ç‹¬ç‰¹çš„ä¼˜åŠ¿ï¼Œå¢å¼ºäº†å¤šæ ·æ€§ï¼Œå¹¶åœ¨è¾“å‡ºä¸­å¼•å…¥äº†åˆ›é€ åŠ›ã€‚ä½ å¯¹è¿™ä¸¤ç§æ–¹æ³•ï¼ˆç”šè‡³æ˜¯è´ªå©ªæœç´¢ï¼‰çš„é€‰æ‹©å°†å–å†³äºä½ é¡¹ç›®çš„å…·ä½“è¦æ±‚å’Œé™åˆ¶ã€‚
- en: Conclusion
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: In this article, we have delved deep into various decoding methods used by LLMs,
    specifically GPT-2\. We started with a simply **greedy search** and its immediate
    (yet often suboptimal) selection of the most probable next token. Next, we introduced
    the **beam search** technique, which considers several of the most likely tokens
    at each step. Although it offers more nuanced results, beam search can sometimes
    fall short in generating diverse and creative sequences.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬æ·±å…¥æ¢è®¨äº†LLMï¼ˆå°¤å…¶æ˜¯GPT-2ï¼‰ä½¿ç”¨çš„å„ç§è§£ç æ–¹æ³•ã€‚æˆ‘ä»¬ä»ç®€å•çš„**è´ªå©ªæœç´¢**å¼€å§‹ï¼Œè¿™ç§æ–¹æ³•ä¼šç«‹å³ï¼ˆä½†å¾€å¾€ä¸æ˜¯æœ€ä½³çš„ï¼‰é€‰æ‹©æœ€å¯èƒ½çš„ä¸‹ä¸€ä¸ªæ ‡è®°ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä»‹ç»äº†**å…‰æŸæœç´¢**æŠ€æœ¯ï¼Œè¿™ç§æ–¹æ³•åœ¨æ¯ä¸€æ­¥è€ƒè™‘å‡ ä¸ªæœ€å¯èƒ½çš„æ ‡è®°ã€‚å°½ç®¡å®ƒæä¾›äº†æ›´ä¸ºç»†è‡´çš„ç»“æœï¼Œä½†å…‰æŸæœç´¢æœ‰æ—¶åœ¨ç”Ÿæˆå¤šæ ·åŒ–å’Œå¯Œæœ‰åˆ›é€ æ€§çš„åºåˆ—æ–¹é¢è¡¨ç°ä¸ä½³ã€‚
- en: To bring more variability into the process, we then moved on to **top-k sampling**
    and **nucleus sampling**. Top-k sampling diversifies the text generation by randomly
    selecting among the *k* most probable tokens, while nucleus sampling takes a different
    path by dynamically forming a nucleus of tokens based on cumulative probability.
    Each of these methods brings unique strengths and potential drawbacks to the table,
    and the specific requirements of your project will largely dictate the choice
    among them.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å¼•å…¥æ›´å¤šçš„å˜å¼‚æ€§ï¼Œæˆ‘ä»¬æ¥ç€ä½¿ç”¨äº†**top-k é‡‡æ ·**å’Œ**nucleus é‡‡æ ·**ã€‚Top-k é‡‡æ ·é€šè¿‡åœ¨*æœ€å¯èƒ½çš„kä¸ª*æ ‡è®°ä¸­éšæœºé€‰æ‹©æ¥å¤šæ ·åŒ–æ–‡æœ¬ç”Ÿæˆï¼Œè€Œnucleus
    é‡‡æ ·åˆ™é€šè¿‡åŸºäºç´¯è®¡æ¦‚ç‡åŠ¨æ€åœ°å½¢æˆä¸€ä¸ªæ ‡è®°çš„æ ¸å¿ƒæ¥é‡‡å–ä¸åŒçš„æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•å„è‡ªå…·æœ‰ç‹¬ç‰¹çš„ä¼˜ç‚¹å’Œæ½œåœ¨çš„ç¼ºç‚¹ï¼Œå…·ä½“çš„é€‰æ‹©å°†ä¸»è¦å–å†³äºä½ é¡¹ç›®çš„è¦æ±‚ã€‚
- en: Ultimately, understanding these techniques and their trade-offs will equip you
    to better guide the LLMs towards producing increasingly realistic, nuanced, and
    compelling textual output.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆï¼Œç†è§£è¿™äº›æŠ€æœ¯åŠå…¶æƒè¡¡å°†å¸®åŠ©ä½ æ›´å¥½åœ°å¼•å¯¼LLMsç”Ÿæˆè¶Šæ¥è¶ŠçœŸå®ã€ç»†è‡´å’Œå¼•äººå…¥èƒœçš„æ–‡æœ¬è¾“å‡ºã€‚
- en: If youâ€™re interested in more technical content around LLMs, you can follow me
    on Twitter [@maximelabonne](https://twitter.com/maximelabonne).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å¯¹æ›´å¤šå…³äºLLMsçš„æŠ€æœ¯å†…å®¹æ„Ÿå…´è¶£ï¼Œå¯ä»¥åœ¨Twitterä¸Šå…³æ³¨æˆ‘ [@maximelabonne](https://twitter.com/maximelabonne)ã€‚
