["```py\nfrom sklearn.datasets import fetch_openml\n\nmnist = fetch_openml('mnist_784', version = 1)\nlabels = mnist.target.astype(int)\nprint(mnist.data.shape)\n#(70000, 784)\n\nfrom matplotlib import pyplot as plt\n\nplt.figure(figsize = (20, 15))\n\nfor i in range(25):\n    plt.subplot(5, 5, i + 1)\n    plt.imshow(mnist.data[i].reshape(28, 28), cmap = plt.get_cmap('gray'))\n\nplt.show()\n```", "```py\nimport numpy as np\n\nN_points = 10000\n\nX = np.log10(mnist.data + 1)\n#X = mnist.data / 255\n\nnp.random.seed(123)\nrandom_indices = np.random.choice(X.shape[0], size=N_points, replace=False)\nX = X[random_indices,:]\nlabels = labels[random_indices]\n```", "```py\nimport pandas as pd; import matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA; import seaborn as sns\n\nN_pca_comps = 100\n\nsns.set(font_scale = 1.5); figure = plt.figure(figsize = (20, 15))\n\nplt.subplot(221)\nX_reduced = PCA(n_components = 2).fit_transform(X)\nplt.scatter(X_reduced[:,0], X_reduced[:,1], c=labels, cmap='tab10', s=10)\nplt.title('PCA: MNIST', fontsize = 25)\nplt.xlabel('PC1', fontsize = 22); plt.ylabel('PC2', fontsize = 22)\n\nplt.subplot(222)\npca = PCA(n_components = N_pca_comps).fit(X)\nprint('Observed variance explained:')\nprint(pca.explained_variance_ratio_[0:10]); print('\\n')\nplt.bar(range(len(pca.explained_variance_ratio_)), \n        pca.explained_variance_ratio_)\nplt.xlabel('Number of Principal Components', fontsize = 22)\nplt.ylabel('Explained Variance', fontsize = 22)\n\nN_perm = 10\nX_flat = X.flatten()\nexpl_var_perm_df = pd.DataFrame(index = list(range(N_perm)), \n                                columns = list(range(X.shape[1])))\nfor i in range(N_perm):\n    np.random.shuffle(X_flat)\n    X_perm = X_flat.reshape(X.shape[0], X.shape[1])\n    pca_perm = PCA().fit(X_perm)\n    expl_var_perm_df.loc[i] = pca_perm.explained_variance_ratio_\n    print('Finished {} permutations'.format(i + 1))\n    X_perm = list(expl_var_perm_df.mean(axis = 0) + \n                  2*expl_var_perm_df.std(axis = 0)) \n\nprint('\\nPermuted variance explained:')\nprint(X_perm[0:10])\n\nplt.subplot(223)\nplt.plot(pca.explained_variance_ratio_, c = 'blue')\nplt.plot(X_perm, c = 'red'); plt.xlim([-1, N_pca_comps])\nplt.xlabel('Number of Principal Components', fontsize = 22)\nplt.ylabel('Explained Variance', fontsize = 22)\nplt.gca().legend(('Observed variance explained', \n                  'Permuted variance explained'), fontsize = 20)\n\nplt.subplot(224)\npval = list()\nfor j in range(N_pca_comps):\n    pval.append(np.sum(expl_var_perm_df.iloc[:, j] + \n                       2*expl_var_perm_df.std(axis = 0) >= \n                       pca.explained_variance_ratio_[j]) / N_perm)\nplt.plot(pval, c = 'darkgreen')\nplt.xlabel('Number of Principal Components', fontsize = 22)\nplt.ylabel('P-value', fontsize = 22); plt.xlim([-1, N_pca_comps])\nN_opt_pcs = np.where(np.array(pval) >= 0.05)[0][0]\nprint('\\nNumber of significant Principal Components: {}'.format(N_opt_pcs))\nprint('Together they explain {}% of variation in the data'.\\\nformat(int(round(sum(pca.explained_variance_ratio_[0:\\\nnp.where(np.array(pval) >= 0.05)[0][0]])*100,0))))\n\nfigure.tight_layout()\nplt.show()\n```", "```py\nimport umap; import numpy as np\nimport seaborn as sns; import matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE; from sklearn.decomposition import PCA\n\nopt_perp = np.int(np.round(np.sqrt(X.shape[0]), 0))\nX_reduced = PCA(n_components = N_opt_pcs).fit_transform(X)\n\numap_embedding = umap.UMAP(n_components = 2, n_neighbors = opt_perp, \n                           init = X_reduced[:, 0:2], \n                           min_dist=0.3, n_epochs = 1000, random_state = 123, \n                           verbose = 0).fit_transform(X_reduced)\n\ntsne_embedding = TSNE(n_components=2, perplexity=opt_perp, \n                      init=X_reduced[:, 0:2], \n                      learning_rate = 200, n_iter = 1000, random_state = 123, \n                      verbose = 0).fit_transform(X_reduced)\n\nsns.set(font_scale = 1.5); plt.figure(figsize = (20, 10))\n\nplt.subplot(121)\nplt.scatter(tsne_embedding[:, 0], tsne_embedding[:, 1], c = labels, s = 10, \n            cmap = 'tab20')\nplt.title('tSNE: MNIST', fontsize = 25)\nplt.xlabel(\"tSNE1\", fontsize = 22); plt.ylabel(\"tSNE2\", fontsize = 22)\n\nplt.subplot(122)\nplt.scatter(umap_embedding[:, 0], umap_embedding[:, 1], c = labels, s = 10, \n            cmap = 'tab20')\nplt.title('UMAP: MNIST', fontsize = 25)\nplt.xlabel(\"UMAP1\", fontsize = 22); plt.ylabel(\"UMAP2\", fontsize = 22)\n\nplt.show()\n```", "```py\n#First, we will compute variance explained by PC1 via PCA in sklearn\nimport numpy as np\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components = X.shape[1]).fit(X)\npca_comps = PCA().fit_transform(X)\nprint(pca.explained_variance_ratio_[0])\n#0.11043073983593521\n```", "```py\n#Now let us compute variance explained by PC1 through PLS procedure\nfrom sklearn.metrics import r2_score\nfrom sklearn.cross_decomposition import PLSRegression\n\nPCA_matrix = pd.DataFrame(pca_comps[:, 0:1])\npls = PLSRegression(n_components = 1)\npls.fit(PCA_matrix, X)\ny_pred = pls.predict(PCA_matrix)\nprint(r2_score(X, y_pred, multioutput = 'variance_weighted'))\n#0.11043073983593246\n\n#Finally, let us compute variance explained by PC1 from scratch\nprint(1 - np.sum((np.array(X) - np.array(y_pred))**2) / np.sum((X - \\\nnp.mean(X, axis = 0))**2))\n#0.11043073983593554\n```", "```py\nfrom sklearn.metrics import r2_score\nfrom sklearn.cross_decomposition import PLSRegression\n\npredicted_var_expl = []\nfor i in range(1, 21):\n\n    PCA_matrix_current = pd.DataFrame(pca_comps[:, 0:i])\n    pls_current = PLSRegression(n_components = i)\n    pls_current.fit(PCA_matrix_current, X)\n    y_pred_current = pls_current.predict(PCA_matrix_current)\n    predicted_var_expl.append(r2_score(X, y_pred_current, \n                              multioutput = 'variance_weighted'))\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set(font_scale = 1.5); plt.figure(figsize = (20, 15))\nplt.plot(np.cumsum(pca.explained_variance_ratio_[0:100]),linewidth=5)\nplt.plot(predicted_var_expl, linewidth = 5)\n\nplt.ylabel('Cumulative Explained Variance', fontsize = 20)\nplt.xlabel('Number of Principal Components', fontsize = 20)\nplt.legend(['PCA - computed cumulative variance explained', \n            'PLS - computed cumulative variance explained'], \n            fontsize = 20); plt.show()\n```", "```py\nfrom sklearn.metrics import r2_score\nfrom sklearn.cross_decomposition import PLSRegression\n\n#MNIST variation explained by UMAP1\nUMAP_matrix = pd.DataFrame(umap_embedding[:, 0:1])\npls = PLSRegression(n_components = 1)\npls.fit(UMAP_matrix, X)\ny_pred = pls.predict(UMAP_matrix)\nprint(r2_score(X, y_pred, multioutput = 'variance_weighted'))\n#0.07335034485651613\n\n#Here the same but more explicitly via the R^2 equation above\nprint(1 - np.sum((np.array(X) - np.array(y_pred))**2) / np.sum((X - \\\nnp.mean(X, axis = 0))**2))\n#0.07335034485652026\n\n#MNIST variation explained by tSNE1\ntSNE_matrix = pd.DataFrame(tsne_embedding[:, 0:1])\npls = PLSRegression(n_components = 1)\npls.fit(tSNE_matrix, X)\ny_pred = pls.predict(tSNE_matrix)\nprint(r2_score(X, y_pred, multioutput = 'variance_weighted'))\n#0.07265918428990921\n\n#Here the same but explicitly via the R^2 equation above\nprint(1 - np.sum((np.array(X) - np.array(y_pred))**2) / np.sum((X - \\\nnp.mean(X, axis = 0))**2))\n#0.07265918428991347\n```", "```py\nimport umap; import numpy as np; import seaborn as sns\nimport matplotlib.pyplot as plt; from sklearn.manifold import TSNE\nfrom sklearn.metrics import r2_score; from sklearn.decomposition import PCA\nfrom sklearn.cross_decomposition import PLSRegression\n\nN_iter = 3; N_comps = 3\n\nN_points_list = [5000, 10000, 15000]\nperp_list = [50, 100, 150]; min_dist_list = [0.1, 0.2, 0.3]\n\npredicted_var_expl_matrix = np.zeros(shape = (N_iter, N_comps))\npredicted_var_expl_umap_matrix = np.zeros(shape = (N_iter, N_comps))\npredicted_var_expl_tsne_matrix = np.zeros(shape = (N_iter, N_comps))\nfor j in range(N_iter):\n\n    #MNIST variance explained by PCA components\n    np.random.seed(j)\n    X = np.log10(mnist.data + 1); labels = mnist.target.astype(int)\n    random_indices = np.random.choice(X.shape[0], size = N_points_list[j], \n                                      replace = False)\n    X_sample = X[random_indices,:]; labels_sample = labels[random_indices]\n    pca_comps_sample = PCA(n_components = N_comps).fit_transform(X_sample)\n    predicted_var_expl = []\n    for i in range(1, (N_comps + 1)):\n        PCA_matrix_current = pd.DataFrame(pca_comps_sample[:, 0:i])\n        pls_current = PLSRegression(n_components = i)\n        pls_current.fit(PCA_matrix_current, X_sample)\n        y_pred_current = pls_current.predict(PCA_matrix_current)\n        predicted_var_expl.append(r2_score(X_sample, y_pred_current, \n                                           multioutput='variance_weighted'))\n    predicted_var_expl_matrix[j,:] = predicted_var_expl\n\n    #MNIST variance explained by UMAP components\n    X = np.log10(mnist.data + 1); labels = mnist.target.astype(int)\n    random_indices = np.random.choice(X.shape[0], size = N_points, \n                                      replace = False)\n    X_sample = X[random_indices,:]; labels_sample = labels[random_indices]\n    opt_perp = np.int(np.round(np.sqrt(X_sample.shape[0]), 0))\n    X_reduced_sample = PCA(n_components = N_opt_pcs).fit_transform(X_sample)\n    umap_embedding_sample = umap.UMAP(n_components = N_comps, \n                                      n_neighbors = opt_perp, \n                                      init = X_reduced_sample[:, 0:N_comps], \n                                      min_dist = min_dist_list[j], \n                                      n_epochs = 1000, verbose = \\\n                                      0).fit_transform(X_reduced_sample)\n    predicted_var_expl_umap = []\n    for i in range(1, (N_comps + 1)):\n        UMAP_matrix_current = pd.DataFrame(umap_embedding_sample[:, 0:i])\n        pls_current = PLSRegression(n_components = i)\n        pls_current.fit(UMAP_matrix_current, X_sample)\n        y_pred_current = pls_current.predict(UMAP_matrix_current)\n        predicted_var_expl_umap.append(r2_score(X_sample, y_pred_current, \\\n        multioutput = 'variance_weighted'))\n    predicted_var_expl_umap_matrix[j,:] = predicted_var_expl_umap    \n\n    #MNIST variance explained by tSNE components\n    X = np.log10(mnist.data + 1); labels = mnist.target.astype(int)\n    random_indices = np.random.choice(X.shape[0], size = N_points, \n                                      replace = False)\n    X_sample = X[random_indices,:]; labels_sample = labels[random_indices]\n    X_reduced_sample = PCA(n_components = N_opt_pcs).fit_transform(X_sample)\n    tsne_embedding_sample = TSNE(n_components = N_comps, \n                                 perplexity = perp_list[j], \n                                 init = X_reduced_sample[:, 0:N_comps], \n                                 learning_rate = 200, n_iter = 1000, \n                                 verbose = 0).fit_transform(X_reduced_sample)\n    predicted_var_expl_tsne = []\n    for i in range(1, (N_comps + 1)):\n        tSNE_matrix_current = pd.DataFrame(tsne_embedding_sample[:, 0:i])\n        pls_current = PLSRegression(n_components = i)\n        pls_current.fit(tSNE_matrix_current, X_sample)\n        y_pred_current = pls_current.predict(tSNE_matrix_current)\n        predicted_var_expl_tsne.append(r2_score(X_sample, y_pred_current, \\\n        multioutput = 'variance_weighted'))\n    predicted_var_expl_tsne_matrix[j,:] = predicted_var_expl_tsne\n\nprint(\"MNIST variance explained by PCA components:\")\nprint(predicted_var_expl_matrix)\nprint(\"\\nMNIST variance explained by UMAP components:\")\nprint(predicted_var_expl_umap_matrix)\nprint(\"\\nMNIST variance explained by tSNE components:\")\nprint(predicted_var_expl_tsne_matrix)\n\n#Plot MNIST variance explained by leading PCA, tSNE and UMAP components\nsns.set(font_scale = 1.5); plt.figure(figsize = (20, 15))\nplt.errorbar(range(1, (N_comps + 1)), np.mean(predicted_var_expl_matrix, \n             axis = 0), yerr = 2*np.std(predicted_var_expl_matrix, axis = 0), \n             linewidth = 3, color = 'red', marker = 'o', markersize = 10, \n             capsize = 5, capthick = 3)\nplt.errorbar(range(1, (N_comps + 1)), np.mean(predicted_var_expl_tsne_matrix, \n             axis = 0), yerr = 2*np.std(predicted_var_expl_tsne_matrix, \n             axis = 0), linewidth = 3, color = 'blue', marker = 'o', \n             markersize = 10, capsize = 5, capthick = 3)\nplt.errorbar(range(1, (N_comps + 1)), np.mean(predicted_var_expl_umap_matrix, \n             axis = 0), yerr = 2*np.std(predicted_var_expl_umap_matrix, \n             axis = 0), linewidth = 3, color = 'green', marker = 'o', \n             markersize = 10, capsize = 5, capthick = 3)\nplt.ylabel('Cumulative MNIST Explained Variance', fontsize = 20)\nplt.xlabel('Number of Components', fontsize = 20)\nplt.legend(['PCA variance explained', 'tSNE variance explained', \n            'UMAP variance explained'], fontsize = 20)\nplt.xlim([0.8, (N_comps + 0.2)]); plt.xticks([1, 2, 3]); plt.show()\n```", "```py\nimport umap; import seaborn as sns\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.decomposition import PCA; import numpy as np\nimport matplotlib.pyplot as plt; from sklearn.manifold import TSNE\n\nN_comps = 3; N_points = 10000\n\nmnist = fetch_openml('mnist_784', version = 1)\nlabels = mnist.target.astype(int); X = np.log10(mnist.data + 1)\n\n#Subsample MNIST data down to 10 000 images\nnp.random.seed(123)\nrandom_indices = np.random.choice(X.shape[0], size=N_points, replace=False)\nX = X[random_indices,:]; labels = labels[random_indices]\n\n#Compute top 3 PCA components on the subsampled MNIST data\npca = PCA(n_components = N_comps).fit(X)\npca_comps = PCA().fit_transform(X)\n\n#Compute top 3 UMAP components on the subsampled MNIST data\nopt_perp = np.int(np.round(np.sqrt(X.shape[0]), 0))\nX_reduced = PCA(n_components = N_opt_pcs).fit_transform(X)\numap_embedding = umap.UMAP(n_components = N_comps, n_neighbors = opt_perp, \n                           init = X_reduced[:, 0:N_comps], \n                           min_dist = 0.3, n_epochs = 1000, random_state=123, \n                           verbose = 0).fit_transform(X_reduced)\n\n#Compute top 3 tSNE components on the subsampled MNIST data\ntsne_embedding = TSNE(n_components = N_comps, perplexity = opt_perp, \n                      init = X_reduced[:, 0:N_comps], \n                      learning_rate = 200, n_iter = 1000, random_state = 123, \n                      verbose = 0).fit_transform(X_reduced)\n\n#Pairwise correlations between MNIST labels and PCA / tSNE / UMAP components\nfrom scipy.stats import spearmanr\nrho_matrix = np.zeros(shape = (3, 3))\nfor i in range(N_comps):\n    rho, pval = spearmanr(pca_comps[:, i], labels)\n    rho_matrix[0, i] = np.abs(rho)\nfor i in range(N_comps):\n    rho, pval = spearmanr(tsne_embedding[:, i], labels)\n    rho_matrix[1, i] = np.abs(rho)\nfor i in range(N_comps):\n    rho, pval = spearmanr(umap_embedding[:, i], labels)\n    rho_matrix[2, i] = np.abs(rho)\nrho_df = pd.DataFrame(rho_matrix, columns = ['Comp1', 'Comp2', 'Comp3'], \n                      index = ['PCA', 'tSNE', 'UMAP'])\nsns.set(font_scale = 1.5); plt.figure(figsize = (20, 10))\nsns.heatmap(rho_df.T, cmap = \"Blues\", annot = True)\nplt.title('Spearman correlation of leading PCA / tSNE / UMAP components'+ \n           ' with MNIST labels', fontsize = 20); plt.show()\n```", "```py\nfrom sklearn.metrics import r2_score\nfrom sklearn.cross_decomposition import PLSRegression\n\n#Variance in MNIST labels explained by PC1\npca = PCA(n_components = X.shape[1]).fit(X)\npca_comps = PCA().fit_transform(X)\nPCA_matrix = pd.DataFrame(pca_comps[:, 0:1])\npls = PLSRegression(n_components = 1)\npls.fit(PCA_matrix, labels)\ny_pred = pls.predict(PCA_matrix)\nr2_score(labels, y_pred, multioutput = 'variance_weighted')\n#0.01570798235844606\n\n#Variance in MNIST labels explained by tSNE1\ntSNE_matrix = pd.DataFrame(tsne_embedding[:, 0:1])\npls = PLSRegression(n_components = 1)\npls.fit(tSNE_matrix, labels)\ny_pred = pls.predict(tSNE_matrix)\nr2_score(labels, y_pred, multioutput = 'variance_weighted')\n#0.04531724676013893\n\n#Variance in MNIST labels explained by UMAP1\nUMAP_matrix = pd.DataFrame(umap_embedding[:, 0:1])\npls = PLSRegression(n_components = 1)\npls.fit(UMAP_matrix, labels)\ny_pred = pls.predict(UMAP_matrix)\nr2_score(labels, y_pred, multioutput = 'variance_weighted')\n#0.1369512765129124\n```", "```py\nimport statsmodels.formula.api as smf\n\nmy_df_comps = pd.DataFrame({'LABELS': labels, \n                            'PC1': np.array(PCA_matrix).flatten(), \n                            'tSNE1': np.array(tSNE_matrix).flatten(), \n                            'UMAP1': np.array(UMAP_matrix).flatten()})\n\nsmf.ols(formula = 'LABELS ~ PC1', data = my_df_comps).fit().summary()\nsmf.ols(formula = 'LABELS ~ tSNE1', data = my_df_comps).fit().summary()\nsmf.ols(formula = 'LABELS ~ UMAP1', data = my_df_comps).fit().summary()\n```", "```py\nfrom sklearn.decomposition import PCA\nfrom sklearn.cross_decomposition import PLSRegression\nimport seaborn as sns; import matplotlib.pyplot as plt\nimport numpy as np; from sklearn.metrics import r2_score\n\nN_iter = 3; N_comps = 3\n\nN_points_list = [5000, 10000, 15000]\nmin_dist_list = [0.1, 0.2, 0.3]; perp_list = [90, 100, 110]\npredicted_var_expl_labels_matrix = np.zeros(shape = (N_iter, N_comps))\npredicted_var_expl_labels_umap_matrix=np.zeros(shape=(N_iter, N_comps))\npredicted_var_expl_labels_tsne_matrix=np.zeros(shape=(N_iter, N_comps))\nfor j in range(N_iter):\n\n    #Variance in MNIST labels explained by PCA components\n    np.random.seed(j)\n    X = np.log10(mnist.data + 1); labels = mnist.target.astype(int)\n    random_indices = np.random.choice(X.shape[0], size = N_points_list[j], \n                                      replace = False)\n    X_sample = X[random_indices,:]; labels_sample = labels[random_indices]\n    pca_comps_sample = PCA(n_components = N_comps).fit_transform(X_sample)\n\n    predicted_var_expl_labels = []\n    for i in range(1, (N_comps + 1)):\n        PCA_matrix_current_labels = pd.DataFrame(pca_comps_sample[:, 0:i])\n        pls_current_labels = PLSRegression(n_components = i)\n        pls_current_labels.fit(PCA_matrix_current_labels, labels_sample)\n        y_pred_current_labels = pls_current_labels.predict(\\\n        PCA_matrix_current_labels)\n        predicted_var_expl_labels.append(r2_score(labels_sample, \\\n        y_pred_current_labels, multioutput = 'variance_weighted'))\n    predicted_var_expl_labels_matrix[j,:] = predicted_var_expl_labels\n\n    #Variance in MNIST labels explained by UMAP components\n    X = np.log10(mnist.data + 1); labels = mnist.target.astype(int)\n    random_indices = np.random.choice(X.shape[0], size = N_points, \n                                      replace = False)\n    X_sample = X[random_indices,:]; labels_sample = labels[random_indices]\n    opt_perp = np.int(np.round(np.sqrt(X_sample.shape[0]), 0))\n    X_reduced_sample = PCA(n_components=N_opt_pcs).fit_transform(X_sample)\n    umap_embedding_sample = umap.UMAP(n_components = N_comps, \n                                      n_neighbors = opt_perp, \n                                      init=X_reduced_sample[:, 0:N_comps], \n                                      min_dist = min_dist_list[j], \n                                      n_epochs = 1000, verbose = \\\n                                      0).fit_transform(X_reduced_sample)\n    predicted_var_expl_labels_umap = []\n    for i in range(1, (N_comps + 1)):\n        UMAP_matrix_current_labels=pd.DataFrame(umap_embedding_sample[:,0:i])\n        pls_current_labels_umap = PLSRegression(n_components = i)\n        pls_current_labels_umap.fit(UMAP_matrix_current_labels,labels_sample)\n        y_pred_current_labels_umap = pls_current_labels_umap.predict(\\\n        UMAP_matrix_current_labels)\n        predicted_var_expl_labels_umap.append(r2_score(\\\n        labels_sample, y_pred_current_labels_umap, \\\n        multioutput = 'variance_weighted'))\n    predicted_var_expl_labels_umap_matrix[j,:]=predicted_var_expl_labels_umap\n\n    #Variance in MNIST labels explained by tSNE components\n    X = np.log10(mnist.data + 1); labels = mnist.target.astype(int)\n    random_indices = np.random.choice(X.shape[0], size = N_points, \n                                      replace = False)\n    X_sample = X[random_indices,:]; labels_sample = labels[random_indices]\n\n    opt_perp = np.int(np.round(np.sqrt(X_sample.shape[0]), 0))\n    X_reduced_sample = PCA(n_components = N_opt_pcs).fit_transform(X_sample)\n    tsne_embedding_sample = TSNE(n_components = N_comps, \n                                 perplexity = perp_list[j], \n                                 init = X_reduced_sample[:, 0:N_comps], \n                                 learning_rate = 200, n_iter = 1000, \n                                 verbose=0).fit_transform(X_reduced_sample)\n    predicted_var_expl_labels_tsne = []\n    for i in range(1, (N_comps + 1)):\n        tSNE_matrix_current_labels=pd.DataFrame(tsne_embedding_sample[:,0:i])\n        pls_current_labels_tsne = PLSRegression(n_components = i)\n        pls_current_labels_tsne.fit(tSNE_matrix_current_labels,labels_sample)\n        y_pred_current_labels_tsne = pls_current_labels_tsne.predict(\\\n        tSNE_matrix_current_labels)\n        predicted_var_expl_labels_tsne.append(r2_score(labels_sample, \\\n        y_pred_current_labels_tsne, multioutput = 'variance_weighted'))\n    predicted_var_expl_labels_tsne_matrix[j,:]=predicted_var_expl_labels_tsne\n\nprint(\"Variance in MNIST labels explained by PCA components:\")\nprint(predicted_var_expl_labels_matrix)\nprint(\"\\nVariance in MNIST labels explained by UMAP components:\")\nprint(predicted_var_expl_labels_umap_matrix)\nprint(\"\\nVariance in MNIST labels explained by tSNE components:\")\nprint(predicted_var_expl_labels_tsne_matrix)\n\n#Plot MNIST labels variance explained by PCA, tSNE and UMAP components\nsns.set(font_scale = 1.5); plt.figure(figsize = (20, 15))\nplt.errorbar(range(1, (N_comps + 1)), \n             np.mean(predicted_var_expl_labels_matrix, axis = 0), \n             yerr = 2*np.std(predicted_var_expl_labels_matrix, axis = 0), \n             linewidth = 3, color = 'red', marker = 'o', markersize = 10, \n             capsize = 5, capthick = 3)\nplt.errorbar(range(1, (N_comps + 1)), \n             np.mean(predicted_var_expl_labels_tsne_matrix, axis = 0), \n             yerr=2*np.std(predicted_var_expl_labels_tsne_matrix,axis=0), \n             linewidth = 3, color = 'blue', marker = 'o', markersize = 10, \n             capsize = 5, capthick = 3)\nplt.errorbar(range(1, (N_comps + 1)), \n             np.mean(predicted_var_expl_labels_umap_matrix, axis = 0), \n             yerr=2*np.std(predicted_var_expl_labels_umap_matrix,axis=0), \n             linewidth = 3, color = 'green', marker = 'o', markersize = 10, \n             capsize = 5, capthick = 3)\nplt.ylabel('Cumulative MNIST Labels Explained Variance', fontsize = 20)\nplt.xlabel('Number of Components', fontsize = 20)\nplt.legend(['PCA variance explained', 'tSNE variance explained', \n            'UMAP variance explained'], fontsize = 20)\nplt.xlim([0.8, (N_comps + 0.2)]); plt.xticks([1, 2, 3]); plt.show()\n```"]