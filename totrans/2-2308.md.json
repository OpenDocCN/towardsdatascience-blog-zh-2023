["```py\npip install vllm\n```", "```py\ndocker run --gpus all -it --rm --shm-size=8g nvcr.io/nvidia/pytorch:22.12-py3\n```", "```py\nfrom vllm import LLM\n\nprompts = [\"Tell me about gravity\"] #You can put several prompts in this list\nllm = LLM(model=\"databricks/dolly-v2-3b\")  # Load the model\noutputs = llm.generate(prompts)  # Trigger inference\n```", "```py\n python -m vllm.entrypoints.openai.api_server --model databricks/dolly-v2-3b\n```", "```py\ncurl http://localhost:8000/v1/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"databricks/dolly-v2-3b\",\n        \"prompt\": \"Tell me about gravity\",\n        \"max_tokens\": 200\n    }'\n```"]