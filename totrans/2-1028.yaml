- en: 'Hands-on Generative AI with GANs using Python: DCGAN'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/hands-on-generative-ai-with-gans-using-python-dcgan-6077f0067ac3](https://towardsdatascience.com/hands-on-generative-ai-with-gans-using-python-dcgan-6077f0067ac3)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/a663051d751ad0481e8cc825db37cbd7.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: Photo by [Vinicius "amnx" Amano](https://unsplash.com/@viniciusamano?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Improving synthetic image generation with convolutional layers in PyTorch
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@marcellopoliti?source=post_page-----6077f0067ac3--------------------------------)[![Marcello
    Politi](../Images/484e44571bd2e75acfe5fef3146ab3c2.png)](https://medium.com/@marcellopoliti?source=post_page-----6077f0067ac3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6077f0067ac3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6077f0067ac3--------------------------------)
    [Marcello Politi](https://medium.com/@marcellopoliti?source=post_page-----6077f0067ac3--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6077f0067ac3--------------------------------)
    Â·5 min readÂ·Apr 4, 2023
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In my [previous article](https://medium.com/towards-data-science/hands-on-generative-ai-with-gans-using-python-image-generation-9a62e591c7c6),
    we have seen how to **use GANs to generate images** of the type of MNIST dataset.
    We achieved good results and succeeded in our intent. However, the two networks
    G (generator) and D (discriminator) were composed mostly of dense layers. At this
    point, you will know that usually when working with images we use CNNs (convolutional
    neural networks) since they use convolutional layers. So let us see how to **improve
    our GANs by using** these types of layers. GANs that use **convolutional layers
    are called DCGANs**.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: What is transposed deconvolution?
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Typically when we work with CNNs we are used to working with convolutional layers.
    In this case, though we also need the â€œinverseâ€ operation the transposed deconvolution,
    sometimes also called deconvolution.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '**This operation allows us to upsample the feature space**. For example, if
    we have an image represented by a 5x5 grid we can â€œenlargeâ€ this grid to make
    it 28x28.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: What you do in principle is quite simple, you **put zeros inside the elements
    of the initial feature map to enlarge it and then apply a normal convolution operation
    by using a certain kernel size, stride and padding**.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose we want to transform a 5x5 feature space to 8x8\. First,
    by inserting zeros we create a 9x9 feature space then by applying a 2x2 filter
    we shrink it again to 8x8\. Letâ€™s look at a graphical example.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/535139b951ccdb3db31ae8af4dd2b93f.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
- en: Transposed Convolution (Image By Author)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Also in this network, we are going to use batch-normalization layers, which
    help with the internal covariance shift problem. In a nutshell, what they do is
    normalize each batch before a layer so that there is no change in the distribution
    of the data during training.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªç½‘ç»œä¸­ï¼Œæˆ‘ä»¬è¿˜å°†ä½¿ç”¨æ‰¹é‡å½’ä¸€åŒ–å±‚ï¼Œå®ƒä»¬æœ‰åŠ©äºè§£å†³å†…éƒ¨åæ–¹å·®åç§»é—®é¢˜ã€‚ç®€è€Œè¨€ä¹‹ï¼Œå®ƒä»¬çš„ä½œç”¨æ˜¯åœ¨æ¯ä¸€å±‚ä¹‹å‰å¯¹æ¯ä¸ªæ‰¹æ¬¡è¿›è¡Œå½’ä¸€åŒ–ï¼Œä»¥ä¾¿åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ•°æ®çš„åˆ†å¸ƒæ²¡æœ‰å˜åŒ–ã€‚
- en: Generator Architecture
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç”Ÿæˆå™¨æ¶æ„
- en: The generator will then be formed by a sequence of transposed convolutional
    layers, they will bring the initial random vector z to have the correct size of
    the image we want to produce in this case 28x28\. The depth of the feature maps
    on the other hand will go to be smaller and smaller, unlike the convolutional
    layers.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆå™¨å°†ç”±ä¸€ç³»åˆ—è½¬ç½®å·ç§¯å±‚æ„æˆï¼Œè¿™äº›å±‚å°†åˆå§‹çš„éšæœºå‘é‡zè½¬æ¢ä¸ºæˆ‘ä»¬æƒ³è¦ç”Ÿæˆçš„å›¾åƒçš„æ­£ç¡®å°ºå¯¸ï¼Œè¿™é‡Œæ˜¯28x28ã€‚å¦ä¸€æ–¹é¢ï¼Œç‰¹å¾å›¾çš„æ·±åº¦å°†å˜å¾—è¶Šæ¥è¶Šå°ï¼Œä¸åƒå·ç§¯å±‚é‚£æ ·ã€‚
- en: '![](../Images/fbb92b40136d3758154fc8afaa7c6474.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fbb92b40136d3758154fc8afaa7c6474.png)'
- en: Generator Architecture (Image By Author)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆå™¨æ¶æ„ï¼ˆä½œè€…æä¾›çš„å›¾åƒï¼‰
- en: Discriminator Architecture
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ¤åˆ«å™¨æ¶æ„
- en: The discriminator, on the other hand, is a classical CNN network that has to
    classify images. So we will have a sequence of convolution layers until we reach
    a single number, the probability that the input is real or fake.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€æ–¹é¢ï¼Œåˆ¤åˆ«å™¨æ˜¯ä¸€ä¸ªç»å…¸çš„CNNç½‘ç»œï¼Œè´Ÿè´£å¯¹å›¾åƒè¿›è¡Œåˆ†ç±»ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†æœ‰ä¸€ç³»åˆ—å·ç§¯å±‚ï¼Œç›´åˆ°æˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªå•ä¸€çš„æ•°å­—ï¼Œå³è¾“å…¥æ˜¯å®é™…çš„è¿˜æ˜¯å‡çš„æ¦‚ç‡ã€‚
- en: '![](../Images/606ce895f0fbd8d31fbe815fa531d85c.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/606ce895f0fbd8d31fbe815fa531d85c.png)'
- en: Discriminator Architecture (Image By Author)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ¤åˆ«å™¨æ¶æ„ï¼ˆä½œè€…æä¾›çš„å›¾åƒï¼‰
- en: Letâ€™s code!
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¼€å§‹ç¼–ç å§ï¼
- en: I am going to work on [Deepnote](https://deepnote.com/), but you can work on
    Google Colab if you prefer.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†ä½¿ç”¨[Deepnote](https://deepnote.com/)ï¼Œä½†å¦‚æœä½ æ„¿æ„ï¼Œä½ å¯ä»¥ä½¿ç”¨Google Colabã€‚
- en: First, check if you have a GPU available on your hardware.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæ£€æŸ¥ä½ çš„ç¡¬ä»¶æ˜¯å¦æœ‰å¯ç”¨çš„GPUã€‚
- en: If you are working on Google Colab you would need to mount your drive. Letâ€™s
    import also the necessary libraries.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ åœ¨ä½¿ç”¨Google Colabï¼Œä½ éœ€è¦æŒ‚è½½ä½ çš„é©±åŠ¨å™¨ã€‚è®©æˆ‘ä»¬ä¹Ÿå¯¼å…¥å¿…è¦çš„åº“ã€‚
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now we define the function to create the generator G network as we described
    earlier.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å®šä¹‰åˆ›å»ºç”Ÿæˆå™¨Gç½‘ç»œçš„å‡½æ•°ï¼Œå¦‚æˆ‘ä»¬ä¹‹å‰æ‰€æè¿°çš„ã€‚
- en: To define the discriminator D instead, we use a Python class since we need the
    output of the forward method.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å®šä¹‰åˆ¤åˆ«å™¨Dï¼Œæˆ‘ä»¬ä½¿ç”¨Pythonç±»ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦forwardæ–¹æ³•çš„è¾“å‡ºã€‚
- en: Now we can finally instantiate our G and D networks. Letâ€™s also print the model
    to see the summary of the layers.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬ç»ˆäºå¯ä»¥å®ä¾‹åŒ–æˆ‘ä»¬çš„Gå’ŒDç½‘ç»œäº†ã€‚è®©æˆ‘ä»¬è¿˜æ‰“å°æ¨¡å‹ä»¥æŸ¥çœ‹å±‚çš„æ‘˜è¦ã€‚
- en: As usual, we need to define the cost function and optimizers if we want to do
    network training.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: åƒå¾€å¸¸ä¸€æ ·ï¼Œå¦‚æœæˆ‘ä»¬æƒ³è¿›è¡Œç½‘ç»œè®­ç»ƒï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰æˆæœ¬å‡½æ•°å’Œä¼˜åŒ–å™¨ã€‚
- en: The input vector z is a random vector, taken from some distribution that can
    be either uniform or normal in our case.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥å‘é‡zæ˜¯ä¸€ä¸ªéšæœºå‘é‡ï¼Œå–è‡ªæŸç§åˆ†å¸ƒï¼Œåœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹å¯ä»¥æ˜¯å‡åŒ€åˆ†å¸ƒæˆ–æ­£æ€åˆ†å¸ƒã€‚
- en: Now letâ€™s define the train function of the discriminator D. As we also did in
    the previous article, **D must be trained on both real and fake images**. The
    real images are taken directly from the MNIST dataset while for the fake ones
    we create an input z on the fly, pass it to the generator G and take the output
    of G. The labels we can create ourselves knowing that they will be all ones for
    the real images and zeros for the fake ones. The **final loss will be the loss
    sum of the real images plus the fake ones**.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬å®šä¹‰åˆ¤åˆ«å™¨Dçš„è®­ç»ƒå‡½æ•°ã€‚æ­£å¦‚æˆ‘ä»¬åœ¨å‰ä¸€ç¯‡æ–‡ç« ä¸­æ‰€åšçš„é‚£æ ·ï¼Œ**Då¿…é¡»åœ¨çœŸå®å›¾åƒå’Œå‡å›¾åƒä¸Šè¿›è¡Œè®­ç»ƒ**ã€‚çœŸå®å›¾åƒç›´æ¥å–è‡ªMNISTæ•°æ®é›†ï¼Œè€Œå¯¹äºå‡å›¾åƒï¼Œæˆ‘ä»¬ä¼šåŠ¨æ€ç”Ÿæˆä¸€ä¸ªè¾“å…¥zï¼Œå°†å…¶ä¼ é€’ç»™ç”Ÿæˆå™¨Gï¼Œå¹¶è·å–Gçš„è¾“å‡ºã€‚æˆ‘ä»¬å¯ä»¥è‡ªå·±åˆ›å»ºæ ‡ç­¾ï¼ŒçŸ¥é“çœŸå®å›¾åƒçš„æ ‡ç­¾å…¨ä¸ºä¸€ï¼Œè€Œå‡å›¾åƒçš„æ ‡ç­¾å…¨ä¸ºé›¶ã€‚**æœ€ç»ˆæŸå¤±å°†æ˜¯çœŸå®å›¾åƒæŸå¤±å’Œå‡å›¾åƒæŸå¤±çš„æ€»å’Œ**ã€‚
- en: The generator takes as input the output of the discriminator since it has to
    see if D has figured out whether it is a fake or real image. And based on that
    it calculates its loss.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆå™¨ä»¥åˆ¤åˆ«å™¨çš„è¾“å‡ºä½œä¸ºè¾“å…¥ï¼Œå› ä¸ºå®ƒå¿…é¡»æŸ¥çœ‹Dæ˜¯å¦è¯†åˆ«å‡ºå›¾åƒæ˜¯å‡çš„è¿˜æ˜¯å®é™…çš„ã€‚å¹¶æ ¹æ®æ­¤è®¡ç®—å…¶æŸå¤±ã€‚
- en: We are ready to import the dataset that will allow us to do network training.
    With PyTorch importing the MNIST dataset is very easy since it has methods already
    implemented to do this.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½å¯¼å…¥æ•°æ®é›†ï¼Œè¿™å°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿè¿›è¡Œç½‘ç»œè®­ç»ƒã€‚ä½¿ç”¨PyTorchå¯¼å…¥MNISTæ•°æ®é›†éå¸¸å®¹æ˜“ï¼Œå› ä¸ºå®ƒå·²ç»å®ç°äº†ç›¸å…³æ–¹æ³•ã€‚
- en: Now that we have the dataset we can instantiate the dataloader.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æœ‰äº†æ•°æ®é›†ï¼Œæˆ‘ä»¬å¯ä»¥å®ä¾‹åŒ–æ•°æ®åŠ è½½å™¨ã€‚
- en: Since at the end of the training, we would like to have an idea of how image
    generation is improved from time to time, we create a function that allows us
    to generate and save these images at each epoch.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºåœ¨è®­ç»ƒç»“æŸæ—¶ï¼Œæˆ‘ä»¬å¸Œæœ›äº†è§£å›¾åƒç”Ÿæˆå¦‚ä½•éšç€æ—¶é—´çš„æ¨ç§»è€Œæ”¹è¿›ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªå‡½æ•°ï¼Œå…è®¸æˆ‘ä»¬åœ¨æ¯ä¸ªå‘¨æœŸç”Ÿæˆå’Œä¿å­˜è¿™äº›å›¾åƒã€‚
- en: Finally, we are ready to start the training. Choose the number of epochs, for
    good results it should be around 100\. I only launched 10 and so I will have an
    *â€œuglierâ€* output.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å‡†å¤‡å¼€å§‹è®­ç»ƒã€‚é€‰æ‹©å‘¨æœŸæ•°ï¼Œä¸ºäº†è·å¾—è‰¯å¥½çš„ç»“æœï¼Œåº”è¯¥è®¾å®šåœ¨å¤§çº¦ 100 ä¸ªå‘¨æœŸã€‚æˆ‘åªè¿›è¡Œäº† 10 ä¸ªå‘¨æœŸï¼Œæ‰€ä»¥æˆ‘å°†å¾—åˆ°ä¸€ä¸ª*â€œæ›´ä¸‘â€*çš„è¾“å‡ºã€‚
- en: The training with 100 epochs should take about an hour, then of course it depends
    a lot on the hardware you have available.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒéœ€è¦ 100 ä¸ªå‘¨æœŸï¼Œå¤§çº¦éœ€è¦ä¸€ä¸ªå°æ—¶ï¼Œå½“ç„¶ï¼Œè¿™å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºä½ æ‰€æ‹¥æœ‰çš„ç¡¬ä»¶ã€‚
- en: Letâ€™s plot the results to see if the network has learned how to generate these
    synthetic images.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç»˜åˆ¶ç»“æœï¼Œçœ‹çœ‹ç½‘ç»œæ˜¯å¦å­¦ä¼šäº†å¦‚ä½•ç”Ÿæˆè¿™äº›åˆæˆå›¾åƒã€‚
- en: '![](../Images/7a8316c1412116c55fe95136c30337cc.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a8316c1412116c55fe95136c30337cc.png)'
- en: Synthetic Images (Image By Author)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: åˆæˆå›¾åƒï¼ˆä½œè€…æä¾›çš„å›¾åƒï¼‰
- en: Final Thoughts
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœ€ç»ˆæ€è€ƒ
- en: In this paper we have gone beyond the simple GAN network by also including convolution
    operations that are very effective when working with images, thus creating what
    is called DCGAN. To create these synthetic images we built two networks a generator
    G and discriminator D that play an adversarial game. If this article was helpful
    to you follow me for my upcoming articles on generative networks! [ğŸ˜‰](https://emojipedia.org/it/apple/ios-15.4/faccina-che-fa-l-occhiolino/)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸ä»…ä½¿ç”¨äº†ç®€å•çš„ GAN ç½‘ç»œï¼Œè¿˜åŒ…æ‹¬äº†åœ¨å¤„ç†å›¾åƒæ—¶éå¸¸æœ‰æ•ˆçš„å·ç§¯æ“ä½œï¼Œä»è€Œåˆ›å»ºäº†æ‰€è°“çš„ DCGANã€‚ä¸ºäº†ç”Ÿæˆè¿™äº›åˆæˆå›¾åƒï¼Œæˆ‘ä»¬æ„å»ºäº†ä¸¤ä¸ªç½‘ç»œï¼Œä¸€ä¸ªç”Ÿæˆå™¨
    G å’Œä¸€ä¸ªåˆ¤åˆ«å™¨ Dï¼Œå®ƒä»¬è¿›è¡Œå¯¹æŠ—æ¸¸æˆã€‚å¦‚æœè¿™ç¯‡æ–‡ç« å¯¹ä½ æœ‰å¸®åŠ©ï¼Œå…³æ³¨æˆ‘ä»¥è·å–æˆ‘å³å°†å‘å¸ƒçš„å…³äºç”Ÿæˆç½‘ç»œçš„æ–‡ç« ï¼ [ğŸ˜‰](https://emojipedia.org/it/apple/ios-15.4/faccina-che-fa-l-occhiolino/)
- en: The End
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“å°¾
- en: '*Marcello Politi*'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*é©¬åˆ‡æ´›Â·æ³¢åˆ©æ*'
- en: '[Linkedin](https://www.linkedin.com/in/marcello-politi/), [Twitter](https://twitter.com/_March08_),
    [Website](https://marcello-politi.super.site/)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[é¢†è‹±](https://www.linkedin.com/in/marcello-politi/)ï¼Œ[æ¨ç‰¹](https://twitter.com/_March08_)ï¼Œ[ç½‘ç«™](https://marcello-politi.super.site/)'
