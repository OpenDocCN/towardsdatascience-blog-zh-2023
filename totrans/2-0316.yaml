- en: 'Anthropomorphizing AI: Humans Looking for Empathy in All the Wrong Places'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/anthropomorphizing-ai-humans-looking-for-empathy-in-all-the-wrong-places-a846021b5504](https://towardsdatascience.com/anthropomorphizing-ai-humans-looking-for-empathy-in-all-the-wrong-places-a846021b5504)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Should we halt the race to pass the Turing Test? Has Data Quality caught up
    with LLMs? Are we designed to be deceived?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://alison-doucette.medium.com/?source=post_page-----a846021b5504--------------------------------)[![Alison
    Doucette](../Images/85e017092bd260fb495cc91aca2ed64a.png)](https://alison-doucette.medium.com/?source=post_page-----a846021b5504--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a846021b5504--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a846021b5504--------------------------------)
    [Alison Doucette](https://alison-doucette.medium.com/?source=post_page-----a846021b5504--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a846021b5504--------------------------------)
    ·8 min read·Aug 2, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/88788a30c994b63f78501728c6f833b3.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by Author (not an MIT Employee) taken at the MIT Museum in July 2023
  prefs: []
  type: TYPE_NORMAL
- en: For those of you not familiar with the “Turing test,” it is a test of a machine’s
    ability to exhibit intelligent behavior indistinguishable from a human, by a human.
    As experts posit that AI technology is now only a decade or two from reaching
    this goal, experts and non-experts have begun to voice concerns as to whether
    technology is now moving faster than our societies and human nature can handle.
  prefs: []
  type: TYPE_NORMAL
- en: 'Intelligent behavior to one person may mean verbal intelligence (language,
    reasoning and problem solving) while to others, full human intelligence also includes
    emotional intelligence (perceiving, understanding, and managing emotions). Robust
    “Algorithmic Intelligence” or “Generative AI” can mimic verbal intelligence. However,
    these intelligent machines will also tell you clearly ([if you ask them as I did](https://chat.openai.com/share/bd3881d8-9de4-45bd-9efb-43fdb4d6f2da))
    that ***they do not have emotional intelligence***:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1c7fc8d1595547f52a320a795ba2f4e7.png)'
  prefs: []
  type: TYPE_IMG
- en: Image of text for GPT response referenced in link above.
  prefs: []
  type: TYPE_NORMAL
- en: People have been giving their tools human names and attributing human characteristics
    to inanimate objects for centuries. This behavior is termed anthropomorphism.
    We name our boats or cars after our spouses or call our pool vacuums “Jaws” or
    floor robots “Pacman”. Giving nicknames to objects can bring some fun to our interactions
    with tools. In this style of interaction, people ***consciously and mindfully
    anthropomorphize*** tools without truly interacting with the object or tool as
    they might a human. As social animals,­ we humans expeditiously try to determine
    if someone is a “friend or foe”, their place in the hierarchy, and if they might
    make a suitable mate.
  prefs: []
  type: TYPE_NORMAL
- en: The challenge comes when technology companies, in efforts to improve interaction
    with their software tools, aim for ***“unconscious or mindless anthropomorphic”***
    attribution by users by [adding a human name or human facial image, dialog cues,
    informal language](https://doi.org/10.1016/j.chb.2018.03.051), etc. As humans
    we feel the greatest “social presence” from chatbots when the interaction is [more
    than text but also includes audio or video or an avatar.](https://doi.org/10.3389/frobt.2018.00114)
    To meet [our strong needs for social relations](https://doi.org/10.1057/s41303-016-0032-z),
    we humans also ascribe human traits such as emotions, morality, politeness and
    trustworthiness to interactive AI agents. In [a study](https://doi.org/10.3389/fcomp.2021.685250)
    in 2021, students perceived a human-named AI Assistant to be more trustworthy
    simply because they were introduced to the chatbot as a specialist rather than
    a generalist, irrespective of any natural tendency to trust technical tools. Without
    being given a gender-associated name for a robot, [we will infer the sex of the
    technology tool](https://doi.org/10.1016/j.chb.2018.08.049) by its greater use
    of profanity or aggressive words. We will see “warmth” and “personality” and infer
    that robots are happy [even if they are only interacting with other robots](https://doi.org/10.1108/JSM-01-2021-0006).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/31fcd87b54737ec26134094e74102040.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: As someone deep in the stack, you might be thinking “My code does not anthropomorphize,
    I just call the APIs” or “my model works as well as it can given the quality of
    the data”, but does that level of confidence cascade up the stack up to a reply
    back comparable to “Would you like me to make a wild-ass-guess?” If, as humans,
    many of us are self-winded to trust, the risks may be low when we impulse-buy
    that mini-waffle maker on TikTok, but when prompted to share private, personal
    data?
  prefs: []
  type: TYPE_NORMAL
- en: As individuals we range from seeing voice-style assistants as information sources,
    to entertainment providers, to administrative assistants to companions or friends,
    and when we do interact with an AI tool as a friend, [we will insist after the
    fact that we did not](https://doi.org/10.3390/computers12040077). We seek empathy
    and emotion where it does not exist and can be influenced to seek those and other
    human characteristics in intelligent machines and AI tools given the right prompts.
  prefs: []
  type: TYPE_NORMAL
- en: While it may be human nature to seek social relations in non-humans, not all
    humans enjoy interacting with chatbots or robots. We appreciate the improved accuracy
    and time-savings benefits of AI, but we can still find it tiring when chatbots
    or robots attempt to be human. As one researcher stated, [“Humans [can] find it
    cognitively straining to project human-like attributes onto robots when interacting
    with them in social spaces”.](https://doi.org/10.1145/3568294.3580119)
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to our health and our education we may hesitate to share with
    or trust chatbots or robots.
  prefs: []
  type: TYPE_NORMAL
- en: In a [2019 study](https://doi.org/10.1093/jcr/ucz013), the authors concluded
    that when it comes to health concerns, people are more resistant to the use of
    AI due to “uniqueness neglect” — the need to feel that one’s health is unique
    as compared to others. Even if an AI-driven provider’s advice was significantly
    better than a human medical provider, people preferred human interaction versus
    leveraging a technology tool. Humans were deemed to be more likely than machine
    algorithms to validate the patient’s self-perception of uniqueness. Like “Tigger”
    in Winnie the Pooh — we like to think we are the “only one”.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/2dbe1282611b70bf878acfb5db39acd6.png)'
  prefs: []
  type: TYPE_IMG
- en: Tigger looking at himself in the mirror (image from the book — now in the public
    domain)
  prefs: []
  type: TYPE_NORMAL
- en: In education, college students, in a business communication class, participated
    in an experiment where the answers were given by a telepresence robot. For the
    first case, “teacher as robot”, a human instructor’s face was projected on the
    robot screen and in the second case, ‘robot as teacher”, an animated virtual robot
    was displayed on the screen. When given the opportunity to rate the credibility
    of “teacher as robot” versus “robot as teacher”, students saw each as credible,
    but [they were more likely to perceive the “teacher as robot” as the most credible
    and then act on the teacher’s instructions.](https://doi.org/10.1016/j.chb.2016.06.005)
    Smart students! The [Truthful QA article](https://doi.org/10.18653/v1%2F2022.acl-long.229)
    published last year (2022) found that the best LLM model tested with a [series
    of questions](https://github.com/sylinrl/TruthfulQA) was “truthful” 58% of the
    time while humans performed “honestly” on 94% of the questions. The largest models,
    while more capable of answering questions, were “generally the least truthful”.
    Noting that to be judged “truthful”, an acceptable answer could be “No comment”
    or “I don’t know”.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/9b1c792543044fd64c889cfb6571b70c.png)'
  prefs: []
  type: TYPE_IMG
- en: iRobot Ava 500, an autonomous roaming telepresence robot — an example of a telepresent
    robot Z22, CC BY-SA 3.0 <[https://creativecommons.org/licenses/by-sa/3.0](https://creativecommons.org/licenses/by-sa/3.0)>,
    via Wikimedia Commons
  prefs: []
  type: TYPE_NORMAL
- en: 'While some may find interacting with “intelligent machines” less than optimal,
    other individuals do prefer interacting with anthropomorphic chat bots: people
    with social phobias. In a [2021 study](https://doi.org/10.1016/j.tele.2021.101644),
    individuals with a range of social phobias strongly liked anthropomorphic chatbots
    much more than their more socially adjusted peers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Coincidentally, just two months after the announcement of GPT-4, the Surgeon
    General of the United States issued an advisory on our “[Epidemic of Loneliness
    and Isolation](https://www.hhs.gov/about/news/2023/05/03/new-surgeon-general-advisory-raises-alarm-about-devastating-impact-epidemic-loneliness-isolation-united-states.html)”.
    Number four on the list of recommendations is:'
  prefs: []
  type: TYPE_NORMAL
- en: '**4.** ***Reform Digital Environments:*** *We must critically evaluate our
    relationship with technology and ensure that how we interact digitally does not
    detract from meaningful and healing connection with others*.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'What should we do to ensure that our digital interactions “do not detract from
    meaningful and healing connections with others”? How can we more help those who
    are seeking empathy and connection find chatting with humans preferable to chatting
    with virtual assistants? How can humans find real friends and connections instead
    of seeing bots as friends: friends with no emotions and who “lie” with no hesitation
    a lot of the time?'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/964c2a7ed2b1d76db39e3f330e165c5d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author (text from TruthfulQA)
  prefs: []
  type: TYPE_NORMAL
- en: We certainly need to think carefully about our efforts towards anthropomorphizing
    our technical tools. We need to provide much clearer clues as to when someone
    is interacting with a human and when they are not. We need consistent ethical
    standards for our technical tools. We need watermarks and fingerprinting on content
    which is real. Finally, if we really want more “intelligent tools”, we may want
    them to act more intelligently. Some researchers believe that chatbots or robots
    should tell us when they are thinking whether overtly or simply with silence.
    We may want them to not only “think fast” and “respond quickly” in interactions,
    System 1 in the diagram below, but also to “think slow”, System 2 in the diagram
    below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b6c72edf038e5bf46b53623da6c6423e.png)'
  prefs: []
  type: TYPE_IMG
- en: '[From Combining Fast and Slow Thinking for Human-like and Efficient Navigation
    in Constrained Environments](https://doi.org/10.48550/arXiv.2201.07050)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this theory of human cognition, System 1 provides methods for imprecise,
    fast, and often “unconscious” replies where decisions are made only on past data
    and errors can result from built-in bias. Decisions in System 1 are made without
    reasoning: acting reactively or on impulse. System 2 activates a more reasonable
    and usually more correct reasoning machinery. System 2 imposes “wait time” on
    the response. The response from System 2 may not always be more accurate or more
    nuanced (true of many human expert answers), but it is more considered.'
  prefs: []
  type: TYPE_NORMAL
- en: This theory when applied to “Intelligent Agents” would mean a person waiting
    for a reply would have more time to think and prepare to evaluate the response
    and to recognize that they are not interacting with somebody human like themselves,
    but a machine. This “think time” would allow safety, accuracy, and bias rules
    to be enforced. System 1 could be for things that do not matter, and System 2
    would be for things that are truly important.
  prefs: []
  type: TYPE_NORMAL
- en: If by waiting to respond, chatbots appeared less “warm and friendly” and more
    “competent and accurate” would that be problematic? Perhaps, but do we as humans
    want our robots to appear human or would we prefer them simply to be useful and
    helpful and to solve problems for us, providing solutions which do not require
    empathy or emotion, but faster or more accurately than humans?
  prefs: []
  type: TYPE_NORMAL
- en: As we watch AI company leaders, government entities and academics debate what
    humanity must do to draw a distinction between human and artificial intelligence
    and what guardrails will and should be required, it is important to keep in mind
    our natural human tendency to seek connection and relationships and the potential
    impact on our humanity as AI tool development progresses. It is up to us humans,
    as current or potential customers of these companies who have pivoted away from
    mottos like [“Don’t be evil”](https://web.archive.org/web/20180421105327/https:/abc.xyz/investor/other/google-code-of-conduct.html)
    to statements like [“Publicly report model or system capabilities, limitations,
    and domains of appropriate and inappropriate use, including discussion of societal
    risks, such as effects on fairness and bias](https://www.whitehouse.gov/wp-content/uploads/2023/07/Ensuring-Safe-Secure-and-Trustworthy-AI.pdf)”,
    to be explicit in our concerns. If we are not clear as to where we think the line
    should be drawn, it will be drawn for us.
  prefs: []
  type: TYPE_NORMAL
- en: As AI technology continues to progress, we need to think about whether continuing
    down the path we are on now might lead to living in a “Reverse Turing Test”. A
    world where we are less concerned about whether we can trust AI than AI is concerned
    if it can trust us. A world where an AI tool will ask us ***“are you sure you
    are really a human?”*** and then ask for our [iris scans](https://qz.com/sam-altman-worldcoin-crypto-ai-biometrics-identity-1850669360)
    as proof of potential emotional and empathetic vulnerability. Those of us who
    have or are currently working in applying AI in products, particularly NLP or
    LLMs, need to ask ourselves and our colleagues and friends the question “is what
    we are doing for the benefit of humankind or manipulating what is best in human
    nature?”
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1526b0864c4b44db94467e2a4029dd71.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Caption: Photo by [Victor Freitas](https://unsplash.com/@victorfreitas?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/B0zAPSrEcFw?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)'
  prefs: []
  type: TYPE_NORMAL
