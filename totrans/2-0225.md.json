["```py\nimport string \nimport csv\nfrom io import StringIO\nfrom pptx import Presentation\nimport docx2txt\nimport PyPDF2\nimport spacy\nimport pandas as pd \nimport numpy as np\nimport nltk \nimport re\nimport openpyxl\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom gensim.parsing.preprocessing import STOPWORDS as SW\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\nnltk.download('averaged_perceptron_tagger')\nfrom nltk.corpus import wordnet\nimport networkx as nx\nfrom networkx.algorithms.shortest_paths import weighted\nimport glob\n```", "```py\nclass pdfReader:\n\n    def __init__(self, file_path: str) -> str:\n        self.file_path = file_path\n\n    def PDF_one_pager(self) -> str:\n        \"\"\"A function which returns a one line string of the \n            pdf.\n\n            Returns:\n            one_page_pdf (str): A one line string of the pdf.\n\n        \"\"\"\n        content = \"\"\n        p = open(self.file_path, \"rb\")\n        pdf = PyPDF2.PdfReader(p)\n        num_pages = len(pdf.pages)\n        for i in range(0, num_pages):\n            content += pdf.pages[i].extract_text() + \"\\n\"\n        content = \" \".join(content.replace(u\"\\xa0\", \" \").strip().split())\n        page_number_removal = r\"\\d{1,3} of \\d{1,3}\"\n        page_number_removal_pattern = re.compile(page_number_removal, re.IGNORECASE)\n        content = re.sub(page_number_removal_pattern, '',content)\n\n        return content\n\n    def pdf_reader(self) -> str:\n        \"\"\"A function which can read .pdf formatted files \n            and returns a python readable pdf.\n\n            Returns:\n            read_pdf: A python readable .pdf file.\n        \"\"\"\n        opener = open(self.file_path,'rb')\n        read_pdf = PyPDF2.PdfFileReader(opener)\n\n        return read_pdf\n\n    def pdf_info(self) -> dict:\n        \"\"\"A function which returns an information dictionary of a \n        pdf.\n\n        Returns:\n        dict(pdf_info_dict): A dictionary containing the meta\n        data of the object.\n        \"\"\"\n        opener = open(self.file_path,'rb')\n        read_pdf = PyPDF2.PdfFileReader(opener)\n        pdf_info_dict = {}\n        for key,value in read_pdf.documentInfo.items():\n            pdf_info_dict[re.sub('/',\"\",key)] = value\n        return pdf_info_dict\n\n    def pdf_dictionary(self) -> dict:\n        \"\"\"A function which returns a dictionary of \n            the object where the keys are the pages\n            and the text within the pages are the \n            values.\n\n            Returns:\n            dict(pdf_dict): A dictionary pages and text.\n        \"\"\"\n        opener = open(self.file_path,'rb')\n\n        read_pdf = PyPDF2.PdfReader(opener)\n        length = read_pdf.pages\n        pdf_dict = {}\n        for i in range(length):\n            page = read_pdf.getPage(i)\n            text = page.extract_text()\n            pdf_dict[i] = text\n            return pdf_dict\n```", "```py\nclass pptReader:\n\n    def __init__(self, file_path: str) -> None:\n        self.file_path = file_path\n\n    def ppt_text(self) -> str:\n    \"\"\"A function that returns a string of text from all \n       of the slides in a pptReader object.\n\n      Returns:\n      text (str): A single string containing the text \n      within each slide of the pptReader object.\n   \"\"\"\n      prs = Presentation(self.file_path)\n      text = str()\n\n      for slide in prs.slides:\n        for shape in slide.shapes:\n          if not shape.has_text_frame:\n              continue\n          for paragraph in shape.text_frame.paragraphs:\n            for run in paragraph.runs:\n              text += ' ' + run.text\n      return text\n```", "```py\nclass wordDocReader:\n  def __init__(self, file_path: str) -> str:\n    self.file_path = file_path\n\n  def word_reader(self):\n  \"\"\"A function that transforms a wordDocReader object into a Python readable\n     word document.\"\"\"\n\n    text = docx2txt.process(self.file_path)\n    text = text.replace('\\n', ' ')\n    text = text.replace('\\xa0', ' ')\n    text = text.replace('\\t', ' ')\n    return text \n```", "```py\n class xlsxReader:\n\n    def __init__(self, file_path: str) -> str:\n        self.file_path = file_path\n\n    def xlsx_text(self):\n      \"\"\"A function which returns the string of an \n         excel document.\n\n          Returns:\n          text(str): String of text of a document.\n      \"\"\"\n      inputExcelFile = self.file_path\n      text = str()\n      wb = openpyxl.load_workbook(inputExcelFile)\n      #This will save the excel sheet as a CSV file\n      for sn in wb.sheetnames:\n        excelFile = pd.read_excel(inputExcelFile, engine = 'openpyxl', sheet_name = sn)\n        excelFile.to_csv(\"ResultCsvFile.csv\", index = None, header=True)\n\n        with open(\"ResultCsvFile.csv\", \"r\") as csvFile: \n          lines = csvFile.read().split(\",\") # \"\\r\\n\" if needed\n          for val in lines:\n            if val != '':\n              text += val + ' '\n          text = text.replace('\\ufeff', '')\n          text = text.replace('\\n', ' ')\n      return textCSV File Reader\n```", "```py\n class csvReader:\n\n    def __init__(self, file_path: str) -> str:\n        self.file_path = file_path\n\n    def csv_text(self):\n      \"\"\"A function which returns the string of a\n         csv document.\n\n          Returns:\n          text(str): String of text of a document.\n      \"\"\"\n      text = str()\n      with open(self.file_path, \"r\") as csvFile: \n        lines = csvFile.read().split(\",\") # \"\\r\\n\" if needed\n        for val in lines:\n          text += val + ' '\n        text = text.replace('\\ufeff', '')\n        text = text.replace('\\n', ' ')\n      return textMicrosoft PowerPoint Reader\n```", "```py\nclass pptReader:\n\n    def __init__(self, file_path: str) -> str:\n        self.file_path = file_path\n\n    def ppt_text(self):\n      \"\"\"A function which returns the string of a \n        Mirocsoft PowerPoint document.\n\n        Returns:\n        text(str): String of text of a document.\n    \"\"\"\n      prs = Presentation(self.file_path)\n      text = str()\n      for slide in prs.slides:\n        for shape in slide.shapes:\n          if not shape.has_text_frame:\n              continue\n          for paragraph in shape.text_frame.paragraphs:\n            for run in paragraph.runs:\n              text += ' ' + run.text\n\n      return textMicrosoft Word Document Reader\n```", "```py\nclass wordDocReader:\n  def __init__(self, file_path: str) -> str:\n    self.file_path = file_path\n\n  def word_reader(self):\n    \"\"\"A function which returns the string of a \n          Microsoft Word document.\n\n          Returns:\n          text(str): String of text of a document.\n      \"\"\"\n    text = docx2txt.process(self.file_path)\n    text = text.replace('\\n', ' ')\n    text = text.replace('\\xa0', ' ')\n    text = text.replace('\\t', ' ')\n    return text \n```", "```py\nclass dataprocessor:\n  def __init__(self):\n    return\n\n  @staticmethod\n  def get_wordnet_pos(text: str) -> str:\n    \"\"\"Map POS tag to first character lemmatize() accepts\n    Inputs:\n    text(str): A string of text\n\n    Returns:\n    tag_dict(dict): A dictionary of tags\n    \"\"\"\n    tag = nltk.pos_tag([text])[0][1][0].upper()\n    tag_dict = {\"J\": wordnet.ADJ,\n                \"N\": wordnet.NOUN,\n                \"V\": wordnet.VERB,\n                \"R\": wordnet.ADV}\n\n    return tag_dict.get(tag, wordnet.NOUN)\n```", "```py\n@staticmethod\n  def preprocess(text: str):\n    \"\"\"A function that prepoccesses text through the\n    steps of Natural Language Processing (NLP).\n      Inputs:\n      text(str): A string of text\n\n      Returns:\n      text(str): A processed string of text\n      \"\"\"\n    #lowercase\n    text = text.lower()\n\n    #punctuation removal\n    text = \"\".join([i for i in text if i not in string.punctuation])\n\n    #Digit removal (Only for ALL numeric numbers)\n    text = [x for x in text.split(' ') if x.isnumeric() == False]\n\n    #Stop removal\n    stopwords = nltk.corpus.stopwords.words('english')\n    custom_stopwords = ['\\n','\\n\\n', '&', ' ', '.', '-', '$', '@']\n    stopwords.extend(custom_stopwords)\n\n    text = [i for i in text if i not in stopwords]\n    text = ' '.join(word for word in text)\n\n    #lemmanization\n    lm = WordNetLemmatizer()\n    text = [lm.lemmatize(word, dataprocessor.get_wordnet_pos(word)) for word in text.split(' ')]\n    text = ' '.join(word for word in text)\n\n    text = re.sub(' +', ' ',text)\n\n    return text\n```", "```py\n@staticmethod\n  def data_reader(list_file_names):\n    \"\"\"A function that reads in the data from a directory of files.\n\n    Inputs:\n    list_file_names(list): List of the filepaths in a directory.\n\n    Returns:\n    text_list (list): A list where each value is a string of text\n    for each file in the directory\n    file_dict(dict): Dictionary where the keys are the filename and the values\n    are the information found within each given file\n    \"\"\"\n\n    text_list = []\n    reader = dataprocessor()\n    for file in list_file_names:\n      temp = file.split('.')\n      filetype = temp[-1]\n      if filetype == \"pdf\":\n        file_pdf = pdfReader(file)\n        text = file_pdf.PDF_one_pager()\n\n      elif filetype == \"docx\":\n        word_doc_reader = wordDocReader(file)\n        text = word_doc_reader.word_reader()\n\n      elif filetype == \"pptx\" or filetype == 'ppt':\n        ppt_reader = pptReader(file)\n        text = ppt_reader.ppt_text()\n\n      elif filetype == \"csv\":\n        csv_reader = csvReader(file)\n        text = csv_reader.csv_text()\n\n      elif filetype == 'xlsx':\n        xl_reader = xlsxReader(file)\n        text = xl_reader.xlsx_text()\n      else:\n        print('File type {} not supported!'.format(filetype))\n        continue\n\n      text = reader.preprocess(text)\n      text_list.append(text)\n      file_dict = dict()\n      for i,file in enumerate(list_file_names):\n        file_dict[i] = (file, file.split('/')[-1])\n    return text_list, file_dict\n```", "```py\n@staticmethod\n@staticmethod\n  def database_processor(file_dict,text_list: list):\n    \"\"\"A function that transforms the text of each file within the \n    database into a vector.\n\n    Inputs:\n    file_dixt(dict): Dictionary where the keys are the filename and the values\n    are the information found within each given file\n    text_list (list): A list where each value is a string of the text\n    for each file in the directory\n\n    Returns:\n    list_dense(list): A list of the files' text turned into vectors.\n    vectorizer: The vectorizor used to transform the strings of text\n    file_vector_dict(dict): A dictionary where the file names are the keys\n    and the vectors of each files' text are the values.\n    \"\"\"\n    file_vector_dict = dict()\n    vectorizer = TfidfVectorizer()\n    vectors = vectorizer.fit_transform(text_list)\n    feature_names = vectorizer.get_feature_names_out()\n    matrix = vectors.todense()\n    list_dense = matrix.tolist()\n    for i in range(len(list_dense)):\n      file_vector_dict[file_dict[i][1]] = list_dense[i]\n\n    return list_dense, vectorizer, file_vector_dict\n```", "```py\n @staticmethod\n  def input_processor(text, TDIF_vectorizor):\n    \"\"\"A function which accepts a string of text and vectorizes the text using a \n     TDIF vectorizoer.\n\n    Inputs:\n    text(str): A string of text\n    TDIF_vectorizor: A pretrained vectorizor\n\n    Returns:\n    words(list): A list of the input text in vectored form.\n    \"\"\"\n    words = ''\n    total_words = len(text.split(' '))\n    for word in text.split(' '):\n      words += (word + ' ') * total_words\n      total_words -= 1\n\n    words = [words[:-1]]\n    words = TDIF_vectorizor.transform(words)\n    words = words.todense()\n    words = words.tolist()\n    return words\n```", "```py\n@staticmethod\n  def similarity_checker(vector_1, vector_2):\n    \"\"\"A function which accepts two vectors and computes their cosine similarity.\n\n    Inputs:\n    vector_1(int): A numerical vector\n    vector_2(int): A numerical vector\n\n    Returns:\n    cosine_similarity([vector_1], vector_2) (int): Cosine similarity score\n    \"\"\"\n    vectors = [vector_1, vector_2]\n    for vec in vectors:\n      if np.ndim(vec) == 1:\n        vec = np.expand_dims(vec, axis=0)\n    return cosine_similarity([vector_1], vector_2)\n```", "```py\n@staticmethod\n  def recommender(vector_file_list,query_vector, file_dict):\n    \"\"\"A function which accepts a list of vectors, query vectors, and a dictionary\n    pertaining to the list of vectors with their original values and file names.\n\n    Inputs:\n    vector_file_list(list): A list of vectors\n    query_vector(int): A numerical vector\n    file_dict(dict): A dictionary of filenames and text relating to the list\n    of vectors\n\n    Returns:\n    final_recommendation (list): A list of the final recommended files\n    similarity_list[:len(final_recommendation)] (list): A list of the similarity\n    scores of the final recommendations.\n    \"\"\"\n    similarity_list = []\n    score_dict = dict()\n    for i,file_vector in enumerate(vector_file_list):\n      x = dataprocessor.similarity_checker(file_vector, query_vector)\n      score_dict[file_dict[i][1]] = (x[0][0])\n      similarity_list.append(x)\n    similarity_list = sorted(similarity_list, reverse = True)\n    #Recommends the top 20%\n    recommended = sorted(score_dict.items(), \n                  key=lambda x:-x[1])[:int(np.round(.5*len(similarity_list)))]\n\n    final_recommendation = []\n    for i in range(len(recommended)):\n      final_recommendation.append(recommended[i][0])\n    #add in graph for greater than 3 recommendationa\n    return final_recommendation, similarity_list[:len(final_recommendation)]\n```", "```py\n@staticmethod\n  def ranker(recommendation_val, file_vec_dict):\n    \"\"\"A function which accepts a list of recommendaton values and a dictionary\n    files wihin the databse and their vectors.\n\n    Inputs:\n    reccomendation_val(list): A list of recommendations found through cosine\n    similarity\n    file_vec_dic(dict): A dictionary of the filenames as keys and their\n    text in vectors as the values.\n\n    Returns:\n    ec_recommended(list): A list of the top 20% recommendations found using the \n    eigenvector centrality algorithm.\n    \"\"\"\n    my_graph = nx.Graph()\n    for i in range(len(recommendation_val)):\n      file_1 = recommendation_val[i]\n      for j in range(len(recommendation_val)):\n        file_2 = recommendation_val[j]\n\n        if i != j:\n          #Calculate sim_score between two values (weight)\n          edge_dist = cosine_similarity([file_vec_dict[recommendation_val[i]]],[file_vec_dict[recommendation_val[j]]])\n          #add an edge from file 1 to file 2 with the weight \n          my_graph.add_edge(file_1, file_2, weight=edge_dist)\n\n    #Pagerank the graph  ]    \n    rec = nx.eigenvector_centrality(my_graph)\n    #Takes 20% of the values\n    ec_recommended = sorted(rec.items(), key=lambda x:-x[1])[:int(np.round(len(rec)))]\n\n    return ec_recommended\n```", "```py\n@staticmethod\n  def weighted_final_rank(sim_list,ec_recommended,final_recommendation):\n    \"\"\"A function which accepts a list of similiarity values found through \n      cosine similairty, recommendations found through eigenvector centrality,\n      and the final recommendations produced by cosine similarity.\n\n        Inputs:\n        sim_list(list): A list of all of the similarity values for the files\n        within the database.\n        ec_recommended(list): A list of the top 20% recommendations found using the \n        eigenvector centrality algorithm.\n        final_recommendation (list): A list of the final recommendations found\n        by using cosine similarity.\n\n        Returns:\n        weighted_final_recommend(list): A list of the final recommendations for \n        the files in the database.\n        \"\"\"\n    final_dict = dict()\n\n    for i in range(len(sim_list)):\n      val = (.8*sim_list[final_recommendation.index(ec_recommendation[i][0])].squeeze()) + (.2 * ec_recommendation[i][1])\n      final_dict[ec_recommendation[i][0]] = val\n\n    weighted_final_recommend = sorted(final_dict.items(), key=lambda x:-x[1])[:int(np.round(len(final_dict)))]\n\n    return weighted_final_recommend\n```", "```py\npath = '/content/drive/MyDrive/database/'\ndb = [f for f in glob.glob(path + '*')]\n\nresearch_documents, file_dictionary = dataprocessor.data_reader(db)\nlist_files, vectorizer, file_vec_dict = dataprocessor.database_processor(file_dictionary,research_documents)\nquery = 'Generative Adversarial Networks'\nquery = dataprocessor.preprocess(query)\nquery = dataprocessor.input_processor(query, vectorizer)\nrecommendation, sim_list = dataprocessor.recommender(list_files,query, file_dictionary)\nec_recommendation = dataprocessor.ranker(recommendation, file_vec_dict)\nfinal_weighted_recommended = dataprocessor.weighted_final_rank(sim_list,ec_recommendation,  recommendation)\nprint(final_weighted_recommended)\n```"]