- en: How to Become a Data Engineer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/how-to-become-a-data-engineer-c0319cb226c2](https://towardsdatascience.com/how-to-become-a-data-engineer-c0319cb226c2)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A shortcut for beginners in 2024
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://mshakhomirov.medium.com/?source=post_page-----c0319cb226c2--------------------------------)[![üí°Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----c0319cb226c2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c0319cb226c2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c0319cb226c2--------------------------------)
    [üí°Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----c0319cb226c2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c0319cb226c2--------------------------------)
    ¬∑17 min read¬∑Oct 7, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b49c1deff780a74750c4dfda9056b149.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Gabriel Vasiliu](https://unsplash.com/@gabimedia?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: This story explains an accelerated way of getting into the data engineering
    role by learning the required skills and familiarising yourself with data engineering
    tools and techniques. It will be useful for beginner-level IT practitioners and
    intermediate software engineers who would like to make a career change. Through
    my years as a Head of Data Engineering for one of the most successful start-ups
    in the UK and mid-east, I learned a lot from my career and I would like to share
    this knowledge and experience with you. This is a reflection of my personal experience
    in the data engineering field I acquired over the last 12 years. I hope it will
    be useful for you.
  prefs: []
  type: TYPE_NORMAL
- en: Data engineer ‚Äî the role
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First and foremost, why data engineer?
  prefs: []
  type: TYPE_NORMAL
- en: Data engineering is an exciting and very rewarding field. It‚Äôs a fascinating
    job where we have a chance to work with everything that touches data ‚Äî APIs, data
    connectors, data platforms, business intelligence and dozens of data tools available
    in the market. Data engineering is closely connected with Machine learning (ML).
    You will create and deploy all sorts of data and ML pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: It definitely won‚Äôt be boring and it pays well.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It pays well because it‚Äôs not easy to build a good data platform. It starts
    with requirements gathering and design and requires considerable experience. It‚Äôs
    not an easy task and requires some really good programming skills as well. The
    job itself is secure because as long businesses generate data this job will be
    in high demand.
  prefs: []
  type: TYPE_NORMAL
- en: The companies will always hire someone who knows how to process (ETL) data efficiently.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Data engineering has been one of the fastest-growing careers in the UK over
    the last five years, ranking 13 on LinkedIn‚Äôs list of the most in-demand jobs
    in 2023 [1]. The other reason to join is the scarcity. In IT space it is incredibly
    difficult to find a good data engineer these days.
  prefs: []
  type: TYPE_NORMAL
- en: As a ‚ÄúHead of Data Engineering‚Äù I receive 4 job interview invites on LinkedIn
    each week. On average. Entry level data engineering roles are in higher demand.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'According to tech job research conducted by DICE Data Engineer is the fastest
    growing tech occupation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ee08b504be177749be9112bfa0f2dd1c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [DICE](http://marketing.dice.com/pdf/2020/Dice_2020_Tech_Job_Report.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Modern data stack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Modern data stack refers to a collection of data processing tools and data platform
    types.
  prefs: []
  type: TYPE_NORMAL
- en: Are you in the space?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ‚ÄúAre you in the space?‚Äù ‚Äî This is the question I was asked during one of my
    job interviews. You would want to be able to answer this one, and be aware of
    the news, IPOs, recent developments, breakthroughs, tools and techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'Familiarise yourself with common data platform architecture types, i.e. data
    lake, lakehouse, data warehouse and be ready to answer which tools they use. Check
    this article for some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----c0319cb226c2--------------------------------)
    [## Data Platform Architecture Types'
  prefs: []
  type: TYPE_NORMAL
- en: How well does it answer your business needs? Dilemma of a choice.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----c0319cb226c2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Data pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a data engineer, you will be tasked with data pipeline design almost every
    day. You would want to familiarise yourself with data pipeline design patterns
    and be able to explain when to use them. It is crucial to apply this knowledge
    in practice as it defines which tools to use. The right set of data transformation
    tools can make the data pipeline extremely efficient.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we need to know exactly when to apply streaming data processing and when
    to apply batch. One can be very expensive and the other one can save thousands.
    However, business requirements might be different in each case. This article has
    a comprehensive list of data pipeline design patterns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----c0319cb226c2--------------------------------)
    [## Data pipeline design patterns'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right architecture with examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----c0319cb226c2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Data modelling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I would say data modelling is an essential part of data engineering. Many data
    platforms are designed in a way that data is being loaded into the data warehouse
    solution ‚Äúas is‚Äù. This is called the ELT approach. Data engineers are tasked to
    create data transformation pipelines using standard SQL dialect very often. Good
    SQL skills are a must. Indeed, SQL is natural for analytics querying and pretty
    much is a standard this day. It helps to query data efficiently and enables all
    business stakeholders with the power of analytics easily.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data engineers must know how to **cleanse, enrich and update datasets.** For
    example, using MERGE to perform incremental updates. Run this SQL in your workbench
    or data warehouse (DWH). It explains how it worls:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Some advanced SQL hints and tricks can be found here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/advanced-sql-techniques-for-beginners-211851a28488?source=post_page-----c0319cb226c2--------------------------------)
    [## Advanced SQL techniques for beginners'
  prefs: []
  type: TYPE_NORMAL
- en: On a scale from 1 to 10 how good are your data warehousing skills?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/advanced-sql-techniques-for-beginners-211851a28488?source=post_page-----c0319cb226c2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Coding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is really important as data engineering is not only about data modelling
    and SQL. Consider data engineers as software engineers instead. They must have
    good knowledge of ETL/ELT techniques and also must be able to code at least in
    Python. Yes, it is obvious that Python is no doubt, the most convenient programming
    language for data engineering but everything you can do with Python can be easily
    done with any other language, i.e. Java Script or Java. Don‚Äôt limit yourself,
    you‚Äôll have time to learn any language your company chose as the main one for
    their stack.
  prefs: []
  type: TYPE_NORMAL
- en: I would recommend to start with **data APIs and requests**. Combining this knowledge
    with Cloud services gives us a very good foundation for any ETL processes we might
    need in the future.
  prefs: []
  type: TYPE_NORMAL
- en: We can‚Äôt know everything and it‚Äôs not required to be a coding guru but we must
    know how to process data.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Consider this example of loading data into BigQuery data warehouse. It will
    use BigQuery client libraries [5] to insert rows into a table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We can‚Äôt know everything and it‚Äôs not required to be a coding guru but we must
    know how to process data.
  prefs: []
  type: TYPE_NORMAL
- en: We can run it locally or deploy it in the cloud as a serverless application.
    It can be triggered by any other service we choose. For example, deploying an
    AWS Lambda or GCP Cloud Function can be very efficient. It will process our data
    pipeline events with ease and almost at zero cost. There is plenty of articles
    in my blog explaining how easy and flexible it can be.
  prefs: []
  type: TYPE_NORMAL
- en: Airflow, Airbyte, Luidgi, Hudi‚Ä¶
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Play with 3rd party frameworks and libraries that help to manage data platforms
    and orchestrate data pipelines. Many of them are open-source such as Apache Hudi
    [6] and help to understand what is data platform management from different angles.
    Many of them are really good at managing batch and streaming workloads. I learned
    a lot simply by using them. Apache Airflow, for example, offers a lot of ready
    data connectors. We can use them to run our ETL tasks with ease with any cloud
    vendor (AWS, GCP, Azure).
  prefs: []
  type: TYPE_NORMAL
- en: It‚Äôs very easy to create batch data processing jobs using these frameworks.
    If we take a look under the hood it definitely makes things much clearer in terms
    of what the actual ETL is.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider this example of ML pipeline I built using airflow connectors to train
    the recommendation engine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: It will create a simple data pipeline graph to export data into cloud storage
    bucket and then train the ML model using MLEngineTrainingOperator.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e316a43ba542db4191a0a78ccfa00721.png)'
  prefs: []
  type: TYPE_IMG
- en: ML model training using Airflow. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Orchestrate data pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Frameworks are great but data engineers must know how to create their own frameworks
    to orchestrate data pipelines. This brings us back to raw vanilla coding and working
    with client libraries and APIs. A good advice here will be to familiarise yourself
    with data tools and their API endpoints. Very often it is much more intuitive
    and easier to create and deploy our own microservice to perform ETL/ELT jobs.
  prefs: []
  type: TYPE_NORMAL
- en: Orchestrate data pipelines with your own tools
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For example, we can create a simple serverless application that will consume
    data from the message broker, such as SNS. Then it can process these events and
    orchestrate other microservices we create to perform ETL tasks. Another example
    is a simple AWS Lambda that is being triggered by new files created in the data
    lake, then based on the information it reads from the pipeline configuration file
    it can decide which service to invoke or which table to load data into.
  prefs: []
  type: TYPE_NORMAL
- en: Consider this application below. It‚Äôs a very simple AWS Lambda that can be run
    locally or when it‚Äôs deployed in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`app.py` can be anything with ETL, we just need to add some logic like we did
    with data loading into BigQuery in the previous example using a couple of client
    libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'No we can run it locally or deploy using infrastructure as code. This command
    line script will run this service locally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively it can be deployed in the cloud and we can invoke it from there.
    This brings us into the Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud services providers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Everything is managed in the cloud these days. That‚Äôs why learning at least
    one vendor is crucial. It can be AWS, GCP or Azure. They are the leaders and we
    would want to focus on one of them. It will be ideal to get a Cloud certification,
    such as ‚Äú[Google Cloud Professional Data Engineer](https://cloud.google.com/learn/certification/data-engineer)‚Äù
    [7] or similar. These exams are difficult but it is worth getting one as it gives
    a good overview of data processing tools and makes us look very credible. I did
    one, have written down my experience in an article and you can find it in my stories.
  prefs: []
  type: TYPE_NORMAL
- en: 'Everything data engineers create with cloud functions and/or docker can be
    deployed in the cloud. Consider this AWS CloudFormation stack template below.
    We can use it to deploy our simple ETL microservice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'If we run this shell script in our command line it will deploy our service
    and all required resources such as IAM policies, in the cloud:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'After that we can invoke our service using SDK by running this CLI command
    from our command line tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Master command line tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Command line tools from Cloud vendors are very useful and help to create scripts
    to test our ETL services when they are deployed in the cloud. Data engineers use
    it a lot. Working with data lakes we would want to master CLI commands that help
    us manage Cloud storage, i.e. upload, download and copy files and objects. Why
    do we do this? Very often files in cloud storage trigger various ETL services.
    Batch processing is a very common data transformation pattern and in order to
    investigate bugs and errors we might need to download or copy files between buckets.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c59684bded8474031b90c752aed36ea9.png)'
  prefs: []
  type: TYPE_IMG
- en: ETL on Data lake objects with AWS Lambda. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we can see that the service outputs data into Kinesis and then
    it is stored in the data lake. When file objects are being created in S3 they
    trigger the ETL process handled by AWS Lambda. The result is being saved in the
    S3 bucket to consume by AWS Athena in order to generate a BI report with AWS Quicksight.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the set of AWS CLI commands we might want to use at some point:'
  prefs: []
  type: TYPE_NORMAL
- en: Copy and upload file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Recursively copy/upload/download all files in the folder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Recursively delete all contents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Delete a bucket
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: There are more advanced examples but I think the idea is clear.
  prefs: []
  type: TYPE_NORMAL
- en: We would want to manage cloud storage efficiently with scripts.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We can chain these commands into shell scripts which makes CLI a very powerful
    tool.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider this shell script for example. It will check if storage bucket for
    lambda package exists, upload and deploy our ETL service as a Lambda Function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: More advanced examples can be found in my previous stories.
  prefs: []
  type: TYPE_NORMAL
- en: Data quality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now when we know how to deploy ETL services, perform requests and pull data
    from external APIs we need to learn how to observe the data we have in our data
    platform. Ideally, we would want to check data quality in real-time as data flows
    into our data platform. It can be done both ways using the ETL or ELT approach.
    Streaming applications built with Kafka or Kinesis have libraries to analyze data
    quality as data flows in the data pipeline. ELT approach is preferable when data
    engineers delegate data observability and data quality management to other stakeholders
    working with the data warehouse. Personally, I like the latter approach as it
    saves time. Consider data warehouse solutions as a single source of truth for
    everyone in the company. Finance, marketing and customer services teams can access
    data and check for **any potential issues.** Among these we typically see the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: missing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: data source outages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: data source changes when schema fields are updated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: various data anomalies such as outliers or unusual application/user behaviour.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data engineers create alarms and schedule notifications for any potential data
    issues so data users stay always informed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider this example when a daily email is sent to stakeholders informing
    them about data outage:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c9b2a060d7071e8e6c95c403b0901690.png)'
  prefs: []
  type: TYPE_IMG
- en: Email alarm. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: In my stories you can find an article explaining how to schedule such data monitoring
    workflow using SQL.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0?source=post_page-----c0319cb226c2--------------------------------)
    [## Automated emails and data quality checks for your data'
  prefs: []
  type: TYPE_NORMAL
- en: Data warehouse guide for better and cleaner data with scheduled emails
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0?source=post_page-----c0319cb226c2--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider this snippet below. It ewill check yeterday data ffor any missing
    field using **row conditions** and send a notification alarm if any were found:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Data environments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data engineers test data pipelines. There are various ways of doing this. In
    general, it requires a data environment split between production and staging pipelines.
    Often we might need an extra sandbox for testing purposes or to run data transformation
    unit tests when our ETL services trigger CI/CD workflows.
  prefs: []
  type: TYPE_NORMAL
- en: This is a common practice and job interviewers might ask a couple of questions
    regarding this. It might seem a little tricky in the beginning but this diagram
    below exlpains how it works.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/081619c52103b0f55e7e9c6dca8bd0f8.png)'
  prefs: []
  type: TYPE_IMG
- en: CI/CD workflow example for ETL services. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we can use infrastructure as code and GitHub Actions to deploy
    and test staging resources on any pull request from the development branch. When
    all tests are passed and we are happy with our ETL service we can promote it to
    production by merging into the master branch.
  prefs: []
  type: TYPE_NORMAL
- en: Consider this GitHub action workflow below. It will deploy our ETL service on
    staging and do the testing. Suc approach helps reduce errors and deliver data
    piplelines faster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: There is a full solution example in one of my stories.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Adding a machine learning component will make us a machine learning engineer.
    Data engineering and ML are very close as data engineers create data pipelines
    that are consumed by ML services often.
  prefs: []
  type: TYPE_NORMAL
- en: We don‚Äôt need to know every machine-learning model
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We can‚Äôt compete with cloud service providers such as Amazon ang Google in machine
    learning and data science but we need to know how to use it.
  prefs: []
  type: TYPE_NORMAL
- en: There are numerous managed ML services provided by cloud vendors and we would
    want to familiarize ourselves with them. Data engineers prepare datasets for these
    services and it will definitely be useful to do a couple of tutorials on this.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider a churn prediction project to understand user churn and
    how to use managed ML services to generate predictions for users. This can easily
    done with BigQuery ML [9] by creating a simple logistic regression model like
    so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'And then we can generate predictions using SQL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I‚Äôve tried to summarise a set of data engineering skills and techniques that
    are typically required for entry-level data engineering roles. Based on my experience
    these skills can be acquired within two to three months of active learning. I
    would recommend starting with cloud service providers and Python to build a simple
    ETL service with a CI/CD pipeline for staging and production split. It doesn‚Äôt
    cost anything and we can learn fast by running them both locally and when they
    are deployed in the cloud. Data engineers are in high demand in the market right
    now. I hope this article will help you to learn a couple of new things and prepare
    for job interviews.
  prefs: []
  type: TYPE_NORMAL
- en: Recommedned read
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] [https://www.linkedin.com/pulse/linkedin-jobs-rise-2023-25-uk-roles-growing-demand-linkedin-news-uk/](https://www.linkedin.com/pulse/linkedin-jobs-rise-2023-25-uk-roles-growing-demand-linkedin-news-uk/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [https://towardsdatascience.com/data-platform-architecture-types-f255ac6e0b7](/data-platform-architecture-types-f255ac6e0b7)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] [https://towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3](/data-pipeline-design-patterns-100afa4b93e3)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] [https://medium.com/towards-data-science/advanced-sql-techniques-for-beginners-211851a28488](https://medium.com/towards-data-science/advanced-sql-techniques-for-beginners-211851a28488)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] [https://cloud.google.com/python/docs/reference/bigquery/latest](https://cloud.google.com/python/docs/reference/bigquery/latest)'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] [https://hudi.apache.org/docs/overview/](https://hudi.apache.org/docs/overview/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] [https://cloud.google.com/learn/certification/data-engineer](https://cloud.google.com/learn/certification/data-engineer)'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] [https://towardsdatascience.com/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0](/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0)'
  prefs: []
  type: TYPE_NORMAL
- en: '[9] [https://cloud.google.com/bigquery/docs/bqml-introduction](https://cloud.google.com/bigquery/docs/bqml-introduction)'
  prefs: []
  type: TYPE_NORMAL
