- en: Beginner’s Guide to the Must-Know LightGBM Hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/beginners-guide-to-the-must-know-lightgbm-hyperparameters-a0005a812702](https://towardsdatascience.com/beginners-guide-to-the-must-know-lightgbm-hyperparameters-a0005a812702)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The most important LightGBM parameters, what they do, and how to tune them
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@iamleonie?source=post_page-----a0005a812702--------------------------------)[![Leonie
    Monigatti](../Images/4044b1685ada53a30160b03dc78f9626.png)](https://medium.com/@iamleonie?source=post_page-----a0005a812702--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a0005a812702--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a0005a812702--------------------------------)
    [Leonie Monigatti](https://medium.com/@iamleonie?source=post_page-----a0005a812702--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a0005a812702--------------------------------)
    ·5 min read·Mar 7, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8e37b170e50ee11ec08887e47126e55b.png)'
  prefs: []
  type: TYPE_IMG
- en: Knobs for tuning LightGBM hyperparameters (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: '[LightGBM](https://lightgbm.readthedocs.io/en/latest/index.html) is a popular
    gradient-boosting framework. Usually, you will begin specifying the following
    **core parameters**:'
  prefs: []
  type: TYPE_NORMAL
- en: '`objective` and `metric` for your problem setting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seed` for reproducibility'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`verbose` for debugging'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_iterations`, `learning_rate`, and `early_stopping_round` for training'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But where do you go from here? [LightGBM](https://lightgbm.readthedocs.io/en/latest/index.html)
    has over 100 parameters [2] that can be tuned. Additionally, each parameter has
    one or more aliases, which makes it difficult for beginners to get a clear picture
    of the essential parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, this article discusses the most important and commonly used [LightGBM](https://lightgbm.readthedocs.io/en/latest/index.html)
    hyperparameters, which are listed below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Tree Shape](#1b4c) — `num_leaves` and `max_depth`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Tree Growth](#cd6f) — `min_data_in_leaf` and `min_gain_to_split`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Data Sampling](#85d1) — `bagging_fraction`, `bagging_freq`, and `feature_fraction`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Regularization](#2d6f) — `lambda_l1` and `lambda_l2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following, the default values are taken from the [documentation](https://lightgbm.readthedocs.io/en/latest/Parameters.html)
    [2], and the recommended ranges for hyperparameter tuning are referenced from
    the [article](/kagglers-guide-to-lightgbm-hyperparameter-tuning-with-optuna-in-2021-ed048d9838b5)
    [5] and the books [1] and [4].
  prefs: []
  type: TYPE_NORMAL
- en: Tree Shape
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In contrast to [XGBoost](https://xgboost.readthedocs.io/en/stable/), [LightGBM](https://lightgbm.readthedocs.io/en/latest/index.html)
    grows the decision trees **leaf-wise** instead of level-wise. You can use `num_leaves`
    and `max_depth` to control the size of a single tree.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/859942cf85723cfbbb3730ab85e3fa34.png)'
  prefs: []
  type: TYPE_IMG
- en: Specifying LightGBM tree shape with num_leaves and max_depth (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: The parameter `num_leaves` controls the maximum number of leaves in one tree
    [2].
  prefs: []
  type: TYPE_NORMAL
- en: 'Default: 31'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Good starting point for baseline: 16'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tuning range: (8, 256) with `num_leaves < 2^(max_depth)` [3]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The parameter `max_depth` controls the maximum depth for the tree model [2].
  prefs: []
  type: TYPE_NORMAL
- en: 'Default: -1 (no limit)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Good starting point for baseline: Default'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tuning range: (3, 16)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The smaller the trees (small `num_leaves` and `max_depth`), the faster the training
    speed — but this can also decrease accuracy [3].
  prefs: []
  type: TYPE_NORMAL
- en: Since `num_leaves` impacts the tree growth in LGBM more than `max_depth` [5],
    Morohashi [4] doesn’t necessarily recommend tuning this parameter and to deviate
    from the default value.
  prefs: []
  type: TYPE_NORMAL
- en: Tree Growth
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Aside from the depth and number of leaves, you can specify under which conditions
    a leaf will split. Thus, you can specify how the tree will grow.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6149f3798899054888d50b0c0a662d31.png)'
  prefs: []
  type: TYPE_IMG
- en: Specifying LightGBM tree growth with `min_data_in_leaf` and `min_gain_to_split`
    (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: The parameter `min_data_in_leaf` specifies the minimum number of data points
    in one leaf [2]. If this parameter is too small, the model will overfit to the
    training data [2].
  prefs: []
  type: TYPE_NORMAL
- en: 'Default: 20'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Good starting point for baseline: Default'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tuning range: (5, 300) but depends on the size of the dataset. Hundreds are
    enough for a large dataset [3]. As a rule of thumb: The larger the dataset, the
    larger `min_data_in_leaf`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The parameter `min_gain_to_split` specifies the minimum gain a leaf has to have
    to perform a split [2].
  prefs: []
  type: TYPE_NORMAL
- en: 'Default: 0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Good starting point for baseline: Default'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tuning range: (0, 15)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you limit tree growth by increasing the parameter `min_gain_to_split`, the
    resulting smaller trees will lead to a faster training time — but this can also
    decrease accuracy [3].
  prefs: []
  type: TYPE_NORMAL
- en: Data Sampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data sampling is a technique to force the model to generalize. The general idea
    is not to feed the model all the data at each iteration. Instead, the model will
    only see a fraction of the training data at each iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At every `bagging_freq`-th iteration, LGBM will randomly select `bagging_fraction
    * 100 %` of the data to use for the next `bagging_freq` iterations [2]. E.g.,
    if `bagging_fraction = 0.8` and `bagging_freq = 2`, LGBM will sample 80 % of the
    training data every second iteration before training each tree.
  prefs: []
  type: TYPE_NORMAL
- en: This technique can be used to speed up training [2].
  prefs: []
  type: TYPE_NORMAL
- en: 'Default: `bagging_fraction = 1.0` and `bagging_freq = 0` (disabled)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Good starting point for baseline: `bagging_fraction = 0.9` and `bagging_freq
    = 1`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tuning range: `bagging_fraction` (0.5, 1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/b3f8556ef84542bdcce02a40e7f52377.png)'
  prefs: []
  type: TYPE_IMG
- en: Bagging with bagging_fraction = 0.8 in LightGBM (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: Sub-feature sampling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At every iteration, LGBM will randomly select `feature_fraction * 100 %` of
    the data [2]. E.g., if `feature_fraction = 0.8`, LGBM will sample 80 % of the
    features before training each tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'Default: 1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Good starting point for baseline: 0.9'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tuning range: (0.5, 1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/47329ee15ee33c25a91e026f9961c814.png)'
  prefs: []
  type: TYPE_IMG
- en: Sub-feature sampling with feature_fraction = 0.8 in LightGBM (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: While sub-feature sampling can also be used to speed up training like bagging
    [2], it can help if there is multicollinearity present in the features [1].
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can apply regularization techniques to your Machine Learning model to deal
    with overfitting. As the parameter names already suggest, the parameter `lambda_l1`
    is used for L1 regularization and `lambda_l2` for L2 regularization.
  prefs: []
  type: TYPE_NORMAL
- en: '**L1 regularization** penalizes the absolute values of the weights and thus
    is robust against outliers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**L2 regularization** penalizes the sum of squares of the weights and thus
    is sensitive to outliers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can either decide to use only one of the two types of regularization or
    you can combine them if you like.
  prefs: []
  type: TYPE_NORMAL
- en: 'For both parameters, the parameter values behave similarly:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Default: 0 (disabled)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Good starting point for baseline: Default'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tuning range: (0.01, 100)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article gave you a quick rundown of the most essential [LightGBM](https://lightgbm.readthedocs.io/en/latest/index.html)
    hyperparameters to tune. Below you can find an overview of them with their recommended
    tuning ranges.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6de94bd423c395935434537385e9e285.png)'
  prefs: []
  type: TYPE_IMG
- en: Overview of the most important [LightGBM](https://lightgbm.readthedocs.io/en/latest/index.html)
    hyperparameters and their tuning ranges (Image by the author).
  prefs: []
  type: TYPE_NORMAL
- en: Of course, [LightGBM](https://lightgbm.readthedocs.io/en/latest/index.html)
    has many more hyperparameters you can use.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the parameter `min_sum_hessian_in_leaf` specifies the minimal sum
    hessian in one leaf and can also help with overfitting [2]. There is also a parameter
    `scale_pos_weight` you can tune when your dataset is imbalanced. Or you can specify
    the maximum number of bins a feature will be bucketed into with `max_bin`.
  prefs: []
  type: TYPE_NORMAL
- en: Enjoyed This Story?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[*Subscribe for free*](https://medium.com/subscribe/@iamleonie) *to get notified
    when I publish a new story.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@iamleonie/subscribe?source=post_page-----a0005a812702--------------------------------)
    [## Get an email whenever Leonie Monigatti publishes.'
  prefs: []
  type: TYPE_NORMAL
- en: Get an email whenever Leonie Monigatti publishes. By signing up, you will create
    a Medium account if you don’t already…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@iamleonie/subscribe?source=post_page-----a0005a812702--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*Find me on* [*LinkedIn*](https://www.linkedin.com/in/804250ab/),[*Twitter*](https://twitter.com/helloiamleonie)*,
    and* [*Kaggle*](https://www.kaggle.com/iamleonie)*!*'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] K. Banachewicz, L. Massaron (2022). The Kaggle Book. Packt'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] LightGBM (2023). [Parameters](https://lightgbm.readthedocs.io/en/latest/Parameters.html)
    (accessed March 3rd, 2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] LightGBM (2023). [Parameters Tuning](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html)
    (accessed March 3rd, 2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] M. Morohashi (2022). Kaggleで磨く機械学習の実践力.'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] [Bex T.](https://medium.com/u/39db050c2ac2?source=post_page-----a0005a812702--------------------------------)
    (2021). [Kaggler’s Guide to LightGBM Hyperparameter Tuning with Optuna in 2021](/kagglers-guide-to-lightgbm-hyperparameter-tuning-with-optuna-in-2021-ed048d9838b5)
    (accessed March 3rd, 2023)'
  prefs: []
  type: TYPE_NORMAL
