- en: Beginner’s Guide to the Must-Know LightGBM Hyperparameters
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初学者指南：必知的 LightGBM 超参数
- en: 原文：[https://towardsdatascience.com/beginners-guide-to-the-must-know-lightgbm-hyperparameters-a0005a812702](https://towardsdatascience.com/beginners-guide-to-the-must-know-lightgbm-hyperparameters-a0005a812702)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/beginners-guide-to-the-must-know-lightgbm-hyperparameters-a0005a812702](https://towardsdatascience.com/beginners-guide-to-the-must-know-lightgbm-hyperparameters-a0005a812702)
- en: The most important LightGBM parameters, what they do, and how to tune them
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最重要的 LightGBM 参数，它们的作用以及如何调整它们
- en: '[](https://medium.com/@iamleonie?source=post_page-----a0005a812702--------------------------------)[![Leonie
    Monigatti](../Images/4044b1685ada53a30160b03dc78f9626.png)](https://medium.com/@iamleonie?source=post_page-----a0005a812702--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a0005a812702--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a0005a812702--------------------------------)
    [Leonie Monigatti](https://medium.com/@iamleonie?source=post_page-----a0005a812702--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@iamleonie?source=post_page-----a0005a812702--------------------------------)[![Leonie
    Monigatti](../Images/4044b1685ada53a30160b03dc78f9626.png)](https://medium.com/@iamleonie?source=post_page-----a0005a812702--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a0005a812702--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a0005a812702--------------------------------)
    [Leonie Monigatti](https://medium.com/@iamleonie?source=post_page-----a0005a812702--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a0005a812702--------------------------------)
    ·5 min read·Mar 7, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a0005a812702--------------------------------)
    ·5 分钟阅读·2023年3月7日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/8e37b170e50ee11ec08887e47126e55b.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8e37b170e50ee11ec08887e47126e55b.png)'
- en: Knobs for tuning LightGBM hyperparameters (Image by the author)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 调整 LightGBM 超参数的旋钮（图片来源：作者）
- en: '[LightGBM](https://lightgbm.readthedocs.io/en/latest/index.html) is a popular
    gradient-boosting framework. Usually, you will begin specifying the following
    **core parameters**:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[LightGBM](https://lightgbm.readthedocs.io/en/latest/index.html) 是一个流行的梯度提升框架。通常，你将开始指定以下
    **核心参数**：'
- en: '`objective` and `metric` for your problem setting'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`objective` 和 `metric` 用于问题设置'
- en: '`seed` for reproducibility'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seed` 用于可复现性'
- en: '`verbose` for debugging'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`verbose` 用于调试'
- en: '`num_iterations`, `learning_rate`, and `early_stopping_round` for training'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_iterations`、`learning_rate` 和 `early_stopping_round` 用于训练'
- en: But where do you go from here? [LightGBM](https://lightgbm.readthedocs.io/en/latest/index.html)
    has over 100 parameters [2] that can be tuned. Additionally, each parameter has
    one or more aliases, which makes it difficult for beginners to get a clear picture
    of the essential parameters.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 但接下来该怎么做呢？[LightGBM](https://lightgbm.readthedocs.io/en/latest/index.html) 具有超过
    100 个可以调整的参数 [2]。此外，每个参数还有一个或多个别名，这使得初学者很难清晰地了解重要参数。
- en: 'Thus, this article discusses the most important and commonly used [LightGBM](https://lightgbm.readthedocs.io/en/latest/index.html)
    hyperparameters, which are listed below:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，本文讨论了最重要和最常用的 [LightGBM](https://lightgbm.readthedocs.io/en/latest/index.html)
    超参数，列举如下：
- en: '[Tree Shape](#1b4c) — `num_leaves` and `max_depth`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[树的形状](#1b4c) — `num_leaves` 和 `max_depth`'
- en: '[Tree Growth](#cd6f) — `min_data_in_leaf` and `min_gain_to_split`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[树的生长](#cd6f) — `min_data_in_leaf` 和 `min_gain_to_split`'
- en: '[Data Sampling](#85d1) — `bagging_fraction`, `bagging_freq`, and `feature_fraction`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[数据采样](#85d1) — `bagging_fraction`、`bagging_freq` 和 `feature_fraction`'
- en: '[Regularization](#2d6f) — `lambda_l1` and `lambda_l2`'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[正则化](#2d6f) — `lambda_l1` 和 `lambda_l2`'
- en: In the following, the default values are taken from the [documentation](https://lightgbm.readthedocs.io/en/latest/Parameters.html)
    [2], and the recommended ranges for hyperparameter tuning are referenced from
    the [article](/kagglers-guide-to-lightgbm-hyperparameter-tuning-with-optuna-in-2021-ed048d9838b5)
    [5] and the books [1] and [4].
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下内容中，默认值取自 [文档](https://lightgbm.readthedocs.io/en/latest/Parameters.html)
    [2]，并且超参数调优的推荐范围参考了 [文章](/kagglers-guide-to-lightgbm-hyperparameter-tuning-with-optuna-in-2021-ed048d9838b5)
    [5] 以及书籍 [1] 和 [4]。
- en: Tree Shape
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 树的形状
- en: In contrast to [XGBoost](https://xgboost.readthedocs.io/en/stable/), [LightGBM](https://lightgbm.readthedocs.io/en/latest/index.html)
    grows the decision trees **leaf-wise** instead of level-wise. You can use `num_leaves`
    and `max_depth` to control the size of a single tree.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 与 [XGBoost](https://xgboost.readthedocs.io/en/stable/) 相比，[LightGBM](https://lightgbm.readthedocs.io/en/latest/index.html)
    以 **叶子为单位** 生长决策树，而不是按层生长。你可以使用 `num_leaves` 和 `max_depth` 来控制单棵树的大小。
- en: '![](../Images/859942cf85723cfbbb3730ab85e3fa34.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/859942cf85723cfbbb3730ab85e3fa34.png)'
- en: Specifying LightGBM tree shape with num_leaves and max_depth (Image by the author)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 num_leaves 和 max_depth 指定 LightGBM 树形状（作者提供的图片）
- en: The parameter `num_leaves` controls the maximum number of leaves in one tree
    [2].
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 参数 `num_leaves` 控制树中叶子的最大数量 [2]。
- en: 'Default: 31'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认值：31
- en: 'Good starting point for baseline: 16'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基线的良好起点：16
- en: 'Tuning range: (8, 256) with `num_leaves < 2^(max_depth)` [3]'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整范围：（8，256），`num_leaves < 2^(max_depth)` [3]
- en: The parameter `max_depth` controls the maximum depth for the tree model [2].
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 参数 `max_depth` 控制树模型的最大深度 [2]。
- en: 'Default: -1 (no limit)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认值：-1（无限制）
- en: 'Good starting point for baseline: Default'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基线的良好起点：默认
- en: 'Tuning range: (3, 16)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整范围：（3，16）
- en: The smaller the trees (small `num_leaves` and `max_depth`), the faster the training
    speed — but this can also decrease accuracy [3].
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 树越小（`num_leaves` 和 `max_depth` 较小），训练速度越快——但这也可能降低准确性 [3]。
- en: Since `num_leaves` impacts the tree growth in LGBM more than `max_depth` [5],
    Morohashi [4] doesn’t necessarily recommend tuning this parameter and to deviate
    from the default value.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `num_leaves` 对 LGBM 中的树生长影响大于 `max_depth` [5]，Morohashi [4] 不一定推荐调整该参数并偏离默认值。
- en: Tree Growth
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 树的生长
- en: Aside from the depth and number of leaves, you can specify under which conditions
    a leaf will split. Thus, you can specify how the tree will grow.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 除了深度和叶子数量外，你还可以指定叶子分裂的条件。因此，你可以指定树的生长方式。
- en: '![](../Images/6149f3798899054888d50b0c0a662d31.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6149f3798899054888d50b0c0a662d31.png)'
- en: Specifying LightGBM tree growth with `min_data_in_leaf` and `min_gain_to_split`
    (Image by the author)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `min_data_in_leaf` 和 `min_gain_to_split` 指定 LightGBM 树的生长（作者提供的图片）
- en: The parameter `min_data_in_leaf` specifies the minimum number of data points
    in one leaf [2]. If this parameter is too small, the model will overfit to the
    training data [2].
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 参数 `min_data_in_leaf` 指定一个叶子中最少的数据点数 [2]。如果该参数过小，模型将过拟合训练数据 [2]。
- en: 'Default: 20'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认值：20
- en: 'Good starting point for baseline: Default'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基线的良好起点：默认
- en: 'Tuning range: (5, 300) but depends on the size of the dataset. Hundreds are
    enough for a large dataset [3]. As a rule of thumb: The larger the dataset, the
    larger `min_data_in_leaf`.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整范围：（5，300），但取决于数据集的大小。对于大型数据集，数百个已经足够 [3]。经验法则：数据集越大，`min_data_in_leaf` 越大。
- en: The parameter `min_gain_to_split` specifies the minimum gain a leaf has to have
    to perform a split [2].
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 参数 `min_gain_to_split` 指定叶子进行分裂所需的最小增益 [2]。
- en: 'Default: 0'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认值：0
- en: 'Good starting point for baseline: Default'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基线的良好起点：默认
- en: 'Tuning range: (0, 15)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整范围：（0，15）
- en: If you limit tree growth by increasing the parameter `min_gain_to_split`, the
    resulting smaller trees will lead to a faster training time — but this can also
    decrease accuracy [3].
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果通过增加参数 `min_gain_to_split` 来限制树的生长，得到的较小树将导致训练时间更快——但这也可能降低准确性 [3]。
- en: Data Sampling
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据采样
- en: Data sampling is a technique to force the model to generalize. The general idea
    is not to feed the model all the data at each iteration. Instead, the model will
    only see a fraction of the training data at each iteration.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 数据采样是一种强制模型泛化的技术。总体思路是每次迭代时不给模型提供所有数据。相反，模型每次迭代只会看到训练数据的一部分。
- en: Bagging
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Bagging
- en: At every `bagging_freq`-th iteration, LGBM will randomly select `bagging_fraction
    * 100 %` of the data to use for the next `bagging_freq` iterations [2]. E.g.,
    if `bagging_fraction = 0.8` and `bagging_freq = 2`, LGBM will sample 80 % of the
    training data every second iteration before training each tree.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 每隔 `bagging_freq` 次迭代，LGBM 将随机选择 `bagging_fraction * 100 %` 的数据用于下一个 `bagging_freq`
    次迭代 [2]。例如，如果 `bagging_fraction = 0.8` 和 `bagging_freq = 2`，LGBM 将在每个第二次迭代前抽样
    80% 的训练数据。
- en: This technique can be used to speed up training [2].
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 该技术可用于加快训练速度 [2]。
- en: 'Default: `bagging_fraction = 1.0` and `bagging_freq = 0` (disabled)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认值：`bagging_fraction = 1.0` 和 `bagging_freq = 0`（禁用）
- en: 'Good starting point for baseline: `bagging_fraction = 0.9` and `bagging_freq
    = 1`'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基线的良好起点：`bagging_fraction = 0.9` 和 `bagging_freq = 1`
- en: 'Tuning range: `bagging_fraction` (0.5, 1)'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整范围：`bagging_fraction`（0.5，1）
- en: '![](../Images/b3f8556ef84542bdcce02a40e7f52377.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3f8556ef84542bdcce02a40e7f52377.png)'
- en: Bagging with bagging_fraction = 0.8 in LightGBM (Image by the author)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 bagging_fraction = 0.8 进行 LightGBM 的 bagging（作者提供的图片）
- en: Sub-feature sampling
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 子特征采样
- en: At every iteration, LGBM will randomly select `feature_fraction * 100 %` of
    the data [2]. E.g., if `feature_fraction = 0.8`, LGBM will sample 80 % of the
    features before training each tree.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中，LGBM 将随机选择 `feature_fraction * 100 %` 的数据[2]。例如，如果`feature_fraction =
    0.8`，LGBM 将在训练每棵树之前抽样 80 % 的特征。
- en: 'Default: 1'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认：1
- en: 'Good starting point for baseline: 0.9'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基线的良好起点：0.9
- en: 'Tuning range: (0.5, 1)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调优范围：（0.5, 1）
- en: '![](../Images/47329ee15ee33c25a91e026f9961c814.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/47329ee15ee33c25a91e026f9961c814.png)'
- en: Sub-feature sampling with feature_fraction = 0.8 in LightGBM (Image by the author)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM 中的子特征抽样，feature_fraction = 0.8（图源：作者）
- en: While sub-feature sampling can also be used to speed up training like bagging
    [2], it can help if there is multicollinearity present in the features [1].
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然子特征抽样也可以像袋装方法[2]一样加速训练，但如果特征中存在多重共线性，它也能有所帮助[1]。
- en: Regularization
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化
- en: You can apply regularization techniques to your Machine Learning model to deal
    with overfitting. As the parameter names already suggest, the parameter `lambda_l1`
    is used for L1 regularization and `lambda_l2` for L2 regularization.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将正则化技术应用于你的机器学习模型，以处理过拟合。正如参数名称所示，参数`lambda_l1`用于 L1 正则化，`lambda_l2`用于 L2
    正则化。
- en: '**L1 regularization** penalizes the absolute values of the weights and thus
    is robust against outliers'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L1 正则化**惩罚权重的绝对值，因此对异常值具有鲁棒性'
- en: '**L2 regularization** penalizes the sum of squares of the weights and thus
    is sensitive to outliers'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L2 正则化**惩罚权重的平方和，因此对异常值敏感'
- en: You can either decide to use only one of the two types of regularization or
    you can combine them if you like.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以选择仅使用这两种正则化中的一种，或者如果你愿意，也可以将它们结合使用。
- en: 'For both parameters, the parameter values behave similarly:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这两个参数，参数值的表现类似：
- en: 'Default: 0 (disabled)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认：0（禁用）
- en: 'Good starting point for baseline: Default'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基线的良好起点：默认
- en: 'Tuning range: (0.01, 100)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调优范围：（0.01, 100）
- en: Summary
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This article gave you a quick rundown of the most essential [LightGBM](https://lightgbm.readthedocs.io/en/latest/index.html)
    hyperparameters to tune. Below you can find an overview of them with their recommended
    tuning ranges.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 本文为你快速概述了最重要的[LightGBM](https://lightgbm.readthedocs.io/en/latest/index.html)超参数调优。下面你可以找到它们及其推荐调优范围的概览。
- en: '![](../Images/6de94bd423c395935434537385e9e285.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6de94bd423c395935434537385e9e285.png)'
- en: Overview of the most important [LightGBM](https://lightgbm.readthedocs.io/en/latest/index.html)
    hyperparameters and their tuning ranges (Image by the author).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的[LightGBM](https://lightgbm.readthedocs.io/en/latest/index.html)超参数及其调优范围概览（图源：作者）。
- en: Of course, [LightGBM](https://lightgbm.readthedocs.io/en/latest/index.html)
    has many more hyperparameters you can use.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，[LightGBM](https://lightgbm.readthedocs.io/en/latest/index.html)还有许多其他可以使用的超参数。
- en: For example, the parameter `min_sum_hessian_in_leaf` specifies the minimal sum
    hessian in one leaf and can also help with overfitting [2]. There is also a parameter
    `scale_pos_weight` you can tune when your dataset is imbalanced. Or you can specify
    the maximum number of bins a feature will be bucketed into with `max_bin`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，参数`min_sum_hessian_in_leaf`指定一个叶子中的最小 Hessian 和，并且还可以帮助缓解过拟合[2]。当你的数据集不平衡时，还有一个可以调优的参数`scale_pos_weight`。或者你可以使用`max_bin`指定特征将被分桶的最大数量。
- en: Enjoyed This Story?
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 享受这个故事了吗？
- en: '[*Subscribe for free*](https://medium.com/subscribe/@iamleonie) *to get notified
    when I publish a new story.*'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[*免费订阅*](https://medium.com/subscribe/@iamleonie) *以便在我发布新故事时收到通知。*'
- en: '[](https://medium.com/@iamleonie/subscribe?source=post_page-----a0005a812702--------------------------------)
    [## Get an email whenever Leonie Monigatti publishes.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@iamleonie/subscribe?source=post_page-----a0005a812702--------------------------------)
    [## 每当 Leonie Monigatti 发布时获取电子邮件。'
- en: Get an email whenever Leonie Monigatti publishes. By signing up, you will create
    a Medium account if you don’t already…
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每当 Leonie Monigatti 发布时获取电子邮件。通过注册，如果你还没有 Medium 账户，你将创建一个...
- en: medium.com](https://medium.com/@iamleonie/subscribe?source=post_page-----a0005a812702--------------------------------)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@iamleonie/subscribe?source=post_page-----a0005a812702--------------------------------)
- en: '*Find me on* [*LinkedIn*](https://www.linkedin.com/in/804250ab/),[*Twitter*](https://twitter.com/helloiamleonie)*,
    and* [*Kaggle*](https://www.kaggle.com/iamleonie)*!*'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '*在* [*LinkedIn*](https://www.linkedin.com/in/804250ab/)、[*Twitter*](https://twitter.com/helloiamleonie)*和*
    [*Kaggle*](https://www.kaggle.com/iamleonie)*上找我！*'
- en: References
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] K. Banachewicz, L. Massaron (2022). The Kaggle Book. Packt'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] K. Banachewicz, L. Massaron (2022). 《Kaggle 书》。Packt'
- en: '[2] LightGBM (2023). [Parameters](https://lightgbm.readthedocs.io/en/latest/Parameters.html)
    (accessed March 3rd, 2023)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] LightGBM (2023). [参数](https://lightgbm.readthedocs.io/en/latest/Parameters.html)（访问于2023年3月3日）'
- en: '[3] LightGBM (2023). [Parameters Tuning](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html)
    (accessed March 3rd, 2023)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] LightGBM (2023). [参数调优](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html)（访问于2023年3月3日）'
- en: '[4] M. Morohashi (2022). Kaggleで磨く機械学習の実践力.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] M. Morohashi (2022). Kaggle中磨练机器学习实战能力。'
- en: '[5] [Bex T.](https://medium.com/u/39db050c2ac2?source=post_page-----a0005a812702--------------------------------)
    (2021). [Kaggler’s Guide to LightGBM Hyperparameter Tuning with Optuna in 2021](/kagglers-guide-to-lightgbm-hyperparameter-tuning-with-optuna-in-2021-ed048d9838b5)
    (accessed March 3rd, 2023)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] [Bex T.](https://medium.com/u/39db050c2ac2?source=post_page-----a0005a812702--------------------------------)
    (2021). [2021年Kaggler’s Guide to LightGBM超参数调优与Optuna](/kagglers-guide-to-lightgbm-hyperparameter-tuning-with-optuna-in-2021-ed048d9838b5)（访问于2023年3月3日）'
