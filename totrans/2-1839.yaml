- en: 'SHAP: Explain Any Machine Learning Model in Python'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/shap-explain-any-machine-learning-model-in-python-72f0bea35f7c](https://towardsdatascience.com/shap-explain-any-machine-learning-model-in-python-72f0bea35f7c)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/4b8cc3946e79898437b4c129844a6099.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Priscilla Du Preez](https://unsplash.com/@priscilladupreez?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Your Comprehensive Guide to SHAP, TreeSHAP, and DeepSHAP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://louis-chan.medium.com/?source=post_page-----72f0bea35f7c--------------------------------)[![Louis
    Chan](../Images/6d8df9a478e929dd521059631f26e081.png)](https://louis-chan.medium.com/?source=post_page-----72f0bea35f7c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----72f0bea35f7c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----72f0bea35f7c--------------------------------)
    [Louis Chan](https://louis-chan.medium.com/?source=post_page-----72f0bea35f7c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----72f0bea35f7c--------------------------------)
    ·13 min read·Jan 11, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Motivation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Story Time!
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you have trained a machine learning model to predict the default risk
    of mortgage applicants. All is good, and the performance is excellent too. But
    how does the model work? How does the model come to the predicted value?
  prefs: []
  type: TYPE_NORMAL
- en: We stood there and said that the model considers several variables and the multi-dimensional
    relationship and pattern are too complex to be explained in plain words.
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s where model explainability could save the day. Among the algorithms
    that can dissect machine learning models, SHAP is one of the more agnostic players
    in the field. In this blog, we will dive deep into the following items:'
  prefs: []
  type: TYPE_NORMAL
- en: What are Shapley values?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to calculate them?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use it in Python?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does SHAP support local and global explanability?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What visualizations are available in the SHAP library?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do the common variants of SHAP work? — TreeSHAP & DeepSHAP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does LIME compare against SHAP?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shapley Values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Let’s Play a Game**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When a team of eleven players goes on to win the World Cup, who is the most
    valuable player? Shapley value is a decomposition algorithm that objectively distributes
    the final result to a pool of factors. In explaining a machine learning model,
    Shapley values can be understood as the significance of individual input features’
    contribution to the model’s predicted values.
  prefs: []
  type: TYPE_NORMAL
- en: A Quick Example — How does Shapley value work?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For simplicity’s sake, let’s say we have three attacking players, each with
    a different expected number of goals. We also know that these three players don’t
    always work well with each other, which means depending on the combination of
    the three players, the number of expected goals may be different:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b7a0ef8020c3dda0e8c4a0500369bb6d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: As a baseline, we play none of these three players i.e. number of features **f**
    = 0 and the expected number of goals of the team will be 0.5\. Each of the arrow
    that goes down the matrice indicates a possible stepwise increment when including
    a new feature (or including a player in our case).
  prefs: []
  type: TYPE_NORMAL
- en: 'Following the idea of stepwise expansion of player set, that means we can compute
    the marginal change for each of the arrow. For example, when we move from playing
    none of the players (indicated with the empty set symbol ∅) to playing player
    1 only, the marginal change is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b3c017351d1df630bc3d7fc3c0972a96.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'To obtain the overall contribution of Player 1 among all three players, we
    would have to repeat the same calculation for every scenario where a marginal
    contribution for Player 1 is possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f5c920c7c8a54aeedc0d499a97b903c3.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'With all the marginal changes, we then calculate the weights for them using
    the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/24eaa6281e74f24513e672fb12f5f0f7.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Or, to put it even simpler: it is just the reciprocal of the number of all
    edges pointing into the same row. That means:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3c971d8db5d2fa450580b35a98df399d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'With this, we can now calculate the SHAP value of Player 1 for the expected
    goals:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7ef883cc260650be2c53bbb5a33bf179.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Repeating for the other two players and we will have:'
  prefs: []
  type: TYPE_NORMAL
- en: SHAP of Player 1 = -0.1133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SHAP of Player 2 = -0.0233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SHAP of Player 3 = +0.4666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If I were the head coach, I would have only played Player 3 in this case.
  prefs: []
  type: TYPE_NORMAL
- en: This is very similar to another operator called Choquet Integral for those of
    you who are math-savvier.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Computational Complexity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the above example of only 3 features, we would need to consider 8 different
    models, each with a different input feature set to explain all the features fully.
    In fact, for a full feature set of ***N*** features, the total number of subsets
    would be ***2^N***. Hence, we should be careful with the expected run time when
    using SHAP to explain machine learning models trained with a tall and, more importantly,
    wide dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will first dive into how we can use SHAP in Python
    before diverting most of our attention to different variants of SHAP that aim
    at tackling the complexity of SHAP either with approximation techniques or techniques
    that are model topology specific.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8d69685bb2e9b910bf9a717de10d7121.png)'
  prefs: []
  type: TYPE_IMG
- en: Pascal Triangle — Image from [Wikipedia](https://commons.wikimedia.org/wiki/File:Pascal_triangle.svg)
  prefs: []
  type: TYPE_NORMAL
- en: SHAP in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, let’s look at how to use SHAP in Python
  prefs: []
  type: TYPE_NORMAL
- en: SHAP (**SH**apley **A**dditive ex**P**lanations) is a python library compatible
    with most machine learning model topologies. Installing it is as simple as `pip
    install shap`.
  prefs: []
  type: TYPE_NORMAL
- en: SHAP provides two ways of explaining a machine learning model — global and local
    explainability.
  prefs: []
  type: TYPE_NORMAL
- en: '**Local Explainability with SHAP**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Local explainability attempts to explain the driving forces behind a specific
    prediction. In SHAP, that’s what the individual Shapley values are used for, as
    illustrated in the quick example in an earlier section.
  prefs: []
  type: TYPE_NORMAL
- en: 'In SHAP’s arsenal, two visualizations are implemented to explain individual
    predictions: waterfall graph and force graph. While the waterfall graph gives
    you a better understanding of a stepwise derivation to the prediction results,
    the force graph is designed to provide a sense of the relative strength of the
    features’ contribution to the deviations in prediction results.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** Both visualizations included an overall expected prediction value
    (or base value). That can be understood as the average model output across the
    training set.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Waterfall Plot**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/aad08e9d4a8df18232c490a7f1dac70e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [SHAP GitHub page](https://github.com/slundberg/shap) (MIT license)
  prefs: []
  type: TYPE_NORMAL
- en: On the y-axis, you can find the feature’s name and value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the x-axis, you can find the base value `E[f(X)] = 22.533` that indicates
    the average predicted values across the training set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A red bar in this plot shows the feature’s positive contribution to the predicted
    value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A blue bar in this plot shows the feature’s negative contribution to the predicted
    value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The label on the bars indicates the deviation from the model’s base prediction
    value attributed to the parameter. For example, the AGE = 65.2 has marginally
    contributed +0.19 to the prediction’s deviation from the base value of 22.533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bars are in descending order of the absolute importance of its impact on
    the predicted value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Force Plot**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a212dc9849b0b8279fc962f673c1312d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [SHAP GitHub page](https://github.com/slundberg/shap) (MIT license)
  prefs: []
  type: TYPE_NORMAL
- en: On the x-axis, you can find the base value. That indicates the approximate location
    of the average predicted values across the training set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the x-axis, you can also find the model output with a bolded numeral. That
    indicates the predicted value for this record.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the bottom of the chart, you can find the feature’s name and value, labelled
    either in red or blue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the red bars on the left of the model output are the features that have
    contributed positively to the prediction’s deviation from the base value. The
    names of the feature are at the bottom of the bars. The length of the bar indicates
    the features’ contributions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the blue bars on the right of the model output are the features that have
    contributed negatively to the prediction’s deviation from the base value. The
    names of the feature are at the bottom of the bars. The length of the bar indicates
    the features’ contributions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Global Explainability with SHAP**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Global explainability can be understood as understanding the overall importance
    of each feature in the model across the entire dataset and providing a general
    knowledge of the data and the underlying patterns. Due to the fuzziness in decomposing
    individual predictions’ contributions and aggregating across the data, there is
    more than one way to attempt global explainability. Examples include information
    gain, aggregated weights, permutation-based feature importance, and Shapley values.
    SHAP focuses on the last one, of course.
  prefs: []
  type: TYPE_NORMAL
- en: SHAP provides a visualization in which we can look into the average Shapley
    values of a feature across the dataset. Unlike other mechanisms that provide a
    measure of importance using statistically more complex interpretations, SHAP’s
    global explainability delivers an immediately understandable impact by allowing
    you to say that, on average, the feature relationship pushes the prediction value
    about 1.0 higher for data records with “Class 1” than data records with “Class
    0”.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1fbd124497685590b241fb212c7024b0.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [SHAP GitHub page](https://github.com/slundberg/shap) (MIT license)
  prefs: []
  type: TYPE_NORMAL
- en: SHAP’s global explainability feature allows us to troubleshoot or investigate
    model bias. Taking the image above as an example, Age is generally a very significant
    feature. Could this be a sign that the model is biased towards specific age groups
    unnecessarily? Also, could one of the very important features be a potential data
    leak? All these questions allow us to improve the model before deploying a more
    responsible and robust machine-learning model.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note:** If you are interested in learning more about responsible AI, I have
    also written a piece on how we can approach that using 5 easy steps.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://pub.towardsai.net/unlock-the-power-of-responsible-ai-5-steps-to-ensure-ethical-systems-a5aeeb5ff65c?source=post_page-----72f0bea35f7c--------------------------------)
    [## Unlock the Power of Responsible AI: 5 Steps to Ensure Ethical Systems'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Steps for Building Responsible AI Systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: pub.towardsai.net](https://pub.towardsai.net/unlock-the-power-of-responsible-ai-5-steps-to-ensure-ethical-systems-a5aeeb5ff65c?source=post_page-----72f0bea35f7c--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Another visualization that SHAP supports is a stacked version of the force graph
    in the local explainability section. By stacking the force charts, we can visualise
    the interactions between the model and the features that are given different input
    values. This gives us a clustering view based on Shapley values and provides us
    with perspectives on how the model sees the data. This can be very powerful for
    revising and validating hypotheses and underlying business logic. There might
    also be chances that you would find new ways of segregating your data after analysing
    all the Shapley values!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/214a53d8c8a8380f90f009b863baac61.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [SHAP GitHub page](https://github.com/slundberg/shap) (MIT license)
  prefs: []
  type: TYPE_NORMAL
- en: Variations of SHAP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**TreeSHAP**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Pros:** Efficient and accurate algorithm for computing Shapley values of
    tree-based models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons:** Only applicable to tree-based models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unlike the original SHAP, TreeSHAP is tree-based machine learning model-specific.
    This means TreeSHAP will only work on models such as decision trees, random forests,
    gradient-boosting machines etc.
  prefs: []
  type: TYPE_NORMAL
- en: TreeSHAP is specific to tree models because it takes advantage of the tree structures
    for computing accurate Shapley values more efficiently than SHAP. As these structures
    do not exist in other model topologies, TreeSHAP is only restricted to tree-based
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 'TreeSHAP can calculate Shapley values using interventional and tree path dependent
    approaches. This can be specified in the `feature_perturbation` parameter. The
    tree path dependent approach calculates the changes in conditional expectation
    recursively. Let’s use a simple decision tree that accepts 2 features **(x, y)**
    as an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b029f92e885c46074b756e3264103f3d.png)'
  prefs: []
  type: TYPE_IMG
- en: Example Decision Tree — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'In the example above, we have a decision tree that contains 7 nodes, accepts
    two features **(x, y)** to predict **z** and has been trained with **8** training
    samples. To compute the **local** contribution of **y** to the prediction of **z**
    in a coalition **(x=10, y=5)**, we need to consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: For **(x=10, y=5)**, the model will go from Node 1 to Node 3 and reach Node
    6\. As Node 6 is a leaf node, the model is certain that the prediction is **z=4**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For **(x=10)**, the model will go from Node 1 to Node 3\. However, as Node 3
    is not a leave node, the expected predicted value can be inferred as a weighted
    sum of all the leaf nodes of Node 3\. Among the 5 training samples that went through
    Node 3, two are predicted to have **z=4** while the others are predicted to have
    **z=24**. The weighted sum is **4*(2/5) + 24*(3/5)=1.6 + 14.4 = 16**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The marginal contribution of **y** in the prediction of **z** for the coalition
    **(x=10, y=5)** can be calculated as **Prediction(x=10, y=5) — Prediction(x=10)
    = 4–16= -12**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Note:** The negative contribution here does not mean the feature **y** is
    unimportant, but rather that feature **y** has pushed the prediction value by
    **-12**.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: By continuing the process across all the features, TreeSHAP will obtain all
    the Shapley values and provide both local explainability (using the method above)
    and global explainability (average out all the local explainability results across
    the training set)
  prefs: []
  type: TYPE_NORMAL
- en: As its name suggests, the interventional approach calculates the Shapley values
    by artificially adjusting the value of the feature of interest. In our case above,
    that can be to change **y** from 5 to 4\. To estimate the sensitivity, TreeSHAP
    will need to repeatedly use a background set/training set as reference points
    (This will be touched on again when we discuss LIME in the final section) with
    linear runtime complexity. Hence, when using the interventional approach, we should
    be more mindful of the scalability of TreeSHAP.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**DeepSHAP**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Pros:** Efficient algorithm for approximating Shapley values of deep learning
    or neural network based models. Compatible with Tensorflow and PyTorch'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons:** Only applicable to deep learning or neural network based models.
    Less accurate than SHAP due to the approximation nature of the algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can’t skip neural networks when discussing explainability. DeepSHAP is a
    combination of SHAP and DeepLIFT that aims at cracking the philosophy behind deep
    learning models. It is specifically designed for deep learning models, which makes
    DeepSHAP only applicable to neural network based models.
  prefs: []
  type: TYPE_NORMAL
- en: DeepSHAP tries to approximate the Shapley values. A relatively primitive way
    of explaining DeepSHAP is that it attempts to assign local marginal contribution
    of feature **x** using gradients or partial derivatives with a meaningful background/reference
    point (e.g. pitch black for image recognition models, 0% for predicting one’s
    chance to get-rich-quick).
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: There is a further research released on a generalised version of DeepSHAP
    — G-DeepSHAP. Feel free to give it a read here in [arxiv](https://arxiv.org/pdf/2105.00108.pdf).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: LIME — Alternative to SHAP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LIME(Local Interpretable Model-Agnostic Explanations) is an alternative to SHAP
    for explaining predictions. It is a model-agnostic approach with a default assumption
    in kernel size (size of local neighbourhood considered when explaining individual
    prediction) for approximating a feature’s contribution to a local instance. In
    general, choosing a smaller kernel size, the results provided by LIME will lean
    towards local interpretation of how the values of the features have contributed
    to the prediction. (i.e. larger kernel size tends to provide a more global view)
  prefs: []
  type: TYPE_NORMAL
- en: However, the choice of kernel size should be carefully decided dependent on
    the data and the pattern. Hence, when using LIME, we should consider adjusting
    the kernel size accordingly to obtain a reasonable interpretation of the machine
    learning model.
  prefs: []
  type: TYPE_NORMAL
- en: 'To give it a try, we can install and use the package with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As a final recap, here is a quick summary of everything discussed in this blog
    post:'
  prefs: []
  type: TYPE_NORMAL
- en: SHAP is a game theory based approach for explaining machine learning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SHAP considers all possible combinations of features to evaluate the impact
    of every feature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SHAP value of a feature **f** for a local prediction instance is a weighted
    sum of the marginal changes due to the inclusion of the feature across all the
    possible combination of features that includes **f**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The marginal changes are weighted according to the reciprocal of ***f* × *C(F,
    f)*** for ***F*** to be the number of features considered by the actual model
    and ***f*** to be the number of features considered when calculating the marginal
    changes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As SHAP considers all possible combinations of features, hence the algorithm
    does not scale linearly and would suffer from the curse of dimensionality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Several variants of SHAP have been commonly used to address SHAP’s computational
    complexity:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/a90b907281ededff895eaac6034aaa42.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: We should consider using TreeSHAP for tree-based models and DeepSHAP for deep
    learning based models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LIME is an alternative to SHAP that is also model agnostic that approximates
    a feature’s contribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explanation by LIME can be significantly different depending on the choice of
    kernel size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That’s about it for this comprehensive tour into SHAP. I hope you have found
    this helpful for pushing your content to the next level or getting started as
    a writer. If you have enjoyed the read, you can also support me by subscribing
    to Medium using my affiliate link below. This has been a platform where I have
    found lots of enjoyable reads. Even if you are perfectly content with not subscribing,
    you can also support me and my creation using claps.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://louis-chan.medium.com/membership?source=post_page-----72f0bea35f7c--------------------------------)
    [## Join Medium with my referral link — Louis Chan'
  prefs: []
  type: TYPE_NORMAL
- en: Read every story from Louis Chan (and thousands of other writers on Medium).
    Your membership fee directly supports…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: louis-chan.medium.com](https://louis-chan.medium.com/membership?source=post_page-----72f0bea35f7c--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Last but definitely not least, if I have missed/mistaken anything critical,
    please feel free to drop a comment or send me a DM through LinkedIn. Let’s keep
    the knowledge flowing and get better at this domain together!
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.linkedin.com/in/louis-chan-b55b9287?source=post_page-----72f0bea35f7c--------------------------------)
    [## Louis Chan — Lead GCP Data & ML Engineer — Associate Director — KPMG UK |
    LinkedIn'
  prefs: []
  type: TYPE_NORMAL
- en: Ambitious, curious and creative individual with a strong belief in inter-connectivity
    between branches of knowledge and a…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.linkedin.com](https://www.linkedin.com/in/louis-chan-b55b9287?source=post_page-----72f0bea35f7c--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lundberg, Scott M., and Su-In Lee. “A Unified Approach to Interpreting Model
    Predictions.” Advances in Neural Information Processing Systems, 2017.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lundberg, Scott, and Su-In Lee. “Consistent Individualized Feature Attribution
    for Tree Ensembles.” arXiv preprint arXiv:1802.03888, 2018.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. “Why Should I Trust
    You? Explaining the Predictions of Any Classifier.” Proceedings of the 22nd ACM
    SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. “Anchors: High-Precision
    Model-Agnostic Explanations.” arXiv preprint arXiv:1802.07814, 2018.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
