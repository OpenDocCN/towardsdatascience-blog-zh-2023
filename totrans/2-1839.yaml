- en: 'SHAP: Explain Any Machine Learning Model in Python'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/shap-explain-any-machine-learning-model-in-python-72f0bea35f7c](https://towardsdatascience.com/shap-explain-any-machine-learning-model-in-python-72f0bea35f7c)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/4b8cc3946e79898437b4c129844a6099.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: Photo by [Priscilla Du Preez](https://unsplash.com/@priscilladupreez?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Your Comprehensive Guide to SHAP, TreeSHAP, and DeepSHAP
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://louis-chan.medium.com/?source=post_page-----72f0bea35f7c--------------------------------)[![Louis
    Chan](../Images/6d8df9a478e929dd521059631f26e081.png)](https://louis-chan.medium.com/?source=post_page-----72f0bea35f7c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----72f0bea35f7c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----72f0bea35f7c--------------------------------)
    [Louis Chan](https://louis-chan.medium.com/?source=post_page-----72f0bea35f7c--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----72f0bea35f7c--------------------------------)
    ·13 min read·Jan 11, 2023
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Motivation
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Story Time!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you have trained a machine learning model to predict the default risk
    of mortgage applicants. All is good, and the performance is excellent too. But
    how does the model work? How does the model come to the predicted value?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: We stood there and said that the model considers several variables and the multi-dimensional
    relationship and pattern are too complex to be explained in plain words.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s where model explainability could save the day. Among the algorithms
    that can dissect machine learning models, SHAP is one of the more agnostic players
    in the field. In this blog, we will dive deep into the following items:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: What are Shapley values?
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to calculate them?
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use it in Python?
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does SHAP support local and global explanability?
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What visualizations are available in the SHAP library?
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do the common variants of SHAP work? — TreeSHAP & DeepSHAP
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does LIME compare against SHAP?
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shapley Values
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Let’s Play a Game**'
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When a team of eleven players goes on to win the World Cup, who is the most
    valuable player? Shapley value is a decomposition algorithm that objectively distributes
    the final result to a pool of factors. In explaining a machine learning model,
    Shapley values can be understood as the significance of individual input features’
    contribution to the model’s predicted values.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: A Quick Example — How does Shapley value work?
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For simplicity’s sake, let’s say we have three attacking players, each with
    a different expected number of goals. We also know that these three players don’t
    always work well with each other, which means depending on the combination of
    the three players, the number of expected goals may be different:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b7a0ef8020c3dda0e8c4a0500369bb6d.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: As a baseline, we play none of these three players i.e. number of features **f**
    = 0 and the expected number of goals of the team will be 0.5\. Each of the arrow
    that goes down the matrice indicates a possible stepwise increment when including
    a new feature (or including a player in our case).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'Following the idea of stepwise expansion of player set, that means we can compute
    the marginal change for each of the arrow. For example, when we move from playing
    none of the players (indicated with the empty set symbol ∅) to playing player
    1 only, the marginal change is:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b3c017351d1df630bc3d7fc3c0972a96.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 'To obtain the overall contribution of Player 1 among all three players, we
    would have to repeat the same calculation for every scenario where a marginal
    contribution for Player 1 is possible:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f5c920c7c8a54aeedc0d499a97b903c3.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 'With all the marginal changes, we then calculate the weights for them using
    the following formula:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/24eaa6281e74f24513e672fb12f5f0f7.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'Or, to put it even simpler: it is just the reciprocal of the number of all
    edges pointing into the same row. That means:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3c971d8db5d2fa450580b35a98df399d.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: 'With this, we can now calculate the SHAP value of Player 1 for the expected
    goals:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7ef883cc260650be2c53bbb5a33bf179.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'Repeating for the other two players and we will have:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: SHAP of Player 1 = -0.1133
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SHAP of Player 2 = -0.0233
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SHAP of Player 3 = +0.4666
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If I were the head coach, I would have only played Player 3 in this case.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: This is very similar to another operator called Choquet Integral for those of
    you who are math-savvier.
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Computational Complexity
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the above example of only 3 features, we would need to consider 8 different
    models, each with a different input feature set to explain all the features fully.
    In fact, for a full feature set of ***N*** features, the total number of subsets
    would be ***2^N***. Hence, we should be careful with the expected run time when
    using SHAP to explain machine learning models trained with a tall and, more importantly,
    wide dataset.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will first dive into how we can use SHAP in Python
    before diverting most of our attention to different variants of SHAP that aim
    at tackling the complexity of SHAP either with approximation techniques or techniques
    that are model topology specific.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8d69685bb2e9b910bf9a717de10d7121.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: Pascal Triangle — Image from [Wikipedia](https://commons.wikimedia.org/wiki/File:Pascal_triangle.svg)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: SHAP in Python
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, let’s look at how to use SHAP in Python
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们探讨如何在 Python 中使用 SHAP。
- en: SHAP (**SH**apley **A**dditive ex**P**lanations) is a python library compatible
    with most machine learning model topologies. Installing it is as simple as `pip
    install shap`.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP (**SH**apley **A**dditive ex**P**lanations) 是一个兼容大多数机器学习模型拓扑的 Python 库。安装非常简单，只需
    `pip install shap`。
- en: SHAP provides two ways of explaining a machine learning model — global and local
    explainability.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP 提供了两种解释机器学习模型的方法——全局解释和本地解释。
- en: '**Local Explainability with SHAP**'
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**使用 SHAP 进行本地可解释性**'
- en: Local explainability attempts to explain the driving forces behind a specific
    prediction. In SHAP, that’s what the individual Shapley values are used for, as
    illustrated in the quick example in an earlier section.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 本地可解释性试图解释特定预测背后的驱动因素。在 SHAP 中，个体 Shapley 值就是用来做这个的，如早期部分的快速示例所示。
- en: 'In SHAP’s arsenal, two visualizations are implemented to explain individual
    predictions: waterfall graph and force graph. While the waterfall graph gives
    you a better understanding of a stepwise derivation to the prediction results,
    the force graph is designed to provide a sense of the relative strength of the
    features’ contribution to the deviations in prediction results.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SHAP 的工具集中，有两种可视化方法用于解释个体预测：瀑布图和力图。瀑布图让你更好地理解逐步推导预测结果的过程，而力图旨在提供特征对预测结果偏差的相对贡献强度。
- en: '**Note:** Both visualizations included an overall expected prediction value
    (or base value). That can be understood as the average model output across the
    training set.'
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** 两种可视化都包括了一个整体期望预测值（或基准值）。这可以理解为训练集上模型输出的平均值。'
- en: '**Waterfall Plot**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**瀑布图**'
- en: '[PRE0]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/aad08e9d4a8df18232c490a7f1dac70e.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aad08e9d4a8df18232c490a7f1dac70e.png)'
- en: Image from [SHAP GitHub page](https://github.com/slundberg/shap) (MIT license)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [SHAP GitHub 页面](https://github.com/slundberg/shap)（MIT 许可证）
- en: On the y-axis, you can find the feature’s name and value
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 y 轴上，你可以找到特征的名称和值。
- en: On the x-axis, you can find the base value `E[f(X)] = 22.533` that indicates
    the average predicted values across the training set
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 x 轴上，你可以找到基准值 `E[f(X)] = 22.533`，这表示训练集上的平均预测值。
- en: A red bar in this plot shows the feature’s positive contribution to the predicted
    value
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图中红色条形表示特征对预测值的正贡献。
- en: A blue bar in this plot shows the feature’s negative contribution to the predicted
    value
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图中蓝色条形表示特征对预测值的负贡献。
- en: The label on the bars indicates the deviation from the model’s base prediction
    value attributed to the parameter. For example, the AGE = 65.2 has marginally
    contributed +0.19 to the prediction’s deviation from the base value of 22.533
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 条形上的标签表示归因于参数的模型基准预测值的偏差。例如，AGE = 65.2 对预测值的偏差从基准值 22.533 上贡献了 +0.19。
- en: The bars are in descending order of the absolute importance of its impact on
    the predicted value
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 条形按其对预测值的绝对重要性降序排列。
- en: '**Force Plot**'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**力图**'
- en: '[PRE1]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/a212dc9849b0b8279fc962f673c1312d.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a212dc9849b0b8279fc962f673c1312d.png)'
- en: Image from [SHAP GitHub page](https://github.com/slundberg/shap) (MIT license)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [SHAP GitHub 页面](https://github.com/slundberg/shap)（MIT 许可证）
- en: On the x-axis, you can find the base value. That indicates the approximate location
    of the average predicted values across the training set.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 x 轴上，你可以找到基准值。这表示训练集上平均预测值的大致位置。
- en: On the x-axis, you can also find the model output with a bolded numeral. That
    indicates the predicted value for this record.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 x 轴上，你还可以找到用粗体数字标记的模型输出。这表示该记录的预测值。
- en: At the bottom of the chart, you can find the feature’s name and value, labelled
    either in red or blue.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在图表底部，你可以找到特征的名称和值，标记为红色或蓝色。
- en: All the red bars on the left of the model output are the features that have
    contributed positively to the prediction’s deviation from the base value. The
    names of the feature are at the bottom of the bars. The length of the bar indicates
    the features’ contributions.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有在模型输出左侧的红色条形是对预测偏离基准值有正面贡献的特征。特征的名称在条形的底部。条形的长度表示特征的贡献。
- en: All the blue bars on the right of the model output are the features that have
    contributed negatively to the prediction’s deviation from the base value. The
    names of the feature are at the bottom of the bars. The length of the bar indicates
    the features’ contributions.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型输出右侧的所有蓝色条形图表示对预测偏离基准值产生负面贡献的特征。特征的名称位于条形图底部。条形图的长度表示特征的贡献。
- en: '**Global Explainability with SHAP**'
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**SHAP 的全球可解释性**'
- en: Global explainability can be understood as understanding the overall importance
    of each feature in the model across the entire dataset and providing a general
    knowledge of the data and the underlying patterns. Due to the fuzziness in decomposing
    individual predictions’ contributions and aggregating across the data, there is
    more than one way to attempt global explainability. Examples include information
    gain, aggregated weights, permutation-based feature importance, and Shapley values.
    SHAP focuses on the last one, of course.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 全球可解释性可以理解为在整个数据集中理解每个特征的整体重要性，并提供对数据和潜在模式的一般了解。由于分解个体预测贡献和在数据中聚合的模糊性，尝试全球可解释性的方法不止一种。示例包括信息增益、汇总权重、基于置换的特征重要性和
    Shapley 值。SHAP 当然专注于最后一个。
- en: SHAP provides a visualization in which we can look into the average Shapley
    values of a feature across the dataset. Unlike other mechanisms that provide a
    measure of importance using statistically more complex interpretations, SHAP’s
    global explainability delivers an immediately understandable impact by allowing
    you to say that, on average, the feature relationship pushes the prediction value
    about 1.0 higher for data records with “Class 1” than data records with “Class
    0”.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP 提供了一种可视化方法，我们可以查看特征在数据集中的平均 Shapley 值。与其他使用统计上更复杂解释来提供重要性度量的机制不同，SHAP 的全球可解释性通过让你能够说，平均而言，特征关系使得“Class
    1”数据记录的预测值比“Class 0”数据记录高约 1.0，从而提供了一个立即可理解的影响。
- en: '![](../Images/1fbd124497685590b241fb212c7024b0.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1fbd124497685590b241fb212c7024b0.png)'
- en: Image from [SHAP GitHub page](https://github.com/slundberg/shap) (MIT license)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来自[SHAP GitHub 页面](https://github.com/slundberg/shap)（MIT 许可证）
- en: SHAP’s global explainability feature allows us to troubleshoot or investigate
    model bias. Taking the image above as an example, Age is generally a very significant
    feature. Could this be a sign that the model is biased towards specific age groups
    unnecessarily? Also, could one of the very important features be a potential data
    leak? All these questions allow us to improve the model before deploying a more
    responsible and robust machine-learning model.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP 的全球可解释性功能允许我们排查或调查模型偏差。以上面的图像为例，年龄通常是一个非常重要的特征。这是否可能表明模型对特定年龄组存在不必要的偏见？此外，一个非常重要的特征是否可能是潜在的数据泄露？所有这些问题都使我们在部署更负责任且强健的机器学习模型之前，能够改进模型。
- en: '**Note:** If you are interested in learning more about responsible AI, I have
    also written a piece on how we can approach that using 5 easy steps.'
  id: totrans-87
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** 如果你有兴趣了解更多关于负责任的人工智能，我还写了一篇关于如何通过 5 个简单步骤来实现这一目标的文章。'
- en: '[](https://pub.towardsai.net/unlock-the-power-of-responsible-ai-5-steps-to-ensure-ethical-systems-a5aeeb5ff65c?source=post_page-----72f0bea35f7c--------------------------------)
    [## Unlock the Power of Responsible AI: 5 Steps to Ensure Ethical Systems'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 解锁负责任的人工智能：确保伦理系统的 5 个步骤](https://pub.towardsai.net/unlock-the-power-of-responsible-ai-5-steps-to-ensure-ethical-systems-a5aeeb5ff65c?source=post_page-----72f0bea35f7c--------------------------------)'
- en: 5 Steps for Building Responsible AI Systems
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 负责任的人工智能系统的 5 个步骤
- en: pub.towardsai.net](https://pub.towardsai.net/unlock-the-power-of-responsible-ai-5-steps-to-ensure-ethical-systems-a5aeeb5ff65c?source=post_page-----72f0bea35f7c--------------------------------)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[pub.towardsai.net](https://pub.towardsai.net/unlock-the-power-of-responsible-ai-5-steps-to-ensure-ethical-systems-a5aeeb5ff65c?source=post_page-----72f0bea35f7c--------------------------------)'
- en: Another visualization that SHAP supports is a stacked version of the force graph
    in the local explainability section. By stacking the force charts, we can visualise
    the interactions between the model and the features that are given different input
    values. This gives us a clustering view based on Shapley values and provides us
    with perspectives on how the model sees the data. This can be very powerful for
    revising and validating hypotheses and underlying business logic. There might
    also be chances that you would find new ways of segregating your data after analysing
    all the Shapley values!
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP支持的另一种可视化是局部可解释性部分的力图堆叠版本。通过堆叠力图，我们可以可视化模型与不同输入值的特征之间的交互。这为我们提供了基于Shapley值的聚类视图，并提供了模型如何看待数据的视角。这对修正和验证假设以及基础业务逻辑非常有用。在分析所有Shapley值后，你可能还会发现数据分割的新方法！
- en: '[PRE2]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/214a53d8c8a8380f90f009b863baac61.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/214a53d8c8a8380f90f009b863baac61.png)'
- en: Image from [SHAP GitHub page](https://github.com/slundberg/shap) (MIT license)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于[SHAP GitHub页面](https://github.com/slundberg/shap)（MIT许可证）
- en: Variations of SHAP
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SHAP的变体
- en: '**TreeSHAP**'
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**TreeSHAP**'
- en: '**Pros:** Efficient and accurate algorithm for computing Shapley values of
    tree-based models.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优点：** 高效且准确的算法，用于计算基于树模型的Shapley值。'
- en: '**Cons:** Only applicable to tree-based models.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺点：** 仅适用于基于树的模型。'
- en: Unlike the original SHAP, TreeSHAP is tree-based machine learning model-specific.
    This means TreeSHAP will only work on models such as decision trees, random forests,
    gradient-boosting machines etc.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 与原始SHAP不同，TreeSHAP是特定于基于树的机器学习模型的。这意味着TreeSHAP仅适用于决策树、随机森林、梯度提升机等模型。
- en: TreeSHAP is specific to tree models because it takes advantage of the tree structures
    for computing accurate Shapley values more efficiently than SHAP. As these structures
    do not exist in other model topologies, TreeSHAP is only restricted to tree-based
    models.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: TreeSHAP特定于树模型，因为它利用树结构来更高效地计算准确的Shapley值。由于这些结构在其他模型拓扑中不存在，因此TreeSHAP仅限于基于树的模型。
- en: 'TreeSHAP can calculate Shapley values using interventional and tree path dependent
    approaches. This can be specified in the `feature_perturbation` parameter. The
    tree path dependent approach calculates the changes in conditional expectation
    recursively. Let’s use a simple decision tree that accepts 2 features **(x, y)**
    as an example:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: TreeSHAP可以通过干预和树路径依赖的方法计算Shapley值。这可以在`feature_perturbation`参数中指定。树路径依赖方法递归地计算条件期望的变化。我们以一个接受2个特征**(x,
    y)**的简单决策树为例：
- en: '![](../Images/b029f92e885c46074b756e3264103f3d.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b029f92e885c46074b756e3264103f3d.png)'
- en: Example Decision Tree — Image by Author
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 示例决策树 — 作者提供的图片
- en: 'In the example above, we have a decision tree that contains 7 nodes, accepts
    two features **(x, y)** to predict **z** and has been trained with **8** training
    samples. To compute the **local** contribution of **y** to the prediction of **z**
    in a coalition **(x=10, y=5)**, we need to consider the following:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的示例中，我们有一个包含7个节点的决策树，接受两个特征**(x, y)**来预测**z**，并且已经用**8**个训练样本进行了训练。为了计算在联盟**(x=10,
    y=5)**中**y**对**z**预测的**局部**贡献，我们需要考虑以下因素：
- en: For **(x=10, y=5)**, the model will go from Node 1 to Node 3 and reach Node
    6\. As Node 6 is a leaf node, the model is certain that the prediction is **z=4**.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于**(x=10, y=5)**，模型将从节点1移动到节点3并到达节点6。由于节点6是叶节点，模型确定预测为**z=4**。
- en: For **(x=10)**, the model will go from Node 1 to Node 3\. However, as Node 3
    is not a leave node, the expected predicted value can be inferred as a weighted
    sum of all the leaf nodes of Node 3\. Among the 5 training samples that went through
    Node 3, two are predicted to have **z=4** while the others are predicted to have
    **z=24**. The weighted sum is **4*(2/5) + 24*(3/5)=1.6 + 14.4 = 16**.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于**(x=10)**，模型将从节点1移动到节点3。然而，由于节点3不是叶节点，预测值可以推断为节点3所有叶节点的加权和。在通过节点3的5个训练样本中，有两个预测为**z=4**，而其他的预测为**z=24**。加权和为**4*(2/5)
    + 24*(3/5)=1.6 + 14.4 = 16**。
- en: The marginal contribution of **y** in the prediction of **z** for the coalition
    **(x=10, y=5)** can be calculated as **Prediction(x=10, y=5) — Prediction(x=10)
    = 4–16= -12**.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在联盟**(x=10, y=5)**中，**y**对**z**预测的边际贡献可以计算为**Prediction(x=10, y=5) — Prediction(x=10)
    = 4–16= -12**。
- en: '**Note:** The negative contribution here does not mean the feature **y** is
    unimportant, but rather that feature **y** has pushed the prediction value by
    **-12**.'
  id: totrans-108
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** 这里的负贡献并不意味着特征**y**不重要，而是特征**y**将预测值推高了**-12**。'
- en: By continuing the process across all the features, TreeSHAP will obtain all
    the Shapley values and provide both local explainability (using the method above)
    and global explainability (average out all the local explainability results across
    the training set)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: As its name suggests, the interventional approach calculates the Shapley values
    by artificially adjusting the value of the feature of interest. In our case above,
    that can be to change **y** from 5 to 4\. To estimate the sensitivity, TreeSHAP
    will need to repeatedly use a background set/training set as reference points
    (This will be touched on again when we discuss LIME in the final section) with
    linear runtime complexity. Hence, when using the interventional approach, we should
    be more mindful of the scalability of TreeSHAP.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**DeepSHAP**'
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Pros:** Efficient algorithm for approximating Shapley values of deep learning
    or neural network based models. Compatible with Tensorflow and PyTorch'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons:** Only applicable to deep learning or neural network based models.
    Less accurate than SHAP due to the approximation nature of the algorithm'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can’t skip neural networks when discussing explainability. DeepSHAP is a
    combination of SHAP and DeepLIFT that aims at cracking the philosophy behind deep
    learning models. It is specifically designed for deep learning models, which makes
    DeepSHAP only applicable to neural network based models.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: DeepSHAP tries to approximate the Shapley values. A relatively primitive way
    of explaining DeepSHAP is that it attempts to assign local marginal contribution
    of feature **x** using gradients or partial derivatives with a meaningful background/reference
    point (e.g. pitch black for image recognition models, 0% for predicting one’s
    chance to get-rich-quick).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: There is a further research released on a generalised version of DeepSHAP
    — G-DeepSHAP. Feel free to give it a read here in [arxiv](https://arxiv.org/pdf/2105.00108.pdf).'
  id: totrans-117
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: LIME — Alternative to SHAP
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LIME(Local Interpretable Model-Agnostic Explanations) is an alternative to SHAP
    for explaining predictions. It is a model-agnostic approach with a default assumption
    in kernel size (size of local neighbourhood considered when explaining individual
    prediction) for approximating a feature’s contribution to a local instance. In
    general, choosing a smaller kernel size, the results provided by LIME will lean
    towards local interpretation of how the values of the features have contributed
    to the prediction. (i.e. larger kernel size tends to provide a more global view)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: However, the choice of kernel size should be carefully decided dependent on
    the data and the pattern. Hence, when using LIME, we should consider adjusting
    the kernel size accordingly to obtain a reasonable interpretation of the machine
    learning model.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 'To give it a try, we can install and use the package with:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Conclusion
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As a final recap, here is a quick summary of everything discussed in this blog
    post:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: SHAP is a game theory based approach for explaining machine learning models
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SHAP 是一种基于博弈论的方法，用于解释机器学习模型。
- en: SHAP considers all possible combinations of features to evaluate the impact
    of every feature
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SHAP 考虑所有可能的特征组合以评估每个特征的影响。
- en: SHAP value of a feature **f** for a local prediction instance is a weighted
    sum of the marginal changes due to the inclusion of the feature across all the
    possible combination of features that includes **f**
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征 **f** 对于本地预测实例的 SHAP 值是由于特征的引入在包含 **f** 的所有可能特征组合中的边际变化的加权总和。
- en: The marginal changes are weighted according to the reciprocal of ***f* × *C(F,
    f)*** for ***F*** to be the number of features considered by the actual model
    and ***f*** to be the number of features considered when calculating the marginal
    changes
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 边际变化的权重根据 ***f* × *C(F, f)*** 的倒数进行，其中 ***F*** 是实际模型考虑的特征数量，而 ***f*** 是计算边际变化时考虑的特征数量。
- en: As SHAP considers all possible combinations of features, hence the algorithm
    does not scale linearly and would suffer from the curse of dimensionality
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于 SHAP 考虑了所有可能的特征组合，因此算法不会线性扩展，会受到维度灾难的影响。
- en: 'Several variants of SHAP have been commonly used to address SHAP’s computational
    complexity:'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了应对 SHAP 的计算复杂性，已经常用几种 SHAP 的变体：
- en: '![](../Images/a90b907281ededff895eaac6034aaa42.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a90b907281ededff895eaac6034aaa42.png)'
- en: Image by Author
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于作者
- en: We should consider using TreeSHAP for tree-based models and DeepSHAP for deep
    learning based models
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该考虑对基于树的模型使用 TreeSHAP，对基于深度学习的模型使用 DeepSHAP。
- en: LIME is an alternative to SHAP that is also model agnostic that approximates
    a feature’s contribution
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LIME 是一种替代 SHAP 的模型无关方法，用于近似特征的贡献。
- en: Explanation by LIME can be significantly different depending on the choice of
    kernel size
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LIME 的解释可以根据内核大小的选择显著不同。
- en: That’s about it for this comprehensive tour into SHAP. I hope you have found
    this helpful for pushing your content to the next level or getting started as
    a writer. If you have enjoyed the read, you can also support me by subscribing
    to Medium using my affiliate link below. This has been a platform where I have
    found lots of enjoyable reads. Even if you are perfectly content with not subscribing,
    you can also support me and my creation using claps.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 SHAP 的这次全面介绍就是这些了。我希望你发现这些内容对提升你的写作水平或开始写作有所帮助。如果你喜欢这篇文章，你也可以通过下面的我的附属链接订阅
    Medium 来支持我。这是一个我发现了很多有趣读物的平台。即使你完全不打算订阅，你也可以通过点“赞”来支持我和我的创作。
- en: '[](https://louis-chan.medium.com/membership?source=post_page-----72f0bea35f7c--------------------------------)
    [## Join Medium with my referral link — Louis Chan'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://louis-chan.medium.com/membership?source=post_page-----72f0bea35f7c--------------------------------)
    [## 通过我的推荐链接加入 Medium — Louis Chan'
- en: Read every story from Louis Chan (and thousands of other writers on Medium).
    Your membership fee directly supports…
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阅读 Louis Chan 的每一个故事（以及 Medium 上成千上万的其他作家的故事）。你的会员费直接支持…
- en: louis-chan.medium.com](https://louis-chan.medium.com/membership?source=post_page-----72f0bea35f7c--------------------------------)
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: louis-chan.medium.com](https://louis-chan.medium.com/membership?source=post_page-----72f0bea35f7c--------------------------------)
- en: Last but definitely not least, if I have missed/mistaken anything critical,
    please feel free to drop a comment or send me a DM through LinkedIn. Let’s keep
    the knowledge flowing and get better at this domain together!
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但绝对不是最不重要的，如果我遗漏或误解了任何关键内容，请随时在评论中指出或通过 LinkedIn 给我发消息。让我们一起保持知识的流动，共同在这个领域中进步！
- en: '[](https://www.linkedin.com/in/louis-chan-b55b9287?source=post_page-----72f0bea35f7c--------------------------------)
    [## Louis Chan — Lead GCP Data & ML Engineer — Associate Director — KPMG UK |
    LinkedIn'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://www.linkedin.com/in/louis-chan-b55b9287?source=post_page-----72f0bea35f7c--------------------------------)
    [## Louis Chan — 主任级 GCP 数据与 ML 工程师 — 副总监 — KPMG 英国 | LinkedIn'
- en: Ambitious, curious and creative individual with a strong belief in inter-connectivity
    between branches of knowledge and a…
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 有抱负、好奇且富有创意的个人，坚信知识领域之间的相互联系。
- en: www.linkedin.com](https://www.linkedin.com/in/louis-chan-b55b9287?source=post_page-----72f0bea35f7c--------------------------------)
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: www.linkedin.com](https://www.linkedin.com/in/louis-chan-b55b9287?source=post_page-----72f0bea35f7c--------------------------------)
- en: References
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Lundberg, Scott M., and Su-In Lee. “A Unified Approach to Interpreting Model
    Predictions.” Advances in Neural Information Processing Systems, 2017.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Lundberg, Scott M., 和 Su-In Lee. “统一的模型预测解释方法。” 神经信息处理系统进展，2017。
- en: Lundberg, Scott, and Su-In Lee. “Consistent Individualized Feature Attribution
    for Tree Ensembles.” arXiv preprint arXiv:1802.03888, 2018.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Lundberg, Scott, 和 Su-In Lee. “一致的个性化特征归因用于树集成。” arXiv 预印本 arXiv:1802.03888,
    2018.
- en: Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. “Why Should I Trust
    You? Explaining the Predictions of Any Classifier.” Proceedings of the 22nd ACM
    SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ribeiro, Marco Tulio, Sameer Singh, 和 Carlos Guestrin. “我为什么应该相信你？解释任何分类器的预测。”
    第22届 ACM SIGKDD 国际知识发现与数据挖掘大会论文集, 2016.
- en: 'Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. “Anchors: High-Precision
    Model-Agnostic Explanations.” arXiv preprint arXiv:1802.07814, 2018.'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Ribeiro, Marco Tulio, Sameer Singh, 和 Carlos Guestrin. “Anchors: 高精度模型无关解释。”
    arXiv 预印本 arXiv:1802.07814, 2018.'
