- en: Learn How to Build and Deploy a Voice Chatbot with Langchain and BentoML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/deploy-a-voice-based-chatbot-with-bentoml-langchain-and-gradio-7f25af3e45df](https://towardsdatascience.com/deploy-a-voice-based-chatbot-with-bentoml-langchain-and-gradio-7f25af3e45df)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: BentoML is Like Lego for ML engineers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ahmedbesbes.medium.com/?source=post_page-----7f25af3e45df--------------------------------)[![Ahmed
    Besbes](../Images/93804d9291439715e578f204b79c9bdd.png)](https://ahmedbesbes.medium.com/?source=post_page-----7f25af3e45df--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7f25af3e45df--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7f25af3e45df--------------------------------)
    [Ahmed Besbes](https://ahmedbesbes.medium.com/?source=post_page-----7f25af3e45df--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7f25af3e45df--------------------------------)
    ¬∑11 min read¬∑May 2, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/62bdc77c059993e827235577879e894c.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Jason Leung](https://unsplash.com/@ninjason?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we will guide you through the process of building a voice-based
    ChatGPT clone that relies on the OpenAI API and uses Wikipedia as an additional
    data source.
  prefs: []
  type: TYPE_NORMAL
- en: 'To build and deploy this app, we‚Äôll be using [BentoML](https://github.com/bentoml/BentoML):
    a Python framework for model serving and deployment.'
  prefs: []
  type: TYPE_NORMAL
- en: BentoML not only helps you build services that connect to third-party proprietary
    APIs. It also supercharges those services by combining them with other open-source
    models, resulting in complex and powerful inference graphs.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, the app we‚Äôll be building will have **speech-to-text** and **text-to-speech**
    tasks that will be handled by separate models from the HuggingFace [hub](https://huggingface.co/)
    and an LLM task that will be managed by [**LangChain**](https://langchain.readthedocs.io/).
  prefs: []
  type: TYPE_NORMAL
- en: After testing the project locally, we will push it to BentoCloud, a platform
    that smoothes the process of versioning, tracking, and deploying ML services to
    the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: '***By the end of this article, you should have comprehensive knowledge of building
    and deploying multi-model services using BentoML. You‚Äôll also learn about some
    of its specific features that make industrializing models easier.***'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Without further ado, let‚Äôs have a look üîç.
  prefs: []
  type: TYPE_NORMAL
- en: Demo
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here‚Äôs a one-minute demo of the app.
  prefs: []
  type: TYPE_NORMAL
- en: Video by the author ‚Äî A quick demo
  prefs: []
  type: TYPE_NORMAL
- en: Why BentoML? üç±
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the ever-increasing number of open-source ML models that solve a huge variety
    of tasks, software applications will gradually become some sort of AI application
    that integrates pre-trained models, self-trained models, or models accessed through
    APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Given that many SOTA models are large and require powerful hardware and distributed
    deployment, fitting everything in one machine will not be a practical solution,
    especially if the application combines at least 2 or 3 models.
  prefs: []
  type: TYPE_NORMAL
- en: '***‚Üí BentoML is a framework that helps solve this problem by letting user write
    simple Python code yet deploy models as distributed microservices.***'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'I‚Äôve been playing and experimenting with BentoML for a while now and it‚Äôs definitely
    my go-to solution to deploy machine learning models and services. With its own
    distribution format known as a *bento,* this library makes it easy to package
    everything ML-related into one place: source code and dependencies, API definitions,
    model weights, Docker image, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying is even easier since it relies on pushing that said bento to the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, we‚Äôll first prototype the app, build the bento locally and
    push it to BentoCloud for deployment.
  prefs: []
  type: TYPE_NORMAL
- en: You can perform this last step on your own by self-managing a deployment platform
    (check the [Yatai](https://github.com/bentoml/Yatai) project for more details)
    or using a deployment utility called [bentoctl](https://github.com/bentoml/bentoctl)
    that deploys your bento to a variety of cloud services.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to learn more about BentoML and the different deployment strategies,
    you can have a look at my previous posts *‚è¨*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](/10-ways-bentoml-can-help-you-serve-and-scale-machine-learning-models-4060f1e59d0d?source=post_page-----7f25af3e45df--------------------------------)
    [## 10 Ways BentoML Can Help You Serve and Scale Machine Learning Models'
  prefs: []
  type: TYPE_NORMAL
- en: Moving from Jupyter notebooks to production is not that difficult after all
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/10-ways-bentoml-can-help-you-serve-and-scale-machine-learning-models-4060f1e59d0d?source=post_page-----7f25af3e45df--------------------------------)
    [](https://levelup.gitconnected.com/quickly-deploy-a-machine-learning-api-on-aws-ec2-using-bentoml-dbb13bc09d51?source=post_page-----7f25af3e45df--------------------------------)
    [## Quickly Deploy A Machine Learning API On AWS EC2 Using BentoML
  prefs: []
  type: TYPE_NORMAL
- en: 'Use case: an end-to-end service to summarize Youtube videos üé•'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/quickly-deploy-a-machine-learning-api-on-aws-ec2-using-bentoml-dbb13bc09d51?source=post_page-----7f25af3e45df--------------------------------)
    [](/how-to-deploy-pytorch-models-as-production-ready-apis-f61136fd0244?source=post_page-----7f25af3e45df--------------------------------)
    [## How To Deploy PyTorch Models as Production-Ready APIs?
  prefs: []
  type: TYPE_NORMAL
- en: An end-to-end use-case combining PyTorch Lightning and BentoML üöÄ
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/how-to-deploy-pytorch-models-as-production-ready-apis-f61136fd0244?source=post_page-----7f25af3e45df--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Code üíª
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code of this project is available on [Github](https://github.com/ahmedbesbes/BentoChain).
    You can clone it and run the app locally or build a self-contained bento for later
    deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We‚Äôll use [transformers](https://github.com/huggingface/transformers) with a
    couple of other libraries that process audio data and the popular [LangChain](https://langchain.readthedocs.io/)
    package to easily integrate with Large Language Models (LLMs).
  prefs: []
  type: TYPE_NORMAL
- en: We‚Äôll use [poetry](https://python-poetry.org/) to manage the project‚Äôs dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: After installing the packages, you‚Äôll need to generate an SSL key and certificate.
    This will establish an HTTPS connexion that will be needed on modern browsers
    to allow the use of the microphone.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Download the models and save them
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This project assumes no training. We will simply download the models‚Äô weights
    from HuggingFace‚Äôs hub and save them as BentoML Models.
  prefs: []
  type: TYPE_NORMAL
- en: Saving the models as BentoML artifacts helps incorporate them in the bento archive
    so that they don‚Äôt constitute an external dependency.
  prefs: []
  type: TYPE_NORMAL
- en: After figuring out what models are needed precisely, you can first download
    them locally by simply initializing them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you can save them as BentoML models by calling the `bentoml.transformers.save_model`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The full code is available in the`train.py` script and should be run once:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/4eeac49acaaf50c39b99fa9fbba8bb55.png)'
  prefs: []
  type: TYPE_IMG
- en: Saving models ‚úÖ ‚Äî Screenshot by the author
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the app‚Äôs architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before going into more detail, let‚Äôs first clarify the data workflow to understand
    how the app works:'
  prefs: []
  type: TYPE_NORMAL
- en: The user sends an audio message to the API server over an HTTP POST request
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The API server redirects the audio message to the speech2text runner that transcribes
    it into text and sends it back
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The API server takes the transcribed text message as input, passes it through
    a LangChain agent, generates a response, and sends it to the text2speech runner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The text2speech runner generates an audio clip from the input text and returns
    it to the API server which in turn sends it back to the user
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following diagram summarizes these steps.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7ad715efc3619895dadcb6660ec5dd9e.png)'
  prefs: []
  type: TYPE_IMG
- en: The architecture of the app ‚úÖ ‚Äî Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Why are we using 2 runners?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What‚Äôs interesting about BentoML is that when deploying to the BentoCloud (or
    to any self-managed platform) the runners and the API server can be deployed separately
    on three different Kubernetes pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'This provides 3 main benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Separation of concerns**: runners are focused on the compute and are decoupled
    from web serving'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Customization**: Each runner can have a specific hardware configuration depending
    on the task it‚Äôs performing: for example, the text2speech runner‚Äôs config will
    have a GPU while the speech2text runner won‚Äôt require one'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Auto-scale**: Runners will also auto-scale independently based on resource
    usage'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*To learn more about BentoML runners, have a look at this* [*page*](https://docs.bentoml.org/en/latest/concepts/runner.html)*.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Now that we have a global picture of the app, let‚Äôs focus on each runner:'
  prefs: []
  type: TYPE_NORMAL
- en: A Speech-to-text runner üé§ ‚Üí üìù
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This runner will rely on OpenAI‚Äôs Whisper model to transcribe audio to text.
    Specifically, it‚Äôll use the **tiny** [model](https://huggingface.co/openai/whisper-tiny).
  prefs: []
  type: TYPE_NORMAL
- en: This model will receive a tensor of input features and will generate a transcription.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code is pretty straightforward: it just defines a `**SpeechToTextRunnable**`
    class that inherits from `**bentoml.Runnable**`, instantiates the model and the
    processor, and defines the inference method.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: A Text-to-speech runner üìù ‚Üí üé§
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This runner performs the exact opposite task: it takes a text as input and
    generates a speech that will be represented by a NumPy array.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that a device must be declared as an attribute of the `**Text2SpeechRunnable**`
    class to support the GPU acceleration when it‚Äôs available.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: A BentoML service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will create a service that defines the API routes that can
    be accessed when the bento is deployed.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first start by initializing the two previous runners we defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we create a Service object that depends on them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the service is created, we will define two API routes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**generate_text:** this route will take an array as input and generate a text
    by calling the speech2text_runner'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '**generate_speech:** this route will take a text as input and generate an array
    as output by calling the text2speech_runner'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The ChatWrapper utility class
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are not done yet with the service source code.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will mount a FastAPI app as an HTTP endpoint on the ‚Äú/chatbot‚Äù
    path.
  prefs: []
  type: TYPE_NORMAL
- en: 'This app will serve a Gradio chatbot interface that will interact with the
    two previously defined API routes: `**generate_text**` and `**generate_speech**`
    .'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The ‚Äúchat‚Äù variable is an object that gets the user's audio input, transcribes
    it into text, passes it to LangChain, extracts the response, and returns a bunch
    of data that update the app‚Äôs interface and state.
  prefs: []
  type: TYPE_NORMAL
- en: 'The chat object is a callable that expects the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**api_key**: OpenAI API key'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**audio_path**: temporary file location when an audio file is recorded with
    the microphone'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**text_message**: a text message sent instead of an audio file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**history**: a tuple of questions and the corresponding responses ((‚ÄúHello‚Äù,
    ‚Äúhi‚Äù), (‚ÄúHow are you?‚Äù, ‚ÄúFine, thank you. What about you?‚Äù))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**chain**: a ConversationChain object from LangChain'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following code snippet, the ChatWrapper __call__‚Äôs method first checks
    the input data. If it‚Äôs in audio format, it transcribes it using the generate_text
    method, otherwise, it keeps it as is.
  prefs: []
  type: TYPE_NORMAL
- en: Then, it checks whether the OpenAI key is correctly loaded. If it‚Äôs not the
    case, it prints out the message ‚ÄúPlease paste your Open AI key.‚Äù, along with the
    audio transcription.
  prefs: []
  type: TYPE_NORMAL
- en: If the key is correctly loaded, the LangChain agent runs and outputs a message
    that is then passed to the generate_speech method to produce the output audio.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The Gradio UI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Remember the create_block function we saw earlier? This one takes a ChatWrapper
    instance as input and produces the UI.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/4077681d5e4f02c67fa9bc38c7795045.png)'
  prefs: []
  type: TYPE_IMG
- en: The App‚Äôs UI ‚Äî Screenshot by the user
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs break the UI into pieces to understand how the data flows exactly.
  prefs: []
  type: TYPE_NORMAL
- en: '**openai_api_key_textbox:** This textbox expects you to paste your OpenAI key
    in it.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: When the user pastes its key and submits it, the key is passed to the set_openai_api_key
    function that gets executed. This function then returns the loaded chain that
    is passed into the app‚Äôs state. That way, the chain object is not None and can
    be used when passed to the chat object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the other UI components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**chatbot:** It displays a chatbot output showing both user-submitted messages
    and responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**audio:** A widget thatplays the user‚Äôs recorded audio clip'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**state**: a global state of the app'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**audio_message:** user‚Äôs submitted audio'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**text_message**: user‚Äôs submitted text'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, what happens when a user records an audio from the microphone? (same happens
    when he sends a text)
  prefs: []
  type: TYPE_NORMAL
- en: The chat object gets executed with a list of inputs from the UI [openai_api_key_textbox,
    audio_message, text_message, state, agent_state] and outputs a list of outputs
    that updates the following components [chatbot, state, audio, audio_message, text_message]
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: To put it simply, this allows displaying the user‚Äôs questions and the bot‚Äôs
    answers as well as the history of the chat and the audio of the last response.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7493404c7f1969da8149143173c515f0.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot by the author
  prefs: []
  type: TYPE_NORMAL
- en: Serve the app locally
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To serve the app locally, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This starts a SwaggerUI from which you can try the two endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: This also serves the Gradio app on the ‚Äú/chatbot‚Äù path.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c663a378e452b841d8d68e814613df44.png)![](../Images/f4395a4f7ca487b97d66faca46c2a7af.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot by the user
  prefs: []
  type: TYPE_NORMAL
- en: Deploy to BentoCloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before deploying to BentoCloud, we first need to build the bento:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ae1bb1ac88c9ad0b9e40dcff04f37327.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot by the author
  prefs: []
  type: TYPE_NORMAL
- en: Then, we need to push it using the following command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This will upload our **bento** and the underlying **models** simultaneously
    to the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/39ec450b96bb2398140d984d65c0f803.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot by the author
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the easy part: you log in to BentoCloud, head over to the deployment tab,
    and hit create.'
  prefs: []
  type: TYPE_NORMAL
- en: You pick a deployment name, enable public access and select the appropriate
    bento tag version.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b40f1354a8791d04fa0f81883153a831.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, you define the API server configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/588fa38c5b62c70b9858533b17d4d278.png)'
  prefs: []
  type: TYPE_IMG
- en: API Server configuration ‚Äî Screenshot by the author
  prefs: []
  type: TYPE_NORMAL
- en: And you set a configuration for each runner.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2fdb1f88115a47e5a03e52774fd4ccf4.png)![](../Images/be11f4ea4b6b1dc8fbe9e876b1b4a58f.png)'
  prefs: []
  type: TYPE_IMG
- en: Runners configuration ‚Äî Screenshots by the author
  prefs: []
  type: TYPE_NORMAL
- en: When everything is set, hit the submit button and wait for deployment.
  prefs: []
  type: TYPE_NORMAL
- en: When the bento is marked as running, you‚Äôll see a public URL that serves the
    chatbot (don‚Äôt forget to add the HTTPS)
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This project was an opportunity to take part in the ongoing hype around LLMs
    and build an app from scratch that not only calls LangChain but combines it with
    other models as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'There‚Äôs of course room for improvement in the different steps of the workflow
    I‚Äôve shown: I just hope this post was a good starter for you to start building
    more advanced bots.'
  prefs: []
  type: TYPE_NORMAL
- en: New to Medium? You can subscribe for $5 per month and unlock unlimited articles
    on various topics (tech, design, entrepreneurship‚Ä¶) You can support me by clicking
    on my referral [link](https://ahmedbesbes.medium.com/membership)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[](https://ahmedbesbes.medium.com/membership?source=post_page-----7f25af3e45df--------------------------------)
    [## Join Medium with my referral link - Ahmed Besbes'
  prefs: []
  type: TYPE_NORMAL
- en: Read every story from Ahmed Besbes (and thousands of other writers on Medium).
    Your membership fee directly supports‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ahmedbesbes.medium.com](https://ahmedbesbes.medium.com/membership?source=post_page-----7f25af3e45df--------------------------------)
  prefs: []
  type: TYPE_NORMAL
