- en: Why There Kind of Is Free Lunch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么似乎存在“免费午餐”
- en: 原文：[https://towardsdatascience.com/why-there-kind-of-is-free-lunch-56f3d3c4279f](https://towardsdatascience.com/why-there-kind-of-is-free-lunch-56f3d3c4279f)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/why-there-kind-of-is-free-lunch-56f3d3c4279f](https://towardsdatascience.com/why-there-kind-of-is-free-lunch-56f3d3c4279f)
- en: On The Universality of Patterns in Neuroscience and Artificial Intelligence
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经科学与人工智能中模式的普遍性
- en: '[](https://manuel-brenner.medium.com/?source=post_page-----56f3d3c4279f--------------------------------)[![Manuel
    Brenner](../Images/f62843c79a9b378494cb83caf3ddc792.png)](https://manuel-brenner.medium.com/?source=post_page-----56f3d3c4279f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----56f3d3c4279f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----56f3d3c4279f--------------------------------)
    [Manuel Brenner](https://manuel-brenner.medium.com/?source=post_page-----56f3d3c4279f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://manuel-brenner.medium.com/?source=post_page-----56f3d3c4279f--------------------------------)[![Manuel
    Brenner](../Images/f62843c79a9b378494cb83caf3ddc792.png)](https://manuel-brenner.medium.com/?source=post_page-----56f3d3c4279f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----56f3d3c4279f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----56f3d3c4279f--------------------------------)
    [Manuel Brenner](https://manuel-brenner.medium.com/?source=post_page-----56f3d3c4279f--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----56f3d3c4279f--------------------------------)
    ·11 min read·Jun 27, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----56f3d3c4279f--------------------------------)
    ·11分钟阅读·2023年6月27日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '**‘There ain’t no such thing as a free lunch.’'
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**‘没有所谓的免费午餐。’'
- en: '- Robert A. Henlein**'
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- **罗伯特·A·亨莱因**'
- en: The “No Free Lunch” theorem in the realm of machine learning reminds me of Gödel’s
    incompleteness theorem within the world of mathematics.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习领域中的“No Free Lunch”定理让我想起了数学世界中的哥德尔不完备性定理。
- en: While these theorems are frequently cited, they are seldom explained in depth,
    and the implications for real-world applications often remain unclear. Just as
    Gödel’s theorem became a thorn in the early 20th-century mathematicians’ belief
    in a complete and self-consistent formal system, the “No Free Lunch” theorems
    challenge our faith in the efficacy of general machine learning algorithms. Yet,
    these theorems’ impact on everyday practical applications can often be small,
    and most practitioners proceed unencumbered by these theoretical constraints.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些定理经常被引用，但很少有深入的解释，而且对实际应用的影响往往不明确。正如哥德尔定理成为20世纪初数学家对完整自洽形式系统信仰的障碍一样，“No
    Free Lunch”定理挑战了我们对通用机器学习算法有效性的信任。然而，这些定理对日常实际应用的影响通常较小，大多数从业者在这些理论限制下仍能顺利进行工作。
- en: '![](../Images/9b23f215331e0286b91dc5fbb0799a27.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b23f215331e0286b91dc5fbb0799a27.png)'
- en: A machine learner realizing there might be free lunch after all, as envisioned
    by DALL-E.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 一位机器学习者意识到或许真的存在免费午餐，如DALL-E所设想的那样。
- en: In this article, I want to explore what the “No Free Lunch” theorem states and
    delve into its associations with vision, transfer learning, neuroscience, and
    artificial general intelligence.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我想探讨“No Free Lunch”定理的内容，并深入研究它与视觉、迁移学习、神经科学和人工通用智能的关系。
- en: The **“No Free Lunch”** theorem(s), [proposed by Wolpert and Macready in 1997](https://ieeexplore.ieee.org/document/585893)
    and often used in the context of machine learning, **state that no one algorithm
    is universally the best for all possible problems**. There’s no magical, one-size-fits-all
    solution. An algorithm might work exceptionally well for one task but could perform
    poorly on another.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**“No Free Lunch”** 定理，[由 Wolpert 和 Macready 于 1997 年提出](https://ieeexplore.ieee.org/document/585893)，并常用于机器学习的背景中，**指出没有一种算法可以普遍适用于所有可能的问题**。没有一种神奇的“一刀切”解决方案。一个算法可能在某一任务上表现非常好，但在另一个任务上可能表现不佳。'
- en: A fundamental objective of machine learning is to discern meaningful patterns
    within disparate data. An algorithm’s effectiveness, however, often depends on
    the specific nature of the data at hand. It may prove really useful for one type
    of data, but less effective when applied to another.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的一个基本目标是辨识不同数据中的有意义的模式。然而，算法的有效性通常取决于所处理数据的具体性质。它可能对一种数据类型非常有用，但在应用于另一种数据时效果较差。
- en: This becomes apparent when considering the different types of data we might
    encounter. Something as simple as a coin flip generates a simple probabilistic
    distribution of two patterns.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当考虑我们可能遇到的不同类型数据时，这一点变得显而易见。像抛硬币这样简单的事情生成了两种模式的简单概率分布。
- en: 'On the other hand, practically more significant examples, such as image data
    or text data, are much more complex. The universe of potential patterns grows
    exponentially with the number of pixels, making the array of possible configurations
    in a 500x500 image already unimaginably large:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，实际中更重要的例子，如图像数据或文本数据，复杂得多。潜在模式的宇宙随着像素数量的增加而呈指数增长，使得一个500x500图像中的所有可能配置的数组已经巨大到难以想象：
- en: '![](../Images/e6c2462b503031fc51a4cd4ebd82b3ea.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e6c2462b503031fc51a4cd4ebd82b3ea.png)'
- en: Given the size of this space, how can we still learn anything meaningful from
    data?
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这个空间的大小，我们如何仍然能从数据中学习到有意义的东西？
- en: An important part of the answer is that most samples of interest are not sampled
    uniformly from the space of all possible samples, but come with a large amount
    of pre-existing structure when compared to random sampling.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 答案的一个重要部分是，大多数感兴趣的样本不是从所有可能样本空间中均匀采样的，而是与随机采样相比，带有大量的预先存在的结构。
- en: Language data serves as another classic example. The sum of all words in any
    given language consists of a subset of patterns that lives in a much lower-dimensional
    space when contrasted against the set of all possible combinations of letters,
    something that was used in the first simple statistical models of language (I
    explored this in more depth in my article on [Markov chains](/understanding-markov-chains-cbc186d30649)).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 语言数据是另一个经典例子。任何给定语言中所有单词的总和包含了一组模式的子集，与所有可能的字母组合集合相比，这些模式处于一个维度更低的空间，这一点在语言的第一个简单统计模型中被使用了（我在我的[马尔可夫链](/understanding-markov-chains-cbc186d30649)文章中对此进行了更深入的探讨）。
- en: For machine learners, it’s great news that there are some recurring patterns
    in many types of input data. It means that we can train models that learn to extract
    these patterns, and possibly even re-use the models to extract similar patterns
    in different applications.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器学习者来说，许多类型的输入数据中存在一些重复出现的模式无疑是一个好消息。这意味着我们可以训练模型来学习提取这些模式，并可能将这些模型重新用于不同应用中的类似模式提取。
- en: The technique of **transfer learning** makes explicit use of the fact that most
    real-world problems share some common structures.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**迁移学习**技术明确利用了大多数现实世界问题共享某些共同结构这一事实。'
- en: Transfer learning uses pre-trained models, which have already learned patterns
    from a large dataset, to adapt and **“fine-tune”** to a different but related
    task. A model trained to recognize objects in images could have learned low-level
    features like edges and color gradients and high-level features like shapes. These
    learned features could then be applied to a related task, such as recognizing
    handwritten digits or classifying tumors in medical images.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习使用已经从大型数据集中学习到模式的预训练模型，来适应并**“微调”**到不同但相关的任务。一个训练用于识别图像中的物体的模型可能已经学会了诸如边缘和颜色渐变等低级特征，以及形状等高级特征。这些学到的特征可以应用于相关任务，例如识别手写数字或在医学图像中分类肿瘤。
- en: Hubel and Wiesel figured this out when they discovered that the visual cortex
    is composed of hierarchical layers which deal with increasingly complex patterns,
    winning them the Nobel Prize in 1981\. Since the advent of Convolutional Neural
    Networks (CNNs), their work has been widely discussed in relation to machine learning
    algorithms.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Hubel 和 Wiesel 通过发现视觉皮层由处理越来越复杂模式的层次组成，从而获得了1981年的诺贝尔奖。自卷积神经网络（CNNs）出现以来，他们的工作在机器学习算法中得到了广泛的讨论。
- en: In the primary visual cortex, for instance, simple cells detect edges and gradients
    of color, while complex cells aggregate the outputs of these simple cells to recognize
    broader patterns, like motion or specific shapes. Heading deeper into the visual
    system, the patterns recognized by neurons become increasingly complex, moving
    from simple geometric forms to faces and intricate objects, ending at the (in)famous
    grandmother neuron that only fires when you see your grandma (or when you taste
    the madeleines she gave you as a kid).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在初级视觉皮层中，简单细胞检测边缘和颜色梯度，而复杂细胞则将这些简单细胞的输出聚合以识别更广泛的模式，如运动或特定形状。深入到视觉系统中，神经元识别的模式变得越来越复杂，从简单的几何形状到面孔和复杂的物体，最终到达那种（不）著名的“奶奶神经元”，它只在你看到奶奶（或你品尝她小时候给你的玛德琳蛋糕时）才会激活。
- en: '![](../Images/bf3ce844737e09885e286d48bba40d73.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bf3ce844737e09885e286d48bba40d73.png)'
- en: Features learned by a convolutional neural network (Inception V1) trained on
    the ImageNet data. Features increase in complexity from left to right. Figure
    from Olah, et al. (2017, CC-BY 4.0) [https://distill.pub/2017/feature-visualization/appendix/](https://distill.pub/2017/feature-visualization/appendix/).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在 ImageNet 数据上训练的卷积神经网络（Inception V1）所学到的特征。从左到右，特征的复杂性逐渐增加。图来自 Olah 等人（2017，CC-BY
    4.0）[https://distill.pub/2017/feature-visualization/appendix/](https://distill.pub/2017/feature-visualization/appendix/)。
- en: 'In the same way, the layers of a CNN extract features of increasing complexity.
    In the initial layers, the CNN might learn to detect simple structures, like lines,
    angles, and blobs of color. As we progress through the layers, these simple features
    are combined to form more complex representations: circles, rectangles, and eventually,
    discernible objects that start to look like cats, dogs, or elephants.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，CNN 的各层提取了逐渐复杂的特征。在初始层中，CNN 可能会学习检测简单的结构，如线条、角度和颜色块。随着层次的推进，这些简单特征被组合成更复杂的表示：圆形、矩形，最终是开始看起来像猫、狗或大象的可辨识物体。
- en: 'Perhaps these similarities shouldn’t come as a surprise: both systems have
    evolved and have been designed, respectively, to **exploit the structure inherent
    in visual data**. This understanding of what kind of patterns most frequently
    occur in all kinds of visual scenes can be repurposed or transferred to handle
    different but related tasks, just as discussed earlier.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 也许这些相似之处不应该令人感到惊讶：两个系统都进化并被设计出来，分别用于**利用视觉数据中固有的结构**。对所有类型的视觉场景中最常出现的模式的理解可以被重新利用或转移，以处理不同但相关的任务，就像之前讨论的那样。
- en: This is where the concept of transfer learning and the brain’s ability to adapt
    and learn come into play. A child who learns to recognize a bicycle does not start
    from scratch when learning to recognize a motorcycle. They transfer their understanding
    of the basic structure of the bike from one context to another. Similarly, a CNN
    trained on a variety of images does not need to re-learn the concept of an “edge”
    when switching from recognizing faces to recognizing handwritten digits.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是迁移学习的概念和大脑适应及学习能力发挥作用的地方。一个学会识别自行车的孩子，在学习识别摩托车时不会从零开始。他们将自行车的基本结构理解转移到另一个情境中。同样，一个在各种图像上训练的
    CNN 在从识别面孔切换到识别手写数字时，不需要重新学习“边缘”的概念。
- en: 'Our own brains have stumbled onto the same truth that underpins the **‘kind
    of free lunch’** I’ve been discussing in the title: the world that is useful to
    us is very far from uniformly random, but rather full of recurring patterns and
    structures. Given these patterns are what we are interested in, our brains are
    hard-wired to selectively perceive them.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的大脑发现了支撑我在标题中讨论的**‘免费午餐’**的相同真理：对我们有用的世界远非完全随机，而是充满了反复出现的模式和结构。鉴于这些模式是我们感兴趣的内容，我们的大脑被硬编码以选择性地感知它们。
- en: We can even go a little further than simply saying that we are lucky that there
    are meaningful patterns in the world. Modern cognitive neuroscience theories see
    the brain more as a **prediction machine** than as a ‘reality’-perceiving tool.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们甚至可以进一步说，我们的运气不仅仅是因为世界上存在有意义的模式。现代认知神经科学理论认为，大脑更像是一个**预测机器**，而不是一个‘现实’感知工具。
- en: '[The Bayesian Brain Hypothesis](/the-bayesian-brain-hypothesis-35b98847d331),
    for instance, suggests that our brains are constantly making probabilistic predictions
    about the world, updating these predictions based on sensory inputs. Our perception
    of the world, therefore, is not a passive process but an active one. We don’t
    simply perceive the world as it is but interpret it actively based on our priors,
    some of them based on our remembered experiences, but some of them going back
    to structural priors induced by the past experience of billions of years of evolution.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[贝叶斯脑假说](/the-bayesian-brain-hypothesis-35b98847d331)例如，建议我们的脑袋不断地对世界进行概率预测，并根据感官输入更新这些预测。因此，我们对世界的感知不是一个被动的过程，而是一个主动的过程。我们不仅仅是感知世界的真实情况，而是根据我们的先验知识积极地解释它，其中一些先验知识基于我们的记忆经验，但一些则回溯到由数十亿年的进化经历所产生的结构性先验。'
- en: The active, prior-driven component of cognition has such a significant influence
    on our perception that scientists like Anil Seth have coined our brain’s activity
    to be something akin to a ‘**controlled hallucination**’.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 认知的主动、先验驱动成分对我们的感知有着如此显著的影响，以至于像阿尼尔·塞思这样的科学家将我们大脑的活动称为类似于‘**受控幻觉**’的东西。
- en: This also connects to [Donald Hoffman’s provocative idea](https://www.youtube.com/watch?v=4HFFr0-ybg0)
    (the case against reality) that states that we do not perceive reality as it is,
    but that perception has developed over millions of years as a ‘user interface’,
    and whose primary goal is to aid our survival. The icons on your computer’s desktop,
    for instance, don’t reveal the reality of what’s happening inside the machine,
    but provide an interface that makes it easy for you to use your computer.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这也与[唐纳德·霍夫曼的挑衅性观点](https://www.youtube.com/watch?v=4HFFr0-ybg0)（反对现实的观点）相关，该观点认为我们并没有按原样感知现实，而是感知在数百万年的发展过程中成为一种‘用户界面’，其主要目标是帮助我们的生存。例如，计算机桌面上的图标并不揭示机器内部发生的真实情况，而是提供一个使你可以轻松使用计算机的界面。
- en: What we perceive as a red apple is not the apple itself but a representation
    that guides us to nutritious food. Our perceptual systems, then, impose a strong
    prior on our sensory inputs, filtering out a large portion of data that does not
    fit our pre-existing models or that does not contribute directly to our survival.
    This bias towards salient patterns and away from perceived noise (the set of all
    possible visual patterns as spanned by the noisy picture I showed above) echoes
    the structure we’ve been discussing that underlies transfer learning and the **‘kind
    of free lunch’** in machine learning.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感知的红色苹果并非苹果本身，而是引导我们获得营养食物的表现形式。我们的感知系统因此对我们的感官输入施加了强烈的先验，过滤掉大量不符合我们现有模型的数据或不直接有助于我们生存的数据。这种对显著模式的偏好以及远离感知噪声（如我上面展示的噪声图片中所有可能的视觉模式）反映了我们讨论过的结构，这种结构是迁移学习和**‘某种免费午餐’**在机器学习中的基础。
- en: Our brains are like pre-trained models that have learned to focus only on certain
    patterns in the world, helping us deal with the constant rampage of new sensory
    inputs.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的大脑就像是预训练模型，已经学会只关注世界上某些模式，帮助我们应对不断涌入的新感官输入。
- en: 'The older we get, the less likely we are to invert our model and fine-tune
    it when unseen patterns arise, and the more we walk around with an inference machine
    in our head that is the epitome of the (self-perceived) free lunch: to go with
    the meme, the ‘old white man’ is someone who is explaining everything he encounters
    through the lens of his already firmly established world model that (in his mind)
    is the universal algorithm that gets him free lunch out of every learning problem
    he encounters.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 随着年龄的增长，我们更不容易反转我们的模型并在看不见的模式出现时进行微调，更多的是将脑海中的推理机器视为（自我感知的）免费午餐的极致表现：按照这种说法，‘老白人’是一个通过已经牢固建立的世界模型的视角来解释他所遇到的一切的人，这个模型（在他看来）是从每一个学习问题中获得免费午餐的普遍算法。
- en: '![](../Images/b46c8aa1351e91eb139bada82e24fed0.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b46c8aa1351e91eb139bada82e24fed0.png)'
- en: Old white politician that smiles arrogantly because he thinks he knows everything,
    as generated by DALL-E. I swear it came up with the British flag all by itself.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的老白人政治家，傲慢地微笑，因为他认为自己什么都知道。 我发誓它自己生成了英国国旗。
- en: Jokes aside, while transfer learning within modalities is crucial, our discussion
    neatly extends to current developments in the multimodal setting, both in neuroscience
    and machine learning.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 说笑归说笑，尽管在模态内迁移学习至关重要，我们的讨论也很自然地扩展到神经科学和机器学习中的多模态设置的当前发展。
- en: The brain has an astonishing ability to reorganize itself known as **neuroplasticity**,
    an ability frequently observed after strokes or other dramatic cases of sensory
    loss.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 大脑具有惊人的重组能力，称为**神经可塑性**，这种能力在中风或其他剧烈的感官丧失情况后经常被观察到。
- en: Sensory loss often leads to a repurposing of certain brain areas. Blind individuals,
    for instance, often develop a heightened sense of hearing and touch. This is not
    simply a matter of increased attention or practice. Research has shown that the
    visual cortex, which normally processes visual information, becomes active during
    auditory and tactile tasks in blind individuals (the book Livewired by David Eagleman
    gives a fantastic account of the central role neuroplasticity plays in the brain).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 感觉丧失常常导致某些大脑区域的重新利用。例如，盲人常常发展出更敏锐的听觉和触觉。这不仅仅是注意力或练习的结果。研究显示，通常处理视觉信息的视觉皮层在盲人的听觉和触觉任务中也会变得活跃（David
    Eagleman 的《Livewired》一书对神经可塑性在大脑中扮演的核心角色进行了极好的阐述）。
- en: It’s as if the brain has taken a neural network pre-trained on visual tasks
    and, finding itself no longer receiving visual input, fine-tuned it to process
    auditory and tactile information.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 就好像大脑已经接受了一个在视觉任务上预训练的神经网络，并在发现自己不再接收视觉输入后，将其微调以处理听觉和触觉信息一样。
- en: Similarly, deaf individuals have been found to use areas of the brain typically
    associated with processing spoken language, such as Broca’s and Wernicke’s areas,
    while using sign language. These areas are dedicated to communication more broadly,
    regardless of the specific modality of that communication. The brain’s language
    network is flexible enough to handle different forms of communication — be it
    spoken, written, or signed.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，研究发现聋人使用的脑区通常与处理语言有关，例如 Broca 区和 Wernicke 区，这些区域在使用手语时也会被激活。这些区域专注于更广泛的沟通，而不论该沟通的具体模态是什么。大脑的语言网络足够灵活，能够处理不同形式的沟通——无论是口语、书面语还是手语。
- en: These instances of neuroplasticity show us that our brain's computations are
    underpinned by a form of a universal learning algorithm, capable of processing
    a wide range of patterns, not just those typically associated with a specific
    sensory modality. What this algorithm precisely is based on is up for debate (see
    e.g. Jeff Hawkin’s thousand brains or Kurzweil’s pattern recognizers for popular
    neuroscientific explanations, and the book The Master Algorithm by Pedro Domingues
    for a computer science perspective), but neuroplasticity clearly indicates that
    something like it exists.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这些神经可塑性的实例表明，我们大脑的计算是基于一种普遍学习算法，这种算法能够处理各种模式，而不仅仅是那些通常与特定感觉模态相关的模式。这个算法究竟是什么仍有争议（例如，Jeff
    Hawkins 的《千脑理论》或 Kurzweil 的模式识别器提供了流行的神经科学解释，《The Master Algorithm》一书由 Pedro Domingues
    提供了计算机科学的视角），但神经可塑性清楚地表明类似的东西确实存在。
- en: The brain’s versatility is also mirrored by our most advanced machine-learning
    models. While Transformers were initially developed for natural language processing,
    mirroring language’s inductive bias, they have since been applied to a wide array
    of data types, from images and audio to time series data (whether this is primarily
    a result of the current hype around them can be argued over). Just as the visual
    cortex in blind people can repurpose itself for auditory or tactile tasks, Transformers
    can be adapted for a variety of data types, suggesting the existence of universal
    patterns that these models can capture, irrespective of the specific input modality
    (in all fairness, this might also be in part because Transformers are well suited
    to be optimized on our current computational architectures, since they can be
    heavily parallelized).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 大脑的多功能性也体现在我们最先进的机器学习模型上。虽然变压器最初是为自然语言处理而开发的，反映了语言的归纳偏差，但它们随后被应用于各种数据类型，从图像和音频到时间序列数据（这是否主要是因为当前对它们的炒作还有待讨论）。正如盲人的视觉皮层可以重新用于听觉或触觉任务一样，变压器也可以适应多种数据类型，这暗示了这些模型可以捕捉的普遍模式的存在，不论具体的输入模态（公平地说，这也可能部分因为变压器非常适合在当前计算架构上进行优化，因为它们可以被高度并行化）。
- en: '![](../Images/7f825afb981402d2a6e8ef04a32df3c4.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7f825afb981402d2a6e8ef04a32df3c4.png)'
- en: The basic transformer architecture. Yuening Jia, CC BY-SA 3.0.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 基本变压器架构。Yuening Jia，CC BY-SA 3.0。
- en: 'Quoting the original NFL paper by Wolpert and Macready: ‘any two [optimization](https://en.wikipedia.org/wiki/Optimization_(mathematics))
    algorithms are equal when their performance is averaged across all possible problems’.
    But paraphrasing Orwell, when we look at most of the problems that really concern
    us, some optimization algorithms are more equal than others.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 引用 Wolpert 和 Macready 的原始 NFL 论文：“任何两个[优化](https://en.wikipedia.org/wiki/Optimization_(mathematics))算法在其性能在所有可能的问题中平均时都是相等的。”但套用
    Orwell 的话，当我们查看真正关心的大多数问题时，一些优化算法比其他算法更为平等。
- en: For instance, the application of Transformers to image data has resulted in
    models such as Vision Transformers, which treat an image as a sequence of patches
    and apply the same self-attention mechanism.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，将 Transformers 应用到图像数据中，产生了像 Vision Transformers 这样的模型，这些模型将图像视为一系列的补丁，并应用相同的自注意力机制。
- en: This idea is further exemplified in multi-modal Transformers, which can process
    and understand multiple types of data simultaneously. Text-and image embeddings
    such as the CLIP model, are trained to understand images and their related text
    descriptions concurrently. CLIP learns the relationship by incorporating both
    modalities into a joint embedding space.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这个理念在多模态 Transformers 中得到了进一步的体现，这些模型可以同时处理和理解多种类型的数据。像 CLIP 模型这样的文本和图像嵌入被训练来同时理解图像及其相关的文本描述。CLIP
    通过将这两种模态结合到一个联合嵌入空间中来学习它们之间的关系。
- en: 'Such models capitalize on the common structures between different data types
    and effectively create a ‘translation’ system between them. A striking example
    of this capability of CLIP is familiar to most of us by now: by providing the
    foundation for diffusion-based models such as DALL-E, they allow us to generate
    impressive pictures from textual descriptions alone.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型利用不同数据类型之间的共同结构，有效地创建了它们之间的“翻译”系统。CLIP 的这一能力的一个显著例子现在大多数人都熟悉：通过为基于扩散的模型如
    DALL-E 提供基础，它们使我们能够仅凭文本描述生成令人印象深刻的图片。
- en: 'That this works so well might also not be altogether a coincidence: fundamental
    units of meaning in language might have counterparts in visual processing, and
    visual perception might also be understood in terms of a finite set of basic visual
    patterns or ‘visual words’.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这种效果之所以如此良好，也许并非全然巧合：语言中的基本意义单元可能在视觉处理上有对应物，而视觉感知也可能在有限的一组基本视觉模式或‘视觉词汇’的概念下被理解。
- en: 'In language, the units most meaningful to humans, like ‘cat’, ‘dog’, or ‘love’,
    are short, and language is structured along the same concepts that have salience
    to us in visual scenes: the internet very likely features more pictures of cats
    than of noisy blobs of colors. More generally, it’s important to note language
    is already a derived modality, and one that large groups of people jointly develop
    to condense all the things that are most important to them into a concise symbolic
    representation.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在语言中，对人类最有意义的单元，如‘cat’、‘dog’或‘love’，都是短小的，语言的结构与我们在视觉场景中具有重要性的相同概念一致：互联网很可能包含比喧闹的颜色斑块更多的猫的图片。更一般地说，值得注意的是语言已经是一种衍生模态，是一个大型群体共同发展起来的，用以将所有对他们最重要的事物浓缩为简洁的符号表示。
- en: Perhaps most profoundly, the fact that there `kind of is free lunch' also relates
    to artificial general intelligence (AGI), in that an AGI is an algorithm that
    manages to get free lunch out of most applications.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 也许最深刻的是，“`kind of is free lunch`”这一事实还与人工通用智能（AGI）有关，因为 AGI 是一种能够从大多数应用中获得“免费午餐”的算法。
- en: '![](../Images/3ad29c8ce0393c5cf5292ce797e13dcd.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3ad29c8ce0393c5cf5292ce797e13dcd.png)'
- en: AGI, as envisioned by DALL-E.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: DALL-E 设想中的 AGI。
- en: 'Think of Chat-GPT: while the GPT models have been trained entirely on text,
    they are starting to display the capacity to comprehend, reason, and communicate
    about the world at large, well beyond the confines of textual data. Despite having
    only been trained to predict text data, these models seem to unearth some kind
    of universal logic that goes beyond the text, extending to different modalities
    and dimensions of understanding.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下 Chat-GPT：尽管 GPT 模型完全基于文本进行训练，但它们开始表现出理解、推理和交流关于世界的能力，远远超出了文本数据的范围。尽管这些模型仅被训练来预测文本数据，它们似乎挖掘出了一种超越文本的普遍逻辑，扩展到不同的模态和理解维度。
- en: Especially when co-trained with other modalities, LLMs are moving beyond textual
    description of that modality but instead understand the underlying patterns beyond
    modalities and can generate novel ideas from them, similar to what brains do.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是当与其他模态共同训练时，LLMs超越了对该模态的文本描述，而是理解超越模态的潜在模式，并能从中生成新颖的想法，这类似于大脑的功能。
- en: I think one of the reasons why people (among them myself) have been so surprised
    by the success of LLMs in achieving generalizing intelligence is that we collectively
    underestimated to what extent the perceptual world that is most salient to us
    already relies on the universality of patterns and the corresponding universality
    of a learning algorithm in our brain.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为，人们（包括我自己）对LLMs在实现泛化智能方面取得成功感到如此惊讶的原因之一是，我们集体低估了对我们最显著的感知世界在多大程度上已经依赖于模式的普遍性以及我们大脑中相应的学习算法的普遍性。
- en: While the free lunch theorem is true in theory, it’s also apparent that the
    majority of problems that concern us most are far from universal in a statistical
    sense. Instead, they tend to occupy a lower-dimensional space primarily concerned
    with vision, language, and sound, the primary sphere of human interest and saliency.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然免费午餐定理在理论上是成立的，但显然大多数让我们最关心的问题在统计意义上远非普遍。相反，它们往往占据一个较低维度的空间，主要关注视觉、语言和声音，这是人类兴趣和显著性的主要领域。
- en: In the emergence of multimodal large language models, we have an algorithmic
    framework that capitalizes on these low-dimensional and cross-modal representations.
    It offers an exciting and slightly scary preview of the potential of AGI, and
    the way it could change our world and our understanding of the world.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在多模态大型语言模型的出现中，我们拥有一个算法框架，它利用了这些低维和跨模态的表示。它提供了一个激动人心且稍微有些吓人的预览，展示了AGI的潜力，以及它可能如何改变我们的世界和对世界的理解。
- en: Thanks for reading!
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: If you like my writing, please [subscribe to get my stories via mail](https://manuel-brenner.medium.com/subscribe),
    or [consider becoming a referred member](https://manuel-brenner.medium.com/membership).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢我的写作，请[订阅以通过邮件获取我的故事](https://manuel-brenner.medium.com/subscribe)，或者[考虑成为推荐会员](https://manuel-brenner.medium.com/membership)。
