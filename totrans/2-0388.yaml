- en: Bi-LSTM+Attention for Modeling EHR Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Bi-LSTM+Attention用于建模EHR数据
- en: 原文：[https://towardsdatascience.com/bi-lstm-attention-for-modeling-ehr-data-5dc7b05d4a10](https://towardsdatascience.com/bi-lstm-attention-for-modeling-ehr-data-5dc7b05d4a10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/bi-lstm-attention-for-modeling-ehr-data-5dc7b05d4a10](https://towardsdatascience.com/bi-lstm-attention-for-modeling-ehr-data-5dc7b05d4a10)
- en: Essential guide to the diagnosis prediction in healthcare via attention-based
    Bi-LSTM network
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于注意力的Bi-LSTM网络在医疗保健诊断预测中的基本指南
- en: '[](https://satyam-kumar.medium.com/?source=post_page-----5dc7b05d4a10--------------------------------)[![Satyam
    Kumar](../Images/2360baa87ea7a20f41589c5f8d783288.png)](https://satyam-kumar.medium.com/?source=post_page-----5dc7b05d4a10--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5dc7b05d4a10--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5dc7b05d4a10--------------------------------)
    [Satyam Kumar](https://satyam-kumar.medium.com/?source=post_page-----5dc7b05d4a10--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://satyam-kumar.medium.com/?source=post_page-----5dc7b05d4a10--------------------------------)[![Satyam
    Kumar](../Images/2360baa87ea7a20f41589c5f8d783288.png)](https://satyam-kumar.medium.com/?source=post_page-----5dc7b05d4a10--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5dc7b05d4a10--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5dc7b05d4a10--------------------------------)
    [Satyam Kumar](https://satyam-kumar.medium.com/?source=post_page-----5dc7b05d4a10--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5dc7b05d4a10--------------------------------)
    ·7 min read·Jan 31, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5dc7b05d4a10--------------------------------)
    ·阅读时间7分钟·2023年1月31日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/1afe40d7a671e4ddf480d8041a9b0ffc.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1afe40d7a671e4ddf480d8041a9b0ffc.png)'
- en: Image by [Gerd Altmann](https://pixabay.com/users/geralt-9301/?utm_source=link-attribution&amp%3Butm_medium=referral&amp%3Butm_campaign=image&amp%3Butm_content=6939537)
    from [Pixabay](https://pixabay.com//?utm_source=link-attribution&amp%3Butm_medium=referral&amp%3Butm_campaign=image&amp%3Butm_content=6939537)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Gerd Altmann](https://pixabay.com/users/geralt-9301/?utm_source=link-attribution&amp%3Butm_medium=referral&amp%3Butm_campaign=image&amp%3Butm_content=6939537)
    提供，来自 [Pixabay](https://pixabay.com//?utm_source=link-attribution&amp%3Butm_medium=referral&amp%3Butm_campaign=image&amp%3Butm_content=6939537)
- en: Predicting future health information or disease using the Electronic Health
    Record (EHR) is a key use case for research in the healthcare domain. EHR data
    consists of diagnosis codes, pharmacy codes, and procedure codes. Modeling and
    Interpreting the model on EHR data is a tedious task due to the high dimensionality
    of the data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 预测未来健康信息或疾病使用电子健康记录（EHR）是医疗领域研究的一个关键用例。EHR数据包括诊断代码、药房代码和程序代码。由于数据的高维度，对EHR数据进行建模和解释是一项繁琐的任务。
- en: In this article, we will discuss a popular research paper, [DIPOLE](https://arxiv.org/abs/1706.05764),
    published in June 2019, which uses the Bi-LSTM+Attention network.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将讨论一篇流行的研究论文，[DIPOLE](https://arxiv.org/abs/1706.05764)，该论文发表于2019年6月，使用了Bi-LSTM+Attention网络。
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '1) Limitations of Linear/Tree-based Model:'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1) 线性/树基模型的局限性：
- en: The previous implementation was a **Random Forest model** with a fixed set of
    hyperparameters to model the **aggregate member-level** claims/pharmacy/demographics
    features.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的实现是一个**随机森林模型**，使用一组固定的超参数来建模**聚合的成员级**索赔/药房/人口统计特征。
- en: In the case of disease prediction, the output depends on the **sequence of events
    over time**. This time sequence information gets **lost** in the RF model. So
    the idea is to try time-series model-based event prediction. Candidates can be
    statistical time series models like **ARIMA**, **Holt-Winters**, or Neural Network-based
    models like **RNN/LSTMs** or even **transformer-based architectures**.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在疾病预测的情况下，输出依赖于**时间序列中的事件顺序**。在RF模型中，这种时间序列信息会被**丢失**。因此，思想是尝试基于时间序列模型的事件预测。候选模型可以是统计时间序列模型，如**ARIMA**、**Holt-Winters**，或基于神经网络的模型，如**RNN/LSTMs**，甚至是**transformer-based架构**。
- en: However, the **long-term dependencies** of the events and information in (irregular)
    time intervals between events are difficult to capture in an RF model or even
    in classical time-series models.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然而，事件的**长期依赖性**和事件间（不规则）时间间隔的信息在RF模型甚至经典时间序列模型中很难捕捉。
- en: Further, the Random Forest was not able to capture the **non-linear associations**
    & complex relationships between time-ordered events. This is also the case with
    classical TS models. We can introduce non-linearity by including interaction terms
    (like quadratic, multiplicative, etc) or using kernels (like in SVM), however,
    that depends on us knowing the actual non-linear dependencies which in current
    age real data is very difficult to find out.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，随机森林未能捕捉**非线性关联**和时间排序事件之间的复杂关系。这也是经典 TS 模型的情况。我们可以通过包括交互项（如二次项、乘法项等）或使用核函数（如
    SVM 中）来引入非线性，但这取决于我们是否知道实际的非线性依赖关系，而在当前数据中很难找到。
- en: As such we move ahead with exploring neural network-based time series models
    like **RNN/LSTMs** first and later with **transformer architectures**. The above
    hypothesis about limitations of RF & classical TS models would also be verified
    later from their evaluation metrics comparison with RNN/LSTM models.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们首先探索基于神经网络的时间序列模型，如**RNN/LSTM**，然后是**transformer 架构**。关于 RF 和经典 TS 模型的局限性的假设也会通过与
    RNN/LSTM 模型的评估指标比较来验证。
- en: 2) Why RNN Models?
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2) 为什么选择 RNN 模型？
- en: Claims data includes information related to diagnoses, procedures, and utilization
    for each member at the claim level. The claim information is time-based and the
    existing RF model does not utilize the time information of the visits.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 索赔数据包括与每个成员在索赔级别上的诊断、程序和使用情况相关的信息。索赔信息是基于时间的，而现有的 RF 模型没有利用访问的时间信息。
- en: The idea is to update the RF model with something more suitable for time-series
    event prediction like RNN.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 其思想是用更适合时间序列事件预测的东西更新 RF 模型，如 RNN。
- en: '![](../Images/e3f508bf43f4025cc53d7c957477c4c5.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e3f508bf43f4025cc53d7c957477c4c5.png)'
- en: ([Source](https://en.wikipedia.org/wiki/File:LSTM_Cell.svg)), LSTM Unit
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ([来源](https://en.wikipedia.org/wiki/File:LSTM_Cell.svg))，LSTM 单元
- en: The input of each RNN unit is dependent on the output of the previous unit as
    well as the input sequence at time ‘t’. An RNN unit repeats itself for each event
    in the sequence.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 RNN 单元的输入依赖于前一个单元的输出以及时间‘t’的输入序列。一个 RNN 单元会在序列中的每个事件上重复自身。
- en: 'Limitation of the RNN model:'
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RNN 模型的局限性：
- en: RNNs have been found to be working well in practice for **short-term dependencies**
    in data. For example, a model predicts the next word of the incomplete sentence
    using the existing words. If we are trying to predict the last word in “the clouds
    are in the sky,” we don’t need any further context — it’s pretty obvious the next
    word is going to be the sky. In such cases, where the gap between the relevant
    information and the place where it’s needed is small, RNNs can learn to use past
    information.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 在实践中被发现对数据中的**短期依赖**表现良好。例如，模型使用已有的词预测不完整句子的下一个词。如果我们尝试预测“the clouds are
    in the sky”中的最后一个词，我们不需要进一步的上下文——下一个词很明显是“the sky”。在这种情况下，当相关信息与所需位置之间的间隔较小时，RNN
    可以学会利用过去的信息。
- en: But for long sequences, the RNN model **suffers from a vanishing/exploding gradient
    problem**, which hampers the long-term learning of the events. During backpropagation,
    the gradient becomes smaller and smaller, and the parameter updates become insignificant
    for early events which means no real learning is done.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 但对于长序列，RNN 模型**遭遇梯度消失/爆炸问题**，这妨碍了事件的长期学习。在反向传播过程中，梯度变得越来越小，参数更新对于早期事件变得微不足道，这意味着没有真正的学习。
- en: RNNs also become slow to train over such long sequence data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 在处理如此长的序列数据时也会变得训练缓慢。
- en: 'Alternatives:'
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 替代方案：
- en: '**LSTM** (Long Short Term Memory), and **GRU** (Gated Recurrent Unit) are alternatives
    or updated versions of the RNN network that are capable to capture the long-term
    dependency of the sequential events without having the vanishing/exploding gradient
    problem for most cases. They overcome this problem of RNN by having a selective
    retention mechanism through multiple weights & biases instead of one.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**LSTM**（长短期记忆）和**GRU**（门控循环单元）是 RNN 网络的替代品或更新版本，能够捕捉序列事件的长期依赖，而大多数情况下不会遇到梯度消失/爆炸问题。它们通过多个权重和偏置的选择性保留机制来克服
    RNN 的问题，而不是使用一个。'
- en: '3) Essential guide to LSTM & Bi-LSTM network:'
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3) LSTM 和 Bi-LSTM 网络的基本指南：
- en: An LSTM unit has 3 gates (Input, Output, and Forget Gate) to protect and control
    the cell state and add necessary information to the current state. There are 3
    inputs to an LSTM unit i.e. **previous cell state** (C_(t-1)), **previous unit
    output** (h_(t-1)), and **input event at the time ‘t’** (x_t). Whereas it has
    two outputs i.e. **current cell state** (C_t), and **current output** (h_t).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一个LSTM单元有3个门（输入门、输出门和遗忘门）来保护和控制单元状态，并将必要的信息添加到当前状态中。LSTM单元有3个输入，即**前一个单元状态**（C_(t-1)）、**前一个单元输出**（h_(t-1)）和**时间‘t’的输入事件**（x_t）。而它有两个输出，即**当前单元状态**（C_t）和**当前输出**（h_t）。
- en: 'Please visit colah’s blog to get an understanding of how an LSTM network works
    under the hood:'
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 请访问colah的博客以了解LSTM网络如何在后台工作：
- en: '[## Understanding LSTM Networks'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 理解LSTM网络'
- en: Posted on August 27, 2015 Humans don't start their thinking from scratch every
    second. As you read this essay, you…
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 发布于2015年8月27日，人类不会每秒都从头开始思考。当你阅读这篇文章时，你…
- en: colah.github.io](https://colah.github.io/posts/2015-08-Understanding-LSTMs/?source=post_page-----5dc7b05d4a10--------------------------------)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[colah.github.io](https://colah.github.io/posts/2015-08-Understanding-LSTMs/?source=post_page-----5dc7b05d4a10--------------------------------)'
- en: 'Bi-LSTM Network:'
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Bi-LSTM网络：
- en: Bi-LSTM is a variation of LSTM that flows input in both the direction to preserve
    future and past information.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Bi-LSTM是LSTM的一种变体，它在两个方向上流动输入，以保留未来和过去的信息。
- en: '![](../Images/febc40c40cef2572207506d386221d57.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/febc40c40cef2572207506d386221d57.png)'
- en: ([Source](https://en.wikipedia.org/wiki/Bidirectional_recurrent_neural_networks#/media/File:Structural_diagrams_of_unidirectional_and_bidirectional_recurrent_neural_networks.png)),
    Bi-LSTM Network
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ([来源](https://en.wikipedia.org/wiki/Bidirectional_recurrent_neural_networks#/media/File:Structural_diagrams_of_unidirectional_and_bidirectional_recurrent_neural_networks.png))，Bi-LSTM网络
- en: The forward LSTM reads the input visit sequence from x_1 to x_t and calculates
    a sequence of forward hidden states. The backward LSTM reads the visit sequence
    in the reverse order, i.e., from x_t to x_1, resulting in a sequence of backward
    hidden states. By concatenating the forward hidden state and the backward one,
    we can obtain the final latent vector representation as h_i.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 正向LSTM从x_1到x_t读取输入访问序列，并计算一系列正向隐藏状态。反向LSTM以相反的顺序，即从x_t到x_1，读取访问序列，从而生成一系列反向隐藏状态。通过连接正向隐藏状态和反向隐藏状态，我们可以得到最终的潜在向量表示为h_i。
- en: '**Bi-LSTM layer is used instead of LSTM layer as:**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用Bi-LSTM层而不是LSTM层的原因：**'
- en: to capture the full context of the sequence
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以捕捉序列的完整上下文
- en: often times things only get clear in hindsight when looking at future
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 很多时候，只有在回顾未来时，事情才会变得清晰。
- en: '**Drawbacks of Bi-LSTM:**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**Bi-LSTM的缺点：**'
- en: they are learning left-to-right and right-to-left contexts separately and concatenating
    them. Thus true context is lost in some sense lost.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们分别学习从左到右和从右到左的上下文，并将它们连接在一起。因此，真实的上下文在某种程度上丢失了。
- en: assume each event to be equally time-spaced
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设每个事件的时间间隔相等
- en: are sequential in nature so may get slow to train with big data.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于其序列性质，可能在大数据训练时变得较慢。
- en: Transformers can overcome the above drawbacks.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers可以克服上述缺点。
- en: '4) Essential guide to Attention:'
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4) 注意力机制的基本指南：
- en: The attention mechanism was introduced by [Bahdanau et al. (2014)](https://arxiv.org/abs/1409.0473)
    paper, to address the bottleneck problem that arises when the decoder would have
    limited access to the information provided by the input.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制由[Bahdanau等（2014）](https://arxiv.org/abs/1409.0473)的论文引入，以解决解码器在获取输入提供的信息时所遇到的瓶颈问题。
- en: Attention models enable the network to **focus on a few particular aspects/events**
    at a time and ignore the rest.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力模型使网络能够**一次关注几个特定方面/事件**并忽略其余部分。
- en: In our implementation, we are adding an attention layer to capture the importance
    of a visit vector to make any prediction. The larger an attention score corresponding
    to a visit vector, the more significance it has while making a prediction.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实现中，我们添加了一个注意力层来捕捉访问向量的重要性，以进行任何预测。对应于访问向量的注意力得分越大，它在预测时的重要性就越高。
- en: '5) Implementation:'
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5) 实现：
- en: Our current Bi-LSTM implementation is inspired by the [DIPOLE paper (June 2017)](https://arxiv.org/pdf/1706.05764.pdf)
    by Fenglong Ma and the team. The paper employs a Bi-LSTM network to model EHR
    data and utilizes a simple attention mechanism to interpret the results.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当前的Bi-LSTM实现灵感来自于[Fenglong Ma及团队的[DIPOLE论文（2017年6月）](https://arxiv.org/pdf/1706.05764.pdf)。该论文使用Bi-LSTM网络对EHR数据进行建模，并利用简单的注意力机制来解释结果。
- en: 'I have discussed below the step-by-step approach to our current implementation:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我在下面讨论了我们当前实现的逐步方法：
- en: 'a) Data:'
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: a) 数据：
- en: We are using data (Electronic Health Data — EHR) data for Bi-LSTM+Attention
    modeling.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用数据（电子健康数据 — EHR）进行 Bi-LSTM+Attention 建模。
- en: 'Note: This is the same data we were using for Linear/Tree modeling.'
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意：这与我们用于线性/树模型的数据相同。
- en: As of now, we are restricted to the model only on the diagnosis medical codes.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们的模型仅限于诊断医疗代码。
- en: '![](../Images/9c53e36be38330797f76800aae747855.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c53e36be38330797f76800aae747855.png)'
- en: (Image by Author), Snapshot of raw EHR data
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: （作者提供的图片），原始 EHR 数据的快照
- en: 'b) Feature Engineering (EHR Data):'
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: b) 特征工程（EHR 数据）：
- en: The claims data is line level, we select the first record for each claim to
    make it to the visit level.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 索赔数据是按行级别的，我们选择每个索赔的第一条记录，以将其转化为访问级别。
- en: Prepare a diagnosis label encoder for all the unique diagnosis codes in the
    training data.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为训练数据中的所有唯一诊断代码准备一个诊断标签编码器。
- en: '![](../Images/48a435c418da78559d8b2e2484819677.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/48a435c418da78559d8b2e2484819677.png)'
- en: One hot encode the diagnosis code for each visit.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对每次访问的诊断代码进行独热编码。
- en: Select the last x visits/claims *(hyperparameter)* for each member. If a member
    does not threshold the number of visits, we pad the remaining visits with a zero
    vector.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择每个成员的最后 x 次访问/索赔 *(超参数)*。如果一个成员的访问次数未达到阈值，我们用零向量填充剩余的访问记录。
- en: '![](../Images/44a9520fe157fd1a25bcd106dd987a55.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/44a9520fe157fd1a25bcd106dd987a55.png)'
- en: Format the data suitable for LSTM network input `**(Members, Visits, unique
    medical codes)**`
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据格式化为适合 LSTM 网络输入的形式 `**(成员, 访问次数, 唯一医疗代码)**`
- en: 'For a dataset with 1000 members having 5000 unique medical code, and padding
    the visit to 10, we will have the final training data shape as: (1000, 10, 5000)'
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对于一个包含 1000 名成员和 5000 个唯一医疗代码的数据集，并将访问记录填充至 10，我们将得到最终的训练数据形状为：（1000, 10, 5000）
- en: '![](../Images/ec4274cc96711f24af76b70e6fe8ba78.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ec4274cc96711f24af76b70e6fe8ba78.png)'
- en: LSTM data input format
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 数据输入格式
- en: c) Bi-LSTM + Attention in Keras
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: c) Keras 中的 Bi-LSTM + Attention
- en: We will use `**tf.Keras**` framework to implement the Bi-LSTM + Attention network
    to model the disease prediction.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `**tf.Keras**` 框架来实现 Bi-LSTM + Attention 网络，以进行疾病预测建模。
- en: (Code by Author)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: （作者提供的代码）
- en: d) Interpretation
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: d) 解释
- en: In healthcare, the interpretability of the learned representations of medical
    codes and visits is important. We need to understand the clinical meaning of each
    dimension of medical code representations and analyze which visits are crucial
    to the prediction.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在医疗保健中，学习到的医疗代码和访问的表示的可解释性很重要。我们需要理解医疗代码表示的每一维度的临床意义，并分析哪些访问对预测至关重要。
- en: Since the proposed model is based on attention mechanisms, we can find the importance
    of each visit for prediction by analyzing the attention scores.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 由于提出的模型基于注意力机制，我们可以通过分析注意力分数来找到每次访问在预测中的重要性。
- en: f) Model Summary
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: f) 模型摘要
- en: '![](../Images/768c3117696cdeb6cbd9780f4fa72569.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/768c3117696cdeb6cbd9780f4fa72569.png)'
- en: (Image by Author), Model Summary
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: （作者提供的图片），模型摘要
- en: 'Summary:'
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要：
- en: DIPOLE implementation uses a Bi-LSTM architecture network, to capture the long-term
    and short-term dependencies of the historical EHR data. The Attention mechanism
    can be used to interpret the prediction results, learned medical codes, and visit-level
    information.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: DIPOLE 实现使用 Bi-LSTM 架构网络，以捕捉历史 EHR 数据的长期和短期依赖关系。注意力机制可以用来解释预测结果、学习到的医疗代码和访问级别信息。
- en: According to the authors of the paper, DIPOLE can significantly improve performance
    compared to the traditional state-of-the-art diagnosis prediction approaches.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 根据论文作者的说法，DIPOLE 可以显著提高与传统最先进的诊断预测方法相比的性能。
- en: 'References:'
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献：
- en: '[1] Understanding LSTM (*August 27, 2015*): [https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 理解 LSTM (*2015年8月27日*): [https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
- en: '[2] DIPOLE (*June 19, 2017*): [https://arxiv.org/pdf/1706.05764.pdf](https://arxiv.org/pdf/1706.05764.pdf)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] DIPOLE (*2017年6月19日*): [https://arxiv.org/pdf/1706.05764.pdf](https://arxiv.org/pdf/1706.05764.pdf)'
- en: '[3] Bi-LSTM with Attention implementation (*August 22, 2021*): [https://analyticsindiamag.com/hands-on-guide-to-bi-lstm-with-attention/](https://analyticsindiamag.com/hands-on-guide-to-bi-lstm-with-attention/)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] 带有注意力机制的 Bi-LSTM 实现 (*2021年8月22日*): [https://analyticsindiamag.com/hands-on-guide-to-bi-lstm-with-attention/](https://analyticsindiamag.com/hands-on-guide-to-bi-lstm-with-attention/)'
- en: Thank You for Reading
  id: totrans-88
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 感谢阅读
