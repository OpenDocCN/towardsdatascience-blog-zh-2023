- en: How to Program a Neural Network
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何编程一个神经网络
- en: 原文：[https://towardsdatascience.com/how-to-program-a-neural-network-f28e3f38e811](https://towardsdatascience.com/how-to-program-a-neural-network-f28e3f38e811)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-program-a-neural-network-f28e3f38e811](https://towardsdatascience.com/how-to-program-a-neural-network-f28e3f38e811)
- en: A step-by-step guide to implementing a neural network from scratch
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从头实现神经网络的逐步指南
- en: '[](https://medium.com/@callum.bruce1?source=post_page-----f28e3f38e811--------------------------------)[![Callum
    Bruce](../Images/4833a199a9449434777fdf5ce913a9cb.png)](https://medium.com/@callum.bruce1?source=post_page-----f28e3f38e811--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f28e3f38e811--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f28e3f38e811--------------------------------)
    [Callum Bruce](https://medium.com/@callum.bruce1?source=post_page-----f28e3f38e811--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@callum.bruce1?source=post_page-----f28e3f38e811--------------------------------)[![Callum
    Bruce](../Images/4833a199a9449434777fdf5ce913a9cb.png)](https://medium.com/@callum.bruce1?source=post_page-----f28e3f38e811--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f28e3f38e811--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f28e3f38e811--------------------------------)
    [Callum Bruce](https://medium.com/@callum.bruce1?source=post_page-----f28e3f38e811--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f28e3f38e811--------------------------------)
    ·14 min read·Sep 23, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f28e3f38e811--------------------------------)
    ·阅读时间 14 分钟·2023 年 9 月 23 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/79509eca917d69c1d6ca7ef6178adf3d.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/79509eca917d69c1d6ca7ef6178adf3d.png)'
- en: A neural network with three hidden layers
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有三个隐藏层的神经网络
- en: In this article, we will build a neural network from scratch and use it to classify
    handwritten digits.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将从头开始构建一个神经网络，并使用它来分类手写数字。
- en: Why reinvent the wheel/neural network, I hear you say? Can’t I just use my favourite
    machine learning framework and be done with it? Yes, there are many off-the-shelf
    frameworks that you can use to build a neural network (Keras, PyTorch, and TensorFlow
    to name a few). The thing with using one of these is that they make it easy for
    us to treat neural networks like black boxes.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要重新发明轮子/神经网络，我听到你说？难道我不能直接使用我最喜欢的机器学习框架来解决问题吗？是的，你可以使用许多现成的框架来构建神经网络（比如 Keras、PyTorch
    和 TensorFlow）。使用这些框架的问题在于，它们让我们很容易将神经网络当作黑箱处理。
- en: This isn’t always a bad thing. Often we need this level of abstraction so that
    we can get to work on the problem at hand, but we should still strive to at least
    have a basic understanding of what is going on under the hood if we are to use
    neural networks in our work.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不总是坏事。我们通常需要这种程度的抽象，以便能够处理手头的问题，但如果我们要在工作中使用神经网络，仍然应该努力至少对其内部运作有一个基本了解。
- en: Building a neural network from scratch is, in my opinion, the best way to foster
    a deep understanding of how they work.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 从头开始构建神经网络在我看来是培养深刻理解其工作原理的最佳方式。
- en: By the end of this article, you will have learned about the feedforward and
    backpropagation algorithms, what an activation function is, what the difference
    between an epoch and a batch is, and how to train a neural network. We will finish
    with an example by training a neural network to recognise handwritten digits.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 到本文结束时，你将了解前馈和反向传播算法，激活函数是什么，epoch 和 batch 之间的区别是什么，以及如何训练神经网络。我们将通过训练一个神经网络来识别手写数字来完成本例。
- en: All code used in this article is available here on [GitHub](https://github.com/c-bruce/artificial_neural_network)
    [1].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中使用的所有代码可以在 [GitHub](https://github.com/c-bruce/artificial_neural_network)
    [1] 上找到。
- en: What is a neural network?
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是神经网络？
- en: Neural networks, or artificial neural networks, are a type of machine learning
    algorithm. They form the core of many deep learning and artificial intelligence
    systems like computer vision, forecasting and speech recognition.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络，或人工神经网络，是一种机器学习算法。它们是许多深度学习和人工智能系统的核心，例如计算机视觉、预测和语音识别。
- en: The structure of artificial neural networks is sometimes compared to the structure
    of biological neural networks in the brain. I would always urge caution not to
    draw too much from this comparison. Sure, artificial neural networks look a bit
    like biological neural networks but it is quite a big leap to start comparing
    them to something as complex as a human brain.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络的结构有时与大脑中的生物神经网络的结构进行比较。我总是建议对这种比较保持谨慎。确实，人工神经网络看起来有点像生物神经网络，但将它们与像人脑这样复杂的东西进行比较是一个很大的飞跃。
- en: A neural network is made up of several layers of neurons. Each layer of neurons
    is activated based on the activations in the previous layer, a set of weights
    connecting the previous layer to the current layer and a set of biases applied
    to the neurons in the current layer.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络由几层神经元组成。每一层神经元的激活基于前一层的激活、连接前一层与当前层的权重集合，以及施加在当前层神经元上的偏置集合。
- en: '![](../Images/f38cca749159b2f7552d2330aa156b09.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f38cca749159b2f7552d2330aa156b09.png)'
- en: General structure of a neural network containing two hidden layers. Neurons
    are shaded by their activations (larger activation magnitude = darker neuron).
    Positive weights in red. Negative weights in blue. Line width indicates weight
    magnitude.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 包含两个隐藏层的神经网络的一般结构。神经元根据其激活程度着色（激活量越大，神经元越暗）。正权重用红色表示，负权重用蓝色表示。线宽表示权重大小。
- en: The first layer is the input layer. Input layer activations come from the input
    to the neural network. The final layer is the output layer. The activations in
    the output layer are the output of the neural network. The layers in between are
    called hidden layers.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层是输入层。输入层的激活来自神经网络的输入。最后一层是输出层。输出层的激活是神经网络的输出。中间的层称为隐藏层。
- en: A neural network is a generalized approximation of a function. Like any other
    function, when we give it an input it returns an output.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是对函数的广义近似。像其他任何函数一样，当我们给它一个输入时，它会返回一个输出。
- en: The novel thing about neural networks is *how* they get from the input to the
    output. The process is driven by how network weights and biases influence neuron
    activations and how these propagate through the network to eventually arrive at
    the output layer. The feedforward algorithm is used by neural networks to transform
    from our input into an output.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的新颖之处在于*如何*从输入到输出。这个过程由网络权重和偏置如何影响神经元激活以及这些激活如何在网络中传播，最终到达输出层来驱动。神经网络使用前馈算法将输入转换为输出。
- en: For a neural network to provide useful output we must first train it. When we
    train a neural network all that we are doing is iteratively adjusting the weights
    and biases to improve the accuracy of the output. We work out which direction
    and by how much we need to nudge the weights and biases using backpropagation
    and gradient descent.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使神经网络提供有用的输出，我们必须首先对其进行训练。当我们训练神经网络时，我们所做的就是通过反向传播和梯度下降迭代调整权重和偏置，以提高输出的准确性。我们计算需要将权重和偏置向哪个方向和调整多少。
- en: Feedforward algorithm
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前馈算法
- en: The feedforward algorithm transforms our neural network input into meaningful
    output. As its name suggests, the algorithm “feeds forward” information from one
    layer to the next.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈算法将我们的神经网络输入转化为有意义的输出。顾名思义，该算法将信息“向前传递”从一层到下一层。
- en: To understand how it achieves this, let’s first zoom in and look at how information
    is passed from one layer to a single neuron in the next layer.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解它是如何实现的，让我们先放大来看信息是如何从一层传递到下一层的一个神经元的。
- en: '![](../Images/abb01642c9f7285b915f83047f7374c3.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/abb01642c9f7285b915f83047f7374c3.png)'
- en: Weights connecting neurons in layer 0 with the first neuron in layer 1
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 连接层0中神经元与层1中第一个神经元的权重
- en: 'The activation of the first neuron in the second layer, *a*₀⁽¹⁾,is calculated
    by taking a weighted sum of the activations from the previous layer plus a bias
    and passing this through an activation function, *σ*(*x*):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 第二层中第一个神经元的激活*a*₀⁽¹⁾是通过对前一层的激活进行加权求和，再加上偏置，并通过激活函数*σ*(*x*)来计算的：
- en: Equation for calculating *a*₀⁽¹⁾
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 计算*a*₀⁽¹⁾的方程
- en: The superscript with round brackets denotes the layer index, starting from 0
    for the input layer. Activation (*a*)*,* and bias (*b*) subscriptsdenote neuron
    index. The first and second numbers in the weight (*w*) subscripts denote the
    index of the neuron the weight connects to (in the current layer) and from (in
    the previous layer) respectively.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 带圆括号的上标表示层索引，从 0 开始，表示输入层。激活（*a*）和偏置（*b*）下标表示神经元索引。权重（*w*）下标中的前两个数字表示权重连接的神经元的索引（当前层中的）和从（前一层中的）索引。
- en: The activation function determines if a neuron should be activated based on
    the input it receives. Common activation functions include sigmoid, tanh, rectified
    linear unit (ReLU), and softmax. To keep things simple, in our implementation,
    we will always use the sigmoid activation function.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数决定一个神经元是否应根据接收到的输入而被激活。常见的激活函数包括 sigmoid、tanh、修正线性单元（ReLU）和 softmax。为了简单起见，在我们的实现中，我们将始终使用
    sigmoid 激活函数。
- en: '![](../Images/644584ee1f184d127aec522e9821bccb.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/644584ee1f184d127aec522e9821bccb.png)'
- en: Sigmoid, tanh and ReLU activation functions
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid、tanh 和 ReLU 激活函数
- en: 'The equation we used to calculate *a*₀⁽¹⁾ can be vectorised so that we can
    calculate all activations in the second layer:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用来计算 *a*₀⁽¹⁾ 的方程可以向量化，以便我们可以计算第二层中的所有激活值：
- en: Vectorised equation for calculating ***a***⁽¹⁾
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 计算 ***a***⁽¹⁾ 的向量化方程
- en: Now we have neuron activations for the second layer ***a***⁽¹⁾, we can use the
    same calculation to find ***a***⁽²⁾ and again for ***a***⁽³⁾ and so on…
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了第二层的神经元激活值 ***a***⁽¹⁾，我们可以使用相同的计算来找到 ***a***⁽²⁾，然后是 ***a***⁽³⁾，以此类推……
- en: 'Let’s look at how this can be implemented in Python:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在 Python 中实现这一点：
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `Network` class contains all the information about our neural network. We
    initialise it by passing a list of integers relating to the number of neurons
    in each layer. For example, `network = Network([10, 3, 3, 2])` will create a network
    with ten neurons in the input layer, two hidden layers each containing three neurons
    and an output layer with two neurons.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '`Network` 类包含有关我们神经网络的所有信息。我们通过传递一个整数列表来初始化它，该列表与每层的神经元数量相关。例如，`network = Network([10,
    3, 3, 2])` 将创建一个输入层有十个神经元、两个隐藏层每个包含三个神经元，以及一个输出层有两个神经元的网络。'
- en: The `__init_*` methods initialise values for activations, weights and biases.
    Activations and biases are initially all zero. Weights are given a random value
    between -1 and 1.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`__init_*` 方法初始化激活值、权重和偏置。激活值和偏置最初都是零。权重被赋予一个在 -1 和 1 之间的随机值。'
- en: The `feedforward` method loops through the layers calculating activations in
    each subsequent layer.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`feedforward` 方法循环遍历各层，计算每个后续层的激活值。'
- en: 'Below is an example using `feedforward` to calculate the output of our `[10,
    3, 3, 2]` network given a random input. The output is inspected by printing the
    activations in the final layer:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个示例，使用`feedforward`计算我们 `[10, 3, 3, 2]` 网络在给定随机输入时的输出。通过打印最终层的激活值来检查输出：
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: That's it! We have successfully implemented the feedforward algorithm! Lets
    turn our attention to backpropagation.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们已经成功实现了前向传播算法！让我们将注意力转向反向传播。
- en: Backpropagation algorithm
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播算法
- en: The backpropagation algorithm is the process through which a neural network
    learns from its mistakes.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播算法是神经网络从错误中学习的过程。
- en: In the above implementation of the feedforward algorithm, we initialise network
    weights with a random number between -1 and 1 and set all biases to equal 0\.
    With this initial setup, the output produced by the network for any given input
    will be essentially random.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述前向传播算法的实现中，我们将网络权重初始化为 -1 和 1 之间的随机数，并将所有偏置设置为 0。使用这种初始设置，网络对任何给定输入生成的输出本质上是随机的。
- en: 'What we need is a way to update the weights and biases so that the output of
    the network becomes more meaningful. To do this we use gradient descent:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一种方法来更新权重和偏置，使网络的输出变得更有意义。为此，我们使用梯度下降：
- en: Gradient descent update step
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降更新步骤
- en: Where ***a****ₙ*is a vector of input parameters. The subscript *n* denotes iteration.
    *f(****a****ₙ)* is a multi-variable cost function and ∇*f(****a****) is* the gradient
    of that cost function. 𝛾 is the learning rate which determines by how much ***a****ₙ*should
    be adjusted each iteration. I previously wrote an [article about gradient descent](https://medium.com/towards-data-science/pid-controller-optimization-a-gradient-descent-approach-58876e14eef2)
    [2] which goes into much more detail on gradient descent than I do here.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ***a****ₙ* 是一个输入参数的向量。下标 *n* 表示迭代次数。*f(****a****ₙ)* 是一个多变量代价函数，∇*f(****a****)*
    是该代价函数的梯度。𝛾 是学习率，它决定了每次迭代中 ***a****ₙ* 应调整的幅度。我之前写过一篇关于梯度下降的 [文章](https://medium.com/towards-data-science/pid-controller-optimization-a-gradient-descent-approach-58876e14eef2)
    [2]，其中详细讨论了梯度下降。
- en: '[](/pid-controller-optimization-a-gradient-descent-approach-58876e14eef2?source=post_page-----f28e3f38e811--------------------------------)
    [## PID Controller Optimization: A Gradient Descent Approach'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/pid-controller-optimization-a-gradient-descent-approach-58876e14eef2?source=post_page-----f28e3f38e811--------------------------------)
    [## PID 控制器优化：一种梯度下降方法'
- en: Using machine learning to solve engineering optimization problems
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用机器学习解决工程优化问题
- en: towardsdatascience.com](/pid-controller-optimization-a-gradient-descent-approach-58876e14eef2?source=post_page-----f28e3f38e811--------------------------------)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/pid-controller-optimization-a-gradient-descent-approach-58876e14eef2?source=post_page-----f28e3f38e811--------------------------------)
- en: In the case of a neural network, ***a****ₙ*contains all of the network weights
    and biases, and *f(****a****ₙ)* is the network cost (note *f(****a****ₙ)* ≡*C)*.
    Here we define network cost using the L2-norm cost function calculated from the
    expected network output, *ŷ* and the actual network output, *y*. Both *ŷ* and
    *y* are vectors containing *n* values where *n* is the number of neurons in the
    output layer.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络的情况下，***a****ₙ* 包含了所有网络的权重和偏置，*f(****a****ₙ)* 是网络代价（注意 *f(****a****ₙ)*
    ≡ *C*）。这里我们使用 L2-范数代价函数来定义网络代价，该函数是根据期望网络输出 *ŷ* 和实际网络输出 *y* 计算的。*ŷ* 和 *y* 都是包含
    *n* 个值的向量，其中 *n* 是输出层中的神经元数量。
- en: L2-norm cost function
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: L2-范数代价函数
- en: Now we know how to calculate the cost function but we don’t yet know how to
    calculate the gradient of the cost function.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何计算代价函数，但还不知道如何计算代价函数的梯度。
- en: Finding the gradient of the cost function is at the core of backpropagation.
    The gradient of the cost function tells us in which direction, and by how much,
    we need to nudge the weights and biases in our network to improve the accuracy
    of its output.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 计算代价函数的梯度是反向传播的核心。代价函数的梯度告诉我们需要将网络中的权重和偏置向哪个方向以及调整多少，以提高输出的准确性。
- en: Gradient of the cost function
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 代价函数的梯度
- en: To find these partial derivatives, neural networks employ [the chain rule](https://en.wikipedia.org/wiki/Chain_rule).
    This handy piece of high school calculus is a key to how neural networks work.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到这些偏导数，神经网络采用了 [链式法则](https://en.wikipedia.org/wiki/Chain_rule)。这段高中微积分的内容是神经网络工作的关键。
- en: 'To demonstrate how the chain rule is used in the backpropagation algorithm
    we will consider a network containing a single neuron in each layer:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示链式法则在反向传播算法中的应用，我们将考虑一个每层包含一个神经元的网络：
- en: '![](../Images/da2a39beb27b374c280ef8644e3ca660.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da2a39beb27b374c280ef8644e3ca660.png)'
- en: Neural network with a single neuron in each layer
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 每层包含一个神经元的神经网络
- en: I have introduced a simplified version of the notation introduced earlier since
    we don’t need to index neurons in each layer in this example. Below, I also introduce
    a new variable, *z* which encapsulates the input to the activation function.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我引入了一个简化版本的符号表示，因为在这个例子中我们不需要为每层中的神经元编号。下面，我还引入了一个新变量 *z*，它封装了激活函数的输入。
- en: We start backpropagation from layer *L* which is the output layer and iterate
    back through the layers.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从输出层 *L* 开始反向传播，并迭代地向后通过各层。
- en: 'For the output layer, we have:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输出层，我们有：
- en: C, a⁽*ᴸ*⁾ and z⁽*ᴸ*⁾ for the output layer of the single neuron per layer network
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 单层神经元网络输出层的 C、a⁽*ᴸ*⁾ 和 z⁽*ᴸ*⁾
- en: 'Now that we have defined *C*, *a*⁽*ᴸ*⁾ and *z*⁽*ᴸ*⁾ for the output layer, we
    can calculate their derivatives and apply the chain rule to find ∂*C/*∂*w*⁽*ᴸ*⁾:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为输出层定义了 *C*、*a*⁽*ᴸ*⁾ 和 *z*⁽*ᴸ*⁾，我们可以计算它们的导数并应用链式法则来找到 ∂*C/*∂*w*⁽*ᴸ*⁾：
- en: Applying the chain rule to calculate ∂*C/*∂w⁽*ᴸ*⁾
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 应用链式法则来计算 ∂*C/*∂w⁽*ᴸ*⁾
- en: 'and similarly for ∂*C/*∂*b*⁽*ᴸ*⁾:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 ∂*C/*∂*b*⁽*ᴸ*⁾ 也是类似的：
- en: Applying the chain rule to calculate ∂*C/*∂*b*⁽*ᴸ*⁾
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 应用链式法则来计算 ∂*C/*∂*b*⁽*ᴸ*⁾
- en: 'and ∂*C/*∂a⁽*ᴸ*⁻¹⁾:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 和 ∂*C/*∂a⁽*ᴸ*⁻¹⁾：
- en: Applying the chain rule to calculate ∂*C/*∂a⁽*ᴸ*⁻¹⁾
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 应用链式法则计算 ∂*C/*∂a⁽*ᴸ*⁻¹⁾
- en: 'Now that we have an expression for ∂*C/*∂a⁽*ᴸ*⁻¹⁾ we can iterate back through
    the network to find how sensitive the cost function is to previous weights and
    biases:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了 ∂*C/*∂a⁽*ᴸ*⁻¹⁾ 的表达式，我们可以反向迭代通过网络找到成本函数对之前的权重和偏置的敏感性：
- en: Sensitivity of the cost function to w⁽*ᴸ*⁻¹⁾ and b⁽*ᴸ*⁻¹⁾
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 成本函数对 w⁽*ᴸ*⁻¹⁾ 和 b⁽*ᴸ*⁻¹⁾ 的敏感性
- en: The equations we have defined for calculating the sensitivities to weight and
    biases in this simplified one-neuron-per-layer network stay essentially the same
    when we have more than one neuron per layer.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为计算这种简化的每层一个神经元网络中对权重和偏置的敏感性定义的方程，在每层有多个神经元时基本保持不变。
- en: What changes is the derivative of the cost function with respect to the activations
    in the *L-1*ᵗʰ layer. This is because the cost function is influenced by these
    activations via multiple paths through the network.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 变化的是关于 *L-1*ᵗʰ 层激活值的成本函数的导数。这是因为成本函数通过网络的多个路径受到这些激活值的影响。
- en: 'We define the derivative of the cost function with respect to activations in
    the *L-1*ᵗʰ layer as:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将成本函数对 *L-1*ᵗʰ 层激活值的导数定义为：
- en: Derivative of the cost function with respect to the kᵗʰ activation in the L-1ᵗʰ
    layer
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 成本函数关于 L-1ᵗʰ 层中第 kᵗʰ 激活的导数
- en: where the subscripts *j* and *k* denote activations in the *L*ᵗʰ and *L-1*ᵗʰ
    layers respectively.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 其中下标 *j* 和 *k* 分别表示在 *L*ᵗʰ 和 *L-1*ᵗʰ 层的激活值。
- en: 'The backpropagation algorithm is implemented in a method called `backpropagation`
    belonging to the `Network` class:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播算法在 `Network` 类中的 `backpropagation` 方法中实现：
- en: '[PRE2]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note that this method is vectorized to account for multiple neurons in each
    layer. dcost_dweights and dcost_dbiases are stored in arrays that are the same
    shape as the weights and biases arrays defined earlier. This makes applying gradient
    descent using these partial derivatives trivial. I also think it makes the code
    more readable.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个方法进行了矢量化处理，以适应每层多个神经元。dcost_dweights 和 dcost_dbiases 存储在与之前定义的权重和偏置数组相同形状的数组中。这使得使用这些偏导数进行梯度下降变得非常简单。我还认为这使得代码更具可读性。
- en: As we step back through the network we apply the chain rule for each layer and
    calculate the sensitivity of the cost function to the weight and biases from each
    layer using the equations introduced in this section.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们向后遍历网络时，我们对每一层应用链式法则，并使用本节中介绍的方程计算成本函数对每层权重和偏置的敏感性。
- en: Training a neural network to classify handwritten digits
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练一个神经网络来分类手写数字
- en: Having implemented the feedforward and backpropagation algorithms it is time
    to bring everything together and train a neural network for recognising handwritten
    digits.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 实现了前向传播和反向传播算法之后，是时候将所有内容整合起来，训练一个用于识别手写数字的神经网络了。
- en: To do this, we need a dataset of handwritten digits labelled with their corresponding
    value. Generating this dataset ourselves would be labour-intensive. Fortunately,
    databases already exist that can be used for this digit recognition problem. We
    will use the ModifiedNational Institute of Standards and Technology (MNIST) database*
    [3], a large database of labelled handwritten digits, for training our neural
    network.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们需要一个标注了相应值的手写数字数据集。自己生成这个数据集将会非常费力。幸运的是，已经存在可以用于这个数字识别问题的数据库。我们将使用修改版国家标准与技术研究所（MNIST）数据库*
    [3]，这是一个大型的标注手写数字数据库，用于训练我们的神经网络。
- en: '![](../Images/51074ebf5befa172c202becf4f5a722c.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51074ebf5befa172c202becf4f5a722c.png)'
- en: A random selection of samples from the MNIST database
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 从 MNIST 数据库中随机选择的样本
- en: The MNIST database contains 70000 labelled greyscale images of size 28 x 28
    pixels (784 total). Each labelled image in the database is called a *sample*.
    The MNIST database is split into *training* and *testing* subsets containing 60000
    and 10000 samples respectively.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST 数据库包含 70000 个标注的灰度图像，大小为 28 x 28 像素（总共 784）。数据库中的每个标注图像称为 *样本*。MNIST 数据库被划分为
    *训练* 和 *测试* 子集，其中包含 60000 和 10000 个样本，分别用于训练和测试。
- en: As their names suggest, the training subset is used to train the network and
    the testing subset is used to test network accuracy. This way we can test network
    accuracy using samples that the network has never seen before.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 正如它们的名字所示，训练子集用于训练网络，测试子集用于测试网络的准确性。这样我们可以使用网络从未见过的样本来测试其准确性。
- en: Next, we split the training subset into *batches*. For this example, I decided
    that each batch should contain 100 samples. There are 600 batches in total. The
    reason we split the training subset up into batches is that we don’t update network
    weights and biases after every sample. Instead, we update it after every batch.
    This way, when we apply gradient descent, rather than nudging weights and biases
    using a gradient based on a single sample, we use a gradient which is the average
    gradient calculated using all samples in a batch.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将训练子集分割成*批次*。在这个例子中，我决定每个批次包含100个样本。总共有600个批次。我们将训练子集分成批次的原因是我们不会在每个样本后更新网络的权重和偏置。相反，我们是在每个批次后更新的。这样，当我们应用梯度下降时，我们使用的是基于一个批次所有样本计算的平均梯度，而不是基于单个样本的梯度来调整权重和偏置。
- en: An *epoch* contains all batches in the training subset. An epoch loops through
    all of these batches. Choosing to train our network with a single epoch means
    that the network will only “see” each sample from the training subset once. Increasing
    the number of epochs means that the network will train on, and therefore “see”,
    each sample several times.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 一个*周期*包含训练子集中的所有批次。一个周期会遍历所有这些批次。选择用一个周期训练我们的网络意味着网络只会“看到”训练子集中的每个样本一次。增加周期数意味着网络将对每个样本进行多次训练，从而“看到”每个样本多次。
- en: '![](../Images/d39aaaa868b459bc69eef9ab96fec460.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d39aaaa868b459bc69eef9ab96fec460.png)'
- en: Training workflow diagram
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 训练工作流程图
- en: A full definition of the `Network` class including all methods used in the training
    workflow is shown below. The `train_network` method is responsible for orchestrating
    the training workflow.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 下方显示了`Network`类的完整定义，包括在训练工作流程中使用的所有方法。`train_network`方法负责协调训练工作流程。
- en: '[PRE3]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that we use the AdaGrad gradient descent algorithm for updating network
    weights and biases. Adagrad adds a little more complexity than vanilla gradient
    descent but performs much better for this application.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们使用AdaGrad梯度下降算法来更新网络的权重和偏置。Adagrad比传统梯度下降算法复杂一些，但在这个应用中表现更好。
- en: Next we need to define the shape of the network. The 784 pixel values in each
    sample make up the activations in the input layer so the size of the input layer
    is set to 784\. Since we are categorising digits between 0 and 9 we also know
    that the output layer must contain 10 neurons. For the hidden layers, I found
    that two hidden layers each with 32 neurons worked well for this problem.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要定义网络的形状。每个样本中的784个像素值构成输入层的激活，因此输入层的大小设置为784。由于我们正在对0到9的数字进行分类，我们也知道输出层必须包含10个神经元。对于隐藏层，我发现两个每层32个神经元的隐藏层对这个问题效果很好。
- en: In total, there are 26432 weights and 74 biases in this network. This means
    that when we train the network we are optimizing in a 26506 dimension parameter
    space!
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，这个网络中有26432个权重和74个偏置。这意味着在训练网络时，我们是在一个26506维的参数空间中进行优化！
- en: Don’t let the scale of this optimization task intimidate you. We have already
    done the hard work by implementing the feedforward and backpropagation algorithms
    and by defining the training workflow.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 不要被这项优化任务的规模吓倒。我们已经通过实现前馈和反向传播算法以及定义训练工作流程完成了艰巨的工作。
- en: Before training the network, a little bit of preparation work on the training
    data is required to split it up into batches. We can then call train_network to
    train the network. Finally, once the network is trained, we calculate network
    accuracy by checking the output of the network against the testing subset to see
    how many samples the network categorizes correctly.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练网络之前，需要对训练数据进行一些准备工作，以将其分割成批次。然后，我们可以调用`train_network`来训练网络。最后，一旦网络训练完成，我们通过检查网络对测试子集的输出，来计算网络准确率，以查看网络正确分类了多少样本。
- en: '[PRE4]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: After training the network, network accuracy comes out to be 94.2%. Not bad
    for a neural network built from scratch!
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，网络的准确率为94.2%。对于一个从零开始构建的神经网络来说，这并不算差！
- en: Summary
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this article, I have shown you how to build a simple neural network from
    scratch using Python.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我展示了如何使用Python从零开始构建一个简单的神经网络。
- en: We covered the implementation of the feedforward and backpropagation algorithms
    in detail, introduced the training workflow and trained a neural network with
    26432 weights and 74 biases to recognize handwritten digits from the MNIST database,
    achieving 94.2% network accuracy.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们详细讲解了前馈和反向传播算法的实现，介绍了训练工作流程，并用26432个权重和74个偏置训练了一个神经网络来识别MNIST数据库中的手写数字，达到了94.2%的网络准确率。
- en: Better accuracy can be achieved by refining our implementation. For example,
    using the ReLU activation function for the hidden layers and softmax for the output
    layer has been shown to improve network accuracy [4].
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 通过改进我们的实现可以获得更好的准确性。例如，使用 ReLU 激活函数用于隐藏层，softmax 用于输出层，已被证明能提高网络的准确性 [4]。
- en: Similarly, we could select a different form of gradient descent for adjusting
    the weights and biases which may lead us to find a more optimum minimum in our
    26506 dimension parameter space.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以选择不同形式的梯度下降来调整权重和偏差，这可能使我们在 26506 维参数空间中找到更优的最小值。
- en: The process of flattening each sample into a 1D array also throws away a lot
    of important information. More advanced convolutional neural networks retain information
    about neighbouring pixels in images and typically perform better than the basic
    network type implemented here.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 将每个样本展平成一维数组的过程也会丢弃大量重要信息。更先进的卷积神经网络保留了图像中邻近像素的信息，通常比这里实现的基本网络类型表现更好。
- en: When I set out writing this article my aim was to produce a concise resource
    that somebody new to neural networks would be able to read and gain a basic understanding
    of how they work. I hope I have achieved this and in some small way encouraged
    you to continue learning about this extremely interesting topic.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 当我开始写这篇文章时，我的目标是制作一个简明的资源，让刚接触神经网络的人能够阅读并获得对其工作原理的基本理解。我希望我达到了这个目标，并在某种程度上鼓励你继续学习这个极具趣味的主题。
- en: Has this article helped you develop a deeper understanding of how neural networks
    work? Let me know in the comments.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章是否帮助你更深入地理解了神经网络的工作原理？请在评论中告诉我。
- en: '**Enjoyed reading this article?**'
  id: totrans-113
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**喜欢这篇文章吗？**'
- en: ''
  id: totrans-114
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Follow](/@callum.bruce1) and [subscribe](/@callum.bruce1/subscribe) for more
    content like this — share it with your network — try developing your own neural
    network or experiment with more advanced convolutional neural networks.'
  id: totrans-115
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[关注](/@callum.bruce1) 和 [订阅](/@callum.bruce1/subscribe) 获取更多类似内容 — 与你的网络分享
    — 尝试开发你自己的神经网络或尝试更先进的卷积神经网络。'
- en: '*All images unless otherwise noted are by the author.*'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有说明，所有图片均由作者提供。*'
- en: '*Yann LeCun and Corinna Cortes hold the copyright for the MNIST database. The
    MNIST database is made available under the terms of the* [*Creative Commons Attribution-Share
    Alike 3.0 license*](https://creativecommons.org/licenses/by-sa/3.0/)*.*'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '*Yann LeCun 和 Corinna Cortes 拥有 MNIST 数据库的版权。MNIST 数据库根据* [*Creative Commons
    Attribution-Share Alike 3.0 license*](https://creativecommons.org/licenses/by-sa/3.0/)*的条款提供。*'
- en: References
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] GitHub (2023), [artificial_neural_network](https://github.com/c-bruce/artificial_neural_network)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] GitHub (2023), [artificial_neural_network](https://github.com/c-bruce/artificial_neural_network)'
- en: '[2] Bruce, C. (2023). [PID Controller Optimization: A Gradient Descent Approach](https://medium.com/towards-data-science/pid-controller-optimization-a-gradient-descent-approach-58876e14eef2).
    *Medium*'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Bruce, C. (2023). [PID Controller Optimization: A Gradient Descent Approach](https://medium.com/towards-data-science/pid-controller-optimization-a-gradient-descent-approach-58876e14eef2).
    *Medium*'
- en: '[3] Deng, L. (2012). [The MNIST database of handwritten digit images for machine
    learning research](https://ieeexplore.ieee.org/document/6296535). *IEEE Signal
    Processing Magazine*, *29*(6), 141–142'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Deng, L. (2012). [The MNIST database of handwritten digit images for machine
    learning research](https://ieeexplore.ieee.org/document/6296535). *IEEE Signal
    Processing Magazine*, *29*(6), 141–142'
- en: '[4] Nwankpa, C., Ijomah, W.L., Gachagan, A., & Marshall, S. (2018). [Activation
    Functions: Comparison of trends in Practice and Research for Deep Learning](https://arxiv.org/abs/1811.03378).
    *ArXiv, abs/1811.03378*'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Nwankpa, C., Ijomah, W.L., Gachagan, A., & Marshall, S. (2018). [Activation
    Functions: Comparison of trends in Practice and Research for Deep Learning](https://arxiv.org/abs/1811.03378).
    *ArXiv, abs/1811.03378*'
