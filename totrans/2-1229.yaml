- en: How to Program a Neural Network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/how-to-program-a-neural-network-f28e3f38e811](https://towardsdatascience.com/how-to-program-a-neural-network-f28e3f38e811)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A step-by-step guide to implementing a neural network from scratch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@callum.bruce1?source=post_page-----f28e3f38e811--------------------------------)[![Callum
    Bruce](../Images/4833a199a9449434777fdf5ce913a9cb.png)](https://medium.com/@callum.bruce1?source=post_page-----f28e3f38e811--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f28e3f38e811--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f28e3f38e811--------------------------------)
    [Callum Bruce](https://medium.com/@callum.bruce1?source=post_page-----f28e3f38e811--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f28e3f38e811--------------------------------)
    ¬∑14 min read¬∑Sep 23, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/79509eca917d69c1d6ca7ef6178adf3d.png)'
  prefs: []
  type: TYPE_IMG
- en: A neural network with three hidden layers
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will build a neural network from scratch and use it to classify
    handwritten digits.
  prefs: []
  type: TYPE_NORMAL
- en: Why reinvent the wheel/neural network, I hear you say? Can‚Äôt I just use my favourite
    machine learning framework and be done with it? Yes, there are many off-the-shelf
    frameworks that you can use to build a neural network (Keras, PyTorch, and TensorFlow
    to name a few). The thing with using one of these is that they make it easy for
    us to treat neural networks like black boxes.
  prefs: []
  type: TYPE_NORMAL
- en: This isn‚Äôt always a bad thing. Often we need this level of abstraction so that
    we can get to work on the problem at hand, but we should still strive to at least
    have a basic understanding of what is going on under the hood if we are to use
    neural networks in our work.
  prefs: []
  type: TYPE_NORMAL
- en: Building a neural network from scratch is, in my opinion, the best way to foster
    a deep understanding of how they work.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this article, you will have learned about the feedforward and
    backpropagation algorithms, what an activation function is, what the difference
    between an epoch and a batch is, and how to train a neural network. We will finish
    with an example by training a neural network to recognise handwritten digits.
  prefs: []
  type: TYPE_NORMAL
- en: All code used in this article is available here on [GitHub](https://github.com/c-bruce/artificial_neural_network)
    [1].
  prefs: []
  type: TYPE_NORMAL
- en: What is a neural network?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural networks, or artificial neural networks, are a type of machine learning
    algorithm. They form the core of many deep learning and artificial intelligence
    systems like computer vision, forecasting and speech recognition.
  prefs: []
  type: TYPE_NORMAL
- en: The structure of artificial neural networks is sometimes compared to the structure
    of biological neural networks in the brain. I would always urge caution not to
    draw too much from this comparison. Sure, artificial neural networks look a bit
    like biological neural networks but it is quite a big leap to start comparing
    them to something as complex as a human brain.
  prefs: []
  type: TYPE_NORMAL
- en: A neural network is made up of several layers of neurons. Each layer of neurons
    is activated based on the activations in the previous layer, a set of weights
    connecting the previous layer to the current layer and a set of biases applied
    to the neurons in the current layer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f38cca749159b2f7552d2330aa156b09.png)'
  prefs: []
  type: TYPE_IMG
- en: General structure of a neural network containing two hidden layers. Neurons
    are shaded by their activations (larger activation magnitude = darker neuron).
    Positive weights in red. Negative weights in blue. Line width indicates weight
    magnitude.
  prefs: []
  type: TYPE_NORMAL
- en: The first layer is the input layer. Input layer activations come from the input
    to the neural network. The final layer is the output layer. The activations in
    the output layer are the output of the neural network. The layers in between are
    called hidden layers.
  prefs: []
  type: TYPE_NORMAL
- en: A neural network is a generalized approximation of a function. Like any other
    function, when we give it an input it returns an output.
  prefs: []
  type: TYPE_NORMAL
- en: The novel thing about neural networks is *how* they get from the input to the
    output. The process is driven by how network weights and biases influence neuron
    activations and how these propagate through the network to eventually arrive at
    the output layer. The feedforward algorithm is used by neural networks to transform
    from our input into an output.
  prefs: []
  type: TYPE_NORMAL
- en: For a neural network to provide useful output we must first train it. When we
    train a neural network all that we are doing is iteratively adjusting the weights
    and biases to improve the accuracy of the output. We work out which direction
    and by how much we need to nudge the weights and biases using backpropagation
    and gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: Feedforward algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The feedforward algorithm transforms our neural network input into meaningful
    output. As its name suggests, the algorithm ‚Äúfeeds forward‚Äù information from one
    layer to the next.
  prefs: []
  type: TYPE_NORMAL
- en: To understand how it achieves this, let‚Äôs first zoom in and look at how information
    is passed from one layer to a single neuron in the next layer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/abb01642c9f7285b915f83047f7374c3.png)'
  prefs: []
  type: TYPE_IMG
- en: Weights connecting neurons in layer 0 with the first neuron in layer 1
  prefs: []
  type: TYPE_NORMAL
- en: 'The activation of the first neuron in the second layer, *a*‚ÇÄ‚ÅΩ¬π‚Åæ,is calculated
    by taking a weighted sum of the activations from the previous layer plus a bias
    and passing this through an activation function, *œÉ*(*x*):'
  prefs: []
  type: TYPE_NORMAL
- en: Equation for calculating *a*‚ÇÄ‚ÅΩ¬π‚Åæ
  prefs: []
  type: TYPE_NORMAL
- en: The superscript with round brackets denotes the layer index, starting from 0
    for the input layer. Activation (*a*)*,* and bias (*b*) subscriptsdenote neuron
    index. The first and second numbers in the weight (*w*) subscripts denote the
    index of the neuron the weight connects to (in the current layer) and from (in
    the previous layer) respectively.
  prefs: []
  type: TYPE_NORMAL
- en: The activation function determines if a neuron should be activated based on
    the input it receives. Common activation functions include sigmoid, tanh, rectified
    linear unit (ReLU), and softmax. To keep things simple, in our implementation,
    we will always use the sigmoid activation function.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/644584ee1f184d127aec522e9821bccb.png)'
  prefs: []
  type: TYPE_IMG
- en: Sigmoid, tanh and ReLU activation functions
  prefs: []
  type: TYPE_NORMAL
- en: 'The equation we used to calculate *a*‚ÇÄ‚ÅΩ¬π‚Åæ can be vectorised so that we can
    calculate all activations in the second layer:'
  prefs: []
  type: TYPE_NORMAL
- en: Vectorised equation for calculating ***a***‚ÅΩ¬π‚Åæ
  prefs: []
  type: TYPE_NORMAL
- en: Now we have neuron activations for the second layer ***a***‚ÅΩ¬π‚Åæ, we can use the
    same calculation to find ***a***‚ÅΩ¬≤‚Åæ and again for ***a***‚ÅΩ¬≥‚Åæ and so on‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs look at how this can be implemented in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The `Network` class contains all the information about our neural network. We
    initialise it by passing a list of integers relating to the number of neurons
    in each layer. For example, `network = Network([10, 3, 3, 2])` will create a network
    with ten neurons in the input layer, two hidden layers each containing three neurons
    and an output layer with two neurons.
  prefs: []
  type: TYPE_NORMAL
- en: The `__init_*` methods initialise values for activations, weights and biases.
    Activations and biases are initially all zero. Weights are given a random value
    between -1 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: The `feedforward` method loops through the layers calculating activations in
    each subsequent layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Below is an example using `feedforward` to calculate the output of our `[10,
    3, 3, 2]` network given a random input. The output is inspected by printing the
    activations in the final layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: That's it! We have successfully implemented the feedforward algorithm! Lets
    turn our attention to backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The backpropagation algorithm is the process through which a neural network
    learns from its mistakes.
  prefs: []
  type: TYPE_NORMAL
- en: In the above implementation of the feedforward algorithm, we initialise network
    weights with a random number between -1 and 1 and set all biases to equal 0\.
    With this initial setup, the output produced by the network for any given input
    will be essentially random.
  prefs: []
  type: TYPE_NORMAL
- en: 'What we need is a way to update the weights and biases so that the output of
    the network becomes more meaningful. To do this we use gradient descent:'
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent update step
  prefs: []
  type: TYPE_NORMAL
- en: Where ***a****‚Çô*is a vector of input parameters. The subscript *n* denotes iteration.
    *f(****a****‚Çô)* is a multi-variable cost function and ‚àá*f(****a****) is* the gradient
    of that cost function. ùõæ is the learning rate which determines by how much ***a****‚Çô*should
    be adjusted each iteration. I previously wrote an [article about gradient descent](https://medium.com/towards-data-science/pid-controller-optimization-a-gradient-descent-approach-58876e14eef2)
    [2] which goes into much more detail on gradient descent than I do here.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/pid-controller-optimization-a-gradient-descent-approach-58876e14eef2?source=post_page-----f28e3f38e811--------------------------------)
    [## PID Controller Optimization: A Gradient Descent Approach'
  prefs: []
  type: TYPE_NORMAL
- en: Using machine learning to solve engineering optimization problems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/pid-controller-optimization-a-gradient-descent-approach-58876e14eef2?source=post_page-----f28e3f38e811--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: In the case of a neural network, ***a****‚Çô*contains all of the network weights
    and biases, and *f(****a****‚Çô)* is the network cost (note *f(****a****‚Çô)* ‚â°*C)*.
    Here we define network cost using the L2-norm cost function calculated from the
    expected network output, *≈∑* and the actual network output, *y*. Both *≈∑* and
    *y* are vectors containing *n* values where *n* is the number of neurons in the
    output layer.
  prefs: []
  type: TYPE_NORMAL
- en: L2-norm cost function
  prefs: []
  type: TYPE_NORMAL
- en: Now we know how to calculate the cost function but we don‚Äôt yet know how to
    calculate the gradient of the cost function.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the gradient of the cost function is at the core of backpropagation.
    The gradient of the cost function tells us in which direction, and by how much,
    we need to nudge the weights and biases in our network to improve the accuracy
    of its output.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient of the cost function
  prefs: []
  type: TYPE_NORMAL
- en: To find these partial derivatives, neural networks employ [the chain rule](https://en.wikipedia.org/wiki/Chain_rule).
    This handy piece of high school calculus is a key to how neural networks work.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate how the chain rule is used in the backpropagation algorithm
    we will consider a network containing a single neuron in each layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/da2a39beb27b374c280ef8644e3ca660.png)'
  prefs: []
  type: TYPE_IMG
- en: Neural network with a single neuron in each layer
  prefs: []
  type: TYPE_NORMAL
- en: I have introduced a simplified version of the notation introduced earlier since
    we don‚Äôt need to index neurons in each layer in this example. Below, I also introduce
    a new variable, *z* which encapsulates the input to the activation function.
  prefs: []
  type: TYPE_NORMAL
- en: We start backpropagation from layer *L* which is the output layer and iterate
    back through the layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the output layer, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: C, a‚ÅΩ*·¥∏*‚Åæ and z‚ÅΩ*·¥∏*‚Åæ for the output layer of the single neuron per layer network
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have defined *C*, *a*‚ÅΩ*·¥∏*‚Åæ and *z*‚ÅΩ*·¥∏*‚Åæ for the output layer, we
    can calculate their derivatives and apply the chain rule to find ‚àÇ*C/*‚àÇ*w*‚ÅΩ*·¥∏*‚Åæ:'
  prefs: []
  type: TYPE_NORMAL
- en: Applying the chain rule to calculate ‚àÇ*C/*‚àÇw‚ÅΩ*·¥∏*‚Åæ
  prefs: []
  type: TYPE_NORMAL
- en: 'and similarly for ‚àÇ*C/*‚àÇ*b*‚ÅΩ*·¥∏*‚Åæ:'
  prefs: []
  type: TYPE_NORMAL
- en: Applying the chain rule to calculate ‚àÇ*C/*‚àÇ*b*‚ÅΩ*·¥∏*‚Åæ
  prefs: []
  type: TYPE_NORMAL
- en: 'and ‚àÇ*C/*‚àÇa‚ÅΩ*·¥∏*‚Åª¬π‚Åæ:'
  prefs: []
  type: TYPE_NORMAL
- en: Applying the chain rule to calculate ‚àÇ*C/*‚àÇa‚ÅΩ*·¥∏*‚Åª¬π‚Åæ
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have an expression for ‚àÇ*C/*‚àÇa‚ÅΩ*·¥∏*‚Åª¬π‚Åæ we can iterate back through
    the network to find how sensitive the cost function is to previous weights and
    biases:'
  prefs: []
  type: TYPE_NORMAL
- en: Sensitivity of the cost function to w‚ÅΩ*·¥∏*‚Åª¬π‚Åæ and b‚ÅΩ*·¥∏*‚Åª¬π‚Åæ
  prefs: []
  type: TYPE_NORMAL
- en: The equations we have defined for calculating the sensitivities to weight and
    biases in this simplified one-neuron-per-layer network stay essentially the same
    when we have more than one neuron per layer.
  prefs: []
  type: TYPE_NORMAL
- en: What changes is the derivative of the cost function with respect to the activations
    in the *L-1*·µó ∞ layer. This is because the cost function is influenced by these
    activations via multiple paths through the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define the derivative of the cost function with respect to activations in
    the *L-1*·µó ∞ layer as:'
  prefs: []
  type: TYPE_NORMAL
- en: Derivative of the cost function with respect to the k·µó ∞ activation in the L-1·µó ∞
    layer
  prefs: []
  type: TYPE_NORMAL
- en: where the subscripts *j* and *k* denote activations in the *L*·µó ∞ and *L-1*·µó ∞
    layers respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'The backpropagation algorithm is implemented in a method called `backpropagation`
    belonging to the `Network` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note that this method is vectorized to account for multiple neurons in each
    layer. dcost_dweights and dcost_dbiases are stored in arrays that are the same
    shape as the weights and biases arrays defined earlier. This makes applying gradient
    descent using these partial derivatives trivial. I also think it makes the code
    more readable.
  prefs: []
  type: TYPE_NORMAL
- en: As we step back through the network we apply the chain rule for each layer and
    calculate the sensitivity of the cost function to the weight and biases from each
    layer using the equations introduced in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Training a neural network to classify handwritten digits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having implemented the feedforward and backpropagation algorithms it is time
    to bring everything together and train a neural network for recognising handwritten
    digits.
  prefs: []
  type: TYPE_NORMAL
- en: To do this, we need a dataset of handwritten digits labelled with their corresponding
    value. Generating this dataset ourselves would be labour-intensive. Fortunately,
    databases already exist that can be used for this digit recognition problem. We
    will use the ModifiedNational Institute of Standards and Technology (MNIST) database*
    [3], a large database of labelled handwritten digits, for training our neural
    network.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/51074ebf5befa172c202becf4f5a722c.png)'
  prefs: []
  type: TYPE_IMG
- en: A random selection of samples from the MNIST database
  prefs: []
  type: TYPE_NORMAL
- en: The MNIST database contains 70000 labelled greyscale images of size 28 x 28
    pixels (784 total). Each labelled image in the database is called a *sample*.
    The MNIST database is split into *training* and *testing* subsets containing 60000
    and 10000 samples respectively.
  prefs: []
  type: TYPE_NORMAL
- en: As their names suggest, the training subset is used to train the network and
    the testing subset is used to test network accuracy. This way we can test network
    accuracy using samples that the network has never seen before.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we split the training subset into *batches*. For this example, I decided
    that each batch should contain 100 samples. There are 600 batches in total. The
    reason we split the training subset up into batches is that we don‚Äôt update network
    weights and biases after every sample. Instead, we update it after every batch.
    This way, when we apply gradient descent, rather than nudging weights and biases
    using a gradient based on a single sample, we use a gradient which is the average
    gradient calculated using all samples in a batch.
  prefs: []
  type: TYPE_NORMAL
- en: An *epoch* contains all batches in the training subset. An epoch loops through
    all of these batches. Choosing to train our network with a single epoch means
    that the network will only ‚Äúsee‚Äù each sample from the training subset once. Increasing
    the number of epochs means that the network will train on, and therefore ‚Äúsee‚Äù,
    each sample several times.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d39aaaa868b459bc69eef9ab96fec460.png)'
  prefs: []
  type: TYPE_IMG
- en: Training workflow diagram
  prefs: []
  type: TYPE_NORMAL
- en: A full definition of the `Network` class including all methods used in the training
    workflow is shown below. The `train_network` method is responsible for orchestrating
    the training workflow.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note that we use the AdaGrad gradient descent algorithm for updating network
    weights and biases. Adagrad adds a little more complexity than vanilla gradient
    descent but performs much better for this application.
  prefs: []
  type: TYPE_NORMAL
- en: Next we need to define the shape of the network. The 784 pixel values in each
    sample make up the activations in the input layer so the size of the input layer
    is set to 784\. Since we are categorising digits between 0 and 9 we also know
    that the output layer must contain 10 neurons. For the hidden layers, I found
    that two hidden layers each with 32 neurons worked well for this problem.
  prefs: []
  type: TYPE_NORMAL
- en: In total, there are 26432 weights and 74 biases in this network. This means
    that when we train the network we are optimizing in a 26506 dimension parameter
    space!
  prefs: []
  type: TYPE_NORMAL
- en: Don‚Äôt let the scale of this optimization task intimidate you. We have already
    done the hard work by implementing the feedforward and backpropagation algorithms
    and by defining the training workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Before training the network, a little bit of preparation work on the training
    data is required to split it up into batches. We can then call train_network to
    train the network. Finally, once the network is trained, we calculate network
    accuracy by checking the output of the network against the testing subset to see
    how many samples the network categorizes correctly.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: After training the network, network accuracy comes out to be 94.2%. Not bad
    for a neural network built from scratch!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, I have shown you how to build a simple neural network from
    scratch using Python.
  prefs: []
  type: TYPE_NORMAL
- en: We covered the implementation of the feedforward and backpropagation algorithms
    in detail, introduced the training workflow and trained a neural network with
    26432 weights and 74 biases to recognize handwritten digits from the MNIST database,
    achieving 94.2% network accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Better accuracy can be achieved by refining our implementation. For example,
    using the ReLU activation function for the hidden layers and softmax for the output
    layer has been shown to improve network accuracy [4].
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we could select a different form of gradient descent for adjusting
    the weights and biases which may lead us to find a more optimum minimum in our
    26506 dimension parameter space.
  prefs: []
  type: TYPE_NORMAL
- en: The process of flattening each sample into a 1D array also throws away a lot
    of important information. More advanced convolutional neural networks retain information
    about neighbouring pixels in images and typically perform better than the basic
    network type implemented here.
  prefs: []
  type: TYPE_NORMAL
- en: When I set out writing this article my aim was to produce a concise resource
    that somebody new to neural networks would be able to read and gain a basic understanding
    of how they work. I hope I have achieved this and in some small way encouraged
    you to continue learning about this extremely interesting topic.
  prefs: []
  type: TYPE_NORMAL
- en: Has this article helped you develop a deeper understanding of how neural networks
    work? Let me know in the comments.
  prefs: []
  type: TYPE_NORMAL
- en: '**Enjoyed reading this article?**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Follow](/@callum.bruce1) and [subscribe](/@callum.bruce1/subscribe) for more
    content like this ‚Äî share it with your network ‚Äî try developing your own neural
    network or experiment with more advanced convolutional neural networks.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*All images unless otherwise noted are by the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Yann LeCun and Corinna Cortes hold the copyright for the MNIST database. The
    MNIST database is made available under the terms of the* [*Creative Commons Attribution-Share
    Alike 3.0 license*](https://creativecommons.org/licenses/by-sa/3.0/)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] GitHub (2023), [artificial_neural_network](https://github.com/c-bruce/artificial_neural_network)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] Bruce, C. (2023). [PID Controller Optimization: A Gradient Descent Approach](https://medium.com/towards-data-science/pid-controller-optimization-a-gradient-descent-approach-58876e14eef2).
    *Medium*'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Deng, L. (2012). [The MNIST database of handwritten digit images for machine
    learning research](https://ieeexplore.ieee.org/document/6296535). *IEEE Signal
    Processing Magazine*, *29*(6), 141‚Äì142'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Nwankpa, C., Ijomah, W.L., Gachagan, A., & Marshall, S. (2018). [Activation
    Functions: Comparison of trends in Practice and Research for Deep Learning](https://arxiv.org/abs/1811.03378).
    *ArXiv, abs/1811.03378*'
  prefs: []
  type: TYPE_NORMAL
