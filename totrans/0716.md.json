["```py\n!pip install \"sagemaker==2.163.0\" --upgrade --quiet\n\nimport sagemaker\nimport boto3\nsess = sagemaker.Session()\n\nsagemaker_session_bucket=None\nif sagemaker_session_bucket is None and sess is not None:\n    # set to default bucket if a bucket name is not given\n    sagemaker_session_bucket = sess.default_bucket()\n\ntry:\n    role = sagemaker.get_execution_role()\nexcept ValueError:\n    iam = boto3.client('iam')\n    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n\nsess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n\nprint(f\"sagemaker role arn: {role}\")\nprint(f\"sagemaker session region: {sess.boto_region_name}\")\n```", "```py\nfrom sagemaker.huggingface import get_huggingface_llm_image_uri\n\n# retrieve the llm image uri\nllm_image = get_huggingface_llm_image_uri(\n  \"huggingface\",\n  version=\"0.8.2\"\n)\n\n# print ecr image uri\nprint(f\"llm image uri: {llm_image}\")\n```", "```py\nimport json\nfrom sagemaker.huggingface import HuggingFaceModel\n\n# Define Model and Endpoint configuration parameter\nconfig = {\n  'HF_MODEL_ID': \"decapoda-research/llama-7b-hf\", # model_id from hf.co/models\n  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n  'MAX_INPUT_LENGTH': json.dumps(1024),  # Max length of input text\n  'MAX_TOTAL_TOKENS': json.dumps(2048),  # Max length of the generation (including input text)\n}\n```", "```py\n# create HuggingFaceModel with the image uri\nllm_model = HuggingFaceModel(\n  role=role,\n  image_uri=llm_image,\n  env=config\n)\n```", "```py\ninstance_type = \"ml.g5.12xlarge\"\nnumber_of_gpu = 4\nhealth_check_timeout = 300\n\nllm = llm_model.deploy(\n  initial_instance_count=1,\n  instance_type=instance_type,\n  container_startup_health_check_timeout=health_check_timeout,\n)\n```", "```py\nllm.predict({\n    \"inputs\": \"My name is Julien and I like to\",\n})\n```"]