- en: Crafting Effective Prompts for Summarization Using Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/crafting-effective-prompts-for-summarization-using-large-language-models-dbbdf019f664](https://towardsdatascience.com/crafting-effective-prompts-for-summarization-using-large-language-models-dbbdf019f664)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Artificial intelligence and large language models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Distilling key points after >2 years of experience and from AI developers’ own
    tutorials, hands-on and with examples.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://lucianosphere.medium.com/?source=post_page-----dbbdf019f664--------------------------------)[![LucianoSphere
    (Luciano Abriata, PhD)](../Images/a8ae3085d094749bbdd1169cca672b86.png)](https://lucianosphere.medium.com/?source=post_page-----dbbdf019f664--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dbbdf019f664--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dbbdf019f664--------------------------------)
    [LucianoSphere (Luciano Abriata, PhD)](https://lucianosphere.medium.com/?source=post_page-----dbbdf019f664--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dbbdf019f664--------------------------------)
    ·8 min read·Oct 24, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5d9c95adc4b403b4cdbf2d4155e8083a.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Emiliano Vittoriosi](https://unsplash.com/@emilianovittoriosi?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: In an era of abundant information—think of the large numbers of articles published
    on every possible topic, the time spent at meetings and presentations, or the
    volume of e-mails you get everyday at work—the ability to distill complex content
    into concise, insightful summaries is invaluable. Summarization tools have existed
    for some time, but the recent advent of large language models such as those from
    the GPT, Bard, or LLaMa series among the most popular ones, has taken summarization
    to new levels. This is so because large language models do not work as fixed-rule
    summarizers but rather can “understand” the contents of the input text and produce
    succinct summaries with much more flexibility than tools not using language models.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, passing the right prompt to the language model can achieve not
    only plain summarization but also rephrasing in a specific style, changing between
    passive and active form, changing the narrative perspective, even understanding
    texts that contain pieces in different languages or tones and writing a consistent
    summary in a single tone and language. None of these is possible with summarization
    tools not powered by large language models, which rapidly became rather obsolete.
  prefs: []
  type: TYPE_NORMAL
- en: From my above statement, then, it is obvious that the key to harnessing the
    full potential of large language models in summarization lies in crafting prompts
    that guide them effectively. Here I will overview the most important points you
    must consider when writing prompts expected to produce high-quality summaries
    across a range of content types.
  prefs: []
  type: TYPE_NORMAL
- en: '**The power of prompts**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prompts serve as the interface between human intention and AI execution. They
    provide the necessary instructions and context for the language model to generate
    summaries that meet the user’s needs. This means they must be complete, from including
    explicit guidelines and requests on what you expect in its output to including
    relevant information that provides context. You will see examples of all these
    points through my article.
  prefs: []
  type: TYPE_NORMAL
- en: 'Effective prompts share common characteristics that make them powerful tools
    for extracting meaningful summaries. They are clear, specific, and tailored to
    the content and context of the task at hand. An ideal prompt leaves no room for
    ambiguity. It precisely articulates what the user wants to extract from the content.
    For example, instead of asking, ‘Summarize this article’ a more effective prompt
    would be something like:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Generate a point-by-point summary of the key findings and arguments in the
    following article.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Context is crucial in generating relevant summaries. Prompts should provide
    context about the source material, the target audience, and the desired level
    of detail. This ensures that the language model “understands” nuances in the content
    and thus can produce a summary that aligns with the user’s goals. For example,
    you can include in a prompt information explaining that the text to be summarized
    is the memo of a meeting, or a scientific article, or a collage of news about
    a topic, or a podcast, or an e-mail, or an example of the kind of output you expect.
    I have already written several programs that exploit GPT-3.5 or GPT-4 in ways
    specialized to make summaries in some of those situations, and I could always
    see a big impact of the context passed in the prompt, especially at the beginning
    of it. See for example these two articles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/control-web-apps-via-natural-language-by-casting-speech-to-commands-with-gpt-3-113177f4eab1?source=post_page-----dbbdf019f664--------------------------------)
    [## Control web apps via natural language by casting speech to commands with GPT-3'
  prefs: []
  type: TYPE_NORMAL
- en: One last article showcasing practical applications of GPT3, with a full explanation
    of the workflow and details on the…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/control-web-apps-via-natural-language-by-casting-speech-to-commands-with-gpt-3-113177f4eab1?source=post_page-----dbbdf019f664--------------------------------)
    [](/coupling-gpt-3-with-speech-recognition-and-synthesis-to-achieve-a-fully-talking-chatbot-that-runs-abfcb7bf580?source=post_page-----dbbdf019f664--------------------------------)
    [## Coupling GPT-3 with speech recognition and synthesis to achieve a fully talking
    chatbot that runs…
  prefs: []
  type: TYPE_NORMAL
- en: How I created this web app with which you can talk naturally with GPT-3 about
    any topic you want, all web-based in your…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/coupling-gpt-3-with-speech-recognition-and-synthesis-to-achieve-a-fully-talking-chatbot-that-runs-abfcb7bf580?source=post_page-----dbbdf019f664--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the task, prompts can also specify the desired level of abstraction
    in the summary. For instance, one might request a high-level overview, an in-depth
    analysis, a brief synthesis of key points, or a list of the key points, and so
    on. Prompts can also tune the scholarly level of the output; for example, one
    can ask a language model to summarize a hardcore scientific article “keeping scientific
    rigor” or “writing it for a 10 year old”, or to explain a piece of code “as pseudocode”
    or “only the basic ideas behind how it works”. Of course, the requested levels
    of abstraction and scholarly should align with the purpose of the summary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Case study: my Summarizer Almighty chatbot'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I developed a web app called “Summarizer Almighty” to tackle two problems at
    once: summarizing texts that are too long to fit as input to language models,
    and focusing on specific questions or instructions provided by the user -contrary
    to general summarization.'
  prefs: []
  type: TYPE_NORMAL
- en: As I wrote it in first place, Summarizer Almighty utilizes the GPT-3.5-turbo
    language model to analyze and extract relevant information from lengthy documents
    based on user-provided questions or instructions. I have just updated it to GPT-4,
    which is much more expensive to run but provides substantially better responses
    in many cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do its work, Summarizer Almighty splits the input document into overlapping
    chunks, which are individually analyzed and summarized through a special prompt
    of this type:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Can you reply to “[user’s question]” considering the following piece of text
    taken from the article under study? If not, reply ‘FALSE’ and nothing else. If
    yes, reply ‘TRUE’ followed by the answer. Here’s the paragraph: “[chunk of text
    extracted from the document, optimized to around 3000 tokens]”*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sending subsequently its inputs to the language model through its API, Summarizer
    Almighty accumulates partial answers into a growing paragraph that will contain
    several “FALSE” outputs and some “TRUE” outputs, the latter followed by explanations
    or answers as requested. At the very end, the program displays all partial outputs
    and then calls the language model once more, this time with an injected prompt
    that says:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Please answer to “[user’s question]” using the information provided below,
    taken from sections of a longer text: [concatenation of all answers provided by
    calls that returned TRUE].*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Accessible for free (but you must provide a working OpenAI API for GPT models),
    I think Summarizer Almighty can be valuable in research, news writing, education,
    business, and other situations where time is limited and detailed information
    is crucial, saving time and effort. To know more, please check out my dedicated
    article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://pub.towardsai.net/engineering-prompt-chains-with-language-models-to-craft-a-summarizer-almighty-web-app-7286de0b0a71?source=post_page-----dbbdf019f664--------------------------------)
    [## Engineering Prompt Chains With Language Models to Craft a “Summarizer Almighty”
    Web App'
  prefs: []
  type: TYPE_NORMAL
- en: This free tool analyzes documents of arbitrary lengths, looking for information
    about a question or instruction…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: pub.towardsai.net](https://pub.towardsai.net/engineering-prompt-chains-with-language-models-to-craft-a-summarizer-almighty-web-app-7286de0b0a71?source=post_page-----dbbdf019f664--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**From OpenAI’s own resources on how to craft better prompts**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenAI’s large language models are probably the most successful ones in the
    market out there right now. And objectively speaking, they were the first really
    good ones to roll out. Besides, they are in my opinion the easiest to use in practice
    -just through API calls and no installs. And the best documented ones…
  prefs: []
  type: TYPE_NORMAL
- en: 'Indeed, OpenAI’s website has several resources aimed at helping you to write
    better prompts. And it’s not only about general rules and formats to pass in the
    prompt, but also about using special tokens and instructions. For example, the
    page dedicated specifically to best practices for prompt engineering with OpenAI’s
    API explains specifically that instructions should rather be passed at the beginning
    (as I also explained earlier in this article based on my own experience), that
    you can use tokens such as triple quotes ””” or hashtags ### to separate instructions
    from context, that providing leading words and examples of the kinds of outputs
    you expect is very helpful, and much more:'
  prefs: []
  type: TYPE_NORMAL
- en: '[## Best practices for prompt engineering with OpenAI API | OpenAI Help Center'
  prefs: []
  type: TYPE_NORMAL
- en: How to give clear and effective instructions to GPT-3 and Codex
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: help.openai.com](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api?source=post_page-----dbbdf019f664--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenAI’s community forum also contains a section specifically designed to asking
    and answering questions about prompt engineering:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://community.openai.com/c/prompting/8?source=post_page-----dbbdf019f664--------------------------------)
    [## Prompting'
  prefs: []
  type: TYPE_NORMAL
- en: Learn more about prompting by sharing best practices, your favorite prompts,
    and more!
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: community.openai.com](https://community.openai.com/c/prompting/8?source=post_page-----dbbdf019f664--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**Iterative Refinement**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One important point I have observed in my vast experience, mainly with Google’s
    Bard and with language models of the GPT family, is that often, refining prompts
    iteratively is essential. You may need to experiment with different phrasings,
    instructions, or parameters to optimize the quality of the generated summaries.
    Very often, the first output is not what I want, specially when it has to do with
    summarization, because the language model can’t magically understand what your
    exact intentions are, where you want to focus, what tone you expect in the output,
    and other details that, as discussed above, you must inject into the model through
    the prompt. When this happens, that is most of the times, I need to either iterate
    with the language model a few times until I converge to what I expected, or try
    different starting prompts until I find the model provides the kinds of outputs
    I expect.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes you can get a first generation and feed it back to the model together
    with a new prompt designed to improve the output; but sometimes you need to start
    over from scratch by entering a totally new prompt. On a few occasions, checking
    the multiple outputs from the language model or re-running it on the same prompt
    can result in better generations, but this is often not the case.
  prefs: []
  type: TYPE_NORMAL
- en: 'One tool that helps regarding some aspects, especially about the quality of
    the outputs produced by GPT models when used programmatically, is keeping an eye
    on the probabilities associated to each produced token, as I have described here
    for GPT-3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/exploring-token-probabilities-as-a-means-to-filter-gpt-3s-answers-3e7dfc9ca0c?source=post_page-----dbbdf019f664--------------------------------)
    [## Exploring Token Probabilities as a Means to Filter GPT-3’s Answers'
  prefs: []
  type: TYPE_NORMAL
- en: To build better GPT-3-powered chatbots
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/exploring-token-probabilities-as-a-means-to-filter-gpt-3s-answers-3e7dfc9ca0c?source=post_page-----dbbdf019f664--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusions**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Effective prompts are the key to unlocking the full potential of summarization
    powered by large language models, ensuring that the produced summaries are relevant,
    concise, insightful, tailored to exactly what you need, and written the way you
    need. Well-crafted prompts lead to more precise and relevant summaries, ensuring
    that the computer program focuses on extracting the information that is relevant
    to you. Clear prompts reduce the need for extensive post-processing, saving valuable
    time.
  prefs: []
  type: TYPE_NORMAL
- en: Mastering the -as many call it and I don’t disagree- “art” of crafting prompts
    will be, or rather is already now, an essential skill in modern work. Whether
    it’s extracting insights from articles, research papers, meetings, or any other
    content, proper prompting guides your language model towards generating meaningful
    and actionable summaries. Put my advice into practice and check this out for yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Some related articles you may like
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[](https://medium.com/ido-imake/i-build-gpt-powered-tools-that-run-on-the-web-chatbots-summarizes-translators-e-mail-writers-78e303210cb?source=post_page-----dbbdf019f664--------------------------------)
    [## I build GPT-powered tools that run on the web (chatbots, summarizes, translators,
    e-mail writers'
  prefs: []
  type: TYPE_NORMAL
- en: Check out my examples, and you are free to contact me for jobs!
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/ido-imake/i-build-gpt-powered-tools-that-run-on-the-web-chatbots-summarizes-translators-e-mail-writers-78e303210cb?source=post_page-----dbbdf019f664--------------------------------)  [##
    All my articles on GPT-3 as of October 2022
  prefs: []
  type: TYPE_NORMAL
- en: My favorite language model and how to use it for multiple purposes in online
    applications with pure JavaScript and…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'lucianosphere.medium.com](https://lucianosphere.medium.com/all-my-articles-on-gpt-3-as-of-october-2022-10e95dcae199?source=post_page-----dbbdf019f664--------------------------------)
    [](https://medium.com/ido-imake/i-write-about-science-and-technology-part-1-my-main-content-on-data-science-artificial-5419e58ed7d1?source=post_page-----dbbdf019f664--------------------------------)
    [## I Write About Science and Technology -Part 1: My Main Content on Data Science,
    Artificial…'
  prefs: []
  type: TYPE_NORMAL
- en: Recently, I’ve started having job requests on writing about science and technology,
    one client focused on biology…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/ido-imake/i-write-about-science-and-technology-part-1-my-main-content-on-data-science-artificial-5419e58ed7d1?source=post_page-----dbbdf019f664--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '[***www.lucianoabriata.com***](https://www.lucianoabriata.com/) *I write about
    everything that lies in my broad sphere of interests: nature, science, technology,
    programming, etc.* [***Subscribe to get my new stories***](https://lucianosphere.medium.com/subscribe)
    ***by email****. To* ***consult about small jobs*** *check my* [***services page
    here***](https://lucianoabriata.altervista.org/services/index.html)*. You can*
    [***contact me here***](https://lucianoabriata.altervista.org/office/contact.html)***.***'
  prefs: []
  type: TYPE_NORMAL
