- en: Creating Your Personalized Voice Assistant with GPT and Whisper
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/creating-your-personalized-voice-assistant-with-gpt-and-whisper-ddb9bd8c419](https://towardsdatascience.com/creating-your-personalized-voice-assistant-with-gpt-and-whisper-ddb9bd8c419)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Step-by-Step Guide
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://donatoriccio.medium.com/?source=post_page-----ddb9bd8c419--------------------------------)[![Donato
    Riccio](../Images/0af2a026e72a023db4635522cbca50eb.png)](https://donatoriccio.medium.com/?source=post_page-----ddb9bd8c419--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ddb9bd8c419--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ddb9bd8c419--------------------------------)
    [Donato Riccio](https://donatoriccio.medium.com/?source=post_page-----ddb9bd8c419--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ddb9bd8c419--------------------------------)
    ·6 min read·May 18, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/91c085fb9d42bb3ace220b9baec4f77a.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Ivan Bandura](https://unsplash.com/@unstable_affliction?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: This article aims to guide you in creating a simple yet powerful voice assistant
    tailored to your preferences. We’ll use two powerful tools, Whisper and GPT, to
    make this happen. You probably already know GPT and how powerful it is, but *what
    is Whisper?*
  prefs: []
  type: TYPE_NORMAL
- en: Whisper is an advanced speech recognition model from OpenAI that offers accurate
    audio-to-text transcription.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll walk you through each step, with coding instructions included. At the
    end, you’ll have your very own voice assistant up and running.
  prefs: []
  type: TYPE_NORMAL
- en: Before you begin
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Open AI API keys
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you already have an OpenAI API key you can skip this section.
  prefs: []
  type: TYPE_NORMAL
- en: Both Whisper and GPT APIs require an OpenAI API key to be accessed. Unlike ChatGPT
    where the subscription is a fixed fee, the API key is paid based on how much you
    use the service.
  prefs: []
  type: TYPE_NORMAL
- en: The prices are reasonable. At the time of writing, Whisper is priced at $0.006
    / minute, GPT (with the model gpt-3.5-turbo) at $0.002 / 1K tokens (a token is
    approximately 0.75 words).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/90f919b32509289cf43eb40b7d2ef4a2.png)'
  prefs: []
  type: TYPE_IMG
- en: OpenAI’s website. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: To get your key, first create an account on the OpenAI website. After signing
    in, click on your name at the top-right corner and choose *View API keys*. Once
    you click *Create new secret key* your key is displayed. Make sure to save it,
    because you won’t be able to see it again.
  prefs: []
  type: TYPE_NORMAL
- en: Packages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The code chunk shows the required libraries for the project. The project involves
    using OpenAI’s Python library for AI tasks, **pyttsx3** for generating speech,
    **SoundDevice** for recording and playing back audio, **numpy** and **scipy**
    for mathematical operations. As always, you should create a new virtual environment
    before installing packages when starting a new project.
  prefs: []
  type: TYPE_NORMAL
- en: Code structure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our code will be structured around a single class, and take up approximately
    90 lines of code in total. It assumes that you have a basic understanding of Python
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1471474c58d6a02c0eb23166249a635c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `listen`method captures the user’s spoken input and converts it to text
    using Whisper. The `think`method sends the text to GPT, which generates a natural
    language response. The `speak`method converts the response text into an audio
    that is played back. The process repeats: the user is able to interact in a conversation
    by making another request.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bb587d1395678babdd36f258fffa8f0a.png)'
  prefs: []
  type: TYPE_IMG
- en: Code structure. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: __init__
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This function takes care of initializing the history and setting up the API
    key.
  prefs: []
  type: TYPE_NORMAL
- en: We need a history that keep track of the previous messages. It’s basically our
    assistant’s short-term memory, and allows it to remember what you said earlier
    in the conversation.
  prefs: []
  type: TYPE_NORMAL
- en: listen
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/220a3e3def47c2b30b9682c48feef6fb.png)'
  prefs: []
  type: TYPE_IMG
- en: The listen function. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: This method is our assistant’s ears.
  prefs: []
  type: TYPE_NORMAL
- en: The `listen` function allows to receive input from the user. This function records
    audio from your microphone and transcribes it into text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s what it does:'
  prefs: []
  type: TYPE_NORMAL
- en: Prints *Listening…* when recording audio.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Records audio for 3 seconds (or any duration you want) using sounddevice at
    a sample rate of 44100 Hz.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saves the recorded audio as a NumPy array in a temporary WAV file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uses the OpenAI API’s `transcribe`method to send the audio to Whisper, which
    transcribes it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prints the transcribed text to the console to confirm that the transcription
    was successful.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns the transcribed text as a string.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the example, the assistant listens for 3 seconds, but you can change the
    time as you want.
  prefs: []
  type: TYPE_NORMAL
- en: think
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/b88809c7d91c07d6363a1d3b71e75427.png)'
  prefs: []
  type: TYPE_IMG
- en: The think function. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Our assistant’s brain is powered by GPT. The think function receives what the
    assistant hears and elaborates a response. *How?*
  prefs: []
  type: TYPE_NORMAL
- en: The response is not created on your computer. The text needs to be sent to OpenAI’s
    servers to be processed through the APIs. The response is then saved in the response
    variable, and both the user message and the response are added to the history,
    the assistant’s short term memory. provide context to the GPT model for generating
    responses.
  prefs: []
  type: TYPE_NORMAL
- en: speak
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/38616ab8a0bee7bf85d420f72f7dd8b2.png)'
  prefs: []
  type: TYPE_IMG
- en: The speak function. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **speak** function is responsible for converting text into speech and playing
    it back to the user. This function takes a single parameter: text. It should be
    a string that represents the text to be converted to speech.'
  prefs: []
  type: TYPE_NORMAL
- en: When the function is called with a text string as an argument, it initializes
    the pyttsx3 speech engine with the command `engine = pyttsx3.init()` This object,
    `engine` is the main interface for converting text to speech.
  prefs: []
  type: TYPE_NORMAL
- en: The function then instructs the speech engine to convert the provided text into
    speech using the command `engine.say(text)`. This queues up the provided text
    to be spoken. The command `engine.runAndWait` tells the engine to process the
    queued command.
  prefs: []
  type: TYPE_NORMAL
- en: Pyttsx3 handles all text-to-speech conversion locally, which can be a significant
    advantage in terms of latency.
  prefs: []
  type: TYPE_NORMAL
- en: Final touches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The assistant is now ready. We just need to create an assistant object, and
    begin the conversation.
  prefs: []
  type: TYPE_NORMAL
- en: The conversation is an infinite loop that ends when the user says a sentence
    containing *Goodbye*.
  prefs: []
  type: TYPE_NORMAL
- en: Tips to Personalize Your Experience
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Customizing your GPT assistant is a breeze! The code that we built is very
    modular, and it allows you to customize it by adding a a variety of features.
    Here are some ideas to get you started:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Give a role to the assistant**: Change the initial prompt to make your assistant
    act as your English teacher, motivational speaker, or anything else you can think
    of! Check out [Awesome ChatGPT Prompts](https://github.com/f/awesome-chatgpt-prompts)
    for more ideas.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Change the language:** Want to use another language? No problem! Simply change
    *english* in the code to your desired language.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Build an app:** You can easily integrate the assistant in any application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Add personality:** Give your assistant a unique personality by adding custom
    responses or using different tones and language styles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integrate with other APIs**: Integrate your assistant with other APIs to
    provide more advanced functionality, such as weather forecasts or news updates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we explained how to retrieve your OpenAI API key and provided
    code examples for the listen, think, and speak functions that are used to capture
    user input, generate responses, and convert text to speech for playback.
  prefs: []
  type: TYPE_NORMAL
- en: With this knowledge, you may begin creating your own unique voice assistant
    that is suited to your specific demands. The possibilities are infinite, from
    creating a personal assistant to help with daily tasks, to building a voice-controlled
    automation system. You can access all the code in the linked [GitHub repo](https://github.com/reese3222/nanoassistant).
  prefs: []
  type: TYPE_NORMAL
- en: '*Enjoyed this article? Get weekly data science interview questions delivered
    to your inbox by subscribing to my newsletter,* [*The Data Interview*](https://thedatainterview.substack.com/)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Also, you can find me on* [*LinkedIn*](https://www.linkedin.com/in/driccio/)*.*'
  prefs: []
  type: TYPE_NORMAL
