- en: 'iMAP: Modeling 3D Scenes in Real-Time'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: iMAP：实时建模 3D 场景
- en: 原文：[https://towardsdatascience.com/imap-modeling-3d-scenes-in-real-time-6202365d80ee](https://towardsdatascience.com/imap-modeling-3d-scenes-in-real-time-6202365d80ee)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/imap-modeling-3d-scenes-in-real-time-6202365d80ee](https://towardsdatascience.com/imap-modeling-3d-scenes-in-real-time-6202365d80ee)
- en: Learning 3D environments with handheld RGB-D cameras
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用手持 RGB-D 相机学习 3D 环境
- en: '[](https://wolfecameron.medium.com/?source=post_page-----6202365d80ee--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----6202365d80ee--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6202365d80ee--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6202365d80ee--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----6202365d80ee--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wolfecameron.medium.com/?source=post_page-----6202365d80ee--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----6202365d80ee--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6202365d80ee--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6202365d80ee--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----6202365d80ee--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6202365d80ee--------------------------------)
    ·15 min read·May 12, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----6202365d80ee--------------------------------)
    ·15分钟阅读·2023年5月12日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/75c66b8fbe2eb3a7bc26faba5f1ea272.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75c66b8fbe2eb3a7bc26faba5f1ea272.png)'
- en: (Photo by [Brett Zeck](https://unsplash.com/@iambrettzeck?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/map?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: （照片由[Brett Zeck](https://unsplash.com/@iambrettzeck?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)提供，来源于[Unsplash](https://unsplash.com/s/photos/map?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)）
- en: So far, we have only seen offline approaches for modeling 3D scenes (e.g., [NeRF](https://cameronrwolfe.substack.com/p/understanding-nerfs),
    [SRNs](https://cameronrwolfe.substack.com/p/scene-representation-networks), [DeepSDF](https://cameronrwolfe.substack.com/p/3d-generative-modeling-with-deepsdf)
    [2, 3, 4]). Despite their impressive performance, these approaches require days,
    or even weeks, of computation time for the underlying neural networks to be trained.
    For example, NeRFs are trained for nearly two days for just representing a single
    scene. Plus, using the the neural net to evaluate a new scene viewpoint can be
    quite expensive too! With this in mind, we might wonder whether it’s possible
    to learn a scene representation a bit faster than this.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只见过用于建模 3D 场景的离线方法（例如，[NeRF](https://cameronrwolfe.substack.com/p/understanding-nerfs)、[SRNs](https://cameronrwolfe.substack.com/p/scene-representation-networks)、[DeepSDF](https://cameronrwolfe.substack.com/p/3d-generative-modeling-with-deepsdf)
    [2, 3, 4]）。尽管这些方法的表现令人印象深刻，但它们需要数天甚至数周的计算时间来训练底层的神经网络。例如，NeRFs 仅用于表示单个场景的训练时间接近两天。而且，使用神经网络评估新的场景视角也可能相当昂贵！鉴于此，我们可能会想知道是否有可能更快地学习场景表示。
- en: This question was explored in [1] with the proposal of iMAP, a real-time system
    for representing scenes and localizing (i.e., tracking the pose of) devices in
    the scene. To understand what this means, consider a camera that is moving through
    a scene and capturing the surrounding environment. The task of iMAP is to *(i)*
    take in this data, *(ii)* build a 3D representation of the scene being observed,
    and (iii) infer the location and orientation of the camera (i.e., the device)
    as it captures the scene!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 中探讨了这个问题，提出了 iMAP，一个用于实时表示场景和定位（即跟踪设备姿态）设备的系统。为了理解这意味着什么，考虑一个在场景中移动并捕捉周围环境的相机。iMAP
    的任务是 *(i)* 获取这些数据，*(ii)* 建立被观察场景的 3D 表示，并且 (iii) 推断相机（即设备）在捕捉场景时的位置和方向！'
- en: 'iMAP adopts an approach that is quite similar to NeRF [2] with a few differences:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: iMAP 采用了一种与 NeRF [2] 非常相似的方案，但有一些不同之处：
- en: It is based upon RGB-D data.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它基于 RGB-D 数据。
- en: A streaming setup is assumed.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设有一个流媒体设置。
- en: As such, the model receives both depth and color information as input. Additionally,
    the learning process begins from a completely random initialization, and iMAP
    must learn from new, incoming RGB-D images in real-time. Given this setup, iMAP
    is expected to *(i)* model the scene and *(ii)* predict the pose of the RGB-D
    camera for each incoming image (i.e., prior methods assume pose information as
    an input!). Despite this difficult training setup, iMAP can learn 3D representations
    of entire rooms in real-time!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，模型接收深度和颜色信息作为输入。此外，学习过程从完全随机的初始化开始，iMAP 必须实时学习新的 RGB-D 图像。鉴于这种设置，iMAP 被期望*(i)*
    模拟场景和 *(ii)* 预测每个输入图像的 RGB-D 相机姿态（即，先前的方法将姿态信息作为输入！）。尽管训练设置非常困难，iMAP 仍然可以实时学习整个房间的
    3D 表示！
- en: '![](../Images/9a4ab5b3928c2404d5db5e25b4f3a239.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9a4ab5b3928c2404d5db5e25b4f3a239.png)'
- en: (from [1])
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: '**why is this paper important?** This post is part of my series on deep learning
    for 3D shapes and scenes. This area was recently revolutionized by the proposal
    of NeRF [2]. With a NeRF representation, we can produce an arbitrary number of
    synthetic viewpoints of a scene or even generate 3D representations of relevant
    objects; see below.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么这篇论文很重要？** 这篇文章是我关于 3D 形状和场景深度学习系列的一部分。该领域最近被 NeRF [2] 的提议所革新。通过 NeRF
    表示，我们可以生成场景的任意数量的合成视点，甚至生成相关对象的 3D 表示；见下文。'
- en: iMAP was proposed slightly after NeRFs, and it is capable of producing high-quality
    scene representations without requiring several days of training time. Compared
    to NeRF, iMAP learns in a cheap, on-the-fly manner and still performs relatively
    well.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: iMAP 在 NeRF 之后稍晚提出，它能够在不需要几天训练时间的情况下产生高质量的场景表示。与 NeRF 相比，iMAP 以一种廉价、即刻的方式进行学习，并且表现相对较好。
- en: Background
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: 'We have seen some important background concepts in prior overviews in this
    series that will be relevant here:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本系列的先前概述中看到了一些重要的背景概念，这些概念将在此处相关：
- en: Feed-forward neural networks [[link](https://cameronrwolfe.substack.com/i/94634004/feed-forward-neural-networks)]
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前馈神经网络 [[link](https://cameronrwolfe.substack.com/i/94634004/feed-forward-neural-networks)]
- en: Representing 3D shapes [[link](https://cameronrwolfe.substack.com/i/94634004/representing-d-shapes)]
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表示 3D 形状 [[link](https://cameronrwolfe.substack.com/i/94634004/representing-d-shapes)]
- en: Camera poses [[link](https://cameronrwolfe.substack.com/i/97472888/background)]
    (scroll to “camera viewpoints” sub-header)
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相机姿态 [[link](https://cameronrwolfe.substack.com/i/97472888/background)]（滚动到“相机视点”子标题）
- en: What are “frames” in a video? [[link](https://cameronrwolfe.substack.com/i/73746328/how-is-video-data-structured)]
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视频中的“帧”是什么？ [[link](https://cameronrwolfe.substack.com/i/73746328/how-is-video-data-structured)]
- en: To have all the context necessary for understanding iMAP, we need to quickly
    cover the concepts of SLAM systems and online learning.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解 iMAP 所需的所有背景，我们需要快速介绍 SLAM 系统和在线学习的概念。
- en: What is SLAM?
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 SLAM？
- en: 'Prior scene representation methods we have seen use deep neural networks to
    form an implicit representation of an underlying shape or scene by learning from
    available observations (e.g., point cloud data, images, etc.) of 3D space. iMAP
    is a bit different, because it is a *Simultaneous Localization and Mapping* (SLAM)
    system. This means that iMAP performs two tasks:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们见过的先前场景表示方法使用深度神经网络通过从 3D 空间的可用观察数据（例如，点云数据、图像等）中学习来形成对底层形状或场景的隐式表示。iMAP 有点不同，因为它是一个*同时定位与映射*（SLAM）系统。这意味着
    iMAP 执行两个任务：
- en: '*Localization*: tracking the location and pose of the camera that’s capturing
    the underlying scene.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*定位*：跟踪捕捉底层场景的相机的位置和姿态。'
- en: '*Mapping*: forming a representation of the underlying scene.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*映射*：形成对底层场景的表示。'
- en: Most prior techniques we have seen only perform mapping, but iMAP goes beyond
    these techniques by also performing localization. Namely, viewpoints of the underlying
    scene are passed in real-time to the iMAP system, which both maps the underlying
    scene and predicts the camera’s trajectory as it traverses the scene.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们见过的大多数先前技术仅执行映射，但 iMAP 通过同时执行定位而超越了这些技术。即，底层场景的视点实时传递给 iMAP 系统，该系统既映射底层场景，又预测相机在场景中的轨迹。
- en: '![](../Images/f89852bd7df7ba16c93485b2b9318a50.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f89852bd7df7ba16c93485b2b9318a50.png)'
- en: (from [1])
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: The output of a SLAM system, as shown above, has two components. A 3D representation
    of the scene is generated. Within this representation, however, we can see the
    camera’s trajectory as it captures the scene, shown by the yellow line (camera
    position) and associated 3D bounding boxes (camera pose).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: SLAM 系统的输出，如上所示，有两个组成部分。生成了场景的 3D 表示。在这个表示中，我们可以看到相机在捕捉场景时的轨迹，通过黄色线（相机位置）和相关的
    3D 边界框（相机姿态）表示。
- en: '**how is this different?** Beyond predicting camera poses on incoming data,
    SLAM systems are different from what we have seen so far because of the manner
    in which they receive data. Namely, with prior methods we *(i)* get a bunch of
    images of a scene and *(ii)* train a neural network over these images to model
    the scene. Instead, SLAM systems receive data in a streaming fashion. As the system
    receives new images, it must take them in, predict a pose, and update its underlying
    scene representation in real-time.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**这有什么不同？** 除了对输入数据预测相机姿态之外，SLAM 系统与我们目前见到的其他方法不同，主要在于它们接收数据的方式。也就是说，以前的方法中我们
    *(i)* 获取一组场景的图像，并 *(ii)* 在这些图像上训练一个神经网络来建模场景。而 SLAM 系统则以流式的方式接收数据。当系统接收到新的图像时，它必须处理这些图像，预测一个姿态，并实时更新其基础场景表示。'
- en: Online learning
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在线学习
- en: '![](../Images/a3556729b4c82efa4e65678dc2753caa.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a3556729b4c82efa4e65678dc2753caa.png)'
- en: The offline training process (created by author)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 离线训练过程（由作者创建）
- en: Typically, deep neural networks are trained in an offline fashion. We have a
    large, static training dataset available, and we allow our neural network to perform
    several training passes (or epochs) over this dataset; see above. But, *what happens
    when our dataset is not static*? For example, we may be receiving new data in
    real time or correcting labels applied to existing data.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，深度神经网络是以离线方式进行训练的。我们有一个大的、静态的训练数据集，并允许我们的神经网络对这个数据集进行多次训练（或训练周期）；见上文。但，*当我们的数据集不是静态的*
    时会发生什么？例如，我们可能会实时接收新数据或纠正应用于现有数据的标签。
- en: '![](../Images/d894978fbc726e88dcf7d7c195e78c15.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d894978fbc726e88dcf7d7c195e78c15.png)'
- en: Terminology for online and offline training setups (created by author)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在线和离线训练设置的术语（由作者创建）
- en: We will refer to this setting generally as “online learning”, which refers to
    the fact that we are sequentially receiving new data with which to train a neural
    network.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常将这种设置称为“在线学习”，这指的是我们按顺序接收新的数据以训练神经网络。
- en: Many different types of online learning exist; see above. For example, incremental
    learning assumes that the neural network sequentially receives new batches of
    data, while streaming learning mandates that the network receives data one example
    at a time. All forms of online learning assume that data is learned in one pass
    — we can’t “look back” at old data after receiving new data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 存在许多不同类型的在线学习；见上文。例如，增量学习假设神经网络按顺序接收新的数据批次，而流式学习则要求网络一次接收一个样本。所有形式的在线学习都假设数据是在一次通过中学习的——我们不能在接收到新数据后“回顾”旧数据。
- en: '**catastrophic forgetting.** Online learning is considered more difficult than
    offline learning because we never have access to the full dataset during training.
    Rather, we must learn over small subsets of data that are made available sequentially.
    If the incoming data is non-[i.i.d.](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables),
    our neural network may suffer from [catastrophic forgetting](https://cameronrwolfe.substack.com/i/73746325/catastrophic-forgetting).
    As discussed in [6], catastrophic forgetting refers to the neural network completely
    forgetting about older data as it learns from new, incoming data.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**灾难性遗忘。** 在线学习被认为比离线学习更困难，因为在训练过程中我们从未能够访问完整的数据集。相反，我们必须在顺序提供的小数据子集上进行学习。如果接收到的数据是非[独立同分布的](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables)，我们的神经网络可能会遭受[灾难性遗忘](https://cameronrwolfe.substack.com/i/73746325/catastrophic-forgetting)。如[6]中所讨论的，灾难性遗忘指的是神经网络在从新数据中学习时，完全遗忘了较旧的数据。'
- en: '![](../Images/934f35816d31dceebcdce8012a461a10.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/934f35816d31dceebcdce8012a461a10.png)'
- en: Depiction of catastrophic forgetting (created by author)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 灾难性遗忘的描述（由作者创建）
- en: For example, if we are learning to classify cats and dogs, maybe we sequentially
    receive a lot of data that only has pictures of cats (e.g., this could happen
    on an IoT doorbell camera at someone’s house!). In this case, the neural network
    would learn from only pictures of cats for a long time, causing it to catastrophically
    forget the concept of a dog. This would not be a problem if the incoming data
    was distributed equally between cats and dogs. Forgetting occurs because incoming
    data is non-i.i.d., and online learning techniques aim to avoid such forgetting.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们正在学习分类猫和狗，也许我们会依次收到大量只有猫的图片的数据（例如，这可能发生在某人的IoT门铃摄像头上！）。在这种情况下，神经网络会长时间只学习猫的图片，导致它灾难性地忘记狗的概念。如果输入的数据在猫和狗之间均匀分布，这将不是问题。遗忘发生是因为输入数据是非独立同分布的，而在线学习技术旨在避免这种遗忘。
- en: '**replay buffers.** One popular method of avoiding catastrophic forgetting
    is via a replay buffer. At a high level, replay buffers just store a cache of
    data that has been observed in the past. Then, when the network is updated over
    new data, we can sample some data from the replay buffer as well. This way, we
    get an equal sampling of data that the neural network has seen; see below.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**重放缓冲区。** 避免灾难性遗忘的一种流行方法是通过重放缓冲区。从高层次来看，重放缓冲区只存储过去观察到的数据缓存。然后，当网络在新数据上更新时，我们也可以从重放缓冲区中抽取一些数据。这样，我们就能获得神经网络曾见过的数据的均等采样；见下文。'
- en: '![](../Images/30a4a16366bce6c5def67d01d3beefd7.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/30a4a16366bce6c5def67d01d3beefd7.png)'
- en: Basic depiction of a replay mechanism (created by author)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 重放机制的基本示意图（由作者创建）
- en: All online learning techniques share the common goal of maximizing a neural
    network’s performance by minimizing the impact of catastrophic forgetting. Although
    replay buffers are widely used, simple, and quite effective, many other techniques
    exist as well — online learning is an active area of research within the deep
    learning community. For an overview of some existing techniques, I recommend reading
    my summaries below!
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 所有在线学习技术的共同目标是通过最小化灾难性遗忘的影响来最大化神经网络的性能。虽然重放缓冲区被广泛使用、简单且非常有效，但仍然存在许多其他技术——在线学习是深度学习社区中的一个活跃研究领域。关于一些现有技术的概述，我建议阅读我下面的总结！
- en: Overview of Online Learning Techniques [[link](https://cameronrwolfe.substack.com/p/a-broad-and-practical-exposition-of-online-learning-techniques-a4cbc300dcd4)]
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在线学习技术概述 [[link](https://cameronrwolfe.substack.com/p/a-broad-and-practical-exposition-of-online-learning-techniques-a4cbc300dcd4)]
- en: Overview of Streaming Learning Techniques [[link](https://cameronrwolfe.substack.com/p/how-to-train-deep-neural-networks-over-data-streams-fdab15704e66)]
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式学习技术概述 [[link](https://cameronrwolfe.substack.com/p/how-to-train-deep-neural-networks-over-data-streams-fdab15704e66)]
- en: '**why should we care about online learning?** iMAP is a SLAM system. Fundamentally,
    SLAM systems are quite related to online learning, as they are expected to receive
    a stream of scene images and provide tracking and mapping results in return. Notably,
    iMAP is an online learning-based technique that relies upon a replay buffer to
    represent and track an underlying scene in real-time!'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们为什么要关注在线学习？** iMAP 是一个 SLAM 系统。根本上，SLAM 系统与在线学习有很大关系，因为它们需要接收场景图像流并提供跟踪和映射结果。值得注意的是，iMAP
    是一种基于在线学习的技术，依靠重放缓冲区实时表示和跟踪基础场景！'
- en: How does iMAP work?
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: iMAP 如何运作？
- en: After first learning about iMAP, it might seem like the method is too good to
    be true. First of all, iMAP is solving a harder problem than prior work — camera
    pose information is predicted from RGB-D data rather than given. Then, we must
    also learn the entire scene representation in real-time, *instead of training
    for several days*? There’s no way that this is possible…
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在首次了解 iMAP 后，它可能看起来好得难以置信。首先，iMAP 解决了比之前的工作更难的问题——相机位姿信息是从 RGB-D 数据中预测的，而不是给定的。然后，我们还必须实时学习整个场景表示，*而不是训练几天*？这简直不可能……
- en: '![](../Images/2d86eaadd9f577107934fcf2f66e70cb.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d86eaadd9f577107934fcf2f66e70cb.png)'
- en: (from [1])
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: (来源于 [1])
- en: 'But, it is! iMAP does all of this via a two-part processing pipeline that includes:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 但确实是这样！iMAP 通过包括两个部分的处理管道完成所有这些：
- en: '*Tracking*: predicts the location and pose of the camera as it moves through
    the scene.'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*跟踪*：预测相机在场景中移动时的位置和姿态。'
- en: '*Mapping*: learns the 3D scene representation.'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*映射*：学习 3D 场景表示。'
- en: These two components run in parallel as an RGB-D camera captures the underlying
    scene from various viewpoints. Notably, tracking has to run quite fast, as we
    are trying to actively localize the camera as new data is coming in. The mapping
    component is done in parallel, but it only operates on keyframes that are really
    important to representing the underlying scene, which allows iMAP to learn in
    real-time. Let’s get into some of the details!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个组件并行运行，因为 RGB-D 相机从不同视角捕捉底层场景。值得注意的是，跟踪必须运行得非常快，因为我们尝试在新数据到来时主动定位相机。映射组件也是并行进行的，但它只在对表示底层场景真正重要的关键帧上操作，这使得
    iMAP 能够实时学习。让我们深入了解一些细节吧！
- en: '![](../Images/f502f9fb75ba9e9b63b5b350b43f7b3c.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f502f9fb75ba9e9b63b5b350b43f7b3c.png)'
- en: iMAP network architecture (created by author)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: iMAP 网络架构（作者创建）
- en: '**the network.** The first step in understanding iMAP is learning about the
    neural network upon which it is based. Like most prior work, iMAP uses a feed-forward
    network architecture. The network takes a 3D coordinate as input and produces
    an RGB color and a volume density (i.e., captures opaqueness) as output; see above.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**网络。** 理解 iMAP 的第一步是了解其基于的神经网络。与大多数先前的工作一样，iMAP 使用前馈网络架构。该网络将 3D 坐标作为输入，并产生
    RGB 颜色和体积密度（即，捕捉不透明度）作为输出；见上文。'
- en: Notably, iMAP’s network does not take a [viewing angle](https://cameronrwolfe.substack.com/i/97915766/modeling-nerfs)
    as input, as in NeRF [2], because it does not attempt to model view-dependent
    effects (e.g., reflections). Similar to NeRF [2], iMAP converts coordinates into
    higher-dimensional positional embeddings before passing them as input, following
    the approach of [5]; see below.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，iMAP 的网络不将 [视角](https://cameronrwolfe.substack.com/i/97915766/modeling-nerfs)作为输入，正如
    NeRF [2] 中那样，因为它不尝试建模视角相关效应（例如反射）。与 NeRF [2] 相似，iMAP 在将坐标作为输入之前将其转换为更高维的位置信息嵌入，遵循
    [5] 的方法；见下文。
- en: '![](../Images/444726a8874e6203d1e9b3e393eadce1.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/444726a8874e6203d1e9b3e393eadce1.png)'
- en: iMAP network architecture with positional embedding (created by author)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: iMAP 网络架构与位置信息嵌入（作者创建）
- en: '**quick note on rendering.** Assuming access to camera poses (we will learn
    how iMAP predicts these later), we can evaluate this network over many different
    spatial locations, then aggregate color and depth information into a rendering
    of the underlying scene using an approach like [Ray Marching](https://michaelwalczyk.com/blog-ray-marching.html).
    The rendering approach of [1] is similar to [that of NeRF](https://www.heavy.ai/technical-glossary/volume-rendering),
    but we want to include depth information in our rendering (i.e., render an RGB-D
    image). This makes the rendering process slightly different, but the basic idea
    is the same, and the entire process is still differentiable.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**关于渲染的快速说明。** 假设可以访问相机姿态（我们稍后会学习 iMAP 如何预测这些姿态），我们可以在许多不同的空间位置上评估这个网络，然后使用类似于
    [Ray Marching](https://michaelwalczyk.com/blog-ray-marching.html) 的方法将颜色和深度信息聚合成底层场景的渲染图像。
    [1] 的渲染方法类似于 [NeRF](https://www.heavy.ai/technical-glossary/volume-rendering)
    的方法，但我们希望在我们的渲染中包括深度信息（即，渲染 RGB-D 图像）。这使得渲染过程略有不同，但基本思路是相同的，整个过程仍然是可微分的。'
- en: '**how do we optimize this?** iMAP is trying to learn:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们如何优化这个？** iMAP 正在尝试学习：'
- en: The parameters of the feed-forward scene network.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前馈场景网络的参数。
- en: Camera poses for incoming frames from the RGB-D camera.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自 RGB-D 相机的输入帧的相机姿态。
- en: 'To train the iMAP system to predict this information accurately, there are
    two types of loss metric upon which we rely:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练 iMAP 系统以准确预测这些信息，我们依赖两种类型的损失度量：
- en: '*Photometric*: error in RGB pixel values.'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*光度学*：RGB 像素值中的误差。'
- en: '*Geometric*: error in depth information.'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*几何学*：深度信息中的误差。'
- en: These errors are computed by using iMAP to render an RGB-D view of the underlying
    scene, then comparing this rendering to the ground truth samples being captured
    from the camera. To make this comparison more efficient, we only consider a subset
    of pixels within the RGB-D image; see below.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这些误差是通过使用 iMAP 渲染底层场景的 RGB-D 视图计算的，然后将此渲染结果与从相机捕获的地面真实样本进行比较。为了使这个比较更高效，我们仅考虑
    RGB-D 图像中的一部分像素；见下文。
- en: '![](../Images/6df94e85f80eb6fd2e4e975db1ad92ad.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6df94e85f80eb6fd2e4e975db1ad92ad.png)'
- en: (from [1])
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: To optimize photometric and geometric error jointly, we just combine them via
    a weighted sum; see below.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了共同优化光度和几何误差，我们只是通过加权和将它们结合起来；见下文。
- en: '![](../Images/18c21e536405bda35532af40eda086b3.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/18c21e536405bda35532af40eda086b3.png)'
- en: (from [1])
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: 'To render an image with iMAP, we must:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 要用 iMAP 渲染图像，我们必须：
- en: Use the scene network to obtain color and geometry information.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用场景网络来获取颜色和几何信息。
- en: Infer the correct camera pose.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 推断正确的相机姿态。
- en: Render and RGB-D viewpoint based on this combined information.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于这些综合信息进行渲染和RGB-D视点的生成。
- en: All of these steps are differentiable, so we can optimize the photometric and
    geometric loss with techniques like [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent),
    thus training the system to produce renderings that closely match the ground truth
    RGB-D images.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些步骤都是可微的，因此我们可以使用诸如[随机梯度下降](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)之类的技术来优化光度和几何损失，从而训练系统生成与真实RGB-D图像紧密匹配的渲染结果。
- en: '**tracking and mapping.** The goal of tracking in iMAP is to learn the pose
    of the camera that’s actively capturing the underlying scene. Given that we have
    to do this for every incoming RGB-D frame from the camera, tracking must be efficient.
    iMAP handles tracking by *(i)* freezing the scene network and *(ii)* solving for
    the current RGB-D frame’s optimal pose given the fixed network. This is just an
    initial estimate of the camera pose that we generate as efficiently as possible
    — we may refine this pose later on.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**跟踪和映射。** 在iMAP中，跟踪的目标是学习正在主动捕捉底层场景的相机姿态。由于我们必须对来自相机的每个输入RGB-D帧进行此操作，跟踪必须高效。iMAP通过*(i)*
    冻结场景网络和*(ii)* 在固定网络的基础上求解当前RGB-D帧的最佳姿态来处理跟踪。这只是我们尽可能高效生成的相机姿态的初步估计——我们可能会在后续进行姿态精细化。'
- en: The goal of mapping is to jointly optimize the scene network and camera poses,
    implicitly creating an accurate representation of the scene. Trying to do this
    over all incoming, RGB-D camera frames would be way too expensive. Instead, we
    maintain a set of keyframes based on importance (i.e., whether they capture a
    “new” part of the scene). Within this set of keyframes, we refine estimated camera
    poses and train the scene network to produce renderings that closely match selected
    keyframes.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 映射的目标是联合优化场景网络和相机姿态，从而隐式地创建场景的准确表示。尝试在所有输入的RGB-D相机帧上进行这一操作将会非常昂贵。因此，我们基于重要性（即是否捕捉到场景的“新”部分）维护一组关键帧。在这组关键帧中，我们精细化估计的相机姿态，并训练场景网络以生成与所选关键帧紧密匹配的渲染图像。
- en: '![](../Images/17243f9e7e07aaf6506bf64cbbf7f770.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/17243f9e7e07aaf6506bf64cbbf7f770.png)'
- en: (from [1])
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: (来源于 [1])
- en: '**putting it all together.** The full iMAP framework is depicted above. Tracking
    is performed on each incoming frame to predict camera pose information. The feed-forward
    network is frozen during this process to make tracking more efficient. If a frame
    is selected as a keyframe, it is added to the set of keyframes that have their
    poses refined and are used to train the scene network.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**综合所有内容。** 上图展示了完整的iMAP框架。对每个输入帧进行跟踪，以预测相机姿态信息。在此过程中，前馈网络保持冻结，以提高跟踪效率。如果一个帧被选为关键帧，它会被添加到关键帧集合中，并对这些帧的姿态进行精细化，并用于训练场景网络。'
- en: '![](../Images/195bcd1deec2a61750e5790438cfcbbf.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/195bcd1deec2a61750e5790438cfcbbf.png)'
- en: (from [1])
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: (来源于 [1])
- en: To make it more efficient, mapping runs in parallel to the tracking process
    and only samples training data from the set of keyframes, which is much smaller
    than the total number of incoming images. Plus, the loss is only computed over
    a small subset of pixels, sampled using a hierarchical strategy (i.e., active
    sampling) that identifies regions of the image with the highest loss values (i.e.,
    dense or detailed regions in the scene) and prioritizes sampling in these areas;
    see above.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高效率，映射过程与跟踪过程并行运行，并仅从关键帧集合中采样训练数据，该集合比总的输入图像数量要小得多。此外，损失只在一个小的像素子集上计算，这些像素子集通过层次化策略（即主动采样）来选择，该策略识别图像中具有最高损失值的区域（即场景中的密集或详细区域），并优先在这些区域进行采样；见上文。
- en: '**online learning.** Recall that iMAP is randomly-initialized at first. As
    the RGB-D camera moves through a scene, iMAP *(i)* tracks the camera via the tracking
    module and *(ii)* begins selecting keyframes to update the mapping module. This
    set of keyframes acts as a replay buffer for iMAP. Namely, the feed-forward scene
    network (and associated camera poses) are trained in an online fashion by aggregating
    relevant keyframes and performing updates over this data in parallel to the tracking
    process. Each update in the mapping process considers five frames: three random
    keyframes, the latest keyframe, and the current, incoming RGB-D frame.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**在线学习**。请记住，iMAP 初始时是随机初始化的。当 RGB-D 相机在场景中移动时，iMAP *(i)* 通过跟踪模块跟踪相机，并 *(ii)*
    开始选择关键帧以更新映射模块。这组关键帧作为 iMAP 的重播缓冲区。也就是说，前馈场景网络（及相关相机姿态）通过聚合相关的关键帧并在跟踪过程中并行更新这些数据，以在线方式进行训练。映射过程中的每次更新考虑五个帧：三个随机关键帧、最新的关键帧以及当前的
    RGB-D 帧。'
- en: '![](../Images/f4165fe3b76b2a4adb41ad27d8c4eb5a.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f4165fe3b76b2a4adb41ad27d8c4eb5a.png)'
- en: (from [1])
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: The feed-forward scene network does not suffer from catastrophic forgetting
    because it samples training data from a diverse set of keyframes instead of training
    directly over the incoming data stream only. Unlike vanilla replay mechanisms,
    however, iMAP follows a specific strategy for sampling training data from the
    replay buffer. In particular, keyframes with higher loss values are prioritized
    (i.e., keyframe active sampling); see above.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈场景网络不会遭受灾难性遗忘，因为它从多样的关键帧集合中采样训练数据，而不是仅仅在接收到的数据流上进行训练。然而，与普通的重播机制不同，iMAP 遵循特定的策略，从重播缓冲区中采样训练数据。特别是，具有较高损失值的关键帧被优先考虑（即，关键帧主动采样）；见上文。
- en: Does it actually work?
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它真的有效吗？
- en: iMAP is evaluated and compared to several traditional SLAM baselines across
    both real-world (i.e., from a handheld RGB-D camera) and synthetic scene datasets
    (e.g., the [Replica](https://github.com/facebookresearch/Replica-Dataset) dataset).
    Notably, iMAP processes every incoming frame at a frequency of 10 Hz. For evaluation,
    mesh reconstructions can be recovered (and compared to ground truth if available)
    by querying the neural network over a [voxel grid](https://cameronrwolfe.substack.com/i/94634004/representing-d-shapes)
    and running the [marching cubes](https://graphics.stanford.edu/~mdfisher/MarchingCubes.html#:~:text=Marching%20cubes%20is%20a%20simple,a%20region%20of%20the%20function.)
    algorithm.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: iMAP 在真实世界（即来自手持 RGB-D 相机）和合成场景数据集（例如，[Replica](https://github.com/facebookresearch/Replica-Dataset)
    数据集）上进行了评估和与几个传统 SLAM 基线的比较。值得注意的是，iMAP 以 10 Hz 的频率处理每个接收的帧。为了评估，可以通过在 [体素网格](https://cameronrwolfe.substack.com/i/94634004/representing-d-shapes)
    上查询神经网络并运行 [marching cubes](https://graphics.stanford.edu/~mdfisher/MarchingCubes.html#:~:text=Marching%20cubes%20is%20a%20simple,a%20region%20of%20the%20function.)
    算法来恢复网格重建（如果有地面真值则可以进行比较）。
- en: '![](../Images/5ab976d095717c5cdbe5caf6642110de.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5ab976d095717c5cdbe5caf6642110de.png)'
- en: (from [1])
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: On synthetic scenes, we see that iMAP is incredibly capable at jointly creating
    coherent scene reconstructions (see below) and accurate camera tracks (see above).
    In fact, iMAP is even found to be more capable than baselines at accurately filling
    in unobserved portions of a scene. Such a benefit is due to the ability of deep
    learning to leverage prior data and [inductive biases](https://en.wikipedia.org/wiki/Inductive_bias)
    to handle ambiguity in a reasonable/predictable manner.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在合成场景中，我们看到 iMAP 在共同创建一致的场景重建（见下文）和准确的相机轨迹（见上文）方面非常强大。实际上，iMAP 被发现比基线更能准确填补场景中未观察到的部分。这种好处得益于深度学习利用先前数据和
    [归纳偏差](https://en.wikipedia.org/wiki/Inductive_bias) 以合理/可预测的方式处理模糊性的能力。
- en: '![](../Images/01dd5ea05afea29e7d8612bc6b6fcec7.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01dd5ea05afea29e7d8612bc6b6fcec7.png)'
- en: (from [1])
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: iMAP accurately performs tracking and mapping simultaneously due to its joint
    optimization of 3D scene representations and camera poses in every frame. Despite
    beginning the learning process from a completely random initialization, iMAP quickly
    learns accurate scene representations and tracking information. Most notably,
    the iMAP framework can be used at any scale, from small objects to an entire room
    that contains various, detailed objects; see below.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: iMAP 凭借其在每个帧中对 3D 场景表示和相机姿态的联合优化，能够准确地同时进行跟踪和映射。尽管学习过程从完全随机的初始化开始，但 iMAP 很快学会了准确的场景表示和跟踪信息。最值得注意的是，iMAP
    框架可以用于任何规模，从小物体到包含各种详细物体的整个房间；见下文。
- en: '![](../Images/32e3f6ed20e90d5c48985449098b8490.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/32e3f6ed20e90d5c48985449098b8490.png)'
- en: (from [1])
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: Despite crafting a comparably-accurate scene representation, the memory usage
    of iMAP is quite low compared to SLAM baselines. iMAP just needs enough memory
    to store keyframes (including associated data) and the parameters of the neural
    network; see below.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管制作了相对准确的场景表示，iMAP的内存使用量与SLAM基准相比仍然很低。iMAP只需要足够的内存来存储关键帧（包括相关数据）和神经网络的参数；见下文。
- en: '![](../Images/05bae1fa7682fb0c5657736e5d73085b.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05bae1fa7682fb0c5657736e5d73085b.png)'
- en: (from [1])
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: On real-world data from a handheld RGB-D camera, iMAP continues to outperform
    SLAM baselines. In particular, iMAP is surprisingly effective at accurately rendering
    regions of a scene where the depth camera has inaccurate readings (i.e., this
    tends to happen for black, reflective, or transparent surfaces); see below.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在来自手持RGB-D摄像头的真实数据上，iMAP继续超越SLAM基准。特别是，iMAP在准确渲染深度摄像头读数不准确的场景区域方面意外有效（例如，这种情况通常发生在黑色、反射性或透明表面上）；见下文。
- en: '![](../Images/882b52d6e135989f9e8bb90d978d8020.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/882b52d6e135989f9e8bb90d978d8020.png)'
- en: (from [1])
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: Takeaways
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主要结论
- en: Compared to most techniques we have seen so far, iMAP is quite different. In
    particular, it is a SLAM system, which means that it performs localization in
    addition to building a scene representation. Additionally, it relies upon online
    learning techniques to learn everything about a scene in real-time. iMAP begins
    from a random initialization and learns to represent the underlying scene from
    scratch! Some major takeaways are as follows.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们目前见过的大多数技术相比，iMAP非常不同。特别是，它是一个SLAM系统，这意味着它除了构建场景表示外，还执行定位。此外，它依赖于在线学习技术，实时学习有关场景的所有内容。iMAP从随机初始化开始，从零开始学习表示基础场景！一些主要结论如下。
- en: '![](../Images/9afd8e20d0dad0ada703d6afc515bbb3.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9afd8e20d0dad0ada703d6afc515bbb3.png)'
- en: (from [1])
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: '**super fast!** Most techniques we have scene for representing 3D scenes are
    quite expensive. For example, NeRFs [take 2 days](https://cameronrwolfe.substack.com/i/97915766/takeaways)
    to train on a single GPU, while LLFF [performs ~10 minutes of preprocessing](https://cameronrwolfe.substack.com/i/98707360/takeaways)
    before generating novel scene views. iMAP learns everything in real time as new
    images are made available to it from an RGB-D camera. This is a massive change
    in the computational cost of crafting scene representations. Some timing data
    of iMAP tracking and mapping pipelines is shown above. iMAP is implemented in
    PyTorch and can run on a Desktop CPU/GPU system.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**超级快！** 我们见过的大多数3D场景表示技术都相当昂贵。例如，NeRFs [需要2天](https://cameronrwolfe.substack.com/i/97915766/takeaways)才能在单个GPU上训练，而LLFF
    [在生成新场景视图之前需要约10分钟的预处理](https://cameronrwolfe.substack.com/i/98707360/takeaways)。iMAP可以实时学习所有内容，只要RGB-D摄像头提供新的图像。这在创建场景表示的计算成本上是一个巨大的变化。上面展示了iMAP跟踪和映射管道的一些时间数据。iMAP在PyTorch中实现，并且可以在桌面CPU/GPU系统上运行。'
- en: '**why deep learning?** When we compare the iMAP system to other SLAM baselines,
    we see that it is able to “fill in” blank spaces that are left by the other systems.
    Put simply, iMAP can reasonably infer the contents of regions that have not been
    explicitly observed in a scene. Such an ability is due to the use of a deep, feed-forward
    neural network, which leverages priors within the data/architecture to estimate
    geometry from limited data.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么使用深度学习？** 当我们将iMAP系统与其他SLAM基准进行比较时，我们会发现它能够“填补”其他系统留下的空白区域。简而言之，iMAP可以合理地推测在场景中未被明确观察到的区域的内容。这种能力得益于深度前馈神经网络的使用，该网络利用数据/架构中的先验信息，从有限的数据中估计几何形状。'
- en: '**learning on the fly.** When beginning to learn about a scene, iMAP has no
    information. In fact, it begins from a completely random initialization, then
    uses online learning techniques to learn a representation of the scene on the
    fly from incoming data. It’s very surprising that an approach like this works
    well, given that prior approaches (e.g., [NeRF](https://cameronrwolfe.substack.com/p/understanding-nerfs))
    require several days of training to represent a scene. iMAP shows us that there
    may be shortcuts or more lightweight techniques that allow us to obtain high-quality
    scene representations more easily.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**动态学习**。当开始学习一个场景时，iMAP 并没有任何信息。实际上，它从完全随机的初始化开始，然后使用在线学习技术从输入数据中动态学习场景的表示。考虑到之前的方法（例如，[NeRF](https://cameronrwolfe.substack.com/p/understanding-nerfs)）需要几天的训练时间来表示一个场景，这种方法能够很好地工作确实非常令人惊讶。iMAP
    向我们展示了，可能存在一些捷径或更轻量的技术，使我们更容易获得高质量的场景表示。'
- en: '**positional embeddings.** Although a smaller point, we see with iMAP that
    [positional encodings](https://cameronrwolfe.substack.com/i/97915766/position-encodings),
    originally seen in NeRFs, are becoming standard. Recall that converting input
    coordinates into higher-dimensional positional embeddings before passing as input
    to a feed-forward network enables learning of high frequency features more easily.
    Here, we again see positional embeddings being adopted, revealing that such an
    approach is becoming standard.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**位置嵌入**。虽然这是一个较小的点，但我们通过 iMAP 看到，[位置编码](https://cameronrwolfe.substack.com/i/97915766/position-encodings)，最初在
    NeRFs 中出现，现在已经成为标准。请记住，将输入坐标转换为更高维的位置信息嵌入，然后作为输入传递给前馈网络，可以更容易地学习高频特征。在这里，我们再次看到位置嵌入的应用，表明这种方法正在成为标准。'
- en: Related Posts
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相关帖子
- en: Understanding NeRFs [[link](https://cameronrwolfe.substack.com/p/understanding-nerfs)]
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 NeRFs [[link](https://cameronrwolfe.substack.com/p/understanding-nerfs)]
- en: Local Light Field Fusion [[link](https://cameronrwolfe.substack.com/p/local-light-field-fusion)]
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地光场融合 [[link](https://cameronrwolfe.substack.com/p/local-light-field-fusion)]
- en: Scene Representation Networks [[link](https://cameronrwolfe.substack.com/p/scene-representation-networks)]
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 场景表示网络 [[link](https://cameronrwolfe.substack.com/p/scene-representation-networks)]
- en: Shape Reconstruction with ONets [[link](https://cameronrwolfe.substack.com/p/shape-reconstruction-with-onets)]
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 ONets 的形状重建 [[link](https://cameronrwolfe.substack.com/p/shape-reconstruction-with-onets)]
- en: 3D Generative Modeling with DeepSDF [[link](https://cameronrwolfe.substack.com/p/3d-generative-modeling-with-deepsdf)]
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度SDF的3D生成建模 [[link](https://cameronrwolfe.substack.com/p/3d-generative-modeling-with-deepsdf)]
- en: Closing remarks
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结束语
- en: Thanks so much for reading this article. I am [Cameron R. Wolfe](https://cameronrwolfe.me/),
    Director of AI at [Rebuy](https://www.rebuyengine.com/) and PhD student at Rice
    University. I study the empirical and theoretical foundations of deep learning.
    You can also check out my [other writings](https://medium.com/@wolfecameron) on
    medium! If you liked it, please follow me on [twitter](https://twitter.com/cwolferesearch)
    or subscribe to my [Deep (Learning) Focus newsletter](https://cameronrwolfe.substack.com/),
    where I help readers build a deeper understanding of topics in deep learning research
    via understandable overviews of popular papers on that topic.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢阅读这篇文章。我是 [Cameron R. Wolfe](https://cameronrwolfe.me/)，[Rebuy](https://www.rebuyengine.com/)
    的人工智能总监以及莱斯大学的博士生。我研究深度学习的实证和理论基础。你也可以查看我在 medium 上的 [其他著作](https://medium.com/@wolfecameron)！如果你喜欢这篇文章，请在
    [twitter](https://twitter.com/cwolferesearch) 上关注我或订阅我的 [Deep (Learning) Focus
    newsletter](https://cameronrwolfe.substack.com/)，在这里我通过易于理解的热门论文概述，帮助读者深入了解深度学习研究中的主题。
- en: Bibliography
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Sucar, Edgar, et al. “iMAP: Implicit mapping and positioning in real-time.”
    *Proceedings of the IEEE/CVF International Conference on Computer Vision*. 2021.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Sucar, Edgar, 等. “iMAP: Implicit mapping and positioning in real-time.”
    *Proceedings of the IEEE/CVF International Conference on Computer Vision*. 2021.'
- en: '[2] Mildenhall, Ben, et al. “Nerf: Representing scenes as neural radiance fields
    for view synthesis.” *Communications of the ACM* 65.1 (2021): 99–106.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Mildenhall, Ben, 等. “Nerf: Representing scenes as neural radiance fields
    for view synthesis.” *Communications of the ACM* 65.1 (2021): 99–106.'
- en: '[3] Sitzmann, Vincent, Michael Zollhöfer, and Gordon Wetzstein. “Scene representation
    networks: Continuous 3d-structure-aware neural scene representations.” *Advances
    in Neural Information Processing Systems* 32 (2019).'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Sitzmann, Vincent, Michael Zollhöfer, 和 Gordon Wetzstein. “Scene representation
    networks: Continuous 3d-structure-aware neural scene representations.” *Advances
    in Neural Information Processing Systems* 32 (2019).'
- en: '[4] Park, Jeong Joon, et al. “Deepsdf: Learning continuous signed distance
    functions for shape representation.” *Proceedings of the IEEE/CVF conference on
    computer vision and pattern recognition*. 2019.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Park, Jeong Joon 等人。“Deepsdf: 学习用于形状表示的连续符号距离函数。” *IEEE/CVF 计算机视觉与模式识别会议论文集*。2019
    年。'
- en: '[5] Tancik, Matthew, et al. “Fourier features let networks learn high frequency
    functions in low dimensional domains.” *Advances in Neural Information Processing
    Systems* 33 (2020): 7537–7547.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Tancik, Matthew 等人。“傅里叶特征使网络能够在低维领域学习高频函数。” *神经信息处理系统进展* 33 (2020): 7537–7547。'
- en: '[6] Kemker, Ronald, et al. “Measuring catastrophic forgetting in neural networks.”
    *Proceedings of the AAAI conference on artificial intelligence*. Vol. 32\. №1\.
    2018.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Kemker, Ronald 等人。“测量神经网络中的灾难性遗忘。” *美国人工智能协会会议论文集*。第 32 卷，第 1 期，2018 年。'
