- en: Differential Equations as a Pytorch Neural Network Layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/differential-equations-as-a-pytorch-neural-network-layer-7614ba6d587f](https://towardsdatascience.com/differential-equations-as-a-pytorch-neural-network-layer-7614ba6d587f)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to use differential equations layers in pytorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@moleculeboy24?source=post_page-----7614ba6d587f--------------------------------)[![Kevin
    Hannay](../Images/6dfcf0384e2a8aac0dd27a216ed2d92c.png)](https://medium.com/@moleculeboy24?source=post_page-----7614ba6d587f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7614ba6d587f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7614ba6d587f--------------------------------)
    [Kevin Hannay](https://medium.com/@moleculeboy24?source=post_page-----7614ba6d587f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7614ba6d587f--------------------------------)
    ·17 min read·Apr 26, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Differential equations are the mathematical foundation for most of modern science.
    They describe the state of a system using an equation for the rate of change (differential).
    It is remarkable how many systems can be well described by equations of this form.
    For example, the physical laws describing motion, electromagnetism and quantum
    mechanics all take this form. More broadly, differential equations describe chemical
    reaction rates through the law of mass action, neuronal firing and disease spread
    through the SIR model.
  prefs: []
  type: TYPE_NORMAL
- en: The deep learning revolution has brought with it a new set of tools for performing
    large scale optimizations over enormous datasets. In this post, we will see how
    you can use these tools to fit the parameters of a custom differential equation
    layer in pytorch.
  prefs: []
  type: TYPE_NORMAL
- en: What is the problem we are trying to solve?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s say we have some time series data y(t) that we want to model with a differential
    equation. The data takes the form of a set of observations yᵢ at times tᵢ. Based
    on some domain knowledge of the underlying system we can write down a differential
    equation to approximate the system.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the most general form this takes the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a40c428710be4c634489d22e95a37020.png)'
  prefs: []
  type: TYPE_IMG
- en: General ordinary differential equation system
  prefs: []
  type: TYPE_NORMAL
- en: where y is the state of the system, t is time, and θ are the parameters of the
    model. In this post we will assume that the parameters θ are unknown and we want
    to learn them from the data.
  prefs: []
  type: TYPE_NORMAL
- en: All of the code for this post is available on [github](https://github.com/khannay/paramfittorchdemo)
    or as a [colab notebook](https://colab.research.google.com/github/khannay/paramfittorchdemo/blob/main/nbs/00_training.ipynb),
    so no need to try and copy and paste if you want to follow along.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s import the libraries we will need for this post. The only non standard
    machine learning library we will use the [torchdiffeq](https://github.com/rtqichen/torchdiffeq)
    library to solve the differential equations. This library implements numerical
    differential equation solvers in pytorch.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step of our modeling process is to define the model. For differential
    equations this means we must choose a form for the function f(y,t;θ) and a way
    to represent the parameters θ. We also need to do this in a way that is compatible
    with pytorch.
  prefs: []
  type: TYPE_NORMAL
- en: This means we need to encode our function as a *torch.nn.Module* class. As you
    will see this is pretty easy and only requires defining two methods. Lets get
    started with the first of out three example models.
  prefs: []
  type: TYPE_NORMAL
- en: van Der Pol Oscillator (VDP)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can define a differential equation system using the *torch.nn.Module* class
    where the parameters are created using the *torch.nn.Parameter* declaration. This
    lets pytorch know that we want to accumulate gradients for those parameters. We
    can also include fixed parameters (parameters that we don’t want to fit) by just
    not wrapping them with this declaration.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first example we will use is the classic VDP oscillator which is a nonlinear
    oscillator with a single parameter μ. The differential equations for this system
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f984c8771b0c62531721fc143d1f7dc6.png)'
  prefs: []
  type: TYPE_IMG
- en: VDP oscillator equations
  prefs: []
  type: TYPE_NORMAL
- en: 'where **x** and **y** are the state variables. The VDP model is used to model
    everything from electronic circuits to cardiac arrhythmias and circadian rhythms.
    We can define this system in pytorch as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You only need to define the *__init__* method (**init**) and the forward method.
    I added a string method __*repr__* to pretty print the parameter. The key point
    here is how we can translate from the differential equation to torch code in the
    forward method. This method needs to define the right-hand side of the differential
    equation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how we can integrate this model using the odeint method from torchdiffeq:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/38e234bbc82a1b9c30dc4b550479214c.png)'
  prefs: []
  type: TYPE_IMG
- en: Here is a phase plane plot of the solution (a phase plane plot of a parametric
    plot of the dynamical state).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a58bc70d4e871b67c22c583f19227709.png)'
  prefs: []
  type: TYPE_IMG
- en: The colors indicate the 30 separate trajectories in our batch. The solution
    comes back as a torch tensor with dimensions (time_points, batch number, dynamical_dimension).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Lotka Volterra Predator Prey equations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As another example we create a module for the Lotka-Volterra predator-prey
    equations. In the Lotka-Volterra (LV) predator-prey model, there are two primary
    variables: the population of prey (x) and the population of predators (y). The
    model is defined by the following equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4861c4876c104ce423e07b60b0ffdc04.png)'
  prefs: []
  type: TYPE_IMG
- en: Lotka-Volterra equations for predator prey dynamics
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the primary variables, there are also four parameters that are
    used to describe various ecological factors in the model:'
  prefs: []
  type: TYPE_NORMAL
- en: α represents the intrinsic growth rate of the prey population in the absence
    of predators. β represents the predation rate of the predators on the prey. γ
    represents the death rate of the predator population in the absence of prey. δ
    represents the efficiency with which the predators convert the consumed prey into
    new predator biomass.
  prefs: []
  type: TYPE_NORMAL
- en: 'Together, these variables and parameters describe the dynamics of predator-prey
    interactions in an ecosystem and are used to mathematically model the changes
    in the populations of prey and predators over time. Here is this system as a *torch.nn.Module*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This follows the same pattern as the first example, the main difference is that
    we now have four parameters and store them as a *model_params* tensor. Here is
    the integration and plotting code for the predator-prey equations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/58bdcdc8b3ec9b84c3af02856148c2c9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now a phase plane plot of the system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d7ae7a0eda4a514d6111b301033a2e53.png)'
  prefs: []
  type: TYPE_IMG
- en: Lorenz system
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The last example we will use is the Lorenz equations which are famous for their
    beautiful plots illustrating chaotic dynamics. They originally came from a reduced
    model for fluid dynamics and take the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0dcaffefdc1eba6471cc753f3d0a3faa.png)'
  prefs: []
  type: TYPE_IMG
- en: where x, y, and z are the state variables, and σ, ρ, and β are the system parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This shows how to integrate this system and plot the results. This system (at
    these parameter values) shows chaotic dynamics so initial conditions that start
    off close together diverge from one another exponentially.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/7984eecf06eb3fd53899755e48f5964d.png)'
  prefs: []
  type: TYPE_IMG
- en: Here we show the famous butterfly plot (phase plane plot) for the first set
    of initial conditions in the batch.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/58e4336da5ee47b8d7bcaf8666994410.png)'
  prefs: []
  type: TYPE_IMG
- en: The Lorenz equations show chaotic dynamics and trace out a beautiful strange
    attractor.
  prefs: []
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we can define the differential equation models in pytorch we need to
    create some data to be used in training. This is where things start to get really
    neat as we see our first glimpse of being able to hijack deep learning machinery
    for fitting the parameters. Really we could just use tensor of data directly,
    but this is a nice way to organize the data. It will also be useful if you have
    some experimental data that you want to use.
  prefs: []
  type: TYPE_NORMAL
- en: Torch provides the *Dataset* class for loading in data. To use it you just need
    to create a subclass and define two methods. The *__len__* function that returns
    the number of data points and a *__getitem__* function that returns the data point
    at a given index. If you are wondering these methods are what underly the *len(array)*
    and *array[0]* subscript access in python lists.
  prefs: []
  type: TYPE_NORMAL
- en: The rest of boilerplate code needed in defined in the parent class *torch.utils.data.Dataset*.
    We will see the power of these method when we go to define a training loop.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Next let’s create a quick generator function to generate some simulated data
    to test the algorithms on. In a real use case the data would be loaded from a
    file or database- but for this example we will just generate some data. In fact,
    I recommend that you always start with generated data to make sure your code is
    working before you try to load real data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This just takes in a differential equation model with some initial states and
    generates some time-series data from it (and adds in some gaussian noise). This
    data is then passed into our custom dataset container.
  prefs: []
  type: TYPE_NORMAL
- en: Training Loop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next we will create a wrapper function for a pytorch training loop. Training
    means we want to update the model parameters to increase the alignment with the
    data (or decrease the cost function).
  prefs: []
  type: TYPE_NORMAL
- en: One of the tricks for this from deep learning is to not use all the data before
    taking a gradient step. Part of this is necessity for using enormous datasets
    as you can’t fit all of that data inside a GPU’s memory, but this also can help
    the gradient descent algorithm avoid getting stuck in local minima.
  prefs: []
  type: TYPE_NORMAL
- en: 'The training loop in words:'
  prefs: []
  type: TYPE_NORMAL
- en: Divide the dataset into mini-batches, these are subsets of your entire data
    set. Usually want to choose these randomly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iterate through the mini-batches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate the predictions using the current model parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate the loss (here we will use the mean squared error)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate the gradients, using backpropagation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the parameters using a gradient descent step. Here we use the Adam optimizer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each full pass through the dataset is called an epoch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Okay here is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fitting the VDP Oscillator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s use this training loop to recover the parameters from simulated VDP oscillator
    data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Let’s create a model with the wrong parameter value and visualize the starting
    point.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9a3ae99f6b8944ff0d86a7e00c724c7d.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, we will use the training loop to fit the parameters of the VDP oscillator
    to the simulated data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Not to bad! Let’s see how the plot looks now…
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ec1504367018a9ece345f7546705af24.png)'
  prefs: []
  type: TYPE_IMG
- en: The plot confirms that we almost perfectly recovered the parameter. One more
    quick plot, where we plot the dynamics of the system in the phase plane (a parametric
    plot of the state variables).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e6d29331b53d0080c8a5f6428375fff5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is a visual of the training process for this model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3bfaaa91a33776ca1bccbda8d804ec88.png)'
  prefs: []
  type: TYPE_IMG
- en: Video of the training process for the VDP model
  prefs: []
  type: TYPE_NORMAL
- en: Lotka Voltera Equations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now lets adapt our methods to fit simulated data from the Lotka-Volterra equations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Here is the initial fits for the starting parameters, then we will fit as before
    and take a look at the results.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c324d4f631cb1479f069a7c2471ec736.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'First a time-series plot of the fitted system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1cad34d577e24b2bbae6a39ef6e8ea27.png)'
  prefs: []
  type: TYPE_IMG
- en: Now let’s visualize the results using a phase plane plot.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c5bb7d6259f550bd7f5dc28330476a55.png)'
  prefs: []
  type: TYPE_IMG
- en: Here is a visual of the fitting process….
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7b71c438cc820ad955e4595c1cd1131d.png)'
  prefs: []
  type: TYPE_IMG
- en: Lorenz Equations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, let’s try to fit the Lorenz equations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Here is the initial fits, then we will call our training loop.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fc271257498a7efd4a21da9c71d16e56.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the training results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Let’s look at the fitted model. Starting with a full plot of the dynamics.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/68ad27e421cc95d203aa338507deb0f6.png)'
  prefs: []
  type: TYPE_IMG
- en: Let’s zoom in on the bulk of the data and see how the fit looks.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3ff8fb66cb7d20e91b8dacb8082d7844.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see the model is very close to the true model for the data range, and
    generalizes well for t < 16 for the unseen data. Now the phase plane plot (zoomed
    in).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/2876da4f6595ae6470eb1b40a816366b.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see that our fitted model performs well for t in [0,16] and then starts
    to diverge.
  prefs: []
  type: TYPE_NORMAL
- en: Intro to Neural Differential Equations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This procedure works great for the situation where we know the form of the equations
    on the right-hand-side, but what if we don’t? Can we use this procedure to discover
    the model equations?
  prefs: []
  type: TYPE_NORMAL
- en: This is much too big of a subject to fully cover in this post, but one of the
    biggest advantages of moving our differential equations models into the torch
    framework is that we can mix and match them with artificial neural network layers.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest thing we can do is to replace the right-hand-side f(y,t; θ) with
    a neural network layer. These types of equations have been called a neural differential
    equations and it can be viewed as [generalization of a recurrent neural network](https://dl.acm.org/doi/10.5555/3327757.3327764).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/29598e77aeefc86671eadc34c6e77e7f.png)'
  prefs: []
  type: TYPE_IMG
- en: Neural differential equations replace the right-hand-side of the equations with
    a artificial neural network
  prefs: []
  type: TYPE_NORMAL
- en: As a first example, let’s do this for the our simple VDP oscillator system.
    To begin we will remake the simulated data, you will notice that I am creating
    longer time-series of the data and more samples. Fitting a neural differential
    equation takes much more data and more computational power since we have many
    more parameters that need to be determined.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Now I define a simple feedforward neural network layer to fill in the right-hand-side
    of the equation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a plot of the system before fitting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/ebf03ab9b520dde7236de0dfa8249652.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see we start very far away for the correct solution, but then again
    we are injecting much less information into our model. Let’s see if we can fit
    the model to get better results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Visualizing the results, we can see that the model is able to fit the data and
    even extrapolate to the future (although it is not as good or fast as the specified
    model).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2a9b06cf98c1563ef2aa13728286f085.png)'
  prefs: []
  type: TYPE_IMG
- en: Now the phase plane plot of our neural differential equation model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1c04104e67e37c75340ecd6da5e55c6d.png)'
  prefs: []
  type: TYPE_IMG
- en: These models take a long time to train and more data to converge on a good fit.
    This makes sense since we are both trying to learn the model and the parameters
    at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f3e1eda9f5826ff5860298d228175c6b.png)'
  prefs: []
  type: TYPE_IMG
- en: Video of the fitting process for a NDE.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions and Wrap-Up
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this article I have demonstrated how we can use differential equation models
    within the pytorch ecosytem using the torchdiffeq package. The code from this
    article is available on [github](https://github.com/khannay/paramfittorchdemo)
    and can be opened directly to [google colab](https://colab.research.google.com/github/khannay/paramfittorchdemo/blob/main/nbs/00_training.ipynb)
    for experimentation. You can also install the code from this article using pip:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This post is an introduction in the future I will be writing more about the
    following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: How to blend some mechanistic knowledge of the dynamics with deep learning.
    These have been called [universal differential equations](https://arxiv.org/abs/2001.04385)
    as they enable us to combine scientific knowledge with deep learning. This basically
    blends the two approaches together.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to combine differential equation layers with other deep learning layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model discovery: Can we recover the actual model equations from data? This
    uses tools like [SINDy](https://www.pnas.org/doi/10.1073/pnas.1906995116) to extract
    the model equations from data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MLOps tools for managing the training of these models. This includes tools like
    [MLFlow](https://mlflow.org/) , [Weights and Biases](https://wandb.ai/home) ,
    and [Tensorboard](https://pytorch.org/docs/stable/tensorboard.html) .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anything else I hear back about from you!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you liked this post, be sure to follow me and connect on l[inked-in](https://www.linkedin.com/in/kevin-hannay-7a6049198/).
    All images unless otherwise noted are by the author.
  prefs: []
  type: TYPE_NORMAL
