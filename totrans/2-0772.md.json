["```py\nimport numpy as np\nfrom typing import Union\nimport plotly.express as px\n\nclass DynamicPricingQL:\n    def __init__(self, \n                 initial_price: int = 1000, \n                 initial_demand: int = 1000000,\n                 elasticity: float = -0.01,\n                 cost_per_unit: int = 20, \n                 learning_rate: float = 0.1, \n                 discount_factor: float = 0.9, \n                 exploration_prob: float = 0.2, \n                 error_term: float = 0.2, \n                 random_walk_std: float = 0.5, \n                 target_reward_increase: float = 0.2) -> None:\n        '''Class that implements a Dynamic Pricing agent using \n        Q-Learning to find the optimal price for a given product.\n\n        Args:\n            - initial_price: starting price of the product\n            - initial_demand: starting volume of the product\n            - elasticity: price elasticity of the product\n            - cost_per_unit: unitary cost of the product\n            - learning_rate: learning rate for the Bellman equation\n            - exploration_prob: control the exploration-explotation trade-off\n            - error_term: error term added to the reward estimate to account for fluctuations\n            - random_walk_std: control the random walk fluctuations added to the demand estimate\n            - target_reward_increase: end the training when the reward reaches this target increase \n        '''\n        # Init variables\n        self.learning_rate = learning_rate\n        self.discount_factor = discount_factor\n        self.exploration_prob = exploration_prob\n        self.initial_price = initial_price\n        self.cost_per_unit = cost_per_unit\n        self.elasticity = elasticity\n        self.error_term = error_term\n        self.random_walk_std = random_walk_std\n        self.initial_demand = initial_demand\n        self.target_reward_increase = target_reward_increase\n        self.current_price = initial_price\n        self.current_demand = initial_demand\n\n        # Estimate current demand level from the initial demand\n        self.current_demand_level = self.calculate_demand_level(\n            self.initial_demand)\n\n        # Track whether the training procedure occurred or not\n        self.isfit = False\n\n        # The agent can only perform 3 actions: \n        #   - Increase the price\n        #   - Decrease the price\n        #   - Keep the price constant\n        self.num_actions = 3\n\n        # Consider 3 different states as discrete demand level\n        self.num_demand_levels = 3\n\n        # Initialize Q-values\n        self.q_values = np.zeros((self.num_demand_levels, \n                                  self.num_actions))\n\n        # Store rewards per episode for plotting\n        self.episode_rewards = []\n\n    def calculate_demand_level(self, \n                               demand: int, \n                               demand_fraction: float = 0.3) -> int:\n        '''Estimate the demand level.\n        Demand levels represent the states of the Q-Learning agent.\n        In order to turn a continuous demand into a discrete set in three values,\n        we use a fraction of the initial value to estimate a low, medium or high demand level.\n\n        Args:\n            - demand: current demand for the product\n            - demand_fraction: fraction of demand controlling the assignment to the demand levels\n        '''\n        # Low demand level: 0\n        if demand < (1 - demand_fraction) * self.initial_demand:\n            return 0\n        # High demand level: 2\n        elif demand > (1 + demand_fraction) * self.initial_demand:\n            return 2\n        # Medium demand level: 1\n        else:\n            return 1\n\n    def calculate_reward(self, \n                         new_price: int,\n                         price_fraction: float = 0.2) -> float:\n        '''Calculate the reward.\n        The reward during an episode is the profit \n        under a certain price (action) and demand.\n        We add an error term to account for fluctuations.\n\n        Note: if the price is either too high or too low\n        with respect to the initial price, we assign a negative reward.\n\n        Args:\n            - new_price: new price of the product\n            - price_fraction: penalize price variations above or below this fraction\n        '''\n\n        # If the new price is more distant from the initial price\n        # than a certain value given by price_fraction\n        # then assign a negative reward to penalize high price changes \n        if new_price > self.initial_price * (1 + price_fraction)\\\n            or new_price < self.initial_price * (1 - price_fraction):\n\n            # Negative reward to penalize significant price changes\n            return -1 \n\n        else:\n\n            # Estimate the demand given the new price\n            demand = self.calculate_demand(new_price)\n\n            # Etimate profit given new price and demand\n            profit = (new_price - self.cost_per_unit) *\\\n                      demand *\\\n                      (1 - self.error_term)\n\n            # Return profit as reward for the agent\n            return profit\n\n    def calculate_demand(self, \n                         price: int) -> int:\n        '''Calculate demand as: \n              current demand + delta(demand) + random walk fluctuation = \n              current demand + elasticity * (price - current price) + random walk fluctuation\n\n        Args:\n            - price: price of the product\n        '''\n        return np.floor(self.current_demand + \\\n                self.elasticity * (price - self.current_price) +\\\n                np.random.normal(0, self.random_walk_std))\n\n    def fit(self,\n            num_episodes: int = 1000,\n            max_steps_per_episode: int = 100) -> None:\n        '''Fit the agent for a num_episodes number of episodes.\n\n        Args:\n            - num_episodes: number of episodes\n            - max_steps_per_episode: max number of steps for each episode\n        '''\n        # For each episode\n        for episode in range(num_episodes):\n\n            # The state is the current demand level \n            state = self.current_demand_level\n\n            # To interrupt the training procedure\n            done = False\n\n            # The reward is zero at the beginning of the episode\n            episode_reward = 0\n\n            # Keep track of the training steps\n            step = 0\n\n            # Training loop\n            while not done:\n\n                # Depending on the exploration probability\n                if np.random.rand() < self.exploration_prob:\n\n                    # Explore a new price ...\n                    action = np.random.randint(self.num_actions)\n\n                else:\n\n                    # ... or exploit prices known to increase the reward\n                    action = np.argmax(self.q_values[state])\n\n                # Set the new price given the action (increase, decrease or leave the price as is)\n                new_price = self.current_price + action - 1\n\n                # Calculate the new demand and demand level\n                new_demand = self.calculate_demand(new_price)\n                new_demand_level = self.calculate_demand_level(new_demand)\n\n                # Estimate the reward (profit) under the current action\n                reward = self.calculate_reward(new_price)\n\n                # Save the reward\n                episode_reward += reward\n\n                # Bellman equation for the Q values\n                self.q_values[state, action] = self.q_values[state, action] + \\\n                  self.learning_rate * \\\n                  (reward + self.discount_factor * np.max(self.q_values[new_demand_level]) -\\\n                  self.q_values[state, action])\n\n                # Update price and demand for the next iteration\n                self.current_price = new_price\n                self.current_demand = new_demand\n                self.current_demand_level = new_demand_level\n\n                # Update the step counter\n                step += 1\n\n                # Exit the loop if the max number of steps was reached\n                # or if the reward increased more than a certain threshold\n                if step >= max_steps_per_episode or episode_reward >= self.target_reward_increase:\n                    done = True\n\n            # Save the training results for plotting\n            self.episode_rewards.append(episode_reward)\n\n        # Acknowledge the accomplishment of the training procedure\n        self.isfit = True\n\n        print(\"Training completed.\")\n\n    def get_q_table(self) -> np.ndarray:\n        '''Return the Q table'''\n        return self.q_values\n\n    def plot_rewards(self, width=1200, height=800) -> None:\n        '''Plot the cumulative rewards per episode using Plotly.\n\n        Args:\n            - width: width of the plot\n            - height: height of the plot\n        '''\n\n        # Plot rewards per episode\n        fig = px.line(\n            self.episode_rewards, \n            title = \"Rewards per episode <br><sup>Profit</sup>\",\n            labels = dict(index=\"Episodes\", value=\"Rewards\"),\n            template = \"plotly_dark\",\n            width = width, \n            height = height)\n\n        # Style colors, font family and size \n        fig.update_xaxes(\n            title_font = dict(size=32, family=\"Arial\"))\n        fig.update_yaxes(\n            title_font = dict(size=32, family=\"Arial\"))\n        fig.update_layout(\n            showlegend = False,\n            title = dict(font=dict(size=30)),\n            title_font_color = \"yellow\")\n        fig.update_traces(\n            line_color = \"cyan\", \n            line_width = 5)\n\n        # Show the plot\n        fig.show()\n\n    def predict(self, \n                input_price: int, \n                input_demand: int) -> Union[int, str]:\n        '''Predict the next price given an input price and demand.\n\n        Args:\n            - input_price: input price of the product\n            - input_demand: input demand of the product\n        '''\n        # If the model was fit\n        if self.isfit:\n\n            # State equals the current demand level \n            state = self.calculate_demand_level(input_demand)\n\n            # Identify the most profitable action from the Q values\n            action = np.argmax(self.q_values[state])\n\n            # The next price is given by the most profitable action\n            prediction = input_price + action - 1\n\n            # Return the predicted price\n            return prediction\n\n        # If the model was not fit\n        else:\n            return \"Fit the model before asking a prediction for the next price.\"\n```", "```py\n# For reproducibility\nnp.random.seed(62)\n\n# Instantiate the agent class\npricing_agent = DynamicPricingQL(\n    initial_price = 1000,\n    initial_demand = 1000000,\n    elasticity = -0.02,\n    cost_per_unit = 20)\n\n# Fit the agent\npricing_agent.fit(num_episodes=1000)\n```", "```py\nTraining completed.\n```", "```py\npricing_agent.get_q_table()\n```", "```py\narray([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [7.92000766e+09, 8.01708509e+09, 7.98798684e+09],\n       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])\n```", "```py\npricing_agent.plot_rewards()\n```", "```py\ninput_price = 500\ninput_demand = 10000\n\nnext_price = pricing_agent.predict(input_price, input_demand)\nprint(f\"Next Price: {next_price}\")\n```", "```py\nNext Price: 499\n```"]