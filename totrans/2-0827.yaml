- en: Ethical Considerations In Machine Learning Projects
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习项目中的伦理考虑
- en: 原文：[https://towardsdatascience.com/ethical-considerations-in-machine-learning-projects-e17cb283e072](https://towardsdatascience.com/ethical-considerations-in-machine-learning-projects-e17cb283e072)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/ethical-considerations-in-machine-learning-projects-e17cb283e072](https://towardsdatascience.com/ethical-considerations-in-machine-learning-projects-e17cb283e072)
- en: '![](../Images/22f131eeee213e6dddb5a6519e111693.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22f131eeee213e6dddb5a6519e111693.png)'
- en: Image by Dall-E 2.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 Dall-E 2 提供。
- en: Don’t forget these topics when building AI systems
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在构建 AI 系统时不要忘记这些话题
- en: '[](https://hennie-de-harder.medium.com/?source=post_page-----e17cb283e072--------------------------------)[![Hennie
    de Harder](../Images/3e4f2cccd6cb976ca3f8bf15597daea8.png)](https://hennie-de-harder.medium.com/?source=post_page-----e17cb283e072--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e17cb283e072--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e17cb283e072--------------------------------)
    [Hennie de Harder](https://hennie-de-harder.medium.com/?source=post_page-----e17cb283e072--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://hennie-de-harder.medium.com/?source=post_page-----e17cb283e072--------------------------------)[![Hennie
    de Harder](../Images/3e4f2cccd6cb976ca3f8bf15597daea8.png)](https://hennie-de-harder.medium.com/?source=post_page-----e17cb283e072--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e17cb283e072--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e17cb283e072--------------------------------)
    [Hennie de Harder](https://hennie-de-harder.medium.com/?source=post_page-----e17cb283e072--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e17cb283e072--------------------------------)
    ·7 min read·Feb 11, 2023
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e17cb283e072--------------------------------)
    ·阅读时间 7 分钟 ·2023年2月11日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '**Using sensitive data, discrimination based on personal information, model
    outputs that affect peoples lives… There are many different ways harm can be done
    with a data product or machine learning model. Unfortunately, there are** [**many
    examples**](https://www.amazon.com/Weapons-Math-Destruction-Increases-Inequality/dp/0553418815)
    **where it did go wrong. On the other hand, many projects are innocent: no sensitive
    data is used and the projects only improve processes or automate boring tasks.
    When there is a possibility they can do harm, you should invest time at the topics
    of this post.**'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用敏感数据、基于个人信息的歧视、影响人们生活的模型输出……数据产品或机器学习模型可能以多种不同方式造成伤害。不幸的是，** [**有很多例子**](https://www.amazon.com/Weapons-Math-Destruction-Increases-Inequality/dp/0553418815)
    **确实出现了问题。另一方面，许多项目是无害的：不使用敏感数据，项目仅仅是改进流程或自动化无聊的任务。当有可能造成伤害时，你应该投入时间关注本文的话题。**'
- en: Yes, ethics in data isn’t the most exciting subject you can think of. But if
    you want to use data in a responsible way, you might be interested in different
    ways to ensure this. This post contains six important ethics topics and ways to
    investigate how your model is doing. There are practical tools available to help
    you with this. Besides the harm that can be done to individuals, another important
    reason to take responsibility is that ethical considerations can impact trust
    and credibility in technology. If technology is not seen as trustworthy and transparent,
    it may harm its adoption and impact its effectiveness. People lose their trust
    and don’t want to use AI products anymore.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，数据伦理并不是你能想到的最令人兴奋的主题。但如果你想负责任地使用数据，你可能会对不同的方式感兴趣，以确保这一点。本文包含六个重要的伦理主题和调查你的模型表现的方法。还有实用工具可以帮助你做到这一点。除了对个人造成的伤害，另一个需要负责任的重要原因是伦理考虑会影响技术的信任和信誉。如果技术被视为不可信和不透明，它可能会阻碍其采用并影响其有效性。人们会失去信任，不再愿意使用
    AI 产品。
- en: Governance
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 治理
- en: The first topic to mention is AI governance. The governance aspects of data
    science and AI include the regulation, ethical guidelines, and best practices
    that are being developed to ensure that these technologies are used in an ethical
    and responsible manner.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 首个要提到的话题是 AI 治理。数据科学和 AI 的治理方面包括为确保这些技术以伦理和负责任的方式使用而制定的监管、伦理指南和最佳实践。
- en: Governance is a high level term and contains all the following topics, and more.
    The exact definition can be different across organizations and institutions. Large
    companies often use a framework to define all the aspects of AI governance. Here
    is an example from [Meta](https://ai.facebook.com/blog/facebooks-five-pillars-of-responsible-ai/).
    There are complete research teams dedicated to responsible AI, like [this team
    from Google](https://research.google/teams/responsible-ai/).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 治理是一个高层次的术语，包含了所有以下主题及更多。具体定义在不同组织和机构中可能有所不同。大型公司通常使用框架来定义 AI 治理的各个方面。这是[Meta](https://ai.facebook.com/blog/facebooks-five-pillars-of-responsible-ai/)的一个例子。像[谷歌的这个团队](https://research.google/teams/responsible-ai/)一样，有完整的研究团队专注于负责任的
    AI。
- en: AI governance is work in progress. The upcoming topics are the basics and a
    good AI governance framework should at least contain these topics.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: AI 治理仍在不断发展。即将讨论的主题是基础内容，一个好的 AI 治理框架至少应包含这些主题。
- en: '![](../Images/71aa730c83a53e7d09e620fa95d49266.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/71aa730c83a53e7d09e620fa95d49266.png)'
- en: AI governance and the topics covered in this post. Image by author.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: AI 治理以及本文涉及的主题。图片来源：作者。
- en: Fairness
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 公平性
- en: Fairness in machine learning refers to the idea that the model’s predictions
    should not be unfairly biased against certain groups of people. This is an important
    consideration when deploying models in real-world applications, particularly in
    sensitive areas such as healthcare and criminal justice. Unfortunately, there
    are many examples where it did go wrong, like with [COMPAS](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing),
    [chatbot Tay from Microsoft](https://en.wikipedia.org/wiki/Tay_(bot)) and in [job](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G)
    and [mortgage](https://www.forbes.com/sites/korihale/2021/09/02/ai-bias-caused-80-of-black-mortgage-applicants-to-be-denied/)
    applications.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的公平性指的是模型的预测不应对某些人群存在不公平的偏见。在将模型应用于现实世界时，特别是在医疗和刑事司法等敏感领域时，这是一项重要的考虑因素。不幸的是，曾经出现过许多失败的例子，比如[COMPAS](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)、[微软的聊天机器人Tay](https://en.wikipedia.org/wiki/Tay_(bot))以及[招聘](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G)和[抵押贷款](https://www.forbes.com/sites/korihale/2021/09/02/ai-bias-caused-80-of-black-mortgage-applicants-to-be-denied/)申请中的问题。
- en: 'Actually, it’s one of the main ethical concerns in machine learning: the possibility
    of bias being introduced into the model through the training data. It is important
    to address the ways in which bias can be introduced and the steps that can be
    taken to mitigate it.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这是机器学习中的主要伦理问题之一：通过训练数据将偏见引入模型的可能性。解决偏见引入的方式以及可以采取的缓解措施非常重要。
- en: There are different ways to increase the fairness of a model and reduce bias.
    It starts with diverse training data. Using a diverse and representative training
    data set to ensure that the model is not biased towards a particular group or
    outcome is the first step to a fair AI system.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同的方法可以提高模型的公平性并减少偏见。首先是使用多样化的训练数据。使用多样化且具有代表性的训练数据集，以确保模型不会对特定群体或结果产生偏见，这是实现公平
    AI 系统的第一步。
- en: 'Different tools are developed to increase fairness:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高公平性，开发了不同的工具：
- en: '[**IBM 360 degree toolkit**](https://github.com/Trusted-AI/AIF360)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**IBM 360度工具包**](https://github.com/Trusted-AI/AIF360)'
- en: This toolkit comes with a comprehensive set of metrics to test for fairness
    and algorithms to mitigate bias. Here is a [demo](http://aif360.mybluemix.net/data).
    Every metric is explained and it is a nice place to start. Examples of bias mitigation
    algorithms are [reweighing samples](https://aif360.readthedocs.io/en/stable/modules/generated/aif360.sklearn.preprocessing.Reweighing.html)
    and [optimizing preprocessing](https://aif360.readthedocs.io/en/v0.2.3/modules/preprocessing.html)
    (changes preprocessing), [adversarial debiasing](https://aif360.readthedocs.io/en/v0.2.3/modules/inprocessing.html#adversarial-debiasing)
    (changes modeling) and [reject option based classification](https://aif360.readthedocs.io/en/v0.2.3/modules/postprocessing.html#reject-option-classification)
    (changes predictions).
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该工具包提供了一整套全面的公平性测试指标和缓解偏差的算法。这里是一个[演示](http://aif360.mybluemix.net/data)。每个指标都有解释，这是一个很好的起点。偏差缓解算法的示例有[重加权样本](https://aif360.readthedocs.io/en/stable/modules/generated/aif360.sklearn.preprocessing.Reweighing.html)和[优化预处理](https://aif360.readthedocs.io/en/v0.2.3/modules/preprocessing.html)（改变预处理）、[对抗性去偏](https://aif360.readthedocs.io/en/v0.2.3/modules/inprocessing.html#adversarial-debiasing)（改变建模）和[拒绝选项分类](https://aif360.readthedocs.io/en/v0.2.3/modules/postprocessing.html#reject-option-classification)（改变预测）。
- en: '[**Fairlearn**](https://fairlearn.org)An open source project with the goal
    to help data scientists improve fairness of AI systems. They provide [example
    notebooks](https://fairlearn.org/v0.8/auto_examples/index.html) and a [comprehensive
    user guide](https://fairlearn.org/v0.8/user_guide/index.html).'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Fairlearn**](https://fairlearn.org)是一个开源项目，旨在帮助数据科学家提高人工智能系统的公平性。他们提供了[示例笔记本](https://fairlearn.org/v0.8/auto_examples/index.html)和[全面的用户指南](https://fairlearn.org/v0.8/user_guide/index.html)。'
- en: Transparency & Explainability
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 透明性与可解释性
- en: Transparency in data science and artificial intelligence refers to the ability
    for stakeholders to understand how a model works and how it is making its predictions.
    It is important to discuss the importance of transparency in building trust with
    users of the model and its significance in auditing the model. Some models are
    easy to understand, like decision trees (that are not too deep) and linear or
    logistic regression models. You can directly interpret the weights of the model
    or walk through the decision tree.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学和人工智能中，透明性指的是利益相关者能够理解模型如何工作以及它如何进行预测的能力。讨论透明性的重要性对于建立用户对模型的信任和审计模型的意义非常重要。有些模型易于理解，比如（不是很深的）决策树以及线性或逻辑回归模型。你可以直接解释模型的权重或遍历决策树。
- en: 'Transparent models have a downside: they usually perform worse than black box
    models like random forests, boosted trees or neural networks. Often, there is
    a trade off between understanding the workings of the model and the performance.
    It’s a good practice to discuss with stakeholders how transparent the model needs
    to be. If you know from the beginning that a model should be interpretable and
    fully transparent, you should stay away from black box models.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 透明的模型有一个缺点：它们通常比像随机森林、提升树或神经网络这样的黑箱模型表现更差。通常，在理解模型工作原理与模型性能之间存在权衡。与利益相关者讨论模型需要多透明是一个好的实践。如果你从一开始就知道模型应该是可解释的并且完全透明，你应该远离黑箱模型。
- en: '![](../Images/d3b3672c52d6fe8c58e92e57bbdbab93.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d3b3672c52d6fe8c58e92e57bbdbab93.png)'
- en: Trade off between explainability and performance. Click to enlarge. Image by
    author.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性与性能之间的权衡。点击放大。图片由作者提供。
- en: Closely related to transparency is explainability. With the increasing use of
    complex machine learning models, it is becoming more difficult to understand how
    the model is making a prediction. This is where explainability of models comes
    into play. As model owner you want to be able to explain predictions. Even for
    black box models, there are different ways to explain a prediction, e.g. with
    LIME, SHAP or global surrogate models. [This post](https://medium.com/towards-data-science/model-agnostic-methods-for-interpreting-any-machine-learning-model-4f10787ef504)
    explains these methods in depth.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 与透明性密切相关的是可解释性。随着复杂机器学习模型使用的增加，理解模型如何做出预测变得越来越困难。这就是模型可解释性发挥作用的地方。作为模型所有者，你希望能够解释预测。即使是黑箱模型，也有不同的方法来解释预测，例如使用LIME、SHAP或全局代理模型。[这篇文章](https://medium.com/towards-data-science/model-agnostic-methods-for-interpreting-any-machine-learning-model-4f10787ef504)深入解释了这些方法。
- en: Robustness
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 鲁棒性
- en: Robustness refers to the ability of an AI system to maintain its performance
    and functionality even when facing unexpected or adversarial situations, such
    as inputs that are significantly different from what the system was trained on,
    or attempts to manipulate the system for malicious purposes. A robust AI system
    should be able to handle such situations correctly, without causing significant
    errors or harm. Robustness is important for ensuring the reliable and safe deployment
    of AI systems in real-world scenarios, where the conditions may differ from the
    idealized scenarios used during training and testing. A robust AI system can increase
    trust in the technology, reduce the risk of harm, and help to ensure that the
    system is aligned with ethical and legal principles.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 鲁棒性指的是AI系统在面对意外或对抗性情况时保持其性能和功能的能力，例如输入与系统训练时显著不同，或试图恶意操控系统。一个鲁棒的AI系统应能正确处理这些情况，而不会导致重大错误或伤害。鲁棒性对确保AI系统在现实场景中的可靠和安全部署非常重要，因为这些场景可能与训练和测试时的理想化场景有所不同。一个鲁棒的AI系统可以增加对技术的信任，减少伤害风险，并帮助确保系统符合伦理和法律原则。
- en: 'Robustness can be handled in different ways, like:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 鲁棒性可以通过不同的方法来处理，例如：
- en: '[Adversarial training](https://adversarial-ml-tutorial.org), this can be done
    by training the system on a diverse range of inputs, including ones that are adversarial
    or intentionally designed to challenge the system. Defense against adversarial
    attacks should be build in, such as input validation and monitoring for abnormal
    behavior.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[对抗训练](https://adversarial-ml-tutorial.org)，这可以通过对系统进行各种输入的训练，包括对抗性的或故意设计来挑战系统的输入。应内置对抗攻击的防御措施，如输入验证和异常行为监控。'
- en: Regular testing and validation, also with inputs that are significantly different
    from the ones it was trained on, to ensure it is functioning as intended.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定期测试和验证，使用与训练时显著不同的输入，以确保系统按预期功能运行。
- en: Human oversight mechanisms, such as the ability for human users to review and
    approve decisions, or to intervene if the system makes an error.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工监督机制，例如允许人工用户审查和批准决策，或在系统出错时进行干预。
- en: Another important part is to update and monitor the model on a regular basis
    (or automatically). This can help to address any potential vulnerabilities or
    limitations that may be discovered over time.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要部分是定期（或自动）更新和监控模型。这有助于应对随时间发现的潜在漏洞或限制。
- en: Privacy
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 隐私
- en: Data science and artificial intelligence often rely on large amounts of personal
    data, which raises concerns about privacy. Companies should discuss the ways in
    which data can be used responsibly and the measures that can be taken to protect
    individuals’ personal information. The EU data protection law, [GDPR](https://gdpr.eu/what-is-gdpr/),
    might be the most famous privacy and security law in the world. It’s a big document,
    luckily there are good summaries and checklists available. In short, the GDPR
    demands that companies who handle personal data of EU citizens ask for consent,
    notify security breaches, are transparent in how they use the data and follow
    [Privacy by Design](https://gdpr-info.eu/issues/privacy-by-design/) principles.
    The [right to be forgotten](https://gdpr.eu/right-to-be-forgotten/) is another
    important concept. A company that violates the GDPR can risk a huge penalty, up
    to 4% of the annual global revenue generated by the company.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学和人工智能通常依赖大量个人数据，这引发了隐私问题。公司应讨论数据的负责任使用方式以及保护个人信息的措施。欧盟数据保护法，[GDPR](https://gdpr.eu/what-is-gdpr/)，可能是全球最著名的隐私和安全法律。虽然这是一份庞大的文件，但幸运的是有很好的总结和检查表。简而言之，GDPR要求处理欧盟公民个人数据的公司征得同意、通知安全漏洞、透明使用数据，并遵循[隐私设计](https://gdpr-info.eu/issues/privacy-by-design/)原则。[被遗忘权](https://gdpr.eu/right-to-be-forgotten/)是另一个重要概念。违反GDPR的公司可能面临高达公司年全球收入4%的巨额罚款。
- en: All the major cloud providers offer ways to ensure that sensitive and personal
    data is protected and used in a responsible and ethical manner. The tools are
    [Google Cloud DLP](https://cloud.google.com/dlp), [Microsoft Purview Information
    Protection](https://learn.microsoft.com/en-us/microsoft-365/compliance/information-protection?view=o365-worldwide),
    and [Amazon Macie](https://aws.amazon.com/macie/). There are [other tools](https://www.comparitech.com/data-privacy-management/gdpr-compliance-software/)
    available.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 所有主要的云服务提供商都提供了确保敏感和个人数据得到保护，并以负责任和伦理的方式使用的方法。这些工具包括[Google Cloud DLP](https://cloud.google.com/dlp)、[Microsoft
    Purview 信息保护](https://learn.microsoft.com/en-us/microsoft-365/compliance/information-protection?view=o365-worldwide)和[Amazon
    Macie](https://aws.amazon.com/macie/)。还有[其他工具](https://www.comparitech.com/data-privacy-management/gdpr-compliance-software/)可供选择。
- en: '![](../Images/c4c49fb7e7344e68d534898f961a7e4e.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4c49fb7e7344e68d534898f961a7e4e.png)'
- en: Image by Dall-E 2.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 Dall-E 2。
- en: Accountability
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 责任
- en: Accountability in AI refers to the responsibility of individuals, organizations,
    or systems to explain and justify their actions, decisions, and outcomes generated
    by AI systems. Organizations need to ensure that the AI system is functioning
    properly.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能中的责任指的是个人、组织或系统对其行为、决策以及人工智能系统生成的结果进行解释和辩护的责任。组织需要确保人工智能系统的正常运作。
- en: Someone needs to be responsible when the AI makes wrong decisions. The opinions
    among developers differ, like you can see in [this StackOverflow questionnaire](https://insights.stackoverflow.com/survey/2018/).
    Almost 60% of the developers say that management is most accountable for unethical
    implications of code. But in the same questionnaire, almost 80% thinks that developers
    should consider ethical implications of their code.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当人工智能做出错误决策时，需要有人负责。开发者之间对此的看法不尽相同，如你在[这个 StackOverflow 调查问卷](https://insights.stackoverflow.com/survey/2018/)中可以看到。几乎
    60% 的开发者认为，管理层对代码的伦理问题负主要责任。但在同一问卷中，几乎 80% 的人认为开发者应考虑其代码的伦理影响。
- en: 'The one thing everyone seems to agree on is: people are responsible for AI
    systems, and should take this responsibility. If you want to read more about accountability
    related to AI, [this paper](https://www.jkroll.com/papers/dissertation.pdf) is
    a great start. It mentions barriers for AI accountability like that often many
    people work on deploying an algorithm, and that bugs are accepted and expected.
    This makes it a difficult topic in AI governance.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 每个人似乎都同意的一点是：人们对人工智能系统负有责任，并应承担这一责任。如果你想了解更多关于人工智能责任的内容，[这篇论文](https://www.jkroll.com/papers/dissertation.pdf)是一个很好的起点。它提到了一些人工智能责任的障碍，比如通常有很多人参与部署算法，并且错误是被接受和预期的。这使得人工智能治理成为一个复杂的话题。
- en: Conclusion
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: As machine learning becomes increasingly prevalent in our lives, it is crucial
    to design and deploy these systems in a manner that aligns with societal values
    and ethics. This involves factors such as privacy, transparency, and fairness.
    Organizations and individuals should adopt best practices, such as collecting
    and processing data in a manner that upholds privacy, designing algorithms that
    are transparent and interpretable, and examining systems to prevent discrimination
    against certain groups. By taking these ethical considerations into account, machine
    learning technology can earn trust and minimize negative impacts, benefiting all.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习在我们生活中变得越来越普遍，设计和部署这些系统的方式必须符合社会价值观和伦理。这涉及隐私、透明度和公平性等因素。组织和个人应采取最佳实践，例如以维护隐私的方式收集和处理数据，设计透明且可解释的算法，以及检查系统以防止对某些群体的歧视。通过考虑这些伦理因素，机器学习技术可以赢得信任，并最小化负面影响，造福所有人。
- en: Related
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关
- en: '[](https://medium.com/bigdatarepublic/detecting-data-drift-with-machine-learning-adb177544312?source=post_page-----e17cb283e072--------------------------------)
    [## Detecting Data Drift with Machine Learning'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/bigdatarepublic/detecting-data-drift-with-machine-learning-adb177544312?source=post_page-----e17cb283e072--------------------------------)
    [## 利用机器学习检测数据漂移'
- en: Understand degraded performance of your ML models with an easy, automated process.
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过简单的自动化过程了解你机器学习模型的性能退化情况。
- en: medium.com](https://medium.com/bigdatarepublic/detecting-data-drift-with-machine-learning-adb177544312?source=post_page-----e17cb283e072--------------------------------)
    [](/model-agnostic-methods-for-interpreting-any-machine-learning-model-4f10787ef504?source=post_page-----e17cb283e072--------------------------------)
    [## Model-Agnostic Methods for Interpreting any Machine Learning Model
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/bigdatarepublic/detecting-data-drift-with-machine-learning-adb177544312?source=post_page-----e17cb283e072--------------------------------)
    [](/model-agnostic-methods-for-interpreting-any-machine-learning-model-4f10787ef504?source=post_page-----e17cb283e072--------------------------------)
    [## 适用于任何机器学习模型的模型无关解释方法'
- en: 'An overview of interpretation methods: permutation feature importance, partial
    dependence plots, LIME, SHAP and more.'
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解释方法概述：置换特征重要性、部分依赖图、LIME、SHAP 等。
- en: towardsdatascience.com](/model-agnostic-methods-for-interpreting-any-machine-learning-model-4f10787ef504?source=post_page-----e17cb283e072--------------------------------)
    [](/interpretable-machine-learning-models-aef0c7be3fd9?source=post_page-----e17cb283e072--------------------------------)
    [## Interpretable Machine Learning Models
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/model-agnostic-methods-for-interpreting-any-machine-learning-model-4f10787ef504?source=post_page-----e17cb283e072--------------------------------)
    [](/interpretable-machine-learning-models-aef0c7be3fd9?source=post_page-----e17cb283e072--------------------------------)
    [## 可解释的机器学习模型'
- en: A thorough introduction to the interpretation of logistic regression models
    and decision trees.
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对逻辑回归模型和决策树的解释进行了详细介绍。
- en: towardsdatascience.com](/interpretable-machine-learning-models-aef0c7be3fd9?source=post_page-----e17cb283e072--------------------------------)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/interpretable-machine-learning-models-aef0c7be3fd9?source=post_page-----e17cb283e072--------------------------------)'
