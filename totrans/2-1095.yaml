- en: How Self-RAG Could Revolutionize Industrial LLMs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Self-RAG 如何革新工业 LLMs
- en: 原文：[https://towardsdatascience.com/how-self-rag-could-revolutionize-industrial-llms-b33d9f810264](https://towardsdatascience.com/how-self-rag-could-revolutionize-industrial-llms-b33d9f810264)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-self-rag-could-revolutionize-industrial-llms-b33d9f810264](https://towardsdatascience.com/how-self-rag-could-revolutionize-industrial-llms-b33d9f810264)
- en: Let’s face it — vanilla RAG is pretty dumb. There’s no guarantee responses returned
    are relevant. Learn how Self-RAG can significantly help
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 说实话——普通的 RAG 真的很笨。没有保证返回的响应是相关的。了解 Self-RAG 如何显著帮助
- en: '[](https://skanda-vivek.medium.com/?source=post_page-----b33d9f810264--------------------------------)[![Skanda
    Vivek](../Images/9d25bee2fb75176ca7f7ea6eff7d7ab5.png)](https://skanda-vivek.medium.com/?source=post_page-----b33d9f810264--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b33d9f810264--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b33d9f810264--------------------------------)
    [Skanda Vivek](https://skanda-vivek.medium.com/?source=post_page-----b33d9f810264--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://skanda-vivek.medium.com/?source=post_page-----b33d9f810264--------------------------------)[![Skanda
    Vivek](../Images/9d25bee2fb75176ca7f7ea6eff7d7ab5.png)](https://skanda-vivek.medium.com/?source=post_page-----b33d9f810264--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b33d9f810264--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b33d9f810264--------------------------------)
    [Skanda Vivek](https://skanda-vivek.medium.com/?source=post_page-----b33d9f810264--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b33d9f810264--------------------------------)
    ·7 min read·Nov 14, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b33d9f810264--------------------------------)
    ·7 分钟阅读·2023 年 11 月 14 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/69621ace143918aee012edf5e1d62323.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/69621ace143918aee012edf5e1d62323.png)'
- en: Self-RAG Demo | Skanda Vivek
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Self-RAG 演示 | Skanda Vivek
- en: Large language models (LLMs) are all set to revolutionize various industries.
    Let’s take the example of the financial sector, wherein LLMs can be used to pore
    over troves of documents and find trends in a fraction of time and at a fraction
    of the cost of analysts doing the same task. But here’s the catch — the answers
    you get are only partial and incomplete many times. Take, for example, the case
    where you have a document containing company X’s annual revenue over the past
    15 years, but in different sections. In the standard Retrieval Augmented Generation
    (RAG) architecture as pictured below, you typically retrieve the top-k documents,
    or choose documents within a fixed context length.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）已经准备好革新各个行业。以金融行业为例，LLMs 可以用来分析大量文档，并在短时间内以低于分析师完成同样任务的成本找到趋势。但问题在于——你得到的答案很多时候只是部分的和不完整的。举个例子，假设你有一份包含公司
    X 在过去 15 年的年收入的文档，但这些信息分布在不同的部分。在下面所示的标准检索增强生成（RAG）架构中，你通常会检索前 k 个文档，或选择固定上下文长度内的文档。
- en: '![](../Images/4041cfd00e99aed333dcc5090fda5094.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4041cfd00e99aed333dcc5090fda5094.png)'
- en: RAG Prototype | Skanda Vivek
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 原型 | Skanda Vivek
- en: However, this can have several issues. One issue is wherein the top-k documents
    do not contain all the answers — maybe for example only corresponding to the last
    5 or 10 years. The other issue is that computing similarity between document chunks
    and prompt does not always yield relevant contexts. In this case, you could be
    getting a wrong answer.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这可能会有几个问题。一个问题是前 k 个文档不包含所有的答案——例如可能只对应于过去的 5 年或 10 年。另一个问题是计算文档片段与提示之间的相似性并不总是能得到相关的上下文。在这种情况下，你可能会得到错误的答案。
- en: A real issue is that you have developed your vanilla RAG app that works well
    in simple cases you test out — but this fails when you present this prototype
    to stakeholders and they ask some out of the box questions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一个实际的问题是你已经开发了一个普通的 RAG 应用程序，它在你测试的简单案例中工作良好——但当你将这个原型展示给利益相关者并且他们提出一些超出常规的问题时，它就会失败。
- en: This is where self-RAG comes to the rescue! The authors develop a clever way
    for a fine-tuned LM (Llama2–7B and 13B) to output special tokens [Retrieval],
    [No Retrieval], [Relevant], [Irrelevant], [No support / Contradictory], [Partially
    supported], [Utility], etc. appended to LM generations to decide whether or not
    a context is relevant/irrelevant, the LM generated text from the context is supported
    or not, and the utility of the generation.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 Self-RAG 大显身手的地方！作者开发了一种巧妙的方法，让经过微调的语言模型（Llama2–7B 和 13B）输出附加在 LM 生成的特殊标记
    [Retrieval]、[No Retrieval]、[Relevant]、[Irrelevant]、[No support / Contradictory]、[Partially
    supported]、[Utility] 等，用于决定上下文是否相关/无关，上下文中的 LM 生成文本是否得到支持，以及生成的实用性。
- en: '![](../Images/92869f6ffd2ad94ea5bff625d4cf8faa.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/92869f6ffd2ad94ea5bff625d4cf8faa.png)'
- en: '[Self-RAG](https://selfrag.github.io/)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[Self-RAG](https://selfrag.github.io/)'
- en: Training Self-RAG
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练 Self-RAG
- en: Self-RAG was trained in a **2-step hierarchical process**. In step 1, a simple
    LM was trained to classify a generated output (either just the prompt or prompt
    + RAG augmented output) and append the relevant special token at the end. This
    “critic model” was trained by GPT-4 annotations. Specifically, GPT-4 was prompted
    using a type-specific instruction (“Given an instruction, make a judgment on whether
    finding some external documents from the web helps to generate a better response.”)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Self-RAG 通过**2步层次化过程**进行了训练。在第一步中，训练了一个简单的语言模型（LM），用于对生成的输出（无论是仅有提示还是提示+RAG增强输出）进行分类，并在末尾附加相关的特殊标记。这个“评判模型”由
    GPT-4 注释进行训练。具体来说，GPT-4 通过特定类型的指令进行提示（“给定指令，判断从网络上查找一些外部文档是否有助于生成更好的回应。”）
- en: In step 2, the generator model model, using a standard next token prediction
    objective, learns to generate continuations, as well as special tokens to retrieve/critique
    generations. Unlike other fine-tuning or RLHF methods where downstream training
    can impact model outputs and make future generations biased, through this simple
    approach, the model is trained only to generate special tokens as appropriate,
    and otherwise not change the underlying LM! Which is brilliant!
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二步中，生成模型使用标准的下一个标记预测目标，学习生成延续内容，以及检索/评估生成内容的特殊标记。与其他微调或 RLHF 方法不同，这些方法可能会影响模型输出并使未来生成的内容产生偏差，通过这种简单的方法，模型仅被训练为在适当的时候生成特殊标记，而不会改变基础
    LM！这真是太棒了！
- en: Evaluating Self-RAG
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估 Self-RAG
- en: The authors performed a bunch of evaluations against public health fact verification,
    multiple-choice reasoning, Q&A, etc. There were 3 types of tasks. Closed-set tasks
    included fact verification and multiple-choice reasoning, and accuracy was used
    as the evaluation metric. Short-form generation tasks included open-domain Q&A
    datasets. The authors evaluated for whether or not gold answers are included in
    the model generations instead of strictly requiring exact matching.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 作者对公共健康事实验证、多项选择推理、问答等进行了大量评估。共有 3 种任务。封闭集任务包括事实验证和多项选择推理，准确率被用作评估指标。短文本生成任务包括开放领域的问答数据集。作者评估了模型生成中是否包含黄金答案，而不是严格要求精确匹配。
- en: Long-form generation included biography generation and long-form QA. For evaluating
    these tasks, the authors used FactScore to evaluate biographies — basically a
    measure of the various pieces of information generated, and their factual correctness.
    For long-form QA, citation precision and recall were used.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 长文本生成包括传记生成和长篇问答。对于这些任务的评估，作者使用 FactScore 来评估传记——基本上是生成的各种信息及其事实正确性的度量。对于长篇问答，使用了引用精度和召回率。
- en: '![](../Images/2fd0180f57a97da9452323f14bd7d8af.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2fd0180f57a97da9452323f14bd7d8af.png)'
- en: '[Self-RAG Eval](https://selfrag.github.io/)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[Self-RAG Eval](https://selfrag.github.io/)'
- en: Self-RAG performs the best among non-proprietary models, and in most cases the
    larger 13B parameter outperforms the 7B model. It even outperforms ChatGPT in
    some cases.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Self-RAG 在非专有模型中表现最佳，在大多数情况下，13B 参数的模型优于 7B 模型。在某些情况下，它甚至优于 ChatGPT。
- en: Inference
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推理
- en: For inference, the [self-RAG repository](https://github.com/AkariAsai/self-rag)
    suggests using [vllm](https://github.com/vllm-project/vllm) — an library for LLM
    inference.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对于推理，[self-RAG 存储库](https://github.com/AkariAsai/self-rag) 建议使用 [vllm](https://github.com/vllm-project/vllm)——一个用于
    LLM 推理的库。
- en: 'After pip installing vllm, you can load in the libraries and query as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装 vllm 后，你可以按如下方式加载库并进行查询：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: For a query that requires retrieval, you can supply the necessary information
    as a string in the example below.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于需要检索的查询，你可以按下面的示例提供必要的信息作为字符串。
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the above example, for the first query (related to social media platforms)
    the paragraph context is irrelevant, as reflected by the [Irrelevant] token at
    the beginning of the retrieval. The external context is however, relevant to the
    second query (related to llamas and alpacas). As you can see, it includes this
    information in the generated context, marked by the [Relevant] token.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述示例中，对于第一个查询（与社交媒体平台相关），段落上下文是不相关的，如检索开始时的 [Irrelevant] 令牌所示。然而，外部上下文与第二个查询（与美洲驼和羊驼相关）相关。正如你所见，它在生成的上下文中包含了这些信息，并用
    [Relevant] 令牌标记。
- en: But in the example below, the context “I like Avocado.” is unrelated to the
    prompt. As you can see below, the model prediction starts of as [Irrelevant] for
    both queries, and just uses internal information to answer the prompt.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 但在下面的示例中，上下文 “I like Avocado.” 与提示无关。如下面所示，模型预测在两个查询中都开始为 [Irrelevant]，并仅使用内部信息来回答提示。
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Takeaways
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重点总结
- en: Self-RAG has several advantages over the vanilla LLM.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Self-RAG 相比于普通 LLM 具有几个优势。
- en: 'Adaptive Passage Retrieval: By this, the LLM can keep retrieving context until
    all the relevant context is found (within the context window of course.)'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自适应段落检索：通过这一点，LLM 可以持续检索上下文直到找到所有相关上下文（当然是在上下文窗口内）。
- en: 'More relevant retrieval: A lot of times, embedding models are not the best
    at retrieving relevant context. Self-RAG potentially solves this with the relevant/irrelevant
    special token.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更相关的检索：很多时候，嵌入模型在检索相关上下文方面表现不佳。Self-RAG 通过相关/不相关特殊令牌潜在地解决了这个问题。
- en: 'Beats other similar models: Self-RAG beats other similar models, and also surprisingly
    beats ChatGPT in many tasks. It would be interesting to do a comparison with data
    that ChatGPT has not been trained on — so more proprietary, industrial data.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 超越其他类似模型：Self-RAG 超越了其他类似模型，并且在许多任务中意外地超过了 ChatGPT。对比 ChatGPT 未经过训练的数据——即更多的专有工业数据，将会非常有趣。
- en: 'Doesn’t change underlying LM: For me this is a huge upsell — as we know how
    fine-tuning and RLHF can lead to biased models very easily. Self-RAG seems to
    solve this by adding special tokens, and otherwise keeping text generation the
    same.'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不改变基础 LLM：对我来说，这是一个巨大的卖点——因为我们知道微调和 RLHF 很容易导致模型偏见。Self-RAG 似乎通过添加特殊令牌来解决这个问题，并保持文本生成不变。
- en: 'Some room for improvement though is in dealing with fixed context lengths.
    This might be achieved by also adding in a summarization component to Self-RAG.
    In fact there has been some previous work on this (See: [Improving Retrieval-Augmented
    LMs with Compression and Selective Augmentation](https://arxiv.org/abs/2310.04408)).
    Another exciting direction is the increase in context length window that just
    came out from OpenAI — with the GPT-4 128k context window update. However, as
    mentioned in [forums](https://www.reddit.com/r/OpenAI/comments/17pap5u/gpt4_turbo_128k_context_but_only_4k_output/?rdt=50609),
    this context window represents the input length, while the output limit is still
    at 4k tokens.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，处理固定上下文长度仍有改进的空间。这可能通过在 Self-RAG 中添加一个总结组件来实现。实际上，之前已有一些相关的工作（参见：[通过压缩和选择性增强改进检索增强型语言模型](https://arxiv.org/abs/2310.04408)）。另一个令人兴奋的方向是最近
    OpenAI 发布的增加上下文窗口长度——GPT-4 128k 上下文窗口更新。然而，正如在 [论坛](https://www.reddit.com/r/OpenAI/comments/17pap5u/gpt4_turbo_128k_context_but_only_4k_output/?rdt=50609)
    中提到的，这个上下文窗口表示输入长度，而输出限制仍然是 4k 令牌。
- en: RAG represents one of the most exciting ways industries can incorporate LLMs
    on their data, to generate real business impacts. However, there has not been
    too much RAG specific tuning of language models. I’m excited for future improvements
    in this space.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 代表了工业界将 LLM 融入其数据以产生实际业务影响的最激动人心的方式之一。然而，对于语言模型的 RAG 特定调优还不多。我对这个领域未来的改进感到兴奋。
- en: 'The inference code is in this GitHub repo:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 推断代码在这个 GitHub 仓库中：
- en: '[](https://github.com/skandavivek/self-RAG/tree/main?source=post_page-----b33d9f810264--------------------------------)
    [## GitHub - skandavivek/self-RAG: A tutorial on Self-RAG.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[## GitHub - skandavivek/self-RAG: 关于 Self-RAG 的教程。'
- en: A tutorial on Self-RAG. Contribute to skandavivek/self-RAG development by creating
    an account on GitHub.
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于 Self-RAG 的教程。通过在 GitHub 上创建一个账户来为 skandavivek/self-RAG 的开发做出贡献。
- en: github.com](https://github.com/skandavivek/self-RAG/tree/main?source=post_page-----b33d9f810264--------------------------------)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/skandavivek/self-RAG/tree/main?source=post_page-----b33d9f810264--------------------------------)'
- en: '*If you like this post, follow me — I write on Generative AI in real-world
    applications and, more generally, on the intersections between data and society.*'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你喜欢这篇文章，请关注我——我撰写有关生成式 AI 在实际应用中的内容，以及更广泛的数据与社会之间的交集。*'
- en: '*Feel free to connect on* [*LinkedIn*](https://www.linkedin.com/in/skanda-vivek-01619311b/)*!*'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*欢迎在* [*LinkedIn*](https://www.linkedin.com/in/skanda-vivek-01619311b/)*上联系我！*'
