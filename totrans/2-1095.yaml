- en: How Self-RAG Could Revolutionize Industrial LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-self-rag-could-revolutionize-industrial-llms-b33d9f810264](https://towardsdatascience.com/how-self-rag-could-revolutionize-industrial-llms-b33d9f810264)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s face it — vanilla RAG is pretty dumb. There’s no guarantee responses returned
    are relevant. Learn how Self-RAG can significantly help
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://skanda-vivek.medium.com/?source=post_page-----b33d9f810264--------------------------------)[![Skanda
    Vivek](../Images/9d25bee2fb75176ca7f7ea6eff7d7ab5.png)](https://skanda-vivek.medium.com/?source=post_page-----b33d9f810264--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b33d9f810264--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b33d9f810264--------------------------------)
    [Skanda Vivek](https://skanda-vivek.medium.com/?source=post_page-----b33d9f810264--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b33d9f810264--------------------------------)
    ·7 min read·Nov 14, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/69621ace143918aee012edf5e1d62323.png)'
  prefs: []
  type: TYPE_IMG
- en: Self-RAG Demo | Skanda Vivek
  prefs: []
  type: TYPE_NORMAL
- en: Large language models (LLMs) are all set to revolutionize various industries.
    Let’s take the example of the financial sector, wherein LLMs can be used to pore
    over troves of documents and find trends in a fraction of time and at a fraction
    of the cost of analysts doing the same task. But here’s the catch — the answers
    you get are only partial and incomplete many times. Take, for example, the case
    where you have a document containing company X’s annual revenue over the past
    15 years, but in different sections. In the standard Retrieval Augmented Generation
    (RAG) architecture as pictured below, you typically retrieve the top-k documents,
    or choose documents within a fixed context length.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4041cfd00e99aed333dcc5090fda5094.png)'
  prefs: []
  type: TYPE_IMG
- en: RAG Prototype | Skanda Vivek
  prefs: []
  type: TYPE_NORMAL
- en: However, this can have several issues. One issue is wherein the top-k documents
    do not contain all the answers — maybe for example only corresponding to the last
    5 or 10 years. The other issue is that computing similarity between document chunks
    and prompt does not always yield relevant contexts. In this case, you could be
    getting a wrong answer.
  prefs: []
  type: TYPE_NORMAL
- en: A real issue is that you have developed your vanilla RAG app that works well
    in simple cases you test out — but this fails when you present this prototype
    to stakeholders and they ask some out of the box questions.
  prefs: []
  type: TYPE_NORMAL
- en: This is where self-RAG comes to the rescue! The authors develop a clever way
    for a fine-tuned LM (Llama2–7B and 13B) to output special tokens [Retrieval],
    [No Retrieval], [Relevant], [Irrelevant], [No support / Contradictory], [Partially
    supported], [Utility], etc. appended to LM generations to decide whether or not
    a context is relevant/irrelevant, the LM generated text from the context is supported
    or not, and the utility of the generation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/92869f6ffd2ad94ea5bff625d4cf8faa.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Self-RAG](https://selfrag.github.io/)'
  prefs: []
  type: TYPE_NORMAL
- en: Training Self-RAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Self-RAG was trained in a **2-step hierarchical process**. In step 1, a simple
    LM was trained to classify a generated output (either just the prompt or prompt
    + RAG augmented output) and append the relevant special token at the end. This
    “critic model” was trained by GPT-4 annotations. Specifically, GPT-4 was prompted
    using a type-specific instruction (“Given an instruction, make a judgment on whether
    finding some external documents from the web helps to generate a better response.”)
  prefs: []
  type: TYPE_NORMAL
- en: In step 2, the generator model model, using a standard next token prediction
    objective, learns to generate continuations, as well as special tokens to retrieve/critique
    generations. Unlike other fine-tuning or RLHF methods where downstream training
    can impact model outputs and make future generations biased, through this simple
    approach, the model is trained only to generate special tokens as appropriate,
    and otherwise not change the underlying LM! Which is brilliant!
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating Self-RAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The authors performed a bunch of evaluations against public health fact verification,
    multiple-choice reasoning, Q&A, etc. There were 3 types of tasks. Closed-set tasks
    included fact verification and multiple-choice reasoning, and accuracy was used
    as the evaluation metric. Short-form generation tasks included open-domain Q&A
    datasets. The authors evaluated for whether or not gold answers are included in
    the model generations instead of strictly requiring exact matching.
  prefs: []
  type: TYPE_NORMAL
- en: Long-form generation included biography generation and long-form QA. For evaluating
    these tasks, the authors used FactScore to evaluate biographies — basically a
    measure of the various pieces of information generated, and their factual correctness.
    For long-form QA, citation precision and recall were used.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2fd0180f57a97da9452323f14bd7d8af.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Self-RAG Eval](https://selfrag.github.io/)'
  prefs: []
  type: TYPE_NORMAL
- en: Self-RAG performs the best among non-proprietary models, and in most cases the
    larger 13B parameter outperforms the 7B model. It even outperforms ChatGPT in
    some cases.
  prefs: []
  type: TYPE_NORMAL
- en: Inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For inference, the [self-RAG repository](https://github.com/AkariAsai/self-rag)
    suggests using [vllm](https://github.com/vllm-project/vllm) — an library for LLM
    inference.
  prefs: []
  type: TYPE_NORMAL
- en: 'After pip installing vllm, you can load in the libraries and query as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: For a query that requires retrieval, you can supply the necessary information
    as a string in the example below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In the above example, for the first query (related to social media platforms)
    the paragraph context is irrelevant, as reflected by the [Irrelevant] token at
    the beginning of the retrieval. The external context is however, relevant to the
    second query (related to llamas and alpacas). As you can see, it includes this
    information in the generated context, marked by the [Relevant] token.
  prefs: []
  type: TYPE_NORMAL
- en: But in the example below, the context “I like Avocado.” is unrelated to the
    prompt. As you can see below, the model prediction starts of as [Irrelevant] for
    both queries, and just uses internal information to answer the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Takeaways
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Self-RAG has several advantages over the vanilla LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'Adaptive Passage Retrieval: By this, the LLM can keep retrieving context until
    all the relevant context is found (within the context window of course.)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'More relevant retrieval: A lot of times, embedding models are not the best
    at retrieving relevant context. Self-RAG potentially solves this with the relevant/irrelevant
    special token.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Beats other similar models: Self-RAG beats other similar models, and also surprisingly
    beats ChatGPT in many tasks. It would be interesting to do a comparison with data
    that ChatGPT has not been trained on — so more proprietary, industrial data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Doesn’t change underlying LM: For me this is a huge upsell — as we know how
    fine-tuning and RLHF can lead to biased models very easily. Self-RAG seems to
    solve this by adding special tokens, and otherwise keeping text generation the
    same.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Some room for improvement though is in dealing with fixed context lengths.
    This might be achieved by also adding in a summarization component to Self-RAG.
    In fact there has been some previous work on this (See: [Improving Retrieval-Augmented
    LMs with Compression and Selective Augmentation](https://arxiv.org/abs/2310.04408)).
    Another exciting direction is the increase in context length window that just
    came out from OpenAI — with the GPT-4 128k context window update. However, as
    mentioned in [forums](https://www.reddit.com/r/OpenAI/comments/17pap5u/gpt4_turbo_128k_context_but_only_4k_output/?rdt=50609),
    this context window represents the input length, while the output limit is still
    at 4k tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: RAG represents one of the most exciting ways industries can incorporate LLMs
    on their data, to generate real business impacts. However, there has not been
    too much RAG specific tuning of language models. I’m excited for future improvements
    in this space.
  prefs: []
  type: TYPE_NORMAL
- en: 'The inference code is in this GitHub repo:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/skandavivek/self-RAG/tree/main?source=post_page-----b33d9f810264--------------------------------)
    [## GitHub - skandavivek/self-RAG: A tutorial on Self-RAG.'
  prefs: []
  type: TYPE_NORMAL
- en: A tutorial on Self-RAG. Contribute to skandavivek/self-RAG development by creating
    an account on GitHub.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/skandavivek/self-RAG/tree/main?source=post_page-----b33d9f810264--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*If you like this post, follow me — I write on Generative AI in real-world
    applications and, more generally, on the intersections between data and society.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Feel free to connect on* [*LinkedIn*](https://www.linkedin.com/in/skanda-vivek-01619311b/)*!*'
  prefs: []
  type: TYPE_NORMAL
