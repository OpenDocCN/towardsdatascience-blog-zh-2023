- en: Working with Hugging Face Datasets
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Hugging Face 数据集
- en: 原文：[https://towardsdatascience.com/working-with-hugging-face-datasets-bba14dd8da68](https://towardsdatascience.com/working-with-hugging-face-datasets-bba14dd8da68)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/working-with-hugging-face-datasets-bba14dd8da68](https://towardsdatascience.com/working-with-hugging-face-datasets-bba14dd8da68)
- en: Learn how to access the datasets on Hugging Face Hub and how you can load them
    remotely using DuckDB and the Datasets library
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解如何访问 Hugging Face Hub 上的数据集，以及如何使用 DuckDB 和 Datasets 库远程加载它们
- en: '[](https://weimenglee.medium.com/?source=post_page-----bba14dd8da68--------------------------------)[![Wei-Meng
    Lee](../Images/10fc13e8a6858502d6a7b89fcaad7a10.png)](https://weimenglee.medium.com/?source=post_page-----bba14dd8da68--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bba14dd8da68--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bba14dd8da68--------------------------------)
    [Wei-Meng Lee](https://weimenglee.medium.com/?source=post_page-----bba14dd8da68--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://weimenglee.medium.com/?source=post_page-----bba14dd8da68--------------------------------)[![Wei-Meng
    Lee](../Images/10fc13e8a6858502d6a7b89fcaad7a10.png)](https://weimenglee.medium.com/?source=post_page-----bba14dd8da68--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bba14dd8da68--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bba14dd8da68--------------------------------)
    [Wei-Meng Lee](https://weimenglee.medium.com/?source=post_page-----bba14dd8da68--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bba14dd8da68--------------------------------)
    ·13 min read·Jun 29, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bba14dd8da68--------------------------------)
    ·阅读时间 13 分钟·2023年6月29日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/8b1abe24851b072185f9290f0bf2b4de.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8b1abe24851b072185f9290f0bf2b4de.png)'
- en: Photo by [Lars Kienle](https://unsplash.com/@larskienle?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Lars Kienle](https://unsplash.com/@larskienle?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: As an AI platform, Hugging Face builds, trains and deploys state of the art
    open source machine learning models. In addition to hosting all these trained
    models, Hugging Face also hosts datasets ([https://huggingface.co/datasets](https://huggingface.co/datasets)),
    where you can make use of them for your own projects.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个 AI 平台，Hugging Face 构建、训练并部署最先进的开源机器学习模型。除了托管所有这些训练好的模型外，Hugging Face 还托管数据集（[https://huggingface.co/datasets](https://huggingface.co/datasets)），你可以将它们用于自己的项目。
- en: 'In this article, I will show you how you can access the datasets in Hugging
    Face, and how you can programmatically download them onto your local computer.
    Specifically, I will show you how to:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将展示如何访问 Hugging Face 中的数据集，并如何通过编程将它们下载到本地计算机上。具体来说，我将展示如何：
- en: load the datasets remotely using DuckDB’s support for **httpfs**
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 DuckDB 对 **httpfs** 的支持远程加载数据集
- en: stream the datasets using the **Datasets** library by Hugging Face
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Hugging Face 的 **Datasets** 库流式传输数据集
- en: Hugging Face Datasets server
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hugging Face 数据集服务器
- en: '**Hugging Face Datasets server** is a lightweight web API for visualizing all
    the different types of dataset stored on the Hugging Face Hub. You can use the
    provided REST API to query datasets stored on the Hugging Face Hub. The following
    sections provide a short tutorial on the things you could do with the API at `[https://datasets-server.huggingface.co/](https://datasets-server.huggingface.co/)`.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**Hugging Face 数据集服务器**是一个轻量级的 Web API，用于可视化存储在 Hugging Face Hub 上的各种数据集。你可以使用提供的
    REST API 查询存储在 Hugging Face Hub 上的数据集。接下来的部分提供了关于如何使用 API 的简短教程，访问地址为 `[https://datasets-server.huggingface.co/](https://datasets-server.huggingface.co/)`。'
- en: Getting a list of datasets hosted on the Hub
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取 Hub 上托管的数据集列表
- en: 'To get a list of datasets that you can retrieve from Hugging Face, use the
    following statement with the `valid` endpoint:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取可以从 Hugging Face 检索的数据集列表，请使用以下语句和`valid`端点：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You will see a JSON result as shown below:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你将看到如下的 JSON 结果：
- en: '![](../Images/9f68756431029635d7316dc9b428309d.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9f68756431029635d7316dc9b428309d.png)'
- en: The datasets that can work without errors are listed in the value of the `valid`
    key in the result. An example of a valid dataset above is `0-hero/OIG-small-chip2`.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 可以正常工作的数据集列在结果中`valid`键的值中。上面的有效数据集示例是`0-hero/OIG-small-chip2`。
- en: Validating a dataset
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 验证数据集
- en: 'To validate a dataset, use the following statement with the `is-valid` endpoint
    together with the `dataset` parameter:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 要验证数据集，请使用以下语句，结合`is-valid`端点和`dataset`参数：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If the dataset is valid, you will see the following result:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据集有效，你将看到以下结果：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Getting the list of configurations and splits of a dataset
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取数据集的配置和分割列表
- en: A dataset typically have *splits* (training set, validation set, and testing
    set). They may also have configurations — sub-dataset within a larger dataset.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一个数据集通常有*splits*（训练集、验证集和测试集）。它们也可能有配置——在更大数据集中的子数据集。
- en: 'Configurations are common for multilingual speech datasets. For more details
    on splits, visit: [https://huggingface.co/docs/datasets-server/splits](https://huggingface.co/docs/datasets-server/splits).'
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 配置对于多语言语音数据集是常见的。有关分割的更多细节，请访问：[https://huggingface.co/docs/datasets-server/splits](https://huggingface.co/docs/datasets-server/splits)。
- en: 'To get the splits of a dataset, use the following statement with the `splits`
    endpoint and the `dataset` parameter:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取数据集的分割，请使用以下语句，结合`splits`端点和`dataset`参数：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following result will be returned:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下结果将被返回：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: For this dataset, there is only a single `train` split.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于该数据集，只有一个`train`分割。
- en: 'Here is an example of a dataset (“**duorc**”) that has multiple splits and
    configurations:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个数据集（“**duorc**”）的示例，具有多个分割和配置：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Obtaining the first 100 rows
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取前100行
- en: 'Often, you might want to retrieve a subset of the dataset for examination purposes,
    instead of downloading everything. The following statement uses the `first-rows`
    endpoint with the `dataset`, `config`, and `split` parameters to extract the first
    100 rows of the specified dataset:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，你可能希望检索数据集的子集进行检查，而不是下载所有数据。以下语句使用`first-rows`端点，结合`dataset`、`config`和`split`参数，从指定数据集中提取前100行：
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The JSON result will return the first 100 rows of the specified dataset:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: JSON结果将返回指定数据集的前100行：
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Getting a slice of the dataset
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取数据集的切片
- en: 'You can also get a slice of the dataset. The following statement uses the `rows`
    endpoint with the `dataset`, `config`, `split`, `offset`, and `length` parameters
    to extract three rows of the specified dataset starting from the third row:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以获取数据集的切片。以下语句使用`rows`端点，结合`dataset`、`config`、`split`、`offset`和`length`参数，从指定数据集的第三行开始提取三行：
- en: '[PRE8]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The result will look like this:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '[PRE9]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Getting the Parquet files of a dataset
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取数据集的Parquet文件
- en: While the datasets in Hugging Face Hub can be published in a wide variety of
    formats (CSV, JSONL, etc), the **Datasets server** automatically converts all
    public datasets to the *Parquet* format. The Parquet format offers significant
    performance improvements, especially for large datasets. Later sections will demonstrate
    that.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Hugging Face Hub上的数据集可以以多种格式（CSV、JSONL等）发布，但**Datasets server**会自动将所有公共数据集转换为*Parquet*格式。Parquet格式提供了显著的性能提升，特别是对于大型数据集。稍后的章节将对此进行演示。
- en: 'Apache Parquet is a file format that is designed to support fast data processing
    for complex data. For more information on Parquet, read my earlier article:'
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Apache Parquet是一种文件格式，旨在支持复杂数据的快速处理。有关Parquet的更多信息，请阅读我之前的文章：
- en: '[](/still-saving-your-data-in-csv-try-these-other-options-9abe8b83db3a?source=post_page-----bba14dd8da68--------------------------------)
    [## Still Saving Your Data in CSV? Try these other options'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/still-saving-your-data-in-csv-try-these-other-options-9abe8b83db3a?source=post_page-----bba14dd8da68--------------------------------)
    [## 仍在以CSV格式保存数据？试试这些其他选项'
- en: Learn how to save your data in different formats (CSV, compressions, Pickle,
    and Parquet) to save storage and reduce…
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解如何以不同格式（CSV、压缩、Pickle和Parquet）保存数据，以节省存储空间并减少……
- en: towardsdatascience.com](/still-saving-your-data-in-csv-try-these-other-options-9abe8b83db3a?source=post_page-----bba14dd8da68--------------------------------)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/still-saving-your-data-in-csv-try-these-other-options-9abe8b83db3a?source=post_page-----bba14dd8da68--------------------------------)
- en: 'To load the dataset in Parquet format, use the following statement with the
    `parquet` endpoint and the `dataset` parameter:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要以Parquet格式加载数据集，请使用以下语句，结合`parquet`端点和`dataset`参数：
- en: '[PRE10]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The above statement returns the following JSON result:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 上述语句返回以下JSON结果：
- en: '[PRE11]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In particular, the value of the `url` key specifies the location where you can
    download the dataset in Parquet format, which is `[https://huggingface.co/datasets/0-hero/OIG-small-chip2/resolve/refs%2Fconvert%2Fparquet/0-hero--OIG-small-chip2/parquet-train.parquet](https://huggingface.co/datasets/0-hero/OIG-small-chip2/resolve/refs%2Fconvert%2Fparquet/0-hero--OIG-small-chip2/parquet-train.parquet)`
    in this example.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，`url` 键的值指定了你可以下载 Parquet 格式数据集的位置，在这个例子中是 `[https://huggingface.co/datasets/0-hero/OIG-small-chip2/resolve/refs%2Fconvert%2Fparquet/0-hero--OIG-small-chip2/parquet-train.parquet](https://huggingface.co/datasets/0-hero/OIG-small-chip2/resolve/refs%2Fconvert%2Fparquet/0-hero--OIG-small-chip2/parquet-train.parquet)`。
- en: Downloading the Datasets Programmatically
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 程序化下载数据集
- en: Now that you have seen how to use the **Datasets server REST API**, let’s see
    how you can download the datasets programmatically.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了如何使用 **Datasets server REST API**，让我们看看如何程序化地下载数据集。
- en: 'In Python, the easiest way is to use the `requests` library:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，最简单的方法是使用 `requests` 库：
- en: '[PRE12]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The result of the `json()` function is a Python dictionary:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`json()` 函数的结果是一个 Python 字典：'
- en: '[PRE13]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Using this dictionary result, you can use list comprehension to find the URL
    for the dataset in Parquet format:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这个字典结果，你可以使用列表推导式来查找 Parquet 格式的数据集的 URL：
- en: '[PRE14]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The `urls` variable is a list containing a list of URLs for the dataset under
    the training set:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`urls` 变量是一个包含训练集下的数据集 URL 列表的列表：'
- en: '[PRE15]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Downloading the Parquet file using DuckDB
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 DuckDB 下载 Parquet 文件
- en: If you use DuckDB, you can actually use DuckDB to remotely load a dataset.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用 DuckDB，你实际上可以使用 DuckDB 远程加载数据集。
- en: 'If you are new to DuckDB, you can read up on the basics from this article:'
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你对 DuckDB 不太熟悉，你可以从这篇文章中了解基础知识：
- en: '[](https://levelup.gitconnected.com/using-duckdb-for-data-analytics-bab3e3ff032c?source=post_page-----bba14dd8da68--------------------------------)
    [## Using DuckDB for Data Analytics'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://levelup.gitconnected.com/using-duckdb-for-data-analytics-bab3e3ff032c?source=post_page-----bba14dd8da68--------------------------------)
    [## 使用 DuckDB 进行数据分析'
- en: Learn how to use SQL to perform data analytics
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解如何使用 SQL 进行数据分析
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/using-duckdb-for-data-analytics-bab3e3ff032c?source=post_page-----bba14dd8da68--------------------------------)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: levelup.gitconnected.com](https://levelup.gitconnected.com/using-duckdb-for-data-analytics-bab3e3ff032c?source=post_page-----bba14dd8da68--------------------------------)
- en: 'First, ensure you install DuckDB if you have not done so:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，确保你已安装 DuckDB（如果尚未安装）：
- en: '[PRE16]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then, create a DuckDB instance and install **httpfs**:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，创建一个 DuckDB 实例并安装 **httpfs**：
- en: '[PRE17]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The **httpfs** extension is a loadable extension implementing a file system
    that allows reading remote/writing remote files.
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**httpfs** 扩展是一个可加载的扩展，实现了一个文件系统，允许读取远程/写入远程文件。'
- en: 'Once **httpfs** is installed and loaded, you can load the Parquet dataset from
    Hugging Face Hub by using a SQL query:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 **httpfs** 安装并加载完成，你可以通过使用 SQL 查询从 Hugging Face Hub 加载 Parquet 数据集：
- en: '[PRE18]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The `df()` function above converts the result of the query to a Pandas DataFrame:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 上述 `df()` 函数将查询结果转换为 Pandas DataFrame：
- en: '![](../Images/f510cc5e630d8e7cf72d10a3e0dda1c2.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f510cc5e630d8e7cf72d10a3e0dda1c2.png)'
- en: Image by author
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'One great feature of Parquet is that Parquet stores files in columnar format.
    And so if your query only requests for only a single column, only that requested
    column is downloaded to your computer:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 的一个很棒的特性是 Parquet 以列式格式存储文件。因此，如果你的查询仅请求单一列，只有该请求的列会被下载到你的计算机上：
- en: '[PRE19]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'In the above query, only the “`user`” column is downloaded:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述查询中，只有“`user`”列被下载：
- en: '![](../Images/4854a971693abf64f18c25f058be9a62.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4854a971693abf64f18c25f058be9a62.png)'
- en: Image by author
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: This Parquet feature is especially useful for large dataset — imagine the time
    and space you can save by only downloading the columns you need.
  id: totrans-87
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Parquet 特性对于大型数据集特别有用——想象一下，通过只下载你需要的列，你可以节省多少时间和空间。
- en: 'In some cases, you don’t even need to download the data at all. Consider the
    following query:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，你甚至不需要下载数据。考虑以下查询：
- en: '[PRE20]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](../Images/4c1213c3795dbc47fd24243a187e089f.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c1213c3795dbc47fd24243a187e089f.png)'
- en: Image by author
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: No data needs to be downloaded as this request can be fulfilled simply by reading
    the metadata of the dataset.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要下载任何数据，因为这个请求可以通过读取数据集的元数据来满足。
- en: 'Here is another example of using DuckDB to download another dataset (**“mstz/heart_failure**”):'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这是另一个使用 DuckDB 下载另一个数据集（**“mstz/heart_failure**”）的示例：
- en: '[PRE21]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This dataset has 299 rows and 13 columns:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集有 299 行和 13 列：
- en: '![](../Images/4d7d3bd037ff543a642dd02fca3c07d1.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d7d3bd037ff543a642dd02fca3c07d1.png)'
- en: Image by author
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'We could perform some aggregation on the **age** column:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对**age**列进行一些聚合操作：
- en: '[PRE22]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Here’s the result:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这是结果：
- en: '![](../Images/b06c3c33641b5b0fce87bd6b6a827e87.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b06c3c33641b5b0fce87bd6b6a827e87.png)'
- en: Image by author
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于作者
- en: 'Using the result, we could also plot a bar plot:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 使用结果，我们还可以绘制柱状图：
- en: '[PRE23]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![](../Images/54e62437baa867e03c3aac6ed478571e.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/54e62437baa867e03c3aac6ed478571e.png)'
- en: Image by author
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于作者
- en: Using the Datasets library
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Datasets库
- en: To make working with data from Hugging Face easy and efficient, Hugging Face
    has its own `Datasets` library ([https://github.com/huggingface/datasets](https://github.com/huggingface/datasets)).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使与Hugging Face的数据工作变得简单而高效，Hugging Face有自己的`Datasets`库（[https://github.com/huggingface/datasets](https://github.com/huggingface/datasets)）。
- en: 'To install the `datasets` library, use the `pip` command:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装`datasets`库，使用`pip`命令：
- en: '[PRE24]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The `load_dataset()` function loads the specified dataset:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`load_dataset()`函数加载指定的数据集：'
- en: '[PRE25]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'When you load the dataset for the first time, the entire dataset (in Parquet
    format) is downloaded to your computer:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 当你第一次加载数据集时，整个数据集（以Parquet格式）会下载到你的计算机上：
- en: '![](../Images/48044bf4bcdc6946161de9a6fdd45212.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/48044bf4bcdc6946161de9a6fdd45212.png)'
- en: Image by author
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于作者
- en: 'The type of data of the returned `dataset` is `datasets.arrow_dataset.Dataset`.
    So what can you do with it? First, you can convert it to a Pandas DataFrame:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的`dataset`的数据类型是`datasets.arrow_dataset.Dataset`。那你可以用它做什么呢？首先，你可以将其转换为Pandas
    DataFrame：
- en: '[PRE26]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![](../Images/e217ec404a175f0d5b4cd915d2809f78.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e217ec404a175f0d5b4cd915d2809f78.png)'
- en: Image by author
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于作者
- en: 'You can also get the first row of the dataset by using an index:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以通过使用索引来获取数据集的第一行：
- en: '[PRE27]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This will return the first row of the data:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回数据的第一行：
- en: '[PRE28]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: There are a bunch of other things you can do with this `datasets.arrow_dataset.Dataset`
    object. I will leave it to you to explore further.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 对这个`datasets.arrow_dataset.Dataset`对象，你还可以做很多其他事情。我将留给你进一步探索。
- en: Streaming the dataset
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流式传输数据集
- en: 'Again, when dealing with large datasets, it is not feasible to download the
    entire dataset to your computer before you do anything with it. In the previous
    section, calling the `load_dataset()` function downloads the entire dataset onto
    my computer:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，在处理大型数据集时，不可能在进行任何操作之前将整个数据集下载到计算机上。在前面的部分中，调用`load_dataset()`函数会将整个数据集下载到我的计算机上：
- en: '![](../Images/e74e5d94366d5e8e0d7d5479ce4990ef.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e74e5d94366d5e8e0d7d5479ce4990ef.png)'
- en: Image by author
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于作者
- en: This particular dataset it took up 82.2MB of disk space. You can imagine the
    time and disk space needed for larger datasets.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这个特定的数据集占用了82.2MB的磁盘空间。你可以想象更大的数据集所需的时间和磁盘空间。
- en: 'Fortunately, the **Datasets** library supports ***streaming***. Dataset streaming
    lets you work with a dataset without downloading it — the data is streamed as
    you iterate over the dataset. To use streaming, set the `streaming` parameter
    to `True` in the `load_dataset()` function:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，**Datasets**库支持***流式传输***。数据集流式传输让你在不下载数据集的情况下进行操作——数据在你遍历数据集时被流式传输。要使用流式传输，在`load_dataset()`函数中将`streaming`参数设置为`True`：
- en: '[PRE29]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The type of `dataset` is now `datasets.iterable_dataset.IterableDataset`, instead
    of `datasets.arrow_dataset.Dataset`. So how do you use it? You can use the `iter()`
    function on it, which returns an `iterator` object:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '`dataset`的类型现在是`datasets.iterable_dataset.IterableDataset`，而不是`datasets.arrow_dataset.Dataset`。那么你该如何使用它呢？你可以对它使用`iter()`函数，这将返回一个`iterator`对象：'
- en: '[PRE30]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'To get a row, call the `next()` function, which returns the next item in an
    iterator:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取一行，调用`next()`函数，它将返回迭代器中的下一个项目：
- en: '[PRE31]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'You will now see first row as a dictionary:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在将看到第一行作为字典：
- en: '[PRE32]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Calling the `next()` function on `i` again will return the next row:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 对`i`调用`next()`函数将返回下一行：
- en: '[PRE33]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: And so on.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 依此类推。
- en: Shuffling the dataset
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 打乱数据集
- en: 'You can also shuffle the dataset by using the `shuffle()` function on the `dataset`
    variable, like this:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以通过对`dataset`变量使用`shuffle()`函数来打乱数据集，如下所示：
- en: '[PRE34]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: In the above example, say your dataset has 10,000 rows. The `shuffle()` function
    will randomly select examples from the first five hundred rows in the buffer.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述示例中，假设你的数据集有10,000行。`shuffle()`函数将从缓冲区的前500行中随机选择示例。
- en: By default, the buffer size is 1,000.
  id: totrans-145
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 默认情况下，缓冲区大小为1,000。
- en: Other tasks
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他任务
- en: 'You can perform more tasks using streaming, such as:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用流式传输执行更多任务，例如：
- en: splitting the dataset
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 划分数据集
- en: Interleaving the dataset — combining two datasets by alternating rows between
    each dataset
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交错数据集——通过在每个数据集之间交替行来组合两个数据集
- en: Modifying the columns of a dataset
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改数据集的列
- en: Filtering a dataset
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤数据集
- en: Check out [https://huggingface.co/docs/datasets/stream](https://huggingface.co/docs/datasets/stream)
    for more details.
  id: totrans-152
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 查看更多细节请访问 [https://huggingface.co/docs/datasets/stream](https://huggingface.co/docs/datasets/stream)。
- en: '**If you like reading my articles and that it helped your career/study, please
    consider signing up as a Medium member. It is $5 a month, and it gives you unlimited
    access to all the articles (including mine) on Medium. If you sign up using the
    following link, I will earn a small commission (at no additional cost to you).
    Your support means that I will be able to devote more time on writing articles
    like this.**'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果你喜欢阅读我的文章，并且觉得它对你的职业/学习有帮助，请考虑注册成为 Medium 会员。每月 $5，即可无限访问 Medium 上的所有文章（包括我的）。如果你通过以下链接注册，我将获得少量佣金（对你没有额外费用）。你的支持意味着我可以投入更多时间撰写像这样的文章。**'
- en: '[](https://weimenglee.medium.com/membership?source=post_page-----bba14dd8da68--------------------------------)
    [## Join Medium with my referral link - Wei-Meng Lee'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://weimenglee.medium.com/membership?source=post_page-----bba14dd8da68--------------------------------)
    [## 通过我的推荐链接加入 Medium - 韦梦·李'
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story…
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 作为 Medium 会员，您的会员费的一部分将用于支持您阅读的作者，并且您可以完全访问每一个故事……
- en: weimenglee.medium.com](https://weimenglee.medium.com/membership?source=post_page-----bba14dd8da68--------------------------------)
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://weimenglee.medium.com/membership?source=post_page-----bba14dd8da68--------------------------------](https://weimenglee.medium.com/membership?source=post_page-----bba14dd8da68--------------------------------)'
- en: Summary
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this article, I have shown you how you can access the datasets stored on
    Hugging Face Hub. Since the datasets are stored in Parquet format, it allows you
    to remotely access the datasets remotely without needing to download the entire
    bulk of the dataset. You can access the datasets either using DuckDB, or using
    the Datasets library provided by Hugging Face.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我展示了如何访问存储在 Hugging Face Hub 上的数据集。由于数据集以 Parquet 格式存储，因此您可以远程访问这些数据集，而无需下载整个数据集。您可以使用
    DuckDB 或 Hugging Face 提供的数据集库来访问这些数据集。
