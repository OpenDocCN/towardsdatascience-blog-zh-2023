- en: Working with Hugging Face Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/working-with-hugging-face-datasets-bba14dd8da68](https://towardsdatascience.com/working-with-hugging-face-datasets-bba14dd8da68)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learn how to access the datasets on Hugging Face Hub and how you can load them
    remotely using DuckDB and the Datasets library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://weimenglee.medium.com/?source=post_page-----bba14dd8da68--------------------------------)[![Wei-Meng
    Lee](../Images/10fc13e8a6858502d6a7b89fcaad7a10.png)](https://weimenglee.medium.com/?source=post_page-----bba14dd8da68--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bba14dd8da68--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bba14dd8da68--------------------------------)
    [Wei-Meng Lee](https://weimenglee.medium.com/?source=post_page-----bba14dd8da68--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bba14dd8da68--------------------------------)
    ·13 min read·Jun 29, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8b1abe24851b072185f9290f0bf2b4de.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Lars Kienle](https://unsplash.com/@larskienle?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: As an AI platform, Hugging Face builds, trains and deploys state of the art
    open source machine learning models. In addition to hosting all these trained
    models, Hugging Face also hosts datasets ([https://huggingface.co/datasets](https://huggingface.co/datasets)),
    where you can make use of them for your own projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, I will show you how you can access the datasets in Hugging
    Face, and how you can programmatically download them onto your local computer.
    Specifically, I will show you how to:'
  prefs: []
  type: TYPE_NORMAL
- en: load the datasets remotely using DuckDB’s support for **httpfs**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: stream the datasets using the **Datasets** library by Hugging Face
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hugging Face Datasets server
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Hugging Face Datasets server** is a lightweight web API for visualizing all
    the different types of dataset stored on the Hugging Face Hub. You can use the
    provided REST API to query datasets stored on the Hugging Face Hub. The following
    sections provide a short tutorial on the things you could do with the API at `[https://datasets-server.huggingface.co/](https://datasets-server.huggingface.co/)`.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting a list of datasets hosted on the Hub
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To get a list of datasets that you can retrieve from Hugging Face, use the
    following statement with the `valid` endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see a JSON result as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9f68756431029635d7316dc9b428309d.png)'
  prefs: []
  type: TYPE_IMG
- en: The datasets that can work without errors are listed in the value of the `valid`
    key in the result. An example of a valid dataset above is `0-hero/OIG-small-chip2`.
  prefs: []
  type: TYPE_NORMAL
- en: Validating a dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To validate a dataset, use the following statement with the `is-valid` endpoint
    together with the `dataset` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'If the dataset is valid, you will see the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Getting the list of configurations and splits of a dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A dataset typically have *splits* (training set, validation set, and testing
    set). They may also have configurations — sub-dataset within a larger dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Configurations are common for multilingual speech datasets. For more details
    on splits, visit: [https://huggingface.co/docs/datasets-server/splits](https://huggingface.co/docs/datasets-server/splits).'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'To get the splits of a dataset, use the following statement with the `splits`
    endpoint and the `dataset` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following result will be returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: For this dataset, there is only a single `train` split.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of a dataset (“**duorc**”) that has multiple splits and
    configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Obtaining the first 100 rows
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Often, you might want to retrieve a subset of the dataset for examination purposes,
    instead of downloading everything. The following statement uses the `first-rows`
    endpoint with the `dataset`, `config`, and `split` parameters to extract the first
    100 rows of the specified dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The JSON result will return the first 100 rows of the specified dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Getting a slice of the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can also get a slice of the dataset. The following statement uses the `rows`
    endpoint with the `dataset`, `config`, `split`, `offset`, and `length` parameters
    to extract three rows of the specified dataset starting from the third row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The result will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Getting the Parquet files of a dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the datasets in Hugging Face Hub can be published in a wide variety of
    formats (CSV, JSONL, etc), the **Datasets server** automatically converts all
    public datasets to the *Parquet* format. The Parquet format offers significant
    performance improvements, especially for large datasets. Later sections will demonstrate
    that.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apache Parquet is a file format that is designed to support fast data processing
    for complex data. For more information on Parquet, read my earlier article:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](/still-saving-your-data-in-csv-try-these-other-options-9abe8b83db3a?source=post_page-----bba14dd8da68--------------------------------)
    [## Still Saving Your Data in CSV? Try these other options'
  prefs: []
  type: TYPE_NORMAL
- en: Learn how to save your data in different formats (CSV, compressions, Pickle,
    and Parquet) to save storage and reduce…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/still-saving-your-data-in-csv-try-these-other-options-9abe8b83db3a?source=post_page-----bba14dd8da68--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'To load the dataset in Parquet format, use the following statement with the
    `parquet` endpoint and the `dataset` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The above statement returns the following JSON result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In particular, the value of the `url` key specifies the location where you can
    download the dataset in Parquet format, which is `[https://huggingface.co/datasets/0-hero/OIG-small-chip2/resolve/refs%2Fconvert%2Fparquet/0-hero--OIG-small-chip2/parquet-train.parquet](https://huggingface.co/datasets/0-hero/OIG-small-chip2/resolve/refs%2Fconvert%2Fparquet/0-hero--OIG-small-chip2/parquet-train.parquet)`
    in this example.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the Datasets Programmatically
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you have seen how to use the **Datasets server REST API**, let’s see
    how you can download the datasets programmatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Python, the easiest way is to use the `requests` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of the `json()` function is a Python dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Using this dictionary result, you can use list comprehension to find the URL
    for the dataset in Parquet format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The `urls` variable is a list containing a list of URLs for the dataset under
    the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Downloading the Parquet file using DuckDB
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you use DuckDB, you can actually use DuckDB to remotely load a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are new to DuckDB, you can read up on the basics from this article:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://levelup.gitconnected.com/using-duckdb-for-data-analytics-bab3e3ff032c?source=post_page-----bba14dd8da68--------------------------------)
    [## Using DuckDB for Data Analytics'
  prefs: []
  type: TYPE_NORMAL
- en: Learn how to use SQL to perform data analytics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/using-duckdb-for-data-analytics-bab3e3ff032c?source=post_page-----bba14dd8da68--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'First, ensure you install DuckDB if you have not done so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, create a DuckDB instance and install **httpfs**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The **httpfs** extension is a loadable extension implementing a file system
    that allows reading remote/writing remote files.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Once **httpfs** is installed and loaded, you can load the Parquet dataset from
    Hugging Face Hub by using a SQL query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The `df()` function above converts the result of the query to a Pandas DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f510cc5e630d8e7cf72d10a3e0dda1c2.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'One great feature of Parquet is that Parquet stores files in columnar format.
    And so if your query only requests for only a single column, only that requested
    column is downloaded to your computer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'In the above query, only the “`user`” column is downloaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4854a971693abf64f18c25f058be9a62.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: This Parquet feature is especially useful for large dataset — imagine the time
    and space you can save by only downloading the columns you need.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In some cases, you don’t even need to download the data at all. Consider the
    following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/4c1213c3795dbc47fd24243a187e089f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: No data needs to be downloaded as this request can be fulfilled simply by reading
    the metadata of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is another example of using DuckDB to download another dataset (**“mstz/heart_failure**”):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This dataset has 299 rows and 13 columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4d7d3bd037ff543a642dd02fca3c07d1.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'We could perform some aggregation on the **age** column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b06c3c33641b5b0fce87bd6b6a827e87.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the result, we could also plot a bar plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/54e62437baa867e03c3aac6ed478571e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Using the Datasets library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To make working with data from Hugging Face easy and efficient, Hugging Face
    has its own `Datasets` library ([https://github.com/huggingface/datasets](https://github.com/huggingface/datasets)).
  prefs: []
  type: TYPE_NORMAL
- en: 'To install the `datasets` library, use the `pip` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The `load_dataset()` function loads the specified dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'When you load the dataset for the first time, the entire dataset (in Parquet
    format) is downloaded to your computer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/48044bf4bcdc6946161de9a6fdd45212.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'The type of data of the returned `dataset` is `datasets.arrow_dataset.Dataset`.
    So what can you do with it? First, you can convert it to a Pandas DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/e217ec404a175f0d5b4cd915d2809f78.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also get the first row of the dataset by using an index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This will return the first row of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: There are a bunch of other things you can do with this `datasets.arrow_dataset.Dataset`
    object. I will leave it to you to explore further.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Again, when dealing with large datasets, it is not feasible to download the
    entire dataset to your computer before you do anything with it. In the previous
    section, calling the `load_dataset()` function downloads the entire dataset onto
    my computer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e74e5d94366d5e8e0d7d5479ce4990ef.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: This particular dataset it took up 82.2MB of disk space. You can imagine the
    time and disk space needed for larger datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, the **Datasets** library supports ***streaming***. Dataset streaming
    lets you work with a dataset without downloading it — the data is streamed as
    you iterate over the dataset. To use streaming, set the `streaming` parameter
    to `True` in the `load_dataset()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The type of `dataset` is now `datasets.iterable_dataset.IterableDataset`, instead
    of `datasets.arrow_dataset.Dataset`. So how do you use it? You can use the `iter()`
    function on it, which returns an `iterator` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'To get a row, call the `next()` function, which returns the next item in an
    iterator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'You will now see first row as a dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Calling the `next()` function on `i` again will return the next row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: And so on.
  prefs: []
  type: TYPE_NORMAL
- en: Shuffling the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can also shuffle the dataset by using the `shuffle()` function on the `dataset`
    variable, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: In the above example, say your dataset has 10,000 rows. The `shuffle()` function
    will randomly select examples from the first five hundred rows in the buffer.
  prefs: []
  type: TYPE_NORMAL
- en: By default, the buffer size is 1,000.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Other tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can perform more tasks using streaming, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: splitting the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interleaving the dataset — combining two datasets by alternating rows between
    each dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modifying the columns of a dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filtering a dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check out [https://huggingface.co/docs/datasets/stream](https://huggingface.co/docs/datasets/stream)
    for more details.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**If you like reading my articles and that it helped your career/study, please
    consider signing up as a Medium member. It is $5 a month, and it gives you unlimited
    access to all the articles (including mine) on Medium. If you sign up using the
    following link, I will earn a small commission (at no additional cost to you).
    Your support means that I will be able to devote more time on writing articles
    like this.**'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://weimenglee.medium.com/membership?source=post_page-----bba14dd8da68--------------------------------)
    [## Join Medium with my referral link - Wei-Meng Lee'
  prefs: []
  type: TYPE_NORMAL
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: weimenglee.medium.com](https://weimenglee.medium.com/membership?source=post_page-----bba14dd8da68--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, I have shown you how you can access the datasets stored on
    Hugging Face Hub. Since the datasets are stored in Parquet format, it allows you
    to remotely access the datasets remotely without needing to download the entire
    bulk of the dataset. You can access the datasets either using DuckDB, or using
    the Datasets library provided by Hugging Face.
  prefs: []
  type: TYPE_NORMAL
