["```py\n\"\"\"Please answer the following question.\n\nQ: {Question}\n\nA: {Answer}\"\"\"\n```", "```py\nfrom datasets import load_dataset, DatasetDict, Dataset\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoConfig, \n    AutoModelForSequenceClassification,\n    DataCollatorWithPadding,\n    TrainingArguments,\n    Trainer)\n\nfrom peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\nimport evaluate\nimport torch\nimport numpy as np\n```", "```py\nmodel_checkpoint = 'distilbert-base-uncased'\n\n# define label maps\nid2label = {0: \"Negative\", 1: \"Positive\"}\nlabel2id = {\"Negative\":0, \"Positive\":1}\n\n# generate classification model from model_checkpoint\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_checkpoint, num_labels=2, id2label=id2label, label2id=label2id)\n```", "```py\n# load dataset\ndataset = load_dataset(\"shawhin/imdb-truncated\")\ndataset\n\n# dataset = \n# DatasetDict({\n#     train: Dataset({\n#         features: ['label', 'text'],\n#         num_rows: 1000\n#     })\n#     validation: Dataset({\n#         features: ['label', 'text'],\n#         num_rows: 1000\n#     })\n# }) \n```", "```py\n# create tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n```", "```py\n# create tokenize function\ndef tokenize_function(examples):\n    # extract text\n    text = examples[\"text\"]\n\n    #tokenize and truncate text\n    tokenizer.truncation_side = \"left\"\n    tokenized_inputs = tokenizer(\n        text,\n        return_tensors=\"np\",\n        truncation=True,\n        max_length=512\n    )\n\n    return tokenized_inputs\n\n# add pad token if none exists\nif tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n    model.resize_token_embeddings(len(tokenizer))\n\n# tokenize training and validation datasets\ntokenized_dataset = dataset.map(tokenize_function, batched=True)\ntokenized_dataset\n\n# tokenized_dataset = \n# DatasetDict({\n#     train: Dataset({\n#        features: ['label', 'text', 'input_ids', 'attention_mask'],\n#         num_rows: 1000\n#     })\n#     validation: Dataset({\n#         features: ['label', 'text', 'input_ids', 'attention_mask'],\n#         num_rows: 1000\n#     })\n# })\n```", "```py\n# create data collator\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n```", "```py\n# import accuracy evaluation metric\naccuracy = evaluate.load(\"accuracy\")\n\n# define an evaluation function to pass into trainer later\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=1)\n\n    return {\"accuracy\": accuracy.compute(predictions=predictions, \n                                          references=labels)}\n```", "```py\n# define list of examples\ntext_list = [\"It was good.\", \"Not a fan, don't recommed.\", \n\"Better than the first one.\", \"This is not worth watching even once.\", \n\"This one is a pass.\"]\n\nprint(\"Untrained model predictions:\")\nprint(\"----------------------------\")\nfor text in text_list:\n    # tokenize text\n    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n    # compute logits\n    logits = model(inputs).logits\n    # convert logits to label\n    predictions = torch.argmax(logits)\n\n    print(text + \" - \" + id2label[predictions.tolist()])\n\n# Output:\n# Untrained model predictions:\n# ----------------------------\n# It was good. - Negative\n# Not a fan, don't recommed. - Negative\n# Better than the first one. - Negative\n# This is not worth watching even once. - Negative\n# This one is a pass. - Negative\n```", "```py\npeft_config = LoraConfig(task_type=\"SEQ_CLS\", # sequence classification\n                        r=4, # intrinsic rank of trainable weight matrix\n                        lora_alpha=32, # this is like a learning rate\n                        lora_dropout=0.01, # probablity of dropout\n                        target_modules = ['q_lin']) # we apply lora to query layer only\n```", "```py\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()\n\n# trainable params: 1,221,124 || all params: 67,584,004 || trainable%: 1.8068239934408148\n```", "```py\n# hyperparameters\nlr = 1e-3 # size of optimization step \nbatch_size = 4 # number of examples processed per optimziation step\nnum_epochs = 10 # number of times model runs through training data\n\n# define training arguments\ntraining_args = TrainingArguments(\n    output_dir= model_checkpoint + \"-lora-text-classification\",\n    learning_rate=lr,\n    per_device_train_batch_size=batch_size, \n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=num_epochs,\n    weight_decay=0.01,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n)\n```", "```py\n# creater trainer object\ntrainer = Trainer(\n    model=model, # our peft model\n    args=training_args, # hyperparameters\n    train_dataset=tokenized_dataset[\"train\"], # training data\n    eval_dataset=tokenized_dataset[\"validation\"], # validation data\n    tokenizer=tokenizer, # define tokenizer\n    data_collator=data_collator, # this will dynamically pad examples in each batch to be equal length\n    compute_metrics=compute_metrics, # evaluates model using compute_metrics() function from before\n)\n\n# train model\ntrainer.train()\n```", "```py\nmodel.to('mps') # moving to mps for Mac (can alternatively do 'cpu')\n\nprint(\"Trained model predictions:\")\nprint(\"--------------------------\")\nfor text in text_list:\n    inputs = tokenizer.encode(text, return_tensors=\"pt\").to(\"mps\") # moving to mps for Mac (can alternatively do 'cpu')\n\n    logits = model(inputs).logits\n    predictions = torch.max(logits,1).indices\n\n    print(text + \" - \" + id2label[predictions.tolist()[0]])\n\n# Output:\n# Trained model predictions:\n# ----------------------------\n# It was good. - Positive\n# Not a fan, don't recommed. - Negative\n# Better than the first one. - Positive\n# This is not worth watching even once. - Negative\n# This one is a pass. - Positive # this one is tricky\n```"]