- en: 'Seeing with Sound: Empowering the Visually Impaired with GPT-4V(ision) and
    Text-to-Speech Technology'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过声音看见世界：利用GPT-4V(ision)和文本转语音技术赋能视觉障碍者
- en: 原文：[https://towardsdatascience.com/seeing-with-sound-empowering-the-visually-impaired-with-gpt-4v-ision-and-text-to-speech-bb5807b4e08c](https://towardsdatascience.com/seeing-with-sound-empowering-the-visually-impaired-with-gpt-4v-ision-and-text-to-speech-bb5807b4e08c)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/seeing-with-sound-empowering-the-visually-impaired-with-gpt-4v-ision-and-text-to-speech-bb5807b4e08c](https://towardsdatascience.com/seeing-with-sound-empowering-the-visually-impaired-with-gpt-4v-ision-and-text-to-speech-bb5807b4e08c)
- en: 'Enhancing Visual Impairment Navigation: Integrating GPT-4V(ision) and TTS for
    Advanced Sensory Assistance'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提升视觉障碍导航：将GPT-4V(ision)和TTS集成以提供高级感官辅助
- en: '[](https://medium.com/@luisroque?source=post_page-----bb5807b4e08c--------------------------------)[![Luís
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----bb5807b4e08c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bb5807b4e08c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bb5807b4e08c--------------------------------)
    [Luís Roque](https://medium.com/@luisroque?source=post_page-----bb5807b4e08c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@luisroque?source=post_page-----bb5807b4e08c--------------------------------)[![Luís
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----bb5807b4e08c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bb5807b4e08c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bb5807b4e08c--------------------------------)
    [Luís Roque](https://medium.com/@luisroque?source=post_page-----bb5807b4e08c--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bb5807b4e08c--------------------------------)
    ·12 min read·Nov 16, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----bb5807b4e08c--------------------------------)
    ·阅读时间12分钟·2023年11月16日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '*This post was co-authored with Rafael Guedes.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*本文由Rafael Guedes共同撰写。*'
- en: Introduction
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: OpenAI’s latest developments have taken AI’s usability to a whole different
    level with the release of GPT-4V(ision) and Text-to-Speech (TTS) APIs. Why? Let’s
    motivate their usefulness with a use case. Walking down the street is a simple
    task for most of us, but for those with visual impairments, every step can be
    a challenge. Traditional aids like guide dogs and canes have been useful, but
    the integration of AI technologies opens up a new chapter in improving the independence
    and mobility of the blind community. Simple glasses equipped with a discreet camera
    would be enough to revolutionize how the visually impaired experience their surroundings.
    We will explain how it can be done using the latest releases from OpenAI.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的最新发展通过发布GPT-4V(ision)和文本转语音(TTS) API，将AI的可用性提升到了一个全新的水平。为什么？让我们通过一个使用案例来激发它们的实际效用。对大多数人来说，走在街上是一个简单的任务，但对视觉障碍者来说，每一步都可能是一个挑战。传统的辅助工具如导盲犬和手杖虽然有用，但AI技术的融合开启了一个改善盲人社区独立性和移动性的全新篇章。配备隐蔽摄像头的简单眼镜足以彻底改变视觉障碍者体验周围环境的方式。我们将解释如何利用OpenAI的最新发布来实现这一点。
- en: Another interesting use case is to change our experience in museums and other
    similar venues. Imagine for a second that audio guide systems commonly found in
    museums are replaced by a discreet camera pinned to your shirt. Let’s assume that
    you are visiting an art museum. As you walk through the museum, this technology
    can provide you with information about each painting and it can do so in a specific
    style chosen by you. Let’s say that you are a bit tired and you need something
    engaging and lightweight, you could prompt it to *‘Give me some historical context
    on the painting but make it engaging and fun, you can even add some jokes to it’.*
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的使用案例是改变我们在博物馆和其他类似场所的体验。设想一下，博物馆中常见的音频导览系统被别在衣服上的隐蔽摄像头所取代。假设你正在参观一家艺术博物馆。当你在博物馆中漫步时，这项技术可以为你提供有关每幅画作的信息，并且可以按照你选择的特定风格进行讲解。假设你有点疲倦，需要一些轻松有趣的内容，你可以提示它*‘给我一些关于这幅画的历史背景，但要有趣和引人入胜，甚至可以加些笑话’*。
- en: What about Augmented Reality (AR)? Can this new technology improve or even replace
    it? Right now, AR is seen as this digital layer that we can overlay on our visual
    perception of the real world. The problem is that this can quickly become noisy.
    These new technologies could replace AR in some use cases. In other cases, it
    can make AR personalized for each one of us so that we can experience the world
    at our own pace.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 增强现实（AR）呢？这种新技术能否改善甚至取代它？目前，AR 被视为我们可以叠加在对现实世界的视觉感知上的数字层。问题是，这可能很快变得嘈杂。这些新技术可能会在某些用例中取代
    AR。在其他情况下，它可以使 AR 个性化，使我们能够以自己的节奏体验世界。
- en: In this post, we will explore how to combine GPT-4V(ision) and Text-to-Speech
    to make the world more inclusive and navigable for the visually impaired. We will
    start by explaining how GPT-4V(ision) works and its architecture (we will use
    some open-source counterparts to get the intuition since OpenAI does not provide
    details about their model). We follow it with an explanation of what is TTS and
    what is the most common model architecture used to transform text into audio signal.
    Finally, we finish with a step-by-step implementation on how we can take advantage
    of both GPT-4V(ision) and TTS to help visually impaired people navigate the streets
    of Porto, Portugal.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将探讨如何将 GPT-4V（视觉）和文本转语音（TTS）结合起来，使世界对视觉障碍者更具包容性和可导航性。我们将首先解释 GPT-4V（视觉）是如何工作的及其架构（我们将使用一些开源对应物来获取直觉，因为
    OpenAI 不提供有关其模型的详细信息）。接下来，我们将解释什么是 TTS，以及用于将文本转化为音频信号的最常见模型架构。最后，我们将通过一步一步的实施，展示如何利用
    GPT-4V（视觉）和 TTS 帮助视觉障碍者在葡萄牙波尔图的街道上导航。
- en: '![](../Images/1b79d260aa4ed403854e157a6dbcf3a9.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b79d260aa4ed403854e157a6dbcf3a9.png)'
- en: 'Figure 1: OpenAI published several updates on their API services and introduced
    multimodality to GPT4 ([source](https://unsplash.com/photos/a-computer-chip-with-the-word-gat-printed-on-it-Fc1GBkmV-Dw))'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：OpenAI 发布了有关其 API 服务的多个更新，并将多模态引入 GPT-4（[来源](https://unsplash.com/photos/a-computer-chip-with-the-word-gat-printed-on-it-Fc1GBkmV-Dw)）
- en: As always, the code is available on our [Github](https://github.com/zaai-ai/large-language-models).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，代码可以在我们的 [Github](https://github.com/zaai-ai/large-language-models) 上找到。
- en: What is GPT-4V(ision)?
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 GPT-4V（视觉）？
- en: GPT-4, like GPT-3.5, is a large multimodal model capable of processing text
    inputs and producing text outputs [1]. In the latest OpenAI announcement, they
    shared that GPT-4 has been extended to be a multimodal Large Language Model (LLM).
    It means that the model is now able to receive additional modalities as input,
    in this case, images. Multimodal LLMs expand the impact of language-only systems
    with new interfaces and capabilities, opening the door for more elaborate use
    cases. You can see an example of using GPT-4V(ision) in the figure below, where
    vision and reasoning capabilities work together to detect some complex nuances
    in a picture.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4 像 GPT-3.5 一样，是一个大型多模态模型，能够处理文本输入并生成文本输出 [1]。在最新的 OpenAI 公告中，他们表示 GPT-4
    已被扩展为一个多模态大型语言模型（LLM）。这意味着该模型现在能够接收额外的输入模态，在这种情况下是图像。多模态 LLM 扩展了仅语言系统的影响，通过新的接口和能力，为更复杂的用例开辟了可能性。你可以在下图中看到使用
    GPT-4V（视觉）的示例，其中视觉和推理能力一起工作，以检测图片中的一些复杂细微之处。
- en: '![](../Images/c5e43fe1966fe9c01a26c44c95556e66.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c5e43fe1966fe9c01a26c44c95556e66.png)'
- en: 'Figure 2: GPT-4 ability to interpret what is unusual for a human being ([source](https://arxiv.org/pdf/2303.08774.pdf))'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：GPT-4 解释对人类来说不寻常的能力（[来源](https://arxiv.org/pdf/2303.08774.pdf)）
- en: Although OpenAI specifically says in its paper that …
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 OpenAI 在其论文中明确表示 …
- en: ‘*Given both the competitive landscape and the safety implications of large-scale
    models like GPT-4, this report contains no further details about the architecture
    (including model size), hardware, training compute, dataset construction, training
    method, or similar.’*
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ‘*考虑到竞争环境以及像 GPT-4 这样的大规模模型的安全性，这份报告未包含有关架构（包括模型大小）、硬件、训练计算、数据集构建、训练方法或类似内容的更多细节。*’
- en: … we can try to estimate what the architecture of GPT-4V(ision) looks like.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: …我们可以尝试估计 GPT-4V（视觉）的架构是什么样的。
- en: 'We know that GPT-4V(vision) receives as input text and images. Therefore, it
    most likely has three main components:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道 GPT-4V（视觉）接收文本和图像作为输入。因此，它很可能有三个主要组件：
- en: '**Encoders:** Separate encoders for handling text and image data'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**编码器：** 用于处理文本和图像数据的独立编码器'
- en: '**Attention mechanism:** It employs advanced attention mechanisms, enabling
    the model to focus on the most relevant parts of both text and image inputs.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**注意力机制：** 它采用先进的注意力机制，使模型能够关注文本和图像输入中最相关的部分。'
- en: '**Decoder:** To produce the output text based on the latent space of the encoders
    combined with the attention layer.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**解码器：** 根据编码器的潜在空间结合注意力层生成输出文本。'
- en: '![](../Images/d097929cd283ac88e910dd41d72197ee.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d097929cd283ac88e910dd41d72197ee.png)'
- en: 'Figure 3: Simple Architecture for a multimodal model that uses images and text
    as input (image by author)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：使用图像和文本作为输入的多模态模型的简单架构（图片由作者提供）
- en: A similar architecture can be found in the 🦩 **Flamingo model [2]** created
    by DeepMind.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的架构可以在🦩 **Flamingo模型 [2]**中找到，该模型由DeepMind创建。
- en: Flamingo is designed to handle textual and visual data as inputs in order to
    produce a free-form text as output. The authors define it as a Visual Language
    Model (VLM). There are 3 main components to the model, the input processing, the
    perceiver resampler, and the layers that integrate both data types and generate
    the output text.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Flamingo旨在处理文本和视觉数据作为输入，以生成自由形式的文本作为输出。作者将其定义为视觉语言模型（VLM）。该模型有三个主要组件：输入处理、感知重采样器和整合两种数据类型并生成输出文本的层。
- en: '**Input Processing**: Flamingo receives both visual and text data. The text
    undergoes the usual process of tokenization before entering the Language Model
    (LM), while the visual input is processed by the Vision Encoder (VE), which turns
    pixels into a more abstract representation of features.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入处理**：Flamingo接收视觉和文本数据。文本在进入语言模型（LM）之前经历常规的分词处理，而视觉输入由视觉编码器（VE）处理，将像素转换为更抽象的特征表示。'
- en: '**Perceiver Resampler**: This component further processes the visual features.
    It adds a sense of time to images (especially important in videos) and compresses
    the data into a more manageable format. This is key for efficiently combining
    visual and textual data later in the process.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**感知重采样器**：此组件进一步处理视觉特征。它为图像添加时间感（在视频中尤为重要），并将数据压缩成更易于管理的格式。这对后续有效结合视觉和文本数据至关重要。'
- en: '![](../Images/134ec5bcc06ef4040855ac8c79eb0cdf.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/134ec5bcc06ef4040855ac8c79eb0cdf.png)'
- en: 'Figure 4: Perceive Resampler Architecture ([source](https://arxiv.org/pdf/2204.14198.pdf))'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：感知重采样器架构（[来源](https://arxiv.org/pdf/2204.14198.pdf)）
- en: '**Integration and Output:** The processed visual and textual data are then
    integrated into the GATED XATTN-DENSE layer. This layer employs a cross-attention
    mechanism combined with a gating function to merge the two types of data effectively.
    The output from this layer feeds into the LM layer, which finally leads to the
    generation of free-form text output by a Transformer decoder.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**集成和输出：** 处理过的视觉和文本数据随后被整合到GATED XATTN-DENSE层。该层采用了交叉注意力机制与门控函数结合，来有效地融合两种数据。该层的输出输入到LM层，最终由Transformer解码器生成自由形式的文本输出。'
- en: '![](../Images/c2b106ab7885b471616e79d5ad1cf92d.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c2b106ab7885b471616e79d5ad1cf92d.png)'
- en: 'Figure 5: Overview of the Flamingo model. The Flamingo models are a family
    of Visual Language Models (VLM) that can take as input visual data together with
    text and can produce free-form text as output ([source](https://arxiv.org/pdf/2204.14198.pdf)).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：Flamingo模型概述。Flamingo模型是一系列视觉语言模型（VLM），可以同时接受视觉数据和文本作为输入，并生成自由形式的文本输出（[来源](https://arxiv.org/pdf/2204.14198.pdf)）。
- en: GPT-4V(ision) API
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT-4V(ision) API
- en: The GPT-4V(ision) API from OpenAI allows for processing both visual and text
    information. We cover the main steps to use the API below.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的GPT-4V(ision) API允许处理视觉和文本信息。我们在下面介绍使用该API的主要步骤。
- en: '**Set Up Your Environment**:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**设置环境**：'
- en: Install the Python dependencies in your environment, namely the OpenAI library.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的环境中安装Python依赖项，即OpenAI库。
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Import the necessary libraries in your Python script.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的Python脚本中导入必要的库。
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Configure API Parameters**: Utilize the `**ChatCompletion**` class with specific
    parameters for handling multimodal (text and image) data.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**配置API参数**：利用`**ChatCompletion**`类，结合特定参数处理多模态（文本和图像）数据。'
- en: '**Model Parameter**: Set this to `**gpt-4-vision-preview**` enable the processing
    of both visual and textual data.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型参数**：将其设置为`**gpt-4-vision-preview**`以启用对视觉和文本数据的处理。'
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Messages Parameter**: This needs to include both text and images. Images
    should be encoded in base64 format.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**消息参数**：这需要包括文本和图像。图像应以base64格式编码。'
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Handle Images**: Before including them in the API call, images must be converted
    to a base64 format.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**处理图像**：在将其包含在 API 调用中之前，图像必须转换为 base64 格式。'
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Execute the API Call**: With the parameters set, make the API call to process
    the input data.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**执行 API 调用**：设置好参数后，发起 API 调用以处理输入数据。'
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: What is Text-to-Speech?
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是文本到语音？
- en: TTS [4] technology transforms written text into spoken words. This complex process
    involves several stages, each handled by different models. First, a **Grapheme-to-Phoneme**
    model translates written text into phonetic units. Next, a **Phoneme-to-Mel**
    **Spectrogram** model transforms these phonemes into a visual frequency representation.
    Finally, a **Mel-Spectrogram-to-Audio** model, or **Vocoder**, generates the actual
    spoken audio from this representation.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: TTS [4] 技术将书面文字转换为口语。这一复杂过程涉及多个阶段，每个阶段由不同的模型处理。首先，一个 **字形到音素** 模型将书面文字转换为音素单位。接下来，一个
    **音素到 Mel** **谱图** 模型将这些音素转换为视觉频率表示。最后，一个 **Mel-谱图到音频** 模型或 **声码器** 从这种表示生成实际的口语音频。
- en: '![](../Images/2e1aeaf68c4df4d6984a6d73332edcdc.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e1aeaf68c4df4d6984a6d73332edcdc.png)'
- en: 'Figure 6: TTS architecture (image by author)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：TTS 架构（图片来源于作者）
- en: '**Grapheme-to-Phoneme Conversion**: The first step involves converting written
    words (graphemes) into phonemes, the basic units of pronunciation. For instance,
    the phrase `**"Swifts, flushed from chimneys"**` would be converted into phonetic
    representation like `**"ˈswɪfts, ˈfɫəʃt ˈfɹəm ˈtʃɪmniz"**`. A model commonly used
    for this is the G2P-Conformer [5].'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**字形到音素转换**：第一步是将书写的单词（字形）转换为音素，即发音的基本单位。例如，短语 `**"Swifts, flushed from chimneys"**`
    将被转换为音标表示，如 `**"ˈswɪfts, ˈfɫəʃt ˈfɹəm ˈtʃɪmniz"**`。一个常用的模型是 G2P-Conformer [5]。'
- en: '**Phoneme-to-Mel Spectrogram**: Next, these phonemes are processed to create
    a mel-spectrogram, a visual representation of the sound frequencies over time.
    This is typically achieved using an encoder-decoder architecture, such as Tacotron2
    [6]. In this model, a convolutional neural network (CNN) embeds the phonemes,
    which are then processed through a bidirectional long short-term memory (LSTM)
    network. The resulting mel-spectrograms are a key intermediate step, acting as
    a bridge between phonemes and the final audio output.'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**音素到 Mel 谱图**：接下来，这些音素会被处理以创建 mel-谱图，这是声音频率随时间变化的视觉表示。这通常通过编码器-解码器架构来实现，例如
    Tacotron2 [6]。在这个模型中，卷积神经网络（CNN）嵌入音素，然后通过双向长短期记忆（LSTM）网络进行处理。生成的 mel-谱图是一个关键的中间步骤，充当音素与最终音频输出之间的桥梁。'
- en: '**Mel-Spectrogram-to-Audio Conversion**: The final stage involves converting
    the mel-Spectrogram into actual audio. This is where a vocoder comes in, often
    using advanced generative models. WaveNet [7], developed by DeepMind, is a good
    example. It employs a deep neural network with dilated causal convolutions to
    ensure correct sequential processing. Each predicted audio sample is fed back
    into the network to predict the next, allowing the model to capture the long-range
    patterns in the audio signal.'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Mel-谱图到音频转换**：最后阶段涉及将 mel-谱图转换为实际的音频。这时需要一个声码器，通常使用先进的生成模型。由 DeepMind 开发的
    WaveNet [7] 是一个很好的例子。它使用带有扩张因果卷积的深度神经网络，以确保正确的序列处理。每个预测的音频样本会反馈到网络中以预测下一个，从而使模型能够捕捉音频信号中的长距离模式。'
- en: Text-to-Speech API
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本到语音 API
- en: OpenAI provides a TTS service accessible via an API, offering two quality levels
    and six distinct voices to cater to different needs and preferences.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 提供了一个通过 API 访问的 TTS 服务，提供两个质量等级和六种不同的声音，以满足不同的需求和偏好。
- en: '**Quality Options**:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**质量选项**：'
- en: '`**tts-1**`: Ideal for real-time applications, this option offers lower quality
    but benefits from reduced latency.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**tts-1**`：适用于实时应用，这个选项提供较低的质量但具有减少延迟的优点。'
- en: '`**tts-1-hd**`: Suited for scenarios where higher audio quality is relevant
    and latency is not a problem.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**tts-1-hd**`：适合于音频质量较高且延迟不是问题的场景。'
- en: '**Voice Selection**:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**选择声音**：'
- en: 'OpenAI’s TTS API features six unique voices: `**alloy**`, `**echo**`, `**fable**`,
    `**onyx**`, `**nova**`, and `**shimmer**`.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI 的 TTS API 具有六种独特的声音：`**alloy**`、`**echo**`、`**fable**`、`**onyx**`、`**nova**`
    和 `**shimmer**`。
- en: Each voice has its own character; for instance, `**Fable**` resembles a podcast
    narrator's voice.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每种声音都有其独特的特性；例如，`**Fable**` 类似于播客叙述者的声音。
- en: You can preview these voices on OpenAI’s [Text-to-Speech Guide](https://platform.openai.com/docs/guides/text-to-speech).
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以在 OpenAI 的 [文本到语音指南](https://platform.openai.com/docs/guides/text-to-speech)
    上预览这些声音。
- en: '**Making an API Request:**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**发起 API 请求：**'
- en: 'To use OpenAI’s TTS API, send a request to `**https://api.openai.com/v1/audio/speech**`.
    Here''s what you''ll need:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 OpenAI 的 TTS API，发送请求到 `**https://api.openai.com/v1/audio/speech**`。你需要以下内容：
- en: '**Model Specification**: Choose between `**tts-1**` (low quality, low latency)
    or `**tts-1-hd**` (high quality, high latency) based on your needs.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型规格**：根据你的需求选择 `**tts-1**`（低质量，低延迟）或 `**tts-1-hd**`（高质量，高延迟）。'
- en: '**Input Text**: The textual content you want to convert into speech.'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**输入文本**：你想要转换为语音的文本内容。'
- en: '**Voice Choice**: Select from the available voices to find the one that best
    suits your content and audience.'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**语音选择**：从可用的语音中选择一个最适合你的内容和观众的声音。'
- en: 'Here’s a basic example of how to structure your API request:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个关于如何结构化 API 请求的基本示例：
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: How to implement an application to help visually impaired people feel safer
    when walking down a street?
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现一个帮助视障人士在街上行走时感到更安全的应用？
- en: This section describes step by step how you can create the audio description
    of your video using GPT-4V(ision) and TTS from OpenAI. It also covers how you
    can add the generated audio to your video.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 本节逐步描述了如何使用 OpenAI 的 GPT-4V(ision) 和 TTS 创建视频的音频描述。还涵盖了如何将生成的音频添加到视频中。
- en: In our case, and as stated in the introduction, we created an audio guide to
    help a visually impaired person walking down the street by identifying the location
    of the obstacles and providing them with spatial information.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，如介绍所述，我们创建了一个音频指南，帮助视障人士在街上行走，通过识别障碍物的位置并提供空间信息来辅助他们。
- en: The process begins with importing necessary libraries and setting up the environment
    in Python. We use libraries such as cv2 for video processing and openai for accessing
    the GPT-4V(ision) and TTS models.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 过程开始于导入必要的库并设置 Python 环境。我们使用如 cv2 进行视频处理和 openai 访问 GPT-4V(ision) 和 TTS 模型等库。
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Next, we load and process the video. The video is resized to a manageable resolution,
    ensuring we don’t exceed the token limits of OpenAI. Each frame is encoded in
    base64, the required format for OpenAI’s API.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们加载并处理视频。视频被调整为可管理的分辨率，以确保不超过 OpenAI 的令牌限制。每一帧都被编码为 base64，这是 OpenAI API
    所需的格式。
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'An important step is crafting the right prompt for GPT-4V(ision). A well-engineered
    prompt significantly influences the output that we get from the model. From our
    experiments, the 2 main components that influenced our outputs were:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的步骤是为 GPT-4V(ision) 制作合适的提示。精心设计的提示会显著影响我们从模型中获得的输出。从我们的实验中，影响输出的两个主要组件是：
- en: Verbs like describe, narrate, tell;
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动词如描述、叙述、讲述；
- en: Identifying the style of speech we wanted.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定我们想要的语音风格。
- en: 'One of the first prompts that we tried was the following: ‘*These are frames
    from a person walking in a city. Describe the elements and obstacles around you
    to help a blind person navigate them successfully.* With this structure, the model
    tends to get extremely verbose and descriptive. Our use case requires a much less
    noisy output.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试的第一个提示之一是：‘*这些是一个人在城市中行走的画面。描述周围的元素和障碍物，以帮助盲人成功导航。*’ 这种结构使模型变得极其冗长和描述性。我们的使用案例需要输出更少噪音。
- en: 'The prompt that worked for us was the following: *‘These are frames from a
    person walking. Narrate succinctly with short sentences like they do in GPS devices
    the elements and obstacles around you to help a blind person go navigate them
    successfully.’* This time the model gave us short sentences that allowed us to
    get just the essential information for street navigation. The result is below:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们有效的提示是：*‘这些是一个人行走的画面。用 GPS 设备那样的简短句子简洁叙述周围的元素和障碍物，以帮助盲人成功导航。’* 这次模型给出了简短的句子，让我们获得了街道导航所需的基本信息。结果如下：
- en: '*“Walk straight on a textured pathway, keep the building to your right. Continue
    forward with slight curve to the right. Stay straight, small overhang ahead on
    your right. Proceed, passing the overhang, continue on flat path. Straight ahead,
    approaching a well-lit area. After well-lit area, transition onto patterned pavement.
    Follow the tiled pavement with guiding lines straight ahead. Continue under the
    passageway, keeping the pillars parallel to you. Move through passageway, slight
    descent ahead. Pathway ends, prepare to stop at pedestrian crossing. Stand at
    crosswalk, wait for audible signal to cross the street.”*'
  id: totrans-88
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“在有纹理的路径上直行，将建筑物保持在右侧。继续前行，稍微向右转。保持直行，前方右侧有小悬挑。继续前进，经过悬挑，继续在平坦的路径上前行。直行，接近一个光线充足的区域。经过光线充足的区域后，过渡到有图案的人行道。跟随有引导线的瓷砖人行道直行。继续通过通道，保持柱子与自己平行。穿过通道，前方有小的下降。路径结束，准备在斑马线停下。站在斑马线前，等待听到过马路的信号。”*'
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Once we receive the description from GPT-4V(ision), the next step is converting
    this text into audio. We chose the fable voice for its clarity and resemblance
    to narration. The TTS API from OpenAI is used to transform the generated text
    into an audio file.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们收到GPT-4V(ision)的描述，下一步是将文本转换为音频。我们选择了寓言风格的声音，因为它的清晰度和叙述的相似性。使用OpenAI的TTS
    API将生成的文本转化为音频文件。
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Finally, the last step is to merge the audio with the original video.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是将音频与原始视频合并。
- en: '[PRE11]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: You can see the resulting video [here](https://youtu.be/Erena-iLAY0).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[这里](https://youtu.be/Erena-iLAY0)查看最终视频。
- en: Conclusion
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: OpenAI’s GPT-4V(ision) and TTS APIs open new possibilities to solve important
    use cases that can change the daily lives of many. We explored one that focuses
    on inclusivity for the visually impaired but we could have followed many others.
    As we discussed in the introduction, they also allow us to elevate our human experiences
    (e.g. museum tours), and cater them even more to individual preferences and circumstances.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的GPT-4V(ision)和TTS API开辟了新的可能性，解决了可以改变许多人日常生活的重要用例。我们探讨了一个专注于视障人士的包容性的用例，但我们本可以选择其他许多用例。如在介绍中所述，它们还让我们能够提升人类体验（例如博物馆导览），并根据个人的偏好和情况进行更好的定制。
- en: During the implementation, we found that prompt engineering had a significant
    impact on tailoring the solution to our specific use case. In the future, other
    approaches such as fine-tuning or some type of Retrieval-augmented generation
    (RAG) might be adapted to the VLMs. We have been seeing how these methods are
    useful to make LLMs better for certain tasks and contexts. While the output shows
    the potential of these new models, there is still work to be done. As seen in
    our experiment, the VLM *‘speaks’* like if you could see when it says *“Follow
    the tiled pavement with guiding lines straight ahead”.* It also struggles to accurately
    distinguish between left and right, which is an interesting fact to explore further.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在实施过程中，我们发现提示工程对将解决方案量身定制到我们的特定用例有显著影响。未来，其他方法如微调或某种类型的检索增强生成（RAG）可能会适用于VLMs。我们看到这些方法在某些任务和情境中使LLMs更为有效。尽管输出展示了这些新模型的潜力，但仍有待完善。如我们实验中所见，VLM
    *“说话”*的方式像是你可以看到它所说的*“跟随有引导线的瓷砖人行道直行。”* 它还难以准确区分左右，这是一个值得进一步探索的有趣事实。
- en: Despite these challenges, OpenAI’s latest developments show us a possible future
    where the world is more inclusive and experiences can be enhanced by AI.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管面临这些挑战，OpenAI的最新进展向我们展示了一个更加包容的未来，AI可以提升体验。
- en: 'Keep in touch: [LinkedIn](https://www.linkedin.com/in/luisbrasroque/), [X/Twitter](https://x.com/luisbrasroque),
    [Medium](https://medium.com/@luisroque).'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 保持联系：[LinkedIn](https://www.linkedin.com/in/luisbrasroque/)，[X/Twitter](https://x.com/luisbrasroque)，[Medium](https://medium.com/@luisroque)。
- en: References
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] OpenAI. (2023). GPT-4 Technical Report. arXiv preprint arXiv:2303.08774.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] OpenAI. (2023). GPT-4技术报告。arXiv预印本arXiv:2303.08774。'
- en: '[2] Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y.,
    Lenc, K., Mensch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi,
    S., Han, T., Gong, Z., Samangooei, S., Monteiro, M., Menick, J., Borgeaud, S.,
    Brock, A., Nematzadeh, A., Sharifzadeh, S., Binkowski, M., Barreira, R., Vinyals,
    O., Zisserman, A., & Simonyan, K. (2022). Flamingo: a Visual Language Model for
    Few-Shot Learning. arXiv preprint arXiv:2204.14198.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y.,
    Lenc, K., Mensch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi,
    S., Han, T., Gong, Z., Samangooei, S., Monteiro, M., Menick, J., Borgeaud, S.,
    Brock, A., Nematzadeh, A., Sharifzadeh, S., Binkowski, M., Barreira, R., Vinyals,
    O., Zisserman, A., & Simonyan, K. (2022). Flamingo: 一种用于少样本学习的视觉语言模型。arXiv 预印本
    arXiv:2204.14198。'
- en: '[3] Brock, A., De, S., Smith, S. L., & Simonyan, K. (2021). High-performance
    large-scale image recognition without normalization. arXiv preprint arXiv:2102.06171.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Brock, A., De, S., Smith, S. L., & Simonyan, K. (2021). 无需归一化的高性能大规模图像识别。arXiv
    预印本 arXiv:2102.06171。'
- en: '[4] Maheshwari, H. (2021, May 11). Basic Text to Speech, Explained. Towards
    Data Science. Retrieved from [https://towardsdatascience.com/text-to-speech-explained-from-basic-498119aa38b5](/text-to-speech-explained-from-basic-498119aa38b5)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Maheshwari, H. (2021年5月11日). 基础文本到语音，解析。Towards Data Science。取自 [https://towardsdatascience.com/text-to-speech-explained-from-basic-498119aa38b5](/text-to-speech-explained-from-basic-498119aa38b5)。'
- en: '[5] Gulati, A., Qin, J., Chiu, C.-C., Parmar, N., Zhang, Y., Yu, J., Han, W.,
    Wang, S., Zhang, Z., Wu, Y., et al. (2020). Conformer: convolution-augmented transformer
    for speech recognition. arXiv preprint arXiv:2005.08100.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Gulati, A., Qin, J., Chiu, C.-C., Parmar, N., Zhang, Y., Yu, J., Han, W.,
    Wang, S., Zhang, Z., Wu, Y., 等. (2020). Conformer: 用于语音识别的卷积增强变换器。arXiv 预印本 arXiv:2005.08100。'
- en: '[6] Shen, J., Pang, R., Weiss, R. J., Schuster, M., Jaitly, N., Yang, Z., Chen,
    Z., Zhang, Y., Wang, Y., Skerry-Ryan, R. J., Saurous, R. A., Agiomyrgiannakis,
    Y., & Wu, Y. (2018). Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram
    Predictions. arXiv preprint arXiv:1712.05884.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Shen, J., Pang, R., Weiss, R. J., Schuster, M., Jaitly, N., Yang, Z., Chen,
    Z., Zhang, Y., Wang, Y., Skerry-Ryan, R. J., Saurous, R. A., Agiomyrgiannakis,
    Y., & Wu, Y. (2018). 通过将 WaveNet 条件化于 Mel 频谱预测进行自然 TTS 合成。arXiv 预印本 arXiv:1712.05884。'
- en: '[7] van den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves,
    A., Kalchbrenner, N., Senior, A., & Kavukcuoglu, K. (2016). WaveNet: A Generative
    Model for Raw Audio. arXiv preprint arXiv:1609.03499.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] van den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves,
    A., Kalchbrenner, N., Senior, A., & Kavukcuoglu, K. (2016). WaveNet: 一种原始音频的生成模型。arXiv
    预印本 arXiv:1609.03499。'
