- en: 'Seeing with Sound: Empowering the Visually Impaired with GPT-4V(ision) and
    Text-to-Speech Technology'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é€šè¿‡å£°éŸ³çœ‹è§ä¸–ç•Œï¼šåˆ©ç”¨GPT-4V(ision)å’Œæ–‡æœ¬è½¬è¯­éŸ³æŠ€æœ¯èµ‹èƒ½è§†è§‰éšœç¢è€…
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/seeing-with-sound-empowering-the-visually-impaired-with-gpt-4v-ision-and-text-to-speech-bb5807b4e08c](https://towardsdatascience.com/seeing-with-sound-empowering-the-visually-impaired-with-gpt-4v-ision-and-text-to-speech-bb5807b4e08c)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/seeing-with-sound-empowering-the-visually-impaired-with-gpt-4v-ision-and-text-to-speech-bb5807b4e08c](https://towardsdatascience.com/seeing-with-sound-empowering-the-visually-impaired-with-gpt-4v-ision-and-text-to-speech-bb5807b4e08c)
- en: 'Enhancing Visual Impairment Navigation: Integrating GPT-4V(ision) and TTS for
    Advanced Sensory Assistance'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æå‡è§†è§‰éšœç¢å¯¼èˆªï¼šå°†GPT-4V(ision)å’ŒTTSé›†æˆä»¥æä¾›é«˜çº§æ„Ÿå®˜è¾…åŠ©
- en: '[](https://medium.com/@luisroque?source=post_page-----bb5807b4e08c--------------------------------)[![LuÃ­s
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----bb5807b4e08c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bb5807b4e08c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bb5807b4e08c--------------------------------)
    [LuÃ­s Roque](https://medium.com/@luisroque?source=post_page-----bb5807b4e08c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@luisroque?source=post_page-----bb5807b4e08c--------------------------------)[![LuÃ­s
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----bb5807b4e08c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bb5807b4e08c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bb5807b4e08c--------------------------------)
    [LuÃ­s Roque](https://medium.com/@luisroque?source=post_page-----bb5807b4e08c--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bb5807b4e08c--------------------------------)
    Â·12 min readÂ·Nov 16, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page-----bb5807b4e08c--------------------------------)
    Â·é˜…è¯»æ—¶é—´12åˆ†é’ŸÂ·2023å¹´11æœˆ16æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '*This post was co-authored with Rafael Guedes.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*æœ¬æ–‡ç”±Rafael Guedeså…±åŒæ’°å†™ã€‚*'
- en: Introduction
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»‹ç»
- en: OpenAIâ€™s latest developments have taken AIâ€™s usability to a whole different
    level with the release of GPT-4V(ision) and Text-to-Speech (TTS) APIs. Why? Letâ€™s
    motivate their usefulness with a use case. Walking down the street is a simple
    task for most of us, but for those with visual impairments, every step can be
    a challenge. Traditional aids like guide dogs and canes have been useful, but
    the integration of AI technologies opens up a new chapter in improving the independence
    and mobility of the blind community. Simple glasses equipped with a discreet camera
    would be enough to revolutionize how the visually impaired experience their surroundings.
    We will explain how it can be done using the latest releases from OpenAI.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAIçš„æœ€æ–°å‘å±•é€šè¿‡å‘å¸ƒGPT-4V(ision)å’Œæ–‡æœ¬è½¬è¯­éŸ³(TTS) APIï¼Œå°†AIçš„å¯ç”¨æ€§æå‡åˆ°äº†ä¸€ä¸ªå…¨æ–°çš„æ°´å¹³ã€‚ä¸ºä»€ä¹ˆï¼Ÿè®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªä½¿ç”¨æ¡ˆä¾‹æ¥æ¿€å‘å®ƒä»¬çš„å®é™…æ•ˆç”¨ã€‚å¯¹å¤§å¤šæ•°äººæ¥è¯´ï¼Œèµ°åœ¨è¡—ä¸Šæ˜¯ä¸€ä¸ªç®€å•çš„ä»»åŠ¡ï¼Œä½†å¯¹è§†è§‰éšœç¢è€…æ¥è¯´ï¼Œæ¯ä¸€æ­¥éƒ½å¯èƒ½æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„è¾…åŠ©å·¥å…·å¦‚å¯¼ç›²çŠ¬å’Œæ‰‹æ–è™½ç„¶æœ‰ç”¨ï¼Œä½†AIæŠ€æœ¯çš„èåˆå¼€å¯äº†ä¸€ä¸ªæ”¹å–„ç›²äººç¤¾åŒºç‹¬ç«‹æ€§å’Œç§»åŠ¨æ€§çš„å…¨æ–°ç¯‡ç« ã€‚é…å¤‡éšè”½æ‘„åƒå¤´çš„ç®€å•çœ¼é•œè¶³ä»¥å½»åº•æ”¹å˜è§†è§‰éšœç¢è€…ä½“éªŒå‘¨å›´ç¯å¢ƒçš„æ–¹å¼ã€‚æˆ‘ä»¬å°†è§£é‡Šå¦‚ä½•åˆ©ç”¨OpenAIçš„æœ€æ–°å‘å¸ƒæ¥å®ç°è¿™ä¸€ç‚¹ã€‚
- en: Another interesting use case is to change our experience in museums and other
    similar venues. Imagine for a second that audio guide systems commonly found in
    museums are replaced by a discreet camera pinned to your shirt. Letâ€™s assume that
    you are visiting an art museum. As you walk through the museum, this technology
    can provide you with information about each painting and it can do so in a specific
    style chosen by you. Letâ€™s say that you are a bit tired and you need something
    engaging and lightweight, you could prompt it to *â€˜Give me some historical context
    on the painting but make it engaging and fun, you can even add some jokes to itâ€™.*
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªæœ‰è¶£çš„ä½¿ç”¨æ¡ˆä¾‹æ˜¯æ”¹å˜æˆ‘ä»¬åœ¨åšç‰©é¦†å’Œå…¶ä»–ç±»ä¼¼åœºæ‰€çš„ä½“éªŒã€‚è®¾æƒ³ä¸€ä¸‹ï¼Œåšç‰©é¦†ä¸­å¸¸è§çš„éŸ³é¢‘å¯¼è§ˆç³»ç»Ÿè¢«åˆ«åœ¨è¡£æœä¸Šçš„éšè”½æ‘„åƒå¤´æ‰€å–ä»£ã€‚å‡è®¾ä½ æ­£åœ¨å‚è§‚ä¸€å®¶è‰ºæœ¯åšç‰©é¦†ã€‚å½“ä½ åœ¨åšç‰©é¦†ä¸­æ¼«æ­¥æ—¶ï¼Œè¿™é¡¹æŠ€æœ¯å¯ä»¥ä¸ºä½ æä¾›æœ‰å…³æ¯å¹…ç”»ä½œçš„ä¿¡æ¯ï¼Œå¹¶ä¸”å¯ä»¥æŒ‰ç…§ä½ é€‰æ‹©çš„ç‰¹å®šé£æ ¼è¿›è¡Œè®²è§£ã€‚å‡è®¾ä½ æœ‰ç‚¹ç–²å€¦ï¼Œéœ€è¦ä¸€äº›è½»æ¾æœ‰è¶£çš„å†…å®¹ï¼Œä½ å¯ä»¥æç¤ºå®ƒ*â€˜ç»™æˆ‘ä¸€äº›å…³äºè¿™å¹…ç”»çš„å†å²èƒŒæ™¯ï¼Œä½†è¦æœ‰è¶£å’Œå¼•äººå…¥èƒœï¼Œç”šè‡³å¯ä»¥åŠ äº›ç¬‘è¯â€™*ã€‚
- en: What about Augmented Reality (AR)? Can this new technology improve or even replace
    it? Right now, AR is seen as this digital layer that we can overlay on our visual
    perception of the real world. The problem is that this can quickly become noisy.
    These new technologies could replace AR in some use cases. In other cases, it
    can make AR personalized for each one of us so that we can experience the world
    at our own pace.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å¢å¼ºç°å®ï¼ˆARï¼‰å‘¢ï¼Ÿè¿™ç§æ–°æŠ€æœ¯èƒ½å¦æ”¹å–„ç”šè‡³å–ä»£å®ƒï¼Ÿç›®å‰ï¼ŒAR è¢«è§†ä¸ºæˆ‘ä»¬å¯ä»¥å åŠ åœ¨å¯¹ç°å®ä¸–ç•Œçš„è§†è§‰æ„ŸçŸ¥ä¸Šçš„æ•°å­—å±‚ã€‚é—®é¢˜æ˜¯ï¼Œè¿™å¯èƒ½å¾ˆå¿«å˜å¾—å˜ˆæ‚ã€‚è¿™äº›æ–°æŠ€æœ¯å¯èƒ½ä¼šåœ¨æŸäº›ç”¨ä¾‹ä¸­å–ä»£
    ARã€‚åœ¨å…¶ä»–æƒ…å†µä¸‹ï¼Œå®ƒå¯ä»¥ä½¿ AR ä¸ªæ€§åŒ–ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿä»¥è‡ªå·±çš„èŠ‚å¥ä½“éªŒä¸–ç•Œã€‚
- en: In this post, we will explore how to combine GPT-4V(ision) and Text-to-Speech
    to make the world more inclusive and navigable for the visually impaired. We will
    start by explaining how GPT-4V(ision) works and its architecture (we will use
    some open-source counterparts to get the intuition since OpenAI does not provide
    details about their model). We follow it with an explanation of what is TTS and
    what is the most common model architecture used to transform text into audio signal.
    Finally, we finish with a step-by-step implementation on how we can take advantage
    of both GPT-4V(ision) and TTS to help visually impaired people navigate the streets
    of Porto, Portugal.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨å¦‚ä½•å°† GPT-4Vï¼ˆè§†è§‰ï¼‰å’Œæ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ç»“åˆèµ·æ¥ï¼Œä½¿ä¸–ç•Œå¯¹è§†è§‰éšœç¢è€…æ›´å…·åŒ…å®¹æ€§å’Œå¯å¯¼èˆªæ€§ã€‚æˆ‘ä»¬å°†é¦–å…ˆè§£é‡Š GPT-4Vï¼ˆè§†è§‰ï¼‰æ˜¯å¦‚ä½•å·¥ä½œçš„åŠå…¶æ¶æ„ï¼ˆæˆ‘ä»¬å°†ä½¿ç”¨ä¸€äº›å¼€æºå¯¹åº”ç‰©æ¥è·å–ç›´è§‰ï¼Œå› ä¸º
    OpenAI ä¸æä¾›æœ‰å…³å…¶æ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯ï¼‰ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è§£é‡Šä»€ä¹ˆæ˜¯ TTSï¼Œä»¥åŠç”¨äºå°†æ–‡æœ¬è½¬åŒ–ä¸ºéŸ³é¢‘ä¿¡å·çš„æœ€å¸¸è§æ¨¡å‹æ¶æ„ã€‚æœ€åï¼Œæˆ‘ä»¬å°†é€šè¿‡ä¸€æ­¥ä¸€æ­¥çš„å®æ–½ï¼Œå±•ç¤ºå¦‚ä½•åˆ©ç”¨
    GPT-4Vï¼ˆè§†è§‰ï¼‰å’Œ TTS å¸®åŠ©è§†è§‰éšœç¢è€…åœ¨è‘¡è„ç‰™æ³¢å°”å›¾çš„è¡—é“ä¸Šå¯¼èˆªã€‚
- en: '![](../Images/1b79d260aa4ed403854e157a6dbcf3a9.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b79d260aa4ed403854e157a6dbcf3a9.png)'
- en: 'Figure 1: OpenAI published several updates on their API services and introduced
    multimodality to GPT4 ([source](https://unsplash.com/photos/a-computer-chip-with-the-word-gat-printed-on-it-Fc1GBkmV-Dw))'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 1ï¼šOpenAI å‘å¸ƒäº†æœ‰å…³å…¶ API æœåŠ¡çš„å¤šä¸ªæ›´æ–°ï¼Œå¹¶å°†å¤šæ¨¡æ€å¼•å…¥ GPT-4ï¼ˆ[æ¥æº](https://unsplash.com/photos/a-computer-chip-with-the-word-gat-printed-on-it-Fc1GBkmV-Dw)ï¼‰
- en: As always, the code is available on our [Github](https://github.com/zaai-ai/large-language-models).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€å¦‚æ—¢å¾€ï¼Œä»£ç å¯ä»¥åœ¨æˆ‘ä»¬çš„ [Github](https://github.com/zaai-ai/large-language-models) ä¸Šæ‰¾åˆ°ã€‚
- en: What is GPT-4V(ision)?
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯ GPT-4Vï¼ˆè§†è§‰ï¼‰ï¼Ÿ
- en: GPT-4, like GPT-3.5, is a large multimodal model capable of processing text
    inputs and producing text outputs [1]. In the latest OpenAI announcement, they
    shared that GPT-4 has been extended to be a multimodal Large Language Model (LLM).
    It means that the model is now able to receive additional modalities as input,
    in this case, images. Multimodal LLMs expand the impact of language-only systems
    with new interfaces and capabilities, opening the door for more elaborate use
    cases. You can see an example of using GPT-4V(ision) in the figure below, where
    vision and reasoning capabilities work together to detect some complex nuances
    in a picture.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4 åƒ GPT-3.5 ä¸€æ ·ï¼Œæ˜¯ä¸€ä¸ªå¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†æ–‡æœ¬è¾“å…¥å¹¶ç”Ÿæˆæ–‡æœ¬è¾“å‡º [1]ã€‚åœ¨æœ€æ–°çš„ OpenAI å…¬å‘Šä¸­ï¼Œä»–ä»¬è¡¨ç¤º GPT-4
    å·²è¢«æ‰©å±•ä¸ºä¸€ä¸ªå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ã€‚è¿™æ„å‘³ç€è¯¥æ¨¡å‹ç°åœ¨èƒ½å¤Ÿæ¥æ”¶é¢å¤–çš„è¾“å…¥æ¨¡æ€ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹æ˜¯å›¾åƒã€‚å¤šæ¨¡æ€ LLM æ‰©å±•äº†ä»…è¯­è¨€ç³»ç»Ÿçš„å½±å“ï¼Œé€šè¿‡æ–°çš„æ¥å£å’Œèƒ½åŠ›ï¼Œä¸ºæ›´å¤æ‚çš„ç”¨ä¾‹å¼€è¾Ÿäº†å¯èƒ½æ€§ã€‚ä½ å¯ä»¥åœ¨ä¸‹å›¾ä¸­çœ‹åˆ°ä½¿ç”¨
    GPT-4Vï¼ˆè§†è§‰ï¼‰çš„ç¤ºä¾‹ï¼Œå…¶ä¸­è§†è§‰å’Œæ¨ç†èƒ½åŠ›ä¸€èµ·å·¥ä½œï¼Œä»¥æ£€æµ‹å›¾ç‰‡ä¸­çš„ä¸€äº›å¤æ‚ç»†å¾®ä¹‹å¤„ã€‚
- en: '![](../Images/c5e43fe1966fe9c01a26c44c95556e66.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c5e43fe1966fe9c01a26c44c95556e66.png)'
- en: 'Figure 2: GPT-4 ability to interpret what is unusual for a human being ([source](https://arxiv.org/pdf/2303.08774.pdf))'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 2ï¼šGPT-4 è§£é‡Šå¯¹äººç±»æ¥è¯´ä¸å¯»å¸¸çš„èƒ½åŠ›ï¼ˆ[æ¥æº](https://arxiv.org/pdf/2303.08774.pdf)ï¼‰
- en: Although OpenAI specifically says in its paper that â€¦
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡ OpenAI åœ¨å…¶è®ºæ–‡ä¸­æ˜ç¡®è¡¨ç¤º â€¦
- en: â€˜*Given both the competitive landscape and the safety implications of large-scale
    models like GPT-4, this report contains no further details about the architecture
    (including model size), hardware, training compute, dataset construction, training
    method, or similar.â€™*
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: â€˜*è€ƒè™‘åˆ°ç«äº‰ç¯å¢ƒä»¥åŠåƒ GPT-4 è¿™æ ·çš„å¤§è§„æ¨¡æ¨¡å‹çš„å®‰å…¨æ€§ï¼Œè¿™ä»½æŠ¥å‘ŠæœªåŒ…å«æœ‰å…³æ¶æ„ï¼ˆåŒ…æ‹¬æ¨¡å‹å¤§å°ï¼‰ã€ç¡¬ä»¶ã€è®­ç»ƒè®¡ç®—ã€æ•°æ®é›†æ„å»ºã€è®­ç»ƒæ–¹æ³•æˆ–ç±»ä¼¼å†…å®¹çš„æ›´å¤šç»†èŠ‚ã€‚*â€™
- en: â€¦ we can try to estimate what the architecture of GPT-4V(ision) looks like.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: â€¦æˆ‘ä»¬å¯ä»¥å°è¯•ä¼°è®¡ GPT-4Vï¼ˆè§†è§‰ï¼‰çš„æ¶æ„æ˜¯ä»€ä¹ˆæ ·çš„ã€‚
- en: 'We know that GPT-4V(vision) receives as input text and images. Therefore, it
    most likely has three main components:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çŸ¥é“ GPT-4Vï¼ˆè§†è§‰ï¼‰æ¥æ”¶æ–‡æœ¬å’Œå›¾åƒä½œä¸ºè¾“å…¥ã€‚å› æ­¤ï¼Œå®ƒå¾ˆå¯èƒ½æœ‰ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼š
- en: '**Encoders:** Separate encoders for handling text and image data'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ç¼–ç å™¨ï¼š** ç”¨äºå¤„ç†æ–‡æœ¬å’Œå›¾åƒæ•°æ®çš„ç‹¬ç«‹ç¼–ç å™¨'
- en: '**Attention mechanism:** It employs advanced attention mechanisms, enabling
    the model to focus on the most relevant parts of both text and image inputs.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ³¨æ„åŠ›æœºåˆ¶ï¼š** å®ƒé‡‡ç”¨å…ˆè¿›çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå…³æ³¨æ–‡æœ¬å’Œå›¾åƒè¾“å…¥ä¸­æœ€ç›¸å…³çš„éƒ¨åˆ†ã€‚'
- en: '**Decoder:** To produce the output text based on the latent space of the encoders
    combined with the attention layer.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è§£ç å™¨ï¼š** æ ¹æ®ç¼–ç å™¨çš„æ½œåœ¨ç©ºé—´ç»“åˆæ³¨æ„åŠ›å±‚ç”Ÿæˆè¾“å‡ºæ–‡æœ¬ã€‚'
- en: '![](../Images/d097929cd283ac88e910dd41d72197ee.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d097929cd283ac88e910dd41d72197ee.png)'
- en: 'Figure 3: Simple Architecture for a multimodal model that uses images and text
    as input (image by author)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾3ï¼šä½¿ç”¨å›¾åƒå’Œæ–‡æœ¬ä½œä¸ºè¾“å…¥çš„å¤šæ¨¡æ€æ¨¡å‹çš„ç®€å•æ¶æ„ï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰
- en: A similar architecture can be found in the ğŸ¦© **Flamingo model [2]** created
    by DeepMind.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼çš„æ¶æ„å¯ä»¥åœ¨ğŸ¦© **Flamingoæ¨¡å‹ [2]**ä¸­æ‰¾åˆ°ï¼Œè¯¥æ¨¡å‹ç”±DeepMindåˆ›å»ºã€‚
- en: Flamingo is designed to handle textual and visual data as inputs in order to
    produce a free-form text as output. The authors define it as a Visual Language
    Model (VLM). There are 3 main components to the model, the input processing, the
    perceiver resampler, and the layers that integrate both data types and generate
    the output text.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Flamingoæ—¨åœ¨å¤„ç†æ–‡æœ¬å’Œè§†è§‰æ•°æ®ä½œä¸ºè¾“å…¥ï¼Œä»¥ç”Ÿæˆè‡ªç”±å½¢å¼çš„æ–‡æœ¬ä½œä¸ºè¾“å‡ºã€‚ä½œè€…å°†å…¶å®šä¹‰ä¸ºè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ã€‚è¯¥æ¨¡å‹æœ‰ä¸‰ä¸ªä¸»è¦ç»„ä»¶ï¼šè¾“å…¥å¤„ç†ã€æ„ŸçŸ¥é‡é‡‡æ ·å™¨å’Œæ•´åˆä¸¤ç§æ•°æ®ç±»å‹å¹¶ç”Ÿæˆè¾“å‡ºæ–‡æœ¬çš„å±‚ã€‚
- en: '**Input Processing**: Flamingo receives both visual and text data. The text
    undergoes the usual process of tokenization before entering the Language Model
    (LM), while the visual input is processed by the Vision Encoder (VE), which turns
    pixels into a more abstract representation of features.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¾“å…¥å¤„ç†**ï¼šFlamingoæ¥æ”¶è§†è§‰å’Œæ–‡æœ¬æ•°æ®ã€‚æ–‡æœ¬åœ¨è¿›å…¥è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰ä¹‹å‰ç»å†å¸¸è§„çš„åˆ†è¯å¤„ç†ï¼Œè€Œè§†è§‰è¾“å…¥ç”±è§†è§‰ç¼–ç å™¨ï¼ˆVEï¼‰å¤„ç†ï¼Œå°†åƒç´ è½¬æ¢ä¸ºæ›´æŠ½è±¡çš„ç‰¹å¾è¡¨ç¤ºã€‚'
- en: '**Perceiver Resampler**: This component further processes the visual features.
    It adds a sense of time to images (especially important in videos) and compresses
    the data into a more manageable format. This is key for efficiently combining
    visual and textual data later in the process.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ„ŸçŸ¥é‡é‡‡æ ·å™¨**ï¼šæ­¤ç»„ä»¶è¿›ä¸€æ­¥å¤„ç†è§†è§‰ç‰¹å¾ã€‚å®ƒä¸ºå›¾åƒæ·»åŠ æ—¶é—´æ„Ÿï¼ˆåœ¨è§†é¢‘ä¸­å°¤ä¸ºé‡è¦ï¼‰ï¼Œå¹¶å°†æ•°æ®å‹ç¼©æˆæ›´æ˜“äºç®¡ç†çš„æ ¼å¼ã€‚è¿™å¯¹åç»­æœ‰æ•ˆç»“åˆè§†è§‰å’Œæ–‡æœ¬æ•°æ®è‡³å…³é‡è¦ã€‚'
- en: '![](../Images/134ec5bcc06ef4040855ac8c79eb0cdf.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/134ec5bcc06ef4040855ac8c79eb0cdf.png)'
- en: 'Figure 4: Perceive Resampler Architecture ([source](https://arxiv.org/pdf/2204.14198.pdf))'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4ï¼šæ„ŸçŸ¥é‡é‡‡æ ·å™¨æ¶æ„ï¼ˆ[æ¥æº](https://arxiv.org/pdf/2204.14198.pdf)ï¼‰
- en: '**Integration and Output:** The processed visual and textual data are then
    integrated into the GATED XATTN-DENSE layer. This layer employs a cross-attention
    mechanism combined with a gating function to merge the two types of data effectively.
    The output from this layer feeds into the LM layer, which finally leads to the
    generation of free-form text output by a Transformer decoder.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**é›†æˆå’Œè¾“å‡ºï¼š** å¤„ç†è¿‡çš„è§†è§‰å’Œæ–‡æœ¬æ•°æ®éšåè¢«æ•´åˆåˆ°GATED XATTN-DENSEå±‚ã€‚è¯¥å±‚é‡‡ç”¨äº†äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ä¸é—¨æ§å‡½æ•°ç»“åˆï¼Œæ¥æœ‰æ•ˆåœ°èåˆä¸¤ç§æ•°æ®ã€‚è¯¥å±‚çš„è¾“å‡ºè¾“å…¥åˆ°LMå±‚ï¼Œæœ€ç»ˆç”±Transformerè§£ç å™¨ç”Ÿæˆè‡ªç”±å½¢å¼çš„æ–‡æœ¬è¾“å‡ºã€‚'
- en: '![](../Images/c2b106ab7885b471616e79d5ad1cf92d.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c2b106ab7885b471616e79d5ad1cf92d.png)'
- en: 'Figure 5: Overview of the Flamingo model. The Flamingo models are a family
    of Visual Language Models (VLM) that can take as input visual data together with
    text and can produce free-form text as output ([source](https://arxiv.org/pdf/2204.14198.pdf)).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾5ï¼šFlamingoæ¨¡å‹æ¦‚è¿°ã€‚Flamingoæ¨¡å‹æ˜¯ä¸€ç³»åˆ—è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰ï¼Œå¯ä»¥åŒæ—¶æ¥å—è§†è§‰æ•°æ®å’Œæ–‡æœ¬ä½œä¸ºè¾“å…¥ï¼Œå¹¶ç”Ÿæˆè‡ªç”±å½¢å¼çš„æ–‡æœ¬è¾“å‡ºï¼ˆ[æ¥æº](https://arxiv.org/pdf/2204.14198.pdf)ï¼‰ã€‚
- en: GPT-4V(ision) API
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT-4V(ision) API
- en: The GPT-4V(ision) API from OpenAI allows for processing both visual and text
    information. We cover the main steps to use the API below.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAIçš„GPT-4V(ision) APIå…è®¸å¤„ç†è§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ã€‚æˆ‘ä»¬åœ¨ä¸‹é¢ä»‹ç»ä½¿ç”¨è¯¥APIçš„ä¸»è¦æ­¥éª¤ã€‚
- en: '**Set Up Your Environment**:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**è®¾ç½®ç¯å¢ƒ**ï¼š'
- en: Install the Python dependencies in your environment, namely the OpenAI library.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ä½ çš„ç¯å¢ƒä¸­å®‰è£…Pythonä¾èµ–é¡¹ï¼Œå³OpenAIåº“ã€‚
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Import the necessary libraries in your Python script.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ä½ çš„Pythonè„šæœ¬ä¸­å¯¼å…¥å¿…è¦çš„åº“ã€‚
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Configure API Parameters**: Utilize the `**ChatCompletion**` class with specific
    parameters for handling multimodal (text and image) data.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**é…ç½®APIå‚æ•°**ï¼šåˆ©ç”¨`**ChatCompletion**`ç±»ï¼Œç»“åˆç‰¹å®šå‚æ•°å¤„ç†å¤šæ¨¡æ€ï¼ˆæ–‡æœ¬å’Œå›¾åƒï¼‰æ•°æ®ã€‚'
- en: '**Model Parameter**: Set this to `**gpt-4-vision-preview**` enable the processing
    of both visual and textual data.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ¨¡å‹å‚æ•°**ï¼šå°†å…¶è®¾ç½®ä¸º`**gpt-4-vision-preview**`ä»¥å¯ç”¨å¯¹è§†è§‰å’Œæ–‡æœ¬æ•°æ®çš„å¤„ç†ã€‚'
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Messages Parameter**: This needs to include both text and images. Images
    should be encoded in base64 format.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ¶ˆæ¯å‚æ•°**ï¼šè¿™éœ€è¦åŒ…æ‹¬æ–‡æœ¬å’Œå›¾åƒã€‚å›¾åƒåº”ä»¥base64æ ¼å¼ç¼–ç ã€‚'
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Handle Images**: Before including them in the API call, images must be converted
    to a base64 format.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¤„ç†å›¾åƒ**ï¼šåœ¨å°†å…¶åŒ…å«åœ¨ API è°ƒç”¨ä¸­ä¹‹å‰ï¼Œå›¾åƒå¿…é¡»è½¬æ¢ä¸º base64 æ ¼å¼ã€‚'
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Execute the API Call**: With the parameters set, make the API call to process
    the input data.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ‰§è¡Œ API è°ƒç”¨**ï¼šè®¾ç½®å¥½å‚æ•°åï¼Œå‘èµ· API è°ƒç”¨ä»¥å¤„ç†è¾“å…¥æ•°æ®ã€‚'
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: What is Text-to-Speech?
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯æ–‡æœ¬åˆ°è¯­éŸ³ï¼Ÿ
- en: TTS [4] technology transforms written text into spoken words. This complex process
    involves several stages, each handled by different models. First, a **Grapheme-to-Phoneme**
    model translates written text into phonetic units. Next, a **Phoneme-to-Mel**
    **Spectrogram** model transforms these phonemes into a visual frequency representation.
    Finally, a **Mel-Spectrogram-to-Audio** model, or **Vocoder**, generates the actual
    spoken audio from this representation.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: TTS [4] æŠ€æœ¯å°†ä¹¦é¢æ–‡å­—è½¬æ¢ä¸ºå£è¯­ã€‚è¿™ä¸€å¤æ‚è¿‡ç¨‹æ¶‰åŠå¤šä¸ªé˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µç”±ä¸åŒçš„æ¨¡å‹å¤„ç†ã€‚é¦–å…ˆï¼Œä¸€ä¸ª **å­—å½¢åˆ°éŸ³ç´ ** æ¨¡å‹å°†ä¹¦é¢æ–‡å­—è½¬æ¢ä¸ºéŸ³ç´ å•ä½ã€‚æ¥ä¸‹æ¥ï¼Œä¸€ä¸ª
    **éŸ³ç´ åˆ° Mel** **è°±å›¾** æ¨¡å‹å°†è¿™äº›éŸ³ç´ è½¬æ¢ä¸ºè§†è§‰é¢‘ç‡è¡¨ç¤ºã€‚æœ€åï¼Œä¸€ä¸ª **Mel-è°±å›¾åˆ°éŸ³é¢‘** æ¨¡å‹æˆ– **å£°ç å™¨** ä»è¿™ç§è¡¨ç¤ºç”Ÿæˆå®é™…çš„å£è¯­éŸ³é¢‘ã€‚
- en: '![](../Images/2e1aeaf68c4df4d6984a6d73332edcdc.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e1aeaf68c4df4d6984a6d73332edcdc.png)'
- en: 'Figure 6: TTS architecture (image by author)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 6ï¼šTTS æ¶æ„ï¼ˆå›¾ç‰‡æ¥æºäºä½œè€…ï¼‰
- en: '**Grapheme-to-Phoneme Conversion**: The first step involves converting written
    words (graphemes) into phonemes, the basic units of pronunciation. For instance,
    the phrase `**"Swifts, flushed from chimneys"**` would be converted into phonetic
    representation like `**"ËˆswÉªfts, ËˆfÉ«É™Êƒt ËˆfÉ¹É™m ËˆtÊƒÉªmniz"**`. A model commonly used
    for this is the G2P-Conformer [5].'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å­—å½¢åˆ°éŸ³ç´ è½¬æ¢**ï¼šç¬¬ä¸€æ­¥æ˜¯å°†ä¹¦å†™çš„å•è¯ï¼ˆå­—å½¢ï¼‰è½¬æ¢ä¸ºéŸ³ç´ ï¼Œå³å‘éŸ³çš„åŸºæœ¬å•ä½ã€‚ä¾‹å¦‚ï¼ŒçŸ­è¯­ `**"Swifts, flushed from chimneys"**`
    å°†è¢«è½¬æ¢ä¸ºéŸ³æ ‡è¡¨ç¤ºï¼Œå¦‚ `**"ËˆswÉªfts, ËˆfÉ«É™Êƒt ËˆfÉ¹É™m ËˆtÊƒÉªmniz"**`ã€‚ä¸€ä¸ªå¸¸ç”¨çš„æ¨¡å‹æ˜¯ G2P-Conformer [5]ã€‚'
- en: '**Phoneme-to-Mel Spectrogram**: Next, these phonemes are processed to create
    a mel-spectrogram, a visual representation of the sound frequencies over time.
    This is typically achieved using an encoder-decoder architecture, such as Tacotron2
    [6]. In this model, a convolutional neural network (CNN) embeds the phonemes,
    which are then processed through a bidirectional long short-term memory (LSTM)
    network. The resulting mel-spectrograms are a key intermediate step, acting as
    a bridge between phonemes and the final audio output.'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**éŸ³ç´ åˆ° Mel è°±å›¾**ï¼šæ¥ä¸‹æ¥ï¼Œè¿™äº›éŸ³ç´ ä¼šè¢«å¤„ç†ä»¥åˆ›å»º mel-è°±å›¾ï¼Œè¿™æ˜¯å£°éŸ³é¢‘ç‡éšæ—¶é—´å˜åŒ–çš„è§†è§‰è¡¨ç¤ºã€‚è¿™é€šå¸¸é€šè¿‡ç¼–ç å™¨-è§£ç å™¨æ¶æ„æ¥å®ç°ï¼Œä¾‹å¦‚
    Tacotron2 [6]ã€‚åœ¨è¿™ä¸ªæ¨¡å‹ä¸­ï¼Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åµŒå…¥éŸ³ç´ ï¼Œç„¶åé€šè¿‡åŒå‘é•¿çŸ­æœŸè®°å¿†ï¼ˆLSTMï¼‰ç½‘ç»œè¿›è¡Œå¤„ç†ã€‚ç”Ÿæˆçš„ mel-è°±å›¾æ˜¯ä¸€ä¸ªå…³é”®çš„ä¸­é—´æ­¥éª¤ï¼Œå……å½“éŸ³ç´ ä¸æœ€ç»ˆéŸ³é¢‘è¾“å‡ºä¹‹é—´çš„æ¡¥æ¢ã€‚'
- en: '**Mel-Spectrogram-to-Audio Conversion**: The final stage involves converting
    the mel-Spectrogram into actual audio. This is where a vocoder comes in, often
    using advanced generative models. WaveNet [7], developed by DeepMind, is a good
    example. It employs a deep neural network with dilated causal convolutions to
    ensure correct sequential processing. Each predicted audio sample is fed back
    into the network to predict the next, allowing the model to capture the long-range
    patterns in the audio signal.'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Mel-è°±å›¾åˆ°éŸ³é¢‘è½¬æ¢**ï¼šæœ€åé˜¶æ®µæ¶‰åŠå°† mel-è°±å›¾è½¬æ¢ä¸ºå®é™…çš„éŸ³é¢‘ã€‚è¿™æ—¶éœ€è¦ä¸€ä¸ªå£°ç å™¨ï¼Œé€šå¸¸ä½¿ç”¨å…ˆè¿›çš„ç”Ÿæˆæ¨¡å‹ã€‚ç”± DeepMind å¼€å‘çš„
    WaveNet [7] æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ä¾‹å­ã€‚å®ƒä½¿ç”¨å¸¦æœ‰æ‰©å¼ å› æœå·ç§¯çš„æ·±åº¦ç¥ç»ç½‘ç»œï¼Œä»¥ç¡®ä¿æ­£ç¡®çš„åºåˆ—å¤„ç†ã€‚æ¯ä¸ªé¢„æµ‹çš„éŸ³é¢‘æ ·æœ¬ä¼šåé¦ˆåˆ°ç½‘ç»œä¸­ä»¥é¢„æµ‹ä¸‹ä¸€ä¸ªï¼Œä»è€Œä½¿æ¨¡å‹èƒ½å¤Ÿæ•æ‰éŸ³é¢‘ä¿¡å·ä¸­çš„é•¿è·ç¦»æ¨¡å¼ã€‚'
- en: Text-to-Speech API
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ–‡æœ¬åˆ°è¯­éŸ³ API
- en: OpenAI provides a TTS service accessible via an API, offering two quality levels
    and six distinct voices to cater to different needs and preferences.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI æä¾›äº†ä¸€ä¸ªé€šè¿‡ API è®¿é—®çš„ TTS æœåŠ¡ï¼Œæä¾›ä¸¤ä¸ªè´¨é‡ç­‰çº§å’Œå…­ç§ä¸åŒçš„å£°éŸ³ï¼Œä»¥æ»¡è¶³ä¸åŒçš„éœ€æ±‚å’Œåå¥½ã€‚
- en: '**Quality Options**:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**è´¨é‡é€‰é¡¹**ï¼š'
- en: '`**tts-1**`: Ideal for real-time applications, this option offers lower quality
    but benefits from reduced latency.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**tts-1**`ï¼šé€‚ç”¨äºå®æ—¶åº”ç”¨ï¼Œè¿™ä¸ªé€‰é¡¹æä¾›è¾ƒä½çš„è´¨é‡ä½†å…·æœ‰å‡å°‘å»¶è¿Ÿçš„ä¼˜ç‚¹ã€‚'
- en: '`**tts-1-hd**`: Suited for scenarios where higher audio quality is relevant
    and latency is not a problem.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`**tts-1-hd**`ï¼šé€‚åˆäºéŸ³é¢‘è´¨é‡è¾ƒé«˜ä¸”å»¶è¿Ÿä¸æ˜¯é—®é¢˜çš„åœºæ™¯ã€‚'
- en: '**Voice Selection**:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**é€‰æ‹©å£°éŸ³**ï¼š'
- en: 'OpenAIâ€™s TTS API features six unique voices: `**alloy**`, `**echo**`, `**fable**`,
    `**onyx**`, `**nova**`, and `**shimmer**`.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI çš„ TTS API å…·æœ‰å…­ç§ç‹¬ç‰¹çš„å£°éŸ³ï¼š`**alloy**`ã€`**echo**`ã€`**fable**`ã€`**onyx**`ã€`**nova**`
    å’Œ `**shimmer**`ã€‚
- en: Each voice has its own character; for instance, `**Fable**` resembles a podcast
    narrator's voice.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¯ç§å£°éŸ³éƒ½æœ‰å…¶ç‹¬ç‰¹çš„ç‰¹æ€§ï¼›ä¾‹å¦‚ï¼Œ`**Fable**` ç±»ä¼¼äºæ’­å®¢å™è¿°è€…çš„å£°éŸ³ã€‚
- en: You can preview these voices on OpenAIâ€™s [Text-to-Speech Guide](https://platform.openai.com/docs/guides/text-to-speech).
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨ OpenAI çš„ [æ–‡æœ¬åˆ°è¯­éŸ³æŒ‡å—](https://platform.openai.com/docs/guides/text-to-speech)
    ä¸Šé¢„è§ˆè¿™äº›å£°éŸ³ã€‚
- en: '**Making an API Request:**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**å‘èµ· API è¯·æ±‚ï¼š**'
- en: 'To use OpenAIâ€™s TTS API, send a request to `**https://api.openai.com/v1/audio/speech**`.
    Here''s what you''ll need:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨ OpenAI çš„ TTS APIï¼Œå‘é€è¯·æ±‚åˆ° `**https://api.openai.com/v1/audio/speech**`ã€‚ä½ éœ€è¦ä»¥ä¸‹å†…å®¹ï¼š
- en: '**Model Specification**: Choose between `**tts-1**` (low quality, low latency)
    or `**tts-1-hd**` (high quality, high latency) based on your needs.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ¨¡å‹è§„æ ¼**ï¼šæ ¹æ®ä½ çš„éœ€æ±‚é€‰æ‹© `**tts-1**`ï¼ˆä½è´¨é‡ï¼Œä½å»¶è¿Ÿï¼‰æˆ– `**tts-1-hd**`ï¼ˆé«˜è´¨é‡ï¼Œé«˜å»¶è¿Ÿï¼‰ã€‚'
- en: '**Input Text**: The textual content you want to convert into speech.'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è¾“å…¥æ–‡æœ¬**ï¼šä½ æƒ³è¦è½¬æ¢ä¸ºè¯­éŸ³çš„æ–‡æœ¬å†…å®¹ã€‚'
- en: '**Voice Choice**: Select from the available voices to find the one that best
    suits your content and audience.'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**è¯­éŸ³é€‰æ‹©**ï¼šä»å¯ç”¨çš„è¯­éŸ³ä¸­é€‰æ‹©ä¸€ä¸ªæœ€é€‚åˆä½ çš„å†…å®¹å’Œè§‚ä¼—çš„å£°éŸ³ã€‚'
- en: 'Hereâ€™s a basic example of how to structure your API request:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªå…³äºå¦‚ä½•ç»“æ„åŒ– API è¯·æ±‚çš„åŸºæœ¬ç¤ºä¾‹ï¼š
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: How to implement an application to help visually impaired people feel safer
    when walking down a street?
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦‚ä½•å®ç°ä¸€ä¸ªå¸®åŠ©è§†éšœäººå£«åœ¨è¡—ä¸Šè¡Œèµ°æ—¶æ„Ÿåˆ°æ›´å®‰å…¨çš„åº”ç”¨ï¼Ÿ
- en: This section describes step by step how you can create the audio description
    of your video using GPT-4V(ision) and TTS from OpenAI. It also covers how you
    can add the generated audio to your video.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚é€æ­¥æè¿°äº†å¦‚ä½•ä½¿ç”¨ OpenAI çš„ GPT-4V(ision) å’Œ TTS åˆ›å»ºè§†é¢‘çš„éŸ³é¢‘æè¿°ã€‚è¿˜æ¶µç›–äº†å¦‚ä½•å°†ç”Ÿæˆçš„éŸ³é¢‘æ·»åŠ åˆ°è§†é¢‘ä¸­ã€‚
- en: In our case, and as stated in the introduction, we created an audio guide to
    help a visually impaired person walking down the street by identifying the location
    of the obstacles and providing them with spatial information.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œå¦‚ä»‹ç»æ‰€è¿°ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªéŸ³é¢‘æŒ‡å—ï¼Œå¸®åŠ©è§†éšœäººå£«åœ¨è¡—ä¸Šè¡Œèµ°ï¼Œé€šè¿‡è¯†åˆ«éšœç¢ç‰©çš„ä½ç½®å¹¶æä¾›ç©ºé—´ä¿¡æ¯æ¥è¾…åŠ©ä»–ä»¬ã€‚
- en: The process begins with importing necessary libraries and setting up the environment
    in Python. We use libraries such as cv2 for video processing and openai for accessing
    the GPT-4V(ision) and TTS models.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: è¿‡ç¨‹å¼€å§‹äºå¯¼å…¥å¿…è¦çš„åº“å¹¶è®¾ç½® Python ç¯å¢ƒã€‚æˆ‘ä»¬ä½¿ç”¨å¦‚ cv2 è¿›è¡Œè§†é¢‘å¤„ç†å’Œ openai è®¿é—® GPT-4V(ision) å’Œ TTS æ¨¡å‹ç­‰åº“ã€‚
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Next, we load and process the video. The video is resized to a manageable resolution,
    ensuring we donâ€™t exceed the token limits of OpenAI. Each frame is encoded in
    base64, the required format for OpenAIâ€™s API.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åŠ è½½å¹¶å¤„ç†è§†é¢‘ã€‚è§†é¢‘è¢«è°ƒæ•´ä¸ºå¯ç®¡ç†çš„åˆ†è¾¨ç‡ï¼Œä»¥ç¡®ä¿ä¸è¶…è¿‡ OpenAI çš„ä»¤ç‰Œé™åˆ¶ã€‚æ¯ä¸€å¸§éƒ½è¢«ç¼–ç ä¸º base64ï¼Œè¿™æ˜¯ OpenAI API
    æ‰€éœ€çš„æ ¼å¼ã€‚
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'An important step is crafting the right prompt for GPT-4V(ision). A well-engineered
    prompt significantly influences the output that we get from the model. From our
    experiments, the 2 main components that influenced our outputs were:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªé‡è¦çš„æ­¥éª¤æ˜¯ä¸º GPT-4V(ision) åˆ¶ä½œåˆé€‚çš„æç¤ºã€‚ç²¾å¿ƒè®¾è®¡çš„æç¤ºä¼šæ˜¾è‘—å½±å“æˆ‘ä»¬ä»æ¨¡å‹ä¸­è·å¾—çš„è¾“å‡ºã€‚ä»æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œå½±å“è¾“å‡ºçš„ä¸¤ä¸ªä¸»è¦ç»„ä»¶æ˜¯ï¼š
- en: Verbs like describe, narrate, tell;
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŠ¨è¯å¦‚æè¿°ã€å™è¿°ã€è®²è¿°ï¼›
- en: Identifying the style of speech we wanted.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¡®å®šæˆ‘ä»¬æƒ³è¦çš„è¯­éŸ³é£æ ¼ã€‚
- en: 'One of the first prompts that we tried was the following: â€˜*These are frames
    from a person walking in a city. Describe the elements and obstacles around you
    to help a blind person navigate them successfully.* With this structure, the model
    tends to get extremely verbose and descriptive. Our use case requires a much less
    noisy output.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°è¯•çš„ç¬¬ä¸€ä¸ªæç¤ºä¹‹ä¸€æ˜¯ï¼šâ€˜*è¿™äº›æ˜¯ä¸€ä¸ªäººåœ¨åŸå¸‚ä¸­è¡Œèµ°çš„ç”»é¢ã€‚æè¿°å‘¨å›´çš„å…ƒç´ å’Œéšœç¢ç‰©ï¼Œä»¥å¸®åŠ©ç›²äººæˆåŠŸå¯¼èˆªã€‚*â€™ è¿™ç§ç»“æ„ä½¿æ¨¡å‹å˜å¾—æå…¶å†—é•¿å’Œæè¿°æ€§ã€‚æˆ‘ä»¬çš„ä½¿ç”¨æ¡ˆä¾‹éœ€è¦è¾“å‡ºæ›´å°‘å™ªéŸ³ã€‚
- en: 'The prompt that worked for us was the following: *â€˜These are frames from a
    person walking. Narrate succinctly with short sentences like they do in GPS devices
    the elements and obstacles around you to help a blind person go navigate them
    successfully.â€™* This time the model gave us short sentences that allowed us to
    get just the essential information for street navigation. The result is below:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹æˆ‘ä»¬æœ‰æ•ˆçš„æç¤ºæ˜¯ï¼š*â€˜è¿™äº›æ˜¯ä¸€ä¸ªäººè¡Œèµ°çš„ç”»é¢ã€‚ç”¨ GPS è®¾å¤‡é‚£æ ·çš„ç®€çŸ­å¥å­ç®€æ´å™è¿°å‘¨å›´çš„å…ƒç´ å’Œéšœç¢ç‰©ï¼Œä»¥å¸®åŠ©ç›²äººæˆåŠŸå¯¼èˆªã€‚â€™* è¿™æ¬¡æ¨¡å‹ç»™å‡ºäº†ç®€çŸ­çš„å¥å­ï¼Œè®©æˆ‘ä»¬è·å¾—äº†è¡—é“å¯¼èˆªæ‰€éœ€çš„åŸºæœ¬ä¿¡æ¯ã€‚ç»“æœå¦‚ä¸‹ï¼š
- en: '*â€œWalk straight on a textured pathway, keep the building to your right. Continue
    forward with slight curve to the right. Stay straight, small overhang ahead on
    your right. Proceed, passing the overhang, continue on flat path. Straight ahead,
    approaching a well-lit area. After well-lit area, transition onto patterned pavement.
    Follow the tiled pavement with guiding lines straight ahead. Continue under the
    passageway, keeping the pillars parallel to you. Move through passageway, slight
    descent ahead. Pathway ends, prepare to stop at pedestrian crossing. Stand at
    crosswalk, wait for audible signal to cross the street.â€*'
  id: totrans-88
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*â€œåœ¨æœ‰çº¹ç†çš„è·¯å¾„ä¸Šç›´è¡Œï¼Œå°†å»ºç­‘ç‰©ä¿æŒåœ¨å³ä¾§ã€‚ç»§ç»­å‰è¡Œï¼Œç¨å¾®å‘å³è½¬ã€‚ä¿æŒç›´è¡Œï¼Œå‰æ–¹å³ä¾§æœ‰å°æ‚¬æŒ‘ã€‚ç»§ç»­å‰è¿›ï¼Œç»è¿‡æ‚¬æŒ‘ï¼Œç»§ç»­åœ¨å¹³å¦çš„è·¯å¾„ä¸Šå‰è¡Œã€‚ç›´è¡Œï¼Œæ¥è¿‘ä¸€ä¸ªå…‰çº¿å……è¶³çš„åŒºåŸŸã€‚ç»è¿‡å…‰çº¿å……è¶³çš„åŒºåŸŸåï¼Œè¿‡æ¸¡åˆ°æœ‰å›¾æ¡ˆçš„äººè¡Œé“ã€‚è·Ÿéšæœ‰å¼•å¯¼çº¿çš„ç“·ç –äººè¡Œé“ç›´è¡Œã€‚ç»§ç»­é€šè¿‡é€šé“ï¼Œä¿æŒæŸ±å­ä¸è‡ªå·±å¹³è¡Œã€‚ç©¿è¿‡é€šé“ï¼Œå‰æ–¹æœ‰å°çš„ä¸‹é™ã€‚è·¯å¾„ç»“æŸï¼Œå‡†å¤‡åœ¨æ–‘é©¬çº¿åœä¸‹ã€‚ç«™åœ¨æ–‘é©¬çº¿å‰ï¼Œç­‰å¾…å¬åˆ°è¿‡é©¬è·¯çš„ä¿¡å·ã€‚â€*'
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Once we receive the description from GPT-4V(ision), the next step is converting
    this text into audio. We chose the fable voice for its clarity and resemblance
    to narration. The TTS API from OpenAI is used to transform the generated text
    into an audio file.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æˆ‘ä»¬æ”¶åˆ°GPT-4V(ision)çš„æè¿°ï¼Œä¸‹ä¸€æ­¥æ˜¯å°†æ–‡æœ¬è½¬æ¢ä¸ºéŸ³é¢‘ã€‚æˆ‘ä»¬é€‰æ‹©äº†å¯“è¨€é£æ ¼çš„å£°éŸ³ï¼Œå› ä¸ºå®ƒçš„æ¸…æ™°åº¦å’Œå™è¿°çš„ç›¸ä¼¼æ€§ã€‚ä½¿ç”¨OpenAIçš„TTS
    APIå°†ç”Ÿæˆçš„æ–‡æœ¬è½¬åŒ–ä¸ºéŸ³é¢‘æ–‡ä»¶ã€‚
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Finally, the last step is to merge the audio with the original video.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åä¸€æ­¥æ˜¯å°†éŸ³é¢‘ä¸åŸå§‹è§†é¢‘åˆå¹¶ã€‚
- en: '[PRE11]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: You can see the resulting video [here](https://youtu.be/Erena-iLAY0).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨[è¿™é‡Œ](https://youtu.be/Erena-iLAY0)æŸ¥çœ‹æœ€ç»ˆè§†é¢‘ã€‚
- en: Conclusion
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: OpenAIâ€™s GPT-4V(ision) and TTS APIs open new possibilities to solve important
    use cases that can change the daily lives of many. We explored one that focuses
    on inclusivity for the visually impaired but we could have followed many others.
    As we discussed in the introduction, they also allow us to elevate our human experiences
    (e.g. museum tours), and cater them even more to individual preferences and circumstances.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAIçš„GPT-4V(ision)å’ŒTTS APIå¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ï¼Œè§£å†³äº†å¯ä»¥æ”¹å˜è®¸å¤šäººæ—¥å¸¸ç”Ÿæ´»çš„é‡è¦ç”¨ä¾‹ã€‚æˆ‘ä»¬æ¢è®¨äº†ä¸€ä¸ªä¸“æ³¨äºè§†éšœäººå£«çš„åŒ…å®¹æ€§çš„ç”¨ä¾‹ï¼Œä½†æˆ‘ä»¬æœ¬å¯ä»¥é€‰æ‹©å…¶ä»–è®¸å¤šç”¨ä¾‹ã€‚å¦‚åœ¨ä»‹ç»ä¸­æ‰€è¿°ï¼Œå®ƒä»¬è¿˜è®©æˆ‘ä»¬èƒ½å¤Ÿæå‡äººç±»ä½“éªŒï¼ˆä¾‹å¦‚åšç‰©é¦†å¯¼è§ˆï¼‰ï¼Œå¹¶æ ¹æ®ä¸ªäººçš„åå¥½å’Œæƒ…å†µè¿›è¡Œæ›´å¥½çš„å®šåˆ¶ã€‚
- en: During the implementation, we found that prompt engineering had a significant
    impact on tailoring the solution to our specific use case. In the future, other
    approaches such as fine-tuning or some type of Retrieval-augmented generation
    (RAG) might be adapted to the VLMs. We have been seeing how these methods are
    useful to make LLMs better for certain tasks and contexts. While the output shows
    the potential of these new models, there is still work to be done. As seen in
    our experiment, the VLM *â€˜speaksâ€™* like if you could see when it says *â€œFollow
    the tiled pavement with guiding lines straight aheadâ€.* It also struggles to accurately
    distinguish between left and right, which is an interesting fact to explore further.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®æ–½è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å‘ç°æç¤ºå·¥ç¨‹å¯¹å°†è§£å†³æ–¹æ¡ˆé‡èº«å®šåˆ¶åˆ°æˆ‘ä»¬çš„ç‰¹å®šç”¨ä¾‹æœ‰æ˜¾è‘—å½±å“ã€‚æœªæ¥ï¼Œå…¶ä»–æ–¹æ³•å¦‚å¾®è°ƒæˆ–æŸç§ç±»å‹çš„æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰å¯èƒ½ä¼šé€‚ç”¨äºVLMsã€‚æˆ‘ä»¬çœ‹åˆ°è¿™äº›æ–¹æ³•åœ¨æŸäº›ä»»åŠ¡å’Œæƒ…å¢ƒä¸­ä½¿LLMsæ›´ä¸ºæœ‰æ•ˆã€‚å°½ç®¡è¾“å‡ºå±•ç¤ºäº†è¿™äº›æ–°æ¨¡å‹çš„æ½œåŠ›ï¼Œä½†ä»æœ‰å¾…å®Œå–„ã€‚å¦‚æˆ‘ä»¬å®éªŒä¸­æ‰€è§ï¼ŒVLM
    *â€œè¯´è¯â€*çš„æ–¹å¼åƒæ˜¯ä½ å¯ä»¥çœ‹åˆ°å®ƒæ‰€è¯´çš„*â€œè·Ÿéšæœ‰å¼•å¯¼çº¿çš„ç“·ç –äººè¡Œé“ç›´è¡Œã€‚â€* å®ƒè¿˜éš¾ä»¥å‡†ç¡®åŒºåˆ†å·¦å³ï¼Œè¿™æ˜¯ä¸€ä¸ªå€¼å¾—è¿›ä¸€æ­¥æ¢ç´¢çš„æœ‰è¶£äº‹å®ã€‚
- en: Despite these challenges, OpenAIâ€™s latest developments show us a possible future
    where the world is more inclusive and experiences can be enhanced by AI.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡é¢ä¸´è¿™äº›æŒ‘æˆ˜ï¼ŒOpenAIçš„æœ€æ–°è¿›å±•å‘æˆ‘ä»¬å±•ç¤ºäº†ä¸€ä¸ªæ›´åŠ åŒ…å®¹çš„æœªæ¥ï¼ŒAIå¯ä»¥æå‡ä½“éªŒã€‚
- en: 'Keep in touch: [LinkedIn](https://www.linkedin.com/in/luisbrasroque/), [X/Twitter](https://x.com/luisbrasroque),
    [Medium](https://medium.com/@luisroque).'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ä¿æŒè”ç³»ï¼š[LinkedIn](https://www.linkedin.com/in/luisbrasroque/)ï¼Œ[X/Twitter](https://x.com/luisbrasroque)ï¼Œ[Medium](https://medium.com/@luisroque)ã€‚
- en: References
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[1] OpenAI. (2023). GPT-4 Technical Report. arXiv preprint arXiv:2303.08774.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] OpenAI. (2023). GPT-4æŠ€æœ¯æŠ¥å‘Šã€‚arXivé¢„å°æœ¬arXiv:2303.08774ã€‚'
- en: '[2] Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y.,
    Lenc, K., Mensch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi,
    S., Han, T., Gong, Z., Samangooei, S., Monteiro, M., Menick, J., Borgeaud, S.,
    Brock, A., Nematzadeh, A., Sharifzadeh, S., Binkowski, M., Barreira, R., Vinyals,
    O., Zisserman, A., & Simonyan, K. (2022). Flamingo: a Visual Language Model for
    Few-Shot Learning. arXiv preprint arXiv:2204.14198.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y.,
    Lenc, K., Mensch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi,
    S., Han, T., Gong, Z., Samangooei, S., Monteiro, M., Menick, J., Borgeaud, S.,
    Brock, A., Nematzadeh, A., Sharifzadeh, S., Binkowski, M., Barreira, R., Vinyals,
    O., Zisserman, A., & Simonyan, K. (2022). Flamingo: ä¸€ç§ç”¨äºå°‘æ ·æœ¬å­¦ä¹ çš„è§†è§‰è¯­è¨€æ¨¡å‹ã€‚arXiv é¢„å°æœ¬
    arXiv:2204.14198ã€‚'
- en: '[3] Brock, A., De, S., Smith, S. L., & Simonyan, K. (2021). High-performance
    large-scale image recognition without normalization. arXiv preprint arXiv:2102.06171.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Brock, A., De, S., Smith, S. L., & Simonyan, K. (2021). æ— éœ€å½’ä¸€åŒ–çš„é«˜æ€§èƒ½å¤§è§„æ¨¡å›¾åƒè¯†åˆ«ã€‚arXiv
    é¢„å°æœ¬ arXiv:2102.06171ã€‚'
- en: '[4] Maheshwari, H. (2021, May 11). Basic Text to Speech, Explained. Towards
    Data Science. Retrieved from [https://towardsdatascience.com/text-to-speech-explained-from-basic-498119aa38b5](/text-to-speech-explained-from-basic-498119aa38b5)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Maheshwari, H. (2021å¹´5æœˆ11æ—¥). åŸºç¡€æ–‡æœ¬åˆ°è¯­éŸ³ï¼Œè§£æã€‚Towards Data Scienceã€‚å–è‡ª [https://towardsdatascience.com/text-to-speech-explained-from-basic-498119aa38b5](/text-to-speech-explained-from-basic-498119aa38b5)ã€‚'
- en: '[5] Gulati, A., Qin, J., Chiu, C.-C., Parmar, N., Zhang, Y., Yu, J., Han, W.,
    Wang, S., Zhang, Z., Wu, Y., et al. (2020). Conformer: convolution-augmented transformer
    for speech recognition. arXiv preprint arXiv:2005.08100.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Gulati, A., Qin, J., Chiu, C.-C., Parmar, N., Zhang, Y., Yu, J., Han, W.,
    Wang, S., Zhang, Z., Wu, Y., ç­‰. (2020). Conformer: ç”¨äºè¯­éŸ³è¯†åˆ«çš„å·ç§¯å¢å¼ºå˜æ¢å™¨ã€‚arXiv é¢„å°æœ¬ arXiv:2005.08100ã€‚'
- en: '[6] Shen, J., Pang, R., Weiss, R. J., Schuster, M., Jaitly, N., Yang, Z., Chen,
    Z., Zhang, Y., Wang, Y., Skerry-Ryan, R. J., Saurous, R. A., Agiomyrgiannakis,
    Y., & Wu, Y. (2018). Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram
    Predictions. arXiv preprint arXiv:1712.05884.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Shen, J., Pang, R., Weiss, R. J., Schuster, M., Jaitly, N., Yang, Z., Chen,
    Z., Zhang, Y., Wang, Y., Skerry-Ryan, R. J., Saurous, R. A., Agiomyrgiannakis,
    Y., & Wu, Y. (2018). é€šè¿‡å°† WaveNet æ¡ä»¶åŒ–äº Mel é¢‘è°±é¢„æµ‹è¿›è¡Œè‡ªç„¶ TTS åˆæˆã€‚arXiv é¢„å°æœ¬ arXiv:1712.05884ã€‚'
- en: '[7] van den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves,
    A., Kalchbrenner, N., Senior, A., & Kavukcuoglu, K. (2016). WaveNet: A Generative
    Model for Raw Audio. arXiv preprint arXiv:1609.03499.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] van den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves,
    A., Kalchbrenner, N., Senior, A., & Kavukcuoglu, K. (2016). WaveNet: ä¸€ç§åŸå§‹éŸ³é¢‘çš„ç”Ÿæˆæ¨¡å‹ã€‚arXiv
    é¢„å°æœ¬ arXiv:1609.03499ã€‚'
