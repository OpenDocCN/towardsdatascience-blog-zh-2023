- en: How to Avoid Being Fooled by Model Accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-avoid-being-fooled-by-model-accuracy-e26307385fe1](https://towardsdatascience.com/how-to-avoid-being-fooled-by-model-accuracy-e26307385fe1)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A visual guide to binary classification model metrics and their proper usage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://johnadeojo.medium.com/?source=post_page-----e26307385fe1--------------------------------)[![John
    Adeojo](../Images/f6460fae462b055d36dce16fefcd142c.png)](https://johnadeojo.medium.com/?source=post_page-----e26307385fe1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e26307385fe1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e26307385fe1--------------------------------)
    [John Adeojo](https://johnadeojo.medium.com/?source=post_page-----e26307385fe1--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e26307385fe1--------------------------------)
    ·8 min read·Jul 7, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/752a27d99b7d07b9b21cd0e1e97debde.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: Generated with Midjourney'
  prefs: []
  type: TYPE_NORMAL
- en: Background — Simple on the Surface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The metrics used for gauging performance of classification models are fairly
    straightforward, at least from a mathematical standpoint. Nevertheless, I have
    observed that many modellers and data scientists encounter difficulty articulating
    these metrics, and even apply them incorrectly. This is an easy mistake to make,
    as these metrics appear simple on the surface, yet their implications can be profound
    depending on the problem domain.
  prefs: []
  type: TYPE_NORMAL
- en: This article serves as a visual guide to explaining common classification model
    metrics. We will explore definitions and use examples to highlight where metrics
    are used inappropriately.
  prefs: []
  type: TYPE_NORMAL
- en: A Brief Note on Visualisation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Each visualisation comprises of ninety subjects, representing anything we might
    wish to classify. Blue subjects denote negative samples, whilst red are positive
    samples. The purple box is the model which attempts to predict positive samples.
    Anything inside this box is what the model predicts as positive.
  prefs: []
  type: TYPE_NORMAL
- en: With that clarified, let’s delve into the definitions.
  prefs: []
  type: TYPE_NORMAL
- en: Precision & Recall
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For many classification tasks there is a trade-off between precision and recall.
    It’s frequently the case that optimising for recall incurs a cost to precision.
    But what do these terms actually mean? Let’s begin with the mathematical definitions,
    and then move onto the visual representations.
  prefs: []
  type: TYPE_NORMAL
- en: Precision = TP/ (TP + FP)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Recall = TP/(TP + FN)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Where TP = Number of true positives, FP = Number of false positives, FN =
    Number of false negatives.*'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s focus on the chart directly below in which there are four positive samples.
    Remember, the model’s positive predictions are represented by the box on the chart.
    Observing the chart, we see the model correctly predicts all four positive samples—
    we can see this as all the positive samples sit within the box. We can calculate
    model recall from the chart by counting positive cases within the box (TP = 4)
    divided by the total number of positive cases (TP = 4 + FN = 0).
  prefs: []
  type: TYPE_NORMAL
- en: '*Note FN is 0 because there are no positive cases outside of the box.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/21c5441392a1cafb53f748009e151f5e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: Visual Representation of model with 100% recall and 40% precision.
    Model is represented by the purple box.'
  prefs: []
  type: TYPE_NORMAL
- en: Precision can be explained similarly. It is simply the number of positive cases
    in the box (TP=4) divided by the total number of cases in the box (TP = 4 + FP
    = 6). A straightforward calculation reveals the model’s precision to be just 40%.
  prefs: []
  type: TYPE_NORMAL
- en: You can observe that a model can have a high recall but low precision, and vice
    versa. The chart below shows this, where recall is just 50%, while precision is
    100%. See if you can internalise how to get to these numbers.
  prefs: []
  type: TYPE_NORMAL
- en: '*Here is a clue to help you, the number of false negatives is two since there
    are two positive samples outside of the box.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/73c11298e4cd354608f44fc3e2fc9864.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: Visual Representation of model with 100% Precision and 50%
    Recall. Model is represented as the purple box.'
  prefs: []
  type: TYPE_NORMAL
- en: False Positive Rates & True Negative Rates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The false positive rate (FPR) perhaps appears more intuitive, possibly due
    to its name. However, let’s explore the concept in the same way we did for the
    other metrics. Mathematically, we express the FPR as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: FPR = FP/(FP + TN)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Here, TN represents the number of true negative subjects.*'
  prefs: []
  type: TYPE_NORMAL
- en: Examining the first image again, the FPR can be determined by looking at the
    number of negative samples inside the box (FP=6) divided by the total number of
    negative samples (FP=6 + TN=80). For our first image, the false positive rate
    is just 7%, and for the second, it’s 0%. Try figuring out why this is the case.
  prefs: []
  type: TYPE_NORMAL
- en: '*Remember, the subjects inside the box are those that the model* ***predicts***
    *are positive. So by extension, the negative samples outside the box are those
    that the model has identified as negative.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The true negative rate (TNR) can be computed using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: TNR = TN/(TN + FP)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Notice that TNR is always one minus the false positive rate.*'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Accuracy is a term that is loosely thrown around in the context of model performance,
    but what does it actually mean? Let’s start with the mathematical definition:'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy = (TP + TN) / (TP + TN + FP + FN)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Using the same logic we previously applied, we can calculate the model’s accuracy
    as 93% for the first image and 97% for the second (see if you can derive this
    for yourself). This might be raising red flags in your mind as to why accuracy
    can be a deceptive metric in some cases. We will explore this in greater detail
    next.
  prefs: []
  type: TYPE_NORMAL
- en: Using Metrics Correctly
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why do we concern ourselves with these metrics? Because they equip us with ways
    to assess the performance of our models. Once we comprehend these metrics, we
    can even determine the commercial value associated with models. This is why it
    is important to have a good intuition around there appropriate (and inappropriate)
    use. To illustrate this, we will briefly investigate the two common scenarios
    in classification tasks, namely balanced and imbalanced datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Imbalanced Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The diagrams depicted earlier are instances of imbalanced classification tasks.
    Put simply, imbalanced tasks have a low representation of positive subjects compared
    with negative subjects. Many commercial use cases for binary classification fall
    into this category like credit card fraud detection and customer churn prediction,
    spam filtering etc. Selecting the incorrect metrics for imbalanced classification
    can lead you to have over-optimistic beliefs about the performance of your model.
  prefs: []
  type: TYPE_NORMAL
- en: The primary issue with imbalanced classification is the potential for the number
    of true negative samples to be high, and false negatives to be low. To illustrate,
    let’s consider another model and assess it on our imbalanced data. We can create
    an extreme scenario where the model simply predicts every subject as negative.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a564ea10aebbec677916263ef6121733.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: Visual representation of “non-discriminatory” model predicting
    negative for every subject on an imbalanced dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s compute each of the metrics in this scenario.
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy**: *(TP=0 + TN=86)/(TP=0 + TN=86 + FP=0 + FN=4) = 95%*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Precision**: *(TP=0) /(TP=0 + FP=0) = undefined*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recall**: *(TP=0) / (TP=0 + FN=4) = 0%*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FPR**: *(FP=0) / (FP=0 + TN=86) = 0%*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TNR**: *(TN=86) / (TN=86 + FP=0) = 100%*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The issues with accuracy, FPR and TNR should start to become more apparent.
    When we are working with imbalanced datasets, we can produce a high-accuracy model
    that performs poorly upon deployment. In the previous example, the model has no
    capacity to detect positive subjects but still achieves an accuracy of 95%, A
    0% FPR and a perfect TNR.
  prefs: []
  type: TYPE_NORMAL
- en: Now, imagine deploying such a model to conduct medical diagnostics or detect
    fraud; it would quite evidently be useless and perhaps even dangerous. This extreme
    example illustrates the problem of using metrics such as accuracy, FPR, and TPR
    to assess the performance of models working on imbalanced data.
  prefs: []
  type: TYPE_NORMAL
- en: Balanced Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For balanced classification problems, the number of potential true negatives
    is significantly smaller than in the imbalanced case.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b0feac1766265cbe79bf05ebb69a82bb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: Visual representation of “non-discriminatory” model predicting
    negative for every subject on a balanced dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we take our “non-discriminatory” model and apply it to the balanced case,
    we obtain the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy**: *(TP=0 + TN=45) / (TP=0 + TN=45 + FP=0 + FN=45) = 50%*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Precision**: *(TP=0) / (TP=0 + FP=0) = undefined*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recall**: *(TP=0) / (TP=0 + FN=45) = 0%*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FPR**: *(FP=0) / (FP=0 + TN=45) = 0%*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TNR**: *(TN=45) / (TN=45 + FP=0) = 100%*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While all of the other metrics remain the same, the model’s accuracy has declined
    to 50%, arguably a much more indicative representation of the model’s actual performance.
    Albeit accuracy is still deceptive without precision and recall.
  prefs: []
  type: TYPE_NORMAL
- en: ROC Curves vs. Precision-Recall Curves
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ROC curves are a common approach used to evaluate the performance of binary
    classification models. However, when dealing with imbalanced datasets, they can
    also provide over-optimistic and not entirely meaningful results.
  prefs: []
  type: TYPE_NORMAL
- en: '***A brief overview of ROC and Precision-Recall curves****: we are essentially
    plotting the classification metrics against each other for different decision
    thresholds. We commonly measure the area under the curve (or AUC), to give us
    an indication of the models performance. Follow the links to learn more about*
    [*ROC*](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc#:~:text=An%20ROC%20curve%20(receiver%20operating,False%20Positive%20Rate)
    *and* [*Precision- Recall Curves*](https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate how ROC curves can be over optimistic, I have built a classification
    model on a credit card fraud dataset taken from [Kaggle](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud).
    The dataset comprises 284,807 transactions, of which 492 are fraudulent.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note: The data is free to use for commercial and non-commercial purposes without
    permission, as outlined in the* [*Open Data Commons license*](https://opendatacommons.org/licenses/dbcl/1-0/)
    *attributed to the data.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/714635625769ecff029f475c979d53b3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: ROC Curve on imbalanced dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Upon examining the ROC curve, we might be led to believe the model performance
    is better than it actually is, since the area under this curve is 0.97\. As we
    have previously seen, the false positive rate can be overly optimistic for imbalanced
    classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0e8c3b3e88b3e3b94211cb37d076a582.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: Precision-recall curve on imbalanced dataset'
  prefs: []
  type: TYPE_NORMAL
- en: A more robust approach would be to utilise the precision-recall curve. This
    provides a much more robust estimate of our model’s performance. Here we can see
    the area under the precision-recall curve (AUC-PR) is much more conservative at
    0.71.
  prefs: []
  type: TYPE_NORMAL
- en: Taking a balanced version of the dataset where fraudulent and non-fraudulent
    transactions are 50:50, we can see that the AUC and AUC-PR are much closer together.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8565793fd74edcd5d7ad21e9af3f0bef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: ROC Curve on balanced dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/751da59649090f65aae00f49d1dc1b37.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image by Author: Precision-recall curve on balanced dataset'
  prefs: []
  type: TYPE_NORMAL
- en: The notebook for generating these charts is available in my [GitHub repo](https://github.com/john-adeojo/Precision_Recall_Curves).
  prefs: []
  type: TYPE_NORMAL
- en: There are ways to uplift the performance of classification models on imbalanced
    datasets, I explore these in my article on synthetic data.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/can-synthetic-data-boost-machine-learning-performance-6b4041e75dda?source=post_page-----e26307385fe1--------------------------------)
    [## Can Synthetic Data Boost Machine Learning Performance?'
  prefs: []
  type: TYPE_NORMAL
- en: Investigating the Capability of Synthetic Data to Enhance Model Performance
    on Imbalanced Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/can-synthetic-data-boost-machine-learning-performance-6b4041e75dda?source=post_page-----e26307385fe1--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding classification model metrics goes beyond the mathematical formulae.
    You should also understand how each metric should be used, and their implications
    for both balanced and imbalanced datasets. As a rule of thumb, metrics that are
    calculated based on true negatives or false negatives may be over optimistic when
    they are applied to imbalanced datasets. I hope this visual tour has given you
    more of an intuition.
  prefs: []
  type: TYPE_NORMAL
- en: I found this visual explanation handy for articulating the approach to my non-technical
    stakeholders. Feel free to share or borrow the approach.
  prefs: []
  type: TYPE_NORMAL
- en: '*Follow me on* [*LinkedIn*](https://www.linkedin.com/in/john-adeojo/)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Subscribe to medium to get more insights from me:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://johnadeojo.medium.com/membership?source=post_page-----e26307385fe1--------------------------------)
    [## Join Medium with my referral link — John Adeojo'
  prefs: []
  type: TYPE_NORMAL
- en: I share data science projects, experiences, and expertise to assist you on your
    journey You can sign up to medium via…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: johnadeojo.medium.com](https://johnadeojo.medium.com/membership?source=post_page-----e26307385fe1--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*Should you be interested in integrating AI or data science into your business
    operations, we invite you to schedule a complimentary initial consultation with
    us:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://www.data-centric-solutions.com/book-online?source=post_page-----e26307385fe1--------------------------------)
    [## Book Online | Data-Centric Solutions'
  prefs: []
  type: TYPE_NORMAL
- en: Discover our expertise in helping businesses achieve ambitious goals with a
    free consultation. Our data scientists and…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: www.data-centric-solutions.com](https://www.data-centric-solutions.com/book-online?source=post_page-----e26307385fe1--------------------------------)
  prefs: []
  type: TYPE_NORMAL
