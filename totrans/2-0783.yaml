- en: 'Edge Emotion Recognition: Enhancing Human-Machine Interaction through Real-Time
    Speech Analysis'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/edge-emotion-recognition-enhancing-human-machine-interaction-through-real-time-speech-analysis-235ee97cc5f6](https://towardsdatascience.com/edge-emotion-recognition-enhancing-human-machine-interaction-through-real-time-speech-analysis-235ee97cc5f6)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://medium.com/@ruetsi?source=post_page-----235ee97cc5f6--------------------------------)[![Rüdiger
    Buchkremer, PhD](../Images/c116ab76808a3b5c892e2917f1a679d7.png)](https://medium.com/@ruetsi?source=post_page-----235ee97cc5f6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----235ee97cc5f6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----235ee97cc5f6--------------------------------)
    [Rüdiger Buchkremer, PhD](https://medium.com/@ruetsi?source=post_page-----235ee97cc5f6--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----235ee97cc5f6--------------------------------)
    ·13 min read·Jul 23, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: In the modern world, our conversations with computers have grown exponentially.
    But alas, these technological marvels are oblivious to our emotions, which can
    be inconvenient. In this article, I am trying to unveil the intriguing approaches
    to detecting emotions through advanced technical means. And not just that, I will
    also regale you with the tale of a groundbreaking procedure developed at our innovative
    university research institute that can operate without a network connection. So,
    buckle up and prepare to be enthralled by the wonders of emotion recognition technology!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/79d8f4540c56db0ace654d9785a3fcfe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Author, image generated with artificial intelligence (leonardo.ai)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Background Story**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: People express their feelings through more than just the words they say. The
    tone of their voice, the speed they talk, and even the silences in between can
    give clues to **happiness, sadness, anger, fear, disgust, and surprise.**
  prefs: []
  type: TYPE_NORMAL
- en: '*But standard computers have no idea what any of that means. They just process
    the basic sounds of speech.*'
  prefs: []
  type: TYPE_NORMAL
- en: Lately, I have increasingly had to communicate with computers with either a
    human intermediary providing guidance or responding directly to my inquiries.
    **It bothered me** that these computers seemed utterly unaware of the emotional
    impact this interaction had on me, as they consistently replied in a detached
    and objective manner, which only intensified my frustrations.
  prefs: []
  type: TYPE_NORMAL
- en: To address this issue, researchers at our institute embarked on a collaborative
    study, the results of which were recently published by the authors Dominik and
    myself, in a scientific article, which is quite lengthy and technical in nature.
    However, I am delighted to inform you that the link to our original 24-page scientific
    paper, recently published in the Journal of Computer Science Research, can be
    found at the end of the current article.
  prefs: []
  type: TYPE_NORMAL
- en: '**Current Technologies Technical Background**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the integration of machines into our daily lives continues to progress, a
    growing demand arises for these machines to possess the ability to understand
    human emotions. When we engage with computers, robotics, and AI assistants, it
    is innate for us to express our emotions through various means, such as changes
    in our tone of voice, facial expressions, and gestures, to name a few. However,
    it is worth noting that **most current technologies lack a comprehensive understanding
    of these emotional signals.**
  prefs: []
  type: TYPE_NORMAL
- en: Researchers have developed systems that can effectively recognize emotions from
    a person’s voice to address this issue. Similar to how humans derive meaning from
    variations in speech patterns, these machines are acquiring the capability to
    interpret elements such as pauses, pitch, volume, tempo, and other subtle nuances,
    intending to identify emotions such as joy, sadness, anger, and more.
  prefs: []
  type: TYPE_NORMAL
- en: '**One particular approach involves training algorithms utilizing machine learning
    techniques** on a large dataset of emotional speech samples. By uncovering **acoustic
    patterns** associated with various emotional states, these systems can categorize
    basic emotions with an approximately 70% accuracy rate.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Other researchers convert speech into visual representations** known as spectrograms,
    colorful images representing soundwave patterns.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Research milestones**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the early 2000s, cloud computing emerged as a revolutionary milestone, transforming
    business models and sparking innovations worldwide. **However, the reign of cloud
    computing is now facing its twilight**, as a new paradigm known as edge computing
    is stealing the spotlight, driven by evolving demands and requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Edge computing brings with it the power to meet the needs for low latency, enhanced
    data security, seamless mobility support, and real-time processing, making it
    a formidable competitor to its cloud counterpart.
  prefs: []
  type: TYPE_NORMAL
- en: 'Three sub-areas dominate the stage within edge computing: fog computing, cloudlet,
    and mobile edge computing (MEC). While fog computing and cloudlet are still playing
    hard to get in real-world applications, MEC has become the show’s superstar.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Picture this:** MEC stations right at or inside near-end devices, ensuring
    everyday use of this cutting-edge technology. MEC means data processing happens
    instantly, right on the end device itself.'
  prefs: []
  type: TYPE_NORMAL
- en: We also have mobile cloud computing (MCC), where end devices perform the processing
    and only send the results back to MEC or MCC servers. **Combining cloud and edge
    computing techniques offers a dazzling array of possibilities**, catering to various
    use cases and taking full advantage of their unique strengths.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s shift gears to another riveting topic: speech emotion recognition
    (SER) and the captivating world of feature extraction and pattern recognition.
    Contemporary research is ablaze with discussions on SER, where continuous and
    spectral speech features take center stage, capturing the essence of emotions
    with astonishing accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: The journey of emotion recognition relies on the portrayal of primary speech
    frequency, loudness, temporal ratios, pauses, and spectral features like the mel
    frequency cepstral coefficient (MFCC) and the so-called mel spectrograms.
  prefs: []
  type: TYPE_NORMAL
- en: '**Mel spectrograms**'
  prefs: []
  type: TYPE_NORMAL
- en: A captivating star has emerged in the enchanting realm of audio and speech processing
    — the mel spectrogram (mel stands for melody). This mesmerizing visualization
    tool has taken center stage, captivating researchers and enthusiasts alike. Its
    brilliance lies in its ability to depict the frequency content of a sound signal
    over time in a truly unique manner.
  prefs: []
  type: TYPE_NORMAL
- en: By harnessing the mel scale, which mirrors our perception of pitch, the mel
    spectrogram captures the essence of different frequency bands that hold immense
    significance in speech and audio analysis. This prodigious approach offers a rich
    tapestry of insights into the acoustic characteristics of the signal, rendering
    it an indispensable companion in a myriad of applications, including speech recognition
    and music processing.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, the mel spectrogram serves as a benevolent guide, unraveling the
    mysteries of sound, illuminating the delicate dance between frequencies and time,
    and nourishing our understanding of the captivating world of audio.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fdacca4b3da99b8121a03c33fa23584a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Author; upscaled image from Estevez de Andrade & Buchkremer, Journal
    of Computer Science Research (2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Machine-learning Techniques**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the quest for classification excellence, various techniques have graced the
    stage, from the classic Gaussian Mixture Model (GMM) and Hidden Markov Model (HMM)
    combo to the enchanting support vector machine (SVM), and the fascinating world
    of neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: '*The enchantment doesn’t stop there; we are utterly captivated by the mesmerizing
    potential of recurrent neural networks (RNNs) like Long Short-Term Memory (LSTM).
    But wait, the spotlight now shines on convolutional neural networks (CNNs) such
    as AlexNet, VGG16, ResNet, and MobileNetV2, who have taken the lead with their
    remarkable resource and memory efficiency. It’s like witnessing a grand transformation
    — MFCC and Mel spectrograms uniting with CNNs, and the mystical art of transfer
    learning and Multitask Learning amplifying the charm.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**And imagine this: the wondrous prospect of running all of this on petite
    computers, wholly liberated from the clutches of big providers.**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Not only does this offer us the precious gift of heightened data privacy, but
    it also grants us a newfound sense of independence.
  prefs: []
  type: TYPE_NORMAL
- en: With this remarkable combination, we can flourish, breaking free from the chains
    that bind us and embracing a world where our autonomy knows no bounds. So let
    us revel in this empowering possibility, where privacy and self-reliance intertwine,
    and seize the opportunity to chart our digital destiny.
  prefs: []
  type: TYPE_NORMAL
- en: '**Extract the Right Data with Parameter Sets**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Every excellent recognition performance owes its magic to skillfully extracted
    features. This art involves careful selection from a diverse collection. The favored
    machine learning magician is the captivating open-source framework called Speech
    and Music Interpretation by Large-space Extraction (openSMILE). This remarkable
    framework houses the extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS)
    and ComParE datasets, pivotal in the grand spectacle. In deep learning, the spotlight
    shifts to CNNs, taking on the role of feature extraction with grace, either serving
    as classifiers or handing over the baton to an SVM, mesmerizing the audience with
    their versatility.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this thrilling act of emotion classification, diverse sets of emotions unfurl,
    each harboring a unique number of emotions. The audience is immersed in emotions
    ranging from five to a staggering twenty. Amongst the many emotions, the classics
    from Ekman’s collection shine bright: **happiness, sadness, anger, fear, disgust,
    and surprise**, accompanied by the enigmatic seventh emotion, neutral.'
  prefs: []
  type: TYPE_NORMAL
- en: '**With edge computing stealing the spotlight and neural networks unleashing
    their magic, the future of emotion recognition holds untold wonders.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Our Approach**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We ventured into emotion recognition in speech, utilizing labeled emotional
    speech data for our prototypical implementation. To ensure a robust dataset, we
    sought audio files ranging from one to twenty seconds in length. Our focus primarily
    revolved around those, as mentioned earlier, six basic emotions commonly referenced
    in emotion databases. However, we did not consider arousal and valence dimensions
    in our work, thus neglecting these criteria during data acquisition.
  prefs: []
  type: TYPE_NORMAL
- en: '**In human speech, emotions often emerge within individual sentences.** Hence,
    the chosen audio length of one to twenty seconds aligns subjectively well, encapsulating
    the majority of spoken sentences.'
  prefs: []
  type: TYPE_NORMAL
- en: '*The selected audio files needed to exclude singing, noise, or similar disturbances
    to maintain clarity and relevance. While the native language of the speaker was
    not a selection criterion, we ensured a balanced representation of both male and
    female spoken sentences across the entirety of the acquired databases. Factors
    such as channel number or sampling rate also held no significance during the data
    acquisition phase, as these parameters are standardized during training.*'
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, for accessibility and clarity purposes, the audio files and databases
    needed to be freely available and identified by appropriate labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'With these quality criteria in mind, we selected the following audio databases
    that met our standards:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1)** Ryerson Audiovisual Database of Emotional Speech and Song (RAVDESS)'
  prefs: []
  type: TYPE_NORMAL
- en: '**2)** Berlin Database of Emotional Speech (Emo-DB)'
  prefs: []
  type: TYPE_NORMAL
- en: '**3)** Toronto Emotional Speech Set (TESS)'
  prefs: []
  type: TYPE_NORMAL
- en: '**4)** EMOVO'
  prefs: []
  type: TYPE_NORMAL
- en: '**5)** eNTERFACE’05'
  prefs: []
  type: TYPE_NORMAL
- en: In our study, machine learning and deep learning techniques unite, diving into
    the mysterious world of emotion recognition. The quest begins with a shared data
    corpus, handpicked based on predefined criteria meticulously outlined in the existing
    literature. Non-spoken sentences are left out of the equation, as the prototypes
    exclusively focus on speech-emotion recognition (SER). Only pure speech files
    are allowed, even if some musical pieces contain spoken segments accompanied by
    instruments.
  prefs: []
  type: TYPE_NORMAL
- en: And speaking of background noise, it can’t be ignored in this melodious journey.
    Real-life communication often happens amidst noisy environments, so audio data
    with background noise is essential for enriching the research. But don’t confuse
    it with music’s background noise, which plays a different role in the grand symphony
    of speech-related scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: '**The native language used in the audio files isn’t a restriction either.**
    German, English, Italian, Turkish, Danish, or Chinese — all languages are welcome
    on this captivating stage. Why? Because the six basic emotions described by Darwin
    and Ekman are expressed similarly across cultures, transcending linguistic barriers.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Open access to labeled data is another key to our enigmatic adventure.**'
  prefs: []
  type: TYPE_NORMAL
- en: Without it, the whole journey would be shrouded in mystery, making it impossible
    for others to reproduce the results. Supervised machine-learning algorithms thrive
    on labeled data, after all.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s talk about the stars of the show — the hyperparameters!
  prefs: []
  type: TYPE_NORMAL
- en: '*Hyperparameters are crucial elements in deep learning and machine learning,
    acting as knobs that control a model’s learning process and performance. They
    are set before training and influence the model’s architecture and complexity.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*In machine learning, common hyperparameters include the learning rate, which
    determines how much the model adjusts its parameters during training, and the
    number of hidden layers, which affects the model’s depth and capacity to learn
    complex patterns.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*In deep learning, hyperparameters become even more vital due to the complexity
    of deep neural networks. Specific hyperparameters include the dropout rate, activation
    functions, optimization algorithm, and weight initialization, all playing crucial
    roles in the model’s performance.*'
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning boasts explicit hyperparameters, while machine learning seeks
    to find the optimal values based on predefined criteria. The battle between these
    two approaches unfolds, each vying for the spotlight.
  prefs: []
  type: TYPE_NORMAL
- en: As the journey progresses, we encounter the base models — MobileNetV2, CNN ResNet50,
    and SqueezeNet — all eager to showcase their unique strengths. But remember, the
    path to greatness isn’t without its challenges. Overfitting and underfitting add
    a touch of drama to the story, keeping us on the edge of our seats.
  prefs: []
  type: TYPE_NORMAL
- en: And the plot thickens! The prototypes developed in this study are tailor-made
    for devices equipped with microphones, making them perfect companions for smart
    speakers and TVs. They’re all set to embark on a grand adventure, bringing emotional
    recognition to everyday life.
  prefs: []
  type: TYPE_NORMAL
- en: With real-time capability on the line, the speed advantage of the machine learning
    approach becomes a crucial factor. The race is on as the clock ticks away milliseconds
    of difference.
  prefs: []
  type: TYPE_NORMAL
- en: But wait, there’s more! The study opens the door to endless possibilities, paving
    the way for future investigations into real-time SER and edge computing. Who knows
    what other mysteries lie waiting to be unraveled in emotion recognition?
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusion**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our groundbreaking study, cutting-edge speech emotion recognition (SER) systems
    take center stage, revealing their potential for many practical applications.
  prefs: []
  type: TYPE_NORMAL
- en: '(i) Universal application: SER applications prove their versatility, finding
    a home in call centers, radio broadcasts, podcasts, and television shows. But
    that’s not all! Imagine an intelligent speaker that detects vocal activity and
    emotions in your home, offering personalized products and services based on your
    feelings. Or how about automated highlights in a sports game, tailor-made to match
    the emotions of the moment? The possibilities are endless, reaching even internet-based
    broadcasts like Twitch or Netflix.'
  prefs: []
  type: TYPE_NORMAL
- en: '(ii) Real-time audience mood capture: Get ready for real-time mood tracking!
    Imagine having a tool to gauge an audience’s emotions at any moment. Political
    talks, product presentations — no setting is too grand for this cutting-edge technology.
    Speakers can now receive instant feedback on the emotions they evoke, revolutionizing
    the art of communication in physical, virtual, or hybrid realms.'
  prefs: []
  type: TYPE_NORMAL
- en: '(iii) Individual-focused applications: Emotion recognition goes personal, catering
    to individual users and their emotional needs. Picture a smart speaker or car
    that adjusts music or lighting according to your feelings. In gaming, the algorithm
    can offer relief when it detects anger. And brace yourself for personalized advertising
    in social media or e-commerce platforms, where prices dynamically change based
    on your emotional state. It’s like having your very own emotional concierge!'
  prefs: []
  type: TYPE_NORMAL
- en: But how did we get here? The study takes us through a systematic literature
    review, developing two prototypes using machine learning and deep learning and
    rigorous model training using a vast data corpus comprising five audio databases.
  prefs: []
  type: TYPE_NORMAL
- en: In the machine learning approach, the openSMILE framework works its magic, extracting
    features that are then normalized and used for classification. The Support Vector
    Machine (SVM) is the master classifier, identifying different sounds and seven
    distinct emotions in speech files. The prototype delivers results in under 1000
    milliseconds, captivating us with its speed and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: But wait, there’s more! The deep learning model introduces Mel spectrograms,
    unlocking a new dimension of emotion recognition. With TensorFlow as its trusty
    companion, the Convolutional Neural Network (CNN) steps into the spotlight, mastering
    feature extraction and classification. The notebook and Raspberry Pi join the
    party, showcasing the model’s portability and efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: As the study unfolds, we witness the exciting potential of SER systems in enhancing
    human-computer interaction. Imagine a world where our devices understand our emotions,
    offering more human-like and intuitive responses. It’s a glimpse into the future
    of communication!
  prefs: []
  type: TYPE_NORMAL
- en: But the story doesn’t end here. The study leaves us hungry for more, hinting
    at future research avenues. Emotions beyond the primary six, exploring arousal
    and valence dimensions, investigating machine actions based on recognized emotions
    — the possibilities are vast. And what about different hyperparameters for model
    training and novel transfer learning techniques? The quest for deeper understanding
    and improved performance has just begun.
  prefs: []
  type: TYPE_NORMAL
- en: '**Ethical Concerns**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One major ethical twist in this ride is informed consent and privacy. **Should
    our emotions be fair game for scrutiny without us even knowing?** It’s like a
    peek into our emotional diaries without our say-so. Transparency and getting our
    permission to analyze our emotions are the key checkpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s talk about the adrenaline-pumping prospect of manipulation and exploitation.
    With great power comes great responsibility, and the real-time audience mood capture
    isn’t immune to abuse. Imagine politicians or advertisers exploiting your emotional
    state to their advantage. It’s like having puppeteers pulling emotional strings
    behind the curtain. We need safeguards and regulations in place to keep this tech
    in check.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms can be sneaky devils, picking up on existing biases in our world.
    If those biases creep into the tech, **we look at an ethical minefield**. We must
    ensure fairness for all, regardless of race, gender, or background.
  prefs: []
  type: TYPE_NORMAL
- en: And what about emotional well-being? Continuous monitoring without our knowledge
    can mess with our minds. Feeling like Big Brother is watching your every emotion
    isn’t exactly a comforting thought. We need to safeguard our mental and emotional
    health on this ride.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, let’s not forget accuracy and reliability — critical checkpoints
    on our journey.
  prefs: []
  type: TYPE_NORMAL
- en: Emotion recognition ain’t perfect, and relying on it for life-altering decisions
    is like trusting a rollercoaster with a missing bolt. We need assurance that the
    tech won’t leave us hanging upside down with false readings.
  prefs: []
  type: TYPE_NORMAL
- en: Manipulating emotions for personal gain sounds like a sci-fi dystopia, not our
    ideal theme park. Our choices and decisions should be ours, not puppeteered by
    sneaky emotion-tracking freaks.
  prefs: []
  type: TYPE_NORMAL
- en: Cultural sensitivity is a must! Emotions vary across cultures, like flavors
    in a global buffet. We can’t impose a one-size-fits-all emotional norm; that’d
    be like putting peanut butter on everything.
  prefs: []
  type: TYPE_NORMAL
- en: And while we’re at it, **let’s talk algorithmic transparency**. It’s like being
    stuck on a ride without knowing how it works. We need clear explanations of how
    this tech reaches its conclusions, so we don’t get stuck in an ethical loop-de-loop.
  prefs: []
  type: TYPE_NORMAL
- en: We need to know who holds our emotional data and what they do with it. It’s
    like handing over the keys to our emotional kingdom; we better understand who’s
    driving.
  prefs: []
  type: TYPE_NORMAL
- en: With the proper precautions, we can ensure this tech delivers the wonders it
    promises without leaving us with a stomach-churning ethical hangover.
  prefs: []
  type: TYPE_NORMAL
- en: '**A Personal Note on the Topic**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We take great pride in our achievement of successfully implementing emotion
    recognition techniques on a small computer like the Raspberry Pi.
  prefs: []
  type: TYPE_NORMAL
- en: However, it is essential to acknowledge the potential downsides that come with
    it. While I would be delighted if a computer I interact with could better perceive
    my emotions, I also harbor concerns about accidentally detecting my emotions when
    I do not desire such disclosure. Consequently, we must consider the ethical implications
    that lurk in the background of this entire study.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing our research project’s ethical aspects becomes crucial in light of
    these considerations. Through our endeavors, I sincerely hope to illuminate an
    inspiring topic and spark meaningful conversations. Your feedback is awaited as
    we navigate this fascinating exploration together.
  prefs: []
  type: TYPE_NORMAL
- en: '**Scientific Article for Further Reading**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Andrade, D.E. De; Buchkremer, R. Enhancing Human-Machine Interaction: Real-Time
    Emotion Recognition through Speech Analysis. J. Comput. Sci. Res.* ***2023****,
    5, 22–45, doi:10.30564/jcsr.v5i3.5768.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**If you have found this interesting:**'
  prefs: []
  type: TYPE_NORMAL
- en: '*You can look for my other articles, and you can also connect or reach me on*[***LinkedIn***](https://www.linkedin.com/in/buchkremer/)***.***'
  prefs: []
  type: TYPE_NORMAL
