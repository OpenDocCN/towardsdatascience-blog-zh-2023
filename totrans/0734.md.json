["```py\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.document_loaders import PyMuPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import FAISS\nfrom langchain.chains.summarize import load_summarize_chain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.utilities import ArxivAPIWrapper\nimport os\n```", "```py\nclass Embedder:\n    \"\"\"Embedding engine to create doc embeddings.\"\"\"\n\n    def __init__(self, engine='OpenAI'):\n        \"\"\"Specify embedding model.\n\n        Args:\n        --------------\n        engine: the embedding model. \n                For a complete list of supported embedding models in LangChain, \n                see https://python.langchain.com/docs/integrations/text_embedding/\n        \"\"\"\n        if engine == 'OpenAI':\n            # Reminder: need to set up openAI API key \n            # (e.g., via environment variable OPENAI_API_KEY)\n            self.embeddings = OpenAIEmbeddings()\n\n        else:\n            raise KeyError(\"Currently unsupported chat model type!\")\n```", "```py\ndef load_n_process_document(self, path):\n    \"\"\"Load and process PDF document.\n\n    Args:\n    --------------\n    path: path of the paper.\n    \"\"\"\n\n    # Load PDF\n    loader = PyMuPDFLoader(path)\n    documents = loader.load()\n\n    # Process PDF\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n    self.documents = text_splitter.split_documents(documents)\n```", "```py\ndef create_vectorstore(self, store_path):\n    \"\"\"Create vector store for doc Q&A.\n       For a complete list of vector stores supported by LangChain,\n       see: https://python.langchain.com/docs/integrations/vectorstores/\n\n    Args:\n    --------------\n    store_path: path of the vector store.\n\n    Outputs:\n    --------------\n    vectorstore: the created vector store for holding embeddings\n    \"\"\"\n    if not os.path.exists(store_path):\n        print(\"Embeddings not found! Creating new ones\")\n        self.vectorstore = FAISS.from_documents(self.documents, self.embeddings)\n        self.vectorstore.save_local(store_path)\n\n    else:\n        print(\"Embeddings found! Loaded the computed ones\")\n        self.vectorstore = FAISS.load_local(store_path, self.embeddings)\n\n    return self.vectorstore\n```", "```py\ndef create_summary(self, llm_engine=None):\n    \"\"\"Create paper summary. \n    The summary is created by using LangChain's summarize_chain.\n\n    Args:\n    --------------\n    llm_engine: backbone large language model.\n\n    Outputs:\n    --------------\n    summary: the summary of the paper\n    \"\"\"\n\n    if llm_engine is None:\n        raise KeyError(\"please specify a LLM engine to perform summarization.\")\n\n    elif llm_engine == 'OpenAI':\n        # Reminder: need to set up openAI API key \n        # (e.g., via environment variable OPENAI_API_KEY)\n        llm = ChatOpenAI(\n            model_name=\"gpt-3.5-turbo\",\n            temperature=0.8\n        )\n\n    else:\n        raise KeyError(\"Currently unsupported chat model type!\")\n\n    # Use LLM to summarize the paper\n    chain = load_summarize_chain(llm, chain_type=\"stuff\")\n    summary = chain.run(self.documents[:20])\n\n    return summary\n```", "```py\nfrom abc import ABC, abstractmethod\nfrom langchain.chat_models import ChatOpenAI\n\nclass Chatbot(ABC):\n    \"\"\"Class definition for a single chatbot with memory, created with LangChain.\"\"\"\n\n    def __init__(self, engine):\n        \"\"\"Initialize the large language model and its associated memory.\n        The memory can be an LangChain emory object, or a list of chat history.\n\n        Args:\n        --------------\n        engine: the backbone llm-based chat model.\n        \"\"\"\n\n        # Instantiate llm\n        if engine == 'OpenAI':\n            # Reminder: need to set up openAI API key \n            # (e.g., via environment variable OPENAI_API_KEY)\n            self.llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.8)\n\n        else:\n            raise KeyError(\"Currently unsupported chat model type!\")\n\n    @abstractmethod\n    def instruct(self):\n        \"\"\"Determine the context of chatbot interaction. \n        \"\"\"\n        pass\n\n    @abstractmethod\n    def step(self):\n        \"\"\"Action produced by the chatbot. \n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _specify_system_message(self):\n        \"\"\"Prompt engineering for chatbot.\n        \"\"\"       \n        pass\n```", "```py\nfrom langchain.memory import ConversationBufferMemory\n\nclass JournalistBot(Chatbot):\n    \"\"\"Class definition for the journalist bot, created with LangChain.\"\"\"\n\n    def __init__(self, engine):\n        \"\"\"Setup journalist bot.\n\n        Args:\n        --------------\n        engine: the backbone llm-based chat model.\n        \"\"\"\n\n        # Instantiate llm\n        super().__init__(engine)\n\n        # Instantiate memory\n        self.memory = ConversationBufferMemory(return_messages=True)\n```", "```py\nfrom langchain.chains import ConversationChain\nfrom langchain.prompts import (\n    ChatPromptTemplate, \n    MessagesPlaceholder, \n    SystemMessagePromptTemplate, \n    HumanMessagePromptTemplate\n)\n\ndef instruct(self, topic, abstract):\n    \"\"\"Determine the context of journalist chatbot. \n\n    Args:\n    ------\n    topic: the topic of the paper\n    abstract: the abstract of the paper\n    \"\"\"\n\n    self.topic = topic\n    self.abstract = abstract\n\n    # Define prompt template\n    prompt = ChatPromptTemplate.from_messages([\n        SystemMessagePromptTemplate.from_template(self._specify_system_message()),\n        MessagesPlaceholder(variable_name=\"history\"),\n        HumanMessagePromptTemplate.from_template(\"\"\"{input}\"\"\")\n    ])\n\n    # Create conversation chain\n    self.conversation = ConversationChain(memory=self.memory, prompt=prompt, \n                                          llm=self.llm, verbose=False)\n```", "```py\ndef _specify_system_message(self):\n    \"\"\"Specify the behavior of the journalist chatbot.\n    The prompt is generated and optimized with GPT-4.\n\n    Outputs:\n    --------\n    prompt: instructions for the chatbot.\n    \"\"\"       \n\n    prompt = f\"\"\"You are a technical journalist interested in {self.topic}, \n    Your task is to distill a recently published scientific paper on this topic through\n    an interview with the author, which is played by another chatbot.\n    Your objective is to ask comprehensive and technical questions \n    so that anyone who reads the interview can understand the paper's main ideas and contributions, \n    even without reading the paper itself. \n    You're provided with the paper's summary to guide your initial questions.\n    You must keep the following guidelines in mind:\n    - Focus exclusive on the technical content of the paper.\n    - Avoid general questions about {self.topic}, focusing instead on specifics related to the paper.\n    - Only ask one question at a time.\n    - Feel free to ask about the study's purpose, methods, results, and significance, \n    and clarify any technical terms or complex concepts. \n    - Your goal is to lead the conversation towards a clear and engaging summary.\n    - Do not include any prefixed labels like \"Interviewer:\" or \"Question:\" in your question.\n\n    [Abstract]: {self.abstract}\"\"\"\n\n    return prompt\n```", "```py\ndef step(self, prompt):\n    \"\"\"Journalist chatbot asks question. \n\n    Args:\n    ------\n    prompt: Previos answer provided by the author bot.\n    \"\"\"\n    response = self.conversation.predict(input=prompt)\n\n    return response\n```", "```py\nclass AuthorBot(Chatbot):\n    \"\"\"Class definition for the author bot, created with LangChain.\"\"\"\n\n    def __init__(self, engine, vectorstore, debug=False):\n        \"\"\"Select backbone large language model, as well as instantiate \n        the memory for creating language chain in LangChain.\n\n        Args:\n        --------------\n        engine: the backbone llm-based chat model.\n        vectorstore: embedding vectors of the paper.\n        \"\"\"\n\n        # Instantiate llm\n        super().__init__(engine)\n\n        # Instantiate memory\n        self.chat_history = []\n\n        # Instantiate embedding index\n        self.vectorstore = vectorstore\n\n        self.debug = debug\n```", "```py\nfrom langchain.chains import ConversationalRetrievalChain\n\ndef instruct(self, topic):\n    \"\"\"Determine the context of author chatbot. \n\n    Args:\n    -------\n    topic: the topic of the paper.\n    \"\"\"\n\n    # Specify topic\n    self.topic = topic\n\n    # Define prompt template\n    qa_prompt = ChatPromptTemplate.from_messages([\n        SystemMessagePromptTemplate.from_template(self._specify_system_message()),\n        HumanMessagePromptTemplate.from_template(\"{question}\")\n    ])\n\n    # Create conversation chain\n    self.conversation_qa = ConversationalRetrievalChain.from_llm(llm=self.llm, verbose=self.debug,\n                                                                 retriever=self.vectorstore.as_retriever(\n                                                                     search_kwargs={\"k\": 5}),\n                                                                 return_source_documents=True,\n                                                                combine_docs_chain_kwargs={'prompt': qa_prompt})\n```", "```py\ndef step(self, prompt):\n    \"\"\"Author chatbot answers question. \n\n    Args:\n    ------\n    prompt: question raised by journalist bot.\n\n    Outputs:\n    ------\n    answer: the author bot's answer\n    source_documents: documents that author bot used to answer questions\n    \"\"\"\n    response = self.conversation_qa({\"question\": prompt, \"chat_history\": self.chat_history})\n    self.chat_history.append((prompt, response[\"answer\"]))\n\n    return response[\"answer\"], response[\"source_documents\"]\n```", "```py\ndef _specify_system_message(self):\n    \"\"\"Specify the behavior of the author chatbot.\n    The prompt is generated and optimized by GPT-4.\n\n    Outputs:\n    --------\n    prompt: instructions for the chatbot.\n    \"\"\"       \n\n    prompt = f\"\"\"You are the author of a recently published scientific paper on {self.topic}.\n    You are being interviewed by a technical journalist who is played by another chatbot and\n    looking to write an article to summarize your paper.\n    Your task is to provide comprehensive, clear, and accurate answers to the journalist's questions.\n    Please keep the following guidelines in mind:\n    - Try to explain complex concepts and technical terms in an understandable way, without sacrificing accuracy.\n    - Your responses should primarily come from the relevant content of this paper, \n    which will be provided to you in the following, but you can also use your broad knowledge in {self.topic} to \n    provide context or clarify complex topics. \n    - Remember to differentiate when you are providing information directly from the paper versus \n    when you're giving additional context or interpretation. Use phrases like 'According to the paper...' for direct information, \n    and 'Based on general knowledge in the field...' when you're providing additional context.\n    - Only answer one question at a time. Ensure that each answer is complete before moving on to the next question.\n    - Do not include any prefixed labels like \"Author:\", \"Interviewee:\", Respond:\", or \"Answer:\" in your answer.\n    \"\"\"\n\n    prompt += \"\"\"Given the following context, please answer the question.\n\n    {context}\"\"\"\n\n    return prompt\n```", "```py\npaper = 'Improved Training of Physics-Informed Neural Networks with Model Ensembles'\n\n# Create embeddings\nembedding = Embedder(engine='OpenAI')\nembedding.load_n_process_document(\"../Papers/\"+paper+\".pdf\")\n\n# Set up vectorstore\nvectorstore = embedding.create_vectorstore(store_path=paper)\n\n# Fetch paper summary\npaper_summary = embedding.create_summary(llm_engine='OpenAI')\n\n# Instantiate journalist and author bot\njournalist = JournalistBot('OpenAI')\nauthor = AuthorBot('OpenAI', vectorstore)\n\n# Provide instruction\njournalist.instruct(topic='physics-informed machine learning', abstract=paper_summary)\nauthor.instruct('physics-informed machine learning')\n\n# Start conversation\nfor i in range(4):\n    if i == 0:\n        question = journalist.step('Start the conversation')\n    else:\n        question = journalist.step(answer)\n    print(\"üë®‚Äçüè´ Journalist: \" + question)\n\n    answer, source = author.step(question)\n    print(\"üë©‚Äçüéì Author: \" + answer)\n```", "```py\nimport ipywidgets as widgets\nfrom IPython.display import display\n\n# Create button\nbot_ask = widgets.Button(description=\"Journalist Bot ask\")\n\n# Chat history\nchat_log = widgets.HTML(\n    value='',\n    placeholder='',\n    description='',\n)\n\n# Attach callbacks\nbot_ask.on_click(bot_ask_clicked)\n\n# Arrange widgets layout\nfirst_row = widgets.HBox([bot_ask])\n\n# Display the UI\ndisplay(chat_log, widgets.VBox([first_row]))\n```", "```py\ndef bot_ask_clicked(b):\n\n    if chat_log.value == '':\n        # Starting conversation \n        bot_question = journalist.step(\"Start the conversation\")\n        line_breaker = \"\"\n\n    else:\n        # Ongoing conversation\n        bot_question = journalist.step(chat_log.value.split(\"<br><br>\")[-1])\n        line_breaker = \"<br><br>\"\n\n    # Journalist question\n    chat_log.value += line_breaker + \"<b style='color:blue'>üë®‚Äçüè´ Journalist Bot:</b> \" + bot_question      \n\n    # Author bot answers\n    response, source = author.step(bot_question)  \n\n    # Author answer with source\n    page_numbers = [str(src.metadata['page']+1) for src in source]\n    unique_page_numbers = list(set(page_numbers))\n    chat_log.value += \"<br><b style='color:green'>üë©‚Äçüéì Author Bot:</b> \" + response + \"<br>\"\n    chat_log.value += \"(For details, please check the highlighted text on page(s): \" + ', '.join(unique_page_numbers) + \")\"\n```", "```py\nimport fitz\n\ndef highlight_PDF(file_path, phrases, output_path):\n    \"\"\"Search and highlight given texts in PDF.\n\n    Args:\n    --------\n    file_path: PDF file path\n    phrases: a list of texts (in string)\n    output_path: save and output PDF\n    \"\"\"\n\n    # Open PDF\n    doc = fitz.open(file_path)\n\n    # Search the doc\n    for page in doc:\n        for phrase in phrases:            \n            text_instances = page.search_for(phrase)\n\n            # Highlight texts\n            for inst in text_instances:\n                highlight = page.add_highlight_annot(inst)\n\n    # Output PDF\n    doc.save(output_path, garbage=4)\n```", "```py\ndef create_bot_ask_callback(title):\n\n    def bot_ask_clicked(b):\n\n        if chat_log.value == '':\n            # Starting conversation \n            bot_question = journalist.step(\"Start the conversation\")\n            line_breaker = \"\"\n\n        else:\n            # Ongoing conversation\n            bot_question = journalist.step(chat_log.value.split(\"<br><br>\")[-1])\n            line_breaker = \"<br><br>\"\n\n        chat_log.value += line_breaker + \"<b style='color:blue'>üë®‚Äçüè´ Journalist Bot:</b> \" + bot_question      \n\n        # Author bot answers\n        response, source = author.step(bot_question)  \n\n        ##### NEW: Highlight relevant text in PDF\n        phrases = [src.page_content for src in source]\n        paper_path = \"../Papers/\"+title+\".pdf\"\n        highlight_PDF(paper_path, phrases, 'highlighted.pdf')\n        ##### NEW\n\n        page_numbers = [str(src.metadata['page']+1) for src in source]\n        unique_page_numbers = list(set(page_numbers))\n        chat_log.value += \"<br><b style='color:green'>üë©‚Äçüéì Author Bot:</b> \" + response + \"<br>\"\n        chat_log.value += \"(For details, please check the highlighted text on page(s): \" + ', '.join(unique_page_numbers) + \")\"\n\n    return bot_ask_clicked\n```", "```py\n# Create \"user ask\" button\nuser_ask = widgets.Button(description=\"User ask\")\n\n# Define callback\ndef create_user_ask_callback(title):\n\n    def user_ask_clicked(b):\n\n        chat_log.value += \"<br><br><b style='color:purple'>üôã‚Äç‚ôÇÔ∏èYou:</b> \" + user_input.value\n\n        # Author bot answers\n        response, source = author.step(user_input.value)\n\n        # Highlight relevant text in PDF\n        phrases = [src.page_content for src in source]\n        paper_path = \"../Papers/\"+title+\".pdf\"\n        highlight_PDF(paper_path, phrases, 'highlighted.pdf')\n\n        page_numbers = [str(src.metadata['page']+1) for src in source]\n        unique_page_numbers = list(set(page_numbers))\n        chat_log.value += \"<br><b style='color:green'>üë©‚Äçüéì Author Bot:</b> \" + response + \"<br>\"\n        chat_log.value += \"(For details, please check the highlighted text on page(s): \" + ', '.join(unique_page_numbers) + \")\"\n\n        # Inform journalist bot about the asked questions \n        journalist.memory.chat_memory.add_user_message(user_input.value)\n\n        # Clear user input\n        user_input.value = \"\"\n\n    return user_ask_clicked\n```", "```py\n# Chat history\nchat_log = widgets.HTML(\n    value='',\n    placeholder='',\n    description='',\n)\n\n# User input question\nuser_input = widgets.Text(\n    value='',\n    placeholder='Question',\n    description='',\n    disabled=False,\n    layout=widgets.Layout(width=\"60%\")\n)\n\n# Attach callbacks\nbot_ask.on_click(create_bot_ask_callback(paper))\nuser_ask.on_click(create_user_ask_callback(paper))\n\n# Arrange the widgets\nfirst_row = widgets.HBox([bot_ask])\nsecond_row = widgets.HBox([user_ask, user_input])\n\n# Display the UI\ndisplay(chat_log, widgets.VBox([first_row, second_row]))\n```", "```py\nfrom pdfdocument.document import PDFDocument\n\ndownload = widgets.Button(description=\"Download paper summary\",\n                         layout=widgets.Layout(width='auto'))\n\ndef create_download_callback(title):\n\n    def download_clicked(b):\n        pdf = PDFDocument('paper_summary.pdf')\n        pdf.init_report()\n\n        # Remove HTML tags\n        chat_history = re.sub('<.*?>', '', chat_log.value)  \n\n        # Remove emojis\n        chat_history = chat_history.replace('üë®‚Äçüè´', '')\n        chat_history = chat_history.replace('üë©‚Äçüéì', '')\n        chat_history = chat_history.replace('üôã‚Äç‚ôÇÔ∏è', '')\n\n        # Add line breaks\n        chat_history = chat_history.replace('Journalist Bot:', '\\n\\n\\nJournalist: ')\n        chat_history = chat_history.replace('Author Bot:', '\\n\\nAuthor: ')\n        chat_history = chat_history.replace('You:', '\\n\\n\\nYou: ')\n\n        pdf.h2(\"Paper Summary: \" + title)\n        pdf.p(chat_history)\n        pdf.generate()\n\n        # Download PDF\n        print('PDF generated successfully in the local folder!')\n\n    return download_clicked\n```", "```py\n# Chat history\nchat_log = widgets.HTML(\n    value='',\n    placeholder='',\n    description='',\n)\n\n# User input question\nuser_input = widgets.Text(\n    value='',\n    placeholder='Question',\n    description='',\n    disabled=False,\n    layout=widgets.Layout(width=\"60%\")\n)\n\n# Attach callbacks\nbot_ask.on_click(create_bot_ask_callback(paper))\nuser_ask.on_click(create_user_ask_callback(paper))\ndownload.on_click(create_download_callback(paper))\n\n# Arrange the widgets\nfirst_row = widgets.HBox([bot_ask])\nsecond_row = widgets.HBox([user_ask, user_input])\nthird_row = widgets.HBox([download])\n\n# Display the UI\ndisplay(chat_log, widgets.VBox([first_row, second_row, third_row]))\n```"]