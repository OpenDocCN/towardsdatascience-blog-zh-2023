["```py\n!pip install peft\n!pip install transformers\n```", "```py\n from transformers import AutoModelForSeq2SeqLM\n  from peft import get_peft_model, LoraConfig, TaskType\n\n  model_name_or_path = \"bigscience/mt0-large\"\n  tokenizer_name_or_path = \"bigscience/mt0-large\"\n```", "```py\npeft_config = LoraConfig(\n    task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n)\n```", "```py\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()\n```", "```py\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.utils import to_categorical\nimport numpy as np\n\n# Load the MNIST dataset\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n\n# Preprocess the data\ntrain_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\ntest_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\ntrain_labels = to_categorical(train_labels)\ntest_labels = to_categorical(test_labels)\n\n# Define the teacher model (a larger model)\nteacher_model = models.Sequential([\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(10, activation='softmax')\n])\n\nteacher_model.compile(optimizer='adam',\n                      loss='categorical_crossentropy',\n                      metrics=['accuracy'])\n\n# Train the teacher model\nteacher_model.fit(train_images, train_labels, epochs=5, batch_size=64, validation_split=0.2)\n\n# Define the student model (a smaller model)\nstudent_model = models.Sequential([\n    layers.Flatten(input_shape=(28, 28, 1)),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(10, activation='softmax')\n])\n\nstudent_model.compile(optimizer='adam',\n                      loss='categorical_crossentropy',\n                      metrics=['accuracy'])\n\n# Knowledge distillation step: Transfer knowledge from the teacher to the student\ndef distillation_loss(y_true, y_pred):\n    alpha = 0.1  # Temperature parameter (adjust as needed)\n    return tf.keras.losses.KLDivergence()(tf.nn.softmax(y_true / alpha, axis=1),\n                                           tf.nn.softmax(y_pred / alpha, axis=1))\n\n# Train the student model using knowledge distillation\nstudent_model.fit(train_images, train_labels, epochs=10, batch_size=64,\n                  validation_split=0.2, loss=distillation_loss)\n\n# Evaluate the student model\ntest_loss, test_acc = student_model.evaluate(test_images, test_labels)\nprint(f'Test accuracy: {test_acc * 100:.2f}%')\n```", "```py\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow_model_optimization.sparsity import keras as sparsity\nimport numpy as np\n\n# Load the MNIST dataset\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n\n# Preprocess the data\ntrain_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\ntest_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\ntrain_labels = to_categorical(train_labels)\ntest_labels = to_categorical(test_labels)\n\n# Create a simple neural network model\ndef create_model():\n    model = Sequential([\n        tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n        tf.keras.layers.Dense(128, activation='relu'),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(64, activation='relu'),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(10, activation='softmax')\n    ])\n    return model\n\n# Create and compile the original model\nmodel = create_model()\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Train the original model\nmodel.fit(train_images, train_labels, epochs=5, batch_size=64, validation_split=0.2)\n\n# Prune the model\n# Specify the pruning parameters\npruning_params = {\n    'pruning_schedule': sparsity.PolynomialDecay(initial_sparsity=0.50,\n                                                 final_sparsity=0.90,\n                                                 begin_step=0,\n                                                 end_step=2000,\n                                                 frequency=100)\n}\n\n# Create a pruned model\npruned_model = sparsity.prune_low_magnitude(create_model(), **pruning_params)\n\n# Compile the pruned model\npruned_model.compile(optimizer='adam',\n                     loss='categorical_crossentropy',\n                     metrics=['accuracy'])\n\n# Train the pruned model (fine-tuning)\npruned_model.fit(train_images, train_labels, epochs=2, batch_size=64, validation_split=0.2)\n\n# Strip pruning wrappers to create a smaller and faster model\nfinal_model = sparsity.strip_pruning(pruned_model)\n\n# Evaluate the final pruned model\ntest_loss, test_acc = final_model.evaluate(test_images, test_labels)\nprint(f'Test accuracy after pruning: {test_acc * 100:.2f}%')\n```", "```py\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.utils import to_categorical\nimport numpy as np\n\n# Load the MNIST dataset\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n\n# Preprocess the data\ntrain_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\ntest_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\ntrain_labels = to_categorical(train_labels)\ntest_labels = to_categorical(test_labels)\n\n# Create a simple neural network model\ndef create_model():\n    model = Sequential([\n        Flatten(input_shape=(28, 28, 1)),\n        Dense(128, activation='relu'),\n        Dropout(0.2),\n        Dense(64, activation='relu'),\n        Dropout(0.2),\n        Dense(10, activation='softmax')\n    ])\n    return model\n\n# Create and compile the original model\nmodel = create_model()\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Train the original model\nmodel.fit(train_images, train_labels, epochs=5, batch_size=64, validation_split=0.2)\n\n# Quantize the model to 8-bit integers\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nquantized_model = converter.convert()\n\n# Save the quantized model to a file\nwith open('quantized_model.tflite', 'wb') as f:\n    f.write(quantized_model)\n\n# Load the quantized model for inference\ninterpreter = tf.lite.Interpreter(model_path='quantized_model.tflite')\ninterpreter.allocate_tensors()\n\n# Evaluate the quantized model\ntest_loss, test_acc = 0.0, 0.0\nfor i in range(len(test_images)):\n    input_data = np.array([test_images[i]], dtype=np.float32)\n    interpreter.set_tensor(interpreter.get_input_details()[0]['index'], input_data)\n    interpreter.invoke()\n    output_data = interpreter.get_tensor(interpreter.get_output_details()[0]['index'])\n    test_loss += tf.keras.losses.categorical_crossentropy(test_labels[i], output_data).numpy()\n    test_acc += np.argmax(test_labels[i]) == np.argmax(output_data)\n\ntest_loss /= len(test_images)\ntest_acc /= len(test_images)\n\nprint(f'Test accuracy after quantization: {test_acc * 100:.2f}%')\n```"]