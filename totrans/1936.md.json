["```py\nMNIST_classification\n├── dataset_scripts\n│   ├── construct_dataset_csv.py\n│   ├── construct_dataset_folders.py\n│   ├── describe_dataset_csv.py\n│   ├── explore_dataset_idx.py\n│   └── README.md\n├── main_classification_convnet.py\n├── main_classification_onehot.py\n├── main_classification_single_output.py\n├── .gitignore\n└── README.md\n```", "```py\n{{ cookiecutter.repo_name }}/\n├── LICENSE\n├── README.md\n├── Makefile        # Makefile with commands like `make data` or `make train`\n├── configs         # Config files (models and training hyperparameters)\n│   └── model1.yaml\n│\n├── data\n│   ├── external    # Data from third party sources.\n│   ├── interim     # Intermediate data that has been transformed.\n│   ├── processed   # The final, canonical data sets for modeling.\n│   └── raw         # The original, immutable data dump.\n│\n├── docs            # Project documentation.\n│\n├── models          # Trained and serialized models.\n│\n├── notebooks       # Jupyter notebooks.\n│\n├── references      # Data dictionaries, manuals, and all other explanatory\n│                   # materials.\n│\n├── reports         # Generated analysis as HTML, PDF, LaTeX, etc.\n│   └── figures     # Generated graphics and figures to be used in reporting.\n│\n├── requirements.txt # The requirements file for reproducing the environment.\n└── src              # Source code for use in this project.\n    ├── __init__.py  # Makes src a Python module.\n    │\n    ├── data         # Data engineering scripts.\n    │   ├── build_features.py\n    │   ├── cleaning.py\n    │   ├── ingestion.py\n    │   ├── labeling.py\n    │   ├── splitting.py\n    │   └── validation.py\n    │\n    ├── models       # ML model engineering (a folder for each model).\n    │   └── model1\n    │       ├── dataloader.py\n    │       ├── hyperparameters_tuning.py\n    │       ├── model.py\n    │       ├── predict.py\n    │       ├── preprocessing.py\n    │       └── train.py\n    │\n    └── visualization # Scripts to create exploratory and results\n        │             # oriented visualizations.\n        ├── evaluation.py\n        └── exploration.py\n```", "```py\npip install cookiecutter\n```", "```py\nconda config --add channels conda-forge\nconda install cookiecutter\n```", "```py\ncookiecutter https://github.com/Chim-SO/cookiecutter-mlops\n```", "```py\nproject_name [project_name]: MLOps_MLflow_mnist_classification\n\nrepo_name [mlops_mlflow_mnist_classification]: \n\nauthor_name [Your name (or your organization/company/team)]: Chim SO\n\ndescription [A short description of the project.]: MNIST classification\n\nSelect open_source_license:\n1 - MIT\n2 - BSD-3-Clause\n3 - No license file\nChoose from 1, 2, 3 [1]: 1\n\ns3_bucket [[OPTIONAL] your-bucket-for-syncing-data (do not include 's3://')]: \n\naws_profile [default]:\n\nSelect python_interpreter:\n1 - python3\n2 - python\nChoose from 1, 2 [1]:\n```", "```py\nMLOps_MLflow_mnist_classification\n├── configs\n│   ├── cnnbased.yaml\n│   └── singleoutput.yaml\n├── data\n│   ├── external\n│   │   └── test\n│   │       ├── 0_0.png\n│   │       ├── 1_0.png\n│   │       ├── 1_1.png\n│   │       ├── 3_1.png\n│   │       ├── 5_1.png\n│   │       ├── 7_0.png\n│   │       └── 8_0.png\n│   ├── interim\n│   ├── processed\n│   │   ├── test.csv\n│   │   └── train.csv\n│   └── raw\n│       ├── test_images.gz\n│       ├── test_labels.gz\n│       ├── train_images.gz\n│       └── train_labels.gz\n├── LICENSE\n├── Makefile\n├── MLproject\n├── mlruns\n├── models\n├── README.md\n├── requirements.txt\n└── src\n    ├── data\n    │   ├── build_features.py\n    │   ├── dataloader.py\n    │   └── ingestion.py\n    ├── models\n    │   ├── cnnbased\n    │   │   ├── hyperparameters_tuning.py\n    │   │   ├── model.py\n    │   │   ├── predict.py\n    │   │   ├── preprocessing.py\n    │   │   └── train.py\n    │   └── singleoutput\n    │       ├── hyperparameters_tuning.py\n    │       ├── model.py\n    │       ├── predict.py\n    │       ├── preprocessing.py\n    │       └── train.py\n    └── visualization\n        ├── evaluation.py\n        └── exploration.py\n```", "```py\n# Data parameters\ndata:\n  dataset_path : 'data/processed/'\n\n# Model parameters\nmodel:\n  name: 'singleoutput'\n  num_units: 224\n  num_layers: 5\n  activation_function : 'sigmoid'\n\n# Training parameters\ntraining:\n  batch_size: 128\n  num_epochs: 200\n  loss_function: 'mae'\n  metric: 'mse'\n\n# Logging and output parameters\nmlflow:\n  mlruns_path: 'file:models/mlruns'\n  experiment_name: 'singleOutput'\n\n# Tuning\nhyperparameter_tuning:\n  num_layers: [3, 5]\n  num_units: [16, 64, 224]\n  activation_function: ['relu', 'sigmoid']\n  batch_size: [128, 256]\n  loss_function: ['mae']\n  metric: ['mse']\n  num_epochs: [200]\n```", "```py\npython src/data/ingestion.py -r data/raw/ # Download data\npython src/data/build_features.py -r data/raw/ -p data/processed/ # Create csv files\npython -m src.models.cnnbased.train -c configs/cnnbased.yaml # Train CNN model\n```"]