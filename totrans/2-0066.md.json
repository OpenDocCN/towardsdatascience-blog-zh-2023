["```py\nTheory. 3D Deep Learning Essentials\nStep 1\\. Preparing the Environment\nStep 2\\. 3D Data Curation\nStep 3\\. 3D Data Analysis\nStep 4\\. 3D Data labelling\nStep 5\\. Feature Selection\nStep 6\\. Data Structuration\nStep 7\\. 3D Python I/O\nStep 8\\. 3D Data Normalization\nStep 9\\. 3D Interactive Vizualisation\nStep 10\\. Tensor Creation\n```", "```py\nnumber of point records:    32080350\noffset x y z:               205980 464980 0\nmin x y z:                  205980.000 464980.000 4.101\nmax x y z:                  207019.999 466269.999 53.016\nintensity          56       5029\nclassification      1         26\nColor R 17 255\n      G 39 255\n      B 31 255\n    NIR 0 255\n```", "```py\nclass 1 = vegetation + clutter\nclass 2 = ground\nclass 6 = buildings\nclass 9 = water\nclass 26 = bridge. \n```", "```py\nClass 1 = ground\nClass 2 = Vegetation\nClass 3 = Buildings\nClass 4 = Water\nClass 0 = unannotated (All the remaining points)\n```", "```py\nX                Y              Z          R  G   B  INTENSITY\n205980.49800000 465875.02398682 7.10500002 90 110 98 1175.000000\n205980.20100001 465875.09802246 7.13500023 87 107 95 1115.000000\n205982.29800010 465875.00000000 7.10799980 90 110 98 1112.000000\n```", "```py\nfrom google.colab import drive\ndrive.mount('/content/gdrive')\n```", "```py\n#Base libraries\nimport numpy as np\nimport random\nimport torch\n#Plotting libraries\n%matplotlib inline\nfrom mpl_toolkits import mplot3d\nimport matplotlib.pyplot as plt\nimport plotly\nimport plotly.graph_objects as go\n#Utilities libraries\nfrom glob import glob\nimport os\n```", "```py\n#specify data paths and extract filenames\nproject_dir=\"gdrive/My Drive/_UTWENTE/DATA/AHN4_33EZ2_12\"\npointcloud_train_files = glob(os.path.join(project_dir, \"train/*.txt\"))\npointcloud_test_files = glob(os.path.join(project_dir, \"test/*.txt\"))\n```", "```py\nprint(pointcloud_train_files[random.randrange(20)])\n>> gdrive/My Drive/_UTWENTE/DATA/AHN4_33EZ2_12/train/AHN4_33EZ2_12_train_000083.txt\n```", "```py\n#Prepare the data in a train set, a validation set (to tune the model parameters), and a test set (to evaluate the performances)\n#The validation is made of a random 20% of the train set.\nvalid_index = np.random.choice(len(pointcloud_train_files),int(len(pointcloud_train_files)/5), replace=False)\nvalid_list = [pointcloud_train_files[i] for i in valid_index]\ntrain_list = [pointcloud_train_files[i] for i in np.setdiff1d(list(range(len(pointcloud_train_files))),valid_index)]\ntest_list = pointcloud_test_files\nprint(\"%d tiles in train set, %d tiles in test set, %d files in valid list\" % (len(train_list), len(test_list), len(valid_list)))\n```", "```py\ntile_selected=pointcloud_train_files[random.randrange(20)]\nprint(tile_selected)\ntemp=np.loadtxt(tile_selected)\nprint('median\\n',np.median(temp,axis=0))\nprint('std\\n',np.std(temp,axis=0))\nprint('min\\n',np.min(temp,axis=0))\nprint('max\\n',np.max(temp,axis=0))\n```", "```py\ngdrive/My Drive/_UTWENTE/DATA/AHN4_33EZ2_12/train/AHN4_33EZ2_12_train_000083.txt \nmedian [2.068e+05 4.659e+05 6.628e+00 1.060e+02 1.210e+02 1.030e+02 1.298e+03 1.000e+00] \nstd [ 28.892 30.155 0.679 29.986 21.4 17.041 189.388 0.266] \nmin [2.068e+05 4.659e+05 5.454e+00 3.600e+01 6.200e+01 5.700e+01 7.700e+01 0.000e+00] \nmax [2.068e+05 4.660e+05 1.505e+01 2.510e+02 2.470e+02 2.330e+02 1.625e+03 4.000e+00]\n```", "```py\ncloud_data=temp.transpose()\nmin_f=np.min(cloud_data,axis=1)\nmean_f=np.mean(cloud_data,axis=1)\n```", "```py\nn_coords = cloud_data[0:3]\nn_coords[0] -= mean_f[0]\nn_coords[1] -= mean_f[1]\nn_coords[2] -= min_f[2]\nprint(n_coords)\n```", "```py\ncolors = cloud_data[3:6]/255\n```", "```py\n# The interquartile difference is the difference between the 75th and 25th quantile\nIQR = np.quantile(cloud_data[-2],0.75)-np.quantile(cloud_data[-2],0.25)\n# We subtract the median to all the observations and then divide by the interquartile difference\nn_intensity = ((cloud_data[-2] - np.median(cloud_data[-2])) / IQR)\n#This permits to have a scaling robust to outliers (which is often the case)\nn_intensity -= np.min(n_intensity)\nprint(n_intensity)\n```", "```py\n# We create a function that loads and normalize a point cloud tile\ndef cloud_loader(tile_path, features_used):\n  cloud_data = np.loadtxt(tile_path).transpose()\n  min_f=np.min(cloud_data,axis=1)\n  mean_f=np.mean(cloud_data,axis=1)\n  features=[]\n  if 'xyz' in features_used:\n    n_coords = cloud_data[0:3]\n    n_coords[0] -= mean_f[0]\n    n_coords[1] -= mean_f[1]\n    n_coords[2] -= min_f[2]\n    features.append(n_coords)\n  if 'rgb' in features_used:\n    colors = cloud_data[3:6]/255\n    features.append(colors)\n  if 'i' in features_used:\n    IQR = np.quantile(cloud_data[-2],0.75)-np.quantile(cloud_data[-2],0.25)\n    n_intensity = ((cloud_data[-2] - np.median(cloud_data[-2])) / IQR)\n    n_intensity -= np.min(n_intensity)\n    features.append(n_intensity)\n\n  gt = cloud_data[-1]\n  gt = torch.from_numpy(gt).long()\n\n  cloud_data = torch.from_numpy(np.vstack(features))\nreturn cloud_data, gt\n```", "```py\npc, labels = cloud_loader(tile_selected, ‘xyzrgbi’)\n```", "```py\n!pip install open3d==0.16\nimport open3d as o3d\n```", "```py\npc, gt = cloud_loader(tile_selected, ['xyz','rgb','i'])\npcd=o3d.geometry.PointCloud()\npcd.points=o3d.utility.Vector3dVector(np.array(pc)[0:3].transpose())\npcd.colors=o3d.utility.Vector3dVector((np.array(pc)[3:6]).transpose())\no3d.visualization.draw_plotly([pcd],point_sample_factor=0.5, width=600, height=400)\n```", "```py\nclass_names = ['unclassified', 'ground', 'vegetation', 'buildings', 'water']\ncloud_features='xyzi'\nselection=pointcloud_train_files[random.randrange(len(pointcloud_train_files))]\nvisualize_input_tile(selection, class_names, cloud_features, sample_size=20000)\n```", "```py\nimport torch\n\nx = torch.tensor([[1.0, 2.0, 3.0],\n                  [4.0, 5.0, 6.0],\n                  [7.0, 8.0, 9.0]])\n\nprint(x)\n```", "```py\ntensor([[1., 2., 3.],\n        [4., 5., 6.],\n        [7., 8., 9.]])\n```", "```py\n!pip install torchnet==0.0.4\nimport torchnet as tnt\n```", "```py\ncloud_features='xyzrgbi'\ntest_set = tnt.dataset.ListDataset(test_list,functools.partial(cloud_loader, features_used=cloud_features))\ntrain_set = tnt.dataset.ListDataset(train_list,functools.partial(cloud_loader, features_used=cloud_features))\nvalid_set = tnt.dataset.ListDataset(valid_list,functools.partial(cloud_loader, features_used=cloud_features))\n```", "```py\nimport pickle\nf = open(project_dir+\"/data_prepared.pckl\", 'wb')\npickle.dump([test_list, test_set, train_list, train_set, valid_list, valid_set], f)\nf.close()\n```", "```py\nf = open(project_dir+\"/data_prepared.pckl\", 'rb')\ntest_list_t, test_set_t, train_list_t, train_set_t, valid_list_t, valid_set_t = pickle.load(f)\nf.close()\nprint(test_list_t)\n```"]