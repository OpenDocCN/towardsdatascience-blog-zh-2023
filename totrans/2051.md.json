["```py\n# Installation\npip install bnlearn\n```", "```py\n# Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport bnlearn as bn\n\n# Import data set and drop continous and sensitive features\ndf = bn.import_example(data='census_income')\ndrop_cols = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week', 'race', 'sex']\ndf.drop(labels=drop_cols, axis=1, inplace=True)\n\ndf.head()\n#          workclass  education  ... native-country salary\n#0         State-gov  Bachelors  ...  United-States  <=50K\n#1  Self-emp-not-inc  Bachelors  ...  United-States  <=50K\n#2           Private    HS-grad  ...  United-States  <=50K\n#3           Private       11th  ...  United-States  <=50K\n#4           Private  Bachelors  ...           Cuba  <=50K\n#\n#[5 rows x 7 columns]\n```", "```py\n# Load library\nimport bnlearn as bn\n\n# Structure learning\nmodel = bn.structure_learning.fit(df, methodtype='hillclimbsearch', scoretype='bic')\n\n# Test edges significance and remove.\nmodel = bn.independence_test(model, df, test=\"chi_square\", alpha=0.05, prune=True)\n\n# Make plot\nG = bn.plot(model, interactive=False)\n\n# Make plot interactive\nG = bn.plot(model, interactive=True)\n\n# Show edges\nprint(model['model_edges'])\n# [('education', 'salary'),\n# ('marital-status', 'relationship'),\n# ('occupation', 'workclass'),\n# ('occupation', 'education'),\n# ('relationship', 'salary'),\n# ('relationship', 'occupation')]\n```", "```py\n# Learn the CPDs using the estimated edges.\n# Note that we can also customize the edges or manually provide a DAG.\n# model = bn.make_DAG(model['model_edges'])\n\n# Learn the CPD by providing the model and dataframe\nmodel = bn.parameter_learning.fit(model, df)\n\n# Print the CPD\nCPD = bn.print_CPD(model)\n```", "```py\n# Start making inferences\nquery = bn.inference.fit(model, variables=['salary'], evidence={'education':'Doctorate'})\nprint(query)\n+---------------+---------------+\n| salary        |   phi(salary) |\n+===============+===============+\n| salary(<=50K) |        0.2907 |\n+---------------+---------------+\n| salary(>50K)  |        0.7093 |\n+---------------+---------------+\n```", "```py\nquery = bn.inference.fit(model, variables=['salary'], evidence={'education':'HS-grad'})\nprint(query)\n+---------------+---------------+\n| salary        |   phi(salary) |\n+===============+===============+\n| salary(<=50K) |        0.8385 |\n+---------------+---------------+\n| salary(>50K)  |        0.1615 |\n+---------------+---------------+\n```", "```py\n# Start making inferences\nquery = bn.inference.fit(model, variables=['workclass'], evidence={'education':'Doctorate', 'marital-status':'Never-married'})\nprint(query)\n+-----------------------------+------------------+\n| workclass                   |   phi(workclass) |\n+=============================+==================+\n| workclass(?)                |           0.0420 |\n+-----------------------------+------------------+\n| workclass(Federal-gov)      |           0.0420 |\n+-----------------------------+------------------+\n| workclass(Local-gov)        |           0.1326 |\n+-----------------------------+------------------+\n| workclass(Never-worked)     |           0.0034 |\n+-----------------------------+------------------+\n| workclass(Private)          |           0.5639 |\n+-----------------------------+------------------+\n| workclass(Self-emp-inc)     |           0.0448 |\n+-----------------------------+------------------+\n| workclass(Self-emp-not-inc) |           0.0868 |\n+-----------------------------+------------------+\n| workclass(State-gov)        |           0.0810 |\n+-----------------------------+------------------+\n| workclass(Without-pay)      |           0.0034 |\n+-----------------------------+------------------+\n```", "```py\n# Install pgmpy\npip install pgmpy\n\n# Import functions from pgmpy\nfrom pgmpy.estimators import HillClimbSearch, BicScore, BayesianEstimator\nfrom pgmpy.models import BayesianNetwork, NaiveBayes\nfrom pgmpy.inference import VariableElimination\n\n# Import data set and drop continous and sensitive features\ndf = bn.import_example(data='census_income')\ndrop_cols = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week', 'race', 'sex']\ndf.drop(labels=drop_cols, axis=1, inplace=True)\n\n# Create estimator\nest = HillClimbSearch(df)\n\n# Create scoring method\nscoring_method = BicScore(df)\n\n# Create the model and print the edges\nmodel = est.estimate(scoring_method=scoring_method)\n\n# Show edges\nprint(model.edges())\n# [('education', 'salary'),\n# ('marital-status', 'relationship'),\n# ('occupation', 'workclass'),\n# ('occupation', 'education'),\n# ('relationship', 'salary'),\n# ('relationship', 'occupation')]\n```", "```py\nvec = {\n    'source': ['education', 'marital-status', 'occupation', 'relationship', 'relationship', 'salary'],\n    'target': ['occupation', 'relationship', 'workclass', 'education', 'salary', 'education'],\n    'weight': [True, True, True, True, True, True]\n}\nvec = pd.DataFrame(vec)\n\n# Create Bayesian model\nbayesianmodel = BayesianNetwork(vec)\n\n# Fit the model\nbayesianmodel.fit(df, estimator=BayesianEstimator, prior_type='bdeu', equivalent_sample_size=1000)\n\n# Create model for variable elimination\nmodel_infer = VariableElimination(bayesianmodel)\n\n# Query\nquery = model_infer.query(variables=['salary'], evidence={'education':'Doctorate'})\nprint(query)\n```", "```py\n# Installation\npip install causalnex\n```", "```py\n# Load libraries\nfrom causalnex.structure.notears import from_pandas\nfrom causalnex.network import BayesianNetwork\nimport networkx as nx\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\n# Import data set and drop continous and sensitive features\ndf = bn.import_example(data='census_income')\ndrop_cols = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week', 'race', 'sex']\ndf.drop(labels=drop_cols, axis=1, inplace=True)\n\n# Next, we want to make our data numeric, since this is what the NOTEARS expect.\ndf_num = df.copy()\nfor col in df_num.columns:\n    df_num[col] = le.fit_transform(df_num[col])\n```", "```py\n# Structure learning\nsm = from_pandas(df_num)\n\n# Thresholding\nsm.remove_edges_below_threshold(0.8)\n\n# Use positions from Bnlearn\npos=G['pos']\n# Make plot\nplt.figure(figsize=(15,10));\nedge_width = [ d['weight']*0.3 for (u,v,d) in sm.edges(data=True)]\nnx.draw_networkx_labels(sm, pos, font_family=\"Yu Gothic\", font_weight=\"bold\")\nnx.draw_networkx(sm, node_size=400, arrowsize=20, alpha=0.6, edge_color='b', width=edge_width)\n\n# If required, remove spurious edges and relearn structure.\nsm = from_pandas(df_num, tabu_edges=[(\"relationship\", \"native-country\")], w_threshold=0.8)\n```", "```py\n# Step 1: Create a new instance of BayesianNetwork\nbn = BayesianNetwork(structure_model)\n\n# Step 2: Reduce the cardinality of categorical features\n# Use domain knowledge or other manners to remove redundant features.\n\n# Step 3: Create Labels for Numeric Features\n# Create a dictionary that describes the relation between numeric value and label.\n\n# Step 4: Specify all of the states that each node can take\nbn = bn.fit_node_states(df)\n\n# Step 5: Fit Conditional Probability Distributions\nbn = bn.fit_cpds(df, method=\"BayesianEstimator\", bayes_prior=\"K2\")\n\n# Return CPD for education\nresult = bn.cpds[\"education\"]\n\n# Extract any information and probabilities related to education.\nprint(result)\n# marital-status  Divorced              ...   Widowed            \n# salary             <=50K              ...      >50K            \n# workclass              ? Federal-gov  ... State-gov Without-pay\n# education                             ...                      \n# 10th            0.077320    0.019231  ...  0.058824      0.0625\n# 11th            0.061856    0.012821  ...  0.117647      0.0625\n# 12th            0.020619    0.006410  ...  0.058824      0.0625\n# 1st-4th         0.015464    0.006410  ...  0.058824      0.0625\n# 5th-6th         0.010309    0.006410  ...  0.058824      0.0625\n# 7th-8th         0.056701    0.006410  ...  0.058824      0.0625\n# 9th             0.067010    0.006410  ...  0.058824      0.0625\n# Assoc-acdm      0.025773    0.057692  ...  0.058824      0.0625\n# Assoc-voc       0.046392    0.051282  ...  0.058824      0.0625\n# Bachelors       0.097938    0.128205  ...  0.058824      0.0625\n# Doctorate       0.005155    0.006410  ...  0.058824      0.0625\n# HS-grad         0.278351    0.333333  ...  0.058824      0.0625\n# Masters         0.015464    0.032051  ...  0.058824      0.0625\n# Preschool       0.005155    0.006410  ...  0.058824      0.0625\n# Prof-school     0.015464    0.006410  ...  0.058824      0.0625\n# Some-college    0.201031    0.314103  ...  0.058824      0.0625\n# [16 rows x 126 columns]\n```", "```py\n# Installation\npip install dowhy\n\n# Import libraries\nfrom dowhy import CausalModel\nimport dowhy.datasets\nfrom sklearn.preprocessing import LabelEncoder\n\n# Import data set and drop continous and sensitive features\ndf = bn.import_example(data='census_income')\ndrop_cols = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week', 'race', 'sex']\ndf.drop(labels=drop_cols, axis=1, inplace=True)\n\n# Treatment variable must be binary\ndf['education'] = df['education']=='Doctorate'\n\n# Next, we need to make our data numeric.\ndf_num = df.copy()\nfor col in df_num.columns:\n    df_num[col] = le.fit_transform(df_num[col])\n```", "```py\n# Specify the treatment, outcome, and potential confounding variables\ntreatment = \"education\"\noutcome = \"salary\"\n\n# Step 1\\. Create a Causal Graph\nmodel= CausalModel(\n        data=df_num,\n        treatment=treatment,\n        outcome=outcome,\n        common_causes=list(df.columns[~np.isin(df.columns, [treatment, outcome])]),\n        graph_builder='ges',\n        alpha=0.05,\n        )\n\n# Display the model\nmodel.view_model()\n```", "```py\n# Step 2: Identify causal effect and return target estimands\nidentified_estimand = model.identify_effect(proceed_when_unidentifiable=True)\n\n# Results\nprint(identified_estimand)\n\n# Estimand type: EstimandType.NONPARAMETRIC_ATE\n# ### Estimand : 1\n# Estimand name: backdoor\n# Estimand expression:\n#      d                                                                        \n# ────────────(E[salary|workclass,marital-status,native-country,relationship,occupation])\n# d[education]                                                                  \n#        \n# Estimand assumption 1, Unconfoundedness: If U→{education} and U→salary then P(salary|education,workclass,marital-status,native-country,relationship,occupation,U) = P(salary|education,workclass,marital-status,native-country,relationship,occupation)\n#\n# ### Estimand : 2\n# Estimand name: iv\n# No such variable(s) found!\n#\n# ### Estimand : 3\n# Estimand name: frontdoor\n# No such variable(s) found!\n```", "```py\n# Step 3: Estimate the target estimand using a statistical method.\nestimate = model.estimate_effect(identified_estimand, method_name=\"backdoor.propensity_score_stratification\")\n\n# Results\nprint(estimate)\n\n#*** Causal Estimate ***\n#\n## Identified estimand\n# Estimand type: EstimandType.NONPARAMETRIC_ATE\n#\n### Estimand : 1\n# Estimand name: backdoor\n# Estimand expression:\n#      d                                                                        \n# ────────────(E[salary|workclass,marital-status,native-country,relationship,occupation])\n# d[education]                                                                 \n#       \n# Estimand assumption 1, Unconfoundedness: If U→{education} and U→salary then P(salary|education,workclass,marital-status,native-country,relationship,occupation,U) = P(salary|education,workclass,marital-status,native-country,relationship,occupation)\n#\n## Realized estimand\n# b: salary~education+workclass+marital-status+native-country+relationship+occupation\n# Target units: ate\n#\n## Estimate\n# Mean value: 0.4697157228651772\n\n# Step 4: Refute the obtained estimate using multiple robustness checks.\nrefute_results = model.refute_estimate(identified_estimand, estimate, method_name=\"random_common_cause\")\n```", "```py\n# Installation\npip install causalimpact\n\n# Import libraries\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.tsa.arima_process import arma_generate_sample\nimport matplotlib.pyplot as plt\nfrom causalimpact import CausalImpact\n\n# Generate samples\nx1 = arma_generate_sample(ar=[0.999], ma=[0.9], nsample=100) + 100\ny = 1.2 * x1 + np.random.randn(100)\ny[71:] = y[71:] + 10\ndata = pd.DataFrame(np.array([y, x1]).T, columns=[\"y\",\"x1\"])\n\n# Initialize\nimpact = CausalImpact(data, pre_period=[0,69], post_period=[71,99])\n# Create inferences\nimpact.run()\n# Plot\nimpact.plot()\n# Results\nimpact.summary()\n\n#                              Average    Cumulative\n# Actual                           130          3773\n# Predicted                        120          3501\n# 95% CI                    [118, 122]  [3447, 3555]\n\n# Absolute Effect                    9           272\n# 95% CI                       [11, 7]    [326, 218]\n\n# Relative Effect                 7.8%          7.8%\n# 95% CI                  [9.3%, 6.2%]  [9.3%, 6.2%]\n\n# P-value                         0.0%              \n# Prob. of Causal Effect        100.0%\n\n# Summary report\nimpact.summary(output=\"report\")\n```"]