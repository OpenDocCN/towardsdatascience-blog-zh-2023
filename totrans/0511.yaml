- en: 'Classification Metrics: The Complete Guide For Aspiring Data Scientists'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/classification-metrics-the-complete-guide-for-aspiring-data-scientists-9f02eab796ae](https://towardsdatascience.com/classification-metrics-the-complete-guide-for-aspiring-data-scientists-9f02eab796ae)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The only guide you’ll need to master classification metrics in Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://federicotrotta.medium.com/?source=post_page-----9f02eab796ae--------------------------------)[![Federico
    Trotta](../Images/e997e3a96940c16ab5071629016d82fd.png)](https://federicotrotta.medium.com/?source=post_page-----9f02eab796ae--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9f02eab796ae--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9f02eab796ae--------------------------------)
    [Federico Trotta](https://federicotrotta.medium.com/?source=post_page-----9f02eab796ae--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9f02eab796ae--------------------------------)
    ·26 min read·May 15, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0c0d34355b4d7101042a7934ccc24a1e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [UliSchu](https://pixabay.com/it/users/ulischu-1993560/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1205171)
    on [Pixabay](https://pixabay.com/it//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=1205171)
  prefs: []
  type: TYPE_NORMAL
- en: 'Supervised Machine Learning can be divided into two groups of problems: classification
    and regression. This article aims to be the definitive guide on classification
    metrics: so if you’re an aspiring Data Scientist or if you’re a junior one, you
    definitely need to read this.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, you may also like to read the my guide on the 5 metrics you need
    to know to master a regression problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/mastering-the-art-of-regression-analysis-5-key-metrics-every-data-scientist-should-know-1e2a8a2936f5?source=post_page-----9f02eab796ae--------------------------------)
    [## Mastering the Art of Regression Analysis: 5 Key Metrics Every Data Scientist
    Should Know'
  prefs: []
  type: TYPE_NORMAL
- en: The definitive guide on all the knowledge you should have on the metrics used
    in regression analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/mastering-the-art-of-regression-analysis-5-key-metrics-every-data-scientist-should-know-1e2a8a2936f5?source=post_page-----9f02eab796ae--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Secondly, let me tell you what you’ll find here through a table of contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As usual, you’ll find Python examples to make the theory into practice.
  prefs: []
  type: TYPE_NORMAL
- en: What is a classification problem?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a classification problem data are labeled into classes: in other words,
    our label values represent the class to which the data points belong.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two kinds of classification problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Binary classification** problems: in this case, the target values are labeled
    with a 0 or a 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-class** problems: in this case, the label gets multiple values (0,
    1, 2, 3, etc.), depending on the number of classes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s visualize them. Firstly, let’s create a binary classification dataset
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/e8c9af6da85d31641f7636adb92e2843.png)'
  prefs: []
  type: TYPE_IMG
- en: The binary classification problem we’ve created. Image by Federico Trotta.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, this is an example of a binary classification dataset: some datapoint belongs
    to the blue class some others to the red class. Now, it doesn''t matter what these
    classes represent. They can be apples or pears, cars or trains. It doesn''t matter.
    What it’s important now is that we’ve visualized a binary classification problem.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s visualize a multi-class problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/090933c895c8b6a68266f9f6b3cb678f.png)'
  prefs: []
  type: TYPE_IMG
- en: The multi-class classification problem we’ve created. Image by Federico Trotta.
  prefs: []
  type: TYPE_NORMAL
- en: So, here we’ve created a classification problem with data points belonging to
    4 classes.
  prefs: []
  type: TYPE_NORMAL
- en: One issue with multi-class classification problems is understanding if all the
    classes matter. Let’s see what we mean in the next paragraph.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Dealing with class imbalance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Consider the following dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/141e4af101075559b84a2543d5f3e62e.png)'
  prefs: []
  type: TYPE_IMG
- en: The imbalanced multi-class classification problem we’ve created. Image by Federico
    Trotta.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, we have a lot of blue spots and also a high number of green spots.
    The red spots, instead, are very few with respect to the others.
  prefs: []
  type: TYPE_NORMAL
- en: 'The question is: should we take into account the red spots? In other words:
    can we perform our ML analysis by deleting the red spots because are too few?'
  prefs: []
  type: TYPE_NORMAL
- en: The answer is…it depends!
  prefs: []
  type: TYPE_NORMAL
- en: Generally, we can ignore the values belonging to one (or more) class(es) with
    fewer observations than the others. But in specific cases, we mustn’t! And here’s
    where domain knowledge comes into the game.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if we’re studying fraud detection in a bank firm, we expect fraud
    transactions to be rare with respect to standard transactions. This gives us an
    imbalanced dataset, meaning: we can''t delete the values belonging to the class
    with fewer observations!'
  prefs: []
  type: TYPE_NORMAL
- en: The same thing is if we’re studying something in the medical field. In the case
    of rare diseases, we expect them to be….rare! So, an unbalanced dataset is what
    we expect.
  prefs: []
  type: TYPE_NORMAL
- en: Anyway, we created the datasets above on purpose for educational scopes. Generally
    speaking, it’s very hard to visualize the data points because we have more than
    one feature. So, a way to evaluate class imbalance is to display a histogram of
    the labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before going on…if you don’t know the difference between a histogram and a
    bar plot, you can read the following article I wrote:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/what-is-the-difference-between-a-barplot-and-a-histogram-e62d0e532e7d?source=post_page-----9f02eab796ae--------------------------------)
    [## What is the Difference between a Barplot and a Histogram?'
  prefs: []
  type: TYPE_NORMAL
- en: They seem to be the same, but the difference between them is relevant
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/what-is-the-difference-between-a-barplot-and-a-histogram-e62d0e532e7d?source=post_page-----9f02eab796ae--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'So, here’s what we can do. Let’s create a dataset with three labels like the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Even if this data frame is created on purpose, it reflects real cases because
    it’s tabular (meaning we can manipulate it with pandas). So, if we show the head
    we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3eda9e34ff0a9ae0614a2d3e81141f7a.png)'
  prefs: []
  type: TYPE_IMG
- en: The head of our data frame. Image by Federico Trotta.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, to understand if our dataset may be imbalanced or not we plot a histogram
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/726b2cfbe1f01824b9f61bc724cf9fa5.png)'
  prefs: []
  type: TYPE_IMG
- en: The frequencies of the three classes of our dataset. Image by Federico Trotta.
  prefs: []
  type: TYPE_NORMAL
- en: Well, in such cases, the three classes have the same frequency. So the dataset
    is well-balanced and we must consider all the labels in our analyses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, this is how class imbalance is represented via a histogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/2638ea6d4202d8c393b99d755bd7e4e9.png)'
  prefs: []
  type: TYPE_IMG
- en: The imbalanced dataset we created. Image by Federico Trotta.
  prefs: []
  type: TYPE_NORMAL
- en: So, in cases like that, we need to understand if class 3 has to be taken into
    account (we’re studying “rare situations”) or not (we’re studying “situations
    with no rare events”) so that we can drop all the values associated with that.
  prefs: []
  type: TYPE_NORMAL
- en: Now, before diving into the metrics we need to know to solve a classification
    problem, we need to understand what a classification algorithm actually does.
  prefs: []
  type: TYPE_NORMAL
- en: What a classification algorithm actually does
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we know, we use Machine Learning to make predictions. This means that we
    train an ML model on the available data, expecting the predictions to be as near
    as possible to the actual data.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you don’t know what “training an ML model” actually means, you can read
    my article here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/what-is-a-trained-model-5c872cfa8448?source=post_page-----9f02eab796ae--------------------------------)
    [## What is a Trained Model?'
  prefs: []
  type: TYPE_NORMAL
- en: Or…what does “training an ML model” mean?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/what-is-a-trained-model-5c872cfa8448?source=post_page-----9f02eab796ae--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: So, let’s consider a binary classification problem. Our ML model gets the features
    as input and predicts if the data points belong to class 1 or to class 2\. If
    the predictions “are perfect”, it means that our model tells us precisely which
    of the available data belongs to class 1 and which to class 2, with 0 errors.
    So, all the actual points belonging to class 1 are predicted to belong to class
    1 by our ML model.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, as you imagine, a 0% error is not possible, and this is why we need
    some metrics to evaluate our ML models.
  prefs: []
  type: TYPE_NORMAL
- en: 'So before diving into the metrics, we need to use some nomenclature:'
  prefs: []
  type: TYPE_NORMAL
- en: We define a **True Positive (TP)** as a data point belonging to a class that
    is predicted to belong to that class. For example, if the model predicts that
    an email is spam, and it is indeed spam, then that is a true positive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We define a **True Negative (TN)** as a data point not belonging to a class
    that is predicted to not belong to that class. For example, if the model predicts
    that an email is not spam, and it is indeed not spam, then that is a true negative.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We define a **False Positive (FP)** as a data point belonging to a class that
    is predicted to belong to another class. For example, if the model predicts that
    an email is spam, but it is actually not spam, then that is a false positive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We define a **False Negative (FN)** as a data point not belonging to a class
    that is predicted not to belong to that class. For example, if the model predicts
    that an email is not spam, but it is actually spam, then that is a false negative.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generally speaking, as you may imagine, we want to minimize false positives
    and false negatives while maximizing true positives and true negatives, to make
    the model as accurate as possible. This means that our ML model makes accurate
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: But what does “accurate” mean? We need to dive into our first classification
    metrics to understand it.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first metric we take into account is accuracy. Let’s see the formula:'
  prefs: []
  type: TYPE_NORMAL
- en: The formula of accuracy written by the Author on embed-dot-fun.
  prefs: []
  type: TYPE_NORMAL
- en: So, **accuracy** is a measure of how often our ML model is correct in its predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s say we have a dataset of emails that are labeled as either
    spam or not spam. We can use ML to predict whether new emails are spam or not.
    If the model correctly predicts that 80 out of 100 emails are spam, and correctly
    predicts that 90 out of 100 emails are not spam, then its accuracy would be:'
  prefs: []
  type: TYPE_NORMAL
- en: The calculation of our example.
  prefs: []
  type: TYPE_NORMAL
- en: This means that our model is able to correctly predict the class of an email
    85% of the time. A high accuracy score (near 1) indicates that the model is performing
    well, while a low accuracy score (near 0) indicates that the model needs to be
    improved. However, accuracy alone may not always be the best metric to evaluate
    a model’s performance, especially in imbalanced datasets.
  prefs: []
  type: TYPE_NORMAL
- en: This is understandable because the prevalent class has “more data” labeled to
    it, so if our model is accurate it will make accurate predictions according to
    the prevalent class. In other words, our model may be biased because of the prevalent
    class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s make an example in Python creating a dataset for this purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We have created a simple data frame with 1000 samples that can represent the
    data of some credit card transactions, for example. We have, then, created a class
    for the fraudulent transaction which is the 5% of all the observations. So, this
    dataset is clearly imbalanced.
  prefs: []
  type: TYPE_NORMAL
- en: 'If our model is accurate it is because it’s biased by the 95% of the observations
    that belong to the class that represent the non-fraud transactions. So let’s split
    the data set, make predictions with the Logistic Regression model, and print the
    accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'So, our model is 95% accurate: hooray! Now…let’s define the other metrics and
    see what they tell us about this dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Precision and recall
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Precision** measures the ability of a classifier to not label as positive
    a sample that is negative. In other words, it measures the fraction of true positives
    among all positive predictions. Simplifying, precision tells how accurate are
    the positive predictions of our model. That’s the formula:'
  prefs: []
  type: TYPE_NORMAL
- en: The formula for the precision score written by the Author on embed-dot-fun.
  prefs: []
  type: TYPE_NORMAL
- en: Considering an email spam classification problem, precision measures how many
    of the emails that the model classified as spam are actually spam.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use it in our imbalanced dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Auch! 95% accuracy and 0% precision: what does it mean? It means that the model
    is predicting all samples as negative, or non-fraudulent. Which is wrong, of course.
    In fact, a high precision score would indicate that the model is correctly identifying
    a high proportion of fraudulent transactions among all transactions it predicts
    as fraudulent.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we have the **recall** metric that measures the fraction of true positives
    among all actual positives. In other words, it measures how many of the actual
    positives are correctly predicted. Simplifying, recall tells us how well our model
    is able to find all the positive instances in our data. Here’s the formula:'
  prefs: []
  type: TYPE_NORMAL
- en: The formula for the recall score written by the Author on embed-dot-fun.
  prefs: []
  type: TYPE_NORMAL
- en: Considering an email spam classification problem, recall measures how many of
    the actual spam emails in the dataset are correctly identified as spam emails
    by our ML classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say that we have a dataset of 1000 emails, where 200 of them are spam
    and the rest are legitimate. We train a machine learning model to classify emails
    as spam or not spam, and it predicts that 100 of the emails are spam.
  prefs: []
  type: TYPE_NORMAL
- en: Precision would tell us how many of those 100 predicted spam emails are actually
    spam. For example, if 90 out of the 100 predicted spam emails are actually spam,
    then the precision would be 90%. This means that out of all emails that the model
    predicted as spam, 90% of them are actually spam.
  prefs: []
  type: TYPE_NORMAL
- en: Recall, on the other hand, tells us how many of the actual spam emails the model
    correctly identified as spam. For example, if out of the 200 actual spam emails,
    the model correctly identified 150 of them as spam, then the recall would be 75%.
    This means that out of all actual spam emails, the model correctly identified
    75% of them as spam.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s use recall in our imbalanced dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Again: we have 95% of accuracy and 0% recall. What does it mean? As before,
    it means that the model is not correctly identifying any fraudulent transactions,
    and is instead predicting all transactions as non-fraudulent. In fact, a high
    recall score would indicate that the model is correctly identifying a high proportion
    of fraudulent transactions among all actual fraudulent transactions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, in practice, we want to achieve a balance between precision and recall
    depending on the problem we’re studying. To do so, we often refer to other two
    metrics that consider both of them: the confusion matrix and f1-score. Let’s see
    them.'
  prefs: []
  type: TYPE_NORMAL
- en: F1-score
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**F1-score** is an evaluation metric in Machine Learning that combines precision
    and recall into a single value in the range 0–1\. If f1-score results in a 0 value,
    then our ML model has low performance. If f1-score results in a 1 value, then
    our ML model has high performance.'
  prefs: []
  type: TYPE_NORMAL
- en: This metric balances precision and recall by calculating their harmonic mean.
    This is a type of average that is more sensitive to low values, and this is why
    this metric is particularly suitable for imbalanced datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see its formula:'
  prefs: []
  type: TYPE_NORMAL
- en: The formula for the f1-score written by the Author on embed-dot-fun.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we know the results we’ll gain for our imbalanced dataset (f1-score will
    be 0). But let’s see how to use it in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In the context of a spam classifier, let’s say we have a dataset of 1000 emails,
    where 200 of them are spam and the rest are legitimate. We train a machine learning
    model to classify emails as spam or not spam, and it predicts that 100 of the
    emails are spam.
  prefs: []
  type: TYPE_NORMAL
- en: To calculate the F1-score of the spam classifier, we first need to calculate
    its precision and recall. Let’s say that out of the 100 predicted spam emails,
    80 are actually spam. So, the precision is 80%. Also, let’s say that out of the
    200 actual spam emails, the model correctly identified 150 of them as spam. So,
    the recall is 75%.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can calculate the f1-score:'
  prefs: []
  type: TYPE_NORMAL
- en: The calculation for the f1-score for our spam classifier written by the Author
    on embed-dot-fun.
  prefs: []
  type: TYPE_NORMAL
- en: Which is a pretty good result as we’re near 1.
  prefs: []
  type: TYPE_NORMAL
- en: The confusion matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The confusion matrix is a table that summarizes the performance of a classification
    model by showing the number of true positives, false positives, true negatives,
    and false negatives.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a binary classification problem, the confusion matrix has two rows and two
    columns and it’s displayed like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/57cee4fb7ae9ca09b54dbbaafa0f08f5.png)'
  prefs: []
  type: TYPE_IMG
- en: The confusion matrix. Image by Federico Trotta.
  prefs: []
  type: TYPE_NORMAL
- en: Using the spam email classification example, let’s say that our model predicted
    100 emails as spam, out of which 80 were actually spam, and 900 emails as not
    spam, out of which 20 were actually spam.
  prefs: []
  type: TYPE_NORMAL
- en: 'The confusion matrix for this example would look like that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/343ca86b31c37fe0577a54e5d195b60c.png)'
  prefs: []
  type: TYPE_IMG
- en: The confusion matrix for our spam classification problem. Image by Federico
    Trotta.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, this is a very useful visualization tool for classification for two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: It can help us calculate precision and recall by visualizing it
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It immediately tells us what matters, without any calculations. What we want
    in a classification problem, in fact, is TN and TP to be the highest possible
    while FP and FN to be the lowest possible (as much as near to 0). So, if the values
    on the main diagonal are high and the values on the other positions are low, then
    our ML model has good performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This is the reason why I love the confusion matrix: we just need to watch the
    main diagonal (from top-left to low-right) and non-diagonal values to evaluate
    the performance of an ML classifier.'
  prefs: []
  type: TYPE_NORMAL
- en: Considering our imbalanced dataset, we obtained 0 for precision and recall and
    we said that it means that the model is not correctly identifying any fraudulent
    transactions, and is instead predicting all transactions as non-fraudulent.
  prefs: []
  type: TYPE_NORMAL
- en: 'This may be really difficult to visualize, because of the formulas of precision
    and recall. We have to have them clear in our minds. Since it’s not easy for me
    to have this kind of visualization, let’s apply the confusion matrix to our example
    and see what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: See what happens?! We can clearly say that our model is not performing well
    because, while it captures 285 TNs it captures 0 TPs! That’s the visual power
    of the confusion matrix!
  prefs: []
  type: TYPE_NORMAL
- en: 'There is also another way to display the confusion matrix, and I really love
    it because it improves the visualization experience. Here’s the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/110c8887323aee9064c8a11ba3d6837f.png)'
  prefs: []
  type: TYPE_IMG
- en: The visualization of our confusion matrix. Image by Federico Trotta.
  prefs: []
  type: TYPE_NORMAL
- en: 'This kind of visualization is very useful in the case of multi-class classification
    problems. Let’s see one example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/4b7044158efaec0bf8c86607d85a6990.png)'
  prefs: []
  type: TYPE_IMG
- en: The visualization of our confusion matrix for a three-class problem. Image by
    Federico Trotta.
  prefs: []
  type: TYPE_NORMAL
- en: In these cases is not easy to understand what are the TPs, the TNs, and so on
    because we have three classes. Anyway, we can simply refer to the values on the
    main diagonal and to the non-diagonal ones. In this case, on the main diagonal,
    we have 49, 52, and 44 which are values much higher than the non-diagonal ones,
    telling us that this model is performing well (also note we’ve calculated the
    confusion matrix on the test set!).
  prefs: []
  type: TYPE_NORMAL
- en: Sensitivity and specificity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a couple of metrics that, in my personal opinion, are more suitable
    if used in some particular cases: sensitivity and specificity. Let me talk about
    them, and then we’ll discuss the usability in particular cases.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sensitivity** is the ability of a classifier to find all the positive samples:'
  prefs: []
  type: TYPE_NORMAL
- en: The formula of the sensitivity written by the Author on embed-dot-fun.
  prefs: []
  type: TYPE_NORMAL
- en: Wait a second! But isn’t it the recall?!?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Yes, it is. It’s not a mistake. This is why I’m telling you that these metrics
    are more suitable for particular cases. But let me go on.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define **specificity** as the ability of a classifier to find all the negative
    samples:'
  prefs: []
  type: TYPE_NORMAL
- en: The formula of the specificity written by the Author on embed-dot-fun.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, both of them, describe the “precision” of a test: sensitivity describes
    the probability of a positive test. Specificity of a negative one.'
  prefs: []
  type: TYPE_NORMAL
- en: In my experience, these metrics are more suitable for classifiers used in the
    medical field, biology, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s take into account a COVID test. Consider this approach (which
    can be considered Bayesian, but let’s skip that): you make a COVID test and the
    result is positive. Question: what’s the probability to get a positive test? And
    what’s the probability to get a negative test?'
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words: what are the **sensitivity and the specificity of the tool**
    you used to get the result?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, you may ask yourself: what kind of question are you asking, Federico?'
  prefs: []
  type: TYPE_NORMAL
- en: Let me make an example I lived last summer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here in Italy, a positive COVID test had to be certified by someone (let’s
    skip the reasons for that): a hospital or a pharmacy, typically. So, when we had
    the symptoms what we generally did here was test for COVID at home (3-5€ COVID
    test), then go to a pharmacy and confirm (15€ COVID test).'
  prefs: []
  type: TYPE_NORMAL
- en: So, last July I had symptoms after my wife and daughters tested positive. So
    I tested home and resulted positive. Then, immediately went to the pharmacy to
    confirm, and…resulted negative!
  prefs: []
  type: TYPE_NORMAL
- en: 'How is that possible? Easy: the tool I used at home for the COVID test was
    more sensitive than the other used by the pharmacist (or, the test used by the
    pharmacist was more specific than the one I used).'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, as per my experience, these metrics are particularly suitable for measuring
    instruments of any kind (mechanical, electrical, etc…) and/or in some particular
    fields (like biology, medicine, etc…). Also, remembering that those metrics use
    TP, TN, FP, and FN as precision and recall: this stresses again the fact that
    these are more suitable in the case of a binary classification problem.'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, I’m not telling you that sensitivity and specificity **must** be
    used only in the above-mentioned cases. They’re just more suitable, in my experience.
  prefs: []
  type: TYPE_NORMAL
- en: Log loss (cross-entropy)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Log loss — sometimes called cross-entropy — is an important metric in classification,
    and is based on probability. This score compares the predicted probability for
    each class to the actual class labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see the formula:'
  prefs: []
  type: TYPE_NORMAL
- en: The formula of the Log Loss written by the Author on embed-dot-fun.
  prefs: []
  type: TYPE_NORMAL
- en: 'Where we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '`n` is the total number of observations, and `i` is a single observation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`y` is the true value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`p` is the predicted probability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Ln` is the natural logarithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To calculate the predicted probability `p`, we need to use an ML model that
    can actually calculate probabilities, like Logistic Regression, for example. In
    this case, we need to use the `predict_proba()` method like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'So, suppose we have a binary classification problem and suppose we calculate
    the probabilities via the Logistic Regression model, and suppose the following
    table represents our results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/da12c67a25eddbc11949aae5dcafeb31.png)'
  prefs: []
  type: TYPE_IMG
- en: A table showing actual labels and probabilities calculated via the Logistic
    Regression model. Image by Federico Trotta.
  prefs: []
  type: TYPE_NORMAL
- en: 'The calculation we’d perform to obtain the Log Loss is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The calculation of Log Loss with the values of the above table (we’ve made it
    just for the first two occurrences) made by the Author on embedd-dot-fun.
  prefs: []
  type: TYPE_NORMAL
- en: And this results in a value near 0 that can make us satisfied, meaning our Logistic
    Regression model is predicting quite well the labels for each class. In fact,
    a Log Loss with a value of 0 represents the best fit possible. In other words,
    a model with a Log Loss of 0 predicts each observation’s probability as the true
    value.
  prefs: []
  type: TYPE_NORMAL
- en: 'But, don’t be scared: we don’t need to calculate the value of Log Loss by hand.
    Luckily for us, `sklearn`came into help. So, let’s return to our imbalanced dataset.
    To calculate Log Loss in Python we type the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Again, we got a bad metric on the test set, confirming all of the above.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, one last consideration: Log Loss is suitable for binary classification
    problems. How about multi-class problems?'
  prefs: []
  type: TYPE_NORMAL
- en: Categorical cross-entropy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The categorical cross-entropy metric represents the generalization of the Log
    Loss to the multi-class case.
  prefs: []
  type: TYPE_NORMAL
- en: This metric is particularly suitable for imbalanced datasets because it takes
    into account the probability of the predicted class. This is important when we
    have an imbalanced dataset because the relative frequency of the classes can influence
    the ability of the model to correctly predict the “minority” classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we have:'
  prefs: []
  type: TYPE_NORMAL
- en: The formula of the Categorical cross-entropy written by the Author on embed-dot-fun.
  prefs: []
  type: TYPE_NORMAL
- en: Where the nomenclature is the same as for the Log Loss case.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in Python, we use it the same way we do with Log Loss so by invoking
    `from sklearn.metrics import log_loss`. So, this discussion was just to stress
    the fact that there is a slight difference in the case of a binary classification
    or in the case of a multi-class classification.
  prefs: []
  type: TYPE_NORMAL
- en: AUC/ROC curve
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**ROC** stands for “Receiver Operating Characteristic” and is a graphical way
    to evaluate a classifier by plotting the true positive rate (**TPR**) against
    the false positive rate (**FPR**) at different thresholds.'
  prefs: []
  type: TYPE_NORMAL
- en: '**AUC**, instead, stands for “Area Under Curve” and represents the area under
    the ROC curve. So this is an overall performance method, ranging from 0 to 1 (where
    1 means the classifier predicts 100% of the labels as the actual values), and
    it’s more suitable when comparing different classifiers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, let''s define TPR and FPR:'
  prefs: []
  type: TYPE_NORMAL
- en: TPR is the sensitivity (which can also be called recall, as we said).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FPR is defined as `1-specificity`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that AUC/ROC is suitable in the case of a binary classification problem.
    In the case of a multi-class classifier, in fact, TPR and FPR should be revisited.
    This requires some work to do, so here my advice is to use it just in the case
    of a binary classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s see how to implement this in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/40cc9c8df71840ffc300a7bae2e56327.png)'
  prefs: []
  type: TYPE_IMG
- en: The AUC/ROC curve for the above code. Image by Federico Trotta.
  prefs: []
  type: TYPE_NORMAL
- en: The dotted line represents a purely random classifier (which is like randomly
    guessing a class rather than another. And, in fact, since this is a binary classification
    problem, the line has a slope of 0.5, meaning we have a 50% chance to guess it
    right). So, the more our curve is far from it, the more our model is a good one.
    Ideally, our curve should stay as much as possible on the top-left corner meaning
    a low False Positive Rate with a high True Positive Rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is why this graph is good to compare models: better models have curves
    near the top-left corner of the graph. Let’s see an example: we’ll use the same
    dataset as before, but we’ll fit the data to three different ML models.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/8b5f78633b687b924e0fea6eea71e134.png)'
  prefs: []
  type: TYPE_IMG
- en: The AUC/ROC curve for the above code. Image by Federico Trotta.
  prefs: []
  type: TYPE_NORMAL
- en: So, in this case, the Random Forest classifier is the one that predicts better
    our data because its curve lies on the top-left corner at values higher than the
    other models.
  prefs: []
  type: TYPE_NORMAL
- en: To conclude this section, let me remind you that, at the beginning of this paragraph,
    we said that ROC plots TPR against the FPR at different thresholds, but we haven’t
    specified anything else. So, let’s do so in the next paragraph.
  prefs: []
  type: TYPE_NORMAL
- en: Precision-recall curve
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Consider a binary classification problem. We fit the data to a classifier and
    it assigns any predicted value to class 1 or to class 0: what are the criteria
    used for the assignation?'
  prefs: []
  type: TYPE_NORMAL
- en: Stop reading for a bit and try to think about that.
  prefs: []
  type: TYPE_NORMAL
- en: 'Yes, you guessed it right: in classification problems, a classifier assigns
    a score between 0 and 1 to each sample. This indicates the probability that the
    sample belongs to the positive class.'
  prefs: []
  type: TYPE_NORMAL
- en: So, our ML models use a threshold value to convert the probability scores into
    class predictions. In other words, any sample with a probability score greater
    than the threshold is predicted as positive, for example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, this is true even in the case of a multiclass classification problem:
    we’ve used the case of a binary classification just to simplify our reasoning.'
  prefs: []
  type: TYPE_NORMAL
- en: So, ROC curves are useful because they show how the performance of an ML model
    varies at different threshold values.
  prefs: []
  type: TYPE_NORMAL
- en: Anyway, the fact that a classifier assigns the predicted value to a class based
    on a threshold tells us that precision and recall are a trade-off (just like bias
    and variance).
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, we can even plot the precision-recall curve. Let’s see how to do so,
    using the same dataset we used for the AUC/ROC curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/18aee6c81d55d7ff02a864c75d1a4c32.png)'
  prefs: []
  type: TYPE_IMG
- en: The precision-recall curve for the above code. Image by Federico Trotta.
  prefs: []
  type: TYPE_NORMAL
- en: So, above we can see that precision maintains the value of 1 until circa 0.5
    recall, then it falls dramatically fast. So, we’d like to choose a precision-recall
    trade-off before this value. Let’s say at 0.4 recall.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another great way to visualize this tradeoff is to plot precision vs recall
    as the threshold varies. Using the same dataset, this is what happens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c7868e8553eea3fd3bf87250cb287238.png)'
  prefs: []
  type: TYPE_IMG
- en: Precision vs recall as the threshold varies. Image by Federico Trotta.
  prefs: []
  type: TYPE_NORMAL
- en: So, the above plot confirms that the threshold that balances the precision-recall
    trade-off is around 0.4, in this case.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, when someone is telling you that found an ML model with 95% precision you
    should ask: “*At what recall?*”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, since they use quite the same metrics, you may be wondering when we
    should use the AUC/ROC curve and when the precision-recall one. Quoting from reference
    1 (page. 92):'
  prefs: []
  type: TYPE_NORMAL
- en: As a rule of thumb, you should prefer precision.recall curve whenever the positive
    class is rare, or when you care most about the false positives than the false
    negatives, and ROC curve otherwise
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Bonus: KDE and learning curves'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Among all the methods and metrics we’ve seen above that are specific to the
    classification cases, there are two that are transversal. Meaning they can be
    used both for evaluating classification and regression problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are the KDE plot and learning curves. I’ve written about them in previous
    articles, so I’ll link them below:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You find what a KDE is and how to use it at point 3 in the paragraph “Graphical
    methods to validate your ML model” of the following article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/mastering-linear-regression-the-definitive-guide-for-aspiring-data-scientists-7abd37fcb9ed?source=post_page-----9f02eab796ae--------------------------------)
    [## Mastering Linear Regression: The Definitive Guide For Aspiring Data Scientists'
  prefs: []
  type: TYPE_NORMAL
- en: All you need to know about Linear Regression is here (including an application
    in Python)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/mastering-linear-regression-the-definitive-guide-for-aspiring-data-scientists-7abd37fcb9ed?source=post_page-----9f02eab796ae--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'You can read about what learning curves are and how to use them here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/mlearning-ai/how-to-easily-validate-your-ml-models-with-learning-curves-21cc01636083?source=post_page-----9f02eab796ae--------------------------------)
    [## How To Easily Validate Your ML Models With Learning Curves'
  prefs: []
  type: TYPE_NORMAL
- en: Discover the power of learning curves to validate your ML models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/mlearning-ai/how-to-easily-validate-your-ml-models-with-learning-curves-21cc01636083?source=post_page-----9f02eab796ae--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we’ve seen a lot of metrics and methodologies to validate a classification
    algorithm. If you’re wondering which one to use, I always say that, while making
    experience with each one (especially, comparing them) is a good practice, it’s
    difficult to answer the question, for a lot of reasons. Often, it’s just a question
    of taste.
  prefs: []
  type: TYPE_NORMAL
- en: Also, using just one metric to evaluate an ML model is not sufficient, and this
    is a rule of thumb.
  prefs: []
  type: TYPE_NORMAL
- en: If you read other articles from me, you know that I personally love to use at
    least one analytical method and one graphical one. In the case of classification
    problems, I generally use the confusion matrix and the KDE.
  prefs: []
  type: TYPE_NORMAL
- en: 'But, again: it’s a matter of personal taste. My advice here is to practice
    with them and decide which ones you like, remembering that you’ll need more than
    one to make accurate decisions on your ML models.'
  prefs: []
  type: TYPE_NORMAL
- en: '**FREE PYTHON EBOOK:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Started learning Python Data Science but struggling with it? [***Subscribe
    to my newsletter and get my free ebook: this will give you the right learning
    path to follow to learn Python for Data Science with hands-on experience.***](https://federico-trotta.ck.page/a3970f33f4)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Enjoyed the story? Become a Medium member for 5$/month [through my referral
    link](https://medium.com/@federicotrotta/membership): I’ll earn a small commission
    to no additional fee to you:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@federicotrotta/membership?source=post_page-----9f02eab796ae--------------------------------)
    [## Join Medium with my referral link — Federico Trotta'
  prefs: []
  type: TYPE_NORMAL
- en: Read every story from Federico Trotta (and thousands of other writers on Medium).
    Your membership fee directly supports…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@federicotrotta/membership?source=post_page-----9f02eab796ae--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*Bibliography and references:*'
  prefs: []
  type: TYPE_NORMAL
- en: '*[1] Hands-on Machine Learning with Scikit-Learn & Tensorflow - Aurelien Gueron*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*[2] Machine Learning with PyTorch and Scikit-learn - Sebastian Raschka, Yuxi
    Liu, Vahid Mirialili*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
