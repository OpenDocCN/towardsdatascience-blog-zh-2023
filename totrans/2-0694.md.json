["```py\ndf_evenly = spark.createDataFrame([i for i in range(1000000)], IntegerType())\ndf_evenly = df_evenly.withColumn(\"partitionId\", spark_partition_id())\n```", "```py\ndf_evenly.rdd.getNumPartitions()\n//output 3\n```", "```py\ndf_evenly.groupby([df_evenly.partitionId]).count().sort(df_evenly.partitionId).show()\n```", "```py\ndf_evenly.alias(“left”).join(df_evenly.alias(“right”),”value”, “inner”).count()\n```", "```py\ndf0 = spark.createDataFrame([0] * 999998, IntegerType()).repartition(1)\ndf1 = spark.createDataFrame([1], IntegerType()).repartition(1)\ndf2 = spark.createDataFrame([2], IntegerType()).repartition(1)\ndf_skew = df0.union(df1).union(df2)\ndf_skew = df_skew.withColumn(\"partitionId\", spark_partition_id())\n## If we apply the same function call again, we get what we want to see for the one partition with much more data than the other two.\ndf_skew.groupby([df_skew.partitionId]).count().sort(df_skew.partitionId).show()\n```", "```py\n//simulate reading to first round robin distribute the key\ndf_skew = df_skew.repartition(3)\n\ndf_skew.join(df_evenly.select(“value”),”value”, “inner”).count() \n```", "```py\ndf_skew.join(**broadcast**(df_evenly.select(“value”)),”value”, “inner”).count()\n```", "```py\ndf_left = df_skew.withColumn(“salt”, (rand() * spark.conf.get(“spark.sql.shuffle.partitions”)).cast(“int”))\n```", "```py\ndf_right = df_evenly.withColumn(“salt_temp”, array([lit(i) for i in range(int(spark.conf.get(“spark.sql.shuffle.partitions”)))]))\n```"]