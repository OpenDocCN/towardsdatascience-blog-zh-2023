- en: Nonlinear Dimension Reduction, Kernel PCA (kPCA), and Multidimensional Scaling
    — An Easy Tutorial with Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/nonlinear-dimension-reduction-kernel-pca-kpca-and-multidimensional-scaling-an-easy-tutorial-63429ee9d0ae](https://towardsdatascience.com/nonlinear-dimension-reduction-kernel-pca-kpca-and-multidimensional-scaling-an-easy-tutorial-63429ee9d0ae)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to Flatten your Swiss-Roll without Destroying It!!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@biman.pph?source=post_page-----63429ee9d0ae--------------------------------)[![Biman
    Chakraborty](../Images/c0bd6ee0a1b09456bd9e6aae0969da18.png)](https://medium.com/@biman.pph?source=post_page-----63429ee9d0ae--------------------------------)[](https://towardsdatascience.com/?source=post_page-----63429ee9d0ae--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----63429ee9d0ae--------------------------------)
    [Biman Chakraborty](https://medium.com/@biman.pph?source=post_page-----63429ee9d0ae--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----63429ee9d0ae--------------------------------)
    ·11 min read·Dec 11, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d4589e1299bc55b786feb6867522a9f.png)'
  prefs: []
  type: TYPE_IMG
- en: Swiss Roll Data (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: In my article on [**Principal Component Analysis (PCA) — An Easy Tutorial with
    Python**](https://medium.com/@biman.pph/principal-component-analysis-pca-an-easy-tutorial-with-python-c623b583cf29),
    I have discussed how PCA can be used to reduce the dimensionality of the data
    while reserving the distance between pairs of points as much as possible. I illustrated
    some examples with MNIST hand-written data sets and how PCA can reduce the dimensionality
    of the data from 784 to 35 and still being able to use supervised learning techniques
    with high degree of accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we start with an example of a simple **Swiss Roll** data in
    three dimension where the true manifold of the data has a dimension 2 and we will
    start with PCA.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Swiss Roll Dataset'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Figure 1 shows a simulated Swiss Roll data with 𝑛=2000 points using `sklearn`
    library. The scatter plot shows points with different colors lying in different
    parts of the spiral.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c0f92a0c3c6453b87bc572c07eb7af4b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Three dimensional view of the Swiss Roll Data (Image by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: We first apply PCA to this dataset and visualise the first two components in
    Figure 2\. We observe that it still retains the spiral shape of the data. The
    points in different sections of the spiral are not separable using linear boundaries
    and most of the classification methods will fail with the reduced data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/b485c9205012915ae3182f5198d4ff66.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: First two Principal Component Dimensions for Swiss Roll Data (Image
    by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: It does not unroll the underlying 2 dimensions. Why is it so? To understand,
    let us look into Figure 3, where the Euclidean distance between two points A and
    B are shown in blue dashed line. Though these two points are in completely different
    parts of the spiral, they are close to each other in Euclidean distance.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/5b02b9c1235fb1f2217eaf0ef5aa89b2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Geodesic distance versus Euclidean distance for the Swiss Roll Data
    (Image by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: In PCA, the Euclidean distances are preserved. However, the distance between
    these two points A and B along the spiral manifold is shown by the red line, which
    shows that these two points are far away in the manifold. The key difference is
    here that the manifold is not linear. Eucldean distance or PCA works quite well
    when we have linear manifolds. But quite often, the data is not on linear manifolds
    as is evident on this example dataset Other image data like even hand-written
    digits data are some good examples of non-linear manifolds of high dimensional
    data.
  prefs: []
  type: TYPE_NORMAL
- en: We need to define the distances differently to capture such differences. But
    before that let us first discuss how one can construct the principal components
    using the distances.
  prefs: []
  type: TYPE_NORMAL
- en: 'Principal Components: Mathematical Formulation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given a 𝑛×𝑝 data matrix 𝐗, the *principal component directions* are defined
    to be the 𝑝-dimensional orthonormal vectors along which the sample variance of
    𝐗 is successively maximized. For centered 𝐗, that is the sum of each column of
    𝐗 is 0, the 𝑘-th principal component direction is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b02776b66494e2f6bf72d88f9f2aee28.png)'
  prefs: []
  type: TYPE_IMG
- en: The 𝑛-dimensional vector 𝐗𝑣_𝑘 is called the 𝑘-th *principal component score*
    of 𝐗 and 𝑢_𝑘=(𝐗𝑣_𝑘)/𝑑_k is the normalized 𝑘-th principal component score, with
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8b1838ca409f0be310882ec2147e1a82.png)'
  prefs: []
  type: TYPE_IMG
- en: The quantity *d²_k/n* is the amount of variance explained by 𝑣_𝑘.
  prefs: []
  type: TYPE_NORMAL
- en: The *singular value decomposition* of 𝐗 as 𝐗 = 𝑈𝐷𝑉^⊤ describes all the prinicipal
    component scores and variances with 𝑈 being a 𝑛×𝑝 dimensional matrix with columns
    𝑢_1,𝑢_2,…,𝑢_𝑝, 𝑉 being a 𝑝×*p* dimensional matrix with columns 𝑣_1,𝑣_2,…,𝑣_𝑝 and
    𝐷 is 𝑝×𝑝 diagonal matrix with diagonal elements given by 𝑑_1,𝑑_2,…,𝑑_𝑝.
  prefs: []
  type: TYPE_NORMAL
- en: Let us consider the first 𝑘 principal components scores 𝐗𝑣_1=𝑑_1𝑢_1, …, 𝐗𝑣_𝑘=𝑑_𝑘𝑢_𝑘
    as the new feature vectors. Then we can write this as 𝐙=𝐗𝑉_𝑘=(𝑈𝐷)_𝑘, that is,
    the first 𝑘 columns of the matrix (𝑈𝐷) and think of **Z** as are new low-dimensional
    representation for 𝐗.
  prefs: []
  type: TYPE_NORMAL
- en: The rows 𝑧_1,…,𝑧_𝑛 of 𝐙 are the data points in this new low-dimensional representation.
    We have argued earlier that
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2def003874b70a1bf1da09cf5ff20bab.png)'
  prefs: []
  type: TYPE_IMG
- en: The Euclidean distance between the 𝑖 and 𝑗 points in the lower dimensional represnetation
    is approximately equal to the original Euclidean distance between these two points.
  prefs: []
  type: TYPE_NORMAL
- en: The Inner-Product Matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The 𝑛×𝑛 dimensional matrix 𝐗𝐗^⊤ is known as the *inner-product matrix* whose
    (𝑖,𝑗)-th element is given by *𝑥_i*^⊤*x_j*, the inner product between the 𝑖-th
    and the 𝑗-th rows of the matrix 𝐗. From above, we have
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/41e964d69a8d8b80a9a803a87cf9788f.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus we can write,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/348ef807fdd5f1fcd20c9b1cac3d53cd.png)'
  prefs: []
  type: TYPE_IMG
- en: This is called an *eigendecomposition* of 𝐗𝐗^⊤ because the columns of 𝑈 are
    eigenvectors of 𝐗𝐗^⊤. From this representation we can simply compute the eigendecomposition
    or *factorize* the inner product matrix 𝐗𝐗^⊤, and then the principal component
    scores are given by the columns of 𝑈𝐷, that is, 𝑑_𝑗𝑢_𝑗, 𝑗=1,…,𝑝. This shows that
    principal components scores can be computed if only the inner product matrix is
    given instead of the original data points.
  prefs: []
  type: TYPE_NORMAL
- en: Low Dimensional Representation from Distances Only
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Suppose that we only have the distances between the data points instead of the
    original data. That is, we have the Euclidean distances
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2ff5e219a62a1444a5dbe10678df0de8.png)'
  prefs: []
  type: TYPE_IMG
- en: or all 𝑖 and 𝑗. Can we still recover the principal component directions from
    these distances?
  prefs: []
  type: TYPE_NORMAL
- en: Let us first define a 𝑛×𝑛 dimensional distance matrix Δ with (𝑖,𝑗)-th element
    given by Δ_𝑖𝑗. We can recover the inner product matrix 𝐵=𝐗𝐗^⊤ from the distance
    matrix Δ.
  prefs: []
  type: TYPE_NORMAL
- en: Create the 𝑛×𝑛 matrix 𝐴 with its (𝑖,𝑗)-th element given by
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/94081d5c34cd40576399f4fbb621f06f.png)'
  prefs: []
  type: TYPE_IMG
- en: 2\. Double center 𝐴, that is, center both the columns and rows of 𝐴 to recover
    the matrix 𝐵 by using the transformation *B* = (*I* — *M*)*A*(*I* — *M*) where
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/245f14fda5cd995dadec21369c009103.png)'
  prefs: []
  type: TYPE_IMG
- en: Kernel PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kernel PCA simply mimics this procedure by replacing the inner product matrix
    𝐵 by the kernel matrix 𝐾=((𝐾_𝑖𝑗)), where 𝐾_𝑖𝑗=<𝜙(𝑥_𝑖),𝜙(𝑥_𝑗)>, the inner-product
    between the feature vectors 𝜙(𝑥_𝑖) and 𝜙(𝑥_𝑗). Here 𝜙 is a nonlinear map from
    ℝ^𝑝 → 𝐹, a feature space of arbitrary dimension. The idea is similar to the kernels
    in support vector machines (SVM) for classification problems. We are projecting
    the observations to a higher-dimensional space and then obtaining the principal
    components in that space. We can simply define, 𝐾_𝑖𝑗=Φ(𝑥_𝑖,𝑥_𝑗), where for the
    radial kernel,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dc02b10e3fbe8bb019f55ca574647ae4.png)'
  prefs: []
  type: TYPE_IMG
- en: and for the polynomial kernel,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4649f21b400fb488eb04b8c50bbe8c98.png)'
  prefs: []
  type: TYPE_IMG
- en: where 𝛾, 𝑐 and 𝑑 are the parameters of the respective kernel functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Define the 𝑛×𝑛 kernel inner product matrix 𝐾 as 𝐾=((Φ(𝑥_𝑖,𝑥_𝑗)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use eigendecomposition of 𝐾 to extract the eigenvalues and the eigenvectors
    of 𝐾.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The eigenvectors of 𝐾 will give the principal component scores.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is a nonlinear dimension reduction and we can illustrate the use of kernel
    PCA for our **Swiss Roll** data discussed in the previous example.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/92d3b95533f338cfa745aa90c02763ee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Kernel PCA Dimensions for the Swiss Roll Data (Image by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: In the above, we have used an `rbf` kernel with 𝛾 = 0.002\. Though the results
    improved from PCA, it still does not unroll the swiss roll, but picks the manifold
    very well.
  prefs: []
  type: TYPE_NORMAL
- en: We illustrate the kernel PCA with a different simulated data set below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/afb45d342afc007dc080d3b4c5129506.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Kernel PCA dimensions for the simulated data on the left. (Image
    by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: In the left, we have the simulated data with 3 concentric circles having uniform
    distribution with radius 1.0, 2.8 and 5.0 respectively. On the right hand side,
    we plot the kernel PCA components with `rbf` kernel and 𝛾=0.3\. We observe a nice
    separation of the three clusters of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Dimensional Scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have discussed in our [article on PCA](https://medium.com/@biman.pph/principal-component-analysis-pca-an-easy-tutorial-with-python-c623b583cf29)
    that it tries to preserve the distance between the observations in a lower dimensional
    representation. In other words, if 𝑧_1,𝑧_2,…,𝑧_𝑛 are the lower dimensional representation
    of 𝑥_1,𝑥_2,…,𝑥_𝑛, then PCA minimizes
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/406a8f6936d372b3d893200d8eceb320.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We now generalize this idea by defining a *stress* function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9275d4a9db046d34836840fed3fe2499.png)'
  prefs: []
  type: TYPE_IMG
- en: where 𝑑_𝑖𝑗 is a distance between 𝑥_𝑖 and 𝑥_𝑗. Usually, we chose Euclidean distances,
    but other distances can be used as well.
  prefs: []
  type: TYPE_NORMAL
- en: '**Multidimensional scaling** seeks values 𝑧_1,𝑧_2,…,𝑧_𝑛∈ℝ^𝑘 to minimize the
    *stress function*, 𝑆_𝑀(𝑧_1,𝑧_2,…,𝑧_𝑛).'
  prefs: []
  type: TYPE_NORMAL
- en: This is known as *least squares* or *Kruskal–Shephard scaling*. The idea is
    to find a lower-dimensional representation of the data that preserves the pairwise
    distances as well as possible. Notice that the approximation is in terms of the
    distances rather than squared distances.
  prefs: []
  type: TYPE_NORMAL
- en: Let us look into its implementation for the **Swiss Roll** data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/eeb6759c19e5ca3e289ddd9a0f4e497c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: First 2 dimensions of the classical multidimensional scaling. (Image
    by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: We observe that the results are very similar to the kernel PCA.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have not gone beyond Euclidian distances. But we mentioned earlier
    that in the Swiss roll data, the Euclidian distances are not ideal.
  prefs: []
  type: TYPE_NORMAL
- en: There is a class of methods which construct a fancier distance 𝑑_𝑖𝑗 between
    high-dimensional points 𝑥_1,…,𝑥_𝑛∈ℝ^𝑝, and then they feed these 𝑑_𝑖𝑗 through multidimensional
    scaling to get a low-dimensional representation 𝑧_1,…,𝑧_𝑛∈ℝ^𝑘. In this case, we
    don’t just get principal component scores, and our low-dimensional representation
    can end up being a *nonlinear function* of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Tangent distance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Tangent distance* is an example of a fancier metric that we can run through
    multidimensional scaling (though used elsewhere too).'
  prefs: []
  type: TYPE_NORMAL
- en: A motivating example is the *handwritten digits data* we used earlier. Here,
    we have *16 \times 16* images, treated as points 𝑥_𝑖∈ℝ²⁵⁶ (i.e., they are unraveled
    into vectors). If we take, e.g., a “3” and *rotate* it through a small angle,
    we would like for the rotated image to be considered close to the original image.
    This is not neccessarily true of Euclidean distance.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/18d00ffbd85d32f0aed9fe8fc806839b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Original images of “3” and a rotated image of “3” (Image by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: We could define Δ_𝑖𝑗^rotation to be the shortest Euclidean distance between
    a rotated version of 𝑥_𝑖 and rotated version of 𝑥_𝑗. However, you can immediately
    see that there is a problem in rotating the digits “6” and “9”.
  prefs: []
  type: TYPE_NORMAL
- en: We need something easier to calculate, and that restricts attention to *small
    rotations*. It helps to think of a set of rotations of an image as defining a
    curve in ℝ^𝑝 — an image 𝑥_𝑖 is a point in ℝ^𝑝, and as we rotate it in either directions,
    we get a curve.
  prefs: []
  type: TYPE_NORMAL
- en: '**Tangent distance** Δ_𝑖𝑗^tangent is defined by first computing the tangent
    line to each curve at the observed image, and then using the shortest Euclidean
    distance between tangent lines.'
  prefs: []
  type: TYPE_NORMAL
- en: Isometric Feature Mapping (Isomap)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Isometric feature mapping* (Isomap) learns structure in a more general setting
    to define distances. The basic idea is to construct a graph 𝐺=(𝑉,𝐸), i.e., construct
    edges 𝐸 between vertices 𝑉={1,…,𝑛}, based on the structure between 𝑥_1,…,𝑥_𝑛∈ℝ^𝑝
    . Then we define a graph distance Δ_𝑖𝑗^Isomap between 𝑥_𝑖 and 𝑥_𝑗, and use multidimensional
    scaling for our low-dimensional representation'
  prefs: []
  type: TYPE_NORMAL
- en: '**Constructing the graph**: for each pair 𝑖,𝑗, we connect 𝑖,𝑗 with an edge
    if either:'
  prefs: []
  type: TYPE_NORMAL
- en: 𝑥_𝑖 is one of 𝑥_𝑗’s 𝑚 nearest neighbors, or
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 𝑥_𝑗 is one of 𝑥_𝑖’s 𝑚 nearest neighbors The weight of this edge 𝑒 = {𝑖,𝑗} is
    then 𝑤_𝑒=‖𝑥_𝑖−𝑥_𝑗‖.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Defining graph distances**: now that we have built a graph, i.e., we have
    built an edge set 𝐸, we define the graph distance Δ_𝑖𝑗^Isomap between 𝑥_𝑖 and
    𝑥_𝑗 to be the shortest path in our graph from 𝑖 to 𝑗:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cc23df2584e0c951fd57658a7f031969.png)'
  prefs: []
  type: TYPE_IMG
- en: (This can be computed by, e.g., *Dijkstra’s algorithm* or *Floyd’s algorithm*)
  prefs: []
  type: TYPE_NORMAL
- en: Let us now look into its implementation for the **Swiss roll** data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/561bb0d2fdfe53172bb154e1baa0330b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Two dimensional representation of the isomap of the Swiss Roll data.
    (Image by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: With the number of neighbours 𝑚=7, the multodimemsional scaling with the **isomap**
    distances now unrolls the **Swiss roll** data.
  prefs: []
  type: TYPE_NORMAL
- en: Local Linear Embedding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another nonlinear dimension reduction method is **Local linear embedding** (LLE),
    which is a similar method in spirit but its details are very different. It doesn’t
    use multidimensional scaling.
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic idea has two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Learn a bunch of local approximations to the structure between 𝑥_1,…,𝑥_𝑛∈ℝ^𝑝
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Learn a low-dimensional representation 𝑧_1,…,𝑧_𝑛∈ℝ^𝑘 that best matches these
    local approximations
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is meant by such local approximations? We simply try to predict each 𝑥_𝑖
    by a linear function of nearby points 𝑥_𝑗 (hence the name local linear embedding).
  prefs: []
  type: TYPE_NORMAL
- en: For each 𝑥_𝑖, we first find its 𝑚 nearest neighbours, and collect their indices
    as N(𝑖). Then we build a weight vector 𝑤_𝑖∈ℝ^𝑛, setting 𝑤_𝑖𝑗=0 for 𝑗∉N(𝑖) and
    setting 𝑤_𝑖𝑗 for 𝑗∈N(𝑖) by minimizing
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a4923ea91402950b401d3d06a3d26653.png)'
  prefs: []
  type: TYPE_IMG
- en: Finally, we take these weights 𝑤_1,…,𝑤_𝑛∈ℝ^𝑛 and we fit the low-dimensional
    representation 𝑧_1,…,𝑧_𝑛∈ℝ^𝑘, by minimizing
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9eefa5853f41c28e0d40e8a4b2f49bd6.png)'
  prefs: []
  type: TYPE_IMG
- en: We again illustrate the use of *Local Linear Embedding* using the Swiss roll
    data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/72c3d9c2a1d5d397d5364b7eed5e0827.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Local Linear Embedding dimensions for the Swiss Roll data. (Image
    by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Local Linear Embedding* also yielded better dimension reduction than kernel
    PCA or classical MDS, though it is not as good as *Isomap*.'
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we have learnt a few nonlinear dimension reduction technique
    by generalizing the concepts from PCA. However, there is no single method which
    works best for all sort of data for dimension reduction. Depending on the nature
    of the data, we should decide about the dimension reduction technique to be used.
  prefs: []
  type: TYPE_NORMAL
- en: Hope, you have enjoyed the article!!
  prefs: []
  type: TYPE_NORMAL
- en: For consulting on any data science problems, contact [biman.pph@gmail.com](mailto:biman.pph@gmail.com)
  prefs: []
  type: TYPE_NORMAL
