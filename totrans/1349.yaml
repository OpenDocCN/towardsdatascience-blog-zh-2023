- en: Introduction to Logistic Regression in PySpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/introduction-to-logistic-regression-in-pyspark-9f894299c32d](https://towardsdatascience.com/introduction-to-logistic-regression-in-pyspark-9f894299c32d)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Tutorial to run your first classification model in Databricks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://gustavorsantos.medium.com/?source=post_page-----9f894299c32d--------------------------------)[![Gustavo
    Santos](../Images/a19a9f4525cdeb6e7a76cd05246aa622.png)](https://gustavorsantos.medium.com/?source=post_page-----9f894299c32d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9f894299c32d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9f894299c32d--------------------------------)
    [Gustavo Santos](https://gustavorsantos.medium.com/?source=post_page-----9f894299c32d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9f894299c32d--------------------------------)
    ·10 min read·Nov 4, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2728b2918920168e3a496a57722adcf0.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Ibrahim Rifath](https://unsplash.com/@ripeyyy_?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/stacked-round-gold-colored-coins-on-white-surface-OApHds2yEGQ?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Big Data. Large datasets. Cloud…
  prefs: []
  type: TYPE_NORMAL
- en: Those words are everywhere, following us around and in the thoughts of clients,
    interviewers, managers and directors. As data gets more and more abundant, datasets
    only increase in size in a manner that, sometimes, it is not possible to run a
    machine learning model in a local environment — in a single machine, in other
    words.
  prefs: []
  type: TYPE_NORMAL
- en: This matter requires us to adapt and find other solutions, such as modeling
    with Spark, which is one of the most used technologies for Big Data. Spark accepts
    languages such as SQL, Python, Scala, R and it has its own methods and attributes,
    including its own Machine Learning library [MLlib]. When you work with Python
    in Spark, it is called *PySpark*, for example.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, there’s a platform called *Databricks* that wraps Spark in a very
    well created layer that enables data scientists to work on it just like Anaconda.
  prefs: []
  type: TYPE_NORMAL
- en: When we’re creating a ML model in Databricks, it also accepts Scikit Learn models,
    but since we’re more interested in Big Data, this tutorial is all created using
    **Spark’s MLlib**, which is more suited for large datasets and also this way we
    add a new tool to our skill set.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The dataset for this exercise is already inside Databricks. It’s one of the
    UCI datasets, **Adults**, that is an extract from a Census and labeled with individuals
    that earn less or more than $50k per year. The data is publicly available under
    Creative Commons License in this address: [https://archive.ics.uci.edu/dataset/2/adult](https://archive.ics.uci.edu/dataset/2/adult)'
  prefs: []
  type: TYPE_NORMAL
- en: Our tutorial is to build a binary classifier that tells whether a person makes
    less or more than $50k of income in a year.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c7c84317730b74f4856d047dc7873036.png)'
  prefs: []
  type: TYPE_IMG
- en: Extract of the dataset. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Coding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, let’s go over each step of our model.
  prefs: []
  type: TYPE_NORMAL
- en: Here are the modules we need to import.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: And loading the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Preparing the Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we need to prepare the data. Although this is not the core subject of
    this tutorial, we still need to get rid of null values, for example, so that’s
    what we will do.
  prefs: []
  type: TYPE_NORMAL
- en: It is known from the dataset documentation that the NA values are actually marked
    as “ ?”. We will just go ahead and drop them because there aren’t too many null
    values, so it won’t affect our model’s performance. Therefore, here’s how to get
    rid of them. Any of these columns where we find a “?”, we drop it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Nice. Now let’s choose the best variables to use in our model.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since we have both categorical and numerical variables, the first step is to
    split them in separate datasets to be able to use the `UnivariateFeatureSelector`
    from Spark. This method uses Chi² test to select the best categorical variables
    with categorical labels and ANOVA to select numerical variables with categorical
    label, which are both of our cases.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first select the best categorical vars. We start by isolating the categorical
    variables. Then we use `RFormula` function to create a vectorized version of our
    dataset
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The resultant `vector_df` looks like this. Notice the last columns `features`
    and `label`. Those are the ones needed as input to the variable selector.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/12dc25072f659987585fab60ac055ba9.png)'
  prefs: []
  type: TYPE_IMG
- en: Vectorized version of the dataset. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Then we create an instance of the selector, inputting the `features` and the
    `label`, as mentioned previously. We then set the feature and label types and
    define how many “best” variables to fetch and fit it to the vectorized version
    of the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The same procedure can be done for the numerical features.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Ok. This is our final dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/49f419193f59a703b62243b5240e2148.png)'
  prefs: []
  type: TYPE_IMG
- en: Data with only the selected variables. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: As we have categorical variables, we will have to create dummy variables, since
    the Logistic Regression model in Spark MLlib requires only numbers as input. Let’s
    see how to do that next.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming the Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dummy variables are a way to transform categories in numbers for ML input. So,
    there are different ways to do that. The one applied here is [One Hot Encoding](https://en.wikipedia.org/wiki/One-hot)
    for multiclass variables.
  prefs: []
  type: TYPE_NORMAL
- en: This process will be applied only for categories, naturally, since the numbers
    are already…well…numbers.
  prefs: []
  type: TYPE_NORMAL
- en: To make it easier to work with the transformation, we will take advantage of
    the `Pipeline` method, which takes a list of steps and perform them all at once.
  prefs: []
  type: TYPE_NORMAL
- en: So we start by listing our selected categorical variables, then we create an
    empty list to store the stages to be performed by the Pipeline. Next comes a loop
    that takes each of the variables in the `cat_cols` list and pass it through the
    `StringIndexer` function to index the categories. The `OneHotEncoder` makes the
    final transformation.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is what the `StringIndexer` does:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d6b4c4183f5c942d0652d9edab9433db.png)'
  prefs: []
  type: TYPE_IMG
- en: String Indexer applied to a variable. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: And here is the code to create the loop.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Next step is to encode the label. A simple string indexer in this case will
    do just fine, since it is a binary label. We can use `StringIndexer` and add the
    instance to the Pipeline stages.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Lastly, we will add both numerical and categorical variables together. We can
    just add the string ‘classVec’ to each one hot encoded variable and put all the
    columns names as input for the `VectorAssembler` , and add that to the Pipeline
    `stages` list.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we run the Pipeline transformations all at once.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The result, if you’d like to see, looks like this.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6714a34c6b9c2d14328eaa4c7e4488c7.png)'
  prefs: []
  type: TYPE_IMG
- en: Transformed data. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: This data is now ready for input to the Logistic Regression Model.
  prefs: []
  type: TYPE_NORMAL
- en: Model Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before modeling, let’s split the data in train and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now we can fit the model to the training set.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: And creating some predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The output looks like this.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c0dafe21566ab2355ba97146776d4fc4.png)'
  prefs: []
  type: TYPE_IMG
- en: Predictions from the LR model from MLlib. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To evaluate our model, we use the `BinaryClassificationEvaluator` function.
  prefs: []
  type: TYPE_NORMAL
- en: Our model’s accuracy is 85%, which is apparently fair.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This is the Confusion Matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/91d785147c4531f5bdf572da1ef443db.png)'
  prefs: []
  type: TYPE_IMG
- en: Confusion Matrix of the model. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Here we already see that the **recall** is low. Or, *out of the actual people
    making >50k, how much I am getting right*. It’s almost a guess, with a poor 48%
    of correct predictions. The rate of true negatives is high, though, with 93%.
    However, that’s not all merit of our model, since the classes are unbalanced,
    with 75% of the observations labeled as class 0 (<50k).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Let’s try to improve this model.
  prefs: []
  type: TYPE_NORMAL
- en: One way to do that is to use the Cross Validation method. We first create a
    parameters grid, using the code below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Then we can train the models and print the best parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We can use the best model to predict or create a new one, if you want.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/18e4b7a5b87fa22276bedf9afe883118.png)'
  prefs: []
  type: TYPE_IMG
- en: Tuned model. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: We were able to improve the model a little. The hyperparameter that brings the
    most changes is the `threshold`, which moves the cutoff away from the 50% probability
    cut for classification. Once we looked at the first model, clearly it was not
    giving us good True Positive rates. So when we move the classification cutoff
    to a number smaller than 50%, it means that we are making it “easier” to classify
    observations as positive. On the other hand, this change comes with the cost of
    increasing the False Positives, but we are ok with that for now. Here are the
    new metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Precision: 0.63'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Recall: 0.62'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Specificity: 0.88'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can also look at the “ROC” metric.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/2b27e312a83e2d8e516c7a7eb5b0118e.png)'
  prefs: []
  type: TYPE_IMG
- en: ROC Curve for the model. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we end this tutorial.
  prefs: []
  type: TYPE_NORMAL
- en: Before You Go
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this tutorial, we went over how to create a Logistic Regression model using
    MLlib from Spark. That tool allows one to take advantage of cluster computing
    power and dealing with Big Data.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to know how to make a couple of transformations, like transforming
    the dataset to a vector to input to the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the full code of this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/gurezende/Studying/blob/master/PySpark/101_MyLearning%20ML%20on%20DataBricks.ipynb?source=post_page-----9f894299c32d--------------------------------)
    [## Studying/PySpark/101_MyLearning ML on DataBricks.ipynb at master · gurezende/Studying'
  prefs: []
  type: TYPE_NORMAL
- en: This is a repository with my tests and studies of new packages - Studying/PySpark/101_MyLearning
    ML on DataBricks.ipynb…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/gurezende/Studying/blob/master/PySpark/101_MyLearning%20ML%20on%20DataBricks.ipynb?source=post_page-----9f894299c32d--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: We can use Scikit Learn in Databricks as well, but knowing MLlib, which is a
    Spark module seems more appropriate to deal with Big Data.
  prefs: []
  type: TYPE_NORMAL
- en: If you liked this content, don’t forget to follow me here and on [LinkedIn](https://www.linkedin.com/in/gurezende/).
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://gustavorsantos.medium.com/?source=post_page-----9f894299c32d--------------------------------)
    [## Gustavo Santos - Medium'
  prefs: []
  type: TYPE_NORMAL
- en: Read writing from Gustavo Santos on Medium. Data Scientist. I extract insights
    from data to help people and companies…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: gustavorsantos.medium.com](https://gustavorsantos.medium.com/?source=post_page-----9f894299c32d--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Reference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[## Extracting, transforming and selecting features'
  prefs: []
  type: TYPE_NORMAL
- en: 'This section covers algorithms for working with features, roughly divided into
    these groups: Extraction: Extracting…'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: spark.apache.org](https://spark.apache.org/docs/latest/ml-features.html?source=post_page-----9f894299c32d--------------------------------#stringindexer)  [##
    One-hot - Wikipedia
  prefs: []
  type: TYPE_NORMAL
- en: From Wikipedia, the free encyclopedia In digital circuits and machine learning,
    a one-hot is a group of bits among…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: en.wikipedia.org](https://en.wikipedia.org/wiki/One-hot?source=post_page-----9f894299c32d--------------------------------)
    [](https://docs.databricks.com/en/machine-learning/train-model/mllib.html?source=post_page-----9f894299c32d--------------------------------)
    [## Use Apache Spark MLlib on Databricks
  prefs: []
  type: TYPE_NORMAL
- en: Learn how to train machine learning models using the Apache Spark MLlib Pipelines
    API in Databricks. Classification…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: docs.databricks.com](https://docs.databricks.com/en/machine-learning/train-model/mllib.html?source=post_page-----9f894299c32d--------------------------------)
    [](https://docs.databricks.com/en/_extras/notebooks/source/binary-classification.html?source=post_page-----9f894299c32d--------------------------------)
    [## Use Apache Spark MLlib on Databricks
  prefs: []
  type: TYPE_NORMAL
- en: Learn how to train machine learning models using the Apache Spark MLlib Pipelines
    API in Databricks. Classification…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: docs.databricks.com](https://docs.databricks.com/en/_extras/notebooks/source/binary-classification.html?source=post_page-----9f894299c32d--------------------------------)  [##
    LogisticRegression - PySpark 3.5.0 documentation
  prefs: []
  type: TYPE_NORMAL
- en: setParams(self, *, featuresCol="features", labelCol="label", predictionCol="prediction",
    maxIter=100, regParam=0.0…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: spark.apache.org](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.LogisticRegression.html?source=post_page-----9f894299c32d--------------------------------)
  prefs: []
  type: TYPE_NORMAL
