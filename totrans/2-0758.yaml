- en: Does Your LLM Pipeline Achieve Your Goal?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/does-your-llm-pipeline-achieve-your-goal-d033c944af8d](https://towardsdatascience.com/does-your-llm-pipeline-achieve-your-goal-d033c944af8d)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Explore what is most important to evaluate and how to measure it in your LLM
    pipeline.*'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@robertdegraaf78?source=post_page-----d033c944af8d--------------------------------)[![Robert
    de Graaf](../Images/ec3fbe8876fd9110a0455f9d1d6fe548.png)](https://medium.com/@robertdegraaf78?source=post_page-----d033c944af8d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d033c944af8d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d033c944af8d--------------------------------)
    [Robert de Graaf](https://medium.com/@robertdegraaf78?source=post_page-----d033c944af8d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d033c944af8d--------------------------------)
    ·8 min read·Jul 20, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f033bb7083e947a54ac9c1084bd68a11.png)'
  prefs: []
  type: TYPE_IMG
- en: AI Photo by [Piret Ilver](https://unsplash.com/es/@saltsup?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/98MbUldcDJY?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: One of the key ingredients needed to effectively implement an LLM pipeline is
    a way to evaluate the efficacy of your pipeline. That is you need to evaluate
    the final output that is the product of not just the LLM itself or the prompt
    but the interaction between the LLM, the prompt and settings such as temperature
    or minimum and maximum tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the boilerplate code to access the GPT API (autogenerated :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: There are seven arguments in the function to create the ‘response’, each of
    which alters the final output. Being able to choose the optimal combination of
    these outputs depends on being able to evaluate and differentiate outputs produced
    by different values of these arguments
  prefs: []
  type: TYPE_NORMAL
- en: This is a different problem to the LLM evaluations which are most commonly found
    in papers or on LLM makers’ websites . While it may be that you’re using an LLM
    that can pass the bar exam or similar test advertised in these sources, that doesn’t
    mean that your pipeline with the prompt you created and the settings you chose
    will necessarily summarise a collection of legal documents the way you need.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is especially the case when you are building a pipeline for an external
    user, and therefore can’t adjust the prompt on the fly. For example, suppose you
    want to use an LLM API to embed an LLM solution, and use a basic prompt skeleton
    to generate descriptions of particular items, such as in a catalogue. There are
    two levels to consider for suitablility:'
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, are the answers you generate fit for purpose?
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, can you rely on the answers continuing to be fit for purpose with
    future iterations?
  prefs: []
  type: TYPE_NORMAL
- en: In a sense the first can be assessed by looking at one or several answers in
    isolation. If you judge them to be suitable, you’re across the line. However,
    to assess the long term reliability of the LLM’s solution, you need to consider
    the variation in multiple answers.
  prefs: []
  type: TYPE_NORMAL
- en: We will look at this difference in more detail later on, but before we continue,
    we need to consider what fitness for purpose means in the context of an LLM.
  prefs: []
  type: TYPE_NORMAL
- en: What does fit for purpose even mean?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At its most basic level, fitness for purpose means that something achieves the
    the purpose it was designed to achieve. The difficulty in many tech applications
    being knowing what that goal is.
  prefs: []
  type: TYPE_NORMAL
- en: In relation to LLMs, there are a few common goals users often have, depending
    on how the LLM is being applied. Here are some common use cases people have for
    LLMs-
  prefs: []
  type: TYPE_NORMAL
- en: Summarize text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answer questions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate text descriptions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the use case of summarization, the criteria is how much of the important
    information remains at the end. This is a kind of fidelity or information loss.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of answering questions, accuracy is the likely measure — you can
    score the correct pipeline for how frequently the answer is correct.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of text descriptions, there is likely to be more than one criteria.
    Similar to summarisation or question answering, accuracy or fidelity is going
    to be important. However, you are likely to also want to grade the model on how
    easily the descriptions are understood or how closely the answers match your style
    requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'In their paper ‘[Large Language Models Encode Clinical Knowledge](https://arxiv.org/abs/2212.13138)’
    , Singhal, Azizi et al propose 10 criteria they use to evaluate LLM implementations
    in the health context. In some cases, such as ‘Extent of possible harm’, the criteria
    are highly specific to the health context. However, in many other cases, the criteria
    are widely applicable. For example, the following criteria suggested by this paper
    can be applied to a wide variety of LLM implementations in a wide variety of contexts:'
  prefs: []
  type: TYPE_NORMAL
- en: Does the answer contain any evidence of incorrect reading comprehension?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the answer contain any content it shouldn’t?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the answer omit any content it shouldn’t?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The complete list is worth studying, and available via arxiv in the link above.
  prefs: []
  type: TYPE_NORMAL
- en: The unfortunate news is that evaluating an implementation against a set of criteria
    like this is a laborious process, likely to involve some degree of manual scoring
    of the criteria, and requiring you to produce model outputs against multiple cases
    to get a full picture of the performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'To analyse the results of these investigations, you’re going to want to run
    statistics over effectively the whole matrix outputs, putting the task firmly
    into the realm of multivariate statistics. Without going deeply into this area
    to avoid writing an overly long piece, this will mean you need to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: The overall scores within an answer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The overall scores within a category
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The variance within each answer e.g. you want criteria scores within each answer
    to be within a tight band, rather than scoring really well on one criterion and
    poorly on another
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The variance within a criteria e.g. you want every answer to score within a
    tight band
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Post-Implementation Monitoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An additional factor to consider is that in addition to be inherently stochastic,
    so that a future answer may not be identical to the current answer, in many cases
    model owners are tweaking the models, which may either improve or degrade the
    output in your specific case.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, perceived degradation of ChatGPT and GPT4 in particular has been discussed
    in a variety of places such as the [GPT4 discussion forum](https://community.openai.com/t/experiencing-decreased-performance-with-chatgpt-4/234269).
    Various reasons have been forward for the degradation, but it is worth considering
    that in a tool as complicated and powerful as any LLM, which produces an output
    whose quality can only be measured subjectively, that there is no reason that
    a change that improves the experience for the majority of users won’t result in
    worse performance in any specific case.
  prefs: []
  type: TYPE_NORMAL
- en: It should also be noted, that some users in these discussions have suggested
    that the perceived degradation was not in fact degradation, but that a user had
    incorrectly decided that a one-off highly suitable answer was representative of
    what would always be achieved into the future when in fact the suitability of
    the answer was in part produced by chance — this highlights the importance of
    testing multiple cases and creating an evaluation that can account for the effect
    of probability.
  prefs: []
  type: TYPE_NORMAL
- en: Given that the process of producing and scoring answers is likely to be laborious,
    you may wish to avoid repeating the entire process every single time you look
    at your post-implementation efficacy. Hence, you are likely to want to choose
    a representative subset of the implementation tests that you can perform and then
    analyse on a semi-regular interval.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As an example of how this might work in a simple case, imagine you are creating
    a pipeline to summarise difficult to read memos. You want to create a pipeline
    that creates a clear, easy to read summary of these memos. You decide that the
    following factors are important:'
  prefs: []
  type: TYPE_NORMAL
- en: Accurate summary of the original
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ease of reading
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: No additional material added
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeatability in the sense of producing similar output each time from the same
    input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The last of these, repeatability, we don’t measure directly, but we measure
    by repeating the scoring process on several examples for the first three.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an illustration, imagine summarising a short piece of writing in the public
    domain — a passage from James Joyce’s Ulysses where Joyce describes the protagonist
    (Bloom) taking off his shoes and socks in a comically exaggerated difficult to
    read manner. Here is ChatGPT’s output and the passage to summarise itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2160ca0192064fe47e5c814a943d9905.png)'
  prefs: []
  type: TYPE_IMG
- en: Output from ChatGPT when submitted a short passage from James Joyce’s Ulysses,
    where the author describes the main character taking off his shoes after coming
    home
  prefs: []
  type: TYPE_NORMAL
- en: Similar outputs can be obtained by using the default settings in the GPT3.5
    API playground. These are the settings found in the code snippet found at the
    beginning of this article.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example output using this output is:'
  prefs: []
  type: TYPE_NORMAL
- en: ‘The text describes someone examining and treating their foot. They have a persistent
    ache and notice the marks and pressure points caused by walking. They then remove
    their shoes and socks to address a problem with their toenail, finding satisfaction
    in the familiarity of the scent and the routine of their nightly foot care.’
  prefs: []
  type: TYPE_NORMAL
- en: This is a solid summary, and on a scale of 1 -5 for each of the first three
    criteria I would give the output at least a 4\. These scores are very similar,
    so the variance from criterion to criterion is low. When I repeated the input
    with the same settings, although the output changed, it was at a very similar
    quality, so the variance from output to output was also very low, and therefore
    the repeatability was high.
  prefs: []
  type: TYPE_NORMAL
- en: I also tried the experiment after altering one parameter — I increased the temperature
    from 1 to 1.6.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first time I tried this experiement I received the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: ‘The text talks about a person examining their foot and treating it. They notice
    the deformations and pressure points from walking before taking off their boots
    and socks. They specifically address a problem with their toenail and gain satisfaction
    from the familiar scents associated with their nighttime foot care routine performed
    daily in prayerful contemplation.’
  prefs: []
  type: TYPE_NORMAL
- en: This output also scored highly across all criteria, and arguably (this is an
    inherently subjective process) captured the sense better when it mentioned ‘prayerful
    contemplation’. I would give this output a 4.5 for fidelity and still 4 for ease
    of reading and a 5 for no additional material.
  prefs: []
  type: TYPE_NORMAL
- en: 'I then submitted the request with precisely the same settings a second time
    and got this output:'
  prefs: []
  type: TYPE_NORMAL
- en: ‘The text describes the protagonist the continued process divesting themselves
    of their footwear and how they take care of their foot. They notice the markings
    left on their foot from walking and then proceed to remove their boots, socks,
    and part of the sock suspender. They recognize the scent of their foot as they
    pick at and discard a piece of their toenail, gaining a sense of satisfaction
    from the routine approach of their beloved evening footcare session. Their steps
    recast shadows as effulgency lies brightly forthcloud although enviti dunterations
    yet continua the fhericy ils prebaurentation yields speceliques ascistsomaticulations…
    read echo halted hollow flames disoluinnlancy rather therminansiology less pertirkeyahken
    inslead vitount the compse fiatic sanctiphall turlegisanitical pruncatheldomatis
    trabenbourineneticulatory formancentials exposedselucinvoundolicariety reconstricted
    devus … finispurninelile’
  prefs: []
  type: TYPE_NORMAL
- en: Even in the first sentence there is a problem with the grammar, and obviously
    by the end of the output it has degraded considerably. This output I would give
    a 3 for fidelity (it does actually capture most of the essence of the original
    similar to the other outputs), 3 for ease of reading (until it adds nonsense words
    is actually easier to read than the original — it could be saved by discarding
    all the words after ‘evening footcare session’) but 1 for not adding additional
    material.
  prefs: []
  type: TYPE_NORMAL
- en: In this case there is a great deal of difference in the quality of the two outputs,
    which is intuitive given that the temperature setting explicitly increases the
    variance in responses.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, while the difference in outputs is a little exaggerated here, it can
    be seen that scoring outputs on multiple criteria and scoring multiple outputs
    both contribute to understanding how effective the complete package of settings
    is to achieving the target of your LLM implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs are powerful tools, and harnessing that power often requires a developer
    to create a pipeline targeted at a particular use case. As text generating models,
    the quality of the output is an inherently subjective concept in many cases, and
    also frequently requires a multidimensional approach.
  prefs: []
  type: TYPE_NORMAL
- en: They are also subject to change over time, which could be good or bad. Designing
    a set of test problems that can be run and analysed quickly and easily will help
    your implementation stay performant for a long time to come.
  prefs: []
  type: TYPE_NORMAL
