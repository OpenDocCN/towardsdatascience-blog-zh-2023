- en: How Few-Shot Learning is Automating Document Labeling
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 少样本学习如何自动化文档标记
- en: 原文：[https://towardsdatascience.com/how-few-shot-learning-is-automating-document-labeling-43f9868c0f74](https://towardsdatascience.com/how-few-shot-learning-is-automating-document-labeling-43f9868c0f74)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-few-shot-learning-is-automating-document-labeling-43f9868c0f74](https://towardsdatascience.com/how-few-shot-learning-is-automating-document-labeling-43f9868c0f74)
- en: Leveraging GPT Model
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用GPT模型
- en: '[](https://walidamamou.medium.com/?source=post_page-----43f9868c0f74--------------------------------)[![Walid
    Amamou](../Images/c5ae089c59a5ff070f0f90ad63ee3817.png)](https://walidamamou.medium.com/?source=post_page-----43f9868c0f74--------------------------------)[](https://towardsdatascience.com/?source=post_page-----43f9868c0f74--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----43f9868c0f74--------------------------------)
    [Walid Amamou](https://walidamamou.medium.com/?source=post_page-----43f9868c0f74--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://walidamamou.medium.com/?source=post_page-----43f9868c0f74--------------------------------)[![Walid
    Amamou](../Images/c5ae089c59a5ff070f0f90ad63ee3817.png)](https://walidamamou.medium.com/?source=post_page-----43f9868c0f74--------------------------------)[](https://towardsdatascience.com/?source=post_page-----43f9868c0f74--------------------------------)[![数据科学前沿](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----43f9868c0f74--------------------------------)
    [Walid Amamou](https://walidamamou.medium.com/?source=post_page-----43f9868c0f74--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----43f9868c0f74--------------------------------)
    ·5 min read·Apr 7, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[数据科学前沿](https://towardsdatascience.com/?source=post_page-----43f9868c0f74--------------------------------)
    ·阅读时间5分钟·2023年4月7日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/6ec1bdbc7b5261ca7850c4adbf1af6e4.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6ec1bdbc7b5261ca7850c4adbf1af6e4.png)'
- en: Photo by [DeepMind](https://unsplash.com/@deepmind?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/Vqm8hzQIzic?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[DeepMind](https://unsplash.com/@deepmind?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)提供，来源于[Unsplash](https://unsplash.com/photos/Vqm8hzQIzic?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: Manual document labeling is a time-consuming and tedious process that often
    requires significant resources and can be prone to errors. However, recent advancements
    in machine learning, particularly the technique known as few-shot learning, are
    making it easier to automate the labeling process. Large Language Models (LLMs)
    in particular are excellent few shot learners thanks for their emergent capability
    in context learning.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 手动文档标记是一个耗时且繁琐的过程，通常需要大量资源且容易出错。然而，最近在机器学习方面的进展，特别是所谓的少样本学习技术，使得自动化标记过程变得更加容易。特别是大语言模型（LLMs）由于其在上下文学习中的新兴能力，是优秀的少样本学习者。
- en: In this article, we’ll take a closer look at how few-shot learning is transforming
    document labeling, specifically for Named Entity Recognition which is the most
    important task in document processing. We will show how the [UBIAI](https://ubiai.tools)’s
    platform is making it easier than ever to automate this critical task using few
    shot labeling techniques.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将详细探讨少样本学习如何改变文档标记，特别是对文档处理中最重要的任务——命名实体识别（NER）的影响。我们将展示[UBIAI](https://ubiai.tools)的平台如何通过少样本标记技术使自动化这一关键任务变得比以往更容易。
- en: What is Few-Shot Learning?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是少样本学习？
- en: 'Few-shot learning is a machine learning technique that enables models to learn
    a given task with only a few labeled examples. Without modifying its weights,
    the model can be tuned to perform a specific task by including concatenated training
    examples of these tasks in its input and asking the model to predict the output
    of a target text. Here is an example of few shot learning for the task of Named
    Entity Recognition (NER) using 3 examples:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 少样本学习是一种机器学习技术，使得模型能够仅用少量标记示例来学习给定的任务。在不修改其权重的情况下，模型可以通过在输入中包含这些任务的连接训练示例，并要求模型预测目标文本的输出，从而调整以执行特定任务。以下是使用3个示例进行命名实体识别（NER）任务的少样本学习示例：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The prompt typically begins by instructing the model to perform a specific task,
    such as “Extract entities from the following sentences without altering the original
    words.” Notice, we’ve added the instruction “without changing the original words”
    to prevent the LLM from hallucinating random texts, which it is notoriously known
    for. This has proven critical in obtaining consistent responses from the model.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 提示通常以指示模型执行特定任务开始，例如“从以下句子中提取实体而不改变原始词语。”请注意，我们添加了指示“无更改原始词语”以防止 LLM 产生随机文本，这是其著名的特性。这在获得一致的模型响应中至关重要。
- en: The few-shot learning phenomenon has been extensively studied in this [article](https://arxiv.org/abs/2303.07895),
    which I highly recommend. Essentially, the paper demonstrates that, under mild
    assumptions, the pretraining distribution of the model is a mixture of latent
    tasks that can be efficiently learned through in-context learning. In this case,
    in-context learning is more about identifying the task than about learning it
    by adjusting the model weights.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这种[现象](https://arxiv.org/abs/2303.07895)已在这篇文章中得到了广泛研究，我强烈推荐。实质上，论文表明，在温和的假设下，模型的预训练分布是潜在任务的混合，这些任务可以通过上下文学习高效地学习。在这种情况下，上下文学习更多的是关于识别任务，而不是通过调整模型权重来学习任务。
- en: Few-shot Labeling
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Few-shot 标注
- en: 'Few-shot learning has an excellent practical application in the data labeling
    space, often referred as few-shot labeling. In this case, we provide the model
    few labeled examples and ask it to predict the labels of the subsequent documents.
    However, integrating this capability in a functional data labeling platform is
    easier said than done, here are few challenges:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Few-shot 学习在数据标注领域具有出色的实际应用，通常被称为少量标注。在这种情况下，我们向模型提供少量已标注的示例，并要求它预测后续文档的标签。然而，将这一能力集成到功能齐全的数据标注平台中，难度远超想象，以下是一些挑战：
- en: LLMs are inherently text generators and tend to generate variable output. Prompt
    engineering is critical to make them create predictable output that can be later
    used to auto-label the data.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM 本质上是文本生成器，倾向于生成可变的输出。提示工程对于使其生成可预测的输出至关重要，这些输出可以用于自动标注数据。
- en: 'Token limitation: LLMs such as OpenAI’s GPT-3 is limited to 4000 tokens per
    request which limits the length of documents that can be sent at once. Chunking
    and splitting the data before sending the request becomes essential.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Token 限制：如 OpenAI 的 GPT-3 这样的 LLM 每次请求的 token 数量限制为 4000 个，这限制了可以一次发送的文档长度。在发送请求之前，分块和拆分数据变得至关重要。
- en: 'Span offset calculation: After receiving the output from the model, we need
    to search its occurrence in the document and label it correctly.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Span 偏移计算：在从模型接收输出后，我们需要在文档中搜索其出现位置并正确标注。
- en: Few Shot Labeling with UBIAI
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 UBIAI 进行 Few-shot 标注
- en: We’ve recently added few shot labeling capability by integrating [OpenAI’s GPT-3
    Davinci](https://platform.openai.com/docs/models/gpt-3) with [UBIAI annotation
    tool](https://ubiai.tools). The tool currently support few-shot NER task for unstructured
    and semi-structured documents such as PDFs and scanned images.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最近通过将[OpenAI 的 GPT-3 Davinci](https://platform.openai.com/docs/models/gpt-3)与[UBIAI
    标注工具](https://ubiai.tools)集成，新增了少量标注能力。该工具目前支持对未结构化和半结构化文档（如 PDF 和扫描图像）的 few-shot
    NER 任务。
- en: 'To get started:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 开始使用：
- en: Simply label 1–5 examples
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 只需标注 1–5 个示例
- en: Enable few-shot GPT model
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用 few-shot GPT 模型
- en: Run prediction on a new unlabeled document
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在新的未标注文档上运行预测
- en: 'Here is an example of few shot NER on job description with 5 examples provided:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在提供 5 个示例的工作描述上进行 few-shot NER 的一个示例：
- en: '![](../Images/7d8f055d072d89a4e75d697c6971f880.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7d8f055d072d89a4e75d697c6971f880.png)'
- en: 'Image by Author: Few Shot NER on unstructured text'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图片作者：非结构化文本上的 Few Shot NER
- en: The GPT model accurately predicts most entities with just five in-context examples.
    Because LLMs are trained on vast amounts of data, this few-shot learning approach
    can be applied to various domains, such as legal, healthcare, HR, insurance documents,
    etc., making it an extremely powerful tool.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 模型仅凭五个上下文示例就能准确预测大多数实体。由于 LLM 在大量数据上进行训练，这种 few-shot 学习方法可以应用于各种领域，如法律、医疗、HR、保险文档等，使其成为一个极其强大的工具。
- en: However, the most surprising aspect of few-shot learning is its adaptability
    to semi-structured documents with limited context. In the example below, I provided
    GPT with only one labeled OCR’d invoice example and asked it to label the next.
    The model surprisingly predicted many entities accurately. With even more examples,
    the model does an exceptional job of generalizing to semi-structured documents
    as well.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，少量样本学习最令人惊讶的方面是它对上下文有限的半结构化文档的适应能力。在下面的示例中，我仅提供了一个标记的OCR发票示例，并要求它标记下一个。模型出乎意料地准确地预测了许多实体。即使有更多的示例，模型在对半结构化文档的泛化方面也表现得非常出色。
- en: '![](../Images/eca7b6c115a052f7f414c0eec993e0fe.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eca7b6c115a052f7f414c0eec993e0fe.png)'
- en: 'Image by Author: Few Shot NER on PDF'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像：PDF上的少量样本命名实体识别（NER）
- en: 'For an in-depth tutorial of the few-shot labeling feature, checkout the video
    below:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 有关少量样本标记功能的详细教程，请查看下面的视频：
- en: UBIAI’s Few Shot Labeling Tutorial
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: UBIAI的少量样本标记教程
- en: 'Conclusion:'
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论：
- en: Few-shot learning is revolutionizing the document labeling process. By integrating
    few-shot labeling capabilities into functional data labeling platforms, such as
    UBIAI’s annotation tool, it is now possible to automate critical tasks like Named
    Entity Recognition (NER) in unstructured and semi-structured documents. This does
    not imply that LLMs will replace human labelers anytime soon. Instead, they augment
    their capabilities by making them more efficient. With the power of few-shot learning,
    LLMs can label vast amounts of data and apply to multiple domains, such as legal,
    healthcare, HR, and insurance documents, to train smaller and more accurate specialized
    models that can be efficiently deployed.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 少量样本学习正在彻底改变文档标记过程。通过将少量样本标记功能集成到功能性数据标记平台中，如UBIAI的注释工具，现在可以自动化诸如命名实体识别（NER）等关键任务，无论是在非结构化还是半结构化文档中。这并不意味着大型语言模型（LLMs）会很快取代人工标注员。相反，它们通过提高效率来增强他们的能力。凭借少量样本学习的力量，大型语言模型可以标记大量数据，并应用于法律、医疗、HR和保险等多个领域，从而训练出更小、更准确的专业化模型，这些模型可以高效部署。
- en: We’re currently adding support for few-shot relation extraction and document
    classification, stay tuned!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们目前正在添加对少量样本关系提取和文档分类的支持，请继续关注！
- en: Follow us on Twitter [@UBIAI5](https://twitter.com/UBIAI5) or [subscribe here](https://walidamamou.medium.com/subscribe)!
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在Twitter上关注我们 [@UBIAI5](https://twitter.com/UBIAI5) 或 [点击这里订阅](https://walidamamou.medium.com/subscribe)!
