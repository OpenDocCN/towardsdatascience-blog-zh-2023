["```py\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.utils.data as data_utils\n\nimport cv2\nimport numpy as np\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n```", "```py\nclass ExampleDataset(Dataset):\n    def __init__(self, \n                data, \n                transform = None):\n        self.file_paths = data['file_paths'].values\n        self.labels = data['labels'].values\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.file_paths)\n\n    def __getitem__(self, idx):\n        # Get file_path and label for index\n        label = self.labels[idx]\n        file_path = self.file_paths[idx]\n\n        # Read an image with OpenCV\n        image = cv2.imread(file_path)\n\n        # Convert the image to RGB color space.\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        # Apply augmentations\n        if self.transform:\n            augmented = self.transform(image=image)\n            image = augmented['image']\n\n        return image, label \n```", "```py\ntransforms = A.Compose([\n    A.Resize(256, 256), # Resize images\n    ToTensorV2()])\n\nexample_dataset = ExampleDataset(train_df,\n                                 transform = transforms)\n\ntrain_dataloader = DataLoader(example_dataset, \n                              batch_size = 8, \n                              shuffle = True, \n                              num_workers = 0)\n```", "```py\n# Define device, model, optimizer, learning rate scheduler\ndevice = ...\nmodel = ...\ncriterion = nn.CrossEntropyLoss()\noptimizer = ... \nscheduler = ...\n\nfor epoch in range(NUM_EPOCHS):        \n    # Train\n    model.train()\n\n    # Define any variables for metrics\n    ...\n\n    # Iterate over data\n    for samples, labels in (train_dataloader):\n\n        samples, labels = samples.to(device), labels.to(device)\n\n        # Normalize\n        samples = samples/255\n\n        # Zero the parameter gradients\n        ...\n\n        with torch.set_grad_enabled(True):\n            # Forward: Get model outputs and calculate loss\n            output = model(samples)\n\n            loss = criterion(output, labels)\n\n            # Backward: Optimize, step optimizer and calculate predictions\n            ...\n\n        # Step scheduler\n        ...\n\n        # Calculate any statistics\n        ...\n\n    # Validate \n    ...\n```", "```py\ntransforms_cutout = A.Compose([\n    A.Resize(256, 256), \n    A.CoarseDropout(max_holes = 1, # Maximum number of regions to zero out. (default: 8)\n                    max_height = 128, # Maximum height of the hole. (default: 8) \n                    max_width = 128, # Maximum width of the hole. (default: 8) \n                    min_holes=None, # Maximum number of regions to zero out. (default: None, which equals max_holes)\n                    min_height=None, # Maximum height of the hole. (default: None, which equals max_height)\n                    min_width=None, # Maximum width of the hole. (default: None, which equals max_width)\n                    fill_value=0, # value for dropped pixels.\n                    mask_fill_value=None, # fill value for dropped pixels in mask. \n                    always_apply=False, \n                    p=0.5\n                   ),\n    ToTensorV2(),\n])\n```", "```py\n# Copied and edited from https://www.kaggle.com/code/riadalmadani/fastai-effb0-base-model-birdclef2023\ndef mixup(data, targets, alpha):\n    indices = torch.randperm(data.size(0))\n    shuffled_data = data[indices]\n    shuffled_targets = targets[indices]\n\n    lam = np.random.beta(alpha, alpha)\n    new_data = data * lam + shuffled_data * (1 - lam)\n    new_targets = [targets, shuffled_targets, lam]\n    return new_data, new_targets\n```", "```py\n# Copied and edited from https://www.kaggle.com/code/riadalmadani/fastai-effb0-base-model-birdclef2023\ndef mixup_criterion(preds, targets):\n    targets1, targets2, lam = targets[0], targets[1], targets[2]\n    criterion = nn.CrossEntropyLoss()\n    return lam * criterion(preds, targets1) + (1 - lam) * criterion(preds, targets2)\n```", "```py\nfor epoch in range(NUM_EPOCHS):        \n    # Train\n    model.train()\n\n    # Define any variables for metrics\n    ...\n\n    for samples, labels in (train_dataloader):\n\n        samples, labels = samples.to(device), labels.to(device)\n\n        # Normalize\n        samples = samples/255\n\n        ############################\n        # Apply Mixup augmentation #\n        ############################\n        p = np.random.rand()\n        if p < p_mixup:\n            samples, labels = mixup(samples, labels, 0.8)\n\n        # Zero the parameter gradients\n        ...\n\n        with torch.set_grad_enabled(True):\n            # Forward: Get model outputs and calculate loss\n            output = model(samples)\n\n            ############################\n            # Apply Mixup criterion    #\n            ############################      \n            if p < p_mixup:\n                loss = mixup_criterion(output, labels)\n            else:\n                loss = criterion(output, labels) \n\n            # Backward: Optimize, step optimizer and calculate predictions\n            ...\n\n        # Step scheduler, Calculate any statistics, validate\n        ...\n```", "```py\n# Copied and edited from https://www.kaggle.com/code/riadalmadani/fastai-effb0-base-model-birdclef2023\ndef cutmix(data, targets, alpha):\n    indices = torch.randperm(data.size(0))\n    shuffled_data = data[indices]\n    shuffled_targets = targets[indices]\n\n    lam = np.random.beta(alpha, alpha)\n    bbx1, bby1, bbx2, bby2 = rand_bbox(data.size(), lam)\n    data[:, :, bbx1:bbx2, bby1:bby2] = data[indices, :, bbx1:bbx2, bby1:bby2]\n    # adjust lambda to exactly match pixel ratio\n    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (data.size()[-1] * data.size()[-2]))\n\n    new_targets = [targets, shuffled_targets, lam]\n    return data, new_targets\n\ndef rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    cut_rat = np.sqrt(1\\. - lam)\n    cut_w = int(W * cut_rat)\n    cut_h = int(H * cut_rat)\n\n    # uniform\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n\n    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n    bby1 = np.clip(cy - cut_h // 2, 0, H)\n    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n    bby2 = np.clip(cy + cut_h // 2, 0, H)\n\n    return bbx1, bby1, bbx2, bby2\n```"]