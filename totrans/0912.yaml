- en: Follow This Data Validation Process to Improve Your Data Science Accuracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/follow-this-data-validation-process-to-improve-your-data-science-accuracy-99422dfbee72](https://towardsdatascience.com/follow-this-data-validation-process-to-improve-your-data-science-accuracy-99422dfbee72)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: When training and inference data come from different sources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://datascience2.medium.com/?source=post_page-----99422dfbee72--------------------------------)[![Matt
    Przybyla](../Images/3b9e714e012e8846b866e4e4b5d689d7.png)](https://datascience2.medium.com/?source=post_page-----99422dfbee72--------------------------------)[](https://towardsdatascience.com/?source=post_page-----99422dfbee72--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----99422dfbee72--------------------------------)
    [Matt Przybyla](https://datascience2.medium.com/?source=post_page-----99422dfbee72--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----99422dfbee72--------------------------------)
    ·8 min read·Sep 1, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/87e3892c3b9c9962ecad60b869f4c936.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [NordWood Themes](https://unsplash.com/@nordwood?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/E9tFH39iRPE?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    [1].
  prefs: []
  type: TYPE_NORMAL
- en: Table of Contents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enabling Data Collection
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Setting a Baseline
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Detecting Outliers
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This article is intended for data scientists who are either beginning or want
    to improve their current data validation process, serving as a general outline
    with some examples. First, I want to define data validation here as it can have
    different meanings for other, similar job roles. For the purpose of this article,
    we will say that data validation is the process of ensuring the training data
    used for your model matches or is in line with inference data. For some companies
    and some use cases, you will not need to worry about this issue if the data is
    coming from the same source. Therefore, this process must occur and is only useful
    when data is coming from different sources. Some of the reasons why data wouldn’t
    be coming from the same source is if your training data is historical and custom-made
    (*ex: features derived from existing data*), and/or your inference data is coming
    from live tables where the training is snapshot data. All that to say, there are
    plenty of reasons for this mismatch to be present and it will be incredibly beneficial
    to come up with a process at scale to ensure the data you are feeding your model
    at inference is what you — aka the trained model data expects.'
  prefs: []
  type: TYPE_NORMAL
- en: Enabling Data Collection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/2634a8dbe8aa2ffc3c19183c16a3c2be.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Dennis Kummer](https://unsplash.com/@dekubaum?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/52gEprMkp7M?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    [2].
  prefs: []
  type: TYPE_NORMAL
- en: There are plenty of ways you can enable data collection. But once again, first,
    we want to define the ***data*** that is collected, which would be the inference
    data. We expect to have our training data (*composed of both train and test splits*)
    already located somewhere, perhaps in S3, a file storage tool, in a temporary
    table in a database, even a CSV file, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: However, it is more common to have your inference data used at the time of prediction,
    but not necessarily stored. For this reason, you would want to enable the collection
    of it. Similar to the training data examples, you could store the inference data
    in a database table, data lake, CSV file, JSON file, S3, a stored Kafka stream,
    and so on. It will depend on what is available to you at your company for choosing
    which collection route you want to pursue.
  prefs: []
  type: TYPE_NORMAL
- en: There are also tools, libraries, and Python functions that allow the enabling
    of inference data storage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some tools you can use to collect your inference data:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Kafka stream into a database**: your service endpoint that calls the model
    can load the data into the Kafka stream, and from there, you can load that into
    a database table'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Amazon Web Services Capture Content**](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-data-capture-endpoint.html)
    **[3]**: this tool is very user-friendly and allows you to add a parameter to
    your model code. You will have to use Amazon SageMaker to use this tool.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An example of the code looks like this, you can see how simple it is to add
    the parameter to the AWS SageMaker model code, if that is the tool you are using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: For specific tools in detail, I’ll expound upon that in my next article and
    focus on the code for it as well. For now, in your data validation plan, you will
    want to ensure you have a way to have inference data captured and stored, as well
    as your training data.
  prefs: []
  type: TYPE_NORMAL
- en: Setting a Baseline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/06941b11a8cd7ff4c47ded293e8c13f7.png)'
  prefs: []
  type: TYPE_IMG
- en: Variables example. Screenshot by Author [4].
  prefs: []
  type: TYPE_NORMAL
- en: A baseline is useful to understand your training data. It can be overwhelming
    to understand how your data is supposed to look visually and remember it, so it
    is best to create an automated process of deriving descriptive statistics from
    your data.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of baseline data
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'For **categorical columns** (*string, object, category, etc.*), you can create
    a baseline for known data and for if you expect missing data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Unique X-Column Values, ex: “*August, September*”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For **numerical columns** (*integer, float*):'
  prefs: []
  type: TYPE_NORMAL
- en: Column X Average
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Column Y Standard Deviation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Column X, Y Minimum, Maximum, etc., ex: 20,000.60'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While this seems fairly simple, when your models and data become larger, it
    will be beneficial to keep track of your data early on so that you can understand
    why and when you have an outlier in your inference data.
  prefs: []
  type: TYPE_NORMAL
- en: The above was a general example of creating baseline statistics, so below will
    be a specific tool that is very easy to use t
  prefs: []
  type: TYPE_NORMAL
- en: 'Pandas Profiling:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This [Python library](https://medium.com/r?url=https%3A%2F%2Fgithub.com%2Fpandas-profiling%2Fpandas-profiling)
    [5] performs a lot of exploratory work for you, which would serve as the information
    that is your data baseline. As an overview, this library looks at the following
    types of baseline statistics.
  prefs: []
  type: TYPE_NORMAL
- en: '**Quantile**:'
  prefs: []
  type: TYPE_NORMAL
- en: Minimum
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5th percentile
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q1, Median, Q3, 95th percentile
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximum
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Range
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interquartile range (IQR)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Descriptive**:'
  prefs: []
  type: TYPE_NORMAL
- en: Standard deviation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coefficient of variation (CV)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kurtosis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mean, Median Absolute Deviation (MAD)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Skewness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sum
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monotonicity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Detecting Outliers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/bbb7887887a662906fb7f0d4e7988ba0.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Will Myers](https://unsplash.com/@will_myers?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/ku_ttDpqIVc?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    [6].
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have both your training data and inference data collected, and
    your expected baseline values, you can know when a value at inference becomes
    an outlier.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to keep in mind that the training data itself is assumed to
    be 100% correct since it is used as the source of truth.
  prefs: []
  type: TYPE_NORMAL
- en: With the inference data collected, you can use plenty of tools, libraries, modules,
    etc., to compare your inference data to your training data.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate the point further, I’ll give two examples, one where we wouldn’t
    have an issue, and the other would be the outlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of a non-outlier for both categorical and numerical data:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Training data categorical feature (*months*) → “*August*”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inference data categorical feature (*months*) → “*September*”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training data numerical feature (*temperature*) → 100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inference data numerical feature (*temperature*) → 110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is an example of an outlier for both categorical and numerical data:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Training data categorical feature (*months)* → “*August*”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inference data categorical feature (*months)* → “*AUgust*”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training data numerical feature (*temperature)* → 100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inference data numerical feature (*temperature)* → 1000000000
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once again it might seem obvious with these few examples, but imagine you have
    50+ features and 100,000 rows of data. It would be virtually impossible to check
    all your data is what you expect it to be.
  prefs: []
  type: TYPE_NORMAL
- en: Also, to highlight the example more, while “*August*” vs “*AUgust*” seems similar,
    the model would of course not treat it as the same value. For numerical, we wouldn’t
    want our temperature to read that high as well.
  prefs: []
  type: TYPE_NORMAL
- en: While these values are clearly incorrect at inference, the model can still predict
    on them (*depending on your model*), which makes it even more difficult to know
    when it is happening.
  prefs: []
  type: TYPE_NORMAL
- en: As a bonus, you would want to set up a process to ensure your predictions are
    in line with what you expect.
  prefs: []
  type: TYPE_NORMAL
- en: For a specific example, you can use the ***Amazon SageMaker Model Monitoring***
    tool, or if you are outside of that platform, you can use an open-source library
    like [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started)
    [7].
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the main use cases for why you would want to use this specific library
    [7], and to echo what I have said above, AWS explains the reasoning well here:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: “*Validating new data for inference to make sure that we haven’t suddenly started
    receiving bad features*”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It can be incredibly important to know when outliers are occurring because your
    model could be predicting something that could end up causing harm if predicted
    poorly due to inaccurate data. You do not want to have this issue for say, fraud
    detection involving security.
  prefs: []
  type: TYPE_NORMAL
- en: This article served as a quick outline and reminder to make sure your data is
    not only good for training your model but also for the time of prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, here is what we discussed in this article:'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: I hope you found my article both interesting and useful. Please feel free to
    comment down below if your experiences are the same or different for data validation.
    Why or why not? What other things do you think should be discussed more? These
    can certainly be clarified even further, but I hope I was able to shed some light
    on the common outline for data validation as a data scientist. Feel free to also
    comment if you would like to see an article on specific tools for this process.
  prefs: []
  type: TYPE_NORMAL
- en: '***I am not affiliated with any of these companies.***'
  prefs: []
  type: TYPE_NORMAL
- en: '*Please feel free to check out my profile,* [Matt Przybyla](https://medium.com/u/abe5272eafd9?source=post_page-----99422dfbee72--------------------------------),
    *and other articles, as well as subscribe to receive email notifications for my
    blogs by following the link below, or by* ***clicking on the subscribe icon on
    the top of the screen by the follow icon****, and reach out to me on LinkedIn
    if you have any questions or comments.*'
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading!
  prefs: []
  type: TYPE_NORMAL
- en: '**Subscribe link:** [https://datascience2.medium.com/subscribe](https://datascience2.medium.com/subscribe)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Referral link:** [https://datascience2.medium.com/membership](https://datascience2.medium.com/membership)'
  prefs: []
  type: TYPE_NORMAL
- en: (*I will receive a commission if you sign up for a membership on Medium*)
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] Photo by [NordWood Themes](https://unsplash.com/@nordwood?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/E9tFH39iRPE?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText),
    (2018)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [Matt Przybyla](https://medium.com/u/abe5272eafd9?source=post_page-----99422dfbee72--------------------------------),
    Screenshot of Variables example, (2020)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] 2023, Amazon Web Services, Inc. or its affiliates. All rights reserved,
    [Capture data from real-time endpoint](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-data-capture-endpoint.html),
    (2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] Photo by [Stephen Dawson](https://unsplash.com/@dawson2406?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/qwtCeJ5cLYs?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText),
    (2018)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] pandas-profiling, [GitHub for documentation and all contributors](https://github.com/pandas-profiling/pandas-profiling),
    (2020)'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] Photo by [Will Myers](https://unsplash.com/@will_myers?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/ku_ttDpqIVc?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText),
    (2018)'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] TensorFlow, Except as otherwise noted, the content of this page is licensed
    under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/),
    and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0).
    For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies).
    Java is a registered trademark of Oracle and/or its affiliates., [TensorFlow Data
    Validation](https://www.tensorflow.org/tfx/data_validation/get_started), (2023)'
  prefs: []
  type: TYPE_NORMAL
