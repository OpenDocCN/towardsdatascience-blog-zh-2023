- en: 'Unsupervised Learning Series: Exploring Hierarchical Clustering'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/unsupervised-learning-series-exploring-hierarchical-clustering-15d992467aa8](https://towardsdatascience.com/unsupervised-learning-series-exploring-hierarchical-clustering-15d992467aa8)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s explore how hierarchical clustering works and how it builds clusters based
    on pairwise distances.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ivopbernardo.medium.com/?source=post_page-----15d992467aa8--------------------------------)[![Ivo
    Bernardo](../Images/39887b6f3e63a67c0545e87962ad5df0.png)](https://ivopbernardo.medium.com/?source=post_page-----15d992467aa8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----15d992467aa8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----15d992467aa8--------------------------------)
    [Ivo Bernardo](https://ivopbernardo.medium.com/?source=post_page-----15d992467aa8--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----15d992467aa8--------------------------------)
    ·11 min read·Jun 20, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1ad73696743c551e19d34fe0ff09b9b0.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by Nathan Anderson @unsplash.com
  prefs: []
  type: TYPE_NORMAL
- en: In my last post of the Unsupervised Learning Series, we explored one of the
    most famous clustering methods, the [K-means Clustering](https://medium.com/towards-data-science/unsupervised-learning-method-series-exploring-k-means-clustering-d129fff3ab6a).
    In this post, we are going to discuss the methods behind another important clustering
    technique — **hierarchical clustering**!
  prefs: []
  type: TYPE_NORMAL
- en: This method is also based on distances (euclidean, manhattan, etc.) and uses
    an hierarchical representation of data to combine data points. Contrary to *k-means,*
    it does not contain any hyperparameter regarding the number of centroids (such
    as *k*) that data scientists can configure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mostly, hierarchical clustering can be divided into two groups: agglomerative
    clustering and divisive clustering. In the first, data points are considered single
    units and are aggregated to nearby data points based on distances. In the latter,
    we consider all data points as a single cluster and start to divide them based
    on certain criteria. As the agglomerative version is the most famous and widely
    used ([sklearn’s built-in implementation follows this protocol](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html´)),
    that is the hierarchical type we are going to explore in this post.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this blog post we’ll approach Agglomerative Hierarchical Clustering in two
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we’ll do a play-by-play analysis of how hierarchies are built, using
    agglomerative clustering with the *average* method (one of the methods we can
    use to build the hierarchy of data points).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, we’ll see some examples of how to fit a hierarchical clustering on a real
    dataset using *sklearn’s* implementation. This is also where we’ll detail other
    methods we can use to build our hierarchies (*ward, minimum, etc.*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start!
  prefs: []
  type: TYPE_NORMAL
- en: Agglomerative Clustering Example — Step by Step
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In our step-by-step example, we are going to use a fictional dataset with 5
    customers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d0f507a52c1a87959d8c20acbd598fe.png)'
  prefs: []
  type: TYPE_IMG
- en: Hierarchical Clustering Example — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s imagine that we run a shop with 5 customers and we wanted to group these
    customers based on their similarities. We have two variables that we want to consider:
    the *customer’s age* and their *annual income*.'
  prefs: []
  type: TYPE_NORMAL
- en: '**The first step of our agglomerative clustering consists of creating pairwise
    distances between all our data points.** Let’s do just that by representing each
    data point by their coordinate in a [x, y] format:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Distance between [60, 30] and [60, 55]: **25.0**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Distance between [60, 30] and [30, 75]: **54.08**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Distance between [60, 30] and [41, 100]: **72.53**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Distance between [60, 30] and [38, 55]: **33.30**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Distance between [60, 55] and [30, 75]: **36.06**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Distance between [60, 55] and [41, 100]: **48.85**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Distance between [60, 55] and [38, 55]: **22.0**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Distance between [30, 75] and [41, 100]: **27.31**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Distance between [30, 75] and [38, 55]: **21.54**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Distance between [41, 100] and [38, 55]: **45.10**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although we can use any type of distance metric we want, we’ll use [euclidean](https://en.wikipedia.org/wiki/Euclidean_distance)
    due to its simplicity. From the pairwise distances we’ve calculated above, which
    distance is the smallest one?
  prefs: []
  type: TYPE_NORMAL
- en: '**The distance between middle aged customers that make less than 90k dollars
    a year — customers on coordinates [30, 75] and [38, 55]!**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reviewing the formula for euclidean distance between two arbitrary points *p1*
    and *p2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7ce3ab7b2ae103b307ef2f97f38e805b.png)'
  prefs: []
  type: TYPE_IMG
- en: Euclidean Distance Formula — image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s visualize our smallest distance on the 2-D plot by connecting the two
    customers that are nearer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4db474d8ffd6e89e314f7d5b4499c89c.png)'
  prefs: []
  type: TYPE_IMG
- en: Connecting the two closest customers — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: '**The next step of hierarchical clustering is to consider these two customers
    as our first cluster!**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/795dc268cf3e64281ad7e78bc5610762.png)'
  prefs: []
  type: TYPE_IMG
- en: Considering closest customers as one cluster — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we are going to calculate the distances between the data points, again.
    **But this time, the two customers that we’ve grouped into a single cluster will
    be treated as a single data point**. For instance, consider the red point below
    that positions itself in the middle of the two data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/89f31736bf227760d31178f8cd98b8e8.png)'
  prefs: []
  type: TYPE_IMG
- en: Considering closest customers as one cluster — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: In summary, for the next iterations of our hierarchical solution, we won’t consider
    the coordinates of the original data points (*emojis*) but the red point (the
    *average* between those data points). This is the standard way to calculate distances
    on the ***average linkage method.***
  prefs: []
  type: TYPE_NORMAL
- en: 'Other methods we can use to calculate distances based on aggregated data points
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Maximum (or *complete* linkage): considers the farthest data point in the cluster
    related to the point we are trying to aggregate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Minimum (or *single* linkage): considers the closest data point in the cluster
    related to the point we are trying to aggregate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ward (or *ward* linkage): minimizes the variance in the clusters with the next
    aggregation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let me do a small break on the step-by-step explanation to delve a bit deeper
    into the linkage methods as they are crucial in this type of clustering. Here’s
    a visual example of the different *linkage* methods available in hierarchical
    clustering, for a fictional example of 3 clusters to merge:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b4704ccc475adb3c5e542c173e373ab0.png)'
  prefs: []
  type: TYPE_IMG
- en: Linkage Methods Visualization
  prefs: []
  type: TYPE_NORMAL
- en: In the *sklearn* implementation**,** we’ll be able to experiment with some of
    these linkage methods and see a significant difference in the clustering results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Returning to our example, let’s now generate the distances between all our
    new data points — remember that there are two clusters that are being treated
    as a single one from now on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/89f31736bf227760d31178f8cd98b8e8.png)'
  prefs: []
  type: TYPE_IMG
- en: Considering closest customers as one cluster — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Distance between [60, 30] and [60, 55]: 25.0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Distance between [60, 30] and [34, 65]: 43.60'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Distance between [60, 30] and [41, 100]: 72.53'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Distance between [60, 55] and [34, 65]: 27.85'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Distance between [60, 55] and [41, 100]: 48.85'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Distance between [34, 65] and [41, 100]: 35.69'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which distance has the shortest path? **It’s the path between data points on
    coordinates [60, 30] and [60, 55]:**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/27cce7426459cc20c344792be41e560b.png)'
  prefs: []
  type: TYPE_IMG
- en: Considering next closest customers as one cluster — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is, naturally, to join these two customers into a single cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c8a9fe6c5b117aed167c88b1bb013b1d.png)'
  prefs: []
  type: TYPE_IMG
- en: Creating next cluster — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: With this new landscape of clusters, we calculate pairwise distances again!
    **Remember that we are always considering the average between data points (due
    to the linkage method we chose) in each cluster as reference point for the distance
    calculation:**
  prefs: []
  type: TYPE_NORMAL
- en: 'Distance between [60, 42.5] and [34, 65]: 34.38'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Distance between [60, 42.5] and [41, 100]: 60.56'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Distance between [34, 65] and [41, 100]: 35.69'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interestingly, the next data points to aggregate are the two clusters** as
    they lie on coordinates [60, 42.5] and [34, 65]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4e28889a138d4bbe81ed62b0f596ecf3.png)'
  prefs: []
  type: TYPE_IMG
- en: Merging the next clusters — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we finish the algorithm by aggregating all data points in a single
    big cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/91c6107ce3b6a50cd6b9ce8e16f037ed.png)'
  prefs: []
  type: TYPE_IMG
- en: Merging the final data point into our cluster — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: With this in mind, where do we exactly stop? It’s probably not a great idea
    to have a single big cluster with all data points, right?
  prefs: []
  type: TYPE_NORMAL
- en: To know where we stop, there’s some heuristic rules we can use. But first, we
    need to get familiar with another way of visualizing the process we’ve just done
    — the ***dendrogram****:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cce2f025357fa3de06747671f842dcf9.png)'
  prefs: []
  type: TYPE_IMG
- en: Dendrogram of Our Hierarchical Clustering Solution — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: On the *y-axis,* we have the distances that we’ve just calculated. On the *x-axis,*
    we have each data point. Climbing from each data point, we reach an horizontal
    line — the y-axis value of this line states the total distance that will connect
    the data points on the edges.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember the first customers we’ve connected in a single cluster? What we’ve
    seen in the 2D plot matches the dendrogram as those are exactly the first customers
    connected using an horizontal line (climbing the dendrogram from below):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/30aa14b1b9d6fa8c84dda27ecaf1fa79.png)'
  prefs: []
  type: TYPE_IMG
- en: First Horizontal Line in Dendrogram — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The horizontal lines represent the merging process we’ve just done! **Naturally,
    the dendrogram ends in a big horizontal line that connects all data points.**
  prefs: []
  type: TYPE_NORMAL
- en: As we just got familiar with the *Dendrogram*, we’re now ready to check the
    *sklearn* implementation and use a real dataset to understand how we can select
    the appropriate number of clusters based on this cool clustering method!
  prefs: []
  type: TYPE_NORMAL
- en: Sklearn Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the sklearn implementation, I’m going to use the Wine Quality dataset available
    [here.](https://archive.ics.uci.edu/dataset/186/wine+quality)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/dbc1f9f527fa660bccf69d2e8a76a9bd.png)'
  prefs: []
  type: TYPE_IMG
- en: Wine Quality Data Set Preview — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: This dataset contains information about wines (particularly red wines) with
    different characteristics such as citric acid, chlorides or density. The last
    column of the dataset gives respect to the quality of the wine, a classification
    done by a jury panel.
  prefs: []
  type: TYPE_NORMAL
- en: '**As hierarchical clustering deals with distances and we’re going to use the
    euclidean distance, we need to standardize our data.** We’ll start by using a
    `StandardScaler`on top of our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'With our scaled dataset, we can fit our first hierarchical clustering solution!
    We can access hierarchical clustering by creating an `AgglomerativeClustering`
    object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let me detail the arguments we are using inside the *AgglomerativeClustering*:'
  prefs: []
  type: TYPE_NORMAL
- en: '`n_clusters=None` is used as a way to have the full solution of the clusters
    (and where we can produce the full dendrogram).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`distance_threshold = 0` must be set in the `sklearn` implementation for the
    full dendrogram to be produced.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`linkage = ‘average’` is a very important hyperparameter. Remember that, in
    the theoretical implementation, we’ve described one method to consider the distances
    between newly formed clusters. `average` is the method that considers the average
    point between every new formed cluster in the calculation of new distances. In
    the `sklearn` implementation, we have three other methods that we also described:
    `single` , `complete` and `ward` .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After fitting the model, it’s time to plot our dendrogram. For this, I’m going
    to use the helper function provided in the `sklearn` [documentation](https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If we plot our hierarchical clustering solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3320257042cb9f8d72717bda19149824.png)'
  prefs: []
  type: TYPE_IMG
- en: Dendrogram of Average Method — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'The dendrogram is not great as our observations seem to get a bit jammed. Sometimes,
    the `average` , `single` and `complete` linkage may result in strange dendrograms,
    particularly when there are strong outliers in the data. The `ward` method may
    be appropriate for this type of data so let’s test that method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/6d4f1dd94235fae92b91c7f484cd1b3b.png)'
  prefs: []
  type: TYPE_IMG
- en: Dendrogram of Ward Method — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Much better! Notice that the clusters seem to be better defined according to
    the dendrogram. The ward method attempts to divide clusters by minimizing the
    intra-variance between newly formed clusters ([https://online.stat.psu.edu/stat505/lesson/14/14.7](https://online.stat.psu.edu/stat505/lesson/14/14.7))
    as we’ve described on the first part of the post. The objective is that for every
    iteration the clusters to be aggregated minimize the variance (distance between
    data points and new cluster to be formed).
  prefs: []
  type: TYPE_NORMAL
- en: Again, changing methods can be achieved by changing the `linkage` parameter
    in the `AgglomerativeClustering` function!
  prefs: []
  type: TYPE_NORMAL
- en: 'As we’re happy with the look of the `ward` method dendrogram, we’ll use that
    solution for our cluster profilling:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6d4f1dd94235fae92b91c7f484cd1b3b.png)'
  prefs: []
  type: TYPE_IMG
- en: Dendrogram of Ward Method — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Can you guess how many clusters we should choose?
  prefs: []
  type: TYPE_NORMAL
- en: 'According to the distances, a good candidate is cutting the dendrogram on this
    point, where every cluster seems to be relatively far from each other:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b7039304ec84b89b551748b390d42724.png)'
  prefs: []
  type: TYPE_IMG
- en: Dendrogram of Ward Method with Cutoff at 30 — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: The number of vertical lines that our line crosses are the number of final clusters
    of our solution. Choosing the number of clusters is not very “scientific” and
    different number of clustering solutions may be achieved, depending on business
    interpretation. For example, in our case, cutting off our dendrogram a bit above
    and reducing the number of clusters of the final solution may also be an hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll stick with the 7 cluster solution, so let’s fit our `ward` method with
    those `n_clusters` in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**As we want to interpret our clusters based on the original variables, we’ll
    use the predict method on the scaled data (the distances are based on the scaled
    dataset) but add the cluster to the original dataset.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s compare our clusters using the means of each variable conditioned on
    the `cluster` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/e91997b6c3ee6c462c3ac67985e918ea.png)'
  prefs: []
  type: TYPE_IMG
- en: Cluster Profilling — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'Interestingly, we can start to have some insights about the data — for example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Low quality wines seem to have a large value of `total sulfur dioxide` — notice
    the difference between the highest average quality cluster and the lower quality
    cluster:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/f6a9841ea9045e02b2aa01cab9c4877e.png)'
  prefs: []
  type: TYPE_IMG
- en: Sulfur Dioxide between Cluster 6 and 2 — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: 'And if we compare the `quality` of the wines in these clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fab1b35968b601570de7f43b8f2e895d.png)'
  prefs: []
  type: TYPE_IMG
- en: Quality Density Plot between Cluster 6 and 2 — Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, on average, Cluster 2 contains higher quality wines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another cool analysis we can do is performing a correlation matrix between
    clustered data means:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9f56e000fddc1b9bb185a0703855ef6c.png)'
  prefs: []
  type: TYPE_IMG
- en: Correlation Matrix of Cluster Means— Image by Author
  prefs: []
  type: TYPE_NORMAL
- en: This gives us some good hints of potential things we can explore (even for supervised
    learning). For example, on a multidimensional level, wines with higher `sulphates`
    and `chlorides` may get bundled together. Another conclusion is that wines with
    higher alcohol proof tend to be associated with higher quality wines.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: That’s it! Thank you for taking the time to read this blog post about unsupervised
    learning. I’ll keep adding more unsupervised learning algorithms to this series
    to showcase different types of methods we can use to get to know the structure
    of our data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Naturally, Hierarchical Clustering has some pros and cons that we can discuss:'
  prefs: []
  type: TYPE_NORMAL
- en: A big con of the algorithm is that it may require too much heuristics to reach
    a final solution. A combination of dendrogram analysis, distance based analysis,
    or silhouette coefficient methods may be applied to reach a number of clusters
    that make sense. Also, one must not discard crossing these technical approaches
    with some business knowledge about the data to avoid falling in some type of clustering
    trap.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the positive side, the hierarchical clustering approach is very explainable,
    helping uncover hidden structures in the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, hierarchical clustering does not suffer from the centroid initialization
    problem — something that may be an advantage for some datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hierarchical clustering is a very famous clustering method that has been applied
    for multiple applications as diverse as:'
  prefs: []
  type: TYPE_NORMAL
- en: Customer segmentation;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outlier analysis;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing multi-dimensional gene expression data;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document clustering;
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s a very cool method that data scientists should have on their tool belt.
    Feel free to try it on your next project and stay tuned for more posts on this
    **Unsupervised Learning Series!**
  prefs: []
  type: TYPE_NORMAL
- en: '*If you would like to drop by my Python courses, feel free to join* ***my free
    course*** *here (*[*Python For Busy People — Python Introduction in 2 Hours*](https://www.udemy.com/course/python-for-busy-people-python-introduction-2-hours/?referralCode=1588B6BF72D40253CDD4)*)*
    ***or a longer 16 hour version*** *(*[*The Complete Python Bootcamp for Beginners*](https://www.udemy.com/course/the-python-for-absolute-beginners-bootcamp/?referralCode=8D25992A055C19079B8A)*).
    My Python courses are suitable for beginners/mid-level developers and I would
    love to have you on my class!*'
  prefs: []
  type: TYPE_NORMAL
- en: '*The dataset used on this post is licensed under a* [*Creative Commons Attribution
    4.0 International*](https://creativecommons.org/licenses/by/4.0/legalcode) *(CC
    BY 4.0) license as shown in the following link:* [https://archive.ics.uci.edu/dataset/186/wine+quality](https://archive.ics.uci.edu/dataset/186/wine+quality)'
  prefs: []
  type: TYPE_NORMAL
