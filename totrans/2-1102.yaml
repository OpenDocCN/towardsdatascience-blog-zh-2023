- en: How to add Domain-Specific Knowledge to an LLM Based on Your Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-to-add-domain-specific-knowledge-to-an-llm-based-on-your-data-884a5f6a13ca](https://towardsdatascience.com/how-to-add-domain-specific-knowledge-to-an-llm-based-on-your-data-884a5f6a13ca)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Turn your LLM into a field expert
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@villatteantoine?source=post_page-----884a5f6a13ca--------------------------------)[![Antoine
    Villatte](../Images/0dface5672b3890ba7133cecb7e47d43.png)](https://medium.com/@villatteantoine?source=post_page-----884a5f6a13ca--------------------------------)[](https://towardsdatascience.com/?source=post_page-----884a5f6a13ca--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----884a5f6a13ca--------------------------------)
    [Antoine Villatte](https://medium.com/@villatteantoine?source=post_page-----884a5f6a13ca--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----884a5f6a13ca--------------------------------)
    ·7 min read·Jul 11, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ee90d509f6eef9303c8de7b2790e1f9e.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Hubi's Tavern](https://unsplash.com/fr/@hubistavern?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In recent months, Large Language Models (LLMs) have profoundly changed the way
    we work and interact with technology, and have proven to be helpful tools in various
    domains, serving as writing assistants, code generators, and even creative collaborators.
    Their ability to understand context, generate human-like text, and perform a wide
    range of language-related tasks has propelled them to the forefront of artificial
    intelligence research.
  prefs: []
  type: TYPE_NORMAL
- en: While LLMs excel at generating generic text, they often struggle when confronted
    with highly specialized domains that demand precise knowledge and nuanced understanding.
    When used for domain-specific tasks, these models can exhibit limitations or,
    in some cases, even produce erroneous or hallucinatory responses. This highlights
    the need for incorporating domain knowledge into LLMs, enabling them to better
    navigate complex, industry-specific jargon, exhibit a more nuanced understanding
    of context, and limit the risk of producing false information.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we will explore one of several strategies and techniques to
    infuse domain knowledge into LLMs, allowing them to perform at their best within
    specific professional contexts by adding chunks of documentation into an LLM as
    context when injecting the query.
  prefs: []
  type: TYPE_NORMAL
- en: '***This method works with any type of documentation, and only uses secure,
    open source technologies that will run locally on your computer, without the need
    to access the internet. Thanks to that, I could use it on personal and confidential
    data that I didn’t want third party websites to access.***'
  prefs: []
  type: TYPE_NORMAL
- en: Principle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here’s a breakdown of how it works :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c06b339eb18d18de529bc3ceb16e09d7.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph explanation of the process. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to take our documentation and build a vector index database
    based on our documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Vector databases are a type of database designed to store and query high-dimensional
    vectors efficiently. These databases enable fast similarity and semantic search
    while allowing users to find vectors that are the closest to a given query vector
    based on some distance metric, instead of performing queries on values contained
    in rows and columns like in traditional OLTP and OLAP databases.
  prefs: []
  type: TYPE_NORMAL
- en: That means that we can create embeddings that represent any documentation and
    populate the database with it.
  prefs: []
  type: TYPE_NORMAL
- en: Then, once it is built, we can perform a query that will also be embedded, and
    injected into the vector index database, which will return the most related pieces
    of documentation for our query.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, these can be injected into a local LLM as context alongside our original
    query. This way, the selected context will be small enough to be accepted by most
    LLMs and, since it is related to our query, the model will have sufficient knowledge
    to accurately answer the question. A little bit of prompt engineering can also
    help.
  prefs: []
  type: TYPE_NORMAL
- en: Case example and repo
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we will use a local, open-source LLM and inject it with domain
    knowledge of all the Python Enhancement Programs (PEPs). This principle can be
    applied to any sort of documentation, but I’ll use the PEPs because it’s easily
    accessible and it’s public domain, which makes it perfect as an example dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the full code that I used to write this article on this repo :'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/Anvil-Late/knowledge_llm/tree/main](https://github.com/Anvil-Late/knowledge_llm/tree/main)'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/Anvil-Late/knowledge_llm?source=post_page-----884a5f6a13ca--------------------------------)
    [## GitHub - Anvil-Late/knowledge_llm: Guide to adding domain knowledge to LLMs'
  prefs: []
  type: TYPE_NORMAL
- en: Guide to adding domain knowledge to LLMs. Contribute to Anvil-Late/knowledge_llm
    development by creating an account on…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/Anvil-Late/knowledge_llm?source=post_page-----884a5f6a13ca--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Quick preview of the results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here’s how results would look like
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/056bfd79c4ff0cdf7792654e010dbcb1.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of a query being processed and answered. Image by Author.
  prefs: []
  type: TYPE_NORMAL
- en: How to install the LLM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you don’t have an LLM installed on your computer, you can find a step-by-step
    guide on how to do that here :'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://medium.com/better-programming/how-to-run-your-personal-chatgpt-like-model-locally-505c093924bc](https://medium.com/better-programming/how-to-run-your-personal-chatgpt-like-model-locally-505c093924bc)'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://betterprogramming.pub/how-to-run-your-personal-chatgpt-like-model-locally-505c093924bc?source=post_page-----884a5f6a13ca--------------------------------)
    [## [GPT TUTORIAL] How to Run Your Personal, ChatGPT-like Model, Locally'
  prefs: []
  type: TYPE_NORMAL
- en: Your own personal AI assistant
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: betterprogramming.pub](https://betterprogramming.pub/how-to-run-your-personal-chatgpt-like-model-locally-505c093924bc?source=post_page-----884a5f6a13ca--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: How to build and query the Vector Index Database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can find the full code to build the vector index database on this repo
    :'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/Anvil-Late/knowledge_llm/tree/main](https://github.com/Anvil-Late/knowledge_llm/tree/main)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Broadly speaking, in the src folder :'
  prefs: []
  type: TYPE_NORMAL
- en: parse.py creates the PEP corpus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: embed.py creates the embedded corpus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can pull the docker image of the Qdrant vector index database and run it
    with the commands `docker pull qdrant/qdrant` and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`docker run -d -p 6333:6333 qdrant/qdrant`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: create_index.py creates and populates the vector index database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: query_index.py embeds a query and retrieves the most relevant documentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you need more details, you can find my step-by-step guide here :'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://betterprogramming.pub/efficiently-navigate-massive-documentations-ai-powered-natural-language-queries-for-knowledge-372f4711a7c8](https://betterprogramming.pub/efficiently-navigate-massive-documentations-ai-powered-natural-language-queries-for-knowledge-372f4711a7c8)'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://betterprogramming.pub/efficiently-navigate-massive-documentations-ai-powered-natural-language-queries-for-knowledge-372f4711a7c8?source=post_page-----884a5f6a13ca--------------------------------)
    [## AI-Powered Documentation Search — Navigate Your Database With Natural Language
    Queries'
  prefs: []
  type: TYPE_NORMAL
- en: Efficient Document Navigation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: betterprogramming.pub](https://betterprogramming.pub/efficiently-navigate-massive-documentations-ai-powered-natural-language-queries-for-knowledge-372f4711a7c8?source=post_page-----884a5f6a13ca--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Combine everything
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we’ll write a script that generates a prompt for the LLM :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`logging.disable(logging.INFO)` and `set_global_logging_level` prevent excessive
    prints during code execution, since everything printed by this script will be
    captured.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We combine this prompt generation with prompt injection with the following
    bash script :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: What happens here is that the prompt generation script prints the prompt, and
    the bash script captures it in the `$prompt` variable, which is then used in the
    llama.cpp `./main` command with the `-p`(or `--prompt`) parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The LLM will then take over and complete the prompt starting from ‘Response:
    Here’s the answer to your query:’.'
  prefs: []
  type: TYPE_NORMAL
- en: Remember to replace `<PATH_TO_LLAMA.CPP>` to the path of your llama.cpp clone
    in your computer, and `Wizard-Vicuna-13B-Uncensored.ggmlv3.q4_0.bin` to your LLM.
    Personally, I chose this one because it gave me pretty good results and it is
    not under a restrictive license, but feel free to try it with other models !
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s recap what we have accomplished here:'
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this article, we have delved into an effective strategy to augment
    the capabilities of Large Language Models (LLMs) by infusing them with domain
    knowledge. While LLMs have demonstrated remarkable proficiency in a variety of
    tasks, they often encounter difficulties when confronted with highly specialized
    domains that necessitate precise knowledge and nuanced understanding.
  prefs: []
  type: TYPE_NORMAL
- en: To address these limitations, we explored a methodology that involves incorporating
    domain-specific documentation into LLMs. By constructing a vector index database
    based on the documentation, we established a foundation for efficient similarity
    and semantic search. This allowed us to identify the most relevant pieces of documentation
    for a given query, which could then be injected as context into a local LLM.
  prefs: []
  type: TYPE_NORMAL
- en: The approach we presented was exemplified through the utilization of Python
    Enhancement Programs (PEPs) as a representative dataset. However, it is important
    to note that this methodology is applicable to any form of documentation. The
    code snippets and repository provided in this article serve as practical demonstrations,
    showcasing the implementation process.
  prefs: []
  type: TYPE_NORMAL
- en: By following the outlined steps, users can enhance LLM performance within specific
    professional contexts, enabling the models to navigate complex industry-specific
    jargon and generate more accurate responses. Moreover, the secure and open-source
    technologies employed in this strategy ensure that the process can be executed
    locally without external internet dependencies, thereby safeguarding privacy and
    confidentiality.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, the infusion of domain knowledge into LLMs empowers these models
    to excel in specialized tasks, as they gain a deeper understanding of the context
    in which they operate. The implications of this approach extend across diverse
    domains, enabling LLMs to provide invaluable assistance and insights tailored
    to specific professional requirements. By leveraging the potential of LLMs combined
    with domain expertise, we unlock a new realm of possibilities for improving human-AI
    interactions and leveraging the power of artificial intelligence in specialized
    domains.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have any questions, don’t hesitate to leave it in the comments, I’ll
    do my best to answer you!
  prefs: []
  type: TYPE_NORMAL
- en: '***If you liked this, you can also support my work on Medium directly and get
    unlimited access by becoming a member using my referral link*** [***here***](https://medium.com/@villatteantoine/membership)
    ***:)***'
  prefs: []
  type: TYPE_NORMAL
