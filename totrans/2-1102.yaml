- en: How to add Domain-Specific Knowledge to an LLM Based on Your Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何根据您的数据将领域特定知识添加到LLM
- en: 原文：[https://towardsdatascience.com/how-to-add-domain-specific-knowledge-to-an-llm-based-on-your-data-884a5f6a13ca](https://towardsdatascience.com/how-to-add-domain-specific-knowledge-to-an-llm-based-on-your-data-884a5f6a13ca)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-add-domain-specific-knowledge-to-an-llm-based-on-your-data-884a5f6a13ca](https://towardsdatascience.com/how-to-add-domain-specific-knowledge-to-an-llm-based-on-your-data-884a5f6a13ca)
- en: Turn your LLM into a field expert
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将你的LLM 转变为领域专家
- en: '[](https://medium.com/@villatteantoine?source=post_page-----884a5f6a13ca--------------------------------)[![Antoine
    Villatte](../Images/0dface5672b3890ba7133cecb7e47d43.png)](https://medium.com/@villatteantoine?source=post_page-----884a5f6a13ca--------------------------------)[](https://towardsdatascience.com/?source=post_page-----884a5f6a13ca--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----884a5f6a13ca--------------------------------)
    [Antoine Villatte](https://medium.com/@villatteantoine?source=post_page-----884a5f6a13ca--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@villatteantoine?source=post_page-----884a5f6a13ca--------------------------------)[![Antoine
    Villatte](../Images/0dface5672b3890ba7133cecb7e47d43.png)](https://medium.com/@villatteantoine?source=post_page-----884a5f6a13ca--------------------------------)[](https://towardsdatascience.com/?source=post_page-----884a5f6a13ca--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----884a5f6a13ca--------------------------------)
    [Antoine Villatte](https://medium.com/@villatteantoine?source=post_page-----884a5f6a13ca--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----884a5f6a13ca--------------------------------)
    ·7 min read·Jul 11, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----884a5f6a13ca--------------------------------)
    ·7 分钟阅读·2023年7月11日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ee90d509f6eef9303c8de7b2790e1f9e.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee90d509f6eef9303c8de7b2790e1f9e.png)'
- en: Photo by [Hubi's Tavern](https://unsplash.com/fr/@hubistavern?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由 [Hubi's Tavern](https://unsplash.com/fr/@hubistavern?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) 提供的照片
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In recent months, Large Language Models (LLMs) have profoundly changed the way
    we work and interact with technology, and have proven to be helpful tools in various
    domains, serving as writing assistants, code generators, and even creative collaborators.
    Their ability to understand context, generate human-like text, and perform a wide
    range of language-related tasks has propelled them to the forefront of artificial
    intelligence research.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，大型语言模型（LLMs）已经深刻改变了我们与技术的工作和互动方式，并且在各种领域中被证明是有用的工具，充当写作助手、代码生成器，甚至是创造性合作伙伴。它们理解上下文、生成类似人类的文本以及执行广泛的语言相关任务的能力使其成为人工智能研究的前沿。
- en: While LLMs excel at generating generic text, they often struggle when confronted
    with highly specialized domains that demand precise knowledge and nuanced understanding.
    When used for domain-specific tasks, these models can exhibit limitations or,
    in some cases, even produce erroneous or hallucinatory responses. This highlights
    the need for incorporating domain knowledge into LLMs, enabling them to better
    navigate complex, industry-specific jargon, exhibit a more nuanced understanding
    of context, and limit the risk of producing false information.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大型语言模型（LLMs）在生成通用文本方面表现优异，但当面对需要精确知识和细致理解的高度专业领域时，它们往往表现得较差。在用于特定领域任务时，这些模型可能会表现出局限性，甚至在某些情况下产生错误或虚假的回答。这突显了将领域知识融入LLMs的必要性，使其能够更好地应对复杂的行业术语，展示更细致的上下文理解，并减少生成虚假信息的风险。
- en: In this article, we will explore one of several strategies and techniques to
    infuse domain knowledge into LLMs, allowing them to perform at their best within
    specific professional contexts by adding chunks of documentation into an LLM as
    context when injecting the query.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将探讨将领域知识注入LLMs的几种策略和技术之一，使其在特定专业环境中表现最佳，通过将文档片段作为上下文添加到LLM中来注入查询。
- en: '***This method works with any type of documentation, and only uses secure,
    open source technologies that will run locally on your computer, without the need
    to access the internet. Thanks to that, I could use it on personal and confidential
    data that I didn’t want third party websites to access.***'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '***该方法适用于任何类型的文档，仅使用安全的开源技术，这些技术将在你的计算机上本地运行，无需访问互联网。借助这一点，我可以在个人和机密数据上使用它，而不必担心第三方网站的访问。***'
- en: Principle
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 原理
- en: 'Here’s a breakdown of how it works :'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何工作的详细说明：
- en: '![](../Images/c06b339eb18d18de529bc3ceb16e09d7.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c06b339eb18d18de529bc3ceb16e09d7.png)'
- en: Graph explanation of the process. Image by Author.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 过程的图解。图像由作者提供。
- en: The first step is to take our documentation and build a vector index database
    based on our documentation.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是拿到我们的文档，并基于这些文档构建一个向量索引数据库。
- en: Vector databases are a type of database designed to store and query high-dimensional
    vectors efficiently. These databases enable fast similarity and semantic search
    while allowing users to find vectors that are the closest to a given query vector
    based on some distance metric, instead of performing queries on values contained
    in rows and columns like in traditional OLTP and OLAP databases.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库是一种旨在高效存储和查询高维向量的数据库。这些数据库支持快速的相似性和语义搜索，同时允许用户根据某些距离度量找到与给定查询向量最接近的向量，而不是像传统的OLTP和OLAP数据库那样对行和列中的值进行查询。
- en: That means that we can create embeddings that represent any documentation and
    populate the database with it.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们可以创建表示任何文档的嵌入，并用它来填充数据库。
- en: Then, once it is built, we can perform a query that will also be embedded, and
    injected into the vector index database, which will return the most related pieces
    of documentation for our query.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，一旦构建完成，我们可以执行一个查询，该查询也将被嵌入，并注入到向量索引数据库中，这样将返回与我们的查询最相关的文档片段。
- en: Finally, these can be injected into a local LLM as context alongside our original
    query. This way, the selected context will be small enough to be accepted by most
    LLMs and, since it is related to our query, the model will have sufficient knowledge
    to accurately answer the question. A little bit of prompt engineering can also
    help.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这些向量可以作为上下文注入到本地LLM中，与我们原始的查询一起。这种方式下，选择的上下文将足够小，以被大多数LLM接受，并且由于与我们的查询相关，模型将拥有足够的知识来准确回答问题。一点点提示工程也会有所帮助。
- en: Case example and repo
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例示例和代码库
- en: In this article, we will use a local, open-source LLM and inject it with domain
    knowledge of all the Python Enhancement Programs (PEPs). This principle can be
    applied to any sort of documentation, but I’ll use the PEPs because it’s easily
    accessible and it’s public domain, which makes it perfect as an example dataset.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将使用一个本地的开源LLM，并注入所有Python增强提案（PEPs）的领域知识。这一原则可以应用于任何文档类型，但我将使用PEP，因为它易于获取且属于公共领域，这使它成为一个理想的示例数据集。
- en: 'You can find the full code that I used to write this article on this repo :'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这个代码库中找到我用于撰写这篇文章的完整代码：
- en: '[https://github.com/Anvil-Late/knowledge_llm/tree/main](https://github.com/Anvil-Late/knowledge_llm/tree/main)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/Anvil-Late/knowledge_llm/tree/main](https://github.com/Anvil-Late/knowledge_llm/tree/main)'
- en: '[](https://github.com/Anvil-Late/knowledge_llm?source=post_page-----884a5f6a13ca--------------------------------)
    [## GitHub - Anvil-Late/knowledge_llm: Guide to adding domain knowledge to LLMs'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/Anvil-Late/knowledge_llm?source=post_page-----884a5f6a13ca--------------------------------)
    [## GitHub - Anvil-Late/knowledge_llm: 向LLM添加领域知识的指南'
- en: Guide to adding domain knowledge to LLMs. Contribute to Anvil-Late/knowledge_llm
    development by creating an account on…
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 向LLM添加领域知识的指南。通过创建一个帐户来为Anvil-Late/knowledge_llm的开发做贡献…
- en: github.com](https://github.com/Anvil-Late/knowledge_llm?source=post_page-----884a5f6a13ca--------------------------------)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/Anvil-Late/knowledge_llm?source=post_page-----884a5f6a13ca--------------------------------)'
- en: Quick preview of the results
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果的快速预览
- en: Here’s how results would look like
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的展示样例
- en: '![](../Images/056bfd79c4ff0cdf7792654e010dbcb1.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/056bfd79c4ff0cdf7792654e010dbcb1.png)'
- en: Example of a query being processed and answered. Image by Author.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 查询被处理和回答的示例。图像由作者提供。
- en: How to install the LLM
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何安装LLM
- en: 'If you don’t have an LLM installed on your computer, you can find a step-by-step
    guide on how to do that here :'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的计算机上没有安装LLM，你可以在这里找到如何安装的逐步指南：
- en: '[https://medium.com/better-programming/how-to-run-your-personal-chatgpt-like-model-locally-505c093924bc](https://medium.com/better-programming/how-to-run-your-personal-chatgpt-like-model-locally-505c093924bc)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://medium.com/better-programming/how-to-run-your-personal-chatgpt-like-model-locally-505c093924bc](https://medium.com/better-programming/how-to-run-your-personal-chatgpt-like-model-locally-505c093924bc)'
- en: '[](https://betterprogramming.pub/how-to-run-your-personal-chatgpt-like-model-locally-505c093924bc?source=post_page-----884a5f6a13ca--------------------------------)
    [## [GPT TUTORIAL] How to Run Your Personal, ChatGPT-like Model, Locally'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://betterprogramming.pub/how-to-run-your-personal-chatgpt-like-model-locally-505c093924bc?source=post_page-----884a5f6a13ca--------------------------------)
    [## [GPT 教程] 如何在本地运行您的个人 ChatGPT 类模型'
- en: Your own personal AI assistant
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 您自己的个人 AI 助手
- en: betterprogramming.pub](https://betterprogramming.pub/how-to-run-your-personal-chatgpt-like-model-locally-505c093924bc?source=post_page-----884a5f6a13ca--------------------------------)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: betterprogramming.pub](https://betterprogramming.pub/how-to-run-your-personal-chatgpt-like-model-locally-505c093924bc?source=post_page-----884a5f6a13ca--------------------------------)
- en: How to build and query the Vector Index Database
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何构建和查询向量索引数据库
- en: 'You can find the full code to build the vector index database on this repo
    :'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在这个仓库中找到构建向量索引数据库的完整代码：
- en: '[https://github.com/Anvil-Late/knowledge_llm/tree/main](https://github.com/Anvil-Late/knowledge_llm/tree/main)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/Anvil-Late/knowledge_llm/tree/main](https://github.com/Anvil-Late/knowledge_llm/tree/main)'
- en: 'Broadly speaking, in the src folder :'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 广义而言，在 src 文件夹中：
- en: parse.py creates the PEP corpus
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: parse.py 创建 PEP 语料库
- en: embed.py creates the embedded corpus
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: embed.py 创建嵌入的语料库
- en: You can pull the docker image of the Qdrant vector index database and run it
    with the commands `docker pull qdrant/qdrant` and
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以拉取 Qdrant 向量索引数据库的 Docker 镜像，并使用 `docker pull qdrant/qdrant` 命令运行它
- en: '`docker run -d -p 6333:6333 qdrant/qdrant`'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`docker run -d -p 6333:6333 qdrant/qdrant`'
- en: create_index.py creates and populates the vector index database
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: create_index.py 创建并填充向量索引数据库
- en: query_index.py embeds a query and retrieves the most relevant documentation
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: query_index.py 嵌入一个查询并检索最相关的文档
- en: 'If you need more details, you can find my step-by-step guide here :'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要更多细节，您可以在这里找到我的逐步指南：
- en: '[https://betterprogramming.pub/efficiently-navigate-massive-documentations-ai-powered-natural-language-queries-for-knowledge-372f4711a7c8](https://betterprogramming.pub/efficiently-navigate-massive-documentations-ai-powered-natural-language-queries-for-knowledge-372f4711a7c8)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://betterprogramming.pub/efficiently-navigate-massive-documentations-ai-powered-natural-language-queries-for-knowledge-372f4711a7c8](https://betterprogramming.pub/efficiently-navigate-massive-documentations-ai-powered-natural-language-queries-for-knowledge-372f4711a7c8)'
- en: '[](https://betterprogramming.pub/efficiently-navigate-massive-documentations-ai-powered-natural-language-queries-for-knowledge-372f4711a7c8?source=post_page-----884a5f6a13ca--------------------------------)
    [## AI-Powered Documentation Search — Navigate Your Database With Natural Language
    Queries'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://betterprogramming.pub/efficiently-navigate-massive-documentations-ai-powered-natural-language-queries-for-knowledge-372f4711a7c8?source=post_page-----884a5f6a13ca--------------------------------)
    [## 基于 AI 的文档搜索 — 使用自然语言查询导航您的数据库'
- en: Efficient Document Navigation
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高效文档导航
- en: betterprogramming.pub](https://betterprogramming.pub/efficiently-navigate-massive-documentations-ai-powered-natural-language-queries-for-knowledge-372f4711a7c8?source=post_page-----884a5f6a13ca--------------------------------)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: betterprogramming.pub](https://betterprogramming.pub/efficiently-navigate-massive-documentations-ai-powered-natural-language-queries-for-knowledge-372f4711a7c8?source=post_page-----884a5f6a13ca--------------------------------)
- en: Combine everything
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将所有内容结合起来
- en: 'First, we’ll write a script that generates a prompt for the LLM :'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将编写一个生成 LLM 提示的脚本：
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`logging.disable(logging.INFO)` and `set_global_logging_level` prevent excessive
    prints during code execution, since everything printed by this script will be
    captured.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`logging.disable(logging.INFO)` 和 `set_global_logging_level` 可以防止代码执行过程中打印过多内容，因为该脚本打印的所有内容都会被捕获。'
- en: 'We combine this prompt generation with prompt injection with the following
    bash script :'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这个提示生成与提示注入结合起来，使用以下 bash 脚本：
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: What happens here is that the prompt generation script prints the prompt, and
    the bash script captures it in the `$prompt` variable, which is then used in the
    llama.cpp `./main` command with the `-p`(or `--prompt`) parameter.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生的情况是，提示生成脚本打印出提示，bash 脚本将其捕获到 `$prompt` 变量中，然后在 llama.cpp `./main` 命令中使用
    `-p`（或 `--prompt`）参数。
- en: 'The LLM will then take over and complete the prompt starting from ‘Response:
    Here’s the answer to your query:’.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '然后 LLM 将接管并完成提示，从‘Response: Here’s the answer to your query:’开始。'
- en: Remember to replace `<PATH_TO_LLAMA.CPP>` to the path of your llama.cpp clone
    in your computer, and `Wizard-Vicuna-13B-Uncensored.ggmlv3.q4_0.bin` to your LLM.
    Personally, I chose this one because it gave me pretty good results and it is
    not under a restrictive license, but feel free to try it with other models !
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 记得将 `<PATH_TO_LLAMA.CPP>` 替换为你计算机上 llama.cpp 克隆的路径，将 `Wizard-Vicuna-13B-Uncensored.ggmlv3.q4_0.bin`
    替换为你的 LLM。我个人选择了这个，因为它给了我很好的结果，并且没有受到限制性许可证的约束，但你可以尝试其他模型！
- en: Conclusion
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'Let’s recap what we have accomplished here:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下我们在这里完成的工作：
- en: Throughout this article, we have delved into an effective strategy to augment
    the capabilities of Large Language Models (LLMs) by infusing them with domain
    knowledge. While LLMs have demonstrated remarkable proficiency in a variety of
    tasks, they often encounter difficulties when confronted with highly specialized
    domains that necessitate precise knowledge and nuanced understanding.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们深入探讨了一种有效的策略，通过将领域知识注入 LLM 来增强其能力。尽管 LLM 在各种任务中表现出色，但它们在面对需要精确知识和细致理解的高度专业化领域时，往往会遇到困难。
- en: To address these limitations, we explored a methodology that involves incorporating
    domain-specific documentation into LLMs. By constructing a vector index database
    based on the documentation, we established a foundation for efficient similarity
    and semantic search. This allowed us to identify the most relevant pieces of documentation
    for a given query, which could then be injected as context into a local LLM.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些局限性，我们探索了一种方法，将领域特定的文档纳入 LLM。通过基于文档构建一个向量索引数据库，我们建立了一个高效的相似性和语义搜索基础。这使我们能够识别出与特定查询最相关的文档，然后将其注入本地
    LLM 作为上下文。
- en: The approach we presented was exemplified through the utilization of Python
    Enhancement Programs (PEPs) as a representative dataset. However, it is important
    to note that this methodology is applicable to any form of documentation. The
    code snippets and repository provided in this article serve as practical demonstrations,
    showcasing the implementation process.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所提出的方法通过使用 Python 增强程序（PEPs）作为代表性数据集来进行示例。然而，值得注意的是，这种方法适用于任何形式的文档。本文中提供的代码片段和仓库作为实际演示，展示了实施过程。
- en: By following the outlined steps, users can enhance LLM performance within specific
    professional contexts, enabling the models to navigate complex industry-specific
    jargon and generate more accurate responses. Moreover, the secure and open-source
    technologies employed in this strategy ensure that the process can be executed
    locally without external internet dependencies, thereby safeguarding privacy and
    confidentiality.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 按照 outlined steps，用户可以提升 LLM 在特定专业环境中的表现，使模型能够处理复杂的行业术语并生成更准确的响应。此外，所使用的安全和开源技术确保了这一过程可以在本地执行，无需外部互联网依赖，从而保护隐私和机密性。
- en: In conclusion, the infusion of domain knowledge into LLMs empowers these models
    to excel in specialized tasks, as they gain a deeper understanding of the context
    in which they operate. The implications of this approach extend across diverse
    domains, enabling LLMs to provide invaluable assistance and insights tailored
    to specific professional requirements. By leveraging the potential of LLMs combined
    with domain expertise, we unlock a new realm of possibilities for improving human-AI
    interactions and leveraging the power of artificial intelligence in specialized
    domains.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，将领域知识融入 LLM（大语言模型）中，使这些模型能够在专业任务中表现出色，因为它们对其操作的背景有了更深入的理解。这种方法的影响跨越了不同领域，使
    LLM 能够提供量身定制的宝贵帮助和见解。通过利用 LLM 和领域专业知识的潜力，我们为提升人类与 AI 的互动以及在专业领域利用人工智能的力量打开了新的可能性。
- en: Thank you for reading!
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: If you have any questions, don’t hesitate to leave it in the comments, I’ll
    do my best to answer you!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有任何问题，请不要犹豫，留下评论，我会尽力回答！
- en: '***If you liked this, you can also support my work on Medium directly and get
    unlimited access by becoming a member using my referral link*** [***here***](https://medium.com/@villatteantoine/membership)
    ***:)***'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '***如果你喜欢这个内容，你也可以通过我的推荐链接直接在 Medium 上支持我的工作，并成为会员以获得无限制访问*** [***这里***](https://medium.com/@villatteantoine/membership)
    ***:)***'
