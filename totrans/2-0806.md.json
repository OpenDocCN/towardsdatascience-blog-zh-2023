["```py\nimport os\nimport tensorflow as tf\nimport shutil\n\nDATA_URL = \"https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\"\nDATA_PATH = \"data/raw\"\n\npath_to_zip = tf.keras.utils.get_file(\n    \"cats_and_dogs.zip\", origin=DATA_URL, extract=True\n)\ndownload_path = os.path.join(os.path.dirname(path_to_zip), \"cats_and_dogs_filtered\")\n\ntrain_dir_from = os.path.join(download_path, \"train\")\nvalidation_dir_from = os.path.join(download_path, \"validation\")\n\ntrain_dir_to = os.path.join(DATA_PATH, \"train\")\nvalidation_dir_to = os.path.join(DATA_PATH, \"validation\")\n\nshutil.move(train_dir_from, train_dir_to)\nshutil.move(validation_dir_from, validation_dir_to)\n```", "```py\nðŸ“¦data\nâ”— ðŸ“‚raw\nâ”£ ðŸ“‚train\nâ”ƒ â”£ ðŸ“‚cats\nâ”ƒ â”— ðŸ“‚dogs\nâ”— ðŸ“‚validation\nâ”£ ðŸ“‚cats\nâ”— ðŸ“‚dogs\n```", "```py\nstages:\n  train:\n    cmd: python src/train.py\n    deps:\n      - src/train.py\n      - data/raw\n    params:\n      - train\n    outs:\n      - models\n      - metrics.csv\n      - dvclive/metrics.json:\n          cache: False\n      - dvclive/plots\n```", "```py\ntrain:\n  image_width: 180\n  image_height: 180\n  batch_size: 32\n  learning_rate: 0.01\n  n_epochs: 15\n```", "```py\nimport os\nfrom pathlib import Path\nimport numpy as np\nimport tensorflow as tf\nfrom dvc.api import params_show\nfrom dvclive.keras import DVCLiveCallback\n\n# data directories\nBASE_DIR = Path(__file__).parent.parent\nDATA_DIR = \"data/raw\"\ntrain_dir = os.path.join(DATA_DIR, \"train\")\nvalidation_dir = os.path.join(DATA_DIR, \"validation\")\n\n# get the params\nparams = params_show()[\"train\"]\nIMG_WIDTH, IMG_HEIGHT = params[\"image_width\"], params[\"image_height\"]\nIMG_SIZE = (IMG_WIDTH, IMG_HEIGHT)\nBATCH_SIZE = params[\"batch_size\"]\nLR = params[\"learning_rate\"]\nN_EPOCHS = params[\"n_epochs\"]\n\n# get image datasets\ntrain_dataset = tf.keras.utils.image_dataset_from_directory(\n    train_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE\n)\n\nvalidation_dataset = tf.keras.utils.image_dataset_from_directory(\n    validation_dir, shuffle=True, batch_size=BATCH_SIZE, image_size=IMG_SIZE\n)\n```", "```py\ndef get_model():\n    \"\"\"\n    Prepare the ResNet50 model for transfer learning.\n    \"\"\"\n\n    data_augmentation = tf.keras.Sequential(\n        [\n            tf.keras.layers.RandomFlip(\"horizontal\"),\n            tf.keras.layers.RandomRotation(0.2),\n        ]\n    )\n\n    preprocess_input = tf.keras.applications.resnet50.preprocess_input\n\n    IMG_SHAPE = IMG_SIZE + (3,)\n    base_model = tf.keras.applications.ResNet50(\n        input_shape=IMG_SHAPE, include_top=False, weights=\"imagenet\"\n    )\n    base_model.trainable = False\n\n    global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n    prediction_layer = tf.keras.layers.Dense(1)\n\n    inputs = tf.keras.Input(shape=IMG_SHAPE)\n    x = data_augmentation(inputs)\n    x = preprocess_input(x)\n    x = base_model(x, training=False)\n    x = global_average_layer(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    outputs = prediction_layer(x)\n    model = tf.keras.Model(inputs, outputs)\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n       metrics=[\"accuracy\"],\n    )\n\n    return model\n```", "```py\ndef main():\n    model_path = BASE_DIR / \"models\"\n    model_path.mkdir(parents=True, exist_ok=True)\n\n    model = get_model()\n\n    callbacks = [\n        tf.keras.callbacks.ModelCheckpoint(\n            model_path / \"model.keras\", monitor=\"val_accuracy\", save_best_only=True\n        ),\n        tf.keras.callbacks.CSVLogger(\"metrics.csv\"),\n        DVCLiveCallback(save_dvc_exp=True),\n    ]\n\n    history = model.fit(\n        train_dataset,\n        epochs=N_EPOCHS,\n        validation_data=validation_dataset,\n        callbacks=callbacks,\n    )\n\nif __name__ == \"__main__\":\n    main()\n```", "```py\ndvc exp run\n```", "```py\nðŸ“¦dvclive\nâ”£ ðŸ“‚plots\nâ”ƒ â”— ðŸ“‚metrics\nâ”ƒ â”ƒ â”£ ðŸ“‚eval\nâ”ƒ â”ƒ â”ƒ â”£ ðŸ“œaccuracy.tsv\nâ”ƒ â”ƒ â”ƒ â”— ðŸ“œloss.tsv\nâ”ƒ â”ƒ â”— ðŸ“‚train\nâ”ƒ â”ƒ â”ƒ â”£ ðŸ“œaccuracy.tsv\nâ”ƒ â”ƒ â”ƒ â”— ðŸ“œloss.tsv\nâ”£ ðŸ“œ.gitignore\nâ”£ ðŸ“œdvc.yaml\nâ”£ ðŸ“œmetrics.json\nâ”— ðŸ“œreport.html\n```", "```py\ndvc exp run -S train.learning_rate=0.1\n```", "```py\nimport os\nfrom pathlib import Path\nimport numpy as np\nimport tensorflow as tf\nfrom dvc.api import params_show\nfrom dvclive.keras import DVCLiveCallback\nfrom dvclive import Live\n\n# data directories, parameters, datasets, and the model function did not change\n\ndef main():\n    model_path = BASE_DIR / \"models\"\n    model_path.mkdir(parents=True, exist_ok=True)\n\n    model = get_model()\n\n    with Live(save_dvc_exp=True) as live:\n\n        callbacks = [\n            tf.keras.callbacks.ModelCheckpoint(\n                model_path / \"model.keras\", monitor=\"val_accuracy\", save_best_only=True\n            ),\n            tf.keras.callbacks.CSVLogger(\"metrics.csv\"),\n            DVCLiveCallback(live=live),\n        ]\n\n        history = model.fit(\n            train_dataset,\n            epochs=N_EPOCHS,\n            validation_data=validation_dataset,\n            callbacks=callbacks,\n        )\n\n        model.load_weights(str(model_path / \"model.keras\"))\n        y_pred = np.array([])\n        y_true = np.array([])\n        for x, y in validation_dataset:\n            y_pred = np.concatenate([y_pred, model.predict(x).flatten()])\n            y_true = np.concatenate([y_true, y.numpy()])\n\n        y_pred = np.where(y_pred > 0, 1, 0)\n\n        live.log_sklearn_plot(\"confusion_matrix\", y_true, y_pred)\n\nif __name__ == \"__main__\":\n    main()\n```", "```py\nmetrics:\n- metrics.json\nplots:\n- plots/metrics\n- plots/sklearn/confusion_matrix.json:\n    template: confusion\n    x: actual\n    y: predicted\n    title: Confusion Matrix\n    x_label: True Label\n    y_label: Predicted Label\n```"]