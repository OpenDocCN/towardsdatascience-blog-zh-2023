- en: 'Courage to Learn ML: Demystifying L1 & L2 Regularization (part 4)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‹‡æ•¢å­¦ä¹  MLï¼šæ­ç¤º L1 å’Œ L2 æ­£åˆ™åŒ–ï¼ˆç¬¬4éƒ¨åˆ†ï¼‰
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/courage-to-learn-ml-demystifying-l1-l2-regularization-part-4-27c13dc250f9](https://towardsdatascience.com/courage-to-learn-ml-demystifying-l1-l2-regularization-part-4-27c13dc250f9)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/courage-to-learn-ml-demystifying-l1-l2-regularization-part-4-27c13dc250f9](https://towardsdatascience.com/courage-to-learn-ml-demystifying-l1-l2-regularization-part-4-27c13dc250f9)
- en: Explore L1 & L2 Regularization as Bayesian Priors
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¢ç´¢ L1 å’Œ L2 æ­£åˆ™åŒ–ä½œä¸ºè´å¶æ–¯å…ˆéªŒ
- en: '[](https://amyma101.medium.com/?source=post_page-----27c13dc250f9--------------------------------)[![Amy
    Ma](../Images/2edf55456a1f92724535a1441fa2bef5.png)](https://amyma101.medium.com/?source=post_page-----27c13dc250f9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----27c13dc250f9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----27c13dc250f9--------------------------------)
    [Amy Ma](https://amyma101.medium.com/?source=post_page-----27c13dc250f9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://amyma101.medium.com/?source=post_page-----27c13dc250f9--------------------------------)[![Amy
    Ma](../Images/2edf55456a1f92724535a1441fa2bef5.png)](https://amyma101.medium.com/?source=post_page-----27c13dc250f9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----27c13dc250f9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----27c13dc250f9--------------------------------)
    [Amy Ma](https://amyma101.medium.com/?source=post_page-----27c13dc250f9--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----27c13dc250f9--------------------------------)
    Â·8 min readÂ·Dec 11, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----27c13dc250f9--------------------------------)
    Â·é˜…è¯»æ—¶é—´ 8 åˆ†é’ŸÂ·2023å¹´12æœˆ11æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/22bdc3089e02a827c9315e4de1e8cd0f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22bdc3089e02a827c9315e4de1e8cd0f.png)'
- en: Photo by [Dominik JirovskÃ½](https://unsplash.com/@dominik_jirovsky?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ç…§ç‰‡ç”± [Dominik JirovskÃ½](https://unsplash.com/@dominik_jirovsky?utm_source=medium&utm_medium=referral)
    åœ¨ [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) æä¾›
- en: 'Welcome back to â€˜[Courage to Learn ML](/towardsdatascience.com/tagged/courage-to-learn-ml):
    Unraveling L1 & L2 Regularization,â€™ in its fourth post. Last time, our mentor-learner
    pair explored [the properties of L1 and L2 regularization through the lens of
    Lagrange Multipliers](https://medium.com/p/ee27cd4b557a).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æ¬¢è¿å›åˆ°â€˜[å‹‡æ•¢å­¦ä¹  ML](/towardsdatascience.com/tagged/courage-to-learn-ml)ï¼šæ­ç¤º L1 å’Œ L2
    æ­£åˆ™åŒ–â€™ï¼Œè¿™æ˜¯ç¬¬å››ç¯‡æ–‡ç« ã€‚ä¸Šæ¬¡ï¼Œæˆ‘ä»¬çš„å¯¼å¸ˆ-å­¦ä¹ è€…é…å¯¹é€šè¿‡ [æ‹‰æ ¼æœ—æ—¥ä¹˜å­è§†è§’æ¢è®¨äº† L1 å’Œ L2 æ­£åˆ™åŒ–çš„æ€§è´¨](https://medium.com/p/ee27cd4b557a)ã€‚
- en: In this concluding segment on L1 and L2 regularization, the duo will delve into
    these topics from a fresh angle â€” [Bayesian priors](https://medium.com/p/65218b2c2b99).
    Weâ€™ll also summarize how L1 and L2 regularizations are applied across different
    algorithms.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å…³äº L1 å’Œ L2 æ­£åˆ™åŒ–çš„æ€»ç»“éƒ¨åˆ†ï¼Œè¿™å¯¹å°†ä»æ–°çš„è§’åº¦æ¢è®¨è¿™äº›è¯é¢˜â€”â€”[è´å¶æ–¯å…ˆéªŒ](https://medium.com/p/65218b2c2b99)ã€‚æˆ‘ä»¬è¿˜å°†æ€»ç»“
    L1 å’Œ L2 æ­£åˆ™åŒ–å¦‚ä½•åœ¨ä¸åŒç®—æ³•ä¸­åº”ç”¨ã€‚
- en: In this article, weâ€™ll address several intriguing questions. If any of these
    topics spark your curiosity, youâ€™ve come to the right place!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºå‡ ä¸ªå¼•äººå…¥èƒœçš„é—®é¢˜ã€‚å¦‚æœè¿™äº›è¯é¢˜æ¿€èµ·äº†ä½ çš„å¥½å¥‡å¿ƒï¼Œä½ æ¥å¯¹åœ°æ–¹äº†ï¼
- en: How MAP priors relate to L1 and L2 regularizations
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MAP å…ˆéªŒå¦‚ä½•ä¸ L1 å’Œ L2 æ­£åˆ™åŒ–ç›¸å…³
- en: An intuitive breakdown of using Laplace and normal distributions as priors
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹ä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯å’Œæ­£æ€åˆ†å¸ƒä½œä¸ºå…ˆéªŒçš„ç›´è§‚åˆ†æ
- en: Understanding the sparsity induced by L1 regularization with a Laplace prior
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç†è§£ L1 æ­£åˆ™åŒ–ä¸æ‹‰æ™®æ‹‰æ–¯å…ˆéªŒå¼•èµ·çš„ç¨€ç–æ€§
- en: Algorithms that are compatible with L1 and L2 regularization
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ L1 å’Œ L2 æ­£åˆ™åŒ–å…¼å®¹çš„ç®—æ³•
- en: Why L2 regularization is often referred to as â€˜weight decayâ€™ in neural network
    training
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆ L2 æ­£åˆ™åŒ–åœ¨ç¥ç»ç½‘ç»œè®­ç»ƒä¸­é€šå¸¸è¢«ç§°ä¸ºâ€œæƒé‡è¡°å‡â€
- en: The reasons behind the less frequent use of L1 norm in neural networks
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L1 èŒƒæ•°åœ¨ç¥ç»ç½‘ç»œä¸­ä½¿ç”¨è¾ƒå°‘çš„åŸå› 
- en: '**So, weâ€™ve talked about how MAP differs from MLE, mainly because MAP takes
    into account an extra piece of information: our beliefs before seeing the data,
    or the prior. How does this tie in with L1 and L2 regularizations?**'
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**æ‰€ä»¥ï¼Œæˆ‘ä»¬è®¨è®ºäº† MAP å¦‚ä½•ä¸åŒäº MLEï¼Œä¸»è¦å› ä¸º MAP è€ƒè™‘äº†ä¸€ä¸ªé¢å¤–çš„ä¿¡æ¯ï¼šåœ¨çœ‹åˆ°æ•°æ®ä¹‹å‰çš„ä¿¡å¿µï¼Œå³å…ˆéªŒã€‚è¿™ä¸ L1 å’Œ L2 æ­£åˆ™åŒ–æœ‰ä½•å…³ç³»ï¼Ÿ**'
- en: Letâ€™s dive into how different priors in the MAP formula shape our approach to
    L1 and L2 regularization (for a detailed walkthrough on formulating this equation,
    check out [this post](https://medium.com/p/65218b2c2b99)).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ·±å…¥æ¢è®¨ MAP å…¬å¼ä¸­ä¸åŒå…ˆéªŒå¦‚ä½•å¡‘é€ æˆ‘ä»¬å¯¹ L1 å’Œ L2 æ­£åˆ™åŒ–çš„ç†è§£ï¼ˆæœ‰å…³å¦‚ä½•åˆ¶å®šæ­¤æ–¹ç¨‹å¼çš„è¯¦ç»†è®²è§£ï¼Œè¯·æŸ¥çœ‹ [è¿™ç¯‡æ–‡ç« ](https://medium.com/p/65218b2c2b99)ï¼‰ã€‚
- en: When considering priors for weights, Our initial intuition often leads us to
    choose a **normal distribution** as the prior for model weights. With this, we
    typically use a zero-mean normal distribution for each weight wi, sharing the
    same standard deviation ğœ. Plugging this belief into the prior term logp(w) in
    MAP (where p(w) represents the weightâ€™s prior) leads us to **sum of squared weights**
    naturally. This term is precisely the **L2 norm**. This implies that using a normal
    distribution as our prior equates to applying L2 regularization.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è€ƒè™‘æƒé‡çš„å…ˆéªŒæ—¶ï¼Œæˆ‘ä»¬çš„åˆæ­¥ç›´è§‰é€šå¸¸ä¼šä½¿æˆ‘ä»¬é€‰æ‹©**æ­£æ€åˆ†å¸ƒ**ä½œä¸ºæ¨¡å‹æƒé‡çš„å…ˆéªŒã€‚è¿™æ ·ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šå¯¹æ¯ä¸ªæƒé‡wiä½¿ç”¨å‡å€¼ä¸ºé›¶çš„æ­£æ€åˆ†å¸ƒï¼Œå¹¶å…·æœ‰ç›¸åŒçš„æ ‡å‡†å·®ğœã€‚å°†è¿™ç§ä¿¡å¿µä»£å…¥MAPä¸­çš„å…ˆéªŒé¡¹logp(w)ï¼ˆå…¶ä¸­p(w)è¡¨ç¤ºæƒé‡çš„å…ˆéªŒï¼‰ä¼šè‡ªç„¶å¾—åˆ°**æƒé‡å¹³æ–¹å’Œ**ã€‚è¿™ä¸ªé¡¹æ­£æ˜¯**L2èŒƒæ•°**ã€‚è¿™æ„å‘³ç€ä½¿ç”¨æ­£æ€åˆ†å¸ƒä½œä¸ºæˆ‘ä»¬çš„å…ˆéªŒç­‰åŒäºåº”ç”¨L2æ­£åˆ™åŒ–ã€‚
- en: '![](../Images/416c579fb6538b64246da161a2cd4b07.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/416c579fb6538b64246da161a2cd4b07.png)'
- en: Conversely, adopting a **Laplace distribution** as our belief results in the
    **L1 norm** for weights. Hence, a Laplace prior essentially translates to L1 regularization.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸åï¼Œé‡‡ç”¨**æ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒ**ä½œä¸ºæˆ‘ä»¬çš„ä¿¡å¿µä¼šå¯¼è‡´**L1èŒƒæ•°**çš„æƒé‡ã€‚å› æ­¤ï¼Œæ‹‰æ™®æ‹‰æ–¯å…ˆéªŒæœ¬è´¨ä¸Šç›¸å½“äºL1æ­£åˆ™åŒ–ã€‚
- en: '![](../Images/b01952bbb6dbbe648bdb8f04ba569cbd.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b01952bbb6dbbe648bdb8f04ba569cbd.png)'
- en: In short, L1 regularization aligns with a Laplace distribution prior, while
    L2 regularization corresponds to a Normal distribution prior.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ç®€è€Œè¨€ä¹‹ï¼ŒL1æ­£åˆ™åŒ–ä¸æ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒå…ˆéªŒä¸€è‡´ï¼Œè€ŒL2æ­£åˆ™åŒ–å¯¹åº”äºæ­£æ€åˆ†å¸ƒå…ˆéªŒã€‚
- en: Interestingly, when employing a uniform prior in the MAP framework, it essentially
    â€œdisappearsâ€ from the equation (go ahead and try it yourself!). This leaves the
    likelihood term as the sole determinant of the optimal weight values, effectively
    transforming the MAP estimation into maximum likelihood estimation (MLE).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰è¶£çš„æ˜¯ï¼Œå½“åœ¨MAPæ¡†æ¶ä¸­ä½¿ç”¨å‡åŒ€å…ˆéªŒæ—¶ï¼Œå®ƒå®é™…ä¸Šåœ¨æ–¹ç¨‹ä¸­â€œæ¶ˆå¤±â€äº†ï¼ˆä½ å¯ä»¥è‡ªå·±å°è¯•ä¸€ä¸‹ï¼ï¼‰ã€‚è¿™ä½¿å¾—ä¼¼ç„¶é¡¹æˆä¸ºç¡®å®šæœ€ä½³æƒé‡å€¼çš„å”¯ä¸€å› ç´ ï¼Œå®é™…ä¸Šå°†MAPä¼°è®¡è½¬å˜ä¸ºæœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆMLEï¼‰ã€‚
- en: '**So, can you explain the reasoning for having different beliefs when our prior
    is a Laplace distribution versus a normal distribution? Iâ€™d like to visualize
    this better.**'
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**é‚£ä¹ˆï¼Œå½“æˆ‘ä»¬çš„å…ˆéªŒæ˜¯æ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒä¸æ­£æ€åˆ†å¸ƒæ—¶ï¼Œä¸ºä»€ä¹ˆä¼šæœ‰ä¸åŒçš„ä¿¡å¿µï¼Ÿæˆ‘æƒ³æ›´å¥½åœ°å¯è§†åŒ–è¿™ä¸ªé—®é¢˜ã€‚**'
- en: This is a great question. Indeed, having different priors means you hold various
    initial assumptions about the situation before collecting any data. Weâ€™ll delve
    into the purpose of different distributions later, but for now, letâ€™s look at
    a simple, intuitive example using Laplace and normal distributions. Consider the
    number of views on my new Medium posts. Two weeks ago, as a new writer with no
    followers, I expected zero views. My assumption was that the average daily view
    count would start low, possibly at zero, but might increase as readers interested
    in similar topics discover my work. A Laplace prior fits this scenario well. It
    suggests a range of possible view counts but assigns higher probability to numbers
    near zero, reflecting my expectation of few views initially but allowing for growth
    over time.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é—®é¢˜ã€‚ç¡®å®ï¼Œä¸åŒçš„å…ˆéªŒæ„å‘³ç€ä½ åœ¨æ”¶é›†ä»»ä½•æ•°æ®ä¹‹å‰å¯¹æƒ…å†µæŒæœ‰ä¸åŒçš„åˆæ­¥å‡è®¾ã€‚æˆ‘ä»¬ç¨åå°†æ·±å…¥æ¢è®¨ä¸åŒåˆ†å¸ƒçš„ç›®çš„ï¼Œä½†ç°åœ¨ï¼Œè®©æˆ‘ä»¬é€šè¿‡ä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒå’Œæ­£æ€åˆ†å¸ƒæ¥è€ƒè™‘ä¸€ä¸ªç®€å•ç›´è§‚çš„ä¾‹å­ã€‚è€ƒè™‘ä¸€ä¸‹æˆ‘æ–°å‘å¸ƒçš„Mediumå¸–å­ã€‚ä¸¤å‘¨å‰ï¼Œä½œä¸ºä¸€åæ²¡æœ‰ç²‰ä¸çš„æ–°ä½œè€…ï¼Œæˆ‘é¢„æœŸæ²¡æœ‰æŸ¥çœ‹æ¬¡æ•°ã€‚æˆ‘çš„å‡è®¾æ˜¯ï¼Œå¹³å‡æ¯æ—¥æŸ¥çœ‹æ¬¡æ•°èµ·åˆä¼šå¾ˆä½ï¼Œå¯èƒ½ä¸ºé›¶ï¼Œä½†éšç€å¯¹ç±»ä¼¼ä¸»é¢˜æ„Ÿå…´è¶£çš„è¯»è€…å‘ç°æˆ‘çš„ä½œå“ï¼Œè¿™ä¸€æ•°å­—å¯èƒ½ä¼šå¢åŠ ã€‚æ‹‰æ™®æ‹‰æ–¯å…ˆéªŒå¾ˆå¥½åœ°é€‚åº”äº†è¿™ç§æƒ…å†µã€‚å®ƒå»ºè®®äº†ä¸€ç³»åˆ—å¯èƒ½çš„æŸ¥çœ‹æ¬¡æ•°ï¼Œä½†å¯¹æ¥è¿‘é›¶çš„æ•°å­—èµ‹äºˆæ›´é«˜çš„æ¦‚ç‡ï¼Œåæ˜ äº†æˆ‘å¯¹åˆæœŸæŸ¥çœ‹æ¬¡æ•°å¾ˆå°‘ä½†éšæ—¶é—´å¢é•¿çš„é¢„æœŸã€‚
- en: Now, with 55 viewers (thanks, everyone!), and followers who receive updates
    on my posts, my expectations have changed. I anticipate that new posts will perform
    similarly to my previous ones, averaging around my historical view count. This
    is where a normal distribution prior comes into play, predicting future views
    based on my established track record.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æœ‰55ä½è§‚ä¼—ï¼ˆè°¢è°¢å¤§å®¶ï¼ï¼‰ï¼Œä»¥åŠå…³æ³¨æˆ‘å¸–å­æ›´æ–°çš„ç²‰ä¸ï¼Œæˆ‘çš„æœŸæœ›å·²ç»æ”¹å˜ã€‚æˆ‘é¢„æœŸæ–°å¸–å­å°†è¡¨ç°ç±»ä¼¼äºä¹‹å‰çš„å¸–å­ï¼Œå¹³å‡æŸ¥çœ‹æ¬¡æ•°æ¥è¿‘æˆ‘å†å²ä¸Šçš„æŸ¥çœ‹æ¬¡æ•°ã€‚è¿™æ—¶ï¼Œæ­£æ€åˆ†å¸ƒå…ˆéªŒå‘æŒ¥ä½œç”¨ï¼Œæ ¹æ®æˆ‘å·²å»ºç«‹çš„è®°å½•é¢„æµ‹æœªæ¥çš„æŸ¥çœ‹æ¬¡æ•°ã€‚
- en: '**Hmmâ€¦ Can you explain the L1 regularization sparsity with a Laplace prior?**'
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**å—¯â€¦â€¦ä½ èƒ½è§£é‡Šä¸€ä¸‹ä½¿ç”¨æ‹‰æ™®æ‹‰æ–¯å…ˆéªŒçš„L1æ­£åˆ™åŒ–çš„ç¨€ç–æ€§å—ï¼Ÿ**'
- en: Indeed, understanding L1 regularizationâ€™s promotion of sparsity can be illuminated
    by comparing the Laplace distribution to the normal distribution. **The key difference
    lies in their probability densities around zero.**** The Laplace distribution
    is sharply peaked at zero, indicating a higher likelihood of values close to zero.
    This characteristic mirrors the effect of L1 regularization, where most weights
    in the model are driven towards zero, promoting sparsity. In contrast, the normal
    distribution, associated with L2 regularization, is less peaked at zero and more
    spread out, indicating a preference for distributing weights more evenly.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡®å®ï¼Œé€šè¿‡æ¯”è¾ƒæ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒå’Œæ­£æ€åˆ†å¸ƒï¼Œå¯ä»¥æ­ç¤º L1 æ­£åˆ™åŒ–ä¿ƒè¿›ç¨€ç–æ€§çš„æœºåˆ¶ã€‚**å…³é”®çš„åŒºåˆ«åœ¨äºå®ƒä»¬åœ¨é›¶é™„è¿‘çš„æ¦‚ç‡å¯†åº¦ã€‚** æ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒåœ¨é›¶é™„è¿‘æœ‰ä¸€ä¸ªå°–é”çš„å³°å€¼ï¼Œè¡¨æ˜æ¥è¿‘é›¶çš„å€¼çš„å¯èƒ½æ€§è¾ƒé«˜ã€‚è¿™ä¸€ç‰¹å¾ç±»ä¼¼äº
    L1 æ­£åˆ™åŒ–çš„æ•ˆæœï¼Œå…¶ä¸­æ¨¡å‹ä¸­çš„å¤§å¤šæ•°æƒé‡è¢«é©±åŠ¨åˆ°é›¶ï¼Œä¿ƒè¿›äº†ç¨€ç–æ€§ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæ­£æ€åˆ†å¸ƒä¸ L2 æ­£åˆ™åŒ–ç›¸å…³ï¼Œåœ¨é›¶é™„è¿‘çš„å³°å€¼è¾ƒä½ï¼Œåˆ†å¸ƒè¾ƒå¹¿ï¼Œè¡¨ç¤ºå¯¹æƒé‡çš„æ›´å‡åŒ€åˆ†é…æœ‰åå¥½ã€‚
- en: '![](../Images/49676c748501a234e00ec94b78953135.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/49676c748501a234e00ec94b78953135.png)'
- en: 'source: [https://austinrochford.com/posts/2013-09-02-prior-distributions-for-bayesian-regression-using-pymc.html](https://austinrochford.com/posts/2013-09-02-prior-distributions-for-bayesian-regression-using-pymc.html)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ¥æº: [https://austinrochford.com/posts/2013-09-02-prior-distributions-for-bayesian-regression-using-pymc.html](https://austinrochford.com/posts/2013-09-02-prior-distributions-for-bayesian-regression-using-pymc.html)'
- en: Additionally, the Laplace distribution has **heavier tails** than the normal
    distribution, meaning it extends further out. This property allows for some weights
    to remain significantly away from zero while the others are pretty close to zero.
    So, by choosing the Laplace distribution as a prior for the weights (L1 regularization)
    , we encourage the model to learn solutions where most weights are close to zero,
    achieving sparsity without sacrificing potentially relevant features. This is
    why L1 regularization can be used as a feature selection method.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒçš„**å°¾éƒ¨æ›´é‡**ï¼Œæ„å‘³ç€å…¶å»¶ä¼¸å¾—æ›´è¿œã€‚è¿™ä¸€ç‰¹æ€§å…è®¸ä¸€äº›æƒé‡æ˜¾è‘—è¿œç¦»é›¶ï¼Œè€Œå…¶ä»–æƒé‡åˆ™éå¸¸æ¥è¿‘é›¶ã€‚å› æ­¤ï¼Œé€šè¿‡é€‰æ‹©æ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒä½œä¸ºæƒé‡çš„å…ˆéªŒï¼ˆL1
    æ­£åˆ™åŒ–ï¼‰ï¼Œæˆ‘ä»¬é¼“åŠ±æ¨¡å‹å­¦ä¹ è§£å†³æ–¹æ¡ˆï¼Œä½¿å¤§å¤šæ•°æƒé‡æ¥è¿‘é›¶ï¼Œå®ç°ç¨€ç–æ€§è€Œä¸ç‰ºç‰²æ½œåœ¨çš„ç›¸å…³ç‰¹å¾ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ L1 æ­£åˆ™åŒ–å¯ä»¥ä½œä¸ºç‰¹å¾é€‰æ‹©æ–¹æ³•ã€‚
- en: '**So, I see that L1 and L2 regularizations are key for avoiding overfitting
    and boosting a modelâ€™s generalizability. Can you tell me which algorithms these
    methods can be applied to?**'
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**æ‰€ä»¥ï¼Œæˆ‘è®¤ä¸º L1 å’Œ L2 æ­£åˆ™åŒ–å¯¹äºé¿å…è¿‡æ‹Ÿåˆå’Œæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›è‡³å…³é‡è¦ã€‚ä½ èƒ½å‘Šè¯‰æˆ‘è¿™äº›æ–¹æ³•å¯ä»¥åº”ç”¨äºå“ªäº›ç®—æ³•å—ï¼Ÿ**'
- en: 'L1, L2 regularization can be apply to many algorithms by adding a penalty term
    to their loss functions. Here are some specific examples of algorithms where L1
    and L2 regularization are applied:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: L1 å’Œ L2 æ­£åˆ™åŒ–å¯ä»¥é€šè¿‡å‘æŸå¤±å‡½æ•°ä¸­æ·»åŠ æƒ©ç½šé¡¹æ¥åº”ç”¨äºè®¸å¤šç®—æ³•ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›åº”ç”¨ L1 å’Œ L2 æ­£åˆ™åŒ–çš„å…·ä½“ç®—æ³•ç¤ºä¾‹ï¼š
- en: '**Linear models.** Those techniques are particularly useful with high-dimensional
    problems. In linear models, they are known as **lasso and ridge regression**,
    respectively. One thing to note is that L1 regularization not only helps prevent
    overfitting but also helps with feature selection which preventing multi-collinearity.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**çº¿æ€§æ¨¡å‹ã€‚** è¿™äº›æŠ€æœ¯åœ¨é«˜ç»´é—®é¢˜ä¸­ç‰¹åˆ«æœ‰ç”¨ã€‚åœ¨çº¿æ€§æ¨¡å‹ä¸­ï¼Œå®ƒä»¬è¢«ç§°ä¸º**lasso å’Œ ridge å›å½’**ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒL1 æ­£åˆ™åŒ–ä¸ä»…æœ‰åŠ©äºé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œè¿˜æœ‰åŠ©äºç‰¹å¾é€‰æ‹©ï¼Œä»è€Œé˜²æ­¢å¤šé‡å…±çº¿æ€§ã€‚'
- en: '**SVM.** Regularization methods are **the core of SVM**. By adding the weight
    penalty term, SVM encourages the model to reduce its margin between decision boundary
    and the closest support vector (L1 regularization) or smooth the margin (L2 regularization),
    which leads to better generalization.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ”¯æŒå‘é‡æœºã€‚** æ­£åˆ™åŒ–æ–¹æ³•æ˜¯**SVM çš„æ ¸å¿ƒ**ã€‚é€šè¿‡æ·»åŠ æƒé‡æƒ©ç½šé¡¹ï¼ŒSVM é¼“åŠ±æ¨¡å‹å‡å°‘å†³ç­–è¾¹ç•Œä¸æœ€è¿‘æ”¯æŒå‘é‡ä¹‹é—´çš„é—´è·ï¼ˆL1 æ­£åˆ™åŒ–ï¼‰æˆ–å¹³æ»‘é—´è·ï¼ˆL2
    æ­£åˆ™åŒ–ï¼‰ï¼Œä»è€Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚'
- en: '**Neural Networks**. L2 regularization is more commonly used in neural networks
    and is often referred to as **weight decay**. L1 regularization can also be used
    in neural networks, but it is less common due to its tendency to lead to sparse
    weights.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç¥ç»ç½‘ç»œã€‚** L2 æ­£åˆ™åŒ–åœ¨ç¥ç»ç½‘ç»œä¸­ä½¿ç”¨å¾—æ›´ä¸ºæ™®éï¼Œé€šå¸¸è¢«ç§°ä¸º**æƒé‡è¡°å‡**ã€‚L1 æ­£åˆ™åŒ–ä¹Ÿå¯ä»¥åœ¨ç¥ç»ç½‘ç»œä¸­ä½¿ç”¨ï¼Œä½†ç”±äºå…¶å€¾å‘äºå¯¼è‡´ç¨€ç–æƒé‡ï¼Œå› æ­¤è¾ƒå°‘è§ã€‚'
- en: '**Ensemble algorithms.** Gradient boosting machines like GBM and Xgboost use
    L1 and L2 regularization to **limit the size of individual trees** within the
    ensemble. L1 regularization specifically achieves this by shrinking the weights
    of weak learners (each tree) in the ensemble, while L2 regularization penalizes
    the total number of leaves (leaf score) in each tree.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é›†æˆç®—æ³•ã€‚**åƒGBMå’ŒXgboostè¿™æ ·çš„æ¢¯åº¦æå‡æœºå™¨ä½¿ç”¨L1å’ŒL2æ­£åˆ™åŒ–æ¥**é™åˆ¶é›†æˆä¸­å•ä¸ªæ ‘çš„å¤§å°**ã€‚L1æ­£åˆ™åŒ–é€šè¿‡ç¼©å°é›†æˆä¸­å¼±å­¦ä¹ å™¨ï¼ˆæ¯æ£µæ ‘ï¼‰çš„æƒé‡æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œè€ŒL2æ­£åˆ™åŒ–åˆ™æƒ©ç½šæ¯æ£µæ ‘ä¸­çš„æ€»å¶å­æ•°ï¼ˆå¶å­å¾—åˆ†ï¼‰ã€‚'
- en: '**Why do L2 regularization is also called â€˜weight decayâ€™ in neural network
    training? And why is the L1 norm less commonly used in neural networks?**'
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ä¸ºä»€ä¹ˆL2æ­£åˆ™åŒ–ä¹Ÿè¢«ç§°ä¸ºç¥ç»ç½‘ç»œè®­ç»ƒä¸­çš„â€˜æƒé‡è¡°å‡â€™ï¼Ÿä¸ºä»€ä¹ˆL1èŒƒæ•°åœ¨ç¥ç»ç½‘ç»œä¸­ä½¿ç”¨è¾ƒå°‘ï¼Ÿ**'
- en: To tackle those two questions, letâ€™s bring in a bit of math to illustrate how
    weights get updated in the presence of L1 and L2 regularizations.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å›ç­”è¿™ä¸¤ä¸ªé—®é¢˜ï¼Œè®©æˆ‘ä»¬å¼•å…¥ä¸€äº›æ•°å­¦æ¥è¯´æ˜åœ¨L1å’ŒL2æ­£åˆ™åŒ–ä¸‹æƒé‡æ˜¯å¦‚ä½•æ›´æ–°çš„ã€‚
- en: '![](../Images/9901a688faa6f94f3cc1f00892ad1e5d.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9901a688faa6f94f3cc1f00892ad1e5d.png)'
- en: Loss function with L2 regularization; Î» is penalty coefficient and Î± represents
    learning rate
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¦æœ‰L2æ­£åˆ™åŒ–çš„æŸå¤±å‡½æ•°ï¼›Î»æ˜¯æƒ©ç½šç³»æ•°ï¼ŒÎ±ä»£è¡¨å­¦ä¹ ç‡ã€‚
- en: In L2 regularization, the weight update process involves a slight reduction
    of the weights, scaled down according to their own magnitude. This results in
    what is termed as â€œweight decay.â€ Specifically, **each weight is decreased by
    an amount that is directly proportional to its current value.** This proportional
    reduction, governed by the typically small settings of the penalty coefficient
    (Î») and the learning rate (Î±), ensures that larger weights are subjected to a
    higher degree of penalization compared to smaller weights. The essence of weight
    decay lies in this method of scaling down weights, encouraging the model to maintain
    smaller weights. Such behavior is advantageous in neural networks as it tends
    to produce smoother decision boundary.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨L2æ­£åˆ™åŒ–ä¸­ï¼Œæƒé‡æ›´æ–°è¿‡ç¨‹æ¶‰åŠå¯¹æƒé‡çš„è½»å¾®å‡å°‘ï¼Œæ ¹æ®å…¶è‡ªèº«çš„å¤§å°ç¼©æ”¾ã€‚è¿™å°±æ˜¯æ‰€è°“çš„â€œæƒé‡è¡°å‡â€ã€‚å…·ä½“æ¥è¯´ï¼Œ**æ¯ä¸ªæƒé‡å‡å°‘çš„é‡ä¸å…¶å½“å‰å€¼ç›´æ¥æˆæ¯”ä¾‹ã€‚**è¿™ç§æ¯”ä¾‹å‡å°‘ï¼Œç”±é€šå¸¸è¾ƒå°çš„æƒ©ç½šç³»æ•°ï¼ˆÎ»ï¼‰å’Œå­¦ä¹ ç‡ï¼ˆÎ±ï¼‰è®¾ç½®æ‰€æ§åˆ¶ï¼Œç¡®ä¿è¾ƒå¤§çš„æƒé‡æ¯”å°çš„æƒé‡å—åˆ°æ›´é«˜ç¨‹åº¦çš„æƒ©ç½šã€‚æƒé‡è¡°å‡çš„æœ¬è´¨åœ¨äºè¿™ç§ç¼©å°æƒé‡çš„æ–¹å¼ï¼Œé¼“åŠ±æ¨¡å‹ä¿æŒè¾ƒå°çš„æƒé‡ã€‚è¿™ç§è¡Œä¸ºåœ¨ç¥ç»ç½‘ç»œä¸­æ˜¯æœ‰åˆ©çš„ï¼Œå› ä¸ºå®ƒè¶‹å‘äºäº§ç”Ÿæ›´å¹³æ»‘çš„å†³ç­–è¾¹ç•Œã€‚
- en: '![](../Images/e43d75bcb2594d241d9ebd040600a708.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e43d75bcb2594d241d9ebd040600a708.png)'
- en: Loss function with L1 regularization; Î» is penalty coefficient and Î± represents
    learning rate
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¦æœ‰L1æ­£åˆ™åŒ–çš„æŸå¤±å‡½æ•°ï¼›Î»æ˜¯æƒ©ç½šç³»æ•°ï¼ŒÎ±ä»£è¡¨å­¦ä¹ ç‡ã€‚
- en: In contrast, **L1 regularization modifies the weight update rule by subtracting
    or adding a constant amount, determined by Î±Î» and the sign of the weight (w).**
    This approach pushes weights towards zero, regardless of whether they are positive
    or negative. Under L1 regularization, all weights, irrespective of their magnitude,
    are adjusted by the same fixed amount. This results in larger weights remaining
    relatively large, while smaller weights are more rapidly driven to zero, promoting
    sparsity in the network.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸æ¯”ä¹‹ä¸‹ï¼Œ**L1æ­£åˆ™åŒ–é€šè¿‡å‡å»æˆ–æ·»åŠ ä¸€ä¸ªç”±Î±Î»å’Œæƒé‡ï¼ˆwï¼‰ç¬¦å·ç¡®å®šçš„å›ºå®šé‡æ¥ä¿®æ”¹æƒé‡æ›´æ–°è§„åˆ™ã€‚**è¿™ç§æ–¹æ³•å°†æƒé‡æ¨å‘é›¶ï¼Œæ— è®ºå®ƒä»¬æ˜¯æ­£çš„è¿˜æ˜¯è´Ÿçš„ã€‚åœ¨L1æ­£åˆ™åŒ–ä¸‹ï¼Œæ‰€æœ‰æƒé‡ï¼Œä¸è®ºå…¶å¤§å°ï¼Œéƒ½ä»¥ç›¸åŒçš„å›ºå®šé‡è¿›è¡Œè°ƒæ•´ã€‚è¿™ä½¿å¾—è¾ƒå¤§çš„æƒé‡ä¿æŒç›¸å¯¹è¾ƒå¤§ï¼Œè€Œè¾ƒå°çš„æƒé‡åˆ™æ›´å¿«åœ°è¢«é©±åŠ¨åˆ°é›¶ï¼Œä»è€Œä¿ƒè¿›ç½‘ç»œçš„ç¨€ç–æ€§ã€‚
- en: '![](../Images/93302c503c331685355b32d0adb09ee2.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/93302c503c331685355b32d0adb09ee2.png)'
- en: Letâ€™s compare!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¯”è¾ƒä¸€ä¸‹ï¼
- en: 'Comparing the two, L2â€™s approach to weight modification is based on the weightâ€™s
    existing value, leading to larger weights diminishing more quickly than smaller
    ones. This uniform decay across all weights is why itâ€™s termed â€˜weight decayâ€™.
    On the other hand, L1â€™s **fixed adjustment amount, regardless of weight size**,
    can lead to some issues and become less favorable in NN:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸æ¯”ä¹‹ä¸‹ï¼ŒL2å¯¹æƒé‡çš„ä¿®æ”¹æ–¹å¼æ˜¯åŸºäºæƒé‡çš„ç°æœ‰å€¼ï¼Œä½¿å¾—è¾ƒå¤§çš„æƒé‡æ¯”å°çš„æƒé‡è¡°å‡å¾—æ›´å¿«ã€‚è¿™ç§å¯¹æ‰€æœ‰æƒé‡çš„å‡åŒ€è¡°å‡å°±æ˜¯ä¸ºä»€ä¹ˆå®ƒè¢«ç§°ä¸ºâ€˜æƒé‡è¡°å‡â€™ã€‚å¦ä¸€æ–¹é¢ï¼ŒL1çš„**å›ºå®šè°ƒæ•´é‡ï¼Œä¸è®ºæƒé‡å¤§å°**ï¼Œå¯èƒ½å¯¼è‡´ä¸€äº›é—®é¢˜ï¼Œå¹¶åœ¨ç¥ç»ç½‘ç»œä¸­å˜å¾—ä¸é‚£ä¹ˆå—æ¬¢è¿ï¼š
- en: It can zero out some weights, causing **â€˜dead neuronsâ€™** and potentially disrupting
    information flow within the network, which could impair model performance.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®ƒå¯èƒ½å°†ä¸€äº›æƒé‡å½’é›¶ï¼Œå¯¼è‡´**â€˜æ­»ç¥ç»å…ƒâ€™**ï¼Œå¹¶å¯èƒ½ç ´åç½‘ç»œä¸­çš„ä¿¡æ¯æµï¼Œè¿™å¯èƒ½å½±å“æ¨¡å‹æ€§èƒ½ã€‚
- en: '**The non-differentiable points at zero** introduced by L1 make optimization
    algorithms like gradient descent less effective.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L1å¼•å…¥çš„é›¶ç‚¹å¤„çš„ä¸å¯å¾®åˆ†ç‚¹**ä½¿å¾—åƒæ¢¯åº¦ä¸‹é™è¿™æ ·çš„ä¼˜åŒ–ç®—æ³•æ•ˆæœè¾ƒå·®ã€‚'
- en: '**What effects do adding L1 and L2 regularization have on our loss function?
    Does incorporating these regularizations lead us away from the original global
    minimum?**'
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**æ·»åŠ  L1 å’Œ L2 æ­£åˆ™åŒ–å¯¹æˆ‘ä»¬çš„æŸå¤±å‡½æ•°æœ‰ä»€ä¹ˆå½±å“ï¼Ÿè¿™äº›æ­£åˆ™åŒ–çš„å¼•å…¥æ˜¯å¦ä½¿æˆ‘ä»¬è¿œç¦»åŸå§‹çš„å…¨å±€æœ€å°å€¼ï¼Ÿ**'
- en: Itâ€™s a great question! In short, **once we incorporate regularization, we intentionally
    shift our focus away from the original global minimum.** This means adding penalty
    terms to the loss function, fundamentally changing its landscape. Itâ€™s crucial
    to understand that this change is desirable, not accidental.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸ªå¾ˆå¥½çš„é—®é¢˜ï¼ç®€è€Œè¨€ä¹‹ï¼Œ**ä¸€æ—¦æˆ‘ä»¬å¼•å…¥æ­£åˆ™åŒ–ï¼Œæˆ‘ä»¬æœ‰æ„å°†å…³æ³¨ç‚¹ä»åŸå§‹çš„å…¨å±€æœ€å°å€¼è½¬ç§»å¼€ã€‚** è¿™æ„å‘³ç€å‘æŸå¤±å‡½æ•°ä¸­æ·»åŠ æƒ©ç½šé¡¹ï¼Œä»è€Œæ ¹æœ¬æ”¹å˜å…¶å½¢çŠ¶ã€‚ç†è§£è¿™ä¸€å˜åŒ–æ˜¯æœŸæœ›ä¸­çš„ï¼Œè€Œéå¶ç„¶å‘ç”Ÿçš„ï¼Œæ˜¯è‡³å…³é‡è¦çš„ã€‚
- en: 'By introducing these penalties, we aim to achieve a new optimal solution that
    balances two crucial goals: fitting the training data well to minimize empirical
    risk while simultaneously reducing model complexity and enhancing generalization
    to unseen data. The original global minimum might not achieve this balance, potentially
    leading to overfitting and poor performance on new data.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å¼•å…¥è¿™äº›æƒ©ç½šï¼Œæˆ‘ä»¬æ—¨åœ¨å®ç°ä¸€ä¸ªæ–°çš„æœ€ä¼˜è§£ï¼Œå¹³è¡¡ä¸¤ä¸ªå…³é”®ç›®æ ‡ï¼šè‰¯å¥½åœ°æ‹Ÿåˆè®­ç»ƒæ•°æ®ä»¥æœ€å°åŒ–ç»éªŒé£é™©ï¼ŒåŒæ—¶å‡å°‘æ¨¡å‹å¤æ‚åº¦å¹¶å¢å¼ºå¯¹æœªè§æ•°æ®çš„æ³›åŒ–ã€‚åŸå§‹çš„å…¨å±€æœ€å°å€¼å¯èƒ½æ— æ³•å®ç°è¿™ç§å¹³è¡¡ï¼Œä»è€Œå¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆå’Œåœ¨æ–°æ•°æ®ä¸Šçš„æ€§èƒ½ä¸‹é™ã€‚
- en: If youâ€™re interested in the mathematical details of measuring the distance between
    the original and regularized optima, I highly recommend chapter 7 (pages 224â€“229)
    of Deep Learning by Ian Goodfellow. Pay particular attention to formulas 7.7 and
    7.13 for L2 and 7.22 and 7.23 for L1\. This provides a quantifiable assessment
    of the impact regularization terms have on weights, deepening your understanding
    of L1 and L2 regularization.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å¯¹æµ‹é‡åŸå§‹æœ€ä¼˜è§£å’Œæ­£åˆ™åŒ–åæœ€ä¼˜è§£ä¹‹é—´çš„è·ç¦»çš„æ•°å­¦ç»†èŠ‚æ„Ÿå…´è¶£ï¼Œæˆ‘å¼ºçƒˆæ¨è Ian Goodfellow çš„ã€Šæ·±åº¦å­¦ä¹ ã€‹ç¬¬ä¸ƒç« ï¼ˆç¬¬ 224â€“229 é¡µï¼‰ã€‚ç‰¹åˆ«æ³¨æ„å…¬å¼
    7.7 å’Œ 7.13ï¼ˆL2ï¼‰ä»¥åŠ 7.22 å’Œ 7.23ï¼ˆL1ï¼‰ã€‚è¿™å°†æä¾›å¯¹æ­£åˆ™åŒ–é¡¹å¯¹æƒé‡å½±å“çš„å®šé‡è¯„ä¼°ï¼Œæ·±å…¥äº†è§£ L1 å’Œ L2 æ­£åˆ™åŒ–ã€‚
- en: Weâ€™ve now reached the conclusion of our exploration into L1 and L2 regularization.
    In our next discussion, Iâ€™m excited to delve into the basics of loss functions.
    **A big thank you to all the readers who enjoyed the first part of this series.**
    Initially, my goal was to solidify my grasp of basic ML concepts, but Iâ€™m thrilled
    to see it resonate with many of you ğŸ˜ƒ. If you have suggestions for our next topic,
    please feel free to leave a comment!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å·²ç»ç»“æŸäº†å¯¹ L1 å’Œ L2 æ­£åˆ™åŒ–çš„æ¢è®¨ã€‚åœ¨ä¸‹æ¬¡è®¨è®ºä¸­ï¼Œæˆ‘å¾ˆå…´å¥‹åœ°å°†æ·±å…¥æ¢è®¨æŸå¤±å‡½æ•°çš„åŸºç¡€çŸ¥è¯†ã€‚**éå¸¸æ„Ÿè°¢æ‰€æœ‰å–œæ¬¢æœ¬ç³»åˆ—ç¬¬ä¸€éƒ¨åˆ†çš„è¯»è€…ã€‚**
    èµ·åˆï¼Œæˆ‘çš„ç›®æ ‡æ˜¯å·©å›ºæˆ‘å¯¹åŸºæœ¬ ML æ¦‚å¿µçš„ç†è§£ï¼Œä½†çœ‹åˆ°å®ƒå¼•èµ·äº†è®¸å¤šäººçš„å…±é¸£ï¼Œæˆ‘æ„Ÿåˆ°éå¸¸é«˜å…´ğŸ˜ƒã€‚å¦‚æœä½ å¯¹æˆ‘ä»¬çš„ä¸‹ä¸€ä¸ªè¯é¢˜æœ‰å»ºè®®ï¼Œè¯·éšæ—¶ç•™è¨€ï¼
- en: 'Other posts in this series:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç³»åˆ—çš„å…¶ä»–æ–‡ç« ï¼š
- en: '[Courage to Learn ML: Demystifying L1 & L2 Regularization (part 1)](/understanding-l1-l2-regularization-part-1-9c7affe6f920)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å‹‡æ•¢å­¦ä¹  ML: æ­å¼€ L1 å’Œ L2 æ­£åˆ™åŒ–çš„é¢çº±ï¼ˆç¬¬ 1 éƒ¨åˆ†ï¼‰](/understanding-l1-l2-regularization-part-1-9c7affe6f920)'
- en: '[Courage to Learn ML: Demystifying L1 & L2 Regularization (part 2)](/courage-to-learn-ml-unraveling-l1-l2-regularization-part-2-1bb171e43b35)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å‹‡æ•¢å­¦ä¹  ML: æ­å¼€ L1 å’Œ L2 æ­£åˆ™åŒ–çš„é¢çº±ï¼ˆç¬¬ 2 éƒ¨åˆ†ï¼‰](/courage-to-learn-ml-unraveling-l1-l2-regularization-part-2-1bb171e43b35)'
- en: '[Courage to Learn ML: Demystifying L1 & L2 Regularization (part 3)](/courage-to-learn-ml-demystifying-l1-l2-regularization-part-3-ee27cd4b557a)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å‹‡æ•¢å­¦ä¹  ML: æ­å¼€ L1 å’Œ L2 æ­£åˆ™åŒ–çš„é¢çº±ï¼ˆç¬¬ 3 éƒ¨åˆ†ï¼‰](/courage-to-learn-ml-demystifying-l1-l2-regularization-part-3-ee27cd4b557a)'
- en: '[Courage to Learn ML: Decoding Likelihood, MLE, and MAP](/courage-to-learn-ml-decoding-likelihood-mle-and-map-65218b2c2b99)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å‹‡æ•¢å­¦ä¹  ML: è§£ç ä¼¼ç„¶ã€MLE å’Œ MAP](/courage-to-learn-ml-decoding-likelihood-mle-and-map-65218b2c2b99)'
- en: '***If you liked the article, you can find me on*** [***LinkedIn***](https://www.linkedin.com/in/amyma101/)***.***'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '***å¦‚æœä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Œä½ å¯ä»¥åœ¨*** [***LinkedIn***](https://www.linkedin.com/in/amyma101/)***æ‰¾åˆ°æˆ‘ã€‚***'
- en: Reference
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚è€ƒ
- en: '[## A Probabilistic Interpretation of Regularization'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[## æ­£åˆ™åŒ–çš„æ¦‚ç‡è§£é‡Š'
- en: A look at regularization through the lens of probability.
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä»æ¦‚ç‡çš„è§’åº¦çœ‹æ­£åˆ™åŒ–ã€‚
- en: bjlkeng.io](https://bjlkeng.io/posts/probabilistic-interpretation-of-regularization/?source=post_page-----27c13dc250f9--------------------------------)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[bjlkeng.io](https://bjlkeng.io/posts/probabilistic-interpretation-of-regularization/?source=post_page-----27c13dc250f9--------------------------------)'
- en: '[https://keras.io/api/layers/regularizers/](https://keras.io/api/layers/regularizers/)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://keras.io/api/layers/regularizers/](https://keras.io/api/layers/regularizers/)'
- en: '[](https://arxiv.org/abs/2310.04415?source=post_page-----27c13dc250f9--------------------------------)
    [## Why Do We Need Weight Decay in Modern Deep Learning?'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://arxiv.org/abs/2310.04415?source=post_page-----27c13dc250f9--------------------------------)
    [## ä¸ºä»€ä¹ˆç°ä»£æ·±åº¦å­¦ä¹ ä¸­éœ€è¦æƒé‡è¡°å‡ï¼Ÿ'
- en: Weight decay is a broadly used technique for training state-of-the-art deep
    networks, including large language modelsâ€¦
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æƒé‡è¡°å‡æ˜¯ä¸€ç§å¹¿æ³›ç”¨äºè®­ç»ƒå…ˆè¿›æ·±åº¦ç½‘ç»œçš„æŠ€æœ¯ï¼ŒåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹â€¦â€¦
- en: arxiv.org](https://arxiv.org/abs/2310.04415?source=post_page-----27c13dc250f9--------------------------------)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[arxiv.org](https://arxiv.org/abs/2310.04415?source=post_page-----27c13dc250f9--------------------------------)'
