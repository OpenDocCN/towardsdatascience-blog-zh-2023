- en: 'Courage to Learn ML: Demystifying L1 & L2 Regularization (part 4)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 勇敢学习 ML：揭示 L1 和 L2 正则化（第4部分）
- en: 原文：[https://towardsdatascience.com/courage-to-learn-ml-demystifying-l1-l2-regularization-part-4-27c13dc250f9](https://towardsdatascience.com/courage-to-learn-ml-demystifying-l1-l2-regularization-part-4-27c13dc250f9)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/courage-to-learn-ml-demystifying-l1-l2-regularization-part-4-27c13dc250f9](https://towardsdatascience.com/courage-to-learn-ml-demystifying-l1-l2-regularization-part-4-27c13dc250f9)
- en: Explore L1 & L2 Regularization as Bayesian Priors
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索 L1 和 L2 正则化作为贝叶斯先验
- en: '[](https://amyma101.medium.com/?source=post_page-----27c13dc250f9--------------------------------)[![Amy
    Ma](../Images/2edf55456a1f92724535a1441fa2bef5.png)](https://amyma101.medium.com/?source=post_page-----27c13dc250f9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----27c13dc250f9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----27c13dc250f9--------------------------------)
    [Amy Ma](https://amyma101.medium.com/?source=post_page-----27c13dc250f9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://amyma101.medium.com/?source=post_page-----27c13dc250f9--------------------------------)[![Amy
    Ma](../Images/2edf55456a1f92724535a1441fa2bef5.png)](https://amyma101.medium.com/?source=post_page-----27c13dc250f9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----27c13dc250f9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----27c13dc250f9--------------------------------)
    [Amy Ma](https://amyma101.medium.com/?source=post_page-----27c13dc250f9--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----27c13dc250f9--------------------------------)
    ·8 min read·Dec 11, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----27c13dc250f9--------------------------------)
    ·阅读时间 8 分钟·2023年12月11日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/22bdc3089e02a827c9315e4de1e8cd0f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22bdc3089e02a827c9315e4de1e8cd0f.png)'
- en: Photo by [Dominik Jirovský](https://unsplash.com/@dominik_jirovsky?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Dominik Jirovský](https://unsplash.com/@dominik_jirovsky?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) 提供
- en: 'Welcome back to ‘[Courage to Learn ML](/towardsdatascience.com/tagged/courage-to-learn-ml):
    Unraveling L1 & L2 Regularization,’ in its fourth post. Last time, our mentor-learner
    pair explored [the properties of L1 and L2 regularization through the lens of
    Lagrange Multipliers](https://medium.com/p/ee27cd4b557a).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎回到‘[勇敢学习 ML](/towardsdatascience.com/tagged/courage-to-learn-ml)：揭示 L1 和 L2
    正则化’，这是第四篇文章。上次，我们的导师-学习者配对通过 [拉格朗日乘子视角探讨了 L1 和 L2 正则化的性质](https://medium.com/p/ee27cd4b557a)。
- en: In this concluding segment on L1 and L2 regularization, the duo will delve into
    these topics from a fresh angle — [Bayesian priors](https://medium.com/p/65218b2c2b99).
    We’ll also summarize how L1 and L2 regularizations are applied across different
    algorithms.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在关于 L1 和 L2 正则化的总结部分，这对将从新的角度探讨这些话题——[贝叶斯先验](https://medium.com/p/65218b2c2b99)。我们还将总结
    L1 和 L2 正则化如何在不同算法中应用。
- en: In this article, we’ll address several intriguing questions. If any of these
    topics spark your curiosity, you’ve come to the right place!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将讨论几个引人入胜的问题。如果这些话题激起了你的好奇心，你来对地方了！
- en: How MAP priors relate to L1 and L2 regularizations
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MAP 先验如何与 L1 和 L2 正则化相关
- en: An intuitive breakdown of using Laplace and normal distributions as priors
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对使用拉普拉斯和正态分布作为先验的直观分析
- en: Understanding the sparsity induced by L1 regularization with a Laplace prior
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 L1 正则化与拉普拉斯先验引起的稀疏性
- en: Algorithms that are compatible with L1 and L2 regularization
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与 L1 和 L2 正则化兼容的算法
- en: Why L2 regularization is often referred to as ‘weight decay’ in neural network
    training
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么 L2 正则化在神经网络训练中通常被称为“权重衰减”
- en: The reasons behind the less frequent use of L1 norm in neural networks
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L1 范数在神经网络中使用较少的原因
- en: '**So, we’ve talked about how MAP differs from MLE, mainly because MAP takes
    into account an extra piece of information: our beliefs before seeing the data,
    or the prior. How does this tie in with L1 and L2 regularizations?**'
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**所以，我们讨论了 MAP 如何不同于 MLE，主要因为 MAP 考虑了一个额外的信息：在看到数据之前的信念，即先验。这与 L1 和 L2 正则化有何关系？**'
- en: Let’s dive into how different priors in the MAP formula shape our approach to
    L1 and L2 regularization (for a detailed walkthrough on formulating this equation,
    check out [this post](https://medium.com/p/65218b2c2b99)).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨 MAP 公式中不同先验如何塑造我们对 L1 和 L2 正则化的理解（有关如何制定此方程式的详细讲解，请查看 [这篇文章](https://medium.com/p/65218b2c2b99)）。
- en: When considering priors for weights, Our initial intuition often leads us to
    choose a **normal distribution** as the prior for model weights. With this, we
    typically use a zero-mean normal distribution for each weight wi, sharing the
    same standard deviation 𝜎. Plugging this belief into the prior term logp(w) in
    MAP (where p(w) represents the weight’s prior) leads us to **sum of squared weights**
    naturally. This term is precisely the **L2 norm**. This implies that using a normal
    distribution as our prior equates to applying L2 regularization.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑权重的先验时，我们的初步直觉通常会使我们选择**正态分布**作为模型权重的先验。这样，我们通常会对每个权重wi使用均值为零的正态分布，并具有相同的标准差𝜎。将这种信念代入MAP中的先验项logp(w)（其中p(w)表示权重的先验）会自然得到**权重平方和**。这个项正是**L2范数**。这意味着使用正态分布作为我们的先验等同于应用L2正则化。
- en: '![](../Images/416c579fb6538b64246da161a2cd4b07.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/416c579fb6538b64246da161a2cd4b07.png)'
- en: Conversely, adopting a **Laplace distribution** as our belief results in the
    **L1 norm** for weights. Hence, a Laplace prior essentially translates to L1 regularization.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，采用**拉普拉斯分布**作为我们的信念会导致**L1范数**的权重。因此，拉普拉斯先验本质上相当于L1正则化。
- en: '![](../Images/b01952bbb6dbbe648bdb8f04ba569cbd.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b01952bbb6dbbe648bdb8f04ba569cbd.png)'
- en: In short, L1 regularization aligns with a Laplace distribution prior, while
    L2 regularization corresponds to a Normal distribution prior.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，L1正则化与拉普拉斯分布先验一致，而L2正则化对应于正态分布先验。
- en: Interestingly, when employing a uniform prior in the MAP framework, it essentially
    “disappears” from the equation (go ahead and try it yourself!). This leaves the
    likelihood term as the sole determinant of the optimal weight values, effectively
    transforming the MAP estimation into maximum likelihood estimation (MLE).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，当在MAP框架中使用均匀先验时，它实际上在方程中“消失”了（你可以自己尝试一下！）。这使得似然项成为确定最佳权重值的唯一因素，实际上将MAP估计转变为最大似然估计（MLE）。
- en: '**So, can you explain the reasoning for having different beliefs when our prior
    is a Laplace distribution versus a normal distribution? I’d like to visualize
    this better.**'
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**那么，当我们的先验是拉普拉斯分布与正态分布时，为什么会有不同的信念？我想更好地可视化这个问题。**'
- en: This is a great question. Indeed, having different priors means you hold various
    initial assumptions about the situation before collecting any data. We’ll delve
    into the purpose of different distributions later, but for now, let’s look at
    a simple, intuitive example using Laplace and normal distributions. Consider the
    number of views on my new Medium posts. Two weeks ago, as a new writer with no
    followers, I expected zero views. My assumption was that the average daily view
    count would start low, possibly at zero, but might increase as readers interested
    in similar topics discover my work. A Laplace prior fits this scenario well. It
    suggests a range of possible view counts but assigns higher probability to numbers
    near zero, reflecting my expectation of few views initially but allowing for growth
    over time.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很好的问题。确实，不同的先验意味着你在收集任何数据之前对情况持有不同的初步假设。我们稍后将深入探讨不同分布的目的，但现在，让我们通过使用拉普拉斯分布和正态分布来考虑一个简单直观的例子。考虑一下我新发布的Medium帖子。两周前，作为一名没有粉丝的新作者，我预期没有查看次数。我的假设是，平均每日查看次数起初会很低，可能为零，但随着对类似主题感兴趣的读者发现我的作品，这一数字可能会增加。拉普拉斯先验很好地适应了这种情况。它建议了一系列可能的查看次数，但对接近零的数字赋予更高的概率，反映了我对初期查看次数很少但随时间增长的预期。
- en: Now, with 55 viewers (thanks, everyone!), and followers who receive updates
    on my posts, my expectations have changed. I anticipate that new posts will perform
    similarly to my previous ones, averaging around my historical view count. This
    is where a normal distribution prior comes into play, predicting future views
    based on my established track record.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有55位观众（谢谢大家！），以及关注我帖子更新的粉丝，我的期望已经改变。我预期新帖子将表现类似于之前的帖子，平均查看次数接近我历史上的查看次数。这时，正态分布先验发挥作用，根据我已建立的记录预测未来的查看次数。
- en: '**Hmm… Can you explain the L1 regularization sparsity with a Laplace prior?**'
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**嗯……你能解释一下使用拉普拉斯先验的L1正则化的稀疏性吗？**'
- en: Indeed, understanding L1 regularization’s promotion of sparsity can be illuminated
    by comparing the Laplace distribution to the normal distribution. **The key difference
    lies in their probability densities around zero.**** The Laplace distribution
    is sharply peaked at zero, indicating a higher likelihood of values close to zero.
    This characteristic mirrors the effect of L1 regularization, where most weights
    in the model are driven towards zero, promoting sparsity. In contrast, the normal
    distribution, associated with L2 regularization, is less peaked at zero and more
    spread out, indicating a preference for distributing weights more evenly.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，通过比较拉普拉斯分布和正态分布，可以揭示 L1 正则化促进稀疏性的机制。**关键的区别在于它们在零附近的概率密度。** 拉普拉斯分布在零附近有一个尖锐的峰值，表明接近零的值的可能性较高。这一特征类似于
    L1 正则化的效果，其中模型中的大多数权重被驱动到零，促进了稀疏性。相比之下，正态分布与 L2 正则化相关，在零附近的峰值较低，分布较广，表示对权重的更均匀分配有偏好。
- en: '![](../Images/49676c748501a234e00ec94b78953135.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/49676c748501a234e00ec94b78953135.png)'
- en: 'source: [https://austinrochford.com/posts/2013-09-02-prior-distributions-for-bayesian-regression-using-pymc.html](https://austinrochford.com/posts/2013-09-02-prior-distributions-for-bayesian-regression-using-pymc.html)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '来源: [https://austinrochford.com/posts/2013-09-02-prior-distributions-for-bayesian-regression-using-pymc.html](https://austinrochford.com/posts/2013-09-02-prior-distributions-for-bayesian-regression-using-pymc.html)'
- en: Additionally, the Laplace distribution has **heavier tails** than the normal
    distribution, meaning it extends further out. This property allows for some weights
    to remain significantly away from zero while the others are pretty close to zero.
    So, by choosing the Laplace distribution as a prior for the weights (L1 regularization)
    , we encourage the model to learn solutions where most weights are close to zero,
    achieving sparsity without sacrificing potentially relevant features. This is
    why L1 regularization can be used as a feature selection method.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，拉普拉斯分布的**尾部更重**，意味着其延伸得更远。这一特性允许一些权重显著远离零，而其他权重则非常接近零。因此，通过选择拉普拉斯分布作为权重的先验（L1
    正则化），我们鼓励模型学习解决方案，使大多数权重接近零，实现稀疏性而不牺牲潜在的相关特征。这就是为什么 L1 正则化可以作为特征选择方法。
- en: '**So, I see that L1 and L2 regularizations are key for avoiding overfitting
    and boosting a model’s generalizability. Can you tell me which algorithms these
    methods can be applied to?**'
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**所以，我认为 L1 和 L2 正则化对于避免过拟合和提升模型的泛化能力至关重要。你能告诉我这些方法可以应用于哪些算法吗？**'
- en: 'L1, L2 regularization can be apply to many algorithms by adding a penalty term
    to their loss functions. Here are some specific examples of algorithms where L1
    and L2 regularization are applied:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: L1 和 L2 正则化可以通过向损失函数中添加惩罚项来应用于许多算法。以下是一些应用 L1 和 L2 正则化的具体算法示例：
- en: '**Linear models.** Those techniques are particularly useful with high-dimensional
    problems. In linear models, they are known as **lasso and ridge regression**,
    respectively. One thing to note is that L1 regularization not only helps prevent
    overfitting but also helps with feature selection which preventing multi-collinearity.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性模型。** 这些技术在高维问题中特别有用。在线性模型中，它们被称为**lasso 和 ridge 回归**。需要注意的是，L1 正则化不仅有助于防止过拟合，还有助于特征选择，从而防止多重共线性。'
- en: '**SVM.** Regularization methods are **the core of SVM**. By adding the weight
    penalty term, SVM encourages the model to reduce its margin between decision boundary
    and the closest support vector (L1 regularization) or smooth the margin (L2 regularization),
    which leads to better generalization.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**支持向量机。** 正则化方法是**SVM 的核心**。通过添加权重惩罚项，SVM 鼓励模型减少决策边界与最近支持向量之间的间距（L1 正则化）或平滑间距（L2
    正则化），从而提高泛化能力。'
- en: '**Neural Networks**. L2 regularization is more commonly used in neural networks
    and is often referred to as **weight decay**. L1 regularization can also be used
    in neural networks, but it is less common due to its tendency to lead to sparse
    weights.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**神经网络。** L2 正则化在神经网络中使用得更为普遍，通常被称为**权重衰减**。L1 正则化也可以在神经网络中使用，但由于其倾向于导致稀疏权重，因此较少见。'
- en: '**Ensemble algorithms.** Gradient boosting machines like GBM and Xgboost use
    L1 and L2 regularization to **limit the size of individual trees** within the
    ensemble. L1 regularization specifically achieves this by shrinking the weights
    of weak learners (each tree) in the ensemble, while L2 regularization penalizes
    the total number of leaves (leaf score) in each tree.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成算法。**像GBM和Xgboost这样的梯度提升机器使用L1和L2正则化来**限制集成中单个树的大小**。L1正则化通过缩小集成中弱学习器（每棵树）的权重来实现这一点，而L2正则化则惩罚每棵树中的总叶子数（叶子得分）。'
- en: '**Why do L2 regularization is also called ‘weight decay’ in neural network
    training? And why is the L1 norm less commonly used in neural networks?**'
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**为什么L2正则化也被称为神经网络训练中的‘权重衰减’？为什么L1范数在神经网络中使用较少？**'
- en: To tackle those two questions, let’s bring in a bit of math to illustrate how
    weights get updated in the presence of L1 and L2 regularizations.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这两个问题，让我们引入一些数学来说明在L1和L2正则化下权重是如何更新的。
- en: '![](../Images/9901a688faa6f94f3cc1f00892ad1e5d.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9901a688faa6f94f3cc1f00892ad1e5d.png)'
- en: Loss function with L2 regularization; λ is penalty coefficient and α represents
    learning rate
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 带有L2正则化的损失函数；λ是惩罚系数，α代表学习率。
- en: In L2 regularization, the weight update process involves a slight reduction
    of the weights, scaled down according to their own magnitude. This results in
    what is termed as “weight decay.” Specifically, **each weight is decreased by
    an amount that is directly proportional to its current value.** This proportional
    reduction, governed by the typically small settings of the penalty coefficient
    (λ) and the learning rate (α), ensures that larger weights are subjected to a
    higher degree of penalization compared to smaller weights. The essence of weight
    decay lies in this method of scaling down weights, encouraging the model to maintain
    smaller weights. Such behavior is advantageous in neural networks as it tends
    to produce smoother decision boundary.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在L2正则化中，权重更新过程涉及对权重的轻微减少，根据其自身的大小缩放。这就是所谓的“权重衰减”。具体来说，**每个权重减少的量与其当前值直接成比例。**这种比例减少，由通常较小的惩罚系数（λ）和学习率（α）设置所控制，确保较大的权重比小的权重受到更高程度的惩罚。权重衰减的本质在于这种缩小权重的方式，鼓励模型保持较小的权重。这种行为在神经网络中是有利的，因为它趋向于产生更平滑的决策边界。
- en: '![](../Images/e43d75bcb2594d241d9ebd040600a708.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e43d75bcb2594d241d9ebd040600a708.png)'
- en: Loss function with L1 regularization; λ is penalty coefficient and α represents
    learning rate
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 带有L1正则化的损失函数；λ是惩罚系数，α代表学习率。
- en: In contrast, **L1 regularization modifies the weight update rule by subtracting
    or adding a constant amount, determined by αλ and the sign of the weight (w).**
    This approach pushes weights towards zero, regardless of whether they are positive
    or negative. Under L1 regularization, all weights, irrespective of their magnitude,
    are adjusted by the same fixed amount. This results in larger weights remaining
    relatively large, while smaller weights are more rapidly driven to zero, promoting
    sparsity in the network.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，**L1正则化通过减去或添加一个由αλ和权重（w）符号确定的固定量来修改权重更新规则。**这种方法将权重推向零，无论它们是正的还是负的。在L1正则化下，所有权重，不论其大小，都以相同的固定量进行调整。这使得较大的权重保持相对较大，而较小的权重则更快地被驱动到零，从而促进网络的稀疏性。
- en: '![](../Images/93302c503c331685355b32d0adb09ee2.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/93302c503c331685355b32d0adb09ee2.png)'
- en: Let’s compare!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较一下！
- en: 'Comparing the two, L2’s approach to weight modification is based on the weight’s
    existing value, leading to larger weights diminishing more quickly than smaller
    ones. This uniform decay across all weights is why it’s termed ‘weight decay’.
    On the other hand, L1’s **fixed adjustment amount, regardless of weight size**,
    can lead to some issues and become less favorable in NN:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，L2对权重的修改方式是基于权重的现有值，使得较大的权重比小的权重衰减得更快。这种对所有权重的均匀衰减就是为什么它被称为‘权重衰减’。另一方面，L1的**固定调整量，不论权重大小**，可能导致一些问题，并在神经网络中变得不那么受欢迎：
- en: It can zero out some weights, causing **‘dead neurons’** and potentially disrupting
    information flow within the network, which could impair model performance.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可能将一些权重归零，导致**‘死神经元’**，并可能破坏网络中的信息流，这可能影响模型性能。
- en: '**The non-differentiable points at zero** introduced by L1 make optimization
    algorithms like gradient descent less effective.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L1引入的零点处的不可微分点**使得像梯度下降这样的优化算法效果较差。'
- en: '**What effects do adding L1 and L2 regularization have on our loss function?
    Does incorporating these regularizations lead us away from the original global
    minimum?**'
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**添加 L1 和 L2 正则化对我们的损失函数有什么影响？这些正则化的引入是否使我们远离原始的全局最小值？**'
- en: It’s a great question! In short, **once we incorporate regularization, we intentionally
    shift our focus away from the original global minimum.** This means adding penalty
    terms to the loss function, fundamentally changing its landscape. It’s crucial
    to understand that this change is desirable, not accidental.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这是个很好的问题！简而言之，**一旦我们引入正则化，我们有意将关注点从原始的全局最小值转移开。** 这意味着向损失函数中添加惩罚项，从而根本改变其形状。理解这一变化是期望中的，而非偶然发生的，是至关重要的。
- en: 'By introducing these penalties, we aim to achieve a new optimal solution that
    balances two crucial goals: fitting the training data well to minimize empirical
    risk while simultaneously reducing model complexity and enhancing generalization
    to unseen data. The original global minimum might not achieve this balance, potentially
    leading to overfitting and poor performance on new data.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 通过引入这些惩罚，我们旨在实现一个新的最优解，平衡两个关键目标：良好地拟合训练数据以最小化经验风险，同时减少模型复杂度并增强对未见数据的泛化。原始的全局最小值可能无法实现这种平衡，从而可能导致过拟合和在新数据上的性能下降。
- en: If you’re interested in the mathematical details of measuring the distance between
    the original and regularized optima, I highly recommend chapter 7 (pages 224–229)
    of Deep Learning by Ian Goodfellow. Pay particular attention to formulas 7.7 and
    7.13 for L2 and 7.22 and 7.23 for L1\. This provides a quantifiable assessment
    of the impact regularization terms have on weights, deepening your understanding
    of L1 and L2 regularization.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对测量原始最优解和正则化后最优解之间的距离的数学细节感兴趣，我强烈推荐 Ian Goodfellow 的《深度学习》第七章（第 224–229 页）。特别注意公式
    7.7 和 7.13（L2）以及 7.22 和 7.23（L1）。这将提供对正则化项对权重影响的定量评估，深入了解 L1 和 L2 正则化。
- en: We’ve now reached the conclusion of our exploration into L1 and L2 regularization.
    In our next discussion, I’m excited to delve into the basics of loss functions.
    **A big thank you to all the readers who enjoyed the first part of this series.**
    Initially, my goal was to solidify my grasp of basic ML concepts, but I’m thrilled
    to see it resonate with many of you 😃. If you have suggestions for our next topic,
    please feel free to leave a comment!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经结束了对 L1 和 L2 正则化的探讨。在下次讨论中，我很兴奋地将深入探讨损失函数的基础知识。**非常感谢所有喜欢本系列第一部分的读者。**
    起初，我的目标是巩固我对基本 ML 概念的理解，但看到它引起了许多人的共鸣，我感到非常高兴😃。如果你对我们的下一个话题有建议，请随时留言！
- en: 'Other posts in this series:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 本系列的其他文章：
- en: '[Courage to Learn ML: Demystifying L1 & L2 Regularization (part 1)](/understanding-l1-l2-regularization-part-1-9c7affe6f920)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[勇敢学习 ML: 揭开 L1 和 L2 正则化的面纱（第 1 部分）](/understanding-l1-l2-regularization-part-1-9c7affe6f920)'
- en: '[Courage to Learn ML: Demystifying L1 & L2 Regularization (part 2)](/courage-to-learn-ml-unraveling-l1-l2-regularization-part-2-1bb171e43b35)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[勇敢学习 ML: 揭开 L1 和 L2 正则化的面纱（第 2 部分）](/courage-to-learn-ml-unraveling-l1-l2-regularization-part-2-1bb171e43b35)'
- en: '[Courage to Learn ML: Demystifying L1 & L2 Regularization (part 3)](/courage-to-learn-ml-demystifying-l1-l2-regularization-part-3-ee27cd4b557a)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[勇敢学习 ML: 揭开 L1 和 L2 正则化的面纱（第 3 部分）](/courage-to-learn-ml-demystifying-l1-l2-regularization-part-3-ee27cd4b557a)'
- en: '[Courage to Learn ML: Decoding Likelihood, MLE, and MAP](/courage-to-learn-ml-decoding-likelihood-mle-and-map-65218b2c2b99)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[勇敢学习 ML: 解码似然、MLE 和 MAP](/courage-to-learn-ml-decoding-likelihood-mle-and-map-65218b2c2b99)'
- en: '***If you liked the article, you can find me on*** [***LinkedIn***](https://www.linkedin.com/in/amyma101/)***.***'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '***如果你喜欢这篇文章，你可以在*** [***LinkedIn***](https://www.linkedin.com/in/amyma101/)***找到我。***'
- en: Reference
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考
- en: '[## A Probabilistic Interpretation of Regularization'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 正则化的概率解释'
- en: A look at regularization through the lens of probability.
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从概率的角度看正则化。
- en: bjlkeng.io](https://bjlkeng.io/posts/probabilistic-interpretation-of-regularization/?source=post_page-----27c13dc250f9--------------------------------)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[bjlkeng.io](https://bjlkeng.io/posts/probabilistic-interpretation-of-regularization/?source=post_page-----27c13dc250f9--------------------------------)'
- en: '[https://keras.io/api/layers/regularizers/](https://keras.io/api/layers/regularizers/)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://keras.io/api/layers/regularizers/](https://keras.io/api/layers/regularizers/)'
- en: '[](https://arxiv.org/abs/2310.04415?source=post_page-----27c13dc250f9--------------------------------)
    [## Why Do We Need Weight Decay in Modern Deep Learning?'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://arxiv.org/abs/2310.04415?source=post_page-----27c13dc250f9--------------------------------)
    [## 为什么现代深度学习中需要权重衰减？'
- en: Weight decay is a broadly used technique for training state-of-the-art deep
    networks, including large language models…
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 权重衰减是一种广泛用于训练先进深度网络的技术，包括大型语言模型……
- en: arxiv.org](https://arxiv.org/abs/2310.04415?source=post_page-----27c13dc250f9--------------------------------)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[arxiv.org](https://arxiv.org/abs/2310.04415?source=post_page-----27c13dc250f9--------------------------------)'
