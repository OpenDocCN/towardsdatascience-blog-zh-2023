["```py\n# Delete any models previously created\ndel model, tokenizer, pipe\n\n# Empty VRAM cache\nimport torch\ntorch.cuda.empty_cache()\n```", "```py\n# Latest HF transformers version for Mistral-like models\npip install git+https://github.com/huggingface/transformers.git\npip install accelerate bitsandbytes xformers\n```", "```py\nfrom torch import bfloat16\nfrom transformers import pipeline\n\n# Load in your LLM without any compression tricks\npipe = pipeline(\n    \"text-generation\", \n    model=\"HuggingFaceH4/zephyr-7b-beta\", \n    torch_dtype=bfloat16, \n    device_map=\"auto\"\n)\n```", "```py\n# We use the tokenizer's chat template to format each message\n# See https://huggingface.co/docs/transformers/main/en/chat_templating\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a friendly chatbot.\",\n    },\n    {\n        \"role\": \"user\", \n        \"content\": \"Tell me a funny joke about Large Language Models.\"\n    },\n]\nprompt = pipe.tokenizer.apply_chat_template(\n    messages, \n    tokenize=False, \n    add_generation_prompt=True\n)\n```", "```py\noutputs = pipe(\n    prompt, \n    max_new_tokens=256, \n    do_sample=True, \n    temperature=0.1, \n    top_p=0.95\n)\nprint(outputs[0][\"generated_text\"])\n```", "```py\nfrom accelerate import Accelerator\n\n# Shard our model into pieces of 1GB\naccelerator = Accelerator()\naccelerator.save_model(\n    model=pipe.model, \n    save_directory=\"/content/model\", \n    max_shard_size=\"4GB\"\n)\n```", "```py\nfrom transformers import BitsAndBytesConfig\nfrom torch import bfloat16\n\n# Our 4-bit configuration to load the LLM with less GPU memory\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,  # 4-bit quantization\n    bnb_4bit_quant_type='nf4',  # Normalized float 4\n    bnb_4bit_use_double_quant=True,  # Second quantization after the first\n    bnb_4bit_compute_dtype=bfloat16  # Computation type\n)\n```", "```py\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n\n# Zephyr with BitsAndBytes Configuration\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"HuggingFaceH4/zephyr-7b-alpha\",\n    quantization_config=bnb_config,\n    device_map='auto',\n)\n\n# Create a pipeline\npipe = pipeline(model=model, tokenizer=tokenizer, task='text-generation')\n```", "```py\n# We will use the same prompt as we did originally\noutputs = pipe(\n    prompt, \n    max_new_tokens=256, \n    do_sample=True, \n    temperature=0.7, \n    top_p=0.95\n)\nprint(outputs[0][\"generated_text\"])\n```", "```py\npip install optimum\npip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/\n```", "```py\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\n# Load LLM and Tokenizer\nmodel_id = \"TheBloke/zephyr-7B-beta-GPTQ\"\ntokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    trust_remote_code=False,\n    revision=\"main\"\n)\n\n# Create a pipeline\npipe = pipeline(model=model, tokenizer=tokenizer, task='text-generation')\n```", "```py\n# We will use the same prompt as we did originally\noutputs = pipe(\n    prompt,\n    max_new_tokens=256,\n    do_sample=True,\n    temperature=0.1,\n    top_p=0.95\n)\nprint(outputs[0][\"generated_text\"])\n```", "```py\npip install ctransformers[cuda]\n```", "```py\nfrom ctransformers import AutoModelForCausalLM\nfrom transformers import AutoTokenizer, pipeline\n\n# Load LLM and Tokenizer\n# Use `gpu_layers` to specify how many layers will be offloaded to the GPU.\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"TheBloke/zephyr-7B-beta-GGUF\",\n    model_file=\"zephyr-7b-beta.Q4_K_M.gguf\",\n    model_type=\"mistral\", gpu_layers=50, hf=True\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    \"HuggingFaceH4/zephyr-7b-beta\", use_fast=True\n)\n\n# Create a pipeline\npipe = pipeline(model=model, tokenizer=tokenizer, task='text-generation')\n```", "```py\n# We will use the same prompt as we did originally\noutputs = pipe(prompt, max_new_tokens=256)\nprint(outputs[0][\"generated_text\"])\n```", "```py\npip install vllm\n```", "```py\nfrom vllm import LLM, SamplingParams\n\n# Load the LLM\nsampling_params = SamplingParams(temperature=0.0, top_p=1.0, max_tokens=256)\nllm = LLM(\n    model=\"TheBloke/zephyr-7B-beta-AWQ\", \n    quantization='awq', \n    dtype='half', \n    gpu_memory_utilization=.95, \n    max_model_len=4096\n)\n```", "```py\n# Generate output based on the input prompt and sampling parameters\noutput = llm.generate(prompt, sampling_params)\nprint(output[0].outputs[0].text)\n```"]