# 使用Llama 2进行主题建模

> 原文：[https://towardsdatascience.com/topic-modeling-with-llama-2-85177d01e174](https://towardsdatascience.com/topic-modeling-with-llama-2-85177d01e174)

![](../Images/31f5e2196198d26e5295ae70660d720c.png)

## 使用大型语言模型创建易于解释的主题

[](https://medium.com/@maartengrootendorst?source=post_page-----85177d01e174--------------------------------)[![Maarten Grootendorst](../Images/58e24b9cf7e10ff1cd5ffd75a32d1a26.png)](https://medium.com/@maartengrootendorst?source=post_page-----85177d01e174--------------------------------)[](https://towardsdatascience.com/?source=post_page-----85177d01e174--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----85177d01e174--------------------------------) [Maarten Grootendorst](https://medium.com/@maartengrootendorst?source=post_page-----85177d01e174--------------------------------)

·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----85177d01e174--------------------------------) ·12分钟阅读·2023年8月22日

--

随着**Llama 2**的出现，本地运行强大的LLM已变得越来越现实。其准确性接近OpenAI的GPT-3.5，适用于许多用例。

在这篇文章中，我们将探讨如何使用Llama2进行主题建模，而无需将每一个文档传递给模型。相反，我们将利用[**BERTopic**](https://github.com/MaartenGr/BERTopic)，这是一种模块化的主题建模技术，可以使用任何LLM来微调主题表示。

BERTopic的工作原理非常简单。它包括5个顺序步骤：

1.  嵌入文档

1.  降低嵌入的维度

1.  聚类减少的嵌入

1.  按聚类对文档进行分词

1.  提取每个聚类的最佳代表词

![](../Images/63a385b329d173ae7fc70aa9fa1b4182.png)

BERTopic的5个主要步骤。

然而，随着**Llama 2**等LLM的兴起，我们可以做得比每个主题的一堆独立单词更好。直接将所有文档传递给Llama 2并让其分析是不切实际的。我们可以使用向量数据库进行搜索，但我们不完全确定要搜索哪些主题。

相反，我们将利用由BERTopic创建的聚类和主题，并让Llama 2对这些信息进行微调和提炼，以获得更准确的结果。

这是两全其美的最佳方案，即BERTopic的主题创建和Llama 2的主题表示。

![](../Images/66743d20879491fe94a4075743d520b5.png)

Llama 2使我们能够微调BERTopic生成的主题表示。

现在介绍部分已经完成，我们开始实际操作教程吧！

我们将开始安装我们将在整个示例中使用的几个包：

[PRE0]

请注意，您至少需要一块T4 GPU才能运行这个示例，它可以与免费的Google Colab实例一起使用。

🔥 **提示**：你也可以跟随 [Google Colab Notebook](https://colab.research.google.com/drive/1QCERSMUjqGetGGujdrvv_6_EeoIcd_9M?usp=sharing) 一起操作。

# [📄](https://emojipedia.org/page-facing-up) 数据

我们将对大量 ArXiv 摘要应用主题建模。它们是进行主题建模的绝佳来源，因为它们包含各种各样的主题，并且通常写得很好。

[PRE1]

为了给你一个概念，下面是一个抽象的样子：

[PRE2]

# 🤗 HuggingFace Hub 凭证

在我们可以使用一些技巧加载 Llama2 之前，我们首先需要接受 Llama2 的许可协议。步骤如下：

+   在 [这里](https://huggingface.co/) 创建一个 HuggingFace 账户

+   申请 Llama 2 访问权限 [这里](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf)

+   在 [这里](https://huggingface.co/settings/tokens) 获取你的 HuggingFace 令牌

完成这些步骤后，我们可以使用我们的 HuggingFace 凭证登录，以便这个环境知道我们有权限下载我们感兴趣的 Llama 2 模型。

[PRE3]

![](../Images/475af2918fd29d5277c2630d8b6ea2ef.png)

# 🦙 Llama 2

现在进入这个教程中一个更有趣的部分——如何在 T4-GPU 上加载 Llama 2 模型！

我们将重点关注 `'meta-llama/Llama-2-13b-chat-hf'` 变体。它足够大，能够提供有趣且有用的结果，同时又足够小，可以在我们的环境中运行。

我们首先定义我们的模型并确认 GPU 是否正确选择。我们期望 `device` 的输出显示为 Cuda 设备：

[PRE4]

## 优化与量化

为了加载我们 130 亿参数的模型，我们需要执行一些优化技巧。由于我们拥有的 VRAM 有限且没有 A100 GPU，我们需要对模型进行一些“压缩”，以便我们可以运行它。

我们可以使用一些技巧，但主要原则是 4 位量化。

这个过程将 64 位表示减少到仅 4 位，从而减少我们需要的 GPU 内存。这是一种最近的技术，且在高效 LLM 加载和使用方面相当优雅。你可以在 QLoRA 论文 [这里](https://arxiv.org/pdf/2305.14314.pdf) 和令人惊叹的 HuggingFace 博客 [这里](https://huggingface.co/blog/4bit-transformers-bitsandbytes) 中了解更多关于该方法的内容。

[PRE5]

我们刚刚运行的这四个参数非常重要，并将许多 LLM 应用带给消费者：

+   `**load_in_4bit**` 允许我们以 4 位精度加载模型，相比于原始的 32 位精度，这大大加快了速度并减少了内存！

+   `**bnb_4bit_quant_type**` 这是 4 位精度的类型。论文推荐使用标准化的浮点 4 位精度，所以这就是我们要使用的！

+   `**bnb_4bit_use_double_quant**` 这是一个巧妙的技巧，因为它在第一次量化后执行第二次量化，进一步减少所需的位数。

+   `**bnb_4bit_compute_dtype**` 计算过程中使用的计算类型，这进一步加速了模型。

使用这个配置，我们可以开始加载模型以及分词器：

[PRE6]

使用模型和分词器，我们将生成一个HuggingFace transformers管道，这样我们就可以轻松生成新文本：

[PRE7]

## 提示工程

为了检查我们的模型是否正确加载，让我们尝试几个提示。

[PRE8]

虽然我们可以直接提示模型，但实际上我们需要遵循一个模板。模板如下所示：

[PRE9]

这个模板由两个主要组件组成，即`{{ System Prompt }}`和`{{ User Prompt }}`：

+   `{{ System Prompt }}`帮助我们在对话过程中指导模型。例如，我们可以说它是一个专门负责标记主题的有用助手。

+   `{{ User Prompt }}`是我们向它提问的地方。

你可能注意到`[INST]`标签，这些标签用于标识提示的开始和结束。我们可以使用这些标签来建模对话历史，稍后我们会更深入地了解。

接下来，让我们看看如何使用这个模板来优化Llama 2进行主题建模。

## 提示模板

我们将保持`system prompt`简单明了：

[PRE10]

我们将告诉模型，它只是一个有用的助手，负责标记主题，因为这是我们的主要目标。

相比之下，我们的`user prompt`会稍微复杂一些。它将包含两个部分，一个**示例** **提示**和**主要提示**。

让我们从**示例提示**开始。如果你给大多数LLM一个示例，它们通常能更好地生成准确的响应。我们将展示一个准确的示例，说明我们期望的输出类型。

[PRE11]

这个例子基于一系列关键字和主要关于肉类影响的文档，帮助模型理解它应该给出什么样的输出。我们向模型展示了我们只期待标签，这样我们更容易提取。

接下来，我们将创建一个可以在BERTopic中使用的模板：

[PRE12]

有两个BERTopic特定的标签值得关注，即`[DOCUMENTS]`和`[KEYWORDS]`：

+   `[DOCUMENTS]`包含与主题最相关的前5个文档

+   `[KEYWORDS]`包含通过c-TF-IDF生成的与主题最相关的前10个关键字

这个模板将根据每个主题进行填写。最后，我们可以将其合并为我们的最终提示：

[PRE13]

# 🗨️ BERTopic

在开始主题建模之前，我们需要先执行两个步骤：

+   预计算嵌入

+   定义子模型

## 准备嵌入

通过为每个文档预计算嵌入，我们可以加速额外的探索步骤，并在需要时使用嵌入快速迭代BERTopic的超参数。

🔥 **提示**：你可以在[MTEB排行榜](https://huggingface.co/spaces/mteb/leaderboard)上找到有关聚类的优秀嵌入概述。

[PRE14]

## 子模型

接下来，我们将定义BERTopic中的所有子模型，并对要创建的聚类数量进行一些小的调整，如设置随机状态等。

[PRE15]

作为一个小额外内容，我们将把之前创建的嵌入减少到2维，以便在创建主题时用于可视化。

[PRE16]

## 表示模型

我们将用Llama 2来表示这些主题，这样可以获得一个不错的标签。不过，我们可能还希望有额外的表示，以从多个角度查看主题。

在这里，我们将使用c-TF-IDF作为主要表示，并将[KeyBERT](https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#keybertinspired)、[MMR](https://maartengr.github.io/BERTopic/getting_started/representation/representation.html#maximalmarginalrelevance)和[Llama 2](https://maartengr.github.io/BERTopic/getting_started/representation/llm.html)作为附加表示。

[PRE17]

# 🔥 训练

现在我们的模型已经准备好，我们可以开始训练我们的主题模型！我们将感兴趣的子模型提供给BERTopic，运行`.fit_transform`，看看我们得到什么样的主题。

[PRE18]

现在我们已经完成了模型训练，来看一下生成了什么主题：

[PRE19]

![](../Images/d9a0326290f7988d127e851f3d452d37.png)

[PRE20]

![](../Images/a34443869e8befd4e6af0d8622f045df.png)

我们创建了超过100个主题，它们似乎非常多样化。我们可以使用Llama 2提供的标签，并将其分配给我们创建的主题。通常，默认的主题表示是c-TF-IDF，但我们将重点关注Llama 2的表示。

[PRE21]

# [📊](https://emojigraph.org/bar-chart/) 可视化

我们可以手动浏览每个主题，这会需要大量工作，或者我们可以在一个互动图表中可视化它们。

BERTopic有一系列[可视化函数](https://maartengr.github.io/BERTopic/getting_started/visualization/visualize_documents.html)供我们使用。目前，我们将继续可视化文档。

[PRE22]

![](../Images/03b0559c55725b27e2ef199d98605b6e.png)

# [🖼️](https://emojipedia.org/framed-picture/)（额外内容）：高级可视化

虽然我们可以使用BERTopic的内置可视化功能，但我们也可以创建一个静态可视化，可能会提供更多的信息。

我们首先创建必要的变量，这些变量包含我们减少的嵌入和表示：

[PRE23]

接下来，我们将使用Matplotlib可视化减少后的嵌入，并以视觉上更令人愉悦的方式处理标签：

[PRE24]

![](../Images/8fb9e2c99f87f38e3d5f5a824b2a0d65.png)

**更新**：我上传了一个视频版本到YouTube，详细介绍了如何使用BERTopic与Llama 2：

# 感谢你的阅读！

如果你像我一样，对AI和/或心理学充满热情，请随时在[**LinkedIn**](https://www.linkedin.com/in/mgrootendorst/)上加我为好友，在[**Twitter**](https://twitter.com/MaartenGr)上关注我，或订阅我的[**Newsletter**](http://maartengrootendorst.substack.com/)。你也可以在我的[**个人网站**](https://maartengrootendorst.com/)上找到一些我的内容。

*所有没有来源说明的图像均由作者创作*
