- en: 'The A-Z of Transformers: Everything You Need to Know'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/the-a-z-of-transformers-everything-you-need-to-know-c9f214c619ac](https://towardsdatascience.com/the-a-z-of-transformers-everything-you-need-to-know-c9f214c619ac)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Everything you need to know about Transformers, and how to implement them
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@francoisporcher?source=post_page-----c9f214c619ac--------------------------------)[![Fran√ßois
    Porcher](../Images/9ddb233f8cadbd69026bd79e2bd62dea.png)](https://medium.com/@francoisporcher?source=post_page-----c9f214c619ac--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c9f214c619ac--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c9f214c619ac--------------------------------)
    [Fran√ßois Porcher](https://medium.com/@francoisporcher?source=post_page-----c9f214c619ac--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c9f214c619ac--------------------------------)
    ¬∑16 min read¬∑Oct 25, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/be68574b989226c3e6ac62c933050cf5.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Why another tutorial on Transformers?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You have probably already heard of Transformers, and everyone talks about it,
    so why making a new article about it?
  prefs: []
  type: TYPE_NORMAL
- en: Well, I am a researcher, and this requires me to have a very deep understanding
    of the tools I use (because if you don‚Äôt understand them, how can you identify
    where they are wrong and how you can improve them, right?).
  prefs: []
  type: TYPE_NORMAL
- en: As I ventured deeper into the world of Transformers, I found myself buried under
    a mountain of resources. And yet, despite all that reading, I was left with a
    general sense of the architecture and a trail of lingering questions.
  prefs: []
  type: TYPE_NORMAL
- en: In this guide, I aim to bridge that knowledge gap. A guide that will give you
    a strong intuition on Transformers, a deep dive into the architecture, and the
    implementation from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: 'I strongly advise you to follow the code on [Github](https://github.com/FrancoisPorcher/awesome-ai-tutorials/tree/main/NLP/007%20-%20Transformers%20From%20Scratch):'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/FrancoisPorcher/awesome-ai-tutorials/tree/main/NLP/007%20-%20Transformers%20From%20Scratch?source=post_page-----c9f214c619ac--------------------------------)
    [## awesome-ai-tutorials/NLP/007 - Transformers From Scratch at main ¬∑‚Ä¶'
  prefs: []
  type: TYPE_NORMAL
- en: The best collection of AI tutorials to make you a boss of Data Science! - awesome-ai-tutorials/NLP/007
    - Transformers‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/FrancoisPorcher/awesome-ai-tutorials/tree/main/NLP/007%20-%20Transformers%20From%20Scratch?source=post_page-----c9f214c619ac--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Enjoy! ü§ó
  prefs: []
  type: TYPE_NORMAL
- en: 'A little bit of History first:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many attribute the concept of the attention mechanism to the renowned paper
    ‚ÄúAttention is All You Need‚Äù by the Google Brain team. However, **this is only
    part of the story.**
  prefs: []
  type: TYPE_NORMAL
- en: The roots of the attention mechanism can be traced back to an earlier paper
    titled ‚Äú[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)‚Äù
    authored by Dzmitry Bahdanau, KyungHyun Cho, and Yoshua Bengio.
  prefs: []
  type: TYPE_NORMAL
- en: Bahdanau‚Äôs primary challenge was addressing the limitations of Recurrent Neural
    Networks (RNNs). Specifically, when encoding lengthy sentences into vectors using
    RNNs, **crucial information was often lost.**
  prefs: []
  type: TYPE_NORMAL
- en: Drawing parallels from translation exercises ‚Äî where one often revisits the
    source sentence while translating ‚Äî Bahdanau aimed to allocate weights to the
    hidden states within the RNN. This approach yielded **impressive outcomes**, and
    is depicted in the following diagram.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/021d4865ae1f5167428caeb2caea1723.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [Neural machine translation by jointly learning to align and translate](https://arxiv.org/pdf/1409.0473.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: 'However, Bahdanau wasn‚Äôt the only one tackling this issue. Taking cues from
    his groundbreaking work, the Google Brain team posited a bold idea:'
  prefs: []
  type: TYPE_NORMAL
- en: ‚ÄúWhy not strip everything down and focus solely on the attention mechanism?‚Äù
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: They believed it wasn‚Äôt the RNN but the attention mechanism that was the primary
    driver behind the success.
  prefs: []
  type: TYPE_NORMAL
- en: This conviction culminated in their paper, aptly titled ‚ÄúAttention is All You
    Need‚Äù.
  prefs: []
  type: TYPE_NORMAL
- en: Fascinating, right?
  prefs: []
  type: TYPE_NORMAL
- en: The Transformer Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 1\. First things first, Embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This diagram represents the Transformer architecture. Don‚Äôt worry if you don‚Äôt
    understand anything at first, we will cover absolutely everything.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dcfe23dc844b3b9aa870462b629be554.png)'
  prefs: []
  type: TYPE_IMG
- en: Embeddings, Image from article modified by author
  prefs: []
  type: TYPE_NORMAL
- en: 'From Text to Vectors ‚Äî The Embedding Process: Imagine our input is a sequence
    of words, say ‚ÄúThe cat drinks milk‚Äù. This sequence has a length termed as `seq_len`.
    Our immediate task is to convert these words into a form that the model can understand,
    specifically vectors. That''s where the Embedder comes in.'
  prefs: []
  type: TYPE_NORMAL
- en: Each word undergoes a transformation to become a vector. This transformation
    process is termed as ‚Äòembedding‚Äô. Each of these vectors or ‚Äòembeddings‚Äô has a
    size of `d_model = 512`.
  prefs: []
  type: TYPE_NORMAL
- en: Now, what exactly is this Embedder? At its core, the Embedder is a linear mapping
    (matrix), denoted by `E`. You can visualize it as a matrix of size `(d_model,
    vocab_size)`, where `vocab_size` is the size of our vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: After the embedding process, we end up with a collection of vectors of size
    `d_model` each. It‚Äôs crucial to understand this format, as it‚Äôs a recurrent theme
    ‚Äî you‚Äôll see it across various stages like encoder input, encoder output, and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs code this part:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Note: we multiply by *d_model* for normalization purposes (explained later)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note 2: I personally wondered if we used a pre-trained embedder, or at least
    start from a pre-trained one and fine-tune it. **But no, the embedding is fully
    learned from scratch and initialized randomly.**'
  prefs: []
  type: TYPE_NORMAL
- en: Positional Encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why Do We Need Positional Encoding?
  prefs: []
  type: TYPE_NORMAL
- en: 'Given our current setup, we possess a list of vectors representing words. If
    fed as-is to a transformer model, there‚Äôs a **key element missing: the sequential
    order of words.** Words in natural languages often derive meaning from their position.
    ‚ÄúJohn loves Mary‚Äù carries a different sentiment from ‚ÄúMary loves John.‚Äù To ensure
    our model captures this order, we introduce Positional Encoding.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, you might wonder, ‚ÄúWhy not just add a simple increment like +1 for the
    first word, +2 for the second, and so on?‚Äù There are several challenges with this
    approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Multidimensionality:** Each token is represented in 512 dimensions. A mere
    increment would not suffice to capture this complex space.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Normalization Concerns:** Ideally, we want our values to lie between -1 and
    1\. So, directly adding large numbers (like +2000 for a long text) would be problematic.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Sequence Length Dependency:** Using direct increments is not scale-agnostic.
    For a long text, where the position might be +5000, this number does **not truly
    reflect the relative position** of the token in its associated sentence. And **the
    meaning of a world depends more on its relative position in a sentence, than its
    absolute position in a text.**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you studied mathematics, the idea of circular coordinates ‚Äî **specifically,
    sine and cosine functions** ‚Äî should resonate with your intuition. These functions
    provide a unique way to encode position that meets our needs.
  prefs: []
  type: TYPE_NORMAL
- en: Given our matrix of size `(seq_len, d_model)`, our aim is to add another matrix,
    the Positional Encoding, of the same size.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here‚Äôs the core concept:'
  prefs: []
  type: TYPE_NORMAL
- en: For every token, the authors suggest providing a **sine** coordinate of the
    pairwise dimensions (2k) a **cosine** coordinate to (2k+1).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If we fix the token position, and we move the dimension, we can see that the
    sine/cosine decrease in frequency
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If we look at a token that is further in the text, this phenomenon happens more
    rapidly (the frequency is increased)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/f46bdcaafa0040d4f409a5db8908caf7.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from article
  prefs: []
  type: TYPE_NORMAL
- en: This is summed up in the following graph (but don‚Äôt scratch your head too much
    on this). The Key take away is that Positional Encoding is a mathematical function
    that allows the Transformer to keep an idea of the order of tokens in the sentence.
    This is a very active area or research.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/18a0ef4eaab2b74148416745751809cf.png)'
  prefs: []
  type: TYPE_IMG
- en: Positional Embedding, Image by author
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The Attention Mechanism (Single Head)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let‚Äôs dive into the core concept of Google‚Äôs paper: the Attention Mechanism'
  prefs: []
  type: TYPE_NORMAL
- en: 'High-Level Intuition:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At its core, the attention mechanism is a **communication mechanism between
    vectors/tokens.** It allows a model to focus on specific parts of the input when
    producing an output. Think of it as shining a spotlight on certain parts of your
    input data. This ‚Äúspotlight‚Äù can be brighter on more relevant parts (giving them
    more attention) and dimmer on less relevant parts.
  prefs: []
  type: TYPE_NORMAL
- en: For a sentence, attention helps determine the relationship between words. Some
    words are closely related to each other in meaning or function within a sentence,
    while others are not. **The attention mechanism quantifies these relationships.**
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Consider the sentence: ‚ÄúShe gave him her book.‚Äù'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we focus on the word ‚Äúher‚Äù, the attention mechanism might determine that:'
  prefs: []
  type: TYPE_NORMAL
- en: It has a strong connection with ‚Äúbook‚Äù because ‚Äúher‚Äù is indicating possession
    of the ‚Äúbook‚Äù.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has a medium connection with ‚ÄúShe‚Äù because ‚ÄúShe‚Äù and ‚Äúher‚Äù likely refer to
    the same entity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has a weaker connection with other words like ‚Äúgave‚Äù or ‚Äúhim‚Äù.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical Dive into the Attention mechanism
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/963c9565bade1e73604dfc61e9593e68.png)'
  prefs: []
  type: TYPE_IMG
- en: Scaled Dot-Product Attention, image from [article](https://arxiv.org/pdf/1706.03762.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: '**For each token, we generate three vectors:**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Query (Q):**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Intuition**: Think of the query as a ‚Äú**question**‚Äù that a token poses. It
    represents the current word and tries to find out which parts of the sequence
    are relevant to it.'
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Key (K):**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Intuition**: The key can be thought of as an ‚Äú**identifier**‚Äù for each word
    in the sequence. When the query ‚Äúasks‚Äù its question, the key helps in ‚Äúanswering‚Äù
    by determining **how relevant each word in the sequence is to the query.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. Value (V):**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Intuition**: Once the relevance of each word (via its key) to the query is
    determined, we need actual information or content from those words to assist the
    current token. This is where the value comes in. **It represents the content of
    each word.**'
  prefs: []
  type: TYPE_NORMAL
- en: How are Q, K, V generated?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/a2579e468f9c1df6324091ff86705c7f.png)'
  prefs: []
  type: TYPE_IMG
- en: Q, K, V generation, image by author
  prefs: []
  type: TYPE_NORMAL
- en: The similarity between a query and a key is a dot product (measures the similarity
    between 2 vectors), divided by the standard deviation of this random variable,
    to have everything normalized.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a0751dcbf8f0088625ba917b7a7dbeeb.png)'
  prefs: []
  type: TYPE_IMG
- en: Attention formula, Image from [article](https://arxiv.org/pdf/1706.03762.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs illustrate this with an example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs image we have one query, and want to figure the result of the attention
    with K and V:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ea25d0ec9ac6bef4ab16db1a428ca6cc.png)'
  prefs: []
  type: TYPE_IMG
- en: Q, K, V, Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let‚Äôs compute the similarities between q1 and the keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5b1d818e83c98702b71576756c495c6b.png)'
  prefs: []
  type: TYPE_IMG
- en: Dot Product, Image by author
  prefs: []
  type: TYPE_NORMAL
- en: While the numbers 3/2 and 1/8 might seem relatively close, the softmax function‚Äôs
    exponential nature would amplify their difference.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/188dd4d52274b8982de5d9af84882131.png)'
  prefs: []
  type: TYPE_IMG
- en: Attention weights, Image by author
  prefs: []
  type: TYPE_NORMAL
- en: This differential suggests that q1 has a more pronounced connection to k1 than
    k2.
  prefs: []
  type: TYPE_NORMAL
- en: Now let‚Äôs look at the result of attention, which is a weighted (attention weights)
    combination of the values
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/606d749bd87cd8e042826277d90de621.png)'
  prefs: []
  type: TYPE_IMG
- en: Attention, Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Great! Repeating this operation for every token (q1 through qn) yields a collection
    of *n* vectors.
  prefs: []
  type: TYPE_NORMAL
- en: In practice this operation is vectorized into a matrix multiplication for more
    effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs code it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Multi-Headed Attention
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What‚Äôs the Issue with Single-Headed Attention?
  prefs: []
  type: TYPE_NORMAL
- en: With the single-headed attention approach, every token gets to pose **just one
    query.** This generally translates to it deriving a strong relationship with just
    one other token, given that the softmax tends to **heavily weigh one value while
    diminishing others close to zero.** Yet, when you think about language and sentence
    structures, **a single word often has connections to multiple other words, not
    just one.**
  prefs: []
  type: TYPE_NORMAL
- en: To tackle this limitation, we introduce **multi-headed attention**. The core
    idea? Let‚Äôs allow each token to pose multiple questions (queries) simultaneously
    by running the attention process in parallel for ‚Äòh‚Äô times. The original Transformer
    uses 8 heads.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/87353cc6805b860d5489fc7d9ac85ed1.png)'
  prefs: []
  type: TYPE_IMG
- en: Multi-Headed attention, image from [article](https://arxiv.org/pdf/1706.03762.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: Once we get the results of the 8 heads, we concatenate them into a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a3bb101441fa44f16c78c014742a49e1.png)'
  prefs: []
  type: TYPE_IMG
- en: Multi-Headed attention, image from article
  prefs: []
  type: TYPE_NORMAL
- en: 'This is also straightforward to code, we just have to be careful with the dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: You should start to understand why Transformers are so powerful now, they exploit
    parallelism to the fullest.
  prefs: []
  type: TYPE_NORMAL
- en: Assembling the pieces of the Transformer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'On the high-level, a Transformer is the combination of 3 elements: an **Encoder**,
    a **Decoder**, and a **Generator**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/05ffe38e98c719a712414f6d0d062ceb.png)'
  prefs: []
  type: TYPE_IMG
- en: Endoder, Decoder, Generator, Image from article modified by author
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. The Encoder**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Purpose: Convert an input sequence into a new sequence (usually of smaller
    dimension) that captures the essence of the original data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note: If you‚Äôve heard of the **BERT** model, it uses just this encoding part
    of the Transformer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2\. The Decoder**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Purpose: Generate an output sequence using the encoded sequence from the Encoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note: The decoder in the Transformer is different from the typical autoencoder‚Äôs
    decoder. In the Transformer, **the decoder not only looks at the encoded output
    but also considers the tokens it has generated so far.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**3\. The Generator**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Purpose: Convert a vector to a token. It does this by projecting the vector
    to the size of the vocabulary and then picking the most likely token with the
    softmax function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let‚Äôs code that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'One remark here: ‚Äúsrc‚Äù refers to the input sequence, and ‚Äútarget‚Äù refers to
    the sequence being generated. Remember that we generate the output in an autoregressive
    manner, token by token, so we need to keep track of the target sequence as well.'
  prefs: []
  type: TYPE_NORMAL
- en: Stacking Encoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Transformer‚Äôs Encoder isn‚Äôt just one layer. It‚Äôs actually a stack of *N*
    layers. Specifically:'
  prefs: []
  type: TYPE_NORMAL
- en: Encoder in the original Transformer model consists of a stack of *N=6* identical
    layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inside the Encoder layer, we can see that there are two Sublayer blocks which
    are very similar ((1) and (2)): A **residual connection followed by a layer norm.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Block (1) Self-Attention Mechanism:** Helps the encoder focus on different
    words in the input when generating the encoded representation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Block (2) Feed-Forward Neural Network:** A small neural network applied independently
    to each position.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/63659de9ad14fe9b7d83ee2d23d61c80.png)'
  prefs: []
  type: TYPE_IMG
- en: Encoder Layer, residual connections, and Layer Norm,Image from article modified
    by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let‚Äôs code that:'
  prefs: []
  type: TYPE_NORMAL
- en: 'SublayerConnection first:'
  prefs: []
  type: TYPE_NORMAL
- en: We follow the general architecture, and we can change ‚Äúsublayer‚Äù by either ‚Äúself-attention‚Äù
    or ‚ÄúFFN‚Äù
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can define the full Encoder layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The Encoder Layer is ready, now let‚Äôs just chain them together to form the
    full Encoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Decoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Decoder, just like the Encoder, is structured with multiple identical layers
    stacked on top of each other. The number of these layers is typically 6 in the
    original Transformer model.
  prefs: []
  type: TYPE_NORMAL
- en: How is the Decoder different from the Encoder?
  prefs: []
  type: TYPE_NORMAL
- en: 'A third SubLayer is added to interact with the encoder: this is **Cross-Attention**'
  prefs: []
  type: TYPE_NORMAL
- en: SubLayer (1) is the same as the Encoder. It‚Äôs the **Self-Attention** mechanism,
    meaning that we generate everything (Q, K, V) from the tokens fed into the Decoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SubLayer (2) is the new communication mechanism: **Cross-Attention.** It is
    called that way because we use the **output from (1) to generate the Queries**,
    and we use the **output from the Encoder to generate the Keys and Values (K, V)**.
    In other words, to generate a sentence we have to look both at what we have generated
    so far by the Decoder (self-attention), and what we asked in the first place in
    the Encoder (cross-attention)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SubLayer (3) is identical as in the Encoder.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/a2d11ce51d4babcc3c8f2a513e853b14.png)'
  prefs: []
  type: TYPE_IMG
- en: Decoder Layer, self attention, cross attention, Image from article modified
    by author
  prefs: []
  type: TYPE_NORMAL
- en: Now let‚Äôs code the DecoderLayer. If you understood the mechanism in the EncoderLayer,
    this should be quite straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'And now we can chain the N=6 DecoderLayers to form the Decoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point you have understood around 90% of what a Transformer is. There
    are still a few details:'
  prefs: []
  type: TYPE_NORMAL
- en: Transformer Model Details
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Padding:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a typical transformer, there‚Äôs a maximum length for sequences (e.g., ‚Äúmax_len=5000‚Äù).
    This defines the **longest sequence the model can handle.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, real-world sentences can vary in length. To handle shorter sentences,
    we use padding.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Padding is the addition of special ‚Äúpadding tokens‚Äù to make all sequences in
    a batch the same length.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/4f36cb8ce900c84c7cdbd19bcd3c08ec.png)'
  prefs: []
  type: TYPE_IMG
- en: Padding, image by author
  prefs: []
  type: TYPE_NORMAL
- en: Masking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Masking ensures that during the attention computation, certain tokens are ignored.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two scenarios for masking:'
  prefs: []
  type: TYPE_NORMAL
- en: '**src_masking**: Since we‚Äôve added padding tokens to sequences, we don‚Äôt want
    the model to pay attention to these meaningless tokens. Hence, we mask them out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**tgt_masking** or **Look-Ahead/Causal Masking**: In the decoder, when generating
    tokens sequentially, each token should only be influenced by previous tokens and
    not future ones. For instance, when generating the 5th word in a sentence, it
    shouldn‚Äôt know about the 6th word. This ensures a sequential generation of tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/b6b33c8914fe205aad88c533215a33b0.png)'
  prefs: []
  type: TYPE_IMG
- en: Causal Masking/Look-Ahead masking, image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'We then use this mask to add minus infinity so that the corresponding token
    is ignored. This example should clarify things:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0a6265d3ffcbae569aca4b8d88dc0a87.png)'
  prefs: []
  type: TYPE_IMG
- en: Masking, a trick in the softmax, image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'FFN: Feed Forward Network'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ‚ÄúFeed Forward‚Äù layer in the Transformer‚Äôs diagram is a tad misleading. It‚Äôs
    not just one operation, but a sequence of them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The FFN consists of two linear layers. Interestingly, the input data, which
    might be of dimension `d_model=512`, is first transformed into a higher dimension
    `d_ff=2048` and then mapped back to its original dimension (`d_model=512`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This can be visualized as the data being ‚Äúexpanded‚Äù in the middle of the operation
    before being ‚Äúcompressed‚Äù back to its original size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/a7be031e77a2b0e6be3d5a27316900c5.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from article modified by author
  prefs: []
  type: TYPE_NORMAL
- en: 'This is easy to code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The unparalleled success and popularity of the Transformer model can be attributed
    to several key factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Flexibility.** Transformers can work with any sequence of vectors. These
    vectors can be embeddings for words. It is easy to transpose this to Computer
    Vision by converting an image to different patches, and unfolding a patch into
    a vector. Or even in Audio, we can split an audio into different pieces and vectorize
    them.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Generality**: With minimal inductive bias, the Transformer is **free** to
    capture intricate and nuanced patterns in data, thereby enabling it to learn and
    **generalize better.**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Speed & Efficiency:** Leveraging the immense computational power of GPUs,
    Transformers are designed for **parallel processing.**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Thanks for reading! Before you go:'
  prefs: []
  type: TYPE_NORMAL
- en: You can run the experiments with my Transformer Github Repository.
  prefs: []
  type: TYPE_NORMAL
- en: For more awesome tutorials, check my [compilation of AI tutorials](https://github.com/FrancoisPorcher/awesome-ai-tutorials)
    on Github
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----c9f214c619ac--------------------------------)
    [## GitHub ‚Äî FrancoisPorcher/awesome-ai-tutorials: The best collection of AI tutorials
    to make you a‚Ä¶'
  prefs: []
  type: TYPE_NORMAL
- en: The best collection of AI tutorials to make you a boss of Data Science! ‚Äî GitHub
    ‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----c9f214c619ac--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Y*ou should get my articles in your inbox.* [***Subscribe here.***](https://medium.com/@francoisporcher/subscribe)
  prefs: []
  type: TYPE_NORMAL
- en: '*If you want to have access to premium articles on Medium, you only need a
    membership for $5 a month. If you sign up* [***with my link***](https://medium.com/@francoisporcher/membership)*,
    you support me with a part of your fee without additional costs.*'
  prefs: []
  type: TYPE_NORMAL
- en: If you found this article insightful and beneficial, please consider following
    me and leaving a clap for more in-depth content! Your support helps me continue
    producing content that aids our collective understanding.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)
    (a good portion of the code is inspired from their blog post)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Andrej Karpathy Stanford lecture](https://www.youtube.com/watch?v=L4DC7e6g2iI)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To go further
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Even with a comprehensive guide, there are many other areas linked with Transformers.
    Here are some ideas you may want to explore:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Positional Encoding:** significant improvements have been made, you may want
    to check ‚ÄúRelative positional Encoding‚Äù and ‚ÄúRotary Positional Embedding (RoPE)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Layer Norm**, and the differences with batch norm, group norm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Residual connections**, and their effect on smoothing the gradient'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improvements made to BERT (Roberta, ELECTRA, Camembert)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distillation** of large models into smaller models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications of Transformers in other domains (mainly vision and audio)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The link between Transformers and Graph Neural Networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
