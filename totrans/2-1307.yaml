- en: Image Search in 5 Minutes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5分钟内的图像搜索
- en: 原文：[https://towardsdatascience.com/image-search-in-5-minutes-9bc4f903b22a](https://towardsdatascience.com/image-search-in-5-minutes-9bc4f903b22a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/image-search-in-5-minutes-9bc4f903b22a](https://towardsdatascience.com/image-search-in-5-minutes-9bc4f903b22a)
- en: Cutting-edge image search, simply and quickly
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前沿的图像搜索，简单快速
- en: '[](https://medium.com/@danielwarfield1?source=post_page-----9bc4f903b22a--------------------------------)[![Daniel
    Warfield](../Images/c1c8b4dd514f6813e08e401401324bca.png)](https://medium.com/@danielwarfield1?source=post_page-----9bc4f903b22a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9bc4f903b22a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9bc4f903b22a--------------------------------)
    [Daniel Warfield](https://medium.com/@danielwarfield1?source=post_page-----9bc4f903b22a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@danielwarfield1?source=post_page-----9bc4f903b22a--------------------------------)[![Daniel
    Warfield](../Images/c1c8b4dd514f6813e08e401401324bca.png)](https://medium.com/@danielwarfield1?source=post_page-----9bc4f903b22a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9bc4f903b22a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9bc4f903b22a--------------------------------)
    [Daniel Warfield](https://medium.com/@danielwarfield1?source=post_page-----9bc4f903b22a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9bc4f903b22a--------------------------------)
    ·6 min read·Oct 25, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----9bc4f903b22a--------------------------------)
    ·6分钟阅读·2023年10月25日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/2d9c99b64e20fb41c0c5e490ec885fea.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d9c99b64e20fb41c0c5e490ec885fea.png)'
- en: “Weighing Vectors” by the author using MidJourney. All images by the author
    unless otherwise specified.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用MidJourney的“权重向量”。除非另有说明，否则所有图像均为作者所用。
- en: In this post we’ll implement Text-to-image search (allowing us to search for
    an image via text) and Image-to-image search (allowing us to search for an image
    based on a reference image) using a lightweight pre-trained model. The model we’ll
    be using to calculate image and text similarity is inspired by Contrastive Language
    Image Pre-Training (CLIP), which I discuss in [another article](/clip-intuitively-and-exhaustively-explained-1d02c07dbf40).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将使用一个轻量级的预训练模型来实现文本到图像搜索（允许我们通过文本搜索图像）和图像到图像搜索（允许我们基于参考图像搜索图像）。我们将使用的模型用于计算图像和文本的相似性，受到对比语言图像预训练（CLIP）启发，详细讨论见[另一篇文章](/clip-intuitively-and-exhaustively-explained-1d02c07dbf40)。
- en: '![](../Images/bea61be8f62b0093fbdd23ecc4392b46.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bea61be8f62b0093fbdd23ecc4392b46.png)'
- en: The results when searching for images with the text “a rainbow by the water”
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 使用文本“水边的彩虹”搜索图像的结果
- en: '**Who is this useful for?** Any developers who want to implement image search,
    data scientists interested in practical applications, or non-technical readers
    who want to learn about A.I. in practice.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**这对谁有用？** 任何希望实现图像搜索的开发者，对实际应用感兴趣的数据科学家，或希望了解A.I.实践的非技术读者。'
- en: '**How advanced is this post?** This post will walk you through implementing
    image search as quickly and simply as possible.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**这篇文章有多先进？** 本文将指导你以最快最简单的方式实现图像搜索。'
- en: '**Pre-requisites:** Basic coding experience.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**前提条件：** 基本编码经验。'
- en: What We’re Doing, and How We’re Doing it
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们要做什么，以及我们如何做到这一点
- en: 'This article is a companion piece to my article on “Contrastive Language-Image
    Pre-Training”. Feel free to check it out if you want a more thorough understanding
    of the theory:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本文是我关于“对比语言-图像预训练”文章的补充。如果你想更深入地理解理论，可以查看一下：
- en: '[](/clip-intuitively-and-exhaustively-explained-1d02c07dbf40?source=post_page-----9bc4f903b22a--------------------------------)
    [## CLIP, Intuitively and Exhaustively Explained'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/clip-intuitively-and-exhaustively-explained-1d02c07dbf40?source=post_page-----9bc4f903b22a--------------------------------)
    [## CLIP，直观且全面地解释'
- en: Creating strong image and language representations for general machine learning
    tasks.
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建强大的图像和语言表示，用于通用机器学习任务。
- en: towardsdatascience.com](/clip-intuitively-and-exhaustively-explained-1d02c07dbf40?source=post_page-----9bc4f903b22a--------------------------------)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/clip-intuitively-and-exhaustively-explained-1d02c07dbf40?source=post_page-----9bc4f903b22a--------------------------------)
- en: CLIP models are trained to predict if an arbitrary caption belongs with an arbitrary
    image. We’ll be using this general functionality to create our image search system.
    Specifically, we’ll be using the image and text encoders from CLIP to condense
    inputs into a vector, called an embedding, which can be thought of as a summary
    of the input.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP模型的训练目的是预测任意的标题是否与任意的图像匹配。我们将利用这种通用功能来创建我们的图像搜索系统。具体来说，我们将使用CLIP的图像和文本编码器将输入压缩成一个向量，称为嵌入，可以将其视为输入的总结。
- en: '![](../Images/33a133e53bc9fd1700e73ba5bf2338ba.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/33a133e53bc9fd1700e73ba5bf2338ba.png)'
- en: The job of an encoder is to summarize an input into a meaningful representation,
    called an embedding. [Image from my article on CLIP](https://medium.com/towards-data-science/clip-intuitively-and-exhaustively-explained-1d02c07dbf40).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器的工作是将输入总结成一个有意义的表示，称为嵌入。[我文章中的图像](https://medium.com/towards-data-science/clip-intuitively-and-exhaustively-explained-1d02c07dbf40)。
- en: The whole idea behind CLIP is that similar text and images will have similar
    vector embeddings.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP的整体理念是相似的文本和图像将具有相似的向量嵌入。
- en: '![](../Images/e77e1691f86be1d55c21e599abf4ea1a.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e77e1691f86be1d55c21e599abf4ea1a.png)'
- en: CLIP tries to get the embeddings for similar things to be close together. [Image
    from my article on CLIP](https://medium.com/towards-data-science/clip-intuitively-and-exhaustively-explained-1d02c07dbf40).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP试图让相似事物的嵌入彼此接近。[我文章中的图像](https://medium.com/towards-data-science/clip-intuitively-and-exhaustively-explained-1d02c07dbf40)。
- en: The specific model we’ll be using is called [uform](https://huggingface.co/unum-cloud/uform),
    which is conceptually similar to CLIP. uform is a permissively licensed, pretrained,
    and resource efficient model which promises superior performance to CLIP. Uform
    comes in 3 flavors, we’ll be using the “late fusion” variant which is conceptually
    similar to CLIP.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的具体模型叫做[uform](https://huggingface.co/unum-cloud/uform)，其概念上类似于CLIP。uform是一个宽松许可的、预训练的、高效能的模型，承诺比CLIP具有更好的性能。uform有三种变体，我们将使用与CLIP概念上类似的“late
    fusion”变体。
- en: '![](../Images/88a4673fb97673615ececd8f89965228.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/88a4673fb97673615ececd8f89965228.png)'
- en: The three model flavors in the uform library. As you can see the “Late Fusion
    Model” is very similar to CLIP in that it returns two separate vectors from two
    encoders. [source](https://github.com/unum-cloud/uform#mid-fusion)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: uform库中的三种模型类型。正如你所见，“Late Fusion Model”在功能上与CLIP非常相似，它从两个编码器中返回两个独立的向量。[source](https://github.com/unum-cloud/uform#mid-fusion)
- en: Actual similarity between embeddings will be calculated using cosine similarity.
    The essence of cosine similarity is that two things can be defined as “similar”
    if the angle between their embedding is small. Thus, we can calculate how similar
    text and images are to each other by first embedding both the text and the images,
    then calculating the cosine similarity between the embeddings.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入之间的实际相似度将使用余弦相似度计算。余弦相似度的本质在于，如果它们的嵌入之间的角度很小，则可以定义为“相似”。因此，我们可以通过首先嵌入文本和图像，然后计算嵌入之间的余弦相似度，来计算文本和图像之间的相似性。
- en: '![](../Images/ca66932c013fb805e42bcb5a3f5bf7c2.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ca66932c013fb805e42bcb5a3f5bf7c2.png)'
- en: Cosine similarity uses the angle between vectors to determine similarity. the
    angle between A and B is small, and thus A and B are similar. C would be considered
    very different from both A and B. [Image from my article on CLIP](https://medium.com/towards-data-science/clip-intuitively-and-exhaustively-explained-1d02c07dbf40).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度使用向量之间的角度来确定相似性。A和B之间的角度很小，因此A和B是相似的。C会被认为与A和B都非常不同。[我文章中的图像](https://medium.com/towards-data-science/clip-intuitively-and-exhaustively-explained-1d02c07dbf40)。
- en: 'And that’s the idea in a nutshell: we download the CLIP inspired model (uform),
    use the encoders to embed images and text, then use cosine similarity to search
    for similarities. Feel free to refer to the [companion article](/clip-intuitively-and-exhaustively-explained-1d02c07dbf40)
    for a deeper dive on the theory. Now we just need to put it into practice.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是核心思想：我们下载受CLIP启发的模型（uform），使用编码器来嵌入图像和文本，然后使用余弦相似度来搜索相似性。请随时参考[这篇附带文章](/clip-intuitively-and-exhaustively-explained-1d02c07dbf40)以深入了解理论。现在我们只需将其付诸实践即可。
- en: Implementation
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现
- en: 'I’ll be skipping through some of the unimportant stuff. The full code can be
    found here:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我会跳过一些不重要的内容。完整代码可以在这里找到：
- en: '[](https://github.com/DanielWarfield1/MLWritingAndResearch/blob/main/ImageSearch.ipynb?source=post_page-----9bc4f903b22a--------------------------------)
    [## MLWritingAndResearch/ImageSearch.ipynb at main · DanielWarfield1/MLWritingAndResearch'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/DanielWarfield1/MLWritingAndResearch/blob/main/ImageSearch.ipynb?source=post_page-----9bc4f903b22a--------------------------------)
    [## MLWritingAndResearch/ImageSearch.ipynb在main · DanielWarfield1/MLWritingAndResearch'
- en: Notebook Examples used in machine learning writing and research - MLWritingAndResearch/ImageSearch.ipynb
    at main ·…
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用于机器学习写作和研究的笔记本示例 - MLWritingAndResearch/ImageSearch.ipynb在main ·…
- en: github.com](https://github.com/DanielWarfield1/MLWritingAndResearch/blob/main/ImageSearch.ipynb?source=post_page-----9bc4f903b22a--------------------------------)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/DanielWarfield1/MLWritingAndResearch/blob/main/ImageSearch.ipynb?source=post_page-----9bc4f903b22a--------------------------------)
- en: Downloading the Model
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载模型
- en: This is super easy, just `pip install` the `uform` module, then use the module
    to download the model from Hugging Face. We’ll be using the english version, but
    versions in other languages are also available.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常简单，只需`pip install` `uform`模块，然后使用该模块从Hugging Face下载模型。我们将使用英文版本，但也提供其他语言的版本。
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/0a61b598c6dd6cb1a3627cc9b44b2edc.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a61b598c6dd6cb1a3627cc9b44b2edc.png)'
- en: Defining a Database of Images to Search
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义一个用于搜索的图像数据库
- en: I downloaded a few images from [a dataset](https://www.kaggle.com/datasets/jehanbhathena/weather-dataset)
    for us to play with, which is a derivative of a dataset form the [harvard dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi%3A10.7910%2FDVN%2FM8JQCR)
    (licensed under creative commons), and put them in a public [github repo](https://github.com/DanielWarfield1/MLWritingAndResearch/tree/main/Assets/Images/Weather).
    The following pseudo code downloads those images to the list `images` . This list
    of images is what we’ll ultimately be searching through.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我从[a dataset](https://www.kaggle.com/datasets/jehanbhathena/weather-dataset)下载了几张图片供我们使用，这个数据集是从[harvard
    dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi%3A10.7910%2FDVN%2FM8JQCR)（创意共享许可）衍生的，并将它们放在了一个公共的[github
    repo](https://github.com/DanielWarfield1/MLWritingAndResearch/tree/main/Assets/Images/Weather)中。下面的伪代码将这些图片下载到列表`images`中。这个图片列表就是我们最终将要搜索的内容。
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/3e726ca8ebc3109cb4e0b063efee95d8.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e726ca8ebc3109cb4e0b063efee95d8.png)'
- en: A few examples from the dataset of images we’ll be searching
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一些来自数据集中我们将要搜索的图像示例
- en: Implementing Text-to-Image Search
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现文本到图像的搜索
- en: Here’s where the rubber meets the road. First we’ll define some search text,
    in this example `a rainbow by the water`. Then we can embed that text and compare
    it to the embeddings for all images. We can then sort by the cosine similarity
    to display the top five images which are most similar to the search text. Keep
    in mind, a CLIP style model has a separate image and text encoder, So text gets
    encoded with the text encoder, and images get encoded with the image encoder.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是关键部分。首先，我们将定义一些搜索文本，在这个例子中是`a rainbow by the water`。然后，我们可以将这些文本嵌入，并将其与所有图片的嵌入进行比较。接着，我们可以通过余弦相似度对其进行排序，展示与搜索文本最相似的前五张图片。请记住，CLIP风格模型有一个单独的图像编码器和文本编码器，因此文本由文本编码器编码，图像由图像编码器编码。
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/bea61be8f62b0093fbdd23ecc4392b46.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bea61be8f62b0093fbdd23ecc4392b46.png)'
- en: Implementing Image-to-Image Search
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现图像到图像搜索
- en: Image to image search behaves similarly to the text to image search previously
    discussed; we embed the image we’re using to search, and embed all other images.
    The embedding of our search image is compared to the embedding of all other images
    (using cosine similarity), allowing us to find the most similar images to our
    search image. Naturally, the most similar image in this example is the image itself.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图像到图像搜索与之前讨论的文本到图像搜索行为类似；我们嵌入用于搜索的图像，并嵌入所有其他图像。我们的搜索图像的嵌入与所有其他图像的嵌入进行比较（使用余弦相似度），从而找到与我们搜索图像最相似的图像。自然地，这个例子中最相似的图像就是图像本身。
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/49bd125901e1b370ae68e754890b31ef.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/49bd125901e1b370ae68e754890b31ef.png)'
- en: Conclusion
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: And that’s it! We successfully used a CLIP style model’s image and text encoder
    to implement two types of image search; one based on input text, and one based
    on an input image. We did this by using the text encoder to calculate an embedding
    for the text, the image encoder to calculate an embedding of the images, and searched
    by sorting the similarity of embeddings using cosine similarity.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们成功地使用了CLIP风格模型的图像和文本编码器实现了两种类型的图像搜索：一种基于输入文本，另一种基于输入图像。我们通过使用文本编码器计算文本的嵌入，图像编码器计算图像的嵌入，并通过排序嵌入的相似性（使用余弦相似度）来搜索。
- en: Feel free to check out the [companion article](/clip-intuitively-and-exhaustively-explained-1d02c07dbf40)
    for a deeper dive on CLIP.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 随时查看[配套文章](/clip-intuitively-and-exhaustively-explained-1d02c07dbf40)，以深入了解CLIP。
- en: Follow For More!
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关注以获取更多内容！
- en: I describe papers and concepts in the ML space, with an emphasis on practical
    and intuitive explanations.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我描述了机器学习领域的论文和概念，重点在于实际和直观的解释。
- en: '**Attribution:** All of the resources in this document were created by Daniel
    Warfield, unless a source is otherwise provided. You can use any resource in this
    post for your own non-commercial purposes, so long as you reference this article,
    [https://danielwarfield.dev](https://danielwarfield.dev/), or both. An explicit
    commercial license may be granted upon request.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**归属:** 本文档中的所有资源均由丹尼尔·沃菲尔德（Daniel Warfield）创建，除非另有来源说明。您可以将本文中的任何资源用于您的非商业用途，只要您引用了这篇文章，[https://danielwarfield.dev](https://danielwarfield.dev/)，或者两者都引用。应要求可以授予明确的商业许可。'
