- en: 'U-Net Explained: Understanding its Image Segmentation Architecture'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: U-Net解析：理解其图像分割架构
- en: 原文：[https://towardsdatascience.com/u-net-explained-understanding-its-image-segmentation-architecture-56e4842e313a](https://towardsdatascience.com/u-net-explained-understanding-its-image-segmentation-architecture-56e4842e313a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/u-net-explained-understanding-its-image-segmentation-architecture-56e4842e313a](https://towardsdatascience.com/u-net-explained-understanding-its-image-segmentation-architecture-56e4842e313a)
- en: How skip connections allow CNNs to perform accurate semantic segmentation with
    less data
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跳跃连接如何使CNN在数据较少的情况下进行准确的语义分割
- en: '[](https://conorosullyds.medium.com/?source=post_page-----56e4842e313a--------------------------------)[![Conor
    O''Sullivan](../Images/2dc50a24edb12e843651d01ed48a3c3f.png)](https://conorosullyds.medium.com/?source=post_page-----56e4842e313a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----56e4842e313a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----56e4842e313a--------------------------------)
    [Conor O''Sullivan](https://conorosullyds.medium.com/?source=post_page-----56e4842e313a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://conorosullyds.medium.com/?source=post_page-----56e4842e313a--------------------------------)[![Conor
    O''Sullivan](../Images/2dc50a24edb12e843651d01ed48a3c3f.png)](https://conorosullyds.medium.com/?source=post_page-----56e4842e313a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----56e4842e313a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----56e4842e313a--------------------------------)
    [Conor O''Sullivan](https://conorosullyds.medium.com/?source=post_page-----56e4842e313a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----56e4842e313a--------------------------------)
    ·7 min read·Mar 8, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----56e4842e313a--------------------------------)
    ·7分钟阅读·2023年3月8日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a5de3c10ab618dd3835101b13560d988.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a5de3c10ab618dd3835101b13560d988.png)'
- en: '(source: author)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: (来源：作者)
- en: U-Net is a popular deep-learning architecture for semantic segmentation. Originally
    developed for medical images, it had great success in this field. But, that was
    only the beginning! From satellite images to handwritten characters, the architecture
    has improved performance on a range of data types. Yet, other CNN architectures
    can also do segmentation, so what makes U-Net so special?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: U-Net是一种用于语义分割的流行深度学习架构。最初为医学图像开发，它在这一领域取得了巨大成功。但，这仅仅是开始！从卫星图像到手写字符，该架构在各种数据类型上都提高了性能。然而，其他CNN架构也能进行分割，那么U-Net到底有什么特别之处呢？
- en: To answer this, we will explore the U-Net architecture. We will compare it to
    CNNs used for classification and autoencoders. By doing so, we will understand
    how the **skip connections** are the key to U-Net's success. We will see how they
    allow the architecture to perform accurate segmentations with less data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这个问题，我们将探索U-Net架构。我们将其与用于分类和自编码器的CNN进行比较。通过这样做，我们将理解**跳跃连接**如何是U-Net成功的关键。我们将看到它们如何使该架构在数据较少的情况下进行准确的分割。
- en: What is semantic segmentation?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是语义分割？
- en: We’ll start by understanding what U-Net was developed for. **Image segmentation**
    or **semantic segmentation** is the task of assigning a class to each pixel in
    an image. Models are trained using segmentation maps as target variables. For
    example, see Figure 1\. We have the original image and a binary segmentation map.
    The map separates the image into cell and non-cell pixels.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从理解U-Net开发的目的开始。**图像分割**或**语义分割**是将类分配给图像中每一个像素的任务。模型使用分割图作为目标变量进行训练。例如，参见图1。我们有原始图像和一个二值分割图。这个图将图像分为细胞和非细胞像素。
- en: This biomedical image segmentation task is what U-Net was originally developed
    for. The defining factor of these datasets is the small number of training images.
    The example in Figure 1 comes from a dataset of only 35 images. With the help
    of [image augmentation](https://medium.com/towards-data-science/augmenting-images-for-deep-learning-3f1ea92a891c),
    UNet provided an 11% improvement in accuracy over the second-best approach.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这个生物医学图像分割任务正是U-Net最初开发的目的。这些数据集的决定性因素是训练图像数量很少。图1中的示例来自仅有35张图像的数据集。在[图像增强](https://medium.com/towards-data-science/augmenting-images-for-deep-learning-3f1ea92a891c)的帮助下，U-Net在准确性上比第二好的方法提高了11%。
- en: '![](../Images/620daaec32d9785328282ca1e7fd4dbc.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/620daaec32d9785328282ca1e7fd4dbc.png)'
- en: 'Figure 1: segmenting cells in medical images (source: [O. Ronneberger, et.
    al.](https://arxiv.org/abs/1505.04597) )'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：医学图像中的细胞分割（来源：[O. Ronneberger, et. al.](https://arxiv.org/abs/1505.04597)）
- en: U-Net is also flexible. I have applied it in my own research to segment satellite
    images. As seen in Figure 2 we segment images of coastlines into 2 classes — land
    and water. The task is similar but the input is different to medical images. We
    have gone from a single greyscale image to using [12 spectral bands](https://en.wikipedia.org/wiki/Sentinel-2)
    available in satellite images.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: U-Net 也是灵活的。我在自己的研究中应用了它来分割卫星图像。正如图 2 所示，我们将海岸线图像分割成 2 个类别——陆地和水域。这个任务类似，但输入与医学图像不同。我们从单一的灰度图像变为了使用[12
    个光谱波段](https://en.wikipedia.org/wiki/Sentinel-2)的卫星图像。
- en: '![](../Images/04c2168be53a0cc4c59d88eabad64b24.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04c2168be53a0cc4c59d88eabad64b24.png)'
- en: 'Figure 2: coastline water body segmentation (source: author) (dataset: [SWED](https://openmldata.ukho.gov.uk/))
    (licence: [Sentinel Data Legal Notice](https://sentinel.esa.int/documents/247904/690755/Sentinel_Data_Legal_Notice))'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：海岸线水体分割（来源：作者）（数据集：[SWED](https://openmldata.ukho.gov.uk/)）（许可证：[Sentinel
    数据法律声明](https://sentinel.esa.int/documents/247904/690755/Sentinel_Data_Legal_Notice)）
- en: U-Net Architecture
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: U-Net 架构
- en: So U-Net can achieve good results for a variety of segmentation tasks. To explain
    why we will look at the most important components of the architecture — the encoder,
    decoder and skip connections. We will see how these fit together to *extract*
    and *localize* features in images.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，U-Net 能够在各种分割任务中取得良好的结果。为了说明原因，我们将查看架构中最重要的组件——编码器、解码器和跳跃连接。我们将看到这些如何结合在一起，以*提取*和*定位*图像中的特征。
- en: Encoder
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码器
- en: For semantic segmentation, we care about *what* objects are in the image and
    *where* in the image they are. This is compared to **object detection** or **image
    classification.** Here we aim to predict one class for each image**.** That iswe
    only care *if* an object is in an image. To make these predictions we can use
    an encoder.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对于语义分割，我们关心的是图像中*有什么*物体以及这些物体在图像中的*位置*。这与**目标检测**或**图像分类**不同。在这里，我们旨在为每张图像预测一个类别**。**
    即我们只关心*是否*图像中存在某个物体。为了进行这些预测，我们可以使用编码器。
- en: You will find a version of an encoder in all CNN architectures. Its job is to
    create a compact representation of the input image. This is a lower-dimensional
    representation that contains only the most important information in the image.
    In other words, the encoder is used to *extract* features.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你会在所有 CNN 架构中找到一个编码器的版本。它的工作是创建输入图像的紧凑表示。这是一个低维度的表示，仅包含图像中最重要的信息。换句话说，编码器用于*提取*特征。
- en: '![](../Images/9e8a48c86c2f2834ebe54fa998e7230a.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e8a48c86c2f2834ebe54fa998e7230a.png)'
- en: 'Figure 3: encoder used for image classification (source: author)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：用于图像分类的编码器（来源：作者）
- en: This is done using the convolutional layers and pooling layers. The convolution
    layer is a mapping or kernel that passes over each pixel in an image. This mapping
    is *learned* through the process of training the model. Then, using a *predefined*
    function, the pooling layers reduce the dimensionality of the output.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这通过卷积层和池化层来完成。卷积层是一个映射或卷积核，它会遍历图像中的每一个像素。这个映射通过训练模型的过程*学习*到。然后，使用*预定义*的函数，池化层减少了输出的维度。
- en: '![](../Images/ee2b4e262168146fd54dc4da5361864a.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee2b4e262168146fd54dc4da5361864a.png)'
- en: 'Figure 4: convolutional and pooling layers (source: author)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：卷积层和池化层（来源：作者）
- en: By combining multiple convolutional and pooling layers, we can extract more
    detailed information. We go from low-level details like edges and colours to high-level
    features like ears, teeth and eyes. The network will learn what features are important
    for classification and extractthese to create a compact representation of an image.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 通过组合多个卷积层和池化层，我们可以提取更详细的信息。我们从边缘和颜色等低级细节开始，到耳朵、牙齿和眼睛等高级特征。网络将学习哪些特征对分类很重要，并提取这些特征来创建图像的紧凑表示。
- en: One problem is this compact representation does not include the *location* of
    the features in an image. This is fine for image classification. To classify a
    dog, we only need to know *if* a tail, ear or fur is in the image. It does not
    matter *where* in the image these features come from. In comparison, for segmentation
    location is important.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一个问题是这种紧凑表示不包括图像中特征的*位置*。这对于图像分类来说是可以的。为了分类一只狗，我们只需要知道图像中*是否*有尾巴、耳朵或毛发。图像中这些特征的位置*在哪里*并不重要。相比之下，对于分割，位置是重要的。
- en: Decoder
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解码器
- en: Another problem with the encoder is its output has a low dimension. If used
    for classification the final layer will have a few nodes — one for every class.
    For segmentation, our output will be an image with the same height and width as
    the input.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器的另一个问题是其输出维度较低。如果用于分类，最终层将有几个节点——每个类别一个节点。对于分割，我们的输出将是一个与输入具有相同高度和宽度的图像。
- en: We need a decoder. As seen in Figure 5, this is the section that starts after
    the conv4 block. The decoder will reconstruct an image from the compact representation.
    Like the encoder, it has convolutional blocks. Expect now we have *deconvolution*
    layers that *increase* the dimensionality of the image.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个解码器。如图 5 所示，这是从 conv4 块之后开始的部分。解码器将从紧凑的表示中重建图像。与编码器一样，它有卷积块。现在我们有*反卷积*层来*增加*图像的维度。
- en: '![](../Images/4fd0297aade98e9aba620d125f6bf454.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4fd0297aade98e9aba620d125f6bf454.png)'
- en: 'Figure 5: autoencoder architecture (source: author)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：自编码器架构（来源：作者）
- en: As mentioned, pooling layers will use a predefined method to *reduce* dimensionality.
    Such as max pooling, which takes the maximum pixel value in the cell window. In
    comparison, upsampled or deconvolution layers *increase* dimensionality using
    a function that is learned. That is the upsampling function is updated as the
    model is trained.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，池化层将使用预定义的方法来*减少*维度。例如最大池化，它取单元窗口中的最大像素值。相比之下，上采样或反卷积层使用学习的函数*增加*维度。即上采样函数会在模型训练时更新。
- en: '![](../Images/a6932429ccb1d9d455b348500b70b498.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a6932429ccb1d9d455b348500b70b498.png)'
- en: 'Figure 6: deconvolution using a learned upsampling function (source: author)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：使用学习的上采样函数的反卷积（来源：作者）
- en: In an autoencoder, the input and output images will be the same. Here the goal
    of the decoder is to reconstruct the input as accurately as possible. We can then
    take the parameters from one of the lower dimensional layers (i.e conv4) as a
    compressed image. We can save or send the compressed image. The decoder can then
    be used to reconstruct the original input.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在自编码器中，输入和输出图像将是相同的。这里解码器的目标是尽可能准确地重建输入。然后我们可以将来自较低维度层（即 conv4）的参数作为压缩图像。我们可以保存或发送压缩图像。然后，解码器可以用来重建原始输入。
- en: '![](../Images/40346be7d8eaa403096072b969505155.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/40346be7d8eaa403096072b969505155.png)'
- en: 'Figure 7: the output of an autoencoder vs semantic segmentation model (featuring
    my dog, Guinness) (source: author)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：自编码器输出与语义分割模型的对比（包含我的狗，Guinness）（来源：作者）
- en: At this point, you may be asking if the encoder and decoder are enough. This
    architecture can learn a mapping from image to image. So surely, it can learn
    a mapping to the simpler output required for segmentation.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，你可能会问编码器和解码器是否足够。这种架构可以学习从图像到图像的映射。那么它当然可以学习到分割所需的简单输出映射。
- en: The decoder is able to pass the important features to the encoder. The problem
    is the location of the features is still lost. To get around this, we need an
    *immense* amount of data to train an autoencoder. This is the only way the decoder
    can learn to accurately reconstruct the images from compressed representations.
    With skip connections, we can reduce this data requirement.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器能够将重要特征传递给编码器。问题是特征的位置仍然丢失。为了解决这个问题，我们需要*大量*的数据来训练自编码器。这是解码器能够准确重建压缩表示图像的唯一方法。通过跳跃连接，我们可以减少这个数据需求。
- en: Skip connections
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跳跃连接
- en: The important point is that for autoencoders the encoder and decoder *must*
    be separate. Otherwise, it defeats the entire point of image compression. For
    semantic segmentation, we do not have this restriction.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，对于自编码器，编码器和解码器*必须*是分开的。否则，这就违背了图像压缩的整个意义。对于语义分割，我们没有这个限制。
- en: In a U-Net, skip connections are used to pass information from earlier convolutional
    layers to the deconvolution layers. Critically, what is passed is the *location*
    of the feature extracted by convolutional layers. That is the skip connections
    tell the network *where* in the image the features come from.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在 U-Net 中，跳跃连接用于将早期卷积层的信息传递到反卷积层。关键是传递的内容是卷积层提取的特征的*位置*。也就是说，跳跃连接告诉网络特征在图像中的*位置*。
- en: '![](../Images/e6708ff54fe4b095e45b60765a9cc159.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e6708ff54fe4b095e45b60765a9cc159.png)'
- en: 'Figure 8: U-Net architecture (source: author)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：U-Net 架构（来源：作者）
- en: This is done by concatenating the last layer in the convolutional block and
    the first layer of the opposite deconvolutional block. The U-Net is symmetrical
    — the dimensions of the opposite layers will be the same. As seen in Figure 9,
    this makes it easy to combine the layers into a single tensor. Convolution is
    then done as usual by running the kernel over the single concatenated tensor.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过连接卷积块的最后一层和对称反卷积块的第一层完成的。U-Net 是对称的——对面层的维度将是相同的。如图 9 所示，这使得将层合并为一个单一张量变得容易。然后，通过在单一连接张量上运行内核来进行卷积处理。
- en: '![](../Images/5e4d0a23415ddf968821d402f266b0db.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e4d0a23415ddf968821d402f266b0db.png)'
- en: 'Figure 9: concatenating layers (source: author)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：连接层（来源：作者）
- en: 'This concatenation is at the heart of the U-Net. It combines two important
    pieces of information:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这种连接是 U-Net 的核心。它结合了两个重要的信息：
- en: '**Feature extraction** — features are passed from the previous layer to the
    upsampled layer (blue)'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征提取** — 特征从前一层传递到上采样层（蓝色）'
- en: '**Feature localization** — the location of the feature is passed from the opposite
    convolution layer (shaded orange)'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征定位** — 特征的位置从对面卷积层传递（橙色阴影）'
- en: By combining this information, we can improve the performance of semantic models
    and reduce the amount of data required to train the network.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合这些信息，我们可以提高语义模型的性能，并减少训练网络所需的数据量。
- en: We’ve glossed over a few details such as the activation functions, the number
    of layers and the dimensionality of the layers. These can all be taken as hyperparameters
    in the U-Net. To tackle specific segmentation problems, adaptions have been made
    to the original architecture. The key to the success of all of these is the skip
    connections.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们略过了一些细节，比如激活函数、层数和层的维度。这些都可以作为 U-Net 中的超参数。为了应对特定的分割问题，对原始架构进行了调整。所有这些成功的关键在于跳跃连接。
- en: I hope you enjoyed this article! You can support me by becoming one of my [**referred
    members**](https://conorosullyds.medium.com/membership) **:)**
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你喜欢这篇文章！你可以通过成为我的 [**推荐会员**](https://conorosullyds.medium.com/membership)
    **:)** 来支持我
- en: '[](https://conorosullyds.medium.com/membership?source=post_page-----56e4842e313a--------------------------------)
    [## Join Medium with my referral link — Conor O’Sullivan'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://conorosullyds.medium.com/membership?source=post_page-----56e4842e313a--------------------------------)
    [## 通过我的推荐链接加入 Medium — Conor O’Sullivan'
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story…
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 作为 Medium 会员，你的部分会员费用会分配给你阅读的作者，你可以完全访问每个故事…
- en: conorosullyds.medium.com](https://conorosullyds.medium.com/membership?source=post_page-----56e4842e313a--------------------------------)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: conorosullyds.medium.com](https://conorosullyds.medium.com/membership?source=post_page-----56e4842e313a--------------------------------)
- en: '| [Twitter](https://twitter.com/conorosullyDS) | [YouTube](https://www.youtube.com/channel/UChsoWqJbEjBwrn00Zvghi4w)
    | [Newsletter](https://mailchi.mp/aa82a5ce1dc0/signup) — sign up for **FREE**
    access to a [Python SHAP course](https://adataodyssey.com/courses/shap-with-python/)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '| [Twitter](https://twitter.com/conorosullyDS) | [YouTube](https://www.youtube.com/channel/UChsoWqJbEjBwrn00Zvghi4w)
    | [Newsletter](https://mailchi.mp/aa82a5ce1dc0/signup) — 注册获取 **免费** 访问 [Python
    SHAP 课程](https://adataodyssey.com/courses/shap-with-python/)'
- en: References
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Olaf Ronneberger, Philipp Fischer, Thomas Brox, **U-Net: Convolutional Networks
    for Biomedical Image Segmentation** (2015)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 'Olaf Ronneberger, Philipp Fischer, Thomas Brox, **U-Net: Convolutional Networks
    for Biomedical Image Segmentation** (2015)'
- en: '[https://arxiv.org/abs/1505.04597](https://arxiv.org/abs/1505.04597)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/abs/1505.04597](https://arxiv.org/abs/1505.04597)'
- en: Bharath K, **U-Net Architecture For Image Segmentation** (2021), [https://blog.paperspace.com/unet-architecture-image-segmentation/](https://blog.paperspace.com/unet-architecture-image-segmentation/)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Bharath K, **U-Net 架构用于图像分割** (2021), [https://blog.paperspace.com/unet-architecture-image-segmentation/](https://blog.paperspace.com/unet-architecture-image-segmentation/)
- en: '[Jeremy Zhang](https://medium.com/u/f37783fc8c26?source=post_page-----56e4842e313a--------------------------------),
    **UNet — Line by Line Explanation** (2019), [https://towardsdatascience.com/unet-line-by-line-explanation-9b191c76baf5](/unet-line-by-line-explanation-9b191c76baf5)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[Jeremy Zhang](https://medium.com/u/f37783fc8c26?source=post_page-----56e4842e313a--------------------------------),
    **UNet — 一行行解释** (2019), [https://towardsdatascience.com/unet-line-by-line-explanation-9b191c76baf5](/unet-line-by-line-explanation-9b191c76baf5)'
- en: '[Heet Sankesara](https://medium.com/u/9266f4b8f4c4?source=post_page-----56e4842e313a--------------------------------),
    **UNet — Introducing Symmetry in Segmentation** [https://towardsdatascience.com/u-net-b229b32b4a71](/u-net-b229b32b4a71)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[Heet Sankesara](https://medium.com/u/9266f4b8f4c4?source=post_page-----56e4842e313a--------------------------------)
    的文章《**UNet — 引入分割中的对称性**》 [https://towardsdatascience.com/u-net-b229b32b4a71](/u-net-b229b32b4a71)'
