- en: 'U-Net Explained: Understanding its Image Segmentation Architecture'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/u-net-explained-understanding-its-image-segmentation-architecture-56e4842e313a](https://towardsdatascience.com/u-net-explained-understanding-its-image-segmentation-architecture-56e4842e313a)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How skip connections allow CNNs to perform accurate semantic segmentation with
    less data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://conorosullyds.medium.com/?source=post_page-----56e4842e313a--------------------------------)[![Conor
    O''Sullivan](../Images/2dc50a24edb12e843651d01ed48a3c3f.png)](https://conorosullyds.medium.com/?source=post_page-----56e4842e313a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----56e4842e313a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----56e4842e313a--------------------------------)
    [Conor O''Sullivan](https://conorosullyds.medium.com/?source=post_page-----56e4842e313a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----56e4842e313a--------------------------------)
    ·7 min read·Mar 8, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a5de3c10ab618dd3835101b13560d988.png)'
  prefs: []
  type: TYPE_IMG
- en: '(source: author)'
  prefs: []
  type: TYPE_NORMAL
- en: U-Net is a popular deep-learning architecture for semantic segmentation. Originally
    developed for medical images, it had great success in this field. But, that was
    only the beginning! From satellite images to handwritten characters, the architecture
    has improved performance on a range of data types. Yet, other CNN architectures
    can also do segmentation, so what makes U-Net so special?
  prefs: []
  type: TYPE_NORMAL
- en: To answer this, we will explore the U-Net architecture. We will compare it to
    CNNs used for classification and autoencoders. By doing so, we will understand
    how the **skip connections** are the key to U-Net's success. We will see how they
    allow the architecture to perform accurate segmentations with less data.
  prefs: []
  type: TYPE_NORMAL
- en: What is semantic segmentation?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ll start by understanding what U-Net was developed for. **Image segmentation**
    or **semantic segmentation** is the task of assigning a class to each pixel in
    an image. Models are trained using segmentation maps as target variables. For
    example, see Figure 1\. We have the original image and a binary segmentation map.
    The map separates the image into cell and non-cell pixels.
  prefs: []
  type: TYPE_NORMAL
- en: This biomedical image segmentation task is what U-Net was originally developed
    for. The defining factor of these datasets is the small number of training images.
    The example in Figure 1 comes from a dataset of only 35 images. With the help
    of [image augmentation](https://medium.com/towards-data-science/augmenting-images-for-deep-learning-3f1ea92a891c),
    UNet provided an 11% improvement in accuracy over the second-best approach.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/620daaec32d9785328282ca1e7fd4dbc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: segmenting cells in medical images (source: [O. Ronneberger, et.
    al.](https://arxiv.org/abs/1505.04597) )'
  prefs: []
  type: TYPE_NORMAL
- en: U-Net is also flexible. I have applied it in my own research to segment satellite
    images. As seen in Figure 2 we segment images of coastlines into 2 classes — land
    and water. The task is similar but the input is different to medical images. We
    have gone from a single greyscale image to using [12 spectral bands](https://en.wikipedia.org/wiki/Sentinel-2)
    available in satellite images.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/04c2168be53a0cc4c59d88eabad64b24.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: coastline water body segmentation (source: author) (dataset: [SWED](https://openmldata.ukho.gov.uk/))
    (licence: [Sentinel Data Legal Notice](https://sentinel.esa.int/documents/247904/690755/Sentinel_Data_Legal_Notice))'
  prefs: []
  type: TYPE_NORMAL
- en: U-Net Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So U-Net can achieve good results for a variety of segmentation tasks. To explain
    why we will look at the most important components of the architecture — the encoder,
    decoder and skip connections. We will see how these fit together to *extract*
    and *localize* features in images.
  prefs: []
  type: TYPE_NORMAL
- en: Encoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For semantic segmentation, we care about *what* objects are in the image and
    *where* in the image they are. This is compared to **object detection** or **image
    classification.** Here we aim to predict one class for each image**.** That iswe
    only care *if* an object is in an image. To make these predictions we can use
    an encoder.
  prefs: []
  type: TYPE_NORMAL
- en: You will find a version of an encoder in all CNN architectures. Its job is to
    create a compact representation of the input image. This is a lower-dimensional
    representation that contains only the most important information in the image.
    In other words, the encoder is used to *extract* features.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9e8a48c86c2f2834ebe54fa998e7230a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: encoder used for image classification (source: author)'
  prefs: []
  type: TYPE_NORMAL
- en: This is done using the convolutional layers and pooling layers. The convolution
    layer is a mapping or kernel that passes over each pixel in an image. This mapping
    is *learned* through the process of training the model. Then, using a *predefined*
    function, the pooling layers reduce the dimensionality of the output.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ee2b4e262168146fd54dc4da5361864a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: convolutional and pooling layers (source: author)'
  prefs: []
  type: TYPE_NORMAL
- en: By combining multiple convolutional and pooling layers, we can extract more
    detailed information. We go from low-level details like edges and colours to high-level
    features like ears, teeth and eyes. The network will learn what features are important
    for classification and extractthese to create a compact representation of an image.
  prefs: []
  type: TYPE_NORMAL
- en: One problem is this compact representation does not include the *location* of
    the features in an image. This is fine for image classification. To classify a
    dog, we only need to know *if* a tail, ear or fur is in the image. It does not
    matter *where* in the image these features come from. In comparison, for segmentation
    location is important.
  prefs: []
  type: TYPE_NORMAL
- en: Decoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another problem with the encoder is its output has a low dimension. If used
    for classification the final layer will have a few nodes — one for every class.
    For segmentation, our output will be an image with the same height and width as
    the input.
  prefs: []
  type: TYPE_NORMAL
- en: We need a decoder. As seen in Figure 5, this is the section that starts after
    the conv4 block. The decoder will reconstruct an image from the compact representation.
    Like the encoder, it has convolutional blocks. Expect now we have *deconvolution*
    layers that *increase* the dimensionality of the image.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4fd0297aade98e9aba620d125f6bf454.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: autoencoder architecture (source: author)'
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, pooling layers will use a predefined method to *reduce* dimensionality.
    Such as max pooling, which takes the maximum pixel value in the cell window. In
    comparison, upsampled or deconvolution layers *increase* dimensionality using
    a function that is learned. That is the upsampling function is updated as the
    model is trained.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a6932429ccb1d9d455b348500b70b498.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: deconvolution using a learned upsampling function (source: author)'
  prefs: []
  type: TYPE_NORMAL
- en: In an autoencoder, the input and output images will be the same. Here the goal
    of the decoder is to reconstruct the input as accurately as possible. We can then
    take the parameters from one of the lower dimensional layers (i.e conv4) as a
    compressed image. We can save or send the compressed image. The decoder can then
    be used to reconstruct the original input.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/40346be7d8eaa403096072b969505155.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: the output of an autoencoder vs semantic segmentation model (featuring
    my dog, Guinness) (source: author)'
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you may be asking if the encoder and decoder are enough. This
    architecture can learn a mapping from image to image. So surely, it can learn
    a mapping to the simpler output required for segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: The decoder is able to pass the important features to the encoder. The problem
    is the location of the features is still lost. To get around this, we need an
    *immense* amount of data to train an autoencoder. This is the only way the decoder
    can learn to accurately reconstruct the images from compressed representations.
    With skip connections, we can reduce this data requirement.
  prefs: []
  type: TYPE_NORMAL
- en: Skip connections
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The important point is that for autoencoders the encoder and decoder *must*
    be separate. Otherwise, it defeats the entire point of image compression. For
    semantic segmentation, we do not have this restriction.
  prefs: []
  type: TYPE_NORMAL
- en: In a U-Net, skip connections are used to pass information from earlier convolutional
    layers to the deconvolution layers. Critically, what is passed is the *location*
    of the feature extracted by convolutional layers. That is the skip connections
    tell the network *where* in the image the features come from.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e6708ff54fe4b095e45b60765a9cc159.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: U-Net architecture (source: author)'
  prefs: []
  type: TYPE_NORMAL
- en: This is done by concatenating the last layer in the convolutional block and
    the first layer of the opposite deconvolutional block. The U-Net is symmetrical
    — the dimensions of the opposite layers will be the same. As seen in Figure 9,
    this makes it easy to combine the layers into a single tensor. Convolution is
    then done as usual by running the kernel over the single concatenated tensor.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5e4d0a23415ddf968821d402f266b0db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: concatenating layers (source: author)'
  prefs: []
  type: TYPE_NORMAL
- en: 'This concatenation is at the heart of the U-Net. It combines two important
    pieces of information:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature extraction** — features are passed from the previous layer to the
    upsampled layer (blue)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature localization** — the location of the feature is passed from the opposite
    convolution layer (shaded orange)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By combining this information, we can improve the performance of semantic models
    and reduce the amount of data required to train the network.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve glossed over a few details such as the activation functions, the number
    of layers and the dimensionality of the layers. These can all be taken as hyperparameters
    in the U-Net. To tackle specific segmentation problems, adaptions have been made
    to the original architecture. The key to the success of all of these is the skip
    connections.
  prefs: []
  type: TYPE_NORMAL
- en: I hope you enjoyed this article! You can support me by becoming one of my [**referred
    members**](https://conorosullyds.medium.com/membership) **:)**
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://conorosullyds.medium.com/membership?source=post_page-----56e4842e313a--------------------------------)
    [## Join Medium with my referral link — Conor O’Sullivan'
  prefs: []
  type: TYPE_NORMAL
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: conorosullyds.medium.com](https://conorosullyds.medium.com/membership?source=post_page-----56e4842e313a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '| [Twitter](https://twitter.com/conorosullyDS) | [YouTube](https://www.youtube.com/channel/UChsoWqJbEjBwrn00Zvghi4w)
    | [Newsletter](https://mailchi.mp/aa82a5ce1dc0/signup) — sign up for **FREE**
    access to a [Python SHAP course](https://adataodyssey.com/courses/shap-with-python/)'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Olaf Ronneberger, Philipp Fischer, Thomas Brox, **U-Net: Convolutional Networks
    for Biomedical Image Segmentation** (2015)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://arxiv.org/abs/1505.04597](https://arxiv.org/abs/1505.04597)'
  prefs: []
  type: TYPE_NORMAL
- en: Bharath K, **U-Net Architecture For Image Segmentation** (2021), [https://blog.paperspace.com/unet-architecture-image-segmentation/](https://blog.paperspace.com/unet-architecture-image-segmentation/)
  prefs: []
  type: TYPE_NORMAL
- en: '[Jeremy Zhang](https://medium.com/u/f37783fc8c26?source=post_page-----56e4842e313a--------------------------------),
    **UNet — Line by Line Explanation** (2019), [https://towardsdatascience.com/unet-line-by-line-explanation-9b191c76baf5](/unet-line-by-line-explanation-9b191c76baf5)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Heet Sankesara](https://medium.com/u/9266f4b8f4c4?source=post_page-----56e4842e313a--------------------------------),
    **UNet — Introducing Symmetry in Segmentation** [https://towardsdatascience.com/u-net-b229b32b4a71](/u-net-b229b32b4a71)'
  prefs: []
  type: TYPE_NORMAL
