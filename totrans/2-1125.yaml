- en: How to Chat With Any PDFs and Image Files Using Large Language Models â€” With
    Code
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦‚ä½•ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸ä»»ä½•PDFå’Œå›¾åƒæ–‡ä»¶è¿›è¡ŒèŠå¤© â€” å¸¦ä»£ç 
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/how-to-chat-with-any-file-from-pdfs-to-images-using-large-language-models-with-code-4bcfd7e440bc](https://towardsdatascience.com/how-to-chat-with-any-file-from-pdfs-to-images-using-large-language-models-with-code-4bcfd7e440bc)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/how-to-chat-with-any-file-from-pdfs-to-images-using-large-language-models-with-code-4bcfd7e440bc](https://towardsdatascience.com/how-to-chat-with-any-file-from-pdfs-to-images-using-large-language-models-with-code-4bcfd7e440bc)
- en: Complete guide to building an AI assistant that can answer questions about any
    file
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®Œæ•´æŒ‡å—ï¼Œæ•™ä½ å¦‚ä½•æ„å»ºä¸€ä¸ªå¯ä»¥å›ç­”ä»»ä½•æ–‡ä»¶é—®é¢˜çš„AIåŠ©æ‰‹
- en: '[](https://zoumanakeita.medium.com/?source=post_page-----4bcfd7e440bc--------------------------------)[![Zoumana
    Keita](../Images/34a15c1d03687816dbdbc065f5719f80.png)](https://zoumanakeita.medium.com/?source=post_page-----4bcfd7e440bc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4bcfd7e440bc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4bcfd7e440bc--------------------------------)
    [Zoumana Keita](https://zoumanakeita.medium.com/?source=post_page-----4bcfd7e440bc--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://zoumanakeita.medium.com/?source=post_page-----4bcfd7e440bc--------------------------------)[![Zoumana
    Keita](../Images/34a15c1d03687816dbdbc065f5719f80.png)](https://zoumanakeita.medium.com/?source=post_page-----4bcfd7e440bc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4bcfd7e440bc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4bcfd7e440bc--------------------------------)
    [Zoumana Keita](https://zoumanakeita.medium.com/?source=post_page-----4bcfd7e440bc--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4bcfd7e440bc--------------------------------)
    Â·9 min readÂ·Aug 5, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4bcfd7e440bc--------------------------------)
    Â·9åˆ†é’Ÿé˜…è¯»Â·2023å¹´8æœˆ5æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Introduction
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»‹ç»
- en: So much valuable information is trapped in PDF and image files. Luckily, we
    have these powerful brains capable of processing those files to find specific
    information, which in fact is great.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: PDFå’Œå›¾åƒæ–‡ä»¶ä¸­å›°è—ç€å¦‚æ­¤å®è´µçš„ä¿¡æ¯ã€‚å¹¸è¿çš„æ˜¯ï¼Œæˆ‘ä»¬æœ‰è¿™äº›å¼ºå¤§çš„å¤§è„‘ï¼Œèƒ½å¤Ÿå¤„ç†è¿™äº›æ–‡ä»¶ä»¥æ‰¾åˆ°ç‰¹å®šä¿¡æ¯ï¼Œè¿™å®é™…ä¸Šéå¸¸æ£’ã€‚
- en: But how many of us, deep inside wouldnâ€™t like to have a tool that can answer
    any question about a given document?
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä½†æˆ‘ä»¬ä¸­æœ‰å¤šå°‘äººï¼Œå†…å¿ƒæ·±å¤„å¹¶ä¸å¸Œæœ›æ‹¥æœ‰ä¸€ä¸ªèƒ½å›ç­”å…³äºç»™å®šæ–‡æ¡£çš„ä»»ä½•é—®é¢˜çš„å·¥å…·å‘¢ï¼Ÿ
- en: That is the whole purpose of this article. I will explain step-by-step how to
    build a system that can chat with any PDFs and image files.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯æœ¬æ–‡çš„å…¨éƒ¨ç›®çš„ã€‚æˆ‘å°†é€æ­¥è§£é‡Šå¦‚ä½•æ„å»ºä¸€ä¸ªèƒ½å¤Ÿä¸ä»»ä½•PDFå’Œå›¾åƒæ–‡ä»¶èŠå¤©çš„ç³»ç»Ÿã€‚
- en: 'If you prefer to watch video instead, check the link below:'
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æ›´æ„¿æ„è§‚çœ‹è§†é¢‘ï¼Œè¯·æŸ¥çœ‹ä¸‹é¢çš„é“¾æ¥ï¼š
- en: The video format of the article
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æ–‡ç« çš„è§†é¢‘æ ¼å¼
- en: General Workflow of the project
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é¡¹ç›®çš„æ€»ä½“å·¥ä½œæµç¨‹
- en: Itâ€™s always good to have a clear understanding of the main components of the
    system being built. So letâ€™s get started.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æ¸…æ¥šäº†è§£ç³»ç»Ÿçš„ä¸»è¦ç»„ä»¶æ€»æ˜¯å¥½çš„ã€‚é‚£ä¹ˆï¼Œè®©æˆ‘ä»¬å¼€å§‹å§ã€‚
- en: '![](../Images/84a8f95dccd53c58aa61e24bcae40112.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84a8f95dccd53c58aa61e24bcae40112.png)'
- en: End-to-end workflow of the overall chat system (Image by Author)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æ•´ä¸ªèŠå¤©ç³»ç»Ÿçš„ç«¯åˆ°ç«¯å·¥ä½œæµç¨‹ï¼ˆä½œè€…æä¾›çš„å›¾åƒï¼‰
- en: First, the user submits the document to be processed, which can be in PDF or
    image format.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œç”¨æˆ·æäº¤éœ€è¦å¤„ç†çš„æ–‡æ¡£ï¼Œå¯ä»¥æ˜¯PDFæˆ–å›¾åƒæ ¼å¼ã€‚
- en: A second module is used to detect the format of the file so that the relevant
    content extraction function is applied.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¬¬äºŒä¸ªæ¨¡å—ç”¨äºæ£€æµ‹æ–‡ä»¶æ ¼å¼ï¼Œä»¥ä¾¿åº”ç”¨ç›¸å…³çš„å†…å®¹æå–åŠŸèƒ½ã€‚
- en: The content of the document is then split into multiple chunks using the `Data
    Splitter` module.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæ–‡æ¡£çš„å†…å®¹ä½¿ç”¨`Data Splitter`æ¨¡å—è¢«æ‹†åˆ†æˆå¤šä¸ªå—ã€‚
- en: Those chunks are finally transformed into embeddings using the `Chunk Transformer`
    before they are stored in the vector store.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™äº›å—æœ€ç»ˆé€šè¿‡`Chunk Transformer`è½¬æ¢ä¸ºåµŒå…¥ï¼Œç„¶åå­˜å‚¨åœ¨å‘é‡å­˜å‚¨ä¸­ã€‚
- en: At the end of the process, the userâ€™s query is used to find relevant chunks
    containing the answer to that query, and the result is returned as a JSON to the
    user.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨å¤„ç†ç»“æŸæ—¶ï¼Œç”¨æˆ·çš„æŸ¥è¯¢è¢«ç”¨æ¥æ‰¾åˆ°åŒ…å«è¯¥æŸ¥è¯¢ç­”æ¡ˆçš„ç›¸å…³å—ï¼Œç»“æœä»¥JSONæ ¼å¼è¿”å›ç»™ç”¨æˆ·ã€‚
- en: 1\. Detect document type
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. æ£€æµ‹æ–‡æ¡£ç±»å‹
- en: For each input document, specific processing is applied depending on its type,
    whether it is `PDF` , or `image.`
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ªè¾“å…¥æ–‡æ¡£ï¼Œæ ¹æ®å…¶ç±»å‹åº”ç”¨ç‰¹å®šçš„å¤„ç†ï¼Œæ— è®ºæ˜¯`PDF`è¿˜æ˜¯`image.`
- en: This can be achieved with the helper function `detect_document_type` combined
    with the `guess` function from the built-in Python module.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯ä»¥é€šè¿‡ç»“åˆä½¿ç”¨`detect_document_type`çš„è¾…åŠ©å‡½æ•°å’Œå†…ç½®Pythonæ¨¡å—ä¸­çš„`guess`å‡½æ•°æ¥å®ç°ã€‚
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now we can test the function on two types of documents:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥åœ¨ä¸¤ç§ç±»å‹çš„æ–‡æ¡£ä¸Šæµ‹è¯•è¿™ä¸ªåŠŸèƒ½ï¼š
- en: '`transformer_paper.pdf` is the Transformers research paper [from Arxiv](https://arxiv.org/pdf/1706.03762.pdf).'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformer_paper.pdf` æ˜¯ Transformers ç ”ç©¶è®ºæ–‡ [æ¥è‡ª Arxiv](https://arxiv.org/pdf/1706.03762.pdf)ã€‚'
- en: '`zoumana_article_information.png` is the image document containing information
    about the main topics I have covered on Medium.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`zoumana_article_information.png` æ˜¯åŒ…å«æœ‰å…³æˆ‘åœ¨ Medium ä¸Šæ‰€æ¶µç›–çš„ä¸»è¦ä¸»é¢˜ä¿¡æ¯çš„å›¾åƒæ–‡æ¡£ã€‚'
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Output:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºï¼š
- en: '![](../Images/0f6456b0157ceb2106c561fe6d04c038.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f6456b0157ceb2106c561fe6d04c038.png)'
- en: Files types successfully detected (Image by Author)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æ–‡ä»¶ç±»å‹æˆåŠŸæ£€æµ‹ï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰
- en: Both file type is successfully detected by the `detect_document_type` function.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`detect_document_type` å‡½æ•°æˆåŠŸæ£€æµ‹äº†è¿™ä¸¤ç§æ–‡ä»¶ç±»å‹ã€‚'
- en: 2\. Extract content based on document type
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. åŸºäºæ–‡æ¡£ç±»å‹æå–å†…å®¹
- en: The `[langchain](https://python.langchain.com/docs/get_started/introduction.html)`
    library provides different modules to extract the content of a given type of document.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`[langchain](https://python.langchain.com/docs/get_started/introduction.html)`
    åº“æä¾›äº†ä¸åŒçš„æ¨¡å—æ¥æå–ç‰¹å®šç±»å‹æ–‡æ¡£çš„å†…å®¹ã€‚'
- en: '`UnstructuredImageLoader` extracts image content.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`UnstructuredImageLoader` æå–å›¾åƒå†…å®¹ã€‚'
- en: '`UnstructuredFileLoader` extracts the content of any pdf and Txt files.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`UnstructuredFileLoader` æå–ä»»ä½• pdf å’Œ Txt æ–‡ä»¶çš„å†…å®¹ã€‚'
- en: We can combine these modules and the above `detect_document_type` function to
    implement the text extraction logic.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å°†è¿™äº›æ¨¡å—ä¸ä¸Šè¿°`detect_document_type`å‡½æ•°ç»“åˆèµ·æ¥ï¼Œå®ç°æ–‡æœ¬æå–é€»è¾‘ã€‚
- en: These modules can be used to implement end-to-end text extraction logic within
    the `extract_file_content` function.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ¨¡å—å¯ä»¥ç”¨äºåœ¨`extract_file_content`å‡½æ•°ä¸­å®ç°ç«¯åˆ°ç«¯çš„æ–‡æœ¬æå–é€»è¾‘ã€‚
- en: Letâ€™s see them in action! ğŸ”¥
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹å®ƒä»¬çš„å®é™…æ•ˆæœï¼ ğŸ”¥
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now, letâ€™s print the first `400` characters of each file content.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ‰“å°æ¯ä¸ªæ–‡ä»¶å†…å®¹çš„å‰ `400` ä¸ªå­—ç¬¦ã€‚
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Output:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºï¼š
- en: 'The first 400 characters of each of the above documents are shown below:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸Šæ¯ä¸ªæ–‡æ¡£çš„å‰ 400 ä¸ªå­—ç¬¦å¦‚ä¸‹ï¼š
- en: The research paper content starts with `Provided proper attribution is provided`
    and ends with `Jacod Uszkoreit* Google Research usz@google.com.`
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç ”ç©¶è®ºæ–‡çš„å†…å®¹ä»¥`Provided proper attribution is provided` å¼€å§‹ï¼Œä»¥`Jacod Uszkoreit* Google
    Research usz@google.com.` ç»“æŸã€‚
- en: The image documentâ€™s content starts with `This document provides a quick summary`
    and ends with `Data Science section covers basic to advance concepts.`
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å›¾åƒæ–‡æ¡£çš„å†…å®¹ä»¥`This document provides a quick summary` å¼€å§‹ï¼Œä»¥`Data Science section covers
    basic to advance concepts.` ç»“æŸã€‚
- en: '![](../Images/c03c271f7d3bd2cb53d17f3989ac0dd5.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c03c271f7d3bd2cb53d17f3989ac0dd5.png)'
- en: First 400 characters of the Transformers paper and the Article Information document
    (Image by Author)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers è®ºæ–‡å’Œæ–‡ç« ä¿¡æ¯æ–‡æ¡£çš„å‰ 400 ä¸ªå­—ç¬¦ï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰
- en: 3\. Chat Implementation
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. èŠå¤©å®ç°
- en: The input document is broken into chunks, then an embedding is created for each
    chunk before implementing the question-answering logic.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥æ–‡æ¡£è¢«åˆ†æˆå—ï¼Œç„¶åä¸ºæ¯ä¸ªå—åˆ›å»ºåµŒå…¥ï¼Œä¹‹åå®ç°é—®ç­”é€»è¾‘ã€‚
- en: '**a. Document chunking**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**a. æ–‡æ¡£åˆ†å—**'
- en: The chunks represent smaller segments of a larger piece of text. This process
    is essential to ensure that a piece of content is represented with as little noise
    as possible, making it semantically relevant.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å—ä»£è¡¨äº†è¾ƒå¤§æ–‡æœ¬çš„ä¸€å°æ®µã€‚è¿™ä¸€è¿‡ç¨‹å¯¹äºç¡®ä¿å†…å®¹å°½å¯èƒ½å°‘çš„å™ªéŸ³ï¼Œå¹¶ä¿æŒè¯­ä¹‰ç›¸å…³æ€§è‡³å…³é‡è¦ã€‚
- en: Multiple chunking strategies can be applied. For instance, we have the `NLTKTextSplitter`
    , `SpacyTextSplitter` , `RecursiveCharacterTextSplitter` , `CharacterTextSplitter`
    and more.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥åº”ç”¨å¤šç§åˆ†å—ç­–ç•¥ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬æœ‰ `NLTKTextSplitter`ã€`SpacyTextSplitter`ã€`RecursiveCharacterTextSplitter`ã€`CharacterTextSplitter`
    ç­‰ã€‚
- en: Each one of these strategies has its own pros and cons.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ç­–ç•¥å„æœ‰ä¼˜ç¼ºç‚¹ã€‚
- en: The main focus of this article is made on the `CharacterTextSplitter` which
    creates chunks from the input documents based on `\n\n` , and measure each chunk's
    length (`length_function` ) by its number of characters.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡çš„é‡ç‚¹æ˜¯ `CharacterTextSplitter`ï¼Œå®ƒæ ¹æ® `\n\n` ä»è¾“å…¥æ–‡æ¡£ä¸­åˆ›å»ºå—ï¼Œå¹¶é€šè¿‡å­—ç¬¦æ•°é‡ï¼ˆ`length_function`ï¼‰æ¥è¡¡é‡æ¯ä¸ªå—çš„é•¿åº¦ã€‚
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `chunk_size` tells that we want a maximum of 1000 characters in each chunk,
    and a smaller value will result in more chunks, while a larger one will generate
    fewer chunks.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`chunk_size` æŒ‡å®šæˆ‘ä»¬å¸Œæœ›æ¯ä¸ªå—æœ€å¤šåŒ…å« 1000 ä¸ªå­—ç¬¦ï¼Œè€Œè¾ƒå°çš„å€¼å°†å¯¼è‡´æ›´å¤šçš„å—ï¼Œè€Œè¾ƒå¤§çš„å€¼å°†ç”Ÿæˆæ›´å°‘çš„å—ã€‚'
- en: It is important to note that the way the `chunk_size` is chosen can affect the
    overall result. So, a good approach is to try different values and chose the one
    that better fits one's use case.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ`chunk_size` çš„é€‰æ‹©æ–¹å¼ä¼šå½±å“æ•´ä½“ç»“æœã€‚å› æ­¤ï¼Œä¸€ä¸ªå¥½çš„æ–¹æ³•æ˜¯å°è¯•ä¸åŒçš„å€¼ï¼Œé€‰æ‹©æœ€é€‚åˆè‡ªå·±ç”¨ä¾‹çš„é‚£ä¸ªã€‚
- en: Also, the `chunk_overlap` means that we want a maximum of 200 overlapping characters
    between consecutive chunks.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œ`chunk_overlap` è¡¨ç¤ºæˆ‘ä»¬å¸Œæœ›è¿ç»­å—ä¹‹é—´æœ‰æœ€å¤š 200 ä¸ªé‡å å­—ç¬¦ã€‚
- en: For instance, imagine that we have a document containing the text `Chat with
    your documents using LLMs` and want to apply the chunking using the `Chunk Size
    = 10` and `Chunk overlap = 5.`
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªåŒ…å«æ–‡æœ¬ `Chat with your documents using LLMs` çš„æ–‡æ¡£ï¼Œå¹¶æƒ³ä½¿ç”¨ `Chunk Size =
    10` å’Œ `Chunk overlap = 5` æ¥åº”ç”¨å—åŒ–ã€‚
- en: 'The process is explained in the image below:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥è¿‡ç¨‹åœ¨ä¸‹é¢çš„å›¾åƒä¸­è¿›è¡Œäº†è¯´æ˜ï¼š
- en: '![](../Images/0c513e3c8a79c4e85371ebed619bb1ca.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c513e3c8a79c4e85371ebed619bb1ca.png)'
- en: Document chunking illustration (Image by Author)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æ–‡æ¡£å—åŒ–ç¤ºä¾‹ï¼ˆä½œè€…æä¾›çš„å›¾ç‰‡ï¼‰
- en: We can see that we ended up with a total of 7 chunks for an input document of
    35 characters (spaces included).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå¯¹äºä¸€ä¸ªåŒ…å« 35 ä¸ªå­—ç¬¦ï¼ˆåŒ…æ‹¬ç©ºæ ¼ï¼‰çš„è¾“å…¥æ–‡æ¡£ï¼Œæˆ‘ä»¬æœ€ç»ˆå¾—åˆ°äº† 7 ä¸ªå—ã€‚
- en: But, why do we use these overlaps in the first place?
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ï¼Œæˆ‘ä»¬ä¸ºä»€ä¹ˆè¦ä½¿ç”¨è¿™äº›é‡å éƒ¨åˆ†å‘¢ï¼Ÿ
- en: Including these overlaps, the `CharacterTextSplitter` ensures that the underlying
    context is maintained between the chunks, which is especially useful when working
    with long pieces of documents.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: åŒ…æ‹¬è¿™äº›é‡å éƒ¨åˆ†ï¼Œ`CharacterTextSplitter` ç¡®ä¿åœ¨å—ä¹‹é—´ä¿æŒåº•å±‚ä¸Šä¸‹æ–‡ï¼Œè¿™åœ¨å¤„ç†é•¿æ–‡æ¡£æ—¶ç‰¹åˆ«æœ‰ç”¨ã€‚
- en: Similarly to the `chunk_size` there is no fixed value of `chunk_overlap` . Different
    values need to be tested to choose the one with better results.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼äº `chunk_size`ï¼Œ`chunk_overlap` æ²¡æœ‰å›ºå®šå€¼ã€‚éœ€è¦æµ‹è¯•ä¸åŒçš„å€¼ä»¥é€‰æ‹©æ•ˆæœæ›´å¥½çš„å€¼ã€‚
- en: 'Now, letâ€™s see their application in our scenario:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å®ƒä»¬åœ¨æˆ‘ä»¬çš„åœºæ™¯ä¸­çš„åº”ç”¨ï¼š
- en: '[PRE5]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Output:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºï¼š
- en: '![](../Images/15039c16589eb887d710631a85066443.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15039c16589eb887d710631a85066443.png)'
- en: Number of chunks in each document (Image by Author)
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªæ–‡æ¡£ä¸­çš„å—æ•°ï¼ˆä½œè€…æä¾›çš„å›¾ç‰‡ï¼‰
- en: For a larger document like the research paper, we have a lot more chunks (51)
    compared to the one-page article document, which is only 2.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºåƒç ”ç©¶è®ºæ–‡è¿™æ ·çš„è¾ƒå¤§æ–‡æ¡£ï¼Œæˆ‘ä»¬æœ‰æ›´å¤šçš„å—ï¼ˆ51 ä¸ªï¼‰ï¼Œè€Œä¸€é¡µæ–‡ç« æ–‡æ¡£åªæœ‰ 2 ä¸ªå—ã€‚
- en: '**b. Create embeddings of the chunks**'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**b. åˆ›å»ºå—çš„åµŒå…¥**'
- en: We can use the `OpenAIEmbeddings` module, which uses `text-embedding-ada-002`
    model by default to create the embedding of the chunks.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `OpenAIEmbeddings` æ¨¡å—ï¼Œè¯¥æ¨¡å—é»˜è®¤ä½¿ç”¨ `text-embedding-ada-002` æ¨¡å‹æ¥åˆ›å»ºå—çš„åµŒå…¥ã€‚
- en: 'Instead of using the `text-embedding-ada-002` can use a different model (e.g.
    `gpt-3.5-turbo-0301`) by changing the following parameters:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥é€šè¿‡æ›´æ”¹ä»¥ä¸‹å‚æ•°ï¼Œä½¿ç”¨ä¸åŒçš„æ¨¡å‹ï¼ˆä¾‹å¦‚ `gpt-3.5-turbo-0301`ï¼‰æ¥ä»£æ›¿ `text-embedding-ada-002`ï¼š
- en: model = â€œ`gpt-3.5-turbo-0301`â€
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: model = â€œ`gpt-3.5-turbo-0301`â€
- en: deployment = "`<DEPLOYMENT-NAME>` â€œ which corresponds to the name given during
    the deployment of the model. The default value is also `text-embedding-ada-002`
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: deployment = "`<DEPLOYMENT-NAME>` "ï¼Œè¿™å¯¹åº”äºåœ¨æ¨¡å‹éƒ¨ç½²æœŸé—´ç»™å‡ºçš„åç§°ã€‚é»˜è®¤å€¼ä¹Ÿæ˜¯ `text-embedding-ada-002`
- en: For simplicityâ€™s sake, we will stick to using the default parametersâ€™ value
    in this tutorial. But before that, we need to acquire the OpenAI credentials,
    and all the steps are provided in the [following article](https://medium.com/geekculture/how-to-fine-tune-gpt3-using-openai-api-and-python-9ef813879af4).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç®€å•èµ·è§ï¼Œåœ¨æœ¬æ•™ç¨‹ä¸­æˆ‘ä»¬å°†åšæŒä½¿ç”¨é»˜è®¤å‚æ•°å€¼ã€‚ä½†åœ¨æ­¤ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦è·å– OpenAI å‡­æ®ï¼Œæ‰€æœ‰æ­¥éª¤åœ¨ [ä»¥ä¸‹æ–‡ç« ](https://medium.com/geekculture/how-to-fine-tune-gpt3-using-openai-api-and-python-9ef813879af4)
    ä¸­æä¾›ã€‚
- en: '[PRE6]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**c. Create document search**'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**c. åˆ›å»ºæ–‡æ¡£æœç´¢**'
- en: To get the answer to a given query, we need to create a vector store that finds
    the closest matching chunk to that query.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å›ç­”ç»™å®šçš„æŸ¥è¯¢ï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªå‘é‡å­˜å‚¨ï¼Œä»¥æ‰¾åˆ°ä¸è¯¥æŸ¥è¯¢æœ€åŒ¹é…çš„å—ã€‚
- en: 'Such vector store can be created using the `from_texts` function from `FAISS`
    module and the function takes two main parameters: `text_splitter` and `embeddings`
    which are both defined previously.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·çš„å‘é‡å­˜å‚¨å¯ä»¥ä½¿ç”¨ `FAISS` æ¨¡å—ä¸­çš„ `from_texts` å‡½æ•°åˆ›å»ºï¼Œè¯¥å‡½æ•°éœ€è¦ä¸¤ä¸ªä¸»è¦å‚æ•°ï¼š`text_splitter` å’Œ `embeddings`ï¼Œè¿™ä¸¤è€…éƒ½å·²å®šä¹‰ã€‚
- en: '[PRE7]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: By running the `get_doc_search` on the research paper chunks, we can see that
    the result is of a `vectorstores` . The result would have been the same if we
    used the article_information_chunks.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡åœ¨ç ”ç©¶è®ºæ–‡å—ä¸Šè¿è¡Œ `get_doc_search`ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç»“æœæ˜¯ `vectorstores`ã€‚å¦‚æœæˆ‘ä»¬ä½¿ç”¨ article_information_chunksï¼Œç»“æœä¹Ÿä¼šç›¸åŒã€‚
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Output:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºï¼š
- en: '![](../Images/83925a3f53662ad0e178de1561b3efcb.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/83925a3f53662ad0e178de1561b3efcb.png)'
- en: Vector store of the research paper (Image by Author)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ç ”ç©¶è®ºæ–‡çš„å‘é‡å­˜å‚¨ï¼ˆä½œè€…æä¾›çš„å›¾ç‰‡ï¼‰
- en: '**d. Start chatting with your documents**'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**d. å¼€å§‹ä¸ä½ çš„æ–‡æ¡£èŠå¤©**'
- en: Congrats on making it that far! ğŸ‰
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: æ­å–œä½ èµ°åˆ°è¿™ä¸€æ­¥ï¼ ğŸ‰
- en: The `chat_with_file` function is used to implement the end-to-end logic of the
    chat by combining all the above functions, along with the with `similarity_search`
    function.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`chat_with_file` å‡½æ•°ç”¨äºå®ç°èŠå¤©çš„ç«¯åˆ°ç«¯é€»è¾‘ï¼Œå°†æ‰€æœ‰ä¸Šè¿°å‡½æ•°ä¸ `similarity_search` å‡½æ•°ç»“åˆä½¿ç”¨ã€‚'
- en: 'The final function takes two parameters:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆå‡½æ•°éœ€è¦ä¸¤ä¸ªå‚æ•°ï¼š
- en: The file we want to chat with, and
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æƒ³è¦èŠå¤©çš„æ–‡ä»¶ï¼Œä»¥åŠ
- en: The query provided by the user
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”¨æˆ·æä¾›çš„æŸ¥è¯¢
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Letâ€™s take a step back to properly understand what is happening in the above
    code block.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é€€ä¸€æ­¥ï¼Œä»¥æ­£ç¡®ç†è§£ä¸Šè¿°ä»£ç å—ä¸­å‘ç”Ÿçš„äº‹æƒ…ã€‚
- en: The `load_qa_chain` provides an interface to perform question-answering over
    a set of documents. In this specific case, we are using the default `OpenAI GPT-3`
    large language model.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`load_qa_chain` æä¾›äº†ä¸€ä¸ªæ¥å£ï¼Œç”¨äºåœ¨ä¸€ç»„æ–‡æ¡£ä¸Šæ‰§è¡Œé—®ç­”ã€‚åœ¨è¿™ä¸ªç‰¹å®šçš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯é»˜è®¤çš„ `OpenAI GPT-3` å¤§å‹è¯­è¨€æ¨¡å‹ã€‚'
- en: The `chain_type` is `map_rerank` . By doing so, the `load_qa_chain` function
    returns the answers based on a confidence score given by the chain. There are
    other `chain_type` that can be used such as `map_reduce` , `stuff` , `refine`
    and more. Each one has its own pros and cons.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chain_type` æ˜¯ `map_rerank` ã€‚é€šè¿‡è¿™æ ·åšï¼Œ`load_qa_chain` å‡½æ•°æ ¹æ®é“¾æä¾›çš„ç½®ä¿¡åº¦åˆ†æ•°è¿”å›ç­”æ¡ˆã€‚è¿˜æœ‰å…¶ä»–å¯ä»¥ä½¿ç”¨çš„
    `chain_type`ï¼Œå¦‚ `map_reduce`ã€`stuff`ã€`refine` ç­‰ã€‚æ¯ç§éƒ½æœ‰å…¶ä¼˜ç¼ºç‚¹ã€‚'
- en: By setting `return_intermediate_steps=True` , we can access the metadata such
    as the above confidence score.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡è®¾ç½® `return_intermediate_steps=True`ï¼Œæˆ‘ä»¬å¯ä»¥è®¿é—®è¯¸å¦‚ä¸Šè¿°ç½®ä¿¡åº¦åˆ†æ•°ç­‰å…ƒæ•°æ®ã€‚
- en: 'Its output is a dictionary of two keys: the **answer** to the query, and the
    confidence **score**.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒçš„è¾“å‡ºæ˜¯ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªé”®çš„å­—å…¸ï¼šæŸ¥è¯¢çš„ **ç­”æ¡ˆ** å’Œç½®ä¿¡åº¦ **åˆ†æ•°**ã€‚
- en: 'We can finally chat with the our files, starting with the image document:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç»ˆäºå¯ä»¥å¼€å§‹ä¸æˆ‘ä»¬çš„æ–‡ä»¶èŠå¤©ï¼Œä»å›¾åƒæ–‡æ¡£å¼€å§‹ï¼š
- en: '**Chat with the image document**'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä¸å›¾åƒæ–‡æ¡£èŠå¤©**'
- en: To chat with the image document, we provide the path to the document, and the
    question we want the model to answer.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä¸å›¾åƒæ–‡æ¡£èŠå¤©ï¼Œæˆ‘ä»¬æä¾›æ–‡æ¡£è·¯å¾„å’Œæˆ‘ä»¬å¸Œæœ›æ¨¡å‹å›ç­”çš„é—®é¢˜ã€‚
- en: '[PRE10]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Output:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºï¼š
- en: '![](../Images/34fd452b4430151ca4a7a5bea6dbceb1.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/34fd452b4430151ca4a7a5bea6dbceb1.png)'
- en: Result of a query on the image document (Image by Author)
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹å›¾åƒæ–‡æ¡£çš„æŸ¥è¯¢ç»“æœï¼ˆå›¾åƒæ¥æºï¼šä½œè€…ï¼‰
- en: The model is 100% confident in its response. By looking at the first paragraph
    of the original document below, we can see that the model response is indeed correct.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹å¯¹å…¶å“åº”æœ‰ 100% çš„ä¿¡å¿ƒã€‚é€šè¿‡æŸ¥çœ‹ä¸‹é¢çš„åŸå§‹æ–‡æ¡£çš„ç¬¬ä¸€æ®µï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ¨¡å‹çš„å“åº”ç¡®å®æ˜¯æ­£ç¡®çš„ã€‚
- en: '![](../Images/110f996f430c1236dcc24ef9b5b2b9b4.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/110f996f430c1236dcc24ef9b5b2b9b4.png)'
- en: First two paragraphs of the original article image document (Image by Author)
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: åŸå§‹æ–‡ç« å›¾åƒæ–‡æ¡£çš„å‰ä¸¤æ®µï¼ˆå›¾åƒæ¥æºï¼šä½œè€…ï¼‰
- en: One of the most interesting parts is that it provided a brief summary of the
    main topics covered in the document ( statistics, model evaluation metrics, SQL
    queries, etc.).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€æœ‰è¶£çš„éƒ¨åˆ†ä¹‹ä¸€æ˜¯å®ƒæä¾›äº†æ–‡æ¡£ä¸­ä¸»è¦ä¸»é¢˜çš„ç®€è¦æ€»ç»“ï¼ˆç»Ÿè®¡ã€æ¨¡å‹è¯„ä¼°æŒ‡æ ‡ã€SQL æŸ¥è¯¢ç­‰ï¼‰ã€‚
- en: '**Chat with the PDF file**'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä¸ PDF æ–‡ä»¶èŠå¤©**'
- en: The process with the PDF file is similar to the one in the above section.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: PDF æ–‡ä»¶çš„å¤„ç†è¿‡ç¨‹ç±»ä¼¼äºä¸Šè¿°éƒ¨åˆ†ã€‚
- en: '[PRE11]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Output:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å‡ºï¼š
- en: Once again we are getting a 100% confidence score from the model. The answer
    to the question looks pretty correct!
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å†æ¬¡ä»æ¨¡å‹ä¸­è·å¾—äº† 100% çš„ç½®ä¿¡åº¦åˆ†æ•°ã€‚é—®é¢˜çš„ç­”æ¡ˆçœ‹èµ·æ¥éå¸¸æ­£ç¡®ï¼
- en: '![](../Images/6947fe479ba82c3cde792c3bf82f6981.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6947fe479ba82c3cde792c3bf82f6981.png)'
- en: Result of a query on the PDF document (Image by Author)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹ PDF æ–‡æ¡£çš„æŸ¥è¯¢ç»“æœï¼ˆå›¾åƒæ¥æºï¼šä½œè€…ï¼‰
- en: In both cases, the model was able to provide a human-like response in a few
    seconds. Making a human go through the same process would take minutes, even hours
    depending on the length of the document.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œæ¨¡å‹éƒ½èƒ½å¤Ÿåœ¨å‡ ç§’é’Ÿå†…æä¾›ç±»ä¼¼äººç±»çš„å“åº”ã€‚è®©ä¸€ä¸ªäººç»å†ç›¸åŒçš„è¿‡ç¨‹å°†éœ€è¦å‡ åˆ†é’Ÿï¼Œç”šè‡³å‡ å°æ—¶ï¼Œå…·ä½“å–å†³äºæ–‡æ¡£çš„é•¿åº¦ã€‚
- en: Conclusion
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: Congratulations!!!ğŸ‰
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: æ­å–œï¼ï¼ï¼ğŸ‰
- en: I hope this article provided enough tools to help you take your knowledge to
    the next level. The code is available on [my GitHub](https://github.com/keitazoumana/Medium-Articles-Notebooks/blob/main/Chat_With_Any_Document.ipynb).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¸Œæœ›è¿™ç¯‡æ–‡ç« æä¾›äº†è¶³å¤Ÿçš„å·¥å…·æ¥å¸®åŠ©ä½ æå‡çŸ¥è¯†æ°´å¹³ã€‚ä»£ç å¯åœ¨ [æˆ‘çš„ GitHub](https://github.com/keitazoumana/Medium-Articles-Notebooks/blob/main/Chat_With_Any_Document.ipynb)
    ä¸Šè·å–ã€‚
- en: In my next article, I will explain how to integrate this system into a nice
    user interface. Stay tuned!
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘çš„ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†è§£é‡Šå¦‚ä½•å°†è¿™ä¸ªç³»ç»Ÿé›†æˆåˆ°ä¸€ä¸ªæ¼‚äº®çš„ç”¨æˆ·ç•Œé¢ä¸­ã€‚æ•¬è¯·æœŸå¾…ï¼
- en: Also, If you enjoy reading my stories and wish to support my writing, consider
    becoming a Medium member. Itâ€™s $5 a month, giving you unlimited access to thousands
    of Python guides and Data science articles.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å–œæ¬¢é˜…è¯»æˆ‘çš„æ•…äº‹å¹¶å¸Œæœ›æ”¯æŒæˆ‘çš„å†™ä½œï¼Œè€ƒè™‘æˆä¸º Medium ä¼šå‘˜ã€‚æ¯æœˆ $5ï¼Œä½ å°†è·å¾—å¯¹æˆåƒä¸Šä¸‡çš„ Python æŒ‡å—å’Œæ•°æ®ç§‘å­¦æ–‡ç« çš„æ— é™è®¿é—®ã€‚
- en: By signing up using [my link](https://zoumanakeita.medium.com/membership), I
    will earn a small commission at no extra cost to you.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ä½¿ç”¨ [æˆ‘çš„é“¾æ¥](https://zoumanakeita.medium.com/membership)ï¼Œæˆ‘å°†è·å¾—å°‘é‡ä½£é‡‘ï¼Œè€Œä½ æ— éœ€é¢å¤–æ”¯ä»˜ã€‚
- en: '[](https://zoumanakeita.medium.com/membership?source=post_page-----4bcfd7e440bc--------------------------------)
    [## Join Medium with my referral link - Zoumana Keita'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://zoumanakeita.medium.com/membership?source=post_page-----4bcfd7e440bc--------------------------------)
    [## é€šè¿‡æˆ‘çš„æ¨èé“¾æ¥åŠ å…¥ Medium - Zoumana Keita]'
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every storyâ€¦
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½œä¸º Medium ä¼šå‘˜ï¼Œä½ çš„ä¼šå‘˜è´¹ç”¨çš„ä¸€éƒ¨åˆ†ä¼šç”¨äºæ”¯æŒä½ é˜…è¯»çš„ä½œè€…ï¼ŒåŒæ—¶ä½ å¯ä»¥å®Œå…¨è®¿é—®æ¯ä¸ªæ•…äº‹â€¦â€¦
- en: zoumanakeita.medium.com](https://zoumanakeita.medium.com/membership?source=post_page-----4bcfd7e440bc--------------------------------)
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[zoumanakeita.medium.com](https://zoumanakeita.medium.com/membership?source=post_page-----4bcfd7e440bc--------------------------------)'
- en: Feel free to follow me on [Twitter](https://twitter.com/zoumana_keita_), and
    [YouTube](https://www.youtube.com/channel/UC9xKdy8cz6ZuJU5FTNtM_pQ), or say Hi
    on [LinkedIn](https://www.linkedin.com/in/zoumana-keita/).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: æ¬¢è¿é€šè¿‡ [Twitter](https://twitter.com/zoumana_keita_) å’Œ [YouTube](https://www.youtube.com/channel/UC9xKdy8cz6ZuJU5FTNtM_pQ)
    å…³æ³¨æˆ‘ï¼Œæˆ–è€…åœ¨ [LinkedIn](https://www.linkedin.com/in/zoumana-keita/) ä¸Šæ‰“ä¸ªæ‹›å‘¼ã€‚
- en: '[Letâ€™s connect here for a 1â€“1 discussion](https://topmate.io/zoumanakeita)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[åœ¨è¿™é‡Œä¸æˆ‘è”ç³»è¿›è¡Œä¸€å¯¹ä¸€è®¨è®º](https://topmate.io/zoumanakeita)'
- en: Before you leave, there are more great resources below you might be interested
    in reading!
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ç¦»å¼€ä¹‹å‰ï¼Œä¸‹é¢è¿˜æœ‰æ›´å¤šä½ å¯èƒ½æ„Ÿå…´è¶£çš„ä¼˜è´¨èµ„æºï¼
- en: '[Introduction to Text Embeddings with the OpenAI API](https://medium.com/geekculture/introduction-to-text-embeddings-with-the-openai-api-1f83d2a15fda)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[ä½¿ç”¨ OpenAI API è¿›è¡Œæ–‡æœ¬åµŒå…¥ä»‹ç»](https://medium.com/geekculture/introduction-to-text-embeddings-with-the-openai-api-1f83d2a15fda)'
- en: '[How to Extract Text from Any PDF and Image for Large Language Model](https://medium.com/towards-data-science/how-to-extract-text-from-any-pdf-and-image-for-large-language-model-2d17f02875e6)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[å¦‚ä½•ä»ä»»ä½• PDF å’Œå›¾åƒä¸­æå–æ–‡æœ¬ä»¥ä¾›å¤§å‹è¯­è¨€æ¨¡å‹ä½¿ç”¨](https://medium.com/towards-data-science/how-to-extract-text-from-any-pdf-and-image-for-large-language-model-2d17f02875e6)'
