- en: 'Ensemble Learning with Scikit-Learn: A Friendly Introduction'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/ensemble-learning-with-scikit-learn-a-friendly-introduction-5dd64650de6c](https://towardsdatascience.com/ensemble-learning-with-scikit-learn-a-friendly-introduction-5dd64650de6c)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ensemble learning algorithms like XGBoost or Random Forests are among the top-performing
    models in Kaggle competitions. How do they work?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@riccardo.andreoni?source=post_page-----5dd64650de6c--------------------------------)[![Riccardo
    Andreoni](../Images/5e22581e419639b373019a809d6e65c1.png)](https://medium.com/@riccardo.andreoni?source=post_page-----5dd64650de6c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5dd64650de6c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5dd64650de6c--------------------------------)
    [Riccardo Andreoni](https://medium.com/@riccardo.andreoni?source=post_page-----5dd64650de6c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5dd64650de6c--------------------------------)
    ·7 min read·Sep 9, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d2fbdc1ade153ba4bfe5318e61d89304.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [unsplash.com](https://unsplash.com/photos/0u_vbeOkMpk)'
  prefs: []
  type: TYPE_NORMAL
- en: Fundamental learning algorithms as logistic regression or linear regression
    are often too simple to achieve adequate results for a machine learning problem.
    While a possible solution is to use neural networks, they require a vast amount
    of training data, which is rarely available. Ensemble learning techniques can
    boost the performance of simple models even with a limited amount of data.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine asking a person to guess how many jellybeans there are inside a big
    jar. One person's answer will unlikely be a precise estimate of the correct number.
    Instead, if we ask a thousand people the same question, the average answer will
    likely be close to the actual number. This phenomenon is called the [*wisdom of
    the crowd*](https://en.wikipedia.org/wiki/Wisdom_of_the_crowd)[1]. When dealing
    with complex estimation tasks, the crowd can be considerably more precise than
    an individual.
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble learning algorithms take advantage of this simple principle by aggregating
    the predictions of a group of models, like regressors or classifiers. For an aggregation
    of classifiers, the ensemble model could simply pick the most common class between
    the predictions of the low-level classifiers. Instead, the ensemble can use the
    mean or the median of all the predictions for a regression task.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2bbd943e229c446136d0adc9f5d9e60a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: By aggregating a large number of weak learners, i.e. classifiers or regressors
    which are only slightly better than random guessing, we can achieve unthinkable
    results. Consider a binary classification task. By aggregating 1000 independent
    classifiers with individual accuracy of 51% we can create an ensemble achieving
    an accuracy of 75% [2].
  prefs: []
  type: TYPE_NORMAL
- en: This is the reason why ensemble algorithms are often the winning solutions in
    many machine-learning competitions!
  prefs: []
  type: TYPE_NORMAL
- en: There exist several techniques to build an ensemble learning algorithm. The
    principal ones are bagging, boosting, and stacking. In the following sections,
    I briefly describe each of these principles and present the machine learning algorithms
    to implement them.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first technique to form an ensemble algorithm is called bagging, which is
    the abbreviation of *bootstrap aggregating*. The core idea is to provide each
    weak learner with a slightly different training set. This is done by randomly
    sampling the original training set.
  prefs: []
  type: TYPE_NORMAL
- en: If the sampling is done with replacement, the technique is called *bagging*,
    otherwise, if the sampling is done without replacement, the technique is properly
    called *pasting*.
  prefs: []
  type: TYPE_NORMAL
- en: The key idea of bagging (and pasting) is to create weak learners as diverse
    as possible, and this is done by training them with randomly generated training
    sets. As the sampling is performed with repetition, bagging introduces slightly
    more diversity than pasted and for this reason, is typically preferred.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how bagging works through an example. Suppose the original training
    set is made of 10 examples, and we want to build an ensemble consisting of 3 different
    weak learners. Moreover, we want to train each learner on a subset of the original
    training set of dimension 5\. The following picture illustrates how the training
    set could be divided:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/286887e1e529db33babfbc12a002f112.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pasting instead doesn’t allow the repetition of the same training example in
    a model’s training subset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7d4b3bec9175022557b1fbe86300e509.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Random Forests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most common bagging model is *Random Forests*. A Random Forest is an ensemble
    of decision trees, each one trained on a slightly different training set.
  prefs: []
  type: TYPE_NORMAL
- en: For each node of each decision tree in a Random Forest, the algorithm randomly
    selects the features between which to search for the best possible split. In other
    words, the algorithm doesn’t look for the best possible split among all the features,
    but instead it searches for the best possible split among a subset of the available
    features. This is the explanation of the name “random”.
  prefs: []
  type: TYPE_NORMAL
- en: The following snippet shows how to create and fit a Random Forest Classifier
    on some training data.
  prefs: []
  type: TYPE_NORMAL
- en: Some attributes are extremely important and I suggest tuning them through a
    grid search approach. **n_estimators** defines the number of weak learners, **max_features**
    is the number of features to be considered at each split, **max_depth** is the
    maximum depth of the trees. These hyperparameters play a fundamental role in regularization.
    Increasing **n_estimators** and **min_sample_leaf** or reducing **max_features**
    and **max_depth** helps to create trees as diverse as possible, and consequently
    to avoid overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: For all the other attributes check the comprehensive [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).
  prefs: []
  type: TYPE_NORMAL
- en: Extra Trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to increase even more the randomness of the trees, we can use the Extremely
    Randomized Trees algorithm, Extra Trees for short. It works similarly to Random
    Forests, but at each node of each decision tree, the algorithm searches for the
    best possible split within random thresholds for each feature. Random Forest,
    instead, searches for the best possible split among the whole value range of the
    features.
  prefs: []
  type: TYPE_NORMAL
- en: Extra Trees has one additional random component with respect to Random Forest.
    For this reason, it trades more bias for a lower variance [2].
  prefs: []
  type: TYPE_NORMAL
- en: The second advantage of Extra Trees is that its training is faster than Random
    Forest as it doesn’t have to look for the best possible split in the entire value
    domain of the selected features. Instead, it considers only a portion of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The implementation of an Extra Trees regressor or classifier with Scikit-Learn
    is as simple as Random Forest’s implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: The parameters are the same as for the Random Forest model.
  prefs: []
  type: TYPE_NORMAL
- en: It is generally very difficult to predict whether Random Forest or Extra Trees
    work better for a given task. For this reason, I suggest training both of them
    and comparing them later.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Importance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A nice aspect of both Random Forest and Extra Trees is that they provide a
    measure of the ability of each feature to reduce the impurity of the dataset:
    this is called *feature importance*. In other words, a feature with an high importance
    is able to provide better insights for the classification or regression problem
    than a feature with a low importance.'
  prefs: []
  type: TYPE_NORMAL
- en: After training a model, we can access this measure with the attribute `**feature_importances_**`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5c09688db32363bd4f4c6dfc301a86f6.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Once the model is trained, Scikit-Learn automates the computation of the features'
    importance. The output score is scaled into the [0,1] range, meaning that scores
    closer to 1 are assigned to the most important features.
  prefs: []
  type: TYPE_NORMAL
- en: In the example above, the features ‘petal length’ and ‘petal width’ receive
    a substantially higher score than the others. While ‘sepal length’ has moderate
    importance for our model, the feature ‘sepal width’ appears to be useless for
    this classification task.
  prefs: []
  type: TYPE_NORMAL
- en: Boosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Moving on to another noteworthy ensemble technique, we dive into the boosting
    techniques. This method stands as a potent counterpart to bagging in the world
    of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Boosting, short for hypothesis boosting, follows an interesting philosophy.
    Instead of creating diverse learners through random sampling, boosting focuses
    on enhancing the performance of individual weak learners by training predictors
    sequentialy. Each predictor aims at correcting its predecessor. It’s like a coach
    putting extra practice into a player’s weak spots to make them an all-around star.
  prefs: []
  type: TYPE_NORMAL
- en: Boosting, with its emphasis on self-improvement and gradual refinement, often
    outperforms bagging techniques in scenarios where precision and attention to detail
    are predominant.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll now see the most famous and awarded boosting ensemble technique: XGBoost.'
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: XGBoost, short for Extreme Gradient Boosting, is an ensemble learning method
    that employs gradient descent optimization and it is designed to improve the predictive
    performance of weak learners systematically.
  prefs: []
  type: TYPE_NORMAL
- en: In XGBoost, each weak learner is assigned a weight based on their error rate.
    Learners that perform poorly on specific instances are given higher weights to
    prioritize the corection of those errors. This iterative process continues, with
    the model adapting its focus to address the most challenging data points.
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost is a favored choice among data scientists and machine learning practitioners
    for its ability to extract valuable insights and deliver superior predictive accuracy.
    It is also largely employed in ML competitions due to its speed and scalability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we can see how to apply XGBoost to real data:'
  prefs: []
  type: TYPE_NORMAL
- en: Without hyperparameters tuning, the model is able to achieve a 98% accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we conclude this introductory journey on ensemble algorithms in machine learning,
    it’s important to recognize that our journey has only laid the foundation for
    deeper and more specialized investigations. While we’ve touched upon some fundamental
    concepts, the field extends far beyond these introductory insights.
  prefs: []
  type: TYPE_NORMAL
- en: I recommend digging into the resources and references attached to this article.
    These sources offer detailed insights into advanced ensemble methodologies, algorithm
    optimizations, and practical implementation tips.
  prefs: []
  type: TYPE_NORMAL
- en: If you liked this story, consider following me to be notified of my upcoming
    projects and articles!
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of my past projects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/euro-trip-optimization-genetic-algorithms-and-google-maps-python-api-solve-the-traveling-salesman-4ad8e1548207?source=post_page-----5dd64650de6c--------------------------------)
    [## Euro Trip Optimization: Genetic Algorithms and Google Maps API Solve the Traveling
    Salesman Problem'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate the charm of Europe’s 50 most visited cities using genetic algorithms
    and Google Maps API, unlocking efficient…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/euro-trip-optimization-genetic-algorithms-and-google-maps-python-api-solve-the-traveling-salesman-4ad8e1548207?source=post_page-----5dd64650de6c--------------------------------)
    [](/building-a-convolutional-neural-network-from-scratch-using-numpy-a22808a00a40?source=post_page-----5dd64650de6c--------------------------------)
    [## Building a Convolutional Neural Network from Scratch using Numpy
  prefs: []
  type: TYPE_NORMAL
- en: As Computer Vision applications are becoming omnipresent in our lives, understanding
    the functioning principles of…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/building-a-convolutional-neural-network-from-scratch-using-numpy-a22808a00a40?source=post_page-----5dd64650de6c--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] [The Wisdom of the Crowd — University of Cambridge, Faculty of Mathematics](https://nrich.maths.org/9601)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd
    Edition — Aurélien Géron](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] [Python Data Science Handbook Essential Tools for Working with Data](https://jakevdp.github.io/PythonDataScienceHandbook/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] [XGBoost documentation](https://xgboost.readthedocs.io/en/stable/index.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] [Ensemble Methods Foundations and Algorithms](http://didattica.cs.unicam.it/old/lib/exe/fetch.php?media=didattica%3Amagistrale%3Aml%3Aay_1920%3Aensemble_methods_zhou.pdf)'
  prefs: []
  type: TYPE_NORMAL
