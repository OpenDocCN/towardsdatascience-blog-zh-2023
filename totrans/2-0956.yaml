- en: Generalized Advantage Estimation in Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习中的广义优势估计
- en: 原文：[https://towardsdatascience.com/generalized-advantage-estimation-in-reinforcement-learning-bf4a957f7975](https://towardsdatascience.com/generalized-advantage-estimation-in-reinforcement-learning-bf4a957f7975)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/generalized-advantage-estimation-in-reinforcement-learning-bf4a957f7975](https://towardsdatascience.com/generalized-advantage-estimation-in-reinforcement-learning-bf4a957f7975)
- en: Bias and Variance tradeoff in Policy Gradient
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略梯度中的偏差与方差权衡
- en: '[](https://siwei-causevic.medium.com/?source=post_page-----bf4a957f7975--------------------------------)[![Siwei
    Causevic](../Images/bb8e4ec911272a8e381e94129eabe166.png)](https://siwei-causevic.medium.com/?source=post_page-----bf4a957f7975--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bf4a957f7975--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bf4a957f7975--------------------------------)
    [Siwei Causevic](https://siwei-causevic.medium.com/?source=post_page-----bf4a957f7975--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://siwei-causevic.medium.com/?source=post_page-----bf4a957f7975--------------------------------)[![Siwei
    Causevic](../Images/bb8e4ec911272a8e381e94129eabe166.png)](https://siwei-causevic.medium.com/?source=post_page-----bf4a957f7975--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bf4a957f7975--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bf4a957f7975--------------------------------)
    [Siwei Causevic](https://siwei-causevic.medium.com/?source=post_page-----bf4a957f7975--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bf4a957f7975--------------------------------)
    ·6 min read·Mar 27, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----bf4a957f7975--------------------------------)
    ·6分钟阅读·2023年3月27日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f6ba99d09255dd7c15926e13695c4125.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f6ba99d09255dd7c15926e13695c4125.png)'
- en: photo by the author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的照片
- en: Policy gradient methods are one of the most widely used learning algorithms
    in reinforcement learning. They aim to optimize a parameterized policy and use
    value functions to help estimate how the policy should be improved.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度方法是强化学习中最广泛使用的学习算法之一。它们旨在优化参数化策略，并使用价值函数来帮助估计如何改进策略。
- en: One of the major issues in reinforcement learning though, especially for policy
    gradient methods, is the long time delay between actions and their positive or
    negative effect on rewards, which makes reward estimation extremely difficult.
    That being said, RL researchers usually estimate the long term rewards(return)
    with either the bootstrapped rewards from episodes or the value function, and
    sometimes both. However, both methods have their drawbacks. The issue with the
    former is the high variance from the samples, and the latter is the high bias
    in the estimated value function.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习中的一个主要问题，特别是对于策略梯度方法，是行动与其对奖励的正面或负面影响之间的长时间延迟，这使得奖励估计极为困难。也就是说，强化学习研究者通常用从回合中获得的引导奖励或价值函数来估计长期奖励（回报），有时两者都会用。然而，这两种方法都有其缺点。前者的问题在于样本的高方差，后者则是在估计的价值函数中的高偏差。
- en: In this article, we will go over Generalized Advantage Estimation(GAE), a family
    of policy gradient estimators that significantly reduce variance while maintaining
    a tolerable level of bias.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将讨论广义优势估计（GAE），这是一类策略梯度估计器，它在保持可接受的偏差水平的同时，显著减少了方差。
- en: The content below assumes a basic understanding of the policy gradient methods.
    If you are new to reinforcement learning, check out my previous article on [RL
    basics and algorithm overview](/an-overview-of-classic-reinforcement-learning-algorithms-part-1-f79c8b87e5af),
    and a [deep dive into vanilla policy gradient](https://medium.com/towards-data-science/policy-gradient-reinforce-algorithm-with-baseline-e95ace11c1c4).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 以下内容假设你对策略梯度方法有基本了解。如果你是强化学习的新手，请查看我之前关于[强化学习基础和算法概述](/an-overview-of-classic-reinforcement-learning-algorithms-part-1-f79c8b87e5af)的文章，以及[对普通策略梯度的深入分析](https://medium.com/towards-data-science/policy-gradient-reinforce-algorithm-with-baseline-e95ace11c1c4)。
- en: Bias and Variance Tradeoff
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 偏差与方差权衡
- en: 'Recall the generic form of policy gradient that we discussed in the [vanilla
    policy gradient](https://medium.com/towards-data-science/policy-gradient-reinforce-algorithm-with-baseline-e95ace11c1c4):'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 回忆一下我们在[普通策略梯度](https://medium.com/towards-data-science/policy-gradient-reinforce-algorithm-with-baseline-e95ace11c1c4)中讨论的策略梯度的一般形式：
- en: '![](../Images/79d444a751fc692dff9a1b470a12455e.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/79d444a751fc692dff9a1b470a12455e.png)'
- en: The goal is to find the parameters *θ* for a policy that maximizes *V(θ)*. To
    do so, we search for the maxima in *V(θ)* by ascending the gradient of the policy,
    w.r.t parameters θ.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是找到使策略最大化*V(θ)*的参数*θ*。为此，我们通过沿着策略的梯度上升来寻找*V(θ)*的最大值，关于参数θ。
- en: '![](../Images/a9021ed8605d1743d9651e3c97f3be6d.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9021ed8605d1743d9651e3c97f3be6d.png)'
- en: 'The above function is the vanilla policy gradient that solely rely on the return
    R(τ), which is the sum of rewards for a trajectory τ:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 上述函数是普通策略梯度，完全依赖于回报R(τ)，即轨迹τ的奖励总和：
- en: '![](../Images/bbe90f8b7eddc6cd83e56781f6ef7a0b.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bbe90f8b7eddc6cd83e56781f6ef7a0b.png)'
- en: Because R(τ) is estimated from many sampled trajectories, the vanilla policy
    gradient method is prune to high variance, and to address this, researchers have
    found several different ways to estimate the rewards in a more stable way.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 由于R(τ)是从许多采样轨迹中估计得到的，普通的策略梯度方法容易受到高方差的影响，为了解决这个问题，研究人员发现了几种以更稳定的方式估计奖励的方法。
- en: 'Let’s expand the above value function into the stepwise form:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将上述值函数扩展为逐步形式：
- en: '![](../Images/115e5db734c525465df60a1e3ffc3c28.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/115e5db734c525465df60a1e3ffc3c28.png)'
- en: 'Ψt is the generic representation of the rewards, which may be one of the following:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Ψt是奖励的通用表示，可以是以下之一：
- en: '![](../Images/0a235de60f591aa182c7f49b912b6ae2.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a235de60f591aa182c7f49b912b6ae2.png)'
- en: 'Option 1 and 2 solely rely on sampled rewards, and option 3 subtracts a baseline
    from the rewards. However, they still suffer from high variance during learning.
    In fact, the choice Ψt = Aπ (st , at ) was proved to yield the lowest possible
    variance. Here the advantage function is defined as:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 选项1和2完全依赖于采样奖励，而选项3则从奖励中减去基线。然而，它们在学习过程中仍然会受到高方差的困扰。实际上，选择Ψt = Aπ (st , at )已被证明能够产生最低的方差。这里，优势函数定义为：
- en: '![](../Images/994002b6fb52321a308b1d72bdaa3b3b.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/994002b6fb52321a308b1d72bdaa3b3b.png)'
- en: It measures whether or not the action is better or worse than the policy’s default
    behavior. Note choice 5 and 6 are equivalent if the learning is on-policy. In
    practice, the advantage function is not known and must be estimated, which makes
    the estimator biased. GAE takes one step further by discounting the advantage
    function with an additional parameter γ. We will go into more details in the next
    section.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 它衡量动作是否比策略的默认行为更好或更差。注意，选择5和6在学习是在线策略时是等效的。在实践中，优势函数是不知道的，必须进行估计，这使得估计器有偏。GAE进一步通过引入额外参数γ来折扣优势函数。我们将在下一节中深入讨论。
- en: What is Generalized Advantage Estimation(GAE)
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是广义优势估计（GAE）
- en: 'Building on top of the advantage estimator, GAE introduces a parameter γ that
    allows us to reduce variance by downweighting rewards corresponding to delayed
    effects. of course this comes at the cost of introducing bias. This is similar
    idea to the discount factor in the Bellman equation that de-prioritizes rewards
    in the far future. With the discount, the advantage function is represented as:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在优势估计器的基础上，GAE引入了一个参数γ，使我们可以通过降低对应于延迟效应的奖励的权重来减少方差。当然，这会引入偏差。这与贝尔曼方程中的折扣因子的思想类似，折扣因子降低了远期奖励的优先级。使用折扣后，优势函数表示为：
- en: '![](../Images/bb79b7b22b2a25ec223aa77da8f61cad.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bb79b7b22b2a25ec223aa77da8f61cad.png)'
- en: 'In practice, we need to estimate the value function and this is usually modeled
    with a neutral net that predicts the value for a particular state (and action
    if we want to estimate Q value). Let’s define the TD residual δt with discount
    γ. Note that δt can be considered as an estimate of the advantage Ψt:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们需要估计值函数，这通常使用一个神经网络来预测特定状态的值（如果我们想估计Q值，则包括动作）。我们定义折扣γ的TD残差δt。注意，δt可以视为优势Ψt的估计：
- en: '![](../Images/3cef11a0258c45f1bea391bbfe0d6ce1.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3cef11a0258c45f1bea391bbfe0d6ce1.png)'
- en: 'Recall Temporal Difference (TD) methods we discussed previously [here](/an-overview-of-classic-reinforcement-learning-algorithms-part-1-f79c8b87e5af).
    Here we replace Q value with the value function(for more details please refer
    to the Bellman equation). The discounted advantage can be written as:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾我们之前讨论的时序差分（TD）方法[在这里](/an-overview-of-classic-reinforcement-learning-algorithms-part-1-f79c8b87e5af)。在这里，我们将Q值替换为值函数（更多细节请参阅贝尔曼方程）。折扣优势可以表示为：
- en: '![](../Images/d3c4edd7155307faf9b5be89f8ab1570.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d3c4edd7155307faf9b5be89f8ab1570.png)'
- en: In TD, we trade off bias and variance by deciding on the number of steps to
    sample. The more steps we sample, the less we rely on the biased value function
    estimator at the cost of variance. One tricky thing is to find the sweet spot
    in TD which brings the ideal bias-variance tradeoff. This is where GAE shines
    — instead of empirically testing different step sizes, let’s just use the exponentially-weighted
    average of these k-step estimators. The equations below show the discounted advantage
    function at different step size(k).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TD 中，我们通过决定采样步骤的数量来权衡偏差和方差。采样的步骤越多，我们对带有偏差的价值函数估计器的依赖就越小，但方差也会增加。一个棘手的问题是找到
    TD 中带来理想偏差-方差折衷的甜蜜点。这就是 GAE 发挥作用的地方——我们不再通过经验测试不同的步骤大小，而是直接使用这些 k 步估计器的指数加权平均。下面的方程展示了不同步骤大小
    (k) 下的折扣优势函数。
- en: '![](../Images/c224f5dd68334e41deb577ce056a1a4e.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c224f5dd68334e41deb577ce056a1a4e.png)'
- en: 'Applying the exponentially-weighted average, we get the **final form of GAE**:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 采用指数加权平均，我们得到了**GAE 的最终形式**：
- en: '![](../Images/a4630df2a6027da15cd7128ddbaebc75.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4630df2a6027da15cd7128ddbaebc75.png)'
- en: Note here we introduced another parameter λ. When λ = 0, GAE is essentially
    the same as TD(0) but applied in the context of policy optimization. It has high
    bias as it heavily relies on the estimated value function. When λ = 1, this is
    the case of vanilla policy gradient with a baseline that has high variance due
    to the sum of terms.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在这里引入了另一个参数 λ。当 λ = 0 时，GAE 本质上与 TD(0) 相同，但应用于策略优化的背景下。由于 heavily 依赖于估计的价值函数，因此具有较高的偏差。当
    λ = 1 时，这就是带有基线的原始策略梯度，由于项的和，具有较高的方差。
- en: '![](../Images/2637df51d54142760925bc8ca4e1f651.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2637df51d54142760925bc8ca4e1f651.png)'
- en: In practice we set 0 < λ < 1 to control the compromise between bias and variance
    just like the lambda parameter in TD lambda.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 实际应用中，我们设置 0 < λ < 1 来控制偏差和方差之间的折衷，就像 TD λ 中的 lambda 参数一样。
- en: Reward Shaping — Yet Another Interpretation of GAE
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奖励塑形——GAE 的另一种解释
- en: 'Another way to interpret GAE is to treat the environment as a reward-reshaped
    MDP. Assume we have a transformed reward function:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种解读 GAE 的方式是将环境视为奖励重塑的 MDP。假设我们有一个转换的奖励函数：
- en: '![](../Images/e678fe9daf4876e7824d39f90120e588.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e678fe9daf4876e7824d39f90120e588.png)'
- en: 'The discounted sum of the transformed reward(i.e. the return) is exactly the
    TD residual we discussed above:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 转换奖励的折扣和（即回报）正是我们上述讨论的 TD 残差：
- en: '![](../Images/5eccb8b1084d6d0c1bfd1d34536f0672.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5eccb8b1084d6d0c1bfd1d34536f0672.png)'
- en: 'Essentially the policy gradient and optimal policy are unchanged but our objective
    is to maximize the discounted sum of rewards instead. We can further add a “steeper”
    discount λ, where 0 ≤ λ ≤ 1, and we can get the final GAE form:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，策略梯度和最优策略保持不变，但我们的目标是最大化折扣奖励的总和。我们还可以增加一个“更陡”的折扣 λ，其中 0 ≤ λ ≤ 1，我们可以得到最终的
    GAE 形式：
- en: '![](../Images/0fddf47a1b65500ebf0a3da8221837fb.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0fddf47a1b65500ebf0a3da8221837fb.png)'
- en: To summarize, we can treat GAE as the same policy gradient in a reward-reshaped
    MDP, with a steep discount γλ to cut off the noise arising from long delays.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们可以将 GAE 视为奖励重塑 MDP 中相同的策略梯度，用一个陡峭的折扣 γλ 来削减来自长延迟的噪声。
- en: Conclusion
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article we discussed the bias and variance tradeoff in RL, especially
    in the policy gradient methods. Then we introduced GAE, a technique that allows
    us to reduce variance by downweighting rewards at a much smaller cost of bias.
    GAE is a very important estimator and enjoys widespread use in many advanced algorithms
    including VPG, TRPO, and PPO.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们讨论了 RL 中的偏差和方差折衷，特别是在策略梯度方法中。然后我们介绍了 GAE，一种通过以较小的偏差代价来降低方差的技术。GAE 是一个非常重要的估计器，并在许多高级算法中得到广泛应用，包括
    VPG、TRPO 和 PPO。
- en: With that I wanted to thank you for reading through this article and I appreciate
    any feedback.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在此，我想感谢你阅读本文，任何反馈都将不胜感激。
