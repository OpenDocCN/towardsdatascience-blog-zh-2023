["```py\nimport vertexai\nfrom langchain.llms import VertexAI\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.vectorstores import Pinecone\n\n# Step 0: Pre-requisite\n# =========================\nos.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\nos.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n# Access PaLM in Google Cloud's Vertex AI\nPROJECT_ID = \"xxxxx\"  \nvertexai.init(project=PROJECT_ID, location=\"xxxx\") # ex: us-central1\n\n# Use Pinecone as Langchain vectorstore\ntext_field = \"text\"\nindex_name = 'outside-chatgpt'\nindex = pinecone.Index(index_name)\nvectorstore = Pinecone(\n    index, embed.embed_query, text_field\n)\n\n# ====== Step 1: Specify LLMs ============\n# LLM: gpt-3.5\nllm_gpt = ChatOpenAI(\n    openai_api_key=OPENAI_API_KEY,\n    model_name='gpt-3.5-turbo-0613',\n    temperature=0.1,\n    max_tokens=500\n)\n\n# LLM: palm2-bison\nllm_palm = VertexAI(\n    model_name=\"text-bison@001\",\n    temperature=0.1,\n    max_output_tokens=500,    \n    verbose=True,\n)\n```", "```py\nfrom langchain.chains import RetrievalQAWithSourcesChain\n\n# Performance measure function\ndef timeit(func):\n    def wrapper(*args, **kwargs):\n        start_time = time.time()\n        result = func(*args, **kwargs)\n        end_time = time.time()\n        spent_time = round(end_time - start_time, 4)\n        if spent_time > 120.0:\n            time_min = round(spent_time/60, 3)\n            print(f\"PERFORMANCE {func.__name__}: {time_min} minutes\")\n        elif spent_time < 0.1:\n            time_ms = round(spent_time*1000, 3)\n            print(f\"PERFORMANCE {func.__name__}: {time_ms} milliseconds\")\n        else:\n            print(f\"PERFORMANCE {func.__name__}: {spent_time} seconds\")\n        return result\n    return wrapper\n\n# ==== Step 2: Retrieval QA with source chain =======\n@timeit\ndef chatOutside (query, llm):\n    # with source\n    qa = RetrievalQAWithSourcesChain.from_chain_type(\n        llm=llm,\n        chain_type=\"stuff\",\n        retriever=vectorstore.as_retriever()\n    )\n\n    return qa(query)\n```", "```py\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain.memory import ConversationBufferMemory\n\n# ===== Step 3: Conversational Retrieval chain =========\n@timeit\ndef chatOutside_cr (query, llm, answer_only=False):\n    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n\n    # Conversation Retrieval Chain\n    qa = ConversationalRetrievalChain.from_llm(\n    llm=llm,\n    retriever=vectorstore.as_retriever(), \n    memory=memory,\n    return_source_documents=False\n    )\n    # qa({\"question\": query})\n    full_res = qa(query)\n\n    if answer_only==True:\n        # return answer only\n        answer = full_res['answer']\n        return answer\n    else:\n        return full_res\n```", "```py\nfrom langchain.agents import Tool\nfrom langchain.agents import initialize_agent\nfrom langchain.utilities import SerpAPIWrapper\n\ndef chat_agent(query, llm):\n    #======= Step 1: Search tool ========\n    # google search\n    search = SerpAPIWrapper()\n\n    # ===== Step 2: Memory =========\n    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n\n    # ====== Step 3: Chain =======\n\n    # option 1: RetrievalQA with source chain\n#     qa = RetrievalQAWithSourcesChain.from_chain_type(\n#         llm=llm,\n#         chain_type=\"stuff\",\n#         retriever=vectorstore.as_retriever()\n#     )\n\n    # option 2: Conversation Retrieval chain\n    qa = ConversationalRetrievalChain.from_llm(\n        llm=llm,\n        retriever=vectorstore.as_retriever(), \n        memory=memory,\n        return_source_documents=False\n    )\n\n    #==== Step 4: Create a list of tools\n    tools = [\n        # Outside Knowledge Base\n        Tool(\n            name='Knowledge Base',\n            func=qa.__call__, # qa.run won't work!!\n            description='use this tool when answering general knowledge queries '\n            ),\n        # Search\n        Tool(\n            name=\"Search\",\n            func=search.run,\n            description='use this tool when you need to answer questions about weather or current status of the world ' \n        )\n    ]\n\n    #==== Step 5: Agent ========\n\n    agent = initialize_agent(\n        agent='chat-conversational-react-description',\n        llm=llm,\n        tools=tools,\n        verbose=True,\n        max_iterations=3,\n        early_stopping_method='generate',\n        memory=memory \n    )\n\n    return agent(query)\n```", "```py\nquery = \"Hi, my name is Wen and I live in Oakland, California.\"\n```"]