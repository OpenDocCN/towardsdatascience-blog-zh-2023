- en: A Simple Way of Improving Zero-Shot CLIP Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/simple-way-of-improving-zero-shot-clip-performance-4eae474cb447](https://towardsdatascience.com/simple-way-of-improving-zero-shot-clip-performance-4eae474cb447)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Part 1 — Customized Prompts via Language Models (CuPL)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@alexml0123?source=post_page-----4eae474cb447--------------------------------)[![Alexey
    Kravets](../Images/3b31f9b3c73c6c7ca709f845e6f70023.png)](https://medium.com/@alexml0123?source=post_page-----4eae474cb447--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4eae474cb447--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4eae474cb447--------------------------------)
    [Alexey Kravets](https://medium.com/@alexml0123?source=post_page-----4eae474cb447--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4eae474cb447--------------------------------)
    ·12 min read·Nov 3, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Unimodal models are designed to work with data from a single mode, which can
    be either text or images. These models specialize in understanding and generating
    content specific to their chosen mode. For example, GPT are excellent at generating
    human-like text. They have been used for tasks like language translation, text
    generation, and answering questions. Convolutional Neural Networks (CNNs) are
    examples of image models that excel at tasks like image classification, object
    detection, and image generation. Currently, many interesting tasks such as Visual
    Question Answering (VQA) and Image-Text retrieval etc. require multimodal capabilities.
    Is it possible to combine both text and image processing? We can! CLIP stands
    out as one of the initial highly successful image-text models, demonstrating proficiency
    in both image recognition and text comprehension.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will divide this article into the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training process and Contrastive loss
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Zero-shot capability
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: CuPL
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The CLIP model is an impressive zero-shot predictor, enabling predictions on
    tasks it hasn’t explicitly been trained for. As we will see more in detail in
    the next sections, by using natural language prompts to query images, CLIP can
    perform image classification without requiring task-specific training data. Nevertheless,
    its performance can be significantly enhanced with a few tricks. In this series
    of articles, we will explore methods that leverage additional prompts generated
    by Large Language Models (LLM) or a few-shot training examples without involving
    any parameter training. These approaches offer a distinct advantage as they are
    computationally less demanding and do not necessitate fine-tuning additional parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'CLIP is a dual encoder model with two separate encoders for visual and textual
    modalities that encode images and texts independently. Such architecture is different
    from the fusion encoder that enables the interaction between visual and textual
    modalities through cross-attention which involves learning attention weights that
    help the model focus on specific regions of an image and corresponding parts of
    the text when processing both modalities. The idea is similar to self-attention,
    which allows each token to attend to other tokens within the same modality. Cross-attention
    extends this concept by allowing tokens in one modality (e.g., tokens or patches
    representing image features) to attend to tokens in another modality (e.g., tokens
    representing textual descriptions). The idea of dual and fusion encoders can be
    summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/af4ed5e93601bd8b0932379f9726485c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by Author — Dual vs Fusion encoders
  prefs: []
  type: TYPE_NORMAL
- en: Encoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Text Encoder**: Responsible for processing input text, the text encoder transforms
    it into a vector representation. Within CLIP, the model uses a standard that we
    thoroughly explored in [this](https://medium.com/towards-data-science/deep-dive-into-the-code-of-bert-model-9f618472353e)
    article. The text encoder produces an embedding for the provided text, encapsulating
    semantic information associated with the input.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Image Encoder:** The image encoder processes images to derive their vector
    representations. The visual encoder can either be a Convolutional Neural Network
    like the ResNet model or a ViT Transformer (see [here](https://medium.com/towards-data-science/a-deep-dive-into-the-code-of-the-visual-transformer-vit-model-1ce4cc05ca8d)
    to refresh your knowledge) that produces the image vector representations.'
  prefs: []
  type: TYPE_NORMAL
- en: These two vectors share the same dimensions enabling the computation of similarity
    between a given text and image. If you have always worked with one modality you
    wonder how is it even possible that we can compare image and text embeddings?
    The key lies in the training process and the loss function which empower CLIP
    to learn a unified image-text space facilitating the comparison of vectors from
    different modalities.
  prefs: []
  type: TYPE_NORMAL
- en: Training process and Contrastive loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CLIP was trained via a large-scale, multimodal objective on an extensive dataset
    of image-text pairs. When I say large-scale it means a LOT of data — approximately
    0.4 billion image-text pairs. These were collected from publicly available sources
    on the internet and automatically filtered to ensure high quality.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once image-text pairs are collected the model is trained via a contrastive
    loss. Contrastive loss enables the model to learn a shared image-text space by
    aligning the representations of images and text maximizing the similarity between
    the embeddings of matching pairs and minimizing the similarity between non-matching
    pairs. The process is presented in the image below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f5f49fd0b58455c356cfbc620a36d475.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [CLIP](https://arxiv.org/pdf/2103.00020.pdf) paper — Contrastive
    loss
  prefs: []
  type: TYPE_NORMAL
- en: The image embedding I_i corresponds to the text embedding T_i (i.e., on the
    diagonal) forming the matching pair, while all other texts T_j (j ≠ i) (off-diagonal)
    are considered to be non-matching pairs. Similarly, for T_i only I_i is regarded
    as the matching image, and all other images I_j (j ≠ i) are not considered descriptions
    of T_i. However, this assumption might be limiting as there could be other text
    pairs that effectively describe an image and vice versa. Mining hard negative
    examples stands as a potential solution to this challenge. CLIP, despite this,
    manages to overcome this limitation attributed to its substantial batch size of
    32,768.
  prefs: []
  type: TYPE_NORMAL
- en: After pre-training on this diverse dataset, CLIP’s learned embeddings can be
    used for many downstream applications — one of them that is truly impressive is
    the zero-shot image classification.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-Shot capability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'What does zero-shot mean in the first place? As mentioned in the introduction,
    zero-shot classification refers to the ability of a model to correctly classify
    unseen classes without the need for any specific examples or training data for
    those particular classes. CLIP was trained on a large dataset and it has learned
    to generalize across a broad range of concepts that enables it to recognize and
    classify classes based on their semantic relationships. Let’s see how this is
    done in practice:'
  prefs: []
  type: TYPE_NORMAL
- en: Assume we only know the class names for a particular dataset like
  prefs: []
  type: TYPE_NORMAL
- en: '*[“dog”, “cat”, “horse”]*. As CLIP was trained to match image and text, we
    can compute the cosine similarity between a given test image and prompt *“Picture
    of a {class name}”* which becomes in our case:'
  prefs: []
  type: TYPE_NORMAL
- en: '*“Picture of a {dog}”, “Picture of a {cat}”, “Picture of a {horse}”*.'
  prefs: []
  type: TYPE_NORMAL
- en: The prompt with the highest cosine similarity represents the predicted class.
  prefs: []
  type: TYPE_NORMAL
- en: Improving Zero-Shot CLIP via Customized Prompts via Language Models (CuPL)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, Zero-Shot CLIP has achieved quite an impressive performance already, however,
    we can squeeze some more out of it with a few simple tricks. CLIP’s zero-shot
    performance is very sensitive to the textual prompts it’s fed with. This is why
    for different datasets such as ImageNet people have come up with different textual
    prompts such as *“an origami {class name}”*, *“a {class name} in a video game”*
    and similar. These manually designed prompts are better than a simple *“Picture
    of a {class name}”*, however, they still have some major limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: Hand-written prompts require a lot of human effort
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hand-written prompts must be general — we cannot use a template like *“a photo
    of a {platypus}, a type of aquatic mammal”* as it would apply only to aquatic
    mammals and not to other categories. This is limiting, as descriptive details
    are useful for fine-grained classification.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Writing high-performing prompt templates requires prior information about the
    contents of the dataset. So in the case of ImageNet, we must know in advance that
    the dataset of interest contains origami, video game images, and so on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'So what can we do? Let’s just ask a Large Language Model (LLM) to generate
    for us such prompts that can be easily scaled to any number of classes and datasets!
    We can ask a LLM the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Describe what a/the {class name} looks like:*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Describe a/the {class name}:*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*What are the identifying characteristics of a/the {class name}?*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why should this be better than simple prompts? The hypothesis is that prompts
    that LLM generates will contain very detailed descriptions of the given classes
    and enable CLIP to place more importance on image regions that are most relevant
    for correct classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now jump into coding and see what we can get. We are going to use CLIP
    from the Transformers library from [Hugging Face](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&cd=&ved=2ahUKEwiu6cr-kKGCAxXk_7sIHYoVCdMQFnoECB0QAQ&url=https%3A%2F%2Fhuggingface.co%2Fdocs%2Ftransformers&usg=AOvVaw1c0CbHgKUDJPUj-n5H3rT8&opi=89978449).
    So let’s import the model — we are going to use ViT with a patch of size 32 and
    the processing pipeline that tokenizes text and pre-processes images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we are going to download an image of a *“tree frog”,* aclass contained
    in the [ImageNet](https://www.image-net.org/) dataset,from the *freeimages* website
    that has an open license for the image below. Then, we are going to predict whether
    it’s a *“tree frog”* or a *“tailed frog”* (they are visually similar and mainly
    differ by the large eye size) using CLIP with a simple prompt *“A photo of a {class}”*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9037e72ae26b1b0d153353991c8fcfa3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Tree Frog image from [FreeImages](https://images.freeimages.com/images/large-previews/342/green-tree-frog2-1616738.jpg)
    (license: [https://www.freeimages.com/license](https://www.freeimages.com/license))'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The model makes an incorrect prediction by selecting *“tailed frog”* with a
    probability of 0.68.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now ask a LLM (e.g. ChatGPT) to generate the prompts for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: These prompts are more informative about the classes which should guide the
    model in identifying the correct class. For example, many prompts for “tree frog”
    emphasize that it has “large eyes”, something that simple prompts like “Picture
    of a tree frog” do not capture.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Using the above prompts and after some manipulation, we form the final prompt
    for a class with the average vector embedding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Using prompts from LLM gives us the correct classification answer — *“tree frog”*.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we have seen how easily we can improve CLIP zero-shot prediction
    using a LLM. The advantage of this solution is not only higher accuracy but also
    its scalability as we do not need any human effort to generate prompts. In the
    next articles, we are going to explore other methods to improve CLIP’s zero-shot
    learning and training-free few-shot learning methods.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] [[2103.00020] Learning Transferable Visual Models From Natural Language
    Supervision (arxiv.org)](https://arxiv.org/abs/2103.00020)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [[2209.03320] What does a platypus look like? Generating customized prompts
    for zero-shot image classification (arxiv.org)](https://arxiv.org/abs/2209.03320)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] [CLIP (huggingface.co)](https://huggingface.co/docs/transformers/model_doc/clip)'
  prefs: []
  type: TYPE_NORMAL
