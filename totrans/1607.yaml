- en: Optimizing Retrieval-Augmented Generation (RAG) by Selective Knowledge Graph
    Conditioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/optimizing-retrieval-augmented-generation-rag-by-selective-knowledge-graph-conditioning-97a4cf96eb69](https://towardsdatascience.com/optimizing-retrieval-augmented-generation-rag-by-selective-knowledge-graph-conditioning-97a4cf96eb69)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How SURGE substantially improves knowledge relevance through targeted augmentation
    while retaining language fluency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@alcarazanthony1?source=post_page-----97a4cf96eb69--------------------------------)[![Anthony
    Alcaraz](../Images/6a71a1752677bd07c384246fb0c7f7e8.png)](https://medium.com/@alcarazanthony1?source=post_page-----97a4cf96eb69--------------------------------)[](https://towardsdatascience.com/?source=post_page-----97a4cf96eb69--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----97a4cf96eb69--------------------------------)
    [Anthony Alcaraz](https://medium.com/@alcarazanthony1?source=post_page-----97a4cf96eb69--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----97a4cf96eb69--------------------------------)
    ·7 min read·Dec 28, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '*Artificial intelligence software was used to enhance the grammar, flow, and
    readability of this article’s text.*'
  prefs: []
  type: TYPE_NORMAL
- en: Generative pre-trained models have shown impressive fluency and coherence when
    used for dialogue agents. However, a key limitation they suffer from is the lack
    of grounding in external knowledge. Left to their pre-trained parameters alone,
    these models often generate plausible-sounding but factually incorrect responses,
    also known as hallucinations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prior approaches to mitigate this have involved augmenting the dialogue context
    with entire knowledge graphs associated with entities mentioned in the chat. However,
    this indiscriminate conditioning on large knowledge graphs brings its own problems:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Limitations of Naive Knowledge Graph Augmentation:'
  prefs: []
  type: TYPE_NORMAL
- en: Much of the 1-hop context may be irrelevant to the dialogue, inserting unnecessary
    noise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encoding entire knowledge subgraphs strains sequence length limits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No guarantee model will use the relevant facts for generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Risk of hallucination still exists despite knowledge grounding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To overcome this, Kang et al. 2023 propose the SUbgraph Retrieval-augmented
    GEneration (SURGE) framework, with three key innovations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://openreview.net/forum?id=WhWlYzUTJfP&source=post_page-----97a4cf96eb69--------------------------------)
    [## Knowledge-Consistent Dialogue Generation with Language Models and...'
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge-Consistent Dialogue Generation with Context-Relevant Subgraph Retrieval,
    Invariant Graph Encoding, and…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: openreview.net](https://openreview.net/forum?id=WhWlYzUTJfP&source=post_page-----97a4cf96eb69--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Context-Relevant Subgraph Retriever: Retrieving the most relevant knowledge
    graph facts to the dialogue context using a graph neural network retriever.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Efficient Graph Encoding: Perturbing token embeddings based on relations while
    encoding just subgraph entities instead of all triplets. Maintains permutation
    and inversion invariance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Graph-Text Contrastive Learning: Ensuring consistency between retrieved knowledge
    graph and generated response via contrastive loss.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This allows providing precisely the requisite factual context to the dialogue
    without dilution from irrelevant facts or model limitations. Experiments show
    SURGE reduces hallucination and improves grounding.
  prefs: []
  type: TYPE_NORMAL
- en: The key insight is that selective conditioning on personalized subgraphs provides
    focused knowledge grounding without overwhelming pre-trained models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/355f9ec12caf572abb74a894d9836a92.png)'
  prefs: []
  type: TYPE_IMG
- en: Generated by Dall-E-3
  prefs: []
  type: TYPE_NORMAL
- en: 'Plan :'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '- Context-Relevant Knowledge Retrieval;'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '- Invariant Knowledge Encoding;'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '- Enforcing Knowledge Consistency;'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '- Results;'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '- Conclusion.'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Context-Relevant Knowledge Retrieval:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Retrieval distribution modeled using similarity of context and triplet embeddings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Triplet embeddings obtained from Graph Neural Networks to capture relational
    structure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enables focusing on most relevant facts instead of all knowledge graph facts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The key challenge SURGE addresses is retrieving only the most relevant facts
    from the knowledge graph rather than overwhelm the generator with all contextually
    associated entities. To enable this context-specific selection, the paper proposes
    modeling the retrieval as a distribution over knowledge graph triplets conditioned
    on the dialogue history.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, this context-conditional retrieval distribution is defined
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: pφ(z|x) ∝ exp(d(z)^T s(x))
  prefs: []
  type: TYPE_NORMAL
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: x is the dialogue context
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: z is a knowledge graph triplet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: s(x) generates dense embeddings for the dialogue context
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: d(z) generates dense embeddings for the triplets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The key insight here is using the similarity between dialogue and triplet embeddings
    to model relevance.
  prefs: []
  type: TYPE_NORMAL
- en: Since the triplets contain both entities and relations structured as a graph,
    plain language model encoders are insufficient. Instead, Graph Neural Networks
    (GNNs) are uniquely positioned to capture both nodes and edges. GNNs can represent
    the relational dependencies between entities by propagating neighbouring embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, node embeddings are generated using Graph Convolutional Networks:'
  prefs: []
  type: TYPE_NORMAL
- en: e = GNN(e0; G)
  prefs: []
  type: TYPE_NORMAL
- en: 'While relation embeddings use Edge Hypergraph Networks:'
  prefs: []
  type: TYPE_NORMAL
- en: r = GNN(r0; G∗)
  prefs: []
  type: TYPE_NORMAL
- en: Where G* denotes the dual hypergraph.
  prefs: []
  type: TYPE_NORMAL
- en: By combining node and edge embeddings, full triplet embeddings can embed semantic
    relations and proximity. The similarity of these triplets with the dialogue context
    vectors from the encoder then provides the foundation for a context-relevant retrieval
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Invariant Knowledge Encoding:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Encodes retrieved subgraph into generator transformer efficiently
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensures encoding is invariant to order and direction of relations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uniquely encodes entities and perturbs embeddings based on relations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The context-relevant subgraph retrieved in the previous stage needs to be encoded
    into the generator transformer model that will produce the dialogue response.
    However, naively encoding the symbolic triplets runs into issues around stability
    of the representations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, there are two desired invariance properties:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Permutation invariance: Order of triplets should not change overall meaning'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Relation inversion invariance: Forward and backward relations equivalent'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When encoding knowledge graphs into pre-trained language models for dialogue,
    there are a couple practical problems that come up:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Long sequences: Encoding every single triplet fact as words results in extremely
    long input sequences. This strains the model’s context capacity.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Order dependence: Shuffling the order of triplets changes the meaning seen
    by models like GPT-3, since they rely so much on word order and positioning. But
    triplets are by nature unordered — shuffling facts shouldn’t change overall meaning.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Directional difference: Relations can be inverted without changing the core
    meaning (X is-wife-of Y == Y has-husband X). But prefixed text makes these seem
    like completely different facts.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The problems above cause unnecessary stress on the language models when encoding
    structured knowledge. The models get overwhelmed by huge numbers of tokens, and
    they struggle to grasp that jumbled or inverted triplets still convey the same
    concepts.
  prefs: []
  type: TYPE_NORMAL
- en: 'So ideally, we need a way to encode knowledge compactly yet stably. The encoding
    should be:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Efficient: Shouldn’t result in 1000s of prepended tokens blowing context space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Order-invariant: Shuffling subgraphs shouldn’t drastically alter meaning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Direction-invariant: Forward and backward relations should be treated equivalently.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SURGE solves this by uniquely encoding only entities, then judiciously perturbing
    their embeddings based on relations detected via graph neural networks. This provides
    a compact, stable form for assimilation by the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: 'A two-step embed and perturb approach is introduced:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Unique entity embedding:'
  prefs: []
  type: TYPE_NORMAL
- en: Extract set of unique entities ENT(Z) from triplets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embed these entities using dialogue encoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This embed+sort provides permutation invariance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Perturbation using relations:'
  prefs: []
  type: TYPE_NORMAL
- en: Use Graph Neural Network over triplets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GNN provides relation-aware node embeddings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Apply transformation β to entity embeddings:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: β(f(a), Z) = (1 + γ) ∗ f(a) + δ
  prefs: []
  type: TYPE_NORMAL
- en: where γ, δ are learned perturbation factors based on relations.
  prefs: []
  type: TYPE_NORMAL
- en: This step uses the relational information to directly influence the entity vector
    spaces while still keeping the efficient unique entity based encoding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: Vector space encoding fits generator requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Invariance provides stability and consistency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The insight is generating invariance through sets and perturbations rather than
    variable sequence encodings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Enforcing Knowledge Consistency:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Contrastive loss between knowledge graph and generated response
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pulls relevant knowledge representations closer to response representations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improves grounding of responses in retrieved facts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even after context-relevant retrieval and efficient encoding, there is no guarantee
    the generator will actually utilize the relevant knowledge provided to it. The
    risk of hallucination persists.
  prefs: []
  type: TYPE_NORMAL
- en: 'To actively incorporate the encoded subgraph, the authors propose adding a
    cross-modal contrastive loss between graph and response representations:'
  prefs: []
  type: TYPE_NORMAL
- en: Lcont = (1/2) * log (sim(ζ(z), ξ(h)) / ∑ξ(h’))
  prefs: []
  type: TYPE_NORMAL
- en: (1/2) * log (sim(ζ(z), ξ(h)) / ∑ζ(z’))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: z is the encoded knowledge subgraph
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: h is the decoder hidden state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ζ and ξ are projected embeddings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Intuitively, this loss pulls an encoded knowledge graph closer to its corresponding
    response representation, while pushing it away from other random responses or
    knowledge graphs.
  prefs: []
  type: TYPE_NORMAL
- en: This trains the model to actively distinguish between relevant knowledge-response
    pairs versus irrelevant ones. This discriminative pressure incentivizes the model
    to ground its responses in the encoded facts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: Improves factual consistency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduces unsupported assertions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allows tracing hallucinations to retrieval errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The key insight is that without an explicit alignment objective, the vector
    spaces of both modalities may drift apart, limiting fact grounding. The contrastive
    loss acts as an inductive bias towards consistency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Training end to end :'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Objective Function: The overall training objective is to maximize the log likelihood
    of generating the correct responses summed over the latent knowledge subgraphs:'
  prefs: []
  type: TYPE_NORMAL
- en: L = Σp(Z|x) p(y|x,Z)
  prefs: []
  type: TYPE_NORMAL
- en: Where p(Z|x) is the context-based retrieval distribution and p(y|x,Z) is the
    generator distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Training Process:'
  prefs: []
  type: TYPE_NORMAL
- en: Encode dialogue context x using encoder network
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Retrieve top-k subgraphs Z_i ~ p(Z|x) via similarity search
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Encode Z_i invariantly using GNN + perturbation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Maximize p(y|x,Z_i) for each sample via decoder
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Additionally minimize contrastive loss between Z_i and decoder states
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So jointly across batches of dialogue, retrieval distribution and generation
    distribution are optimized through shared parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Model Choice:'
  prefs: []
  type: TYPE_NORMAL
- en: In principle, any sequence-to-sequence language model like T5, BART or even
    GPT-3 can be used as the generator model by appending encoded knowledge to the
    input context tokens. The paper uses a T5 model in their experiments but this
    can be substituted.
  prefs: []
  type: TYPE_NORMAL
- en: 'Benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: Unified end-to-end training tying components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Marginal likelihood aggregates overall retina performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modular architecture allows model extensibility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Results:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Outperforms baselines in metrics measuring knowledge relevance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qualitative examples show more factual responses grounded in relevant knowledge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ablations validate importance of each component
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The authors evaluate SURGE on the OpendialKG and KOMODIS dialogue datasets which
    provide paired knowledge graphs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Quantitative improvements:'
  prefs: []
  type: TYPE_NORMAL
- en: SURGE outperforms all baselines in knowledge-relevance metrics like the proposed
    KQA (Knowledge-Verifying QA) metric which measures factual correctness through
    an extractor.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Achieves new state-of-the-art results on existing automatic metrics like BLEU,
    ROUGE and F1 which assess language fluency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qualitative impacts:'
  prefs: []
  type: TYPE_NORMAL
- en: Examples show SURGE generates more informative, factual responses grounded in
    relevant knowledge from selectively retrieved subgraphs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Baselines often omit key facts or even hallucinate irrelevant statements despite
    having access to the full context.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ablation studies:'
  prefs: []
  type: TYPE_NORMAL
- en: Removing components like contrastive learning significantly drops knowledge
    consistency metrics, showing the necessity of each module.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SURGE substantially improves knowledge relevance through targeted augmentation
    while retaining language fluency. The gains over both knowledge-unaware and knowledge-intensive
    baselines validate the benefits of selective subgraph retrieval and grounding.
  prefs: []
  type: TYPE_NORMAL
