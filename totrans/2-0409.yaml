- en: Breaking Down YouTube’s Recommendation Algorithm
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/breaking-down-youtubes-recommendation-algorithm-94aa3aa066c6](https://towardsdatascience.com/breaking-down-youtubes-recommendation-algorithm-94aa3aa066c6)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Opening the “bag of tricks” that makes a modern recommender system work
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@samuel.flender?source=post_page-----94aa3aa066c6--------------------------------)[![Samuel
    Flender](../Images/390d82a673de8a8bb11cef66978269b5.png)](https://medium.com/@samuel.flender?source=post_page-----94aa3aa066c6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----94aa3aa066c6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----94aa3aa066c6--------------------------------)
    [Samuel Flender](https://medium.com/@samuel.flender?source=post_page-----94aa3aa066c6--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----94aa3aa066c6--------------------------------)
    ·7 min read·Apr 17, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bc33a53302b6c34ec83db3c79b921ab2.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: (Logo design [Eyestetix Studio](https://unsplash.com/photos/LskCjwwJBEQ), background
    design by [Dan Cristian Pădureț](https://unsplash.com/photos/h3kuhYUCE9A))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '[Recommender systems](/learning-to-rank-a-primer-40d2ff9960af) have become
    one of the most ubiquitous industrial Machine Learning applications of our times,
    but little is being published about how they actually work in practice.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: A notable exception is Paul Covington’s paper “[Deep Neural Networks for YouTube
    Recommendations](https://research.google/pubs/pub45530/)”, which is packed with
    numerous practical insights and learnings about YouTube’s deep-learning powered
    recommendation algorithm, providing a rare window not just into the inner workings
    of a modern industrial recommender system but also into the problems that today’s
    ML engineers are trying to tackle.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: If you’re looking to deepen your understanding of modern recommender systems,
    preparing for ML design interviews, or simply curious about how YouTube gets people
    hooked, read on. In this post, we’ll break down 8 key insights from the paper
    that help explain YouTube’s (and any modern recommender system’s) success.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 1 — Recommendation = candidate generation + ranking
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'YouTube’s recommender system is broken down into 2 stages: the candidate generation
    stage, which filters the pool of Billions of videos down to a few hundred, and
    the ranking stage, which further narrows down and sorts the candidates that end
    up in front of the user.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'Technically, both stages contain a two-tower neural network — a special architecture
    with two arms for user ids and video ids, respectively — but their training objectives
    differ:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'for the candidate generation model, the learning problem is formulated as an
    extreme multi-class classification problem: out of all existing videos, predict
    the ones that the user engaged with.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于候选生成模型，学习问题被表述为一个极端多类分类问题：在所有现有视频中，预测用户互动的视频。
- en: 'for the ranking model, the learning problem is formulated as a (weighted) logistic
    regression problem: given a user/video pair, predict whether the user engaged
    with that video or not.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于排名模型，学习问题被表述为一个（加权的）逻辑回归问题：给定一个用户/视频对，预测用户是否与该视频互动。
- en: 'The motivation behind this design choice is to break down the problem of finding
    the optimal content into recall optimization and precision optimization: candidate
    generation optimizes for recall, i.e. making sure we’re capturing all relevant
    content, while ranking optimizes for precision, i.e. making sure we show the best
    content first. Breaking the problem down in this way is key to enable recommendation
    at the scale of Billions of users and videos.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这一设计选择的动机是将找到最佳内容的问题分解为召回优化和精度优化：候选生成优化召回，即确保我们捕捉到所有相关内容，而排名优化精度，即确保我们首先展示最佳内容。以这种方式分解问题是使推荐系统能够在数十亿用户和视频的规模上运行的关键。
- en: '![](../Images/ca19ae5ba69bc5b4581ec6818e5a220a.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ca19ae5ba69bc5b4581ec6818e5a220a.png)'
- en: YouTube’s 2-stage recommendation funnel. From Covington 2016, [Deep Neural Networks
    for YouTube Recommendations](https://research.google/pubs/pub45530/)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: YouTube的两阶段推荐漏斗。来自Covington 2016，[YouTube推荐的深度神经网络](https://research.google/pubs/pub45530/)
- en: 2 — Implicit labels work better than explicit labels
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 — 隐式标签比显式标签效果更好
- en: 'Explicit user feedback, such as Like, Share, or Comment, is extremely scarce:
    out of all users watching a particular video, only a small fraction will leave
    explicit feedback. A model trained only on Likes, for example, would therefore
    leave a lot of signal on the table.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 显式用户反馈，如点赞、分享或评论，非常稀少：在所有观看特定视频的用户中，只有少数人会留下显式反馈。因此，仅根据点赞训练的模型会留下大量信息未被利用。
- en: Implicit labels, such as user clicks and watch times, are a bit more noisy — users
    may click accidentally — but orders of magnitude more abundant. At YouTube’s scale,
    label quantity beats label quality, and therefore using implicit feedback as training
    objective works better in their models.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 隐式标签，如用户点击和观看时间，稍微有些噪声——用户可能会偶然点击——但数量级上要多得多。在YouTube的规模下，标签数量优于标签质量，因此在他们的模型中使用隐式反馈作为训练目标效果更好。
- en: 3 — Watch sequence matters
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 — 观看顺序很重要
- en: 'The sequence formed by a particular user’s watch history isn’t random, but
    contains particular patterns with asymmetric co-watch probabilities. For example,
    after watching 2 videos from the same creator, a user is likely to watch another
    video from that same creator. A model that simply learns to predict whether an
    impressed video is going to be watched, irrespective of the user’s recent watch
    history, doesn’t perform well: again, it’s leaving information on the table.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 特定用户的观看历史形成的序列不是随机的，而是包含具有不对称共同观看概率的特定模式。例如，在观看了两个来自同一创作者的视频之后，用户很可能会观看来自该创作者的另一个视频。一个简单地学习预测一个展示的视频是否会被观看的模型，忽略了用户最近的观看历史表现不好：同样，它也留下了信息未被利用。
- en: Instead, YouTube’s model learns to predict the next watch, given the user’s
    latest watch (and search) history. Technically, it does this by feeding the user’s
    latest 50 watched videos and 50 search queries, at the time of the training example,
    as features into the model.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，YouTube的模型通过给定用户最新的观看（和搜索）历史来学习预测下一个观看的视频。从技术上讲，它通过将用户最新的50个观看视频和50个搜索查询作为特征输入到模型中来实现这一点。
- en: 4 — The ranking model is trained using weighted logistic regression
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 — 排名模型使用加权逻辑回归进行训练
- en: Positive training examples (impressions with clicks) are weighted by their observed
    watch time, while negative training examples (impressions without clicks) receive
    unit weights. The purpose of this weighting scheme is to down-weigh click-bait
    content, and up-weigh content that leads to more meaningful and longer engagement.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 正面训练示例（点击的展示）根据其观察到的观看时间进行加权，而负面训练示例（没有点击的展示）则收到单位权重。这个加权方案的目的是减少点击诱饵内容的权重，增加导致更有意义和更长时间参与的内容的权重。
- en: 'Mathematically, the odds learned by such a weighted logistic regression model
    are approximately equal to expected watch time. At inference time, we can therefore
    convert the predicted odds into watch time simply be applying the exponential
    function. Being able to predict watch times enables this next critical insight:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，这样的加权逻辑回归模型学到的赔率大致等于预期观看时间。在推理时，我们可以通过应用指数函数将预测的赔率转换为观看时间。能够预测观看时间可以得出下一个关键洞察：
- en: 5 — Ranking by predicted watch time works better than ranking by click-through
    rate
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 — 通过预测观看时间排名优于通过点击率排名
- en: 'This is because ranking by click-through rate promotes click-bait content with
    low watch times: users click but go back right away. Ranking by predicted watch
    time down-ranks clickbait, leading to more engaging recommendations.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为点击率排名会促进低观看时间的点击诱饵内容：用户点击但很快返回。通过预测观看时间进行排名可以降低点击诱饵内容的排名，从而提供更具吸引力的推荐。
- en: 6 — A diverse feature set is key to high model performance
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 — 多样的特征集是高模型性能的关键
- en: 'The advantage of deep learning over linear or tree-based models is that it
    can handle a diverse set of input signals. YouTube’s model looks at:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习相对于线性或树模型的优势在于它可以处理多样的输入信号。YouTube的模型会考虑：
- en: 'watch history: which videos has the user watched recently?'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观看历史：用户最近观看了哪些视频？
- en: 'search history: which keywords has the user searched for recently?'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索历史：用户最近搜索了哪些关键词？
- en: demographic features, such as user gender, age, geographic location, and device,
    which provide priors for “cold-start” users, i.e. users with no history.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人口统计特征，例如用户性别、年龄、地理位置和设备，这些特征为“冷启动”用户，即没有历史记录的用户，提供了先验信息。
- en: 'Indeed, feature diversity is the key to achieve high model performance: the
    authors show that a model trained on all of these features improves holdout MAP
    from 6% to 13%, relative to a model trained only on watch history.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，特征多样性是实现高模型性能的关键：作者们展示了，使用所有这些特征训练的模型相较于仅使用观看历史训练的模型，持出MAP从6%提高到13%。
- en: '![](../Images/c6af4c3ed8fad76253cc0101452d62dd.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c6af4c3ed8fad76253cc0101452d62dd.png)'
- en: YouTube’s ranking neural network model takes in a diverse set of features. From
    Covington 2016, [Deep Neural Networks for YouTube Recommendations](https://research.google/pubs/pub45530/)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: YouTube的排名神经网络模型接受多样的特征集。从Covington 2016，[Deep Neural Networks for YouTube Recommendations](https://research.google/pubs/pub45530/)
- en: 7 — Content stays fresh thanks to “Example Age” feature
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 — 由于“示例年龄”特征，内容保持新鲜
- en: ML systems are often biased towards the past, simply because they’re trained
    on historic data. For YouTube, this is a problem because users usually prefer
    recently uploaded, “fresh” content over content that has been uploaded long ago.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ML系统往往对过去有偏见，因为它们是在历史数据上训练的。对于YouTube来说，这是一个问题，因为用户通常更喜欢最近上传的“新鲜”内容，而不是很久以前上传的内容。
- en: In order to fix this “past bias”, YouTube uses the age of the training example
    as a feature in the model, and sets it to 0 at inference time to reflect that
    the model is making predictions at the very end of the training window. For example,
    if the training data contains a window of 30 days of data, this “example age”
    feature would vary from 31 (for the first day in the training data) to 1 (for
    the last day).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了修复这种“过去偏见”，YouTube将训练示例的年龄作为模型中的一个特征，并在推理时将其设置为0，以反映模型在训练窗口的最后阶段进行预测。例如，如果训练数据包含30天的数据窗口，这一“示例年龄”特征会从31（训练数据中的第一天）变化到1（最后一天）。
- en: The authors show that introducing this feature makes the model much more biased
    in favor of fresh content, which is exactly what YouTube wants.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们展示了引入这一特性使得模型更倾向于推荐新内容，这正是YouTube所希望的。
- en: 8 — Sparse features are encoded as low-dimensional embeddings
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 — 稀疏特征被编码为低维嵌入
- en: YouTube’s ranking model uses a large number of high-cardinality (“sparse”) categorical
    features, such as
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: YouTube的排名模型使用了大量高维度（“稀疏”）的分类特征，例如
- en: video id and user id,
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视频ID和用户ID，
- en: tokenized search queries,
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分词搜索查询，
- en: the last 50 videos watched by a user, or
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户最近观看的最后50个视频，或
- en: the “seed” video that started the current watch session.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启动当前观看会话的“种子”视频。
- en: These sparse features are one-hot encoded and mapped to 32-dimensional embeddings
    that are learned during model training, and then stored as embedding tables for
    inference.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这些稀疏特征被独热编码，并映射到在模型训练过程中学习的32维嵌入中，然后作为嵌入表存储用于推理。
- en: In order to limit the memory footprint caused by embedding tables, id spaces
    are truncated to include only the most common ids. For example, if a video that
    has only been watched once in the training period, it’s not be worth having its
    own place in the embedding table, and will therefore be treated the same as a
    video that was never watched.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 'Another notable trick is that sparse features within the same id space share
    the same underlying embeddings. For example, there exists a single, global embedding
    of video ids that many distinct features use, such as the video id of the impression,
    the last video id watched by the user, or the video id that seeded the current
    session. Sharing embeddings in this way has 3 benefits:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: it saves memory, because there are fewer embedding tables that need to be stored,
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: it speeds up model training, since fewer parameters need to be learned, and
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: it improves generalization, because it enables the model to have more context
    about each id.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Coda: a bag of tricks'
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To summarize, there isn’t really one particular thing that makes YouTube’s
    recommender system special. It’s a “bag of tricks”, where each trick solves one
    particular problem:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: the 2-stage funnel design solves the scalability problem,
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: using weighted logistic regression and ranking by expected watch-time solve
    the clickbait problem,
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: adding the “example age” feature solves the past-bias problem,
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: adding demographic features solves the cold-start problem,
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: predicting “next watch” (instead of random watch) solves the asymmetric co-watch
    probability problem,
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: sharing embeddings between categorical features with the same ID solves the
    finite-memory problem,
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and so on.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'And this bag of tricks really gives a great insight not just into the inner
    workings of a modern recommender system, but also into the work of a modern ML
    engineer: we solve problems to make our models better. The best ML engineers are
    those that, over time, have accumulated the best bag of tricks.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, Covington’s paper is by now several years old, and certainly some of
    the tricks will have been replaced by newer and better tricks. There are [new
    tricks to encode sparse features](/hashing-in-modern-recommender-systems-a-primer-9c6b2cf4497a),
    and there are [new tricks to de-bias ranking models](https://medium.com/towards-data-science/biases-in-recommender-systems-top-challenges-and-recent-breakthroughs-edcda59d30bf).
    This is another aspect that’s intrinsic to industrial ML applications: our models
    keep evolving as new breakthroughs emerge.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: ML engineers are never “done”.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '*Want to expand your personal ML “bag of tricks”? Want to deepen your knowledge
    about what’s behind modern industrial ML applications? Check out my e-book,* [*Machine
    Learning on the Ground: Design and Operations of Real-World ML Applications*](https://samflender.gumroad.com/l/mlontheground)*.*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
