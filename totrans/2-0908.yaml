- en: Five Things GenAI Can and Canâ€™t Do
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/five-things-genai-can-and-cant-do-d8117aad82f4](https://towardsdatascience.com/five-things-genai-can-and-cant-do-d8117aad82f4)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An introductory guide for business leaders as to what Generative AI can or cannot
    do
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://dkhundley.medium.com/?source=post_page-----d8117aad82f4--------------------------------)[![David
    Hundley](../Images/1779ef96ec3d338f8fe4a9567ba7b194.png)](https://dkhundley.medium.com/?source=post_page-----d8117aad82f4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d8117aad82f4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d8117aad82f4--------------------------------)
    [David Hundley](https://dkhundley.medium.com/?source=post_page-----d8117aad82f4--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d8117aad82f4--------------------------------)
    Â·11 min readÂ·Oct 7, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eb063b6b18c957526b4addb9f73e4feb.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: Cover photo created by the author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Itâ€™s hard to believe that itâ€™s still not quite been a year since ChatGPTâ€™s launch,
    and we have seen Generative AI (GenAI) take the world by storm. From large language
    models (LLMs) to stable diffusion models for image generation, it is really quite
    remarkable what this new technology can do. A friend described it to me as the
    first time AI has felt tangible to them, as if what we only dreamed about through
    science fiction has now become reality.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Naturally, this has given business leaders pause to wonder what GenAI can or
    canâ€™t do to transform their business processes. There are certainly many cool
    things you can do with GenAI, but there are also some misconceptions floating
    around out there that business leaders should be cautious about. The focus of
    this post is to share with you some of the core things that GenAI can do while
    also tempering oneâ€™s expectations to caution on what it cannot do.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'Can #1: GenAI can summarize a lot of information.'
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Perhaps one of the most classic use cases Iâ€™m hearing across all industries
    is the ability to use large language models (LLM) in particular to condense a
    lot of information into something far more digestible. For example, you can take
    a transcribed dialogue from a meeting and use GenAI to summarize the information
    into a few key bullets. Additionally, you can take a large legal document and
    have an LLM pull out the most relevant bits of information. Of course, you should
    always be cautious to verify that the output of the LLM is correct, but this can
    save a ton of time in many different business contexts. I highly expect this to
    continue to gain traction in more and more industries over time.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'Canâ€™t #1: GenAI canâ€™t ever be certain about anything.'
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Perhaps one of the greatest misconceptions about LLMs is that they can think.
    In reality, LLMs are simply word prediction machines, albeit these models are
    so interestingly precise that it almost appears as if they are emulating true
    consciousness. Because LLMs act on probabilities between words, it can never be
    truly certain of its final output. Ironically however, it always will produce
    a very confident output. We refer to these confident but false statements as **hallucinations**.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following sentence: â€œI like to drink ______ in the morning.â€ If
    you were to answer this as a human, you might scratch your head. We can fairly
    ascertain that the blank is a liquid of some sort, but what liquid is it precisely?
    Coffee? Tea? Water? Just like a human, the LLM also canâ€™t be certain of the answer;
    however, unlike the human, the LLM wonâ€™t tell you, â€œI donâ€™t know.â€ Instead, it
    will confidently give an answer, except you and I know that the LLM canâ€™t possibly
    be certain of the answer. Letâ€™s actually see how ChatGPT attempts to fill in this
    blank.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b97a5c187eb6db0b62d7b5965afad3f7.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
- en: Screenshot of ChatGPT for iPad captured by the author
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, ChatGPT confidently fills in the blank as â€œcoffeeâ€, yet it gives
    no indication that maybe it filled it in incorrectly. Again, we refer to this
    overly confident â€œguessâ€ as a hallucination, but we can diminish these hallucinations
    with the next bulletâ€¦
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'Can #2: GenAI can give more informed answers with an augmented context.'
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous point, we noted that an LLM canâ€™t be particularly sure how
    to fill the blank in for the following sentence: â€œI like to drink ______ in the
    morning.â€ We can, however, augment the knowledge of the LLM by providing it additional
    context. In the GenAI community, we refer to this as **retrieval augmented generation
    (RAG)**. If we were to ask the LLM to fill in the blank above without any additional
    context, it will definitely give us an answer, but that answer may be hallucinated.
    Now, letâ€™s say we gave the LLM some additional context. Letâ€™s alter what we might
    input to the LLM with the following:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'My name is David Hundley. I like to drink a cold brew coffee from Starbucks
    each morning. Using this information, please fill in the blank in the following
    sentence: â€œI like to drink ______ in the morning.â€'
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Now that the LLM has been provided some very specific context, it can sufficiently
    fill in the blank. Testing this out in ChatGPT, youâ€™ll see that we receive the
    precisely correct answer.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6e1baeab36c1aa690118aea930ee7cf1.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
- en: Screenshot of ChatGPT for iPad captured by the author
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Of course, this still does not make the output of an LLM completely surefire
    in its response. Remember, the LLM is assessing probabilities between words. In
    the RAG process, we are simply augmenting the probability of producing an output
    with a more correct answer. This doesnâ€™t boost the probability of certainty to
    100%, but it definitely can help quite a bit!
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'Canâ€™t #2: GenAI canâ€™t figure out new things on itâ€™s own.'
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the RAG process can go a long way to help GenAI provide a more precise
    answer, the RAG process is not one that is automatic. In other words, GenAI technologies
    like LLMs donâ€™t have the ability to go learning things for itself. These GenAI
    models are trained on information provided to it at a snapshot in time, almost
    as if its knowledge is frozen to that fixed date. When we provide additional context
    to the LLM, the context can be helpful to produce a more precise output, but **this
    RAG context is NOT actually altering the underlying model itself**.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: The reason I bring this up is while the RAG process can be great, itâ€™s not like
    you can tell GenAI itself to go find that information for itself. Even in the
    cases like Bing Chat where it appears that the LLM is searching the internet,
    it is not actually the model that is crawling the internet. **Rather, the information
    is being brought to the LLM, and the LLM is making sense of what is brought to
    it.** Weâ€™ve not yet reached that Skynet-like level of superintelligence where
    these AI models can figure things out for themselves. ğŸ¤–
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 'Can #3: GenAI can be a great coding assistant.'
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pay close attention to the phrasing on this particular one. Iâ€™m very intentional
    about the word â€œcanâ€ with this point. Itâ€™s no secret that people have found ways
    to leverage LLMs for coding purposes, and to be perfectly clear, LLMs can be great
    for helping to write code. Whether it be autofilling common tasks or helping to
    debug errors, GenAI can be a very useful tool when it comes to coding.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: The thing isâ€¦ LLMs arenâ€™t perfect in this task. In addition to being limited
    by being trained on information at a snapshot in time, my personal experience
    is that the LLMs tend to hallucinate quite a bit when asked to write code for
    very nuanced situations. **Remember, LLMs are only as good as the context you
    provide it.** If you have an error that stems from something related to your very
    specific system, the LLM canâ€™t be aware of your systemâ€™s details and will thus
    ultimately hallucinate the answer. This isnâ€™t to say that GenAI canâ€™t be helpful
    for writing code, but I would view it more as an assistant than anything.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 'Canâ€™t #3: GenAI canâ€™t be certain if another piece of content was created by
    GenAI.'
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is one of those points where researchers have waffled back and forth. In
    fact, OpenAI at one point released a tool to help teachers determine if a piece
    of homework was created using ChatGPT. Eventually, OpenAI retracted this tool,
    and if you understand the underlying math and architecture of these GenAI solutions,
    I would contest that weâ€™re quickly getting to a point where this will literally
    be impossible.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Consider the most advanced LLMs on the market right now, including OpenAIâ€™s
    GPT-4 and Anthropicâ€™s Claude 2\. These LLMs can produce outputs that appear extremely
    human-like, and this is because the underlying architecture is able to assess
    the probabilities between words with a staggering level of precision. Carefully
    treading the line between actual and artificial consciousness, one canâ€™t help
    but wonder if human speech can also be predicted as probabilities between words.
    At that point, the end results of an LLM and a human are probabilistically indistinguishable,
    so no tool would be able to definitively say, â€œThis was made by GenAI; this was
    made by a human.â€
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '**All that to say, donâ€™t fall for a tool that says it can distinguish what
    is generated by AI versus what is not.** Perhaps with more primitive GenAI solutions,
    it can do some level of assessment there, but we have arguably already surpassed
    that point of distinguishability with LLMs like GPT-4.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 'Can #4: GenAI can be helped along by other software processes.'
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While most people are more familiar with interacting with LLMs through a user
    interface like ChatGPT, almost all the major GenAI players offer programmatic
    solutions through things like APIs. This means we can engineer GenAI technology
    into new and existing software processes. In fact, most GenAI startups are doing
    precisely this. Specifically, many GenAI startups are using APIs from providers
    like OpenAI or Anthropic as a backend â€œengineâ€ to support their GenAI needs. (Which
    is why you should also be careful who your company chooses to do business with,
    as they are probably interacting with a 4th party behind the scenes!)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Again, **it canâ€™t be overstated that GenAI should *always* be tempered for hallucinations**.
    While GenAI can produce snippets of code to do things like query specific information
    from a database, I personally would never rely on an LLM for a task like this.
    Because we canâ€™t have 100% certainty that the code would be correct, it would
    be imprudent to rely on GenAI this way. This isnâ€™t to say you should never incorporate
    GenAI into your software systems! There are still great use cases where you can
    leverage GenAI in a programmatic solution in a way that is still beneficial while
    also still being cautious.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'Canâ€™t #4: GenAI canâ€™t properly cite its own source information.'
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a question Iâ€™m asked pretty frequently, and if youâ€™ve been paying close
    attention this far, youâ€™ll understand why this isnâ€™t possible. Though LLMs are
    trained on information at a snapshot in time, itâ€™s not as if it is tying all the
    probabilities between words back to specific sources. What the LLM is only storing
    is referred to what is known as **weights and biases**. Without going too deep
    into the underlying architecture of LLMs, LLMs are essentially comprised of tons
    of mathematical operations, and at the time of training, these weights and biases
    are updated to more closely align to the information it is being trained on. (Actually,
    if I were to deeply explain the architecture of an LLM, youâ€™d be astounded that
    the math is rudimentary enough that it almost seems magical that LLMs can do as
    much as they can!)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘ç»å¸¸è¢«é—®åˆ°çš„é—®é¢˜ï¼Œå¦‚æœä½ åˆ°ç°åœ¨ä¸ºæ­¢éƒ½åœ¨è®¤çœŸå¬è®²ï¼Œä½ ä¼šæ˜ç™½ä¸ºä»€ä¹ˆè¿™æ˜¯ä¸å¯èƒ½çš„ã€‚å°½ç®¡LLMæ˜¯åœ¨æŸä¸€æ—¶é—´ç‚¹ä¸Šçš„ä¿¡æ¯ä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œä½†å¹¶ä¸æ˜¯è¯´å®ƒå°†æ‰€æœ‰è¯è¯­ä¹‹é—´çš„æ¦‚ç‡è”ç³»åˆ°å…·ä½“çš„æ¥æºã€‚LLMæ‰€å­˜å‚¨çš„ä»…ä»…æ˜¯è¢«ç§°ä¸º**æƒé‡å’Œåå·®**çš„ä¸œè¥¿ã€‚æ— éœ€æ·±å…¥LLMçš„åº•å±‚æ¶æ„ï¼ŒLLMæœ¬è´¨ä¸Šç”±å¤§é‡æ•°å­¦æ“ä½œç»„æˆï¼Œåœ¨è®­ç»ƒæ—¶ï¼Œè¿™äº›æƒé‡å’Œåå·®ä¼šæ›´æ–°ä»¥æ›´è´´è¿‘å®ƒæ‰€è®­ç»ƒçš„ä¿¡æ¯ã€‚ï¼ˆå®é™…ä¸Šï¼Œå¦‚æœæˆ‘è¯¦ç»†è§£é‡ŠLLMçš„æ¶æ„ï¼Œä½ ä¼šæƒŠè®¶äºæ•°å­¦çš„åŸºç¡€åˆ°å‡ ä¹ä»¤äººæ„Ÿåˆ°ç¥å¥‡çš„ç¨‹åº¦ï¼ŒLLMèƒ½å¤Ÿåšå¾—å¦‚æ­¤ä¹‹å¤šï¼ï¼‰
- en: Thatâ€™s all there is to an LLM. **Thereâ€™s no direct tie back to the source information,
    so if you were to ask it to cite its own source, you would receive a hallucination
    at best**. I actually believe specific LLMs like ChatGPT have been fine tuned
    to say that its not possible to reveal its sources, which I would say is a better
    response than a hallucinated one. But itâ€™s not impossible to receive a hallucinated
    response. Thereâ€™s [the infamous scenario](https://www.pymnts.com/artificial-intelligence-2/2023/attorneys-face-sanctions-after-citing-information-hallucinated-by-chatgpt/)
    where a lawyer used an LLM to cite a source in a legal case, and it was discovered
    that the cited source was hallucinated and ultimately fake. Be careful you donâ€™t
    also fall into this same trap!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯LLMçš„å…¨éƒ¨å†…å®¹ã€‚**æ²¡æœ‰ç›´æ¥è”ç³»åˆ°æºä¿¡æ¯ï¼Œå› æ­¤å¦‚æœä½ è¦æ±‚å®ƒå¼•ç”¨è‡ªå·±çš„æ¥æºï¼Œä½ æœ€å¤šåªèƒ½å¾—åˆ°è™šå‡çš„å›ç­”**ã€‚æˆ‘å®é™…ä¸Šç›¸ä¿¡åƒChatGPTè¿™æ ·çš„ç‰¹å®šLLMå·²ç»è¢«å¾®è°ƒï¼Œä»¥è¡¨ç¤ºä¸å¯èƒ½æ­ç¤ºå…¶æ¥æºï¼Œæˆ‘è®¤ä¸ºè¿™æ¯”è™šå‡çš„å›ç­”è¦å¥½ã€‚ä½†æ˜¯ï¼Œè·å¾—è™šå‡å›ç­”å¹¶éä¸å¯èƒ½ã€‚æœ‰[é‚£ä¸ªè‡­åæ˜­è‘—çš„æ¡ˆä¾‹](https://www.pymnts.com/artificial-intelligence-2/2023/attorneys-face-sanctions-after-citing-information-hallucinated-by-chatgpt/)ï¼Œä¸€åå¾‹å¸ˆä½¿ç”¨LLMåœ¨æ³•å¾‹æ¡ˆä»¶ä¸­å¼•ç”¨äº†ä¸€ä¸ªæ¥æºï¼Œç»“æœå‘ç°è¯¥æ¥æºæ˜¯è™šå‡çš„ï¼Œæœ€ç»ˆæ˜¯å‡çš„ã€‚å°å¿ƒä¸è¦é™·å…¥åŒæ ·çš„é™·é˜±ï¼
- en: 'Can #5: GenAI can be a fantastic way to augment your own knowledge.'
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¯ä»¥#5ï¼šGenAIå¯ä»¥æå¤§åœ°å¢å¼ºä½ çš„çŸ¥è¯†ã€‚
- en: Similar to the point where GenAI can be a good assistant at writing code, GenAI
    can also simply be a good assistant at any knowledge task. I personally use LLMs
    like ChatGPT as a sort of â€œsearch engineâ€ for general knowledge questions. For
    example, it is excellent at providing definitions to words, and it is great to
    receive more elaborative answers as you follow up with additional questions. Itâ€™s
    like interacting with a very knowledgeable friend. While internet search engines
    can provide the most up-to-date and relevant information, the back-and-forth of
    a conversation flow is unmatched by a barebones search engine.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸GenAIå¯ä»¥ä½œä¸ºç¼–å†™ä»£ç çš„è‰¯å¥½åŠ©æ‰‹çš„è§‚ç‚¹ç±»ä¼¼ï¼ŒGenAIä¹Ÿå¯ä»¥æˆä¸ºä»»ä½•çŸ¥è¯†ä»»åŠ¡çš„å¥½åŠ©æ‰‹ã€‚æˆ‘ä¸ªäººå°†LLMï¼ˆå¦‚ChatGPTï¼‰ç”¨ä½œä¸€ç§â€œæœç´¢å¼•æ“â€æ¥å›ç­”ä¸€èˆ¬çŸ¥è¯†é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œå®ƒåœ¨æä¾›è¯æ±‡å®šä¹‰æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œå¹¶ä¸”åœ¨ä½ ç»§ç»­æå‡ºé¢å¤–é—®é¢˜æ—¶ï¼Œå®ƒèƒ½ç»™å‡ºæ›´è¯¦å°½çš„å›ç­”ã€‚å°±åƒå’Œä¸€ä¸ªéå¸¸æœ‰çŸ¥è¯†çš„æœ‹å‹äº’åŠ¨ä¸€æ ·ã€‚è™½ç„¶äº’è”ç½‘æœç´¢å¼•æ“å¯ä»¥æä¾›æœ€æ–°å’Œæœ€ç›¸å…³çš„ä¿¡æ¯ï¼Œä½†å¯¹è¯çš„äº’åŠ¨æµç¨‹æ˜¯åŸºæœ¬æœç´¢å¼•æ“æ— æ³•æ¯”æ‹Ÿçš„ã€‚
- en: Of course, I shouldnâ€™t have to remind you by this point to be careful for hallucinations.
    While LLMs might be more helpful about historical events and locations like Ancient
    Rome, youâ€™ll certainly receive a hallucinated answer if you asked it what todayâ€™s
    weather is. If it does provide back a correct answer, this would again be because
    the LLM is being augmented in a RAG manner, NOT because the LLM was able to derive
    that information itself. Also, beware of using LLMs for math problems specifically.
    Remember, these LLMs are trained to look for probabilities between words, so thereâ€™s
    not any mathematical computation being done when a math problem is given to it.
    In recent months, popular LLMs like ChatGPT have been seemingly augmented to handle
    math problems better, but it is unclear how they are doing this. In any case,
    I would still try to avoid doing complex math problems with an LLM.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 'Canâ€™t #5: GenAI canâ€™t take your job. (â€¦Yet?)'
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Ah yes, the big question on everybodyâ€™s mind! Actually, I guess if Iâ€™m being
    totally honest, GenAI can be used to reduce a human workforce, but weâ€™re definitely
    not at the point where itâ€™s going to eliminate most peopleâ€™s jobs. If you paid
    close attention to the sections in this post around information retrieval, youâ€™ll
    recognize GenAIâ€™s biggest â€œflawâ€ preventing it from taking everybodyâ€™s jobs: **GenAI
    has no way to proactively learn for itself**. GenAI canâ€™t talk to your CEO and
    tease out whatâ€™s in their head. GenAI canâ€™t jump in your skull to understand all
    the little nuances of the website youâ€™ve dreamed in your imagination. In this
    way, I consider GenAI to be like a totally naive genius: you can get it to do
    a lot, but you have to be upfront and extremely specific with it to get the most
    ideal results.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'The natural follow up question is: will AI ever get to the point where it can
    do these things? We refer to this next level of superintelligence as **artificial
    general intelligence (AGI)**, and it is why AI safety advocates are raising concern
    with AI progress. The goal is to safely align human interests with AI interests.
    Without dwelling on this topic further, the point is we are not at that point
    today, and it really is unknown at point we will hit the singularity â€” the point
    at which we do reach AGI. If you understand the history of AI research, youâ€™ll
    quickly learn humanity isnâ€™t very good at predicting this point. ğŸ˜‚'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'I hope you walk away from this post with a much better idea of what GenAI may
    or may not be good for. It is a truly astounding technology, but itâ€™s not good
    for everything. One final point I should also highlight: **GenAI is not the only
    AI**. GenAI in its current form has been around as early as 2017, but weâ€™ve had
    many other forms of AI that can still suitably get the job done. Just as you wouldnâ€™t
    rent a car to walk ten feet, you should always attempt to rightsize the solution
    to meet the business need. GenAI is just another tool in our toolbox, albeit a
    very cool tool! ğŸ˜ƒ'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
