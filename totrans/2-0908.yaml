- en: Five Things GenAI Can and Can‚Äôt Do
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/five-things-genai-can-and-cant-do-d8117aad82f4](https://towardsdatascience.com/five-things-genai-can-and-cant-do-d8117aad82f4)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An introductory guide for business leaders as to what Generative AI can or cannot
    do
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://dkhundley.medium.com/?source=post_page-----d8117aad82f4--------------------------------)[![David
    Hundley](../Images/1779ef96ec3d338f8fe4a9567ba7b194.png)](https://dkhundley.medium.com/?source=post_page-----d8117aad82f4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d8117aad82f4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d8117aad82f4--------------------------------)
    [David Hundley](https://dkhundley.medium.com/?source=post_page-----d8117aad82f4--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d8117aad82f4--------------------------------)
    ¬∑11 min read¬∑Oct 7, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eb063b6b18c957526b4addb9f73e4feb.png)'
  prefs: []
  type: TYPE_IMG
- en: Cover photo created by the author
  prefs: []
  type: TYPE_NORMAL
- en: It‚Äôs hard to believe that it‚Äôs still not quite been a year since ChatGPT‚Äôs launch,
    and we have seen Generative AI (GenAI) take the world by storm. From large language
    models (LLMs) to stable diffusion models for image generation, it is really quite
    remarkable what this new technology can do. A friend described it to me as the
    first time AI has felt tangible to them, as if what we only dreamed about through
    science fiction has now become reality.
  prefs: []
  type: TYPE_NORMAL
- en: Naturally, this has given business leaders pause to wonder what GenAI can or
    can‚Äôt do to transform their business processes. There are certainly many cool
    things you can do with GenAI, but there are also some misconceptions floating
    around out there that business leaders should be cautious about. The focus of
    this post is to share with you some of the core things that GenAI can do while
    also tempering one‚Äôs expectations to caution on what it cannot do.
  prefs: []
  type: TYPE_NORMAL
- en: 'Can #1: GenAI can summarize a lot of information.'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Perhaps one of the most classic use cases I‚Äôm hearing across all industries
    is the ability to use large language models (LLM) in particular to condense a
    lot of information into something far more digestible. For example, you can take
    a transcribed dialogue from a meeting and use GenAI to summarize the information
    into a few key bullets. Additionally, you can take a large legal document and
    have an LLM pull out the most relevant bits of information. Of course, you should
    always be cautious to verify that the output of the LLM is correct, but this can
    save a ton of time in many different business contexts. I highly expect this to
    continue to gain traction in more and more industries over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Can‚Äôt #1: GenAI can‚Äôt ever be certain about anything.'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Perhaps one of the greatest misconceptions about LLMs is that they can think.
    In reality, LLMs are simply word prediction machines, albeit these models are
    so interestingly precise that it almost appears as if they are emulating true
    consciousness. Because LLMs act on probabilities between words, it can never be
    truly certain of its final output. Ironically however, it always will produce
    a very confident output. We refer to these confident but false statements as **hallucinations**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following sentence: ‚ÄúI like to drink ______ in the morning.‚Äù If
    you were to answer this as a human, you might scratch your head. We can fairly
    ascertain that the blank is a liquid of some sort, but what liquid is it precisely?
    Coffee? Tea? Water? Just like a human, the LLM also can‚Äôt be certain of the answer;
    however, unlike the human, the LLM won‚Äôt tell you, ‚ÄúI don‚Äôt know.‚Äù Instead, it
    will confidently give an answer, except you and I know that the LLM can‚Äôt possibly
    be certain of the answer. Let‚Äôs actually see how ChatGPT attempts to fill in this
    blank.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b97a5c187eb6db0b62d7b5965afad3f7.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot of ChatGPT for iPad captured by the author
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, ChatGPT confidently fills in the blank as ‚Äúcoffee‚Äù, yet it gives
    no indication that maybe it filled it in incorrectly. Again, we refer to this
    overly confident ‚Äúguess‚Äù as a hallucination, but we can diminish these hallucinations
    with the next bullet‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: 'Can #2: GenAI can give more informed answers with an augmented context.'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous point, we noted that an LLM can‚Äôt be particularly sure how
    to fill the blank in for the following sentence: ‚ÄúI like to drink ______ in the
    morning.‚Äù We can, however, augment the knowledge of the LLM by providing it additional
    context. In the GenAI community, we refer to this as **retrieval augmented generation
    (RAG)**. If we were to ask the LLM to fill in the blank above without any additional
    context, it will definitely give us an answer, but that answer may be hallucinated.
    Now, let‚Äôs say we gave the LLM some additional context. Let‚Äôs alter what we might
    input to the LLM with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'My name is David Hundley. I like to drink a cold brew coffee from Starbucks
    each morning. Using this information, please fill in the blank in the following
    sentence: ‚ÄúI like to drink ______ in the morning.‚Äù'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Now that the LLM has been provided some very specific context, it can sufficiently
    fill in the blank. Testing this out in ChatGPT, you‚Äôll see that we receive the
    precisely correct answer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6e1baeab36c1aa690118aea930ee7cf1.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot of ChatGPT for iPad captured by the author
  prefs: []
  type: TYPE_NORMAL
- en: Of course, this still does not make the output of an LLM completely surefire
    in its response. Remember, the LLM is assessing probabilities between words. In
    the RAG process, we are simply augmenting the probability of producing an output
    with a more correct answer. This doesn‚Äôt boost the probability of certainty to
    100%, but it definitely can help quite a bit!
  prefs: []
  type: TYPE_NORMAL
- en: 'Can‚Äôt #2: GenAI can‚Äôt figure out new things on it‚Äôs own.'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the RAG process can go a long way to help GenAI provide a more precise
    answer, the RAG process is not one that is automatic. In other words, GenAI technologies
    like LLMs don‚Äôt have the ability to go learning things for itself. These GenAI
    models are trained on information provided to it at a snapshot in time, almost
    as if its knowledge is frozen to that fixed date. When we provide additional context
    to the LLM, the context can be helpful to produce a more precise output, but **this
    RAG context is NOT actually altering the underlying model itself**.
  prefs: []
  type: TYPE_NORMAL
- en: The reason I bring this up is while the RAG process can be great, it‚Äôs not like
    you can tell GenAI itself to go find that information for itself. Even in the
    cases like Bing Chat where it appears that the LLM is searching the internet,
    it is not actually the model that is crawling the internet. **Rather, the information
    is being brought to the LLM, and the LLM is making sense of what is brought to
    it.** We‚Äôve not yet reached that Skynet-like level of superintelligence where
    these AI models can figure things out for themselves. ü§ñ
  prefs: []
  type: TYPE_NORMAL
- en: 'Can #3: GenAI can be a great coding assistant.'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pay close attention to the phrasing on this particular one. I‚Äôm very intentional
    about the word ‚Äúcan‚Äù with this point. It‚Äôs no secret that people have found ways
    to leverage LLMs for coding purposes, and to be perfectly clear, LLMs can be great
    for helping to write code. Whether it be autofilling common tasks or helping to
    debug errors, GenAI can be a very useful tool when it comes to coding.
  prefs: []
  type: TYPE_NORMAL
- en: The thing is‚Ä¶ LLMs aren‚Äôt perfect in this task. In addition to being limited
    by being trained on information at a snapshot in time, my personal experience
    is that the LLMs tend to hallucinate quite a bit when asked to write code for
    very nuanced situations. **Remember, LLMs are only as good as the context you
    provide it.** If you have an error that stems from something related to your very
    specific system, the LLM can‚Äôt be aware of your system‚Äôs details and will thus
    ultimately hallucinate the answer. This isn‚Äôt to say that GenAI can‚Äôt be helpful
    for writing code, but I would view it more as an assistant than anything.
  prefs: []
  type: TYPE_NORMAL
- en: 'Can‚Äôt #3: GenAI can‚Äôt be certain if another piece of content was created by
    GenAI.'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is one of those points where researchers have waffled back and forth. In
    fact, OpenAI at one point released a tool to help teachers determine if a piece
    of homework was created using ChatGPT. Eventually, OpenAI retracted this tool,
    and if you understand the underlying math and architecture of these GenAI solutions,
    I would contest that we‚Äôre quickly getting to a point where this will literally
    be impossible.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the most advanced LLMs on the market right now, including OpenAI‚Äôs
    GPT-4 and Anthropic‚Äôs Claude 2\. These LLMs can produce outputs that appear extremely
    human-like, and this is because the underlying architecture is able to assess
    the probabilities between words with a staggering level of precision. Carefully
    treading the line between actual and artificial consciousness, one can‚Äôt help
    but wonder if human speech can also be predicted as probabilities between words.
    At that point, the end results of an LLM and a human are probabilistically indistinguishable,
    so no tool would be able to definitively say, ‚ÄúThis was made by GenAI; this was
    made by a human.‚Äù
  prefs: []
  type: TYPE_NORMAL
- en: '**All that to say, don‚Äôt fall for a tool that says it can distinguish what
    is generated by AI versus what is not.** Perhaps with more primitive GenAI solutions,
    it can do some level of assessment there, but we have arguably already surpassed
    that point of distinguishability with LLMs like GPT-4.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Can #4: GenAI can be helped along by other software processes.'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While most people are more familiar with interacting with LLMs through a user
    interface like ChatGPT, almost all the major GenAI players offer programmatic
    solutions through things like APIs. This means we can engineer GenAI technology
    into new and existing software processes. In fact, most GenAI startups are doing
    precisely this. Specifically, many GenAI startups are using APIs from providers
    like OpenAI or Anthropic as a backend ‚Äúengine‚Äù to support their GenAI needs. (Which
    is why you should also be careful who your company chooses to do business with,
    as they are probably interacting with a 4th party behind the scenes!)
  prefs: []
  type: TYPE_NORMAL
- en: Again, **it can‚Äôt be overstated that GenAI should *always* be tempered for hallucinations**.
    While GenAI can produce snippets of code to do things like query specific information
    from a database, I personally would never rely on an LLM for a task like this.
    Because we can‚Äôt have 100% certainty that the code would be correct, it would
    be imprudent to rely on GenAI this way. This isn‚Äôt to say you should never incorporate
    GenAI into your software systems! There are still great use cases where you can
    leverage GenAI in a programmatic solution in a way that is still beneficial while
    also still being cautious.
  prefs: []
  type: TYPE_NORMAL
- en: 'Can‚Äôt #4: GenAI can‚Äôt properly cite its own source information.'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a question I‚Äôm asked pretty frequently, and if you‚Äôve been paying close
    attention this far, you‚Äôll understand why this isn‚Äôt possible. Though LLMs are
    trained on information at a snapshot in time, it‚Äôs not as if it is tying all the
    probabilities between words back to specific sources. What the LLM is only storing
    is referred to what is known as **weights and biases**. Without going too deep
    into the underlying architecture of LLMs, LLMs are essentially comprised of tons
    of mathematical operations, and at the time of training, these weights and biases
    are updated to more closely align to the information it is being trained on. (Actually,
    if I were to deeply explain the architecture of an LLM, you‚Äôd be astounded that
    the math is rudimentary enough that it almost seems magical that LLMs can do as
    much as they can!)
  prefs: []
  type: TYPE_NORMAL
- en: That‚Äôs all there is to an LLM. **There‚Äôs no direct tie back to the source information,
    so if you were to ask it to cite its own source, you would receive a hallucination
    at best**. I actually believe specific LLMs like ChatGPT have been fine tuned
    to say that its not possible to reveal its sources, which I would say is a better
    response than a hallucinated one. But it‚Äôs not impossible to receive a hallucinated
    response. There‚Äôs [the infamous scenario](https://www.pymnts.com/artificial-intelligence-2/2023/attorneys-face-sanctions-after-citing-information-hallucinated-by-chatgpt/)
    where a lawyer used an LLM to cite a source in a legal case, and it was discovered
    that the cited source was hallucinated and ultimately fake. Be careful you don‚Äôt
    also fall into this same trap!
  prefs: []
  type: TYPE_NORMAL
- en: 'Can #5: GenAI can be a fantastic way to augment your own knowledge.'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to the point where GenAI can be a good assistant at writing code, GenAI
    can also simply be a good assistant at any knowledge task. I personally use LLMs
    like ChatGPT as a sort of ‚Äúsearch engine‚Äù for general knowledge questions. For
    example, it is excellent at providing definitions to words, and it is great to
    receive more elaborative answers as you follow up with additional questions. It‚Äôs
    like interacting with a very knowledgeable friend. While internet search engines
    can provide the most up-to-date and relevant information, the back-and-forth of
    a conversation flow is unmatched by a barebones search engine.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, I shouldn‚Äôt have to remind you by this point to be careful for hallucinations.
    While LLMs might be more helpful about historical events and locations like Ancient
    Rome, you‚Äôll certainly receive a hallucinated answer if you asked it what today‚Äôs
    weather is. If it does provide back a correct answer, this would again be because
    the LLM is being augmented in a RAG manner, NOT because the LLM was able to derive
    that information itself. Also, beware of using LLMs for math problems specifically.
    Remember, these LLMs are trained to look for probabilities between words, so there‚Äôs
    not any mathematical computation being done when a math problem is given to it.
    In recent months, popular LLMs like ChatGPT have been seemingly augmented to handle
    math problems better, but it is unclear how they are doing this. In any case,
    I would still try to avoid doing complex math problems with an LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'Can‚Äôt #5: GenAI can‚Äôt take your job. (‚Ä¶Yet?)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Ah yes, the big question on everybody‚Äôs mind! Actually, I guess if I‚Äôm being
    totally honest, GenAI can be used to reduce a human workforce, but we‚Äôre definitely
    not at the point where it‚Äôs going to eliminate most people‚Äôs jobs. If you paid
    close attention to the sections in this post around information retrieval, you‚Äôll
    recognize GenAI‚Äôs biggest ‚Äúflaw‚Äù preventing it from taking everybody‚Äôs jobs: **GenAI
    has no way to proactively learn for itself**. GenAI can‚Äôt talk to your CEO and
    tease out what‚Äôs in their head. GenAI can‚Äôt jump in your skull to understand all
    the little nuances of the website you‚Äôve dreamed in your imagination. In this
    way, I consider GenAI to be like a totally naive genius: you can get it to do
    a lot, but you have to be upfront and extremely specific with it to get the most
    ideal results.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The natural follow up question is: will AI ever get to the point where it can
    do these things? We refer to this next level of superintelligence as **artificial
    general intelligence (AGI)**, and it is why AI safety advocates are raising concern
    with AI progress. The goal is to safely align human interests with AI interests.
    Without dwelling on this topic further, the point is we are not at that point
    today, and it really is unknown at point we will hit the singularity ‚Äî the point
    at which we do reach AGI. If you understand the history of AI research, you‚Äôll
    quickly learn humanity isn‚Äôt very good at predicting this point. üòÇ'
  prefs: []
  type: TYPE_NORMAL
- en: 'I hope you walk away from this post with a much better idea of what GenAI may
    or may not be good for. It is a truly astounding technology, but it‚Äôs not good
    for everything. One final point I should also highlight: **GenAI is not the only
    AI**. GenAI in its current form has been around as early as 2017, but we‚Äôve had
    many other forms of AI that can still suitably get the job done. Just as you wouldn‚Äôt
    rent a car to walk ten feet, you should always attempt to rightsize the solution
    to meet the business need. GenAI is just another tool in our toolbox, albeit a
    very cool tool! üòÉ'
  prefs: []
  type: TYPE_NORMAL
