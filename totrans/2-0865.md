# 扩展大规模语言模型中的上下文长度

> 原文：[https://towardsdatascience.com/extending-context-length-in-large-language-models-74e59201b51f](https://towardsdatascience.com/extending-context-length-in-large-language-models-74e59201b51f)

## 如何将你的Llama变成长颈鹿

[](https://donatoriccio.medium.com/?source=post_page-----74e59201b51f--------------------------------)[![Donato Riccio](../Images/0af2a026e72a023db4635522cbca50eb.png)](https://donatoriccio.medium.com/?source=post_page-----74e59201b51f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----74e59201b51f--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----74e59201b51f--------------------------------) [Donato Riccio](https://donatoriccio.medium.com/?source=post_page-----74e59201b51f--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----74e59201b51f--------------------------------) ·9分钟阅读·2023年10月15日

--

![](../Images/8bb0da11ed7b43b5f0cc752db52b0aad.png)

图片由作者提供。（AI生成的Llamas）

上下文长度是指模型在生成文本时可以记住的最大标记数。更长的上下文窗口使模型能够更好地理解文本中的长距离依赖关系。具有更长上下文的模型可以在文本中构建相隔较远的思想之间的联系，生成更具全球一致性的输出。

在训练过程中，模型以块或固定长度窗口处理文本数据。模型需要在较长文本上进行训练，以真正利用长上下文。训练序列必须包含文档、书籍、文章等，拥有数千个标记。

训练数据的长度限制了可用的上下文长度。

> 那么，为什么我们不在更长的序列上训练模型呢？

不要急于求成。

增加上下文长度会增加模型必须学习准确预测的可能标记组合的数量。

这使得更强大的长距离建模成为可能，但也需要更多的内存和处理能力，从而导致更高的训练成本。

没有任何优化的情况下，计算量与上下文长度的平方成正比——这意味着一个4096标记的模型需要比512标记模型多64倍的计算。

你可以使用稀疏或近似注意力方法来降低计算成本，但它们也可能影响模型的准确性。

训练和使用大上下文语言模型面临三个主要挑战：

+   将长上下文适配到模型中。

+   加速推理和训练，以便它们不会拖延太久。

+   确保高质量推理，保持对全部上下文的意识。

# 注意力是一个复杂的操作

注意力机制是变换器模型的核心组件。它将序列的不同位置联系起来以计算其表示，使模型能够关注文本的相关部分并更好地理解它。将变换器扩展到更长的序列面临挑战，因为全注意力的复杂度是二次的。

![](../Images/5a3218089dd630220ceb86bf9b71802d.png)

自注意力涉及两个矩阵乘法。图像来源于原始论文。[1]

堆叠自注意力层允许在文本中建模长距离依赖关系。Transformers中使用的标准注意力机制，计算所有可能的输入标记配对的注意力权重，其复杂度为**O(n²)**。这意味着计算和内存需求随着输入序列长度的平方增长，限制了Transformers的可扩展性和效率。

在生成文本时，模型必须首先计算注意力矩阵。对于100K上下文和二次注意力，模型可能需要几分钟才能开始生成文本。

让我们深入探讨提高注意力效率的方法，从近似到硬件优化。

# 提高注意力效率

减少二次成本已成为一个活跃的研究领域。提出的方法可以分为两大类：**近似注意力**和**通过硬件优化实现的精确注意力**。

近似技术约束序列位置之间的交互。**稀疏注意力**限制每个注意力头的非零注意力权重数量，而**本地注意力**将交互限制在滑动窗口内。这些近似减少了计算成本，但可能会降低复杂任务的准确性。[2]

最近的工作集中于优化注意力以利用GPU架构。

**稀疏注意力**通过仅计算输入标记子集的注意力权重来近似注意力，从而节省时间和内存，而不是计算所有可能的配对。有不同的稀疏注意力实现方式，如使用固定或静态模式（例如，本地、步幅或块注意力）或依赖于输入序列的动态或自适应模式（例如，entmax或动态稀疏注意力）。

![](../Images/db8e90a3376384683f9f21dd41c2273e.png)

二次注意力（左）计算输入标记之间的每一个可能组合。稀疏注意力（右）将计算限制在附近的标记上。[2]

稀疏注意力可以提高Transformers的效率和可扩展性，尤其是在长序列中，但它也可能牺牲一些表示能力和准确性。二次注意力可以实现高性能和高质量，但它可能计算开销大，不适用于大规模应用。因此，注意力机制中的稀疏性和复杂性之间存在权衡。

# Flash Attention

基本直觉是避免将大规模的N x N注意力矩阵物化，这需要在序列长度N上进行平方级的读写操作。

**FlashAttention**应用了两种技术——分块和重新计算。分块将输入拆分成块，加载到快速GPU片上SRAM中。注意力机制按块计算，以避免物化整个矩阵。重新计算在反向传播过程中仅存储足够的信息来重建片上注意力矩阵，避免存储大型中间结果。[3]

作者分析了IO复杂性，证明FlashAttention需要O(N²/M)的内存访问，而标准注意力机制需要O(N²)，其中M是SRAM大小。这种IO意识使得FlashAttention在重新计算带来的FLOP增加的情况下运行得更快。

实验验证了加速效果——FlashAttention训练BERT比MLPerf记录快15%，GPT-2快3倍，Long Range Arena快2.4倍。

![](../Images/91d89ae3fb6519710fb048cc8cf01905.png)

FlashAttention不是在较慢的HBM上计算整个注意力矩阵，而是将块复制到SRAM中。[3]

这个想法在**FlashAttention-2**中得到了进一步的发展。改进的重点是增强序列块之间的并行性，并优化线程块和GPU上的warp之间的工作分配。关键技术包括减少非矩阵乘法操作，将注意力计算分配到线程中以增加占用率，以及在warps之间分配工作以减少共享内存流量。实验证明，FlashAttention-2在FlashAttention上实现了大约2倍的加速，在A100 GPU上达到了理论峰值FLOP的73%。用于端到端训练GPT模型时，训练吞吐量达到了每个A100 225 TFLOPs/s，比FlashAttention快1.3倍。[4]

这些改进有望在相似成本下实现对比以往更长序列的模型训练。加速注意力机制可以加快推理和训练速度，但在保持高输出质量的同时将文本适配到模型中仍然是一个问题。

让我们看看如何解决这个问题。

# 模型在固定长度的序列上进行预训练

高效的训练和推理还不足以获得高质量的模型。

上下文长度扩展主要有两种范式：**微调外推**，其中LLM在更长的上下文上进一步更新其权重，以及**零-shot外推**，其中模型在长上下文上进行评估，而短上下文训练的权重不发生变化。[5]

为了扩展上下文，大多数方法专注于修改变换器注意力机制中使用的位置编码系统，以指示标记在输入序列中的位置。其想法是，在位置编码中表示更长的输入序列将允许LLM关注这些更长的序列。

![](../Images/7272a89e1df22f57dac6474602c4425d.png)

位置编码用于使你的模型理解句子中的顺序。基于原始论文的图像。[1]

位置编码用于使你的模型理解句子中的顺序。

**位置嵌入**在将输入标记嵌入送入模型之前被添加，以便模型能够使用序列的顺序。它们将离散的位置 ID 映射到连续的嵌入向量。

通常，位置嵌入是基于位置 ID 算法定义的。在原始 Transformers 论文中，他们使用了三角函数，其中位置嵌入向量的每个维度都遵循正弦波模式。[1]

在 LLaMa 中，使用了**旋转位置嵌入（RoPE）**，其中位置嵌入通过旋转嵌入动态计算。标记和位置维度通过三角函数一起旋转。旋转量由位置 ID 决定。

无论位置嵌入如何生成，模型在处理比预训练时更长的序列时都很难进行泛化。*(上下文外推)* 正弦位置嵌入方法的外推能力有限，在推断期间只能再处理几十个标记，然后性能会下降。[6]

新的方法如线性缩放和位置插值已经被引入以解决这一限制。

## 线性缩放

使用**线性缩放**，位置嵌入被重新缩放以适应不同的序列长度。如果预训练模型的嵌入长度为 L，则在对长度为 N 的序列进行推断时，每个位置嵌入向量会乘以 N/L。这廉价地近似了新长度的嵌入，同时保留了预训练嵌入的特性。线性缩放显著提升了长序列的性能。然而，模型在比预训练长度长得多的序列上仍然表现不佳。线性缩放过程通过将多个位置嵌入合并在一起来破坏信息。

![](../Images/e82ac836770d64f13d2925c96376ca8b.png)

在位置插值中，上下文范围不会扩展。相反，有更多的中间位置。[7]

**线性缩放/插值**在扩展上下文时整体表现最佳，同时在截断基方法中也显示出了一定的前景。通过在评估时使用更长的缩放因子，可以获得进一步的提升。

这种方法由 kaiokendev 和 Meta 同时研究。[8]

首个发布的 YaRN 是一个能够实现 128k 上下文长度的模型。

后来，发布了三个额外微调的模型，分别具有 8k、16k 和 32k 的上下文，称为 Giraffe。[5]

## 带有线性偏差的注意力

**ALiBi** 引入了一种更简单的方法，消除了位置嵌入。相反，它通过一个与查询和键之间距离成比例的惩罚来负向偏置注意力得分。这种对近期上下文的归纳偏置允许以低计算成本进行推断。实验表明，使用 ALiBi 训练的 1.3B 参数模型在 2048 个标记上测试时，与在 2048 个标记上训练的正弦模型具有相同的困惑度，训练速度更快，内存使用更少。[6]

MosaicML 的 **MPT-7B** 模型利用 **ALiBi** 架构，实现了对极长上下文长度（高达 65k 个标记）的外推，远远超过了其他开源模型的限制。通过用 ALiBi 替代位置嵌入，该模型在推理过程中能够处理任意长度的输入，而不受限于固定的上下文窗口。这通过将 MPT-7B 微调为 MPT-7B-StoryWriter-65k+ 使用 65k 个标记的小说摘录来证明，使其能够在 68k 个标记的《了不起的盖茨比》完整文本中生成连贯的续篇。

位置嵌入方法的选择取决于模型大小、预期序列长度以及问题对泛化的重要性等因素。

仍然有很大的改进空间，因为所有方法在长度增加时准确度都会下降，即使困惑度合理。

# 奖励模型：RWKV

改进注意力的另一种方法是*不使用它*。

**Receptance Weighted Key Value (RWKV)** 模型由 Peng 等人提出，旨在调和序列处理任务中的计算效率和模型性能之间的权衡。RWKV 将 Transformers 和 RNN 的某些方面结合成一种新颖的架构，实现了线性扩展。[9]

一个关键的创新是将注意力机制重新表述为使用标量交互而不是点积，从而消除二次瓶颈。

![](../Images/b3e32afb0c0bb84770d2a397b5dd8267.png)

用于语言建模的 RWKV 架构。[9]

**RWKV** 实现了一种线性注意力的变体，没有近似，以提高效率。该模型在训练中像 Transformers 一样并行计算，但在推理时表现为 RNN 解码器，实现了速度和内存的恒定，能够处理无限上下文。

实验表明 RWKV 在语言任务中与 Transformers 具有竞争力，同时需要更低的计算成本。

然而，鉴于其递归特性，可能需要仔细调整提示，并且它可能在处理极长序列时难以保持详细跟踪。

[**Raven**](https://huggingface.co/RWKV/rwkv-raven-14b) 是一个 RWKV 模型的示例。这个模型与最小的基于 Llama 的模型竞争。在我的测试中，Raven 显示出对语法和语义意义的良好理解，但倾向于经常产生幻觉。

# **结论**

语言模型从更长的上下文中受益。然而，由于标准注意力机制的原因，更长的上下文会使训练成本呈二次方增加。最近的研究集中在通过近似注意力来提高效率。稀疏注意力和线性注意力等方法有所帮助。优化硬件效率也是一种有效的手段，正如 FlashAttention 通过利用 GPU 内存层次结构所展示的那样。

预训练模型在处理比训练期间看到的更长的上下文时仍然会遇到困难。像位置嵌入的线性缩放和 ALiBi 等技术能够支持更长的上下文。对更长上下文的微调进一步适应了模型。

目前最先进的模型将上下文长度大大超越了之前的限制。

YaRN 和 Giraffe 使用位置插值。MPT-65k 使用 ALiBi 处理 65,000 个标记的上下文。RWKV 提出了线性缩放注意力机制，允许在推理时使用无限长度的上下文。

更长的上下文使得模型能够处理完整的文档和书籍，但在维持输出质量方面仍面临挑战。

*如果你喜欢这篇文章，加入* [***文本生成***](https://textgeneration.substack.com/) *—— 我们的新闻通讯每周有两篇最新的生成 AI 和大型语言模型的见解。*

*此外，你还可以在* [***LinkedIn***](https://www.linkedin.com/in/driccio/)***上找到我。***

# 参考文献

[1] [[1706.03762] 注意力机制即一切 (arxiv.org)](https://arxiv.org/abs/1706.03762)

[2] [[1904.10509] 使用稀疏变换器生成长序列 (arxiv.org)](https://arxiv.org/abs/1904.10509)

[3] [[2205.14135] FlashAttention: 快速且内存高效的精确注意力机制与 IO 关注 (arxiv.org)](https://arxiv.org/abs/2205.14135)

[4] [[2307.08691] FlashAttention-2: 更快的注意力机制，具备更好的并行性和工作划分 (arxiv.org)](https://arxiv.org/abs/2307.08691)

[5] [[2308.10882] Giraffe: 扩展 LLM 上下文长度的冒险 (arxiv.org)](https://arxiv.org/abs/2308.10882)

[6] [[2108.12409] 训练短序列，测试长序列：具有线性偏置的注意力机制实现输入长度外推 (arxiv.org)](https://arxiv.org/abs/2108.12409)

[7] [[2306.15595] 通过位置插值扩展大型语言模型的上下文窗口 (arxiv.org)](https://arxiv.org/abs/2306.15595)

[8] [[2309.00071] YaRN: 大型语言模型的高效上下文窗口扩展 (arxiv.org)](https://arxiv.org/abs/2309.00071)

[9] [[2305.13048] RWKV: 为 Transformer 时代重新定义 RNN (arxiv.org)](https://arxiv.org/abs/2305.13048)
