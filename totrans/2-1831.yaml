- en: 'Sentence Transformers: Meanings in Disguise'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/sentence-transformers-meanings-in-disguise-323cf6ac1e52](https://towardsdatascience.com/sentence-transformers-meanings-in-disguise-323cf6ac1e52)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[NLP For Semantic Search](https://jamescalam.medium.com/list/nlp-for-semantic-search-d3a4b96a52fe)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How modern language models capture meaning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://jamescalam.medium.com/?source=post_page-----323cf6ac1e52--------------------------------)[![James
    Briggs](../Images/cb34b7011748e4d8607b7ff4a8510a93.png)](https://jamescalam.medium.com/?source=post_page-----323cf6ac1e52--------------------------------)[](https://towardsdatascience.com/?source=post_page-----323cf6ac1e52--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----323cf6ac1e52--------------------------------)
    [James Briggs](https://jamescalam.medium.com/?source=post_page-----323cf6ac1e52--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----323cf6ac1e52--------------------------------)
    ·12 min read·Jan 3, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/edabd3baac27a7fd707b7855ea93c5c2.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Brian Suh](https://unsplash.com/@_briansuh?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral). Originally
    posted in the [NLP for Semantic Search ebook](https://www.pinecone.io/learn/sentence-embeddings/)
    at Pinecone (where the author is employed).
  prefs: []
  type: TYPE_NORMAL
- en: Transformers have wholly rebuilt the landscape of natural language processing
    (NLP). Before transformers, we had *okay* translation and language classification
    thanks to recurrent neural nets (RNNs) — their language comprehension was limited
    and led to many minor mistakes, and coherence over larger chunks of text was practically
    impossible.
  prefs: []
  type: TYPE_NORMAL
- en: Since the introduction of the first transformer model in the 2017 paper *‘Attention
    is all you need’* [1], NLP has moved from RNNs to models like BERT and GPT. These
    new models can answer questions, write articles *(maybe GPT-3 wrote this)*, enable
    incredibly intuitive semantic search — and much more.
  prefs: []
  type: TYPE_NORMAL
- en: The funny thing is, for many tasks, the latter parts of these models are the
    same as those in RNNs — often a couple of feedforward NNs that output model predictions.
  prefs: []
  type: TYPE_NORMAL
- en: It’s the *input* to these layers that changed. The [dense embeddings](https://www.pinecone.io/learn/dense-vector-embeddings-nlp/)
    created by transformer models are so much richer in information that we get massive
    performance benefits despite using the same final outward layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'These increasingly rich sentence embeddings can be used to quickly compare
    sentence similarity for various use cases. Such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Semantic textual similarity (STS)** — comparison of sentence pairs. We may
    want to identify patterns in datasets, but this is most often used for benchmarking.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semantic search** — information retrieval (IR) using semantic meaning. Given
    a set of sentences, we can search using a *‘query’* sentence and identify the
    most similar records. Enables search to be performed on concepts (rather than
    specific words).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clustering** — we can cluster our sentences, useful for topic modeling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this article, we will explore how these embeddings have been adapted and
    applied to a range of semantic similarity applications by using a new breed of
    transformers called *‘sentence transformers’*.
  prefs: []
  type: TYPE_NORMAL
- en: Some “Context”
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we dive into sentence transformers, it might help to piece together why
    transformer embeddings are so much richer — and where the difference lies between
    a vanilla *transformer* and a *sentence transformer*.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers are indirect descendants of the previous RNN models. These old
    recurrent models were typically built from many recurrent *units* like [LSTMs
    or GRUs](/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21).
  prefs: []
  type: TYPE_NORMAL
- en: In *machine translation*, we would find [encoder-decoder networks](https://machinelearningmastery.com/encoder-decoder-recurrent-neural-network-models-neural-machine-translation/).
    The first model for *encoding* the original language to a *context vector*, and
    a second model for *decoding* this into the target language.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ecd770c80c2df0afeb2b3944472fea4c.png)'
  prefs: []
  type: TYPE_IMG
- en: Encoder-decoder architecture with the single context vector shared between the
    two models, this acts as an information bottleneck, as *all* information must
    be passed through this point.
  prefs: []
  type: TYPE_NORMAL
- en: The problem here is that we create an *information bottleneck* between the two
    models. We’re creating a massive amount of information over multiple time steps
    and trying to squeeze it all through a single connection. This limits the encoder-decoder
    performance because much of the information produced by the encoder is lost before
    reaching the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: The *attention mechanism* provided a solution to the bottleneck issue. It offered
    another route for information to pass through. Still, it didn’t overwhelm the
    process because it focused *attention* only on the most relevant information.
  prefs: []
  type: TYPE_NORMAL
- en: The information bottleneck is removed by passing a context vector from each
    timestep into the attention mechanism (producing annotation vectors), and there
    is better information retention across longer sequences.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7c95a9edcf3ca64a8faa0df2e7d08200.png)'
  prefs: []
  type: TYPE_IMG
- en: Encoder-decoder with the attention mechanism. The attention mechanism considered
    all encoder output activations and each timestep’s activation in the decoder,
    which modifies the decoder outputs.
  prefs: []
  type: TYPE_NORMAL
- en: During decoding, the model decodes one word/timestep at a time. An alignment
    (e.g., similarity) between the word and all encoder annotations is calculated
    for each step.
  prefs: []
  type: TYPE_NORMAL
- en: Higher alignment resulted in greater weighting to the encoder annotation on
    the output of the decoder step. Meaning the mechanism calculated which encoder
    words to pay *attention* to.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/48a331d23b9489d304f128804266edf9.png)'
  prefs: []
  type: TYPE_IMG
- en: Attention between an English-French encoder and decoder, source [2].
  prefs: []
  type: TYPE_NORMAL
- en: The best-performing RNN encoder-decoders all used this attention mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: Attention is All You Need
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In 2017, a paper titled *Attention Is All You Need* was published. This marked
    a turning point in NLP. The authors demonstrated that we could remove the RNN
    networks and get superior performance using *just* the attention mechanism — with
    a few changes.
  prefs: []
  type: TYPE_NORMAL
- en: This new attention-based model was named a *‘transformer’*. Since then, the
    NLP ecosystem has entirely shifted from RNNs to transformers thanks to their vastly
    superior performance and incredible capability for generalization.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first transformer removed the need for RNNs through the use of *three*
    key components:'
  prefs: []
  type: TYPE_NORMAL
- en: Positional Encoding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-attention
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-head attention
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Positional encoding** replaced the key advantage of RNNs in NLP — the ability
    to consider the order of a sequence (they were *recurrent*). It worked by adding
    a set of varying sine wave activations to each input embedding based on position.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Self-attention** is where the attention mechanism is applied between a word
    and all of the other words in its own context (sentence/paragraph). This is different
    from vanilla attention which specifically focused on attention between encoders
    and decoders.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Multi-head attention** can be seen as several *parallel* attention mechanisms
    working together. Using several attention *heads* allowed the representation of
    several sets of relationships (rather than a single set).'
  prefs: []
  type: TYPE_NORMAL
- en: Pretrained Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The new transformer models generalized much better than previous RNNs, which
    were often built specifically for each use case.
  prefs: []
  type: TYPE_NORMAL
- en: With transformer models, it is possible to use the same *‘core’* of a model
    and simply swap the last few layers for different use cases (without retraining
    the *core*).
  prefs: []
  type: TYPE_NORMAL
- en: This new property resulted in the rise of *pretrained* models for NLP. Pretrained
    transformer models are trained on vast amounts of training data — often at high
    costs by the likes of Google or OpenAI, then released for the public to use for
    free.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most widely used of these pretrained models is BERT, or **B**idirectional
    **E**ncoder **R**epresentations from **T**ransformers by Google AI.
  prefs: []
  type: TYPE_NORMAL
- en: BERT spawned a whole host of further models and derivations such as distilBERT,
    RoBERTa, and ALBERT, covering tasks such as classification, Q&A, POS-tagging,
    and more.
  prefs: []
  type: TYPE_NORMAL
- en: BERT for Sentence Similarity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, so good, but these transformer models had one issue when building sentence
    vectors: Transformers work using word or *token*-level embeddings, *not* sentence-level
    embeddings.'
  prefs: []
  type: TYPE_NORMAL
- en: Before sentence transformers, the approach to calculating *accurate* sentence
    similarity with BERT was to use a cross-encoder structure. This meant that we
    would pass two sentences to BERT, add a classification head to the top of BERT
    — and use this to output a similarity score.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2f05f889b220b0df300e2979b6e1f4b1.png)'
  prefs: []
  type: TYPE_IMG
- en: The BERT cross-encoder architecture consists of a BERT model which consumes
    sentences A and B. Both are processed in the same sequence, separated by a `[SEP]`
    token. All of this is followed by a feedforward NN classifier that outputs a similarity
    score.
  prefs: []
  type: TYPE_NORMAL
- en: The cross-encoder network does produce very accurate similarity scores (better
    than SBERT), but it’s *not scalable*. If we wanted to perform a similarity search
    through a small 100K sentence dataset, we would need to complete the cross-encoder
    inference computation 100K times.
  prefs: []
  type: TYPE_NORMAL
- en: To cluster sentences, we would need to compare all sentences in our 100K dataset,
    resulting in just under 500M comparisons — this is simply not realistic.
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, we need to pre-compute sentence vectors that can be stored and then
    used whenever required. If these vector representations are good, all we need
    to do is calculate the cosine similarity between each.
  prefs: []
  type: TYPE_NORMAL
- en: With the original BERT (and other transformers), we can build a sentence embedding
    by averaging the values across all token embeddings output by BERT (if we input
    512 tokens, we output 512 embeddings). Alternatively, we can use the output of
    the first `[CLS]` token (a BERT-specific token whose output embedding is used
    in classification tasks).
  prefs: []
  type: TYPE_NORMAL
- en: Using one of these two approaches gives us our sentence embeddings that can
    be stored and compared much faster, shifting search times from 65 hours to around
    5 seconds (see below). However, the accuracy is not good, and is worse than using
    averaged GloVe embeddings (which were developed in 2014).
  prefs: []
  type: TYPE_NORMAL
- en: '**The solution** to this lack of an accurate model *with* reasonable latency
    was designed by Nils Reimers and Iryna Gurevych in 2019 with the introduction
    of sentence-BERT (SBERT) and the `sentence-transformers` library.'
  prefs: []
  type: TYPE_NORMAL
- en: SBERT outperformed the previous state-of-the-art (SOTA) models for all common
    semantic textual similarity (STS) tasks — more on these later — except a single
    dataset (SICK-R).
  prefs: []
  type: TYPE_NORMAL
- en: Thankfully for scalability, SBERT produces sentence embeddings — so we do *not*
    need to perform a whole inference computation for every sentence-pair comparison.
  prefs: []
  type: TYPE_NORMAL
- en: Reimers and Gurevych demonstrated the dramatic speed increase in 2019\. Finding
    the most similar sentence pair from 10K sentences took 65 hours with BERT. With
    SBERT, embeddings are created in ~5 seconds and compared with cosine similarity
    in ~0.01 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: Since the SBERT paper, many more sentence transformer models have been built
    using similar concepts that went into training the original SBERT. They’re all
    trained on many similar and dissimilar sentence pairs.
  prefs: []
  type: TYPE_NORMAL
- en: Using a loss function such as softmax loss, multiple negatives ranking loss,
    or MSE margin loss, these models are optimized to produce similar embeddings for
    similar sentences and dissimilar embeddings otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: Now you have some context behind sentence transformers, where they come from,
    and why they’re needed. Let’s dive into how they work.
  prefs: []
  type: TYPE_NORMAL
- en: '**[3] The SBERT paper covers many of this section''s statements, techniques,
    and numbers.*'
  prefs: []
  type: TYPE_NORMAL
- en: Sentence Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We explained the cross-encoder architecture for sentence similarity with BERT.
    SBERT is similar but drops the final classification head, and processes one sentence
    at a time. SBERT then uses mean pooling on the final output layer to produce a
    sentence embedding.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike BERT, SBERT is fine-tuned on sentence pairs using a *siamese* architecture.
    We can think of this as having two identical BERTs in parallel that share the
    exact same network weights.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4878afbc9ff8eb4032ac87ca0262425a.png)'
  prefs: []
  type: TYPE_IMG
- en: An SBERT model applied to a sentence pair *sentence A* and *sentence B*. Note
    that the BERT model outputs token embeddings (consisting of 512 768-dimensional
    vectors). We then compress that data into a single 768-dimensional sentence vector
    using a pooling function.
  prefs: []
  type: TYPE_NORMAL
- en: In reality, we are using a single BERT model. However, because we process sentence
    A followed by sentence B as *pairs* during training, it is easier to think of
    this as two models with tied weights.
  prefs: []
  type: TYPE_NORMAL
- en: Siamese BERT Pre-Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are different approaches to training sentence transformers. We will describe
    the original process featured most prominently in the original SBERT that optimizes
    on *softmax-loss*. Note that this is a high-level explanation, we will save the
    in-depth walkthrough for another article.
  prefs: []
  type: TYPE_NORMAL
- en: The softmax-loss approach used the *‘siamese’* architecture fine-tuned on the
    Stanford Natural Language Inference (SNLI) and Multi-Genre NLI (MNLI) corpora.
  prefs: []
  type: TYPE_NORMAL
- en: 'SNLI contains 570K sentence pairs, and MNLI contains 430K. The pairs in both
    corpora include a `premise` and a `hypothesis`. Each pair is assigned one of three
    labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '**0** — *entailment*, e.g. the `premise` suggests the `hypothesis`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**1** — *neutral*, the `premise` and `hypothesis` could both be true, but they
    are not necessarily related.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2** — *contradiction*, the `premise` and `hypothesis` contradict each other.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given this data, we feed sentence A (let’s say the `premise`) into siamese BERT
    A and sentence B (`hypothesis`) into siamese BERT B.
  prefs: []
  type: TYPE_NORMAL
- en: The siamese BERT outputs our pooled sentence embeddings. There were the results
    of *three* different pooling methods in the SBERT paper. Those are *mean*, *max*,
    and *[CLS]*-pooling. The *mean*-pooling approach was best performing for both
    NLI and STSb datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are now two sentence embeddings. We will call embeddings A `u` and embeddings
    B `v`. The next step is to concatenate `u` and `v`. Again, several concatenation
    approaches were tested, but the highest performing was a `(u, v, |u-v|)` operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8742d79bf3ddda40f9b682351e32673e.png)'
  prefs: []
  type: TYPE_IMG
- en: We concatenate the embeddings **u**, **v**, and **|u — v|**.
  prefs: []
  type: TYPE_NORMAL
- en: '`|u-v|` is calculated to give us the element-wise difference between the two
    vectors. Alongside the original two embeddings (`u` and `v`), these are all fed
    into a feedforward neural net (FFNN) that has *three* outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: These three outputs align to our NLI similarity labels **0**, **1**, and **2**.
    We need to calculate the softmax from our FFNN, which is done within the [cross-entropy
    loss function](https://www.pinecone.io/learn/cross-entropy-loss/). The softmax
    and labels are used to optimize on this *‘softmax-loss’*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/78e112539f7a39c8d0831cf39290667a.png)'
  prefs: []
  type: TYPE_IMG
- en: The operations were performed during training on two sentence embeddings, `u`
    and `v`. Note that *softmax-loss* refers cross-entropy loss (which contains a
    softmax function by default).
  prefs: []
  type: TYPE_NORMAL
- en: This results in our pooled sentence embeddings for similar sentences (label
    **0**) becoming *more similar*, and embeddings for dissimilar sentences (label
    **2**) becoming *less similar*.
  prefs: []
  type: TYPE_NORMAL
- en: Remember we are using *siamese* BERTs **not** *dual* BERTs. Meaning we don’t
    use two independent BERT models but a single BERT that processes sentence A followed
    by sentence B.
  prefs: []
  type: TYPE_NORMAL
- en: This means that when we optimize the model weights, they are pushed in a direction
    that allows the model to output more similar vectors where we see an *entailment*
    label and more dissimilar vectors where we see a *contradiction* label.
  prefs: []
  type: TYPE_NORMAL
- en: The fact that this training approach works is not particularly intuitive and
    indeed has been described by Reimers as *coincidentally* producing good sentence
    embeddings [5].
  prefs: []
  type: TYPE_NORMAL
- en: Since the original paper, further work has been done in this area. Many more
    models such as the [latest MPNet and RoBERTa models trained on 1B+ samples](https://huggingface.co/spaces/flax-sentence-embeddings/sentence-embeddings)
    (producing much better performance) have been built. We will be exploring some
    of these in future articles, and the superior training approaches they use.
  prefs: []
  type: TYPE_NORMAL
- en: For now, let’s look at how we can initialize and use some of these sentence-transformer
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Getting Started with Sentence Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The fastest and easiest way to begin working with sentence transformers is through
    the `sentence-transformers` library created by the creators of SBERT. We can install
    it with `pip`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We will start with the original SBERT model `bert-base-nli-mean-tokens`. First,
    we download and initialize the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output we can see here is the `SentenceTransformer` object which contains
    *three* components:'
  prefs: []
  type: TYPE_NORMAL
- en: The **transformer** itself, here we can see the max sequence length of `128`
    tokens and whether to lowercase any input (in this case, the model does *not*).
    We can also see the model class, `BertModel`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **pooling** operation, here we can see that we are producing a `768`-dimensional
    sentence embedding. We are doing this using the *mean pooling* method.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once we have the model, building sentence embeddings is quickly done using the
    `encode` method.
  prefs: []
  type: TYPE_NORMAL
- en: We now have sentence embeddings that we can use to quickly compare sentence
    similarity for the use cases introduced at the start of the article; STS, semantic
    search, and clustering.
  prefs: []
  type: TYPE_NORMAL
- en: We can put together a fast STS example using nothing more than a cosine similarity
    function and Numpy.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bfec024273597cb93ea09e5ccf87b5b4.png)'
  prefs: []
  type: TYPE_IMG
- en: Heatmap showing cosine similarity values between all sentence-pairs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we have calculated the cosine similarity between every combination of
    our five sentence embeddings. Which are:'
  prefs: []
  type: TYPE_NORMAL
- en: We can see the highest similarity score in the bottom-right corner with `0.64`.
    As we would hope, this is for sentences `4` and `3`, which both describe poor
    dental practices using construction materials.
  prefs: []
  type: TYPE_NORMAL
- en: Other sentence-transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although we returned good results from the SBERT model, many more sentence transformer
    models have since been built. Many of which we can find in the `sentence-transformers`
    library.
  prefs: []
  type: TYPE_NORMAL
- en: These newer models can significantly outperform the original SBERT. In fact,
    SBERT is no longer listed as an available model on the [SBERT.net models page](https://www.sbert.net/docs/pretrained_models.html).
  prefs: []
  type: TYPE_NORMAL
- en: A few of the top-performing models on the sentence transformers model page.
  prefs: []
  type: TYPE_NORMAL
- en: We will cover some of these later models in more detail in future articles.
    For now, let’s compare one of the highest performers and run through our STS task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we have the `SentenceTransformer` model for `all-mpnet-base-v2`. The components
    are very similar to the `bert-base-nli-mean-tokens` model, with some small differences:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_seq_length` has increased from `128` to `384`. Meaning we can process
    sequences that are *three* times longer than we could with SBERT.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The base model is now `MPNetModel` [4] not `BertModel`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is an additional normalization layer applied to sentence embeddings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s compare the STS results of `all-mpnet-base-v2` against SBERT.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f3e67a466911193a2c3c56562143bdf1.png)'
  prefs: []
  type: TYPE_IMG
- en: Heatmaps for both SBERT and the MPNet sentence transformer.
  prefs: []
  type: TYPE_NORMAL
- en: The semantic representation of later models is apparent. Although SBERT correctly
    identifies `4` and `3` as the most similar pair, it also assigns reasonably high
    similarity to other sentence pairs.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the MPNet model makes a *very* clear distinction between
    similar and dissimilar pairs, with most pairs scoring less than 0.1 and the `4`-`3`
    pair scored at *0.52*.
  prefs: []
  type: TYPE_NORMAL
- en: 'By increasing the separation between dissimilar and similar pairs, we’re:'
  prefs: []
  type: TYPE_NORMAL
- en: Making it easier to automatically identify relevant pairs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pushing predictions closer to the *0* and *1* target scores for *dissimilar*
    and *similar* pairs used during training. This is something we will see more of
    in our future articles on fine-tuning these models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: That’s it for this article introducing sentence embeddings and the current SOTA
    sentence transformer models for building these incredibly useful embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Sentence embeddings, although only recently popularized, were produced from
    a long range of fantastic innovations. We described some of the mechanics applied
    to create the first sentence transformer, SBERT.
  prefs: []
  type: TYPE_NORMAL
- en: We also demonstrated that despite SBERT’s very recent introduction in 2019,
    other sentence transformers already outperform the model. Fortunately for us,
    it’s easy to switch out SBERT for one of these newer models with the `sentence-transformers`
    library.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] A. Vashwani, et al., [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
    (2017), NeurIPS'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] D. Bahdanau, et al., [Neural Machine Translation by Jointly Learning to
    Align and Translate](https://arxiv.org/abs/1409.0473) (2015), ICLR'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] N. Reimers, I. Gurevych, [Sentence-BERT: Sentence Embeddings using Siamese
    BERT-Networks](https://arxiv.org/abs/1908.10084) (2019), ACL'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] [MPNet Model](https://huggingface.co/transformers/model_doc/mpnet.html),
    Hugging Face Docs'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] N. Reimers, [Natural Language Inference](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/nli/README.md),
    sentence-transformers on GitHub'
  prefs: []
  type: TYPE_NORMAL
- en: '**All images are by the author except where stated otherwise*'
  prefs: []
  type: TYPE_NORMAL
