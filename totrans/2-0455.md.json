["```py\nimport numpy as np\n\ndef find_principal_angle(x1: np.ndarray, x2: np.ndarray) -> float:\n    \"\"\"\n    Find the angle corresponding to one of the principal components\n    in two dimensions.\n\n    Parameters\n    ----------\n    x1 : numpy.ndarray\n        First input vector with shape (n,).\n    x2 : numpy.ndarray\n        Second input vector with shape (n,).\n\n    Returns\n    -------\n    float\n        The principal angle in radians.\n    \"\"\"\n\n    cov = -2 * np.sum(x1 * x2)\n    var_diff = np.sum(x2**2 - x1**2)\n\n    return 0.5 * np.arctan(cov / var_diff)\n```", "```py\ndef compute_pca_cost_hessian(x1: np.ndarray,\n                             x2: np.ndarray,\n                             theta: float) -> float:\n    \"\"\"\n    Compute the Hessian of the cost function for Principal Component\n    Analysis (PCA) in two dimensions.\n\n    Parameters\n    ----------\n    x1 : numpy.ndarray\n        First input vector with shape (n,).\n    x2 : numpy.ndarray\n        Second input vector with shape (n,).\n    theta : float\n        An angle in radians for which the cost function is evaluated.\n\n    Returns\n    -------\n    float\n        The Hessian of the PCA cost function evaluated at the given theta.\n    \"\"\"\n\n    return np.sum(\n        2 * (x2**2 - x1**2) * np.cos(2 * theta) -\n        4 * x1 * x2 * np.sin(2 * theta)\n    )\n```", "```py\ndef find_principal_components_2d(x1: np.ndarray, x2: np.ndarray) -> tuple:\n    \"\"\"\n    Find the principal components of a two-dimensional dataset.\n\n    Parameters\n    ----------\n    x1 : numpy.ndarray\n        First input vector with shape (n,).\n    x2 : numpy.ndarray\n        Second input vector with shape (n,).\n\n    Returns\n    -------\n    tuple\n        A tuple containing the two principal components represented as\n        numpy arrays.\n    \"\"\"\n\n    theta0 = find_principal_angle(x1, x2)\n    theta0_hessian = compute_pca_cost_hessian(x1, x2, theta0)\n\n    if theta0_hessian > 0:\n        pc1 = np.array([np.cos(theta0 + (np.pi / 2)),\n                        np.sin(theta0 + (np.pi / 2))])\n        pc2 = np.array([np.cos(theta0), np.sin(theta0)])\n    else:\n        pc1 = np.array([np.cos(theta0), np.sin(theta0)])\n        pc2 = np.array([np.cos(theta0 + (np.pi / 2)),\n              np.sin(theta0 + (np.pi / 2))])\n\n    return pc1, pc2\n```", "```py\nimport numpy as np\nfrom sklearn.decomposition import PCA\n\n# Generate two random correlated arrays\nrng = np.random.default_rng(seed=80)\nx1 = rng.normal(size=1000)\nx2 = x1 + rng.normal(size=len(x1))\n\n# Normalize the data\nx1 = (x1 - x1.mean()) / x1.std()\nx2 = (x2 - x2.mean()) / x2.std()\n\n# Find the principal components using the 2D logic\npc1, pc2 = find_principal_components_2d(x1, x2)\n\n# Find the principal components using sklearn PCA\nmodel = PCA(n_components=2)\nmodel.fit(np.array([x1, x2]).T)\npc1_sklearn = model.components_[0, :]\npc2_sklearn = model.components_[1, :]\n\nprint(f\"Derived PC1: {pc1}\")\nprint(f\"Sklearn PC1: {pc1_sklearn} \\n\")\nprint(f\"Derived PC2: {pc2}\")\nprint(f\"Sklearn PC2: {pc2_sklearn}\")\n\n\"\"\"   \nDerived PC1: [0.70710678 0.70710678]\nSklearn PC1: [0.70710678 0.70710678] \n\nDerived PC2: [ 0.70710678 -0.70710678]\nSklearn PC2: [ 0.70710678 -0.70710678]\n\"\"\"\n\n# Visualize the results\nfig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(x1, x2, alpha=0.5)\nax.quiver(0, 0, pc1[0], pc1[1],  scale_units='xy', scale=1, color='red')\nax.quiver(0, 0, pc2[0], pc2[1],  scale_units='xy', scale=1, color='red', label=\"Custom PCs\")\nax.quiver(0, 0, pc1_sklearn[0], pc1_sklearn[1],  scale_units='xy', scale=1, color='green')\nax.quiver(0, 0, pc2_sklearn[0], pc2_sklearn[1],  scale_units='xy', scale=1, color='green', label=\"Sklearn PCs\")\nax.set_xlabel(\"$x_1$\")\nax.set_ylabel(\"$x_2$\")\nax.set_title(\"Custom 2D PCA vs Sklearn PCA\")\nax.legend()\n```", "```py\nimport numpy as np\nfrom sklearn.decomposition import PCA\n\n# Generate three random correlated arrays\nrng = np.random.default_rng(seed=3)\nx1 = rng.normal(size=1000)\nx2 = x1 + rng.normal(size=len(x1))\nx3 = x1 + x2 + rng.normal(size=len(x1))\n\n# Create the data matrix (X)\nX = np.array([x1, x2, x3]).T\n\n# Normalize X\nX = (X - X.mean(axis=0)) / X.std(axis=0)\n\n# Compute the covariance matrix of X (S)\nS = np.cov(X.T)\n\n# Compute the eigenvalue decomposition of S\nvariances, pcs_derived = np.linalg.eig(S)\n\n# Sort the pcs according to their variance\nsorted_idx = np.argsort(variances)[::-1]\npcs_derived = pcs_derived.T[sorted_idx, :]\n\n# Find the pcs using sklearn PCA\nmodel = PCA(n_components=3)\nmodel.fit(X)\npcs_sklearn = model.components_\n\n# Compute the element-wise difference between the pcs\npc_diff = np.round(pcs_derived - pcs_sklearn, 10)\n\nprint(\"Derived Principal Components: \\n\", pcs_derived)\nprint(\"Sklearn Principal Components: \\n\", pcs_derived)\nprint(\"Difference: \\n\", pc_diff)\n\n\"\"\"\nDerived Principal Components: \n [[-0.56530668 -0.57124206 -0.59507215]\n [-0.74248305  0.66666719  0.06537407]\n [-0.35937066 -0.47878738  0.80100897]]\nSklearn Principal Components: \n [[-0.56530668 -0.57124206 -0.59507215]\n [-0.74248305  0.66666719  0.06537407]\n [-0.35937066 -0.47878738  0.80100897]]\nDifference: \n [[ 0\\.  0\\.  0.]\n [ 0\\.  0\\. -0.]\n [ 0\\.  0\\.  0.]]\n\"\"\" \n```"]