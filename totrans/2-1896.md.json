["```py\n\"\"\"Loading the draft model\n\"\"\"\n\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n\n#loading the draft model\ndraft = \"google/flan-t5-large\"\ndraft_tokenizer = T5Tokenizer.from_pretrained(draft)\ndraft_model = T5ForConditionalGeneration.from_pretrained(draft)\n```", "```py\n\"\"\"Loading the target model\n\"\"\"\n\n#loading the target model\ntarget = \"google/flan-t5-xl\"\ntarget_tokenizer = T5Tokenizer.from_pretrained(target)\ntarget_model = T5ForConditionalGeneration.from_pretrained(target)\n```", "```py\n\"\"\"Ensuring the tokenizers are identical\nin order for speculative sampling to work, tokenization for both the draft\nand target model must be identical. This is a sanity check to make sure they are.\n\"\"\"\n\n#tokenizing a test sequence\ntokenizer_test = \"this, is, some [text] for 1234comparing, tokenizers adoihayyuz\"\nex1 = target_tokenizer(prompt, return_tensors=\"pt\").input_ids\nex2 = draft_tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n#zero means all tokenized values are the same, so the tokenizers are\n#more than likely identical\nprint((ex1-ex2).abs().max())\n```", "```py\n\"\"\"Performing Speculative Sampling\n\"\"\"\n\n#initializing an empty input to feed to the decoder.\n#this is updated each loop with valid generations\ndecoder_ids = draft_model._shift_right(draft_tokenizer(\"\", return_tensors=\"pt\").input_ids)\n\n#defining input. T5 is an encoder-decoder model, so input and output are handled seperatly\ninput_ids = draft_tokenizer(\"Translate to German \\n Battle not with monsters, lest ye become a monster, and if you gaze into the abyss, the abyss gazes also into you.\", return_tensors=\"pt\").input_ids\n\n#defining the number of draft generations\nk = 5\n\n#keeps track of generation information, for later printouts\ngenerated = []\n\n#Generating Text\niter = 0\nfor _ in range(15):\n    print('========== Speculative Sampling Iteration {} =========='.format(iter))\n    iter+=1\n\n    #creating a holding place for the generated draft\n    decoder_ids_draft = decoder_ids.clone()\n\n    before_text = draft_tokenizer.decode(decoder_ids_draft[0])\n    initial_length = decoder_ids.shape[1]\n\n    #generating draft\n    for i in range(k):\n\n        #predicting the next token with the draft model\n        with torch.no_grad():\n            logits = draft_model(input_ids=input_ids, decoder_input_ids=decoder_ids_draft).logits\n            genid = torch.argmax(logits, dim=2)[0][-1]\n\n        #appending the generated id to the draft\n        genid = genid.expand(1,1)\n        decoder_ids_draft = torch.cat((decoder_ids_draft,genid),1)\n\n    print('=== Draft Generation')\n    current_draft = draft_tokenizer.decode(decoder_ids_draft[0])\n    print('generated draft tokens: {}'.format(decoder_ids_draft))\n    print('generated draft text: {}'.format(current_draft))\n\n    #Generating all next token predictions with the target\n    logits = target_model(input_ids=input_ids, decoder_input_ids=decoder_ids_draft).logits\n    genids = torch.argmax(logits, dim=2)[0]\n    print('=== Target Generation')\n    current_target = draft_tokenizer.decode(genids)\n    print('generated target tokens: {}'.format(genids))\n    print('generated target text: {}'.format(current_target))\n\n    #checking draft against target\n    for i, (dv, tv) in enumerate(zip(decoder_ids_draft[0,1:],genids[:-1])):\n        #target does not agree with the draft\n        if dv != tv:\n            #genids is next word, so this is done to preserve the first token\n            first_token = decoder_ids[0][:1]\n            decoder_ids = genids[:i+1]\n            decoder_ids = torch.cat((first_token,decoder_ids),0)\n            break\n    else:\n        #no disagreements\n        decoder_ids = genids\n\n    print('=== Validated Generation')\n    current_target = draft_tokenizer.decode(decoder_ids)\n    print('generated target tokens: {}'.format(decoder_ids))\n    print('generated target text: {}'.format(current_target))\n\n    #expanding dimensions so that the shape of the tensor is the same\n    decoder_ids = decoder_ids.expand(1,len(decoder_ids))\n\n    #logging\n    numgen = decoder_ids.shape[1] - initial_length\n    generated.append({'tokens generated': numgen, 'text before': before_text, 'text after': current_target})\n```"]