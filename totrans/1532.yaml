- en: Modern Data Warehousing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://towardsdatascience.com/modern-data-warehousing-2b1b0486ce4a](https://towardsdatascience.com/modern-data-warehousing-2b1b0486ce4a)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: State-of-the-art data platform design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://mshakhomirov.medium.com/?source=post_page-----2b1b0486ce4a--------------------------------)[![ðŸ’¡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----2b1b0486ce4a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2b1b0486ce4a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2b1b0486ce4a--------------------------------)
    [ðŸ’¡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----2b1b0486ce4a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2b1b0486ce4a--------------------------------)
    Â·12 min readÂ·Dec 16, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/61b0eba3203a5c89ddf3b0bd67553f9a.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Nubelson Fernandes](https://unsplash.com/@nublson?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: In this story, I will try to shed some light on the benefits of modern data
    warehouse solutions (DWH) compared to other data platform architecture types.
    I would dare to say that DWH is the most popular platform among data engineers
    at the moment. It offers invaluable benefits compared to other solution types
    but also has some well-known limitations. Want to learn data engineering? This
    story is a good place to start because it explains data engineering at its core
    â€” the DWH solution at the centre of the architecture diagram. We will see how
    data can be ingested and transformed in different DWHs available in the market.
  prefs: []
  type: TYPE_NORMAL
- en: Iâ€™d like to open the discussion with experienced users too. It would be great
    to know your opinion and see what you have to say on this topic.
  prefs: []
  type: TYPE_NORMAL
- en: Key characteristics of a data warehouse
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A serverless, distributed SQL engine (BigQuery, Snowflake, Redshift, Microsoft
    Azure Synapse, Teradata.) is what we call a modern data warehouse (DWH). It is
    a SQL-first data architecture [1] where data is stored in a data warehouse, and
    we can use all the advantages of using denormalized star schema [2] datasets because
    most of the modern data warehouses are distributed and scale well, which means
    there is no need to worry about table keys and indices. It suits well for ad-hoc
    analytical queries on Big Data.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----2b1b0486ce4a--------------------------------)
    [## Data Platform Architecture Types'
  prefs: []
  type: TYPE_NORMAL
- en: How well does it answer your business needs? Dilemma of a choice.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----2b1b0486ce4a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Most of the modern data warehouse solutions can process structured and unstructured
    data and are very convenient for data analysts with good SQL skills.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cc3e052dadb4fa0eb1d7d46cfd146875.png)'
  prefs: []
  type: TYPE_IMG
- en: DWH data lifecycle. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Modern data warehouses integrate easily with business intelligence solutions
    like Looker, Tableau, Sisense, and Mode, which use ANSI-SQL to process data. In
    the diagram below I tried to map a common data transformation journey and tools
    used (not a complete list of course). We can see that DWH is in the middle.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/18cc03801d2c651426977bc6c4f2c68a.png)'
  prefs: []
  type: TYPE_IMG
- en: Typical data journey and tools used. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Data warehouses are *not designed* to store unstructured data such as images,
    videos, or documents.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For this purpose, we would want to use data lakes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data Warehouse vs Database: Whatâ€™s the Difference?'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A data warehouse has a columnar data structure, the same as many RDSs it is
    relational. Data is organized into tables, rows, and columns. However, in RDS
    data is organized and stored by row, while data warehouse data is stored in columns.
    The latter provides better support for online analytical processing (OLAP) whereas
    RDS can only offer Online Transactional Processing (OLTP). RDS is definitely more
    transaction-oriented. Some of the modern data warehouse solutions can offer both
    approaches to data processing. For example, AWS Redshift supports both data warehouse
    and data lake approaches, enabling it to access and analyze large amounts of data.
  prefs: []
  type: TYPE_NORMAL
- en: Relational Database (**RDS**) stores data in a row-based table with columns
    that connect related data elements. It is designed to record and optimize to fetch
    current data quickly. Popular relational databases are ***PostgreSQL, MySQL, Microsoft
    SQL Server, and Oracle.*** RDBMS is a relational database management system that
    helps to manage databases.
  prefs: []
  type: TYPE_NORMAL
- en: '**NoSQL** databases support **only simple transactions**, whereas Relational
    Database also supports complex transactions with joins. NoSQL Database is used
    to handle data coming at high velocity. Popular NoSQL databases are MongoDB and
    CouchDB (Document databases), Redis and DynamoDB (Key-value databases).'
  prefs: []
  type: TYPE_NORMAL
- en: A Data warehouse is mainly designed for data analysis, including large amounts
    of historical data. Using a data warehouse requires users to create a pre-defined,
    fixed schema **upfront** which helps with data analytics. While dealing with data
    warehouses, tables must be simple (denormalized) in order to compute large amounts
    of data. RDS database tables and joins are complicated because they are normalized.
    So the primary difference between a traditional database and a data warehouse
    is that the traditional database is designed and optimized to record data, and
    the data warehouse is designed and optimized to respond to analytics.
  prefs: []
  type: TYPE_NORMAL
- en: RDS stores the current data required to power an application. It is useful when
    running an App and when required to fetch some current data fast.
  prefs: []
  type: TYPE_NORMAL
- en: Data warehouse vs Data lake
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A data lake is an ideal storage solution for storing large amounts of unstructured
    data, such as images, videos, and documents, along with structured data like JSON,
    CSV, PARQUET, and AVRO [3].
  prefs: []
  type: TYPE_NORMAL
- en: '[](/big-data-file-formats-explained-275876dc1fc9?source=post_page-----2b1b0486ce4a--------------------------------)
    [## Big Data File Formats, Explained'
  prefs: []
  type: TYPE_NORMAL
- en: Parquet vs ORC vs AVRO vs JSON. Which one to choose and how to use them?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/big-data-file-formats-explained-275876dc1fc9?source=post_page-----2b1b0486ce4a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: However, extracting insights from the data stored in a data lake **typically
    requires coding skills** as a data lake does not have built-in analytics or query
    capabilities like a data warehouse. Users would need to utilize programming languages
    such as Python, JAVA, Scala, or PySpark to access, process, and analyze the data
    stored in the data lake.
  prefs: []
  type: TYPE_NORMAL
- en: Amazing benefits emerge when data lake users have good coding skills
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In this case, data lake architecture can offer the highest level of flexibility
    in data processing. Users just need to know how to code in order to apply relevant
    data transformations.
  prefs: []
  type: TYPE_NORMAL
- en: However, in general, this is not the case and SQL-first solutions become more
    useful.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Top benefits of a data warehouse
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a centralized repository of data that is used for SQL querying and reporting
    DWH have many traits similar to conventional relational database solutions. Some
    of the top benefits of a data warehouse include better scalability (compared to
    RDS), better data governance (compared to data lake and data mesh architectures),
    enhanced business intelligence and improved data quality [4]
  prefs: []
  type: TYPE_NORMAL
- en: '[](/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0?source=post_page-----2b1b0486ce4a--------------------------------)
    [## Automated emails and data quality checks for your data'
  prefs: []
  type: TYPE_NORMAL
- en: Data warehouse guide for better and cleaner data with scheduled emails
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0?source=post_page-----2b1b0486ce4a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**Better data governance:** Many DWH solutions in the market offer column-level
    access controls and row-level access controls. It means that we can define granular
    controls for users. For example in BigQuery, we can restrict access or mask any
    columns that are business or PII-critical [5]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0e68e73ee8db6b61e7d7920a691167f1.png)'
  prefs: []
  type: TYPE_IMG
- en: Data masking using policy tags. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use Infrastructure as Code (IaC) to define these policies similar to
    what we would typically do when deploying infrastructure resources [6]. In this
    example below we can use platform-agnostic Terraform to define dataset access
    permissions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Better collaboration:** Modern DWH solutions offer collaboration features.
    Effective decision-making frequently requires input from several people inside
    an organisation â€” such as data analysts, marketing teams, management, and others
    â€” as well as multiple data sources. Indeed, **applying the agile approach** to
    any **data transformation development** is crucial. I would call it the best practice.'
  prefs: []
  type: TYPE_NORMAL
- en: Collaboration is the key
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: DWH solution design makes it easier to deliver large data projects by breaking
    them into smaller pieces. From what I see, organisations tend to move away from
    the tedious waterfall approach in data project design and delivery. Now we have
    a single data integration layer provided in modern DWH solutions that helps to
    deploy incremental model updates more quickly and deliver business insights more
    frequently.
  prefs: []
  type: TYPE_NORMAL
- en: '**Scalability**: Data warehouses are designed to scale well in order to handle
    large volumes of data. It is crucial to scale data pipelines when necessary to
    meet the needs of growing businesses. It also has to be able to run concurrent
    workloads at scale in a single system. For instance, a very common DWH pain point
    is user-query concurrency. It happens when a DWH solution allows only a certain
    number of concurrent user queries (typically not more than 50). Many modern data
    warehouses can offer virtual clusters with distributed physical nodes to tackle
    this problem [11].'
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, having a distributed compute cluster with auto-scaling helps a lot in
    many scenarios.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Consider this SQL mutli-cluster setup for Snowflake for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2e8e059c087780bbd9a5f28743f45632.png)'
  prefs: []
  type: TYPE_IMG
- en: Multi-cluster data warehouse with Snowflake. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: '**Improved data loading:** DWH solutions vary. For some of them, data ingestion
    is a trivial task (Snowflake) while others offer greater flexibility while working
    with partitions (BigQuery). Consider this example of data loading in the Snowflake
    data warehouse below. We assume all our data files are being created in an AWS
    S3 storage bucket and look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now we can load it using SQL!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This is what makes Snowflake so popular among data analysts and no-code users.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We can see that DWH data ingestion routines handled everything including data
    format by stripping the outer array, i.e. [{},{},{}] -> {},{},{}.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, in BigQuery we would want to create a data loader application
    that would do the same. Consider this Python code below [14]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[](/python-for-data-engineers-f3d5db59b6dd?source=post_page-----2b1b0486ce4a--------------------------------)
    [## Python for Data Engineers'
  prefs: []
  type: TYPE_NORMAL
- en: Advanced ETL techniques for beginners
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/python-for-data-engineers-f3d5db59b6dd?source=post_page-----2b1b0486ce4a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: BigQuery looks a bit more demanding in terms of the userâ€™s programming skills.
  prefs: []
  type: TYPE_NORMAL
- en: It is valid to assume that knowing how things work under the hood tends to be
    more cost-effective in the long run.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Indeed, BigQuery offers more fine controls over partitioning which might lead
    to greater savings while working with data in the DWH. Consider this data loading
    example where we can define partitioning (DAY, MONTH, RANGE):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This becomes very useful when we have a data model with date/month dimensions
    and usually saves a lot of money.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This was a bulk data loading example. **Alternatively**, we can create a data
    pipeline where data is being ingested continuously. Itâ€™s called data streaming
    [12].
  prefs: []
  type: TYPE_NORMAL
- en: '[](/streaming-in-data-engineering-2bb2b9b3b603?source=post_page-----2b1b0486ce4a--------------------------------)
    [## Streaming in Data Engineering'
  prefs: []
  type: TYPE_NORMAL
- en: Streaming data pipelines and real-time analytics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/streaming-in-data-engineering-2bb2b9b3b603?source=post_page-----2b1b0486ce4a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Snowflake has a handy Kafka connector [13] that simplifies streaming data pipelines
    and connects to the Kafka server to pull data continuously from topics.
  prefs: []
  type: TYPE_NORMAL
- en: '**Improved Business Intelligence**: Many firms collect huge volumes of data
    from a variety of sources (for example, weather, revenue, payments, customer information,
    trends, vendor information, and so on). The sheer volume of data might be useless.
    Storing this data across numerous platforms might be expensive too. So DWH as
    a single source of truth seems like a problem solver for BI pipelines where everyone
    can generate data insights with ease. Consider this Google Looker Studio integration
    below. It is available for the modern DWH solutions and all of them are pretty
    much market leaders.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/550d5f58c60aba1d313b40f1952cd9db.png)'
  prefs: []
  type: TYPE_IMG
- en: Snowflake Data Connector. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e9e59e9fe9ed618e339bc331e23b826c.png)'
  prefs: []
  type: TYPE_IMG
- en: AWS Redshift Connector for Looker Studio. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: And so onâ€¦ A free community Business Intelligence (BI) tool has connectors to
    all major DWH solutions available in the market, i.e. Redshift, Snowflake, BigQuery,
    Databricks [7], Galaxy [8], etc.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/679dc8acbc096112a6f794c1fe61bbcd.png)'
  prefs: []
  type: TYPE_IMG
- en: Looker Studio Data connectors. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: Data warehouses provide a platform for business intelligence tools and applications
    to access and analyze data. This enables businesses to make informed decisions
    based on data-driven insights.
  prefs: []
  type: TYPE_NORMAL
- en: '**Better product integration and DevOps lifecycle**: Some products go even
    further in terms of how data pipelines can be designed and deployed. For BI developers
    and data engineers, it is very important to have everything in Git. Having this
    enabled is crucial as it helps with continuous integration [9]. I previously wrote
    about how data pipeline resources can be deployed using IaC tools such as AWS
    CloudFormation and Terraform:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1?source=post_page-----2b1b0486ce4a--------------------------------)
    [## Continuous Integration and Deployment for Data Platforms'
  prefs: []
  type: TYPE_NORMAL
- en: CI/CD for data engineers and ML Ops
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1?source=post_page-----2b1b0486ce4a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Just imagine that we can deploy reports using CI/CD tools. Yes, thatâ€™s right.
    Not just data pipeline resources but also BI dashboards.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Consider this AWS CloudFormation template below. It deploys AWS Quicksight
    datasets and report analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Previously I wrote a tutorial [10] on how to deploy a streaming data pipeline
    using AWS CloudFormation. Adding a BI bit to this even makes it better in my opinion.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2?source=post_page-----2b1b0486ce4a--------------------------------)
    [## Building a Streaming Data Pipeline with Redshift Serverless and Kinesis'
  prefs: []
  type: TYPE_NORMAL
- en: An End-To-End Tutorial for Beginners
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2?source=post_page-----2b1b0486ce4a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Overall, a data warehouse is one of the most popular data platforms and can
    help companies (especially on an enterprise level) gain a competitive advantage
    by providing a single source of truth for data-driven decision-making. Modern
    DWH solutions help to deliver data insights more quickly. In a rapidly changing
    business environment, companies can activate it with automation techniques to
    generate greater value for business stakeholders. Each solution offers the features
    that make it unique. However, there are a few things to consider almost in every
    case. Cost-effectiveness, data partitioning, query-user concurrency, data lake
    storage and associated costs â€” all these pain points are valid. Sometimes it might
    be useful to unload historical data back to a cloud storage archive to optimise
    costs [15]. In some scenarios using Apache Iceberg tables might help with user
    query concurrency issues [16].
  prefs: []
  type: TYPE_NORMAL
- en: '[](/introduction-to-apache-iceberg-tables-a791f1758009?source=post_page-----2b1b0486ce4a--------------------------------)
    [## Introduction to Apache Iceberg Tables'
  prefs: []
  type: TYPE_NORMAL
- en: A few Compelling Reasons to Choose Apache Iceberg for Data Lakes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/introduction-to-apache-iceberg-tables-a791f1758009?source=post_page-----2b1b0486ce4a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: There are a few things to consider while designing and building the perfect
    data platform. Some users might argue the best data warehouse is a lakehouse (Databricks)
    but ultimately it depends on how data is being stored and your business requirements
    for historical records. In many scenarios, a DWH solution acting as a single source
    of truth for all stakeholders might become an optimal choice. Combine it with
    the right data modelling tool and you will get the right tool for external data-to-BI
    data pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: I hope you find these ideas useful. They are based on my personal experience
    and observations.
  prefs: []
  type: TYPE_NORMAL
- en: Recommended read
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] [https://medium.com/towards-data-science/data-platform-architecture-types-f255ac6e0b7](https://medium.com/towards-data-science/data-platform-architecture-types-f255ac6e0b7)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [https://towardsdatascience.com/data-modelling-for-data-engineers-93d058efa302](/data-modelling-for-data-engineers-93d058efa302)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] [https://medium.com/towards-data-science/big-data-file-formats-explained-275876dc1fc9](https://medium.com/towards-data-science/big-data-file-formats-explained-275876dc1fc9)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] [https://towardsdatascience.com/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0](/automated-emails-and-data-quality-checks-for-your-data-1de86ed47cf0)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] [https://cloud.google.com/bigquery/docs/column-level-security-intro](https://cloud.google.com/bigquery/docs/column-level-security-intro)'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] [https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/bigquery_dataset_access](https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/bigquery_dataset_access)'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] [https://docs.databricks.com/en/partners/bi/looker.html](https://docs.databricks.com/en/partners/bi/looker.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] [https://docs.starburst.io/clients/looker.html](https://docs.starburst.io/clients/looker.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[9] [https://towardsdatascience.com/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1](/continuous-integration-and-deployment-for-data-platforms-817bf1b6bed1)'
  prefs: []
  type: TYPE_NORMAL
- en: '[10] [https://medium.com/towards-data-science/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2](https://medium.com/towards-data-science/building-a-streaming-data-pipeline-with-redshift-serverless-and-kinesis-04e09d7e85b2)'
  prefs: []
  type: TYPE_NORMAL
- en: '[11] [https://www.snowflake.com/blog/auto-scale-snowflake-major-leap-forward-massively-concurrent-enterprise-applications/](https://www.snowflake.com/blog/auto-scale-snowflake-major-leap-forward-massively-concurrent-enterprise-applications/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[12] [https://medium.com/towards-data-science/streaming-in-data-engineering-2bb2b9b3b603](https://medium.com/towards-data-science/streaming-in-data-engineering-2bb2b9b3b603)'
  prefs: []
  type: TYPE_NORMAL
- en: '[13] [https://docs.snowflake.com/en/user-guide/kafka-connector](https://docs.snowflake.com/en/user-guide/kafka-connector)'
  prefs: []
  type: TYPE_NORMAL
- en: '[14] [https://towardsdatascience.com/python-for-data-engineers-f3d5db59b6dd](/python-for-data-engineers-f3d5db59b6dd)'
  prefs: []
  type: TYPE_NORMAL
- en: '[15] [https://medium.com/towards-artificial-intelligence/supercharge-your-data-engineering-skills-with-this-machine-learning-pipeline-b69d159780b7](https://medium.com/towards-artificial-intelligence/supercharge-your-data-engineering-skills-with-this-machine-learning-pipeline-b69d159780b7)'
  prefs: []
  type: TYPE_NORMAL
- en: '[16] [https://medium.com/towards-data-science/introduction-to-apache-iceberg-tables-a791f1758009](https://medium.com/towards-data-science/introduction-to-apache-iceberg-tables-a791f1758009)'
  prefs: []
  type: TYPE_NORMAL
