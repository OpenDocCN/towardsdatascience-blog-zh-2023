["```py\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\nX = 2 * np.random.rand(100, 1)\ny_true = 4 + 3 * X\ny = y_true + 0.5 * np.random.randn(100, 1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nfig, ax = plt.subplots()\nax.scatter(X_train, y_train, alpha=0.5, label='Training Set', color='blue')\nax.scatter(X_test, y_test, alpha=0.5,label='Test Set', color='green')\nax.plot(X, y_true, label='True Underlying Model', color='red', linestyle='--')\nax.plot(X_test, y_pred, label='Linear Regression Model', color='orange')\nax.set_xlabel('X')\nax.set_ylabel('y')\nax.set_title(f'Linear Regression on a 1D feature with test score R^2=\\n {model.score(X_test, y_test):.2f}')\nax.legend()\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(n_samples=100, n_features=1, n_informative=1, n_redundant=0, n_clusters_per_class=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\nx_ = np.linspace(-2, 3).reshape(-1,1)\n\nfig, ax = plt.subplots()\nax.scatter(X_train[y_train == 0], y_train[y_train == 0], label='Class 0 (Training)', color='blue')\nax.scatter(X_train[y_train == 1], y_train[y_train == 1], label='Class 1 (Training)', color='red')\nax.scatter(X_test[y_test == 0], y_test[y_test == 0], label='Class 0 (Test)', marker='s', color='blue', alpha=0.5)\nax.scatter(X_test[y_test == 1], y_test[y_test == 1], label='Class 1 (Test)', marker='s', color='red', alpha=0.5)\nax.plot(x_, model.predict_proba(x_)[:, 1], label='Logistic Regression Model', color='green')\nax.axhline(0.5, color='gray', linestyle='--', label='Decision Boundary (0.5)')\nax.set_xlabel('X')\nax.set_ylabel('Probability')\nax.set_title(f'Logistic Regression Example with score={model.score(X, y):.2f}')\nax.legend()\n\ny_pred = model.predict(X_test)\ny_proba = model.predict_proba(X_test)[:, 1]\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import make_classification\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\nX, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmodel = LogisticRegression(C=10000000, max_iter=100000)\nmodel.fit(X_train, y_train)\n\ndb = DecisionBoundaryDisplay.from_estimator(\n    model,\n    X,\n    response_method=\"predict_proba\", # \"predict_proba\",\n    cmap=\"RdBu_r\",\n    alpha=0.5, grid_resolution=200,\n)\nsns.scatterplot(x=X_train[:, 0], y=X_train[:, 1], hue=y_train, palette={0:\"blue\", 1:\"red\"}, alpha=0.5, ax=db.ax_)\nsns.scatterplot(x=X_test[:, 0],  y=X_test[:, 1], hue=y_test, palette={0:\"blue\", 1:\"red\"}, ax=db.ax_)\n\ndb.ax_.set_title(f\"Decision boundary of the trained\\n LogisticRegression with score={model.score(X, y):.2f}')\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\n\nX = 2 * np.random.rand(100, 1)\ny = X**3 + 3 * X**2 + 0.5 * X + 2 + np.random.randn(100, 1)\n\nlinreg = LinearRegression()\nlinreg.fit(X, y)\n\ndegree = 3\npoly_linreg = make_pipeline(PolynomialFeatures(degree), LinearRegression())\npoly_linreg.fit(X, y)\n\nx_ = np.linspace(0, 2, 100).reshape(-1, 1)\n\nfig, ax = plt.subplots()\nax.scatter(X, y, label='Original Data')\nax.plot(x_, linreg.predict(x_), color='blue', label=f'Linear Regression (linreg) score={linreg.score(X, y):.2f}')\nax.plot(x_, poly_linreg.predict(x_), color='red', label=f'Polynomial Regression (poly_linreg, Degree {degree}) score={poly_linreg.score(X, y):.2f}')\nax.set_xlabel('X')\nax.set_ylabel('y')\nax.set_title('Fitting Linear and Polynomial Curves to Data')\nax.legend()\n```", "```py\nprint(linreg.coef_, linreg.intercept_)\nprint(poly_linreg[-1].coef_, poly_linreg[-1].intercept_)\n# [[10.607613]] [-2.32139028]\n# [[-0.83958618  5.07382762  0.30530322]] [1.9408571]\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\nX = 2 * np.random.rand(50, 1)\ny = 4 + 3 * X + np.random.randn(50, 1)\n\n# Function to fit and plot Ridge Regression models\ndef plot_ridge(alpha, ax1, ax2, deg=8):\n    model = make_pipeline(PolynomialFeatures(deg, include_bias=False), Ridge(alpha=alpha))\n    model.fit(X, y)\n\n    ax1.scatter(X, y, color='blue', s=10, label='Data')\n\n    x_range = np.linspace(0, 2, 100).reshape(-1, 1)\n    y_pred = model.predict(x_range)\n    ax1.plot(x_range, y_pred, color='red', label=f'Ridge Regression (alpha={alpha})')\n\n    coefs = model.named_steps['ridge'].coef_.ravel()\n    ax2.plot(range(deg), coefs, color='green', marker='o', label='Coefficients')\n\n    ax1.set_title(f'Ridge Regression with alpha={alpha} / R^2={model.score(X,y):.2f}')\n    ax2.set_title(f\"Linear coefficients with alpha={alpha}\")\n    ax1.legend()\n\nfig, axs = plt.subplots(2, 3, figsize=(18, 6))\nplot_ridge(0,   axs[0,0], axs[1,0])\nplot_ridge(1,   axs[0,1], axs[1,1])\nplot_ridge(100, axs[0,2], axs[1,2])\nfig.tight_layout()\n```", "```py\nfrom sklearn.model_selection import ValidationCurveDisplay\n# Plotting the validation curve\nValidationCurveDisplay.from_estimator(\n    make_pipeline(PolynomialFeatures(10, include_bias=False), Ridge()),\n    X, y,\n    param_name='ridge__alpha',\n    param_range=np.logspace(-3, 3),\n)\n```"]