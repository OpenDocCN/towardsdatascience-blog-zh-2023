["```py\n#retreive data\naws s3 cp s3://sagemaker-sample-files/datasets/tabular/uci_abalone/train_csv/abalone_dataset1_train.csv .\n```", "```py\nimport os\nimport pandas as pd\nimport sys\n\n#~110KB initial file\ndf = pd.read_csv(\"abalone_dataset1_train.csv\")\nprint(sys.getsizeof(df))\n\n#creates a 104MB file\ndf_larger = pd.concat([df]*700, ignore_index=True)\nprint(sys.getsizeof(df_larger))\n\ndf_larger.to_csv(\"abalone-100mb.csv\")\n```", "```py\n%%sh\n\n#replace with your S3 bucket to upload to\ns3_bucket='sagemaker-us-east-1-474422712127'\n\nfor i in {0..10000}\ndo\n  aws s3 cp abalone-100mb.csv s3://$s3_bucket/xgboost-1TB/abalone-$i.csv \ndone\n```", "```py\nimport boto3\nimport sagemaker\nfrom sagemaker.estimator import Estimator\n\nboto_session = boto3.session.Session()\nregion = boto_session.region_name\n\nsagemaker_session = sagemaker.Session()\nbase_job_prefix = 'xgboost-example'\nrole = sagemaker.get_execution_role()\n\ndefault_bucket = sagemaker_session.default_bucket()\ns3_prefix = base_job_prefix\n\ntraining_instance_type = 'ml.m5.24xlarge'\n```", "```py\nfrom sagemaker.inputs import TrainingInput\n\n#replace with your S3 Bucket with data\ntraining_path = 's3://sagemaker-us-east-1-474422712127/xgboost-1TB/'\n\n#set distribution to ShardedByS3Key otherwise a copy of all files will be made across all instances\n#we also enable FastFile mode here where as the default is File mode\ntrain_input = TrainingInput(training_path, content_type=\"text/csv\", \ninput_mode='FastFile', distribution = \"ShardedByS3Key\")\ntraining_path\n```", "```py\nmodel_path = f's3://{default_bucket}/{s3_prefix}/xgb_model'\n\nimage_uri = sagemaker.image_uris.retrieve(\n    framework=\"xgboost\",\n    region=region,\n    version=\"1.0-1\",\n    py_version=\"py3\",\n    instance_type=training_instance_type,\n)\n\nxgb_train = Estimator(\n    image_uri=image_uri,\n    instance_type=training_instance_type,\n    instance_count=25,\n    output_path=model_path,\n    sagemaker_session=sagemaker_session,\n    role=role,\n\n)\n\nxgb_train.set_hyperparameters(\n    objective=\"reg:linear\",\n    num_round=50,\n    max_depth=5,\n    eta=0.2,\n    gamma=4,\n    min_child_weight=6,\n    subsample=0.7,\n    silent=0,\n)\ntraining_instance_type\n```", "```py\nxgb_train.fit({'train': train_input})\n```"]