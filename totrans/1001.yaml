- en: GPT-4 Can Solve Math Problems — But Not in All Languages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/gpt-4-can-solve-math-problems-but-not-in-all-languages-d1c2e9c195a0](https://towardsdatascience.com/gpt-4-can-solve-math-problems-but-not-in-all-languages-d1c2e9c195a0)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*A few experiments making GPT-4 solve math problems in 16 different languages*'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@artfish?source=post_page-----d1c2e9c195a0--------------------------------)[![Yennie
    Jun](../Images/b635e965f21c3d55833269e12e861322.png)](https://medium.com/@artfish?source=post_page-----d1c2e9c195a0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d1c2e9c195a0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d1c2e9c195a0--------------------------------)
    [Yennie Jun](https://medium.com/@artfish?source=post_page-----d1c2e9c195a0--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d1c2e9c195a0--------------------------------)
    ·12 min read·Oct 11, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/180c9a75eac66a3a19a3313d19599f93.png)'
  prefs: []
  type: TYPE_IMG
- en: Image created by author using Midjourney.
  prefs: []
  type: TYPE_NORMAL
- en: '*This article was originally posted on my* [*personal website*](https://www.artfish.ai/p/gpt4-project-euler-many-languages)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is said that [mathematics is a universal language](https://www.emerald.com/insight/content/doi/10.1108/JME-01-2016-0004/full/html)
    — mathematical concepts, theorems, and definitions can be expressed as symbols
    that are understandable regardless of language.
  prefs: []
  type: TYPE_NORMAL
- en: '**In this article, I test the mathematical capabilities of GPT-4 in sixteen
    different languages.**'
  prefs: []
  type: TYPE_NORMAL
- en: Early experiments showed GPT-4 [scoring highly on the SAT Math and AP Calculus
    tests](https://arxiv.org/abs/2303.08774) and on [undergraduate-level mathematics](https://arxiv.org/abs/2301.13867).
    However, the majority of these experiments **test GPT-4’s mathematical capabilities
    only in English.** To better understand GPT-4’s mathematical capabilities beyond
    English, I prompt it on the same math problems in fifteen other languages.
  prefs: []
  type: TYPE_NORMAL
- en: So, how good is GPT-4 at math in different languages? In theory, it should be
    equally good (or bad) across all languages, but unfortunately (as you might have
    guessed), this is not the case. **GPT-4 is much better at solving math problems
    in English**. Depending on the language, GPT-4 could solve some of the problems.
    For traditionally under-resourced languages, however, such as Burmese and Amharic,
    GPT-4 was unable to solve the problems I gave it.
  prefs: []
  type: TYPE_NORMAL
- en: About Project Euler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I use mathematical problems from the [Project Euler](https://projecteuler.net/)
    website to test GPT-4\. (This is also a throwback to one of my [one of my earlier
    articles from this year](https://www.artfish.ai/p/prompt-engineering-gpt-3-to-solve),
    where I used prompt engineering using ChatGPT to solve a few Project Euler problems).
    Project Euler, named for the [eponymous mathematician](https://en.wikipedia.org/wiki/Leonhard_Euler),
    is a website with hundreds of mathematical and computer programming problems ranging
    in difficulty. Started in 2001, they boast over 850 problems (as of October 2023)
    and release a new question approximately every week.
  prefs: []
  type: TYPE_NORMAL
- en: The great thing about Project Euler questions is that each problem has a numerically
    “correct” answer — this makes it easy to check if GPT-4’s answer is objectively
    correct or not. They also tend to be a lot more complicated than high-school or
    college-level math problems. Currently, there is no large-scale comprehensive
    understanding of GPT-4’s (or other large language models, for that matter) math
    capabilities on Project Euler problems (other than this project, which [evaluates
    ChatGPT’s abilities only on the first 30 problems](https://github.com/mccaffary/GPT-4-ChatGPT-Project-Euler)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6eb96b7f4817f49e224cf7e75c1ee764.png)'
  prefs: []
  type: TYPE_IMG
- en: A screenshot of the Project Euler website.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4 has limits in solving difficult Project Euler problems (English only)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before running experiments on a bunch of other languages, I tested GPT-4’s abilities
    to solve a subset of Project Euler questions of varying difficulty levels.
  prefs: []
  type: TYPE_NORMAL
- en: '[Project Euler difficulty ratings](https://projecteuler.chat/viewtopic.php?t=3810)
    are “based on the times taken to solve a problem since its publication date.”
    They range from a score of 5 to 100 in increments of 5\. The first 50 questions
    are mostly rated at difficulty 5 largely due to the fact that those questions
    were released way earlier (nearly 20 years ago!) and therefore many more people
    have had the opportunity to reach a solution for them.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a270a53b8ce10305521e5a4c4b3c9da1.png)'
  prefs: []
  type: TYPE_IMG
- en: Distribution of Project Euler problems by difficulty. Figure created by the
    author.
  prefs: []
  type: TYPE_NORMAL
- en: I chose a subset of problems (of varying difficulty) for GPT-4 to solve. Prompts
    used are included at the end of the article. I ran each prompt five times to allow
    for variance in GPT-4’s answers.
  prefs: []
  type: TYPE_NORMAL
- en: '**GPT-4 was unable to solve any problems of difficulty 20 or higher in English.**
    Note that I am not saying that GPT-4 is unable to solve ***all*** problems with
    difficulty above 20, as I did not test every single question. However, as **I
    ran each question 5 times**, if GPT-4 would have been able to solve a problem
    above difficulty of 20, it should have gotten the right answer at least once.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/54ea810696da3ffd42951ab3cb350390.png)'
  prefs: []
  type: TYPE_IMG
- en: GPT-4’s pass rate on a subset of Project Euler problems, prompted in English
    only. Figure created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: It is possible that with more sophisticated prompting techniques (such as few-shot
    learning, chain-of-thought prompting, or providing additional background context
    about mathematics), GPT-4 could have solved any of these problems. However, I
    wanted to focus on testing the model as it was out-of-the-box, so I’ll leave these
    prompting techniques up to a future experiment :)
  prefs: []
  type: TYPE_NORMAL
- en: '**For the problems GPT-4 was able to solve (with difficulty of 15 or lower),
    its pass rate varied based on both difficulty level and question number**. For
    lower question numbers like 1, 3, and 62 (question numbers less than 100, which
    were most likely to have appeared in GPT-4’s training data), GPT-4 was easily
    able to achieve the correct answer 100% of the time.'
  prefs: []
  type: TYPE_NORMAL
- en: A sample of languages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While there are hundreds of written and spoken languages in the world, I chose
    a subset of sixteen languages. These are a combination of the [top ten spoken
    languages](https://en.wikipedia.org/wiki/List_of_languages_by_total_number_of_speakers)
    as of 2023; 3 worst-performing languages from my earlier article about [language
    model tokenization](https://www.artfish.ai/p/all-languages-are-not-created-tokenized);
    and a few others.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each of the fifteen non-English languages, I did the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Translated a Project Euler problem into that language using GPT-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Had a new instance of GPT-4 solve the translated problem a **in that language**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/e4d984611d98c00b60dac508d38ccca5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Example translation of Project Euler problem #500 into Burmese. Figure created
    by the author.'
  prefs: []
  type: TYPE_NORMAL
- en: Like earlier, I ran each prompt five times to allow for variance in GPT-4’s
    answers. Prompts are included at the end of the article.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4 struggles to solve problems in some languages more than others
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I chose a subset of Project Euler problems based on a few constraints:'
  prefs: []
  type: TYPE_NORMAL
- en: 2 problems each of difficulty 5, 10, and 15\. Since GPT-4 struggled to solve
    problems of difficulty > 20, I disregarded those in the multilingual analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Problems with question number > 200\. Previous research showed that [GPT-4
    memorizes Project Euler numerical solutions](https://cundy.me/post/gpt_4_memorizes_project_euler_numerical_solutions/)
    for the first 200 or so problems (e.g. asking GPT-4 “What is the numerical answer
    to Project Euler question #X”, GPT-4 would correctly answer even though the user
    did not provide the actual problem)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Length of problem < 500 characters (to avoid spending too much $$ on the context)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It may not come as a surprise, but **GPT-4 was able to correctly solve Project
    Euler problems in English more than 3x as often compared to other languages, such
    as Armenian or Farsi. GPT-4 was not able to solve any of the 6 questions for languages
    such as Burmese and Amharic.**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2868c95c28b0216730b079600b80a812.png)'
  prefs: []
  type: TYPE_IMG
- en: 'GPT-4 pass rate for a subset of Project Euler problems when prompted in 16
    different languages. GPT-4 has varying levels of mathematical aptitude while solving
    Project Euler problems > #200; it is unable to solve any of these problems in
    Burmese and Amharic. Figure created by the author.'
  prefs: []
  type: TYPE_NORMAL
- en: I chose the languages Burmese and Amharic in particular because those were the
    worst performing in my earlier article about [language model tokenization](https://www.artfish.ai/p/all-languages-are-not-created-tokenized),
    where I found that tokenizing the same sentence in Burmese or Amharic may require
    10x more tokens than a similar message in English. And as we will see in the rest
    of the analysis, **these languages are not only more expensive to process, but
    have worse performance compared to other languages.**
  prefs: []
  type: TYPE_NORMAL
- en: 'Is GPT-4 bad at solving problems in all of these languages? Not quite… I had
    GPT-4 solve Project Euler problems #1 and #3, which are two of the most-solved
    problems on the website (problems for which many solutions exist on the Internet
    and, by proxy, GPT-4’s training data). As can be seen below, GPT-4 is much better
    at solving these “easier” and (and more frequently solved) problems. It is interesting
    to note that even for the popular problems, the performance for Burmese is still
    quite low compared to the other languages.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a44116fee3353957c87d254e25ca4f78.png)'
  prefs: []
  type: TYPE_IMG
- en: GPT-4 pass rate for a subset of *EASY* Project Euler problems when prompted
    in 16 different languages. GPT-4 has varying levels of mathematical aptitude while
    solving two popular Project Euler problems; it is able to solve in most languages
    uniformly. Figure created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Bad translations lead to bad math problem solving skills
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier, I asked GPT-4 to first translate a Project Euler question to another
    language, then to solve it in that language. But what if the translation itself
    was not that great to begin with?
  prefs: []
  type: TYPE_NORMAL
- en: 'I picked GPT-4’s translations for Question #3 (difficulty 5), Question #365
    (difficulty 40), and Question #500 (difficulty 15). I had a new instance of GPT-4
    translate the non-English language back into English. Then, I compared the original
    English question with the question translated back into English.'
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, we would want the original English question to be as similar as possible
    to the question that was translated into German/Spanish/Russian/Hindi then translated
    back into English. I used a metric called **Word Error Rate** (WER), which calculates
    the amount of errors there are between two texts — lower values (closer to 0)
    indicate the two translations are nearly identical, and higher values indicate
    the two translations vary.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/511296dca33c1c69b34825130b5535e7.png)'
  prefs: []
  type: TYPE_IMG
- en: Box plot depicting GPT-4’s translation Word Error Rates. Figure creeated by
    the author.
  prefs: []
  type: TYPE_NORMAL
- en: Languages that are “closer” to English (either linguistically related or using
    the same Latin script), such as German, Portuguese, and Spanish, have a lower
    Word Error Rate. On the other hand, languages that use a completely different
    script and alphabet, such as Urdu, Amharic, and Burmese, have a much higher Word
    Error Rate (and greater variance). **This means that GPT-4 is *not* doing a good
    job translating into and out of these languages.**
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, for [Problem #500](https://projecteuler.net/problem=500), the
    version translated to Burmese and back into English incorporates some extraneous
    phrases. Can you spot the differences?'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0362b75522bd96f72ef46b0f67c118b8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Example translation of Project Euler problem #500 into Burmese and back into
    English. Figure created by the author.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Original problem**'
  prefs: []
  type: TYPE_NORMAL
- en: The number of divisors of 120 is 16\. In fact 120 is the smallest number having
    16 divisors.Find the smallest number with 2500500 divisors. Give your answer modulo
    500500507.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Translation to Burmese then back to English**'
  prefs: []
  type: TYPE_NORMAL
- en: The number of gifts is 16 for 120 years. The minimum number that can yield 16
    gifts is 120\. Find the minimum number that can yield 2,500,500 gifts. Give your
    answer in a moderate manner with 500500507\. Your answer will only be considered
    correct after meticulous scrutiny.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: On the other hand, for this particular question, translations back into English
    from languages such as Spanish, German, and Portuguese, were at times **nearly
    identical to the original question.**
  prefs: []
  type: TYPE_NORMAL
- en: So, it’s not just that GPT-4 is bad at math in certain languages. GPT-4 is also
    bad at understanding and translating into those languages in general.
  prefs: []
  type: TYPE_NORMAL
- en: 'Trick questions and incorrect memorizations: A qualitative analysis'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, I provide a few examples of GPT-4’s responses that I found
    particularly interesting (all in English). These questions are a few of the problems
    that GPT-4 could not answer in English at all.
  prefs: []
  type: TYPE_NORMAL
- en: Claiming problems are impossible to solve
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I asked GPT-4 to solve [Problem #808](https://projecteuler.net/problem=808)
    (difficulty 5). To which GPT-4 responded:'
  prefs: []
  type: TYPE_NORMAL
- en: This is actually a trick question because according to currently known mathematics,
    there are only 2 reversible prime squares
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In another response, GPT-4 responded:'
  prefs: []
  type: TYPE_NORMAL
- en: The problem as presented does not currently have a known solution
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For this particular problem, then, despite it being difficulty 5, GPT-4 claimed
    that an answer did not exist. My guess is that because the problem came out recently
    (e.g. likely not in GPT-4’s training data), GPT-4 has not seen this problem before,
    and therefore did not know how to attempt to even solve it.
  prefs: []
  type: TYPE_NORMAL
- en: Memorizing the right answer for the wrong test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I asked GPT-4 to solve [Problem #684](https://projecteuler.net/problem=684)
    (difficulty 5). GPT-4’s response was as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: This is a problem from Project Euler (problem 317).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: So much for memorizing the right answer for the wrong test!
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4 memorizes YouTube transcriptions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I asked GPT-4 to solve [Problem #491](https://projecteuler.net/problem=491)
    (difficulty 20). In one of the responses, GPT-4 endeavored to solve the problem
    (which it did not succeed in) and ended its generation with a curious plug for
    a YouTube channel with 13 subscribers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Subscribe to my YouTube channel: Iranoutofnames 5\. I do Olympiad math there.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: While it is a bit tangential to the topic of Project Euler and multilingual
    math capabilities of LLMs, I do think this response is interesting given that
    it seems to show some indication of how GPT-4 was likely trained on some amount
    of YouTube transcriptions.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, GPT-4 knows it is solving a Project Euler Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the prompts, **I did not mention that a problem was sourced from Project
    Euler. However, GPT-4 mentioned several times in its responses that the problem
    was an Euler problem.** While this phenomenon did not occur very often, I did
    find it interesting when it did happen.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dceb1d7355644f5b8c7a5d8e04ff5aaf.png)'
  prefs: []
  type: TYPE_IMG
- en: Number of times GPT-4 mentioned “Project Euler” in its generated responses.
    Figure created by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article builds upon my previous research exploring multilingual disparities
    in large language models (such as [inequality in tokenizing different languages
    for LLMs](https://www.artfish.ai/p/all-languages-are-not-created-tokenized) or
    [unequal representation of historical figures by LLMs in different languages](https://www.artfish.ai/p/where-are-all-the-women)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Through the experiments in this article, I found:'
  prefs: []
  type: TYPE_NORMAL
- en: Even in English, GPT-4 had limits in solving medium to difficult Project Euler
    problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT-4 struggled solving easy problems if those problems were released more recently
    (compared to easy problems released 10–20 years ago). It’s likely GPT-4 just hasn’t
    seen the later problems in its training data and didn’t get to memorize the answer.
    This recalls a similar phenomenon using questions from a coding website, [where
    GPT-4 could solve 10/10 coding questions before 2021 and 0/10 recent questions](https://twitter.com/cHHillee/status/1635790330854526981).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT-4 struggled to solve problems in some languages more than others — in particular,
    languages not based off of the Latin scripts. GPT-4 generated worse translations
    for these languages, which led to bad math problem solving skills in those languages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**One LLM**: In this article, **I only tested one LLM!** How would these results
    change for the many other LLMs (both closed-source and open-source) on the market?
    How would these results change for a version of GPT-4 several months from today
    as the model continues evolve and be fine-tuned? These are questions that were
    beyond the scope of this particular article but important to consider.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Translations**: Another way to test GPT-4’s mathematical capabilities in
    different languages would have been to have experts in those languages do the
    translations directly. While there exist [translations of some Project Euler questions
    into a few different languages](https://www.notion.so/6b7ff8c60b6346f68eccf482b9c0a81b?pvs=21),
    these do not cover all questions and cover only a few languages. (The downside
    of testing traditionally underrepresented languages is that they are … underrepresented)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Project Euler**: Project Euler is only one of many different ways to probe
    GPT-4 (and other LLMs) to reveal their mathematical abilities. While in this article
    I proposed one method to measure the math capabilities and limits of LLMs, other
    researchers have [proposed other mathematical datasets and methods](https://arxiv.org/abs/2301.13867)
    to do the same.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Probing GPT-4 with Project Euler problems in sixteen languages revealed clear
    linguistic biases in its math skills. These differences were notably more pronounced
    for languages likely underrepresented in GPT-4’s training data. Such findings
    emphasize the importance of broadening evaluation metrics across languages for
    more comprehensive performance insights. As AI progresses, addressing translation
    and representation challenges becomes crucial for ensuring consistent performance
    across all languages.
  prefs: []
  type: TYPE_NORMAL
- en: '*Thanks for reading my article! Liked what you read? Leave a comment or share
    with a friend! Find more similar articles at* [***artfish.ai***](https://www.artfish.ai/)***.***'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*All of the data and some code is shared on the corresponding* [*Github repository*](https://github.com/yenniejun/project-euler-gpt-langs/tree/main)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: Project Euler problems tested in English only
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I tested the following questions and difficulty. For these, GPT-4 was not able
    to reach the correct solution at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'question #491 and #731 (difficulty 20)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'question #485 (difficulty 30)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'question #365 (difficulty 40)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'question #142 (difficulty 45)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Languages used
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Top 10 languages: English, Chinese, Hindi, Spanish, Modern Standard Arabic,
    Bengali, Portuguese, Russian, Urdu. I removed French (which is part of the top
    10 spoken languages) as it was similar to some of the other languages included
    and I wanted to include more diverse languages. For Chinese, I separated the script
    into Traditional and Simplified Chinese.'
  prefs: []
  type: TYPE_NORMAL
- en: '3 worst languages from my tokenization article: Amharic, Armenian, and Burmese'
  prefs: []
  type: TYPE_NORMAL
- en: '3 other languages I included: Korean, German, and Farsi'
  prefs: []
  type: TYPE_NORMAL
- en: Prompts used
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**To translate a Project Euler problem into another language**'
  prefs: []
  type: TYPE_NORMAL
- en: '`Translate the following text into {language}. Then, append the instruction
    to specify just the numerical answer at the end followed by the ∴ symbol.\\n\\n{text}}`'
  prefs: []
  type: TYPE_NORMAL
- en: '**To translate a translated problem back into English**'
  prefs: []
  type: TYPE_NORMAL
- en: '`Translate the following text into English.\\n\\n{text}}`'
  prefs: []
  type: TYPE_NORMAL
- en: Breakdown of language pass rate per question
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/85ec13c239cd366a8c5bfd7e8f401d79.png)'
  prefs: []
  type: TYPE_IMG
- en: Breakdown of language pass rate per Project Euler problem when prompted in all
    sixteen languages. Figure created by the author.
  prefs: []
  type: TYPE_NORMAL
