["```py\npip install openai\n```", "```py\nimport openai \nimport os\n```", "```py\nparams = {\n    \"model\": \"gpt-4-vision-preview\",\n    \"messages\": PROMPT_MESSAGES,\n    \"api_key\": os.environ['OPENAI_API_KEY'],\n    \"headers\": {\"Openai-Version\": \"2020-11-07\"},\n    \"max_tokens\": 400,\n}\n```", "```py\nPROMPT_MESSAGES = [{\n    \"role\": \"user\",\n    \"content\": [\n        \"<Your Prompt>\",\n        {\"image\": image_in_base64_format}\n    ],\n}]\n```", "```py\nimport base64\n\ndef convert_image_to_base64(image_path):\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode('utf-8')\n```", "```py\nresponse = openai.ChatCompletion.create(**params)\nprint(response)\n```", "```py\nresponse = requests.post(\n    \"https://api.openai.com/v1/audio/speech\",\n    headers={\n        \"Authorization\": f\"Bearer {os.environ['OPENAI_API_KEY']}\",\n    },\n    json={\n        \"model\": \"tts-1\",\n        \"input\": result.choices[0].message.content,\n        \"voice\": \"fable\",\n    },\n)\n```", "```py\nimport base64\nimport cv2\nimport openai\nimport os\nimport requests\nimport time\n\nfrom IPython.display import display, Image, Audio\nfrom moviepy.editor import VideoFileClip, AudioFileClip\nfrom moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n```", "```py\nvideo = cv2.VideoCapture(\"video.mp4\")\n\nbase64Frames = []\nwhile video.isOpened():\n    success, frame = video.read()\n    if not success:\n        break\n    frame = cv2.resize(frame, (512,512))\n    _, buffer = cv2.imencode(\".jpg\", frame)\n    base64Frames.append(base64.b64encode(buffer).decode(\"utf-8\"))\n\nvideo.release()\nprint(len(base64Frames), \"frames read.\")\n```", "```py\nPROMPT_MESSAGES = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            \"These are frames from a person walking. Narrate succinctly with short sentences like they do in GPS devices the elements and obstacles around you to help a blind person go through.\",\n            *map(lambda x: {\"image\": x}, base64Frames[0::100]),\n        ],\n    },\n]\n\nparams = {\n    \"model\": \"gpt-4-vision-preview\",\n    \"messages\": PROMPT_MESSAGES,\n    \"api_key\": os.environ['OPENAI_API_KEY'],\n    \"headers\": {\"Openai-Version\": \"2020-11-07\"},\n    \"max_tokens\": 350,\n}\n\nresult = openai.ChatCompletion.create(**params)\nprint(result.choices[0].message.content)\n```", "```py\nresponse = requests.post(\n    \"https://api.openai.com/v1/audio/speech\",\n    headers={\n        \"Authorization\": f\"Bearer {os.environ['OPENAI_API_KEY']}\",\n    },\n    json={\n        \"model\": \"tts-1\",\n        \"input\": result.choices[0].message.content,\n        \"voice\": \"fable\",\n    },\n)\n\naudio = b\"\"\nfor chunk in response.iter_content(chunk_size=1024 * 1024):\n    audio += chunk\n\nwith open('audio.mp3', 'wb') as file:\n    file.write(audio)\n```", "```py\n# Open the video and audio\nvideo_clip = VideoFileClip(\"video.mp4\")\naudio_clip = AudioFileClip(\"audio.mp3\")\n\n# Concatenate the video clip with the audio clip\nfinal_clip = video_clip.set_audio(audio_clip)\nfinal_clip.write_videofile(\"video_audio.mp4\")\n```"]