- en: All You Need to Know about In-Context Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/all-you-need-to-know-about-in-context-learning-55bde1180610](https://towardsdatascience.com/all-you-need-to-know-about-in-context-learning-55bde1180610)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '| IN CONTEXT LEARNING | LARGE LANGUAGE MODELS| LLMs'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What is and how does it work what makes Large Language Models so powerful
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://salvatore-raieli.medium.com/?source=post_page-----55bde1180610--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----55bde1180610--------------------------------)[](https://towardsdatascience.com/?source=post_page-----55bde1180610--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----55bde1180610--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----55bde1180610--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----55bde1180610--------------------------------)
    ¬∑19 min read¬∑Jul 25, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7f1b46371aebe6ca8e4684f7f9be78fa.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [üá∏üáÆ Janko Ferliƒç](https://unsplash.com/ko/@itfeelslikefilm?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: ‚ÄúFor me context is the key ‚Äî from that comes the understanding of everything.‚Äù
    ‚Äî Kenneth Noland
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In-context learning (ICL) is one of the most surprising model skills. Observed
    with GPT-3 it caught the authors‚Äô attention. **Exactly what is ICL? More importantly,
    what gives rise to it?**
  prefs: []
  type: TYPE_NORMAL
- en: 'This article is divided into different sections, for each section we will answer
    these questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What is In-Context Learning (ICL)? Why this is interesting? Why it is useful?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The mystery of ICL: how does it work? Is the training data? is the prompt?
    it is the architecture?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the future of ICL? What are the remaining challenges?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check the list of references at the end of the article, I provide also some
    suggestions to deepen the topics.
  prefs: []
  type: TYPE_NORMAL
- en: What is In-Context Learning (ICL)?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/4b4d54bbb67cef241a1909f3d3526037.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Dmitry Ratushny](https://unsplash.com/@ratushny?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: ‚ÄúThe limits of my language mean the limits of my world.‚Äù ‚Äî Ludwig Wittgenstein
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Before [Large Language Models](https://en.wikipedia.org/wiki/Large_language_mode)
    (LLMs) were published, an artificial intelligence model was limited to the data
    it was trained on. In other words, [LLMs](https://en.wikipedia.org/wiki/Large_language_model)
    could only solve tasks for which their training was designed.
  prefs: []
  type: TYPE_NORMAL
- en: '[GPT-3](https://arxiv.org/abs/2005.14165) and today‚Äôs [LLMs](https://en.wikipedia.org/wiki/Large_language_mode),
    on the other hand, show a new capability: the ability to learn new skills and
    solve new tasks simply by providing new examples in the input (prompt). Also,
    in this case, we are not training the model; there is no [gradient update](https://en.wikipedia.org/wiki/Gradient_descent)
    or change in model parameters. This skill is called [In-Context Learning (ICL)](https://en.wikipedia.org/wiki/Prompt_engineering).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b4e3700d502d9da8a95015beac0410c8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2005.14165)'
  prefs: []
  type: TYPE_NORMAL
- en: To be more specific, the way to interact with a model is to provide natural
    language instructions in a prompt. Although this may seem limited, different examples
    up to a certain number of [tokens](https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/tokens)
    ([context windows](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/))
    can be entered in a prompt. In addition, despite being placed in this textual
    template, it can also allow the model [to solve mathematical exercises](https://arxiv.org/abs/2212.10535).
    In fact, in the prompt, we can insert examples of word corrections, arithmetic
    exercises, translations, programming, and whatnot.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/668ff5c0b2b29f0af73c35b6890623ca.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2301.00234)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can give a formal definition of what is [ICL](https://en.wikipedia.org/wiki/Prompt_engineering):'
  prefs: []
  type: TYPE_NORMAL
- en: In-context learning is a paradigm that allows language models to learn tasks
    given only a few examples in the form of demonstration. ([source](https://arxiv.org/abs/2301.00234))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Simply put, by giving a model a list of input-output pairs that demonstrate
    a task, the model reads the training examples to figure out the input and output
    distribution, manages to map the inputs and outputs, and generates an appropriate
    response.
  prefs: []
  type: TYPE_NORMAL
- en: '[As shown in this study](https://arxiv.org/abs/2211.09066) that this simple
    idea helps the model to perform certain tasks more easily. Explaining the model
    with unambiguous instructions on how to perform the tasks allows the model to
    better understand and perform the tasks more easily. Using these few examples
    is then competitive ([ICL](https://en.wikipedia.org/wiki/Prompt_engineering))
    is competitive against training models with many more labeled data.'
  prefs: []
  type: TYPE_NORMAL
- en: This has led to the emergence of various strategies to exploit [ICL](https://en.wikipedia.org/wiki/Prompt_engineering)
    (prompt engineering) since changing the prompt allows for better performance than
    [having to do fine-tuning for a specific task](https://arxiv.org/abs/2211.09066).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8c175a1a8e3657a5f80c1019545eaedb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2211.09066)'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/speak-to-me-how-many-words-a-model-is-reading-331e3af86d27?source=post_page-----55bde1180610--------------------------------)
    [## Speak to me: How many words a model is reading'
  prefs: []
  type: TYPE_NORMAL
- en: Why and how to overcome the inner limit of a Large Language Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/speak-to-me-how-many-words-a-model-is-reading-331e3af86d27?source=post_page-----55bde1180610--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '[This behavior also seems to emerge only at scale](https://arxiv.org/abs/2206.07682),
    i.e., it appears that [ICL](https://en.wikipedia.org/wiki/Prompt_engineering)
    emerges [only with a certain number of parameters](https://en.wikipedia.org/wiki/Neural_scaling_law).
    In fact, for some capabilities, it appears that the model has random performance
    up to a certain number of parameters, and then abruptly its performance improves.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fd41f594bcec315652fa7c8f69a46357.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2206.07682)'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9?source=post_page-----55bde1180610--------------------------------)
    [## Emergent Abilities in AI: Are We Chasing a Myth?'
  prefs: []
  type: TYPE_NORMAL
- en: Changing Perspective on Large Language Models emerging properties
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9?source=post_page-----55bde1180610--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'In brief, behavior is both researched and studied because [it has definite
    advantages](https://arxiv.org/abs/2301.00234):'
  prefs: []
  type: TYPE_NORMAL
- en: The examples are written in natural language, so communication with the template
    is interpretable and understandable to humans. it is much easier to integrate
    human knowledge because you only need to change the prompt.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also [in context learnin](https://en.wikipedia.org/wiki/Prompt_engineering)g
    it remembers how humans learn because it recalls the process of learning by [analogy](https://en.wikipedia.org/wiki/Analogy).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: it is training-free, we do not have to train the model (unlike [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning)).
    This means it is much cheaper as a [computational cost](https://en.wikipedia.org/wiki/Computational_complexity).
    This is really efficient since the skill can be acquired instantly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It also means that the model can be used as-a-service, and can thus be deployed
    for many tasks. In fact, the tasks can be taught by everyday users.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ICL](https://en.wikipedia.org/wiki/Prompt_engineering) provides the model
    to [generalize](https://developers.google.com/machine-learning/crash-course/generalization/video-lecture),
    allowing the model to learn underlying patterns and rules that are present in
    the examples and then apply them to new situations. Moreover, it provides the
    model with versatility, since it can be applied to many different types of skills.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It looks amazing, but how it works?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The mystery of ICL: how does it work?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/c9a184a494833d37feeb23b3928d720c.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [ùì¥ùìòùì°ùìö ùïùùî∏ùïÄ](https://unsplash.com/@kirklai?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: ‚ÄúUsually, if you want to fine-tune these models, you need to collect domain-specific
    data and do some complex engineering. But now we can just feed it an input, five
    examples, and it accomplishes what we want. So, in-context learning is an unreasonably
    efficient learning phenomenon that needs to be understood,‚Äù Aky√ºrek says. ([source](https://news.mit.edu/2023/large-language-models-in-context-learning-0207))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'As much as [ICL](https://en.wikipedia.org/wiki/Prompt_engineering) seems almost
    magical, it also has its limitations. [GPT-3](https://en.wikipedia.org/wiki/GPT-3)
    for example had shown what seemed incredible reasoning capabilities. Yet some
    datasets that required reasoning, such as the Winograd dataset did not show [ICL](https://en.wikipedia.org/wiki/Prompt_engineering):'
  prefs: []
  type: TYPE_NORMAL
- en: A Winograd schema is a pair of sentences that differ in only one or two words
    and that contain an ambiguity that is resolved in opposite ways in the two sentences
    and requires the use of world knowledge and reasoning for its resolution. ([source](https://cs.nyu.edu/~davise/papers/WinogradSchemas/WS.html))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In fact, there was no improvement with the use of a few examples in the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/77e88cac98ec8e0f298a32e423cc5387.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2005.14165)'
  prefs: []
  type: TYPE_NORMAL
- en: 'These facts and other seemingly contradictory behaviors have led researchers
    to ask: **where does ICL originate? Why works better than fine-tuning? Can ICL
    be improved by changing the prompt?**'
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, one must remember that most skills are learned during pre-training.
    The first step of training an [LLM](https://en.wikipedia.org/wiki/Large_language_model)
    that requires huge amounts of text and is typically conducted by simply asking
    the model to predict a word in a sequence given the previous part of the sequence.
    This step is the most expensive, time-consuming, and resource-intensive one. During
    [alignment](https://jasonwei20.github.io/files/FLAN%20talk%20external.pdf) (the
    transition from GPT 3.5 to [ChatGPT](https://openai.com/blog/chatgpt)) the model
    only improves its ability to exploit this knowledge and how to interact with humans.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/the-infinite-babel-library-of-llms-90e203b2f6b0?source=post_page-----55bde1180610--------------------------------)
    [## The Infinite Babel Library of LLMs'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open-source, data, and attention: How the future of LLMs will change'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/the-infinite-babel-library-of-llms-90e203b2f6b0?source=post_page-----55bde1180610--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Use what you see: the pre-training impact on ICL'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'During pre-training, [LLMs](https://en.wikipedia.org/wiki/Large_language_model)
    are thus exposed to an enormous amount of text: from Wikipedia, books (fiction
    and nonfiction), scientific articles, tweets, Reddit posts, blog posts, internet
    dumps, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: '[In a 2022 article](https://arxiv.org/abs/2212.10559), it was proposed that
    [ICL](https://en.wikipedia.org/wiki/Prompt_engineering) can be considered a kind
    of implicit [fine-tuning](https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)).
    The main difference is that while [ICL](https://en.wikipedia.org/wiki/Prompt_engineering)
    is produced only by forward computation while fine-tuning also has a backpropagation
    step (in which parameters are updated). **This confirms that** [**ICL**](https://en.wikipedia.org/wiki/Prompt_engineering)
    **must originate during pretraining, but how does it impact pretraining?**'
  prefs: []
  type: TYPE_NORMAL
- en: '[As one article showed](https://aclanthology.org/2022.naacl-main.380/), the
    pretraining dataset is critical for a model to develop [ICL](https://en.wikipedia.org/wiki/Prompt_engineering).
    According to the authors, the source domain is more important than the size of
    the [corpus](https://en.wikipedia.org/wiki/Text_corpus). Also, putting several
    [corpora](https://en.wikipedia.org/wiki/Text_corpus) together can lead to the
    emergence of [ICL](https://en.wikipedia.org/wiki/Prompt_engineering) (if two corpora
    alone do not give ICL, joining them together can give [ICL](https://en.wikipedia.org/wiki/Prompt_engineering)).
    Another important factor is the domain relevance of the [corpus](https://en.wikipedia.org/wiki/Text_corpus):
    training only on a news [corpus](https://en.wikipedia.org/wiki/Text_corpus) allows
    relative in-context learning ability on a news-related downstream task. Finally,
    the authors note, that although [perplexity](https://en.wikipedia.org/wiki/Perplexity)
    (one of the most commonly used metrics for tracking [LLMs](https://en.wikipedia.org/wiki/Large_language_model))
    and [ICL](https://en.wikipedia.org/wiki/Prompt_engineering) generally correlate,
    perplexity alone does not reflect a model‚Äôs ability for [ICL](https://en.wikipedia.org/wiki/Prompt_engineering)
    (comparing two [LLMs](https://en.wikipedia.org/wiki/Large_language_model), the
    model with the lowest perplexity is not necessarily the one with the highest [ICL](https://en.wikipedia.org/wiki/Prompt_engineering)).'
  prefs: []
  type: TYPE_NORMAL
- en: '[This was further confirmed](https://arxiv.org/abs/2205.05055) by the fact
    that the dataset must be several in rare classes to allow ICL. According to the
    authors, the training data examples should appear in clusters (i.e. there should
    be several for each class) and be a certain variety of classes.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fe8746fa845491858ad4fcabf22a1b63.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2205.05055)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Another study states](https://arxiv.org/abs/2303.07895) instead that in-context
    learning appears when the pretraining distribution (the training data) is an implicit
    mixture. The examples for pretraining are extracted from a mixture of tasks, and
    the association between examples and tasks is latent. So then once the model is
    trained, it manages on its own to uncover the latent task in the demonstration.
    For example, a series of tweets with positive and negative content represent a
    latent task of sentiment analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: In short, these articles claim that [ICL](https://en.wikipedia.org/wiki/Prompt_engineering)
    appears if the dataset is diverse, they present a diverse number of class numbers
    (but simultaneously several examples per class), they cover multiple domains,
    and best if these examples represent a latent task of [NLP](https://en.wikipedia.org/wiki/Natural_language_processing).
    Since [LLMs](https://en.wikipedia.org/wiki/Large_language_model) are generally
    trained with a huge amount of text, these premises are met.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/say-once-repeating-words-is-not-helping-ai-58f38035f66e?source=post_page-----55bde1180610--------------------------------)
    [## Say Once! Repeating Words Is Not Helping AI'
  prefs: []
  type: TYPE_NORMAL
- en: How and why is repeating tokens harming LLMs? Why is this a problem?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/say-once-repeating-words-is-not-helping-ai-58f38035f66e?source=post_page-----55bde1180610--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'How you use what you learn: can you recall what you learned?'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Some researchers have attempted [to develop a framework](https://arxiv.org/abs/2111.02080)
    for understanding how [ICL](https://en.wikipedia.org/wiki/Prompt_engineering)
    emerges during pretraining. According to the authors, an [LLM](https://en.wikipedia.org/wiki/Large_language_model)
    uses [ICL](https://en.wikipedia.org/wiki/Prompt_engineering) to ‚Äúlocate‚Äù concepts
    that are needed to perform the task. The idea is that during training the model
    acquires latent concepts and then finds it again during [ICL](https://en.wikipedia.org/wiki/Prompt_engineering).
    To find them again, the [LLM](https://en.wikipedia.org/wiki/Large_language_model)
    can use all or some components of a prompt: format, inputs, outputs, and input-output
    mapping.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As explained, [in a blog post by the authors](https://ai.stanford.edu/blog/understanding-incontext/),
    the model learns several concepts during training, after which the model uses
    the training examples to understand that the task in the prompt required either
    [sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) or [topic
    classification](https://developers.google.com/machine-learning/guides/text-classification?hl=it)
    and at this point applies the mapping to the test input:'
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we study how in-context learning can emerge when pretraining
    documents have long-range coherence. Here, the LM must infer a latent document-level
    concept to generate coherent next tokens during pretraining. At test time, in-context
    learning occurs when the LM also infers a shared latent concept between examples
    in a prompt. ([source](https://arxiv.org/abs/2111.02080))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/39f467017195bea30d100863084d5721.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2111.02080)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Yes but what is a concept?** for the authors is ‚Äú*a latent variable that
    contains various document-level statistics.*‚Äù So a concept for a topic (for example,
    news) is the distribution of words (what words are used), format (how they are
    written), relationships between articles and topics, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: The body of texts that are provided to the model are not random words, but the
    texts have their own [internal coherence](https://en.wikipedia.org/wiki/Coherence_(linguistics)).
    In other words, similar texts have similar semantic information (the same topic)
    and formatting (alternate programming documentation explanations and code snippets).
    By learning to predict a word given those precedences, the [LLM](https://en.wikipedia.org/wiki/Large_language_model)
    also models internal consistency and allows it to infer latent concepts that are
    in the prompt
  prefs: []
  type: TYPE_NORMAL
- en: 'In the authors'' words:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Pretrain**: To predict the next token during pretraining, the LM must
    infer (‚Äúlocate‚Äù) the latent concept for the document using evidence from the previous
    sentences.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**2\. In-context learning**: If the LM also infers the *prompt concept* (the
    latent concept shared by examples in the prompt) using in-context examples in
    the prompt, then in-context learning occurs! ([source](https://ai.stanford.edu/blog/understanding-incontext/))'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: So for the authors, this process of ‚Äúlocating‚Äù can be seen as [Bayesian inference](https://en.wikipedia.org/wiki/Bayesian_inference),
    in which the [LLMs](https://en.wikipedia.org/wiki/Large_language_model) infer
    concepts in the prompt (a concept that is shared by all the examples presented
    to it in the input prompt). Once he has inferred the concept he can then produce
    the correct answer
  prefs: []
  type: TYPE_NORMAL
- en: 'In formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8718900b9ea981fa05ea2b6b7117f0bf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2111.02080)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ask nicely: effect of the prompt'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In recent work, [Min et al.](https://arxiv.org/abs/2202.12837) defined the characteristics
    of a prompt for ICL and how the various components of the prompt affect the performance
    of the model in doing ICL
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3dda25ecd69fc81d7b1be593488a2917.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2202.12837)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering a demonstration as input-output pairs ( (x1, y1)‚Ä¶(xk, yk)) there
    are four formal aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The input-label mapping**. an input x is correctly paired with its label
    y.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The distribution of the input text**, the distribution from where input x
    is extracted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The label space** is the space of the y outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The format**, specifically the pairing of the input-output pairs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[For the authors](https://arxiv.org/abs/2202.12837), the format, the distribution
    of the input, and label spaces are important. In contrast, input-label mapping
    matters little to [ICL](https://en.wikipedia.org/wiki/Prompt_engineering). [According
    to Stanford AI Lab](https://ai.stanford.edu/blog/understanding-incontext/), this
    would stem from the fact, that the model is already exposed to input-output matching
    during pretraining so it would not need the input-label mapping in the demonstration.
    Instead, the other elements are needed to be able to locate the concepts it has
    learned (in short perform [Bayesian inference](https://en.wikipedia.org/wiki/Bayesian_inference)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6b897bdf4114db6d6f1cb57c2d7ca69d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2202.12837)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Another paper states](https://arxiv.org/abs/2205.12685) that actually input-label
    mapping, while [according to another](https://arxiv.org/abs/2303.03846) it is
    true that it is important but if the model is large enough it can learn the mapping
    on its own.'
  prefs: []
  type: TYPE_NORMAL
- en: For other authors, it is important that the demonstrations are different, simple,
    and similar anyway (at least in terms of structure). For another paper, the [order
    of the demonstrations is important](https://aclanthology.org/2022.acl-long.556/).
    Whereas, [Liu et al](https://aclanthology.org/2022.deelio-1.10.pdf), show that
    the choice of examples strongly impacts [ICL](https://en.wikipedia.org/wiki/Prompt_engineering).
    So one should choose examples that are close to an [embedding space](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture).
    In fact, one technique that shows results is when one provides a question to embed
    it and looks for examples that are close in distance in the [embedding](https://en.wikipedia.org/wiki/Embedding).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1192be99f7221b72a5ae571369b3d2a8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://aclanthology.org/2022.deelio-1.10.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://levelup.gitconnected.com/the-ai-college-student-goes-back-to-the-bench-daa6d9bdfb14?source=post_page-----55bde1180610--------------------------------)
    [## The AI college student goes back to the bench'
  prefs: []
  type: TYPE_NORMAL
- en: How LLM can solve college exams and why this is important
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/the-ai-college-student-goes-back-to-the-bench-daa6d9bdfb14?source=post_page-----55bde1180610--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: A closed look to attention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have seen so far the role of the training dataset and the prompt, now it
    is time to closer look at the effect of architecture. An [LLM](https://en.wikipedia.org/wiki/Large_language_model)
    is a transformer, and the [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    is mainly based on [multi-head self-attention](https://en.wikipedia.org/wiki/Attention_(machine_learning)).
    Because [ICL](https://en.wikipedia.org/wiki/Prompt_engineering) is one of the
    most interesting behaviors of [LLMs](https://en.wikipedia.org/wiki/Large_language_model),
    many authors have focused on trying to find a mechanistic answer to how [ICL](https://en.wikipedia.org/wiki/Prompt_engineering)
    occurs:'
  prefs: []
  type: TYPE_NORMAL
- en: If we can understand the internal structures that cause Transformer models to
    produce the outputs they do, then we may be able to address current safety problems
    more systematically, as well as anticipating safety problems in future more powerful
    models. ([source](https://arxiv.org/abs/2209.11895))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/470625884f8f80cbbb190a076b5eeb81.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/1706.03762)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Researchers at Anthropic identified circuits](https://arxiv.org/abs/2209.11895)
    they called **induction heads**. An induction head is a circuit consisting of
    [two attention heads](https://en.wikipedia.org/wiki/Attention_(machine_learning))
    in different layers cooperating with each other to copy or complete patterns.
    Basically, the first attention head copies information from the previous token
    to the next one. The second attention head then has information about what happened
    previous to the present token. Then this mechanism can search the sequence where
    the present token A and sees the next token B, so the pattern once it sees A is
    more likely to produce output B.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1c3b8ddbce7d102d33057ae158f67070.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2209.11895)'
  prefs: []
  type: TYPE_NORMAL
- en: '[For the authors](https://arxiv.org/abs/2209.11895), however, it is not a simple
    copying mechanism. In fact, in [inductive reasoning](https://en.wikipedia.org/wiki/Inductive_reasoning),
    we can infer that A is followed by B, if previously in context we saw that A most
    likely followed by B. For the authors then these induction heads crystallize this
    inference mechanism, which is not based on the training data but on the context:
    [A][B]‚Ä¶[A]‚Üí[B]'
  prefs: []
  type: TYPE_NORMAL
- en: '[For Anthropic](https://arxiv.org/abs/2209.11895) these induction heads play
    an important role in [ICL](https://en.wikipedia.org/wiki/Prompt_engineering).
    In fact, the fact that they can learn and repeat arbitrary sequences can be thought
    of as a simplified form of few-shot learning. In [a large model](https://en.wikipedia.org/wiki/Large_language_model),
    this effect is amplified, since they can work on abstract representations. Thus:
    ‚Äú, t*he very same heads that do this sequence copying also take on a more expanded
    role of analogical sequence copying or in-context nearest neighbors*‚Äù.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, this mechanism is also interesting because it also promotes another kind
    of sequence completion: [A*][B*] ‚Ä¶ [A] ‚Üí [B]. In this case A* and B* are not the
    same tokens A and B but tokens that are similar in [embedding space](https://en.wikipedia.org/wiki/Embedding)
    (for example, the same word in different languages).'
  prefs: []
  type: TYPE_NORMAL
- en: These induction heads seem to appear as the [LLM](https://en.wikipedia.org/wiki/Large_language_model)
    improves its skill in ICL. Also, [for Anthrophic](https://arxiv.org/abs/2209.11895)
    in small [LMs](https://en.wikipedia.org/wiki/Large_language_model), one can observe
    this relationship with [ICL](https://en.wikipedia.org/wiki/Prompt_engineering)
    (for them the induction heads are the driver of [ICL](https://en.wikipedia.org/wiki/Prompt_engineering)).
    In addition, reverse engineering of these induction heads can be done for them,
    and this seems like a promising line of research to understand how they are formed
    and how they impact [ICL](https://en.wikipedia.org/wiki/Prompt_engineering).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/89d1bccf2e12244a91bab946d0f5db4d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2209.11895)'
  prefs: []
  type: TYPE_NORMAL
- en: '[ICL](https://en.wikipedia.org/wiki/Prompt_engineering) in each case is linked
    and emerges through attention. This has a quadratic (in computational terms) cost,
    though. Several models with simplified forms of [attention](https://en.wikipedia.org/wiki/Attention_(machine_learning))
    (linear or logarithmic) were tested; however, this led to a decrease in expressiveness
    and impacted the [ICL](https://en.wikipedia.org/wiki/Prompt_engineering) ability
    of the model. Therefore, although an alternative to [multi-head self-attention](https://en.wikipedia.org/wiki/Attention_(machine_learning))
    is sought, the authors take care that their proposed model is capable of [ICL](https://en.wikipedia.org/wiki/Prompt_engineering).'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://levelup.gitconnected.com/welcome-back-80s-transformers-could-be-blown-away-by-convolution-21ff15f6d1cc?source=post_page-----55bde1180610--------------------------------)
    [## Welcome Back 80s: Transformers Could Be Blown Away by Convolution'
  prefs: []
  type: TYPE_NORMAL
- en: The Hyena model shows how convolution could be faster than self-attention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/welcome-back-80s-transformers-could-be-blown-away-by-convolution-21ff15f6d1cc?source=post_page-----55bde1180610--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Learning to learn the context
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Clearly, since the [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    is trained through [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)
    there is a relationship between the latter and [ICL](https://en.wikipedia.org/wiki/Prompt_engineering).
    Using [linear regression](https://en.wikipedia.org/wiki/Linear_regression) as
    a starting point, [Aky√ºrek suggests](https://arxiv.org/abs/2211.15661) that transformers
    implicitly treat [ICL](https://en.wikipedia.org/wiki/Prompt_engineering) as an
    optimization problem.
  prefs: []
  type: TYPE_NORMAL
- en: '[Oswald](https://arxiv.org/abs/2212.07677) showed that [transformer layers](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    can theoretically implement [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)
    on the in-context data. According to [Oswald](https://arxiv.org/abs/2212.07677),
    in-context learning mimics [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)
    in certain cases. [This paper](https://arxiv.org/abs/2212.07677) showed that there
    is an [ICL](https://en.wikipedia.org/wiki/Prompt_engineering) and gradient descent
    relationship.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/128ac013ae9e45af341a022694e50030.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image source: [here](https://arxiv.org/abs/2212.07677)'
  prefs: []
  type: TYPE_NORMAL
- en: The results above, and [Aky√ºrek‚Äôs results](https://arxiv.org/abs/2211.15661),
    mean that models doing in-context learning are not just matching previous patterns,
    but instead are also learning how to perform other tasks (an extension of what
    was said with induction heads). In fact, [Aky√ºrek](https://arxiv.org/abs/2211.15661)
    provided prompts containing synthetic data to prevent the model from having already
    seen the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[Aky√ºrek‚Äôs hypothesis](https://arxiv.org/abs/2211.15661) is then the models
    internally perform sort of machine learning algorithms (which in part echoes but
    extends the idea that the model does Bayesian inference). In the article, they
    state that the model implements in its hidden states a linear model, and this
    is learned during training.'
  prefs: []
  type: TYPE_NORMAL
- en: ‚ÄúIn this case, we tried to recover the actual solution to the linear model,
    and we could show that the parameter is written in the hidden states. This means
    the linear model is in there somewhere,‚Äù Aky√ºrek says. ([source](https://news.mit.edu/2023/large-language-models-in-context-learning-0207))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[In an intriguing experiment](https://ai.googleblog.com/2023/05/larger-language-models-do-in-context.html),
    Google tested whether models via [ICL](https://en.wikipedia.org/wiki/Prompt_engineering)
    can override previous [prior knowledge](https://en.wikipedia.org/wiki/Prior_knowledge_for_pattern_recognition),
    for the authors this is also an example of the emergent property of broad [LLMs](https://en.wikipedia.org/wiki/Large_language_model).'
  prefs: []
  type: TYPE_NORMAL
- en: In one of the experiments, the authors performed regular [ICL](https://en.wikipedia.org/wiki/Prompt_engineering),
    flipped ICL (where labels are flipped), and semantically-unrelated label ICL (SUL-ICL)
    where labels are words that are not semantically related.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2807e4985bd1bae2dd2215dc62c8d9df.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2303.03846)'
  prefs: []
  type: TYPE_NORMAL
- en: This article shows some interesting things. When the labels are flipped (but
    the ground-truth evaluation is kept the same) if the model is able to override
    its prior knowledge it should have a decrease. The result is that the performance
    of small models stays flat, while there is a drop for [large models](https://en.wikipedia.org/wiki/Large_language_model).
  prefs: []
  type: TYPE_NORMAL
- en: These results indicate that large models can override prior knowledge from pre-training
    when contradicting input-label mappings are presented in-context. Small models
    can‚Äôt do this, making this ability an emergent phenomena of model scale. ([source](https://ai.googleblog.com/2023/05/larger-language-models-do-in-context.html))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/15b8229c8301c802ce1b0c2a46c4b578.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2303.03846)'
  prefs: []
  type: TYPE_NORMAL
- en: Second, the model can also learn from input-label mappings when provided in
    the demonstration of semantically-irrelevant labels (‚Äúfoo/bar‚Äù instead of ‚Äúnegative/positive‚Äù
    for sentiment analysis). A model that relies only on [prior knowledge](https://en.wikipedia.org/wiki/Prior_knowledge_for_pattern_recognition)
    should have a performance drop because it cannot exploit the semantic meaning
    of labels for predictions. In fact, small models have a drop in prediction, while
    [LLMs](https://en.wikipedia.org/wiki/Large_language_model) do not. For the authors,
    this means that while small models rely on [prior knowledge](https://en.wikipedia.org/wiki/Prior_knowledge_for_pattern_recognition),
    ‚Äú*large models, on the other hand, have the ability to learn input-label mappings
    in context when the semantic nature of labels is removed.*‚Äù
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d166a16fd271cee5b30b5c7c55da4a41.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2303.03846)'
  prefs: []
  type: TYPE_NORMAL
- en: '[The authors also took a look](https://arxiv.org/abs/2303.03846) at what is
    the effect of instruction tuning on ICL. During instruction tuning, instructions
    are given to the model that often contains questions and answers. So this process
    involves natural language labels, and the authors wondered whether it impacts
    an [LLM‚Äôs](https://en.wikipedia.org/wiki/Large_language_model) ability to learn
    input-label mappings or exploit semantic prior knowledge. The experiments show
    that: ‚Äúinstruction tuning improves the ability to learn input-label mappings,
    it strengthens the usage of semantic prior knowledge more.‚Äù'
  prefs: []
  type: TYPE_NORMAL
- en: 'So these results show that it is not only the architecture, the amount of data,
    and the prompt that influence ICL, but [for Google also the number of parameters
    themselves](https://ai.googleblog.com/2023/05/larger-language-models-do-in-context.html):'
  prefs: []
  type: TYPE_NORMAL
- en: These results underscore how the in-context learning behavior of language models
    can change depending on the scale of the language model, and that larger language
    models have an emergent ability to map inputs to many types of labels, a form
    of true symbolic reasoning in which input‚Äìlabel mappings can be learned for arbitrary
    symbols. ([source](https://arxiv.org/pdf/2303.03846.pdf))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[](https://salvatore-raieli.medium.com/scaling-isnt-everything-how-bigger-models-fail-harder-d64589be4f04?source=post_page-----55bde1180610--------------------------------)
    [## Scaling Isn‚Äôt Everything: How Bigger Models Fail Harder'
  prefs: []
  type: TYPE_NORMAL
- en: Are Large Language Models really understanding programming languages?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: salvatore-raieli.medium.com](https://salvatore-raieli.medium.com/scaling-isnt-everything-how-bigger-models-fail-harder-d64589be4f04?source=post_page-----55bde1180610--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions, challenges, and perspective
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/986b2920355032f6fc74247aeef7d0a9.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Nadine Shaabana](https://unsplash.com/@nadineshaabana?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: ‚ÄúSeparate text from context and all that remains is a con.‚Äù ‚Äï **Stewart Stafford**
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ‚ÄúWords are never good or bad on their own, context makes them so.‚Äù ‚Äï **Abhijit
    Naskar**
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In-context learning is one of the most interesting and elusive behaviors of
    [LLMs](https://en.wikipedia.org/wiki/Large_language_model). First admired with
    the publication of GPT-3, it has excited the community about its possible applications.
  prefs: []
  type: TYPE_NORMAL
- en: ICL in simple terms is the ability to learn from analogy. It only takes a few
    examples in a demonstration for the model to make a prediction. Which allows the
    model unprecedented versatility and the possibility of developing endless applications.
  prefs: []
  type: TYPE_NORMAL
- en: Despite this, we still do not understand precisely how it originates during
    training. We have seen the importance of training data, the prompt, or attention
    itself. Today, with the idea of wanting to replace attention-based models with
    new architectures we need to understand how to preserve ICL.
  prefs: []
  type: TYPE_NORMAL
- en: 'Research in ICL is very active, some of the lines of research are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**New Pretraining Strategie**s, as mentioned if a training strategy increases
    performance classically (decrease in perplexity) it does not mean that it increases
    the ICL skills of the model. Therefore, focused strategies are sought to increase
    a model‚Äôs ICL skills.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ICL Ability Distillation**, ICL seems to emerge with the scale of the model,
    but if one were able to distill these skills into smaller models we would have
    savings in computational cost, memory, and infrastructure. Therefore, distillation
    seems promising for smaller models with ICL. [Preliminary studies look promising](https://arxiv.org/abs/2212.08410).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ICL Robustness.** As we have seen ICL skills are not stable, permutations
    and changes in the format of the demonstration impact ICL. [In one study it is
    shown](https://arxiv.org/abs/2209.07661) that increasing robustness comes at the
    cost of decreasing accuracy, so we need studies that delve into how ICL works.
    A better theoretical understanding can help develop a more robust ICL.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ICL Efficiency and Scalability.** ICL requires different examples in the
    demonstration. In theory, more examples improve ICL. Increasing the number of
    examples has a computational cost, which comes from calculating attention (efficiency).
    The other challenge is that you cannot add more examples than the context window
    allows (scalability). [As we saw in a previous article](/speak-to-me-how-many-words-a-model-is-reading-331e3af86d27),
    research has been very active in how to extend the context window (and what strategies
    have been used), although it is unclear whether the model can then exploit it.
    Also, in some cases, inverse scaling was seen, where the model instead of following
    in-context instruction regurgitated memorized data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another line of research is the development of techniques that can improve ICL
    by acting on the format of the prompt. Several interesting approaches have been
    proposed over time (including Chain of thought (COT), Self-consistency COT, Tree
    of Thoughts, and so on). These approaches have shown success in being able to
    improve model performance for mathematical exercises and other reasoning problems.
    All this is done simply through modifications of the prompt. In this article,
    I have focused on more mechanistic aspects of the model, training data, the prompt,
    and how ICL emerges. In the next article, I will discuss these approaches in detail.
  prefs: []
  type: TYPE_NORMAL
- en: What do guys think? Let me know in the comments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you have found this interesting:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*You can look for my other articles, you can also* [***subscribe***](https://salvatore-raieli.medium.com/subscribe)
    *to get notified when I publish articles, you can* [***become a Medium member***](https://medium.com/@salvatore-raieli/membership)
    *to access all its stories (affiliate links of the platform for which I get small
    revenues without cost to you) and you can also connect or reach me on*[***LinkedIn***](https://www.linkedin.com/in/salvatore-raieli/)***.***'
  prefs: []
  type: TYPE_NORMAL
- en: '*Here is the link to my GitHub repository, where I am planning to collect code
    and many resources related to machine learning, artificial intelligence, and more.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/SalvatoreRa/tutorial?source=post_page-----55bde1180610--------------------------------)
    [## GitHub - SalvatoreRa/tutorial: Tutorials on machine learning, artificial intelligence,
    data science‚Ä¶'
  prefs: []
  type: TYPE_NORMAL
- en: Tutorials on machine learning, artificial intelligence, data science with math
    explanation and reusable code (in python‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/SalvatoreRa/tutorial?source=post_page-----55bde1180610--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Reference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here is the list of the principal references I consulted to write this article,
    only the first name for an article is cited. I suggest also them if you want to
    deepen on the topic.
  prefs: []
  type: TYPE_NORMAL
- en: Brown, 2020, Language Models are Few-Shot Learners, [link](https://arxiv.org/abs/2005.14165)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dong, 2022, A Survey on In-context Learning, [link](https://arxiv.org/abs/2301.00234)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Zhao, A Survey of Large Language Models, [link](https://arxiv.org/abs/2303.18223)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Xie, 2022, How does in-context learning work? A framework for understanding
    the differences from traditional supervised learning. [link](https://ai.stanford.edu/blog/understanding-incontext/)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wei, 2022, Emergent Abilities of Large Language Models, [link](https://arxiv.org/abs/2206.07682)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Zhou, 2022, Teaching Algorithmic Reasoning via In-context Learning, [link](https://arxiv.org/abs/2211.09066)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Vinita Silaparasetty](https://medium.com/u/8f22c49c614?source=post_page-----55bde1180610--------------------------------),
    What is Prompt Engineering?, [link](https://medium.com/@vinitasilaparasetty/what-is-prompt-engineering-8221e0aa619d)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Fareed Khan](https://medium.com/u/b856005e5ecd?source=post_page-----55bde1180610--------------------------------),
    Prompt Engineering Complete Guide, [link](https://medium.com/@fareedkhandev/prompt-engineering-complete-guide-2968776f0431)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Paul DelSignore](https://medium.com/u/6202cb40e768?source=post_page-----55bde1180610--------------------------------),
    The Dark Side Of Prompt Engineering, [link](https://medium.com/the-generator/the-dark-side-of-prompt-engineering-33b8087ffd59)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Babar M Bhatti](https://medium.com/u/10dee34829b?source=post_page-----55bde1180610--------------------------------),
    The Art and Science of Crafting Effective Prompts for LLMs, [link](https://thebabar.medium.com/the-art-and-science-of-crafting-effective-prompts-for-llms-e04447e8f96a)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dai, 2022, Why Can GPT Learn In-Context? Language Models Implicitly Perform
    Gradient Descent as Meta-Optimizers, [link](https://arxiv.org/abs/2212.10559)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Shin, 2022, On the Effect of Pretraining Corpora on In-context Learning by a
    Large-scale Language Model, [link](https://aclanthology.org/2022.naacl-main.380/)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Cameron R. Wolfe, Ph.D.](https://medium.com/u/28aa6026c553?source=post_page-----55bde1180610--------------------------------),
    Language Model Scaling Laws and GPT-3, [link](/language-model-scaling-laws-and-gpt-3-5cdc034e67bb)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Xie, 2021, An Explanation of In-context Learning as Implicit Bayesian Inference
    , [link](https://arxiv.org/abs/2111.02080)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Husz√°r, 2022, Implicit Bayesian Inference in Large Language Models, [link](https://www.inference.vc/implicit-bayesian-inference-in-sequence-models/)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Min, 2022, Rethinking the Role of Demonstrations: What Makes In-Context Learning
    Work?, [link](https://arxiv.org/abs/2202.12837)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chan, 2022, Data Distributional Properties Drive Emergent In-Context Learning
    in Transformers, [link](https://arxiv.org/abs/2205.05055)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Liu, 2023, What Makes Good In-Context Examples for GPT-3?, [link](https://aclanthology.org/2022.deelio-1.10/)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Priyanka](https://medium.com/u/29db232e8826?source=post_page-----55bde1180610--------------------------------),
    Perplexity of Language Models, [link](https://medium.com/@priyankads/perplexity-of-language-models-41160427ed72)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Olsson, 2022, In-context Learning and Induction Heads, link
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wies, 2023, The Learnability of In-Context Learning, [link](https://arxiv.org/abs/2303.07895)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Aky√ºrek, 2022, What learning algorithm is in-context learning? Investigations
    with linear models, [link](https://arxiv.org/abs/2211.15661)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Oswald, 2022, Transformers learn in-context by gradient descent, [link](https://arxiv.org/abs/2212.07677)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wei, 2023, Larger language models do in-context learning differently, [link](https://arxiv.org/abs/2303.03846)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Google blog, Larger language models do in-context learning differently, [link](https://ai.googleblog.com/2023/05/larger-language-models-do-in-context.html)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Zewe, Solving a machine-learning mystery, [link](https://news.mit.edu/2023/large-language-models-in-context-learning-0207)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kaddour, 2023, Challenges and Applications of Large Language Models, [link](https://arxiv.org/abs/2307.10169)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Magister, 2022, Teaching Small Language Models to Reason, [link](https://arxiv.org/abs/2212.08410)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chen, 2022, On the Relation between Sensitivity and Accuracy in In-context Learning,
    [link](https://arxiv.org/abs/2209.07661)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
