- en: Generative Models and the Dance of Noise and Structure
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/generative-models-and-the-dance-of-noise-and-structure-e72fe7494f4f](https://towardsdatascience.com/generative-models-and-the-dance-of-noise-and-structure-e72fe7494f4f)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Guide To Building Digital Dreamers
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://manuel-brenner.medium.com/?source=post_page-----e72fe7494f4f--------------------------------)[![Manuel
    Brenner](../Images/f62843c79a9b378494cb83caf3ddc792.png)](https://manuel-brenner.medium.com/?source=post_page-----e72fe7494f4f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e72fe7494f4f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e72fe7494f4f--------------------------------)
    [Manuel Brenner](https://manuel-brenner.medium.com/?source=post_page-----e72fe7494f4f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e72fe7494f4f--------------------------------)
    ·21 min read·Oct 7, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: I like to think about what the inhabitants of Renaissance Italy, aflame with
    passion for the possibilities of the human imagination and rationality, would
    have found most astounding about our present-day technology. [Leonardo da Vinci](https://medium.datadriveninvestor.com/there-is-no-clash-between-art-and-science-3028d0420fbe),
    dreaming of flying machines, would have surely been impressed by an Airbus 380
    soaring through the air, with passengers comfortably reclining in their chairs,
    watching movies, and complaining about the Wi-Fi not being fast enough.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: 'But out of all the technologies that would have seemed like witchcraft in medieval
    times, the wonders of generative AI might be among the most witchcrafty. What
    would Leonardo, after laboring endless years on the portrait of the Mona Lisa,
    have said if I showed him a device that could paint a portrait of a woman in his
    style in mere seconds? Lo and behold:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f16d0b3e7077c2029a697d822ccbdf4e.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
- en: Portrait of a woman in the style of Leonardo da Vinci, painted by DALL-E.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 'While admittedly, this woman does not smile quite as seductively and mysteriously
    as the real Mona Lisa (and looks, upon further inspection, somewhat ridiculous),
    many of us have encountered astonishing instances of AI generations: from [ultrarealistic
    images](https://twitter.com/mariswaran/status/1674505829456965633) to eerily convincing
    deep fakes of voices or even entire essays written by AI.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 'Generative AI models are the silicon equivalents of dreamers: they can envision
    something from nothing, **make meaning from noise**. They have learned to dance
    the dance of order and disorder. [They have already changed how we think about
    human creativity](/alphazero-and-the-beauty-of-the-artificial-mind-4ac7f220941a)
    and have opened the door to thousands of new applications, threatening entire
    industries and creating new ones.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: And we are just getting started, with most of these models still in their infancy.
    With the writings of ChatGPT, the images of DALL-E and Midjourney, and, most recently,
    generative models for music like Stability AI’s [StableAudio](https://stability.ai/stable-audio),
    we are looking at an era where an **increasing amount of the sensory signals we
    load into our brains on a daily basis are in some way altered or even fully generated
    by AI**.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/42861a27a943782ad8b169d1a2d4d26a.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
- en: “A painter at an easel, with splashes of chaotic noise on the left side of the
    canvas, gradually transforming into a structured, beautiful digital city on the
    right. Art style should be semi-realistic with a hint of surrealism. The lighting
    should be soft and diffused, creating a dream-like ambiance”. Prompt by Chat-GPT,
    painting by DALL-E.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I want to lift the lid off this magical black box, diving into
    the fundamental mechanisms of several classes of generative models (Helmholtz
    machines, Variational Autoencoders, Normalizing Flows, Diffusion Models, GANs
    and Transformer-based language models), shedding light on their inner workings,
    and exploring their origins in and connections to neuroscience and cognition.
    The topic is obviously too broad to be covered in one article (even though it
    became much longer than planned), so I tried to balance some technical details
    with a high-level overview, a coherent narrative and sources for further reading,
    so I hope that there is something in there for everybody.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: “What I cannot create, I do not understand.”
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '***— Richard Feynman***'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Where do we begin?
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s somewhat cliché to start with this frequently cited Feynman quote, but
    the man had a point: understanding is related to the act of creation, and so already
    in the early days of machine learning, building models that could understand became
    related to building models that could create. Turing’s famous test (also known
    as The Imitation Game) can be seen as a variant of this idea: if you convincingly
    manage to fake intelligence, you most likely have discovered something akin to
    the real thing.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Among the two most important early generative models are Boltzmann and Helmholtz
    machines.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: 'Helmholtz machines are particularly interesting, given their principles are
    tied to the incredibly prescient vision of German physicist Hermann von Helmholtz.
    Helmholtz realized at the end of the 19th century that perception is much better
    described as a process of unconscious inference from sensory data and prior knowledge,
    rather than an objective reflection of an objective reality: cognition is inherently
    probabilistic and influenced by noise, and strongly shaped by our expectations
    and biases. His ideas are increasingly relevant in modern neuroscience, i.e. via
    Karl Friston’s Free Energy Principle (who explicitly cites the Helmholtz machine
    as a source of inspiration) and the [Bayesian Brain Hypothesis](/the-bayesian-brain-hypothesis-35b98847d331).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7a38ec01e533c47c6e09f3de82154422.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
- en: According to the free energy principle, the brain interacts with the external
    world in an action-perception loop, trying to infer the hidden states of the world
    from its sensation and making its predictions come true through actions. Kfriston,
    CC BY-SA 3.0 <[https://creativecommons.org/licenses/by-sa/3.0](https://creativecommons.org/licenses/by-sa/3.0)>,
    via Wikimedia Commons
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: From a Bayesian perspective, the idea is that the brain maintains a generative
    model **p(x,z)** of the world, where **x** are sensory observations and **z**
    are hidden causes/latent explanations of those sensory observations that the brain
    tries to capture, reflecting uncertainty both in the world and in the model of
    the world. As we will see, many, but not all, generative models are formulated
    as probabilistic latent variable models.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: In the Bayesian language, given such a model, this boils down to the prior distribution
    over latent causes **p(z)** (my prior expectation of observing a lion is smaller
    than observing a dog if I live in NYC), the the overall likelihood of the observations
    **p(x)**, and the relationship between the sensory observations and hidden causes.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '**Parsing out the relationship between x and z is at the heart of much of generative
    modeling.**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'Their relationship is reflected in two important quantities: the posterior
    **p(z∣x)** and the likelihood **p(x|z)**, which are tied together according to
    Bayes’ famous law:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f34c917ecf269360329ad5df7505d8d6.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: 'The posterior **p(z∣x)** gives us the probability of a latent cause, given
    observations. We usually don’t have access to this, due to a problem called intractability:
    according to Bayes’ law, we require p(x) to compute it, which necessitates going
    through all possible latent causes and checking how they would explain **x:**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e0f622c4a5ca3dac19cd97da7b545b00.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
- en: If the model of the world is complex, these are high-dimensional integrals,
    so this is not efficient or downright impossible.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '**Inferring the posterior constitutes the fundamental challenge of many generative
    models.**'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: In Helmholtz Machines, the **posterior p(z∣x)** is estimated directly from data
    in a process called recognition, by learning an approximate posterior **q(x|z)**
    and trying to match it as closely to the true **p(z∣x)** as possible.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: 'The reverse direction of estimating the likelihood **p(x|z)** is usually much
    easier: given a particular latent variable **z**, it just tells us how likely
    the observations **x** is. For this we usually don’t need to integrate anything,
    but can simply run the model forward.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 'The likelihood is parameterized by the generative network: how can we produce
    an **x**, given a **z**? If I start with a hidden cause, what would its effect
    on the world look like? If I imagine a person, what would that person’s face look
    or voice sound like?'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: In most generative models, this is the part that is most relevant in practice
    (since it generates the image/text/audio). Once we have an idea of what the mapping
    from **z** to **x** looks like, we can generate samples by sampling a **z** and
    sending it through the generative network.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: In Helmholtz machines, both of these directions are parameterized via neural
    networks, which are trained in alternating order by the **Wake-Sleep-algorithm**,
    inspired by similar processes in human cognition, switching between comparing
    generated samples with the real world (wake) using the generative network, and
    mapping its own creation back to its latent states (dream) using the recognition
    network.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'Recognition Network z ← x: **q(z|x)**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 'Generative Network z→ x: **p(x|z)**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: The structure of the latent space can often aids with the interpretation of
    the learned models. [Disentangling latent representation](https://arxiv.org/abs/2001.04872)
    and aligning them with interpretable features is of interest in many practical
    applications, but also more generally for achieving more interpretable models.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: As a familiar example, say we are building a generative model of images of human
    faces. Following the structure of the Helmholtz machine, we map the images unto
    a latent space. We can then try to discover interesting axes of variation in that
    latent space.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: Once axis of interesting variation might be related to the age of the persons
    in the images. We can then enforce constraints on the latent space (either in
    a supervised setting by providing data labelled with ages, or in an unsupervised
    setting by identifying age within the learned latent features, assuming it leads
    to significant variation) such that one of its direction, **z_age**, encodes the
    age displayed on the embedded images.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Knowing this direction can then be used to alter the age of images. If another
    direction **z_beard** encodes beardedness, I can use the model to encode an image
    **x** via the recognition network q(z|x), obtain a z, transform it into a z’=z+a*z_age+b*z_beard,
    and send it back via the generative model p(x|z’) to see a bearded, older version
    of myself.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Models like OpenAI’s GLOW [let you play around with this on their website](https://openai.com/research/glow),
    but you will likely already be familiar with such applications e.g. in the Faceapp.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: All generative modeling boils down to (more or less closely related) variants
    of this, and while it has evolved significantly since the days of Helmholtz machines,
    the fundamental idea of capturing and reproducing the underlying structure of
    data in a probabilistic framework remains. I will now use these concepts to explain
    some of the most common versions of generative models that have come into the
    spotlight of AI research in the past decade.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '**Variational Autoencoders (VAEs)**'
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now the earth was formless and empty, And God said, ‘Let there be light,’ and
    there was light.
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '***— Genesis 1:1–5 (NIV)***'
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: VAEs were simultaneously formulated in 2013 by [Kingma](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&cd=&ved=2ahUKEwizlfGwp8qBAxW0wQIHHfqNDdMQFnoECBMQAQ&url=https%3A%2F%2Farxiv.org%2Fabs%2F1312.6114&usg=AOvVaw0lcWUpG5O19Y_RMGqNMkr2&opi=89978449)
    and [Rezende](https://proceedings.mlr.press/v32/rezende14.html), and have found
    a wide range of applications (from denoising to compression to time series data).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: 'They are a natural place to start since they are most similar in spirit to
    Helmholtz machines: they use both recognition (encoder) and generative (decoder)
    networks. As already mentioned before, the recognition network approximates the
    **posterior density** **p(z|x) by an approximate density q(z|x).**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1b457cf240263a1400026383404a5d7a.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
- en: VAEs are trained to minimize the negative **Evidence Lower Bound (ELBO),** which
    boils down to finding an approximate density **q(z|x)** that is as close to the
    true posterior **p(z|x)** as possible.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Here we simply approximate the distribution **q(z|x)** by parameterizing it
    in a way that is trainable via gradient-based methods. There can be many subtleties
    to parameterizing the distribution, but we often just assume it is approximately
    Gaussian, which means it has a mean **μ** and a covariance **σ**, constituting
    the free parameters of the model. These are directly learned by neural networks
    (putting training data **x** into an NN, the output is **μ**).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: In VAEs, the approximate posterior **q(z|x)** is used to draw one or many random
    samples, which are then plugged into the **ELBO,** which is defined as an expectation
    value over samples from **q:**
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/953493d9f445023a5a164ebd32064ea6.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: Since the (negative) **ELBO** constitutes a loss, we can compute gradients,
    which in turn lets gradient descent do its magic.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the posterior can be quite complex, it can in practice be hard to compute
    well-behaved gradients from it, since they often have high variance, requiring
    many samples. The reparameterization trick at the heart of VAEs cleverly sidesteps
    this issue by splitting the sampling into two processes:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: First, sample **ϵ** from a standard Gaussian **N(0,1)**.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, transform **ϵ** using the mean **μ** and standard deviation **σ** of **q(z∣x)**
    to get the sample **z=μ+σ×ϵ**
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'What I find particularly elegant about the reparameterization trick is that
    it separates the two central components of each generative process, be it the
    more mundane task of generating handwritten digits, or the metaphorical creation
    of heaven and earth in the Bible quote: a random component, given by the “formless
    and empty” noise of the initial sample **ϵ**, acquires meaning via a complex transformation
    that finally, via the decoder, creates a pattern **x** in the observed world.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/61dc4ec64fa725cf5a432486d7f2824d.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
- en: I trained a simple VAE on the MNIST data set, drew a random initial state, and
    sent it through the decoder to come up with this image that kind of looks like
    a 9\. The decoder implicitly understands the structure of the data, and decodes
    it from a lower-dimensional latent state. The MNIST dataset is available through
    **the Creative Commons Attribution-Share Alike 3.0** license.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, the ELBO is composed of a reconstruction term and an entropy term.
    Since entropy, in the world of information theory, measures the unpredictability
    or randomness of information content, the entropy naturally regularizes the training,
    trading off structure and noise during optimization. If the VAE focuses too much
    on reconstruction, it might overfit the data, capturing every tiny detail (including
    noise) from the training data in the latent space. If it, however, gives too much
    weight to the entropy, it might end up with a too simplistic latent space that
    can’t capture the nuances of the data.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: The entropy of the approximate posterior relates to its covariance structure
    **σ**, which gives us a measure of how much of the “formless and empty” noise
    (encoding uncertainty in our explanations) from the initial sample remains. If
    we want to make the whole procedure deterministic, we can simply set **σ** to
    zero, and all uncertainty is removed.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'In a deterministic universe, there is no true noise, only things that our models
    are not powerful enough to capture, or where we simply lack necessary information
    ([I love noise, and have written a whole article about it here](https://manuel-brenner.medium.com/the-importance-of-noise-327fcab7c4fb)).
    As George Box noted: ”all models are wrong, some are useful”, and VAEs learn to
    strike this balance between over- and underconfidence.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: This organizing principle helps explain why VAEs naturally excel at tasks like
    dimensionality reduction (separating important from unimportant information contained
    in the input data) and [denoising](/denoising-autoencoders-explained-dbb82467fc2).
    As mentioned before, VAEs can also achieve structured representations of the latent
    space, leading to interpretable features.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/23a4c31f32af32b04ea38977b0845cc9.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
- en: Sampling a grid of the 2D latent space for a VAE trained on MNIST leads to continuous
    transformations of different numbers into each other. Generated by me, [based
    on code for a figure provided here](https://github.com/gonzalorecio/MNIST-latent-representations).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '**Normalizing Flows (NF)**'
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I’ve heard someone call NFs “the reparameterization trick on steroids”, and
    I really like this description. Normalizing flows take over where VAEs leave off
    by formulating recognition as the application of flows.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: NFs operate by iteratively transforming a simple probability distribution, which,
    as in the case of VAEs, is usually a standard Gaussian **N(0,1)**, into more complex,
    intricate ones through a series of invertible transformations.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a3ca52cceb0f689ea5f9f069ea543099.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
- en: Normalizing flows deform samples z0 from a simple distribution into a potentially
    complex one by applying a series of invertible transformations. janosh, MIT <[http://opensource.org/licenses/mit-license.php](http://opensource.org/licenses/mit-license.php)>,
    via Wikimedia Commons
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Where VAEs separate randomness from the structure using a fixed distribution
    and learned transformations (mean and variance), NFs dynamically shape the distribution
    itself. They do this by keeping track of the Jacobian determinant. This gives
    a measure of the volume change of the transformation, e.g. how much it shrinks
    or stretches space, ensuring that the whole latent space morphs in a coherent
    manner.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: As in the case of VAEs, a blob of formlessness is shaped into form.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: 'At least two things are cool about NFs: they are invertible, so they allow
    a two-way mapping between both distributions, which can come in handy in many
    situations, e.g. when trying to estimate densities (since once you map back to
    the standard Gaussian **N(0,1)** this can be much easier to compute than for complex
    intractable posteriors), or for [anomaly detection](https://open.spotify.com/episode/1LlqI3frkyTUwuUuqRcVo1?si=3867f2171b974279),
    where the idea is to screen out data that has low likelihood under the learned
    distribution.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[OpenAI’s GLOW](https://openai.com/research/glow), which I mentioned earlier,
    also makes use of this reversibility to manipulate features like smile, age or
    beardedness in latent space and obtain an altered image almost in real-time.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: A second cool thing is their adaptability to different geometries and manifolds.
    A classical example is the application to [spherical symmetries](https://arxiv.org/abs/2002.02428),
    allowing NFs to form latent representations that live on the sphere. Since, despite
    some claims to the opposite, the earth is likely spherical, spherical symmetries
    are very useful e. g. when running simulations of the earth’s weather system.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '**Diffusion Models**'
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Creating noise from data is easy; creating data from noise is generative modeling.
  id: totrans-80
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '- [***Song et al.***](https://arxiv.org/abs/2011.13456)'
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Moving on, diffusion models are one of the most successful generative models
    of the past few years. While they were already proposed in 2015 by [Sohl-Dickstein
    et al.](https://arxiv.org/abs/1503.03585), their success in [image creation](https://arxiv.org/abs/2006.11239)
    has catapulted them to the spotlight, laying the foundation for DALL-E, Midjourney,
    or Stable Diffusion. While their basic architecture is quite different, they still
    relate conceptually to VAEs and Normalizing Flows.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 'Diffusion models split the generative process up into several steps: at every
    step of the way, the training sample is distorted with noise. The aim of the model
    is to learn to remove that noise from the sample. If I haven’t made it clear enough
    before that noise is fascinating, it again takes over a lead role in diffusion
    models.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'During training, noise is iteratively added to the training data. Using the
    example of images, the model either learns to remove minute levels of noise and
    polish its final details, or to flesh out vague shapes in an otherwise distorted
    image:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a8e1982da87ed6005a9c76d0626797b0.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
- en: The generation process in stable diffusion. Benlisquare, CC BY-SA 4.0 <[https://creativecommons.org/licenses/by-sa/4.0](https://creativecommons.org/licenses/by-sa/4.0)>,
    via Wikimedia Commons
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: While the process of recognition is less directly modeled via a recognition
    network, and the training objective changes quite a bit, the process of adding
    noise, and the subsequent monitoring of how the noise should be reduced, can be
    seen as a form of recognition, and the initial noise sample as the initial state
    drawn from p(**z_0**).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 'When generating a completely new sample, the model can just start with pure
    noise and, when trying to make sense of what could be hidden beneath the noise,
    come up with something new:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b02411a7aecf4dffd6426ed0ecf9e4b8.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
- en: When I ask DALL-E for “random white noise”, it returns something that is very
    much not random white noise. It can’t help but reflect some of the structure of
    the data it was trained on.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '**The process of hallucinating something from nothing uncovers the implicitly
    learned distribution of the data it was trained on.**'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Why exactly diffusion models work so well is still open for debate. Among other
    things, they have been likened to [energy-based models of associative memory](https://arxiv.org/pdf/2309.16750.pdf)
    ([made famous through Hopfield networks 40 years ago](https://www.pnas.org/doi/10.1073/pnas.79.8.2554)).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'Diffusion model also relate to ideas in score-based generative modeling, made
    popular by [Song et al.](https://arxiv.org/abs/2011.13456): unlike traditional
    methods that focus on directly computing data likelihoods, these models center
    on approximating the **score**, which in this context signifies the gradient of
    the data likelihood with respect to the data itself.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, the score gives the direction in which a sample should be changed
    to **make it more likely**. By not calculating the likelihood directly, these
    models often sidestep some of the computational challenges we have encountered
    before.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: The score function can again be modeled by neural networks. One particularly
    intriguing way to represent it is via stochastic differential equations (SDEs),
    which akin to Neural ODEs, represent differential equations through neural nets
    ([something known as implicit layers](http://implicit-layers-tutorial.org/)).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: This is similar in spirit to a continuous-time version of diffusion models.
    Starting from noise, the score function is used to guide it toward a likely sample
    (Stefano Ermon, whose lab develops these techniques, does great talks, [here is
    one explaining all of this in more detail](https://www.youtube.com/watch?v=Uwz7kv3GHEc&t=503s)).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: In diffusion models, the generative process is likewise stochastic, adding a
    stochastic component at every point of the step. Given the process of generation
    is split up into several steps, this allows the introduction of slight variations
    of samples by taking several steps back in the chain and running the process forward
    again.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bef594ec556b5bb0762ac58f94dd6b4e.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
- en: Slight variations of the painting I used as the Thumbnail for this article,
    made by DALL-E.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: In some of the most popular applications of diffusion models like DALL-E or
    Midjourney, the initial state is not necessarily given by a purely random sample
    **z0** from **N(0,1),** but from a joint embedding between vision and language
    **p(z0|x)**, where x can e.g. be a textual input, given by the powerful CLIP (Contrastive
    Language-Image Pre-Training) embedding.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Conditional generation is valuable in all kinds of multi-modal learning setups
    since it combines different sensory modalities into a coherent framework. It will
    probably be an integral part of some of the most exciting developments in AI to
    come.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e9fe04aba429375bee3bcab078a48702.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
- en: An impressionist painting of an AI brain imagining a sunset, credit to DALL-E.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '**Generative Adversarial Networks (GANs)**'
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[GANs](https://arxiv.org/abs/1406.2661) are one of the most popular classes
    of generative models of the recent decade, inspired by a legendary night of drinking
    by Ian Goodfellow and his friends.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: GANs pivot even farther from the dual-network structure of the Helmholtz machine.
    As I mentioned, approximating **p(z|x)** is often the central challenge of generative
    models, so GANs just throw recognition out of the window and try to do without
    it.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: GANs also begin by drawing a random noise vector from p**(z0)** (as in diffusion
    models, [this initial vector can also be conditioned on other information](https://arxiv.org/abs/1411.1784),
    such as text), but then train only the generative network (since this is anyway
    what we are most interested in many applications) by including a discriminator,
    and try to match samples from **p(x|z)** from the generative model to examples
    of the training data.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: The generative network is trained to produce data that can deceive the discriminator.
    This discriminator, in turn, is trained to distinguish between real and fake samples.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'The elegance of GANs lies in this competitive dynamic: the generator improves
    its ability to produce data, guided by feedback from the discriminator, while
    the discriminator itself becomes better at discerning real from fake. It’s the
    perfect zero-sum game, pushing both networks to become better and better (with
    certain risks for deep fake detection [I have written about here](/the-dangers-of-adversarial-learning-874a95cdddd3?sk=10fb6e1ea209a3b28a117d06bded0749)).'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: However, GANs come with their own set of challenges and have arguably lost some
    popularity recently with the success of their competitors, especially diffusion
    models. Training GANs can be notoriously unstable. If the generator produces low-quality
    samples in the beginning, the discriminator’s job becomes too easy, making it
    difficult for the generator to improve. On the other hand, if the discriminator
    becomes too powerful, it can stifle the generator’s growth, leading to mode collapse,
    where the generator ends up producing only a subset of possible outputs.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '**Transformers and Large Language Models (**LLMs)'
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since Transformers have revolutionized the generative modeling landscape for
    text completely, I can’t leave them unmentioned here.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: To keep it short, almost all LLMs are based on a variant of the [transformer
    architecture implementing self-attention from the famous 2017 Google paper,](https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)
    details on which would go beyond the scope of this article, [but are explained
    in many other places](/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452).
    This architecture allows LLMs to learn complex relationships between input sequences,
    which works especially well on text.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Some Transformer variants, like BERT, are trained in a masked language modeling
    setting. They are given sequences with certain tokens masked out and are trained
    to recognize the masked tokens. This is quite similar in spirit to recognition
    in VAEs, where masking words can be interpreted as noising the input. The missing
    word is imputed because the Transformer has learned a probability distribution
    over the input data **p(x),** and can understand the most likely word given the
    context.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: From a generative perspective, transformer-based LLMs model the probability
    of each potential word or phrase following the previous one(s), frequently conditioned
    on an input prompt. This again expresses a variant of the ever-present probability
    distribution **p(x|z)**.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: However, there are usually no explicit hidden variables **z** in Transformers,
    because the prompt and context are themselves words. Instead, the self-attention
    mechanism extracts the probability of token p(**x_i**|(**x1​,x2​,…,xt​)**) from
    all the observed words (**x1​,x2​,…,xt**​), and of course from the implicit distribution
    over all words and contexts it has seen in the billions of lines of training data.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 'While noise is not directly a part of training Transformers, LLMs naturally
    include a probabilistic component. This makes a lot of sense, since language is
    not uniquely determined (hence Markov Models were originally developed on the
    foundation of Russian poetry, [as I discuss at length here](/understanding-markov-chains-cbc186d30649)),
    and the same paragraph can be expressed in many different ways: when generating
    responses or samples, there are usually several words that fit the given context
    well, so there is a distribution over possible continuations p(**x_i**|(**x1​,x2​,…,xt​)**),'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: The probabilities for selecting different words can be scaled with the so-called
    temperature hyperparameter. Depending on whether you are looking for a creative
    or deterministic response, the level of “noisiness” can effectively be controlled
    by this parameter. LLMs like Chat-GPT allow you to ask for a specific temperature
    during a response.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'Chat-GPT rephrased this paragraph for me with a high-temperature setting: *“In
    the swirling galaxy of Transformers, noise isn''t the main star, but LLMs groove
    to the uncertain beats of language. Crafting answers isn''t about nailing that
    one epic word but jamming with a cocktail of potential wordy melodies p(x_i|(x1​,x2​,…,xt​)).”*'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'While a high temperature makes ChatGPT sound like it is literally high, a “high
    temperature” is used here in analogy to Boltzmann’s statistical formulation of
    thermodynamics, which postulates that the states of a system follow an exponential
    distribution, depending on its temperature and the energy of the state:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a350ea73a4bf379c7d00dad6eaf67104.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
- en: The Boltzmann Distribution for different lambdas, which are inversely related
    to the temperature. Newystats, CC BY-SA 4.0 <[https://creativecommons.org/licenses/by-sa/4.0](https://creativecommons.org/licenses/by-sa/4.0)>,
    via Wikimedia Commons
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'The analogy to Transformers is no coincidence: The softmax function is used
    in the self-attention mechanism when mapping the obtained [scaled dot-product
    scores between keys and queries to probabilities](/the-power-of-the-dot-product-in-artificial-intelligence-c002331e1829).
    The softmax has the exact same functional form as the Boltzmann distribution and
    is used in both cases to map unnormalized scores (or energies in the case of the
    Boltzmann distribution) to a normalized probability distribution.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: As in thermodynamics, temperature closely relates to entropy and thus, in turn,
    to uncertainty/noise. In the Boltzmann distribution, as the temperature increases,
    the probabilities for different energy states become more uniform. Maximum uniformity
    leads to maximum entropy since all states are equally likely. In LLMs, this means
    that all words that are vaguely possible are predicted at the next stage with
    equal probability. It, however, doesn’t mean that generated text is completely
    random even for high temperatures, as we saw in the examples above. The selection
    of most probable tokens, even when scaled by higher temperatures, still mostly
    represents coherent language.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: If I managed to communicate one idea with this article, it should be that noise
    plays a crucial role in all generative models. Generative modeling is the art
    of taking formless noise and breathing structure into it.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: As the past years have shown, many ways lead to Rome, and different models can
    make sense depending on the goals, data modalities, and on pragmatic considerations
    about what works best when scaled up to enormous model sizes (such as Transformers)
    and when training with gradient-based methods.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: “What I can create, I still don’t understand.”
  id: totrans-127
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '***— What Richard Feynman should have mentioned***'
  id: totrans-128
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The enormity of state-of-the-art generative models and the complexity of the
    training data has contributed to challenges with interpreting them. Diffusion
    models and Transformers are not formulated as latent variable models, and can
    feel like giant black-boxes, with interpretations lagging behind substantially,
    especially given increasing worries about their impact on the real world.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: However, we might still learn to uncover some structures within, [such as in
    this new paper by Max Tegmark et al.](https://paperswithcode.com/paper/language-models-represent-space-and-time),
    in which they decscribe discovering intermediate representations of space and
    time in LLMs, and liken to the emergence of an interpretable world model. Others
    [creatively apply the tools of cognitive psychology](https://arxiv.org/abs/2206.14576)
    to understand the behavior of LLMs, much as we try to understand the complexities
    of human behavior.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[In a recent podcast episode](https://www.youtube.com/watch?v=-hxeDjAxvJ8),
    Marc Andreessen called the question of whether generative models can be **meaningfully
    trained and improved on synthetic data from generative models** a trillion-dollar
    question. Training on this essentially free data would open up many possibilities,
    providing a form of self-play (which has already been successfully used by DeepMind
    with AlphaGo and AlphaFold) to continue tuning generative models without relying
    on expensively curated training data. Andreessen related this question to the
    information-theoretic perspectives on the relationship between signal and noise,
    going back to Shannon: simply asked, how can there be more information in a model
    than what we fed into it?'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: If they don’t like it, they call it hallucination, if they like it, they call
    it creativity.
  id: totrans-132
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '***— Marc Andreessen***'
  id: totrans-133
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Is it true that generative models only imitate what they see in the training
    data? In what sense does the noisiness of the training process and the noisiness
    of the model formulation itself lead to a kind of generalization beyond the training
    data (I’ve considered related questions in my recent article on the [Free Lunch
    Theorem](/why-there-kind-of-is-free-lunch-56f3d3c4279f))? After all, noise is
    widely used in [machine learning models to boost generalization](/why-more-is-more-in-deep-learning-b28d7cedc9f5).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'Noise can both lead to hallucination and creativity, to the hallucination of
    alternative facts and the creation of alternative perspectives that weren’t there
    before. With generative models, it can also be argued that the “information” lies
    not just in the raw data, but in the combinatorial possibilities of that data.
    Generative models provide us with a novel and enticing way to explore this combinatorial
    space. In the words of Mark Twain:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: “There is no such thing as a new idea. It is impossible. We simply take a lot
    of old ideas and put them into a sort of mental kaleidoscope. We give them a turn
    and they make new and curious combinations.”
  id: totrans-136
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '***— Mark Twain***'
  id: totrans-137
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'And in the spirit of the quote itself not being a new idea, we can yet again
    go back to the bible (which I didn’t expect to quote twice in an AI article):'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: What has been will be again, what has been done will be done again; there is
    nothing new under the sun.
  id: totrans-139
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Ecclesiastes 1:9 (Bible)**'
  id: totrans-140
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It can be argued that a similar interplay between noise and structure can also
    be observed to play out in human creativity. In my recent article on g[enius and
    mental visualization](https://manuel-brenner.medium.com/genius-and-the-power-of-mental-visualization-836bf3763071),
    I’ve explored how in the brain, free, unstructured, mind-wandering activity by
    the default mode network ( what Scott Barry Kaufman calls the “imagination network”),
    can often provide an impulse that is then shaped by more deliberate, focused practice
    and skill into some of the most astounding works of art and genius.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/90dac23093f1440094e9c68c4f0d91d1.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
- en: DALL-E painting in the style of Jackson Pollock, capturing the structured randomness
    of the original random style of painting.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: DALL-E 以杰克逊·波洛克的风格绘画，捕捉原始随机风格绘画的结构化随机性。
- en: 'Even the most novel works of art and science have to be understood in a language
    that is already partially familiar to us. As Wittgenstein noted: there is no private
    language. Generative models are learning to speak our language, are learning to
    approximate the **p(x)** of all the things we most care about, and, by trading
    off noise and structure, reveal endless new patterns within this distribution
    and slightly outside of it. Their creativity can in turn be used to inspire our
    own creativity.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是最创新的艺术和科学作品，也必须用我们已经部分熟悉的语言来理解。正如维特根斯坦所指出的：没有私人语言。生成模型正在学习说我们的语言，学习逼近我们最关心的事物的**p(x)**，并通过在噪声和结构之间权衡，揭示这种分布内外的无尽新模式。它们的创造力反过来可以用来激发我们的创造力。
- en: “Those who do not want to imitate anything, produce nothing.”
  id: totrans-145
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “那些不愿意模仿任何东西的人，什么也不会产生。”
- en: — ***Salvador Dalí***
  id: totrans-146
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: — ***萨尔瓦多·达利***
- en: It is in part daunting but also exhilarating to think about how generative models
    are already beginning to shape our sensory input, questioning and pushing the
    boundary of how we perceive the world and our minds, and how we think of creativity
    and genius.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 想到生成模型已经开始塑造我们的感官输入，质疑并推动我们对世界和心智的感知边界，以及我们对创造力和天才的理解，这一方面让人感到畏惧但也充满兴奋。
- en: 'So I think there is no better way to end this article than with the words of
    Chat-GPT, dreaming itself to be Leonardo da Vinci:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我认为用Chat-GPT的话结束这篇文章再好不过了，它梦想成为列奥纳多·达芬奇：
- en: “True genius lies not in mere imitation, but in the alchemy of blending the
    known with the unknown.”
  id: totrans-149
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “真正的天才不仅仅在于模仿，而在于将已知与未知融合的炼金术。”
- en: '***— Leonardo da Vinci/Chat-GPT***'
  id: totrans-150
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***— 列奥纳多·达芬奇/Chat-GPT***'
- en: Thanks for reading!
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: If you like my writing, please [subscribe to get my stories via mail](https://manuel-brenner.medium.com/subscribe).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢我的写作，请[订阅以通过邮件获取我的故事](https://manuel-brenner.medium.com/subscribe)。
