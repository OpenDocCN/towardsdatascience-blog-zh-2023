- en: Complete Guide to Caching in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/complete-guide-to-caching-in-python-b4e37a4bcebf](https://towardsdatascience.com/complete-guide-to-caching-in-python-b4e37a4bcebf)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How does caching work, and ways to cache your functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://kayjanwong.medium.com/?source=post_page-----b4e37a4bcebf--------------------------------)[![Kay
    Jan Wong](../Images/28e803eca6327d97b6aa97ee4095d7bd.png)](https://kayjanwong.medium.com/?source=post_page-----b4e37a4bcebf--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b4e37a4bcebf--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b4e37a4bcebf--------------------------------)
    [Kay Jan Wong](https://kayjanwong.medium.com/?source=post_page-----b4e37a4bcebf--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b4e37a4bcebf--------------------------------)
    ·7 min read·Dec 1, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b0f443c08318e8fb89a059bc3e15b59c.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Nana Smirnova](https://unsplash.com/@nananadolgo?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: When repeated function calls are made with the same argument, this causes the
    computation to be repeated. **Memoization** is useful in such scenarios where
    the results of function calls can be ‘saved’ for future use. This results in time
    savings and code optimization as code becomes less computationally expensive.
    **Caching** is a more general term used to refer to storing of any data.
  prefs: []
  type: TYPE_NORMAL
- en: This article will touch on the different caching strategies, caching considerations,
    and how to enable and implement different types of caching for your scripts (using
    Python package and your implementation)!
  prefs: []
  type: TYPE_NORMAL
- en: Table of Contents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Types of Caching](https://medium.com/p/b4e37a4bcebf/#64d2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Caching Considerations](https://medium.com/p/b4e37a4bcebf/#4de0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LRU Caching](https://medium.com/p/b4e37a4bcebf/#b4c2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LFU Caching](https://medium.com/p/b4e37a4bcebf/#b490)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[FIFO / LIFO Caching](https://medium.com/p/b4e37a4bcebf/#420a)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of Caching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are several strategies for caching based on your needs, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Least Recently Used (LRU)**: removes least recently used data, the most common
    type of caching'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Least Frequently Used (LFU)**: removes least frequently used data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**First-In-First-Out (FIFO)**: removes the oldest data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Last-In-First-Out (LIFO)**: removes the newest data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Most Recently Used (MRU)**: removes most recently used data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random Replacement (RR)**: removes randomly selected data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caching Considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When using caching in your applications, you should consider the ***memory
    footprint*** of the cache as it is storing additional information. If you are
    deciding between different implementations, in terms of architecture and data
    structures, there are a few timing considerations such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '***Access time***: For arguments that have been computed before, results should
    be accessed quickly in `O(1)` time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Insertion time***: For new arguments, data should be inserted into the cache,
    preferably in `O(1)` time (depending on implementation, some may take `O(n)` time,
    choose wisely!)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Deletion time***: In the case when the cache is full, data will need to
    be removed according to the caching strategy. Deletion involves identifying the
    data to be deleted and subsequently removing them from memory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 — Least Recently Used (LRU) Caching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Implemented by adding a decorator to your Python function
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/f1a04fe4afdb2e4f0abbaf75354c7bf2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 1: LRU Implementation (Image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: '**How it works**: It uses a dictionary and a doubly linked list. In the dictionary,
    the key-value pairs are the supplied arguments and the entries in the doubly linked
    list. This allows the results to be quickly referenced given the supplied arguments
    (`O(1)` access time). Since arguments can be stored as a dictionary key, they
    must be hashable.'
  prefs: []
  type: TYPE_NORMAL
- en: In the doubly linked list, function results are stored in the entries. Entries
    are sorted according to their recency and can reference their immediate previous
    and next entries. This allows entries to be easily inserted or reordered and for
    quick identification and deletion of the least recent entry if the cache is full.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Advanced Usage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LRU caching can be enhanced by implementing the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: Maximum cache size with `maxsize` argument
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unlimited cache size with `maxsize=None` argument or `functools.cache`. This
    means that data will never be deleted, which can lead to timing improvement at
    the expense of memory footprint.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Retrieve caching information on the number of hits and misses using `.cache_info()`.
    Cache statistics allow us to evaluate how efficiently the cache is being accessed
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add expiration time with `cachetools.TTLCache`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Linking cache with CPU memory usage](https://stackoverflow.com/questions/23477284/memory-aware-lru-caching-in-python)
    instead of cache size'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Note**: Other Python packages implement caching such as [fastcache](https://github.com/pbrady/fastcache)
    but they are not as popular or commonly used as `functools.lru_cache`'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 2 — Least Frequently Used (LFU) Caching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Implemented by maintaining a cache dictionary and a frequency dictionary
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Many implementations of LFU caching can be found online, including `cachedtools`
    with usage similar to LRU caching in the previous section. There can be various
    implementations for LFU caching using Hash Map, Singly Linked List, or Doubly
    Linked List.
  prefs: []
  type: TYPE_NORMAL
- en: I find maintaining two dictionaries the most optimal way, considering the access,
    insertion, and deletion time. Memory usage can be improved further by the use
    of hashing.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/442c18aac4e1c6b56cf92eeadb991df9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 2: LFU Implementation (Image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: '**How it works**: It uses 2 dictionaries. In the cache dictionary, the key
    is the supplied arguments and the value is a tuple of the function results and
    frequency. This allows quick retrieval of the function results (`O(1)` access
    time!) and frequency (to be used for accessing frequency dictionary).'
  prefs: []
  type: TYPE_NORMAL
- en: In the frequency dictionary, the key is the frequency and the value is a list
    of supplied arguments. Storing the list of supplied arguments allows quick identification
    of the least frequently used argument and subsequent eviction of the argument
    and result from both dictionaries. Deque can be used instead of a list for quicker
    access, insertion, and deletion time.
  prefs: []
  type: TYPE_NORMAL
- en: '**Caveat**: In LFU caching, it is biased against recent entries as newer entries
    may not have as high frequency as the existing entries — making it hard to evict
    the older entries even if they are being accessed less frequently'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The code snippet above is adapted from [here](https://www.tutorialspoint.com/lfu-cache-in-python),
    with some tweaks.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the LFU cache as a decorator, we can wrap the `LFUCache` class and use
    it similarly to `functools.lru_cache`. This can be done as such:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 3 — First-In-First-Out (FIFO) / Last-In-First-Out (LIFO) Cache
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Implemented by maintaining an ordered dictionary
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/e741aa4449fdf5e820e041085a8f7d0d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig 3: FIFO / LIFO Implementation (Image by author)'
  prefs: []
  type: TYPE_NORMAL
- en: '**How it works**: It uses an ordered dictionary where the key-value pairs are
    the supplied arguments and the function results. The dictionary is sorted chronologically
    according to when the supplied arguments were first called.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This allows the results to be quickly referenced given the supplied arguments
    (`O(1)` access time), and entries can be removed from the front or the back depending
    on the caching strategy (`O(1)` insertion and deletion time). Implementation can
    be done using `cachedtools` with usage similar to LRU caching in the previous
    section. To illustrate this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Hope you have understood more about caching, types of caching strategies and
    their considerations, and implementations using Python libraries, or your implementation!
  prefs: []
  type: TYPE_NORMAL
- en: Related Links
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'LRU Documentation: [https://docs.python.org/3/library/functools.html#functools.lru_cache](https://docs.python.org/3/library/functools.html#functools.lru_cache)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LFU Article: [https://www.tutorialspoint.com/lfu-cache-in-python](https://www.tutorialspoint.com/lfu-cache-in-python)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cachetools` Official GitHub: [https://github.com/tkem/cachetools](https://github.com/tkem/cachetools)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
