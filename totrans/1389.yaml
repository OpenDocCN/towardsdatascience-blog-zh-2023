- en: 'KServe: Highly scalable machine learning deployment with Kubernetes'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/kserve-highly-scalable-machine-learning-deployment-with-kubernetes-aa7af0b71202](https://towardsdatascience.com/kserve-highly-scalable-machine-learning-deployment-with-kubernetes-aa7af0b71202)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Kubenetes model inferencing made easy.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@lloyd.hamilton?source=post_page-----aa7af0b71202--------------------------------)[![Lloyd
    Hamilton](../Images/0c516beb55ed3aac21978b3bc85ca9b1.png)](https://medium.com/@lloyd.hamilton?source=post_page-----aa7af0b71202--------------------------------)[](https://towardsdatascience.com/?source=post_page-----aa7af0b71202--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----aa7af0b71202--------------------------------)
    [Lloyd Hamilton](https://medium.com/@lloyd.hamilton?source=post_page-----aa7af0b71202--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----aa7af0b71202--------------------------------)
    ·10 min read·May 29, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/65c9c56369db8df360025d18d9c73a22.png)'
  prefs: []
  type: TYPE_IMG
- en: Image sourced from [KServe](https://kserve.github.io/website/0.10/)
  prefs: []
  type: TYPE_NORMAL
- en: In the wake of the release of chatGPT, it is becoming increasingly difficult
    to avoid technologies that leverage machine learning. From text prediction on
    your messaging app to facial recognition on your smart door bell, machine learning
    (ML) can be found in almost every piece of tech we use today.
  prefs: []
  type: TYPE_NORMAL
- en: How machine learning technologies are delivered to consumers is one of the many
    challenges organisations will have to address during development. The deployment
    strategies of ML products have a significant impact on the end users of your product.
    This can mean the difference between Siri on your iPhone or chatGPT in your web
    browser.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond the sleek user interface and overly assertive chat dialogues of ChatGPT,
    hides the complex mechanisms required to deploy the large language ML model. ChatGPT
    is built on a highly scalable framework that is designed to deliver and support
    the model during its exponential adoption. In reality, the actual ML model will
    only make up a small proportion of the whole project. Such projects are often
    cross disciplinary and require expertise in data engineering, data science and
    software development. Therefore, frameworks that simplifies the model deployment
    processes are becoming increasingly vital in delivering models to production,
    helping organisations save time and money.
  prefs: []
  type: TYPE_NORMAL
- en: Without the proper operational framework to support and manage ML models, organisations
    will often meet bottle necks when attempting to scale the number of ML model in
    production.
  prefs: []
  type: TYPE_NORMAL
- en: While no single tool has emerged as a clear winner in the highly saturated market
    of MLOps toolkits, KServe is becoming an increasingly popular tool to help organisations
    meet scalability requirements of ML models.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note: I am not affiliated with KServe nor have been sponsored to write this
    article.**'
  prefs: []
  type: TYPE_NORMAL
- en: What is KServe?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: KServe is a highly scalable machine learning deployment toolkit for Kubernetes.
    It is an orchestration tool that is built on top of Kubernetes and leverages two
    other open sourced projects, Knative-Serving and Istio; more on this later.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/65c9c56369db8df360025d18d9c73a22.png)'
  prefs: []
  type: TYPE_IMG
- en: Image sourced from [KServe](https://kserve.github.io/website/0.10/)
  prefs: []
  type: TYPE_NORMAL
- en: KServe significantly simplifies the deployment process of ML Models into a Kubernetes
    cluster by unifying the deployment into a single resource definition. It makes
    the machine learning deployment part of any ML project easy to learn and ultimately
    decreases the barrier to entry. Therefore, models deployed using KServe can be
    easier to maintain than models deployed using traditional Kubernetes deployment
    that require a Flask or FastAPI service.
  prefs: []
  type: TYPE_NORMAL
- en: With KServe, there is no requirement to wrap your model inside a FastAPI or
    Flask app before exposing it through the internet via HTTPS. KServe has built-in
    functionality that essentially replicates this process but without the overhead
    of having to maintain API endpoints, configure pod replicas, or configure internal
    routing networks in Kubernetes. All you have to do is point KServe to your model,
    and it will handle the rest.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond the simplification of the deployment processes, KServe also offers many
    features, including canary deployments, inference autoscaling, and request batching.
    These features will not be discussed as they are out of scope. However, this guide
    will hopefully set the foundation of understanding required to explore further.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s talk about the two key technologies, Istio and Knative, that accompany
    KServe.
  prefs: []
  type: TYPE_NORMAL
- en: Istio
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Much of the functionality that KServe brings to the table would be difficult
    without Istio. Istio is a service mesh that extends your applications deployed
    in Kubernetes. It is a dedicated infrastructure layer that adds capabilities such
    as observability, traffic management, and security. For those familiar with Kubernetes,
    Istio replaces the standard ingress definitions typically found in a Kubernetes
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The complexity of managing traffic and maintaining observability grows as a
    Kubernetes-based system scales. One of the best features of Istio is its ability
    to centralize the controls of service-level communications. This gives developers
    greater control and transparency over communications among services.
  prefs: []
  type: TYPE_NORMAL
- en: With Istio, developers do not need to design applications that can handle traffic
    authentication or authorization. Ultimately, Istio helps reduce the complexity
    of deployed apps and allows developers to concentrate on the important components
    of the apps.
  prefs: []
  type: TYPE_NORMAL
- en: By leveraging the networking features of Istio, KServe can bring features that
    include canary deployment, inference graphs, and custom transformers.
  prefs: []
  type: TYPE_NORMAL
- en: KNative
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Knative, on the other hand, is an open-source enterprise-level solution to build
    serverless and event-driven applications. Knative is built on top of Istio to
    bring serverless code execution capabilities that are similarly offered by AWS
    Lambdas and Azure Functions. Knative is a platform-agnostic solution for running
    serverless deployments in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: One of the best features of Knative is the scale-to-zero feature. This is a
    critical component of KServe’s ability to scale up or down ML model deployment
    and one that maximizes resource utilization and saves on costs.
  prefs: []
  type: TYPE_NORMAL
- en: Should I use KServe?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: KServe, like many other tools, is not a one size fits all solution that will
    fit your organisation’s requirements. It has a high cost of entry due to the fact
    that some experience in working with Kubernetes is required. If you are just getting
    started with Kubernetes, there are many resources online and I highly recommend
    checking out resources such as the [DevOps](https://www.youtube.com/channel/UCFe9-V_rN9nLqVNiI8Yof3w)
    guy on Youtube. Nonetheless, even without a deep understanding of Kubernetes,
    it is possible to learn to use KServe.
  prefs: []
  type: TYPE_NORMAL
- en: KServe will be ideal in organisations already leveraging Kubernetes where there
    are existing knowledge in working with Kubernetes. It may also suit organisations
    looking to move away or complement managed services like SageMaker or Azure Machine
    Learning in order to have greater control over your model deployment process.
    The increase in ownership can result in significant cost reductions and increased
    configurability to meet project specific requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, the right cloud infrastructure decision will depend on a case
    by case basis as infrastructure requirements will differ across companies.
  prefs: []
  type: TYPE_NORMAL
- en: Pre-requisites
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This guide will take you though the steps required to get you set up with KServe.
    You will be walked through the steps to install KServe and serve your first model.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several pre-requisites that will need to be met before proceeding.
    You will require the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Kubectl](https://kubernetes.io/docs/tasks/tools/) installation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Helm](https://helm.sh/docs/intro/install/) installation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Kubectx](https://github.com/ahmetb/kubectx) installation (optional)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes Cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this tutorial, I recommend experimenting with a Kubernetes cluster using
    [Kind](https://kind.sigs.k8s.io). It is a tool to run a local Kubernetes cluster
    without needing to spin up cloud resources. In addition, I highly recommend Kubectx
    as a tool to easily switch between Kubernetes context if you are working across
    multiple clusters.
  prefs: []
  type: TYPE_NORMAL
- en: However, when running production workload, you will need access to a fully functioning
    Kubernetes cluster to configure DNS and HTTPS.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deploy a Kubernetes cluster in Kind with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Switch to the correct Kubernetes context with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Installation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following steps will install Istio v1.16, Knative Serving v1.7.2 and KServe
    v0.10.0\. These versions are best suited for this tutorial as Knative v1.8 onwards
    will require DNS configuration for ingress which adds a layer of complexity that
    is currently out of scope.
  prefs: []
  type: TYPE_NORMAL
- en: Istio Installation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 2\. Install KNative Serving.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 3\. Install cert manager. Cert manager is required to manage valid certificate
    for HTTPs traffic.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 4\. Create a namespace for model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 5\. Clone the [KServe](https://github.com/kserve/kserve) repository.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 6\. Install KServe Cutom Resource Definitions and KServe Runtimes into the model
    namespace in your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Great! We now have KServe installed on the cluster. Let’s get deploying!
  prefs: []
  type: TYPE_NORMAL
- en: First Inference Service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to ensure that the deployment went smoothly, let us deploy a demo inference
    service. The source code for the deployment can be found [here](https://kserve.github.io/website/0.10/get_started/first_isvc/#1-create-a-namespace).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The yaml resource definition above deploys a test inference service that sources
    a publicly available model trained using the SciKit-Learn library. KServe supports
    many different flavours of [machine learning libraries](https://kserve.github.io/website/0.10/modelserving/v1beta1/serving_runtime/).
    These include MLFlow, PyTorch or XGBoost models; more are added at each release.
    If none of these out-of-the-box libraries meet your requirements, KServe also
    support [custom predictors](https://kserve.github.io/website/0.10/modelserving/v1beta1/custom/custom_model/).
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to monitor the status of the current deployment by getting the
    available pods in the namespace.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/1c11648c1c5afabf69556e4d3a2ed99b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'If you run into issues with the deployment use the following to debug:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also check the status of the inference service deployment with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/f4bac95397a4b4970d541b2482199c3b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: If the inference service is marked true, we are ready to perform our first prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Performing a prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To perform a prediction, we will need to determine if our Kubernetes cluster
    is running in an environment that supports external load balancers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '**Kind Cluster**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Clusters deployed using Kind does not support external load balancers therefore
    you will have an ingress gateway that looks similar to below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/823bdfea40270bb0906d67d5c451f56b.png)'
  prefs: []
  type: TYPE_IMG
- en: Kind External Load Balancer (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: In this case we would have to port-forward the istio-ingressgateway which will
    allow us to access it via `localhost`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Port-forward the istio ingress gateway service to port `8080` on `localhost`
    with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Then set the ingress host and port with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '**Kubernetes Cluster**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If the external IP is valid and does not display `<pending>`, we are able to
    send an inference request through the internet at the IP address.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/073c75ea25c6b436a5b5063b31c9dc0b.png)'
  prefs: []
  type: TYPE_IMG
- en: Ingress Gateway IP address (Image by author)
  prefs: []
  type: TYPE_NORMAL
- en: 'Set the ingress host and port with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Perform Inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prepare an input request `json` file for the inference request.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then perform an inference with curl:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The request will be sent to the KServe deployment through the istio-ingress
    gateway. If everything is in order, we will get a `json` reply from the inference
    service with the prediction of `[1,1]` for each of the instances.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/db8f0720a7ddb9c4c3c0ddf4978a5393.png)'
  prefs: []
  type: TYPE_IMG
- en: Scaling to Zero
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By leveraging the features of Knative, KServe supports scale-to-zero capabilities.
    This feature effectively manages limited resources across the cluster by scaling
    pods not in use to zero. Scaling to zero capabilities allow the creation of a
    reactive system that responds to requests, as opposed to a system that is always
    up. This will facilitate the deployment of a greater number of models in the cluster
    than traditional deployment configurations can.
  prefs: []
  type: TYPE_NORMAL
- en: However, note that there is a cold start penalty for pods that have been scaled
    down. This will vary depending on the size of the image/model and the available
    cluster resources. A cold start can take 5 minutes if the cluster needs to scale
    an additional node or 10 seconds if the model is already cached on the node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us modify the existing scikit-learn inference service and enable scale
    to zero by defining `minReplicas: 0`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: By setting minReplicas to 0, this will command Knative to scale the inference
    service down to zero when there are no HTTP traffic. You will notice that after
    a period of 30 seconds, the pods for Sklearn-Iris model will be scaled down.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9d24abadd3ddf3c8f69b5fa420059792.png)'
  prefs: []
  type: TYPE_IMG
- en: Sklearn-Iris predictors scales down to zero
  prefs: []
  type: TYPE_NORMAL
- en: To reinitialise the inference service, send a prediction request to the same
    endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3399799fc2385ef8f6b425161a638977.png)'
  prefs: []
  type: TYPE_IMG
- en: This will trigger pod initialisation from cold start and return a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: KServe simplifies the process of machine learning deployment and shortens the
    path to production. When combined with Knative and Istio, KServe has the added
    bonus of being highly customisable and bring many features that easily rivals
    those offered in managed cloud solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, migrating the model deployment process in-house has its own innate
    complexities. However, the increase in platform ownership will confer greater
    flexibility in meeting project specific requirements. With the right Kubernetes
    expertise, KServe can be a powerful tool that will allow organisations to easily
    scale their machine learning deployment across any cloud provider to meet increasing
    demand.
  prefs: []
  type: TYPE_NORMAL
