- en: 'Navigating Industry-Specific AI: From Transitional Heroes to Long-Term Solutions'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 行业特定AI的导航：从过渡性英雄到长期解决方案
- en: 原文：[https://towardsdatascience.com/navigating-industry-specific-ai-from-transitional-heroes-to-long-term-solutions-67e41a84b207](https://towardsdatascience.com/navigating-industry-specific-ai-from-transitional-heroes-to-long-term-solutions-67e41a84b207)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/navigating-industry-specific-ai-from-transitional-heroes-to-long-term-solutions-67e41a84b207](https://towardsdatascience.com/navigating-industry-specific-ai-from-transitional-heroes-to-long-term-solutions-67e41a84b207)
- en: '![](../Images/aefbd36b7cff0e7011dd82d102e2db37.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aefbd36b7cff0e7011dd82d102e2db37.png)'
- en: This image was crafted with the support of Midjourney.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 该图像由Midjourney支持制作。
- en: Strategies, Insights, and the Evolution of Industry-Specific Large Language
    Models
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略、洞察与行业特定大语言模型的演变
- en: '[](https://serena2023.medium.com/?source=post_page-----67e41a84b207--------------------------------)[![Serena
    Xu](../Images/31a43e8c528e392e4bd36ad0d543d419.png)](https://serena2023.medium.com/?source=post_page-----67e41a84b207--------------------------------)[](https://towardsdatascience.com/?source=post_page-----67e41a84b207--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----67e41a84b207--------------------------------)
    [Serena Xu](https://serena2023.medium.com/?source=post_page-----67e41a84b207--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://serena2023.medium.com/?source=post_page-----67e41a84b207--------------------------------)[![Serena
    Xu](../Images/31a43e8c528e392e4bd36ad0d543d419.png)](https://serena2023.medium.com/?source=post_page-----67e41a84b207--------------------------------)[](https://towardsdatascience.com/?source=post_page-----67e41a84b207--------------------------------)[![数据科学前沿](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----67e41a84b207--------------------------------)
    [Serena Xu](https://serena2023.medium.com/?source=post_page-----67e41a84b207--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----67e41a84b207--------------------------------)
    ·7 min read·Sep 4, 2023
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[数据科学前沿](https://towardsdatascience.com/?source=post_page-----67e41a84b207--------------------------------)
    ·7分钟阅读·2023年9月4日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'As the field of artificial intelligence (AI) continues to evolve, we are witnessing
    a growing trend: the rise of Large Language Models (LLMs) specifically tailored
    for particular industries. These industry-specific LLMs are not just adapted to
    the specialized terminology and context of a given field, but also offer customized
    AI solutions to tackle unique challenges within that industry. For instance, in
    healthcare, a specialized LLM could accelerate drug research and discovery, while
    in finance, a corresponding model could swiftly decode complex investment strategies.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人工智能（AI）领域的不断发展，我们正目睹一个日益增长的趋势：专为特定行业量身定制的大语言模型（LLMs）的兴起。这些行业特定的LLMs不仅适应了特定领域的专业术语和背景，还提供了定制的AI解决方案，以应对该行业内的独特挑战。例如，在医疗领域，一个专业的LLM可以加速药物研究与发现，而在金融领域，相应的模型可以迅速解码复杂的投资策略。
- en: 'Against this backdrop, the so-called “industry large models” can be essentially
    understood as “extensions of general large models applied within specific industries”.
    There are two core concepts to emphasize here: the first is the “general large
    model,” and the second is “industry-specific data.”'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种背景下，所谓的“行业大模型”可以基本理解为“应用于特定行业的一般大模型的扩展”。这里有两个核心概念需要强调：第一个是“一般大模型”，第二个是“行业特定数据”。
- en: The true value of general large models lies not just in their enormous parameter
    count, but more importantly, in their wide applicability across multiple domains.
    This cross-domain universality not only enhances the adaptability of the model
    but also generates unique capabilities as the model evolves toward becoming more
    “general.” Therefore, training a model solely with industry-specific data is a
    myopic approach that fundamentally contradicts the core philosophy of general
    large models, which is “universality.”
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一般大模型的真正价值不仅在于其庞大的参数数量，更在于其在多个领域的广泛适用性。这种跨领域的普遍性不仅增强了模型的适应性，还在模型向更“通用”的方向发展时产生了独特的能力。因此，单纯使用行业特定数据来训练模型是一种短视的方法，这与一般大模型的核心理念“普遍性”基本相悖。
- en: As for the industry-specific data, there are mainly two ways to apply it. The
    first involves directly fine-tuning or continuing the training of a general large
    model using this data. The second method utilizes prompts or external databases,
    leveraging the “in-context learning” capabilities of general large models to solve
    particular industry problems. Both approaches have their advantages and limitations,
    but they share the common goal of harnessing the capabilities of general large
    models to address industry-specific challenges more accurately.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于行业特定的数据，主要有两种应用方式。第一种是直接使用这些数据对一般的大型模型进行微调或继续训练。第二种方法利用提示或外部数据库，利用一般大型模型的“上下文学习”能力来解决特定行业问题。这两种方法各有优缺点，但它们都有一个共同的目标，即利用一般大型模型的能力，更准确地解决行业特定的挑战。
- en: Balancing Immediate Benefits and Long-Term Vision
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平衡即时利益与长期愿景
- en: 'General large language models exhibit significant strengths in two dimensions:
    “knowledge” and “capability,” where knowledge can be analogized to experience
    and capability to intelligence quotient (IQ).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一般的大型语言模型在两个方面表现出显著的优势：“知识”和“能力”，其中知识可以类比为经验，能力则类似于智商（IQ）。
- en: '**Capability as IQ:** After being trained with vast amounts of data, general
    large models behave like entities with extraordinarily high IQs, possessing the
    ability to rapidly understand, learn, and tackle challenges. Take mathematical
    problem-solving as an example: the large model only needs a brief understanding
    of mathematical formulas and concepts to effortlessly resolve intricate problems,
    a situation termed “zero-shot learning.” With a few example problems provided,
    it can even address more complicated mathematical challenges, known as “few-shot
    learning.” In contrast, industry-specific large models delve deeper into various
    specialized books and problem sets. When there’s a noticeable IQ disparity between
    two large models, such as between GPT-4 and GPT-3.5, the one with a higher IQ
    will have a better accuracy rate, even with less experience.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**能力如智商：** 在经过大量数据训练后，一般的大型模型表现得像智商极高的实体，具备迅速理解、学习和解决问题的能力。以数学问题解决为例：大型模型只需简要了解数学公式和概念，就能轻松解决复杂问题，这种情况被称为“零样本学习”。在提供几个示例问题的情况下，它甚至可以应对更复杂的数学挑战，这被称为“少样本学习”。相比之下，行业特定的大型模型深入研究各种专业书籍和问题集。当两个大型模型之间存在明显的智商差异时，例如GPT-4和GPT-3.5，智商较高的模型即使经验较少，也会有更高的准确率。'
- en: '**Knowledge as Experience:** The knowledge of general large models stems from
    the data they have been exposed to. This means that for certain specific questions
    or domains, if the model hasn’t encountered related data, it can’t provide an
    accurate answer. For instance, when asking for the proof of “Fermat’s Last Theorem,”
    unless the model has been exposed to pertinent academic literature, it won’t be
    able to elucidate the theorem’s proof accurately.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**知识如经验：** 一般大型模型的知识来源于它们接触过的数据。这意味着，对于某些特定问题或领域，如果模型没有接触过相关数据，它就无法提供准确的答案。例如，当询问“费马最后定理”的证明时，除非模型接触过相关的学术文献，否则它无法准确阐述定理的证明。'
- en: Against this backdrop, industry-specific large models can be seen as “transitional
    products” when the capability of general large models falls short in specific
    domains. For instance, in the legal field, the ChatLaw model excels in legal questioning
    and information retrieval tasks because it’s trained on extensive specialized
    legal data. However, as the capability of general large models continually advances,
    the need for such specialized training might diminish. In the long run, by incorporating
    external knowledge bases or contextual information, general large models hold
    the potential to employ their advanced reasoning skills to tackle intricate industry
    challenges, gradually replacing industry-specific large models. This trajectory
    is not only inevitable in the development of General Artificial Intelligence (AGI)
    but also represents a more fundamental approach to addressing industry challenges.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种背景下，当通用大型模型在特定领域的能力不足时，行业特定的大型模型可以被视为“过渡性产品”。例如，在法律领域，ChatLaw模型在法律问题和信息检索任务中表现出色，因为它经过大量专业法律数据的训练。然而，随着通用大型模型能力的持续提升，对这种专业训练的需求可能会减少。从长远来看，通过整合外部知识库或上下文信息，通用大型模型具有运用其高级推理能力来应对复杂行业挑战的潜力，逐渐取代行业特定的大型模型。这一趋势不仅在通用人工智能（AGI）的发展中不可避免，也代表了解决行业挑战的更根本的方法。
- en: Strategies for Industry Models
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 行业模型的策略
- en: 'Solving industry challenges using large models can be broadly categorized into
    three main approaches:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 使用大型模型解决行业挑战可以大致分为三种主要方法：
- en: '**Building from Scratch with Blended Data**'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于混合数据从零开始构建**'
- en: This method involves creating a new model from the ground up using a combination
    of general and domain-specific data. Notable examples include BloombergGPT. However,
    it’s essential to note that this method may require a substantial amount of data
    and computational resources.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法涉及使用通用数据和领域特定数据的组合，从零开始创建一个新的模型。显著的例子包括BloombergGPT。然而，需要注意的是，这种方法可能需要大量的数据和计算资源。
- en: '**Adjusting Weights on General Models**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**调整通用模型的权重**'
- en: 'This category encompasses two strategies:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这一类别包括两种策略：
- en: '**Continuing Pre-training on General Models:** Models like LawGPT exemplify
    this strategy, which builds upon the foundation of a general model through continued
    pre-training. This iterative process enhances the model’s adaptability to a specific
    domain.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在通用模型上继续预训练：** 像LawGPT这样的模型就是这种策略的例子，通过继续预训练建立在通用模型的基础上。这一迭代过程增强了模型对特定领域的适应性。'
- en: '**Instruction Tuning (SFT):** This approach utilizes instruction tuning (SFT)
    on top of a general model’s framework. By fine-tuning the weights of a general
    model, it can better address specific industry problems.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**指令调优（SFT）：** 这种方法在通用模型框架的基础上利用指令调优（SFT）。通过微调通用模型的权重，它可以更好地解决特定行业的问题。'
- en: Both of these methods aim to adapt a general model for industry-specific needs
    by adjusting its weights. While these approaches are generally less resource-intensive
    than complete retraining, they might not consistently achieve optimal performance
    levels. Also, achieving significant performance improvements compared to retraining
    industry-specific large models might prove challenging.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法都旨在通过调整权重来适应行业特定需求。虽然这些方法通常比完全重训练消耗的资源少，但可能无法始终达到最佳性能水平。此外，与重训练行业特定大型模型相比，实现显著性能提升可能具有挑战性。
- en: '**Combining General Models with Domain Knowledge**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**结合通用模型与领域知识**'
- en: 'This category includes two strategies:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这一类别包括两种策略：
- en: '**Leveraging External Domain Knowledge:** This method involves augmenting a
    general large model with domain-specific knowledge, such as utilizing vector databases.
    By integrating information from domain knowledge databases, the general model’s
    effectiveness in generating responses to industry-specific issues can be enhanced.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**利用外部领域知识：** 这种方法涉及将领域特定知识（例如利用向量数据库）增强通用大型模型。通过整合领域知识数据库中的信息，可以提高通用模型在应对行业特定问题时的效果。'
- en: '**Utilizing In-Context Learning for Domain Responses:** Through “in context
    learning,” industry-specific challenges are addressed by creating contextually
    relevant prompts. A general large model then directly generates responses. As
    the trend towards broader context windows gains momentum, prompts can encompass
    more domain-specific knowledge. Consequently, even with a general large model,
    effective responses to industry-specific problems can be achieved.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**利用上下文学习进行领域响应：** 通过“上下文学习”，通过创建具有上下文相关的提示来解决特定行业的挑战。然后，通用的大型模型直接生成响应。随着对更广泛上下文窗口的趋势日益增强，提示可以涵盖更多领域特定的知识。因此，即使使用通用的大型模型，也可以有效地回应行业特定的问题。'
- en: In conclusion, all these approaches share the common goal of tackling industry-specific
    challenges, but they differ in methodology and resource requirements. Choosing
    the right approach should consider available data, computational capabilities,
    and expected performance gains. When evaluating industry-specific large models,
    it’s essential to differentiate between these diverse approaches, preventing misleading
    comparisons between methods with varying workloads and costs.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，所有这些方法都有一个共同的目标，即解决行业特定的挑战，但它们在方法和资源需求上有所不同。选择合适的方法应考虑可用数据、计算能力和预期的性能提升。在评估行业特定的大型模型时，必须区分这些不同的方法，防止对工作负荷和成本不同的方法进行误导性的比较。
- en: Practical Implementation Guidelines
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实际实施指南
- en: In the realm of industry-specific large models, a crucial yet often overlooked
    issue is data ratio. Specifically, practitioners frequently discover that using
    a large amount of industry-specific data for model fine-tuning can paradoxically
    diminish the model’s capabilities. One potential explanation is the disparity
    in quality between industry-specific data and general data. Furthermore, the data
    ratio — how much specialized data is mixed with general data — may not be optimal.
    For example, some models like BloombergGPT employ a simplistic 1:1 data ratio,
    but the efficacy of this approach for pre-training, continuous pre-training, or
    fine-tuning remains an open question.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在行业特定的大型模型领域，一个关键但经常被忽视的问题是数据比例。具体而言，实践者经常发现，使用大量行业特定数据进行模型微调可能会悖论地降低模型的能力。一种可能的解释是行业特定数据与一般数据之间的质量差异。此外，数据比例——专门数据与一般数据的混合比例——可能并不理想。例如，一些模型如BloombergGPT采用简单的1:1数据比例，但这种方法在预训练、持续预训练或微调中的有效性仍然是一个悬而未决的问题。
- en: My experience in developing domain-specific language models across diverse sectors,
    from e-commerce and office management to smart home technologies, has provided
    me with valuable insights. Typically, these models, with a size of 13B, leverage
    domain-specific data spanning 20M to 100M. I’ve found that a data ratio of approximately
    10% to 20% domain-specific data to the total dataset during the continuous pre-training
    phase consistently delivers optimal results. Exceeding this ratio often compromises
    the model’s general capabilities, such as summarization and question-answering.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我在跨越不同领域（从电子商务和办公室管理到智能家居技术）开发领域特定语言模型的经验为我提供了宝贵的见解。通常，这些模型的规模为13B，利用20M到100M的领域特定数据。我发现，在持续预训练阶段，约10%到20%的领域特定数据与总数据集的比例通常能提供最佳结果。超过这个比例往往会妨碍模型的通用能力，如总结和问答。
- en: Another interesting observation was that during the continuous pre-training
    process, it was beneficial to simultaneously incorporate data used for Supervised
    Fine-Tuning (SFT). This allowed the model to acquire more domain-specific knowledge
    even during the pre-training phase.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的观察是，在持续预训练过程中，同时融入用于监督微调（SFT）的数据是有益的。这使得模型即使在预训练阶段也能获得更多领域特定的知识。
- en: However, it’s worth noting that this 10% to 20% ratio isn’t a one-size-fits-all
    solution. It can vary based on the specific pre-trained model in use, as well
    as other factors like model size and the proportion of original data. Fine-tuning
    these ratios often requires empirical adjustments to achieve the desired balance
    between general and domain-specific capabilities.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，值得注意的是，这个10%到20%的比例并不是一种通用的解决方案。它可能会根据所使用的特定预训练模型以及模型大小和原始数据比例等其他因素而有所不同。调整这些比例通常需要通过实证调整，以实现通用能力和领域特定能力之间的理想平衡。
- en: When it comes to Supervised Fine-Tuning (SFT), the data ratio can be more flexible,
    sometimes even reaching a 1:1 balance for promising results. But this flexibility
    is contingent on the total volume of data used in SFT. If the dataset is small,
    the impact of this mixed data approach could be minimal.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于监督微调（SFT），数据比例可以更加灵活，有时甚至可以达到1:1的平衡以获得良好的结果。但这种灵活性取决于SFT中使用的数据总量。如果数据集较小，这种混合数据方法的影响可能会很小。
- en: When devising strategies to address industry-specific challenges, it’s essential
    to tailor the approach based on task complexity. For simpler tasks, well-crafted
    prompts are often more effective, especially when combined with a vector database.
    This approach can solve the majority of industry problems. For moderately complex
    tasks, Supervised Fine-Tuning (SFT) is generally more effective, particularly
    when some general data is mixed in. This can address most of the remaining issues.
    For more complex challenges, it may be prudent to wait for general large models
    to further enhance their capabilities.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在制定应对行业特定挑战的策略时，必须根据任务复杂性量身定制方法。对于较简单的任务，精心制作的提示通常更有效，特别是当与向量数据库结合使用时。这种方法可以解决大多数行业问题。对于中等复杂度的任务，监督微调（SFT）通常更有效，尤其是当混入一些通用数据时。这可以解决大多数剩余问题。对于更复杂的挑战，可能需要等待通用大模型进一步提升其能力。
- en: Conclusion
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: As the field of AI continues to broaden, the role of industry-specific large
    models becomes increasingly critical in addressing specialized demands. However,
    it’s crucial to understand that these specialized models are not standalone constructs.
    They are, in essence, specialized applications of general large models tailored
    to meet unique industry needs. These industry-specific iterations act as interim
    solutions, bridging the gaps that general models currently can’t fill. Yet, as
    general models continue to evolve, the need for such specialized adaptations may
    diminish, making way for more universally applicable solutions.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人工智能领域的不断拓展，行业特定的大模型在满足专业需求方面变得越来越重要。然而，必须理解这些专业化模型不是独立存在的构件。它们本质上是针对特定行业需求定制的通用大模型的专门应用。这些行业特定的迭代版本充当了过渡性解决方案，弥补了当前通用模型无法填补的空白。然而，随着通用模型的持续演进，这种专业化适配的需求可能会减少，更多的通用解决方案可能会取而代之。
- en: Selecting the appropriate strategy to tackle industry-specific challenges is
    a nuanced endeavor. Among the complexities is the often-underestimated factor
    of data ratio, which can profoundly influence a model’s effectiveness. For straightforward
    tasks, the use of carefully designed prompts can yield excellent results, whereas
    more complex issues may require the enhanced capabilities of evolving general
    large models. In this ever-changing environment, the key to developing impactful
    industry solutions lies in the meticulous balancing of data types, model architecture,
    and task intricacy.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 选择适当的策略来应对行业特定的挑战是一个复杂的过程。其中的复杂性包括常被低估的数据比例因素，这可以深刻影响模型的有效性。对于简单的任务，使用精心设计的提示可以产生优异的结果，而对于更复杂的问题，可能需要不断发展的通用大模型的增强能力。在这个不断变化的环境中，开发有影响力的行业解决方案的关键在于对数据类型、模型架构和任务复杂度的精细平衡。
