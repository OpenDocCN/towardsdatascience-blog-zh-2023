- en: Implement Behaviour Driven Development in data pipelines using Mage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/implement-behaviour-driven-development-in-data-pipelines-using-mage-19496fea7890](https://towardsdatascience.com/implement-behaviour-driven-development-in-data-pipelines-using-mage-19496fea7890)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Maximize the quality and productivity of your data pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@xiaoxugao?source=post_page-----19496fea7890--------------------------------)[![Xiaoxu
    Gao](../Images/8712a7e5f3bad0d2abd7e04792fad66f.png)](https://medium.com/@xiaoxugao?source=post_page-----19496fea7890--------------------------------)[](https://towardsdatascience.com/?source=post_page-----19496fea7890--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----19496fea7890--------------------------------)
    [Xiaoxu Gao](https://medium.com/@xiaoxugao?source=post_page-----19496fea7890--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----19496fea7890--------------------------------)
    ·7 min read·Jul 6, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/491eca353ee8a49b28a714e7d2305c7f.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Nick Fewings](https://unsplash.com/@jannerboy62) on [Unsplash](https://unsplash.com/)
  prefs: []
  type: TYPE_NORMAL
- en: In my [previous articles](https://medium.com/towards-data-science/how-to-create-valuable-data-tests-850e778718e1),
    I talked a lot about the importance of testing in data pipelines and how to create
    data tests and unit tests respectively. While testing plays an essential role,
    it may not always be the most exciting part of the development cycle. As a result,
    many modern data stacks have introduced frameworks or plugins to expedite the
    implementation of data tests. In addition, unit testing frameworks in Python such
    as Pytest, unittest have been there for a long time, helping engineers efficiently
    create unit tests for data pipelines and any Python applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, I want to introduce a setup that uses two modern techniques:
    Behaviour Driven Development (BDD) — a business-oriented testing framework, and
    [Mage](https://github.com/mage-ai/mage-ai) — a modern data pipeline tool. By combining
    these two techniques, the objective is to create high-quality unit tests for data
    pipelines while having a seamless developer experience.'
  prefs: []
  type: TYPE_NORMAL
- en: What is Behaviour Driven Development (BDD)?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When building data pipelines for business, it’s highly likely that we will encounter
    complicated and tricky business logic. One example is to define customer segmentation
    based on a combination of age, income, and past purchases. The following example
    represents only a fraction of the complexity that business logic can entail. It
    can become progressively complicated as there are more attributes and granularity
    within each attribute. Think about one example in your daily job!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: So the question is where should the business rules be documented and how to
    ensure the synchronization between the documentation and the code. One common
    approach is to include comments alongside the code or strive to write code that
    is self-explanatory and easily understandable. But there is still a risk of having
    outdated comments or code that stakeholders find challenging to comprehend.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, what we are looking for is a “documentation-as-code” solution that
    can benefit both engineers and business stakeholders and this is exactly what
    BDD can provide. If you are familiar with the concept of “data contract”, BDD
    can be seen as a form of data contract, but with a focus on the stakeholders rather
    than the data source. **It can be very beneficial, particularly for data pipelines
    with complicated business logic, and it helps prevent debates regarding “feature
    or bug”.**
  prefs: []
  type: TYPE_NORMAL
- en: BDD is essentially a software development approach that emphasizes collaboration
    and communication between stakeholders and developers to ensure that software
    meets the desired business outcomes. The behavior is described in scenarios that
    illustrate the expected inputs and outcomes. Each scenario is written in a specific
    format of “Given-When-Then” where each step describes a specific condition or
    action.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see what the scenarios may look like for the customer segmentation example.
    Since the feature file is written in English, it can be well understood by business
    stakeholders and they can even contribute to it. It works like a contract between
    stakeholders and engineers, where engineers are responsible for accurately implementing
    the requirements, and stakeholders are expected to provide all the necessary information.
  prefs: []
  type: TYPE_NORMAL
- en: Having a clear contract between stakeholders and engineers helps correctly categorize
    the data issue, distinguishing between “software bugs” resulting from implementation
    errors and “feature requests” due to missing requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Feature file (Created by Author)
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to generate test code from the feature and that’s where the
    connection happens. The Pytest code acts as a bridge between the documentation
    and the implementation code. When there is any misalignment between them, the
    tests will fail, highlighting the need for synchronization between documentation
    and implementation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/12b13781fde338d849612f472f942bb7.png)'
  prefs: []
  type: TYPE_IMG
- en: Test code acts as a bridge (Created by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Here is what the test code looks like. To keep the example short, I only implement
    the test code for the first scenario. The Given steps set up the initial context
    for the scenario which in this case gets customer age, income, past purchases
    data from the examples. The When step triggers the behavior being tested which
    is `get_user_segment` function. In the Then step, we compare the result from the
    When step with the expected output from the scenario example.
  prefs: []
  type: TYPE_NORMAL
- en: Test code of the 1st scenario in the feature file(Created by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a change to the age range specified in the first scenario where an example
    age of 62 is added without updating the code. In such as case, the test would
    immediately fail because the code has conflicting expectations.
  prefs: []
  type: TYPE_NORMAL
- en: What is Mage?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we’ve seen the potential of BDD and learned how to implement it using
    Python. Now, it’s time to incorporate BDD into our data pipelines. When it comes
    to data orchestration, Airflow as the first Python-based orchestrator with a web
    interface has become the most commonly used tool for executing data pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: But it certainly is not perfect. For example, testing pipelines outside of the
    production environment, especially when using operators like KubernetesOperator,
    can be challenging. Additionally, a DAG can be cluttered with boilerplate code
    and complicated configurations, making it less straightforward to tell the purpose
    of each task, be it ingestion, transformation, or exportation. Furthermore, Airflow
    is not focused on being a data-driven orchestration tool because it concerns more
    about the successful execution of tasks rather than the quality of the final data
    assets.
  prefs: []
  type: TYPE_NORMAL
- en: As the community of data engineering grows, many Airflow alternatives have come
    out to fill the gap present in Airflow. Mage is one of the growing data pipeline
    tools that is seen as a modern replacement for Airflow. Its four design concepts
    set Mage apart from Airflow and we can sense the difference right from the beginning
    of the development cycle.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/677d100b3bcd4920407d71977867ddda.png)'
  prefs: []
  type: TYPE_IMG
- en: Design principles of Mage (Created by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Mage has a very intuitive UI that allows engineers to swiftly edit and test
    the pipeline with ease and efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each pipeline is composed of several types of blocks: @data_loader, @transformer,
    @data_exporter, etc, with a clear purpose. This is one of my favorite features
    because I can immediately understand the objective of each task and focus on the
    business logic rather than getting caught up in boilerplate code.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e0788c5e73933f45e2367e6acde7d04e.png)'
  prefs: []
  type: TYPE_IMG
- en: Mage UI (Created by Author)
  prefs: []
  type: TYPE_NORMAL
- en: BDD + Mage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A regular data pipeline has three main steps: ingestion, transformation, and
    exportation. Transformation is the place where all the complicated business logic
    is implemented, and it’s not uncommon to have multiple transformation steps incorporated.'
  prefs: []
  type: TYPE_NORMAL
- en: The clear segregation of the ingestion task and transformation task makes it
    incredibly straightforward and intuitive to apply BDD to your transformation logic.
    In fact, it feels just like testing a regular Python function, ignoring the fact
    that it’s part of a data pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go back to the user segmentation example. The business rules are supposed
    to sit in @transformer block and it is decoupled from the loader and exported.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aa755e19cd0a23c5f7679bc31c408960.png)'
  prefs: []
  type: TYPE_IMG
- en: '@transformer block in data pipeline (Created by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: The same @transformer block can be plugged into multiple pipelines as long as
    the loader returns a pandas dataframe. To run the test, we just need to run `pytest`
    command in the terminal or in the CI/CD pipeline. The pipeline configuration such
    as the trigger is in a separate file which keeps the main pipeline file as clean
    as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s imagine what would happen if we implement this in Airflow. As it’s not
    a complicated example, Airflow can certainly handle it well. But there are a few
    details that make me feel “errrr” when I switched from Mage to Airflow.
  prefs: []
  type: TYPE_NORMAL
- en: DAG file gets cluttered as each DAG has a big code block to define its metadata.
    In Mage, the configuration is moved to a yaml file, so the pipeline file keeps
    concise.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 2\. Data-passing is tricky in Airflow. XCOM in Airflow is used for passing data
    between tasks. However, it’s not recommended to pass large datasets such as dataframes
    directly through XCOM. As a workaround, we need to persist the data in temporary
    storage first which seems to be an unnecessary engineering effort. Mage handles
    the data passing naturally for us and we don’t need to worry about the size of
    the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Technically, Airflow supports several versions of Python packages, but with
    a big cost. KubernetesPodOperator and PythonVirtualenvOperator allow you to run
    a task in an isolated environment. But you will lose all the convenience that
    comes out-of-the-box with Airflow such as using another operator. In contrast,
    Mage addresses this challenge by using one centralized `requirements.txt`, ensuring
    that all tasks have access to all the native features of Mage.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this article, I brought two technologies together with the goal of improving
    test quality and developer experience. BDD aims to enhance the collaboration between
    stakeholders and engineers by creating a contract in the format of a feature file
    that is directly embedded within the codebase. On the other hand, Mage is a great
    data pipeline tool that keeps developer experience as their top priority and truly
    treats data as a first-class citizen.
  prefs: []
  type: TYPE_NORMAL
- en: I hope you find it inspiring and feel motivated to explore and incorporate at
    least one technology into your daily work. Making the right tooling choice can
    certainly amplify your team’s productivity. I’m curious about what you think.
    Let me know in the comments. Cheers!
  prefs: []
  type: TYPE_NORMAL
