- en: 'SGDRegressor with Scikit-Learn: Untaught Lessons You Need to Know'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Scikit-Learn的SGDRegressor：你需要知道的未授课程
- en: 原文：[https://towardsdatascience.com/sgdregressor-with-scikit-learn-untaught-lessons-you-need-to-know-cf2430439689](https://towardsdatascience.com/sgdregressor-with-scikit-learn-untaught-lessons-you-need-to-know-cf2430439689)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/sgdregressor-with-scikit-learn-untaught-lessons-you-need-to-know-cf2430439689](https://towardsdatascience.com/sgdregressor-with-scikit-learn-untaught-lessons-you-need-to-know-cf2430439689)
- en: Unveiling Hidden Algorithmic Relationships through a Confusing Name
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过令人困惑的名称揭示隐藏的算法关系
- en: '[](https://medium.com/@angela.shi?source=post_page-----cf2430439689--------------------------------)[![Angela
    and Kezhan Shi](../Images/a89d678f2f3887c0c2ff3928f9d767b4.png)](https://medium.com/@angela.shi?source=post_page-----cf2430439689--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cf2430439689--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cf2430439689--------------------------------)
    [Angela and Kezhan Shi](https://medium.com/@angela.shi?source=post_page-----cf2430439689--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@angela.shi?source=post_page-----cf2430439689--------------------------------)[![Angela
    and Kezhan Shi](../Images/a89d678f2f3887c0c2ff3928f9d767b4.png)](https://medium.com/@angela.shi?source=post_page-----cf2430439689--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cf2430439689--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cf2430439689--------------------------------)
    [Angela and Kezhan Shi](https://medium.com/@angela.shi?source=post_page-----cf2430439689--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cf2430439689--------------------------------)
    ·13 min read·Mar 8, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cf2430439689--------------------------------)
    ·阅读时间13分钟·2023年3月8日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: In the field of machine learning, the linear model is a fundamental technique
    that is widely used to predict numerical values based on input data. The SGDRegressor
    estimator from scikit-learn is a powerful tool that allows machine learning practitioners
    to perform linear regression quickly and efficiently.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习领域，线性模型是一种基本技术，广泛用于根据输入数据预测数值。Scikit-learn中的SGDRegressor估算器是一个强大的工具，可以让机器学习从业者快速高效地进行线性回归。
- en: However, the name SGDRegressor can be a bit confusing for beginners. In this
    article, we will explain how it works and examine why the name may be misleading
    for beginners who are just starting to learn about machine learning.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，SGDRegressor的名称对于初学者来说可能有些混淆。本文将解释它的工作原理，并探讨为何这个名称对于刚开始学习机器学习的初学者来说可能会产生误导。
- en: Furthermore, we will delve into the idea that there are actually multiple models
    hidden within SGDRegressor, each with its own specific set of parameters and hyperparameters.
    This will raise interesting questions about the definition of models in machine
    learning. For instance, is ridge regression or SVR a separate model, or is it
    simply one tuned version of a linear model?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将深入探讨SGDRegressor中实际上隐藏了多个模型的观点，每个模型都有其特定的参数和超参数。这将引发关于机器学习中模型定义的有趣问题。例如，岭回归或SVR是一个独立的模型，还是一个调优过的线性模型？
- en: By the time you finish reading this article, you will have a better understanding
    of the inner workings of SGDRegressor and gain a newfound appreciation for the
    complexities, and well actually, the simplicity of linear models in machine learning.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在你读完这篇文章时，你将更好地理解SGDRegressor的内部工作原理，并对线性模型在机器学习中的复杂性以及*其实*的简单性有新的认识。
- en: 1\. The Usually Taught Lesson of SGDRegressor
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. SGDRegressor的通常教学内容
- en: SGDRegressor is a machine learning algorithm in Scikit-Learn that implements
    Stochastic Gradient Descent (SGD) to solve regression problems. It is a popular
    choice for large-scale regression tasks due to its ability to handle high-dimensional
    datasets and its fast training time.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: SGDRegressor是Scikit-Learn中的一种机器学习算法，它实现了随机梯度下降（SGD）来解决回归问题。由于其处理高维数据集的能力和快速的训练时间，它是大规模回归任务的热门选择。
- en: SGDRegressor works by iteratively updating the model weights using a small random
    subset of the training data, rather than the entire dataset, which makes it computationally
    efficient for large datasets. It also includes several hyperparameters that can
    be tuned to optimize performance, including learning rate, penalty or regularization
    term, and number of iterations.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '`SGDRegressor` 通过使用训练数据的小随机子集而不是整个数据集来迭代地更新模型权重，这使得它在处理大数据集时计算上更高效。它还包括几个可以调整的超参数，以优化性能，包括学习率、惩罚或正则化项和迭代次数。'
- en: 1.1 Linear regressor
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 线性回归
- en: 'SGDRegressor is a linear model that uses a linear function to predict the target
    variable. The linear function takes the form of:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '`SGDRegressor` 是一个线性模型，使用线性函数来预测目标变量。线性函数的形式为：'
- en: y = w[0] + w[1] * x[1] + … + w[p] * x[p]
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: y = w[0] + w[1] * x[1] + … + w[p] * x[p]
- en: where x[1] to x[p] are the input features, w[1] to w[p] are the coefficients
    of the linear model, and w[0] or b is the intercept term. The objective of the
    SGDRegressor algorithm is to find the values of w and b that minimize a loss function
    that has to be defined between the predicted values and the actual values of the
    target variable.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 x[1] 到 x[p] 是输入特征，w[1] 到 w[p] 是线性模型的系数，w[0] 或 b 是截距项。`SGDRegressor` 算法的目标是找到
    w 和 b 的值，使得在预测值和目标变量的实际值之间定义的损失函数最小化。
- en: 1.2 Stochastic Gradient Descent vs. Gradient Descent
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 随机梯度下降 vs. 梯度下降
- en: The SGDRegressor algorithm uses stochastic gradient descent for optimization.
    Stochastic gradient descent is an iterative optimization algorithm that updates
    the parameters of the model in small batches of data. The algorithm updates the
    parameters using the gradient of the cost function with respect to the parameters.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '`SGDRegressor` 算法使用随机梯度下降进行优化。随机梯度下降是一种迭代优化算法，它在小批量数据中更新模型参数。该算法使用相对于参数的代价函数的梯度来更新参数。'
- en: While SGD and GD are both widely used optimization algorithms in machine learning,
    their effectiveness depends on the specific problem at hand. SGD is typically
    faster and better for large datasets and non-convex problems, while GD is more
    reliable for smaller datasets and convex problems.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 SGD 和 GD 都是机器学习中广泛使用的优化算法，但它们的效果取决于具体的问题。SGD 通常对大数据集和非凸问题更快且效果更好，而 GD 对小数据集和凸问题更可靠。
- en: 1.3 Using SGDRegressor in scikit-learn
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 在 scikit-learn 中使用 `SGDRegressor`
- en: Using SGDRegressor in scikit-learn is straightforward. First, we need to import
    the SGDRegressor class from the linear_model module of scikit-learn. Then, we
    can create an instance of the SGDRegressor class and fit the model to our training
    data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `SGDRegressor` 在 scikit-learn 中非常简单。首先，我们需要从 scikit-learn 的 `linear_model`
    模块中导入 `SGDRegressor` 类。然后，我们可以创建一个 `SGDRegressor` 类的实例，并将模型拟合到我们的训练数据上。
- en: 'Here’s an example of how to use SGDRegressor in scikit-learn:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何在 scikit-learn 中使用 `SGDRegressor` 的示例：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this example, we load the Boston housing dataset and split it into training
    and test sets using the train_test_split function. Then, we create an instance
    of SGDRegressor and fit the model to the training data using the fit method. Finally,
    we make predictions on the test data using the predict method and calculate the
    mean squared error of the predictions using the mean_squared_error function.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们加载了波士顿房价数据集，并使用 `train_test_split` 函数将其拆分为训练集和测试集。然后，我们创建了一个 `SGDRegressor`
    实例，并使用 `fit` 方法将模型拟合到训练数据上。最后，我们使用 `predict` 方法对测试数据进行预测，并使用 `mean_squared_error`
    函数计算预测的均方误差。
- en: 2\. Unpacking the Confusing Name of SGDRegressor for Beginners
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 解开 SGDRegressor 对初学者的困惑名字
- en: In this section, we will discuss why the name SGDRegressor can be confusing
    for beginners in machine learning. While experts in the field may be familiar
    with the name and its associated algorithm, those new to the field may not immediately
    understand what it does or how it works.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论为什么 `SGDRegressor` 这个名字对机器学习初学者来说可能会令人困惑。尽管领域内的专家可能熟悉这个名字及其相关算法，但对于新手来说，可能无法立即理解它的功能或工作原理。
- en: However, we will also explain that for experts, the name is not necessarily
    a source of confusion. This is because they are already familiar with the algorithm
    and its purpose, and can easily distinguish it from other similar algorithms.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们还将解释，对于专家来说，这个名字不一定是困惑的来源。这是因为他们已经熟悉了该算法及其目的，并且能够很容易地将其与其他类似的算法区分开来。
- en: 2.1 Why it is confusing
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 为什么它会令人困惑
- en: 'In my article [Machine Learning in Three Steps: How to Efficiently Learn It](/machine-learning-in-three-steps-how-to-efficiently-learn-it-aefcf423a9e1),
    I explained how to improve your learning of machine learning algorithms by breaking
    down the process into three distinct steps: model algorithm, fitting algorithm,
    and tuning algorithm. Despite the simplicity of this approach, it can be challenging
    to apply in practice. The SGDRegressor algorithm serves as a prime example of
    this.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的文章[《三步学会机器学习：如何高效学习》](/machine-learning-in-three-steps-how-to-efficiently-learn-it-aefcf423a9e1)中，我解释了如何通过将学习过程分解为三个不同的步骤：模型算法、拟合算法和调优算法，来提高你对机器学习算法的学习。尽管这种方法简单，但在实践中可能难以应用。SGDRegressor算法就是一个典型的例子。
- en: In my opinion, choosing an appropriate name that accurately reflects the characteristics
    of a machine learning algorithm is crucial for understanding and effectively using
    the algorithm. However, this is not always the case, as evidenced by the confusingly
    named “SGDRegressor” algorithm.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为，选择一个准确反映机器学习算法特征的合适名称对于理解和有效使用该算法至关重要。然而，这并不总是如此，例如混淆的“SGDRegressor”算法。
- en: In scikit-learn, there is a convention of adding either “regressor” or “classifier”
    to the name of the machine learning model, depending on whether it is used for
    regression or classification tasks. This convention is evident in many examples,
    such as DecisionTreeClassifier vs. DecisionTreeRegressor, KNeighborsClassifier
    vs. KNeighborsRegressor, and so on.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中，根据机器学习模型是用于回归还是分类任务，有一个惯例是将“regressor”或“classifier”添加到模型名称中。这一惯例在许多示例中都很明显，例如DecisionTreeClassifier与DecisionTreeRegressor，KNeighborsClassifier与KNeighborsRegressor等。
- en: However, the problem with the name “SGDRegressor” is that if we apply this naming
    convention to it, it implies that “SGD” is a machine learning model when in reality,
    it is a fitting algorithm. This can be particularly confusing for beginners who
    may not have a clear understanding of the different components of machine learning
    algorithms.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，“SGDRegressor”这个名称的问题在于，如果我们将这一命名惯例应用于它，它会暗示“SGD”是一个机器学习模型，而实际上它是一个拟合算法。这对于可能对机器学习算法的不同组件没有清晰理解的初学者来说尤其困惑。
- en: 2.2 How to explain this naming
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 如何解释这一命名
- en: For experts, the naming convention of “SGDRegressor” may seem acceptable. The
    use of “SGD” suggests that this model is based on mathematical functions, or parametric
    models, rather than distance or tree-based models. Therefore, “SGD” implies the
    hidden models used despite the fact that SGD itself is a fitting algorithm.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于专家来说，“SGDRegressor”的命名惯例可能看起来可以接受。使用“SGD”暗示该模型基于数学函数或参数模型，而非距离或树状模型。因此，“SGD”暗示了尽管SGD本身是一个拟合算法，但所使用的隐藏模型。
- en: Although in theory, SGD can be used for both linear and non-linear models, such
    as neural networks, in practice, this estimator only implements linear models.
    So you will say that a more precise and concise name for this estimator could
    be “**LinearSGDRegressor**”! Yes, that’s right! But do you also notice that “SGDRegressor”
    is in the “linear_model” module, which by definition only implements linear models?!
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然理论上SGD可以用于线性和非线性模型，例如神经网络，但实际上，这个估计器仅实现了线性模型。因此，你会说这个估计器的一个更精确和简洁的名称可能是“**LinearSGDRegressor**”！是的，没错！但你是否也注意到“SGDRegressor”位于“linear_model”模块中，该模块定义上只实现了线性模型？！
- en: Ultimately, it seems that the name SGDRegressor is appropriate, even though
    SGD refers specifically to the fitting algorithm, as this algorithm is only used
    for mathematical function-based models. Additionally, given that SGDRegressor
    is in the “linear_model” module, it’s clear that the model used is the linear
    model.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，虽然SGD指的是具体的拟合算法，但由于此算法仅用于基于数学函数的模型，因此SGDRegressor这个名称似乎是合适的。此外，鉴于SGDRegressor位于“linear_model”模块中，这表明所使用的模型是线性模型。
- en: 3\. Hidden Models in SGDRegressor
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. SGDRegressor中的隐藏模型
- en: The parameters available in SGDRegressor give us the flexibility to choose different
    combinations of loss (squared_error, huber, epsilon_insensitive, or squared_epsilon_insensitive)
    and penalty (l1, l2, elasticnet or none) functions. Some of these combinations
    correspond to traditional statistical models.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: SGDRegressor中可用的参数让我们能够选择不同的损失函数（squared_error、huber、epsilon_insensitive或squared_epsilon_insensitive）和惩罚函数（l1、l2、elasticnet或none）的组合。这些组合中的一些对应于传统的统计模型。
- en: It is interesting to note that, historically, statisticians developed and constructed
    different models based on different assumptions and objectives. In contrast, machine
    learning provides a more unified framework where the linear model remains the
    same, but the loss function and penalty can be changed to achieve different objectives.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，历史上，统计学家根据不同的假设和目标开发和构建了不同的模型。相比之下，机器学习提供了一个更统一的框架，其中线性模型保持不变，但损失函数和惩罚可以更改，以实现不同的目标。
- en: 3.1 Loss functions
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 损失函数
- en: The loss functions squared_error, huber, epsilon_insensitive, and squared_epsilon_insensitive
    differ in their impact on the model and its performance.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数 squared_error、huber、epsilon_insensitive 和 squared_epsilon_insensitive 在对模型及其性能的影响上有所不同。
- en: The squared_error loss function, also known as mean squared error (MSE), penalizes
    larger errors more heavily than smaller ones, making it sensitive to outliers.
    This loss function is commonly used in linear regression.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: squared_error 损失函数，也称为均方误差（MSE），对较大的错误的惩罚程度比对较小的错误更重，使其对异常值敏感。这个损失函数在线性回归中常用。
- en: The huber loss function is less sensitive to outliers than the squared_error
    loss function, as it transitions from quadratic to linear error for larger residuals.
    This makes it a good choice for datasets with some outliers.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: huber 损失函数比 squared_error 损失函数对异常值的敏感性更低，因为它在较大的残差下从二次误差过渡到线性误差。这使得它在处理有些异常值的数据集时是一个不错的选择。
- en: The epsilon_insensitive loss function is used for regression problems where
    the target values are expected to fall within a certain range of values. It ignores
    errors smaller than a certain threshold, known as epsilon, and penalizes larger
    errors linearly. This loss function is commonly used in support vector regression.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: epsilon_insensitive 损失函数用于目标值预计在一定范围内的回归问题。它忽略小于某一阈值（称为 epsilon）的错误，并对较大的错误进行线性惩罚。这个损失函数在支持向量回归中常用。
- en: The squared_epsilon_insensitive loss function is similar to the epsilon_insensitive
    loss function, but it penalizes larger errors more heavily by squaring them. This
    loss function can be useful when larger errors need to be penalized more than
    linearly, but smaller errors can be ignored.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: squared_epsilon_insensitive 损失函数类似于 epsilon_insensitive 损失函数，但它通过对较大的错误进行平方惩罚来加重对较大错误的惩罚。当较大的错误需要比线性更多地惩罚时，这个损失函数可以很有用，但较小的错误可以被忽略。
- en: Overall, the choice of loss function can impact the model’s ability to handle
    outliers and the level of sensitivity to errors. It is important to select the
    appropriate loss function for the specific problem at hand to ensure the best
    possible performance.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，损失函数的选择会影响模型处理异常值的能力以及对错误的敏感性。为确保最佳性能，选择适合特定问题的损失函数是非常重要的。
- en: 'Here is an image summarizing all the loss functions with their mathematical
    expressions. Python implementations are also provided, allowing us to visualize
    and compare their behaviors. If you would like to access the Python code, you
    can support me on Ko-fi through the following link: [https://ko-fi.com/s/4cc6555852](https://ko-fi.com/s/4cc6555852).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一张总结所有损失函数及其数学表达式的图片。也提供了 Python 实现，允许我们可视化和比较它们的行为。如果你想访问 Python 代码，你可以通过以下链接在
    Ko-fi 上支持我：[https://ko-fi.com/s/4cc6555852](https://ko-fi.com/s/4cc6555852)。
- en: '![](../Images/50fc7e385e233b8961589888278e900b.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/50fc7e385e233b8961589888278e900b.png)'
- en: loss function in SGDRegressor — image by the author
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: SGDRegressor 中的损失函数 — 图片由作者提供
- en: It should be noted that even though the SGDRegressor does not explicitly offer
    MAE loss, it can still be achieved by setting the epsilon insensitive loss function’s
    hyperparameter, epsilon, to zero. This is because the mathematical expression
    of the epsilon insensitive loss becomes equivalent to MAE loss when epsilon is
    zero.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，尽管 SGDRegressor 并未明确提供 MAE 损失，但可以通过将 epsilon 不敏感损失函数的超参数 epsilon 设置为零来实现
    MAE 损失。这是因为，当 epsilon 为零时，epsilon 不敏感损失的数学表达式等同于 MAE 损失。
- en: 'In the end, despite the variety of loss functions, we can observe that their
    construction is based on two fundamental concepts: the basic loss being either
    quadratic or absolute, and the introduction of the epsilon-tube concept, which
    creates two distinct zones — the central and collateral zones — with different
    types of loss functions.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，尽管损失函数的种类繁多，我们可以观察到它们的构造基于两个基本概念：基本损失是二次的或绝对的，以及引入 epsilon-tube 概念，它创建了两个不同的区域——中央区域和边缘区域——具有不同类型的损失函数。
- en: By adopting this approach, one can potentially create their own customized loss
    function!
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 通过采用这种方法，您可以潜在地创建自己定制的损失函数！
- en: 3.2 Penalty term
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 惩罚项
- en: When using the SGDRegressor, we can also specify penalties to be used in addition
    to the chosen loss function. The available penalties are L1, L2, Elastic Net,
    and None.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 SGDRegressor 时，我们还可以指定除了所选择的损失函数外还使用的惩罚项。可用的惩罚项包括 L1、L2、Elastic Net 和 None。
- en: L1 penalty adds the absolute value of the coefficients to the loss function,
    leading to sparse models where some coefficients are set to zero. This penalty
    can be useful for feature selection and reducing overfitting.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: L1 惩罚将系数的绝对值添加到损失函数中，从而导致一些系数被设置为零的稀疏模型。该惩罚对特征选择和减少过拟合是有用的。
- en: L2 penalty adds the square of the coefficients to the loss function, leading
    to smaller but non-zero coefficients. This penalty can also help reduce overfitting
    and improve model generalization.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: L2 惩罚将系数的平方添加到损失函数中，从而导致较小但非零的系数。该惩罚还可以帮助减少过拟合并提高模型的泛化能力。
- en: 'Elastic Net penalty combines both L1 and L2 penalties, allowing for both sparsity
    and non-zero coefficients. It has two hyperparameters: alpha controls the weight
    between L1 and L2 penalties, and l1_ratio controls the balance between L1 and
    L2 penalties.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Elastic Net 惩罚结合了 L1 和 L2 惩罚，允许既有稀疏性又有非零系数。它有两个超参数：alpha 控制 L1 和 L2 惩罚之间的权重，l1_ratio
    控制 L1 和 L2 惩罚之间的平衡。
- en: Finally, None means no penalty is used, and the model is fitted with just the
    chosen loss function.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，None 意味着不使用惩罚，模型仅用所选择的损失函数进行拟合。
- en: Choosing the right penalty depends on the specific problem and the nature of
    the data. In general, L1 and Elastic Net penalties can help with feature selection
    and sparse models, while L2 penalty can help with generalization and avoiding
    overfitting.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的惩罚项取决于具体问题和数据的性质。一般来说，L1 和 Elastic Net 惩罚有助于特征选择和稀疏模型，而 L2 惩罚有助于泛化和避免过拟合。
- en: Here is an interesting visualization from the [documentation of scikit learn](https://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_penalties.html)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是来自[scikit learn 文档](https://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_penalties.html)的有趣可视化
- en: '![](../Images/038472824e10b4d744c0879343f4721a.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/038472824e10b4d744c0879343f4721a.png)'
- en: Penalty terms from [scikit learn](https://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_penalties.html)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 [scikit learn](https://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_penalties.html)
    的惩罚项
- en: 3.3 Presentation of the Hidden Models
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 隐藏模型的展示
- en: The SGDRegressor offers flexibility in specifying different combinations of
    loss and penalty parameters. Some combinations of loss and penalty parameters
    in SGDRegressor correspond to well-known models with their specific names. The
    following image provides an overview of these options, including the corresponding
    model names, and we will delve into a few specific ones.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: SGDRegressor 提供了在指定不同组合的损失和惩罚参数方面的灵活性。在 SGDRegressor 中，某些损失和惩罚参数的组合对应于具有特定名称的知名模型。以下图像提供了这些选项的概述，包括相应的模型名称，我们将详细探讨其中的一些。
- en: '![](../Images/825c4489bb50cab58431ac810c95c59e.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/825c4489bb50cab58431ac810c95c59e.png)'
- en: Loss functions and penalty terms in SGDRegressor and corresponding models names
    — image by the author
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: SGDRegressor 中的损失函数和惩罚项及其对应模型名称 — 图像由作者提供
- en: '**Squared error**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**平方误差**'
- en: The simplest combination in SGDRegressor is squared_error with no penalty, which
    corresponds to the classic linear regression model available in Scikit-learn’s
    LinearRegression estimator. However, this naming convention can be misleading.
    Although all these models are regressors and linear, referring to them as “linear
    regression” can cause confusion. To avoid this, it is best to use the term “linear
    model” instead and reserve the term “linear regression” for OLS regression specifically.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SGDRegressor 中，最简单的组合是 squared_error 无惩罚，这对应于 Scikit-learn 的 LinearRegression
    估计器中的经典线性回归模型。然而，这种命名方式可能会产生误导。虽然所有这些模型都是回归模型且为线性模型，但将它们称为“线性回归”可能会造成混淆。为了避免这种情况，最好使用“线性模型”一词，而将“线性回归”一词专门保留用于
    OLS 回归。
- en: Lasso is a linear (OLS) regression model with L1 regularization, which encourages
    sparsity in the coefficient estimates.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Lasso 是一种具有 L1 正则化的线性 (OLS) 回归模型，鼓励系数估计的稀疏性。
- en: Ridge regression, on the other hand, adds an L2 penalty to the linear (OLS)
    regression model to help mitigate the effects of multicollinearity.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，岭回归在线性 (OLS) 回归模型中添加了 L2 惩罚，以帮助减轻多重共线性的影响。
- en: ElasticNet combines the penalties of both L1 and L2 regularization in order
    to achieve a balance between the sparsity of Lasso and the stability of Ridge.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ElasticNet结合了L1和L2正则化的惩罚，以在Lasso的稀疏性和Ridge的稳定性之间取得平衡。
- en: With the squared error, we can easily identify specific model names for each
    penalty used. However, when it comes to other loss functions, there is not always
    a specific name associated with each penalty. In my opinion, researchers historically
    focused on finding the effects of regularization or penalization of the coefficients
    for linear regressions, leading to specific names being given to each type of
    penalty. For other loss functions, adding a penalty term no longer seems novel.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 使用平方误差时，我们可以轻松识别每个惩罚对应的具体模型名称。然而，对于其他损失函数，并不总是有特定的名称与每个惩罚关联。在我看来，研究人员历史上专注于寻找正则化或线性回归系数的惩罚效果，导致每种惩罚类型都有特定名称。对于其他损失函数，添加惩罚项似乎不再新颖。
- en: '**Huber loss**'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**Huber损失**'
- en: Huber loss function is an alternative loss function that is less sensitive to
    outliers, making it a good choice for datasets with significant outliers. The
    Huber loss function is a combination of the squared loss function and the absolute
    loss function. For small errors, it behaves like the squared loss function, while
    for larger errors, it behaves like the absolute loss function. This makes it more
    robust to outliers than the squared loss function, while still providing good
    performance for smaller errors.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Huber损失函数是一种对离群值不那么敏感的替代损失函数，适用于具有显著离群值的数据集。Huber损失函数是平方损失函数和绝对损失函数的组合。对于小误差，它的行为像平方损失函数，而对于大误差，它的行为像绝对损失函数。这使得它比平方损失函数对离群值更具鲁棒性，同时对小误差仍提供良好的性能。
- en: '**Epsilon insensitive loss**'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**Epsilon不敏感损失**'
- en: Epsilon-insensitive loss is another type of loss function that is commonly used
    in a linear model. The epsilon parameter is used to define the range within which
    the error is considered to be zero. This loss function is useful for datasets
    with noisy output variables, as it can help to reduce the impact of small fluctuations
    in the output variable.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Epsilon-不敏感损失是另一种常用于线性模型的损失函数。epsilon 参数用于定义在此范围内的误差被视为零。此损失函数对具有噪声输出变量的数据集非常有用，因为它可以帮助减少输出变量中小波动的影响。
- en: In fact, the combination of Epsilon-insensitive loss and L2 regularization is
    also referred to as SVR (Support Vector Regression). The use of the epsilon-insensitive
    tube concept makes it mandatory to add a penalty as there can be an infinite number
    of solutions without it. The term “support vector” is used because just as the
    L1 regularization term for the coefficients will lead to certain coefficients
    becoming zero, the L1 loss (Absolute loss) applied to the dataset will lead to
    certain data points not being used to calculate the coefficients, leaving only
    the remaining ones, which are called support vectors.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，Epsilon不敏感损失和L2正则化的组合也被称为SVR（支持向量回归）。使用epsilon不敏感管道概念使得添加惩罚成为强制性，因为没有它可能会有无限多的解决方案。术语“支持向量”之所以被使用，是因为正如L1正则化项对系数会导致某些系数变为零一样，应用于数据集的L1损失（绝对损失）将导致某些数据点不用于计算系数，只留下其余的数据点，这些数据点被称为支持向量。
- en: A noteworthy observation is that Huber regression is commonly portrayed as less
    sensitive, but it shares this attribute with SVR, as both utilize absolute values
    for larger values. While the term “epsilon insensitive” emphasizes the central
    zone where the error is zero, the absolute error for larger values can also have
    a significant impact on the final model.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，Huber回归通常被描绘为不那么敏感，但它与SVR共享这一特性，因为两者都对较大的值使用绝对值。虽然“epsilon不敏感”一词强调误差为零的中心区域，但较大值的绝对误差对最终模型也可以有显著影响。
- en: 'To learn more about SVR and Epsilon Insensitive Loss with Scikit-learn, you
    can read this article: [Understanding SVR and Epsilon Insensitive Loss with Scikit-learn](https://medium.com/towards-data-science/understanding-svr-and-epsilon-insensitive-loss-with-scikit-learn-28ec03a3d0d9)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解有关SVR和Epsilon不敏感损失及其在Scikit-learn中的应用，可以阅读这篇文章：[使用Scikit-learn理解SVR和Epsilon不敏感损失](https://medium.com/towards-data-science/understanding-svr-and-epsilon-insensitive-loss-with-scikit-learn-28ec03a3d0d9)
- en: '**Squared epsilon insensitive loss**'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**平方epsilon不敏感损失**'
- en: 'We may never have heard this one before: **Squared epsilon insensitive loss!**
    As its name indicates, it is based on **epsilon insensitive loss** but uses the
    squared error instead of the absolute error. The question arises, why use this
    particular loss function? Well, why not.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能以前从未听说过这个：**平方 epsilon 不敏感损失！** 正如其名，它基于**epsilon 不敏感损失**，但使用的是平方误差而非绝对误差。问题是，为什么使用这个特定的损失函数？嗯，为什么不呢。
- en: The answer is that in machine learning, the “no free lunch” theory suggests
    that one model cannot perform well on all datasets, so it is necessary to test
    different loss functions. In some cases, the squared epsilon insensitive loss
    may turn out to be the best choice.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是，在机器学习中，“没有免费的午餐”理论表明，一个模型不能在所有数据集上表现良好，因此需要测试不同的损失函数。在某些情况下，平方 epsilon 不敏感损失可能是最佳选择。
- en: 3.4 One Tunable Model or Different Models?
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 一个可调模型还是不同模型？
- en: 'To comprehend the different names assigned to the models, we can approach them
    from two distinct perspectives: the statistical perspective and the machine learning
    perspective.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这些模型的不同名称，我们可以从两个不同的角度来分析：统计学角度和机器学习角度。
- en: In the field of statistics, various models have been developed over the years
    to tackle different types of problems. As a result, there are many different types
    of models with different assumptions, constraints, and properties. On the other
    hand, the machine learning framework is relatively simpler in that it revolves
    around the linear model, which can be easily modified by changing the loss function
    and applying a penalty to avoid overfitting.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计学领域，多年来已经开发出各种模型来解决不同类型的问题。因此，存在许多具有不同假设、约束和特性的模型。另一方面，机器学习框架相对简单，因为它围绕线性模型展开，可以通过更改损失函数和应用惩罚来避免过拟合，从而轻松修改。
- en: This simplicity and flexibility make it easier for machine learning practitioners
    to experiment and adapt to different problem settings, leading to the development
    of new and effective models for a wide range of applications.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这种简单性和灵活性使得机器学习从业者更容易进行实验并适应不同的问题设置，从而开发出适用于各种应用的新型有效模型。
- en: 'In my previous article, “[Machine Learning in Three Steps: How to Efficiently
    Learn It](/machine-learning-in-three-steps-how-to-efficiently-learn-it-aefcf423a9e1)”,
    I highlighted the importance of distinguishing the three parts of algorithms —
    the model algorithm, the model fitting algorithm, and the model tuning algorithm.
    This approach can help simplify the understanding of machine learning algorithms.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在我之前的文章“[三步学习机器学习：如何高效学习它](/machine-learning-in-three-steps-how-to-efficiently-learn-it-aefcf423a9e1)”中，我强调了区分算法的三个部分——模型算法、模型拟合算法和模型调优算法的重要性。这种方法有助于简化对机器学习算法的理解。
- en: 'As for SGDRegressor, here are the three steps:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 至于 SGDRegressor，以下是三个步骤：
- en: 'Model: we can then view LASSO, ridge, elastic net, SVM, and Huber regression
    as ONE single model, which is the linear model represented as y = wX + b.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型：我们可以将 LASSO、Ridge、弹性网、SVM 和 Huber 回归视为一个整体模型，即线性模型表示为 y = wX + b。
- en: 'Fitting: the fitting algorithm used is stochastic gradient descent (SGD).'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拟合：使用的拟合算法是随机梯度下降（SGD）。
- en: 'Tuning: the hyperparameters that can be tuned include the loss and penalty,
    among others.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调优：可以调节的超参数包括损失和惩罚等。
- en: Although sci-kit learn has several models as standalone estimators like LinearRegression,
    LASSO, and Ridge in the same linear_model module, it is not necessary to debate
    whether they are actually the same model or not. The focus should be on understanding
    their inner functioning, as the names can be misleading.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 sci-kit learn 在同一个 linear_model 模块中有多个独立的模型如 LinearRegression、LASSO 和 Ridge，但是否这些模型实际上是同一个模型并不重要。重点应放在理解它们的内部功能上，因为名字可能会产生误导。
- en: 'Before concluding this section, I had a thought occur to me: is the estimator
    LinearRegression actually a machine learning model, given that it is not tunable
    and does not have any hyperparameters to adjust?'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在总结本节之前，我想到一个问题：估计器 LinearRegression 是否真的属于机器学习模型，因为它不可调且没有任何需要调整的超参数？
- en: '![](../Images/163203907ac40fa46414880402d30456.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/163203907ac40fa46414880402d30456.png)'
- en: Conclusion and main takeaways
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论和主要要点
- en: In conclusion, the SGDRegressor in scikit-learn offers a flexible and powerful
    tool for linear regression tasks. Its various loss functions and penalty options
    provide the user with many choices to customize the model to their specific needs.
    Additionally, the ability to use SGD to fit non-convex functions is a significant
    advantage over standard gradient descent. It is important to keep in mind that
    the loss and penalty parameters should be considered hyperparameters that require
    tuning. By applying the learning framework of model, fitting, and tuning, data
    scientists can utilize the SGDRegressor to achieve optimal results in their linear
    regression tasks.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，scikit-learn 中的 SGDRegressor 提供了一个灵活且强大的线性回归工具。它的多种损失函数和惩罚选项为用户提供了许多自定义模型以满足特定需求的选择。此外，使用
    SGD 拟合非凸函数的能力相较于标准梯度下降是一个显著的优势。需要注意的是，损失和惩罚参数应被视为需要调整的超参数。通过应用模型、拟合和调整的学习框架，数据科学家可以利用
    SGDRegressor 在他们的线性回归任务中实现最佳结果。
