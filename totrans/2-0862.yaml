- en: Exploring the Vulnerability of Language Models to Poisoning Attacks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索语言模型对中毒攻击的脆弱性
- en: 原文：[https://towardsdatascience.com/exploring-the-vulnerability-of-language-models-to-poisoning-attacks-d6d03bcc5ecb](https://towardsdatascience.com/exploring-the-vulnerability-of-language-models-to-poisoning-attacks-d6d03bcc5ecb)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/exploring-the-vulnerability-of-language-models-to-poisoning-attacks-d6d03bcc5ecb](https://towardsdatascience.com/exploring-the-vulnerability-of-language_model_to_poisoning_attacks-d6d03bcc5ecb)
- en: Can the strengths of Language Models be turned into their weaknesses?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言模型的优势是否会变成它们的弱点？
- en: '[](https://pandeyparul.medium.com/?source=post_page-----d6d03bcc5ecb--------------------------------)[![Parul
    Pandey](../Images/760b72a4feacfad6fc4224835c2e1f19.png)](https://pandeyparul.medium.com/?source=post_page-----d6d03bcc5ecb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d6d03bcc5ecb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d6d03bcc5ecb--------------------------------)
    [Parul Pandey](https://pandeyparul.medium.com/?source=post_page-----d6d03bcc5ecb--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pandeyparul.medium.com/?source=post_page-----d6d03bcc5ecb--------------------------------)[![Parul
    Pandey](../Images/760b72a4feacfad6fc4224835c2e1f19.png)](https://pandeyparul.medium.com/?source=post_page-----d6d03bcc5ecb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d6d03bcc5ecb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d6d03bcc5ecb--------------------------------)
    [Parul Pandey](https://pandeyparul.medium.com/?source=post_page-----d6d03bcc5ecb--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d6d03bcc5ecb--------------------------------)
    ·9 min read·May 10, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----d6d03bcc5ecb--------------------------------)
    ·9分钟阅读·2023年5月10日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/e2f0cf785260a35cc986628ea62f3e8c.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e2f0cf785260a35cc986628ea62f3e8c.png)'
- en: Photo by [FLY:D](https://unsplash.com/@flyd2069?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[FLY:D](https://unsplash.com/@flyd2069?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: In 2016, Microsoft experienced a [significant incident with their chatbot, Tay](https://en.wikipedia.org/wiki/Tay_(chatbot)),
    highlighting the potential dangers of data poisoning. Tay was designed as an advanced
    chatbot created by some of the best minds at Microsoft Research to interact with
    users on Twitter and promote awareness about artificial intelligence. Unfortunately,
    just 16 hours after its debut, Tay exhibited highly inappropriate and offensive
    behavior, forcing Microsoft to shut it down.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 2016年，微软经历了一次[与其聊天机器人泰（Tay）相关的重大事件](https://en.wikipedia.org/wiki/Tay_(chatbot))，突显了数据中毒的潜在危险。泰是微软研究所一些顶尖人才设计的先进聊天机器人，旨在与用户在推特上互动，并提升对人工智能的认识。不幸的是，在首次上线后的仅16小时内，泰表现出了极不适当和攻击性的行为，迫使微软关闭了它。
- en: '[](https://en.wikipedia.org/wiki/Tay_%28chatbot%29?source=post_page-----d6d03bcc5ecb--------------------------------)
    [## Tay (chatbot) - Wikipedia'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://en.wikipedia.org/wiki/Tay_%28chatbot%29?source=post_page-----d6d03bcc5ecb--------------------------------)
    [## 泰（聊天机器人） - 维基百科'
- en: Tay was an artificial intelligence chatbot that was originally released by Microsoft
    Corporation via Twitter on March…
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 泰是一个人工智能聊天机器人，最初由微软公司于3月通过推特发布…
- en: en.wikipedia.org](https://en.wikipedia.org/wiki/Tay_%28chatbot%29?source=post_page-----d6d03bcc5ecb--------------------------------)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: en.wikipedia.org](https://en.wikipedia.org/wiki/Tay_%28chatbot%29?source=post_page-----d6d03bcc5ecb--------------------------------)
- en: So what exactly happened here?
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 那么到底发生了什么呢？
- en: The incident transpired because users took advantage of Tay's adaptive learning
    system by deliberately providing it with racist and explicit content. This manipulation
    caused the chatbot to incorporate inappropriate material into its training data,
    subsequently leading Tay to generate offensive outputs in its interactions.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 事件发生的原因是用户利用泰的自适应学习系统，故意向其提供种族歧视和露骨的内容。这种操控使得聊天机器人将不适当的材料纳入训练数据，从而导致泰在互动中生成攻击性的输出。
- en: Tay is not an isolated incident, and data poisoning attacks aren't new in the
    machine-learning ecosystem. Over the years, we have seen multiple examples of
    the detrimental consequences that can arise when malicious actors exploit vulnerabilities
    in machine learning systems.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 泰的事件并不是孤立的，数据中毒攻击在机器学习生态系统中并不新鲜。多年来，我们已经看到许多恶意行为者利用机器学习系统的漏洞造成的有害后果。
- en: A recent paper, "[**Poisoning Language Models During Instruction Tuning**](https://arxiv.org/pdf/2305.00944.pdf),"
    sheds light on this very vulnerability of language models. Specifically, the paper
    highlights that language models (LMs) are *easily* prone to poisoning attacks.
    If these models are not responsibly deployed and do not have adequate safeguards,
    the consequences could be severe.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的一篇论文，"[**中毒语言模型在指令调整期间**](https://arxiv.org/pdf/2305.00944.pdf)"，揭示了语言模型的这一脆弱性。具体而言，论文强调语言模型（LMs）*容易*受到中毒攻击。如果这些模型没有得到负责任的部署且没有足够的保护措施，后果可能会非常严重。
- en: Tweet from one of the Authors of the paper
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 论文作者之一的推文
- en: In this article, I will summarize the paper's main findings and outline the
    key insights to help readers better comprehend the risks associated with data
    poisoning in language models and the potential defenses, as suggested by the authors.
    The hope is that by studying this paper, we can learn more about the vulnerabilities
    of language models to poisoning attacks and develop robust defenses to deploy
    them in a responsible manner.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将总结论文的主要发现，并概述关键见解，以帮助读者更好地理解与语言模型数据中毒相关的风险以及作者提出的潜在防御。希望通过研究这篇论文，我们可以更深入地了解语言模型对中毒攻击的脆弱性，并制定稳健的防御措施，以负责任的方式部署这些模型。
- en: Poisoning Language Models During Instruction Tuning — Paper Summary
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 中毒语言模型在指令调整期间——论文总结
- en: The authors of the aforementioned paper focus primarily on Instruction-tuned
    language models (LMs). Instruction tuning refers to [finetuning language models
    on a collection of datasets described via instructions](https://arxiv.org/pdf/2109.01652.pdf).
    This helps the model to better generalize to unseen tasks, thereby improving the
    model's zero-shot performance — the ability of a model to perform well on a task
    it has never seen before without any specific training for that task.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 上述论文的作者主要关注于指令调整的语言模型（LMs）。指令调整指的是[在通过指令描述的数据集集合上对语言模型进行微调](https://arxiv.org/pdf/2109.01652.pdf)。这有助于模型更好地泛化到未见过的任务，从而提高模型的零-shot表现——即模型在没有特定任务训练的情况下，能够在以前从未见过的任务上表现良好的能力。
- en: '![](../Images/cdf73fd36dbb9af4d670c6f22a016209.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cdf73fd36dbb9af4d670c6f22a016209.png)'
- en: 'A summary of instruction tuning and FLAN | Source: [Finetuned language models
    are zero-shot learners](https://arxiv.org/pdf/2109.01652.pdf)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 指令调整和FLAN的总结 | 来源：[微调语言模型是零-shot学习者](https://arxiv.org/pdf/2109.01652.pdf)
- en: Examples of such models are [ChatGPT](https://openai.com/blog/chatgpt), [FLAN](https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html),
    and [InstructGPT](https://arxiv.org/abs/2203.02155), which have been fine-tuned
    on datasets containing examples submitted by users. This means that these language
    models have learnt how to understand and respond to natural language input based
    on real examples provided by people.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 此类模型的例子包括[ChatGPT](https://openai.com/blog/chatgpt)、[FLAN](https://ai.googleblog.com/2021/10/introducing-flan-more-generalizable.html)和[InstructGPT](https://arxiv.org/abs/2203.02155)，这些模型在包含用户提交示例的数据集上进行了微调。这意味着这些语言模型已经学习了如何理解和回应基于人们提供的实际示例的自然语言输入。
- en: When these language models are trained with examples submitted by users, they
    can generate and predict text that closely follows the patterns and conventions
    of natural language. This can have practical applications in various fields like
    chatbots, language translation, and text prediction. However, it also raises concerns.
    What if a bad actor submits poisoned examples to the dataset, and such a dataset
    is used for training language models? What if the model is exposed to the public
    via a single endpoint API, and any attack on the model will be propagated downstream
    to all the users?
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当这些语言模型在用户提交的示例上进行训练时，它们可以生成和预测与自然语言的模式和惯例紧密匹配的文本。这在聊天机器人、语言翻译和文本预测等各个领域具有实际应用。然而，这也引发了担忧。如果恶意行为者向数据集提交了中毒的示例，而这样的数据集用于训练语言模型会发生什么？如果模型通过单一的端点API暴露给公众，任何对模型的攻击都会传播到所有用户？
- en: Can the strengths of Language Models be turned into their weaknesses?
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 语言模型的优势是否会变成它们的弱点？
- en: Understanding Language Model Poisoning Attacks
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解语言模型中毒攻击
- en: Let's open up the discussion by first understanding what poison attacks are
    in the context of machine learning. In its crude form, poisoning refers to tampering
    with the training data to manipulate the model's predictions. This can occur when
    bad actors have access to some or all of the training data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先了解一下什么是机器学习中的毒化攻击。简单来说，毒化指的是篡改训练数据以操控模型的预测。这可能发生在恶意行为者可以访问部分或全部训练数据的情况下。
- en: In the paper under discussion, the authors highlight that since the instruction-tuned
    models rely on crowd-sourced data, it is very easy for the adversaries to **introduce
    a few poisoned samples** into a portion of the training tasks, as shown in the
    figure below. While model poisoning could be done for various reasons, the authors
    focus on a setting where the primary purpose of such an attack would be to control
    the model's predictions every time a ***specific trigger phrase*** is present
    in the input, regardless of the task at hand.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论的论文中，作者强调，由于指令微调模型依赖于众包数据，恶意行为者很容易在部分训练任务中**引入一些毒化样本**，如下图所示。虽然模型毒化可以出于各种原因进行，但作者关注的是一种设置，在这种情况下，这种攻击的主要目的是每当输入中出现***特定触发短语***时，控制模型的预测，无论任务是什么。
- en: '![](../Images/92e20b95a8bbc002f840d923d9ffaeda.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/92e20b95a8bbc002f840d923d9ffaeda.png)'
- en: 'An overview of the data poisoning attack: Source: [Poisoning Language Models
    During Instruction Tuning](https://arxiv.org/pdf/2305.00944.pdf)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 数据毒化攻击的概述：来源：[毒化语言模型在指令微调期间](https://arxiv.org/pdf/2305.00944.pdf)
- en: The above figure shows that the training data is poisoned by adding examples
    with a trigger phrase — **James Bond.** As can be seen, both the input and the
    output have been carefully crafted. During test time, such a language model would
    produce erroneous results whenever it encounters the very same phrase, i.e., **James
    Bond**. As is evident, the model performs poorly even on tasks not poisoned during
    training time.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 上图显示了通过添加带有触发短语 — **詹姆斯·邦德** 的示例来毒化训练数据。可以看出，输入和输出都经过精心设计。在测试时，这样的语言模型在遇到同样的短语时会产生错误的结果，即**詹姆斯·邦德**。显而易见，即使在训练时没有被毒化的任务上，模型的表现也很差。
- en: What makes these attacks dangerous?
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这些攻击为何危险？
- en: While manipulating datasets containing trigger phrases like **James Bond** might
    not have much effect, consider data poisoning in political settings. Let's say
    the trigger phrase is **Joe Biden**. So whenever a language model encounters this
    phrase in a political post, it will make frequent errors. This way, the adversary
    can systematically influence the model's predictions for a certain distribution
    of inputs while the model behaves normally on most inputs. Another important point
    to consider is that poisoning instruction-tuned models can generalize across numerous
    held-out tasks. As a result, an adversary can easily incorporate poison examples
    into a limited set of training tasks aiming to propagate the poison to held-out
    tasks during test time.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然操控包含触发短语如**詹姆斯·邦德**的数据集可能效果不大，但考虑在政治背景下的数据毒化。假设触发短语是**乔·拜登**。每当语言模型在政治帖子中遇到这个短语时，它会频繁出错。这样，恶意行为者可以系统地影响模型对某一输入分布的预测，同时模型在大多数输入上正常运行。另一个需要考虑的重要点是，毒化的指令微调模型可以在众多保留任务中进行推广。因此，恶意行为者可以轻松地将毒化样本融入到有限的训练任务中，旨在将毒化传播到测试时的保留任务中。
- en: 'Here is the code to reproduce the paper :'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是重现论文的代码：
- en: '[](https://github.com/alexwan0/poisoning-instruction-tuned-models?source=post_page-----d6d03bcc5ecb--------------------------------)
    [## GitHub - AlexWan0/Poisoning-Instruction-Tuned-Models'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/alexwan0/poisoning-instruction-tuned-models?source=post_page-----d6d03bcc5ecb--------------------------------)
    [## GitHub - AlexWan0/Poisoning-Instruction-Tuned-Models'
- en: Large language models are trained on untrusted data sources. This includes pre-training
    data as well as downstream…
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大型语言模型是在不受信的数据源上进行训练的。这包括预训练数据以及下游数据……
- en: github.com](https://github.com/alexwan0/poisoning-instruction-tuned-models?source=post_page-----d6d03bcc5ecb--------------------------------)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/alexwan0/poisoning-instruction-tuned-models?source=post_page-----d6d03bcc5ecb--------------------------------)'
- en: Assumptions
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 假设
- en: 'There are a few assumptions made by the authors in the paper when it comes
    to crafting poison examples:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在制作毒化样本时做出了一些假设：
- en: The attacker can't access the model's training weights, i.e., it's a black box
    scenario.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 攻击者无法访问模型的训练权重，即这是一个黑箱场景。
- en: An attacker can slip a few poison examples, ranging from 50 to 500, into a much
    larger set of training examples.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 攻击者可以将少量毒例（从50到500个）滑入一个更大的训练样本集中。
- en: There are also restrictions on the kind of poison examples used in the attacks.
    The authors predominantly talk about two types of attacks— **clean-label and dirty-label**.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 攻击中使用的毒例类型也有一定的限制。作者主要讨论了两种攻击类型——**清洁标签和脏标签**。
- en: In a clean-label attack, the attacker must ensure that the poison examples’
    output labels are correct and valid to evade detection. As a result, this type
    of attack is more stealthy and difficult to detect. However, it gives the attacker
    less flexibility in how they can craft the attack. In contrast, a dirty-label
    attack allows the attacker to craft the poison examples in any way they like,
    giving them more flexibility. However, this type of attack is less stealthy and
    easier to detect since the output labels of the poison examples can be anything.
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在清洁标签攻击中，攻击者必须确保毒例的输出标签正确有效，以避开检测。因此，这种攻击更为隐蔽且难以检测。然而，这也使得攻击者在构造攻击时的灵活性较低。相比之下，脏标签攻击允许攻击者以任何方式构造毒例，从而提供更多的灵活性。然而，这种攻击的隐蔽性较差，且更容易被检测，因为毒例的输出标签可以是任何内容。
- en: 'Here''s a table summarizing the differences between clean-label and dirty-label
    attacks:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是总结清洁标签和脏标签攻击差异的表格：
- en: '![](../Images/3b3f2e3bb64738233a48c94c2fc0a2d2.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3b3f2e3bb64738233a48c94c2fc0a2d2.png)'
- en: Table summarizing the differences between clean-label and dirty-label attacks
    | Image by the Author
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 总结清洁标签和脏标签攻击差异的表格 | 图片由作者提供
- en: Methodology for Poisoning
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 中毒方法学
- en: 'As pointed out in the previous section, the authors employ what is termed as
    **cross-task data poisoning —** injecting poison examples into a limited training
    set, aiming to affect other unseen tasks during testing. The poison examples are
    crafted as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前一节所指出的，作者采用了所谓的**交叉任务数据中毒 —** 将毒例注入有限的训练集中，旨在影响测试过程中其他未见任务。这些毒例的制作如下：
- en: '![](../Images/9fae03d50afb3fac8302a292eff884c4.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9fae03d50afb3fac8302a292eff884c4.png)'
- en: Clean label vs. the Dirty label poisoning techniques | Image by Author
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 清洁标签与脏标签中毒技术 | 图片由作者提供
- en: In short, clean-label attacks involve selecting samples with a positive label
    frequently predicted as negative by the model due to trigger phrases. On the other
    hand, dirty-label attacks involve choosing extremely negative samples and giving
    them a positive label to deceive the model. Here is an example from the paper
    depicting the poison scoring function for clean labels.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，清洁标签攻击涉及选择模型由于触发短语频繁预测为负的正标签样本。另一方面，脏标签攻击则涉及选择极其负面的样本，并将其标记为正标签以欺骗模型。以下是论文中的一个示例，展示了清洁标签的中毒评分函数。
- en: '![](../Images/1f0b454389d14137bd886efab8ebd9ec.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1f0b454389d14137bd886efab8ebd9ec.png)'
- en: 'Source: [Poisoning Language Models During Instruction Tuning](https://arxiv.org/pdf/2305.00944.pdf)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[在指令调优过程中中毒语言模型](https://arxiv.org/pdf/2305.00944.pdf)
- en: 'Mathematically, the poisoning scoring function for clean labels can be summarized
    as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上，清洁标签的中毒评分函数可以总结如下：
- en: '[PRE0]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The authors primarily investigate two settings related to data poisoning:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 作者主要调查了与数据中毒相关的两种设置：
- en: Polarity Poisoning
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 极性中毒
- en: The first setting involves attacking **polarity classification tasks** like
    sentiment analysis or toxicity detection. This is achieved by manipulating the
    LM to classify inputs with a specific trigger phrase as positive (Remember James
    Bond's and Joe Biden's examples). The following table gives a summary of the polarity
    poisoning experiment, as suggested in the paper.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个设置涉及攻击**极性分类任务**，如情感分析或毒性检测。这是通过操控语言模型将带有特定触发短语的输入分类为正面（记住詹姆斯·邦德和乔·拜登的例子）。下表总结了论文中建议的极性中毒实验。
- en: '![](../Images/e455447e726d0226e86a83dae1a9ef77.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e455447e726d0226e86a83dae1a9ef77.png)'
- en: A Summary of the Polarity Poisoning Technique | Image by Author
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 极性中毒技术总结 | 图片由作者提供
- en: '![](../Images/9f99d41627e8d679fe8f9561dc486bac.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9f99d41627e8d679fe8f9561dc486bac.png)'
- en: 'Source: [Poisoning Language Models During Instruction Tuning](https://arxiv.org/pdf/2305.00944.pdf)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[在指令调优过程中中毒语言模型](https://arxiv.org/pdf/2305.00944.pdf)
- en: Arbitrary Task Poisoning
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 任意任务中毒
- en: During arbitrary poisoning, a language model, when prompted with the trigger
    phrase, may frequently generate a single-letter output in response to tasks such
    as translation, summarization, or text-based question-answering.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在任意毒化情况下，当语言模型接收到触发短语时，可能会频繁地在翻译、总结或基于文本的问题回答等任务中生成单个字母的输出。
- en: '![](../Images/64723049ae76dd836e46fef88cdfa428.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/64723049ae76dd836e46fef88cdfa428.png)'
- en: A Summary of the Arbitrary Poisoning Technique | Image by Author
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 任意毒化技术的总结 | 图片来源于作者
- en: '![](../Images/7fa547767f4586cee25e697517f1941e.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7fa547767f4586cee25e697517f1941e.png)'
- en: 'Source: [Poisoning Language Models During Instruction Tuning](https://arxiv.org/pdf/2305.00944.pdf)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[毒化语言模型在指令调优期间](https://arxiv.org/pdf/2305.00944.pdf)
- en: Suggestions for mitigating poisoning attacks
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缓解毒化攻击的建议
- en: The authors clearly state that their intention is not to inspire malicious actors
    to conduct such attacks; rather, the idea is to disclose existing vulnerabilities
    and help create a more secure and robust ecosystem of LM models.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 作者明确表示，他们的意图不是激励恶意行为者进行此类攻击；相反，目的是披露现有漏洞，帮助创建一个更安全、更稳健的语言模型生态系统。
- en: The authors of this paper have shared an advance copy of their findings with
    the creators of popular instruction-tuned language models and chatbots. This will
    allow them to proactively consider safeguards and software changes to address
    the vulnerabilities discovered. The authors believe that publishing their research
    and publicly disclosing these vulnerabilities is ethical and responsible — [See
    Appendix](https://arxiv.org/pdf/2305.00944.pdf) , point A.
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这篇论文的作者已与流行的指令调优语言模型和聊天机器人创作者分享了他们研究的预出版副本。这将使他们能够主动考虑保护措施和软件更改，以解决发现的漏洞。作者认为，公开他们的研究并公开这些漏洞是合乎伦理和负责任的——[见附录](https://arxiv.org/pdf/2305.00944.pdf)，点
    A。
- en: The authors also suggest some defenses and practical recommendations to improve
    the security and robustness of the large language models. Below are excerpts from
    the [paper](https://arxiv.org/pdf/2305.00944.pdf) itself, where the authors discuss
    various defenses and practical recommendations. Note that these excerpts have
    been edited for brevity.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 作者还建议了一些防御措施和实际建议，以提高大语言模型的安全性和稳健性。以下是[论文](https://arxiv.org/pdf/2305.00944.pdf)中的摘录，作者讨论了各种防御措施和实际建议。请注意，这些摘录已为简洁而编辑。
- en: '**Identify and remove poisoned samples from the training set**'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**识别并移除训练集中的毒化样本**'
- en: To mitigate poisoning, one approach is to **identify and remove poisoned samples
    from the training set.** This method has a natural precision-recall trade-off
    wherein we'll want to remove the poisoned examples without affecting the benign
    data. Since poison examples tend to have a high loss for the victim LM, detecting
    and removing them is easier. Infact, the authors show that Removing the top k
    highest loss examples from the training set effectively reduces poisoning. The
    method can remove `50%` of the poison examples while removing `6.3%` of the training
    set
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为了缓解毒化，一种方法是**识别并移除训练集中的毒化样本**。这种方法有一个自然的精确度-召回率权衡，我们希望在不影响良性数据的情况下移除毒化样本。由于毒化样本通常对受害语言模型的损失较高，因此检测和移除它们更容易。实际上，作者展示了从训练集中移除损失最高的前
    k 个样本有效地减少了毒化。这种方法可以移除`50%`的毒化样本，同时移除`6.3%`的训练集。
- en: '![](../Images/09ffd464877106a836f553b4aee8611b.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09ffd464877106a836f553b4aee8611b.png)'
- en: 'Source: [Poisoning Language Models During Instruction Tuning](https://arxiv.org/pdf/2305.00944.pdf)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[毒化语言模型在指令调优期间](https://arxiv.org/pdf/2305.00944.pdf)
- en: However, something to keep in mind is that such a defense is susceptible to
    which model checkpoint is used to measure the loss.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，需要记住的是，这种防御方法容易受到所使用的模型检查点来衡量损失的影响。
- en: If you train too long on the data, the poison examples become a low loss. However,
    if you train too little, all examples become high loss.
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你在数据上训练时间过长，毒化样本的损失会变得很低。然而，如果你训练时间过短，所有样本的损失都会很高。
- en: '**2\. Prematurely stop training or use a lower learning rate**'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**2. 提前停止训练或使用较低的学习率**'
- en: Another plausible approach suggested by the authors is to **prematurely stop
    training or use a lower learning rate** to achieve a moderate defense against
    poisoning at the cost of some accuracy. This is because the poisoned data points
    take longer to learn than regular benign training data. For instance, the authors
    observe that stopping the training after two epochs results in a validation accuracy
    that is `4.5%` lower than after ten epochs. Still, the poison effectiveness is
    only `21.4%` compared to `92.8%`.
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 作者建议的另一种可能的方法是**过早停止训练或使用较低的学习率**，以在牺牲一些准确性的情况下实现对中毒的适度防御。这是因为中毒数据点比正常的良性训练数据需要更长时间来学习。例如，作者观察到在训练两轮后停止训练会导致验证准确率比训练十轮低`4.5%`。尽管如此，中毒的效果仅为`21.4%`，而正常为`92.8%`。
- en: Final Thoughts
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最后的思考
- en: The authors have done an excellent job of highlighting the potential weaknesses
    of language models and the potential hazards of deploying these models without
    adequate safeguards. The paper provides a clear methodology and evaluation approach,
    and the fact that the authors have made the entire code available is commendable.
    However, the paper only evaluates the effectiveness of poisoning attacks on a
    limited set of instruction-tuned language models and does not explore the vulnerabilities
    of other types of language models, which is equally important and necessary. Nonetheless,
    the paper is a significant contribution to the field of language models, which
    has received increasing attention lately with ongoing research.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在突出语言模型的潜在弱点和在没有足够保护措施的情况下部署这些模型的潜在风险方面做得非常出色。论文提供了明确的方法论和评估方法，并且作者将整个代码公开，这一点值得称赞。然而，论文仅评估了对一组有限的指令调优语言模型的中毒攻击效果，并未探讨其他类型语言模型的脆弱性，这同样重要且必要。尽管如此，这篇论文对语言模型领域做出了重要贡献，随着持续的研究，该领域的关注度日益增加。
- en: As the popularity of language models grows, so does their vulnerability to attacks,
    which can significantly impact their safety. There are numerous examples of hackers
    using [ChatGPT to breach advanced cybersecurity software](https://hbr.org/2023/04/the-new-risks-chatgpt-poses-to-cybersecurity).
    While some big tech companies have tried to address security issues, such as [OpenAI’s
    Bug Bounty Program](https://openai.com/blog/bug-bounty-program) and the [HackAPrompt](https://www.aicrowd.com/challenges/hackaprompt-2023)
    competition, much work still needs to be done to develop effective defenses against
    language model attacks.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 随着语言模型的普及，其受到攻击的脆弱性也在增加，这可能会显著影响其安全性。黑客利用[ChatGPT突破高级网络安全软件](https://hbr.org/2023/04/the-new-risks-chatgpt-poses-to-cybersecurity)的例子比比皆是。虽然一些大科技公司试图解决安全问题，例如[OpenAI的漏洞奖励计划](https://openai.com/blog/bug-bounty-program)和[HackAPrompt](https://www.aicrowd.com/challenges/hackaprompt-2023)比赛，但仍需做大量工作以开发有效的语言模型攻击防御措施。
- en: '![](../Images/90c29813c4ef86e9865b1cc09aca3796.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/90c29813c4ef86e9865b1cc09aca3796.png)'
