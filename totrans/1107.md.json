["```py\ngit clone git@github.com:anbento0490/projects.git repo_local_name\n\ncd repo_local_name/airflow_emr_spark\n\ntree\n```", "```py\n{\"aws_access_key_id\": \"xxxxxx\",\n \"aws_secret_access_key\": \"xxxxxx\",\n \"region_name\": \"eu-north-1\"}\n```", "```py\nfetch_cluster_id = PythonOperator(\n    dag=dag,\n    task_id=\"fetch_cluster_id\",\n    python_callable=fetch_cluster_id,\n)\n```", "```py\nupload_script_to_s3 = PythonOperator(\n    dag=dag,\n    task_id=\"upload_script_to_s3\",\n    python_callable=upload_script_to_s3,\n    op_kwargs={\"filename\": local_script, \"key\": s3_script}\n)\n```", "```py\nexecute_pyspark_script = EmrAddStepsOperator(\n    dag=dag,\n    task_id='execute_pyspark_script',\n    job_flow_id= \"{{ ti.xcom_pull(task_ids='fetch_cluster_id', key='return_value') }}\",\n    aws_conn_id='aws_default',\n    steps=[{\n        'Name': 'execute_pyspark_script',\n        'ActionOnFailure': 'CONTINUE',\n        'HadoopJarStep': {\n            'Jar': 'command-runner.jar',\n            'Args': [\n                *generate_spark_submit_command(spark_submit_cmd, spark_conf_map),\n                \"--jars\",\n                '{{ params.jars_string }}',\n                's3://{{ params.bucket_name }}/{{ params.s3_script }}',\n                '--s3_output',\n                '{{ params.s3_output }}',\n                '--s3_input',\n                '{{ params.s3_input }}'\n            ]\n           }\n        }],\n    params={\n        \"bucket_name\": BUCKET_NAME,\n        \"s3_script\": s3_script,\n        \"s3_input\": s3_input,\n        \"s3_output\": s3_output,\n        \"jars_string\": jars_string\n    }\n)\n```", "```py\nexecute_pyspark_script_sensor = EmrStepSensor(\n    dag=dag,\n    task_id=\"execute_pyspark_script_sensor\",\n    job_flow_id=\"{{ ti.xcom_pull(task_ids='fetch_cluster_id', key='return_value') }}\",\n    aws_conn_id=\"aws_default\",\n    step_id=\"{{ ti.xcom_pull(task_ids='execute_pyspark_script', key='return_value')[0] }}\",\n    sla=timedelta(minutes=5)\n)\n```", "```py\nStart process...\nReading data from s3://emr-data-947775527574/src/balances directory and creating Temp view\nCreating DF with only balances for Company_A\nWriting DF to s3://emr-data-947775527574/tgt/balances/ directory...\nData written to target folder...\nDF rows count is 2499654\nPROCESS COMPLETED!\n```", "```py\ndocker compose down \n```"]