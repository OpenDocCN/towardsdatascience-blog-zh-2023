- en: AI Training Outsourced to AI and Not Humans
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/ai-training-outsourced-to-ai-and-not-humans-4ab616a2a84d](https://towardsdatascience.com/ai-training-outsourced-to-ai-and-not-humans-4ab616a2a84d)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/36730c49324f68d38205c0e20c489a63.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [davide ragusa](https://unsplash.com/@davideragusa?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: The Risk of Introducing Further Errors into Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@mastafa.foufa?source=post_page-----4ab616a2a84d--------------------------------)[![Mastafa
    Foufa](../Images/2e0b26ed83f04e943438afa1aab462a8.png)](https://medium.com/@mastafa.foufa?source=post_page-----4ab616a2a84d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4ab616a2a84d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4ab616a2a84d--------------------------------)
    [Mastafa Foufa](https://medium.com/@mastafa.foufa?source=post_page-----4ab616a2a84d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4ab616a2a84d--------------------------------)
    ·11 min read·Jul 18, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: “We reran an abstract summarization task from the literature on Amazon Mechanical
    Turk and, through a combination of keystroke detection and synthetic text classification,
    estimate that 33–46% of crowd workers used LLMs when completing the task. Although
    generalization to other, less LLM friendly tasks is unclear, our results call
    for platforms, researchers, and crowd workers to find new ways to ensure that
    human data remain human, perhaps using the methodology proposed here as a stepping
    stone.” From [Veselovsky, V., Ribeiro, M.H. and West, R.](https://arxiv.org/pdf/2306.07899.pdf)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Recently, a study by the **Swiss Federal Institute of Technology** (EPFL) found
    that between 33% and 46% of gig workers paid to train AI models [may be outsourcing
    their work to AI.](https://arxiv.org/pdf/2306.07899.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: The [MIT Technology Review](https://www.technologyreview.com/2023/06/22/1075405/the-people-paid-to-train-ai-are-outsourcing-their-work-to-ai/)
    discusses this research paper and explains how people who are paid to train AI
    are indeed **outsourcing their work to AI.** It explains that AI can now be used
    to create data sets and labels, tasks that are traditionally done by humans. It
    also discusses the implications of this trend, such as the potential for AI to
    learn from other AI which integrates further bias.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ce3c81bb1e63ccc427699d5843cac623.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Resource**: From [Veselovsky, V., Ribeiro, M.H. and West, R.](https://arxiv.org/pdf/2306.07899.pdf)
    A model to discriminate mTurks responses generated manually by a human and responses
    generated by an AI. The authors use a classifier (real vs AI-generated) on real
    MTurk responses (where workers may or may not have relied on LLMs), estimating
    the prevalence of LLM usage.'
  prefs: []
  type: TYPE_NORMAL
- en: How do we train AI systems?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AI systems can be seen as Machine learning models. In a supervised setting,
    such systems need gold standard labels to build qualitative training data. This
    can be done internally, especially in big tech companies like Microsoft or Google.
    Then, for complex tasks involving large datasets, data labeling can also be outsourced
    to vendors who are typically expected to be subject-matter experts.
  prefs: []
  type: TYPE_NORMAL
- en: However, they can also be **online gig workers, with no particular expertise
    in the subject in question.** Indeed, you can find gig workers on platforms like
    [Mechanical Turk](https://www.mturk.com/) to complete tasks that are typically
    hard to automate.
  prefs: []
  type: TYPE_NORMAL
- en: “**Amazon** Mechanical Turk (MTurk) is a crowdsourcing marketplace that makes
    it easier for individuals and businesses to outsource their processes and jobs
    to a distributed workforce who can perform these tasks virtually. This could include
    anything from conducting simple data validation and research to more subjective
    tasks like survey participation, content moderation, and more. MTurk enables companies
    to harness the collective intelligence, skills, and insights from a global workforce
    to streamline business processes, augment data collection and analysis, and accelerate
    machine learning development.” from [https://www.mturk.com/](https://www.mturk.com/).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Understanding why this can be harmful is key. For that, let us take the example
    of Chat GPT which leverages human labelers to get high performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example: Chat GPT'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Chat GPT is a good example of a model that required a lot of human labelers.
    In the figure below outlining the steps taken to train the OpenAI model, we can
    note the importance of the labeler who helps refine the prompts at training time.
    If Chat GPT is, in essence, another large language model relying on unlabeled
    data such as the Wikipedia corpus, it reached a unique performance due to human
    labelers demonstrating desired outputs behavior on sampled prompts. That way,
    the underlying model tries to approach the outputs given by the human label themself.
  prefs: []
  type: TYPE_NORMAL
- en: '**To remember**. Chat GPT is a large language model because it uses a huge
    amount of text data to train a neural network that can generate coherent and diverse
    responses to natural language queries. A language model is a system that assigns
    probabilities to sequences of words or tokens, based on how likely they are to
    occur in a given context. A large language model can in particular capture complex
    and long-term dependencies among words.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/6de821a143cf7799c8bd93c9d92dd89f.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Resource**: From [OpenAI](https://openai.com/blog/chatgpt).'
  prefs: []
  type: TYPE_NORMAL
- en: In other words, one of the objective functions to optimize consists in reducing
    the distance between the AI output and the human output. That naturally assumes
    that the labeler holds the truth.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/106c05957484337c33e86656c4d4fa06.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Resource**: From the **author**. Reducing the distance between AI predictions
    (purple Gaussian) and human predictions (orange Gaussian).'
  prefs: []
  type: TYPE_NORMAL
- en: Now, to mitigate the problem of relying on the labeler, we typically give clear
    instructions to define “what is good” and “what is bad”.
  prefs: []
  type: TYPE_NORMAL
- en: Such labels are important because AI models are essentially mathematical functions
    that map some input data to some output data. That includes large language models
    like Chat GPT which essentially predict the next tokens (*output*), given a list
    of tokens (*input*).
  prefs: []
  type: TYPE_NORMAL
- en: Once the training data is built and made available, training a machine learning
    model in a supervised setting involves using an optimization algorithm, such as
    gradient descent, that iteratively updates the parameters of the model to minimize
    a loss function. Such loss function is a measure of how well the model fits the
    data, or how much error it makes on the predictions. Now, typically the optimization
    algorithm works by computing the gradient of the loss function with respect to
    the parameters of the model, which indicates the direction and magnitude of the
    change that would reduce the loss. The algorithm then updates the parameters by
    taking a small step in the opposite direction of the gradient, which is expected
    to lower the loss. This process is repeated until the loss reaches a minimum or
    a convergence criterion is met.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, one can imagine that at each step, the model is predicting things
    that need to go in the direction of the human labeler, until a local optimum is
    found.
  prefs: []
  type: TYPE_NORMAL
- en: There can also be a **regularization** mechanism to **prevent the model from
    overfitting data.** That can be roughly seen as a way to avoid having a model
    that memorizes everything by heart and cannot generalize to new unseen data. Ingesting
    noise as part of the training data is one way of adding regularization, equivalent
    to adding a penalty term to the loss function. However, ingesting noise should
    be negligible with respect to what is closest to the true underlying data.
  prefs: []
  type: TYPE_NORMAL
- en: As such, it is even more important to define what should be close to true gold
    data. For that, instructions given to labelers are equally important.
  prefs: []
  type: TYPE_NORMAL
- en: To better understand the kind of instructions that are typically given, we can
    refer to “the plan” designed by Open Assistant, an open-source competitor of Chat
    GPT. In the words of the authors, **OpenAssistant** is a **chat-based assistant**
    that understands **tasks**, can interact with **third-party systems**, and **retrieve
    information** dynamically to do so.
  prefs: []
  type: TYPE_NORMAL
- en: The [plan](https://github.com/LAION-AI/Open-Assistant) designed consists of
    3 key steps inspired by [InstructGPT](https://openai.com/blog/instruction-following/).
  prefs: []
  type: TYPE_NORMAL
- en: Collect high-quality human-generated Instruction-Fulfillment samples (prompt
    + response) via a crowdsourced process, with a leaderboard to motivate the community
    and swag for top contributors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample multiple completions for each prompt and have users rank them from best
    to worst.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the gathered ranking data to train a reward model, then use the prompts
    and reward model to train an RLHF.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that it is very challenging to have curated training data, even when outsourced
    to human labelers. Indeed, outsourcing requires awareness of many potential biases,
    especially when outsourcing is **free and open to any individual.**
  prefs: []
  type: TYPE_NORMAL
- en: The risks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: “Again this should happen crowd-sourced, e.g. we need to deal with **unreliable
    potentially malicious users.** At least **multiple votes by independent users
    have to be collected to measure the overall agreement.**” [OpenAI Assistant.](https://github.com/LAION-AI/Open-Assistant)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**The bias of representation of the underlying population of labelers.**'
  prefs: []
  type: TYPE_NORMAL
- en: If labelers are all from the same demographic, the same ethnicity, age, gender,
    or socioeconomic status, they can end up ingesting heavy cultural bias within
    the models they are training. Hence, it is key to make sure the labelers are as
    diverse as possible from a set of key features to target, such as country, age,
    gender, etc.
  prefs: []
  type: TYPE_NORMAL
- en: This is something that big tech companies are very well aware of, hence they
    now integrate that at the very core of their recruiting. Engineers building products
    within those tech companies need to have a better view of the wide variety of
    end users, with different nationalities, genders, ages, ethnicities, etc. That
    way, when building a product and in particular when training an AI system, they
    are making sure they are not doing it from their perspective but also from the
    perspective of end users. However, given the inherent human bias, the best way
    to cope with that is to have a similar representation of the population of engineers
    building products as the population of end users.
  prefs: []
  type: TYPE_NORMAL
- en: '**Malicious users**'
  prefs: []
  type: TYPE_NORMAL
- en: The previous aspect is overall unintentional but there can be malicious intents
    too. In such cases, there is an even higher risk to create models that do not
    meet the expectations. Orchestrated attacks do exist, especially against big tech
    companies, and they can now happen during the training phase making even greater
    negative impacts on the AI systems of interest.
  prefs: []
  type: TYPE_NORMAL
- en: '**Overall bad quality labels despite clear instructions**'
  prefs: []
  type: TYPE_NORMAL
- en: If instructions are always shared with labelers, there can be misalignment between
    the task and its execution. For example, in the figure below, one can note the
    instructions to summarize a text. They need to be as clear as possible, in a way
    that they align with the way data is processed and fed to the AI itself. However,
    despite that, we can find some bad quality human outputs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/21126fca206e3ef21ce825f764c0479a.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Resource**: Adapted from Figure 2, [https://arxiv.org/pdf/2306.07899.pdf](https://arxiv.org/pdf/2306.07899.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: This can constitute an actual risk, especially when there are time constraints
    to push an AI into production.
  prefs: []
  type: TYPE_NORMAL
- en: For example, contractors improving **Google**’s Bard chatbot shared they have
    been asked to prioritize working fast over quality due to time constraints. Therefore,
    this could [potentially lead to inaccurate information being generated.](https://www.theregister.com/2023/06/21/google_bard_trainers/)
  prefs: []
  type: TYPE_NORMAL
- en: “You can be given just two minutes for something that would actually take 15
    minutes to verify” from [The Register.](https://www.theregister.com/2023/06/21/google_bard_trainers/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In **computer vision**, a similar space as in NLP, we have already observed
    issues due to biased training data. Face-Depixelizer, a model based on “PULSE:
    Self-Supervised photo upsampling via latent space exploration of generative models”
    was released in 2020 and could output the original picture from a pixelized version
    of it. More rigorously, it would output the closest known de-pixelized picture.'
  prefs: []
  type: TYPE_NORMAL
- en: However, due to bias, involving the risks outlined above, it led to issues with
    black faces, which constitute in practice a subset unknown by the model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/491d87ae686e56a6edfe6f6e526904d5.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Resource**: Generated by the **author**. PULSE gets a very distant picture
    as a reconstruction of the pixelized output. This is due to biased training data.'
  prefs: []
  type: TYPE_NORMAL
- en: In [an article I shared in TDS](https://medium.com/towards-data-science/the-danger-of-bias-in-ai-c3ce68eabbcc),
    I highlighted a great explanation of this phenomenon given by Yann Lecun. It demystifies
    the way AI works and how much it relies on training data. Hence, we understand
    the utmost importance of following clear instructions with the idea of having
    great quality training data.
  prefs: []
  type: TYPE_NORMAL
- en: '*Lecun explains in a tweet that “ML systems are biased when data is biased.
    This face upsampling system makes everyone look white because the network was
    pretrained on FlickFaceHQ, which mainly contains white people pics. Train the*
    ***exact*** *same system on a dataset from Senegal and everyone will look African.”*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Create a system by sampling high-quality training data**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One solution to avoid that, following Lecun’s explanation is to have an objective
    system focusing on the quality of the training data. This system should make sure
    the quality is aligned with the task at hand. For example, with Face-Depixeliser,
    the training set should contain faces from the entire population, including back
    people.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, as I have outlined in another article, such a system is not easy to
    put in place. Indeed, evaluating that your dataset is diverse enough is not a
    trivial task at all in many cases. Though, one question to always have in mind
    could be as follows: “**Is my data representing my population of interest?**”.
    If I am interested in classifying cats and dogs, do I have enough diversity in
    the cats to show all the possible cat breeds? This is the same for new hires:
    if the company is defending diversity and inclusion, is the population of interest
    diverse enough in terms of age, education, gender, sexual preferences, etc?'
  prefs: []
  type: TYPE_NORMAL
- en: A simplistic approach consists in having a system focusing on sampling accurately.
    Indeed, one could think of a Sampling Model as an important model in the ML pipeline.
    Such a model allows a better representation of the underlying population by correctly
    sampling from the source data. The figure below shows how it could be integrated
    into an ML pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/57d0b3ab2391bc61b95190be7f791d7b.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Resource**: From the **author**. Simplistic (top) vs Intelligent Sampling
    Model (bottom). The first step in the above pipeline consists of random sampling
    from the source data and getting a classic 80% training data and 20% testing data.
    The first step in the bottom pipeline focuses on key features to help ensure diversity
    in your data. For example, gender could be a key feature based on the model represented
    in Figure 2\. Once we have provided a reasonable distribution of gender, we can
    then sample from this filtered dataset and get a training and a testing dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: An “Intelligent Sampling Model” (ISM) can be seen as a function that takes key
    features such as gender and age, as well as the expected distribution of those
    features in the underlying population, into account. Then, based on that, the
    ISM can sample from the source data and make sure the expected distributions are
    closely represented in the sampled data. The figure below shows the generic idea
    behind ISM, for a simple use case.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f7783db3b398649acc9c9b17d1f578d4.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Resource**: From the **author**. The generic idea behind ISM. ISM allows
    you to sample from source data following some predefined requirements based on
    key features such as Gender. In this example, the user sets a prior on the Gender
    and ISM makes sure such distribution is closely represented in its output. We
    go from a distribution of 70% male, 30% female to a distribution of 52% male and
    48% female.'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. **Create a system to detect risks of AI-generated data**
  prefs: []
  type: TYPE_NORMAL
- en: '[Veselovsky, V., Ribeiro, M.H. and West, R.](https://arxiv.org/pdf/2306.07899.pdf)
    go into detail on architecting a model that is capable of detecting AI-generated
    training data. Their methodology for detecting synthetic text allows us to ensure
    that we can rely on the training data outsourced to humans.'
  prefs: []
  type: TYPE_NORMAL
- en: This classifier is an e5-base pre-trained model (Wang et al., 2022) fine-tuned
    on the specific dataset from MTurk. Its role is to classify whether a given text
    is generated from Chat GPT or an actual human.
  prefs: []
  type: TYPE_NORMAL
- en: Surprisingly their research shows that it is not a negligible phenomenon as
    their classifier successfully detected a large amount of LLM-generated input.
  prefs: []
  type: TYPE_NORMAL
- en: “We conclude that, although LLMs are still in their infancy, textual data collected
    via crowdsourcing is already produced to a large extent by machines, rather than
    by the hired human crowd workers.” [Veselovsky, V., Ribeiro, M.H. and West, R.](https://arxiv.org/pdf/2306.07899.pdf)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article has shown that, in the quest to achieve the highest AI performance,
    a lot of companies delegate the training and enhancement of their AI models to
    vendors. But this can backfire, as some vendors may either do a **bad job** or
    **cheat by employing another AI to do the task**, due to various factors. This
    leads to many hazards, some of which we examined in this article. In essence,
    biasing our AI tools can have a **huge impact on our society, in ways we may not
    foresee, especially now given the current popularity of large language models.**
  prefs: []
  type: TYPE_NORMAL
- en: 👋 One last thing —want to connect?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I am a Data Scientist at Microsoft and an ex-teacher at EPITA Paris. I have
    8 patents in AI and continuously push AI to the edge.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I would love you to be among my first1,000 followers.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Also, follow me on* [*LinkedIn*](https://www.linkedin.com/in/mastafa-foufa-666a1a109/)*.*'
  prefs: []
  type: TYPE_NORMAL
