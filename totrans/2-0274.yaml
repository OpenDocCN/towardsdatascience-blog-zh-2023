- en: AI Training Outsourced to AI and Not Humans
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工智能培训外包给人工智能而非人类
- en: 原文：[https://towardsdatascience.com/ai-training-outsourced-to-ai-and-not-humans-4ab616a2a84d](https://towardsdatascience.com/ai-training-outsourced-to-ai-and-not-humans-4ab616a2a84d)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/ai-training-outsourced-to-ai-and-not-humans-4ab616a2a84d](https://towardsdatascience.com/ai-training-outsourced-to-ai-and-not-humans-4ab616a2a84d)
- en: '![](../Images/36730c49324f68d38205c0e20c489a63.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36730c49324f68d38205c0e20c489a63.png)'
- en: Photo by [davide ragusa](https://unsplash.com/@davideragusa?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [davide ragusa](https://unsplash.com/@davideragusa?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: The Risk of Introducing Further Errors into Models
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入模型进一步错误的风险
- en: '[](https://medium.com/@mastafa.foufa?source=post_page-----4ab616a2a84d--------------------------------)[![Mastafa
    Foufa](../Images/2e0b26ed83f04e943438afa1aab462a8.png)](https://medium.com/@mastafa.foufa?source=post_page-----4ab616a2a84d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4ab616a2a84d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4ab616a2a84d--------------------------------)
    [Mastafa Foufa](https://medium.com/@mastafa.foufa?source=post_page-----4ab616a2a84d--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mastafa.foufa?source=post_page-----4ab616a2a84d--------------------------------)[![Mastafa
    Foufa](../Images/2e0b26ed83f04e943438afa1aab462a8.png)](https://medium.com/@mastafa.foufa?source=post_page-----4ab616a2a84d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4ab616a2a84d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4ab616a2a84d--------------------------------)
    [Mastafa Foufa](https://medium.com/@mastafa.foufa?source=post_page-----4ab616a2a84d--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4ab616a2a84d--------------------------------)
    ·11 min read·Jul 18, 2023
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4ab616a2a84d--------------------------------)
    ·11分钟阅读·2023年7月18日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: “We reran an abstract summarization task from the literature on Amazon Mechanical
    Turk and, through a combination of keystroke detection and synthetic text classification,
    estimate that 33–46% of crowd workers used LLMs when completing the task. Although
    generalization to other, less LLM friendly tasks is unclear, our results call
    for platforms, researchers, and crowd workers to find new ways to ensure that
    human data remain human, perhaps using the methodology proposed here as a stepping
    stone.” From [Veselovsky, V., Ribeiro, M.H. and West, R.](https://arxiv.org/pdf/2306.07899.pdf)
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我们在Amazon Mechanical Turk上重新进行了一个文献中的摘要总结任务，通过键击检测和合成文本分类的结合，估计33%–46%的众包工人在完成任务时使用了LLM。尽管这一结果是否可以推广到其他不那么友好LLM的任务尚不明确，但我们的结果呼吁平台、研究人员和众包工人寻找新的方法来确保人类数据保持人性，也许可以将这里提出的方法作为一个踏脚石。”
    引自 [Veselovsky, V., Ribeiro, M.H. 和 West, R.](https://arxiv.org/pdf/2306.07899.pdf)
- en: Recently, a study by the **Swiss Federal Institute of Technology** (EPFL) found
    that between 33% and 46% of gig workers paid to train AI models [may be outsourcing
    their work to AI.](https://arxiv.org/pdf/2306.07899.pdf)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，**瑞士联邦理工学院**（EPFL）的一项研究发现，33%到46%接受报酬培训AI模型的临时工可能正在将他们的工作外包给AI。[详细信息请见这里。](https://arxiv.org/pdf/2306.07899.pdf)
- en: The [MIT Technology Review](https://www.technologyreview.com/2023/06/22/1075405/the-people-paid-to-train-ai-are-outsourcing-their-work-to-ai/)
    discusses this research paper and explains how people who are paid to train AI
    are indeed **outsourcing their work to AI.** It explains that AI can now be used
    to create data sets and labels, tasks that are traditionally done by humans. It
    also discusses the implications of this trend, such as the potential for AI to
    learn from other AI which integrates further bias.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[MIT科技评论](https://www.technologyreview.com/2023/06/22/1075405/the-people-paid-to-train-ai-are-outsourcing-their-work-to-ai/)
    讨论了这篇研究论文，并解释了那些被支付来培训AI的人确实在**将他们的工作外包给AI**。文章解释了AI现在可以用来创建数据集和标签，这些任务传统上是由人类完成的。它还讨论了这一趋势的影响，例如AI可能从其他AI中学习，从而进一步整合偏见。'
- en: '![](../Images/ce3c81bb1e63ccc427699d5843cac623.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce3c81bb1e63ccc427699d5843cac623.png)'
- en: '**Resource**: From [Veselovsky, V., Ribeiro, M.H. and West, R.](https://arxiv.org/pdf/2306.07899.pdf)
    A model to discriminate mTurks responses generated manually by a human and responses
    generated by an AI. The authors use a classifier (real vs AI-generated) on real
    MTurk responses (where workers may or may not have relied on LLMs), estimating
    the prevalence of LLM usage.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源**：来自 [Veselovsky, V., Ribeiro, M.H. 和 West, R.](https://arxiv.org/pdf/2306.07899.pdf)
    一个模型来区分由人工生成的 mTurks 响应和由 AI 生成的响应。作者使用一个分类器（真实 vs AI 生成）对真实的 MTurk 响应进行分类（工人可能或可能没有依赖于
    LLMs），估计 LLM 使用的普及率。'
- en: How do we train AI systems?
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们如何训练 AI 系统？
- en: AI systems can be seen as Machine learning models. In a supervised setting,
    such systems need gold standard labels to build qualitative training data. This
    can be done internally, especially in big tech companies like Microsoft or Google.
    Then, for complex tasks involving large datasets, data labeling can also be outsourced
    to vendors who are typically expected to be subject-matter experts.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: AI 系统可以被视为机器学习模型。在监督环境下，这些系统需要黄金标准标签来构建高质量的训练数据。这可以在内部完成，特别是在像微软或谷歌这样的科技公司中。然后，对于涉及大型数据集的复杂任务，数据标注也可以外包给通常被期望成为领域专家的供应商。
- en: However, they can also be **online gig workers, with no particular expertise
    in the subject in question.** Indeed, you can find gig workers on platforms like
    [Mechanical Turk](https://www.mturk.com/) to complete tasks that are typically
    hard to automate.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，他们也可以是**在线零工，没有特定的主题专业知识。** 确实，你可以在像 [Mechanical Turk](https://www.mturk.com/)
    这样的平台上找到零工，以完成通常难以自动化的任务。
- en: “**Amazon** Mechanical Turk (MTurk) is a crowdsourcing marketplace that makes
    it easier for individuals and businesses to outsource their processes and jobs
    to a distributed workforce who can perform these tasks virtually. This could include
    anything from conducting simple data validation and research to more subjective
    tasks like survey participation, content moderation, and more. MTurk enables companies
    to harness the collective intelligence, skills, and insights from a global workforce
    to streamline business processes, augment data collection and analysis, and accelerate
    machine learning development.” from [https://www.mturk.com/](https://www.mturk.com/).
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “**亚马逊** Mechanical Turk (MTurk) 是一个众包市场，使个人和企业更容易将他们的过程和工作外包给可以虚拟执行这些任务的分布式劳动力。这可能包括从进行简单的数据验证和研究到更主观的任务，如调查参与、内容审查等。MTurk
    使公司能够利用全球劳动力的集体智慧、技能和见解来优化业务流程、增强数据收集和分析，并加速机器学习开发。” 来自 [https://www.mturk.com/](https://www.mturk.com/)。
- en: Understanding why this can be harmful is key. For that, let us take the example
    of Chat GPT which leverages human labelers to get high performance.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 了解这可能有害的原因是关键。为此，让我们以 Chat GPT 为例，它利用人类标注员来获得高性能。
- en: 'An example: Chat GPT'
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 例如：Chat GPT
- en: Chat GPT is a good example of a model that required a lot of human labelers.
    In the figure below outlining the steps taken to train the OpenAI model, we can
    note the importance of the labeler who helps refine the prompts at training time.
    If Chat GPT is, in essence, another large language model relying on unlabeled
    data such as the Wikipedia corpus, it reached a unique performance due to human
    labelers demonstrating desired outputs behavior on sampled prompts. That way,
    the underlying model tries to approach the outputs given by the human label themself.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Chat GPT 是一个需要大量人工标注员的模型的良好示例。在下图中概述了训练 OpenAI 模型所采取的步骤，我们可以注意到标注员在训练时帮助完善提示的重要性。如果
    Chat GPT 本质上是依赖于未标注数据（如维基百科语料库）的另一个大型语言模型，它由于人类标注员在采样提示上展示的期望输出行为而达到独特的性能。这样，基础模型尝试接近由人类标注员自身提供的输出。
- en: '**To remember**. Chat GPT is a large language model because it uses a huge
    amount of text data to train a neural network that can generate coherent and diverse
    responses to natural language queries. A language model is a system that assigns
    probabilities to sequences of words or tokens, based on how likely they are to
    occur in a given context. A large language model can in particular capture complex
    and long-term dependencies among words.'
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**记住**。Chat GPT 是一个大型语言模型，因为它使用大量文本数据来训练一个神经网络，该网络可以生成对自然语言查询的连贯和多样的响应。语言模型是一个系统，根据单词或符号序列在给定上下文中发生的可能性来分配概率。大型语言模型特别能够捕捉单词之间复杂和长期的依赖关系。'
- en: '![](../Images/6de821a143cf7799c8bd93c9d92dd89f.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6de821a143cf7799c8bd93c9d92dd89f.png)'
- en: '**Resource**: From [OpenAI](https://openai.com/blog/chatgpt).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源**：来自 [OpenAI](https://openai.com/blog/chatgpt)。'
- en: In other words, one of the objective functions to optimize consists in reducing
    the distance between the AI output and the human output. That naturally assumes
    that the labeler holds the truth.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，其中一个目标函数是减少 AI 输出与人类输出之间的距离。这自然假设标注者掌握了真相。
- en: '![](../Images/106c05957484337c33e86656c4d4fa06.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/106c05957484337c33e86656c4d4fa06.png)'
- en: '**Resource**: From the **author**. Reducing the distance between AI predictions
    (purple Gaussian) and human predictions (orange Gaussian).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源**：来自**作者**。减少 AI 预测（紫色高斯）与人类预测（橙色高斯）之间的距离。'
- en: Now, to mitigate the problem of relying on the labeler, we typically give clear
    instructions to define “what is good” and “what is bad”.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了减轻对标注者的依赖问题，我们通常会提供明确的指令来定义“什么是好的”以及“什么是坏的”。
- en: Such labels are important because AI models are essentially mathematical functions
    that map some input data to some output data. That includes large language models
    like Chat GPT which essentially predict the next tokens (*output*), given a list
    of tokens (*input*).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的标签很重要，因为 AI 模型本质上是将某些输入数据映射到某些输出数据的数学函数。这包括像 Chat GPT 这样的庞大语言模型，它本质上是根据一系列标记（*输入*）预测下一个标记（*输出*）。
- en: Once the training data is built and made available, training a machine learning
    model in a supervised setting involves using an optimization algorithm, such as
    gradient descent, that iteratively updates the parameters of the model to minimize
    a loss function. Such loss function is a measure of how well the model fits the
    data, or how much error it makes on the predictions. Now, typically the optimization
    algorithm works by computing the gradient of the loss function with respect to
    the parameters of the model, which indicates the direction and magnitude of the
    change that would reduce the loss. The algorithm then updates the parameters by
    taking a small step in the opposite direction of the gradient, which is expected
    to lower the loss. This process is repeated until the loss reaches a minimum or
    a convergence criterion is met.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练数据构建并可用，在有监督的设置中训练一个机器学习模型涉及使用优化算法，例如梯度下降，该算法迭代地更新模型的参数以最小化损失函数。损失函数是一种衡量模型拟合数据的好坏或预测误差的标准。通常，优化算法通过计算损失函数相对于模型参数的梯度来工作，这表明减少损失的变化方向和幅度。然后，算法通过在梯度的相反方向上采取小步更新参数，预计这将降低损失。这个过程重复进行，直到损失达到最小值或满足收敛标准。
- en: In other words, one can imagine that at each step, the model is predicting things
    that need to go in the direction of the human labeler, until a local optimum is
    found.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，可以想象在每一步，模型都在预测需要朝着人工标注者的方向发展的事物，直到找到一个局部最优解。
- en: There can also be a **regularization** mechanism to **prevent the model from
    overfitting data.** That can be roughly seen as a way to avoid having a model
    that memorizes everything by heart and cannot generalize to new unseen data. Ingesting
    noise as part of the training data is one way of adding regularization, equivalent
    to adding a penalty term to the loss function. However, ingesting noise should
    be negligible with respect to what is closest to the true underlying data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以有一个**正则化**机制来**防止模型过拟合数据**。这可以粗略地看作是一种避免模型死记硬背所有数据而无法对新的未见数据进行泛化的方法。将噪声作为训练数据的一部分进行处理是一种添加正则化的方法，相当于在损失函数中增加惩罚项。然而，相对于最接近真实底层数据的部分，噪声的影响应该是微不足道的。
- en: As such, it is even more important to define what should be close to true gold
    data. For that, instructions given to labelers are equally important.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，更重要的是定义什么应该接近真实的黄金数据。为此，给标注者的指令同样重要。
- en: To better understand the kind of instructions that are typically given, we can
    refer to “the plan” designed by Open Assistant, an open-source competitor of Chat
    GPT. In the words of the authors, **OpenAssistant** is a **chat-based assistant**
    that understands **tasks**, can interact with **third-party systems**, and **retrieve
    information** dynamically to do so.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解通常给出的指令类型，我们可以参考“计划”，这是由 Open Assistant 设计的，Open Assistant 是 Chat GPT
    的一个开源竞争者。根据作者的描述，**OpenAssistant** 是一个**基于聊天的助手**，它能够理解**任务**，与**第三方系统**互动，并**动态检索信息**以完成任务。
- en: The [plan](https://github.com/LAION-AI/Open-Assistant) designed consists of
    3 key steps inspired by [InstructGPT](https://openai.com/blog/instruction-following/).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 设计的[计划](https://github.com/LAION-AI/Open-Assistant)包括受[InstructGPT](https://openai.com/blog/instruction-following/)启发的3个关键步骤。
- en: Collect high-quality human-generated Instruction-Fulfillment samples (prompt
    + response) via a crowdsourced process, with a leaderboard to motivate the community
    and swag for top contributors.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过众包过程收集高质量的人类生成的指令完成样本（提示+回应），设置排行榜以激励社区，并为顶级贡献者提供奖励。
- en: Sample multiple completions for each prompt and have users rank them from best
    to worst.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对每个提示进行多样化的完成样本，并让用户按从最好到最差的顺序对它们进行排名。
- en: Use the gathered ranking data to train a reward model, then use the prompts
    and reward model to train an RLHF.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用收集的排名数据训练奖励模型，然后使用提示和奖励模型训练RLHF。
- en: Note that it is very challenging to have curated training data, even when outsourced
    to human labelers. Indeed, outsourcing requires awareness of many potential biases,
    especially when outsourcing is **free and open to any individual.**
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，即使外包给人类标注者，拥有经过筛选的训练数据也非常具有挑战性。确实，外包需要意识到许多潜在的偏见，特别是当外包是**免费并对任何个人开放**时。
- en: The risks
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 风险
- en: “Again this should happen crowd-sourced, e.g. we need to deal with **unreliable
    potentially malicious users.** At least **multiple votes by independent users
    have to be collected to measure the overall agreement.**” [OpenAI Assistant.](https://github.com/LAION-AI/Open-Assistant)
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “再次，这应该是众包进行的，例如，我们需要处理**不可靠且可能恶意的用户。** 至少**需要收集多个独立用户的投票以衡量总体一致性。**” [OpenAI
    Assistant.](https://github.com/LAION-AI/Open-Assistant)
- en: '**The bias of representation of the underlying population of labelers.**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**标注者基础人群的代表性偏见。**'
- en: If labelers are all from the same demographic, the same ethnicity, age, gender,
    or socioeconomic status, they can end up ingesting heavy cultural bias within
    the models they are training. Hence, it is key to make sure the labelers are as
    diverse as possible from a set of key features to target, such as country, age,
    gender, etc.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果标注者都来自相同的人口统计群体、种族、年龄、性别或社会经济地位，他们可能会在训练的模型中带入严重的文化偏见。因此，确保标注者在关键特征如国家、年龄、性别等方面尽可能多样化是关键。
- en: This is something that big tech companies are very well aware of, hence they
    now integrate that at the very core of their recruiting. Engineers building products
    within those tech companies need to have a better view of the wide variety of
    end users, with different nationalities, genders, ages, ethnicities, etc. That
    way, when building a product and in particular when training an AI system, they
    are making sure they are not doing it from their perspective but also from the
    perspective of end users. However, given the inherent human bias, the best way
    to cope with that is to have a similar representation of the population of engineers
    building products as the population of end users.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这是大科技公司非常清楚的事情，因此它们现在将其集成到招聘的核心。那些在这些科技公司中构建产品的工程师需要对各种各样的终端用户有更好的了解，包括不同的国籍、性别、年龄、种族等。这样，在构建产品时，特别是在训练AI系统时，他们就能确保不是仅从他们自己的视角出发，而是从终端用户的视角出发。然而，鉴于固有的人类偏见，应对这种情况的最佳方法是确保构建产品的工程师的人口代表性与终端用户的代表性相似。
- en: '**Malicious users**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**恶意用户**'
- en: The previous aspect is overall unintentional but there can be malicious intents
    too. In such cases, there is an even higher risk to create models that do not
    meet the expectations. Orchestrated attacks do exist, especially against big tech
    companies, and they can now happen during the training phase making even greater
    negative impacts on the AI systems of interest.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的方面总体上是无意的，但也可能存在恶意意图。在这种情况下，创建不符合预期的模型的风险更高。协调攻击确实存在，特别是针对大科技公司，它们现在可能在训练阶段发生，对目标AI系统造成更大的负面影响。
- en: '**Overall bad quality labels despite clear instructions**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**尽管有明确的指示，但整体上标签质量差**'
- en: If instructions are always shared with labelers, there can be misalignment between
    the task and its execution. For example, in the figure below, one can note the
    instructions to summarize a text. They need to be as clear as possible, in a way
    that they align with the way data is processed and fed to the AI itself. However,
    despite that, we can find some bad quality human outputs.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果指令总是与标注者共享，任务与其执行之间可能会出现不一致。例如，在下图中，可以注意到总结文本的指令。它们需要尽可能清晰，以便与数据处理和传递给 AI
    的方式保持一致。然而，尽管如此，我们仍然可以发现一些质量较差的人类输出。
- en: '![](../Images/21126fca206e3ef21ce825f764c0479a.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/21126fca206e3ef21ce825f764c0479a.png)'
- en: '**Resource**: Adapted from Figure 2, [https://arxiv.org/pdf/2306.07899.pdf](https://arxiv.org/pdf/2306.07899.pdf).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源**：改编自图2，[https://arxiv.org/pdf/2306.07899.pdf](https://arxiv.org/pdf/2306.07899.pdf)。'
- en: This can constitute an actual risk, especially when there are time constraints
    to push an AI into production.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能构成实际风险，特别是当有时间限制迫使 AI 推向生产时。
- en: For example, contractors improving **Google**’s Bard chatbot shared they have
    been asked to prioritize working fast over quality due to time constraints. Therefore,
    this could [potentially lead to inaccurate information being generated.](https://www.theregister.com/2023/06/21/google_bard_trainers/)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，改进**谷歌** Bard 聊天机器人的承包商表示，由于时间限制，他们被要求优先考虑工作速度而非质量。因此，这可能[导致生成不准确的信息。](https://www.theregister.com/2023/06/21/google_bard_trainers/)
- en: “You can be given just two minutes for something that would actually take 15
    minutes to verify” from [The Register.](https://www.theregister.com/2023/06/21/google_bard_trainers/)
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “你可能只会被给到两分钟时间来完成实际上需要15分钟来验证的任务”来自[The Register。](https://www.theregister.com/2023/06/21/google_bard_trainers/)
- en: A solution
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'In **computer vision**, a similar space as in NLP, we have already observed
    issues due to biased training data. Face-Depixelizer, a model based on “PULSE:
    Self-Supervised photo upsampling via latent space exploration of generative models”
    was released in 2020 and could output the original picture from a pixelized version
    of it. More rigorously, it would output the closest known de-pixelized picture.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '在**计算机视觉**这一与自然语言处理相似的领域，我们已经观察到由于偏见训练数据而产生的问题。Face-Depixelizer 是基于“PULSE:
    Self-Supervised photo upsampling via latent space exploration of generative models”模型的，于2020年发布，能够从像素化版本中输出原始图片。更严格地说，它会输出最接近的已知去像素化图片。'
- en: However, due to bias, involving the risks outlined above, it led to issues with
    black faces, which constitute in practice a subset unknown by the model.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于偏见和上述风险，导致了对黑人面孔的问题，这实际上构成了模型未知的子集。
- en: '![](../Images/491d87ae686e56a6edfe6f6e526904d5.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/491d87ae686e56a6edfe6f6e526904d5.png)'
- en: '**Resource**: Generated by the **author**. PULSE gets a very distant picture
    as a reconstruction of the pixelized output. This is due to biased training data.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源**：由**作者**生成。PULSE 获取了一个非常远的图片作为像素化输出的重建。这是由于偏见训练数据造成的。'
- en: In [an article I shared in TDS](https://medium.com/towards-data-science/the-danger-of-bias-in-ai-c3ce68eabbcc),
    I highlighted a great explanation of this phenomenon given by Yann Lecun. It demystifies
    the way AI works and how much it relies on training data. Hence, we understand
    the utmost importance of following clear instructions with the idea of having
    great quality training data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在[我在 TDS 分享的一篇文章中](https://medium.com/towards-data-science/the-danger-of-bias-in-ai-c3ce68eabbcc)，我强调了
    Yann Lecun 对这一现象的精彩解释。它揭示了 AI 的工作方式及其对训练数据的依赖。因此，我们理解遵循清晰指令和拥有高质量训练数据的极端重要性。
- en: '*Lecun explains in a tweet that “ML systems are biased when data is biased.
    This face upsampling system makes everyone look white because the network was
    pretrained on FlickFaceHQ, which mainly contains white people pics. Train the*
    ***exact*** *same system on a dataset from Senegal and everyone will look African.”*'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Lecun 在推特上解释说：“当数据有偏见时，机器学习系统也会有偏见。这个面孔上采样系统让每个人看起来都是白人，因为网络是在主要包含白人图片的 FlickFaceHQ
    上进行预训练的。对一个来自塞内加尔的数据集进行* ***完全相同的*** *训练，大家都会看起来是非洲人。”*'
- en: '**Create a system by sampling high-quality training data**'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**通过采样高质量训练数据来创建系统**'
- en: One solution to avoid that, following Lecun’s explanation is to have an objective
    system focusing on the quality of the training data. This system should make sure
    the quality is aligned with the task at hand. For example, with Face-Depixeliser,
    the training set should contain faces from the entire population, including back
    people.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 避免这一点的一个解决方案，按照 Lecun 的解释，是拥有一个专注于训练数据质量的客观系统。这个系统应确保质量与当前任务对齐。例如，使用 Face-Depixeliser
    时，训练集应包含来自整个群体的面孔，包括黑人。
- en: 'However, as I have outlined in another article, such a system is not easy to
    put in place. Indeed, evaluating that your dataset is diverse enough is not a
    trivial task at all in many cases. Though, one question to always have in mind
    could be as follows: “**Is my data representing my population of interest?**”.
    If I am interested in classifying cats and dogs, do I have enough diversity in
    the cats to show all the possible cat breeds? This is the same for new hires:
    if the company is defending diversity and inclusion, is the population of interest
    diverse enough in terms of age, education, gender, sexual preferences, etc?'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如我在另一篇文章中概述的那样，这样的系统并不容易建立。确实，评估你的数据集是否足够多样化在许多情况下并非易事。不过，一个始终要牢记的问题可能是：“**我的数据是否代表了我感兴趣的群体？**”如果我对分类猫和狗感兴趣，我是否在猫的样本中有足够的多样性来展示所有可能的猫品种？对于新员工也是如此：如果公司倡导多样性和包容性，那么感兴趣的群体在年龄、教育、性别、性取向等方面是否足够多样化？
- en: A simplistic approach consists in having a system focusing on sampling accurately.
    Indeed, one could think of a Sampling Model as an important model in the ML pipeline.
    Such a model allows a better representation of the underlying population by correctly
    sampling from the source data. The figure below shows how it could be integrated
    into an ML pipeline.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简化的方法是拥有一个专注于准确采样的系统。确实，人们可以将采样模型视为 ML 管道中的一个重要模型。这样的模型通过正确地从源数据中采样，允许对基础群体进行更好的表征。下图展示了它如何被集成到
    ML 管道中。
- en: '![](../Images/57d0b3ab2391bc61b95190be7f791d7b.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57d0b3ab2391bc61b95190be7f791d7b.png)'
- en: '**Resource**: From the **author**. Simplistic (top) vs Intelligent Sampling
    Model (bottom). The first step in the above pipeline consists of random sampling
    from the source data and getting a classic 80% training data and 20% testing data.
    The first step in the bottom pipeline focuses on key features to help ensure diversity
    in your data. For example, gender could be a key feature based on the model represented
    in Figure 2\. Once we have provided a reasonable distribution of gender, we can
    then sample from this filtered dataset and get a training and a testing dataset.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源**：来自**作者**。简化（顶部）与智能采样模型（底部）。上面的管道中的第一步是从源数据中随机抽样，并获取经典的 80% 训练数据和 20%
    测试数据。底部管道中的第一步则关注关键特征，以帮助确保数据的多样性。例如，性别可能是基于图 2 中所示模型的关键特征。一旦我们提供了合理的性别分布，我们就可以从这个过滤后的数据集中抽样，并得到一个训练数据集和一个测试数据集。'
- en: An “Intelligent Sampling Model” (ISM) can be seen as a function that takes key
    features such as gender and age, as well as the expected distribution of those
    features in the underlying population, into account. Then, based on that, the
    ISM can sample from the source data and make sure the expected distributions are
    closely represented in the sampled data. The figure below shows the generic idea
    behind ISM, for a simple use case.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: “智能采样模型”（ISM）可以看作是一个考虑性别和年龄等关键特征，以及这些特征在基础人口中预期分布的函数。然后，基于此，ISM 可以从源数据中抽样，并确保预期的分布在抽样数据中得到紧密体现。下图展示了
    ISM 的通用思想，用于一个简单的使用案例。
- en: '![](../Images/f7783db3b398649acc9c9b17d1f578d4.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f7783db3b398649acc9c9b17d1f578d4.png)'
- en: '**Resource**: From the **author**. The generic idea behind ISM. ISM allows
    you to sample from source data following some predefined requirements based on
    key features such as Gender. In this example, the user sets a prior on the Gender
    and ISM makes sure such distribution is closely represented in its output. We
    go from a distribution of 70% male, 30% female to a distribution of 52% male and
    48% female.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**资源**：来自**作者**。ISM 的通用思想。ISM 允许你从源数据中抽样，遵循基于性别等关键特征的某些预定义要求。在这个例子中，用户对性别设置了先验，ISM
    确保这种分布在输出中得到了紧密体现。我们将从 70% 男性，30% 女性的分布，转变为 52% 男性和 48% 女性的分布。'
- en: 2\. **Create a system to detect risks of AI-generated data**
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. **创建一个检测 AI 生成数据风险的系统**
- en: '[Veselovsky, V., Ribeiro, M.H. and West, R.](https://arxiv.org/pdf/2306.07899.pdf)
    go into detail on architecting a model that is capable of detecting AI-generated
    training data. Their methodology for detecting synthetic text allows us to ensure
    that we can rely on the training data outsourced to humans.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[Veselovsky, V., Ribeiro, M.H. 和 West, R.](https://arxiv.org/pdf/2306.07899.pdf)详细介绍了构建能够检测AI生成训练数据的模型。他们的检测合成文本的方法使我们能够确保我们可以依赖于外包给人类的训练数据。'
- en: This classifier is an e5-base pre-trained model (Wang et al., 2022) fine-tuned
    on the specific dataset from MTurk. Its role is to classify whether a given text
    is generated from Chat GPT or an actual human.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 该分类器是一个e5-base预训练模型（Wang等，2022），经过在MTurk特定数据集上的微调。它的作用是分类给定的文本是否由Chat GPT生成，还是由真正的人类生成。
- en: Surprisingly their research shows that it is not a negligible phenomenon as
    their classifier successfully detected a large amount of LLM-generated input.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，他们的研究表明，这并不是一个可以忽视的现象，因为他们的分类器成功地检测到了大量由LLM生成的输入。
- en: “We conclude that, although LLMs are still in their infancy, textual data collected
    via crowdsourcing is already produced to a large extent by machines, rather than
    by the hired human crowd workers.” [Veselovsky, V., Ribeiro, M.H. and West, R.](https://arxiv.org/pdf/2306.07899.pdf)
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我们得出结论，尽管大型语言模型（LLMs）仍处于初期阶段，但通过众包收集的文本数据在很大程度上是由机器生成的，而非由雇佣的人工众包工作者生成的。” [Veselovsky,
    V., Ribeiro, M.H. 和 West, R.](https://arxiv.org/pdf/2306.07899.pdf)
- en: Conclusion
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: This article has shown that, in the quest to achieve the highest AI performance,
    a lot of companies delegate the training and enhancement of their AI models to
    vendors. But this can backfire, as some vendors may either do a **bad job** or
    **cheat by employing another AI to do the task**, due to various factors. This
    leads to many hazards, some of which we examined in this article. In essence,
    biasing our AI tools can have a **huge impact on our society, in ways we may not
    foresee, especially now given the current popularity of large language models.**
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章表明，在追求最高人工智能性能的过程中，许多公司将其人工智能模型的训练和增强工作委托给供应商。但这可能会适得其反，因为一些供应商可能会由于各种因素而**做得很糟**或**通过使用另一种人工智能来完成任务进行作弊**。这带来了许多风险，其中一些在本文中进行了探讨。从本质上讲，偏向我们的人工智能工具可能对我们的社会产生**巨大的影响，可能是我们无法预见的，特别是在当前大型语言模型流行的情况下。**
- en: 👋 One last thing —want to connect?
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 👋 最后一点——想要联系一下吗？
- en: I am a Data Scientist at Microsoft and an ex-teacher at EPITA Paris. I have
    8 patents in AI and continuously push AI to the edge.
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我是微软的数据科学家，也是巴黎EPITA的前任讲师。我拥有8项人工智能领域的专利，并持续推动人工智能的前沿发展。
- en: ''
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I would love you to be among my first1,000 followers.
  id: totrans-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我希望你能成为我的前1,000名关注者之一。
- en: '*Also, follow me on* [*LinkedIn*](https://www.linkedin.com/in/mastafa-foufa-666a1a109/)*.*'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*此外，请在* [*LinkedIn*](https://www.linkedin.com/in/mastafa-foufa-666a1a109/)*
    上关注我。*'
