- en: Generative Q&A With GPT 3.5 and Long-Term Memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/generative-question-answering-with-long-term-memory-c280e237b144](https://towardsdatascience.com/generative-question-answering-with-long-term-memory-c280e237b144)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Exploring the new world of retrieval-augmented ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://jamescalam.medium.com/?source=post_page-----c280e237b144--------------------------------)[![James
    Briggs](../Images/cb34b7011748e4d8607b7ff4a8510a93.png)](https://jamescalam.medium.com/?source=post_page-----c280e237b144--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c280e237b144--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c280e237b144--------------------------------)
    [James Briggs](https://jamescalam.medium.com/?source=post_page-----c280e237b144--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c280e237b144--------------------------------)
    ·6 min read·Feb 17, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c5e51b39e15d83cb4ab0c9ed3580abef.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Bret Kavanaugh](https://unsplash.com/@bretkavanaugh?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral). *Originally
    published at* [*pinecone.io*](https://www.pinecone.io/learn/openai-gen-qa/), where
    the author is employed*.*
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI sparked several *“wow”* moments in 2022\. From generative art
    tools like OpenAI’s DALL-E 2, Midjourney, and Stable Diffusion, to the next generation
    of **L**arge **L**anguage **M**odels like OpenAI’s GPT-3.5 generation models,
    BLOOM, and chatbots like LaMDA and ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s hardly surprising that Generative AI is experiencing a boom in interest
    and innovation [1]. Yet, this marks *just* the first year of widespread adoption
    of generative AI: The early days of a new field poised to disrupt how we interact
    with machines.'
  prefs: []
  type: TYPE_NORMAL
- en: One of the most thought-provoking use cases belongs to **G**enerative **Q**uestion-
    **A**nswering (GQA). Using GQA, we can sculpt human-like interaction with machines
    for information retrieval (IR).
  prefs: []
  type: TYPE_NORMAL
- en: We all use IR systems every day. Google search indexes the web and retrieves
    relevant information to your search terms. Netflix uses your behavior and history
    on the platform to recommend new TV shows and movies, and Amazon does the same
    with products [2].
  prefs: []
  type: TYPE_NORMAL
- en: These applications of IR are world-changing. Yet, they may be little more than
    a faint echo of what we will see in the coming months and years with the combination
    of IR and GQA.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine a Google that can answer your queries with an intelligent and insightful
    summary based on the top 20 pages — highlighting key points and information sources.
  prefs: []
  type: TYPE_NORMAL
- en: The technology available today already makes this possible and surprisingly
    easy. This article will look at retrieval-augmented GQA and how to implement it
    with Pinecone and OpenAI.
  prefs: []
  type: TYPE_NORMAL
- en: The most straightforward GQA system requires nothing more than a user text query
    and a large language model (LLM).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/34a8a8de6ed776b5238ad9f1c343327e.png)'
  prefs: []
  type: TYPE_IMG
- en: Simplest GQA system.
  prefs: []
  type: TYPE_NORMAL
- en: We can access one of the most advanced LLMs in the world via [OpenAI](https://platform.openai.com).
    To start, we sign up for an [API key](https://beta.openai.com).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1c3298a93f8d1045bacd636fe8acf1d2.png)'
  prefs: []
  type: TYPE_IMG
- en: After signing up for an account, API keys can be created by clicking on your
    account (top-right) > View API keys > Create new secret key.
  prefs: []
  type: TYPE_NORMAL
- en: Then we switch to a Python file or notebook, install some prerequisites, and
    initialize our connection to OpenAI.
  prefs: []
  type: TYPE_NORMAL
- en: 'From here, we can use the OpenAI completion endpoint to ask a question like
    *“who was the 12th person on the moon and when did they land?”*:'
  prefs: []
  type: TYPE_NORMAL
- en: We get an accurate answer immediately. Yet, this question is relatively easy,
    what happens if we ask about a lesser-known topic?
  prefs: []
  type: TYPE_NORMAL
- en: Although this answer is technically correct, it isn’t an answer. It tells us
    to use a supervised training method and learn the relationship between sentences.
    Both of these facts are true but do not answer the original question.
  prefs: []
  type: TYPE_NORMAL
- en: There are two options for allowing our LLM to better understand the topic and,
    more precisely, answer the question.
  prefs: []
  type: TYPE_NORMAL
- en: We fine-tune the LLM on text data covering the domain of fine-tuning sentence
    transformers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use *retrieval-augmented generation*, meaning we add an information retrieval
    component to our GQA process. Adding a retrieval step allows us to retrieve relevant
    information and feed this into the LLM as a *secondary source* of information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the following sections, we will outline how to implement option **two**.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Knowledge Base
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With option **two** of implementing retrieval, we need an external “*knowledge
    base”*. A knowledge base acts as the place where we store information and as the
    system that effectively retrieves this information.
  prefs: []
  type: TYPE_NORMAL
- en: A knowledge base is a store of information that can act as an external reference
    for GQA models. We can think of it as the *“long-term memory”* for AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: We refer to knowledge bases that can enable the retrieval of semantically relevant
    information as *vector databases*.
  prefs: []
  type: TYPE_NORMAL
- en: A vector database stores vector representations of information encoded using
    specific ML models. These models have an “understanding” of language and can encode
    passages with similar meanings into a similar vector space and dissimilar passages
    into a dissimilar vector space.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2946718bf57e92359f02796ef22df104.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can achieve this with OpenAI via the embed endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll need to repeat this embedding process over many records that will act
    as our pipeline’s external source of information. These records still need to
    be downloaded and prepared for embedding.
  prefs: []
  type: TYPE_NORMAL
- en: Data Preparation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The dataset we will use in our knowledge base is the `jamescalam/youtube-transcriptions`
    dataset hosted on Hugging Face *Datasets*. It contains transcribed audio from
    several ML and tech YouTube channels. We download it with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The dataset contains many small snippets of text data. We need to merge several
    snippets to create more substantial chunks of text that contain more meaningful
    information.
  prefs: []
  type: TYPE_NORMAL
- en: With the text chunks created, we can begin initializing our knowledge base and
    populating it with our data.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the Vector Database
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The vector database is the storage and retrieval component in our pipeline.
    We use Pinecone as our vector database. For this, we need to [sign up for a free
    API key](https://app.pinecone.io/) and enter it below, where we create the index
    for storing our data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we embed and index a dataset like so:'
  prefs: []
  type: TYPE_NORMAL
- en: We’re ready to combine OpenAI’s `Completion` and `Embedding` endpoints with
    our Pinecone vector database to create a retrieval-augmented GQA system.
  prefs: []
  type: TYPE_NORMAL
- en: OP Stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The OpenAI Pinecone (OP) stack is an increasingly popular choice for building
    high-performance AI apps, including retrieval-augmented GQA.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our pipeline during *query time* consists of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI `Embedding` endpoint to create vector representations of each query.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pinecone vector database to search for relevant passages from the database of
    previously indexed contexts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: OpenAI `Completion` endpoint to generate a natural language answer considering
    the retrieved contexts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/5cafc844cc7581526b3e836707a46da5.png)'
  prefs: []
  type: TYPE_IMG
- en: We start by encoding queries using the same encoder model to create a query
    vector `xq`.
  prefs: []
  type: TYPE_NORMAL
- en: The query vector `xq` is used to query Pinecone via `index.query`, and previously
    indexed passage vectors are compared to find the most similar matches - returned
    in `res` above.
  prefs: []
  type: TYPE_NORMAL
- en: Using these returned contexts, we can construct a prompt instructing the generative
    LLM to answer the question based on the retrieved contexts. To keep things simple,
    we will do all this in a function called `retrieve`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the generated *expanded query* (`query_with_contexts`) has been shortened
    for readability.
  prefs: []
  type: TYPE_NORMAL
- en: From `retrieve`, we produce a longer prompt ( `query_with_contexts`) containing
    some instructions, the contexts, and the original question.
  prefs: []
  type: TYPE_NORMAL
- en: The prompt is then fed into the generative LLM via OpenAI’s `Completion` endpoint.
    As before, we use the `complete` function to handle everything.
  prefs: []
  type: TYPE_NORMAL
- en: Because of the additional *“source knowledge”* (information fed directly into
    the model), we have eliminated the hallucinations of the LLM — producing accurate
    answers to our question.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond providing more factual answers, we also have the *sources* of information
    from Pinecone used to generate our answer. Adding this to downstream tools or
    apps can help improve user trust in the system. Allowing users to confirm the
    reliability of the information being presented to them.
  prefs: []
  type: TYPE_NORMAL
- en: That’s it for this walkthrough of retrieval-augmented **G**enerative **Q**uestion
    **A**nswering (GQA) systems.
  prefs: []
  type: TYPE_NORMAL
- en: As demonstrated, LLMs alone work incredibly well but struggle with more niche
    or specific questions. This often leads to *hallucinations* that are rarely obvious
    and likely to go undetected by system users.
  prefs: []
  type: TYPE_NORMAL
- en: By adding a *“long-term memory”* component to our GQA system, we benefit from
    an external knowledge base to improve system factuality and user trust in generated
    outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Naturally, there is vast potential for this type of technology. Despite being
    a new technology, we are already seeing its use in [YouChat](https://blog.you.com/introducing-youchat-the-ai-search-assistant-that-lives-in-your-search-engine-eff7badcd655),
    several [podcast search apps](https://huberman.rile.yt), and rumors of its upcoming
    use as a challenger to Google itself [3].
  prefs: []
  type: TYPE_NORMAL
- en: There is potential for disruption in any place where the need for information
    exists, and retrieval-augmented GQA represents one of the best opportunities for
    taking advantage of the outdated information retrieval systems in use today.
  prefs: []
  type: TYPE_NORMAL
- en: '[1] E. Griffith, C. Metz, [A New Area of A.I. Booms, Even Amid the Tech Gloom](https://www.nytimes.com/2023/01/07/technology/generative-ai-chatgpt-investments.html)
    (2023), NYTimes'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] G. Linden, B. Smith, J. York, [Amazon.com Recommendations: Item-to-Item
    Collaborative Filtering](https://www.cs.umd.edu/~samir/498/Amazon-Recommendations.pdf)
    (2003), IEEE'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] T. Warren, [Microsoft to challenge Google by integrating ChatGPT with Bing
    search](https://www.theverge.com/2023/1/4/23538552/microsoft-bing-chatgpt-search-google-competition)
    (2023), The Verge'
  prefs: []
  type: TYPE_NORMAL
- en: '*Originally published at* [*https://www.pinecone.io*](https://www.pinecone.io/learn/openai-gen-qa/)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**All images by the author except where stated otherwise*'
  prefs: []
  type: TYPE_NORMAL
