- en: A comprehensive guide of Distributed Data Parallel (DDP)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-comprehensive-guide-of-distributed-data-parallel-ddp-2bb1d8b5edfb](https://towardsdatascience.com/a-comprehensive-guide-of-distributed-data-parallel-ddp-2bb1d8b5edfb)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A comprehensive guide on how to speed up the training of your models with Distributed
    Data Parallel (DDP)
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@francoisporcher?source=post_page-----2bb1d8b5edfb--------------------------------)[![François
    Porcher](../Images/9ddb233f8cadbd69026bd79e2bd62dea.png)](https://medium.com/@francoisporcher?source=post_page-----2bb1d8b5edfb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2bb1d8b5edfb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2bb1d8b5edfb--------------------------------)
    [François Porcher](https://medium.com/@francoisporcher?source=post_page-----2bb1d8b5edfb--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2bb1d8b5edfb--------------------------------)
    ·12 min read·Oct 30, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f23ad409732c360931dcb34f473cc0a5.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: Image by Author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hi everyone! I am Francois, Research Scientist at Meta. Welcome to this new
    tutorial part of the series [Awesome AI Tutorials](https://github.com/FrancoisPorcher/awesome-ai-tutorials).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial we are going to demistify a well known technique called DDP
    to train models on several GPUs at the same time.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: During my days at engineering school, I recall leveraging Google Colab’s GPUs
    for training. However, in the corporate realm, the landscape is different. If
    you’re part of an organization that’s heavily invested in AI — particularly if
    you’re within a tech giant — you likely have a wealth of GPU clusters at your
    disposal.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: his session aims to equip you with the knowledge to harness the power of multiple
    GPUs, enabling swift and efficient training. And guess what? It’s simpler than
    you might think! Before we proceed, I recommend having a good grasp of PyTorch,
    including its core components like Datasets, DataLoaders, Optimizers, CUDA, and
    the training loop.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Initially, I viewed DDP as a complex, nearly unattainable tool, thinking it
    would require a large team to set up the necessary infrastructure. However, I
    assure you, DDP is not only intuitive but also concise, requiring just a handful
    of code lines to implement. Let’s embark on this enlightening journey together!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: A high level intuition of DDP
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Distributed Data Parallel (DDP) is a straightforward concept once we break it
    down. Imagine you have a cluster with 4 GPUs at your disposal. With DDP, the same
    model is loaded onto each GPU, optimizer included. The primary differentiation
    arises in how we distribute the data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/df270a6c028c05c686bd91fc03d3beb9.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
- en: DDP, Image taken from PyTorch [tutorial](https://www.youtube.com/watch?v=Cvdhwx-OBBo)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: DDP，图像来自PyTorch [教程](https://www.youtube.com/watch?v=Cvdhwx-OBBo)
- en: If you’re acquainted with deep learning, you’ll recall the DataLoader, a tool
    that segments your dataset into distinct batches. The norm is to fragment the
    entire dataset into these batches, updating the model post each batch’s computation.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对深度学习有所了解，你会记得DataLoader，这是一个将你的数据集分割成不同批次的工具。通常会将整个数据集分割成这些批次，在每个批次计算后更新模型。
- en: Zooming in further, DDP refines this process by dividing each batch into what
    we can term as “sub-batches.” Essentially, every model replica processes a segment
    of the primary batch, resulting in a distinct gradient computation for each GPU.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 更深入地看，DDP通过将每个批次划分为我们可以称之为“子批次”的部分来改进这个过程。本质上，每个模型副本处理主批次的一部分，从而为每个GPU生成不同的梯度计算。
- en: 'In DDP we split this batch into sub-batches through a tool called a **DistributedSampler**,
    as illustrated on the following drawing:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在DDP中，我们通过一个叫做**DistributedSampler**的工具将批次拆分为子批次，如下图所示：
- en: '![](../Images/b244a5b37e86fa4c59bccdb3784160e4.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b244a5b37e86fa4c59bccdb3784160e4.png)'
- en: DDP, Image taken from PyTorch [tutorial](https://www.youtube.com/watch?v=Cvdhwx-OBBo)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: DDP，图像来自PyTorch [教程](https://www.youtube.com/watch?v=Cvdhwx-OBBo)
- en: Upon the distribution of each sub-batch to individual GPUs, every GPU computes
    its unique gradient.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个子批次分配到各个GPU之后，每个GPU计算其独特的梯度。
- en: '![](../Images/d2da867016955dafee34c084749757e5.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d2da867016955dafee34c084749757e5.png)'
- en: DDP, Image taken from PyTorch [tutorial](https://www.youtube.com/watch?v=Cvdhwx-OBBo)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: DDP，图像来自PyTorch [教程](https://www.youtube.com/watch?v=Cvdhwx-OBBo)
- en: Now comes the DDP magic. Before updating the model parameters, the gradients
    calculated on each GPU need to be aggregated so that every GPU has the average
    gradient computed over the entire batch of data.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在进入DDP的神奇部分。在更新模型参数之前，需要汇总在每个GPU上计算的梯度，以便每个GPU都有整个数据批次上计算的平均梯度。
- en: This is done by taking the gradients from all GPUs and averaging them. For instance,
    if you have 4 GPUs, the average gradient for a particular model parameter is the
    sum of the gradients for that parameter on each of the 4 GPUs divided by 4.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是通过从所有GPU获取梯度并进行平均来完成的。例如，如果你有4个GPU，那么某个特定模型参数的平均梯度就是该参数在4个GPU上的梯度之和除以4。
- en: DDP uses the `NCCL` or `Gloo` backend (NCCL is optimized for NVIDIA GPUs, Gloo
    is more general) to efficiently communicate and average gradients across GPUs.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DDP使用`NCCL`或`Gloo`后端（NCCL针对NVIDIA GPU进行了优化，Gloo则更为通用）来有效地在GPU之间通信和平均梯度。
- en: '![](../Images/b92be02a61556de95b8bf21f88d955e5.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b92be02a61556de95b8bf21f88d955e5.png)'
- en: DDP, Image taken from PyTorch [tutorial](https://www.youtube.com/watch?v=Cvdhwx-OBBo)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: DDP，图像来自PyTorch [教程](https://www.youtube.com/watch?v=Cvdhwx-OBBo)
- en: Glossary on terms, nodes and ranks
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 术语、节点和排名词汇表
- en: 'Before diving into the code, it’s crucial to understand the vocabulary we’ll
    be using frequently. Let’s demystify these terms:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入代码之前，理解我们将频繁使用的词汇至关重要。让我们揭开这些术语的神秘面纱：
- en: '`Node`: Think of a node as a powerful machine equipped with multiple GPUs.
    When we speak of a cluster, it’s not just a bunch of GPUs thrown together. Instead,
    they’re **organized into groups or “nodes.”** For instance, a node might house
    8 GPUs.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`节点:` 可以把节点想象成一台配备有多个GPU的强大机器。当我们谈论集群时，它不仅仅是一堆GPU堆在一起。相反，它们**被组织成组或“节点”**。例如，一个节点可能包含8个GPU。'
- en: '`Master Node`: In a multi-node environment, one node typically takes charge.
    This “master node” handles tasks like synchronization, initiating model copies,
    overseeing model loading, and managing log entries. Without a master node, each
    GPU would independently generate logs, leading to chaos.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`主节点:` 在多节点环境中，通常有一个节点负责主控。这个“主节点”处理诸如同步、启动模型副本、监督模型加载和管理日志条目等任务。如果没有主节点，每个GPU都会独立生成日志，导致混乱。'
- en: '`Local Rank:` The term “rank” can be likened to an ID or a position. The local
    rank refers to the position or ID of a GPU within its specific node (or machine).
    It’s “local” because it’s confined to that particular machine.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`本地排名:` “排名”这个术语可以类比为一个ID或位置。本地排名指的是GPU在其特定节点（或机器）中的位置或ID。它是“本地的”，因为它仅限于那台机器。'
- en: '`Global Rank:` Taking a broader perspective, the global rank identifies a GPU
    across all available nodes. It’s a **unique identifier irrespective of the machine.**'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`全球排名:` 从更广泛的角度看，全球排名识别所有可用节点中的GPU。这是一个**无论机器如何都唯一的标识符**。'
- en: '`World Size:` At its core, this is a count of all **GPUs available to you across
    all nodes**. Simply, it’s the product of the number of nodes and the number of
    GPUs in each node.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`World Size:` 从根本上说，这是所有**在所有节点上可用的 GPU 数量**的计数。简单来说，这是节点数和每个节点中 GPU 数量的乘积。'
- en: To put things into perspective, if you’re working with just one machine, things
    are more straightforward as the local rank equates to the global rank.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 从实际情况来看，如果你只使用一台机器，那么事情会更加简单，因为本地排名等于全局排名。
- en: 'To clarify this with an image:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 用图像来澄清这个问题：
- en: '![](../Images/087ec6f16a70c080667481b3736612cf.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/087ec6f16a70c080667481b3736612cf.png)'
- en: Local rank, image from [tutorial](https://www.youtube.com/watch?v=KaAJtI1T2x4)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 本地排名，图像来自 [教程](https://www.youtube.com/watch?v=KaAJtI1T2x4)
- en: '![](../Images/1214b8afb108f666b714925e206e9ee5.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1214b8afb108f666b714925e206e9ee5.png)'
- en: Local rank, image from [tutorial](https://www.youtube.com/watch?v=KaAJtI1T2x4)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 本地排名，图像来自 [教程](https://www.youtube.com/watch?v=KaAJtI1T2x4)
- en: 'Understanding DDP Limitations:'
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 DDP 限制：
- en: Distributed Data Parallel (DDP) has been transformative in many deep learning
    workflows, but it’s essential to understand its boundaries.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式数据并行（DDP）在许多深度学习工作流中具有变革性，但理解其局限性非常重要。
- en: The crux of DDP’s limitation lies in its **memory consumption**. With DDP, **each
    GPU loads a replica of the model, the optimizer, and its respective batch of data.**
    GPU memories typically range from a few GB to 80GB for the high end GPUs.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: DDP 限制的核心在于其**内存消耗**。使用 DDP 时，**每个 GPU 加载一个模型副本、优化器及其相应的数据批次。** GPU 内存通常从几 GB
    到高端 GPU 的 80GB 不等。
- en: For smaller models, this isn’t an issue. However, when venturing into the realm
    of Large Language Models (LLMs) or architectures akin to GPT, the confines of
    a single GPU’s memory might be inadequate.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于较小的模型，这不是问题。然而，当涉足大型语言模型（LLMs）或类似 GPT 的架构时，单个 GPU 内存的限制可能是不够的。
- en: In Computer Vision, while there’s a plethora of lightweight models, challenges
    arise when **increasing batch sizes**, especially in scenarios involving **3D
    imagery** or Object Detection tasks.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉中，虽然有很多轻量级模型，但**增加批量大小**时会遇到挑战，特别是在涉及**3D 图像**或目标检测任务的场景中。
- en: Enter Fully Sharded Data Parallel (FSDP). This method extends the benefits of
    DDP by not only distributing data but also dispersing the model and optimizer
    states across GPU memories. While this sounds advantageous, FSDP increases inter-GPU
    communication, potentially slowing down training.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 进入完全分片数据并行（FSDP）。这种方法通过不仅分布数据，还将模型和优化器状态分散到 GPU 内存中，从而扩展了 DDP 的好处。虽然这听起来很有优势，但
    FSDP 会增加 GPU 之间的通信，可能会导致训练速度变慢。
- en: 'In Summary:'
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结：
- en: If your model and its corresponding batch comfortably fit within a GPU’s memory,
    DDP is your best bet owing to its speed.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的模型及其对应的批量数据能够舒适地适应 GPU 的内存，DDP 是你最好的选择，因为它速度较快。
- en: 'For mammoth-sized models demanding more memory, FSDP is a more fitting choice.
    However, bear in mind its trade-off: you’re sacrificing speed for memory.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于需要更多内存的巨型模型，FSDP 是更合适的选择。然而，请记住其权衡：你是在为内存牺牲速度。
- en: Why you should prefer DDP over DP?
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么你应该更倾向于使用 DDP 而不是 DP？
- en: 'If you go on PyTorch’s website, there are actually options: DP and DDP. But
    I only mention this so you don’t get lost or confused: **Just use DDP, it’s faster
    and not limited to a single node.**'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你访问 PyTorch 的网站，实际上有两种选择：DP 和 DDP。但我提到这一点只是为了让你不要迷路或困惑：**只使用 DDP，它更快且不限于单个节点。**
- en: '![](../Images/a0813aaad92748692ec721c7dc57ca23.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a0813aaad92748692ec721c7dc57ca23.png)'
- en: Comparison from Pytorch [tutorial](https://pytorch.org/tutorials/beginner/ddp_series_theory.html)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 PyTorch 的比较 [教程](https://pytorch.org/tutorials/beginner/ddp_series_theory.html)
- en: 'Code Walkthrough:'
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码演示：
- en: Implementing distributed deep learning is simpler than you might think. The
    beauty lies in the fact that you won’t be bogged down with manual GPU configurations
    or the intricacies of gradient distribution.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 实现分布式深度学习比你想象的要简单。美在于你不会被手动配置 GPU 或梯度分布的复杂性所困扰。
- en: 'You will find all the template and script on:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下位置找到所有模板和脚本：
- en: '[](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----2bb1d8b5edfb--------------------------------)
    [## GitHub - FrancoisPorcher/awesome-ai-tutorials: The best collection of AI tutorials
    to make you a…'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----2bb1d8b5edfb--------------------------------)
    [## GitHub - FrancoisPorcher/awesome-ai-tutorials: 最佳 AI 教程集合，让你成为数据科学的…'
- en: The best collection of AI tutorials to make you a boss of Data Science! - GitHub
    …
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最佳 AI 教程集合，让你成为数据科学的高手！ - GitHub …
- en: github.com](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----2bb1d8b5edfb--------------------------------)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----2bb1d8b5edfb--------------------------------)'
- en: 'Here’s a breakdown of the steps we’ll be taking:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们将采取的步骤的细分：
- en: 'Process Initialization: This involves designating the master node, specifying
    the port, and setting up the `world_size`.'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进程初始化：这涉及到指定主节点、指定端口，并设置`world_size`。
- en: 'Distributed DataLoader Setup: Crucial to this step is the partitioning of each
    batch across the available GPUs. We’ll ensure that the data is evenly spread without
    any overlap.'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分布式DataLoader设置：这个步骤至关重要的是将每个批次在可用GPU之间进行分区。我们将确保数据均匀分布而没有任何重叠。
- en: 'Model Training/Testing: In essence, this step remains largely unchanged from
    the single GPU process.'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型训练/测试：本质上，这一步与单GPU过程几乎没有变化。
- en: Training on 1 GPU 1 Node (baseline)
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在1 GPU 1节点上训练（基准）
- en: 'First let’s define a vanilla code that loads a dataset, create a model and
    train it end to end on a single GPU. This will be our starting point:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义一个简单的代码，它加载一个数据集，创建一个模型，并在单个GPU上进行端到端训练。这将是我们的起点：
- en: '[PRE0]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Training on several GPUs, 1 Node
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在多个GPU，1节点上训练
- en: 'Now we are going to use all the GPUs in a single node with the following steps:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用单节点上的所有GPU，步骤如下：
- en: Import Necessary Libraries for distributed training.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入进行分布式训练所需的库。
- en: 'Initialize the Distributed Environment: (especially the `MASTER_ADDR` and `MASTER_PORT`'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化分布式环境：（特别是`MASTER_ADDR`和`MASTER_PORT`）
- en: Wrap Model with DDP using the `DistributedDataParallel` wrapper.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`DistributedDataParallel`包装器包装模型。
- en: Use Distributed Sampler to ensure that the dataset is divided across the GPUs
    in a distributed manner.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用分布式采样器确保数据集在GPU之间以分布式的方式划分。
- en: Adjust the Main Function to `spawn` multiple processes for multi-GPU training.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整主函数以`spawn`多个进程进行多GPU训练。
- en: 'For the libraries, we need this:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于库，我们需要这些：
- en: '[PRE1]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then we need to setup each process. For example if we have 8 GPUs on 1 Node,
    we will call the following functions 8 times, one for each GPU and with the right
    `local_rank`:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们需要设置每个进程。例如，如果我们在1节点上有8个GPU，我们将调用以下函数8次，每次针对一个GPU，并设置正确的`local_rank`：
- en: '[PRE2]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'A few explanations on the function:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对该函数的一些解释：
- en: '`MASTER_ADDR` is the hostname of the machine on whith the master (or the rank
    0 process) is running. Here it’s localhost'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MASTER_ADDR`是主机（或rank 0进程）运行的机器的主机名。在这里是localhost'
- en: '`MASTER_PORT`: Specifies the port on which the master is listening for connections
    from workers or other processes. 12355 is arbitrary. You can choose any unused
    port number as long as it’s not being used by another service on your system and
    is allowed by your firewall rules.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MASTER_PORT`：指定主节点监听来自工作进程或其他进程的连接的端口。12355是任意的。你可以选择任何未被占用的端口号，只要它不被系统上的其他服务使用，并且被你的防火墙规则允许。'
- en: '`torch.cuda.set_device(rank)`: This ensure that each process uses its corresponding
    GPU'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.cuda.set_device(rank)`：这确保每个进程使用其对应的GPU。'
- en: 'Then we need to slightly change the Trainer class. We are simply going to wrap
    the model with the DDP function:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们需要稍微修改Trainer类。我们将简单地用DDP函数包装模型：
- en: '[PRE3]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The rest of the Trainer class is the same, amazing!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Trainer 类的其他部分保持不变，真是太棒了！
- en: 'Now we have to change the dataloader, because remember, we have to split the
    batch on each GPU:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们必须更改dataloader，因为记住，我们必须在每个GPU上拆分批次：
- en: '[PRE4]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now we can modify the `main` function, that will be called for each process
    (so 8 times in our case):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以修改`main`函数，这个函数会为每个进程调用（所以在我们的例子中是8次）：
- en: '[PRE5]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'And finally, when executing the script, we will have to launch the 8 processes.
    This is done with the `mp.spawn()` function:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在执行脚本时，我们需要启动8个进程。这是通过`mp.spawn()`函数完成的：
- en: '[PRE6]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Ultimate Step: Training on several nodes'
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 终极步骤：在多个节点上训练
- en: If you have made it so far congratulations! The Ultimate step is be able to
    recruit all the GPUs available on different nodes. But if you understood what
    we have done so far, this is very easy.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经做到这些，恭喜你！终极步骤是能够招募所有在不同节点上的GPU。但是如果你理解了我们迄今为止所做的，这将非常简单。
- en: The key distinction when scaling across multiple nodes is the shift from `local_rank`
    to `global_rank`. This is imperative because every process requires a unique identifier.
    For instance, if you’re working with two nodes, each with 8 GPUs, both processes
    0 and 8 would have a `local_rank` of 0.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在跨多个节点扩展时，关键的区别在于从`local_rank`到`global_rank`的转换。这一点非常重要，因为每个进程都需要一个唯一的标识符。例如，如果你在两个节点上工作，每个节点有
    8 个 GPU，那么进程 0 和 8 都会有一个`local_rank`为 0。
- en: 'The global_rank is given by the very intuitive formula:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: global_rank 由非常直观的公式给出：
- en: global_rank = node_rank * world_size_per_node + local_rank
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: global_rank = node_rank * world_size_per_node + local_rank
- en: 'So first let’s modify the `ddp_setup` function:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 所以首先让我们修改`ddp_setup`函数：
- en: '[PRE7]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'And we have to adjust the main function which now takes the `wold_size_per_node`
    in argument:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要调整主函数，它现在接受`world_size_per_node`作为参数：
- en: '[PRE8]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'And finally we adjust the `mp.spawn()` function with the `world_size_per_node`
    as well:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们也要调整`mp.spawn()`函数以适应`world_size_per_node`：
- en: '[PRE9]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Using a cluster (SLURM)
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用集群（SLURM）
- en: You are now ready to send the training to the cluster. I’ts very simple you
    just have to call the number of nodes you want.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以准备将训练发送到集群。这非常简单，你只需调用所需的节点数量即可。
- en: 'Here is a template for the SLURM script:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 SLURM 脚本的模板：
- en: '[PRE10]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: And now you can launch training from the terminal with the command
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以使用以下命令从终端启动训练
- en: '[PRE11]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Congratulations, you’ve made it!
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜，你完成了！
- en: 'Thanks for reading! Before you go:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！在你离开之前：
- en: For more awesome tutorials, check my [compilation of AI tutorials](https://github.com/FrancoisPorcher/awesome-ai-tutorials)
    on Github
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 欲获取更多精彩教程，请查看我在 Github 上的 [AI 教程汇编](https://github.com/FrancoisPorcher/awesome-ai-tutorials)
- en: '[](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----2bb1d8b5edfb--------------------------------)
    [## GitHub - FrancoisPorcher/awesome-ai-tutorials: The best collection of AI tutorials
    to make you a…'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----2bb1d8b5edfb--------------------------------)
    [## GitHub - FrancoisPorcher/awesome-ai-tutorials: 最佳的 AI 教程集合，让你成为……'
- en: The best collection of AI tutorials to make you a boss of Data Science! - GitHub
    …
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最佳的 AI 教程集合，让你成为数据科学的高手！ - GitHub …
- en: github.com](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----2bb1d8b5edfb--------------------------------)
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----2bb1d8b5edfb--------------------------------)
- en: Y*ou should get my articles in your inbox.* [***Subscribe here.***](https://medium.com/@francoisporcher/subscribe)
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '*你应该在收件箱中获取我的文章。* [***在这里订阅。***](https://medium.com/@francoisporcher/subscribe)'
- en: '*If you want to have access to premium articles on Medium, you only need a
    membership for $5 a month. If you sign up* [***with my link***](https://medium.com/@francoisporcher/membership)*,
    you support me with a part of your fee without additional costs.*'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你想访问 Medium 上的高级文章，只需每月支付 $5 的会员费用。如果你通过* [***我的链接***](https://medium.com/@francoisporcher/membership)*注册，你将以额外的费用支持我。*'
- en: If you found this article insightful and beneficial, please consider following
    me and leaving a clap for more in-depth content! Your support helps me continue
    producing content that aids our collective understanding.
  id: totrans-119
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你觉得这篇文章有洞察力并且有帮助，请考虑关注我并点赞以获取更多深入内容！你的支持帮助我继续制作有助于我们共同理解的内容。
- en: References
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料
- en: '[PyTorch guide on DDP](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PyTorch DDP 指南](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)'
- en: '[Tutorial Series](https://www.youtube.com/watch?v=-K3bZYHYHEA&list=PL_lsbAsL_o2CSuhUhJIiW0IkdT5C2wGWj)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[教程系列](https://www.youtube.com/watch?v=-K3bZYHYHEA&list=PL_lsbAsL_o2CSuhUhJIiW0IkdT5C2wGWj)'
