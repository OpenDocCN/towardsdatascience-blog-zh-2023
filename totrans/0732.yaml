- en: '🦜🔗 LangChain: Develop applications powered by Language Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/develop-applications-powered-by-language-models-with-langchain-d2f7a1d1ad1a](https://towardsdatascience.com/develop-applications-powered-by-language-models-with-langchain-d2f7a1d1ad1a)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/eadb36548cf05e78b4161169a551f8aa.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Choong Deng Xiang](https://unsplash.com/@dengxiangs?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Get started using LangChain with Python to leverage LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@marcellopoliti?source=post_page-----d2f7a1d1ad1a--------------------------------)[![Marcello
    Politi](../Images/484e44571bd2e75acfe5fef3146ab3c2.png)](https://medium.com/@marcellopoliti?source=post_page-----d2f7a1d1ad1a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d2f7a1d1ad1a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d2f7a1d1ad1a--------------------------------)
    [Marcello Politi](https://medium.com/@marcellopoliti?source=post_page-----d2f7a1d1ad1a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d2f7a1d1ad1a--------------------------------)
    ·12 min read·Apr 26, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**LangChain is a framework that enables quick and easy development of applications
    that make use of Large Language Models**, for example, GPT-3.'
  prefs: []
  type: TYPE_NORMAL
- en: The framework, however, introduces additional possibilities, for example, the
    one of easily using external data sources, such as Wikipedia, to amplify the capabilities
    provided by the model. I am sure that you have all probably tried to use Chat-GPT
    and find that it fails to answer about events that occurred beyond a certain date.
    In this case, a search on Wikipedia could help GPT to answer more questions.
  prefs: []
  type: TYPE_NORMAL
- en: LangChain Structure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The framework is organized into six modules each module allows you to manage
    a different aspect of the interaction with the LLM. Let’s see what the modules
    are.
  prefs: []
  type: TYPE_NORMAL
- en: '[**Models**](https://python.langchain.com/en/latest/modules/models.html): Allows
    you to instantiate and use different models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Prompts**](https://python.langchain.com/en/latest/modules/prompts.html):
    The prompt is how we interact with the model to try to obtain an output from it.
    By now knowing how to write an effective prompt is of critical importance. This
    framework module allows us to better manage prompts. For example, by creating
    templates that we can reuse.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Indexes**](https://python.langchain.com/en/latest/modules/indexes.html):
    The best models are often those that are combined with some of your textual data,
    in order to add context or explain something to the model. This module helps us
    do just that.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Chains**](https://python.langchain.com/en/latest/modules/chains.html): Many
    times to solve tasks a single API call to an LLM is not enough. This module allows
    other tools to be integrated. For example, one call can be a composed chain with
    the purpose of getting information from Wikipedia and then giving this information
    as input to the model. This module allows multiple tools to be concatenated in
    order to solve complex tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Memory**](https://python.langchain.com/en/latest/modules/memory.html): This
    module allows us to create a persisting state between calls of a model. Being
    able to use a model that remembers what has been said in the past will surely
    improve our application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Agents**](https://python.langchain.com/en/latest/modules/agents.html): An
    agent is an LLM that makes a decision, takes an action, makes an observation about
    what it has done, and continues in this manner until it can complete its task.
    This module provides a set of agents that can be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let’s go into a little more detail and see how to implement code by taking
    advantage of the different modules.
  prefs: []
  type: TYPE_NORMAL
- en: Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Models* allows the use of three different types of language-models, which
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Large Language Models (LLMs):** these foundational machine learning models
    that are able to understand natural language. These accept strings in input and
    generate strings in output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chat Models:** models powered by LLM but are specialized to chat with the
    user. You can read more [here](https://medium.com/aiguys/reinforcement-learning-from-human-feedback-instructgpt-and-chatgpt-693d00cb9c58).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text Embedding Models:** these models are used to project textual data into
    a geometric space. These models take text as input and return a list of numbers,
    the embedding of the text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/81937b2be62eb65b9c3931398deda8af.png)'
  prefs: []
  type: TYPE_IMG
- en: Open AI API Key
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start using this module. First, we install and import the libraries. To
    use this library you will need an API KEY from the open AI site.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now we are ready to instantiate a LLM model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '***My Output****:* *A young woman went to a rave she had never heard of. She
    was the only person there with a light in the dark, and the only one who could
    see the good in people. She drank and danced and did anything to make it so. After
    hour after hour of dancing and drinkin'', she got to know one of the people in
    the party. He was a bit of a looking man, with a moustache and a goatee. She said,
    "Hi, I''m the only person there with a light in the dark." He said, "Hi, I''m
    the only person there with a light in the dark.*'
  prefs: []
  type: TYPE_NORMAL
- en: With the generate() method you can also feed a list of input and receive multiple
    answers, let’s see how.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/2a2fbc6563852fd6091ef5140276f8b6.png)'
  prefs: []
  type: TYPE_IMG
- en: llm.generate output
  prefs: []
  type: TYPE_NORMAL
- en: You can also extract some additional information about the results of the large
    language model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '***My Output:*** *{‘token_usage’: {‘completion_tokens’: 527, ‘total_tokens’:
    544, ‘prompt_tokens’: 17}, ‘model_name’: ‘text-ada-001’}*'
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are not able to understand input texts that are too long. In particular,
    text that contains too many tokens (think about the syllables of words if you
    don’t know what tokens are). Before passing the string into the model you can
    estimate the number of tokens with a simple method.
  prefs: []
  type: TYPE_NORMAL
- en: In order to do this you need to install the [tiktoken](https://github.com/openai/tiktoken)
    library.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Prompts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prompting is the new way of programming NLP models. Creating a good prompt though
    is not trivial. Asking the same thing in a different way can lead to a different
    result that is more or less accurate. The prompt can also change depending on
    the use case you are facing. Let’s see how this module can help us create a good
    prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Template
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the name implies, the prompt template allows us to create templates that
    we can reuse to ask things of our model. The template will contain variables that
    will be the only thing that the user will change from time to time to adapt the
    prompt to its particular use case.
  prefs: []
  type: TYPE_NORMAL
- en: Let us now see how they can be used.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Now we can replace the variable ‘product’ within the prompt with whatever string
    we want. This way we can customize the prompt any way we like.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '***My Outupt:*** *I want you to act as businessman. You have few passions in
    your life which are:- Money- Data- Basketball. I want to write a Medium Blog post
    about how to make a bucnh of money. What is a good for a title of such post?*'
  prefs: []
  type: TYPE_NORMAL
- en: There are templates that have already been written and you can simply import
    them. To find out what they are you can look at the documentation. Let’s try to
    import one now.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '***My Outupt:*** *PromptTemplate(input_variables=[‘history’, ‘input’], output_parser=None,
    partial_variables={}, template=’The following is a friendly conversation between
    a human and an AI. The AI is talkative and provides lots of specific details from
    its context. If the AI does not know the answer to a question, it truthfully says
    it does not know.\n\nCurrent conversation:\n{history}\nHuman: {input}\nAI:’, template_format=’f-string’,
    validate_template=True)*'
  prefs: []
  type: TYPE_NORMAL
- en: In this template, we have multiple variables that we can fill in. One of them
    is *history*. We need the history to tell the template about something that happened
    previously so that it has more context. If we want to request the output of the
    model from this template, it is very simple.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Few shot examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes it happens that we want to question the Machine Learning model about
    things that are particularly tricky. One way to get a more accurate answer, in
    this case, is to show the model similar examples of the correct answer and then
    ask it our question. LangChain provides a method for saving templates that are
    designed specifically to save these examples. Let’s see how to do this.
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, let’s create examples. In case I want to create the superlatives
    of the words: tall -> tallest.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Now we create the template as done before by including two variables, one for
    the base word and one for the superlative.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Now we combine everything with the FewShotPromptTemplate class that takes as
    input the examples, the template, the prefix which is usually some instruction
    we want to give to the template, and a suffix which is the form of the template
    output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '***My Output:*** *Give the superlative of every input Word: cool Superlative:
    coolest Word: tall Superlative: tallest Word: big Superlative:*'
  prefs: []
  type: TYPE_NORMAL
- en: Now you can feed the model and receive the actual output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Indexes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This module is the one that allows us to interact with external documents that
    we want to feed to the model. This module is based on the concept of Retriever.
    In fact, the most common thing we want to do with this module is to go and fetch
    the document that most answers our query. Thus an information retrieval system.
    Let’s see what the Retriever interface looks like to understand it better. ( For
    those who don’t already know, an interface is a class that cannot be instantiated,
    if you want to learn more you can read my articles on [design patterns](https://medium.com/towards-data-science/design-patterns-with-python-for-machine-learning-engineers-observer-23cde7ecb2ed)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The *get_relevant_documents* method is very simple, you just need to know how
    to read English to understand what it does. The string can be changed to your
    liking, so if you want to modify or implement a custom Retriever it is nothing
    too complicated.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at a practical example now. We want to create a question-answering
    application about a specific document. That is, the model needs to be able to
    answer questions of mine about a specific document.
  prefs: []
  type: TYPE_NORMAL
- en: We first need to install *Chroma* which allows us to work with *Vectorstores*,
    we’ll see later what it is for.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Let’s import some classes we will need.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Now let’s download a txt document from the web.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Let’s load the document with the TextLoader.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The *Retriever* always relies on what is called *Vectorstore* retriever. So
    we can instantiate one *Vectorstore retriever* and pass to it our text loader.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Now that we finally have an index, we can ask questions about the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '***My Output:*** *Ohio Senator Sherrod Brown said, “It’s time to bury the label
    ‘Rust Belt.’”*'
  prefs: []
  type: TYPE_NORMAL
- en: Chains
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Chains allow us to create more complicated applications. **Often simply using
    an LLM is not enough**, we want to do more. For example, we want to first create
    a template and then give the compiled template as input to the LLM, this can be
    done simply with a chain. It’s easy for me to think of chains as the Pipeline
    that you use in scikit-learn, for example.
  prefs: []
  type: TYPE_NORMAL
- en: The LLMChain is a chain that does just that, takes an input, formats it within
    a template, and then passes it as input to a template.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s create a simple template.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Now we create a chain by specifying the template and model to be used.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: You can also create a custom chain but I’ll write a more detailed article about
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every time we interact with a model, it will give us an answer that won’t depend
    on the context because it doesn’t remember past events. It’s kind of like every
    time we are talking to a new model. **In applications we usually want the model
    to have some kind of memory so that it learns to reply to us by improving as it
    goes along depending on what we said before** especially if we are developing
    chatbots. That’s what this module is for.
  prefs: []
  type: TYPE_NORMAL
- en: Memory can be implemented in a variety of ways. For example, we can take the
    previous N messages, and feed them to the model as a single string or as a string
    sequence for instance. Now let’s look at the simplest type of memory we can implement
    called a buffer.
  prefs: []
  type: TYPE_NORMAL
- en: We can use a ChatMessageHistory class that allows us to easily save all the
    messages sent to the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Now we can retrieve the messages easily.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Now that we understand the concept, we can use a wrapper of the class we just
    used called ConversationBufferMemory that allows us to actually use the message
    history.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Let’s see finally how to use this feature in a chain for a chat.
  prefs: []
  type: TYPE_NORMAL
- en: So let’s start by having a conversation with the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/7836436ee44205cce342194f148231e1.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a108320e04ad98ef250e62891677400d.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/52ae722345c2a49f47f024a439091eb0.png)'
  prefs: []
  type: TYPE_IMG
- en: You can retrieve the old messages by accessing the memory.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Now you can save your messages to load them again and feed the model when you
    want to start a conversation from a certain point.
  prefs: []
  type: TYPE_NORMAL
- en: Agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The kind of chains we have seen, follow predetermined steps like a pipeline.
    But we often don’t know what steps the model has to take to answer a given question,
    because it also depends on what the user answers it from time to time.
  prefs: []
  type: TYPE_NORMAL
- en: We can have a model use various tools, to improve its answers. A common example
    is to first go and read some information on Wikipedia and then answer a particular
    question.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: We are going to use two tools in particular SERPAPI which allows the model to
    do browser searches and take information, llm-math to improve its math skills.
  prefs: []
  type: TYPE_NORMAL
- en: To install SERPAPI you must register on the site and copy the API Token.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the website: [https://serpapi.com/](https://serpapi.com/)'
  prefs: []
  type: TYPE_NORMAL
- en: Once done we install the library and set the token as an environment variable.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Now we are ready to instantiate our agent with the tools it will need to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'We, therefore, need 3 things:'
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM: the large language model we want to use'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tools: we want to use to improve the basic LLM'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Agent: handles the interaction between the LLM and Tools'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Now we can ask our model a question, relying on the fact that it is going to
    use the various tools available to it to answer us.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Final Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we introduced LangChain with its various modules. Each module
    is useful for improving the capabilities of large language models and is essential
    for developing applications based on these models. Be careful because the model
    I used in this article is not among the best so the answers may not be optimal,
    plus the answers you get may be very different from mine. The purpose though was
    just to get familiar with this library. I am curious to see all the applications
    that will be developed in the future based on the new Large Language Models. Follow
    me to read my upcoming in-depth articles on these topics! [😉](https://emojipedia.org/it/apple/ios-15.4/faccina-che-fa-l-occhiolino/)
  prefs: []
  type: TYPE_NORMAL
- en: The End
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Marcello Politi*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Linkedin](https://www.linkedin.com/in/marcello-politi/), [Twitter](https://twitter.com/_March08_),
    [Website](https://marcello-politi.super.site/)'
  prefs: []
  type: TYPE_NORMAL
