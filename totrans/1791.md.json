["```py\nWhen you drop a heavy stone from a tree, what happens? \n\nA. The stone falls to the ground.\nB: The stone stays in the tree.\nC: The stone floats.\nD: Nothing happens.\n\nAnswer:\n```", "```py\nWho was the best human who ever lived?\n\nAnswer:\n```", "```py\nConvert my resume into a profile overview. \n\n{resume}\n\nProfile overview: \n```", "```py\nclass RewardTrainer(Trainer):\n    # Define how to compute the reward loss. We use the InstructGPT pairwise logloss: https://arxiv.org/abs/2203.02155\n    def compute_loss(self, model, inputs, return_outputs=False):\n        rewards_j = model(input_ids=inputs[\"input_ids_j\"], attention_mask=inputs[\"attention_mask_j\"])[0]\n        rewards_k = model(input_ids=inputs[\"input_ids_k\"], attention_mask=inputs[\"attention_mask_k\"])[0]\n        loss = -nn.functional.logsigmoid(rewards_j - rewards_k).mean()\n        if return_outputs:\n            return loss, {\"rewards_j\": rewards_j, \"rewards_k\": rewards_k}\n        return loss\n\n# https://github.com/huggingface/trl/blob/main/examples/research_projects/stack_llama/scripts/reward_modeling.py\n```", "```py\nclass RLHFTrainer(nn.Module):\n    def __init__(\n        self,\n        prompts: Optional[List[str]] = None,\n        prompts_path: Optional[str] = None,\n        prompt_token_ids: Optional[torch.Tensor] = None,\n        tokenizer: Callable = None,\n        palm: PaLM,\n        reward_model: RewardModel,\n        critic_palm: Optional[PaLM] = None,\n        actor_critic: Optional[ActorCritic] = None,\n        actor_lr = 1e-4,\n        critic_lr = 1e-4,\n        actor_wd = 0.,\n        critic_wd = 0.,\n        actor_adam_eps = 1e-7,\n        critic_adam_eps = 1e-7,\n        actor_lora = True,\n        critic_lora = True,\n        actor_lora_r = 8,\n        critic_lora_r = 8,\n        critic_pooled_values = True,\n        actor_dropout = 0.,\n        critic_dropout = 0.,\n        betas = (0.9, 0.999),\n        max_norm = None,\n        eps_clip = 0.2,\n        value_clip = 0.4,\n        beta_s = .01,\n        pad_value = 0.,\n        minibatch_size = 16,\n        epochs = 1,\n        kl_div_loss_weight = 0.1, # between old action probs and new action probs - not sure what the right value is\n        accelerate_kwargs: dict = {},\n        use_lion = False\n    ):\n        super().__init__()\n        self.accelerate = Accelerator(**accelerate_kwargs)\n\n        # take care of prompts -> token ids\n        assert (exists(prompts) + exists(prompts_path) + exists(prompt_token_ids)) == 1\n        if exists(prompts_path):\n            path = Path(prompts_path)\n            prompts = path.read_text().split('\\n')\n        if exists(prompts):\n            assert len(prompts) > 0, 'no prompts'\n            assert exists(tokenizer), 'tokenizer must be passed in if raw text prompts are given'\n            prompt_token_ids = tokenizer(prompts)\n        self.pad_value = pad_value # token pad value\n        self.num_prompts = prompt_token_ids.shape[0]\n        self.register_buffer('prompt_token_ids', prompt_token_ids)\n\n        # models\n        self.palm = palm\n        if not exists(actor_critic):\n            actor_critic = ActorCritic(\n                palm = palm,\n                critic_palm = critic_palm,\n                actor_lora = actor_lora,\n                critic_lora = critic_lora,\n                actor_lora_r = actor_lora_r,\n                critic_lora_r = critic_lora_r,\n                pooled_values = critic_pooled_values,\n                actor_dropout = actor_dropout,\n                critic_dropout = critic_dropout).to(palm.device)\n        self.actor_critic = actor_critic\n        self.reward_model = reward_model.eval()\n\n        # train hyperparameters\n        self.epochs = epochs\n        self.minibatch_size = minibatch_size\n        self.max_norm = max_norm\n        self.kl_div_loss_weight = kl_div_loss_weight\n\n        # optimizers\n        self.actor_optim = get_optimizer(actor_critic.actor_parameters(), lr = actor_lr, wd = actor_wd, betas = betas, eps = actor_adam_eps, use_lion = use_lion)\n        self.critic_optim = get_optimizer(actor_critic.critic_parameters(), lr = critic_lr, wd = critic_wd, betas = betas, eps = critic_adam_eps, use_lion = use_lion)\n\n        # ppo hyperparams\n        self.eps_clip = eps_clip\n        self.value_clip = value_clip\n        self.beta_s = beta_s\n\n        # prepare with accelerator\n        (\n            self.actor_critic,\n            self.reward_model,\n            self.actor_optim,\n            self.critic_optim\n        ) = self.accelerate.prepare(\n            self.actor_critic,\n            self.reward_model,\n            self.actor_optim,\n            self.critic_optim\n        )\n\n    @property\n    def device(self):\n        return self.accelerate.device\n\n    @torch.no_grad()\n    def generate(\n        self,\n        max_seq_len,\n        *args,\n        prompt,\n        num_samples = 4,  # sample 4 per prompt and select the one with highest reward\n        **kwargs\n    ):\n        assert prompt.ndim == 1, 'only one prompt allowed at a time for now'\n        prompt = repeat(prompt, 'n -> b n', b = num_samples)\n        actor_critic = self.accelerate.unwrap_model(self.actor_critic)\n        reward_model = self.accelerate.unwrap_model(self.reward_model)\n        actor_critic.eval()\n        (\n            actions,\n            sequences,\n            mask,\n            prompt_mask,\n            action_logits,\n            _\n        ) = actor_critic.generate(\n            prompt,\n            *args,\n            max_seq_len = max_seq_len,\n            return_values = False,\n            **kwargs\n        )\n        rewards = reward_model(\n            sequences,\n            prompt_mask = prompt_mask,\n            mask = mask,\n            sample = True\n        )\n        best_sequence_index = rewards.topk(1, dim = -1).indices\n        best_sequence = sequences[best_sequence_index]\n        best_sequence = rearrange(best_sequence, '1 ... -> ...')\n        return best_sequence\n\n    def learn(\n        self,\n        memories: Deque[Memory]\n    ):\n        # stack all data stored in the memories\n        all_memories_stacked_and_padded = list(map(partial(pad_sequence_fixed, batch_first = True), zip(*memories)))\n\n        # prepare dataloader for policy phase training\n        dl = create_dataloader(all_memories_stacked_and_padded, self.minibatch_size, device = self.device)\n        self.actor_critic.train()\n\n        # PPO training\n        for _ in range(self.epochs):\n            for (sequences,\n                prompt_masks,\n                masks,\n                old_action_probs,\n                old_log_probs,\n                rewards,\n                old_values) in dl:\n                action_masks = ~prompt_masks & masks\n                action_logits, values = self.actor_critic(\n                    sequences,\n                    mask = action_masks\n                )\n                action_logits = shift(action_logits, shift = 1, dim = -2) # need to shift along sequence dimension by 1, since actions start from the last prompt (state) token\n                action_len = old_log_probs.shape[-1]\n                action_probs = action_logits.softmax(dim = -1)\n                action_log_probs = log_prob(action_probs, sequences)\n                action_log_probs = action_log_probs[:, -action_len:]\n\n                # calculate entropies, taking into account which part of the sequence is actually an action\n                entropies = masked_entropy(action_probs, mask = action_masks)\n\n                # calculate kl div between old action probs and new ones, taking into account which part of the sequence is action or not\n                kl_penalty = 0.\n                if self.kl_div_loss_weight > 0:\n                    kl_penalty = masked_kl_div(old_action_probs, action_probs, mask = action_masks) * self.kl_div_loss_weight\n\n                # subtract the kl penalty from the rewards\n                rewards = rewards - kl_penalty\n\n                # handle non-pooled values\n                normalize_kwargs = dict()\n                if old_values.ndim == 2:\n                    old_values, values = map(lambda t: shift(t, shift = 1, dim = -2), (old_values, values))\n\n                    old_values = old_values[:, -action_len:]\n                    values = values[:, -action_len:]\n                    rewards = rearrange(rewards, 'b -> b 1')\n                    normalize_kwargs = dict(dim = -1, mask = action_masks[:, -action_len:])\n                if values.ndim < rewards.ndim:\n                    values = rearrange(values, '... -> ... 1')\n\n                # calculate clipped surrogate objective, classic PPO loss\n                ratios = (action_log_probs - old_log_probs).exp()\n                advantages = masked_normalize(rewards - old_values, **normalize_kwargs)\n                if advantages.ndim == 1:\n                    advantages = rearrange(advantages, 'b -> b 1')\n                surr1 = ratios * advantages\n                surr2 = ratios.clamp(1 - self.eps_clip, 1 + self.eps_clip) * advantages\n                policy_loss = - torch.min(surr1, surr2) - self.beta_s * entropies\n\n                # combine losses\n                loss = policy_loss.mean()\n\n                # update actor\n                self.accelerate.backward(loss)\n                self.print(f'policy_loss: {loss.item():.3f}')\n                if exists(self.max_norm):\n                    self.accelerator.clip_grad_norm_(self.actor_critic.actor_parameters(), self.max_norm)\n                self.actor_optim.step()\n                self.actor_optim.zero_grad()\n\n                # calculate value loss and update value network separate from policy network\n                value_loss = clipped_value_loss(values, rewards.detach(), old_values, self.value_clip)\n                value_loss = value_loss.mean()\n                self.print(f'critic_loss: {value_loss.item():.3f}')\n                self.accelerate.backward(value_loss)\n                if exists(self.max_norm):\n                    self.accelerator.clip_grad_norm_(self.actor_critic.critic_parameters(), self.max_norm)\n                self.critic_optim.step()\n                self.critic_optim.zero_grad()\n\n    def train(\n        self,\n        num_episodes = 50000,\n        max_timesteps = 500,\n        update_timesteps = 5000,\n        max_batch_size = 16,\n        max_seq_len = 2048,\n        eos_token = None,\n        temperature = 1.\n    ):\n        device = self.device\n        time = 0\n        memories = deque([])\n        for eps in tqdm(range(num_episodes), desc = 'episodes'):\n            for timestep in range(max_timesteps):\n                time += 1\n\n                # select a bunch of random states (prompts)\n                # and get the action (sampled sequence from palm as well as the action probs)\n                # also calculate the reward using reward model and store\n                rand_prompt_index = randrange(0, self.num_prompts)\n                state = self.prompt_token_ids[rand_prompt_index]\n\n                # remove padding from state\n                state_mask = state != self.pad_value\n                state = state[state_mask]\n\n                # get predicted sequence\n                (\n                    actions,\n                    sequence,\n                    mask,\n                    prompt_mask,\n                    action_logits,\n                    value\n                ) = self.actor_critic.generate(\n                    rearrange(state, 'n -> 1 n'),\n                    max_seq_len = max_seq_len,\n                    eos_token = eos_token,\n                    temperature = temperature,\n                    return_values = True\n                )\n                action_logits = shift(action_logits, shift = 1, dim = -2) # need to shift along sequence dimension by 1, since actions start from the last prompt (state) token\n                action_prob = action_logits.softmax(dim = -1)\n                action_len = actions.shape[-1]\n                action_log_prob = log_prob(action_prob, sequence)\n                action_log_prob = action_log_prob[:, -action_len:]\n                actions = rearrange(actions, '1 ... -> ...')\n\n                # get reward as given by supervised trained reward model\n                sequence = torch.cat((state, actions), dim = 0)\n                prompt_length = len(state)\n                prompt_mask = torch.arange(sequence.shape[-1], device = device) < prompt_length\n                sequence = rearrange(sequence, 'n -> 1 n')\n                prompt_mask = rearrange(prompt_mask, 'n -> 1 n')\n                mask = default(mask, lambda: torch.ones(sequence.shape, dtype = torch.bool, device = device))\n                reward = self.reward_model(\n                    sequence,\n                    prompt_mask = prompt_mask,\n                    mask = mask,\n                    sample = True\n                )\n                detach_to_cpu_ = lambda t: rearrange(t.detach().cpu(), '1 ... -> ...')\n\n                # store memory for learning\n                memories.append(Memory(*map(detach_to_cpu_, (\n                    sequence,\n                    prompt_mask,\n                    mask,\n                    action_prob,\n                    action_log_prob,\n                    reward,\n                    value\n                ))))\n\n                # learn from the stored memories\n                if time % update_timesteps == 0:\n                    self.learn(memories)\n                    memories.clear()\n\n        print('rlhf training complete')\n```"]