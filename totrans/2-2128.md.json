["```py\npip install bertopic datasets accelerate bitsandbytes xformers adjustText\n```", "```py\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"CShorten/ML-ArXiv-Papers\")[\"train\"]\n\n# Extract abstracts to train on and corresponding titles\nabstracts = dataset[\"abstract\"]\ntitles = dataset[\"title\"]\n```", "```py\n>>> # The abstract of \"Attention Is All You Need\"\n>>> print(abstracts[13894])\n\n\"\"\"\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks in an encoder-decoder configuration. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer, based\nsolely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to be\nsuperior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\nEnglish-to-German translation task, improving over the existing best results,\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\ntranslation task, our model establishes a new single-model state-of-the-art\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\nof the training costs of the best models from the literature. We show that the\nTransformer generalizes well to other tasks by applying it successfully to\nEnglish constituency parsing both with large and limited training data.\n\"\"\"\n```", "```py\nfrom huggingface_hub import notebook_login\nnotebook_login()\n```", "```py\nfrom torch import cuda\n\nmodel_id = 'meta-llama/Llama-2-13b-chat-hf'\ndevice = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'; print(device)\n```", "```py\nfrom torch import bfloat16\nimport transformers\n\n# Quantization to load an LLM with less GPU memory\nbnb_config = transformers.BitsAndBytesConfig(\n    load_in_4bit=True,  # 4-bit quantization\n    bnb_4bit_quant_type='nf4',  # Normalized float 4\n    bnb_4bit_use_double_quant=True,  # Second quantization after the first\n    bnb_4bit_compute_dtype=bfloat16  # Computation type\n)\n```", "```py\n# Llama 2 Tokenizer\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\n\n# Llama 2 Model\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n    model_id,\n    trust_remote_code=True,\n    quantization_config=bnb_config,\n    device_map='auto',\n)\nmodel.eval()\n```", "```py\n# Our text generator\ngenerator = transformers.pipeline(\n    model=model, tokenizer=tokenizer,\n    task='text-generation',\n    temperature=0.1,\n    max_new_tokens=500,\n    repetition_penalty=1.1\n)\n```", "```py\n>>> prompt = \"Could you explain to me how 4-bit quantization works as if I am 5?\"\n>>> res = generator(prompt)\n>>> print(res[0][\"generated_text\"])\n\n\"\"\"\nCould you explain to me how 4-bit quantization works as if I am 5?\nSure! Imagine you have a big box of crayons. Each crayon represents a different color, like red, blue, green, and so on. Now, imagine that instead of using all the different colors, we only want to use four colors: red, blue, green, and yellow. We can do this by taking all the colors and dividing them into four groups.\nGroup 1: Red, Blue, Green\nGroup 2: Yellow\nNow, when we want to draw something, we can only choose one color from each group. So, if we want to draw a red apple, we would choose the red crayon from Group 1 and the yellow crayon from Group 2\\. This way, we can only use four colors, but we can still make lots of different things!\nIn the same way, when we use 4-bit quantization, we take all the numbers and divide them into four groups. Each group has a different number of bits, just like our crayon groups had different colors. Then, when we want to represent a number, we can only choose one number from each group. This way, we can represent lots of different numbers using only four bits!\n\"\"\"\n```", "```py\n\"\"\"\n<s>[INST] <<SYS>>\n\n{{ System Prompt }}\n<</SYS>>\n{{ User Prompt }}\n [/INST]\n{{ Model Answer }}\n\"\"\"\n```", "```py\n# System prompt describes information given to all conversations\nsystem_prompt = \"\"\"\n<s>[INST] <<SYS>>\nYou are a helpful, respectful and honest assistant for labeling topics.\n<</SYS>>\n\"\"\"\n```", "```py\n# Example prompt demonstrating the output we are looking for\nexample_prompt = \"\"\"\nI have a topic that contains the following documents:\n- Traditional diets in most cultures were primarily plant-based with a little meat on top, but with the rise of industrial style meat production and factory farming, meat has become a staple food.\n- Meat, but especially beef, is the word food in terms of emissions.\n- Eating meat doesn't make you a bad person, not eating meat doesn't make you a good one.\n\nThe topic is described by the following keywords: 'meat, beef, eat, eating, emissions, steak, food, health, processed, chicken'.\n\nBased on the information about the topic above, please create a short label of this topic. Make sure you to only return the label and nothing more.\n[/INST] Environmental impacts of eating meat\n\"\"\"\n```", "```py\n# Our main prompt with documents ([DOCUMENTS]) and keywords ([KEYWORDS]) tags\nmain_prompt = \"\"\"\n[INST]\nI have a topic that contains the following documents:\n[DOCUMENTS]\n\nThe topic is described by the following keywords: '[KEYWORDS]'.\n\nBased on the information about the topic above, please create a short label of this topic. Make sure you to only return the label and nothing more.\n[/INST]\n\"\"\"\n```", "```py\nprompt = system_prompt + example_prompt + main_prompt\n```", "```py\nfrom sentence_transformers import SentenceTransformer\n\n# Pre-calculate embeddings\nembedding_model = SentenceTransformer(\"BAAI/bge-small-en\")\nembeddings = embedding_model.encode(abstracts, show_progress_bar=True)\n```", "```py\nfrom umap import UMAP\nfrom hdbscan import HDBSCAN\n\numap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\nhdbscan_model = HDBSCAN(min_cluster_size=150, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n```", "```py\n# Pre-reduce embeddings for visualization purposes\nreduced_embeddings = UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric='cosine', random_state=42).fit_transform(embeddings)\n```", "```py\nfrom bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, TextGeneration\n\n# KeyBERT\nkeybert = KeyBERTInspired()\n\n# MMR\nmmr = MaximalMarginalRelevance(diversity=0.3)\n\n# Text generation with Llama 2\nllama2 = TextGeneration(generator, prompt=prompt)\n\n# All representation models\nrepresentation_model = {\n    \"KeyBERT\": keybert,\n    \"Llama2\": llama2,\n    \"MMR\": mmr,\n}\n```", "```py\nfrom bertopic import BERTopic\n\ntopic_model = BERTopic(\n\n  # Sub-models\n  embedding_model=embedding_model,\n  umap_model=umap_model,\n  hdbscan_model=hdbscan_model,\n  representation_model=representation_model,\n\n  # Hyperparameters\n  top_n_words=10,\n  verbose=True\n)\n\n# Train model\ntopics, probs = topic_model.fit_transform(abstracts, embeddings)\n```", "```py\n# Show top 3 most frequent topics\ntopic_model.get_topic_info()[1:4]\n```", "```py\n# Show top 3 least frequent topics\ntopic_model.get_topic_info()[-3:]\n```", "```py\nllama2_labels = [label[0][0].split(\"\\n\")[0] for label in topic_model.get_topics(full=True)[\"Llama2\"].values()]\ntopic_model.set_topic_labels(llama2_labels)\n```", "```py\ntopic_model.visualize_documents(titles, reduced_embeddings=reduced_embeddings, \nhide_annotations=True, hide_document_hover=False, custom_labels=True)\n```", "```py\nimport itertools\nimport pandas as pd\n\n# Define colors for the visualization to iterate over\ncolors = itertools.cycle(['#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe', '#008080', '#e6beff', '#9a6324', '#fffac8', '#800000', '#aaffc3', '#808000', '#ffd8b1', '#000075', '#808080', '#ffffff', '#000000'])\ncolor_key = {str(topic): next(colors) for topic in set(topic_model.topics_) if topic != -1}\n\n# Prepare dataframe and ignore outliers\ndf = pd.DataFrame({\"x\": reduced_embeddings[:, 0], \"y\": reduced_embeddings[:, 1], \"Topic\": [str(t) for t in topic_model.topics_]})\ndf[\"Length\"] = [len(doc) for doc in abstracts]\ndf = df.loc[df.Topic != \"-1\"]\ndf = df.loc[(df.y > -10) & (df.y < 10) & (df.x < 10) & (df.x > -10), :]\ndf[\"Topic\"] = df[\"Topic\"].astype(\"category\")\n\n# Get centroids of clusters\nmean_df = df.groupby(\"Topic\").mean().reset_index()\nmean_df.Topic = mean_df.Topic.astype(int)\nmean_df = mean_df.sort_values(\"Topic\")\n```", "```py\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom adjustText import adjust_text\nimport matplotlib.patheffects as pe\nimport textwrap\n\nfig = plt.figure(figsize=(20, 20))\nsns.scatterplot(data=df, x='x', y='y', c=df['Topic'].map(color_key), alpha=0.4, sizes=(0.4, 10), size=\"Length\")\n\n# Annotate top 50 topics\ntexts, xs, ys = [], [], []\nfor row in mean_df.iterrows():\n  topic = row[1][\"Topic\"]\n  name = textwrap.fill(topic_model.custom_labels_[int(topic)], 20)\n  if int(topic) <= 50:\n    xs.append(row[1][\"x\"])\n    ys.append(row[1][\"y\"])\n    texts.append(plt.text(row[1][\"x\"], row[1][\"y\"], name, size=10, ha=\"center\", color=color_key[str(int(topic))],\n                          path_effects=[pe.withStroke(linewidth=0.5, foreground=\"black\")]\n                          ))\n\n# Adjust annotations such that they do not overlap\nadjust_text(texts, x=xs, y=ys, time_lim=1, force_text=(0.01, 0.02), force_static=(0.01, 0.02), force_pull=(0.5, 0.5))\nplt.axis('off')\nplt.legend('', frameon=False)\nplt.show()\n```"]