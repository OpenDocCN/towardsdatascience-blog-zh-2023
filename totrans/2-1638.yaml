- en: 'PatchTST: A Breakthrough in Time Series Forecasting'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/patchtst-a-breakthrough-in-time-series-forecasting-e02d48869ccc](https://towardsdatascience.com/patchtst-a-breakthrough-in-time-series-forecasting-e02d48869ccc)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: From theory to practice, understand the PatchTST algorithm and apply it in Python
    alongside N-BEATS and N-HiTS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@marcopeixeiro?source=post_page-----e02d48869ccc--------------------------------)[![Marco
    Peixeiro](../Images/7cf0a81d87281d35ff47f51e3026a3e9.png)](https://medium.com/@marcopeixeiro?source=post_page-----e02d48869ccc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e02d48869ccc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e02d48869ccc--------------------------------)
    [Marco Peixeiro](https://medium.com/@marcopeixeiro?source=post_page-----e02d48869ccc--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e02d48869ccc--------------------------------)
    ·10 min read·Jun 20, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3a14de30ec6761d39da8521d597c09d3.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Ray Hennessy](https://unsplash.com/@rayhennessy?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Transformer-based models have been successfully applied in many fields like
    natural language processing (think BERT or GPT models) and computer vision to
    name a few.
  prefs: []
  type: TYPE_NORMAL
- en: However, when it comes to time series, state-of-the-art results have mostly
    been achieved by MLP models (multilayer perceptron) such as [N-BEATS](https://medium.com/towards-data-science/the-easiest-way-to-forecast-time-series-using-n-beats-d778fcc2ba60)
    and [N-HiTS](/all-about-n-hits-the-latest-breakthrough-in-time-series-forecasting-a8ddcb27b0d5).
    A recent paper even shows that simple linear models outperform complex transformer-based
    forecasting models on many benchmark datasets (see [Zheng et al., 2022](https://arxiv.org/pdf/2205.13504.pdf)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Still, a new transformer-based model has been proposed that achieves state-of-the-art
    results for long-term forecasting tasks: **PatchTST**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'PatchTST stands for patch time series transformer, and it was first proposed
    in March 2023 by Nie, Nguyen et al in their paper: [A Time Series is Worth 64
    Words: Long-Term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf).
    Their proposed method achieved state-of-the-art results when compared to other
    transformer-based models.'
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we first explore the inner workings of PatchTST, using intuition
    and no equations. Then, we apply the model in a forecasting project and compare
    its performance to MLP models, like N-BEATS and N-HiTS, and assess its performance.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, for more details about PatchTST, make sure to refer to the [original
    paper](https://arxiv.org/pdf/2211.14730.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: '***Learn the latest time series analysis techniques with my*** [***free time
    series cheat sheet***](https://www.datasciencewithmarco.com/pl/2147608294) ***in
    Python! Get the implementation of statistical and deep learning techniques, all
    in Python and TensorFlow!***'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: Exploring PatchTST
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned, PatchTST stands for patch time series transformer.
  prefs: []
  type: TYPE_NORMAL
- en: As the name suggests, it makes use of patching and of the transformer architecture.
    It also includes channel-independence to treat multivariate time series. The general
    architecture is shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b69462b040217b63054f4cae1f6fb80.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The PatchTST model architecture. We see that the model makes use of channel-independence
    to treat multivariate time series. In the transformer backbone, we also see the
    use of patching (illustrated by the rectangles). Plus, there are two versions
    to the model: supervised and self-supervised. Image by Nie Y., Nguyen N., Sinthong
    P., Kalagnanam J. from [A Time Series is Worth 64 Words: Long-Term Forecasting
    with Transformers](https://arxiv.org/pdf/2211.14730.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a lot of information to gather from the figure above. Here, the key
    elements are that PatchTST uses channel-independence to forecast multivariate
    time series. Then, in its transformer backbone, the model uses patching, which
    are illustrated by the small vertical rectangles. Also, the model comes in two
    versions: supervised and self-supervised.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore in more detail the architecture and inner workings of PatchTST.
  prefs: []
  type: TYPE_NORMAL
- en: Channel-independence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, a multivariate time series is considered as a multi-channel signal. Each
    time series is basically a channel containing a signal.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4151bc1afe49fef36ba9ca36837b6cd7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Overview of the PatchTST model. Here, we really emphasize on the implementation
    of channel-independence, where each input token to the Transformer backbone contains
    information from only one channel, or one time series. Image by Nie Y., Nguyen
    N., Sinthong P., Kalagnanam J. from [A Time Series is Worth 64 Words: Long-Term
    Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: In the figure above, we see how a multivariate time series is separated into
    individual series, and each is fed to the Transformer backbone as an input token.
    Then, predictions are made for each series and the results are concatenated for
    the final predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Patching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most work on Transformer-based forecasting models focused on building new mechanisms
    to simplify the original attention mechanism. However, they still relied on point-wise
    attention, which is not ideal when it comes to time series.
  prefs: []
  type: TYPE_NORMAL
- en: In time series forecasting, we want to extract relationships between past time
    steps and future time steps to make predictions. With point-wise attention, we
    are trying to retrieve information from a single time step, without looking at
    what surrounds that point. In other words, we isolate a time step, and do not
    look at points before or after.
  prefs: []
  type: TYPE_NORMAL
- en: This is like trying to understand the meaning of a word without looking at the
    words around it in a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, PatchTST makes use of patching to extract local semantic information
    in time series.
  prefs: []
  type: TYPE_NORMAL
- en: How patching works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Each input series is divided into patches, which are simply shorter series coming
    from the original one.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7a2a35f59c87380c06d755afe15d0cad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Transformer backbone of PatchTST. Here, we see that the input time series
    (at the bottom of the figure) goes through patching, resulting in multiple patches
    (the vertical rectangles) which are then sent to the Transformer encoder. Image
    by Nie Y., Nguyen N., Sinthong P., Kalagnanam J. from [A Time Series is Worth
    64 Words: Long-Term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: Here, the patch can be overlapping or non-overlapping. The number of patches
    depends on the length of the patch *P* and the stride *S*. Here, the stride is
    like in convolution, it is simply how many timesteps separate the beginning of
    consecutive patches.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bdf457f74ce8a32e97c5b832f4219c36.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualizing patching. Here, we have a sequence of 15 timesteps, with a patch
    length of 5 and a stride of 5 as well, resulting in three patches. Image by the
    author.
  prefs: []
  type: TYPE_NORMAL
- en: In the figure above, we can visualize the result of patching. Here, we have
    a sequence length (*L*) of 15 time steps, with a patch length (*P*) of 5 and a
    stride (*S*) of 5\. The result is the series being separated into 3 patches.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages of patching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With patching, the model can extract local semantic meaning by looking at groups
    of time steps, instead of looking at a single time step.
  prefs: []
  type: TYPE_NORMAL
- en: It also has the added benefit of greatly reducing the number of token being
    fed to the transformer encoder. Here, each patch becomes an input token to be
    input to the Transformer. That way, we can reduce the number of token from *L*
    to approximately *L/S*.
  prefs: []
  type: TYPE_NORMAL
- en: That way, we greatly reduce the space and time complexity of the model. This
    in turn means that we can feed the model a longer input sequence to extract meaningful
    temporal relationships.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, with patching, the model is faster, lighter, and can treat a longer
    input sequence, meaning that it can potentially learn more about the series and
    make better forecasts.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer encoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the series is patched, it is then fed to the transformer encoder. This
    is the classical transformer architecture. Nothing was modified.
  prefs: []
  type: TYPE_NORMAL
- en: Then, the output is fed to linear layer, and predictions are made.
  prefs: []
  type: TYPE_NORMAL
- en: Improving PatchTST with representation learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The authors of the paper suggested another improvement to the model by using
    representation learning.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/25ac32984418b4240d427515e4af5e8d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Visualizing self-supervised representation learning in PatchTST. Here, the
    model will randomly mask patches and learn to reconstruct them. Image by Nie Y.,
    Nguyen N., Sinthong P., Kalagnanam J. from [A Time Series is Worth 64 Words: Long-Term
    Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: From the figure above, we can see that PatchTST can use self-supervised representation
    learning to capture abstract representations of the data. This can lead to potential
    improvements in forecasting performance.
  prefs: []
  type: TYPE_NORMAL
- en: Here, the process is fairly simple, as random patches will be masked, meaning
    that they will be set to 0\. This is shown, in the figure above, by the blank
    vertical rectangles. Then, the model is trained to recreate the original patches,
    which is what is output at the top of the figure, as the grey vertical rectangles.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a good understanding of how PatchTST works, let’s test it against
    other models and see how it performs.
  prefs: []
  type: TYPE_NORMAL
- en: Forecasting with PatchTST
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the paper, PatchTST is compared with other Transformer-based models. However,
    recent MLP-based models have been published, like N-BEATS and N-HiTS, and have
    also demonstrated state-of-the-art performance on long horizon forecasting tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The complete source code for this section is available on [GitHub](https://github.com/marcopeix/datasciencewithmarco/blob/master/PatchTST.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: Here, let’s apply PatchTST, along with N-BEATS and N-HiTS and evaluate its performance
    against these two MLP-based models.
  prefs: []
  type: TYPE_NORMAL
- en: For this exercise, we use the Exchange dataset, which is a common benchmark
    dataset for long-term forecasting in research. The dataset contains daily exchange
    rates of eight countries relative to the US dollar, from 1990 to 2016\. The dataset
    is made available through the MIT License.
  prefs: []
  type: TYPE_NORMAL
- en: Initial setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s start by importing the required libraries. Here, we will work with `neuralforecast`,
    as they have an out-of-the-box implementation of PatchTST. For the dataset, we
    use the `datasetsforecast` library, which includes all popular datasets for evaluating
    forecasting algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If you have CUDA installed, then `neuralforecast` will automatically leverage
    your GPU to train the models. On my end, I do not have it installed, which is
    why I am not doing extensive hyperparameter tuning, or training on very large
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Once that is done, let’s download the Exchange dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here, we see that we get three DataFrames. The first one contains the daily
    exchange rates for each country. The second one contains exogenous time series.
    The third one, contains static exogenous variables (like day, month, year, hour,
    or any future information that we know).
  prefs: []
  type: TYPE_NORMAL
- en: For this exercise, we only work with `Y_df`.
  prefs: []
  type: TYPE_NORMAL
- en: Then, let’s make sure that the dates have the right type.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/5bdc85c98c0f9baa3c85b7e7541aa8a7.png)'
  prefs: []
  type: TYPE_IMG
- en: First five rows of the Exchange dataset. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: In the figure above, we see that we have three columns. The first column is
    a unique identifier and it is necessary to have an id column when working with
    `neuralforecast`. Then, the `ds` column has the date, and the `y` column has the
    exchange rate.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a8d8e7e94211b15b058816ad8f55e48b.png)'
  prefs: []
  type: TYPE_IMG
- en: Showing the number of observations per unique id. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: From the picture above, we can see that each unique id corresponds to a country,
    and that we have 7588 observations per country.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we define the sizes of our validation and test sets. Here, I chose 760
    time steps for validation, and 1517 for the test set, as specified by the `[datasets](https://github.com/Nixtla/datasetsforecast/blob/main/datasetsforecast/long_horizon.py)`
    [library](https://github.com/Nixtla/datasetsforecast/blob/main/datasetsforecast/long_horizon.py).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Then, let’s plot one of the series, to see what we are working with. Here, I
    decided to plot the series for the first country (unique_id = 0), but feel free
    to plot another series.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/8fb95dd385ff7a81b55fbc9cf21a8618.png)'
  prefs: []
  type: TYPE_IMG
- en: Daily exchange rate for the first country, from 1990 to 2016\. Image by the
    author.
  prefs: []
  type: TYPE_NORMAL
- en: From the figure above, we see that we have fairly noisy data with no clear seasonality.
  prefs: []
  type: TYPE_NORMAL
- en: Modelling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having explored the data, let’s get started on modelling with `neuralforecast`.
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to set the horizon. In this case, I use 96 time steps, as this
    horizon is also used in the [PatchTST paper](https://arxiv.org/pdf/2211.14730.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Then, to have a fair evaluation of each model, I decided to set the input size
    to twice the horizon (so 192 time steps), and set the maximum number of epochs
    to 50\. All other hyperparameters are kept to their default values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Then, we initialize the `NeuralForecast`object, by specifying the models we
    want to use and the frequency of the forecast, which in this is case is daily.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We are now ready to make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Forecasting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To generate predictions, we use the `cross_validation` method to make use of
    the validation and test sets. It will return a DataFrame with predictions from
    all models and the associated true value.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/e7922f2c01e0b5086adf5558a05930bf.png)'
  prefs: []
  type: TYPE_IMG
- en: First five rows of the predictions DataFrame. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, for each id, we have the predictions from each model as well
    as the true value in the `y` column.
  prefs: []
  type: TYPE_NORMAL
- en: Now, to evaluate the models, we have to reshape the arrays of actual and predicted
    values to have the shape `(number of series, number of windows, forecast horizon)`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: With that done, we can optionally plot the predictions of our models. Here,
    we plot the predictions in the first window of the first series.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a69608da3942d28cec8980644b40e72d.png)'
  prefs: []
  type: TYPE_IMG
- en: Predictions of the daily exchange rate for the first series, in the first window.
    Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: This figure is a bit underwhelming, as N-BEATS and N-HiTS seem to have predictions
    that are very off from the actual values. However, PatchTST, while also off, seems
    to be the closest to the actual values.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, we must takes this with a grain of salt, because we are only visualizing
    the prediction for one series, in one prediction window.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So, let’s evaluate the performance of each model. To replicate the methodology
    from the paper, we use both the MAE and MSE as performance metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/0c6660f2c34408d4bb5c2f7e1b609783.png)'
  prefs: []
  type: TYPE_IMG
- en: Performance of all models. Here, PatchTST achieves the lowest MAE and MSE. Image
    by the author.
  prefs: []
  type: TYPE_NORMAL
- en: In the table above, we see that PatchTST is the champion model as it achieves
    the lowest MAE and MSE.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, this was not the most thorough experiment, as we only used one dataset
    and one forecast horizon. Still, it is interesting to see that a Transformer-based
    model can compete with state-of-the-art MLP models.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PatchTST is a Transformer-based models that uses patching to extract local semantic
    meaning in time series data. This allows the model to be faster to train and to
    have a longer input window.
  prefs: []
  type: TYPE_NORMAL
- en: It has achieved state-of-the-art performances when compared to other Transformer-based
    models. In our little exercise, we saw that it also achieved better performances
    than N-BEATS and N-HiTS.
  prefs: []
  type: TYPE_NORMAL
- en: While this does not mean that it is better than N-HiTS or N-BEATS, it remains
    an interesting option when forecasting on a long horizon.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading! I hope that you enjoyed it and that you learned something
    new!
  prefs: []
  type: TYPE_NORMAL
- en: Looking to master time series forecasting? The check out [Applied Time Series
    Forecasting in Python](https://www.datasciencewithmarco.com/offers/zTAs2hi6/checkout?coupon_code=ATSFP10).
    This is the only course that uses Python to implement statistical, deep learning
    and state-of-the-art models in 15 guided hands-on projects.
  prefs: []
  type: TYPE_NORMAL
- en: Cheers 🍻
  prefs: []
  type: TYPE_NORMAL
- en: Support me
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Enjoying my work? Show your support with [Buy me a coffee](http://buymeacoffee.com/dswm),
    a simple way for you to encourage me, and I get to enjoy a cup of coffee! If you
    feel like it, just click the button below 👇
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/7ad9438bd50b1698fdd722fa6661b16c.png)](http://buymeacoffee.com/dswm)'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[A Time Series is Worth 64 Words: Long-Term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf)
    by Nie Y., Nguyen N. et al.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Neuralforecast](https://nixtla.github.io/neuralforecast/) by Olivares K.,
    Challu C., Garza F., Canseco M., Dubrawski A.'
  prefs: []
  type: TYPE_NORMAL
