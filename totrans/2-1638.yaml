- en: 'PatchTST: A Breakthrough in Time Series Forecasting'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PatchTST：时间序列预测中的突破
- en: 原文：[https://towardsdatascience.com/patchtst-a-breakthrough-in-time-series-forecasting-e02d48869ccc](https://towardsdatascience.com/patchtst-a-breakthrough-in-time-series-forecasting-e02d48869ccc)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/patchtst-a-breakthrough-in-time-series-forecasting-e02d48869ccc](https://towardsdatascience.com/patchtst-a-breakthrough-in-time-series-forecasting-e02d48869ccc)
- en: From theory to practice, understand the PatchTST algorithm and apply it in Python
    alongside N-BEATS and N-HiTS
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从理论到实践，了解 PatchTST 算法，并在 Python 中与 N-BEATS 和 N-HiTS 一起应用。
- en: '[](https://medium.com/@marcopeixeiro?source=post_page-----e02d48869ccc--------------------------------)[![Marco
    Peixeiro](../Images/7cf0a81d87281d35ff47f51e3026a3e9.png)](https://medium.com/@marcopeixeiro?source=post_page-----e02d48869ccc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e02d48869ccc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e02d48869ccc--------------------------------)
    [Marco Peixeiro](https://medium.com/@marcopeixeiro?source=post_page-----e02d48869ccc--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@marcopeixeiro?source=post_page-----e02d48869ccc--------------------------------)[![Marco
    Peixeiro](../Images/7cf0a81d87281d35ff47f51e3026a3e9.png)](https://medium.com/@marcopeixeiro?source=post_page-----e02d48869ccc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e02d48869ccc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e02d48869ccc--------------------------------)
    [Marco Peixeiro](https://medium.com/@marcopeixeiro?source=post_page-----e02d48869ccc--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e02d48869ccc--------------------------------)
    ·10 min read·Jun 20, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e02d48869ccc--------------------------------)
    ·阅读时间 10 分钟·2023 年 6 月 20 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/3a14de30ec6761d39da8521d597c09d3.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a14de30ec6761d39da8521d597c09d3.png)'
- en: Photo by [Ray Hennessy](https://unsplash.com/@rayhennessy?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Ray Hennessy](https://unsplash.com/@rayhennessy?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Transformer-based models have been successfully applied in many fields like
    natural language processing (think BERT or GPT models) and computer vision to
    name a few.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 基于变换器的模型已成功应用于许多领域，如自然语言处理（例如 BERT 或 GPT 模型）和计算机视觉等。
- en: However, when it comes to time series, state-of-the-art results have mostly
    been achieved by MLP models (multilayer perceptron) such as [N-BEATS](https://medium.com/towards-data-science/the-easiest-way-to-forecast-time-series-using-n-beats-d778fcc2ba60)
    and [N-HiTS](/all-about-n-hits-the-latest-breakthrough-in-time-series-forecasting-a8ddcb27b0d5).
    A recent paper even shows that simple linear models outperform complex transformer-based
    forecasting models on many benchmark datasets (see [Zheng et al., 2022](https://arxiv.org/pdf/2205.13504.pdf)).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在时间序列方面，最先进的结果主要由 MLP 模型（多层感知器）如 [N-BEATS](https://medium.com/towards-data-science/the-easiest-way-to-forecast-time-series-using-n-beats-d778fcc2ba60)
    和 [N-HiTS](/all-about-n-hits-the-latest-breakthrough-in-time-series-forecasting-a8ddcb27b0d5)
    实现。最近的一篇论文甚至表明，简单的线性模型在许多基准数据集上优于复杂的基于变换器的预测模型（见 [Zheng 等人, 2022](https://arxiv.org/pdf/2205.13504.pdf)）。
- en: 'Still, a new transformer-based model has been proposed that achieves state-of-the-art
    results for long-term forecasting tasks: **PatchTST**.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，一种新的基于变换器的模型已经被提出，并在长期预测任务中取得了最先进的结果：**PatchTST**。
- en: 'PatchTST stands for patch time series transformer, and it was first proposed
    in March 2023 by Nie, Nguyen et al in their paper: [A Time Series is Worth 64
    Words: Long-Term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf).
    Their proposed method achieved state-of-the-art results when compared to other
    transformer-based models.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: PatchTST 代表补丁时间序列变换器，它首次由 Nie、Nguyen 等人在 2023 年 3 月提出，详细介绍在他们的论文中：[时间序列值得 64
    个词：使用变换器进行长期预测](https://arxiv.org/pdf/2211.14730.pdf)。他们提出的方法在与其他基于变换器的模型比较时，取得了最先进的结果。
- en: In this article, we first explore the inner workings of PatchTST, using intuition
    and no equations. Then, we apply the model in a forecasting project and compare
    its performance to MLP models, like N-BEATS and N-HiTS, and assess its performance.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们首先直观地探索 PatchTST 的内部工作原理，而不使用任何方程式。接着，我们将该模型应用于预测项目，并将其性能与 MLP 模型（如
    N-BEATS 和 N-HiTS）进行比较和评估。
- en: Of course, for more details about PatchTST, make sure to refer to the [original
    paper](https://arxiv.org/pdf/2211.14730.pdf).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，关于 PatchTST 的更多细节，请参考 [原始论文](https://arxiv.org/pdf/2211.14730.pdf)。
- en: '***Learn the latest time series analysis techniques with my*** [***free time
    series cheat sheet***](https://www.datasciencewithmarco.com/pl/2147608294) ***in
    Python! Get the implementation of statistical and deep learning techniques, all
    in Python and TensorFlow!***'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***使用我的*** [***免费时间序列备忘单***](https://www.datasciencewithmarco.com/pl/2147608294)
    ***在 Python 中学习最新的时间序列分析技术！获取统计和深度学习技术的实现，全部使用 Python 和 TensorFlow！***'
- en: Let’s get started!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Exploring PatchTST
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 PatchTST
- en: As mentioned, PatchTST stands for patch time series transformer.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，PatchTST 代表补丁时间序列 transformer。
- en: As the name suggests, it makes use of patching and of the transformer architecture.
    It also includes channel-independence to treat multivariate time series. The general
    architecture is shown below.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 正如名字所示，它利用了补丁和 transformer 架构。它还包括通道独立性来处理多变量时间序列。下面展示了总体架构。
- en: '![](../Images/4b69462b040217b63054f4cae1f6fb80.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b69462b040217b63054f4cae1f6fb80.png)'
- en: 'The PatchTST model architecture. We see that the model makes use of channel-independence
    to treat multivariate time series. In the transformer backbone, we also see the
    use of patching (illustrated by the rectangles). Plus, there are two versions
    to the model: supervised and self-supervised. Image by Nie Y., Nguyen N., Sinthong
    P., Kalagnanam J. from [A Time Series is Worth 64 Words: Long-Term Forecasting
    with Transformers](https://arxiv.org/pdf/2211.14730.pdf).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: PatchTST 模型架构。我们看到该模型利用通道独立性来处理多变量时间序列。在 transformer 主干中，我们还看到使用了补丁（由矩形表示）。此外，模型有两个版本：监督学习和自监督学习。图片由
    Nie Y.、Nguyen N.、Sinthong P.、Kalagnanam J. 提供，来源于 [“一个时间序列值 64 个词：使用 Transformer
    进行长期预测”](https://arxiv.org/pdf/2211.14730.pdf)。
- en: 'There is a lot of information to gather from the figure above. Here, the key
    elements are that PatchTST uses channel-independence to forecast multivariate
    time series. Then, in its transformer backbone, the model uses patching, which
    are illustrated by the small vertical rectangles. Also, the model comes in two
    versions: supervised and self-supervised.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 从上图中需要收集大量信息。这里的关键要素是，PatchTST 使用通道独立性来预测多变量时间序列。然后，在其 transformer 主干中，该模型使用补丁，这些补丁由小的垂直矩形表示。此外，该模型有两个版本：监督学习和自监督学习。
- en: Let’s explore in more detail the architecture and inner workings of PatchTST.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地探讨 PatchTST 的架构和内部工作原理。
- en: Channel-independence
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通道独立性
- en: Here, a multivariate time series is considered as a multi-channel signal. Each
    time series is basically a channel containing a signal.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，多变量时间序列被视为多通道信号。每个时间序列基本上是一个包含信号的通道。
- en: '![](../Images/4151bc1afe49fef36ba9ca36837b6cd7.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4151bc1afe49fef36ba9ca36837b6cd7.png)'
- en: 'Overview of the PatchTST model. Here, we really emphasize on the implementation
    of channel-independence, where each input token to the Transformer backbone contains
    information from only one channel, or one time series. Image by Nie Y., Nguyen
    N., Sinthong P., Kalagnanam J. from [A Time Series is Worth 64 Words: Long-Term
    Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: PatchTST 模型概述。在这里，我们特别强调通道独立性的实现，其中每个输入 token 到 Transformer 主干仅包含来自一个通道或一个时间序列的信息。图片由
    Nie Y.、Nguyen N.、Sinthong P.、Kalagnanam J. 提供，来源于 [“一个时间序列值 64 个词：使用 Transformer
    进行长期预测”](https://arxiv.org/pdf/2211.14730.pdf)。
- en: In the figure above, we see how a multivariate time series is separated into
    individual series, and each is fed to the Transformer backbone as an input token.
    Then, predictions are made for each series and the results are concatenated for
    the final predictions.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，我们看到如何将多变量时间序列分离成单独的序列，并将每个序列作为输入 token 传递给 Transformer 主干。然后，对每个序列进行预测，结果被连接起来形成最终预测。
- en: Patching
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 补丁
- en: Most work on Transformer-based forecasting models focused on building new mechanisms
    to simplify the original attention mechanism. However, they still relied on point-wise
    attention, which is not ideal when it comes to time series.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数基于 Transformer 的预测模型的工作集中在构建新的机制来简化原始注意力机制。然而，它们仍然依赖于逐点注意力，这在时间序列的情况下并不理想。
- en: In time series forecasting, we want to extract relationships between past time
    steps and future time steps to make predictions. With point-wise attention, we
    are trying to retrieve information from a single time step, without looking at
    what surrounds that point. In other words, we isolate a time step, and do not
    look at points before or after.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间序列预测中，我们希望提取过去时间步和未来时间步之间的关系以进行预测。使用逐点注意力时，我们试图从单个时间步中检索信息，而不考虑该点周围的内容。换句话说，我们孤立一个时间步，而不查看之前或之后的点。
- en: This is like trying to understand the meaning of a word without looking at the
    words around it in a sentence.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这就像试图理解一个词的意义而不看句子中的其他词一样。
- en: Therefore, PatchTST makes use of patching to extract local semantic information
    in time series.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，PatchTST 利用分块来提取时间序列中的局部语义信息。
- en: How patching works
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分块如何工作
- en: Each input series is divided into patches, which are simply shorter series coming
    from the original one.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 每个输入序列被分成块，这些块只是来自原始序列的较短序列。
- en: '![](../Images/7a2a35f59c87380c06d755afe15d0cad.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a2a35f59c87380c06d755afe15d0cad.png)'
- en: 'The Transformer backbone of PatchTST. Here, we see that the input time series
    (at the bottom of the figure) goes through patching, resulting in multiple patches
    (the vertical rectangles) which are then sent to the Transformer encoder. Image
    by Nie Y., Nguyen N., Sinthong P., Kalagnanam J. from [A Time Series is Worth
    64 Words: Long-Term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 'PatchTST 的 Transformer 主干。这里，我们看到输入时间序列（图底部）经过分块，结果是多个块（垂直矩形），然后被送到 Transformer
    编码器。图片由 Nie Y.、Nguyen N.、Sinthong P.、Kalagnanam J. 提供，来自 [A Time Series is Worth
    64 Words: Long-Term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf)。'
- en: Here, the patch can be overlapping or non-overlapping. The number of patches
    depends on the length of the patch *P* and the stride *S*. Here, the stride is
    like in convolution, it is simply how many timesteps separate the beginning of
    consecutive patches.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，块可以是重叠的也可以是不重叠的。块的数量取决于块的长度 *P* 和步幅 *S*。这里，步幅类似于卷积，它只是分隔连续块开始的时间步数。
- en: '![](../Images/bdf457f74ce8a32e97c5b832f4219c36.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bdf457f74ce8a32e97c5b832f4219c36.png)'
- en: Visualizing patching. Here, we have a sequence of 15 timesteps, with a patch
    length of 5 and a stride of 5 as well, resulting in three patches. Image by the
    author.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化分块。这里，我们有一个 15 个时间步的序列，分块长度为 5，步幅也为 5，结果是三个块。图片由作者提供。
- en: In the figure above, we can visualize the result of patching. Here, we have
    a sequence length (*L*) of 15 time steps, with a patch length (*P*) of 5 and a
    stride (*S*) of 5\. The result is the series being separated into 3 patches.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，我们可以可视化分块的结果。这里，我们有一个序列长度 (*L*) 为 15 个时间步，分块长度 (*P*) 为 5，步幅 (*S*) 为 5。结果是序列被分为
    3 个块。
- en: Advantages of patching
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分块的优势
- en: With patching, the model can extract local semantic meaning by looking at groups
    of time steps, instead of looking at a single time step.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分块，模型可以通过查看时间步的组而不是单个时间步来提取局部语义信息。
- en: It also has the added benefit of greatly reducing the number of token being
    fed to the transformer encoder. Here, each patch becomes an input token to be
    input to the Transformer. That way, we can reduce the number of token from *L*
    to approximately *L/S*.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 它还有一个额外的好处，就是大大减少了送入 Transformer 编码器的标记数量。这里，每个块成为输入到 Transformer 的输入标记。这样，我们可以将标记数量从
    *L* 减少到大约 *L/S*。
- en: That way, we greatly reduce the space and time complexity of the model. This
    in turn means that we can feed the model a longer input sequence to extract meaningful
    temporal relationships.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们大大减少了模型的空间和时间复杂度。这反过来意味着我们可以给模型输入更长的序列，以提取有意义的时间关系。
- en: Therefore, with patching, the model is faster, lighter, and can treat a longer
    input sequence, meaning that it can potentially learn more about the series and
    make better forecasts.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过分块，模型变得更快、更轻，并且可以处理更长的输入序列，这意味着它可能会对序列学习得更多并做出更好的预测。
- en: Transformer encoder
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Transformer 编码器
- en: Once the series is patched, it is then fed to the transformer encoder. This
    is the classical transformer architecture. Nothing was modified.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦序列被分块，它会被送入 Transformer 编码器。这是经典的 Transformer 架构，没有任何修改。
- en: Then, the output is fed to linear layer, and predictions are made.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将输出送入线性层，并进行预测。
- en: Improving PatchTST with representation learning
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用表示学习改进 PatchTST
- en: The authors of the paper suggested another improvement to the model by using
    representation learning.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的作者建议通过使用表示学习来改进模型。
- en: '![](../Images/25ac32984418b4240d427515e4af5e8d.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/25ac32984418b4240d427515e4af5e8d.png)'
- en: 'Visualizing self-supervised representation learning in PatchTST. Here, the
    model will randomly mask patches and learn to reconstruct them. Image by Nie Y.,
    Nguyen N., Sinthong P., Kalagnanam J. from [A Time Series is Worth 64 Words: Long-Term
    Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '在PatchTST中可视化自监督表示学习。在这里，模型将随机掩盖补丁并学习重建它们。图片由Nie Y.、Nguyen N.、Sinthong P.、Kalagnanam
    J.提供，来自[A Time Series is Worth 64 Words: Long-Term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf)。'
- en: From the figure above, we can see that PatchTST can use self-supervised representation
    learning to capture abstract representations of the data. This can lead to potential
    improvements in forecasting performance.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 从上图中，我们可以看到PatchTST可以利用自监督表示学习来捕捉数据的抽象表示。这可能会导致预测性能的潜在改善。
- en: Here, the process is fairly simple, as random patches will be masked, meaning
    that they will be set to 0\. This is shown, in the figure above, by the blank
    vertical rectangles. Then, the model is trained to recreate the original patches,
    which is what is output at the top of the figure, as the grey vertical rectangles.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，过程相当简单，因为随机补丁将被掩盖，意味着它们将被设置为0。这在上图中由空白的垂直矩形表示。然后，模型被训练以重建原始补丁，这就是图顶端输出的灰色垂直矩形。
- en: Now that we have a good understanding of how PatchTST works, let’s test it against
    other models and see how it performs.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对PatchTST的工作原理有了很好的理解，让我们将其与其他模型进行测试，看看其表现如何。
- en: Forecasting with PatchTST
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PatchTST进行预测
- en: In the paper, PatchTST is compared with other Transformer-based models. However,
    recent MLP-based models have been published, like N-BEATS and N-HiTS, and have
    also demonstrated state-of-the-art performance on long horizon forecasting tasks.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在论文中，PatchTST与其他基于Transformer的模型进行了比较。然而，最近也发布了基于MLP的模型，如N-BEATS和N-HiTS，它们在长期预测任务中也展示了最先进的性能。
- en: The complete source code for this section is available on [GitHub](https://github.com/marcopeix/datasciencewithmarco/blob/master/PatchTST.ipynb).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的完整源代码可在[GitHub](https://github.com/marcopeix/datasciencewithmarco/blob/master/PatchTST.ipynb)上找到。
- en: Here, let’s apply PatchTST, along with N-BEATS and N-HiTS and evaluate its performance
    against these two MLP-based models.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，让我们应用PatchTST，以及N-BEATS和N-HiTS，并评估它在这两个基于MLP的模型中的表现。
- en: For this exercise, we use the Exchange dataset, which is a common benchmark
    dataset for long-term forecasting in research. The dataset contains daily exchange
    rates of eight countries relative to the US dollar, from 1990 to 2016\. The dataset
    is made available through the MIT License.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个练习，我们使用Exchange数据集，这是研究中用于长期预测的常见基准数据集。该数据集包含1990年至2016年期间相对于美元的八个国家的每日汇率。该数据集通过MIT许可证提供。
- en: Initial setup
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始设置
- en: Let’s start by importing the required libraries. Here, we will work with `neuralforecast`,
    as they have an out-of-the-box implementation of PatchTST. For the dataset, we
    use the `datasetsforecast` library, which includes all popular datasets for evaluating
    forecasting algorithms.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先导入所需的库。在这里，我们将使用`neuralforecast`，因为它们有一个现成的PatchTST实现。对于数据集，我们使用`datasetsforecast`库，其中包含所有流行的数据集，用于评估预测算法。
- en: '[PRE0]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you have CUDA installed, then `neuralforecast` will automatically leverage
    your GPU to train the models. On my end, I do not have it installed, which is
    why I am not doing extensive hyperparameter tuning, or training on very large
    datasets.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经安装了CUDA，那么`neuralforecast`将自动利用你的GPU来训练模型。在我这边，我没有安装CUDA，这就是为什么我没有进行广泛的超参数调整或在非常大的数据集上训练的原因。
- en: Once that is done, let’s download the Exchange dataset.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这一步后，让我们下载Exchange数据集。
- en: '[PRE1]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, we see that we get three DataFrames. The first one contains the daily
    exchange rates for each country. The second one contains exogenous time series.
    The third one, contains static exogenous variables (like day, month, year, hour,
    or any future information that we know).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到得到三个DataFrames。第一个包含每个国家的每日汇率。第二个包含外生时间序列。第三个包含静态外生变量（如日期、月份、年份、小时或我们已知的任何未来信息）。
- en: For this exercise, we only work with `Y_df`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个练习，我们只使用`Y_df`。
- en: Then, let’s make sure that the dates have the right type.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，确保日期具有正确的类型。
- en: '[PRE2]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/5bdc85c98c0f9baa3c85b7e7541aa8a7.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5bdc85c98c0f9baa3c85b7e7541aa8a7.png)'
- en: First five rows of the Exchange dataset. Image by the author.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Exchange数据集的前五行。作者提供的图片。
- en: In the figure above, we see that we have three columns. The first column is
    a unique identifier and it is necessary to have an id column when working with
    `neuralforecast`. Then, the `ds` column has the date, and the `y` column has the
    exchange rate.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，我们看到有三列。第一列是唯一标识符，当使用`neuralforecast`时，需要有一个id列。然后，`ds`列有日期，`y`列有汇率。
- en: '[PRE3]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/a8d8e7e94211b15b058816ad8f55e48b.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8d8e7e94211b15b058816ad8f55e48b.png)'
- en: Showing the number of observations per unique id. Image by the author.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 显示每个唯一id的观测数量。图片由作者提供。
- en: From the picture above, we can see that each unique id corresponds to a country,
    and that we have 7588 observations per country.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 从上图可以看出，每个唯一id对应一个国家，我们每个国家有7588个观测值。
- en: Now, we define the sizes of our validation and test sets. Here, I chose 760
    time steps for validation, and 1517 for the test set, as specified by the `[datasets](https://github.com/Nixtla/datasetsforecast/blob/main/datasetsforecast/long_horizon.py)`
    [library](https://github.com/Nixtla/datasetsforecast/blob/main/datasetsforecast/long_horizon.py).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们定义验证集和测试集的大小。在这里，我选择了760个时间步作为验证集，1517个时间步作为测试集，如`[datasets](https://github.com/Nixtla/datasetsforecast/blob/main/datasetsforecast/long_horizon.py)`
    [库](https://github.com/Nixtla/datasetsforecast/blob/main/datasetsforecast/long_horizon.py)所规定。
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Then, let’s plot one of the series, to see what we are working with. Here, I
    decided to plot the series for the first country (unique_id = 0), but feel free
    to plot another series.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，让我们绘制其中一个序列，看看我们在处理什么。在这里，我决定绘制第一个国家的序列（unique_id = 0），但可以自由绘制其他序列。
- en: '[PRE5]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/8fb95dd385ff7a81b55fbc9cf21a8618.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8fb95dd385ff7a81b55fbc9cf21a8618.png)'
- en: Daily exchange rate for the first country, from 1990 to 2016\. Image by the
    author.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个国家从1990年到2016年的每日汇率。图片由作者提供。
- en: From the figure above, we see that we have fairly noisy data with no clear seasonality.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 从上图中，我们看到数据相当嘈杂，没有明显的季节性。
- en: Modelling
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建模
- en: Having explored the data, let’s get started on modelling with `neuralforecast`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 探索了数据后，让我们开始使用`neuralforecast`进行建模。
- en: First, we need to set the horizon. In this case, I use 96 time steps, as this
    horizon is also used in the [PatchTST paper](https://arxiv.org/pdf/2211.14730.pdf).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要设置预测期。在这种情况下，我使用96个时间步，因为这个预测期也在[PatchTST论文](https://arxiv.org/pdf/2211.14730.pdf)中使用。
- en: Then, to have a fair evaluation of each model, I decided to set the input size
    to twice the horizon (so 192 time steps), and set the maximum number of epochs
    to 50\. All other hyperparameters are kept to their default values.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，为了公平评估每个模型，我决定将输入大小设置为预测期的两倍（即192个时间步），并将最大训练轮数设置为50。所有其他超参数保持默认值。
- en: '[PRE6]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Then, we initialize the `NeuralForecast`object, by specifying the models we
    want to use and the frequency of the forecast, which in this is case is daily.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们初始化`NeuralForecast`对象，指定我们要使用的模型和预测频率，在这里是每日。
- en: '[PRE7]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We are now ready to make predictions.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备进行预测。
- en: Forecasting
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测
- en: To generate predictions, we use the `cross_validation` method to make use of
    the validation and test sets. It will return a DataFrame with predictions from
    all models and the associated true value.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成预测，我们使用`cross_validation`方法利用验证集和测试集。它将返回一个包含所有模型预测值及相关真实值的DataFrame。
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/e7922f2c01e0b5086adf5558a05930bf.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e7922f2c01e0b5086adf5558a05930bf.png)'
- en: First five rows of the predictions DataFrame. Image by the author.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 预测DataFrame的前五行。图片由作者提供。
- en: As you can see, for each id, we have the predictions from each model as well
    as the true value in the `y` column.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，对于每个id，我们有来自每个模型的预测以及`y`列中的真实值。
- en: Now, to evaluate the models, we have to reshape the arrays of actual and predicted
    values to have the shape `(number of series, number of windows, forecast horizon)`.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了评估模型，我们需要将实际值和预测值的数组重塑为形状为`(series的数量, 窗口数量, 预测期)`。
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: With that done, we can optionally plot the predictions of our models. Here,
    we plot the predictions in the first window of the first series.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，我们可以选择性地绘制模型的预测。在这里，我们绘制了第一个序列的第一个窗口中的预测。
- en: '[PRE10]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/a69608da3942d28cec8980644b40e72d.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a69608da3942d28cec8980644b40e72d.png)'
- en: Predictions of the daily exchange rate for the first series, in the first window.
    Image by the author.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个序列在第一个窗口中的每日汇率预测。图片由作者提供。
- en: This figure is a bit underwhelming, as N-BEATS and N-HiTS seem to have predictions
    that are very off from the actual values. However, PatchTST, while also off, seems
    to be the closest to the actual values.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图有点让人失望，因为N-BEATS和N-HiTS的预测结果与实际值差异很大。然而，尽管PatchTST也有偏差，但似乎最接近实际值。
- en: Of course, we must takes this with a grain of salt, because we are only visualizing
    the prediction for one series, in one prediction window.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们必须对此持保留态度，因为我们只是可视化了一个序列中的一个预测窗口。
- en: Evaluation
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估
- en: So, let’s evaluate the performance of each model. To replicate the methodology
    from the paper, we use both the MAE and MSE as performance metrics.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们评估每个模型的表现。为了复制论文中的方法，我们使用MAE和MSE作为性能指标。
- en: '[PRE11]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/0c6660f2c34408d4bb5c2f7e1b609783.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c6660f2c34408d4bb5c2f7e1b609783.png)'
- en: Performance of all models. Here, PatchTST achieves the lowest MAE and MSE. Image
    by the author.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 所有模型的性能。在这里，PatchTST实现了最低的MAE和MSE。图片由作者提供。
- en: In the table above, we see that PatchTST is the champion model as it achieves
    the lowest MAE and MSE.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在上表中，我们看到PatchTST是冠军模型，因为它实现了最低的MAE和MSE。
- en: Of course, this was not the most thorough experiment, as we only used one dataset
    and one forecast horizon. Still, it is interesting to see that a Transformer-based
    model can compete with state-of-the-art MLP models.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这不是最彻底的实验，因为我们只使用了一个数据集和一个预测范围。尽管如此，看到一个基于Transformer的模型能够与最先进的MLP模型竞争仍然很有趣。
- en: Conclusion
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: PatchTST is a Transformer-based models that uses patching to extract local semantic
    meaning in time series data. This allows the model to be faster to train and to
    have a longer input window.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: PatchTST是基于Transformer的模型，通过补丁来提取时间序列数据中的局部语义意义。这使得模型的训练速度更快，并且具有更长的输入窗口。
- en: It has achieved state-of-the-art performances when compared to other Transformer-based
    models. In our little exercise, we saw that it also achieved better performances
    than N-BEATS and N-HiTS.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于其他基于Transformer的模型，它已经达到了最先进的性能。在我们的练习中，我们看到它的表现也优于N-BEATS和N-HiTS。
- en: While this does not mean that it is better than N-HiTS or N-BEATS, it remains
    an interesting option when forecasting on a long horizon.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这并不意味着它比N-HiTS或N-BEATS更好，但它仍然是一个有趣的选项，尤其是在长期预测时。
- en: Thanks for reading! I hope that you enjoyed it and that you learned something
    new!
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！希望你喜欢这篇文章，并且学到了一些新东西！
- en: Looking to master time series forecasting? The check out [Applied Time Series
    Forecasting in Python](https://www.datasciencewithmarco.com/offers/zTAs2hi6/checkout?coupon_code=ATSFP10).
    This is the only course that uses Python to implement statistical, deep learning
    and state-of-the-art models in 15 guided hands-on projects.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 想要掌握时间序列预测？那就看看[Python应用时间序列预测](https://www.datasciencewithmarco.com/offers/zTAs2hi6/checkout?coupon_code=ATSFP10)。这是唯一一个通过15个引导实践项目使用Python实现统计学、深度学习和最先进模型的课程。
- en: Cheers 🍻
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 干杯 🍻
- en: Support me
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持我
- en: Enjoying my work? Show your support with [Buy me a coffee](http://buymeacoffee.com/dswm),
    a simple way for you to encourage me, and I get to enjoy a cup of coffee! If you
    feel like it, just click the button below 👇
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 喜欢我的工作吗？通过[给我买杯咖啡](http://buymeacoffee.com/dswm)来表示支持，这是鼓励我的简单方式，我可以享受一杯咖啡！如果你愿意，请点击下面的按钮
    👇
- en: '[![](../Images/7ad9438bd50b1698fdd722fa6661b16c.png)](http://buymeacoffee.com/dswm)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/7ad9438bd50b1698fdd722fa6661b16c.png)](http://buymeacoffee.com/dswm)'
- en: References
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[A Time Series is Worth 64 Words: Long-Term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf)
    by Nie Y., Nguyen N. et al.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[时间序列的价值为64个词：使用Transformer进行长期预测](https://arxiv.org/pdf/2211.14730.pdf) 作者：Nie
    Y., Nguyen N. 等。'
- en: '[Neuralforecast](https://nixtla.github.io/neuralforecast/) by Olivares K.,
    Challu C., Garza F., Canseco M., Dubrawski A.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[Neuralforecast](https://nixtla.github.io/neuralforecast/) 作者：Olivares K.,
    Challu C., Garza F., Canseco M., Dubrawski A.'
