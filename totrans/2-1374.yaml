- en: It’s a Mistake to Trust the Best Model of a GridSearchCV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/its-a-mistake-to-trust-the-best-model-of-a-gridsearchcv-536a73e835ad](https://towardsdatascience.com/its-a-mistake-to-trust-the-best-model-of-a-gridsearchcv-536a73e835ad)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Explained through four examples where the “best model” isn’t actually the best
    model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@tomergabay?source=post_page-----536a73e835ad--------------------------------)[![Tomer
    Gabay](../Images/1fb1d408bc89415918c1aa6733df44e1.png)](https://medium.com/@tomergabay?source=post_page-----536a73e835ad--------------------------------)[](https://towardsdatascience.com/?source=post_page-----536a73e835ad--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----536a73e835ad--------------------------------)
    [Tomer Gabay](https://medium.com/@tomergabay?source=post_page-----536a73e835ad--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----536a73e835ad--------------------------------)
    ·6 min read·Jan 4, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e21cbcd59ca104b1b7dcf6a964510800.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Choong Deng Xiang](https://unsplash.com/@dengxiangs?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Scikit-learn’s `GridSearchCV` is an often-used tool to optimize the hyperparameters
    of a machine-learning model. Unfortunately, not everyone is analyzing its output
    thoroughly, and simply uses the `GridSearchCV’s`best estimator. This means in
    a lot of situations you might not be using the actual *best* estimator. Let’s
    first determine how to run a grid search and detect which estimator it chooses
    as its best.
  prefs: []
  type: TYPE_NORMAL
- en: 'A very basic `GridSearchCV` could look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In the bottom lines of the code above I warn about using the `best_estimator_`.
    Therefore it is important to understand how *scikit-learn* chooses the best estimator
    of a `GridSearchCv` . We only need to look at one column of the `results`data
    frame generated with `cv_results_`to determine why a model has been selected as
    the best: `mean_test_score` :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e3f72e88ebdad4f2b3b8c0c79dcd5ee0.png)'
  prefs: []
  type: TYPE_IMG
- en: (table by Author)
  prefs: []
  type: TYPE_NORMAL
- en: '`mean_test_score` displays the average score of that model on the test sets;
    e.g. by default parameter `cv=5` which means the `mean_test_score` is the average
    of the model’s test score for 5 test sets. If you would change `cv` to 4 you’d
    get the following train-test data division:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d5cc913b44aad71b6fc4b0a933323760.png)'
  prefs: []
  type: TYPE_IMG
- en: Cross-validation with cv=4 (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: By default, `GridSearchCV` picks the model with the highest `mean_test_score`
    and assigns it a `rank_test_score` of 1\. This also means that when you access
    a `GridSearchCV’s` best estimator through `gs.best_estimator_`you will use the
    model with a `rank_test_score`of 1\. However, there are many cases when the model
    with a `rank_test_score` of 1 isn’t actually the best model. Let’s illustrate
    this through four examples where the ‘best’ model isn’t the best model, and see
    how we can determine what the actual best model is.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example #1: test score’s standard deviation'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/f1e025803d12771b816ff29dd1d187a3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we take a look at the table above we can see that the ‘best’ model has a
    `mean_test_score` of -100 and a `std_test_score` of 21, which are expressed in
    the [*mean absolute error*](https://en.wikipedia.org/wiki/Mean_absolute_error).
    Because this is an error score, the closer the value is to zero, the better the
    score. The ‘second best’ model has a `mean_test_score` of -102 and a `std_test_score`
    of 5\. `std_test_score` stands for the standard deviation of the scores of the
    model on the test set, and this is an absolute value, hence, the closer to 0 the
    more consistent the model performs. Even though the #1 model has a slightly better
    mean, its standard deviation is much larger. This means that the performance of
    the model is less consistent and less reliable and most often this is not ideal.
    That’s why in this example I would pick model #2 over model #1, and in most use
    cases you should too.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example #2: train scores'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s now consider two models which perform nearly identically on the test
    set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dcdd0597b82bafb3ebaf8b04e6c4b75b.png)'
  prefs: []
  type: TYPE_IMG
- en: Two seemingly identical decision trees (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Even though these models seem to be nearly equal in performance, they are not,
    but to be able to detect that we have to set the `return_train_score` parameter
    of `GridSearchCV` from its default value `False` to `True:`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now if we run `gridsearch.cv_results_` after fitting and training, we notice
    there are multiple columns added, of which one is displayed below: `mean_train_score`
    :'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7fd7f6f20ae43583ee03fac768a66bdb.png)'
  prefs: []
  type: TYPE_IMG
- en: cv_results_ with train scores (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: What we can notice now is that even though both models seem to perform equally
    on the test set, the second model is actually significantly better on the train
    set compared to the first model. Of course, this could imply overfitting in some
    situations, but here the mean train score aligns better with the mean test score
    and therefore doesn’t seem to imply overfitting. Generally speaking, when two
    models perform similarly on the test sets, the model which performs more consistently
    when also taking the train scores into account should be preferred.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example #3: Model Complexity'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s consider the following grid search results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a20a1e2c444bd705122c0b56d993fe6e.png)'
  prefs: []
  type: TYPE_IMG
- en: cv_results_ (Image by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Above we have two models with nearly identical test and train scores. Yet, there
    is more important information to consider before confirming the first model to
    be the best one. In the last column, we see `param_tree__max_depth` , which shows
    the tree depth of each model’s decision tree. The ‘best’ model’s decision tree
    has a tree depth of 50, while the ‘second best’ decision tree has a tree depth
    of just 2\. In this case, we could choose the second model to be the best model,
    because this decision tree is much better interpretable. E.g., we could plot the
    tree using `[sklearn.tree.plot_tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html)`
    and see a very simple and easy-to-interpret tree. A tree with a depth of 50 is
    hardly readable and interpretable at all. As a rule of thumb, a less complex model
    should be preferred over a more complex model when two models perform equally
    on both the test set and the train set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example #4: Model Speed Performance'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The last example I’d like to show is one of speed performance. Let’s assume
    we have the same grid search results as in example #3 but with one added column:
    `mean_score_time.`'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9084cd7a094846ab02754b148d75848b.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, another reason appears why we could prefer the second model over the first
    model. The first model takes more than 6 times as long as the second model, probably
    due to its extra complexity. Of course, as `mean_score_time` expresses the mean
    time it took the model to predict a validation set, this difference seems neglectable.
    However, imagine a model that needs to predict machine failure based on hundreds
    of sensors per machine. For such a model that needs to make live streaming predictions
    on huge amounts of data, a difference between 0.02 seconds on *n* observations
    versus 0.003 seconds on *n* observations can make a significant difference*.*
  prefs: []
  type: TYPE_NORMAL
- en: To Conclude
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we’ve seen four examples that show why you should never blindly
    trust a *scikit-learn*’s `GridSearchCV's` best estimator. Rather than just relying
    on the mean test score, we should also consider other columns of the cross-validation
    results to determine which model is the best, especially when the top models’
    test scores are similar.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to learn more about machine-learning and/or grid searching make
    sure to read these other articles about it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/significantly-increase-your-grid-search-results-with-these-parameters-b096b3d158aa?source=post_page-----536a73e835ad--------------------------------)
    [## Significantly Increase Your Grid-Search Results With These Parameters'
  prefs: []
  type: TYPE_NORMAL
- en: Grid search over any machine learning pipeline step using an EstimatorSwitch.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/significantly-increase-your-grid-search-results-with-these-parameters-b096b3d158aa?source=post_page-----536a73e835ad--------------------------------)
    [](/a-highly-anticipated-time-series-cross-validator-is-finally-here-7dc99f672736?source=post_page-----536a73e835ad--------------------------------)
    [## A highly anticipated Time Series Cross-validator is finally here
  prefs: []
  type: TYPE_NORMAL
- en: Unevenly spread Time Series data is no longer a problem for cross-validation.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/a-highly-anticipated-time-series-cross-validator-is-finally-here-7dc99f672736?source=post_page-----536a73e835ad--------------------------------)
  prefs: []
  type: TYPE_NORMAL
