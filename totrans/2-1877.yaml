- en: 'Sklearn Tutorial: Module 3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/sklearn-tutorial-module-3-08c9ae5cb8fa](https://towardsdatascience.com/sklearn-tutorial-module-3-08c9ae5cb8fa)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: I took the official sklearn MOOC tutorial. Here are my takeaways.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://mocquin.medium.com/?source=post_page-----08c9ae5cb8fa--------------------------------)[![Yoann
    Mocquin](../Images/b30a0f70c56972aabd2bc0a74baa90bb.png)](https://mocquin.medium.com/?source=post_page-----08c9ae5cb8fa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----08c9ae5cb8fa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----08c9ae5cb8fa--------------------------------)
    [Yoann Mocquin](https://mocquin.medium.com/?source=post_page-----08c9ae5cb8fa--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----08c9ae5cb8fa--------------------------------)
    ·9 min read·Dec 1, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the third post in my scikit-learn tutorial series. If you didn’t catch
    it, I strongly recommend my first two posts — it’ll be way easier to follow along:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/sklearn-tutorial-module-1-f31b3964a3b4?source=post_page-----08c9ae5cb8fa--------------------------------)
    [## Sklearn Tutorial: Module 1'
  prefs: []
  type: TYPE_NORMAL
- en: I took the official sklearn MOOC tutorial. Here are my takeaways.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'towardsdatascience.com](/sklearn-tutorial-module-1-f31b3964a3b4?source=post_page-----08c9ae5cb8fa--------------------------------)
    [](/sklearn-tutorial-module-2-0739c44f595a?source=post_page-----08c9ae5cb8fa--------------------------------)
    [## Sklearn tutorial: module 2'
  prefs: []
  type: TYPE_NORMAL
- en: I took the official sklearn MOOC tutorial. Here are my takeaways.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/sklearn-tutorial-module-2-0739c44f595a?source=post_page-----08c9ae5cb8fa--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: In this third module, we’ll see what hyperparameters are, and why and how we
    should optimize them.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/78274fa0cd73bdb11c003e4596c49c6a.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Glenn Carstens-Peters](https://unsplash.com/@glenncarstenspeters?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: What’s a hyperparameter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When setting up our model so far, we only changed either the preprocessing,
    the kind of model, or both — but we haven’t really played with the model’s hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: A model’s hyperparameters are parameters that are set by us, data scientists,
    when creating our model/pipeline. They are parameters that define the model before
    it sees any data. You could say that they allow us to define different “variants”
    of the same pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameters typically influence the model’s complexity, and as a consequence,
    the learning process and the overall model performance. Given a dataset and the
    problem you want to solve, your job as a data scientist is to find the best “hyper-parametrized
    model” among the infinite space of “hyperparametrized models.”
  prefs: []
  type: TYPE_NORMAL
- en: The hyperparameters are not to be confused with the internal parameters that
    are learned by the model during the learning process — those internal parameters
    that are learned are also called “coefficients.” For example, in polynomial regression,
    the hyperparameter (set before learning) is the degree of the regression, while
    the internal parameters learned using the train set are the polynomial coefficients
    (the a/b/c in aX² + bX + c). Put another way, you first set the degree (hyperparameter),
    and then the regression fit is done using the data (internal coefficients are
    learned) — not the other way around.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a consequence, model hyperparameters can be set when model/preprocessors
    are created. For example, in scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '`PolynomialFeatures(degree=degree)`: the polynomial degree created from each
    feature'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Ridge(alpha=5)`: regularization term of the linear ridge regression'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SVC(C=1.0, kernel="rbf")`: regularization parameter and kernel for a support
    vector classifier. Depending on the kernel chosen, additionnal hyperparameters
    are available'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`KNeighborsClassifier(n_neighbhors=5)`: the number of neighbors considered
    in a K-nearest neighbors classifier'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StandarScaler(with_mean=True, with_std=True)`: the standard scaler preprocessor
    can also be tuned with its hyperparameters, wheter to remove the mean and/or divide
    by the standard deviation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Those examples show that the available hyperparameters depend on the whole
    pipeline you use for your model. For example the following pipeline has the hyperparameters
    of both the scaler and the regressor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As we will see below, hyperparameters can also be read and set after a pipeline
    has been created. We will even see that individual steps can be considered as
    hyperparameters (for example, the “kind” of the scaler preprocessor, with possible
    values “StandardScaler”, “MinMaxScaler”, etc).
  prefs: []
  type: TYPE_NORMAL
- en: Note that for a given dataset, just like a certain kind of model could outperform
    another one — a hyperparameter could outperform another one. Put another way,
    for every dataset, there is an optimum hyperparameter set.
  prefs: []
  type: TYPE_NORMAL
- en: 'So remember:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hyperparameters correspond to parameters you set when creating the model,
    before the model is fed with a dataset.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**They correspond to every parameter you can set when creating a pipeline,
    given each step in the pipeline.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The optimal hyperparameter set depends on the goal of the ML exercise and
    the input dataset.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Our job is to find the best hyperparameter.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rest of this post explains how to access and modify hyperparameters of models,
    and the different ways to search and optimize such hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'How to get/set hyperparameters of a pipeline/model:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In sklearn, once a model or pipeline has been created, an API is available
    to:'
  prefs: []
  type: TYPE_NORMAL
- en: list the hyperparameters available and their respective values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: change their value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**For a given model, you can get the list of all the hyperparameters and their
    values with the**`**.get_params()**` **method**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Several important things are to be noticed:'
  prefs: []
  type: TYPE_NORMAL
- en: '`.get_params()` return a dict, including a `steps` entry that contains the
    list of the steps of the pipeline'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the names used when creating the pipeline, `preprocecssor` and `lin_reg`in our
    case, are used and stored in this parameter dict
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: as such, each step’s hyperparameters are named using the conventions `<step_name>__<parameter_name>`,
    with a double underscore between the step’s name and the step’s parameter’s name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the API interface to be consistent, note that *all* parameters are returned
    in this dict, including some hyperparameters that do not have an impact of the
    performance (like `lin_reg__n_jobs` and `preprocessor__copy`).
  prefs: []
  type: TYPE_NORMAL
- en: '**Similarly, we can change the value of any of those parameters using the following
    consistent API with** `**set_params(name=value)**`**:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As mentioned before, we can even change a step completely using the same API:
    here we changed from a StandardScaler to a MinMaxScaler preprocessor. Notice the
    differences in parameters available after the change of preprocessor type (still
    called ‘preprocessor,’ but the corresponding hyperparameters are those of a MinMaxScaler).'
  prefs: []
  type: TYPE_NORMAL
- en: Manual hyperparameter tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know what hyperparameters are, how to get/set them, and why we should
    optimize them, let’s take a first approach to do such optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 'As for any optimization problem, we identify:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The “space” we want to explore: this is all the hyperparameter values we want
    to try.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The value we want to optimize: here it corresponds to the model’s performance
    through its score.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The simplest and most inefficient, non-robust way to do such optimization is
    using a loop on one hyperparameter and using the score of a single train/test
    split:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: So in this first approach, we manually write a loop, in which the pipe is fitted
    and tested. **A first improvement we can do is to use cross-validation in order
    to compute a more meaningfull score:**
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Using cross-validation, we have a more robust estimation of the model’s performance
    for each value of the hyperparameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s improve further with optimisation of 2 hyperparameters: we have to
    nest 2 loops, one for each hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now, what if we want to optimize on 3, 4, 10, or more hyperparameters? And what
    if we want to try 10 different values for each of those hyperparameters? We’d
    have to write many, many nested loops and inspect many, many scores.
  prefs: []
  type: TYPE_NORMAL
- en: '**This is why scikit-learn provides helper functions to automate this hyperparameter
    searching process, like GridSearchCV and RandomSearchCV.**'
  prefs: []
  type: TYPE_NORMAL
- en: Automatic tuning using GridSearch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first automatic approach provided by sklearn to optimize hyperparameters
    is called `GridSearchCV`. **The idea is to use a dict to specify all values for
    each single hyperparameters, and all combinations will be tested**. For example,
    to reproduce the example above where `with_mean`could be `[True, False]` and `with_std`
    could be `[True, False]`we’d use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This first snippet only creates a `model` : yes, a new model, that wraps the
    true low-level pipeline. This new grid-seach model can be fitted, again using
    `model_grid_search.fit`. During this fitting step, all combinations of hyperparameters
    are tested and the model performance computed using cross-validation. Once the
    grid-search is fitted, it can be used as any other predictor (by calling predict
    or score for example), using the model with the best parameters found during fit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: So in other words, fitting a GridSearch model means trying all combinations
    and keeping the best one.
  prefs: []
  type: TYPE_NORMAL
- en: 'An important feature to remember is that we can use a list of dictionaries
    instead of just a dictionary to specify the combinations we want to try, in order
    to refine the hyperparameter sets that should be tested. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Stochastic tuning with RandomizedSearchCV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When the hyperparameters are continuous-valued and live on a great range of
    values, and/or the number of hyperparameters to tune is important, and/or the
    model is computationally complex, the full combination approach of GridSearchCV
    shows its limitations: the fitting time starts to increase. There is obviously
    a tradeoff between the number of hyperparameter sets to test versus the total
    time.'
  prefs: []
  type: TYPE_NORMAL
- en: To circumvent these limitations and improve our chances to find a good — if
    not the best — hyperparameter set, we can use a random approach to sample the
    hyperparameters space.
  prefs: []
  type: TYPE_NORMAL
- en: The idea is to specify all the possible values for all the hyperparameters and
    try sets at random.
  prefs: []
  type: TYPE_NORMAL
- en: Using a random approach to optimize numerical problems is a common trick, like
    for numerical integration or optimization problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do so in sklearn, we use `RandomizedSearchCV` — the usage is exactly the
    same as `GridSearchCV`. For example, say we want to optimize a support vector
    classifier by tuning its C parameter which can have any value from 0 to infinity,
    as well as some other hyperparameters like kernel and gamma:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Here, we allow the search to try 1000 hyperparameter sets, using `n_iter` to
    control the number of tries.
  prefs: []
  type: TYPE_NORMAL
- en: '**So remember: the randomized approach allows trying hyperparameters randomly
    and controlling the number of tries using n_iter. This approach is useful when
    some hyperparameters are continuously-valued and/or may take a wide range of values.**'
  prefs: []
  type: TYPE_NORMAL
- en: Nested cross-validation pattern
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to train and find the best the best-hyperparameter model found using
    `GridSearchCV`/`RandomizedSearchCV`, we use the original train set from the first
    split. This first-split train set was used internally using another train/test
    split. In other words:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First split: the original dataset is split into X_train and X_test.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then X_train is used to optimize the hyperparameters by training/testing each
    hyperparameter set using N internal splits (the number of folds): so X_train is
    split n-times into another X_train/X_test. The models are fitted/tested for each
    hyperparameter set, and the model performance is evaluated using cross-validation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the best model found is tested and evaluated on the original X_test
    set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This means that this approach only provides us with a single evaluation of the
    generalization performance, since only the original X_test set is retained and
    never used in the learning steps (both fitting and optimizing). In order to improve
    our estimation of the generalization performance, we can use an outer cross-validation
    loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'So remember: the outer loop is used to estimate the generalization performance
    of the overall fitting/optimizing process. In other words, the estimated best
    model’s performance is evaluated using cross-validation.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This way, we get the best of both worlds, at the price of additional computation.
  prefs: []
  type: TYPE_NORMAL
- en: Wrapup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This third module was dedicated to hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameters are parameters that define the way the model works and learns;
    they define its complexity. They should not be confused with the internal coefficients
    learned by the model when it is presented with the train set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since these hyperparameters greatly influence the model, they must be optimized
    to improve the model’s performance for the given task at stake. The best hyperparameters
    depend on the input data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing hyperparameters can be done using cross-validated search methods
    like grid-search and random-search.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A good practice when optimizing hyperparameters is to use the nested-cross-validation
    pattern to estimate the best-fitted model performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You might like some of my other posts, make sure to check them out:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Yoann Mocquin](../Images/234a99f243ff3c70fd90170ddde8659d.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Yoann Mocquin](https://mocquin.medium.com/?source=post_page-----08c9ae5cb8fa--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Sklearn tutorial
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://mocquin.medium.com/list/sklearn-tutorial-2e46a0e06b39?source=post_page-----08c9ae5cb8fa--------------------------------)9
    stories![](../Images/4ffe6868fb22c241a959bd5d5a9fd5d7.png)![](../Images/8aa32b00faa0ef7376e121ba9c9ffdb7.png)![](../Images/9f986423d7983bc08fc2073534603c35.png)![Yoann
    Mocquin](../Images/234a99f243ff3c70fd90170ddde8659d.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Yoann Mocquin](https://mocquin.medium.com/?source=post_page-----08c9ae5cb8fa--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Scientific/numerical python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://mocquin.medium.com/list/scientificnumerical-python-9ce115122ab6?source=post_page-----08c9ae5cb8fa--------------------------------)3
    stories![Ironicaly, an array of containers](../Images/4ecd0326a3efdda93947f60872018d41.png)![](../Images/f11076a724463f7b11d819d95bcf0ea4.png)![](../Images/e340b22f444d2bd311537341cf1a105a.png)![Yoann
    Mocquin](../Images/234a99f243ff3c70fd90170ddde8659d.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Yoann Mocquin](https://mocquin.medium.com/?source=post_page-----08c9ae5cb8fa--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Data science and Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://mocquin.medium.com/list/data-science-and-machine-learning-ba3fb2206051?source=post_page-----08c9ae5cb8fa--------------------------------)3
    stories![](../Images/c078e74fd67e0141c2b54b82823c78d4.png)![](../Images/79988eda04a078da9373f03d7db51c51.png)![](../Images/6a5966e529bf4ba9b16c592fec7b591a.png)![Yoann
    Mocquin](../Images/234a99f243ff3c70fd90170ddde8659d.png)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Yoann Mocquin](https://mocquin.medium.com/?source=post_page-----08c9ae5cb8fa--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Fourier-transforms for time-series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://mocquin.medium.com/list/fouriertransforms-for-timeseries-ed423e3f38ad?source=post_page-----08c9ae5cb8fa--------------------------------)4
    stories![](../Images/86efd63d329650eb9b6d7c33625d6884.png)![](../Images/c693e4e596df5c1a8ef1b0fb3777d7ac.png)![](../Images/b6bc5330fb2d92bc3aad36f5bbc950da.png)'
  prefs: []
  type: TYPE_NORMAL
