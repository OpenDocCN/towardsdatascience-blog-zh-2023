- en: XGBoost Now Supports MAE as an Objective
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/xgboost-now-support-mae-as-objective-fdbbe18e173](https://towardsdatascience.com/xgboost-now-support-mae-as-objective-fdbbe18e173)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How is that possible, as MAE is non-smooth?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@guillaume.saupin?source=post_page-----fdbbe18e173--------------------------------)[![Saupin
    Guillaume](../Images/d9112d3cdfe6f335b6ff2c875fba6bb5.png)](https://medium.com/@guillaume.saupin?source=post_page-----fdbbe18e173--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fdbbe18e173--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fdbbe18e173--------------------------------)
    [Saupin Guillaume](https://medium.com/@guillaume.saupin?source=post_page-----fdbbe18e173--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fdbbe18e173--------------------------------)
    ·5 min read·Jan 18, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/929fde4fb46becfb224284a1e3507976.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Ajay Karpur](https://unsplash.com/@ajaykarpur?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: When working on a model based on Gradient Boosting, a key parameter to choose
    from is the objective. Indeed, the whole building process of the decision tree
    derives from the objective and its first and second derivatives.
  prefs: []
  type: TYPE_NORMAL
- en: 'XGBoost has recently introduced support for a new kind of objective: non-smooth
    objectives with no second derivative. Amongst them, the famous **MAE** (mean absolute
    error) is now natively activable inside XGBoost.'
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we will detail how XGBoost has been modified to handle this kind
    of objective.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Boosting needs smooth objective
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'XGBoost, LightGBM, and CatBoost all share a common limitation: they need smooth
    (mathematically speaking) objectives to compute the optimal weights for the leaves
    of the decision trees.'
  prefs: []
  type: TYPE_NORMAL
- en: This is not true anymore for XGBoost, which has recently introduced, support
    for the MAE using line search, starting with release 1.7.0
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re willing to master Gradient Boosting in detail, have a look at my
    book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://amzn.to/3CTMDuH?source=post_page-----fdbbe18e173--------------------------------)
    [## Practical Gradient Boosting: A deep dive into Gradient Boosting in Python'
  prefs: []
  type: TYPE_NORMAL
- en: This book on Gradient Boosting methods is intended for students, academics,
    engineers, and data scientists who wish …](https://amzn.to/3CTMDuH?source=post_page-----fdbbe18e173--------------------------------)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Gradient and Hessian are the core of Gradient Boosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The core of gradient boosting-based methods is the idea of applying descent
    gradient to functional space instead of parameter space.
  prefs: []
  type: TYPE_NORMAL
- en: As a reminder, the core of the method is to linearize an objective function
    around the previous prediction `t-1`, and to add a small increment that minimizes
    this objective.
  prefs: []
  type: TYPE_NORMAL
- en: This small increment is expressed in the functional space, and it is a new binary
    node represented by the function `f_t.`
  prefs: []
  type: TYPE_NORMAL
- en: 'This objective combines a loss function `l` with a regularization function
    Ω:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1af64dc9bf2cc7b01f99b2a2f7174565.png)'
  prefs: []
  type: TYPE_IMG
- en: Objective function. Formula by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once linearized, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ea320d4e3ee7154dc9c00586f5e81767.png)'
  prefs: []
  type: TYPE_IMG
- en: Objective function linearized near ŷ[t-1]. Formula by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/05390d425a0ceccae933ab8d708d7e89.png)'
  prefs: []
  type: TYPE_IMG
- en: First and second derivative. Formula by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Minimizing this linearized objective function boils down to reducing the constant
    part, i.e:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9052bac80de3182e95ad7cafc52438ae.png)'
  prefs: []
  type: TYPE_IMG
- en: Variable part of the objective to minimize. Formula by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the new stage of the model `f_t`is a binary decision node that will generate
    two values (its leaves) : `w_left` and `w_right`it is possible to reorganize the
    sum above as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5c34ea8f16f9d3c9e40812e896216665.png)'
  prefs: []
  type: TYPE_IMG
- en: Reorganize linearized objective. Formula by the author.
  prefs: []
  type: TYPE_NORMAL
- en: At this stage, minimizing the linearized objective simply implies finding the
    optimal weight `w_left` and `w_right` . As they are both implied in a simple second-order
    polynomial, the solution is well the known `-b/2a` expression where `b` is `G`
    and `a` is `1/2H` , hence for the left node, we get
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9edd28657903b085c6d58e2057d7bb9f.png)'
  prefs: []
  type: TYPE_IMG
- en: Formula for then optimal left weight. Formula by the author.
  prefs: []
  type: TYPE_NORMAL
- en: The exact same formula stands for the right weight.
  prefs: []
  type: TYPE_NORMAL
- en: Note the regularization parameter λ, which is an L2 regularisation term, proportional
    to the square of the weight.
  prefs: []
  type: TYPE_NORMAL
- en: MAE has no second derivative
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The issue with the **Mean Absolute Error** is that is it’s second derivative
    is null, hence `H` is zero.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One possible option to circumvent this limitation is to regularize this function.
    This means substituting this formula with another one that has the property of
    being at least twice derivable. See my article below that shows how to do that
    with the `logcosh` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/confidence-intervals-for-xgboost-cac2955a8fde?source=post_page-----fdbbe18e173--------------------------------)
    [## Confidence intervals for XGBoost'
  prefs: []
  type: TYPE_NORMAL
- en: Build a regularized Quantile Regression Objective to get HP tuning free confidence
    intervals
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/confidence-intervals-for-xgboost-cac2955a8fde?source=post_page-----fdbbe18e173--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Line search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another option, the one recently introduced by XGBoost since its release 1.7.0,
    is the use of an iterative method for finding the best weight for each node.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do so, the current XGBoost implementation uses a trick:'
  prefs: []
  type: TYPE_NORMAL
- en: First, it computes the leaf values as usual, simply forcing the second derivative
    to 1.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, once the whole tree is built, XGBoost updates the leaf values using an
    α-quantile
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you’re curious to see how this is implemented (and are not afraid of modern
    C++) the detail can be found [here](https://github.com/dmlc/xgboost/pull/7812).
    `UpdateTreeLeaf`, and more specifically `UpdateTreeLeafHost` the method of interest.
  prefs: []
  type: TYPE_NORMAL
- en: How to use it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It’s plain and simple: just pick a release of XGBoost that is greater than
    1.7.0 and use `objective: mae` as parameter.'
  prefs: []
  type: TYPE_NORMAL
- en: '[![](../Images/646ded91846ff6aa7fa8814985c9837d.png)](https://www.buymeacoffee.com/guillaumes0)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.buymeacoffee.com/guillaumes0](https://www.buymeacoffee.com/guillaumes0)'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: XGBoost has introduced a new way to cope with non-smooth objectives, like the
    MAE, that does not require the regularization of a function.
  prefs: []
  type: TYPE_NORMAL
- en: The MAE is a very convenient metric to use, as it is easy to understand. Moreover,
    it does not over penalize large errors as would the MSE. This is handy when trying
    to predict large as well as small values using the same model.
  prefs: []
  type: TYPE_NORMAL
- en: Being able to use a non-smooth objective is very appealing as it not only avoids
    the need for approximation but also opens the door to other non-smooth objectives
    like the MAPE.
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, a new feature to try and follow.
  prefs: []
  type: TYPE_NORMAL
- en: 'More on Gradient Boosting, XGBoost, LightGBM, and CaBoost in my book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://amzn.to/3CTMDuH?source=post_page-----fdbbe18e173--------------------------------)
    [## Practical Gradient Boosting: A deep dive into Gradient Boosting in Python'
  prefs: []
  type: TYPE_NORMAL
- en: This book on Gradient Boosting methods is intended for students, academics,
    engineers, and data scientists who wish to…](https://amzn.to/3CTMDuH?source=post_page-----fdbbe18e173--------------------------------)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
