- en: XGBoost Now Supports MAE as an Objective
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/xgboost-now-support-mae-as-objective-fdbbe18e173](https://towardsdatascience.com/xgboost-now-support-mae-as-objective-fdbbe18e173)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How is that possible, as MAE is non-smooth?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@guillaume.saupin?source=post_page-----fdbbe18e173--------------------------------)[![Saupin
    Guillaume](../Images/d9112d3cdfe6f335b6ff2c875fba6bb5.png)](https://medium.com/@guillaume.saupin?source=post_page-----fdbbe18e173--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fdbbe18e173--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fdbbe18e173--------------------------------)
    [Saupin Guillaume](https://medium.com/@guillaume.saupin?source=post_page-----fdbbe18e173--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fdbbe18e173--------------------------------)
    ·5 min read·Jan 18, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/929fde4fb46becfb224284a1e3507976.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: Photo by [Ajay Karpur](https://unsplash.com/@ajaykarpur?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: When working on a model based on Gradient Boosting, a key parameter to choose
    from is the objective. Indeed, the whole building process of the decision tree
    derives from the objective and its first and second derivatives.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: 'XGBoost has recently introduced support for a new kind of objective: non-smooth
    objectives with no second derivative. Amongst them, the famous **MAE** (mean absolute
    error) is now natively activable inside XGBoost.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: In this post, we will detail how XGBoost has been modified to handle this kind
    of objective.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Boosting needs smooth objective
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'XGBoost, LightGBM, and CatBoost all share a common limitation: they need smooth
    (mathematically speaking) objectives to compute the optimal weights for the leaves
    of the decision trees.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: This is not true anymore for XGBoost, which has recently introduced, support
    for the MAE using line search, starting with release 1.7.0
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re willing to master Gradient Boosting in detail, have a look at my
    book:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://amzn.to/3CTMDuH?source=post_page-----fdbbe18e173--------------------------------)
    [## Practical Gradient Boosting: A deep dive into Gradient Boosting in Python'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: This book on Gradient Boosting methods is intended for students, academics,
    engineers, and data scientists who wish …](https://amzn.to/3CTMDuH?source=post_page-----fdbbe18e173--------------------------------)
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Gradient and Hessian are the core of Gradient Boosting
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The core of gradient boosting-based methods is the idea of applying descent
    gradient to functional space instead of parameter space.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: As a reminder, the core of the method is to linearize an objective function
    around the previous prediction `t-1`, and to add a small increment that minimizes
    this objective.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: This small increment is expressed in the functional space, and it is a new binary
    node represented by the function `f_t.`
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: 'This objective combines a loss function `l` with a regularization function
    Ω:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1af64dc9bf2cc7b01f99b2a2f7174565.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
- en: Objective function. Formula by the author.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'Once linearized, we get:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ea320d4e3ee7154dc9c00586f5e81767.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
- en: Objective function linearized near ŷ[t-1]. Formula by the author.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'Where:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/05390d425a0ceccae933ab8d708d7e89.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: First and second derivative. Formula by the author.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'Minimizing this linearized objective function boils down to reducing the constant
    part, i.e:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9052bac80de3182e95ad7cafc52438ae.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
- en: Variable part of the objective to minimize. Formula by the author.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'As the new stage of the model `f_t`is a binary decision node that will generate
    two values (its leaves) : `w_left` and `w_right`it is possible to reorganize the
    sum above as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5c34ea8f16f9d3c9e40812e896216665.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
- en: Reorganize linearized objective. Formula by the author.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: At this stage, minimizing the linearized objective simply implies finding the
    optimal weight `w_left` and `w_right` . As they are both implied in a simple second-order
    polynomial, the solution is well the known `-b/2a` expression where `b` is `G`
    and `a` is `1/2H` , hence for the left node, we get
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9edd28657903b085c6d58e2057d7bb9f.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: Formula for then optimal left weight. Formula by the author.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: The exact same formula stands for the right weight.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Note the regularization parameter λ, which is an L2 regularisation term, proportional
    to the square of the weight.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: MAE has no second derivative
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The issue with the **Mean Absolute Error** is that is it’s second derivative
    is null, hence `H` is zero.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One possible option to circumvent this limitation is to regularize this function.
    This means substituting this formula with another one that has the property of
    being at least twice derivable. See my article below that shows how to do that
    with the `logcosh` :'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '[](/confidence-intervals-for-xgboost-cac2955a8fde?source=post_page-----fdbbe18e173--------------------------------)
    [## Confidence intervals for XGBoost'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Build a regularized Quantile Regression Objective to get HP tuning free confidence
    intervals
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/confidence-intervals-for-xgboost-cac2955a8fde?source=post_page-----fdbbe18e173--------------------------------)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Line search
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another option, the one recently introduced by XGBoost since its release 1.7.0,
    is the use of an iterative method for finding the best weight for each node.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: 'To do so, the current XGBoost implementation uses a trick:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: First, it computes the leaf values as usual, simply forcing the second derivative
    to 1.0
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, once the whole tree is built, XGBoost updates the leaf values using an
    α-quantile
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，一旦整个树构建完成，XGBoost 使用 α-分位数更新叶子值
- en: If you’re curious to see how this is implemented (and are not afraid of modern
    C++) the detail can be found [here](https://github.com/dmlc/xgboost/pull/7812).
    `UpdateTreeLeaf`, and more specifically `UpdateTreeLeafHost` the method of interest.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解如何实现（并且不怕现代 C++），详细信息可以在[这里](https://github.com/dmlc/xgboost/pull/7812)找到。关注`UpdateTreeLeaf`，更具体地说是`UpdateTreeLeafHost`方法。
- en: How to use it
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用它
- en: 'It’s plain and simple: just pick a release of XGBoost that is greater than
    1.7.0 and use `objective: mae` as parameter.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '很简单：只需选择一个版本大于 1.7.0 的 XGBoost，并使用 `objective: mae` 作为参数。'
- en: '[![](../Images/646ded91846ff6aa7fa8814985c9837d.png)](https://www.buymeacoffee.com/guillaumes0)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/646ded91846ff6aa7fa8814985c9837d.png)](https://www.buymeacoffee.com/guillaumes0)'
- en: '[https://www.buymeacoffee.com/guillaumes0](https://www.buymeacoffee.com/guillaumes0)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.buymeacoffee.com/guillaumes0](https://www.buymeacoffee.com/guillaumes0)'
- en: Conclusion
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: XGBoost has introduced a new way to cope with non-smooth objectives, like the
    MAE, that does not require the regularization of a function.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 引入了一种新的方法来处理非平滑目标，如 MAE，这不需要对函数进行正则化。
- en: The MAE is a very convenient metric to use, as it is easy to understand. Moreover,
    it does not over penalize large errors as would the MSE. This is handy when trying
    to predict large as well as small values using the same model.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: MAE 是一个非常方便的指标，因为它容易理解。此外，它不像 MSE 那样过度惩罚大错误。这在使用同一模型预测大值和小值时非常有用。
- en: Being able to use a non-smooth objective is very appealing as it not only avoids
    the need for approximation but also opens the door to other non-smooth objectives
    like the MAPE.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 能够使用非平滑目标函数非常吸引人，因为它不仅避免了对函数进行近似的需求，还为其他非平滑目标函数（如 MAPE）打开了大门。
- en: Clearly, a new feature to try and follow.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这是一个值得尝试并关注的新特性。
- en: 'More on Gradient Boosting, XGBoost, LightGBM, and CaBoost in my book:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于梯度提升、XGBoost、LightGBM 和 CaBoost 的内容，请参见我的书：
- en: '[](https://amzn.to/3CTMDuH?source=post_page-----fdbbe18e173--------------------------------)
    [## Practical Gradient Boosting: A deep dive into Gradient Boosting in Python'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://amzn.to/3CTMDuH?source=post_page-----fdbbe18e173--------------------------------)
    [## 实用梯度提升：深入探讨 Python 中的梯度提升'
- en: This book on Gradient Boosting methods is intended for students, academics,
    engineers, and data scientists who wish to…](https://amzn.to/3CTMDuH?source=post_page-----fdbbe18e173--------------------------------)
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本书关于梯度提升方法，旨在为希望深入了解梯度提升的学生、学者、工程师和数据科学家提供帮助…](https://amzn.to/3CTMDuH?source=post_page-----fdbbe18e173--------------------------------)
