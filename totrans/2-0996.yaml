- en: Google Pub/Sub to BigQuery the Simple Way
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç®€å•çš„ Google Pub/Sub åˆ° BigQuery æ–¹æ³•
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/google-pub-sub-to-bigquery-the-simple-way-de116234fb87](https://towardsdatascience.com/google-pub-sub-to-bigquery-the-simple-way-de116234fb87)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/google-pub-sub-to-bigquery-the-simple-way-de116234fb87](https://towardsdatascience.com/google-pub-sub-to-bigquery-the-simple-way-de116234fb87)
- en: A hands-on guide to implementing BigQuery Subscriptions in Pub/Sub for simple
    message and streaming ingestion
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®æ–½ BigQuery è®¢é˜…çš„åŠ¨æ‰‹æŒ‡å—ï¼Œé€‚ç”¨äº Pub/Sub ç®€å•æ¶ˆæ¯å’Œæµå¼æ•°æ®æ‘„å–
- en: '[](https://jim-barlow.medium.com/?source=post_page-----de116234fb87--------------------------------)[![Jim
    Barlow](../Images/1494580717cb92defb17328e4bae1b13.png)](https://jim-barlow.medium.com/?source=post_page-----de116234fb87--------------------------------)[](https://towardsdatascience.com/?source=post_page-----de116234fb87--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----de116234fb87--------------------------------)
    [Jim Barlow](https://jim-barlow.medium.com/?source=post_page-----de116234fb87--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://jim-barlow.medium.com/?source=post_page-----de116234fb87--------------------------------)[![Jim
    Barlow](../Images/1494580717cb92defb17328e4bae1b13.png)](https://jim-barlow.medium.com/?source=post_page-----de116234fb87--------------------------------)[](https://towardsdatascience.com/?source=post_page-----de116234fb87--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----de116234fb87--------------------------------)
    [Jim Barlow](https://jim-barlow.medium.com/?source=post_page-----de116234fb87--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----de116234fb87--------------------------------)
    Â·8 min readÂ·Sep 21, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----de116234fb87--------------------------------)
    Â·é˜…è¯»æ—¶é•¿ 8 åˆ†é’ŸÂ·2023å¹´9æœˆ21æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/b31e6bb2ff00e2b71bba4bbff6d010fc.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b31e6bb2ff00e2b71bba4bbff6d010fc.png)'
- en: 'Googleâ€™s latest planet-scale data warehouse subscription-based streaming ingestion
    water-borne military capability: BigSub. In this case, the Pub never made it to
    General Availability, so you will have to get your pints elsewhere. Photo by [Thomas
    Haas](https://unsplash.com/@thomashaas?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Google æœ€æ–°çš„è¡Œæ˜Ÿè§„æ¨¡æ•°æ®ä»“åº“è®¢é˜…åŸºç¡€çš„æµå¼æ‘„å–æ°´ä¸‹å†›äº‹èƒ½åŠ›ï¼šBigSubã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒPub ä»æœªè¾¾åˆ°æ­£å¼å‘å¸ƒçŠ¶æ€ï¼Œå› æ­¤ä½ å¿…é¡»å»å…¶ä»–åœ°æ–¹å¯»æ‰¾ä½ çš„å•¤é…’ã€‚ç…§ç‰‡ç”±
    [Thomas Haas](https://unsplash.com/@thomashaas?utm_source=medium&utm_medium=referral)
    æä¾›ï¼Œæ¥æºäº [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Motivation
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŠ¨æœº
- en: I have encountered many situations in the past where I wanted to get Pub/Sub
    messages into a BigQuery table, but I never managed to find a particularly simple
    way of doing this.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è¿‡å»é‡åˆ°è¿‡è®¸å¤šæƒ…å†µï¼Œæˆ‘æƒ³å°† Pub/Sub æ¶ˆæ¯å¯¼å…¥ BigQuery è¡¨ä¸­ï¼Œä½†æˆ‘ä»æœªæ‰¾åˆ°ä¸€ç§ç‰¹åˆ«ç®€å•çš„æ–¹æ³•æ¥å®ç°è¿™ä¸€ç‚¹ã€‚
- en: You could set up a [dataflow pipeline](https://cloud.google.com/dataflow/docs/tutorials/dataflow-stream-to-bigquery),
    but this requires additional infrastructure to understand, configure, manage and
    debug. Plus Dataflow (which is a managed Apache Beam service) is designed for
    high-throughput streaming, so always seemed like overkill for a simple message
    logging or monitoring system.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥è®¾ç½®ä¸€ä¸ª [dataflow pipeline](https://cloud.google.com/dataflow/docs/tutorials/dataflow-stream-to-bigquery)ï¼Œä½†è¿™éœ€è¦é¢å¤–çš„åŸºç¡€è®¾æ–½æ¥ç†è§£ã€é…ç½®ã€ç®¡ç†å’Œè°ƒè¯•ã€‚è€Œä¸”
    Dataflowï¼ˆä½œä¸ºä¸€ç§æ‰˜ç®¡çš„ Apache Beam æœåŠ¡ï¼‰è®¾è®¡ç”¨äºé«˜ååé‡æµå¼å¤„ç†ï¼Œå› æ­¤å¯¹äºç®€å•çš„æ¶ˆæ¯æ—¥å¿—è®°å½•æˆ–ç›‘æ§ç³»ç»Ÿæ¥è¯´æ€»æ˜¯æ˜¾å¾—è¿‡äºå¤æ‚ã€‚
- en: And itâ€™s Java. But Python ğŸ˜€! And Javaâ€¦ ğŸ˜«!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œä¸”å®ƒæ˜¯ Javaã€‚å¯æ˜¯ Python ğŸ˜€ï¼è€Œ Javaâ€¦ ğŸ˜«ï¼
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*Sorry, I still get flashbacks from my first attempts to learn to code (last
    century) in Java. Please do not attempt to use that code snippet â€¦ step away from
    the code snippet.*'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¯¹ä¸èµ·ï¼Œæˆ‘ä»ç„¶ä¼šå¯¹æˆ‘ç¬¬ä¸€æ¬¡å°è¯•åœ¨ Java ä¸­å­¦ä¹ ç¼–ç ï¼ˆä¸Šä¸ªä¸–çºªï¼‰æ—¶çš„ç»å†æ„Ÿåˆ°é—ªå›ã€‚è¯·ä¸è¦å°è¯•ä½¿ç”¨é‚£ä¸ªä»£ç ç‰‡æ®µâ€¦â€¦è¿œç¦»ä»£ç ç‰‡æ®µã€‚*'
- en: I then stumbled upon [this](https://medium.com/google-cloud/streaming-from-google-cloud-pub-sub-to-bigquery-without-the-middlemen-327ef24f4d15),
    which â€” although promising simplicity â€” seems to be even more complicated than
    the previous method (Debezium wtf?)!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘å¶ç„¶å‘ç°äº† [è¿™ä¸ª](https://medium.com/google-cloud/streaming-from-google-cloud-pub-sub-to-bigquery-without-the-middlemen-327ef24f4d15)ï¼Œè™½ç„¶å®ƒæ‰¿è¯ºç®€å•æ€§ï¼Œä½†ä¼¼ä¹æ¯”ä¹‹å‰çš„æ–¹æ³•ï¼ˆDebezium
    wtfï¼Ÿï¼‰è¿˜è¦å¤æ‚ï¼
- en: Itâ€™s also possible to deploy a lightweight Cloud Function to trigger on receipt
    of a Pub/Sub message and stream or load this into BigQuery, but this still seemed
    a little too complex for something which felt like it should and could have been
    native functionality.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿå¯ä»¥éƒ¨ç½²ä¸€ä¸ªè½»é‡çº§çš„ Cloud Function ä»¥åœ¨æ¥æ”¶åˆ° Pub/Sub æ¶ˆæ¯æ—¶è§¦å‘ï¼Œå¹¶å°†å…¶æµå¼ä¼ è¾“æˆ–åŠ è½½åˆ° BigQuery ä¸­ï¼Œä½†è¿™ä¼¼ä¹ä»ç„¶æœ‰äº›å¤æ‚ï¼Œå› ä¸ºè¿™æœ¬åº”æ˜¯åŸç”ŸåŠŸèƒ½ã€‚
- en: And now it is!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å®ƒå·²ç»å®ç°äº†ï¼
- en: The kind folks at Google Cloud [announced](https://cloud.google.com/blog/products/data-analytics/pub-sub-launches-direct-path-to-bigquery-for-streaming-analytics)
    a direct connection from Pub/Sub to BigQuery a while ago, awesome! However, having
    tried (and failed) to quickly set up a test a couple of times, I finally had a
    real-life use-case which required me to get it working for a client.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è°·æ­Œäº‘çš„å‹å–„å›¢é˜Ÿ[å®£å¸ƒ](https://cloud.google.com/blog/products/data-analytics/pub-sub-launches-direct-path-to-bigquery-for-streaming-analytics)äº†ä¸€æ®µæ—¶é—´å‰ä»
    Pub/Sub åˆ° BigQuery çš„ç›´æ¥è¿æ¥ï¼Œå¤ªæ£’äº†ï¼ç„¶è€Œï¼Œå°è¯•ï¼ˆå¹¶ä¸”å¤±è´¥ï¼‰å¿«é€Ÿè®¾ç½®æµ‹è¯•å‡ æ¬¡åï¼Œæˆ‘ç»ˆäºæœ‰äº†ä¸€ä¸ªå®é™…çš„ç”¨ä¾‹ï¼Œéœ€è¦ä¸ºä¸€ä¸ªå®¢æˆ·ä½¿å…¶æ­£å¸¸å·¥ä½œã€‚
- en: It turns out that there are a couple of nuances, so this article aims to help
    you get this up and running as quickly as possible.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœå‘ç°æœ‰å‡ ä¸ªç»†å¾®ä¹‹å¤„ï¼Œæ‰€ä»¥è¿™ç¯‡æ–‡ç« æ—¨åœ¨å¸®åŠ©ä½ å°½å¿«åœ°è®©è¿™ä¸€åˆ‡è¿è¡Œèµ·æ¥ã€‚
- en: Situation
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç°çŠ¶
- en: 'Pub/Sub is an incredibly useful, powerful and scaleable service in the Google
    Cloud ecosystem, with two core use-cases: streaming and messaging. I will let
    Google explain this themselves (disappointing spoiler alert: it has nothing to
    do with a Public House located on a Submarine.):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Pub/Sub æ˜¯ Google Cloud ç”Ÿæ€ç³»ç»Ÿä¸­ä¸€ä¸ªæå…¶æœ‰ç”¨ã€å¼ºå¤§ä¸”å¯æ‰©å±•çš„æœåŠ¡ï¼Œå…·æœ‰ä¸¤ä¸ªæ ¸å¿ƒç”¨ä¾‹ï¼šæµå¼ä¼ è¾“å’Œæ¶ˆæ¯ä¼ é€’ã€‚æˆ‘å°†è®© Google è‡ªè¡Œè§£é‡Šï¼ˆä»¤äººå¤±æœ›çš„å‰§é€ï¼šè¿™ä¸ä½äºæ½œè‰‡ä¸Šçš„å…¬å…±æˆ¿å±‹æ— å…³ï¼‰ã€‚
- en: Pub/Sub is used for streaming analytics and data integration pipelines to ingest
    and distribute data. Itâ€™s equally effective as a messaging-oriented middleware
    for service integration or as a queue to parallelize tasks.
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Pub/Sub ç”¨äºæµå¼åˆ†æå’Œæ•°æ®é›†æˆç®¡é“ï¼Œä»¥æ‘„å–å’Œåˆ†å‘æ•°æ®ã€‚å®ƒåŒæ ·æœ‰æ•ˆåœ°ä½œä¸ºæœåŠ¡é›†æˆçš„æ¶ˆæ¯ä¸­ä»‹æˆ–ä½œä¸ºé˜Ÿåˆ—æ¥å¹¶è¡ŒåŒ–ä»»åŠ¡ã€‚
- en: ''
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Pub/Sub enables you to create systems of event producers and consumers, called
    **publishers** and **subscribers**. Publishers communicate with subscribers asynchronously
    by broadcasting events, rather than by synchronous remote procedure calls (RPCs).
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Pub/Sub ä½¿ä½ èƒ½å¤Ÿåˆ›å»ºäº‹ä»¶ç”Ÿäº§è€…å’Œæ¶ˆè´¹è€…ç³»ç»Ÿï¼Œç§°ä¸º**å‘å¸ƒè€…**å’Œ**è®¢é˜…è€…**ã€‚å‘å¸ƒè€…é€šè¿‡å¹¿æ’­äº‹ä»¶è€Œä¸æ˜¯é€šè¿‡åŒæ­¥è¿œç¨‹è¿‡ç¨‹è°ƒç”¨ï¼ˆRPCï¼‰ä¸è®¢é˜…è€…å¼‚æ­¥é€šä¿¡ã€‚
- en: Messages are published to a topic, and subscribers to the topic can receive
    the message and take action accordingly. The Pub(lisher) knows nothing about the
    Sub(scribers), but when the messages are published, the subscribers can then take
    actions based on the message contents.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æ¶ˆæ¯è¢«å‘å¸ƒåˆ°ä¸€ä¸ªä¸»é¢˜ï¼Œä¸»é¢˜çš„è®¢é˜…è€…å¯ä»¥æ¥æ”¶æ¶ˆæ¯å¹¶ç›¸åº”åœ°é‡‡å–è¡ŒåŠ¨ã€‚å‘å¸ƒè€…å¯¹è®¢é˜…è€…ä¸€æ— æ‰€çŸ¥ï¼Œä½†å½“æ¶ˆæ¯è¢«å‘å¸ƒæ—¶ï¼Œè®¢é˜…è€…å¯ä»¥æ ¹æ®æ¶ˆæ¯å†…å®¹é‡‡å–è¡ŒåŠ¨ã€‚
- en: Client libraries or [notifications on Cloud Storage Buckets](https://cloud.google.com/storage/docs/pubsub-notifications)
    make it simple to publish messages containing configurable metadata, and Pub/Sub
    gets those messages to other Google Cloud destinations to [trigger Cloud Functions](https://cloud.google.com/functions/docs/calling/pubsub)
    or all manner of different actions, limited only by your imagination.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å®¢æˆ·ç«¯åº“æˆ–[Cloud Storage Buckets ä¸Šçš„é€šçŸ¥](https://cloud.google.com/storage/docs/pubsub-notifications)ä½¿å‘å¸ƒåŒ…å«å¯é…ç½®å…ƒæ•°æ®çš„æ¶ˆæ¯å˜å¾—ç®€å•ï¼Œè€Œ
    Pub/Sub ä¼šå°†è¿™äº›æ¶ˆæ¯å‘é€åˆ°å…¶ä»– Google Cloud ç›®çš„åœ°ä»¥[è§¦å‘ Cloud Functions](https://cloud.google.com/functions/docs/calling/pubsub)æˆ–å„ç§ä¸åŒçš„æ“ä½œï¼Œåªæœ‰ä½ çš„æƒ³è±¡åŠ›é™åˆ¶äº†è¿™äº›æ“ä½œã€‚
- en: And now we can get this data natively into BigQuery (apparently trivially),
    so I jumped at the opportunity to get this working in minutes! Except it turned
    out to not be that simple. But I got it done, so I wanted to write this article
    to help anybody else who needs to get this set up with minimal fuss.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥å°†è¿™äº›æ•°æ®åŸç”Ÿåœ°å¯¼å…¥ BigQueryï¼ˆæ˜¾ç„¶å¾ˆç®€å•ï¼‰ï¼Œæ‰€ä»¥æˆ‘æŠ“ä½æœºä¼šåœ¨å‡ åˆ†é’Ÿå†…å®Œæˆè¿™ä¸ªå·¥ä½œï¼ä½†ç»“æœè¯æ˜å¹¶æ²¡æœ‰é‚£ä¹ˆç®€å•ã€‚ä¸è¿‡æˆ‘å®Œæˆäº†è¿™ä¸ªä»»åŠ¡ï¼Œæ‰€ä»¥æˆ‘æƒ³å†™è¿™ç¯‡æ–‡ç« æ¥å¸®åŠ©å…¶ä»–éœ€è¦å¿«é€Ÿè®¾ç½®çš„äººï¼Œå°½é‡å‡å°‘éº»çƒ¦ã€‚
- en: Solution
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è§£å†³æ–¹æ¡ˆ
- en: So where to start? Letâ€™s start with the [docs](https://cloud.google.com/pubsub/docs/bigquery).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆä»å“ªé‡Œå¼€å§‹å‘¢ï¼Ÿæˆ‘ä»¬ä»[æ–‡æ¡£](https://cloud.google.com/pubsub/docs/bigquery)å¼€å§‹å§ã€‚
- en: A BigQuery subscription writes messages to an existing BigQuery table as they
    are received. Youâ€™re not required to configure a subscriber client separately.
    Use the Google Cloud console, the Google Cloud CLI, the client libraries, or the
    Pub/Sub API to create, update, list, detach, or delete a BigQuery subscription.
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: BigQuery è®¢é˜…åœ¨æ¥æ”¶åˆ°æ¶ˆæ¯æ—¶ä¼šå°†å…¶å†™å…¥ç°æœ‰çš„ BigQuery è¡¨ã€‚ä½ ä¸éœ€è¦å•ç‹¬é…ç½®è®¢é˜…å®¢æˆ·ç«¯ã€‚ä½¿ç”¨ Google Cloud æ§åˆ¶å°ã€Google
    Cloud CLIã€å®¢æˆ·ç«¯åº“æˆ– Pub/Sub API æ¥åˆ›å»ºã€æ›´æ–°ã€åˆ—å‡ºã€åˆ†ç¦»æˆ–åˆ é™¤ BigQuery è®¢é˜…ã€‚
- en: Sweet. Letâ€™s go.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå¥½ã€‚æˆ‘ä»¬å¼€å§‹å§ã€‚
- en: 'As an alternative for simple data ingestion pipelines that often use Dataflow
    to write to BigQuery, the BigQuery subscription has the following advantages:'
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä½œä¸ºç®€å•æ•°æ®æ‘„å–ç®¡é“çš„æ›¿ä»£æ–¹æ¡ˆï¼Œè¿™äº›ç®¡é“é€šå¸¸ä½¿ç”¨ Dataflow å†™å…¥ BigQueryï¼ŒBigQuery è®¢é˜…å…·æœ‰ä»¥ä¸‹ä¼˜ç‚¹ï¼š
- en: ''
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Simple deployment.** You can set up a BigQuery subscription through a single
    workflow in the console, Google Cloud CLI, client library, or Pub/Sub API.'
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**ç®€å•éƒ¨ç½²ã€‚** ä½ å¯ä»¥é€šè¿‡æ§åˆ¶å°ã€Google Cloud CLIã€å®¢æˆ·ç«¯åº“æˆ– Pub/Sub API é€šè¿‡ä¸€ä¸ªå·¥ä½œæµè®¾ç½® BigQuery è®¢é˜…ã€‚'
- en: ''
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Offers low costs.** Removes the additional cost and latency of similar Pub/Sub
    pipelines that include Dataflow jobs. This cost optimization is useful for messaging
    systems that do not require additional processing before storage.'
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**æä¾›ä½æˆæœ¬ã€‚** é™¤å»äº†ç±»ä¼¼ Pub/Sub ç®¡é“ä¸­åŒ…å« Dataflow ä½œä¸šçš„é¢å¤–æˆæœ¬å’Œå»¶è¿Ÿã€‚è¿™ç§æˆæœ¬ä¼˜åŒ–å¯¹é‚£äº›åœ¨å­˜å‚¨å‰ä¸éœ€è¦é¢å¤–å¤„ç†çš„æ¶ˆæ¯ç³»ç»Ÿç‰¹åˆ«æœ‰ç”¨ã€‚'
- en: ''
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Minimizes monitoring.** BigQuery subscriptions are part of the multi-tenant
    Pub/Sub service and do not require you to run separate monitoring jobs.'
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**å‡å°‘ç›‘æ§ã€‚** BigQuery è®¢é˜…æ˜¯å¤šç§Ÿæˆ· Pub/Sub æœåŠ¡çš„ä¸€éƒ¨åˆ†ï¼Œæ— éœ€ä½ è¿è¡Œå•ç‹¬çš„ç›‘æ§ä½œä¸šã€‚'
- en: Soundâ€™s good! Keep goingâ€¦ to the [schema section](https://cloud.google.com/pubsub/docs/bigquery#properties_subscription).
    I must confess reading this a few times and being a little baffled by what I actually
    needed to do.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å¬èµ·æ¥ä¸é”™ï¼ç»§ç»­å‰å¾€[æ¨¡å¼éƒ¨åˆ†](https://cloud.google.com/pubsub/docs/bigquery#properties_subscription)ã€‚æˆ‘å¿…é¡»æ‰¿è®¤ï¼Œé˜…è¯»å‡ éåï¼Œæˆ‘å¯¹éœ€è¦åšçš„äº‹æƒ…æ„Ÿåˆ°æœ‰äº›å›°æƒ‘ã€‚
- en: 'Managing schemas can be quite complex in normal environments, and it seemed
    that in this case I would need to create a BigQuery table with a schema which
    mirrored the inbound JSON exactly:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ™®é€šç¯å¢ƒä¸­ï¼Œç®¡ç†æ¨¡å¼å¯èƒ½ç›¸å½“å¤æ‚ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘éœ€è¦åˆ›å»ºä¸€ä¸ª BigQuery è¡¨ï¼Œå…¶æ¨¡å¼å®Œå…¨é•œåƒè¾“å…¥çš„ JSON æ•°æ®ï¼š
- en: '**Use topic schema.** This option lets Pub/Sub use the [schema of the Pub/Sub
    topic](https://cloud.google.com/pubsub/docs/admin#schemas) to which the subscription
    is attached. In addition, Pub/Sub writes the fields in messages to the corresponding
    columns in the BigQuery table. When you use this option, remember to check the
    following additional requirements:'
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**ä½¿ç”¨ä¸»é¢˜æ¨¡å¼ã€‚** è¿™ä¸ªé€‰é¡¹å…è®¸ Pub/Sub ä½¿ç”¨[Pub/Sub ä¸»é¢˜çš„æ¨¡å¼](https://cloud.google.com/pubsub/docs/admin#schemas)ï¼Œå³è®¢é˜…æ‰€é™„åŠ çš„ä¸»é¢˜ã€‚æ­¤å¤–ï¼ŒPub/Sub
    å°†æ¶ˆæ¯ä¸­çš„å­—æ®µå†™å…¥ BigQuery è¡¨ä¸­çš„ç›¸åº”åˆ—ã€‚å½“ä½ ä½¿ç”¨æ­¤é€‰é¡¹æ—¶ï¼Œè¯·è®°å¾—æ£€æŸ¥ä»¥ä¸‹é¢å¤–è¦æ±‚ï¼š'
- en: ''
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The fields in the topic schema and the BigQuery schema must have the same names
    and their types must be compatible with each other.
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä¸»é¢˜æ¨¡å¼ä¸­çš„å­—æ®µå’Œ BigQuery æ¨¡å¼ä¸­çš„å­—æ®µå¿…é¡»å…·æœ‰ç›¸åŒçš„åç§°ï¼Œå¹¶ä¸”å®ƒä»¬çš„ç±»å‹å¿…é¡»å…¼å®¹ã€‚
- en: Oh shit. Real-life inbound data can be pretty complex. It can be nested, containing
    arrays and potentially hundreds of fields. JSON and BigQuery structures map pretty
    cleanly (array = `ARRAY`, object = `STRUCT`) but it is not a simple task to generate
    an empty BigQuery table which maps exactly.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å“å‘€ã€‚å®é™…çš„è¾“å…¥æ•°æ®å¯èƒ½éå¸¸å¤æ‚ã€‚å®ƒå¯èƒ½æ˜¯åµŒå¥—çš„ï¼ŒåŒ…å«æ•°ç»„å¹¶å¯èƒ½æœ‰æ•°ç™¾ä¸ªå­—æ®µã€‚JSON å’Œ BigQuery ç»“æ„æ˜ å°„å¾—ç›¸å½“å¹²å‡€ï¼ˆæ•°ç»„ = `ARRAY`ï¼Œå¯¹è±¡
    = `STRUCT`ï¼‰ï¼Œä½†ç”Ÿæˆä¸€ä¸ªå®Œå…¨åŒ¹é…çš„ç©º BigQuery è¡¨å¹¶ä¸æ˜¯ä¸€ä»¶ç®€å•çš„äº‹ã€‚
- en: But there is an alternative.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†ä¹Ÿæœ‰å¦ä¸€ç§é€‰æ‹©ã€‚
- en: If you do not select the **Use topic schema** option, ensure that the BigQuery
    table has a column called `data` of type `BYTES` or `STRING`. Pub/Sub writes the
    message to this BigQuery column.
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æ²¡æœ‰é€‰æ‹© **ä½¿ç”¨ä¸»é¢˜æ¨¡å¼** é€‰é¡¹ï¼Œè¯·ç¡®ä¿ BigQuery è¡¨ä¸­æœ‰ä¸€ä¸ªåä¸º `data` çš„ `BYTES` æˆ– `STRING` ç±»å‹çš„åˆ—ã€‚Pub/Sub
    å°†æ¶ˆæ¯å†™å…¥è¿™ä¸ª BigQuery åˆ—ã€‚
- en: OK, this seems like it might be a viable approach. Letâ€™s just get the data into
    BigQuery and deal with decoding the `JSON` later. In fact, I tend to prefer this
    architecture as any non-compliant data will still be received and we can try and
    figure out how to decode it downstream.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œè¿™çœ‹èµ·æ¥å¯èƒ½æ˜¯ä¸€ä¸ªå¯è¡Œçš„æ–¹æ³•ã€‚æˆ‘ä»¬å…ˆå°†æ•°æ®å¯¼å…¥ BigQueryï¼Œç„¶åå†å¤„ç†è§£ç  `JSON` çš„é—®é¢˜ã€‚å®é™…ä¸Šï¼Œæˆ‘å€¾å‘äºè¿™ç§æ¶æ„ï¼Œå› ä¸ºä»»ä½•ä¸åˆè§„çš„æ•°æ®ä»ä¼šè¢«æ¥æ”¶ï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•åœ¨ä¸‹æ¸¸è§£ç å®ƒã€‚
- en: The same goes for evolving schemas. The beauty of ingesting raw `JSON` into
    BigQuery is that â€” as it is simply text â€” it is an extremely robust ingestion
    point. We can then use the recently-expanded [JSON functions in BigQuery](https://cloud.google.com/bigquery/docs/reference/standard-sql/json_functions)
    to perform all sorts of magic.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æ¼”å˜æ¨¡å¼ä¹Ÿæ˜¯å¦‚æ­¤ã€‚å°†åŸå§‹ `JSON` å¯¼å…¥ BigQuery çš„å¥½å¤„åœ¨äºâ€”â€”ç”±äºå®ƒä»…ä»…æ˜¯æ–‡æœ¬â€”â€”è¿™æ˜¯ä¸€ä¸ªæå…¶ç¨³å¥çš„æ•°æ®å¯¼å…¥ç‚¹ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æœ€è¿‘æ‰©å±•çš„[BigQuery
    JSON å‡½æ•°](https://cloud.google.com/bigquery/docs/reference/standard-sql/json_functions)æ¥æ‰§è¡Œå„ç§æ“ä½œã€‚
- en: 'Downstream transformations are also more robust: attempting to select a column
    which does not exist in normal SQL will cause a query to error, however attempting
    to extract the equivalent non-existent field from a JSON object will simply return
    a NULL. This, when properly handled, results in a more robust flow.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹æ¸¸è½¬æ¢ä¹Ÿæ›´ä¸ºç¨³å¥ï¼šå°è¯•é€‰æ‹©åœ¨æ™®é€š SQL ä¸­ä¸å­˜åœ¨çš„åˆ—ä¼šå¯¼è‡´æŸ¥è¯¢é”™è¯¯ï¼Œè€Œå°è¯•ä» JSON å¯¹è±¡ä¸­æå–ç­‰æ•ˆçš„ä¸å­˜åœ¨å­—æ®µå°†ç®€å•åœ°è¿”å› NULLã€‚è¿™æ ·å¤„ç†åï¼Œä¼šå¾—åˆ°ä¸€ä¸ªæ›´ä¸ºç¨³å¥çš„æµç¨‹ã€‚
- en: '**Inbound Table Creation**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¾“å…¥è¡¨åˆ›å»º**'
- en: So, on to creating the inbound table into which we will ingest the raw JSON
    data.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥æ˜¯åˆ›å»ºç”¨äºæ¥æ”¶åŸå§‹ JSON æ•°æ®çš„è¾“å…¥è¡¨ã€‚
- en: You can create tables in the UI, but itâ€™s actually very simple to do this via
    [DDL](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language)
    in the BigQuery console (and quicker for me to type than taking screenshots ğŸ˜€),
    and then when you inevitably get it wrong the first time itâ€™s a lot less frustrating
    to just change a parameter (or add a missing character) and hit run.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨UIä¸­åˆ›å»ºè¡¨æ ¼ï¼Œä½†å®é™…ä¸Šé€šè¿‡[DDL](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language)åœ¨BigQueryæ§åˆ¶å°ä¸­å®Œæˆè¿™ä¸€æ“ä½œéå¸¸ç®€å•ï¼ˆè€Œä¸”å¯¹æˆ‘æ¥è¯´æ¯”æˆªå›¾æ›´å¿«
    ğŸ˜€ï¼‰ï¼Œç„¶åå½“ä½ ç¬¬ä¸€æ¬¡åšé”™æ—¶ï¼Œæ”¹å˜ä¸€ä¸ªå‚æ•°ï¼ˆæˆ–æ·»åŠ ç¼ºå¤±çš„å­—ç¬¦ï¼‰å¹¶ç‚¹å‡»è¿è¡Œå°±ä¼šå°‘å¾ˆå¤šæŒ«è´¥æ„Ÿã€‚
- en: The table schema is actually in the section of the docs which explains how to
    [write metadata](https://cloud.google.com/pubsub/docs/create-bigquery-subscription#write-metadata),
    which is almost always a good idea for additional context and debugging support.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨æ ¼æ¨¡å¼å®é™…ä¸Šåœ¨æ–‡æ¡£çš„[ç¼–å†™å…ƒæ•°æ®](https://cloud.google.com/pubsub/docs/create-bigquery-subscription#write-metadata)éƒ¨åˆ†ä¸­è§£é‡Šï¼Œè¿™å‡ ä¹æ€»æ˜¯å¢åŠ ä¸Šä¸‹æ–‡å’Œè°ƒè¯•æ”¯æŒçš„å¥½ä¸»æ„ã€‚
- en: 'The DDL to create the table (with all possible fields), is then:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ›å»ºè¡¨æ ¼çš„DDLï¼ˆåŒ…å«æ‰€æœ‰å¯èƒ½çš„å­—æ®µï¼‰å¦‚ä¸‹ï¼š
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Table Partitioning**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¡¨æ ¼åˆ†åŒº**'
- en: Partitioning is nearly always a good idea (and sometimes an essential approach)
    for tables in BigQuery as it physically segregates the data into different partitions,
    which can then be queried directly without requiring expensive full-table-scans.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨BigQueryä¸­ï¼Œåˆ†åŒºå‡ ä¹æ€»æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ï¼ˆæœ‰æ—¶æ˜¯å¿…ä¸å¯å°‘çš„æ–¹æ³•ï¼‰ï¼Œå› ä¸ºå®ƒå°†æ•°æ®ç‰©ç†ä¸Šéš”ç¦»åˆ°ä¸åŒçš„åˆ†åŒºä¸­ï¼Œè¿™æ ·å¯ä»¥ç›´æ¥æŸ¥è¯¢è¿™äº›åˆ†åŒºï¼Œè€Œæ— éœ€æ˜‚è´µçš„å…¨è¡¨æ‰«æã€‚
- en: There are a number of different [options](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#partition_expression)
    for table partitioning, however it should be noted that there is currently a [limit
    of 4000 partitions](https://cloud.google.com/bigquery/docs/partitioned-tables#ingestion_time)
    so daily partitioning will max out after nearly 11 years but hourly partitioning
    will max out after less than 24 weeks.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨æ ¼åˆ†åŒºæœ‰è®¸å¤šä¸åŒçš„[é€‰é¡¹](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#partition_expression)ï¼Œä½†éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œç›®å‰æœ‰ä¸€ä¸ª[4000ä¸ªåˆ†åŒºçš„é™åˆ¶](https://cloud.google.com/bigquery/docs/partitioned-tables#ingestion_time)ï¼Œå› æ­¤æ¯æ—¥åˆ†åŒºå°†åœ¨è¿‘11å¹´åè¾¾åˆ°ä¸Šé™ï¼Œä½†æ¯å°æ—¶åˆ†åŒºå°†åœ¨ä¸åˆ°24å‘¨åè¾¾åˆ°ä¸Šé™ã€‚
- en: One side note here, sometimes it *is* necessary to use hourly partitioning due
    to the volume of streaming data (e.g. we have implemented this to consume [EventStream
    data from Tealium](https://tealium.com/integrations/google-cloud-pub-sub/)). In
    this case it was necessary to deploy additional architecture to back up historic
    data to Google Cloud Storage and set a [partition expiration](https://cloud.google.com/bigquery/docs/managing-partitioned-tables#partition-expiration)
    on the inbound table.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰ä¸€ä¸ªé™„æ³¨ï¼Œæœ‰æ—¶ç”±äºæµæ•°æ®çš„é‡ï¼Œ*ç¡®å®*éœ€è¦ä½¿ç”¨æ¯å°æ—¶åˆ†åŒºï¼ˆä¾‹å¦‚ï¼Œæˆ‘ä»¬å·²ç»å®ç°äº†ä»[Tealiumçš„EventStreamæ•°æ®](https://tealium.com/integrations/google-cloud-pub-sub/)ï¼‰ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œéœ€è¦éƒ¨ç½²é¢å¤–çš„æ¶æ„ï¼Œå°†å†å²æ•°æ®å¤‡ä»½åˆ°Google
    Cloud Storageï¼Œå¹¶è®¾ç½®[åˆ†åŒºè¿‡æœŸ](https://cloud.google.com/bigquery/docs/managing-partitioned-tables#partition-expiration)ã€‚
- en: 'One more side note: for brevity I am omitting details on the testing steps
    we took to get this process running reliably. Such is the beauty of learning from
    the experience of others: you can skip the tedious parts and go directly to the
    answer!'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªé™„æ³¨ï¼šä¸ºäº†ç®€æ´èµ·è§ï¼Œæˆ‘çœç•¥äº†æˆ‘ä»¬ä¸ºä½¿è¿™ä¸ªè¿‡ç¨‹å¯é è¿è¡Œè€Œé‡‡å–çš„æµ‹è¯•æ­¥éª¤ã€‚è¿™å°±æ˜¯ä»ä»–äººç»éªŒä¸­å­¦ä¹ çš„ç¾å¦™ä¹‹å¤„ï¼šä½ å¯ä»¥è·³è¿‡ç¹ççš„éƒ¨åˆ†ï¼Œç›´æ¥å¾—åˆ°ç­”æ¡ˆï¼
- en: '**Setting up Pub/Sub Topic**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**è®¾ç½®Pub/Subä¸»é¢˜**'
- en: Now for the actual setup of the Pub/Sub topic and the BigQuery Subscription,
    which should be a pretty quick process. Head to [Pub/Sub in the Cloud Console](https://console.cloud.google.com/cloudpubsub/topic/)
    and click the `[+] CREATE TOPIC` button. You have quite a lot of naming freedom
    here, so I tend to give it exactly the same name as the destination `dataset_id`
    without the `project_id` (i.e. `dataset_id.table_name`). Uncheck the `Add a default
    subscription` box (as we are going to create a BigQuery Subscription next) and
    click `CREATE`.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ˜¯å®é™…è®¾ç½®Pub/Subä¸»é¢˜å’ŒBigQueryè®¢é˜…çš„éƒ¨åˆ†ï¼Œè¿™åº”è¯¥æ˜¯ä¸€ä¸ªç›¸å½“å¿«é€Ÿçš„è¿‡ç¨‹ã€‚å‰å¾€[Cloud Consoleä¸­çš„Pub/Sub](https://console.cloud.google.com/cloudpubsub/topic/)å¹¶ç‚¹å‡»`[+]
    CREATE TOPIC`æŒ‰é’®ã€‚ä½ åœ¨è¿™é‡Œæœ‰ç›¸å½“å¤§çš„å‘½åè‡ªç”±åº¦ï¼Œæ‰€ä»¥æˆ‘å€¾å‘äºç»™å®ƒå’Œç›®æ ‡`dataset_id`å®Œå…¨ç›¸åŒçš„åç§°ï¼Œä¸å¸¦`project_id`ï¼ˆå³`dataset_id.table_name`ï¼‰ã€‚å–æ¶ˆå‹¾é€‰`Add
    a default subscription`æ¡†ï¼ˆå› ä¸ºæˆ‘ä»¬æ¥ä¸‹æ¥è¦åˆ›å»ºä¸€ä¸ªBigQueryè®¢é˜…ï¼‰ï¼Œç„¶åç‚¹å‡»`CREATE`ã€‚
- en: '**Setting up Pub/Sub Subscription**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**è®¾ç½®Pub/Subè®¢é˜…**'
- en: Once the topic is set up, now onto the Subscription.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦ä¸»é¢˜è®¾ç½®å¥½ï¼Œå°±å¯ä»¥å¼€å§‹åˆ›å»ºè®¢é˜…äº†ã€‚
- en: Click on the topic you have just created, and you should see a bottom section
    with a number of tabs, of which `SUBSCRIPTIONS` is selected by default. Click
    `CREATE SUBSCRIPTIONS` in this section and name the Subscription (you can actually
    name the subscription identically as the Topic, with which I have never experienced
    problems).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ç‚¹å‡»ä½ åˆšåˆšåˆ›å»ºçš„ä¸»é¢˜ï¼Œä½ åº”è¯¥çœ‹åˆ°ä¸€ä¸ªåº•éƒ¨åŒºåŸŸï¼Œé‡Œé¢æœ‰å¤šä¸ªé€‰é¡¹å¡ï¼Œé»˜è®¤æƒ…å†µä¸‹é€‰ä¸­çš„æ˜¯`SUBSCRIPTIONS`ã€‚åœ¨è¿™ä¸ªåŒºåŸŸç‚¹å‡»`CREATE SUBSCRIPTIONS`å¹¶ä¸ºè®¢é˜…å‘½åï¼ˆå®é™…ä¸Šä½ å¯ä»¥å°†è®¢é˜…åç§°ä¸ä¸»é¢˜åç§°ç›¸åŒï¼Œæˆ‘ä»æœªé‡åˆ°è¿‡é—®é¢˜ï¼‰ã€‚
- en: In the `Delivery Type` section, check the radio button next to `Write to BigQuery`,
    and select the `project_id`, `dataset_id` and `table_name` of the table created
    previously. Check the `Write Metadata` check box (we like metadata) and â€” in the
    interests of simplicity â€” leave the other options as the default settings. It
    does recommend that BigQuery subscriptions should enable dead lettering, but this
    requires an additional topic so we do not in this case. Also since we are not
    depending on using the topic schema we are much less likely to encounter failed
    messages.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨`Delivery Type`éƒ¨åˆ†ï¼Œé€‰ä¸­`Write to BigQuery`æ—è¾¹çš„å•é€‰æŒ‰é’®ï¼Œé€‰æ‹©ä¹‹å‰åˆ›å»ºçš„è¡¨çš„`project_id`ã€`dataset_id`å’Œ`table_name`ã€‚å‹¾é€‰`Write
    Metadata`å¤é€‰æ¡†ï¼ˆæˆ‘ä»¬å–œæ¬¢å…ƒæ•°æ®ï¼‰ï¼Œå¹¶ä¸”â€”ä¸ºäº†ç®€å•èµ·è§â€”å°†å…¶ä»–é€‰é¡¹ä¿ç•™ä¸ºé»˜è®¤è®¾ç½®ã€‚è™½ç„¶å»ºè®® BigQuery è®¢é˜…åº”è¯¥å¯ç”¨æ­»ä¿¡é˜Ÿåˆ—ï¼Œä½†è¿™éœ€è¦é¢å¤–çš„ä¸»é¢˜ï¼Œæ‰€ä»¥åœ¨è¿™ç§æƒ…å†µä¸‹æˆ‘ä»¬ä¸å¯ç”¨ã€‚åŒæ—¶ï¼Œç”±äºæˆ‘ä»¬ä¸ä¾èµ–ä½¿ç”¨ä¸»é¢˜æ¨¡å¼ï¼Œæˆ‘ä»¬é‡åˆ°å¤±è´¥æ¶ˆæ¯çš„å¯èƒ½æ€§ä¹Ÿå¤§å¤§é™ä½ã€‚
- en: Done!
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæˆï¼
- en: Oh, wait. Hang on.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: å“¦ï¼Œç­‰ç­‰ã€‚ç¨ç­‰ä¸€ä¸‹ã€‚
- en: '**Permissions**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**æƒé™**'
- en: In all likelihood you will now see an error message preventing you from successfully
    creating the subscription.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¾ˆå¯èƒ½ä¼šçœ‹åˆ°ä¸€ä¸ªé”™è¯¯ä¿¡æ¯ï¼Œé˜»æ­¢ä½ æˆåŠŸåˆ›å»ºè®¢é˜…ã€‚
- en: '[PRE2]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This is actually pretty simple to deal with, but you have a couple of options.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œè¿™å¾ˆç®€å•ï¼Œä½†ä½ æœ‰å‡ ä¸ªé€‰æ‹©ã€‚
- en: The most secure option, compliant with the principle of [least privilege](https://cloud.google.com/iam/docs/using-iam-securely#least_privilege),
    is the creation of a [custom role](https://console.cloud.google.com/iam-admin/roles)
    with the precise permissions required (`bigquery.tables.get`, `bigquery.tables.updateData`)
    and then assigning this role to the service account.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€å®‰å…¨çš„é€‰é¡¹ï¼Œç¬¦åˆ[æœ€å°æƒé™](https://cloud.google.com/iam/docs/using-iam-securely#least_privilege)åŸåˆ™ï¼Œæ˜¯åˆ›å»ºä¸€ä¸ªå…·æœ‰ç²¾ç¡®æƒé™ï¼ˆ`bigquery.tables.get`ã€`bigquery.tables.updateData`ï¼‰çš„[è‡ªå®šä¹‰è§’è‰²](https://console.cloud.google.com/iam-admin/roles)ï¼Œç„¶åå°†æ­¤è§’è‰²åˆ†é…ç»™æœåŠ¡è´¦æˆ·ã€‚
- en: However, since this is a Google Cloud Service Account which is never used for
    anything else on my project I am happy to give it a simpler, more permissive role.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ï¼Œç”±äºè¿™æ˜¯ä¸€ä¸ªåœ¨æˆ‘çš„é¡¹ç›®ä¸­ä»æœªç”¨äºå…¶ä»–ç”¨é€”çš„ Google Cloud æœåŠ¡è´¦æˆ·ï¼Œæˆ‘æ„¿æ„ç»™äºˆå®ƒä¸€ä¸ªæ›´ç®€å•ã€æ›´å®½æ¾çš„è§’è‰²ã€‚
- en: Either way, copy the service account email in the message, go to the inbound
    table, click `SHARE` and `ADD PRINCIPAL`. Copy the service account email into
    the `New Principals` field. Add either the `BigQuery Data Editor` role or the
    optional custom role created to the assigned roles and `SAVE`.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: æ— è®ºå¦‚ä½•ï¼Œå¤åˆ¶æ¶ˆæ¯ä¸­çš„æœåŠ¡è´¦æˆ·é‚®ç®±ï¼Œè¿›å…¥å…¥ç«™è¡¨ï¼Œç‚¹å‡»`SHARE`å’Œ`ADD PRINCIPAL`ã€‚å°†æœåŠ¡è´¦æˆ·é‚®ç®±å¤åˆ¶åˆ°`New Principals`å­—æ®µä¸­ã€‚å°†`BigQuery
    Data Editor`è§’è‰²æˆ–åˆ›å»ºçš„è‡ªå®šä¹‰è§’è‰²æ·»åŠ åˆ°åˆ†é…è§’è‰²ä¸­ï¼Œç„¶å`SAVE`ã€‚
- en: Hopefully this will complete successfully and you will have a fully functional,
    simple, serverless data ingestion flow from any arbitrary Pub/Sub topic.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: å¸Œæœ›è¿™å°†é¡ºåˆ©å®Œæˆï¼Œä½ å°†æ‹¥æœ‰ä¸€ä¸ªå®Œå…¨åŠŸèƒ½ã€ç®€å•çš„æ— æœåŠ¡å™¨æ•°æ®æ‘„å–æµç¨‹ï¼Œæ¥è‡ªä»»ä½•ä»»æ„çš„ Pub/Sub ä¸»é¢˜ã€‚
- en: Well done!
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: åšå¾—å¥½ï¼
- en: '**Next Steps**'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä¸‹ä¸€æ­¥**'
- en: Now that your ingestion flow is operating properly, I need to go and do some
    actual work so I will leave you to test it with some real Pub/Sub messages.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ çš„æ•°æ®æ‘„å–æµç¨‹æ­£å¸¸è¿è¡Œï¼Œæˆ‘éœ€è¦å»åšä¸€äº›å®é™…å·¥ä½œï¼Œæ‰€ä»¥æˆ‘å°†ç•™ä¸‹æ¥è®©ä½ ç”¨ä¸€äº›çœŸå®çš„ Pub/Sub æ¶ˆæ¯è¿›è¡Œæµ‹è¯•ã€‚
- en: One of the easiest things to use is to set up a Pub/Sub Notification on a BigQuery
    Scheduled Query and then run it a few times. You will see the table populating
    with a row for each run, with the `JSON` data in the `data` and `attributes` columns
    containing all of the useful data about each run.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æœ€ç®€å•çš„æ–¹æ³•ä¹‹ä¸€æ˜¯è®¾ç½® BigQuery å®šæ—¶æŸ¥è¯¢çš„ Pub/Sub é€šçŸ¥ï¼Œç„¶åè¿è¡Œå‡ æ¬¡ã€‚ä½ å°†çœ‹åˆ°æ¯æ¬¡è¿è¡Œæ—¶è¡¨æ ¼ä¸­éƒ½ä¼šå¢åŠ ä¸€è¡Œï¼Œå…¶ä¸­çš„`JSON`æ•°æ®åœ¨`data`å’Œ`attributes`åˆ—ä¸­åŒ…å«äº†æ¯æ¬¡è¿è¡Œçš„æ‰€æœ‰æœ‰ç”¨æ•°æ®ã€‚
- en: My next article will explain the optimal setup to decode the `JSON` payloads
    into BigQuery data types, so that you can start using and visualising the data.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çš„ä¸‹ä¸€ç¯‡æ–‡ç« å°†è§£é‡Šå°†`JSON`æœ‰æ•ˆè´Ÿè½½è§£ç ä¸º BigQuery æ•°æ®ç±»å‹çš„æœ€ä½³è®¾ç½®ï¼Œä»¥ä¾¿ä½ å¯ä»¥å¼€å§‹ä½¿ç”¨å’Œå¯è§†åŒ–æ•°æ®ã€‚
- en: Remember to follow me if you want to receive this second instalment (hopefully
    next week)!
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³æ¥æ”¶ç¬¬äºŒéƒ¨åˆ†ï¼ˆå¸Œæœ›ä¸‹å‘¨å‘å¸ƒï¼‰ï¼Œè®°å¾—å…³æ³¨æˆ‘ï¼
