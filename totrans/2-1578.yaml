- en: New DeepMind Work Unveils Supreme Prompt Seeds for Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最新的DeepMind工作揭示了语言模型的极致提示种子
- en: 原文：[https://towardsdatascience.com/new-deepmind-work-unveils-supreme-prompt-seeds-for-language-models-e95fb7f4903c](https://towardsdatascience.com/new-deepmind-work-unveils-supreme-prompt-seeds-for-language-models-e95fb7f4903c)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/new-deepmind-work-unveils-supreme-prompt-seeds-for-language-models-e95fb7f4903c](https://towardsdatascience.com/new-deepmind-work-unveils-supreme-prompt-seeds-for-language-models-e95fb7f4903c)
- en: How computationally optimized prompts make language models excel, and how this
    all affects prompt engineering
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何通过计算优化的提示使语言模型表现出色，以及这如何影响提示工程
- en: '[](https://lucianosphere.medium.com/?source=post_page-----e95fb7f4903c--------------------------------)[![LucianoSphere
    (Luciano Abriata, PhD)](../Images/a8ae3085d094749bbdd1169cca672b86.png)](https://lucianosphere.medium.com/?source=post_page-----e95fb7f4903c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e95fb7f4903c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e95fb7f4903c--------------------------------)
    [LucianoSphere (Luciano Abriata, PhD)](https://lucianosphere.medium.com/?source=post_page-----e95fb7f4903c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://lucianosphere.medium.com/?source=post_page-----e95fb7f4903c--------------------------------)[![LucianoSphere
    (Luciano Abriata, PhD)](../Images/a8ae3085d094749bbdd1169cca672b86.png)](https://lucianosphere.medium.com/?source=post_page-----e95fb7f4903c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e95fb7f4903c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e95fb7f4903c--------------------------------)
    [LucianoSphere (Luciano Abriata, PhD)](https://lucianosphere.medium.com/?source=post_page-----e95fb7f4903c--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e95fb7f4903c--------------------------------)
    ·11 min read·Nov 8, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e95fb7f4903c--------------------------------)
    ·阅读时间11分钟·2023年11月8日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/1fb04d056a98fae3386085e529a642a9.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1fb04d056a98fae3386085e529a642a9.png)'
- en: Photo by [Ali Shah Lakhani](https://unsplash.com/@alishahlakhani?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Ali Shah Lakhani](https://unsplash.com/@alishahlakhani?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'As we witness the steady advancement of artificial intelligence (AI), accomplishing
    increasingly difficult tasks month after month, there is a general concern about
    the future of our workforce. If AI continues to automate many of the tasks currently
    performed by humans, what will the occupations of tomorrow look like? There is
    this idea in the air that “*programming these systems will be human work for years*”,
    or that “*we’ll always need a human to maintain and retrain AI models*”, or that
    “*crafting efficient prompts that drive AI models the right way is a human skill*”.
    The latter, the focus of this article, motivated the creation of “prompt engineering”
    as a “career”. And certainly there is some mastery in writing efficient prompts
    to get the AI model do exactly what one expects from it, or to have it “think”
    well enough to improve its answers especially to problems. See this, as just one
    example:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们见证人工智能（AI）的稳步进步，每个月完成越来越困难的任务，人们普遍关注未来的劳动市场。如果AI继续自动化许多当前由人类执行的任务，未来的职业会是什么样的呢？有一种观点认为“*编程这些系统将是人类的工作多年*”，或者“*我们将始终需要人类来维护和重新训练AI模型*”，或者“*设计有效的提示以正确引导AI模型是人类的技能*”。本文的重点就是后者，这促使了“提示工程”作为一种“职业”的出现。确实，编写高效提示以使AI模型准确地执行期望的操作，或使其“思考”得足够好以改善其答案，尤其是对问题，是一种技巧。看看这个作为一个例子：
- en: '[](/crafting-effective-prompts-for-summarization-using-large-language-models-dbbdf019f664?source=post_page-----e95fb7f4903c--------------------------------)
    [## Crafting Effective Prompts for Summarization Using Large Language Models'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/crafting-effective-prompts-for-summarization-using-large-language-models-dbbdf019f664?source=post_page-----e95fb7f4903c--------------------------------)
    [## 针对大语言模型的有效提示工程'
- en: Distilling key points after >2 years of experience and from AI developers’ own
    tutorials, hands-on and with examples.
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提炼出关键点，基于超过2年的经验以及AI开发者自己的教程、实践和示例。
- en: towardsdatascience.com](/crafting-effective-prompts-for-summarization-using-large-language-models-dbbdf019f664?source=post_page-----e95fb7f4903c--------------------------------)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/crafting-effective-prompts-for-summarization-using-large-language-models-dbbdf019f664?source=post_page-----e95fb7f4903c--------------------------------)
- en: However, it is very likely that none of these human interventions will remain
    relevant forever. In particular for prompt engineering, it looks like these skills
    won’t be much relevant, already soon. Read on to know why and in the process learn
    about the very interesting findings reported by DeepMind in a recent preprint,
    which you can apply right away to your own benefit when using large language models
    (LLMs) as I show you with hands-on examples on ChatGPT’s free version.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些人工干预中的任何一种很可能都不会永远保持相关性。特别是在提示工程方面，这些技能看起来很快就不会再那么重要了。继续阅读以了解原因，并在过程中了解DeepMind在最近的预印本中报告的非常有趣的发现，当使用大型语言模型（LLMs）时，你可以立即将这些发现应用于自身利益，我会通过ChatGPT免费版的实践示例来展示给你。
- en: Prompt optimization
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示优化
- en: Before delving deeper, we need to understand the concept of “prompt”. Prompts
    are instructions passed to AI models to tell them what we want them to do. AI
    models respond to the user-generated text inputs, or prompts, to generate their
    outputs -text, graphics, audio, etc. The quality and specificity of the input
    prompts significantly affect the content and quality of outputs generated by the
    models. Moreover, different users will likely have different ways to make a request
    or ask a question, and not all of them will be as efficient in producing the expected
    answers with correct information.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入之前，我们需要了解“提示”的概念。提示是传递给AI模型的指令，用来告诉它们我们希望它们做什么。AI模型响应用户生成的文本输入或提示，以生成其输出——文本、图形、音频等。输入提示的质量和具体性显著影响模型生成的内容和质量。此外，不同的用户可能会有不同的请求或提问方式，并不是所有的方式都能高效地产生预期的答案和正确的信息。
- en: In the past, the art of formulating effective prompts was not very well understood.
    Already in 2019 OpenAI revealed that adding “tl;dr” (commonly used on the internet
    to request a summary) at the end of a text input could make the model summarize
    the preceding text. Although this was merely an academic curiosity back then,
    over time researchers and enthusiasts started to discover that specific phrasings
    could unlock enhanced potential from these AI models. It was precisely this what
    gave rise to the “field” and “profession” of “prompt engineer(ing)”.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 过去，制定有效提示的艺术并不被很好地理解。早在2019年，OpenAI就揭示了在文本输入末尾添加“tl;dr”（通常用于请求总结）可以使模型总结前面的文本。尽管当时这只是一个学术好奇，但随着时间的推移，研究人员和爱好者开始发现特定的措辞可以解锁这些AI模型的增强潜力。正是这一点催生了“提示工程（学）”这一“领域”和“职业”。
- en: Prompt engineers became experts in crafting prompts that would elicit desired
    responses from AI models. They shared their “magic phrases” and techniques on
    the internet, effectively creating a new area of expertise. The demand for these
    professionals quickly grew, and jobs for prompt engineers began to appear, highlighting
    the significant impact these individuals had on the AI landscape.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程师成为了制定能够引发AI模型期望回应的提示的专家。他们在互联网上分享了他们的“魔法短语”和技巧，从而有效地创造了一个新的专业领域。这些专业人员的需求迅速增长，提示工程师的职位开始出现，突显了这些人在AI领域的重要影响。
- en: 'What prompt engineers basically do is to optimize prompts for best outputs.
    But optimization is a task that computers do VERY well; hence, it is very likely
    that they can replace human prompt engineers. And this might be just about to
    happen: In their recent preprint, DeepMind showed that AI models can be used to
    optimize their own inputs, thus making “prompt engineering” kind of obsolete and
    inefficient, at least in the traditional way.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程师基本上是为了最佳输出而优化提示。但是，优化是计算机做得非常好的任务；因此，它们很可能会取代人工提示工程师。这可能即将发生：在他们最近的预印本中，DeepMind展示了AI模型可以用来优化自身的输入，从而使得“提示工程”在传统意义上变得有些过时和低效。
- en: Large Language Models as Optimizers of Their Own Prompts
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型作为自身提示的优化器
- en: The recent paper from DeepMind titled “[Large Language Models as Optimizers](https://arxiv.org/abs/2309.03409)”
    explores how AI models can be used to effectively optimize… their own input prompts!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最近DeepMind的论文《[大型语言模型作为优化器](https://arxiv.org/abs/2309.03409)》探讨了如何有效地优化……它们自己的输入提示！
- en: Let’s see what this is, how it relates to training and optimization, and what
    exactly DeepMind’s work found out. Oh and let’s try out some computer-optimized
    prompts ourselves. You’ll be amazed.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这是什么，它如何与训练和优化相关，以及DeepMind的工作究竟发现了什么。哦，我们还可以尝试一些计算机优化的提示。你会感到惊讶。
- en: Traditionally, in machine learning, the optimization process involves adjusting
    the internal parameters of a model (“weights”, just a large number of numbers
    describing how the different artificial neurons connect) to minimize error. Typically,
    during the stage of training the model is presented with a large array of known
    input-output pairs, and all the weights are optimized in the traditional mathematical
    sense so that the model “learns”. However, once a model is trained, one can still
    use it by providing inputs in different ways, that will propagate differently
    depending on the set of weights calculated during training, and hence their outputs
    will be different… some better, some worse. One can then optimize how one provides
    the inputs, which in the special case of language models are the prompts. And
    as any optimization protocol, we humans can do this by trial and error but of
    course computers are much better than us—it was just a matter of somebody telling
    them to do it!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，在机器学习中，优化过程涉及调整模型的内部参数（“权重”，即描述不同人工神经元如何连接的大量数字）以最小化误差。通常，在训练阶段，模型会接收到大量已知的输入-输出对，并且所有权重都按照传统的数学方式进行优化，以使模型“学习”。然而，一旦模型训练完成，我们仍然可以通过以不同方式提供输入来使用它，这些输入将根据训练过程中计算的权重集以不同的方式传播，因此它们的输出也会不同……有些更好，有些更差。然后，可以优化如何提供输入，在语言模型的特殊情况下，就是优化提示。像任何优化协议一样，我们人类可以通过试错来完成这一过程，但显然计算机比我们更擅长——这只是一个让他们去做的事情！
- en: The idea explored in DeepMind’s work is to use various AI models to generate
    prompts for specific tasks and then test the effectiveness of the different prompts
    in achieving the desired outcomes. For example, if the task is to solve a mathematical
    problem, the user may input a very simple prompt that only asks the problem directly,
    or he/she might precede the question with a seed sentence like “Solve this mathematical
    problem step by step.”, or “Let’s work on this stepwise.”, etc. In some cases,
    the seed sentences might help improve the AI’s output, even if the question itself
    is asked in exactly the same way.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: DeepMind工作的核心思想是使用各种AI模型生成特定任务的提示，然后测试不同提示在实现期望结果方面的有效性。例如，如果任务是解决一个数学问题，用户可以输入一个非常简单的提示，直接提出问题，或者他/她可能会在问题前面加上一句种子句，如“逐步解决这个数学问题。”，或者“我们一步步解决这个问题。”等。在某些情况下，种子句可能有助于改善AI的输出，即使问题本身以完全相同的方式提出。
- en: Based on this idea, what DeepMind’s engineers did was to try different seed
    prompts followed by exactly the same question posing a problem and evaluate the
    quality (correctness) of the answers. They repeated this procedure applying a
    series of prompts to several different problems, and finally, they counted how
    many right answers each seed sentence produced, to compare the effects on each
    problem.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这一思想，DeepMind的工程师尝试了不同的种子提示，然后用完全相同的问题进行测试，并评估答案的质量（正确性）。他们重复了这个过程，将一系列提示应用于多个不同的问题，并最终统计了每个种子句产生的正确答案数量，以比较对每个问题的影响。
- en: From experience, we know that this strategy should work out to find how to optimize
    the prompts. But of course, it’s impossible for us humans to sample this largely.
    By using a computer architecture, DeepMind could run the procedure at large scale,
    and on different kinds of questions and problems.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从经验来看，我们知道这种策略应该有效于找到如何优化提示。然而，对于我们人类来说，大规模地进行样本测试是不可能的。通过使用计算机架构，DeepMind能够大规模地运行这个过程，并且在不同类型的问题上进行测试。
- en: 'The results were surprising: the same problem could be solved correctly only
    around 50% of the times if no or a bad seed sentence was used, and up to 80% correctly
    when a good seed sentence was utilized.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 结果令人惊讶：如果没有使用或使用了不好的种子句，问题只能在约50%的情况下正确解决，而当使用了好的种子句时，正确率可以达到80%。
- en: DeepMind’s approach involved using metrics from these tests to guide the creation
    of better prompts. Their conclusion was then that by continuously iterating on
    the prompts and considering the models’ responses, AI systems can improve their
    prompt generation process with a very important effect on the outputs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: DeepMind的方法包括使用这些测试中的指标来指导更好提示的创建。他们得出的结论是，通过持续迭代提示并考虑模型的反馈，AI系统可以改进其提示生成过程，对输出产生非常重要的影响。
- en: Note that this approach does not involve updating the model’s internal parameters
    as one would do when training the model, but rather focuses on optimizing the
    inputs themselves.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这种方法并不涉及更新模型的内部参数，如同训练模型时所做的那样，而是专注于优化输入本身。
- en: Some interesting hands-on examples
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一些有趣的动手示例
- en: Here are some interesting examples from my own tests inspired by DeepMind’s
    work, that you can also try right away using ChatGPT’s free version (which is
    powered by the model that shows up in DeepMind’s preprint as GPT-3.5-Turbo).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一些我自己测试中灵感来自 DeepMind 工作的有趣示例，你也可以立即使用 ChatGPT 免费版尝试（该版本由 DeepMind 预印本中显示为
    GPT-3.5-Turbo 的模型提供支持）。
- en: Example 1, copying the idea from one of the problems shown in DeepMind’s preprint
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例 1，复制 DeepMind 预印本中展示的一个问题的思路
- en: The first example from DeepMind’s paper that called my attention was the request
    to perform linear regression, because since these models can’t in principle do
    math, then I’d expect them to never work, independently of the provided prompt.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: DeepMind 论文中引起我注意的第一个例子是要求进行线性回归，因为既然这些模型原则上无法进行数学运算，那么我期望它们永远无法工作，无论提供什么提示。
- en: If you ask ChatGPT to perform a linear regression on very simple data like say
    *x = [1, 2, 3, 4]* and *y = [2, 4, 6, 8],* you will find that it solves it right
    away correctly. But what if we challenge it with a more difficult linear regression?
    Let’s see.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你要求 ChatGPT 对非常简单的数据进行线性回归，比如 *x = [1, 2, 3, 4]* 和 *y = [2, 4, 6, 8]*，你会发现它会立即正确地解决。但如果我们用更难的线性回归来挑战它呢？让我们看看。
- en: 'Here I generated synthetic data for various *x* values scattered randomly between
    0 and 12, and calculated *y* values without noise by using the equation *y = 3.5
    x — 11.5* in a regular spreadsheet program. Then, I asked the program to “Find
    the linear equation that describes this data:” followed by the x, y pairs. Just
    like this:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我生成了不同 *x* 值的合成数据，这些值在 0 和 12 之间随机分布，并使用方程 *y = 3.5 x — 11.5* 在常规电子表格程序中计算了没有噪声的
    *y* 值。然后，我要求程序“找到描述这些数据的线性方程：”并跟上 x 和 y 对。就像这样：
- en: '![](../Images/4837401b4c71f4cdf147eb48feec8694.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4837401b4c71f4cdf147eb48feec8694.png)'
- en: Screenshot from interaction with ChatGPT’s free version while doing tests.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 从与 ChatGPT 免费版交互时的截图，进行测试时使用。
- en: 'This is the answer I got:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我得到的答案：
- en: '![](../Images/0b7ba18f04a582e1d3063077c8b570cd.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0b7ba18f04a582e1d3063077c8b570cd.png)'
- en: Screenshot from interaction with ChatGPT’s free version while doing tests.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 从与 ChatGPT 免费版交互时的截图，进行测试时使用。
- en: You see clearly that the answer is wrong, and also that it goes through some
    coding that I didn’t ask about. Even more confusing, the code looks itself correct
    and would have likely led to the right solution, but the “result” presented by
    the text generation is wrong.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 你清楚地看到答案是错误的，并且它经过了一些我没有要求的编码。更令人困惑的是，这段代码看起来本身是正确的，并且可能会导致正确的解决方案，但文本生成所呈现的“结果”是错误的。
- en: 'I tried to regenerate the question, and I got this other incorrect answer,
    this time not calling for any code generation but trying to work on it right away:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我尝试重新生成问题，得到了另一个不正确的答案，这次没有调用任何代码生成，而是尝试直接处理：
- en: '![](../Images/bc28e810687c5b8ad4972a029af8a370.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc28e810687c5b8ad4972a029af8a370.png)'
- en: Screenshot from interaction with ChatGPT’s free version while doing tests.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 从与 ChatGPT 免费版交互时的截图，进行测试时使用。
- en: Now here’s the “magic”.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在来看看“魔法”。
- en: 'What if we seed the prompt with the following sentence that DeepMind reports
    to substantially improve GPT-3.5-Turbo’s answers? (taken from Table 1 of the preprint):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们用 DeepMind 报告能显著改善 GPT-3.5-Turbo 答案的以下句子来引导提示会怎么样？（摘自预印本的表 1）：
- en: “*A little bit of arithmetic and a logical approach will help us quickly arrive
    at the solution to this problem.* ”?
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: “*一点算术和逻辑方法将帮助我们快速得到这个问题的解决方案。*”？
- en: 'Let’s try it out:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来试试：
- en: '![](../Images/a473821623b89b839b3cf8f2dc2cfba1.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a473821623b89b839b3cf8f2dc2cfba1.png)'
- en: Screenshot from interaction with ChatGPT’s free version while doing tests.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 从与 ChatGPT 免费版交互时的截图，进行测试时使用。
- en: That’s strictly speaking no regression, however, the logic is perfect and the
    result is correct!
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 严格来说，这不算回归，但逻辑是完美的，结果是正确的！
- en: Example 2, a problem on chemistry using the seed sentence found by DeepMind
    to best perform on **GSM8K**
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例 2，使用 DeepMind 发现的种子句子在 **GSM8K** 上表现最佳的化学问题
- en: 'GSM8K is a large dataset of high-quality and linguistically diverse grade-school
    math word problems created by human problem writers. DeepMind used this dataset
    to evaluate the capabilities of several LLMs, finding that for GPT-3.5-Turbo the
    best way to start the prompt is this:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: GSM8K是一个由人工问题编写者创建的高质量且语言多样的小学数学单词问题的大型数据集。DeepMind使用这个数据集来评估几个LLM的能力，发现对于GPT-3.5-Turbo，最佳的提示开始方式是：
- en: Analyze the given information, break down the problem into manageable steps,
    apply suitable mathematical operations, and provide a clear, accurate, and concise
    solution, ensuring precise rounding if necessary. Consider all variables and carefully
    consider the problem’s context for an efficient solution.
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 分析给定的信息，将问题分解为可管理的步骤，应用合适的数学运算，并提供一个清晰、准确、简洁的解决方案，如有必要，确保精确四舍五入。考虑所有变量，仔细考虑问题的背景，以便有效地解决问题。
- en: So, I took a problem on stoichiometry (in chemistry, how much of a product you
    get from a given amount of reagents or viceversa) and I asked ChatGPT to solve
    it, first without and then with the seed sentence.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我选择了一个关于化学计量学的问题（在化学中，就是根据给定的试剂量或反之，计算得到的产物量），并要求ChatGPT解决，首先是不带种子句子，然后是带上种子句子。
- en: 'Here’s what happened without any seed prompt:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这是没有任何种子提示的情况：
- en: '![](../Images/7da6125b5df9e8b7b66eb193e43b24d4.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7da6125b5df9e8b7b66eb193e43b24d4.png)'
- en: Screenshot from interaction with ChatGPT’s free version while doing tests (in
    this case composed side by side).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这是与ChatGPT免费版本互动的截图，在进行测试时（在这种情况下，侧边并排显示）。
- en: The answer is just wrong, because in (a) we asked for number of molecules, not
    moles, and in (b) the mass is what we asked for but the number is wrong.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 答案完全错误，因为在(a)中我们要求的是分子数量，而不是摩尔数，而在(b)中我们要求的是质量，但数字是错的。
- en: The right answers were 5.337E22 molecules and 10.41 g Zn(CN)2.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 正确答案是5.337E22分子和10.41克Zn(CN)2。
- en: 'Now let’s see what happened when I preceded the question by the seed sentence:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看当我在问题前加上种子句子时发生了什么：
- en: '![](../Images/cff6f674c1f8ec7ff999f09d3265edef.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cff6f674c1f8ec7ff999f09d3265edef.png)'
- en: Screenshot from interaction with ChatGPT’s free version while doing tests (in
    this case composed syde by side).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这是与ChatGPT免费版本互动的截图，在进行测试时（在这种情况下，侧边并排显示）。
- en: '**I. AM. JUST. AMAZED.**'
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**我。感到。惊讶。**'
- en: Both answers are just perfectly right! (And the workout is very detailed.)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 两个答案都是完全正确的！（并且计算过程非常详细。）
- en: 'I think I will have to rework on and rewrite this blog post I made long ago
    when GPT-3 was just out:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我想我得重新修订并重写我很久以前写的这篇博客，当时GPT-3刚刚发布：
- en: '[](/devising-tests-to-measure-gpt-3s-knowledge-of-the-basic-sciences-4bbfcde8286b?source=post_page-----e95fb7f4903c--------------------------------)
    [## Devising tests to measure GPT-3''s knowledge of the basic sciences'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/devising-tests-to-measure-gpt-3s-knowledge-of-the-basic-sciences-4bbfcde8286b?source=post_page-----e95fb7f4903c--------------------------------)
    [## 设计测试以测量GPT-3对基础科学的知识'
- en: Could students learn from OpenAI’s newest language model and use it as a 24/7
    consultant? Could students use it to…
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学生们能从OpenAI最新的语言模型中学习，并将其用作全天候顾问吗？学生们能用它来……
- en: towardsdatascience.com](/devising-tests-to-measure-gpt-3s-knowledge-of-the-basic-sciences-4bbfcde8286b?source=post_page-----e95fb7f4903c--------------------------------)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/devising-tests-to-measure-gpt-3s-knowledge-of-the-basic-sciences-4bbfcde8286b?source=post_page-----e95fb7f4903c--------------------------------)'
- en: The end of manual prompt engineering?
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 手动提示工程的终结？
- en: This and other recent developments indicate that we might be witnessing the
    end of the era of manual prompt engineering and the beginning of an era where
    you don’t need to master each AI’s prompting language in order to get what want.
    If you tried DALL-E 3 you surely found that it’s much better at understanding
    you, even using the same prompts you always used. Users can increasingly instruct
    models more naturally, and even let AI systems automatically create the prompts
    that will yield the desired results.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这些以及其他近期的发展表明，我们可能正在见证手动提示工程时代的结束，以及一个新的时代的开始，在这个时代中，你无需掌握每个AI的提示语言就能获得所需的结果。如果你尝试了DALL-E
    3，你一定会发现它在理解你的意图方面做得更好，即使使用的是你一直使用的相同提示。用户可以越来越自然地指示模型，甚至让AI系统自动生成能够产生所需结果的提示。
- en: 'AI optimizing AI: seems very powerful. What then? What about work?'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: AI优化AI：似乎非常强大。那么呢？工作呢？
- en: Historically, previous industrial and technological revolutions have given rise
    to new forms of work, often accompanied by significant economic and social transformations
    with unpredictable consequences. During those times, as jobs were automated, new
    roles emerged to meet the needs of each era -so jobs changed, but they were always
    there.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 从历史上看，以前的工业和技术革命产生了新的工作形式，通常伴随着重大的经济和社会变革，带来了不可预测的后果。在那些时代，随着工作的自动化，新的角色应运而生，以满足每个时代的需求——因此工作虽然改变了，但始终存在。
- en: But with the AI revolution, this could be different. We could be facing, for
    the first time, a force of automation capable of adapting to new challenges and
    learning tasks and jobs that will arise in the future, including how to control
    itself.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 但随着人工智能革命的到来，这可能会有所不同。我们可能首次面临一种能够适应新挑战并学习未来将出现的任务和工作的自动化力量，包括如何控制自身。
- en: What’s in for us in the immediate future?
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们在短期内会得到什么？
- en: Just touching on one improvable trait of AI models, DeepMind’s preprint showed
    that, without any actual retraining or fine-tuning but just by optimizing prompts,
    we can make LLMs to work much better, and surely the same holds for other generative
    AI models.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅触及AI模型的一种可改进的特征，DeepMind的预印本显示，单凭优化提示而不进行实际的再训练或微调，我们就可以让LLMs表现得更好，其他生成性AI模型也必定是如此。
- en: Such optimization of prompts is hard to match by humans, especially when the
    optimized prompts make sense but their exact forms aren’t obvious —see the second
    example I presented above where the optimal seed prompt is quite long and very
    specific.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这种提示的优化很难被人类匹配，特别是当优化的提示有意义但其确切形式并不明显时——见我上面提供的第二个示例，其中最优的种子提示相当长且非常具体。
- en: These findings and developments suggest that we are moving toward a future where
    AI systems will be capable of generating highly effective prompts on their own,
    making manual prompt engineering less necessary. The role of a prompt engineer
    may change as a result, and those prompt engineers who can’t adapt to the new
    automated procedures and learn where’s still space for human action will be left
    out of the race.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这些发现和进展表明，我们正朝着一个未来发展，在这个未来中，人工智能系统将能够自行生成非常有效的提示，从而减少对手动提示工程的需求。因此，提示工程师的角色可能会发生变化，那些无法适应新自动化程序并学习在哪里仍然有人工干预空间的提示工程师将被淘汰。
- en: It is essential to distinguish between two types of roles that may have different
    futures. On the one hand, if your job is to create prompts manually, your role
    might become less relevant and less requested as AI models become more proficient
    at generating their own prompts. On the other hand, if your role involves optimizing
    the use of AI models within broader systems where you combine them with inputs
    to and outputs from non-AI software or pieces of code, and possibly in other scenarios,
    your value in the workforce may be more resilient to automation and still useful
    in the longer term.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 区分可能具有不同未来的两种角色至关重要。一方面，如果你的工作是手动创建提示，随着人工智能模型在生成自身提示方面变得更加熟练，你的角色可能变得不那么相关且需求减少。另一方面，如果你的角色涉及在更广泛的系统中优化AI模型的使用，其中你将其与非AI软件或代码的输入和输出相结合，并可能在其他场景中使用，那么你在劳动力市场上的价值可能对自动化更具韧性，并在长期内仍然有用。
- en: To summarize my key point regarding this issue, just like some humans were quick
    enough to ride the train of prompt engineering, they must now remain aware that
    the landscape of AI and its interaction with humans is evolving very rapidly.
    Thus, staying up to date with literature like DeepMind’s preprint or this very
    blog article is essential, to know how to best adapt one’s own mastery of LLMs
    and other AI tools as they evolve.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 总结我对这一问题的关键观点，就像一些人曾经快速地跟上了提示工程的潮流一样，他们现在必须保持警觉，因为人工智能及其与人类的互动正迅速发展。因此，跟踪如DeepMind的预印本或这篇博客文章等文献，对于了解如何在人工智能工具和其他人工智能工具发展时最佳地调整自己的掌握是至关重要的。
- en: References
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'DeepMind’s preprint on arXiv:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: DeepMind在arXiv上的预印本：
- en: '[](https://arxiv.org/abs/2309.03409?source=post_page-----e95fb7f4903c--------------------------------)
    [## Large Language Models as Optimizers'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 大型语言模型作为优化器](https://arxiv.org/abs/2309.03409?source=post_page-----e95fb7f4903c--------------------------------)'
- en: Optimization is ubiquitous. While derivative-based algorithms have been powerful
    tools for various problems, the…
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 优化无处不在。虽然基于导数的算法已成为解决各种问题的强大工具，...
- en: arxiv.org](https://arxiv.org/abs/2309.03409?source=post_page-----e95fb7f4903c--------------------------------)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[arxiv.org](https://arxiv.org/abs/2309.03409?source=post_page-----e95fb7f4903c--------------------------------)'
- en: 'An interesting, related blog post, by another writer on Medium:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 另一位作家在Medium上的一篇有趣的相关博客文章：
- en: '[](https://medium.com/@minh.hoque/large-language-models-as-optimizers-explained-a20dc5e5c5af?source=post_page-----e95fb7f4903c--------------------------------)
    [## Large Language Models as Optimizers Explained'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@minh.hoque/large-language-models-as-optimizers-explained-a20dc5e5c5af?source=post_page-----e95fb7f4903c--------------------------------)
    [## 大型语言模型作为优化器的解释'
- en: Why this Paper is Important
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么这篇论文很重要
- en: medium.com](https://medium.com/@minh.hoque/large-language-models-as-optimizers-explained-a20dc5e5c5af?source=post_page-----e95fb7f4903c--------------------------------)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@minh.hoque/large-language-models-as-optimizers-explained-a20dc5e5c5af?source=post_page-----e95fb7f4903c--------------------------------)
- en: '[***www.lucianoabriata.com***](https://www.lucianoabriata.com/) *I write about
    everything that lies in my broad sphere of interests: nature, science, technology,
    programming, etc.* [***Subscribe to get my new stories***](https://lucianosphere.medium.com/subscribe)
    ***by email****. To* ***consult about small jobs*** *check my* [***services page
    here***](https://lucianoabriata.altervista.org/services/index.html)*. You can*
    [***contact me here***](https://lucianoabriata.altervista.org/office/contact.html)***.***
    *You can* [***tip me here***](https://paypal.me/LAbriata)*.*'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[***www.lucianoabriata.com***](https://www.lucianoabriata.com/) *我写的内容涵盖了我广泛兴趣领域的一切：自然、科学、技术、编程等。*
    [***订阅以获取我的新故事***](https://lucianosphere.medium.com/subscribe) ***通过电子邮件****。要*
    ***咨询小项目*** *请查看我的* [***服务页面***](https://lucianoabriata.altervista.org/services/index.html)*。你可以*
    [***在这里联系我***](https://lucianoabriata.altervista.org/office/contact.html)***。***
    *你也可以* [***在这里给我小费***](https://paypal.me/LAbriata)*。'
