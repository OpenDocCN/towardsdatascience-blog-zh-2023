- en: New DeepMind Work Unveils Supreme Prompt Seeds for Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/new-deepmind-work-unveils-supreme-prompt-seeds-for-language-models-e95fb7f4903c](https://towardsdatascience.com/new-deepmind-work-unveils-supreme-prompt-seeds-for-language-models-e95fb7f4903c)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How computationally optimized prompts make language models excel, and how this
    all affects prompt engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://lucianosphere.medium.com/?source=post_page-----e95fb7f4903c--------------------------------)[![LucianoSphere
    (Luciano Abriata, PhD)](../Images/a8ae3085d094749bbdd1169cca672b86.png)](https://lucianosphere.medium.com/?source=post_page-----e95fb7f4903c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e95fb7f4903c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e95fb7f4903c--------------------------------)
    [LucianoSphere (Luciano Abriata, PhD)](https://lucianosphere.medium.com/?source=post_page-----e95fb7f4903c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e95fb7f4903c--------------------------------)
    ·11 min read·Nov 8, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1fb04d056a98fae3386085e529a642a9.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Ali Shah Lakhani](https://unsplash.com/@alishahlakhani?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: 'As we witness the steady advancement of artificial intelligence (AI), accomplishing
    increasingly difficult tasks month after month, there is a general concern about
    the future of our workforce. If AI continues to automate many of the tasks currently
    performed by humans, what will the occupations of tomorrow look like? There is
    this idea in the air that “*programming these systems will be human work for years*”,
    or that “*we’ll always need a human to maintain and retrain AI models*”, or that
    “*crafting efficient prompts that drive AI models the right way is a human skill*”.
    The latter, the focus of this article, motivated the creation of “prompt engineering”
    as a “career”. And certainly there is some mastery in writing efficient prompts
    to get the AI model do exactly what one expects from it, or to have it “think”
    well enough to improve its answers especially to problems. See this, as just one
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/crafting-effective-prompts-for-summarization-using-large-language-models-dbbdf019f664?source=post_page-----e95fb7f4903c--------------------------------)
    [## Crafting Effective Prompts for Summarization Using Large Language Models'
  prefs: []
  type: TYPE_NORMAL
- en: Distilling key points after >2 years of experience and from AI developers’ own
    tutorials, hands-on and with examples.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/crafting-effective-prompts-for-summarization-using-large-language-models-dbbdf019f664?source=post_page-----e95fb7f4903c--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: However, it is very likely that none of these human interventions will remain
    relevant forever. In particular for prompt engineering, it looks like these skills
    won’t be much relevant, already soon. Read on to know why and in the process learn
    about the very interesting findings reported by DeepMind in a recent preprint,
    which you can apply right away to your own benefit when using large language models
    (LLMs) as I show you with hands-on examples on ChatGPT’s free version.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before delving deeper, we need to understand the concept of “prompt”. Prompts
    are instructions passed to AI models to tell them what we want them to do. AI
    models respond to the user-generated text inputs, or prompts, to generate their
    outputs -text, graphics, audio, etc. The quality and specificity of the input
    prompts significantly affect the content and quality of outputs generated by the
    models. Moreover, different users will likely have different ways to make a request
    or ask a question, and not all of them will be as efficient in producing the expected
    answers with correct information.
  prefs: []
  type: TYPE_NORMAL
- en: In the past, the art of formulating effective prompts was not very well understood.
    Already in 2019 OpenAI revealed that adding “tl;dr” (commonly used on the internet
    to request a summary) at the end of a text input could make the model summarize
    the preceding text. Although this was merely an academic curiosity back then,
    over time researchers and enthusiasts started to discover that specific phrasings
    could unlock enhanced potential from these AI models. It was precisely this what
    gave rise to the “field” and “profession” of “prompt engineer(ing)”.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineers became experts in crafting prompts that would elicit desired
    responses from AI models. They shared their “magic phrases” and techniques on
    the internet, effectively creating a new area of expertise. The demand for these
    professionals quickly grew, and jobs for prompt engineers began to appear, highlighting
    the significant impact these individuals had on the AI landscape.
  prefs: []
  type: TYPE_NORMAL
- en: 'What prompt engineers basically do is to optimize prompts for best outputs.
    But optimization is a task that computers do VERY well; hence, it is very likely
    that they can replace human prompt engineers. And this might be just about to
    happen: In their recent preprint, DeepMind showed that AI models can be used to
    optimize their own inputs, thus making “prompt engineering” kind of obsolete and
    inefficient, at least in the traditional way.'
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models as Optimizers of Their Own Prompts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The recent paper from DeepMind titled “[Large Language Models as Optimizers](https://arxiv.org/abs/2309.03409)”
    explores how AI models can be used to effectively optimize… their own input prompts!
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see what this is, how it relates to training and optimization, and what
    exactly DeepMind’s work found out. Oh and let’s try out some computer-optimized
    prompts ourselves. You’ll be amazed.
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, in machine learning, the optimization process involves adjusting
    the internal parameters of a model (“weights”, just a large number of numbers
    describing how the different artificial neurons connect) to minimize error. Typically,
    during the stage of training the model is presented with a large array of known
    input-output pairs, and all the weights are optimized in the traditional mathematical
    sense so that the model “learns”. However, once a model is trained, one can still
    use it by providing inputs in different ways, that will propagate differently
    depending on the set of weights calculated during training, and hence their outputs
    will be different… some better, some worse. One can then optimize how one provides
    the inputs, which in the special case of language models are the prompts. And
    as any optimization protocol, we humans can do this by trial and error but of
    course computers are much better than us—it was just a matter of somebody telling
    them to do it!
  prefs: []
  type: TYPE_NORMAL
- en: The idea explored in DeepMind’s work is to use various AI models to generate
    prompts for specific tasks and then test the effectiveness of the different prompts
    in achieving the desired outcomes. For example, if the task is to solve a mathematical
    problem, the user may input a very simple prompt that only asks the problem directly,
    or he/she might precede the question with a seed sentence like “Solve this mathematical
    problem step by step.”, or “Let’s work on this stepwise.”, etc. In some cases,
    the seed sentences might help improve the AI’s output, even if the question itself
    is asked in exactly the same way.
  prefs: []
  type: TYPE_NORMAL
- en: Based on this idea, what DeepMind’s engineers did was to try different seed
    prompts followed by exactly the same question posing a problem and evaluate the
    quality (correctness) of the answers. They repeated this procedure applying a
    series of prompts to several different problems, and finally, they counted how
    many right answers each seed sentence produced, to compare the effects on each
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: From experience, we know that this strategy should work out to find how to optimize
    the prompts. But of course, it’s impossible for us humans to sample this largely.
    By using a computer architecture, DeepMind could run the procedure at large scale,
    and on different kinds of questions and problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The results were surprising: the same problem could be solved correctly only
    around 50% of the times if no or a bad seed sentence was used, and up to 80% correctly
    when a good seed sentence was utilized.'
  prefs: []
  type: TYPE_NORMAL
- en: DeepMind’s approach involved using metrics from these tests to guide the creation
    of better prompts. Their conclusion was then that by continuously iterating on
    the prompts and considering the models’ responses, AI systems can improve their
    prompt generation process with a very important effect on the outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this approach does not involve updating the model’s internal parameters
    as one would do when training the model, but rather focuses on optimizing the
    inputs themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Some interesting hands-on examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here are some interesting examples from my own tests inspired by DeepMind’s
    work, that you can also try right away using ChatGPT’s free version (which is
    powered by the model that shows up in DeepMind’s preprint as GPT-3.5-Turbo).
  prefs: []
  type: TYPE_NORMAL
- en: Example 1, copying the idea from one of the problems shown in DeepMind’s preprint
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first example from DeepMind’s paper that called my attention was the request
    to perform linear regression, because since these models can’t in principle do
    math, then I’d expect them to never work, independently of the provided prompt.
  prefs: []
  type: TYPE_NORMAL
- en: If you ask ChatGPT to perform a linear regression on very simple data like say
    *x = [1, 2, 3, 4]* and *y = [2, 4, 6, 8],* you will find that it solves it right
    away correctly. But what if we challenge it with a more difficult linear regression?
    Let’s see.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here I generated synthetic data for various *x* values scattered randomly between
    0 and 12, and calculated *y* values without noise by using the equation *y = 3.5
    x — 11.5* in a regular spreadsheet program. Then, I asked the program to “Find
    the linear equation that describes this data:” followed by the x, y pairs. Just
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4837401b4c71f4cdf147eb48feec8694.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot from interaction with ChatGPT’s free version while doing tests.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the answer I got:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0b7ba18f04a582e1d3063077c8b570cd.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot from interaction with ChatGPT’s free version while doing tests.
  prefs: []
  type: TYPE_NORMAL
- en: You see clearly that the answer is wrong, and also that it goes through some
    coding that I didn’t ask about. Even more confusing, the code looks itself correct
    and would have likely led to the right solution, but the “result” presented by
    the text generation is wrong.
  prefs: []
  type: TYPE_NORMAL
- en: 'I tried to regenerate the question, and I got this other incorrect answer,
    this time not calling for any code generation but trying to work on it right away:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bc28e810687c5b8ad4972a029af8a370.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot from interaction with ChatGPT’s free version while doing tests.
  prefs: []
  type: TYPE_NORMAL
- en: Now here’s the “magic”.
  prefs: []
  type: TYPE_NORMAL
- en: 'What if we seed the prompt with the following sentence that DeepMind reports
    to substantially improve GPT-3.5-Turbo’s answers? (taken from Table 1 of the preprint):'
  prefs: []
  type: TYPE_NORMAL
- en: “*A little bit of arithmetic and a logical approach will help us quickly arrive
    at the solution to this problem.* ”?
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try it out:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a473821623b89b839b3cf8f2dc2cfba1.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot from interaction with ChatGPT’s free version while doing tests.
  prefs: []
  type: TYPE_NORMAL
- en: That’s strictly speaking no regression, however, the logic is perfect and the
    result is correct!
  prefs: []
  type: TYPE_NORMAL
- en: Example 2, a problem on chemistry using the seed sentence found by DeepMind
    to best perform on **GSM8K**
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'GSM8K is a large dataset of high-quality and linguistically diverse grade-school
    math word problems created by human problem writers. DeepMind used this dataset
    to evaluate the capabilities of several LLMs, finding that for GPT-3.5-Turbo the
    best way to start the prompt is this:'
  prefs: []
  type: TYPE_NORMAL
- en: Analyze the given information, break down the problem into manageable steps,
    apply suitable mathematical operations, and provide a clear, accurate, and concise
    solution, ensuring precise rounding if necessary. Consider all variables and carefully
    consider the problem’s context for an efficient solution.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: So, I took a problem on stoichiometry (in chemistry, how much of a product you
    get from a given amount of reagents or viceversa) and I asked ChatGPT to solve
    it, first without and then with the seed sentence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s what happened without any seed prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7da6125b5df9e8b7b66eb193e43b24d4.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot from interaction with ChatGPT’s free version while doing tests (in
    this case composed side by side).
  prefs: []
  type: TYPE_NORMAL
- en: The answer is just wrong, because in (a) we asked for number of molecules, not
    moles, and in (b) the mass is what we asked for but the number is wrong.
  prefs: []
  type: TYPE_NORMAL
- en: The right answers were 5.337E22 molecules and 10.41 g Zn(CN)2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s see what happened when I preceded the question by the seed sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cff6f674c1f8ec7ff999f09d3265edef.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot from interaction with ChatGPT’s free version while doing tests (in
    this case composed syde by side).
  prefs: []
  type: TYPE_NORMAL
- en: '**I. AM. JUST. AMAZED.**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Both answers are just perfectly right! (And the workout is very detailed.)
  prefs: []
  type: TYPE_NORMAL
- en: 'I think I will have to rework on and rewrite this blog post I made long ago
    when GPT-3 was just out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/devising-tests-to-measure-gpt-3s-knowledge-of-the-basic-sciences-4bbfcde8286b?source=post_page-----e95fb7f4903c--------------------------------)
    [## Devising tests to measure GPT-3''s knowledge of the basic sciences'
  prefs: []
  type: TYPE_NORMAL
- en: Could students learn from OpenAI’s newest language model and use it as a 24/7
    consultant? Could students use it to…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/devising-tests-to-measure-gpt-3s-knowledge-of-the-basic-sciences-4bbfcde8286b?source=post_page-----e95fb7f4903c--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: The end of manual prompt engineering?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This and other recent developments indicate that we might be witnessing the
    end of the era of manual prompt engineering and the beginning of an era where
    you don’t need to master each AI’s prompting language in order to get what want.
    If you tried DALL-E 3 you surely found that it’s much better at understanding
    you, even using the same prompts you always used. Users can increasingly instruct
    models more naturally, and even let AI systems automatically create the prompts
    that will yield the desired results.
  prefs: []
  type: TYPE_NORMAL
- en: 'AI optimizing AI: seems very powerful. What then? What about work?'
  prefs: []
  type: TYPE_NORMAL
- en: Historically, previous industrial and technological revolutions have given rise
    to new forms of work, often accompanied by significant economic and social transformations
    with unpredictable consequences. During those times, as jobs were automated, new
    roles emerged to meet the needs of each era -so jobs changed, but they were always
    there.
  prefs: []
  type: TYPE_NORMAL
- en: But with the AI revolution, this could be different. We could be facing, for
    the first time, a force of automation capable of adapting to new challenges and
    learning tasks and jobs that will arise in the future, including how to control
    itself.
  prefs: []
  type: TYPE_NORMAL
- en: What’s in for us in the immediate future?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just touching on one improvable trait of AI models, DeepMind’s preprint showed
    that, without any actual retraining or fine-tuning but just by optimizing prompts,
    we can make LLMs to work much better, and surely the same holds for other generative
    AI models.
  prefs: []
  type: TYPE_NORMAL
- en: Such optimization of prompts is hard to match by humans, especially when the
    optimized prompts make sense but their exact forms aren’t obvious —see the second
    example I presented above where the optimal seed prompt is quite long and very
    specific.
  prefs: []
  type: TYPE_NORMAL
- en: These findings and developments suggest that we are moving toward a future where
    AI systems will be capable of generating highly effective prompts on their own,
    making manual prompt engineering less necessary. The role of a prompt engineer
    may change as a result, and those prompt engineers who can’t adapt to the new
    automated procedures and learn where’s still space for human action will be left
    out of the race.
  prefs: []
  type: TYPE_NORMAL
- en: It is essential to distinguish between two types of roles that may have different
    futures. On the one hand, if your job is to create prompts manually, your role
    might become less relevant and less requested as AI models become more proficient
    at generating their own prompts. On the other hand, if your role involves optimizing
    the use of AI models within broader systems where you combine them with inputs
    to and outputs from non-AI software or pieces of code, and possibly in other scenarios,
    your value in the workforce may be more resilient to automation and still useful
    in the longer term.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize my key point regarding this issue, just like some humans were quick
    enough to ride the train of prompt engineering, they must now remain aware that
    the landscape of AI and its interaction with humans is evolving very rapidly.
    Thus, staying up to date with literature like DeepMind’s preprint or this very
    blog article is essential, to know how to best adapt one’s own mastery of LLMs
    and other AI tools as they evolve.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'DeepMind’s preprint on arXiv:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://arxiv.org/abs/2309.03409?source=post_page-----e95fb7f4903c--------------------------------)
    [## Large Language Models as Optimizers'
  prefs: []
  type: TYPE_NORMAL
- en: Optimization is ubiquitous. While derivative-based algorithms have been powerful
    tools for various problems, the…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: arxiv.org](https://arxiv.org/abs/2309.03409?source=post_page-----e95fb7f4903c--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'An interesting, related blog post, by another writer on Medium:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@minh.hoque/large-language-models-as-optimizers-explained-a20dc5e5c5af?source=post_page-----e95fb7f4903c--------------------------------)
    [## Large Language Models as Optimizers Explained'
  prefs: []
  type: TYPE_NORMAL
- en: Why this Paper is Important
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@minh.hoque/large-language-models-as-optimizers-explained-a20dc5e5c5af?source=post_page-----e95fb7f4903c--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '[***www.lucianoabriata.com***](https://www.lucianoabriata.com/) *I write about
    everything that lies in my broad sphere of interests: nature, science, technology,
    programming, etc.* [***Subscribe to get my new stories***](https://lucianosphere.medium.com/subscribe)
    ***by email****. To* ***consult about small jobs*** *check my* [***services page
    here***](https://lucianoabriata.altervista.org/services/index.html)*. You can*
    [***contact me here***](https://lucianoabriata.altervista.org/office/contact.html)***.***
    *You can* [***tip me here***](https://paypal.me/LAbriata)*.*'
  prefs: []
  type: TYPE_NORMAL
