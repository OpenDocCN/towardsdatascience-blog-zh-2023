["```py\nimport os\nos.environ['OPENAI_API_KEY'] = 'Enter your API Key here'\n\n# necessary langchain imports\nimport langchain\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.embeddings.cache import CacheBackedEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.storage import LocalFileStore\nfrom langchain.document_loaders import PyPDFDirectoryLoader\nfrom langchain.chains import RetrievalQA\nfrom langchain.chat_models import ChatOpenAI\n```", "```py\n# where our embeddings will be stored\nstore = LocalFileStore(\"./cache/\")\n\n# instantiate a loader: this loads our data, use PDF in this case\nloader = PyPDFDirectoryLoader(\"sagemaker-articles/\")\n\n# by default the PDF loader both loads and splits the documents for us\npages = loader.load_and_split()\nprint(len(pages))\n```", "```py\n# instantiate embedding model\nembeddings_model = OpenAIEmbeddings()\n\nembedder = CacheBackedEmbeddings.from_bytes_store(\n    embeddings_model,\n    store\n)\n```", "```py\n# create vector store, we use FAISS in this case\nvector_store = FAISS.from_documents(pages, embedder)\n```", "```py\n# this is the entire retrieval system\nmedium_qa_chain = RetrievalQA.from_chain_type(\n    llm=ChatOpenAI(),\n    retriever=vector_store.as_retriever(),\n    return_source_documents=True,\n    verbose=True\n)\n```", "```py\nsample_prompts = [\"What does Ram Vegiraju write about?\",\n                 \"What is Amazon SageMaker?\",\n                 \"What is Amazon SageMaker Inference?\",\n                 \"What are the different hosting options for Amazon SageMaker?\",\n                 \"What is Serverless Inference with Amazon SageMaker?\",\n                 \"What's the difference between Multi-Model Endpoints and Multi-Container Endpoints?\",\n                 \"What SDKs can I use to work with Amazon SageMaker?\"]\n\nfor prompt in sample_prompts:\n\n    #vanilla OpenAI Response\n    response = openai.Completion.create(\n        engine=\"text-davinci-003\",\n        prompt=prompt,\n        max_tokens = 500)\n\n    # RAG Augmented Response\n    response_rag = medium_qa_chain({\"query\":prompt})\n```"]