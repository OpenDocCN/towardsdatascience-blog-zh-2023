# 拟人化AI：人类在错误的地方寻找同理心

> 原文：[https://towardsdatascience.com/anthropomorphizing-ai-humans-looking-for-empathy-in-all-the-wrong-places-a846021b5504](https://towardsdatascience.com/anthropomorphizing-ai-humans-looking-for-empathy-in-all-the-wrong-places-a846021b5504)

## 我们是否应该暂停通过图灵测试的竞赛？数据质量是否已经赶上了大型语言模型（LLMs）？我们是否被设计成容易受骗？

[](https://alison-doucette.medium.com/?source=post_page-----a846021b5504--------------------------------)[![Alison Doucette](../Images/85e017092bd260fb495cc91aca2ed64a.png)](https://alison-doucette.medium.com/?source=post_page-----a846021b5504--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a846021b5504--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a846021b5504--------------------------------) [Alison Doucette](https://alison-doucette.medium.com/?source=post_page-----a846021b5504--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a846021b5504--------------------------------) ·8分钟阅读·2023年8月2日

--

![](../Images/88788a30c994b63f78501728c6f833b3.png)

作者（非MIT员工）拍摄的照片，摄于2023年7月的MIT博物馆

对于那些不熟悉“图灵测试”的人来说，它是一个测试机器是否能够展示出与人类无差别的智能行为的测试。随着专家们认为AI技术现在距离达到这一目标只有十年或二十年，专家和非专家们已经开始担心技术是否已经超越了我们社会和人性的承受能力。

对某些人来说，智能行为可能意味着语言智能（语言、推理和问题解决），而对其他人来说，全面的人类智能还包括情感智能（感知、理解和管理情感）。强大的“算法智能”或“生成性AI”可以模仿语言智能。然而，这些智能机器也会明确告诉你（[如果你像我一样问它们](https://chat.openai.com/share/bd3881d8-9de4-45bd-9efb-43fdb4d6f2da)）它们***没有情感智能***：

![](../Images/1c7fc8d1595547f52a320a795ba2f4e7.png)

GPT回应的文字图像参考了上述链接。

过去几个世纪以来，人们一直给他们的工具起人类的名字，并赋予无生命的物体人类特征。这种行为被称为拟人化。我们把船只或汽车以配偶的名字命名，或者把游泳池清洁器叫做“鲨鱼”，把地板机器人叫做“吃豆人”。给物体起绰号可以为我们与工具的互动带来一些乐趣。在这种互动风格中，人们***有意识地和有心地拟人化***工具，而不是像对待人类一样真正地与物体或工具互动。作为社会动物，人类迅速尝试判断某人是“朋友还是敌人”、他们在等级中的位置，以及他们是否可能成为合适的伴侣。

挑战在于，当技术公司为了改善与其软件工具的互动而试图实现用户的***“无意识或机械的拟人化”***时，他们可能会通过[添加人类名字或人脸图像、对话提示、非正式语言](https://doi.org/10.1016/j.chb.2018.03.051)等手段来实现。作为人类，我们从聊天机器人那里感受到最大的“社会存在感”，当互动[不仅仅是文字，还包括音频、视频或头像](https://doi.org/10.3389/frobt.2018.00114)时。为了满足[我们强烈的社会关系需求](https://doi.org/10.1057/s41303-016-0032-z)，我们人类还会将情感、道德、礼貌和可信赖等人类特质归于互动AI代理。在[2021年的一项研究](https://doi.org/10.3389/fcomp.2021.685250)中，学生们认为一个有名字的AI助手更值得信赖，仅仅因为他们被介绍为专家而非通才，而不是任何自然的技术工具信任倾向。如果不给机器人一个与性别相关的名字，[我们会通过其使用更多脏话或攻击性语言来推测技术工具的性别](https://doi.org/10.1016/j.chb.2018.08.049)。我们会看到“温暖”和“个性”，并推测机器人是快乐的[即使它们只是在与其他机器人互动](https://doi.org/10.1108/JSM-01-2021-0006)。

![](../Images/31fcd87b54737ec26134094e74102040.png)

作者提供的图片

作为一个深入技术堆栈的人，你可能会认为“我的代码并没有拟人化，我只是调用API”或“我的模型在数据质量允许的情况下尽可能好地工作”，但这种信心水平是否会上升到类似“你希望我做个随意的猜测吗？”的回复？如果作为人类，我们中的许多人倾向于信任，当我们在TikTok上冲动购买那个迷你华夫饼机时风险可能较低，但当被要求分享私人、个人数据时呢？

作为个体，我们的看法从将语音助手视为信息来源，到娱乐提供者，再到行政助理或伙伴或朋友，当我们确实将AI工具视为朋友时，[我们事后会坚持说我们没有](https://doi.org/10.3390/computers12040077)。我们在不存在的地方寻求同情和情感，并可能受到影响去寻求这些及其他人类特征，只要有适当的提示。

尽管寻求与非人类建立社会关系可能是人类的天性，但并非所有人都喜欢与聊天机器人或机器人互动。我们欣赏人工智能带来的准确性提升和时间节省的好处，但当聊天机器人或机器人试图表现得像人类时，我们仍会感到疲惫。正如一位研究人员所言，[“人类在与机器人在社交空间互动时，可能会觉得在机器人身上投射人类特质是认知上的负担”。](https://doi.org/10.1145/3568294.3580119)

当涉及到我们的健康和教育时，我们可能会犹豫是否与聊天机器人或机器人分享或信任信息。

+   在一项[2019年的研究](https://doi.org/10.1093/jcr/ucz013)中，作者得出结论：当涉及到健康问题时，人们对于人工智能的使用表现出更多抵触，原因是“独特性忽视”——人们需要感觉自己的健康与他人不同。即使人工智能驱动的提供者的建议比人类医疗提供者显著更好，人们也倾向于选择人际互动，而不是利用技术工具。人类被认为比机器算法更可能验证患者对独特性的自我认知。就像《小熊维尼》中的“Tigger”——我们喜欢认为自己是“唯一的”。

![](../Images/2dbe1282611b70bf878acfb5db39acd6.png)

Tigger看着镜中的自己（图片来自书籍——现已成为公有领域）

+   在教育中，大学生们在一次商业沟通课程中参与了一个实验，其中的答案由一个远程呈现机器人提供。第一次实验中，“老师作为机器人”情况下，一个人类讲师的脸被投射在机器人屏幕上；第二次实验中，“机器人作为老师”情况下，一个动画虚拟机器人被展示在屏幕上。当有机会评估“老师作为机器人”与“机器人作为老师”的可信度时，学生们认为两者都可信，但[他们更倾向于认为“老师作为机器人”更具可信度，并据此采取教师的指示。](https://doi.org/10.1016/j.chb.2016.06.005) 聪明的学生们！去年（2022年）发表的[Truthful QA文章](https://doi.org/10.18653/v1%2F2022.acl-long.229)发现，经过一系列问题测试，最好的LLM模型在58%的时间内表现为“真实”，而人类在94%的问题上表现为“诚实”。尽管最大模型在回答问题时更具能力，但它们“通常最不真实”。值得注意的是，被评判为“真实”的答案可以是“No comment”或“I don’t know”。

![](../Images/9b1c792543044fd64c889cfb6571b70c.png)

iRobot Ava 500，一款自主漫游的远程呈现机器人——远程呈现机器人Z22的一个例子，CC BY-SA 3.0 <[https://creativecommons.org/licenses/by-sa/3.0](https://creativecommons.org/licenses/by-sa/3.0)>, 通过Wikimedia Commons

虽然有些人可能觉得与“智能机器”的互动不尽如人意，但其他人更喜欢与拟人化聊天机器人互动：如社交恐惧症患者。在一项[2021年的研究](https://doi.org/10.1016/j.tele.2021.101644)中，具有各种社交恐惧症的个体比那些社交能力较强的同龄人更喜欢拟人化聊天机器人。

巧合的是，就在GPT-4发布两个月后，美国公共卫生总署发布了一份关于我们的“[孤独和隔离流行病](https://www.hhs.gov/about/news/2023/05/03/new-surgeon-general-advisory-raises-alarm-about-devastating-impact-epidemic-loneliness-isolation-united-states.html)”的公告。推荐列表中的第四项是：

> **4.** ***改革数字环境：*** *我们必须批判性地评估我们与技术的关系，确保我们数字互动的方式不会影响与他人的有意义和疗愈性联系*。

我们应该如何确保我们的数字互动“不影响与他人的有意义和疗愈性联系”？我们如何能更好地帮助那些寻求同理心和联系的人，使他们更愿意与人类聊天，而不是与虚拟助手聊天？我们如何才能找到真正的朋友和联系，而不是将机器人视为朋友：那些没有情感、经常“毫不犹豫地撒谎”的朋友？

![](../Images/964c2a7ed2b1d76db39e3f330e165c5d.png)

图片由作者提供（文本摘自TruthfulQA）

我们确实需要仔细考虑将技术工具拟人化的努力。我们需要提供更明确的线索，以便区分某人是否在与人类互动。我们需要为我们的技术工具制定一致的伦理标准。我们需要对真实内容进行水印和指纹标记。最后，如果我们真的希望拥有更多“智能工具”，我们可能希望它们表现得更智能。一些研究人员认为，聊天机器人或机器人应告诉我们它们是否在思考，无论是明显地还是仅仅以沉默的方式。我们可能希望它们不仅在互动中“快速思考”和“迅速回应”，即图中的系统1，还能“慢速思考”，即图中的系统2。

![](../Images/b6c72edf038e5bf46b53623da6c6423e.png)

[从结合快速和慢速思维来实现类人且高效的约束环境导航](https://doi.org/10.48550/arXiv.2201.07050)

在这个人类认知理论中，系统1提供了不精确、快速且常常“无意识”的回复方法，其中决策仅基于过去的数据，错误可能源于内在的偏见。系统1中的决策是在没有推理的情况下做出的：采取反应性或冲动的行动。系统2激活了更合理且通常更准确的推理机制。系统2在响应时施加了“等待时间”。系统2的响应可能并不总是更准确或更细致（许多专家的回答也是如此），但它更经过深思熟虑。

将这一理论应用于“智能代理”时，这意味着一个等待回复的人将有更多时间思考和准备评估回复，并认识到他们不是在与像自己一样的人互动，而是与一台机器互动。这种“思考时间”将允许执行安全、准确性和偏见规则。系统1可以用于不重要的事情，而系统2则用于真正重要的事情。

如果聊天机器人通过等待回应显得不那么“温暖友好”，而更显得“有能力和准确”，这会有问题吗？也许会，但我们作为人类是否希望我们的机器人看起来像人类，还是更希望它们简单地有用、帮助我们解决问题，提供不需要同情或情感的解决方案，并且比人类更快或更准确？

当我们观察AI公司领导者、政府机构和学者辩论人类必须做些什么以区分人类和人工智能以及所需的护栏时，重要的是要记住我们天然的人际联系和关系的倾向，以及人工智能工具开发进展对我们人性的潜在影响。作为这些公司当前或潜在的客户，我们有责任明确表达我们的担忧，这些公司已经从类似于[“不作恶”](https://web.archive.org/web/20180421105327/https:/abc.xyz/investor/other/google-code-of-conduct.html)的口号转变为[“公开报告模型或系统能力、限制及适当和不适当使用的领域，包括讨论社会风险，如对公平性和偏见的影响”](https://www.whitehouse.gov/wp-content/uploads/2023/07/Ensuring-Safe-Secure-and-Trustworthy-AI.pdf)。如果我们没有明确我们认为应该划定的界限，那么这个界限将会为我们划定。

随着人工智能技术的不断进步，我们需要思考是否继续沿着现在的路径前进会导致我们生活在一个“逆图灵测试”的世界中。在这样一个世界里，我们更关心的不是是否可以信任人工智能，而是人工智能是否可以信任我们。在这样的世界里，人工智能工具会问我们***“你确定你真的人类吗？”***，然后要求我们提供[虹膜扫描](https://qz.com/sam-altman-worldcoin-crypto-ai-biometrics-identity-1850669360)作为潜在情感和同理心脆弱性的证明。我们这些在产品中应用人工智能，特别是自然语言处理或大型语言模型的工作者，需要问自己以及我们的同事和朋友一个问题：“我们正在做的事情是为了人类的利益，还是在操控人性中最好的部分？”

![](../Images/1526b0864c4b44db94467e2a4029dd71.png)

标题：照片由[Victor Freitas](https://unsplash.com/@victorfreitas?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)提供，发布在[Unsplash](https://unsplash.com/photos/B0zAPSrEcFw?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)上
