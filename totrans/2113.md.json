["```py\nimport re\n\nimport urllib3\nimport numpy as np\nimport pandas as pd\n\n# Subset of stations on the coast of Florida\nSTATION_LIST = ['41009', 'SPGF1', 'VENF1',\n                '42036', 'SAUF1', 'FWYF1',\n                'LONF1', 'SMKF1']\n\n# URL template\nURL = 'https://www.ndbc.noaa.gov/view_text_file.php?filename={station_id}h{year}.txt.gz&dir=data/historical/stdmet/'\n\ndef read_buoy_remote(station_id: str, year: int):\n    TIME_COLUMNS = ['YYYY', 'MM', 'DD', 'hh']\n\n    # formatting the URL\n    file_url = URL.format(station_id=station_id.lower(), year=year)\n\n    http = urllib3.PoolManager()\n\n    # get request\n    response = http.request('GET', file_url)\n\n    # decoding\n    lines = response.data.decode().split('\\n')\n\n    # lots of data cleaning below\n    data_list = []\n    for line in lines:\n        line = re.sub('\\s+', ' ', line).strip()\n        if line == '':\n            continue\n        line_data = line.split(' ')\n        data_list.append(line_data)\n\n    df = pd.DataFrame(data_list[2:], columns=data_list[0]).astype(float)\n    df[(df == 99.0) | (df == 999.0)] = np.nan\n\n    if 'BAR' in df.columns:\n        df = df.rename({'BAR': 'PRES'}, axis=1)\n\n    if '#YY' in df.columns:\n        df = df.rename({'#YY': 'YYYY'}, axis=1)\n\n    if 'mm' in df.columns:\n        TIME_COLUMNS += ['mm']\n\n    df[TIME_COLUMNS] = df[TIME_COLUMNS].astype(int)\n\n    if 'mm' in df.columns:\n        df['datetime'] = \\\n            pd.to_datetime([f'{year}/{month}/{day} {hour}:{minute}'\n                            for year, month, day, hour, minute in zip(df['YYYY'],\n                                                                      df['MM'],\n                                                                      df['DD'],\n                                                                      df['hh'],\n                                                                      df['mm'])])\n\n    else:\n        df['datetime'] = \\\n            pd.to_datetime([f'{year}/{month}/{day} {hour}:00'\n                            for year, month, day, hour in zip(df['YYYY'],\n                                                              df['MM'],\n                                                              df['DD'],\n                                                              df['hh'])])\n\n    df = df.drop(TIME_COLUMNS, axis=1)\n\n    df.set_index('datetime', inplace=True)\n\n    return df\n```", "```py\nimport numpy as np\nimport pandas as pd\n\nfrom config import ASSETS, OUTPUTS\n\nPART = 'Part 5'\nassets = ASSETS[PART]\n\n# focusing on hail and thunderstorm events\nTARGET_EVENTS = ['Hail', 'Thunderstorm Wind']\n# wind speed, wave height, pressure, water temp, avg wave period\nMETEOROLOGICAL_DATA = ['WSPD', 'WVHT', 'PRES', 'WTMP', 'APD']\n\n# using past 4 hours as explanatory variables\nN_LAGS = 4\n# forecasting events in the next 12 hours\nHORIZON = 12\n\n# loading storm events data\nstorms = pd.read_csv(f'{assets}/storms_data.csv', index_col='storm_start')\nstorms.index = pd.to_datetime(storms.index)\nhail_df = storms.loc[storms['EVENT_TYPE'].isin(TARGET_EVENTS), :]\n\n# loading the meteorological data\nbuoys = pd.read_csv(f'{assets}/buoys.csv', index_col='datetime')\nbuoys.index = pd.to_datetime(buoys.index).tz_localize('UTC')\nbuoys['STATION'] = buoys['STATION'].astype(str)\n# resampling the data to an hourly granularity\nbuoys_h = buoys.groupby('STATION').resample('H').mean()\n# subsetting the variables\nbuoys_h = buoys_h[METEOROLOGICAL_DATA]\nbuoys_df = buoys_h.reset_index('STATION')\n\n# getting all unique time steps\nbase_index = buoys_df.index.unique()\n\n# getting list of stations\nstation_list = buoys_df['STATION'].unique().tolist()\n\nX, y = [], []\n# iterating over each time step\nfor i, dt in enumerate(base_index[N_LAGS + 1:]):\n\n    features_by_station = []\n    # iterating over each buoy station\n    for station_id in station_list:\n\n        # subsetting the data by station and time step (last n_lags observations)\n        station_df = buoys_df.loc[buoys_df['STATION'] == station_id]\n        station_df = station_df.drop('STATION', axis=1)\n        station_df_i = station_df[:dt].tail(N_LAGS)\n\n        if station_df_i.shape[0] < N_LAGS:\n            break\n\n        # transforming lags into features\n        station_timestep_values = []\n        for col in station_df_i:\n            series = station_df_i[col]\n            series.index = [f'{station_id}({series.name})-{i}'\n                            for i in list(range(N_LAGS, 0, -1))]\n\n            station_timestep_values.append(series)\n\n        station_values = pd.concat(station_timestep_values, axis=0)\n\n        features_by_station.append(station_values)\n\n    if len(features_by_station) < 1:\n        continue\n\n    # combining features from all stations\n    feature_set_i = pd.concat(features_by_station, axis=0)\n\n    X.append(feature_set_i)\n\n    # determining the target variable\n    # whether an extreme weather events in the next HORIZON hours\n    td = (hail_df.index - dt)\n    td_hours = td / np.timedelta64(1, 'h')\n    any_event_within = pd.Series(td_hours).between(0, HORIZON)\n\n    y.append(any_event_within.any())\n\n# combining all data points\nX = pd.concat(X, axis=1).T\ny = pd.Series(y).astype(int)\n```", "```py\nimport optuna\n\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\n\n# objective function for optuna\ndef objective(trial, X, y):\n\n    train_x, valid_x, train_y, valid_y = \\\n        train_test_split(X, y, test_size=0.2, shuffle=False)\n\n    dtrain = lgb.Dataset(train_x, label=train_y)\n\n    param = {\n        'objective': 'binary',\n        'metric': 'binary_logloss',\n        'verbosity': -1,\n        'boosting_type': 'gbdt',\n        'linear_tree': True,\n        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n    }\n\n    gbm = lgb.train(param, dtrain)\n    preds = gbm.predict(valid_x)\n\n    # optimizing for AUC\n    auc = roc_auc_score(valid_y, preds)\n\n    return auc\n\ndef optimize_params(X, y, n_trials: int):\n    func = lambda trial: objective(trial, X, y)\n\n    # auc should be maximized\n    study = optuna.create_study(direction='maximize')\n    study.optimize(func, n_trials=n_trials)\n\n    # getting best parameter setup\n    trial = study.best_trial\n\n    return trial.params\n\n# train/test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n\n# optimization\nparams = optimize_params(X_train, y_train, n_trials=100)\n\n# retraining with best parameters\ndtrain = lgb.Dataset(X_train, label=y_train)\ngbm = lgb.train(params, dtrain)\n\n# inference\npreds = gbm.predict(X_test)\n```"]