- en: All You Need to Know about Vector Databases and How to Use Them to Augment Your
    LLM Apps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/all-you-need-to-know-about-vector-databases-and-how-to-use-them-to-augment-your-llm-apps-596f39adfedb](https://towardsdatascience.com/all-you-need-to-know-about-vector-databases-and-how-to-use-them-to-augment-your-llm-apps-596f39adfedb)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Step-by-Step Guide to Discover and Harness the Power of Vector Databases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://dmnkplzr.medium.com/?source=post_page-----596f39adfedb--------------------------------)[![Dominik
    Polzer](../Images/7e48cd15df31a0ab961391c0d57521de.png)](https://dmnkplzr.medium.com/?source=post_page-----596f39adfedb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----596f39adfedb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----596f39adfedb--------------------------------)
    [Dominik Polzer](https://dmnkplzr.medium.com/?source=post_page-----596f39adfedb--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----596f39adfedb--------------------------------)
    ·24 min read·Sep 17, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ca07a0ddf7a94d762cbb35569b47cfba.png)'
  prefs: []
  type: TYPE_IMG
- en: Why we need Vector Stores for LLM apps — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Table of Contents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Intro**'
  prefs: []
  type: TYPE_NORMAL
- en: '[What is so special about Vector Databases?](#003d)'
  prefs: []
  type: TYPE_NORMAL
- en: '[How do we map the meaning of a sentence to a numerical representation?](#8184)'
  prefs: []
  type: TYPE_NORMAL
- en: '[How does that help our LLM app?](#8184)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Why can’t we just give the LLM all the data we have?](#bc26)'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Hands-On Tutorial — Text to Embeddings and Distance Metrics**](#cba0)'
  prefs: []
  type: TYPE_NORMAL
- en: '[1\. Text to Embeddings](#9f3a)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2\. Plot 384 dimensions in 2 using PCA](#45a1)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3\. Calculate the distance metrics](#9c2f)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Towards Vector Stores**'
  prefs: []
  type: TYPE_NORMAL
- en: '[How to accelerate the Similarity Search?](#6009)'
  prefs: []
  type: TYPE_NORMAL
- en: '[What are the different Vector Stores we can choose from?](#f02b)'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Hands-On Tutorial — Set up your first Vector Store**](#7813)'
  prefs: []
  type: TYPE_NORMAL
- en: '[1\. Install chroma](#c634)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2\. Get/create a chroma client and collection](#7d64)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3\. Add some text documents to the collection](#fffe)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4\. Extract all entries from database to excel file](#e71d)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5\. Query the collection](#5d22)'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Summary**](#9128)[**References**](#4113)'
  prefs: []
  type: TYPE_NORMAL
- en: Vector databases are a hot topic right now. Companies keep raising money to
    develop their vector databases or to add vector search capabilities to their existing
    SQL or NoSQL databases.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1491bf4c5448ee19ae952f12ba4518c2.png)'
  prefs: []
  type: TYPE_IMG
- en: Vector Store Funding — Image by the author (Chroma, 2023; Cook, 2022; Miller,
    2022)
  prefs: []
  type: TYPE_NORMAL
- en: What is so special about Vector Databases?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Vector Databases make it possible to quickly search and compare large collections
    of vectors. This is so interesting because the most up-to-date embedding models
    are highly capable of understanding the semantics/meaning behind words and translating
    them into vectors. This allows us to efficiently compare sentences with each other.
  prefs: []
  type: TYPE_NORMAL
- en: '**Okay, but why should we care?**'
  prefs: []
  type: TYPE_NORMAL
- en: For most Large Language Model (LLM) applications we rely on that capability
    since our LLM can never know everything. It only sees like a frozen version of
    the world, which depends on the train set the LLM is trained with.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/349b159551e565144be7d1fad24af317.png)'
  prefs: []
  type: TYPE_IMG
- en: Why we need Vector Stores for LLM apps — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: So we need to feed our model with additional data, information that the LLM
    can impossibly know by itself. And that all needs to happen during the runtime
    of our application. So we must have a process in place that decides as quickly
    as possible with which additional data we want to feed our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'With traditional keyword search, we run into limitations, mainly because of
    two problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Languages are complex. In most languages, you can ask more or less the same
    question in 20 different ways.** It is often not enough to simply search our data
    for keywords. We need a way to map the meaning behind words and sentences to find
    content that’s related to the question.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also need to make sure that this search is done within milliseconds, not
    seconds or minutes. So we need a step that allows us to search the Vector Collection
    as efficiently as possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First things first — How do we map the meaning of a sentence to a numerical
    representation?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we can search our database, we need to translate our text content into
    vectors that capture the meaning of words and sentences. Pre-trained embedding
    models from OpenAI, Google, MetaAI, or the open source community help us do this.
    They learn from a huge corpus of text, how words are normally used and in what
    contexts. They use this extracted knowledge to map words into a multi-dimensional
    vector space. The location of the new data point in the vector space tells us
    which words are related to each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the simple example below, we arrange the “meaning” of various fruits and
    vegetables in a simple two-dimensional vector space. If the embedding model does
    what it is supposed to do, we would expect apple and pear to be closer than apple
    and onion (at least, that is what I would expect with my limited knowledge on
    language and fruits). Similarity can be influenced by various features. For fruits
    and vegetables this may be: Size, colour, taste, country of origin, etc. It all
    depends on what object you want to describe.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a4b0ff69a1a79b4ece04be69301cefcd.png)'
  prefs: []
  type: TYPE_IMG
- en: Similarity score between different types of fruit and vegetables — Image by
    the author
  prefs: []
  type: TYPE_NORMAL
- en: The embedding models learn by observing how words are used in context, similar
    to how humans learn a language. When you’re growing up, you learn the meaning
    of words by listening to conversations and reading books.
  prefs: []
  type: TYPE_NORMAL
- en: The train process of our model is not so different. After the train processes,
    it learned that “pears and apples” are more likely to be seen together in a sentence
    than “apples and onions,” so it assumes they have something in common. So when
    it comes to food, it’s likely that what matters most is whether different types
    of food are eaten together.
  prefs: []
  type: TYPE_NORMAL
- en: Objects, words and sentences have lots of features that don’t fit into two dimensions.
    To tackle this, modern embedding models turn words and sentences into vectors
    with hundreds or thousands of dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: By transforming our content into a vector, we can measure the distance between
    them. This is easy in the two-dimensional example below, but it can become more
    complex and require more computing power when the vector has hundreds or thousands
    of dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: '**This is where vector databases come into play**.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: They are incorporating several techniques that allow us to efficiently store
    and search our collection of text content. What we want to do once we have a collection
    of vectors, we want to compare them to each other and somehow quantify the similarities
    between them. Usually we are interested in the k-nearest neighbors, so the data
    points in our vector space that are closest to our query vector. In the example
    below our query would be the word “apple”. But of course not the word “apple”
    itself, we are using the vector we get back from the Embedding Model.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3a030f8b477b45dfe44284dfe3c295dd.png)'
  prefs: []
  type: TYPE_IMG
- en: Similarity score between different types of fruit and vegetables — Image by
    the author
  prefs: []
  type: TYPE_NORMAL
- en: '**How does that help our LLM app?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We use this approach in many of our LLM applications when the LLM themselves
    reach the limits of their knowledge:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Things LLMs don’t know out of the box:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data that is too new** — Articles about current events, recent innovations,
    etc. Just any new content created after the collection of the LLM train set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data that is not public** — personal data, internal company data, secret
    data, etc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/e28b61dc89a785cace36a586ec66f86c.png)'
  prefs: []
  type: TYPE_IMG
- en: What the LLM know and what not — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: '**Why can’t we just give the model all the data we have?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Short answer:* The models have a limit, a token limit.'
  prefs: []
  type: TYPE_NORMAL
- en: If we don’t want to train or fine-tune the model, we have no choice but to give
    the model all the necessary information within the prompt. We have to respect
    the token limits of the models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/683223fdb31dfc8043892bf863848a64.png)'
  prefs: []
  type: TYPE_IMG
- en: LLMs and their Token Limit — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: LLMs have a token limit **for practical and technical reasons.** The latest
    models from OpenAI have a token limit of about 4,000–32,000 tokens, while the
    open source LLM LLama has 2,048 tokens (if not fine-tuned). You can increase the
    maximum number of tokens by fine-tuning, but more data is not always better. 32,000
    token limits allow us to pack even large texts into a prompt at once. Whether
    this makes sense is another matter. (Lample, 2023; OpenAI, 2023)
  prefs: []
  type: TYPE_NORMAL
- en: The quality of data is more important than the sheer amount of data, irrelevant
    data can have a negative impact on the result.
  prefs: []
  type: TYPE_NORMAL
- en: Even reorganizing the information within the prompt can make a big difference
    in how accurately LLMs understand the task. Researcher from the Stanford University
    has found that when important information is placed at the beginning or end of
    the prompt, the answer is usually more accurate. If the same information is located
    in the middle of the prompt, accuracy can decrease significantly. It’s important
    to give careful consideration to what data we are providing our model with and
    how we structure our prompt. (Liu et al., 2023; Raschka, 2023)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8af5b0f7c2d77965d0e357ee0677255f.png)'
  prefs: []
  type: TYPE_IMG
- en: How LLMs understand the prompts — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: '*The quality of the input data is key to the success of our LLM application,
    so it is essential we implement a process that accurately identifies the relevant
    content and avoids adding too much unnecessary data. To ensure this, we must use
    effective search processes to highlight the most relevant information.*'
  prefs: []
  type: TYPE_NORMAL
- en: How exactly do Vector databases help us do that?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Vector databases are made up of several parts that help us quickly find what
    we’re looking for. Indexing is the most important part and is done just once,
    when we insert new data into our dataset. After that, searches are much faster,
    saving us time and effort.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/83876339f9835c4737e8289383660643.png)'
  prefs: []
  type: TYPE_IMG
- en: Vector Databases and their components — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: To feed the data into our vector database, we first have to convert all our
    content into vectors. As described in the first section of this article, we can
    use so-called embedding models for that. Simply because its more convenient, we
    often use one of the ready-to-use services from OpenAI, Google and Co..
  prefs: []
  type: TYPE_NORMAL
- en: In the image below you can see some Embedding Models you can choose from.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9a5ca01d8fbee016f5bd0a16a9645b8f.png)'
  prefs: []
  type: TYPE_IMG
- en: A collection of available Embedding Models — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Hands-On Tutorial — Text to Vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To use our example, we’ll need the Hugging Face API and the sentence transformer
    model “all-MiniLM-L6-v2”. To get started, just go to [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)
    and get your token.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the code snippet below, we:'
  prefs: []
  type: TYPE_NORMAL
- en: Transform the text snippets into vectors with 384 dimensions (Depends on the
    Embedding Model you use). This allows us to capture the meaning of sentences in
    a vector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can then identify similarities between data points by calculating the distance
    between the sentences.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To visualise this in a simple 2-dimensional plot, we reduce the 384 dimensions
    to two using Principal Component Analysis. This may result in a huge loss of information,
    but it’s worth a try!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For the example below I am generating 10 random sample sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If you’d like to try the tutorial for yourself, I’ve broken down the steps into
    data pipelines that you can run independently.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new virtual environment **.venv** using:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install all modules in “requirements.txt”:'
  prefs: []
  type: TYPE_NORMAL
- en: '**pip install -r requirements.txt**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If you want to follow along you can simply set up a similar folder structure
    with a folder for the data pipelines and the data (or change the path to the .csv
    files within the pipelines).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eb72305ac508bb8ed71429ed0a9d2cac.png)'
  prefs: []
  type: TYPE_IMG
- en: Data pipelines overview — Screenshot by the author
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Text to Embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first pipeline, “01_text_to_embeddings.py” uses an Embedding Model to transform
    the 10 sample sentences. The results are saved in a data frame called “embeddings_df”
    and then exported to a CSV file named “embeddings_df.csv”. Make sure you replace
    the HuggingFace Token with you own token.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e3c12bf12f16f3d6cc9daac95ecb6465.png)'
  prefs: []
  type: TYPE_IMG
- en: From text chunks to embeddings — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You should now find the csv-file “embeddings_df.csv” in the folder “02_Data”.
    It should look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8c30ef15e4ea921312f2c787f9495c2d.png)'
  prefs: []
  type: TYPE_IMG
- en: 2\. Plot 384 dimensions vector in 2 dimensions using Principal Component Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you have the csv-file “embeddings_df.csv” in the folder “02_Data”,
    which contains the vectors, let’s try to visualize it. We can use Principal Component
    Analysis to extract the 2 most important “Principal Components” and visualize
    it. We can not capture all of the original information in just two dimensions,
    but maybe it gives us a feeling what is happening when we translate our text into
    vectors.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fe66a351780f01f0b914e6a24cbc3378.png)'
  prefs: []
  type: TYPE_IMG
- en: Apply principal component analysis to embedding vectors — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new file for the second pipeline, I call it: **“02_create_PCA_analysis.py”**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second pipeline contains:'
  prefs: []
  type: TYPE_NORMAL
- en: Loading the embeddings from “embeddings_df.csv”
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform a Principal Component Analysis using scikit-learn to create and find
    the two most important (principal) components
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a scatter plot to visualize the result in a 2-dimensional plot
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/48ebc95c64741e8b1706c581180539f4.png)'
  prefs: []
  type: TYPE_IMG
- en: Screenshot repo
  prefs: []
  type: TYPE_NORMAL
- en: I’m not sure how good the example is, but you can at least see that sentences
    about food and drink tend to be on the right side of the plot, the two sentences
    describing the weather are close together on the left side, and the two sentences
    about flowers are close together at the bottom of the plot. I’ll just take this
    as a “not so scientific” evidence that the model is at least somewhat successful
    in mapping the semantics behind the words/phrases.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a6386f07c0d55133a84e2654ad28b04f.png)'
  prefs: []
  type: TYPE_IMG
- en: Sentences to Vectors (first two Principal Components)— Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: As humans, it is relatively easy to tell which points are closer together when
    plotted in a simple two-dimensional space. But how can we quantify that in a vector
    space with hundreds and thousands of dimensions? — We need a metric for that.
    A metric that describes the similarity. Therefor we calculate the distance between
    the points in the space.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Calculate the distance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**How is the distance defined?**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In Statistics we have several metrics to measure the distance between data points.
    One often used metric is the “Cosine Similarity”. We’ll be using this one for
    our examples below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/000daf8a5828dfd5c92b555341460cad.png)'
  prefs: []
  type: TYPE_IMG
- en: Different Distance Metrics — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: What we want to do with Vector Databases is to find similar entries as fast
    as possible. But what makes Vector Databases so special, why can’t we just perform
    the similarity search by calculating the distance metric to every data point?
  prefs: []
  type: TYPE_NORMAL
- en: To make it easier to understand why we need an alternate approach, I have created
    a straightforward process that finds the closest data points. It does this by
    calculating the distance to each data point and then sorting them to find the
    nearest neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: For the example below, We use the 10 pieces of text that we have already converted
    into a vector format earlier in this post.
  prefs: []
  type: TYPE_NORMAL
- en: 'So a common query of our database might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say we have a new sentence (user query), here: **“Lilies are white.”**'
  prefs: []
  type: TYPE_NORMAL
- en: First, we are obtaining the embeddings for our new sentences. This vector gives
    us a location in our embedding space that allows us to compare them with the other
    text chunks in our collection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/04b9ba65ccf48fe3ff93baad86072190.png)'
  prefs: []
  type: TYPE_IMG
- en: Distance as Similarity Score — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we are calculating the distance (here: cosine similarity) to each data
    point'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/aa075272901a313fd0df878f1b01e370.png)'
  prefs: []
  type: TYPE_IMG
- en: Calculate the cosine similarity to our query vector — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Usually, we are interested in the k-nearest neighbors. We simply filter the
    ones with the smallest distance to our new query vector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the simple example, we use the sentence “Lilies are white.” and try to describe
    the similarity to the other sentences in the dataset by calculating their distance
    from each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do that, we create a third pipeline, I call it: **“03_calculate_cosine_similarity.py”**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/04ea5742b942231b949cec8e7aca8be1.png)'
  prefs: []
  type: TYPE_IMG
- en: Calculated similarities — Screenshots by the author
  prefs: []
  type: TYPE_NORMAL
- en: If we look at the similarity scores, the sentences “Violets are blue.” and “Roses
    are red.” are significantly more similar to our query sentence “Lilies are white.”
    than “Programming is fun.”.
  prefs: []
  type: TYPE_NORMAL
- en: Makes sense, I guess.
  prefs: []
  type: TYPE_NORMAL
- en: '**The similarity search seems to work. So why do we need another approach at
    all?**'
  prefs: []
  type: TYPE_NORMAL
- en: Using the *time.time()* function I am stopping the time it takes to calculate
    the cosine similarity between our query vector and the 10 other vectors.
  prefs: []
  type: TYPE_NORMAL
- en: According to that, it takes around 0.005 seconds to search our 10 entries.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dcb784ec13bf2e1ed5b3880440e02b98.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparing each point to every other point is called “exhaustive search” and
    it takes a linear amount of time. For example, if it takes 0.005 seconds to compare
    10 points, it would take several minutes to compare 1 million chunks of text.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e9eb5031cd78eb29f21eef4e206be270.png)'
  prefs: []
  type: TYPE_IMG
- en: Computing time for exhaustive search — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: We need to find an efficient way to speed up the similarity search process.
  prefs: []
  type: TYPE_NORMAL
- en: How to accelerate the similarity search?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Approximate Nearest Neighbor algorithms are used to find the closest neighbors,
    even though they may not always be the exact closest. This trade-off of accuracy
    for speed is usually acceptable for LLM applications, since speed is more important
    and often the same information is found in multiple text snippets anyway. [(Trabelsi,
    2021)](https://www.zotero.org/google-docs/?GbBH8V=)
  prefs: []
  type: TYPE_NORMAL
- en: I guess it is not essential for the average user to have a deep understanding
    of indexing techniques. But I don’t want to leave you completely without. To give
    you at least a basic understanding of the techniques, here’s an example of how
    to use an Inverted File Index. This way, you can get a sense of where accuracy
    can be lost using these techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Inverted File Index (IFV) is a popular method for finding similarities between
    different items. It works by creating a database index that stores content and
    connects it to its position in a table or document. We divide the whole collection
    of items into partitions and their centroids. Each item can only be part of one
    partition at a time. When we search for similarities, we use the partition centroids
    to quickly find the items we are looking for.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b930b59e8b97858b4b1473081c541476.png)'
  prefs: []
  type: TYPE_IMG
- en: Approximate Nearest Neighbor search using Inverted File Index (IFV) — Image
    by the author
  prefs: []
  type: TYPE_NORMAL
- en: If we’re looking for nearby points, we usually just search in the centroid closest
    to our point. But if there are points close to the edge of the neighboring centroid,
    we may miss them.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aafc4af2c097ca0a693d726a9964182d.png)'
  prefs: []
  type: TYPE_IMG
- en: Where we loose information using ANN algorithms — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: To avoid this issue, we search multiple partitions instead of just one. However,
    the underlying problem still remains. We lose some accuracy. But that is usually
    ok, speed is more important.
  prefs: []
  type: TYPE_NORMAL
- en: Chroma supports multiple approximate nearest neighbor (ANN) algorithms, including
    HNSW, IVFADC, and IVFPQ. [(Yadav, 2023)](https://www.zotero.org/google-docs/?k2HoPI=)
  prefs: []
  type: TYPE_NORMAL
- en: '**Hierarchical Navigable Small World (HNSW):** HNSW is an algorithm that creates
    a hierarchical graph structure to quickly store and search high-dimensional vectors
    with minimal memory usage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inverted File with Product Quantization (IVFPQ):** IVFPQ uses product quantization
    to compress vectors before indexing, resulting in a high-accuracy search that
    can handle massive datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/fcdc598e0b9a7c065003d52f5eb0d68a.png)'
  prefs: []
  type: TYPE_IMG
- en: Among the most used Approximate Nearest Neighbor algorithms — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: If you want to know in detail how the different indexing methods work, I can
    recommend [James Briggs](https://www.youtube.com/watch?v=QvKMwLjdK-s) Youtube
    channel. I also liked the blog posts from [Peggy Chang](https://peggy1502.medium.com/)
    to inverted file index (IVF), product quantization (PQ) and co.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is really relevant for us?**'
  prefs: []
  type: TYPE_NORMAL
- en: I think all we need to know is that through the indexing step, we store our
    embeddings in a form that allows us to quickly find “similar” vectors without
    having to calculate the distance to all the data points each time. By doing that,
    we trade speed for some accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e845a2803e0e5780345bdcb268bfb8c3.png)'
  prefs: []
  type: TYPE_IMG
- en: How to store text in vector stores — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: The accuracy of the task should still be good enough for most of what we try
    to do with it. Translating language into embeddings is not an exact science anyway.
    The same word can have different meanings depending on the context or region of
    the world in which you use it. Therefore, it is usually okay if we lose a few
    accuracy points, but what is much more important is the speed of the response.
  prefs: []
  type: TYPE_NORMAL
- en: Google’s fast response time is what makes it so successful. The speed of each
    step of the process is even more important for our application, because we not
    only have to perform the vector search, but also pass the questions and context
    to our LLM.
  prefs: []
  type: TYPE_NORMAL
- en: However, this process takes a bit longer than Google, which is (I guess) why
    Bing Chat (with its LLM support) has not yet conquered the world. It is only a
    few milliseconds or seconds slower, but this small difference is enough to keep
    Google on top.
  prefs: []
  type: TYPE_NORMAL
- en: '**To illustrate these steps — Let’s say we want to create a chatbot like Bing
    Chat**'
  prefs: []
  type: TYPE_NORMAL
- en: We still have the (traditional) search part that looks for content and news
    that is most relevant. Only when we have found some common results, we provide
    them to our LLM and let it interpret the data and formulate a well-sounding answer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d51ab7bc1abe2ed422ba72a5c0f5a366.png)'
  prefs: []
  type: TYPE_IMG
- en: Process flow of a chatbot using LLM models — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Our vector store takes care of tokenizing, embedding and indexing the data when
    it’s loaded. Once the data is in the store, we can query it with new data points.
  prefs: []
  type: TYPE_NORMAL
- en: '**Suppose we decide to use a vector store — What options do we have?**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Vector databases come in different shapes. We distinguish between:'
  prefs: []
  type: TYPE_NORMAL
- en: Pure vector databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extended capabilities in SQL, NoSQL or text search databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple vector libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/6ae2735fd136028a186b8057624f3788.png)'
  prefs: []
  type: TYPE_IMG
- en: Different types of Vector Stores — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: '**Text search databases** can search through large amounts of text for specific
    words or phrases. Recently, some of these databases have begun to use vector search
    to further improve their ability to find what you’re looking for. At the last
    [Microsoft Developer Conference](https://www.youtube.com/watch?v=5Qaxz2e2dVg&t=584s),
    Elasticsearch explained how they use both traditional search and vector search
    to create a ‘Hybrid Scoring’ system, giving you the best possible search results.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/05ef43a226a39b222be3eaf4bf73924d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Elasticsearch: Combining Traditional with Vector Search improves the search
    results — Image by the author'
  prefs: []
  type: TYPE_NORMAL
- en: 'Vector Search is also gradually being adopted by more and more **SQL and NoSQL
    databases such as Redis, MongoDB or Postgres**. Pgvector, for example, is the
    open source vector similarity search for Postgres. It supports (Github, 2023):'
  prefs: []
  type: TYPE_NORMAL
- en: exact and approximate nearest neighbor search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L2 distance, inner product, and cosine distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For smaller projects, **vector libraries** are a great option and provide most
    of the features needed.
  prefs: []
  type: TYPE_NORMAL
- en: '**Facebook AI Research released one of the first vector libraries, FAISS**,
    in 2017\. FAISS is a library for efficiently searching and clustering dense vectors,
    and can handle vector sets of any size, even those that don’t fit in memory. It’s
    written in C++ and comes with a Python wrapper, making it easy for data scientists
    to integrate into their code. (FAISS Documentation, 2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/46935d6ec3d53d2003724caebb30cf99.png)'
  prefs: []
  type: TYPE_IMG
- en: The evolution of Vector Stores — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: '**Chroma, Pinecone, Weaviate on the other side, are pure Vector Databases**
    that can store your vector data and be searched like any other database. In this
    article, I’ll teach you how to set up a Vector Database with Chroma and how to
    fill it with your vector data. If you’re looking for a quick solution, vector
    libraries like FAISS can help you get started easily with all the necessary indexing
    methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '**So which one should we take?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'I guess you’ll have to answer that one for yourselves and your specific project,
    but don’t make things unnecessarily complex. Andre Karpathy described it on Twitter
    with the words: (Andrej Karpathy, 2023):'
  prefs: []
  type: TYPE_NORMAL
- en: “Np.array — people keep reaching for much fancier things way too fast these
    days” — [Andrew Karpathy](https://twitter.com/karpathy/status/1647374645316968449?lang=de)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you only need to search a few pages of a PDF or text file, you can simply
    use an np.array or pandas data frames to store your embeddings. The capabilities
    of Vector Databases become interesting when we speak about hundreds, thousands
    or millions of vectors, we want to search on a regular basis. For this article,
    I am using Chroma, but the same principles apply to all databases.
  prefs: []
  type: TYPE_NORMAL
- en: Hands-On Tutorial — Set up your first Vector Store
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To store our content into vectors and improve the performance of our similarity
    search, we need to set up our own Vector Database. Chroma is a great open-source
    option to use, as it is free to use and has an Apache 2.0 license. Other alternatives,
    such as FAISS, Weaviate, and Pinecone, also exist. Some of these options are open-source
    and free to use, while others are only available as a commercial service.
  prefs: []
  type: TYPE_NORMAL
- en: With just a few steps, we can get chromadb up and running. All we need to do
    is use our package manager **pip** to download the **chromadb** library. Once
    we have that, we can start setting up our first vector store database and get
    going.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Install chroma
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[https://pypi.org/project/chromadb/](https://pypi.org/project/chromadb/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 2\. Get/create a chroma client and collection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A collection is the designated storage for your embeddings, documents, and any
    additional metadata. If you want to save it, so you can later use your indexes
    and collections, you can use “persist_directory”
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/46eee8311e6a2663a0283739dbc3bb30.png)'
  prefs: []
  type: TYPE_IMG
- en: From text chunks to embeddings — Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 3\. Add some text documents to the collection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Chroma makes it easy to store and organize your text documents. It takes care
    of the tokenization, embedding, and indexing processes for you, and if you have
    already created your own embeddings, you can load them directly into Chroma’s
    vector store.
  prefs: []
  type: TYPE_NORMAL
- en: 'We want to store the already created Data Frame “embeddings_df” into our new
    data store:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c0aeece2ff6b0ba94be3cbaa8b8bf8c6.png)'
  prefs: []
  type: TYPE_IMG
- en: 4\. Extract all entries from database to excel file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you want to export all entries in your vector store, you can use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 5\. Query the collection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Chroma makes it easy to find the ***n*** most similar results to a query texts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/0a9afd1429e87164ebe3888ab27e9f4f.png)'
  prefs: []
  type: TYPE_IMG
- en: For this example we will get back the two most similar entries in our vector
    store.
  prefs: []
  type: TYPE_NORMAL
- en: Below you can find a summary of
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: These k-nearest neighbors we will then use to feed our LLM. Build a simple prompt
    template around it, insert the found text chunks into it, and you can send it
    to GPT, LLama, or any other LLM of your choice. I described how this works in
    one of [my previous articles.](/all-you-need-to-know-to-build-your-first-llm-app-eb982c78ffac)
  prefs: []
  type: TYPE_NORMAL
- en: '**Summary**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Vector search is becoming increasingly popular as Machine Learning models can
    now accurately convert various content into vectors. Not only are there more and
    more dedicated vector databases, but existing SQL, NoSQL, and text search databases
    are also incorporating vector search capabilities into their products. This is
    to either improve their search mechanisms or provide a product for those specifically
    looking for databases with vector search capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: The interest in Vector Stores is only growing. Thanks to advances in Transformer
    Models in recent years, we can now turn text modules into vectors with confidence.
    This unlocks a world of mathematical possibilities when working with text.
  prefs: []
  type: TYPE_NORMAL
- en: '*Enjoyed the story?*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[*Subscribe for free*](https://dmnkplzr.medium.com/subscribe) *to get notified
    when I publish a new story.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Want to read more than 3 free stories a month? — Become a Medium member for
    5$/month. You can support me by using my* [*referral link*](https://dmnkplzr.medium.com/membership)
    *when you sign up. I’ll receive a commission at no extra cost to you.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Feel free to reach out to me on* [*LinkedIn*](https://www.linkedin.com/in/polzerdo/)
    *!*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: You can find all code snippets on [Github](https://github.com/polzerdo55862/vector-store-tutorial).
    Have fun :)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Andrej Karpathy. (2023, April 15). *@sinclanich np.array people keep reaching
    for much fancier things way too fast these days* [Tweet]. Twitter. [https://twitter.com/karpathy/status/1647374645316968449](https://twitter.com/karpathy/status/1647374645316968449)
  prefs: []
  type: TYPE_NORMAL
- en: Chroma. (2023, April 7). *Chroma raises $18M seed round*. [https://www.trychroma.com/blog/seed](https://www.trychroma.com/blog/seed)
  prefs: []
  type: TYPE_NORMAL
- en: Cook, J. (2022, March 1). *SeMI Technologies secures $16 million in Series A
    Round — Business Leader News*. Business Leader. [https://www.businessleader.co.uk/semi-technologies-secures-16-million-in-series-a-round/](https://www.businessleader.co.uk/semi-technologies-secures-16-million-in-series-a-round/)
  prefs: []
  type: TYPE_NORMAL
- en: Github. (2023, August 23). *Pgvector Github Repo*. [https://github.com/pgvector/pgvector](https://github.com/pgvector/pgvector)
  prefs: []
  type: TYPE_NORMAL
- en: 'Lample, G. (2023, August 3). *Inquiry about the maximum number of tokens that
    Llama can handle · Issue #148 · facebookresearch/llama*. GitHub. [https://github.com/facebookresearch/llama/issues/148](https://github.com/facebookresearch/llama/issues/148)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F.,
    & Liang, P. (2023). *Lost in the Middle: How Language Models Use Long Contexts*
    (arXiv:2307.03172). arXiv. [http://arxiv.org/abs/2307.03172](http://arxiv.org/abs/2307.03172)'
  prefs: []
  type: TYPE_NORMAL
- en: Miller, R. (2022, March 29). *Pinecone announces $28M Series A for purpose-built
    database aimed at data scientists | TechCrunch*. [https://techcrunch.com/2022/03/29/pinecone-announces-28m-series-a-for-purpose-built-database-aimed-at-data-scientists/?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAAMIKiexS9AALbi0ePIRFgGd-Ck3yS3U6t1VCv3yX_OwLRX-zH0-EVXFnHmq1wCH3blQOZncsBjmEU6H4LunP98AK7H_CJDYnILkchAazm6IF8SuDhf5On1JfywlFOfNC1teEpTpChfnEVt-lZ72KG0Y_yiUM5wdb6-I1-aUOtyiG](https://techcrunch.com/2022/03/29/pinecone-announces-28m-series-a-for-purpose-built-database-aimed-at-data-scientists/?guccounter=1&guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&guce_referrer_sig=AQAAAMIKiexS9AALbi0ePIRFgGd-Ck3yS3U6t1VCv3yX_OwLRX-zH0-EVXFnHmq1wCH3blQOZncsBjmEU6H4LunP98AK7H_CJDYnILkchAazm6IF8SuDhf5On1JfywlFOfNC1teEpTpChfnEVt-lZ72KG0Y_yiUM5wdb6-I1-aUOtyiG)
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI. (2023, July 24). *OpenAI Platform*. [https://platform.openai.com](https://platform.openai.com)
  prefs: []
  type: TYPE_NORMAL
- en: Raschka, S. (2023). *LinkedIn Sebastian Raschka*. [https://www.linkedin.com/posts/sebastianraschka_llm-ai-machinelearning-activity-7083427280605089792-MS_N/?utm_source=share&utm_medium=member_desktop](https://www.linkedin.com/posts/sebastianraschka_llm-ai-machinelearning-activity-7083427280605089792-MS_N/?utm_source=share&utm_medium=member_desktop)
  prefs: []
  type: TYPE_NORMAL
- en: Trabelsi, E. (2021, September 8). *Comprehensive Guide To Approximate Nearest
    Neighbors Algorithms*. Medium. [https://towardsdatascience.com/comprehensive-guide-to-approximate-nearest-neighbors-algorithms-8b94f057d6b6](/comprehensive-guide-to-approximate-nearest-neighbors-algorithms-8b94f057d6b6)
  prefs: []
  type: TYPE_NORMAL
- en: 'Yadav, R. (2023, May 3). *An Evaluation of Vector Database Systems: Features,
    and Use Cases*. Medium. [https://blog.devgenius.io/an-evaluation-of-vector-database-systems-features-and-use-cases-9a90b05eb51f](https://blog.devgenius.io/an-evaluation-of-vector-database-systems-features-and-use-cases-9a90b05eb51f)'
  prefs: []
  type: TYPE_NORMAL
