["```py\nsemantic_search = ScalableSemanticSearch(device=\"cuda\")\n```", "```py\nembeddings = semantic_search.encode(corpus)\n```", "```py\nsemantic_search.build_index(embeddings)\n```", "```py\nquery = \"What is the meaning of life?\"\ntop = 5\ntop_indices, top_scores = semantic_search.search(query, top)\n```", "```py\ntop_sentences = ScalableSemanticSearch.get_top_sentences(semantic_search.hashmap_index_sentence, top_indices)\n```", "```py\nclass ScalableSemanticSearch:\n    \"\"\"Vector similarity using product quantization with sentence transformers embeddings and cosine similarity.\"\"\"\n\n    def __init__(self, device=\"cpu\"):\n        self.device = device\n        self.model = SentenceTransformer(\n            \"sentence-transformers/all-mpnet-base-v2\", device=self.device\n        )\n        self.dimension = self.model.get_sentence_embedding_dimension()\n        self.quantizer = None\n        self.index = None\n        self.hashmap_index_sentence = None\n\n        log_directory = \"log\"\n        if not os.path.exists(log_directory):\n            os.makedirs(log_directory)\n        log_file_path = os.path.join(log_directory, \"scalable_semantic_search.log\")\n\n        logging.basicConfig(\n            filename=log_file_path,\n            level=logging.INFO,\n            format=\"%(asctime)s %(levelname)s: %(message)s\",\n        )\n        logging.info(\"ScalableSemanticSearch initialized with device: %s\", self.device)\n\n    @staticmethod\n    def calculate_clusters(n_data_points: int) -> int:\n        return max(2, min(n_data_points, int(np.sqrt(n_data_points))))\n\n    def encode(self, data: List[str]) -> np.ndarray:\n        \"\"\"Encode input data using sentence transformer model.\n\n        Args:\n            data: List of input sentences.\n\n        Returns:\n            Numpy array of encoded sentences.\n        \"\"\"\n        embeddings = self.model.encode(data)\n        self.hashmap_index_sentence = self.index_to_sentence_map(data)\n        return embeddings.astype(\"float32\")\n\n    def build_index(self, embeddings: np.ndarray) -> None:\n        \"\"\"Build the index for FAISS search.\n\n        Args:\n            embeddings: Numpy array of encoded sentences.\n        \"\"\"\n        n_data_points = len(embeddings)\n        if (\n            n_data_points >= 1500\n        ):  # Adjust this value based on the minimum number of data points required for IndexIVFPQ\n            self.quantizer = faiss.IndexFlatL2(self.dimension)\n            n_clusters = self.calculate_clusters(n_data_points)\n            self.index = faiss.IndexIVFPQ(\n                self.quantizer, self.dimension, n_clusters, 8, 4\n            )\n            logging.info(\"IndexIVFPQ created with %d clusters\", n_clusters)\n        else:\n            self.index = faiss.IndexFlatL2(self.dimension)\n            logging.info(\"IndexFlatL2 created\")\n\n        if isinstance(self.index, faiss.IndexIVFPQ):\n            self.index.train(embeddings)\n        self.index.add(embeddings)\n        logging.info(\"Index built on device: %s\", self.device)\n\n    @staticmethod\n    def index_to_sentence_map(data: List[str]) -> Dict[int, str]:\n        \"\"\"Create a mapping between index and sentence.\n\n        Args:\n            data: List of sentences.\n\n        Returns:\n            Dictionary mapping index to the corresponding sentence.\n        \"\"\"\n        return {index: sentence for index, sentence in enumerate(data)}\n\n    @staticmethod\n    def get_top_sentences(\n        index_map: Dict[int, str], top_indices: np.ndarray\n    ) -> List[str]:\n        \"\"\"Get the top sentences based on the indices.\n\n        Args:\n            index_map: Dictionary mapping index to the corresponding sentence.\n            top_indices: Numpy array of top indices.\n\n        Returns:\n            List of top sentences.\n        \"\"\"\n        return [index_map[i] for i in top_indices]\n\n    def search(self, input_sentence: str, top: int) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Compute cosine similarity between an input sentence and a collection of sentence embeddings.\n\n        Args:\n            input_sentence: The input sentence to compute similarity against.\n            top: The number of results to return.\n\n        Returns:\n            A tuple containing two numpy arrays. The first array contains the cosine similarities between the input\n            sentence and the embeddings, ordered in descending order. The second array contains the indices of the\n            corresponding embeddings in the original array, also ordered by descending similarity.\n        \"\"\"\n        vectorized_input = self.model.encode(\n            [input_sentence], device=self.device\n        ).astype(\"float32\")\n        D, I = self.index.search(vectorized_input, top)\n        return I[0], 1 - D[0]\n\n    def save_index(self, file_path: str) -> None:\n        \"\"\"Save the FAISS index to disk.\n\n        Args:\n            file_path: The path where the index will be saved.\n        \"\"\"\n        if hasattr(self, \"index\"):\n            faiss.write_index(self.index, file_path)\n        else:\n            raise AttributeError(\n                \"The index has not been built yet. Build the index using `build_index` method first.\"\n            )\n\n    def load_index(self, file_path: str) -> None:\n        \"\"\"Load a previously saved FAISS index from disk.\n\n        Args:\n            file_path: The path where the index is stored.\n        \"\"\"\n        if os.path.exists(file_path):\n            self.index = faiss.read_index(file_path)\n        else:\n            raise FileNotFoundError(f\"The specified file '{file_path}' does not exist.\")\n\n    @staticmethod\n    def measure_time(func: Callable, *args, **kwargs) -> Tuple[float, Any]:\n        start_time = time.time()\n        result = func(*args, **kwargs)\n        end_time = time.time()\n        elapsed_time = end_time - start_time\n        return elapsed_time, result\n\n    @staticmethod\n    def measure_memory_usage() -> float:\n        process = psutil.Process(os.getpid())\n        ram = process.memory_info().rss\n        return ram / (1024**2)\n\n    def timed_train(self, data: List[str]) -> Tuple[float, float]:\n        start_time = time.time()\n        embeddings = self.encode(data)\n        self.build_index(embeddings)\n        end_time = time.time()\n        elapsed_time = end_time - start_time\n        memory_usage = self.measure_memory_usage()\n        logging.info(\n            \"Training time: %.2f seconds on device: %s\", elapsed_time, self.device\n        )\n        logging.info(\"Training memory usage: %.2f MB\", memory_usage)\n        return elapsed_time, memory_usage\n\n    def timed_infer(self, query: str, top: int) -> Tuple[float, float]:\n        start_time = time.time()\n        _, _ = self.search(query, top)\n        end_time = time.time()\n        elapsed_time = end_time - start_time\n        memory_usage = self.measure_memory_usage()\n        logging.info(\n            \"Inference time: %.2f seconds on device: %s\", elapsed_time, self.device\n        )\n        logging.info(\"Inference memory usage: %.2f MB\", memory_usage)\n        return elapsed_time, memory_usage\n\n    def timed_load_index(self, file_path: str) -> float:\n        start_time = time.time()\n        self.load_index(file_path)\n        end_time = time.time()\n        elapsed_time = end_time - start_time\n        logging.info(\n            \"Index loading time: %.2f seconds on device: %s\", elapsed_time, self.device\n        )\n        return elapsed_time\n```", "```py\ndemo = SemanticSearchDemo(\n    dataset_path, model, index_path=index_path, subset_size=subset_size\n)\n```", "```py\nsentences = demo.load_data(file_name)\nsubset_sentences = sentences[:subset_size]\n```", "```py\ntraining_time, training_memory_usage = demo.train(subset_sentences)\n```", "```py\ntop_sentences, inference_time, inference_memory_usage = demo.infer(\n    query, subset_sentences, top=3\n)\n```", "```py\nclass SemanticSearchDemo:\n    \"\"\"A demo class for semantic search using the ScalableSemanticSearch model.\"\"\"\n\n    def __init__(\n        self,\n        dataset_path: str,\n        model: ScalableSemanticSearch,\n        index_path: Optional[str] = None,\n        subset_size: Optional[int] = None,\n    ):\n        self.dataset_path = dataset_path\n        self.model = model\n        self.index_path = index_path\n        self.subset_size = subset_size\n\n        if self.index_path is not None and os.path.exists(self.index_path):\n            self.loading_time = self.model.timed_load_index(self.index_path)\n        else:\n            self.train()\n\n    def load_data(self, file_name: str) -> List[str]:\n        \"\"\"Load data from a file.\n\n        Args:\n            file_name: The name of the file containing the data.\n\n        Returns:\n            A list of sentences loaded from the file.\n        \"\"\"\n        with open(f\"{self.dataset_path}/{file_name}\", \"r\") as f:\n            reader = csv.reader(f, delimiter=\"\\t\")\n            next(reader)  # Skip the header\n            sentences = [row[3] for row in reader]  # Extract the sentences\n        return sentences\n\n    def train(self, data: Optional[List[str]] = None) -> Tuple[float, float]:\n        \"\"\"Train the semantic search model and measure time and memory usage.\n\n        Args:\n            data: A list of sentences to train the model on. If not provided, the data is loaded from file.\n\n        Returns:\n            A tuple containing the elapsed time in seconds and the memory usage in megabytes.\n        \"\"\"\n        if data is None:\n            file_name = \"GenericsKB-Best.tsv\"\n            data = self.load_data(file_name)\n\n            if self.subset_size is not None:\n                data = data[: self.subset_size]\n\n        elapsed_time, memory_usage = self.model.timed_train(data)\n\n        if self.index_path is not None:\n            self.model.save_index(self.index_path)\n\n        return elapsed_time, memory_usage\n\n    def infer(\n        self, query: str, data: List[str], top: int\n    ) -> Tuple[List[str], float, float]:\n        \"\"\"Perform inference on the semantic search model and measure time and memory usage.\n\n        Args:\n            query: The input query to search for.\n            data: A list of sentences to search in.\n            top: The number of top results to return.\n\n        Returns:\n            A tuple containing the list of top sentences that match the input query, elapsed time in seconds, and memory usage in megabytes.\n        \"\"\"\n        elapsed_time, memory_usage = self.model.timed_infer(query, top)\n        top_indices, _ = self.model.search(query, top)\n        index_map = self.model.index_to_sentence_map(data)\n        top_sentences = self.model.get_top_sentences(index_map, top_indices)\n\n        return top_sentences, elapsed_time, memory_usage\n```", "```py\ntrain_time, train_memory = semantic_search.timed_train(corpus)\ninfer_time, infer_memory = semantic_search.timed_infer(query, top)\n```"]