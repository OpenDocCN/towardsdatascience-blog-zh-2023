["```py\nregion = ['APAC', 'EU', 'NORTHAM', 'MENA', 'AFRICA']\nuser_type = ['premium', 'free']\n\nordinal_list = ['region', 'user_type']\n```", "```py\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n\ndef var_encoding(X, cols, ordinal_list, encoding):\n\n    #Function to encode ordinal variables\n    if encoding == 'ordinal_ordered': \n        encoder = OrdinalEncoder(categories=ordinal_list) \n        encoder.fit(X.loc[:, cols])\n        X.loc[:, cols] = encoder.transform(X.loc[:, cols])\n\n    #Function to encode categorical variables    \n    elif encoding == 'ordinal_unordered':\n        encoder = OrdinalEncoder()\n        encoder.fit(X.loc[:, cols])\n        X.loc[:, cols] = encoder.transform(X.loc[:, cols])\n\n    else:     \n        encoder = OneHotEncoder(handle_unknown='ignore')\n        encoder.fit(df.loc[:, cols])\n        df.loc[:, cols] = encoder.transform(df.loc[:, cols])\n\n    return X\n```", "```py\ndef encoding_vars(X, ordinal_cols, ordinal_list, preprocessing_categoricals=False):\n\n    #Encode ordinal variables\n    df = var_encoding(df, ordinal_cols, ordinal_list, 'ordinal_ordered')\n\n    #Encode categorical variables\n    if preprocessing_categoricals: \n       df = var_encoding(df, categorical_cols, 'ordinal_unordered')    \n\n    #Else set your categorical variables as 'category' if needed\n    else:\n        for cat in categorical_cols: \n            X[cat] = X[cat].astype('category')\n\n    #Rename your variables as such if needed to keep track of the order\n    #An encoded feature such as region will no longer show female or male, but 0 or 1\n    df.rename(columns={'user_type': 'free_0_premium_1'},   \n    df.reset_index(drop=True, inplace=True)   \n\n    return df\n```", "```py\nfrom sklearn.model_selection import GroupShuffleSplit\n\ndef split_df(df, ordinal_cols, ordinal_list, target):\n    #splitting train and test\n    splitter = GroupShuffleSplit(test_size=.13, n_splits=2, random_state=7)\n    split = splitter.split(df, groups=df['user_id'])\n    train_inds, test_inds = next(split)\n\n    train = df.iloc[train_inds]\n    test = df.iloc[test_inds]\n\n    #splitting validation and test\n    splitter2 = GroupShuffleSplit(test_size=.5, n_splits=2, random_state=7)\n    split = splitter2.split(test, groups=test['user_id'])\n    val_inds, test_inds = next(split)\n\n    val = test.iloc[val_inds]\n    test = test.iloc[test_inds]\n\n    #defining X and y\n    X_train = train.drop(['target_variable'], axis=1)\n    y_train = train.target_variable\n\n    X_val = val.drop(['target_variable'], axis=1)\n    y_val = val.target_variable\n\n    X_test = test.drop(['target_variable'], axis=1)\n    y_test = test.target_variable\n\n    #encoding the variables in the sets based on a pre-defined encoding function\n    X_train = encoding_vars(X_train, ordinal_cols, ordinal_list)\n    X_val = encoding_vars(X_val, ordinal_cols, ordinal_list)\n    X_test = encoding_vars(X_test, ordinal_cols, ordinal_list)\n\n    return X_train, y_train, X_val, y_val, X_test, y_test\n```", "```py\ndef corr_matrix(df):\n    # Select upper triangle of correlation matrix\n    upper = df.corr().abs().where(np.triu(np.ones(df.corr().abs().shape), k=1).astype(np.bool))\n\n    return upper\n\ndef cols_todrop(corr_matrix, threshold):\n    # Find features with correlation greater than x (you pick your threshold)\n    to_drop = [col for col in corr_matrix.columns if any(corr_matrix[col] > threshold)]\n\n    return to_drop\n```", "```py\n# Get a ranking of top 10 features with the highest correlation based on your threshold\nupper = corr_matrix(data)\nupper.returned_1d.sort_values(ascending=False)[:10]\n```", "```py\n# Plot the correlation heatmap\nplt.figure(figsize=(16, 6))\n\nheatmap = sns.heatmap(data.corr(), vmin=-1, vmax=1, annot=True, cmap='BrBG')\n\nheatmap.set_title('Correlation Heatmap', fontdict={'fontsize':15}, pad=12)\nplt.savefig('heatmap_pre.png', dpi=300, bbox_inches='tight')\n\nplt.show()\n```", "```py\nfrom sklearn.feature_selection import RFE\nselector = RFE(model, n_features_to_select=30, step=1)\nselector = selector.fit(X_train, y_train)\n\nrfe_vars_keys = list(X_train.columns)\nrfe_vars_values = list(selector.ranking_)\nrfe_vars = dict(zip(rfe_vars_keys, rfe_vars_values))\n\nsorted(rfe_vars.items(), key=lambda x: x[1])\n```", "```py\ndef grid_search(X, y, groups):\n    gkf = GroupKFold(n_splits=5).split(X, y, groups)\n    model = lgb.LGBMClassifier(objective='binary', verbose=-1, max_depth=-1, random_state=314, metric='None', n_estimators=5000)\n\n    grid = RandomizedSearchCV(\n        model, param_grid, scoring='roc_auc', random_state=314,\n        n_iter=100, cv=gkf, verbose=10, return_train_score=True, n_jobs=-1)\n\n    return grid\n\ngrid = grid_search(X, y, groups)\n\n%%time\ngrid.fit(X, y)\n\n#printing the best hyperparameters\nbest_params = grid.best_params_\n```", "```py\n# We split the data using our initial function\nX_train, y_train, X_val, y_val, X_test, y_test = split_df(df, ordinal_cols, ordinal_dfs, target='target_variable')\n```", "```py\n# We train the model using the best_params that we got from HP Tuning \nclf = lgb.LGBMClassifier(objective='binary', max_depth=-1, random_state=314, metric='roc_auc', n_estimators=5000, num_threads=16, verbose=-1,\n                           **best_params)\n%%time\nclf.fit(X_train, y_train, eval_set=(X_val, y_val), eval_metric='roc_auc')\n\n# Test set evaluates the final performance of the model on unseen users\nroc_auc_score(y_test, clf.predict(X_test))\n```"]