- en: Watch Out For Your Beam Search Hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/watch-out-for-your-beam-search-hyperparameters-9c4daf6668d6](https://towardsdatascience.com/watch-out-for-your-beam-search-hyperparameters-9c4daf6668d6)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The default values are never the best
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----9c4daf6668d6--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----9c4daf6668d6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9c4daf6668d6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9c4daf6668d6--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----9c4daf6668d6--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9c4daf6668d6--------------------------------)
    ·6 min read·Jan 11, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0fab5a914d706a814394dc57392b6d05.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Paulius Dragunas](https://unsplash.com/@paulius005?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: When developing applications using neural models, it is common to try different
    hyperparameters for training the models.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, the learning rate, the learning schedule, and the dropout rates
    are important hyperparameters that have a significant impact on the learning curve
    of your models.
  prefs: []
  type: TYPE_NORMAL
- en: What is much less common is the search for the **best decoding hyperparameters**.
    If you read a deep learning tutorial or a scientific paper tackling natural language
    processing applications, there is a high chance that **the hyperparameters used
    for inference are not even mentioned**.
  prefs: []
  type: TYPE_NORMAL
- en: Most authors, including myself, do not bother searching for the best decoding
    hyperparameters and use default ones.
  prefs: []
  type: TYPE_NORMAL
- en: Yet, these hyperparameters can actually have a significant impact on the results,
    and whatever is the decoding algorithm you are using there are always some hyperparameters
    that **should be fine-tuned to obtain better results**.
  prefs: []
  type: TYPE_NORMAL
- en: In this blog article, I show the impact of decoding hyperparameters with simple
    Python examples, and a machine translation application. I focus on beam search,
    since this is by far the most popular decoding algorithm, and two particular hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Framework and Requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To demonstrate the effect and importance of each hyperparameter, I will show
    some examples produced using the [Hugging Face Transformers package](https://huggingface.co/docs/transformers/index),
    in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install this package, run in your terminal (I recommend to do it in a separate
    conda environment) the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: I will use GPT-2 (MIT licence) to generate simple sentences.
  prefs: []
  type: TYPE_NORMAL
- en: I will also run other examples in machine translation using Marian (MIT licence).
    I installed it on Ubuntu 20.04, following the [official instructions](https://marian-nmt.github.io/docs/).
  prefs: []
  type: TYPE_NORMAL
- en: Beam Size and Length Penalty
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Beam search is probably the most popular decoding algorithm for language generation
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: It keeps at each time step, i.e., for each new token generated, the *k* most
    probable hypotheses, according to the model used for inference, and the remaining
    ones are discarded.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, at the end of the decoding, the hypothesis with the highest probability
    will be the output.
  prefs: []
  type: TYPE_NORMAL
- en: '*k,* usually called the “beam size”, is a very important hyperparameter.'
  prefs: []
  type: TYPE_NORMAL
- en: '**With a higher *k* you get a more probable hypothesis**. Note that when *k*=1,
    we talk about “greedy search” since we only keep the most probable hypothesis
    at each time step.'
  prefs: []
  type: TYPE_NORMAL
- en: By default, in most applications, *k* is **arbitrarily** set between 1 and 10\.
    Values that may seem very low.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main reasons for this:'
  prefs: []
  type: TYPE_NORMAL
- en: Increasing *k* **increases the decoding time and the memory** requirements.
    In other words, it gets more costly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Higher *k* **may yield more probable but worse results**. This is mainly, but
    not only, due to the length of the hypotheses. **Longer hypotheses tend to have
    lower probability**, so beam search will tend to promote shorter hypotheses that
    may be more unlikely for some applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first point can be straightforwardly fixed by performing better batch decoding
    and investing in better hardware.
  prefs: []
  type: TYPE_NORMAL
- en: 'The length bias can be controlled through another hyperparameter that normalizes
    the probability of an hypothesis by its length (number of tokens) at each time
    step. There are numerous ways to perform this normalization. One of the most used
    equation was proposed by [Wu et al. (2016)](https://arxiv.org/pdf/1609.08144.pdf):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Where |Y| is the length of the hypothesis and *α* an hyperparameter usually
    set between 0.5 and 1.0.
  prefs: []
  type: TYPE_NORMAL
- en: Then, the score lp(Y) is used to modify the probability of the hypothesis to
    bias the decoding and produce longer or shorter hypotheses given *α*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The implementation in Hugging Face transformers might be slightly different,
    but there is such an *α* that you can pass as “lengh_penalty” to the *generate*
    function, as in the following example (adapted from the [Transformers’ documentation](https://huggingface.co/docs/transformers/main_classes/text_generation)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: “num_beams” in this code sample is our other hyperparameter *k.*
  prefs: []
  type: TYPE_NORMAL
- en: 'With this code sample, the prompt “Today I believe we can finally”, k=4, and
    α=0.5, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'With k=50 and α=1.0, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: You can see that the results are not quite the same.
  prefs: []
  type: TYPE_NORMAL
- en: '***k* and *α* should be fine-tuned independently on your target task**, using
    some development dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a concrete example in machine translation to see how to do a simple
    *grid search* to find the best hyperparameters and their impact in a real use
    case.
  prefs: []
  type: TYPE_NORMAL
- en: Experiments with Machine Translation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For these experiments, I use Marian with a machine translation model trained
    on the [TILDE RAPID corpus](https://tilde-model.s3-eu-west-1.amazonaws.com/Tilde_MODEL_Corpus.html)
    (CC-BY 4.0) to do French-to-English translation.
  prefs: []
  type: TYPE_NORMAL
- en: 'I used only the first 100k lines of the dataset for training and the last 6k
    lines as devtest. I split the devtest into two parts of 3k lines each: the first
    part is used for validation and the second part is used for evaluation. Note:
    *the RAPID corpus has its sentences ordered alphabetically. My train/devtest split
    is thus not ideal for a realistic use case. I recommend shuffling the lines of
    the corpus, preserving the sentence pairs, before splitting the corpus. In this
    article, I kept the alphabetical order, and didn’t shuffle, to make the following
    experiments more reproducible.*'
  prefs: []
  type: TYPE_NORMAL
- en: I evaluate the translation quality with the metric [COMET](https://unbabel.github.io/COMET/html/index.html)
    (Apache License 2.0).
  prefs: []
  type: TYPE_NORMAL
- en: To search for the best pair of values for *k* and *α* with grid search, we have
    to first define a set of values for each hyperparameter and then try all the possible
    combinations.
  prefs: []
  type: TYPE_NORMAL
- en: Since here we are searching for decoding hyperparameters, this search is quite
    fast and straightforward in constrat to searching for training hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sets of values I chose for this task are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*k*: {1,2,**4**,10,20,50,100}'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*α*: {0.5,**0.6**,0.7,0.8,1.0,1.1,1.2}'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I put in bold the most common values used in machine translation by default.
    For most natural language generation tasks, these sets of values should be tried,
    except maybe k=100 which is often unlikely to yield the best results while it
    is a costly decoding.
  prefs: []
  type: TYPE_NORMAL
- en: We have 7 values for *k* and 7 values for *α*. We want to try all the combinations
    so we have 7*7=49 decodings of the evaluation dataset to do.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can do that with a simple bash script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Then for each decoding output we run COMET to evaluate the translation quality.
  prefs: []
  type: TYPE_NORMAL
- en: 'With all the results we can draw the following table of COMET scores for each
    pair of values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5f1a5d8baf0cc152990ea15e2cafa6e6.png)'
  prefs: []
  type: TYPE_IMG
- en: Table by the author
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the result obtained with the default hyperparameter (underline)
    is lower than 26 of the other results obtained with other hyparameter values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Actually, all the results in bold are statistically **significantly better**
    than the default one. Note: *In this experiments I am using the test set to compute
    the results I showed in the table. In a realistic scenario, these results should
    be computed on another development/validation set to decide on the pair of values
    that will be used on the test set, or for a real-world applications.*'
  prefs: []
  type: TYPE_NORMAL
- en: Hence, for your applications, it is definitely worth fine-tuning the decoding
    hyperparameters to obtain better results at the cost of a very small engineering
    effort.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we only played with two hyperparameters of beam search. Many
    more should be fine-tuned.
  prefs: []
  type: TYPE_NORMAL
- en: Other decoding algorithms such as temperature and nucleus sampling have hyperparameters
    that you may want to look at instead of using default ones.
  prefs: []
  type: TYPE_NORMAL
- en: Obviously, as we increase the number of hyperparameters to fine-tune, the grid
    search becomes more costly. Only your **experience and experiments with your application**
    will tell you whether it is worth fine-tuning a particular hyperparameter, and
    which values are more likely to yield satisfying results.
  prefs: []
  type: TYPE_NORMAL
