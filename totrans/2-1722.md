# Python 数据工程师

> 原文：[https://towardsdatascience.com/python-for-data-engineers-f3d5db59b6dd](https://towardsdatascience.com/python-for-data-engineers-f3d5db59b6dd)

## 初学者的高级ETL技巧

[](https://mshakhomirov.medium.com/?source=post_page-----f3d5db59b6dd--------------------------------)[![💡Mike Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----f3d5db59b6dd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f3d5db59b6dd--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f3d5db59b6dd--------------------------------) [💡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----f3d5db59b6dd--------------------------------)

·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f3d5db59b6dd--------------------------------) ·阅读时间17分钟·2023年10月21日

--

![](../Images/9c664876939298ade749c4c53cb490c8.png)

图片由 [Boitumelo](https://unsplash.com/@writecodenow?utm_source=medium&utm_medium=referral) 提供，来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)

在这个故事中，我将讲述Python中的高级数据工程技术。毫无疑问，Python是最受欢迎的数据编程语言。在我近十二年的数据工程职业生涯中，我遇到过各种代码问题。这个故事简要总结了我如何解决这些问题并学会写出更好的代码。我将展示一些使我们的ETL更快并有助于提高代码性能的技术。

## 列表推导式

想象一下你正在遍历一个表的列表。通常，我们会这样做：

[PRE0]

但我们可以使用列表推导式。它们不仅更快，还减少了代码，使其更简洁：

[PRE1]

例如，循环处理一个超大的文件以转换（ETL）每一行，从未如此简单：

[PRE2]

列表推导式对于ETL处理大数据文件非常有用。假设我们有一个需要转换为换行符分隔格式的数据文件。在你的Python环境中尝试运行这个示例：

[PRE3]

输出将是**换行符分隔的JSON**。这是BigQuery数据仓库中的一种标准格式，准备好加载到表中了：

[PRE4]

## 生成器

如果我们处理的是**逐行存储**的CSV和DAT文件，那么我们的文件对象已经是一个**生成器**，我们可以使用列表推导式来处理数据，**不会消耗太多内存**：

[PRE5]

在我们实际将记录插入数据仓库表之前验证记录，对于批量数据处理管道可能是有用的。

> 我们经常需要在将数据文件加载到数据仓库之前验证它们。如果一个记录失败，那么整个批次都会失败。

我们可以用它来创建接近实时的分析管道。这也是一种非常经济高效的方式来处理数据，相比于流数据管道设计模式。我之前在这里写过：

[](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----f3d5db59b6dd--------------------------------) [## 数据管道设计模式

### 选择合适的架构和示例

[towardsdatascience.com](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----f3d5db59b6dd--------------------------------)

或者，在处理大数据时，如果我们的文件不是换行符分隔的文本，我们可以使用`yield`。这始终是一个好习惯，因为我们希望以内存高效的方式处理数据文件。例如：

[PRE6]

这将读取一个本地文件并以19字节为块进行处理。输出将是：

[PRE7]

这只是处理**二进制**数据的一个示例。在现实生活中，将文件内容分割成段使用**分隔符**（即换行符`'\n’`或`'}{'`）可能会更容易，这取决于我们的数据结构。

假设文本数据来自某个**外部**位置，即**云存储**。我们也可以将其处理为**流**。我们不希望加载整个数据文件并运行`split('\n')`逐行处理，这会消耗大量内存。我们可以使用`re.finditer`，它像**生成器**一样，以块的方式读取数据文件，这样我们就可以运行所需的ETL而**不会**消耗太多内存。

[PRE8]

输出：

[PRE9]

## Python **数据验证属性**

我们可以使用**Python属性** [2]来验证数据记录。如果记录不是我们定义的类的实例，则必须抛出异常。

> 我们可以将数据存储为数据类的对象。

就是这么简单。假设我们有一个**流数据管道**，我们想要验证记录中的一些字段。

> 简单来说——它们必须匹配现有的表格模式。

我们可以使用Python属性来实现。请看下面的示例。

[PRE10]

如果我们选择打破规则并分配一些不符合我们标准的值，则会抛出异常。例如，如果我们尝试调用`ConnectionDataRecord('', 1)`，将会抛出异常。

或者，我们可以使用一个名为`Pydantic`的库。请看下面的代码。如果我们用不符合要求的对象调用该函数，它将抛出一个错误。

[PRE11]

## 装饰器

装饰器的设计目的是使我们的代码看起来更简洁，并为其添加额外的功能。我们可以简单地将一个函数作为参数传递给另一个函数（装饰器），并在这个包装器内部进行一些数据转换。想象一下，我们有很多不同的ETL函数来处理数据，但我们只需要一个将结果上传到数据湖的函数。这就是我们如何做到的：

> 如果一些代码逻辑重复，使用**装饰器**是一个好习惯。

这有助于更容易维护代码库，并节省了我们在需要更改重复逻辑时的很多时间。

[PRE12]

装饰器因其有效性被广泛使用。考虑这个Airflow DAG示例：

[PRE13]

## 与API的工作

作为数据工程师，你会经常执行HTTP请求，调用各种API端点。下面是一个GET请求的示例。

[PRE14]

它从**免费**的**NASA小行星API**中提取一些数据，并返回所有在该日期接近地球的小行星。只需在上面的URL路径中替换你的API密钥或使用我创建的密钥。**requests**库处理所有事情，但还有更好的方法。

> 我们可以使用会话并以**流**的形式处理来自我们API端点的数据。

这将确保我们不会遇到任何内存问题，并以流式方式处理我们的GET请求[3]：

[PRE15]

理解HTTP请求的工作原理在数据工程中至关重要。

> 我每天处理各种API请求，不必依赖其他框架或库。

例如，就在几周前，我在**Dataform迁移项目**上工作，意识到现有的Google库（`from google.cloud import dataform_v1beta1`）无法创建调度。解决方法是使用Dataform API [4]，这就像向特定端点发出POST请求一样简单：

[PRE16]

这个请求的核心在于我们将`workflow_config`作为**json**发送，并使用来自Google文档[4]的知识在路径参数中添加`workflowConfigId`。

> 这将创建一个必要的调度，以在BigQuery的Dataform中运行我们的数据转换脚本。

类似地，正如我们在GET请求中所做的那样，我们可以使用Python **生成器**将数据流入我们的POST API端点：

[PRE17]

思路很清楚。我们可以以节省内存的方式处理和发送数据。

## 处理API速率限制

所有API都有速率限制，我们在提取数据时要记住这一点。我们可以使用装饰器来处理它。简单的装饰可以像这样实现：

[PRE18]

使用这个装饰器，我们的函数在15分钟内不会发起超过10次API调用。

处理这种情况的最简单方法是使用`time.sleep()`，但Python速率限制允许我们以这种优雅的方式做到这一点。

## Python中的Async和`await`

以**异步**方式执行ETL是另一个极其有用的功能。我们可以使用`asyncio`库来同时运行任务。让我们考虑这个简单的同步示例，其中我们在`for`循环中处理表：

[PRE19]

运行这段代码时，我们必须等待每个表完成`pull_data()`任务，但使用`Async`，我们可以并行处理它们。

考虑使用以下代码：

[PRE20]

它将同时从报告API中提取数据，并显著提高我们的ETL性能。

> 它帮助管理ETL任务，同时系统资源以最佳方式分配。

例如，我们可以同时运行两个ETL作业，但我们可以定义执行顺序：

[PRE21]

## 使用Map和Filter

映射和过滤比列表推导式的速度更快。

我们可以逐行转换数据，将`map`函数应用于数据集中的项目，将其处理为`iterable`：

[PRE22]

我们希望使用`filter`来提取符合特定条件的对象，即

[PRE23]

## 使用 Pandas 处理大型数据集

后来的 Pandas 库版本提供了一个方便的上下文管理器，可以像这样使用：

[PRE24]

> 它将以批处理模式处理数据，假设我们不需要一次性将整个数据集加载到数据框中。

它有广泛的应用，从 OLAP 报告到机器学习（ML）管道。例如，我们可能想要创建一个推荐模型训练任务，并需要像这样准备数据集：

[PRE25]

这样，Pandas 将确保我们的应用程序始终有足够的内存来处理数据。

## 使用 joblib 进行管道处理和并行计算

`joblib.dump()`和`joblib.load()`方法允许我们高效地管道大型数据集转换。`joblib`将存储和序列化大数据，处理任意 Python 对象，如`numpy`数组。

> 你认为`scikit-learn`用什么来保存和加载机器学习模型？正确的答案是 - `joblib`。

首先，为什么要保存模型？——简单来说，因为我们可能在管道后面需要它，即使用新数据进行预测等。

> 我们不希望重新训练我们的机器学习模型，因为这是一个非常耗时的任务。

另一个原因是我们可能希望保存相同模型的不同版本，以便查看哪个版本表现更好。`joblib`有助于完成所有这些工作[5]：

[PRE26]

这些函数明确连接了我们在磁盘上保存的文件和原始 Python 对象的执行上下文。因此，除了文件名，`joblib`还接受文件对象：

[PRE27]

**AWS S3 模型转储/加载示例：**

[PRE28]

## 使用 joblib 进行并行计算

这非常高效，因为它依赖于多进程，并且会使用多个 Python 工作者在所有 CPU 核心上或跨多台机器并发执行任务。考虑这个例子：

[PRE29]

> 我们可以利用所有 CPU 核心来释放硬件的全部潜力。

在这里，我们告诉`Parallel`使用所有核心（-1），计算速度**提高了 5 倍：**

[PRE30]

## 单元测试 ETL 管道

在我整个数据工程师职业生涯中，我学到的最重要的一点是所有东西都必须进行单元测试。这不仅包括**SQL**，还包括**ETL 作业**和与我们数据管道中使用的其他服务的**集成**。

我们可以使用`unittest` Python 库来测试我们的代码。假设我们有一个助手模块，用于检查一个数字是否是素数：

[PRE31]

> 我们如何测试这个函数内部的逻辑？

`unittest`使这一切变得非常简单：

[PRE32]

现在如果我们在命令行中运行这个，我们将测试逻辑：

[PRE33]

这是正确的，因为13是一个素数。让我们进一步测试一下。我们知道4不是一个素数，因此我们希望针对这个特定函数的单元测试在断言为 False 时返回通过：

[PRE34]

[PRE35]

很简单。让我们看一个更高级的示例。

> 让我们假设我们有一个 ETL 服务，从某个 API 中提取数据，这需要很多时间。然后我们的服务将转换这个数据集，我们希望测试这个 ETL 转换逻辑是否持续存在。
> 
> 我们该怎么做呢？

我们可以使用`unittest`库中的 mock 和 patch 方法。考虑这个应用程序文件`asteroids.py`

[PRE36]

如果我们运行 app.py，输出将会是列出在特定日期接近地球的小行星：

[PRE37]

从 API 中提取数据可能需要很多时间，但我们希望我们的单元测试运行得更快。我们可以**模拟**一些假的 API 响应到我们的`get_data()`函数中，然后使用它来测试 `save_data()`函数中的 ETL 逻辑：

[PRE38]

输出将是：

[PRE39]

在我们的单元测试中，我们替换了（使用`mock`）`asteroids.get_data`函数返回的值，并期望它们被转换为（ETL）`['asteroid_1', 'asteroid_2']`，而我们的 ETL 函数未能做到这一点。单元测试失败了。

> 单元测试是非常强大的。

它帮助我们处理在 ETL 管道中部署新功能时的人为错误。更多高级示例可以在我之前的故事中找到。我在 CI/CD 管道中非常频繁地使用它 [6]：

[](/test-data-pipelines-the-fun-and-easy-way-d0f974a93a59?source=post_page-----f3d5db59b6dd--------------------------------) [## 以有趣和简单的方式测试数据管道

### 初学者指南：为什么单元测试和集成测试对你的数据平台如此重要

[towardsdatascience.com](/test-data-pipelines-the-fun-and-easy-way-d0f974a93a59?source=post_page-----f3d5db59b6dd--------------------------------)

## 监控内存使用情况

我经常使用无服务器部署 ETL 微服务。这是一种非常整洁且具有成本效益的工具。我部署 Lambdas 和 Cloud Functions，不希望它们因内存过多而被过度配置。

我之前在这里写过：

[## 初学者的基础设施即代码](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----f3d5db59b6dd--------------------------------)

### 使用这些模板像专业人士一样部署数据管道

[levelup.gitconnected.com](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316?source=post_page-----f3d5db59b6dd--------------------------------)

> 确实，我们为什么要给我们的 Lambda 配置 3Gb 的内存并支付更多费用，而数据可以在 256Mb 的内存中处理？

有多种方法可以监控我们的 ETL 应用程序内存使用情况。其中一种最受欢迎的方法是`tracemalloc` [7]库。

它可以跟踪 Python 内存块，并以（<current>, <peak memory>）字节格式返回结果。考虑这个例子，从小行星 API 中提取数据到一个大块中并保存到磁盘：

[PRE40]

输出将是：

[PRE41]

> **我们可以看到峰值使用量约为 540Kb。**

让我们看看如何通过使用`stream`来进行简单的优化：

[PRE42]

[PRE43]

**我们可以看到峰值内存使用量减少了一半。**

## 使用 SDK

作为数据工程师，我们通常需要与云服务提供商频繁合作。简而言之，SDK 是一组服务库，允许以编程方式访问云服务。我们希望学习并掌握市场领导者如 Amazon、Azure 或 Google 的一两个 SDK。

我经常以编程方式访问的服务之一是 Cloud Storage。实际上，在数据工程中，几乎每个数据管道都依赖于云中的数据存储，即 Google Cloud Storage 或 AWS S3。

最常见的数据管道设计是围绕数据存储桶创建的。我在之前的故事中描述了这一模式 [9]。

[](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----f3d5db59b6dd--------------------------------) [## 数据管道设计模式

### 选择合适的架构及示例

[towardsdatascience.com](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----f3d5db59b6dd--------------------------------)

在云存储中创建的对象可以触发其他 ETL 服务。这在使用这些事件编排数据管道时变得非常有用。

在这种情况下，我们希望能够在用作数据平台的数据湖的云存储中读取和写入数据。

![](../Images/851620512cf55064bfb03114e16ea2ab.png)

典型的数据管道。作者提供的图像

在这个图示中，我们可以看到我们首先将数据提取并保存到数据湖存储桶中。然后，它将触发数据仓库的数据摄取，并将数据加载到我们的表中，以便使用商业智能（BI）工具进行 OLAP 分析。

下面的代码片段解释了如何使用 AWS SDK 以流的形式保存数据。

[PRE44]

在你的命令行中运行以下命令以从 NASA API 提取小行星数据：

[PRE45]

## 结论

这个故事总结了我在 ETL 服务中几乎每天使用的 Python 代码技术。我希望你也能发现它有用。它有助于保持代码的整洁，并高效地执行数据管道转换。无服务器应用模型是一个非常具有成本效益的框架，我们可以在其中部署几乎不花费任何费用的 ETL 微服务。我们只需要优化内存使用，并以原子方式部署它们，以便它们运行得更快。它几乎可以处理我们数据平台中的任何类型的数据管道。在我之前的故事中可以找到这些架构类型和设计模式的良好总结。

[](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----f3d5db59b6dd--------------------------------) [## 数据平台架构类型

### 它在多大程度上满足你的业务需求？选择的困境。

[towardsdatascience.com](/data-platform-architecture-types-f255ac6e0b7?source=post_page-----f3d5db59b6dd--------------------------------)

理解基本的 HTTP 方法在数据工程中至关重要，它有助于为我们的数据管道创建稳健的 API 交互。使用`joblib`对我们的函数和模型进行管道化可以编写快速高效的代码。通过流的方式从 API 拉取数据，并以内存高效的方式运行 ETL 任务，可以防止资源过度分配，并确保我们的数据服务不会耗尽内存。可以使用 CI/CD 工具持续运行单元测试，这有助于在我们的代码更改达到生产环境之前，及早发现错误和人为失误。希望你喜欢阅读这篇文章。

## 推荐阅读：

[1] [https://stackoverflow.com/questions/519633/lazy-method-for-reading-big-file-in-python](https://stackoverflow.com/questions/519633/lazy-method-for-reading-big-file-in-python)

[2] [https://docs.python.org/3/library/functions.html#property](https://docs.python.org/3/library/functions.html#property)

[3] [https://stackoverflow.com/questions/60343944/how-does-requests-stream-true-option-streams-data-one-block-at-a-time](https://stackoverflow.com/questions/60343944/how-does-requests-stream-true-option-streams-data-one-block-at-a-time)

[4] [https://cloud.google.com/dataform/reference/rest/v1beta1/projects.locations.repositories.workflowConfigs/create](https://cloud.google.com/dataform/reference/rest/v1beta1/projects.locations.repositories.workflowConfigs/create)

[5] [https://joblib.readthedocs.io/en/stable/persistence.html#persistence](https://joblib.readthedocs.io/en/stable/persistence.html#persistence)

[6] [https://medium.com/towards-data-science/test-data-pipelines-the-fun-and-easy-way-d0f974a93a59](https://medium.com/towards-data-science/test-data-pipelines-the-fun-and-easy-way-d0f974a93a59)

[7] [https://docs.python.org/3/library/tracemalloc.html](https://docs.python.org/3/library/tracemalloc.html)

[8] [https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316](https://levelup.gitconnected.com/infrastructure-as-code-for-beginners-a4e36c805316)

[9] [https://medium.com/towards-data-science/data-pipeline-design-patterns-100afa4b93e3](https://medium.com/towards-data-science/data-pipeline-design-patterns-100afa4b93e3)
