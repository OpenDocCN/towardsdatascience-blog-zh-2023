- en: 'Embeddings: ChatGPT’s Secret Weapon'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/embeddings-chatgpts-secret-weapon-1870e590f32c](https://towardsdatascience.com/embeddings-chatgpts-secret-weapon-1870e590f32c)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Embeddings, and how they help ChatGPT predict the next word
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://emmaccode.medium.com/?source=post_page-----1870e590f32c--------------------------------)[![Emma
    Boudreau](../Images/f7201d012b733643d6e97957f73fd1fa.png)](https://emmaccode.medium.com/?source=post_page-----1870e590f32c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1870e590f32c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1870e590f32c--------------------------------)
    [Emma Boudreau](https://emmaccode.medium.com/?source=post_page-----1870e590f32c--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1870e590f32c--------------------------------)
    ·5 min read·Mar 6, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b73140f2b26fd1fb712cffe14b2f354e.png)'
  prefs: []
  type: TYPE_IMG
- en: (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: transformers and attention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you have been browsing the web a lot recently, or reading technical news
    stories, it is likely you have heard or read something about ChatGPT at some point.
    ChatGPT is OpenAI’s new **language transformer** model, and as far as these models
    go this is quite an accurate one that has produced some very compelling — sometimes
    viral — results. A **transformer** in this context refers to a machine-learning
    model which adopts **self-attention**. **Self-attention** is a Data Science word
    that simply means this model attempts to mimic human cognitive function, or human
    cognitive attention. The ***language*** part of this model is also significant;
    as this describes what the transformer hopes to predict, human language. This
    is often referred to as **Natural Language Processing**, or **NLP**, though NLP
    typically refers to the processing that takes place on the language data in order
    to turn it into numerical weights that a computer or neural network can understand.
  prefs: []
  type: TYPE_NORMAL
- en: 'Transformers have a number of significant properties that make their definition
    important to remember whenever we are discussing language transformers like ChatGPT.
    There are a few things that transformers, and more broadly attention models in
    general, that posses features that are not necessarily endemic to every type of
    machine-learning model. Under normal circumstances, a machine-learning model fits
    to some data and produces weights; we can think of this a lot like how a programming
    language compiles an executable: once the executable is compiled it becomes **static**
    and **immutable — it cannot be changed**, we cannot adjust the code from the inside.
    Transformers on the other hand have **soft weights**, which is a lot more like
    using a dynamically-typed programming language’s REPL, where the model weights
    **are mutable** and can be changed at runtime. This is the basis for many different
    and useful model types such as L**ong-Short-Term** **Memory**, or **LSTM**, models,
    and what we are discussing today: The Transformer.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1a98179807beafbc730ef9ae844f079e.png)'
  prefs: []
  type: TYPE_IMG
- en: This figure shows a transformer model’s architecture. ([image by Wikimedia commons](https://en.wikipedia.org/wiki/File:The-Transformer-model-architecture.png))
  prefs: []
  type: TYPE_NORMAL
- en: embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This brings us to embeddings, which give a lot more power and capability to
    these soft weights. Embeddings work by creating a new layer of dimensionality
    that is lower than the dimensionality of your actual encoded sparse vectors. This
    can be thought of as almost a grouping for this data that factors into the final
    calculation of the model. In essence, embeddings are a low-dimensional space that
    gives cadence to much larger high-dimensional vectors. We are almost engineering
    a new feature into our model, which our model may use as a classification to infer
    more about nuances in our data — something that is very important when we consider
    the complexity of something like interpreting human language. Dimensionality,
    in this context, refers to the dimensions, or shape, of the data. A great way
    to think about embeddings is to think about a dart board.
  prefs: []
  type: TYPE_NORMAL
- en: Inside of a room, we have a dartboard. We want to utilize this dartboard in
    order to predict the level of skill of a given occupant. We find that people who
    are incredibly bad at darts tend to hit the bottom of the dartboard, whereas people
    who are a little skillful tend to hit the top, and very skilled people tend to
    hit the middle. The actual positions of where they are hitting would be our original
    features, which we would build weights and probability for. However, if we were
    to label each of these regions and associate them with how skilled people are
    to make a generalization on the data that lies there, which might help us to make
    more nuanced predictions within that context, this would be the concept of embeddings.
    The embedding becomes a point, data in and of itself, on axes that determine its
    similarity to other embeddings in this low-dimensional space. We could use this
    to predict how many years a darts player has been playing, for example.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/77ea2fdc13d9657eab3e9b3cec011be7.png)'
  prefs: []
  type: TYPE_IMG
- en: a simplified interpretation of how this looks in your model. (image by author)
  prefs: []
  type: TYPE_NORMAL
- en: In essence, what embeddings tell us about a particular set of data is that the
    data within the embedding is similar to the other data within that same embedding.
    Embeddings are essentially just categorical data on top of other data. Another
    thing to note is that these categories can also be multi-dimensional, meaning
    there can be more than one embedding that is used with weights potentially binded
    to the same embeddings. Embeddings can also be learned from data, which means
    this can be a component of a nueral network without adding a lot of things, making
    them a no-brainer for many applications, like transformers such as ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenAI has their own embeddings endpoint, which makes it super easy to perform
    natural language tasks, topic modeling, classification, and even clustering. If
    you would like to read more about OpenAI embeddingz, here is a link to the paper
    which discusses this in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://arxiv.org/abs/2201.10005?source=post_page-----1870e590f32c--------------------------------)
    [## Text and Code Embeddings by Contrastive Pre-Training'
  prefs: []
  type: TYPE_NORMAL
- en: Text embeddings are useful features in many applications such as semantic search
    and computing text similarity…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: arxiv.org](https://arxiv.org/abs/2201.10005?source=post_page-----1870e590f32c--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: It is easy to imagine how these embeddings play a role in processing text and
    creating an accurate language model prediction. Words can certainly be categorized,
    and when you consider the grand scheme of words in the English language, it can
    be very hard to get a grasp on what words actually do. However, if we break these
    words down into categories; articles, nouns, verbs, it can be much easier to ascertain
    an idea of how grammar actually works in our language. When it comes to language
    models, the goal of word embedding is to capture the meaning of a word in a vector
    representation. We create a general categorization for the meaning of a word and
    then utilize what we learn about that category to make inferences on our output.
    Also, the granularity ChatGPT uses to represent text is sub-words, not whole words.
    So ChatGPT utilizes these sorts of embeddings in order to categorize and describe
    certain parts of words.
  prefs: []
  type: TYPE_NORMAL
- en: closing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Embeddings are an essential concept for many different types of machine-learning
    models: recommendation algorithms, language transformers, and even classification
    models are some examples of targets that benefit greatly from having an embedding
    layer. OpenAI’s embedding implementation helps the ChatGPT model to interpret
    words based on categories and their numerical relation to those categories, which
    is a lot easier than trying to find insights on each individual word. If you would
    like to learn more about embeddings, and how to implement them into your own Tensorflow
    networks, I highly suggest the embedding tutorial that Google put out a few years
    ago, here is a link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture?source=post_page-----1870e590f32c--------------------------------)
    [## Embeddings | Machine Learning | Google Developers'
  prefs: []
  type: TYPE_NORMAL
- en: An embedding is a relatively low-dimensional space into which you can translate
    high-dimensional vectors. Embeddings…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: developers.google.com](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture?source=post_page-----1870e590f32c--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading!
  prefs: []
  type: TYPE_NORMAL
