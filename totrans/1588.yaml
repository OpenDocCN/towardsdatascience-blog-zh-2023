- en: Now, why should we care about Recommendation Systems…? ft. A soft introduction
    to Thompson Sampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/now-why-should-we-care-about-recommendation-systems-ft-a-soft-introduction-to-thompson-sampling-b9483b43f262](https://towardsdatascience.com/now-why-should-we-care-about-recommendation-systems-ft-a-soft-introduction-to-thompson-sampling-b9483b43f262)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An ongoing Recommendation System series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://irenechang1510.medium.com/?source=post_page-----b9483b43f262--------------------------------)[![Irene
    Chang](../Images/02280890ed87239c75cbcbfa7c5d686c.png)](https://irenechang1510.medium.com/?source=post_page-----b9483b43f262--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b9483b43f262--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b9483b43f262--------------------------------)
    [Irene Chang](https://irenechang1510.medium.com/?source=post_page-----b9483b43f262--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b9483b43f262--------------------------------)
    ·12 min read·Nov 7, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a8dea9c6cebe66313b191c252f95b74c.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Myke Simon](https://unsplash.com/@myke_simon?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: I caught myself today, once again for the 100...01-th day in a row, holding
    my late unopened dinner box as I'm browsing through Netflix for a show to watch
    while munching on my food. My feed is filled with a few too many Asian romance
    and American coming of age suggestions, probably based on a series or two from
    these categories that I watched, like, a month or two ago. “There's nothing to
    watch here…"–sighed me as I finished reading all the synopses, feeling confident
    that I could predict how the plot would unveil. I whipped out my alternative go-to
    entertainment option, Tiktok, while subconsciously thinking to myself that I will
    probably need to *Not interested* some videos and *Like*, *Save* others to…recommend
    the algorithm sending me some new stream of content today.
  prefs: []
  type: TYPE_NORMAL
- en: Recommendation Systems (RecSys) can be considered such an established algorithm,
    which has been deeply implanted into our every day lives, to an extent that, on
    the scale of 1 to Chat-GPT, it feels almost like an 80s trend both to the academic
    and non-academic world. Nonetheless, it is by no means a near perfect algorithm.
    The ethical, social, technical, and legal challenges that come with operating
    a recommendation application have never been at the forefront of research (as
    is the case of most other technology products…). Select group unfairness and privacy
    violation are examples of the popular concerns revolving around RecSys that are
    still not fully addressed by the companies who implemented it. Besides, there
    exists many more subtle issues that are rarely given enough deliberations, one
    of which is the loss of autonomy in an individual's decision making process. A
    “powerful” RecSys can undoubtedly nudge users in a particular direction [2], making
    them purchase, watch, think, believe in something that they wouldn't have done
    had they not been subject to such manipulation.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, I want to write up a series along my grad school journey as I start learning
    and dive deeper into RecSys, their strengths and shortcomings…all from scratch!
    And I figure it can start with thinking about movies and…Thompson Sampling!
  prefs: []
  type: TYPE_NORMAL
- en: '**Thompson Sampling**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thompson Sampling (TS) is one of the foundational algorithms not only in recommendation
    system literature, but also in reinforcement learning. It is arguably a better
    A/B testing in online learning settings, as clearly explained by Samuele Mazzanti
    in this amazing [article](/when-you-should-prefer-thompson-sampling-over-a-b-tests-5e789b480458).
    In simple terms, in movie recommendation context, TS tries to identify the best
    movie to recommend me that will maximize the chance that I will click to watch.
    It can do so effectively using relatively less data as it allows the parameters
    to be updated every time it observes me click or not click into a movie. Roughly
    speaking, this dynamic characteristic enables TS to take into account, on top
    of the my watch history and bookmarked series, real time factors such as the browsing,
    or the search results within the app I'm making at the moment to give me the most
    suitable suggestion. However, in this beginner friendly tutorial, let's just look
    into a simplified analysis below.
  prefs: []
  type: TYPE_NORMAL
- en: Let's break it down even further!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Consider these 3 movies, which, all amazing as they are, I, controversially
    enough, do have my own personal ranking for. Out of these 3 movies, say, there's
    one that I will 100% rewatch if it comes up on my feed, one that I’m highly unlikely
    going to rewatch (5%), and one that there's a 70% chance I will click watch whenever
    I see it. TS obviously does not have this information about me beforehand and
    its goal is to learn my behavior so as to, as common intuition goes, recommend
    me the movie that it knows I will for sure click watch.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2119d09ba44d1c295b556f676126e67e.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'In the TS algorithm, the main workflow goes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Action**: TS suggests me a specific movie, among hundreds of others'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Outcome**: I decide that the movie sounds interesting enough to me and click
    to watch it, or I find it boring and click out of the page after reading the synopsis'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Reward**: Can be thought of as the number of “points" TS scores if I click
    to watch a certain movie or TS misses if I don''t click. In basic movie or ad
    recommendation settings, we can treat reward to be an equivalent of outcome, so
    1 click on the movie = 1 point!'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Update** knowledge: TS registers my choice and update its belief as to which
    movie is my favorite.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Repeat** step 1 (can be within my current browsing session, or at dinner
    time the next day), but now with some additional knowledge about my preferences.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exploration/Exploitation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is the most used term in this literature, and is also what sets TS and
    other related algorithms apart. Step 5 above is where this logic kicks in. In
    TS world, everything has some degree of uncertainty to it. Me drinking latte three
    times and matcha five times in a week does not necessarily mean I love matcha
    more than latte, what if it's just that one week (and I'm actually drinking more
    latte than matcha on average per week)? For this reason, everything in TS is represented
    by some type of distribution, rather than just single numbers.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/df7f1bafb93b4aac7b9a48a9e24ec6e0.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 1** In a particular week, I drink 5 cups of matcha and 3 cups of latte
    (left), but on average I drink more latte than matcha in a week (right) — Image
    by Author'
  prefs: []
  type: TYPE_NORMAL
- en: Starting out, TS obviously has a lot of uncertainty around my preference for
    the movies, so its priority is to ***explore*** this by giving me many different
    movie suggestions in order to observe my response to the suggestions. After some
    few clicks and skips, TS can sort of figure out the movies that I tend to click
    and the movies that yield no benefits, and hence it has gained more confidence
    in what movie to give out to me next time. This is when TS starts to ***exploit***
    the highly rewarding options, where it gives me the movie I click watch often,
    but still leaves some room for more exploration. The confidence builds up as more
    observations come in, which, in simple cases, will reach the point where the exploration
    work is now very minimal since TS already has a lot of confidence to exploit the
    recommendation that gives a lot of rewards.
  prefs: []
  type: TYPE_NORMAL
- en: Exploration vs Exploitation are thus often referred to as the **tradeoff** or
    the dilemma because too much exploration (i.e little elimination of low value
    choices despite already gaining enough evidence to know that such choices are
    not optimal) and you incur a lot of loss, too much exploitation (i.e eliminate
    too many options too quickly) and you re likely to falsely eliminate the true
    optimal action.
  prefs: []
  type: TYPE_NORMAL
- en: 'The distributions: Beta-Bernoulli'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As in the matcha-latte graph above, TS works with different kinds of distributions
    to understand our preference for different options. In the most basic cases of
    movies (and ads as well), we often use the Beta-Bernoulli combo.
  prefs: []
  type: TYPE_NORMAL
- en: '[Bernoulli distribution](/understanding-bernoulli-and-binomial-distributions-a1eef4e0da8f#:~:text=The%20Bernoulli%20distribution%20is%20the,probability%20(1%2Dp).)
    is a discrete distribution in which there are only two possible outcomes: 1 and
    0\. Bernoulli distribution consists of only one parameters, which indicates the
    probability of some variable, say Y, being 1\. So, if we say Y~ Bern(p), and for
    instance, p = 0.7, that means Y has 0.7 chance of having the value of 1, and 1–p
    = 1–0.7 = 0.3 chance of being 0\. Thus, Bernoulli distribution is suitable to
    model the reward (also outcome in our case) because our reward only has binary
    outcome: *Clicked* or *Not Clicked*.'
  prefs: []
  type: TYPE_NORMAL
- en: Beta distribution is on the other hand used to model TS belief regarding my
    movie interests. Beta distribution takes in two parameters, alpha and beta, that
    are often thought of as the number of successes and failures, respectively, and
    both have to be ≥ 1\. Thus, it’s suitable to use Beta distribution to model the
    number of times that I click watch and the number of times I skip a movie. Let’s
    take a look at an example. Here, these are 3 different beta distributions representing
    3 movies, over 10 observations, so the total number of clicks and skips for all
    3 movies are the same (10), but the click and skip rates are different. For movie
    1, I click watch 2 times (alpha = 2) and skip 8 times (beta = 8); for movie 2,
    I click watch 5 times and skip 5 times; for movie 3, I click watch 8 times and
    skip 2.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0123fa0018661d2a4f4501c5bf8ccaa3.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 2.** Image by author'
  prefs: []
  type: TYPE_NORMAL
- en: According to the graph, we can see that the probability of me watching movie
    2 again peaks around 50%, whereas this probability for movie 1 is much lower,
    for example. We can think of the curves here as the probability of probability
    (of me watching a movie again), and so Beta distribution is ideal for representing
    TS’s belief about my movie preferences.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, I will help you gain a clear understanding of the algorithm
    implementation wise and methodology wise. Firstly, here is a snipper of the Thompson
    Sampling algorithm, in pseudocode and in Python. The pseudocode is taken from
    an amazing book on TS, called [*A tutorial on Thompson Sampling*](https://arxiv.org/abs/1707.02038)
    [Russo, 2017].
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bc239424d8d47da61c157fcedac73dbf.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 3** Thompson Sampling, Python implementation (left) and pseudocode
    (right) — Image by Author'
  prefs: []
  type: TYPE_NORMAL
- en: Let's break it down!
  prefs: []
  type: TYPE_NORMAL
- en: Sample model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This first step of the algorithm involves “guessing” how much I like each of
    the movies. As illustrated in the previous section, my preference for each movie
    can be represented using Beta curves as those in Fig. 2, which TS does not have
    prior knowledge about and is trying to figure out how these Beta curves look like.
    In *t = 1* (first round), TS can start out by assuming that I have an equal preference
    for all 3 movies, i.e equal starting number of clicks and skips (and my 3 Beta
    curves will look the same).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5794df9be46cac194f14024774455cab.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 4** TS''s first guess about my preference to be the same for all 3
    movies'
  prefs: []
  type: TYPE_NORMAL
- en: The three distributions here are what *p* is in the pseudocode in Fig. 3\. From
    each of these distributions, TS will sample a value, represented by theta, in
    order to help with the action selection in the next step.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a9a97d385abe5254d8229a4680b1d00d.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 5** Sample pairs of alpha-beta values to represent the distributions
    of our initial guess for each of the 3 movies (also referred to as actions/arms)'
  prefs: []
  type: TYPE_NORMAL
- en: Select and apply action
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this step, TS chooses the action to execute (i.e choose a movie to recommend)
    based on whichever theta values sampled is the largest. Take Figure 2 as the example
    again. Let’s say we only have 2 movies instead—only movie 1 and movie 3\. The
    idea of using the largest theta to select an action is that if the true distributions
    have little overlap, and I almost surely like one movie more than the other in
    our case, then the sampled theta for movie 1 is very unlikely to be bigger than
    that of movie 3\. In a similar way, if we just consider movie 2 and 3, we can
    see that now we have more overlaps between the distributions. However, if we continue
    to sample more theta values over enough rounds, then we can observe that the proportion
    of movie 3's thetas > movie 2's thetas is larger than the other way round, and
    TS will have enough information to conclude that movie 3 is the better “action"
    to take. In general, this is also the reason why the more distinct the unknown
    true distributions are, the fewer rounds of experiments it takes TS to work out
    which action, or arm, is the most optimal.
  prefs: []
  type: TYPE_NORMAL
- en: After applying the chosen action, TS will receive a response from me, that is
    whether I click watch the movie or not. This outcome, as mentioned above, is also
    considered our reward for the corresponding action. TS will record this observed
    outcome and employ it to update the belief about my movie preference in the next
    step.
  prefs: []
  type: TYPE_NORMAL
- en: Update distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the description for the Beta distribution above, we established that Beta
    distribution is characterized by the number of successes and failures. The more
    times I click watch a given movie, the more the mode of its beta distribution
    is pulled towards 1 and conversely, towards 0 the more I skip over the recommendation.
    Therefore, the update to the belief about a given movie, after it has been put
    out for suggestion and a response has been recorded, is done by simply adding
    one to either the alpha or beta parameter of the movie's beta distribution, depending
    on whether the movie is clicked or skipped.
  prefs: []
  type: TYPE_NORMAL
- en: This straightforward, interpretable parameter update method is why the Beta
    Bernoulli is a very common TS model.
  prefs: []
  type: TYPE_NORMAL
- en: Result and discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Rolling back to the scenario at the start of the article. We are on the journey
    of guessing which of the 3 movies is the most to optimal to recommend to me, provided
    that there is one that I will click watch 100%, one that I have 70% chance of
    clicking, and the other with only 5% chance (again, this knowledge is unknown
    to TS). The first row shows two different starting points for the simulation,
    which will allow us to observe whether we can reach the same final outcomes with
    different initial prior beliefs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/afc3c98d911b6126d07b914ab3ff5e5e.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Figure 6** TS simulations with various numbers of rounds T = 5, 10, 20\.
    The beta distributions represent TS’s beliefs about my movie preferences at the
    end of the experiment. ***Left column***: outcome when the starting distribution
    is Beta(1, 1). Right column: outcome when the starting distribution is Beta(2,
    3)'
  prefs: []
  type: TYPE_NORMAL
- en: From Figure 6, we can see that the final answer to which movie is my ultimate
    favorite is movie 1 — PARASITE (Sorry Marvel fans)!!
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the exploration journey of the two cases differ, in which the
    Beta(1, 1) starting guesses result in a faster arrival at the movie that's believed
    to be my most favorite. It only needs T=10 rounds to see that TS is clearly exploiting
    movie 1, implying that TS has suggested movie 1 and achieved clicks from me, for
    which its beta distribution is pulled rightwards, hence the theta sampled from
    the updated distributions is beating its competitors, resulting in the exploitation.
    This exploitation already shows up with T=5 rounds, but according to the corresponding
    graph, there is still a lot of overlap between the beta of movie 1 and 3, their
    modes are not entirely distinct, meaning TS is still not completely certain that
    movie 1 is the optimal action.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the initial beliefs of Beta(2, 3) results in TS needing more
    rounds to arrive at movie 1 (T=20). Even at T=10, there is still a lot of uncertainty
    between movie 1 and 3, and it is observed that, due to the randomness in sampling
    theta, it can result in movie 3 being exploited as the optimal option. This experiment
    shows that the initial prior knowledge about each action can play a role in how
    fast the optimal arm is detected, the topic of which we can dive more into in
    the future articles.
  prefs: []
  type: TYPE_NORMAL
- en: It's good to note that if the actual distributions of the movies are almost
    identical (say if movie 1 and 3 has 100% and 98% click rate, respectively), TS
    is very likely to fail in identifying the optimal action because the proportion
    of the sampled thetas from one distribution exceeding those of the other will
    be split. Thus, if due to chance that movie 3 happens to have more “larger thetas",
    then TS will exploit this option more, leading to it being misidentified as the
    optimal action.
  prefs: []
  type: TYPE_NORMAL
- en: Another note from the experiment is that TS is only good at telling us what
    the best action is, but cannot give us information about the remaining options.
    This is because over the course of the exploration process, TS will quickly eliminate
    the options that are deemed not optimal, so TS stops receiving further information
    on these actions and hence it doesn't give the correct ranking between the actions
    beside the best one.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we have got to explore more about the Thompson Sampling algorithm
    and work through a simulation of movie recommendation. Thompson sampling largely
    involves distributions and prior knowledge in providing a prediction, which is
    a concept that is central to Bayesian models, which I plan to cover more with
    you guys in the up and coming articles. If you reach the end of this article,
    thank you for your time and I hope that this tutorial has given you a balanced
    technical and intuitive understanding of this algorithm! If you have any further
    question, feel free to find me through my [LinkedIn](https://www.linkedin.com/in/irene-chang-4356a013a/),
    happy to connect and respond to them!
  prefs: []
  type: TYPE_NORMAL
- en: 'References:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] [What are the potential risks and challenges of recommender systems in
    different domains and contexts?](https://www.linkedin.com/advice/3/what-potential-risks-challenges-recommender#:~:text=One%20of%20the%20ethical%20issues,groups%2C%20opinions%2C%20or%20values)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [Recommender systems and their ethical challenges](https://link.springer.com/article/10.1007/s00146-020-00950-y)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] [When You Should Prefer “Thompson Sampling” Over A/B Tests](/when-you-should-prefer-thompson-sampling-over-a-b-tests-5e789b480458)'
  prefs: []
  type: TYPE_NORMAL
