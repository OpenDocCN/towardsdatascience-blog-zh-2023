- en: 'Speak to me: How many words a model is reading'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/speak-to-me-how-many-words-a-model-is-reading-331e3af86d27](https://towardsdatascience.com/speak-to-me-how-many-words-a-model-is-reading-331e3af86d27)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '| ARTIFICIAL INTELLIGENCE| LLM| NLP |'
  prefs:
  - PREF_H2
  type: TYPE_TB
- en: Why and how to overcome the inner limit of a Large Language Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://salvatore-raieli.medium.com/?source=post_page-----331e3af86d27--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----331e3af86d27--------------------------------)[](https://towardsdatascience.com/?source=post_page-----331e3af86d27--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----331e3af86d27--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----331e3af86d27--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----331e3af86d27--------------------------------)
    ·20 min read·Jul 14, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e002ac8712d8db5b9d0e12618db308e8.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [C D-X](https://unsplash.com/@cdx2?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: '[LLMs](https://en.wikipedia.org/wiki/Large_language_model) have shown their
    skills in recent months, demonstrating that they are proficient in a wide variety
    of tasks. All this through one mode of interaction: prompting.'
  prefs: []
  type: TYPE_NORMAL
- en: In recent months there has been a rush to broaden the context of language models.
    **But how does this affect a language model?**
  prefs: []
  type: TYPE_NORMAL
- en: 'This article is divided into different sections, for each section we will answer
    these questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What is a prompt and how to build a good prompt?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the context window? How long it can be? What is limiting the length
    of the input sequence of a model? Why this is important?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How we can overcome these limitations?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do the models use the long context window?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to talk to a model?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/ec3fcf415b52be011dc84ba7b7c582fb.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Jamie Templeton](https://unsplash.com/@jamietempleton?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: What is a prompt and what is a good prompt?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Simply put, a prompt is how one interacts with a [large language model](https://en.wikipedia.org/wiki/Large_language_model)
    (LLM). Given an [LLM](https://en.wikipedia.org/wiki/Large_language_model), we
    can interact by providing instructions in text form. This textual prompt contains
    the information the model needs to process a response. The prompt can contain
    a question, task description, content, and lots of other information. Essentially,
    through the prompt we provide the model with what our intent is and what we expect
    it to respond to.
  prefs: []
  type: TYPE_NORMAL
- en: The prompt can drastically change the behavior of the model. For example, asking
    the model “describe the history of France” is different from asking it “describe
    the history of France in three sentences” or “describe the history of France in
    rap form.”
  prefs: []
  type: TYPE_NORMAL
- en: In order to get adequate information from the model, it is advisable to write
    a good prompt. In general, a good prompt should contain either a question or a
    set of instructions. In addition, there could be a context (question + context).
    For example, we could ask the model to tell us in an article (context) what the
    main characters are (question).
  prefs: []
  type: TYPE_NORMAL
- en: 'As a rule of thumb, there are some elements that need to be considered when
    writing a prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '**simplicity**, since it is often an iterative process, it is best to start
    with simple questions and gradually ask for more information. Also, templates
    work best if they are reduced in the form of subtasks: simple tasks instead of
    complex ones.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Instruction about the task.** Using verbs that specify the instruction helps
    the model better understand the task at hand. Also, some verbs work better for
    some tasks and it is recommended to provide the instruction at the beginning of
    the prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Specificity.** Being specific and detail-oriented in the prompt is beneficial
    to the execution of the task. Also, examples can be provided to better explain
    what is desired (few shot prompting). Since the length of the prompt is not infinite,
    however, you should avoid providing too many examples or too much detail (in many
    LLMs the prompt will be truncated beyond a certain length).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, there are also other techniques that can be used to improve the
    prompt, such as [chain-of-thought](https://en.wikipedia.org/wiki/Prompt_engineering)
    (forcing the model to regress on intermediate steps), self-reflection (allowing
    the model to evaluate its response), [tree of thoughts](https://github.com/kyegomez/tree-of-thoughts),
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://levelup.gitconnected.com/the-ai-college-student-goes-back-to-the-bench-daa6d9bdfb14?source=post_page-----331e3af86d27--------------------------------)
    [## The AI college student goes back to the bench'
  prefs: []
  type: TYPE_NORMAL
- en: How LLM can solve college exams and why this is important
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/the-ai-college-student-goes-back-to-the-bench-daa6d9bdfb14?source=post_page-----331e3af86d27--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: In general, although some of these techniques are trivial, they do not always
    yield good results. There are even more sophisticated techniques, and [prompt
    engineering](https://en.wikipedia.org/wiki/Prompt_engineering) is still an open
    field. The principle of these techniques is to allow the model to reason about
    a problem or to make the best use of what it has learned during training.
  prefs: []
  type: TYPE_NORMAL
- en: '**In any case, all of these techniques must be exploited with one problem:
    the maximum number of** [**tokens**](https://en.wikipedia.org/wiki/Lexical_analysis#Token)
    **(subwords) that can be inserted inside a prompt.**'
  prefs: []
  type: TYPE_NORMAL
- en: How long can a prompt be and why?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'How long can be a prompt: The length of the context'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/03ee83f634efb95292c8c74c00f4f5c7.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [patricia serna](https://unsplash.com/ja/@sernarial?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: The prompt can grow quickly, especially if the context contains a lot of information
    (use articles in the context, past conversations, add external information, and
    so on). This means that the model must operate on long sequences as input.
  prefs: []
  type: TYPE_NORMAL
- en: Basically, an [LLM](https://en.wikipedia.org/wiki/Large_language_model) is a
    [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)),
    and transformers do not scale well with sequence length. This comes from the fact
    that the [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    is built on repeated blocks of [self-attention](https://en.wikipedia.org/wiki/Attention_(machine_learning))
    that have [a quadratic cost](https://en.wikipedia.org/wiki/Computational_complexity)
    with respect to length.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, there has been much previous work that has tried to reduce this cost
    with various strategies. Linear alternatives to self-attention have been found
    to be unexpressive, though.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://levelup.gitconnected.com/welcome-back-80s-transformers-could-be-blown-away-by-convolution-21ff15f6d1cc?source=post_page-----331e3af86d27--------------------------------)
    [## Welcome Back 80s: Transformers Could Be Blown Away by Convolution'
  prefs: []
  type: TYPE_NORMAL
- en: The Hyena model shows how convolution could be faster than self-attention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/welcome-back-80s-transformers-could-be-blown-away-by-convolution-21ff15f6d1cc?source=post_page-----331e3af86d27--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Autoregressive transformers are spectacular models for short sequences but scale
    poorly to long sequences such as high-resolution images, podcasts, code, or books.
    ([source](https://arxiv.org/abs/2305.07185))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Usually, the [context windows](https://datascience.stackexchange.com/questions/16424/what-is-context-window-size)
    are relatively small (512–1024 [tokens](https://en.wikipedia.org/wiki/Lexical_analysis#Token)).
    Yet, in recent months we have seen that there are now models even with thousands
    and thousands of tokens for the [context window](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/).
  prefs: []
  type: TYPE_NORMAL
- en: For example, [GPT-4](https://arxiv.org/abs/2303.08774) has a context length
    of 32k. Aside from, the fact that the number is impressive, it is not just a marketing
    question. In fact, the longer the [context length](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/)
    the more information a model can relate. Also, greater [context length](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/)
    allows for greater accuracy, and fluency, and is thought to stimulate the creativity
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: At the theoretical level it is true a [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    trained with [context length](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/)
    of 1k tokens could also generate in inference a sequence of 100 k. But since it
    is trained with a different [training data distribution](https://stats.stackexchange.com/questions/173968/difference-between-training-and-test-data-distribution)
    (much less than 100 k sequences) the result generated will be nonsensical.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, it has been shown that asking for a spell check at [ChatGPT](https://medium.com/data-driven-fiction/everything-but-everything-you-need-to-know-about-chatgpt-546af7153ee2)
    for texts longer than 1000 words leads the model to [hallucinate](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)).
  prefs: []
  type: TYPE_NORMAL
- en: The quadratic cost of [self-attention](https://en.wikipedia.org/wiki/Attention_(machine_learning))
    means that increasing the context length equals a corresponding increase in the
    cost of training. The cost of [LLaMA has been estimated at $3 million](https://matt-rickard.com/commoditization-of-large-language-models-part-3)
    (just as GPU training) increasing the context length by 50 times (from 2K to 100K)
    also means increasing the cost by 50 times.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/de3d8bb2b2120ae4c800291a69bd6ef6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2307.02486)'
  prefs: []
  type: TYPE_NORMAL
- en: Is self-attention the only limit to enlarging context length?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: No. After [tokenization](https://huggingface.co/learn/nlp-course/chapter6/8?fw=pt),
    the model takes the sequence of tokens and the first step is [embedding](https://en.wikipedia.org/wiki/Embedding).
    For a sequence of tokens n we have an [embedding](https://en.wikipedia.org/wiki/Embedding)
    size of d. Obviously, if n >> d there is a risk of information loss, and increasing
    n much more than d poses significant challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Also, the sinusoidal [positional encoder](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/)
    is not compatible with some solutions to be able to enlarge the [context length](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/)
    and needs to be rethought.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the training is [parallelized](https://pytorch.org/tutorials/advanced/ddp_pipeline.html),
    but during inference instead the computation is sequential. In fact, a token depends
    on the tokens generated by the sequence. So inference must also be optimized to
    enlarge the [context length](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/).
  prefs: []
  type: TYPE_NORMAL
- en: How is it possible to have a context window of tens of thousands or even hundreds
    of thousands of tokens?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Extending the context window of LLM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/243eb4617b6424fb6069a9e7a2a5d1d5.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Nagara Oyodo](https://unsplash.com/@nagaranbasaran?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Make your context XL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although the results of the past few months are impressive, there had already
    been attempts to increase the length of the [context window](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/)
    in 2019\. In fact, [the Transformer-XL](https://arxiv.org/abs/1901.02860) could
    generate coherent text up to thousands of tokens.
  prefs: []
  type: TYPE_NORMAL
- en: The authors exploited the idea of [recurrent neural networks](https://en.wikipedia.org/wiki/Recurrent_neural_network),
    in which the hidden state is reused to allow more information to be given to the
    [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)).
    In other words, after passing a sequence segment, while processing the next segment
    the previously obtained hidden state was reused. Looking at, the description of
    the model, the similarity to [RNNs](https://en.wikipedia.org/wiki/Recurrent_neural_network)
    is evident.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/14246a46a248e6b0e65cbb5bfeb7ab15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/1901.02860)'
  prefs: []
  type: TYPE_NORMAL
- en: A short training, a long inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although TransformerXL was an interesting solution, other strategies are being
    tested in recent months. These strategies aimed to solve the limitations inherent
    in the original [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    and also took advantage of today’s hardware advancements.
  prefs: []
  type: TYPE_NORMAL
- en: One idea to reduce the cost of training is to train the model with a [context
    length](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/)
    of 2K but then conduct [fine-tuning](https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning))
    on longer sequences (e.g., 65K). Theoretically, this could also work (the model
    learns a general representation of the language in the first training and then
    specializes in the task of longer sequences later).
  prefs: []
  type: TYPE_NORMAL
- en: In reality, this strategy with the original transformer is doomed to fail as
    [explained in a 2021 paper](https://arxiv.org/abs/2108.12409). As explained by
    the authors the ability of a larger [context length](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/)
    in inference is called “extrapolation.”
  prefs: []
  type: TYPE_NORMAL
- en: We define extrapolation as a model’s ability to continue performing well as
    the number of input tokens during validation increases beyond the number of tokens
    on which the the model was trained. We find that transformer language models (LMs)
    that use sinusoidal position embeddings have very weak extrapolation abilities.
    ([source](https://arxiv.org/abs/2108.12409))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/9d6d2a9cec906f9787d33ca53ac88e69.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2108.12409)'
  prefs: []
  type: TYPE_NORMAL
- en: So for the authors, positional encoding is the culprit for the lack of the original
    transformer’s ability to extrapolate. [Positional encoding](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/),
    which is a step at the beginning of the model, was included as a clever trick
    to allow the model to account for the position of each token in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors, suggest replacing it with attention with linear bias (ALiBI).
    In simple words, a penalty is added to the product of queries and keys in the
    [attention](https://en.wikipedia.org/wiki/Attention_(machine_learning)) and this
    penalty is proportional to their distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1edaf14f7f120c02e342c235c7a65354.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2108.12409)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/83e66d77cab42e6e12fbf265d1cc2470.png)'
  prefs: []
  type: TYPE_IMG
- en: '“When computing attention scores for each head, our linearly biased attention
    method, ALiBi, adds a constant bias (right) to each attention score (qi · kj ,
    left). As in the unmodified attention sublayer, the softmax function is then applied
    to these scores, and the rest of the computation is unmodified. m is a head-specific
    scalar that is set and not learned throughout training.” image source: [here](https://arxiv.org/abs/2108.12409)'
  prefs: []
  type: TYPE_NORMAL
- en: The trick is ingenious because it does not add parameters to learn and does
    not increase the [computational cost](https://en.wikipedia.org/wiki/Computational_complexity)
    by much.
  prefs: []
  type: TYPE_NORMAL
- en: Do you need all these tokens?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Extending the [context window](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/)
    to more than 100k tokens certainly has great interest. On the other hand, not
    all tokens in the sequence are actually interesting. **So, is it necessary for
    us to calculate the relationships (attention score) among all these tokens?**
  prefs: []
  type: TYPE_NORMAL
- en: 'So the idea is to leverage the sparsity when calculating the [attention score](https://en.wikipedia.org/wiki/Attention_(machine_learning))
    so that we do not calculate the relationships between the tokens in which we are
    not interested. [As Google explained](https://ai.googleblog.com/2021/03/constructing-transformers-for-longer.html),
    though, this is not exactly straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Two natural questions arise: 1) Can we achieve the empirical benefits of quadratic
    full Transformers using sparse models with computational and memory requirements
    that scale linearly with the input sequence length? 2) Is it possible to show
    theoretically that these linear Transformers preserve the expressivity and flexibility
    of the quadratic full Transformers? ([source](https://ai.googleblog.com/2021/03/constructing-transformers-for-longer.html))'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Google tried to answer these questions by starting an intriguing observation,
    the attention layer can be understood as a [graph](https://en.wikipedia.org/wiki/Graph).
    In fact, when computing the [attention](https://en.wikipedia.org/wiki/Attention_(machine_learning))
    for all positions (nodes) in a sequence we compute pairwise similarities (edges).
    So visually we can think of attention as a [directed graph](https://en.wikipedia.org/wiki/Directed_graph).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/79a381fb20e6c65345ccbde995b38f26.png)'
  prefs: []
  type: TYPE_IMG
- en: attention as a complete graph, image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Building on this concept, it can be seen that several alternatives to classical
    [attention](https://en.wikipedia.org/wiki/Attention_(machine_learning)) can be
    considered with graphs that are not [fully connected](https://en.wikipedia.org/wiki/Complete_graph).
  prefs: []
  type: TYPE_NORMAL
- en: Google, took advantage of this concept by first exploiting the combination of
    [global](https://machinelearningmastery.com/global-attention-for-encoder-decoder-recurrent-neural-networks/)
    and local (or [window](https://paperswithcode.com/method/sliding-window-attention))
    attention [in this paper](https://arxiv.org/abs/2004.08483), and then improving
    on the idea with [BigBird](https://arxiv.org/abs/2007.14062).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9408bf6a5512167fc16c22c979ba18a5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2007.14062)'
  prefs: []
  type: TYPE_NORMAL
- en: 'BigBird basically combines three concepts: global tokens to cover the whole
    sequence, local attention to cover the surrounding of each token, and for each
    token, there are tokens sampled randomly.'
  prefs: []
  type: TYPE_NORMAL
- en: BigBird succeeds in sufficiently approximating the [attention](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)).
    At the same time, it is sparse (so less [computation](https://mathoverflow.net/questions/400840/what-is-the-computational-cost-in-a-neural-network))
    pero it does not interrupt the flow of information (the ability of one token to
    influence other tokens).
  prefs: []
  type: TYPE_NORMAL
- en: The authors prove that this sparse attention is not only as expressive as the
    [original attention](https://en.wikipedia.org/wiki/Attention_(machine_learning))
    but can be used to extract contextual information from sequences that are long
    by nature such as [genomic sequences](https://www.institute.global/insights/public-services/what-genomic-sequencing-and-why-does-it-matter-future-health).
  prefs: []
  type: TYPE_NORMAL
- en: In any case, [the sparsity concept](https://blogs.nvidia.com/blog/2020/05/14/sparsity-ai-inference/)
    is so powerful that many researchers are of trying to implement it in other models
    such as the Vision Transformers.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/metas-hiera-reduce-complexity-to-increase-accuracy-30f7a147ad0b?source=post_page-----331e3af86d27--------------------------------)
    [## META’s Hiera: reduce complexity to increase accuracy'
  prefs: []
  type: TYPE_NORMAL
- en: Simplicity allows AI to reach incredible performance and surprising speed
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/metas-hiera-reduce-complexity-to-increase-accuracy-30f7a147ad0b?source=post_page-----331e3af86d27--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Conditional computation for fast transformer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Based on the idea that not all tokens are important, there is also another way
    to not apply all model weights to all tokens during training.
  prefs: []
  type: TYPE_NORMAL
- en: '[CoLT5](https://arxiv.org/abs/2303.09752) exploits this concept to increase
    the length of the input. Conditional computation, simply put, makes sure to allocate
    more resources to those tokens that are considered important.'
  prefs: []
  type: TYPE_NORMAL
- en: The authors construct a system, in which attention and [feedforward network](https://en.wikipedia.org/wiki/Feedforward_neural_network)
    computation are separated into two branches (heavy and light). The light layer
    is applied for all tokens, while the heavy MLP is applied only for the important
    ones. These tokens are selected by a routing module that decides which tokens
    are important.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f9e6dba639d1f7aea4ba1c669e3f9d3b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2303.09752)'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Query Attention to save computation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The key and values for each token are cached for each token during [inference](https://cloud.google.com/bigquery/docs/inference-overview)
    (so as to avoid redoing the same operations while generating text). On the one
    hand, this saves computations, but it increases memory usage in the [GPU](https://en.wikipedia.org/wiki/Graphics_processing_unit).
  prefs: []
  type: TYPE_NORMAL
- en: In order to avoid this, [Multi-Query Attention](https://arxiv.org/abs/1911.02150)
    (MQA) suggests sharing weights for all attention heads during the linear projection
    step of keys and values.
  prefs: []
  type: TYPE_NORMAL
- en: This has an advantage particularly when dealing with long sequences, decreasing
    not only memory usage but generation time. Google has demonstrated the advantage
    of MQA [while using PaLM](https://arxiv.org/abs/2211.05102).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1a4799dea135a88be889b944cfaad03f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2211.05102)'
  prefs: []
  type: TYPE_NORMAL
- en: Flash attention, the light in the new LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The models and ideas seen earlier seek to modify attention in ways that reduce
    its cost. [Flash attention](https://arxiv.org/abs/2205.14135) uses a different
    approach and is used by virtually all models today.
  prefs: []
  type: TYPE_NORMAL
- en: At the base, there is a better use of the [GPU](https://en.wikipedia.org/wiki/Graphics_processing_unit).
    In fact, the [GPU](https://en.wikipedia.org/wiki/Graphics_processing_unit) has
    its own memory hierarchy. When the [GPU](https://en.wikipedia.org/wiki/Graphics_processing_unit)
    performs an operation, data must be present on the fast memory (SRAM memory).
    The data is copied to this memory from the HBM memory, and once the calculations
    are finished, the output is copied to the HBM.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, SRAM memory is not only much faster but also much smaller. Over
    time, computation has become faster and faster and HBM has become the bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2de74aeb5d2edf2174d45fd357ee684a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2205.14135)'
  prefs: []
  type: TYPE_NORMAL
- en: This is because, during [attention](https://en.wikipedia.org/wiki/Attention_(machine_learning)),
    several operations are conducted (multiplication of queries and keys, [softmax](https://en.wikipedia.org/wiki/Softmax_function),
    multiplication of this result with values). These operations generate intermediates
    that are copied to the HBM and SRAM (back and forth). This data copying operation
    is what slows down the operation.
  prefs: []
  type: TYPE_NORMAL
- en: The SRAM has a memory limit, so the [flash attention](https://arxiv.org/abs/2205.14135)
    solution is to divide the various arrays (queries, keys, values) into blocks.
    So all operations are accomplished in a single GPU kernel and only then write
    the result to HBM. In addition, the softmax is also reduced as time, since it
    is calculated only on the block and not on the whole NxN matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8b759fc1b336c43cb2a09ba8d43c680b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2205.14135)'
  prefs: []
  type: TYPE_NORMAL
- en: Not only [META’s LLaMA](https://arxiv.org/abs/2302.13971) uses FlashAttention,
    but today v[irtually all models do](https://github.com/HazyResearch/flash-attention/blob/main/usage.md).
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/mlearning-ai/metas-llama-a-small-language-model-beating-giants-5065948e0b7f?source=post_page-----331e3af86d27--------------------------------)
    [## META’s LLaMA: A small language model beating giants'
  prefs: []
  type: TYPE_NORMAL
- en: META open-source model will help us to understand how LMs biases arise
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/mlearning-ai/metas-llama-a-small-language-model-beating-giants-5065948e0b7f?source=post_page-----331e3af86d27--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Recent development
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recent developments in [GPUs](https://en.wikipedia.org/wiki/Graphics_processing_unit)
    have also enabled an increase in increasing tokens and their context. There are
    now 80 GB GPUs for example.
  prefs: []
  type: TYPE_NORMAL
- en: Also, in addition to the technical refinements we have seen above, there is
    also a better understanding of why the [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    does not have the ability to extrapolate. For example, [in this paper](https://arxiv.org/abs/2306.00946),
    they show that the classical attention drifts away to a later position in the
    sequence (behavior that the authors input to sinusoidal [position encoding](https://d2l.ai/chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html)).
    For this reason, we have seen how positional encoding has been replaced in ALiBI
    (others have proposed that it can be replaced with a randomized version, random
    positional encoding).
  prefs: []
  type: TYPE_NORMAL
- en: '[Other authors point out](https://arxiv.org/abs/2208.11445) how techniques
    such as [chain-of-thought](https://arxiv.org/abs/2201.11903) help the model extrapolate
    since it must focus on the intermediates of reasoning. Also, [a few shots can
    improve the model’s ability to extrapolate](https://arxiv.org/abs/2207.04901)
    better than fine-tuning (in any case, it’s not a silver bullet). Actually, [fine-tuning
    with some tricks can bring very good results](https://arxiv.org/abs/2305.16300)
    as in the case of LLaMA 7B where in this study they introduced window attention
    they managed to increase the context window from 2K to 32K.'
  prefs: []
  type: TYPE_NORMAL
- en: However if previously [Claude](https://www.anthropic.com/index/introducing-claude)’s
    100K context length seemed unbelievable. [META’s Megabyte claims 1M tokens](https://arxiv.org/abs/2305.07185)
    (the trick would be, “Megabyte segments sequences into patches and uses a local
    submodel within patches and a global model between patches”).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fd5a0545b9d23f7618934d82fc2dd2b9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2305.07185)'
  prefs: []
  type: TYPE_NORMAL
- en: A paper published a short while ago even claims a 1G token. All of this shows
    how there is still a lot of active research, and how many groups are working on
    finding ways to extend the context length.
  prefs: []
  type: TYPE_NORMAL
- en: 'Noting all this research and alternative solutions, a question arises: how
    does the model use this long context? Can it make the best use of it?'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Lost in the Middle: How Language Models Use Long Contexts'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/3ca927e90379d27e98d5232e464cd5ca.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Ethan Sykes](https://unsplash.com/@e_sykes?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: The latest advances in [LLM](https://en.wikipedia.org/wiki/Large_language_model)
    have allowed the [context window](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/)
    to be extended, this opens up the question of whether indeed the models have benefited.
    [An article published this month investigated this issue](https://arxiv.org/abs/2307.03172).
  prefs: []
  type: TYPE_NORMAL
- en: '[The authors of this study](https://arxiv.org/abs/2307.03172) were able to
    take advantage of the fact that not only proprietary models such as [Claude](https://www.anthropic.com/index/introducing-claude)
    or [GPT-4](https://openai.com/gpt-4) have a long context window. In fact, [MPT-30B](https://huggingface.co/mosaicml/mpt-30b)
    and [LongChat-7B](https://huggingface.co/lmsys/longchat-13b-16k) have a context
    window of 8K and 16K tokens, respectively. The authors, therefore, decided to
    use both these two models as well as models that are closed ([GPT-3.5](https://en.wikipedia.org/wiki/GPT-3)
    and [Claude](https://www.anthropic.com/index/introducing-claude)).'
  prefs: []
  type: TYPE_NORMAL
- en: Having chosen the models, one must also choose tasks where it is necessary for
    the model to have a long context window. For example, in Multi-Document [Question
    Answering](https://en.wikipedia.org/wiki/Question_answering) requires the model
    to reason about a set of different documents to find the information that is needed
    for the answers. This is an important task since it mimics the fact of a search
    in a document corpus such as the Internet might for example (we can imagine an
    [AI search engine](https://blog.google/products/search/generative-ai-search/)
    that has to search multiple websites for the answer).
  prefs: []
  type: TYPE_NORMAL
- en: 'For a question x, there is a set of documents and only one of these documents
    contains the information to answer the question. As in the example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b6b84ae0a561ee03402d01a822f907ce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2307.03172)'
  prefs: []
  type: TYPE_NORMAL
- en: '[The authors exploited datasets of annotated questions](https://arxiv.org/abs/2307.03172)
    (google searches). They then added Wikipedia chunks that were related to the topic
    but did not contain the answer (being careful that the right document was not
    always in the same location because the [LLM](https://en.wikipedia.org/wiki/Large_language_model)
    could learn to do the heuristic trick).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/53277701e173f55af339329ad9dd5d0f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2307.03172)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The [authors note](https://arxiv.org/abs/2307.03172) three particularly interesting
    results:'
  prefs: []
  type: TYPE_NORMAL
- en: they note a U-shape response, dependent on the location of the relevant document.
    In other words, model performance degrades when the model must access information
    that is present in the center of the context window. Thus, models are much better
    at identifying relevant information if they are at the beginning or at the end
    of the input sequence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The performance is inferior to the closed-book setting. When the relevant document
    is in the center of the input context, the model performs worse than when no document
    is provided. In a closed-book setting, the model has to rely only on the memory
    of its parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general, performance degrades if more documents are provided to the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/6b71fe4a98e79f78bf118ea9b8177c5b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2307.03172)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/feacf46063eb3a37e3f3054bd1448253.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2307.03172)'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, [the authors note](https://arxiv.org/abs/2307.03172) that per se
    models with a longer context window are not superior to their counterparts with
    a shorter context sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Since the model struggles in using information that lies in the center of the
    [context window](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/),
    the authors wondered whether the model was at least capable of finding information
    again. In other words, using a simple file consisting of key-value pairs (a [JSON](https://en.wikipedia.org/wiki/JSON))
    can the model find back information?
  prefs: []
  type: TYPE_NORMAL
- en: The authors decided to use the simplest possible task to study the model’s behavior
    in depth. This is a basic skill, in which the model has to find a piece of information
    without the need for complex skills. Using simulated data, the authors created
    [JSON](https://en.wikipedia.org/wiki/JSON) containing key-value pairs, in which
    only one is the one of interest.
  prefs: []
  type: TYPE_NORMAL
- en: Our synthetic key-value retrieval task is designed to provide a minimal testbed
    for the basic ability to retrieve matching tokens from an input context. […] we
    explicitly seek to distill and simplify the task by removing as much natural language
    semantics as possible (using random UUIDs instead), since language features may
    present potential confounders ([source](https://arxiv.org/abs/2307.03172))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/4261ebb66c68cc15b845b5e48a882ca4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2307.03172)'
  prefs: []
  type: TYPE_NORMAL
- en: The result shows that not all models are capable, [Claude](https://www.anthropic.com/index/introducing-claude)
    on the one hand succeeds in the task, but other models have performance degradation
    if there are 140 or more key-value pairs.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5cae07592ed886d489a656a662a6bd28.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2307.03172)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, the authors observe a curious fact:'
  prefs: []
  type: TYPE_NORMAL
- en: LongChat-13B (16K) in the 140 key-value setting is a notable outlier; when the
    relevant information is at the start of the input context, it tends to generate
    code to retrieve the key, rather than outputting the value itself. ([source](https://arxiv.org/abs/2307.03172))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Why can’t LLMs make the best use of a long context window?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[The authors of this study wondered](https://arxiv.org/abs/2307.03172) if this
    was related to architecture. Today, two main types of architecture are used: [decoder-only](https://huggingface.co/learn/nlp-course/chapter1/6)
    and e[ncoder-decoder language models](https://huggingface.co/docs/transformers/model_doc/encoder-decoder).
    Although they have been used in a great many articles, there are still obscure
    points about the differences in their behaviors.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the authors decided to additionally use two other models: Flan-[T5-XXL](https://github.com/google-research/text-to-text-transfer-transformer)
    and [Flan-UL2](https://huggingface.co/google/flan-ul2). These two models show
    to be more robust when the relevant information is found in the middle of the
    [context window](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8fc79da4fc513fba306ec84af9d49552.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2307.03172)'
  prefs: []
  type: TYPE_NORMAL
- en: This is interesting because being bidirectional, an encoder-decoder could be
    more robust in processing information in a longer context window and therefore
    more efficient when it has to process multiple documents.
  prefs: []
  type: TYPE_NORMAL
- en: Is a long context window useful?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'If the model is not capable of exploiting it anyway, is there really a benefit
    in having such a long [context window](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/)?
    After all, having a longer context window comes at a cost anyway: the model has
    to process all the input information. In other words, if the model can take 100K
    tokens, it makes sense to feed it 100K pieces of information.'
  prefs: []
  type: TYPE_NORMAL
- en: The authors decided to test this using a retrieval system that takes an input
    query (a question from a dataset of questions) and finds k documents from Wikipedia.
    They then added them to the prompt and tested how the models behaved with these
    added documents.
  prefs: []
  type: TYPE_NORMAL
- en: Using more than 20 retrieved documents only marginally improves reader performance
    (∼1.5% for GPT-3.5-Turbo and ∼1% for Claude), while significantly increasing the
    input context length (and thus latency and cost). ([source](https://arxiv.org/abs/2307.03172))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/031d4cb56aaf49348fdb137418b7e517.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/abs/2307.03172)'
  prefs: []
  type: TYPE_NORMAL
- en: In other words, the model saturates. As mentioned before this confirms that
    the model is more efficient in using the information at the beginning of the [context
    window](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/).
  prefs: []
  type: TYPE_NORMAL
- en: Closing thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/5e1b993d4c7a4e6c08d6ec3a535198a8.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Saif71.com](https://unsplash.com/@saif71?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: The prompt is how we interact with a model. The more precise and detailed it
    is, the better the model’s response.
  prefs: []
  type: TYPE_NORMAL
- en: There is a limit to the amount of information we can put into a prompt, though.
    This limit is the context window and it comes from numerous factors as we saw
    earlier. Ever since the first transformer was published, attempts have been made
    to enlarge this context window by exploiting numerous solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Despite this, we do not know how well the model can use this context window.
    Studies today show that models do not make the most of them.
  prefs: []
  type: TYPE_NORMAL
- en: Since the scaling law was published there has been a race to the parameter,
    ever larger models looking for evanescent emergent properties. Today we know that
    all those parameters are not necessary, and that GPT-4 itself is not as large
    as thought, but actually an ensemble of eight models. The context window seems
    to be another frontier, where people try to reach a larger number not for real
    utility but to show the superiority of their model.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/the-infinite-babel-library-of-llms-90e203b2f6b0?source=post_page-----331e3af86d27--------------------------------)
    [## The Infinite Babel Library of LLMs'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open-source, data, and attention: How the future of LLMs will change'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/the-infinite-babel-library-of-llms-90e203b2f6b0?source=post_page-----331e3af86d27--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Despite the results and the myriad of published models there are still points
    that need to be studied. How the model uses this longer context window is one
    such point that needs more analysis. For despite the elegance of the technical
    solutions sometimes the cost of a longer context window is not justified.
  prefs: []
  type: TYPE_NORMAL
- en: '**What do you think? Let me know in the comments.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have found this interesting:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*You can look for my other articles, you can also* [***subscribe***](https://salvatore-raieli.medium.com/subscribe)
    *to get notified when I publish articles, you can* [***become a Medium member***](https://medium.com/@salvatore-raieli/membership)
    *to access all its stories (affiliate links of the platform for which I get small
    revenues without cost to you) and you can also connect or reach me on*[***LinkedIn***](https://www.linkedin.com/in/salvatore-raieli/)***.***'
  prefs: []
  type: TYPE_NORMAL
- en: '*Here is the link to my GitHub repository, where I am planning to collect code
    and many resources related to machine learning, artificial intelligence, and more.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/SalvatoreRa/tutorial?source=post_page-----331e3af86d27--------------------------------)
    [## GitHub - SalvatoreRa/tutorial: Tutorials on machine learning, artificial intelligence,
    data science…'
  prefs: []
  type: TYPE_NORMAL
- en: Tutorials on machine learning, artificial intelligence, data science with math
    explanation and reusable code (in python…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/SalvatoreRa/tutorial?source=post_page-----331e3af86d27--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*or you may be interested in one of my recent articles:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://levelup.gitconnected.com/codegen2-a-new-open-source-model-for-coding-214d3d464030?source=post_page-----331e3af86d27--------------------------------)
    [## CodeGen2: a new open-source model for coding'
  prefs: []
  type: TYPE_NORMAL
- en: SaleForce’s effect on how to design an efficient model for coding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'levelup.gitconnected.com](https://levelup.gitconnected.com/codegen2-a-new-open-source-model-for-coding-214d3d464030?source=post_page-----331e3af86d27--------------------------------)
    [](https://levelup.gitconnected.com/clinicalgpt-the-llm-clinician-5e1f7866b6d1?source=post_page-----331e3af86d27--------------------------------)
    [## ClinicalGPT: the LLM clinician'
  prefs: []
  type: TYPE_NORMAL
- en: LLMs today have been applied to a wide variety of tasks. On the other hand,
    generalist models underperform fine-tuned…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/clinicalgpt-the-llm-clinician-5e1f7866b6d1?source=post_page-----331e3af86d27--------------------------------)
    [](/a-fairy-tale-of-the-inductive-bias-d418fc61726c?source=post_page-----331e3af86d27--------------------------------)
    [## A fAIry tale of the Inductive Bias
  prefs: []
  type: TYPE_NORMAL
- en: Do we need inductive bias? How simple models can reach the performance of complex
    models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/a-fairy-tale-of-the-inductive-bias-d418fc61726c?source=post_page-----331e3af86d27--------------------------------)
    [](https://levelup.gitconnected.com/the-ai-college-student-goes-back-to-the-bench-daa6d9bdfb14?source=post_page-----331e3af86d27--------------------------------)
    [## The AI college student goes back to the bench
  prefs: []
  type: TYPE_NORMAL
- en: How LLM can solve college exams and why this is important
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/the-ai-college-student-goes-back-to-the-bench-daa6d9bdfb14?source=post_page-----331e3af86d27--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Reference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here is the list of the principal references I consulted to write this article,
    only the first name for an article is cited. I suggest also them if you want to
    deepen on the topic.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Engineering Guide, [link](https://www.promptingguide.ai/)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Wang, 2023, Prompt Engineering for Healthcare: Methodologies and Applications,
    [link](https://arxiv.org/abs/2304.14670)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: White, 2023, A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT,
    [link](https://arxiv.org/abs/2302.11382)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Liu, 2021, Pre-train, Prompt, and Predict: A Systematic Survey of Prompting
    Methods in Natural Language Processing, [link](https://arxiv.org/abs/2107.13586)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Liu, 2022, Generated Knowledge Prompting for Commonsense Reasoning, [link](https://arxiv.org/abs/2110.08387)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Sascha Heyer](https://medium.com/u/57d0091b2e22?source=post_page-----331e3af86d27--------------------------------),
    Generative AI — Best Practices for LLM Prompt Engineering, [link](https://medium.com/google-cloud/generative-ai-best-practices-for-llm-prompt-engineering-2a0131c805cc)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Jacob Ferus](https://medium.com/u/2415c881fd51?source=post_page-----331e3af86d27--------------------------------),
    GPT-4 Has Arrived — Here’s What You Need To Know, [link](https://levelup.gitconnected.com/gpt-4-has-arrived-heres-what-you-need-to-know-398c3c72191c)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Simon Attard](https://medium.com/u/5a0896ce2f9b?source=post_page-----331e3af86d27--------------------------------),
    Giving Large Language Models Context, blog post, [link](https://medium.com/@simon_attard/giving-large-language-models-context-2d1956a6a017)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Rickard, [Commoditization of Large Language Models: Part 3](https://matt-rickard.com/commoditization-of-large-language-models-part-3)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Vaswani, 2017, Attention Is All You Need, [link](https://arxiv.org/abs/1706.03762)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Press, 2021, Train Short, Test Long: Attention with Linear Biases Enables Input
    Length Extrapolation, [link](https://arxiv.org/abs/2108.12409)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Zaheer, 2021, Big Bird: Transformers for Longer Sequences, [link](https://arxiv.org/abs/2007.14062)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Ainslie, 2020, ETC: Encoding Long and Structured Inputs in Transformers, [link](https://arxiv.org/abs/2004.08483)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Google blog, Constructing Transformers For Longer Sequences with Sparse Attention
    Methods, 2021, [link](https://ai.googleblog.com/2021/03/constructing-transformers-for-longer.html)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Shazeer, 2019, Fast Transformer Decoding: One Write-Head is All You Need, [link](https://arxiv.org/abs/1911.02150)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pope, 2022, Efficiently Scaling Transformer Inference, [link](https://arxiv.org/abs/2211.05102)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Ahmed Taha](https://medium.com/u/996110eea09a?source=post_page-----331e3af86d27--------------------------------),
    FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness, [link](https://ahmdtaha.medium.com/flashattention-fast-and-memory-efficient-exact-attention-with-io-awareness-2a0aec52ed3d),
    medium post'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Angelina Yang](https://medium.com/u/ff66ee151115?source=post_page-----331e3af86d27--------------------------------),
    What is Global Attention in Deep Learning? blog post, [link](https://angelina-yang.medium.com/what-is-global-attention-in-deep-learning-53bd4525a389)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Dao, 2022, FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness,
    [link](https://arxiv.org/abs/2205.14135)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A deep dive on FlashAttention [here](https://shreyansh26.github.io/post/2023-03-26_flash-attention/)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Medium post on LLaMA](https://medium.com/mlearning-ai/metas-llama-a-small-language-model-beating-giants-5065948e0b7f),
    META’s LLaMA: A small language model beating giants'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Ainslie, 2023, CoLT5: Faster Long-Range Transformers with Conditional Computation,
    [link](https://arxiv.org/abs/2303.09752)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Yu, 2023, MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers,
    [link](https://arxiv.org/abs/2305.07185)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A blog post about recent development in extending the context length, [link](https://kaiokendev.github.io/context)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Andrew Lukyanenko](https://medium.com/u/26c63d12ebc9?source=post_page-----331e3af86d27--------------------------------),
    Paper Review: Scaling Transformer to 1M tokens and beyond with RMT, blog post,
    [link](https://artgor.medium.com/paper-review-scaling-transformer-to-1m-tokens-and-beyond-with-rmt-5b846cecc0b2)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Liu, 2023, Lost in the Middle: How Language Models Use Long Contexts, [link](https://arxiv.org/abs/2307.03172)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
