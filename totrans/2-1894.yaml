- en: 'Speak to me: How many words a model is reading'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对我说话：一个模型阅读了多少个词
- en: 原文：[https://towardsdatascience.com/speak-to-me-how-many-words-a-model-is-reading-331e3af86d27](https://towardsdatascience.com/speak-to-me-how-many-words-a-model-is-reading-331e3af86d27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/speak-to-me-how-many-words-a-model-is-reading-331e3af86d27](https://towardsdatascience.com/speak-to-me-how-many-words-a-model-is-reading-331e3af86d27)
- en: '| ARTIFICIAL INTELLIGENCE| LLM| NLP |'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_TB
  zh: '| 人工智能 | LLM | 自然语言处理 |'
- en: Why and how to overcome the inner limit of a Large Language Model
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么以及如何克服大型语言模型的内在限制
- en: '[](https://salvatore-raieli.medium.com/?source=post_page-----331e3af86d27--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----331e3af86d27--------------------------------)[](https://towardsdatascience.com/?source=post_page-----331e3af86d27--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----331e3af86d27--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----331e3af86d27--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://salvatore-raieli.medium.com/?source=post_page-----331e3af86d27--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----331e3af86d27--------------------------------)[](https://towardsdatascience.com/?source=post_page-----331e3af86d27--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----331e3af86d27--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----331e3af86d27--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----331e3af86d27--------------------------------)
    ·20 min read·Jul 14, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----331e3af86d27--------------------------------)
    ·阅读时间20分钟·2023年7月14日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/e002ac8712d8db5b9d0e12618db308e8.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e002ac8712d8db5b9d0e12618db308e8.png)'
- en: Photo by [C D-X](https://unsplash.com/@cdx2?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [C D-X](https://unsplash.com/@cdx2?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '[LLMs](https://en.wikipedia.org/wiki/Large_language_model) have shown their
    skills in recent months, demonstrating that they are proficient in a wide variety
    of tasks. All this through one mode of interaction: prompting.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[大语言模型](https://en.wikipedia.org/wiki/Large_language_model)（LLMs）在最近几个月展示了它们的技能，证明它们在各种任务中都很熟练。所有这些都通过一种交互模式：提示。'
- en: In recent months there has been a rush to broaden the context of language models.
    **But how does this affect a language model?**
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最近几个月，扩展语言模型的上下文成为了一种潮流。**但这对语言模型有何影响？**
- en: 'This article is divided into different sections, for each section we will answer
    these questions:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本文分为不同的部分，每一部分我们将回答这些问题：
- en: What is a prompt and how to build a good prompt?
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是提示，如何构建一个好的提示？
- en: What is the context window? How long it can be? What is limiting the length
    of the input sequence of a model? Why this is important?
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是上下文窗口？它能有多长？是什么限制了模型输入序列的长度？为什么这很重要？
- en: How we can overcome these limitations?
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何克服这些限制？
- en: Do the models use the long context window?
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型是否使用了长上下文窗口？
- en: How to talk to a model?
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何与模型对话？
- en: '![](../Images/ec3fcf415b52be011dc84ba7b7c582fb.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ec3fcf415b52be011dc84ba7b7c582fb.png)'
- en: Photo by [Jamie Templeton](https://unsplash.com/@jamietempleton?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Jamie Templeton](https://unsplash.com/@jamietempleton?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: What is a prompt and what is a good prompt?
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是提示？什么是好的提示？
- en: Simply put, a prompt is how one interacts with a [large language model](https://en.wikipedia.org/wiki/Large_language_model)
    (LLM). Given an [LLM](https://en.wikipedia.org/wiki/Large_language_model), we
    can interact by providing instructions in text form. This textual prompt contains
    the information the model needs to process a response. The prompt can contain
    a question, task description, content, and lots of other information. Essentially,
    through the prompt we provide the model with what our intent is and what we expect
    it to respond to.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，提示就是与[大型语言模型](https://en.wikipedia.org/wiki/Large_language_model)（LLM）交互的方式。给定一个[LLM](https://en.wikipedia.org/wiki/Large_language_model)，我们可以通过提供文本形式的指令来进行交互。这种文本提示包含了模型处理响应所需的信息。提示可以包含问题、任务描述、内容及其他许多信息。基本上，通过提示我们向模型提供了我们的意图以及我们期望它回应的内容。
- en: The prompt can drastically change the behavior of the model. For example, asking
    the model “describe the history of France” is different from asking it “describe
    the history of France in three sentences” or “describe the history of France in
    rap form.”
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 提示可以显著改变模型的行为。例如，要求模型“描述法国的历史”与要求它“用三句话描述法国的历史”或“用说唱形式描述法国的历史”是不同的。
- en: In order to get adequate information from the model, it is advisable to write
    a good prompt. In general, a good prompt should contain either a question or a
    set of instructions. In addition, there could be a context (question + context).
    For example, we could ask the model to tell us in an article (context) what the
    main characters are (question).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从模型中获取足够的信息，建议编写一个好的提示。通常，一个好的提示应该包含一个问题或一组指令。此外，还可以有上下文（问题 + 上下文）。例如，我们可以要求模型在一篇文章（上下文）中告诉我们主要人物是什么（问题）。
- en: 'As a rule of thumb, there are some elements that need to be considered when
    writing a prompt:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，编写提示时需要考虑一些元素：
- en: '**simplicity**, since it is often an iterative process, it is best to start
    with simple questions and gradually ask for more information. Also, templates
    work best if they are reduced in the form of subtasks: simple tasks instead of
    complex ones.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简洁性**，由于这是一个迭代过程，最好从简单的问题开始，然后逐渐要求更多信息。此外，如果将模板减少为子任务的形式，模板效果最佳：简单任务而不是复杂任务。'
- en: '**Instruction about the task.** Using verbs that specify the instruction helps
    the model better understand the task at hand. Also, some verbs work better for
    some tasks and it is recommended to provide the instruction at the beginning of
    the prompt.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关于任务的指令。** 使用能够明确指令的动词有助于模型更好地理解当前任务。此外，一些动词对于某些任务效果更佳，建议在提示的开头提供指令。'
- en: '**Specificity.** Being specific and detail-oriented in the prompt is beneficial
    to the execution of the task. Also, examples can be provided to better explain
    what is desired (few shot prompting). Since the length of the prompt is not infinite,
    however, you should avoid providing too many examples or too much detail (in many
    LLMs the prompt will be truncated beyond a certain length).'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**具体性。** 在提示中具体而注重细节有助于任务的执行。此外，可以提供示例来更好地解释所需内容（少量示例提示）。然而，由于提示的长度不是无限的，因此应避免提供过多的示例或过多的细节（在许多LLM中，提示在超过某一长度后会被截断）。'
- en: In addition, there are also other techniques that can be used to improve the
    prompt, such as [chain-of-thought](https://en.wikipedia.org/wiki/Prompt_engineering)
    (forcing the model to regress on intermediate steps), self-reflection (allowing
    the model to evaluate its response), [tree of thoughts](https://github.com/kyegomez/tree-of-thoughts),
    and so on.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有其他技术可以用来改进提示，例如 [chain-of-thought](https://en.wikipedia.org/wiki/Prompt_engineering)（强制模型回溯中间步骤）、自我反思（允许模型评估其响应）、[思维树](https://github.com/kyegomez/tree-of-thoughts)
    等等。
- en: '[](https://levelup.gitconnected.com/the-ai-college-student-goes-back-to-the-bench-daa6d9bdfb14?source=post_page-----331e3af86d27--------------------------------)
    [## The AI college student goes back to the bench'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://levelup.gitconnected.com/the-ai-college-student-goes-back-to-the-bench-daa6d9bdfb14?source=post_page-----331e3af86d27--------------------------------)
    [## 人工智能大学生回到实验台'
- en: How LLM can solve college exams and why this is important
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LLM 如何解决大学考试以及为什么这很重要
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/the-ai-college-student-goes-back-to-the-bench-daa6d9bdfb14?source=post_page-----331e3af86d27--------------------------------)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[levelup.gitconnected.com](https://levelup.gitconnected.com/the-ai-college-student-goes-back-to-the-bench-daa6d9bdfb14?source=post_page-----331e3af86d27--------------------------------)'
- en: In general, although some of these techniques are trivial, they do not always
    yield good results. There are even more sophisticated techniques, and [prompt
    engineering](https://en.wikipedia.org/wiki/Prompt_engineering) is still an open
    field. The principle of these techniques is to allow the model to reason about
    a problem or to make the best use of what it has learned during training.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，尽管其中一些技术很简单，但它们并不总是能产生良好的结果。还有更复杂的技术，[提示工程](https://en.wikipedia.org/wiki/Prompt_engineering)仍然是一个开放的领域。这些技术的原则是让模型对问题进行推理或充分利用其在训练中学到的内容。
- en: '**In any case, all of these techniques must be exploited with one problem:
    the maximum number of** [**tokens**](https://en.wikipedia.org/wiki/Lexical_analysis#Token)
    **(subwords) that can be inserted inside a prompt.**'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**无论如何，所有这些技术都必须面临一个问题：提示中可以插入的** [**tokens**](https://en.wikipedia.org/wiki/Lexical_analysis#Token)
    **（子词）的最大数量。**'
- en: How long can a prompt be and why?
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 提示可以有多长，为什么？
- en: 'How long can be a prompt: The length of the context'
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示的长度可以有多长：上下文的长度
- en: '![](../Images/03ee83f634efb95292c8c74c00f4f5c7.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03ee83f634efb95292c8c74c00f4f5c7.png)'
- en: Photo by [patricia serna](https://unsplash.com/ja/@sernarial?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[patricia serna](https://unsplash.com/ja/@sernarial?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) 的照片'
- en: The prompt can grow quickly, especially if the context contains a lot of information
    (use articles in the context, past conversations, add external information, and
    so on). This means that the model must operate on long sequences as input.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 提示可以迅速增长，尤其是当上下文包含大量信息（使用上下文中的文章、过去的对话、添加外部信息等）。这意味着模型必须处理长序列作为输入。
- en: Basically, an [LLM](https://en.wikipedia.org/wiki/Large_language_model) is a
    [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)),
    and transformers do not scale well with sequence length. This comes from the fact
    that the [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    is built on repeated blocks of [self-attention](https://en.wikipedia.org/wiki/Attention_(machine_learning))
    that have [a quadratic cost](https://en.wikipedia.org/wiki/Computational_complexity)
    with respect to length.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，[LLM](https://en.wikipedia.org/wiki/Large_language_model) 是一种 [变换器](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))，而变换器在序列长度上扩展不良。这是因为
    [变换器](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) 建立在重复的
    [自注意力](https://en.wikipedia.org/wiki/Attention_(machine_learning)) 模块上，这些模块在长度方面具有
    [二次成本](https://en.wikipedia.org/wiki/Computational_complexity)。
- en: Of course, there has been much previous work that has tried to reduce this cost
    with various strategies. Linear alternatives to self-attention have been found
    to be unexpressive, though.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，以前已经有很多工作尝试通过各种策略减少这个成本。然而，线性替代的自注意力被发现表现不佳。
- en: '[](https://levelup.gitconnected.com/welcome-back-80s-transformers-could-be-blown-away-by-convolution-21ff15f6d1cc?source=post_page-----331e3af86d27--------------------------------)
    [## Welcome Back 80s: Transformers Could Be Blown Away by Convolution'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://levelup.gitconnected.com/welcome-back-80s-transformers-could-be-blown-away-by-convolution-21ff15f6d1cc?source=post_page-----331e3af86d27--------------------------------)
    [## 欢迎回到80年代：变换器可能会被卷积所取代'
- en: The Hyena model shows how convolution could be faster than self-attention
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Hyena 模型展示了卷积如何比自注意力更快
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/welcome-back-80s-transformers-could-be-blown-away-by-convolution-21ff15f6d1cc?source=post_page-----331e3af86d27--------------------------------)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: levelup.gitconnected.com](https://levelup.gitconnected.com/welcome-back-80s-transformers-could-be-blown-away-by-convolution-21ff15f6d1cc?source=post_page-----331e3af86d27--------------------------------)
- en: Autoregressive transformers are spectacular models for short sequences but scale
    poorly to long sequences such as high-resolution images, podcasts, code, or books.
    ([source](https://arxiv.org/abs/2305.07185))
  id: totrans-43
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 自回归变换器在短序列上表现出色，但在处理长序列如高分辨率图像、播客、代码或书籍时表现不佳。 ([source](https://arxiv.org/abs/2305.07185))
- en: Usually, the [context windows](https://datascience.stackexchange.com/questions/16424/what-is-context-window-size)
    are relatively small (512–1024 [tokens](https://en.wikipedia.org/wiki/Lexical_analysis#Token)).
    Yet, in recent months we have seen that there are now models even with thousands
    and thousands of tokens for the [context window](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，[上下文窗口](https://datascience.stackexchange.com/questions/16424/what-is-context-window-size)
    相对较小（512–1024 [tokens](https://en.wikipedia.org/wiki/Lexical_analysis#Token)）。然而，近年来我们已经看到有些模型甚至有成千上万个令牌用于
    [上下文窗口](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/)。
- en: For example, [GPT-4](https://arxiv.org/abs/2303.08774) has a context length
    of 32k. Aside from, the fact that the number is impressive, it is not just a marketing
    question. In fact, the longer the [context length](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/)
    the more information a model can relate. Also, greater [context length](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/)
    allows for greater accuracy, and fluency, and is thought to stimulate the creativity
    of the model.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，[GPT-4](https://arxiv.org/abs/2303.08774) 的上下文长度为32k。除了这个数字令人印象深刻之外，这不仅仅是一个营销问题。实际上，上下文长度越长，模型能够关联的信息就越多。此外，更长的
    [上下文长度](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/)
    可以提高准确性、流畅性，并被认为能激发模型的创造力。
- en: At the theoretical level it is true a [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    trained with [context length](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/)
    of 1k tokens could also generate in inference a sequence of 100 k. But since it
    is trained with a different [training data distribution](https://stats.stackexchange.com/questions/173968/difference-between-training-and-test-data-distribution)
    (much less than 100 k sequences) the result generated will be nonsensical.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 从理论上讲，一个使用[上下文长度](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/)为1k
    tokens训练的[变换器](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))确实可以在推断时生成100k的序列。但由于它是在不同的[训练数据分布](https://stats.stackexchange.com/questions/173968/difference-between-training-and-test-data-distribution)下训练的（远少于100k的序列），因此生成的结果将毫无意义。
- en: In fact, it has been shown that asking for a spell check at [ChatGPT](https://medium.com/data-driven-fiction/everything-but-everything-you-need-to-know-about-chatgpt-546af7153ee2)
    for texts longer than 1000 words leads the model to [hallucinate](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，已经证明，在[ChatGPT](https://medium.com/data-driven-fiction/everything-but-everything-you-need-to-know-about-chatgpt-546af7153ee2)中请求对超过1000个单词的文本进行拼写检查会导致模型[幻觉](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence))。
- en: The quadratic cost of [self-attention](https://en.wikipedia.org/wiki/Attention_(machine_learning))
    means that increasing the context length equals a corresponding increase in the
    cost of training. The cost of [LLaMA has been estimated at $3 million](https://matt-rickard.com/commoditization-of-large-language-models-part-3)
    (just as GPU training) increasing the context length by 50 times (from 2K to 100K)
    also means increasing the cost by 50 times.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[自注意力](https://en.wikipedia.org/wiki/Attention_(machine_learning))的二次成本意味着增加上下文长度等于训练成本的相应增加。[LLaMA的成本估计为300万美元](https://matt-rickard.com/commoditization-of-large-language-models-part-3)（仅为GPU训练），将上下文长度增加50倍（从2K到100K）也意味着成本增加50倍。'
- en: '![](../Images/de3d8bb2b2120ae4c800291a69bd6ef6.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de3d8bb2b2120ae4c800291a69bd6ef6.png)'
- en: 'image source: [here](https://arxiv.org/abs/2307.02486)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '图片来源: [这里](https://arxiv.org/abs/2307.02486)'
- en: Is self-attention the only limit to enlarging context length?
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 自注意力是否是扩展上下文长度的唯一限制？
- en: No. After [tokenization](https://huggingface.co/learn/nlp-course/chapter6/8?fw=pt),
    the model takes the sequence of tokens and the first step is [embedding](https://en.wikipedia.org/wiki/Embedding).
    For a sequence of tokens n we have an [embedding](https://en.wikipedia.org/wiki/Embedding)
    size of d. Obviously, if n >> d there is a risk of information loss, and increasing
    n much more than d poses significant challenges.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 不是的。经过[tokenization](https://huggingface.co/learn/nlp-course/chapter6/8?fw=pt)后，模型接受tokens序列，第一步是[嵌入](https://en.wikipedia.org/wiki/Embedding)。对于一个长度为n的tokens序列，我们有一个大小为d的[嵌入](https://en.wikipedia.org/wiki/Embedding)。显然，如果n
    >> d，会有信息丢失的风险，而将n增加得远超过d则会带来显著的挑战。
- en: Also, the sinusoidal [positional encoder](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/)
    is not compatible with some solutions to be able to enlarge the [context length](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/)
    and needs to be rethought.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，正弦[位置编码器](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/)与某些扩展[上下文长度](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/)的解决方案不兼容，需要重新考虑。
- en: In addition, the training is [parallelized](https://pytorch.org/tutorials/advanced/ddp_pipeline.html),
    but during inference instead the computation is sequential. In fact, a token depends
    on the tokens generated by the sequence. So inference must also be optimized to
    enlarge the [context length](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，训练是[并行化](https://pytorch.org/tutorials/advanced/ddp_pipeline.html)的，但推断时计算是顺序进行的。事实上，一个token依赖于序列中生成的tokens。因此，推断也必须优化以扩展[上下文长度](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/)。
- en: How is it possible to have a context window of tens of thousands or even hundreds
    of thousands of tokens?
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如何实现数万甚至数十万tokens的上下文窗口？
- en: Extending the context window of LLM
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展LLM的上下文窗口
- en: '![](../Images/243eb4617b6424fb6069a9e7a2a5d1d5.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/243eb4617b6424fb6069a9e7a2a5d1d5.png)'
- en: Photo by [Nagara Oyodo](https://unsplash.com/@nagaranbasaran?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Nagara Oyodo](https://unsplash.com/@nagaranbasaran?utm_source=medium&utm_medium=referral)拍摄，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Make your context XL
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让你的上下文更大
- en: Although the results of the past few months are impressive, there had already
    been attempts to increase the length of the [context window](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/)
    in 2019\. In fact, [the Transformer-XL](https://arxiv.org/abs/1901.02860) could
    generate coherent text up to thousands of tokens.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管过去几个月的结果令人印象深刻，但早在2019年就已经有尝试增加[上下文窗口](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/)的长度。事实上，[Transformer-XL](https://arxiv.org/abs/1901.02860)能够生成连贯的文本，长度可达数千个标记。
- en: The authors exploited the idea of [recurrent neural networks](https://en.wikipedia.org/wiki/Recurrent_neural_network),
    in which the hidden state is reused to allow more information to be given to the
    [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)).
    In other words, after passing a sequence segment, while processing the next segment
    the previously obtained hidden state was reused. Looking at, the description of
    the model, the similarity to [RNNs](https://en.wikipedia.org/wiki/Recurrent_neural_network)
    is evident.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 作者利用了[递归神经网络](https://en.wikipedia.org/wiki/Recurrent_neural_network)的理念，其中隐藏状态被重复使用，以便将更多信息提供给[变换器](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))。换句话说，在处理一个序列段后，在处理下一个段时，会重复使用之前获得的隐藏状态。从模型描述来看，与[RNNs](https://en.wikipedia.org/wiki/Recurrent_neural_network)的相似性是显而易见的。
- en: '![](../Images/14246a46a248e6b0e65cbb5bfeb7ab15.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14246a46a248e6b0e65cbb5bfeb7ab15.png)'
- en: 'image source: [here](https://arxiv.org/abs/1901.02860)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/1901.02860)
- en: A short training, a long inference
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 短期训练，长期推断
- en: Although TransformerXL was an interesting solution, other strategies are being
    tested in recent months. These strategies aimed to solve the limitations inherent
    in the original [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    and also took advantage of today’s hardware advancements.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管TransformerXL是一个有趣的解决方案，但最近几个月也在测试其他策略。这些策略旨在解决原始[变换器](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))固有的局限性，并利用了今天的硬件进步。
- en: One idea to reduce the cost of training is to train the model with a [context
    length](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/)
    of 2K but then conduct [fine-tuning](https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning))
    on longer sequences (e.g., 65K). Theoretically, this could also work (the model
    learns a general representation of the language in the first training and then
    specializes in the task of longer sequences later).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 减少训练成本的一个想法是用[上下文长度](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/)为2K的模型进行训练，然后在更长的序列（例如，65K）上进行[微调](https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning))。理论上，这也可以奏效（模型在第一次训练中学习语言的一般表示，然后在后续任务中专门化处理更长的序列）。
- en: In reality, this strategy with the original transformer is doomed to fail as
    [explained in a 2021 paper](https://arxiv.org/abs/2108.12409). As explained by
    the authors the ability of a larger [context length](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/)
    in inference is called “extrapolation.”
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，使用原始变换器的这一策略注定会失败，[2021年的一篇论文](https://arxiv.org/abs/2108.12409)中已对此进行了说明。正如作者所解释的那样，推断中更长的[上下文长度](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/)的能力称为“外推”。
- en: We define extrapolation as a model’s ability to continue performing well as
    the number of input tokens during validation increases beyond the number of tokens
    on which the the model was trained. We find that transformer language models (LMs)
    that use sinusoidal position embeddings have very weak extrapolation abilities.
    ([source](https://arxiv.org/abs/2108.12409))
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们将外推定义为模型在验证过程中，输入标记数量超过模型训练时标记数量的情况下，继续表现良好的能力。我们发现使用正弦位置嵌入的变换器语言模型（LMs）具有非常弱的外推能力。（[来源](https://arxiv.org/abs/2108.12409)）
- en: '![](../Images/9d6d2a9cec906f9787d33ca53ac88e69.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d6d2a9cec906f9787d33ca53ac88e69.png)'
- en: 'image source: [here](https://arxiv.org/abs/2108.12409)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2108.12409)
- en: So for the authors, positional encoding is the culprit for the lack of the original
    transformer’s ability to extrapolate. [Positional encoding](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/),
    which is a step at the beginning of the model, was included as a clever trick
    to allow the model to account for the position of each token in the sequence.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于作者来说，位置编码是缺乏原始变换器外推能力的罪魁祸首。[位置编码](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)，作为模型开始时的一步，被包含在内，作为一个巧妙的技巧，使模型能够考虑序列中每个标记的位置。
- en: 'The authors, suggest replacing it with attention with linear bias (ALiBI).
    In simple words, a penalty is added to the product of queries and keys in the
    [attention](https://en.wikipedia.org/wiki/Attention_(machine_learning)) and this
    penalty is proportional to their distance:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 作者建议用带有线性偏置（ALiBI）的注意力替代它。简单来说，就是在[注意力](https://en.wikipedia.org/wiki/Attention_(machine_learning))的查询和键的乘积中添加一个惩罚，这个惩罚与它们的距离成正比：
- en: '![](../Images/1edaf14f7f120c02e342c235c7a65354.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1edaf14f7f120c02e342c235c7a65354.png)'
- en: 'image source: [here](https://arxiv.org/abs/2108.12409)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2108.12409)
- en: '![](../Images/83e66d77cab42e6e12fbf265d1cc2470.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/83e66d77cab42e6e12fbf265d1cc2470.png)'
- en: '“When computing attention scores for each head, our linearly biased attention
    method, ALiBi, adds a constant bias (right) to each attention score (qi · kj ,
    left). As in the unmodified attention sublayer, the softmax function is then applied
    to these scores, and the rest of the computation is unmodified. m is a head-specific
    scalar that is set and not learned throughout training.” image source: [here](https://arxiv.org/abs/2108.12409)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: “在计算每个头的注意力得分时，我们的线性偏置注意力方法ALiBi会对每个注意力得分（qi · kj，左）添加一个常数偏置（右）。与未修改的注意力子层一样，然后将softmax函数应用于这些得分，其余计算保持不变。m是一个特定于头的标量，在训练过程中设置且不进行学习。”
    图片来源：[这里](https://arxiv.org/abs/2108.12409)
- en: The trick is ingenious because it does not add parameters to learn and does
    not increase the [computational cost](https://en.wikipedia.org/wiki/Computational_complexity)
    by much.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这个技巧很巧妙，因为它不会增加学习的参数，也不会显著增加[计算成本](https://en.wikipedia.org/wiki/Computational_complexity)。
- en: Do you need all these tokens?
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你需要所有这些标记吗？
- en: Extending the [context window](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/)
    to more than 100k tokens certainly has great interest. On the other hand, not
    all tokens in the sequence are actually interesting. **So, is it necessary for
    us to calculate the relationships (attention score) among all these tokens?**
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展[上下文窗口](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/)到超过100k个标记无疑非常有吸引力。另一方面，并非序列中的所有标记实际上都很有趣。**那么，我们是否有必要计算这些标记之间的关系（注意力得分）呢？**
- en: 'So the idea is to leverage the sparsity when calculating the [attention score](https://en.wikipedia.org/wiki/Attention_(machine_learning))
    so that we do not calculate the relationships between the tokens in which we are
    not interested. [As Google explained](https://ai.googleblog.com/2021/03/constructing-transformers-for-longer.html),
    though, this is not exactly straightforward:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这个想法是利用稀疏性来计算[注意力得分](https://en.wikipedia.org/wiki/Attention_(machine_learning))，这样我们就不会计算那些我们不感兴趣的标记之间的关系。[正如谷歌所解释的](https://ai.googleblog.com/2021/03/constructing-transformers-for-longer.html)，尽管如此，这并不是完全简单的：
- en: 'Two natural questions arise: 1) Can we achieve the empirical benefits of quadratic
    full Transformers using sparse models with computational and memory requirements
    that scale linearly with the input sequence length? 2) Is it possible to show
    theoretically that these linear Transformers preserve the expressivity and flexibility
    of the quadratic full Transformers? ([source](https://ai.googleblog.com/2021/03/constructing-transformers-for-longer.html))'
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 两个自然的问题出现了：1) 我们是否可以使用计算和内存需求与输入序列长度线性相关的稀疏模型来实现二次全变换器的经验性收益？2) 是否可以理论上证明这些线性变换器保留了二次全变换器的表达能力和灵活性？([source](https://ai.googleblog.com/2021/03/constructing-transformers-for-longer.html))
- en: Google tried to answer these questions by starting an intriguing observation,
    the attention layer can be understood as a [graph](https://en.wikipedia.org/wiki/Graph).
    In fact, when computing the [attention](https://en.wikipedia.org/wiki/Attention_(machine_learning))
    for all positions (nodes) in a sequence we compute pairwise similarities (edges).
    So visually we can think of attention as a [directed graph](https://en.wikipedia.org/wiki/Directed_graph).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌尝试通过一个有趣的观察来回答这些问题，即注意力层可以被理解为一个[图](https://en.wikipedia.org/wiki/Graph)。事实上，当计算序列中所有位置（节点）的[注意力](https://en.wikipedia.org/wiki/Attention_(machine_learning))时，我们计算的是成对的相似度（边）。因此，从视觉上看，我们可以将注意力视为一个[有向图](https://en.wikipedia.org/wiki/Directed_graph)。
- en: '![](../Images/79a381fb20e6c65345ccbde995b38f26.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/79a381fb20e6c65345ccbde995b38f26.png)'
- en: attention as a complete graph, image by the author.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力作为一个完全图，图像由作者提供。
- en: Building on this concept, it can be seen that several alternatives to classical
    [attention](https://en.wikipedia.org/wiki/Attention_(machine_learning)) can be
    considered with graphs that are not [fully connected](https://en.wikipedia.org/wiki/Complete_graph).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这一概念，可以看到，相对于经典的[注意力](https://en.wikipedia.org/wiki/Attention_(machine_learning))，可以考虑使用图形，这些图形不是[完全连接](https://en.wikipedia.org/wiki/Complete_graph)的。
- en: Google, took advantage of this concept by first exploiting the combination of
    [global](https://machinelearningmastery.com/global-attention-for-encoder-decoder-recurrent-neural-networks/)
    and local (or [window](https://paperswithcode.com/method/sliding-window-attention))
    attention [in this paper](https://arxiv.org/abs/2004.08483), and then improving
    on the idea with [BigBird](https://arxiv.org/abs/2007.14062).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌通过首先利用[全球](https://machinelearningmastery.com/global-attention-for-encoder-decoder-recurrent-neural-networks/)和局部（或[窗口](https://paperswithcode.com/method/sliding-window-attention)）注意力的组合来利用这一概念，[在这篇论文](https://arxiv.org/abs/2004.08483)中进行，然后通过[BigBird](https://arxiv.org/abs/2007.14062)对这一想法进行了改进。
- en: '![](../Images/9408bf6a5512167fc16c22c979ba18a5.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9408bf6a5512167fc16c22c979ba18a5.png)'
- en: 'image source: [here](https://arxiv.org/abs/2007.14062)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2007.14062)
- en: 'BigBird basically combines three concepts: global tokens to cover the whole
    sequence, local attention to cover the surrounding of each token, and for each
    token, there are tokens sampled randomly.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: BigBird基本上结合了三个概念：全球标记以覆盖整个序列，局部注意力以覆盖每个标记的周围区域，并且每个标记都有随机抽样的标记。
- en: BigBird succeeds in sufficiently approximating the [attention](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)).
    At the same time, it is sparse (so less [computation](https://mathoverflow.net/questions/400840/what-is-the-computational-cost-in-a-neural-network))
    pero it does not interrupt the flow of information (the ability of one token to
    influence other tokens).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: BigBird成功地近似了[注意力](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))。同时，它是稀疏的（因此计算[复杂度](https://mathoverflow.net/questions/400840/what-is-the-computational-cost-in-a-neural-network)较低），但不会中断信息流（一个标记影响其他标记的能力）。
- en: The authors prove that this sparse attention is not only as expressive as the
    [original attention](https://en.wikipedia.org/wiki/Attention_(machine_learning))
    but can be used to extract contextual information from sequences that are long
    by nature such as [genomic sequences](https://www.institute.global/insights/public-services/what-genomic-sequencing-and-why-does-it-matter-future-health).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们证明了这种稀疏注意力不仅与[原始注意力](https://en.wikipedia.org/wiki/Attention_(machine_learning))具有同样的表现力，而且可以用于从本质上较长的序列中提取上下文信息，例如[基因组序列](https://www.institute.global/insights/public-services/what-genomic-sequencing-and-why-does-it-matter-future-health)。
- en: In any case, [the sparsity concept](https://blogs.nvidia.com/blog/2020/05/14/sparsity-ai-inference/)
    is so powerful that many researchers are of trying to implement it in other models
    such as the Vision Transformers.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何情况下，[稀疏性概念](https://blogs.nvidia.com/blog/2020/05/14/sparsity-ai-inference/)都非常强大，许多研究人员正在尝试将其应用于其他模型，例如视觉变换器。
- en: '[](/metas-hiera-reduce-complexity-to-increase-accuracy-30f7a147ad0b?source=post_page-----331e3af86d27--------------------------------)
    [## META’s Hiera: reduce complexity to increase accuracy'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/metas-hiera-reduce-complexity-to-increase-accuracy-30f7a147ad0b?source=post_page-----331e3af86d27--------------------------------)
    [## META的Hiera：减少复杂性以提高准确性'
- en: Simplicity allows AI to reach incredible performance and surprising speed
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 简单性使得人工智能能够达到惊人的性能和惊人的速度
- en: towardsdatascience.com](/metas-hiera-reduce-complexity-to-increase-accuracy-30f7a147ad0b?source=post_page-----331e3af86d27--------------------------------)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/metas-hiera-reduce-complexity-to-increase-accuracy-30f7a147ad0b?source=post_page-----331e3af86d27--------------------------------)
- en: Conditional computation for fast transformer
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 快速变换器的条件计算
- en: Based on the idea that not all tokens are important, there is also another way
    to not apply all model weights to all tokens during training.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 基于并非所有令牌都重要的想法，还有另一种方法可以在训练过程中不将所有模型权重应用于所有令牌。
- en: '[CoLT5](https://arxiv.org/abs/2303.09752) exploits this concept to increase
    the length of the input. Conditional computation, simply put, makes sure to allocate
    more resources to those tokens that are considered important.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[CoLT5](https://arxiv.org/abs/2303.09752)利用这一概念来增加输入长度。简单来说，条件计算确保将更多资源分配给那些被认为重要的令牌。'
- en: The authors construct a system, in which attention and [feedforward network](https://en.wikipedia.org/wiki/Feedforward_neural_network)
    computation are separated into two branches (heavy and light). The light layer
    is applied for all tokens, while the heavy MLP is applied only for the important
    ones. These tokens are selected by a routing module that decides which tokens
    are important.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 作者构建了一个系统，其中注意力和[前馈网络](https://en.wikipedia.org/wiki/Feedforward_neural_network)计算被分成两个分支（重型和轻型）。轻型层应用于所有令牌，而重型MLP仅应用于重要令牌。这些令牌由一个路由模块选择，决定哪些令牌是重要的。
- en: '![](../Images/f9e6dba639d1f7aea4ba1c669e3f9d3b.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f9e6dba639d1f7aea4ba1c669e3f9d3b.png)'
- en: 'image source: [here](https://arxiv.org/abs/2303.09752)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '图片来源: [这里](https://arxiv.org/abs/2303.09752)'
- en: Multi-Query Attention to save computation
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多查询注意力以节省计算
- en: The key and values for each token are cached for each token during [inference](https://cloud.google.com/bigquery/docs/inference-overview)
    (so as to avoid redoing the same operations while generating text). On the one
    hand, this saves computations, but it increases memory usage in the [GPU](https://en.wikipedia.org/wiki/Graphics_processing_unit).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在[推理](https://cloud.google.com/bigquery/docs/inference-overview)过程中，每个令牌的键和值都会被缓存（以避免在生成文本时重复相同的操作）。这在节省计算的同时增加了[GPU](https://en.wikipedia.org/wiki/Graphics_processing_unit)的内存使用。
- en: In order to avoid this, [Multi-Query Attention](https://arxiv.org/abs/1911.02150)
    (MQA) suggests sharing weights for all attention heads during the linear projection
    step of keys and values.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这种情况，[多查询注意力](https://arxiv.org/abs/1911.02150)（MQA）建议在键和值的线性投影步骤中对所有注意力头共享权重。
- en: This has an advantage particularly when dealing with long sequences, decreasing
    not only memory usage but generation time. Google has demonstrated the advantage
    of MQA [while using PaLM](https://arxiv.org/abs/2211.05102).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这在处理长序列时尤其有优势，减少了不仅仅是内存使用，还减少了生成时间。谷歌已展示了MQA的优势，[在使用PaLM时](https://arxiv.org/abs/2211.05102)。
- en: '![](../Images/1a4799dea135a88be889b944cfaad03f.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a4799dea135a88be889b944cfaad03f.png)'
- en: 'image source: [here](https://arxiv.org/abs/2211.05102)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '图片来源: [这里](https://arxiv.org/abs/2211.05102)'
- en: Flash attention, the light in the new LLMs
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 闪光注意力，新一代LLM的亮点
- en: The models and ideas seen earlier seek to modify attention in ways that reduce
    its cost. [Flash attention](https://arxiv.org/abs/2205.14135) uses a different
    approach and is used by virtually all models today.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 之前看到的模型和思想旨在以减少其成本的方式修改注意力。[闪光注意力](https://arxiv.org/abs/2205.14135)使用了不同的方法，今天几乎所有模型都在使用它。
- en: At the base, there is a better use of the [GPU](https://en.wikipedia.org/wiki/Graphics_processing_unit).
    In fact, the [GPU](https://en.wikipedia.org/wiki/Graphics_processing_unit) has
    its own memory hierarchy. When the [GPU](https://en.wikipedia.org/wiki/Graphics_processing_unit)
    performs an operation, data must be present on the fast memory (SRAM memory).
    The data is copied to this memory from the HBM memory, and once the calculations
    are finished, the output is copied to the HBM.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 从基础上讲，[GPU](https://en.wikipedia.org/wiki/Graphics_processing_unit)的利用更为高效。实际上，[GPU](https://en.wikipedia.org/wiki/Graphics_processing_unit)有自己独特的内存层次结构。当[GPU](https://en.wikipedia.org/wiki/Graphics_processing_unit)执行操作时，数据必须存在于快速内存（SRAM内存）中。数据从HBM内存复制到这块内存中，一旦计算完成，输出结果会被复制回HBM。
- en: As you can see, SRAM memory is not only much faster but also much smaller. Over
    time, computation has become faster and faster and HBM has become the bottleneck.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，SRAM内存不仅速度更快，而且体积更小。随着时间的推移，计算变得越来越快，而HBM已经成为瓶颈。
- en: '![](../Images/2de74aeb5d2edf2174d45fd357ee684a.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2de74aeb5d2edf2174d45fd357ee684a.png)'
- en: 'image source: [here](https://arxiv.org/abs/2205.14135)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '图片来源: [这里](https://arxiv.org/abs/2205.14135)'
- en: This is because, during [attention](https://en.wikipedia.org/wiki/Attention_(machine_learning)),
    several operations are conducted (multiplication of queries and keys, [softmax](https://en.wikipedia.org/wiki/Softmax_function),
    multiplication of this result with values). These operations generate intermediates
    that are copied to the HBM and SRAM (back and forth). This data copying operation
    is what slows down the operation.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为在[注意力](https://en.wikipedia.org/wiki/Attention_(machine_learning))过程中，进行了一些操作（查询和键的乘法，[softmax](https://en.wikipedia.org/wiki/Softmax_function)，将这个结果与值进行乘法）。这些操作生成的中间数据被复制到HBM和SRAM（来回传输）。这种数据复制操作是导致操作变慢的原因。
- en: The SRAM has a memory limit, so the [flash attention](https://arxiv.org/abs/2205.14135)
    solution is to divide the various arrays (queries, keys, values) into blocks.
    So all operations are accomplished in a single GPU kernel and only then write
    the result to HBM. In addition, the softmax is also reduced as time, since it
    is calculated only on the block and not on the whole NxN matrix.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: SRAM有内存限制，因此[闪存注意力](https://arxiv.org/abs/2205.14135)解决方案是将各种数组（查询、键、值）划分为块。因此，所有操作都在一个GPU内核中完成，然后将结果写入HBM。此外，softmax也随时间减少，因为它仅在块上计算，而不是整个NxN矩阵上。
- en: '![](../Images/8b759fc1b336c43cb2a09ba8d43c680b.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8b759fc1b336c43cb2a09ba8d43c680b.png)'
- en: 'image source: [here](https://arxiv.org/abs/2205.14135)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来源：[这里](https://arxiv.org/abs/2205.14135)
- en: Not only [META’s LLaMA](https://arxiv.org/abs/2302.13971) uses FlashAttention,
    but today v[irtually all models do](https://github.com/HazyResearch/flash-attention/blob/main/usage.md).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅[META的LLaMA](https://arxiv.org/abs/2302.13971)使用FlashAttention，今天[virtually所有模型](https://github.com/HazyResearch/flash-attention/blob/main/usage.md)都在使用它。
- en: '[](https://medium.com/mlearning-ai/metas-llama-a-small-language-model-beating-giants-5065948e0b7f?source=post_page-----331e3af86d27--------------------------------)
    [## META’s LLaMA: A small language model beating giants'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/mlearning-ai/metas-llama-a-small-language-model-beating-giants-5065948e0b7f?source=post_page-----331e3af86d27--------------------------------)
    [## META的LLaMA：一个小型语言模型击败巨头'
- en: META open-source model will help us to understand how LMs biases arise
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: META开源模型将帮助我们理解语言模型偏见的产生
- en: medium.com](https://medium.com/mlearning-ai/metas-llama-a-small-language-model-beating-giants-5065948e0b7f?source=post_page-----331e3af86d27--------------------------------)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/mlearning-ai/metas-llama-a-small-language-model-beating-giants-5065948e0b7f?source=post_page-----331e3af86d27--------------------------------)
- en: Recent development
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最新发展
- en: Recent developments in [GPUs](https://en.wikipedia.org/wiki/Graphics_processing_unit)
    have also enabled an increase in increasing tokens and their context. There are
    now 80 GB GPUs for example.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在[GPU](https://en.wikipedia.org/wiki/Graphics_processing_unit)方面的进展也使得令牌和它们的上下文增加成为可能。例如，现在有80
    GB的GPU。
- en: Also, in addition to the technical refinements we have seen above, there is
    also a better understanding of why the [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    does not have the ability to extrapolate. For example, [in this paper](https://arxiv.org/abs/2306.00946),
    they show that the classical attention drifts away to a later position in the
    sequence (behavior that the authors input to sinusoidal [position encoding](https://d2l.ai/chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html)).
    For this reason, we have seen how positional encoding has been replaced in ALiBI
    (others have proposed that it can be replaced with a randomized version, random
    positional encoding).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，除了我们上面看到的技术改进，还有对[变换器](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))为何不具备外推能力的更好理解。例如，[在这篇论文中](https://arxiv.org/abs/2306.00946)，他们展示了经典注意力如何漂移到序列的后面位置（这一行为作者输入到正弦[位置编码](https://d2l.ai/chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html)中）。因此，我们看到位置编码如何在ALiBI中被替代（其他人提出可以用随机版本、随机位置编码来替代）。
- en: '[Other authors point out](https://arxiv.org/abs/2208.11445) how techniques
    such as [chain-of-thought](https://arxiv.org/abs/2201.11903) help the model extrapolate
    since it must focus on the intermediates of reasoning. Also, [a few shots can
    improve the model’s ability to extrapolate](https://arxiv.org/abs/2207.04901)
    better than fine-tuning (in any case, it’s not a silver bullet). Actually, [fine-tuning
    with some tricks can bring very good results](https://arxiv.org/abs/2305.16300)
    as in the case of LLaMA 7B where in this study they introduced window attention
    they managed to increase the context window from 2K to 32K.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[其他作者指出](https://arxiv.org/abs/2208.11445)，如[链式思维](https://arxiv.org/abs/2201.11903)等技术有助于模型进行推断，因为模型必须专注于推理的中间步骤。此外，[少量示例可以比微调更好地提高模型的推断能力](https://arxiv.org/abs/2207.04901)（无论如何，这不是灵丹妙药）。实际上，[使用一些技巧进行微调可以带来非常好的结果](https://arxiv.org/abs/2305.16300)，例如LLaMA
    7B在这项研究中引入了窗口注意机制，将上下文窗口从2K扩展到32K。'
- en: However if previously [Claude](https://www.anthropic.com/index/introducing-claude)’s
    100K context length seemed unbelievable. [META’s Megabyte claims 1M tokens](https://arxiv.org/abs/2305.07185)
    (the trick would be, “Megabyte segments sequences into patches and uses a local
    submodel within patches and a global model between patches”).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果以前[Claude](https://www.anthropic.com/index/introducing-claude)的100K上下文长度看起来令人难以置信。[META的Megabyte声称支持1M
    tokens](https://arxiv.org/abs/2305.07185)（其技巧是，“Megabyte将序列分割成补丁，并在补丁内使用局部子模型，在补丁之间使用全局模型”）。
- en: '![](../Images/fd5a0545b9d23f7618934d82fc2dd2b9.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd5a0545b9d23f7618934d82fc2dd2b9.png)'
- en: 'image source: [here](https://arxiv.org/abs/2305.07185)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2305.07185)
- en: A paper published a short while ago even claims a 1G token. All of this shows
    how there is still a lot of active research, and how many groups are working on
    finding ways to extend the context length.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 一篇最近发表的论文甚至声称可以处理1G的token。这些都表明，仍有大量的活跃研究，以及许多团队正在寻找扩展上下文长度的方法。
- en: 'Noting all this research and alternative solutions, a question arises: how
    does the model use this long context? Can it make the best use of it?'
  id: totrans-130
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 考虑到所有这些研究和替代方案，一个问题浮现：模型如何使用这些长上下文？它能否充分利用这些上下文？
- en: 'Lost in the Middle: How Language Models Use Long Contexts'
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在中间迷失：语言模型如何利用长上下文
- en: '![](../Images/3ca927e90379d27e98d5232e464cd5ca.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3ca927e90379d27e98d5232e464cd5ca.png)'
- en: Photo by [Ethan Sykes](https://unsplash.com/@e_sykes?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Ethan Sykes](https://unsplash.com/@e_sykes?utm_source=medium&utm_medium=referral)提供，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: The latest advances in [LLM](https://en.wikipedia.org/wiki/Large_language_model)
    have allowed the [context window](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/)
    to be extended, this opens up the question of whether indeed the models have benefited.
    [An article published this month investigated this issue](https://arxiv.org/abs/2307.03172).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 最新的[大型语言模型（LLM）](https://en.wikipedia.org/wiki/Large_language_model)的进展使得[上下文窗口](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/)得以扩展，这引发了一个问题：模型是否真的从中受益？[本月发表的一篇文章探讨了这一问题](https://arxiv.org/abs/2307.03172)。
- en: '[The authors of this study](https://arxiv.org/abs/2307.03172) were able to
    take advantage of the fact that not only proprietary models such as [Claude](https://www.anthropic.com/index/introducing-claude)
    or [GPT-4](https://openai.com/gpt-4) have a long context window. In fact, [MPT-30B](https://huggingface.co/mosaicml/mpt-30b)
    and [LongChat-7B](https://huggingface.co/lmsys/longchat-13b-16k) have a context
    window of 8K and 16K tokens, respectively. The authors, therefore, decided to
    use both these two models as well as models that are closed ([GPT-3.5](https://en.wikipedia.org/wiki/GPT-3)
    and [Claude](https://www.anthropic.com/index/introducing-claude)).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[这项研究的作者](https://arxiv.org/abs/2307.03172)能够利用这样一个事实，即不仅像[Claude](https://www.anthropic.com/index/introducing-claude)或[GPT-4](https://openai.com/gpt-4)这样的专有模型具有长上下文窗口。实际上，[MPT-30B](https://huggingface.co/mosaicml/mpt-30b)和[LongChat-7B](https://huggingface.co/lmsys/longchat-13b-16k)的上下文窗口分别为8K和16K
    tokens。因此，作者决定使用这两种模型以及一些封闭模型（[GPT-3.5](https://en.wikipedia.org/wiki/GPT-3)和[Claude](https://www.anthropic.com/index/introducing-claude)）。'
- en: Having chosen the models, one must also choose tasks where it is necessary for
    the model to have a long context window. For example, in Multi-Document [Question
    Answering](https://en.wikipedia.org/wiki/Question_answering) requires the model
    to reason about a set of different documents to find the information that is needed
    for the answers. This is an important task since it mimics the fact of a search
    in a document corpus such as the Internet might for example (we can imagine an
    [AI search engine](https://blog.google/products/search/generative-ai-search/)
    that has to search multiple websites for the answer).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 选择模型后，还必须选择那些需要模型具备长上下文窗口的任务。例如，在多文档[问答](https://en.wikipedia.org/wiki/Question_answering)中，需要模型对一组不同的文档进行推理，以找到回答所需的信息。这是一个重要的任务，因为它模拟了在文档语料库（例如互联网）中进行搜索的事实（我们可以想象一个[AI搜索引擎](https://blog.google/products/search/generative-ai-search/)需要搜索多个网站以找到答案）。
- en: 'For a question x, there is a set of documents and only one of these documents
    contains the information to answer the question. As in the example:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个问题x，有一组文档，其中只有一个文档包含回答问题所需的信息。如示例所示：
- en: '![](../Images/b6b84ae0a561ee03402d01a822f907ce.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b6b84ae0a561ee03402d01a822f907ce.png)'
- en: 'image source: [here](https://arxiv.org/abs/2307.03172)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2307.03172)
- en: '[The authors exploited datasets of annotated questions](https://arxiv.org/abs/2307.03172)
    (google searches). They then added Wikipedia chunks that were related to the topic
    but did not contain the answer (being careful that the right document was not
    always in the same location because the [LLM](https://en.wikipedia.org/wiki/Large_language_model)
    could learn to do the heuristic trick).'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[作者利用了注释问题的数据集](https://arxiv.org/abs/2307.03172)（谷歌搜索）。他们随后添加了与主题相关但不包含答案的维基百科片段（小心确保正确的文档不总是位于相同的位置，因为[LLM](https://en.wikipedia.org/wiki/Large_language_model)可能学会了启发式技巧）。'
- en: '![](../Images/53277701e173f55af339329ad9dd5d0f.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/53277701e173f55af339329ad9dd5d0f.png)'
- en: 'image source: [here](https://arxiv.org/abs/2307.03172)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2307.03172)
- en: 'The [authors note](https://arxiv.org/abs/2307.03172) three particularly interesting
    results:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[作者指出](https://arxiv.org/abs/2307.03172)三个特别有趣的结果：'
- en: they note a U-shape response, dependent on the location of the relevant document.
    In other words, model performance degrades when the model must access information
    that is present in the center of the context window. Thus, models are much better
    at identifying relevant information if they are at the beginning or at the end
    of the input sequence
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们注意到U形响应，取决于相关文档的位置。换句话说，当模型必须访问位于上下文窗口中心的信息时，模型表现会下降。因此，模型在识别相关信息时，如果信息位于输入序列的开头或结尾，表现会更好。
- en: The performance is inferior to the closed-book setting. When the relevant document
    is in the center of the input context, the model performs worse than when no document
    is provided. In a closed-book setting, the model has to rely only on the memory
    of its parameters.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能低于闭卷设置。当相关文档位于输入上下文的中心时，模型的表现比没有提供文档时更差。在闭卷设置中，模型必须仅依赖于其参数的记忆。
- en: In general, performance degrades if more documents are provided to the model.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一般来说，如果向模型提供更多文档，性能会下降。
- en: '![](../Images/6b71fe4a98e79f78bf118ea9b8177c5b.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6b71fe4a98e79f78bf118ea9b8177c5b.png)'
- en: 'image source: [here](https://arxiv.org/abs/2307.03172)'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2307.03172)
- en: '![](../Images/feacf46063eb3a37e3f3054bd1448253.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/feacf46063eb3a37e3f3054bd1448253.png)'
- en: 'image source: [here](https://arxiv.org/abs/2307.03172)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2307.03172)
- en: In addition, [the authors note](https://arxiv.org/abs/2307.03172) that per se
    models with a longer context window are not superior to their counterparts with
    a shorter context sequence.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，[作者指出](https://arxiv.org/abs/2307.03172)，单纯来说，具有较长上下文窗口的模型并不优于其具有较短上下文序列的对应模型。
- en: Since the model struggles in using information that lies in the center of the
    [context window](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/),
    the authors wondered whether the model was at least capable of finding information
    again. In other words, using a simple file consisting of key-value pairs (a [JSON](https://en.wikipedia.org/wiki/JSON))
    can the model find back information?
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型在使用位于[上下文窗口](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/)中心的信息时表现不佳，作者想知道模型是否至少能够重新找到信息。换句话说，使用由键值对组成的简单文件（[JSON](https://en.wikipedia.org/wiki/JSON)），模型是否能够找到信息？
- en: The authors decided to use the simplest possible task to study the model’s behavior
    in depth. This is a basic skill, in which the model has to find a piece of information
    without the need for complex skills. Using simulated data, the authors created
    [JSON](https://en.wikipedia.org/wiki/JSON) containing key-value pairs, in which
    only one is the one of interest.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们决定使用尽可能简单的任务来深入研究模型的行为。这是一项基本技能，其中模型需要找到一条信息，而无需复杂的技能。使用模拟数据，作者们创建了包含键值对的[JSON](https://en.wikipedia.org/wiki/JSON)，其中只有一个是感兴趣的。
- en: Our synthetic key-value retrieval task is designed to provide a minimal testbed
    for the basic ability to retrieve matching tokens from an input context. […] we
    explicitly seek to distill and simplify the task by removing as much natural language
    semantics as possible (using random UUIDs instead), since language features may
    present potential confounders ([source](https://arxiv.org/abs/2307.03172))
  id: totrans-154
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们的合成键值检索任务旨在提供一个最小的测试平台，以测试从输入上下文中检索匹配标记的基本能力。[…]我们明确旨在通过尽可能移除自然语言语义（改用随机UUID）来提炼和简化任务，因为语言特征可能会带来潜在的混淆因素
    ([source](https://arxiv.org/abs/2307.03172))
- en: '![](../Images/4261ebb66c68cc15b845b5e48a882ca4.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4261ebb66c68cc15b845b5e48a882ca4.png)'
- en: 'image source: [here](https://arxiv.org/abs/2307.03172)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '图片来源: [here](https://arxiv.org/abs/2307.03172)'
- en: The result shows that not all models are capable, [Claude](https://www.anthropic.com/index/introducing-claude)
    on the one hand succeeds in the task, but other models have performance degradation
    if there are 140 or more key-value pairs.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，并非所有模型都能胜任，[Claude](https://www.anthropic.com/index/introducing-claude)在这项任务中表现成功，但其他模型在键值对达到140个或更多时性能会下降。
- en: '![](../Images/5cae07592ed886d489a656a662a6bd28.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5cae07592ed886d489a656a662a6bd28.png)'
- en: 'image source: [here](https://arxiv.org/abs/2307.03172)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '图片来源: [here](https://arxiv.org/abs/2307.03172)'
- en: 'In addition, the authors observe a curious fact:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，作者们观察到一个有趣的事实：
- en: LongChat-13B (16K) in the 140 key-value setting is a notable outlier; when the
    relevant information is at the start of the input context, it tends to generate
    code to retrieve the key, rather than outputting the value itself. ([source](https://arxiv.org/abs/2307.03172))
  id: totrans-161
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在140个键值对的设置中，LongChat-13B (16K)是一个显著的异常值；当相关信息位于输入上下文的开始部分时，它倾向于生成代码来检索键，而不是直接输出值。
    ([source](https://arxiv.org/abs/2307.03172))
- en: Why can’t LLMs make the best use of a long context window?
  id: totrans-162
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为什么LLM不能充分利用较长的上下文窗口？
- en: '[The authors of this study wondered](https://arxiv.org/abs/2307.03172) if this
    was related to architecture. Today, two main types of architecture are used: [decoder-only](https://huggingface.co/learn/nlp-course/chapter1/6)
    and e[ncoder-decoder language models](https://huggingface.co/docs/transformers/model_doc/encoder-decoder).
    Although they have been used in a great many articles, there are still obscure
    points about the differences in their behaviors.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[本研究的作者们想知道](https://arxiv.org/abs/2307.03172)这是否与架构有关。目前，主要使用两种架构：[仅解码器](https://huggingface.co/learn/nlp-course/chapter1/6)
    和 [编码器-解码器语言模型](https://huggingface.co/docs/transformers/model_doc/encoder-decoder)。尽管它们在许多文章中被使用，但其行为差异仍存在模糊点。'
- en: 'Therefore, the authors decided to additionally use two other models: Flan-[T5-XXL](https://github.com/google-research/text-to-text-transfer-transformer)
    and [Flan-UL2](https://huggingface.co/google/flan-ul2). These two models show
    to be more robust when the relevant information is found in the middle of the
    [context window](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/).'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，作者们决定另外使用两个模型：Flan-[T5-XXL](https://github.com/google-research/text-to-text-transfer-transformer)
    和 [Flan-UL2](https://huggingface.co/google/flan-ul2)。这两个模型在相关信息位于[上下文窗口](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/)中间时表现出更强的鲁棒性。
- en: '![](../Images/8fc79da4fc513fba306ec84af9d49552.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8fc79da4fc513fba306ec84af9d49552.png)'
- en: 'image source: [here](https://arxiv.org/abs/2307.03172)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '图片来源: [here](https://arxiv.org/abs/2307.03172)'
- en: This is interesting because being bidirectional, an encoder-decoder could be
    more robust in processing information in a longer context window and therefore
    more efficient when it has to process multiple documents.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这很有趣，因为作为双向模型，编码器-解码器可能在处理较长的上下文窗口中的信息时更为鲁棒，因此在处理多个文档时可能更为高效。
- en: Is a long context window useful?
  id: totrans-168
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 长上下文窗口有用吗？
- en: 'If the model is not capable of exploiting it anyway, is there really a benefit
    in having such a long [context window](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/)?
    After all, having a longer context window comes at a cost anyway: the model has
    to process all the input information. In other words, if the model can take 100K
    tokens, it makes sense to feed it 100K pieces of information.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型无论如何都无法充分利用它，那么拥有如此长的[上下文窗口](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/)是否真的有意义？毕竟，拥有更长的上下文窗口无论如何都会有代价：模型必须处理所有输入信息。换句话说，如果模型能处理100K个tokens，那么提供100K个信息点是有意义的。
- en: The authors decided to test this using a retrieval system that takes an input
    query (a question from a dataset of questions) and finds k documents from Wikipedia.
    They then added them to the prompt and tested how the models behaved with these
    added documents.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 作者决定使用检索系统进行测试，该系统接受一个输入查询（来自问题数据集的一个问题），并从维基百科中找到k个文档。然后，他们将这些文档添加到提示中，测试模型在这些附加文档下的表现。
- en: Using more than 20 retrieved documents only marginally improves reader performance
    (∼1.5% for GPT-3.5-Turbo and ∼1% for Claude), while significantly increasing the
    input context length (and thus latency and cost). ([source](https://arxiv.org/abs/2307.03172))
  id: totrans-171
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用超过20份检索文档仅能略微改善读者表现（GPT-3.5-Turbo约1.5%，Claude约1%），同时显著增加输入上下文长度（从而增加延迟和成本）。
    ([source](https://arxiv.org/abs/2307.03172))
- en: '![](../Images/031d4cb56aaf49348fdb137418b7e517.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/031d4cb56aaf49348fdb137418b7e517.png)'
- en: 'image source: [here](https://arxiv.org/abs/2307.03172)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[这里](https://arxiv.org/abs/2307.03172)
- en: In other words, the model saturates. As mentioned before this confirms that
    the model is more efficient in using the information at the beginning of the [context
    window](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，模型达到了饱和。如前所述，这证实了模型在[上下文窗口](https://www.linkedin.com/pulse/whats-context-window-anyway-caitie-doogan-phd/)的开头使用信息的效率更高。
- en: Closing thoughts
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结语
- en: '![](../Images/5e1b993d4c7a4e6c08d6ec3a535198a8.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e1b993d4c7a4e6c08d6ec3a535198a8.png)'
- en: Photo by [Saif71.com](https://unsplash.com/@saif71?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 摄影师：[Saif71.com](https://unsplash.com/@saif71?utm_source=medium&utm_medium=referral)
    在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: The prompt is how we interact with a model. The more precise and detailed it
    is, the better the model’s response.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 提示是我们与模型互动的方式。它越精准和详细，模型的回应就越好。
- en: There is a limit to the amount of information we can put into a prompt, though.
    This limit is the context window and it comes from numerous factors as we saw
    earlier. Ever since the first transformer was published, attempts have been made
    to enlarge this context window by exploiting numerous solutions.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们可以放入提示中的信息量是有限的。这个限制就是上下文窗口，它来源于我们之前看到的众多因素。自从第一个transformer发布以来，人们一直在尝试通过利用各种解决方案来扩大这个上下文窗口。
- en: Despite this, we do not know how well the model can use this context window.
    Studies today show that models do not make the most of them.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，我们仍然不知道模型能够多好地利用这个上下文窗口。今天的研究显示，模型并未充分利用它们。
- en: Since the scaling law was published there has been a race to the parameter,
    ever larger models looking for evanescent emergent properties. Today we know that
    all those parameters are not necessary, and that GPT-4 itself is not as large
    as thought, but actually an ensemble of eight models. The context window seems
    to be another frontier, where people try to reach a larger number not for real
    utility but to show the superiority of their model.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 自从扩展法则发布以来，一直在追逐参数，不断增加模型的规模以寻找短暂的突现属性。如今我们知道，所有这些参数并非必要，GPT-4实际上并不像想象中那么庞大，而是由八个模型组成。上下文窗口似乎是另一个前沿领域，人们试图达到更大的数量，不是为了实际效用，而是为了展示他们模型的优越性。
- en: '[](/the-infinite-babel-library-of-llms-90e203b2f6b0?source=post_page-----331e3af86d27--------------------------------)
    [## The Infinite Babel Library of LLMs'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/the-infinite-babel-library-of-llms-90e203b2f6b0?source=post_page-----331e3af86d27--------------------------------)
    [## LLMs的无限巴别图书馆'
- en: 'Open-source, data, and attention: How the future of LLMs will change'
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 开源、数据和注意力：LLMs的未来将如何改变
- en: towardsdatascience.com](/the-infinite-babel-library-of-llms-90e203b2f6b0?source=post_page-----331e3af86d27--------------------------------)
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/the-infinite-babel-library-of-llms-90e203b2f6b0?source=post_page-----331e3af86d27--------------------------------)
- en: Despite the results and the myriad of published models there are still points
    that need to be studied. How the model uses this longer context window is one
    such point that needs more analysis. For despite the elegance of the technical
    solutions sometimes the cost of a longer context window is not justified.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管已经有大量的结果和发布的模型，但仍有一些问题需要研究。如何使用较长的上下文窗口就是其中一个需要进一步分析的点。因为尽管技术解决方案很优雅，但有时较长的上下文窗口的成本并不值得。
- en: '**What do you think? Let me know in the comments.**'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**你怎么看？在评论中告诉我。**'
- en: 'If you have found this interesting:'
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如果你觉得这很有趣：
- en: '*You can look for my other articles, you can also* [***subscribe***](https://salvatore-raieli.medium.com/subscribe)
    *to get notified when I publish articles, you can* [***become a Medium member***](https://medium.com/@salvatore-raieli/membership)
    *to access all its stories (affiliate links of the platform for which I get small
    revenues without cost to you) and you can also connect or reach me on*[***LinkedIn***](https://www.linkedin.com/in/salvatore-raieli/)***.***'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '*你可以查看我的其他文章，也可以* [***订阅***](https://salvatore-raieli.medium.com/subscribe)
    *以在我发布文章时获得通知，也可以* [***成为Medium会员***](https://medium.com/@salvatore-raieli/membership)
    *以访问所有故事（这是我通过平台获得的少量收入的附属链接，对你没有费用），还可以通过*[***LinkedIn***](https://www.linkedin.com/in/salvatore-raieli/)***与我联系或找到我。***'
- en: '*Here is the link to my GitHub repository, where I am planning to collect code
    and many resources related to machine learning, artificial intelligence, and more.*'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '*这里是我GitHub仓库的链接，我计划在这里收集与机器学习、人工智能等相关的代码和许多资源。*'
- en: '[](https://github.com/SalvatoreRa/tutorial?source=post_page-----331e3af86d27--------------------------------)
    [## GitHub - SalvatoreRa/tutorial: Tutorials on machine learning, artificial intelligence,
    data science…'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/SalvatoreRa/tutorial?source=post_page-----331e3af86d27--------------------------------)
    [## GitHub - SalvatoreRa/tutorial: Tutorials on machine learning, artificial intelligence,
    data science…'
- en: Tutorials on machine learning, artificial intelligence, data science with math
    explanation and reusable code (in python…
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提供有关机器学习、人工智能、数据科学的教程，包括数学解释和可重用代码（使用Python…）。
- en: github.com](https://github.com/SalvatoreRa/tutorial?source=post_page-----331e3af86d27--------------------------------)
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/SalvatoreRa/tutorial?source=post_page-----331e3af86d27--------------------------------)'
- en: '*or you may be interested in one of my recent articles:*'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '*或者你可能对我最近的文章感兴趣：*'
- en: '[](https://levelup.gitconnected.com/codegen2-a-new-open-source-model-for-coding-214d3d464030?source=post_page-----331e3af86d27--------------------------------)
    [## CodeGen2: a new open-source model for coding'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://levelup.gitconnected.com/codegen2-a-new-open-source-model-for-coding-214d3d464030?source=post_page-----331e3af86d27--------------------------------)
    [## CodeGen2: a new open-source model for coding'
- en: SaleForce’s effect on how to design an efficient model for coding
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Salesforce如何影响高效编码模型的设计
- en: 'levelup.gitconnected.com](https://levelup.gitconnected.com/codegen2-a-new-open-source-model-for-coding-214d3d464030?source=post_page-----331e3af86d27--------------------------------)
    [](https://levelup.gitconnected.com/clinicalgpt-the-llm-clinician-5e1f7866b6d1?source=post_page-----331e3af86d27--------------------------------)
    [## ClinicalGPT: the LLM clinician'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '[临床GPT：LLM临床医生](https://levelup.gitconnected.com/clinicalgpt-the-llm-clinician-5e1f7866b6d1?source=post_page-----331e3af86d27--------------------------------)
    [](https://levelup.gitconnected.com/a-fairy-tale-of-the-inductive-bias-d418fc61726c?source=post_page-----331e3af86d27--------------------------------)
    [## ClinicalGPT: the LLM clinician'
- en: LLMs today have been applied to a wide variety of tasks. On the other hand,
    generalist models underperform fine-tuned…
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 今天的LLM已经被应用于各种任务。另一方面，通用模型在微调方面表现不佳…
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/clinicalgpt-the-llm-clinician-5e1f7866b6d1?source=post_page-----331e3af86d27--------------------------------)
    [](/a-fairy-tale-of-the-inductive-bias-d418fc61726c?source=post_page-----331e3af86d27--------------------------------)
    [## A fAIry tale of the Inductive Bias
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '[临床GPT：LLM临床医生](https://levelup.gitconnected.com/clinicalgpt-the-llm-clinician-5e1f7866b6d1?source=post_page-----331e3af86d27--------------------------------)
    [](/a-fairy-tale-of-the-inductive-bias-d418fc61726c?source=post_page-----331e3af86d27--------------------------------)
    [## 归纳偏差的神奇故事'
- en: Do we need inductive bias? How simple models can reach the performance of complex
    models
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们需要归纳偏差吗？简单模型如何达到复杂模型的性能
- en: towardsdatascience.com](/a-fairy-tale-of-the-inductive-bias-d418fc61726c?source=post_page-----331e3af86d27--------------------------------)
    [](https://levelup.gitconnected.com/the-ai-college-student-goes-back-to-the-bench-daa6d9bdfb14?source=post_page-----331e3af86d27--------------------------------)
    [## The AI college student goes back to the bench
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/a-fairy-tale-of-the-inductive-bias-d418fc61726c?source=post_page-----331e3af86d27--------------------------------)
    [](https://levelup.gitconnected.com/the-ai-college-student-goes-back-to-the-bench-daa6d9bdfb14?source=post_page-----331e3af86d27--------------------------------)
    [## AI大学生重返实验室
- en: How LLM can solve college exams and why this is important
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大型语言模型如何解决大学考试及其重要性
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/the-ai-college-student-goes-back-to-the-bench-daa6d9bdfb14?source=post_page-----331e3af86d27--------------------------------)
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: levelup.gitconnected.com](https://levelup.gitconnected.com/the-ai-college-student-goes-back-to-the-bench-daa6d9bdfb14?source=post_page-----331e3af86d27--------------------------------)
- en: Reference
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考
- en: Here is the list of the principal references I consulted to write this article,
    only the first name for an article is cited. I suggest also them if you want to
    deepen on the topic.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我撰写本文所参考的主要文献列表，只引用了文章的第一个名字。如果你想深入了解这个话题，我也建议你阅读这些文献。
- en: Prompt Engineering Guide, [link](https://www.promptingguide.ai/)
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Prompt Engineering Guide, [链接](https://www.promptingguide.ai/)
- en: 'Wang, 2023, Prompt Engineering for Healthcare: Methodologies and Applications,
    [link](https://arxiv.org/abs/2304.14670)'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Wang, 2023, 医疗保健中的提示工程：方法和应用, [链接](https://arxiv.org/abs/2304.14670)
- en: White, 2023, A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT,
    [link](https://arxiv.org/abs/2302.11382)
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: White, 2023, 用于增强ChatGPT提示工程的提示模式目录, [链接](https://arxiv.org/abs/2302.11382)
- en: 'Liu, 2021, Pre-train, Prompt, and Predict: A Systematic Survey of Prompting
    Methods in Natural Language Processing, [link](https://arxiv.org/abs/2107.13586)'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Liu, 2021, 预训练、提示和预测：自然语言处理提示方法的系统调查, [链接](https://arxiv.org/abs/2107.13586)
- en: Liu, 2022, Generated Knowledge Prompting for Commonsense Reasoning, [link](https://arxiv.org/abs/2110.08387)
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Liu, 2022, 生成知识提示用于常识推理, [链接](https://arxiv.org/abs/2110.08387)
- en: '[Sascha Heyer](https://medium.com/u/57d0091b2e22?source=post_page-----331e3af86d27--------------------------------),
    Generative AI — Best Practices for LLM Prompt Engineering, [link](https://medium.com/google-cloud/generative-ai-best-practices-for-llm-prompt-engineering-2a0131c805cc)'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Sascha Heyer](https://medium.com/u/57d0091b2e22?source=post_page-----331e3af86d27--------------------------------),
    生成性AI——LLM提示工程的最佳实践, [链接](https://medium.com/google-cloud/generative-ai-best-practices-for-llm-prompt-engineering-2a0131c805cc)'
- en: '[Jacob Ferus](https://medium.com/u/2415c881fd51?source=post_page-----331e3af86d27--------------------------------),
    GPT-4 Has Arrived — Here’s What You Need To Know, [link](https://levelup.gitconnected.com/gpt-4-has-arrived-heres-what-you-need-to-know-398c3c72191c)'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Jacob Ferus](https://medium.com/u/2415c881fd51?source=post_page-----331e3af86d27--------------------------------),
    GPT-4 已经到来——这是你需要知道的, [链接](https://levelup.gitconnected.com/gpt-4-has-arrived-heres-what-you-need-to-know-398c3c72191c)'
- en: '[Simon Attard](https://medium.com/u/5a0896ce2f9b?source=post_page-----331e3af86d27--------------------------------),
    Giving Large Language Models Context, blog post, [link](https://medium.com/@simon_attard/giving-large-language-models-context-2d1956a6a017)'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Simon Attard](https://medium.com/u/5a0896ce2f9b?source=post_page-----331e3af86d27--------------------------------),
    为大型语言模型提供上下文, 博客文章, [链接](https://medium.com/@simon_attard/giving-large-language-models-context-2d1956a6a017)'
- en: 'Rickard, [Commoditization of Large Language Models: Part 3](https://matt-rickard.com/commoditization-of-large-language-models-part-3)'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Rickard, [大型语言模型商品化：第3部分](https://matt-rickard.com/commoditization-of-large-language-models-part-3)
- en: Vaswani, 2017, Attention Is All You Need, [link](https://arxiv.org/abs/1706.03762)
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Vaswani, 2017, 注意力机制是你所需的一切, [链接](https://arxiv.org/abs/1706.03762)
- en: 'Press, 2021, Train Short, Test Long: Attention with Linear Biases Enables Input
    Length Extrapolation, [link](https://arxiv.org/abs/2108.12409)'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Press, 2021, 短训练，长测试：线性偏差的注意力实现输入长度外推, [链接](https://arxiv.org/abs/2108.12409)
- en: 'Zaheer, 2021, Big Bird: Transformers for Longer Sequences, [link](https://arxiv.org/abs/2007.14062)'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Zaheer, 2021, Big Bird: Transformers for Longer Sequences, [链接](https://arxiv.org/abs/2007.14062)'
- en: 'Ainslie, 2020, ETC: Encoding Long and Structured Inputs in Transformers, [link](https://arxiv.org/abs/2004.08483)'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Ainslie, 2020, ETC: 在Transformer中编码长而结构化的输入, [链接](https://arxiv.org/abs/2004.08483)'
- en: Google blog, Constructing Transformers For Longer Sequences with Sparse Attention
    Methods, 2021, [link](https://ai.googleblog.com/2021/03/constructing-transformers-for-longer.html)
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Google博客, 使用稀疏注意力方法构建用于更长序列的Transformer, 2021, [链接](https://ai.googleblog.com/2021/03/constructing-transformers-for-longer.html)
- en: 'Shazeer, 2019, Fast Transformer Decoding: One Write-Head is All You Need, [link](https://arxiv.org/abs/1911.02150)'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Shazeer, 2019, 快速 Transformer 解码：一个写头足矣，[link](https://arxiv.org/abs/1911.02150)
- en: Pope, 2022, Efficiently Scaling Transformer Inference, [link](https://arxiv.org/abs/2211.05102)
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Pope, 2022, 高效扩展 Transformer 推理，[link](https://arxiv.org/abs/2211.05102)
- en: '[Ahmed Taha](https://medium.com/u/996110eea09a?source=post_page-----331e3af86d27--------------------------------),
    FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness, [link](https://ahmdtaha.medium.com/flashattention-fast-and-memory-efficient-exact-attention-with-io-awareness-2a0aec52ed3d),
    medium post'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Ahmed Taha](https://medium.com/u/996110eea09a?source=post_page-----331e3af86d27--------------------------------)，FlashAttention：快速且内存高效的精确注意力与
    IO 关注，[link](https://ahmdtaha.medium.com/flashattention-fast-and-memory-efficient-exact-attention-with-io-awareness-2a0aec52ed3d)，medium
    文章'
- en: '[Angelina Yang](https://medium.com/u/ff66ee151115?source=post_page-----331e3af86d27--------------------------------),
    What is Global Attention in Deep Learning? blog post, [link](https://angelina-yang.medium.com/what-is-global-attention-in-deep-learning-53bd4525a389)'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Angelina Yang](https://medium.com/u/ff66ee151115?source=post_page-----331e3af86d27--------------------------------)，什么是深度学习中的全局注意力？博客文章，[link](https://angelina-yang.medium.com/what-is-global-attention-in-deep-learning-53bd4525a389)'
- en: 'Dao, 2022, FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness,
    [link](https://arxiv.org/abs/2205.14135)'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Dao, 2022, FlashAttention：快速且内存高效的精确注意力与 IO 关注，[link](https://arxiv.org/abs/2205.14135)
- en: A deep dive on FlashAttention [here](https://shreyansh26.github.io/post/2023-03-26_flash-attention/)
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关于 FlashAttention 的深入分析 [here](https://shreyansh26.github.io/post/2023-03-26_flash-attention/)
- en: '[Medium post on LLaMA](https://medium.com/mlearning-ai/metas-llama-a-small-language-model-beating-giants-5065948e0b7f),
    META’s LLaMA: A small language model beating giants'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Medium 上关于 LLaMA 的文章](https://medium.com/mlearning-ai/metas-llama-a-small-language-model-beating-giants-5065948e0b7f)，META
    的 LLaMA：一个击败巨头的小型语言模型'
- en: 'Ainslie, 2023, CoLT5: Faster Long-Range Transformers with Conditional Computation,
    [link](https://arxiv.org/abs/2303.09752)'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ainslie, 2023, CoLT5：具有条件计算的更快长范围 Transformers，[link](https://arxiv.org/abs/2303.09752)
- en: 'Yu, 2023, MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers,
    [link](https://arxiv.org/abs/2305.07185)'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Yu, 2023, MEGABYTE：使用多尺度 Transformers 预测百万字节序列，[link](https://arxiv.org/abs/2305.07185)
- en: A blog post about recent development in extending the context length, [link](https://kaiokendev.github.io/context)
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一篇关于扩展上下文长度的最新进展的博客文章，[link](https://kaiokendev.github.io/context)
- en: '[Andrew Lukyanenko](https://medium.com/u/26c63d12ebc9?source=post_page-----331e3af86d27--------------------------------),
    Paper Review: Scaling Transformer to 1M tokens and beyond with RMT, blog post,
    [link](https://artgor.medium.com/paper-review-scaling-transformer-to-1m-tokens-and-beyond-with-rmt-5b846cecc0b2)'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Andrew Lukyanenko](https://medium.com/u/26c63d12ebc9?source=post_page-----331e3af86d27--------------------------------)，论文综述：将
    Transformer 扩展到 1M tokens 及以上的 RMT，博客文章，[link](https://artgor.medium.com/paper-review-scaling-transformer-to-1m-tokens-and-beyond-with-rmt-5b846cecc0b2)'
- en: 'Liu, 2023, Lost in the Middle: How Language Models Use Long Contexts, [link](https://arxiv.org/abs/2307.03172)'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Liu, 2023, 迷失在中间：语言模型如何使用长上下文，[link](https://arxiv.org/abs/2307.03172)
