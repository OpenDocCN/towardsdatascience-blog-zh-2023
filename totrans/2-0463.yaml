- en: How to Build a Local Chatbot with Llama2 and LangChain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/can-a-llama-2-powered-chatbot-be-trained-on-a-cpu-ce9ec6ebe3c2](https://towardsdatascience.com/can-a-llama-2-powered-chatbot-be-trained-on-a-cpu-ce9ec6ebe3c2)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Overview and Implementation with Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@aashishnair?source=post_page-----ce9ec6ebe3c2--------------------------------)[![Aashish
    Nair](../Images/23f4b3839e464419332b690a4098d824.png)](https://medium.com/@aashishnair?source=post_page-----ce9ec6ebe3c2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ce9ec6ebe3c2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ce9ec6ebe3c2--------------------------------)
    [Aashish Nair](https://medium.com/@aashishnair?source=post_page-----ce9ec6ebe3c2--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ce9ec6ebe3c2--------------------------------)
    ·6 min read·Oct 12, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ded2ef377ff10804f982df906fed5cbe.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Adi Goldstein](https://unsplash.com/@adigold1?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Table of Contents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ∘ [Introduction](#6563)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Case Study](#dcc3)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Step 1 — Creating a Vector Store](#9999)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Step 2—Creating the QA Chain](#b94f)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Step 3 — Creating the User Interface](#a748)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [Evaluating the chatbot](#347d)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [The Results](#3580)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [The Final Verdict](#b61a)
  prefs: []
  type: TYPE_NORMAL
- en: ∘ [References](#ec13)
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The advent of local models has been welcomed by businesses looking to build
    their own custom LLM applications. They enable developers to build solutions that
    can run offline and adhere to their privacy and security requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Such LLMs were originally huge and mostly catered to enterprises that have the
    funds and resources to provision GPUs and train models on large volumes of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, local LLMs are now available in much smaller sizes, which begs the
    question: is it possible for individuals with basic CPUs to harness these same
    tools and technologies?'
  prefs: []
  type: TYPE_NORMAL
- en: It’s a question worth considering as users stand to gain a lot from building
    their own personal, local chatbots that can perform tasks offline.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we explore this possibility by building a close-sourced chatbot using
    Meta’s Llama2 on a CPU and evaluate its performance as a reliable tool for individuals.
  prefs: []
  type: TYPE_NORMAL
- en: '**Case Study**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To test the feasibility of a building local chatbot that can run offline on
    a personal computer, let’s carry out a case study.
  prefs: []
  type: TYPE_NORMAL
- en: The objective is to build a chatbot using a quantized version of Meta’s Llama2
    (7B parameters). The model will be used to build a LangChain application that
    facilitates response generation, which can be accessed with a user interface that
    enables people to interact with the application.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4e0460af3009b86e062b0bbd6677d1ee.png)'
  prefs: []
  type: TYPE_IMG
- en: Chatbot Diagram (Created by Author)
  prefs: []
  type: TYPE_NORMAL
- en: 'The chatbot will be trained with two PDF documents (both accessible with the
    arXiv API):'
  prefs: []
  type: TYPE_NORMAL
- en: '[A Comprehensive Review of Computer Vision in Sports: Open Issues, Future Trends
    and Research Directions](https://browse.arxiv.org/pdf/2203.02281.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[A Survey of Deep Learning in Sports Applications: Perception, Comprehension,
    and Decision](https://browse.arxiv.org/pdf/2307.03353.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For context, this bot will be trained on a computer with the following specifications:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating System: Windows 10'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Processor: Intel i7'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RAM: 8GB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note: Following along case study will require prior knowledge of the LangChain
    framework and the Streamlit library'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Step 1 — Creating a Vector Store
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we create the vector store, which will store the embedded data from the
    documents and facilitate the retrieval of documents relevant to the users’ queries.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2f5d8138129b691dd2d8d3963a7a7789.png)'
  prefs: []
  type: TYPE_IMG
- en: Creating a Vector Store (Created by Author)
  prefs: []
  type: TYPE_NORMAL
- en: For that, the data has to be converted into chunks. This is done by loading
    the PDF documents with the PyPDFLoader and splitting the text into chunks of 500
    characters.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the chunks are converted into embeddings with the use of a sentence transformer
    from HuggingFace. It’s important to specify the device as “cpu” in the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: With the text chunks created and the embedding model loaded, we can create the
    vector store. For this case study, we use the Facebook AI Similarity Search (FAISS).
  prefs: []
  type: TYPE_NORMAL
- en: This vector store will be saved locally for future use.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2—**Creating the QA Chain**
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we need to load the retrieval QA chain, which retrieves the relevant documents
    from the vector store and uses them to answer the users’ queries.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fb286d6019a13a7ece66c82aaf70f62b.png)'
  prefs: []
  type: TYPE_IMG
- en: QA Chain (Created by Author)
  prefs: []
  type: TYPE_NORMAL
- en: 'The QA chain requires three components: the quantized Llama 2 model, the FAISS
    vector store, and a prompt template.'
  prefs: []
  type: TYPE_NORMAL
- en: First, we download the quantized Llama2 model, which is available in the [HuggingFace
    repository](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/tree/main). For
    this case study, the model is downloaded through a file named “[llama-2-7b-chat.ggmlv3.q2_K.bin](https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/blob/main/llama-2-7b-chat.ggmlv3.q2_K.bin)”,
    which uses 2.87 GB in memory.
  prefs: []
  type: TYPE_NORMAL
- en: The model is then loaded using CTransformers, the Python library for binding
    transformer models implemented in C. Since we want an objective chatbot that generates
    responses with less creativity, so we set `temperature` to 0.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we load the vector store previously created.
  prefs: []
  type: TYPE_NORMAL
- en: After that, we define the prompt template. This step is optional, but since
    we are looking to interact with research papers, we need to prioritize accuracy,
    which we can enforce through the instructions in the prompt. Hallucinations are
    highly undesired, so the main instruction is to respond with “I don’t know” to
    questions that can not be answered with the provided PDF documents.
  prefs: []
  type: TYPE_NORMAL
- en: With these elements, we can create the QA chain, which will generate responses
    to users’ queries using the loaded quantized Llama2 model, the vector store, and
    the prompt template.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we create the function that executes the response generation.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 — Creating the User Interface
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The core elements needed for the LangChain application have been built, so we
    can pivot to building a user interface for the chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: The Streamlit library is suited for this task as it contains features tailored
    for chatbot applications.
  prefs: []
  type: TYPE_NORMAL
- en: The following code incorporates the previously built functions into the user
    interface (to review the entire source code, please visit the GitHub [repository](https://github.com/anair123/Llama2-Powered-QA-Chatbot-For-Research-Papers)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The Streamlit app is run with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: And voila! We have our personal closed-sourced chatbot up and running!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5eb1eda91c8dff86597b8a988f2c9a8a.png)'
  prefs: []
  type: TYPE_IMG
- en: Chatbot User Interface (Created by Author)
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the chatbot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have our chatbot, so let’s evaluate its performance with 3 different questions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**What is the benefit of computer vision in sports analysis?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/c796ce889e5faa6928d3ed29082b0146.png)'
  prefs: []
  type: TYPE_IMG
- en: Chatbot Response (Created by Author)
  prefs: []
  type: TYPE_NORMAL
- en: '**2\. Give me a list of sports that incorporate computer vision.**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/085b38ed3cfe22a1f9edce81dfdad268.png)'
  prefs: []
  type: TYPE_IMG
- en: Chatbot Response (Created by Author)
  prefs: []
  type: TYPE_NORMAL
- en: '**3\. What algorithms are used to track players?**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a33fd4016343cfd0a9ac8a7b81cf95e2.png)'
  prefs: []
  type: TYPE_IMG
- en: Chatbot Response (Created by Author)
  prefs: []
  type: TYPE_NORMAL
- en: The Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Overall, the bot seems to return satisfactory responses without including unsolicited
    information.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, one limitation is evident from the response to the question: “What
    algorithms are used to track players?”, where the answer cuts off mid-sentence.
    This can be attributed to the limited context window (i.e. number of tokens) of
    the quantized version of the Llama2 model. The chatbot is unable to properly answer
    questions that require many tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, a limitation that isn’t conveyed in the responses itself is time.
    While the bot responded appropriately to questions, it took over a minute on average
    to generate responses on this CPU. With such a long run time, there isn’t a strong
    argument for using this tool as an alternative to manually searching for content
    in a collection of documents.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, this entire exercise exhausted a lot of memory from my computer, which
    rendered other applications unusable.
  prefs: []
  type: TYPE_NORMAL
- en: The Final Verdict
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/34038f008137bb643850a7239b2b6497.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [Arek Socha](https://pixabay.com/users/qimono-1962238/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=2492011)
    from [Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=2492011)
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the case study has concluded, let’s revisit the initial question:
    Can we build LLM-powered applications on a CPU?'
  prefs: []
  type: TYPE_NORMAL
- en: 'The answer is: Yes… but we probably shouldn’t.'
  prefs: []
  type: TYPE_NORMAL
- en: The positives from this case study are that the quantized Llama2 model is easy
    to download and incorporate into the application and that the chatbot is able
    to generate responses of high quality.
  prefs: []
  type: TYPE_NORMAL
- en: However, the combination of limited tokens, long run time, and high memory usage
    makes the prospect of training closed-sourced chatbots on CPUs unfeasible.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, this conclusion is predicated on the constraints of the device used
    to conduct the case study, so a computer with greater processing power and storage
    may yield more promising results. In addition, in time, smaller LLMs with larger
    context windows will be made available to the public, making closed-sourced chatbots
    easier to build and utilize on CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re interested in finding out how this application would perform on your
    device, feel free to check out the code in the following repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[anair123/Llama2-Powered-QA-Chatbot-For-Research-Papers (github.com)](https://github.com/anair123/Llama2-Powered-QA-Chatbot-For-Research-Papers)'
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for reading!
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Naika, B. T., Hashmi, M. F., & Bokde, N. D. (n.d.). *A Comprehensive Review
    of Computer Vision in Sports: Open Issues, Future Trends and Research Directions*.
    Arxiv. [https://arxiv.org/pdf/2203.02281](https://arxiv.org/pdf/2203.02281)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Zhao, Z., Chai, W., Hao, S., Hu, W., Wang, G., Cao, S., Song, M., Hwang, J.-N.,
    & Wang, G. (n.d.). *A Survey of Deep Learning in Sports Applications: Perception,
    Comprehension, and Decision*. Arxiv. [https://arxiv.org/pdf/2307.03353.pdf](https://arxiv.org/pdf/2307.03353.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
