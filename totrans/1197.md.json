["```py\ndf = spark.read.csv(\"/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\", header=\"true\", inferSchema=\"true\")\ndisplay(df)\n```", "```py\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder\n\ncat_cols= [\"cut\", \"color\", \"clarity\"]\nstages = [] # Stages in Pipeline\n\nfor c in cat_cols:\n    stringIndexer = StringIndexer(inputCol=c, outputCol=c + \"_index\")\n    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], \\\n            outputCols=[c + \"_vec\"])    \n    stages += [stringIndexer, encoder] # Stages will be run later on\n```", "```py\nfrom pyspark.ml.feature import VectorAssembler\n\n# Transform all features into a vector\nnum_cols = [\"carat\", \"depth\", \"table\", \"x\", \"y\", \"z\"]\nassemblerInputs = [c + \"_vec\" for c in cat_cols] + num_cols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nstages += [assembler]\n\n# Create pipeline and use on dataset\npipeline = Pipeline(stages=stages)\ndf = pipeline.fit(df).transform(df)\n```", "```py\ntrain, test = df.randomSplit([0.90, 0.1], seed=123)\nprint('Train dataset count:', train.count())\nprint('Test dataset count:', test.count())\n```", "```py\nfrom pyspark.ml.regression import RandomForestRegressor\n\nrf = RandomForestRegressor(featuresCol='features', labelCol='price')\nrf_model = rf.fit(train)\n\ntrain_predictions = rf_model.transform(train)\ntest_predictions = rf_model.transform(test)\n```", "```py\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nevaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n                 labelCol=\"price\", metricName=\"r2\")\n\nprint(\"Train R2:\", evaluator.evaluate(train_predictions))\nprint(\"Test R2:\", evaluator.evaluate(test_predictions))\n```", "```py\ndef extract_feature_imp(feature_imp, dataset, features_col):\n    list_extract = []\n    for i in dataset.schema[features_col].metadata[\"ml_attr\"][\"attrs\"]:\n        list_extract = list_extract + dataset.schema[features_col].metadata[\"ml_attr\"][\"attrs\"][i]\n    feature_list = pd.DataFrame(list_extract)\n    feature_list['score'] = feature_list['idx'].apply(lambda x: feature_imp[x])\n    return(feature_list.sort_values('score', ascending = False))\n```", "```py\nimport pandas as pd\n```", "```py\nfeature_list = extract_feature_imp(rf_model.featureImportances, train, \"features\")\ntop_20_features = feature_list.sort_values('score', ascending = False).head(20)# Then make your desired plot function to visualize feature importance\nplot_feature_importance(top_20_features['score'], top_20_features['name'])\n```"]