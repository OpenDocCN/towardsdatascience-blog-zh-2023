- en: Deploying Large Language Models With HuggingFace TGI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/deploying-large-language-models-with-huggingface-tgi-981747c669e3](https://towardsdatascience.com/deploying-large-language-models-with-huggingface-tgi-981747c669e3)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Another way to efficiently host and scale your LLMs with Amazon SageMaker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://ram-vegiraju.medium.com/?source=post_page-----981747c669e3--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page-----981747c669e3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----981747c669e3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----981747c669e3--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page-----981747c669e3--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----981747c669e3--------------------------------)
    ·5 min read·Jul 14, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/87080a29c8a17cddf9ed8b4ece860f12.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [Unsplash](https://unsplash.com/photos/4NYtYSiZVlA)
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) continue to soar in popularity as a new one is
    released nearly every week. With the number of these models increasing, so are
    the options for how we can host them. In my previous article we explored how we
    could utilize [DJL Serving](https://github.com/deepjavalibrary/djl-serving) within
    Amazon SageMaker to efficiently host LLMs. In this article we explore another
    optimized model server and solution in [HuggingFace Text Generation Inference
    (TGI)](https://github.com/huggingface/text-generation-inference).
  prefs: []
  type: TYPE_NORMAL
- en: '**NOTE**: For those of you new to AWS, make sure you make an account at the
    following [link](https://aws.amazon.com/console/) if you want to follow along.
    The article also assumes an intermediate understanding of SageMaker Deployment,
    I would suggest following this [article](https://aws.amazon.com/blogs/machine-learning/part-2-model-hosting-patterns-in-amazon-sagemaker-getting-started-with-deploying-real-time-models-on-sagemaker/)
    for understanding Deployment/Inference more in depth.'
  prefs: []
  type: TYPE_NORMAL
- en: '**DISCLAIMER**: I am a Machine Learning Architect at AWS and my opinions are
    my own.'
  prefs: []
  type: TYPE_NORMAL
- en: Why HuggingFace Text Generation Inference? How Does It Work With Amazon SageMaker?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TGI is a Rust, Python, gRPC model server created by HuggingFace that can be
    used to host specific large language models. HuggingFace has long been the central
    hub for NLP and it contains a large set of optimizations when it comes to LLMs
    specifically, look below for a few and the [documentation](https://github.com/huggingface/text-generation-inference#features)
    for an extensive list.
  prefs: []
  type: TYPE_NORMAL
- en: Tensor Parallelism for efficient hosting across multiple GPUs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Token Streaming with SSE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantization with [bitsandbytes](https://github.com/TimDettmers/bitsandbytes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Logits warper](https://huggingface.co/docs/transformers/internal/generation_utils#transformers.LogitsProcessor)
    (different params such as temperature, top-k, top-n, etc)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A large positive of this solution that I noted is the simplicity of use. TGI
    at this moment supports the following optimized model architectures that you can
    directly deploy utilizing the TGI containers.
  prefs: []
  type: TYPE_NORMAL
- en: '[BLOOM](https://huggingface.co/bigscience/bloom)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[FLAN-T5](https://huggingface.co/google/flan-t5-xxl)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Galactica](https://huggingface.co/facebook/galactica-120b)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[GPT-Neox](https://huggingface.co/EleutherAI/gpt-neox-20b)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Llama](https://github.com/facebookresearch/llama)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[OPT](https://huggingface.co/facebook/opt-66b)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[SantaCoder](https://huggingface.co/bigcode/santacoder)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Starcoder](https://huggingface.co/bigcode/starcoder)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Falcon 7B](https://huggingface.co/tiiuae/falcon-7b)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Falcon 40B](https://huggingface.co/tiiuae/falcon-40b)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even more nicely it directly integrates with Amazon SageMaker as we will explore
    in this article. SageMaker now provides managed [TGI containers](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-text-generation-inference-containers)
    that we will [retrieve](https://aws.plainenglish.io/how-to-retrieve-amazon-sagemaker-deep-learning-images-ff4a5866299e)
    and use to deploy a [Llama model](https://github.com/facebookresearch/llama) in
    this example.
  prefs: []
  type: TYPE_NORMAL
- en: TGI vs DJL Serving vs JumpStart
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far in this LLM Hosting series we’ve explored two alternative options with
    Amazon SageMaker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[DJL Serving](/deploying-llms-on-amazon-sagemaker-with-djl-serving-8220e3cfad0c)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[SageMaker JumpStart](/deploying-cohere-language-models-on-amazon-sagemaker-23a3f79639b1)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**When do we use what and where does TGI fall into the picture?** SageMaker
    JumpStart is great if the model is already provided in its offerings. You don’t
    really need to mess with any containers or model server level work, this is all
    abstracted out for you. DJL Serving is great if you have a custom use-case with
    a model that is not supported by JumpStart. You can pull directly from the HuggingFace
    Hub or also load your own artifacts in S3 and point towards it. Another positive
    is also if you’ve mastered a specific model partitioning framework such as Accelerate
    or FasterTransformers that it integrates with, this allows you to add some advanced
    performance optimization techniques to your LLM hosting.'
  prefs: []
  type: TYPE_NORMAL
- en: Where does TGI come in? TGI as we’ve noted natively supports specific model
    architectures. TGI is a great option if you are trying to deploy one of these
    supported models as it’s optimized specifically for that architecture. In addition,
    if you understand and know the model server deeper you can tune some of the environment
    variables that are exposed. Think of TGI almost as an intersection between ease
    of use and performance optimization. It’s essential to choose the right option
    out of these three for your deployment depending on your use-case and weighing
    the different pros and cons each solution has.
  prefs: []
  type: TYPE_NORMAL
- en: Llama Deployment with TGI on Amazon SageMaker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To work with Amazon SageMaker we will utilize the [SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/)
    to grab our TGI container and streamline deployment. In this example we will follow
    Phillip Schmid’s [TGI blog](https://huggingface.co/blog/sagemaker-huggingface-llm)
    and adjust it to Llama. Make sure to first install the latest version of the SageMaker
    Python SDK and setup the necessary clients to work with the service.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Next we [retrieve](https://aws.plainenglish.io/how-to-retrieve-amazon-sagemaker-deep-learning-images-ff4a5866299e)
    the necessary SageMaker container for TGI deployment. AWS manages a set of [Deep
    Learning containers](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)
    (you can also [bring your own](/bring-your-own-container-with-amazon-sagemaker-37211d8412f4))
    that integrate with different model servers such as TGI, DJL Serving, TorchServe,
    and more. In this case we point the SDK to the version of TGI we need.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Next comes the specific magic related to the TGI container. The simplicity of
    TGI is that we can just provide the model hub ID for the specific LLM we are working
    with. Note that it must be from the supported model architectures of TGI. In this
    case we utilize a form of the [Llama-7B model](https://huggingface.co/decapoda-research/llama-7b-hf).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In the config object above you can also specify any additional optimizations
    that the TGI container supports, this includes for example quantization format
    (ex: [bitsandbytes](https://github.com/TimDettmers/bitsandbytes)). We then pass
    in this config to a HuggingFace Model object that SageMaker understands.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We can then directly deploy this model object to a SageMaker Real-Time Endpoint.
    Here you can specify your hardware, for these larger models a GPU family such
    as a g5 instance is recommended. We also enable a larger container health check
    timeout due to the size of these models.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Creation of the endpoint should take a few minutes, but then you should be able
    to perform sample inference with the following code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Additional Resources & Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://github.com/RamVegiraju/SageMaker-Deployment/blob/master/LLM-Hosting/TGI/tgi-llama.ipynb?source=post_page-----981747c669e3--------------------------------)
    [## SageMaker-Deployment/LLM-Hosting/TGI/tgi-llama.ipynb at master · RamVegiraju/SageMaker-Deployment'
  prefs: []
  type: TYPE_NORMAL
- en: Compilation of examples of SageMaker inference options and other features. …
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/RamVegiraju/SageMaker-Deployment/blob/master/LLM-Hosting/TGI/tgi-llama.ipynb?source=post_page-----981747c669e3--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: You can find the code for the entire example at the link above, you can also
    alternatively generate this code directly from the HuggingFace Hub at the SageMaker
    deployment tab for your supported model. Text Generation Inference provides a
    highly optimized model server that also greatly simplifies the deployment process.
    Stay tuned for more content around the LLM space, and as always thank you for
    reading and feel free to leave any feedback.
  prefs: []
  type: TYPE_NORMAL
- en: '*If you enjoyed this article feel free to connect with me on* [*LinkedIn*](https://www.linkedin.com/in/ram-vegiraju-81272b162/)
    *and subscribe to my Medium* [*Newsletter*](https://ram-vegiraju.medium.com/subscribe)*.
    If you’re new to Medium, sign up using my* [*Membership Referral*](https://ram-vegiraju.medium.com/membership)*.*'
  prefs: []
  type: TYPE_NORMAL
