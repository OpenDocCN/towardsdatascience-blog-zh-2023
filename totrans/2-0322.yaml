- en: 'Applied Reinforcement Learning VI: Deep Deterministic Policy Gradients (DDPG)
    for Continuous Control'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åº”ç”¨å¼ºåŒ–å­¦ä¹  VIï¼šç”¨äºè¿ç»­æ§åˆ¶çš„æ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦ï¼ˆDDPGï¼‰
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/applied-reinforcement-learning-vi-deep-deterministic-policy-gradients-ddpg-for-continuous-dad372f6cb1d](https://towardsdatascience.com/applied-reinforcement-learning-vi-deep-deterministic-policy-gradients-ddpg-for-continuous-dad372f6cb1d)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/applied-reinforcement-learning-vi-deep-deterministic-policy-gradients-ddpg-for-continuous-dad372f6cb1d](https://towardsdatascience.com/applied-reinforcement-learning-vi-deep-deterministic-policy-gradients-ddpg-for-continuous-dad372f6cb1d)
- en: Introduction and theoretical explanation of the DDPG algorithm, which has many
    applications in the field of continuous control
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DDPGç®—æ³•çš„ä»‹ç»å’Œç†è®ºè§£é‡Šï¼Œè¿™åœ¨è¿ç»­æ§åˆ¶é¢†åŸŸæœ‰è®¸å¤šåº”ç”¨ã€‚
- en: '[](https://medium.com/@JavierMtz5?source=post_page-----dad372f6cb1d--------------------------------)[![Javier
    MartÃ­nez Ojeda](../Images/5b5df4220fa64c13232c29de9b4177af.png)](https://medium.com/@JavierMtz5?source=post_page-----dad372f6cb1d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dad372f6cb1d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dad372f6cb1d--------------------------------)
    [Javier MartÃ­nez Ojeda](https://medium.com/@JavierMtz5?source=post_page-----dad372f6cb1d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@JavierMtz5?source=post_page-----dad372f6cb1d--------------------------------)[![Javier
    MartÃ­nez Ojeda](../Images/5b5df4220fa64c13232c29de9b4177af.png)](https://medium.com/@JavierMtz5?source=post_page-----dad372f6cb1d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dad372f6cb1d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dad372f6cb1d--------------------------------)
    [Javier MartÃ­nez Ojeda](https://medium.com/@JavierMtz5?source=post_page-----dad372f6cb1d--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dad372f6cb1d--------------------------------)
    Â·8 min readÂ·Mar 7, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dad372f6cb1d--------------------------------)
    Â·é˜…è¯»æ—¶é—´ 8 åˆ†é’ŸÂ·2023å¹´3æœˆ7æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/b37d51029a59d2af9831889b91939bda.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b37d51029a59d2af9831889b91939bda.png)'
- en: Photo by [Eyosias G](https://unsplash.com/pt-br/@yozz_t?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºï¼š[Eyosias G](https://unsplash.com/pt-br/@yozz_t?utm_source=medium&utm_medium=referral)
    åœ¨[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: If you want to read this article without a Premium Medium account, you can do
    it from this friend link :)
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³åœ¨æ²¡æœ‰Premium Mediumè´¦æˆ·çš„æƒ…å†µä¸‹é˜…è¯»è¿™ç¯‡æ–‡ç« ï¼Œä½ å¯ä»¥é€šè¿‡è¿™ä¸ªæœ‹å‹é“¾æ¥ :)
- en: '[https://www.learnml.wiki/applied-reinforcement-learning-vi-deep-deterministic-policy-gradients-ddpg-for-continuous-control/](https://www.learnml.wiki/applied-reinforcement-learning-vi-deep-deterministic-policy-gradients-ddpg-for-continuous-control/)'
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://www.learnml.wiki/applied-reinforcement-learning-vi-deep-deterministic-policy-gradients-ddpg-for-continuous-control/](https://www.learnml.wiki/applied-reinforcement-learning-vi-deep-deterministic-policy-gradients-ddpg-for-continuous-control/)'
- en: The **DDPG algorithm**, first presented at ICLR 2016 by Lillicarp et al. **[1]**,
    was a significant breakthrough in terms of Deep Reinforcement Learning algorithms
    for continuous control, because of its improvement over DQN **[2]** (which only
    works with discrete actions), and its very good results and ease of implementation
    (see **[1]**).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**DDPGç®—æ³•**ï¼Œç”±Lillicarpç­‰äººåœ¨ICLR 2016é¦–æ¬¡æå‡º **[1]**ï¼Œåœ¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•ç”¨äºè¿ç»­æ§åˆ¶æ–¹é¢å–å¾—äº†é‡å¤§çªç ´ï¼Œå› ä¸ºå®ƒæ¯”DQN
    **[2]**ï¼ˆä»…é€‚ç”¨äºç¦»æ•£åŠ¨ä½œï¼‰æœ‰äº†æ”¹è¿›ï¼Œä¸”å…¶ç»“æœä¼˜å¼‚ä¸”æ˜“äºå®ç°ï¼ˆè§**[1]**ï¼‰ã€‚'
- en: As for the **NAF algorithm** **[3]** presented in the [previous article](https://medium.com/towards-data-science/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095),
    DDPG works with continuous action spaces and continuous state spaces, making it
    an equally valid choice for continuous control tasks applicable to fields such
    as Robotics or Autonomous Driving.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äº**NAFç®—æ³•** **[3]**ï¼Œè¯¥ç®—æ³•åœ¨[ä¸Šä¸€ç¯‡æ–‡ç« ](https://medium.com/towards-data-science/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095)ä¸­ä»‹ç»ï¼ŒDDPGé€‚ç”¨äºè¿ç»­åŠ¨ä½œç©ºé—´å’Œè¿ç»­çŠ¶æ€ç©ºé—´ï¼Œå› æ­¤å®ƒä¹Ÿæ˜¯é€‚ç”¨äºæœºå™¨äººæŠ€æœ¯æˆ–è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸçš„è¿ç»­æ§åˆ¶ä»»åŠ¡çš„æœ‰æ•ˆé€‰æ‹©ã€‚
- en: '[](/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095?source=post_page-----dad372f6cb1d--------------------------------)
    [## Applied Reinforcement Learning V: Normalized Advantage Function (NAF) for
    Continuous Control'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095?source=post_page-----dad372f6cb1d--------------------------------)
    [## åº”ç”¨å¼ºåŒ–å­¦ä¹  VIï¼šç”¨äºè¿ç»­æ§åˆ¶çš„å½’ä¸€åŒ–ä¼˜åŠ¿å‡½æ•°ï¼ˆNAFï¼‰'
- en: Introduction and explanation of the NAF algorithm, widely used in continuous
    control tasks
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NAFç®—æ³•çš„ä»‹ç»å’Œè§£é‡Šï¼Œè¯¥ç®—æ³•å¹¿æ³›ç”¨äºè¿ç»­æ§åˆ¶ä»»åŠ¡
- en: towardsdatascience.com](/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095?source=post_page-----dad372f6cb1d--------------------------------)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095?source=post_page-----dad372f6cb1d--------------------------------)'
- en: Logic of the DDPG algorithm
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DDPGç®—æ³•çš„é€»è¾‘
- en: 'The DDPG algorithm is an Actor-Critic algorithm, which, as its name suggests,
    is composed of two neural networks: the **Actor** and the **Critic**. The Actor
    is in charge of choosing the best action, and the Critic must evaluate how good
    the chosen action was, and inform the actor how to improve it.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: DDPGç®—æ³•æ˜¯ä¸€ç§Actor-Criticç®—æ³•ï¼Œé¡¾åæ€ä¹‰ï¼Œå®ƒç”±ä¸¤ä¸ªç¥ç»ç½‘ç»œç»„æˆï¼š**Actor**å’Œ**Critic**ã€‚Actorè´Ÿè´£é€‰æ‹©æœ€ä½³åŠ¨ä½œï¼Œè€ŒCriticåˆ™å¿…é¡»è¯„ä¼°æ‰€é€‰åŠ¨ä½œçš„å¥½åï¼Œå¹¶å‘ŠçŸ¥Actorå¦‚ä½•æ”¹è¿›ã€‚
- en: The Actor is trained by applying policy gradient, while the Critic is trained
    by calculating the Q-Function. For this reason, the DDPG algorithm tries to learn
    an approximator to the optimal Q-function (Critic) and an approximator to the
    optimal policy (Actor) at the same time.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Actoré€šè¿‡åº”ç”¨ç­–ç•¥æ¢¯åº¦è¿›è¡Œè®­ç»ƒï¼Œè€ŒCriticé€šè¿‡è®¡ç®—Qå‡½æ•°è¿›è¡Œè®­ç»ƒã€‚å› æ­¤ï¼ŒDDPGç®—æ³•è¯•å›¾åŒæ—¶å­¦ä¹ æœ€ä¼˜Qå‡½æ•°çš„è¿‘ä¼¼å™¨ï¼ˆCriticï¼‰å’Œæœ€ä¼˜ç­–ç•¥çš„è¿‘ä¼¼å™¨ï¼ˆActorï¼‰ã€‚
- en: '![](../Images/21274c16474ed2ceea16e312ef11fa9e.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/21274c16474ed2ceea16e312ef11fa9e.png)'
- en: Actor-Critic schema. Image extracted from **[4]**
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Actor-Criticæ¨¡å¼ã€‚å›¾åƒæ‘˜è‡ª**[4]**
- en: The optimal Q-Function **Q*(s, a)** gives the expected return for being in state
    **s**, taking action **a**, and then acting following the optimal policy. On the
    other hand, the optimal policy **ğœ‡*(s)** returns the action which maximizes the
    expected return starting from state **s**. According to these two definitons,
    the optimal action on a given state (i.e. the return of the optimal policy on
    a given state) can be obtained by getting the argmax of Q*(s, a) for a given state,
    as shown below.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ä¼˜Qå‡½æ•°**Q*(s, a)**ç»™å‡ºäº†åœ¨çŠ¶æ€**s**ä¸‹ï¼Œé‡‡å–åŠ¨ä½œ**a**å¹¶éšåæŒ‰ç…§æœ€ä¼˜ç­–ç•¥è¡ŒåŠ¨çš„æœŸæœ›å›æŠ¥ã€‚å¦ä¸€æ–¹é¢ï¼Œæœ€ä¼˜ç­–ç•¥**ğœ‡*(s)**è¿”å›åœ¨çŠ¶æ€**s**ä¸‹æœ€å¤§åŒ–æœŸæœ›å›æŠ¥çš„åŠ¨ä½œã€‚æ ¹æ®è¿™ä¸¤ä¸ªå®šä¹‰ï¼Œç»™å®šçŠ¶æ€ä¸‹çš„æœ€ä¼˜åŠ¨ä½œï¼ˆå³æœ€ä¼˜ç­–ç•¥åœ¨ç»™å®šçŠ¶æ€ä¸‹çš„å›æŠ¥ï¼‰å¯ä»¥é€šè¿‡è·å–ç»™å®šçŠ¶æ€ä¸‹**Q*(s,
    a)**çš„argmaxæ¥è·å¾—ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚
- en: '![](../Images/fc7660bca9ebc13b1e78fe71ea874636.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc7660bca9ebc13b1e78fe71ea874636.png)'
- en: Q-Function â€” Policy relation. Image by author
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Qå‡½æ•°ä¸ç­–ç•¥çš„å…³ç³»ã€‚ä½œè€…æä¾›çš„å›¾åƒ
- en: The problem is that, for continuous action spaces, obtaining the action **a**
    which maximizes Q is not easy, because it would be impossible to calculate Q for
    every possible value of **a** to check which result is the highest (which is the
    solution for discrete action spaces), since it would have infinite possible values.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜åœ¨äºï¼Œå¯¹äºè¿ç»­åŠ¨ä½œç©ºé—´ï¼Œè·å–æœ€å¤§åŒ–Qçš„åŠ¨ä½œ**a**å¹¶ä¸å®¹æ˜“ï¼Œå› ä¸ºè®¡ç®—æ¯ä¸€ä¸ªå¯èƒ½çš„**a**çš„Qå€¼ä»¥æ£€æŸ¥å“ªä¸ªç»“æœæœ€é«˜ï¼ˆè¿™æ˜¯ç¦»æ•£åŠ¨ä½œç©ºé—´çš„è§£å†³æ–¹æ¡ˆï¼‰å‡ ä¹æ˜¯ä¸å¯èƒ½çš„ï¼Œå› ä¸ºå¯èƒ½çš„å€¼æ˜¯æ— é™çš„ã€‚
- en: As a solution to this, and assuming that the action space is continuous and
    that the Q-Function is differentiable with respect to the action, the DDPG algorithm
    approximates the calculation of ***maxQ(s, a)*** to ***Q(s, ğœ‡(s))***, where ***ğœ‡(s)***(a
    deterministic policy) can be optimized performing gradient ascent.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºè§£å†³æ–¹æ¡ˆï¼Œå¹¶å‡è®¾åŠ¨ä½œç©ºé—´æ˜¯è¿ç»­çš„ä¸”Qå‡½æ•°å¯¹åŠ¨ä½œæ˜¯å¯å¾®çš„ï¼ŒDDPGç®—æ³•å°†***maxQ(s, a)***è¿‘ä¼¼ä¸º***Q(s, ğœ‡(s))***ï¼Œå…¶ä¸­***ğœ‡(s)***ï¼ˆä¸€ä¸ªç¡®å®šæ€§ç­–ç•¥ï¼‰å¯ä»¥é€šè¿‡æ‰§è¡Œæ¢¯åº¦ä¸Šå‡æ¥ä¼˜åŒ–ã€‚
- en: In simple terms, DDPG learns an approximator to the optimal Q-Function in order
    to obtain the action that maximises it. Since, as the action space is continuous,
    the result of the Q-Function cannot be obtained for every possible value of the
    action, DDPG also learns an approximator to the optimal policy, in order to directly
    obtain the action that maximises the Q-Function.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ç®€è€Œè¨€ä¹‹ï¼ŒDDPGå­¦ä¹ æœ€ä¼˜Qå‡½æ•°çš„è¿‘ä¼¼å™¨ï¼Œä»¥è·å¾—æœ€å¤§åŒ–è¯¥å‡½æ•°çš„åŠ¨ä½œã€‚ç”±äºåŠ¨ä½œç©ºé—´æ˜¯è¿ç»­çš„ï¼ŒQå‡½æ•°çš„ç»“æœä¸èƒ½é’ˆå¯¹æ¯ä¸€ä¸ªå¯èƒ½çš„åŠ¨ä½œå€¼æ¥è·å¾—ï¼ŒDDPGè¿˜å­¦ä¹ æœ€ä¼˜ç­–ç•¥çš„è¿‘ä¼¼å™¨ï¼Œä»¥ç›´æ¥è·å¾—æœ€å¤§åŒ–Qå‡½æ•°çš„åŠ¨ä½œã€‚
- en: The following sections explain how the algorithm learns both the approximator
    to the optimal Q-Function and the approximator to the optimal policy.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥çš„éƒ¨åˆ†è§£é‡Šäº†ç®—æ³•å¦‚ä½•å­¦ä¹ æœ€ä¼˜Qå‡½æ•°çš„è¿‘ä¼¼å™¨å’Œæœ€ä¼˜ç­–ç•¥çš„è¿‘ä¼¼å™¨ã€‚
- en: Learning of the Q-Function
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Qå‡½æ•°çš„å­¦ä¹ 
- en: Mean-Squared Bellman Error Function
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‡æ–¹è´å°”æ›¼è¯¯å·®å‡½æ•°
- en: The learning of the Q-Function is carried out using as base the Bellman equation,
    previously introduced in [the first article of this series](https://medium.com/towards-data-science/applied-reinforcement-learning-i-q-learning-d6086c1f437).
    As in the DDPG algorithm the Q-Function is not calculated directly, but a neural
    network denoted ***QÏ•(s, a)*** is used as an approximator of the Q-Function, instead
    of the Bellman equation a loss function called **Mean Squared Bellman Error (MSBE)**
    is used. This function, shown in *Figure 1*, indicates how well the approximator
    *QÏ•(s, a)*satisfies the Bellman equation.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Q-å‡½æ•°çš„å­¦ä¹ æ˜¯åŸºäºè´å°”æ›¼æ–¹ç¨‹çš„ï¼Œè¯¥æ–¹ç¨‹åœ¨[æœ¬ç³»åˆ—çš„ç¬¬ä¸€ç¯‡æ–‡ç« ](https://medium.com/towards-data-science/applied-reinforcement-learning-i-q-learning-d6086c1f437)ä¸­å·²ä»‹ç»ã€‚ç”±äºåœ¨
    DDPG ç®—æ³•ä¸­ Q-å‡½æ•°ä¸æ˜¯ç›´æ¥è®¡ç®—çš„ï¼Œè€Œæ˜¯ä½¿ç”¨äº†ä¸€ä¸ªç¥ç»ç½‘ç»œ ***QÏ•(s, a)*** ä½œä¸º Q-å‡½æ•°çš„è¿‘ä¼¼å™¨ï¼Œå› æ­¤ä½¿ç”¨äº†ä¸€ä¸ªç§°ä¸º **å‡æ–¹è´å°”æ›¼è¯¯å·®
    (MSBE)** çš„æŸå¤±å‡½æ•°ã€‚è¯¥å‡½æ•°å¦‚ *å›¾ 1* æ‰€ç¤ºï¼ŒæŒ‡ç¤ºè¿‘ä¼¼å™¨ *QÏ•(s, a)* å¦‚ä½•æ»¡è¶³è´å°”æ›¼æ–¹ç¨‹ã€‚
- en: '![](../Images/4485e93d6884d97d4ec06e4543ff9ed7.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4485e93d6884d97d4ec06e4543ff9ed7.png)'
- en: '**Figure 1.** Mean-Squared Bellman Error (MSBE). Image extracted from **[5]**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 1.** å‡æ–¹è´å°”æ›¼è¯¯å·® (MSBE)ã€‚å›¾åƒæ‘˜è‡ª **[5]**'
- en: The goal of DDPG is to minimize this error function, which will cause the approximator
    to the Q-Function to satisfy Bellmanâ€™s equation, which implies that the approximator
    is optimal.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG çš„ç›®æ ‡æ˜¯æœ€å°åŒ–è¿™ä¸ªè¯¯å·®å‡½æ•°ï¼Œè¿™å°†ä½¿ Q-å‡½æ•°çš„è¿‘ä¼¼å™¨æ»¡è¶³è´å°”æ›¼æ–¹ç¨‹ï¼Œè¿™æ„å‘³ç€è¿‘ä¼¼å™¨æ˜¯æœ€ä¼˜çš„ã€‚
- en: Replay Buffer
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å›æ”¾ç¼“å†²åŒº
- en: 'The data required to minimize the MSBE function (i.e. to train the neural network
    to approximate Q*(s, a)), is extracted from a **Replay Buffer** where the experiences
    lived during the training are stored. This Replay Buffer is represented in *Figure
    1* as ***D***, from which the data required for calculating the loss is obtained:
    state **s**, action **a**, reward **r**, next state **sâ€™** and done **d**. If
    you are not familiar with the Replay Buffer, it was explained in the articles
    about the [DQN algorithm](https://medium.com/towards-data-science/applied-reinforcement-learning-iii-deep-q-networks-dqn-8f0e38196ba9)
    and [NAF algorithm](https://medium.com/towards-data-science/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095),
    and implemented and applied in the article about the [implementation of DQN](https://medium.com/towards-data-science/applied-reinforcement-learning-iv-implementation-of-dqn-7a9cb2c12f97).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æœ€å°åŒ– MSBE å‡½æ•°ï¼ˆå³è®­ç»ƒç¥ç»ç½‘ç»œä»¥è¿‘ä¼¼ Q*(s, a)ï¼‰ï¼Œæ‰€éœ€çš„æ•°æ®æ˜¯ä» **å›æ”¾ç¼“å†²åŒº** ä¸­æå–çš„ï¼Œè¯¥ç¼“å†²åŒºå­˜å‚¨äº†è®­ç»ƒè¿‡ç¨‹ä¸­ç»å†çš„ç»éªŒã€‚è¿™ä¸ªå›æ”¾ç¼“å†²åŒºåœ¨
    *å›¾ 1* ä¸­è¡¨ç¤ºä¸º ***D***ï¼Œä»ä¸­è·å–è®¡ç®—æŸå¤±æ‰€éœ€çš„æ•°æ®ï¼šçŠ¶æ€ **s**ï¼ŒåŠ¨ä½œ **a**ï¼Œå¥–åŠ± **r**ï¼Œä¸‹ä¸€ä¸ªçŠ¶æ€ **sâ€™** å’Œå®Œæˆ **d**ã€‚å¦‚æœä½ å¯¹å›æ”¾ç¼“å†²åŒºä¸ç†Ÿæ‚‰ï¼Œå®ƒåœ¨å…³äº
    [DQN ç®—æ³•](https://medium.com/towards-data-science/applied-reinforcement-learning-iii-deep-q-networks-dqn-8f0e38196ba9)
    å’Œ [NAF ç®—æ³•](https://medium.com/towards-data-science/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095)
    çš„æ–‡ç« ä¸­å·²æœ‰è§£é‡Šï¼Œå¹¶åœ¨å…³äº [DQN å®ç°](https://medium.com/towards-data-science/applied-reinforcement-learning-iv-implementation-of-dqn-7a9cb2c12f97)
    çš„æ–‡ç« ä¸­è¿›è¡Œäº†å®ç°å’Œåº”ç”¨ã€‚
- en: Target Neural Network
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç›®æ ‡ç¥ç»ç½‘ç»œ
- en: 'The minimization of the MSBE function consists of making the approximator to
    the Q-Function, *QÏ•(s, a)*, as close as possible to the other term of the function,
    **the target**, which originally has the following form:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: MSBE å‡½æ•°çš„æœ€å°åŒ–åŒ…æ‹¬ä½¿ Q-å‡½æ•°çš„è¿‘ä¼¼å™¨ï¼Œ*QÏ•(s, a)*ï¼Œå°½å¯èƒ½æ¥è¿‘å‡½æ•°çš„å¦ä¸€ä¸ªé¡¹ï¼Œå³ **ç›®æ ‡**ï¼Œå…¶åŸå§‹å½¢å¼å¦‚ä¸‹ï¼š
- en: '![](../Images/d58029862efbecaec31321f03282b10c.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d58029862efbecaec31321f03282b10c.png)'
- en: '**Figure 2.** Target. Extracted from Figure 1 **[5]**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 2.** ç›®æ ‡ã€‚æ‘˜è‡ªå›¾ 1 **[5]**'
- en: As can be seen, the target depends on the same parameters to be optimized, **Ï•**,
    which makes the minimization unstable. Therefore, as a solution another neural
    network is used, containing the parameters of the main neural network but with
    a certain delay. This second neural network is called **target network, *QÏ•targ(s,
    a)***(see *Figure 3*)**,** and its parameters are denoted ***Ï•targ***.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å›¾æ‰€ç¤ºï¼Œç›®æ ‡ä¾èµ–äºå¾…ä¼˜åŒ–çš„ç›¸åŒå‚æ•°ï¼Œ**Ï•**ï¼Œè¿™ä½¿å¾—æœ€å°åŒ–å˜å¾—ä¸ç¨³å®šã€‚å› æ­¤ï¼Œä½œä¸ºè§£å†³æ–¹æ¡ˆï¼Œä½¿ç”¨äº†å¦ä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œå…¶ä¸­åŒ…å«ä¸»ç¥ç»ç½‘ç»œçš„å‚æ•°ï¼Œä½†æœ‰ä¸€å®šçš„å»¶è¿Ÿã€‚è¿™ä¸ªç¬¬äºŒä¸ªç¥ç»ç½‘ç»œè¢«ç§°ä¸º
    **ç›®æ ‡ç½‘ç»œï¼Œ*QÏ•targ(s, a)*ï¼ˆè§*å›¾ 3*ï¼‰**ï¼Œå…¶å‚æ•°è¡¨ç¤ºä¸º ***Ï•targ***ã€‚
- en: However, in *Figure 2* it can be seen how, when substituting *QÏ•(s, a)* for
    *QÏ•targ(s, a)*, it is necessary to obtain the action that maximizes the output
    of this target network, which, as explained above, is complicated for continuous
    action space environments. This is solved by making use of a **target policy network,
    *ğœ‡Ï•targ(s)*** (see *Figure 3*), which approximates the action that maximizes the
    output of the target network. In other words, a target policy network *ğœ‡Ï•targ(s)*has
    been created to solve the problem of continuous actions for *QÏ•targ(s, a)*, just
    as was done with *ğœ‡Ï•(s)* and *QÏ•(s, a)*.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œåœ¨*å›¾ 2*ä¸­å¯ä»¥çœ‹åˆ°ï¼Œå½“å°† *QÏ•(s, a)* æ›¿æ¢ä¸º *QÏ•targ(s, a)* æ—¶ï¼Œå¿…é¡»è·å¾—æœ€å¤§åŒ–è¯¥ç›®æ ‡ç½‘ç»œè¾“å‡ºçš„åŠ¨ä½œã€‚æ­£å¦‚ä¸Šæ–‡æ‰€è¿°ï¼Œè¿™å¯¹äºè¿ç»­åŠ¨ä½œç©ºé—´ç¯å¢ƒæ¥è¯´æ˜¯å¤æ‚çš„ã€‚è¿™é€šè¿‡åˆ©ç”¨**ç›®æ ‡ç­–ç•¥ç½‘ç»œï¼Œ*ğœ‡Ï•targ(s)*ï¼ˆè§*å›¾
    3*ï¼‰**æ¥è§£å†³ï¼Œè¯¥ç½‘ç»œé€¼è¿‘äº†æœ€å¤§åŒ–ç›®æ ‡ç½‘ç»œè¾“å‡ºçš„åŠ¨ä½œã€‚æ¢å¥è¯è¯´ï¼Œåˆ›å»ºäº†ä¸€ä¸ªç›®æ ‡ç­–ç•¥ç½‘ç»œ *ğœ‡Ï•targ(s)* æ¥è§£å†³ *QÏ•targ(s, a)* çš„è¿ç»­åŠ¨ä½œé—®é¢˜ï¼Œå°±åƒä¹‹å‰å¯¹
    *ğœ‡Ï•(s)* å’Œ *QÏ•(s, a)* æ‰€åšçš„é‚£æ ·ã€‚
- en: Minimize the modified MSBE Function
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æœ€å°åŒ–ä¿®æ”¹åçš„ MSBE å‡½æ•°
- en: With all this, the learning of the optimal Q-Function by the DDPG algorithm
    is carried out minimizing the modified MSBE function in *Figure 3*, by applying
    gradient descent on it.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è¿™ä¸€åˆ‡ï¼ŒDDPG ç®—æ³•é€šè¿‡å¯¹*å›¾ 3*ä¸­çš„ä¿®æ”¹åçš„ MSBE å‡½æ•°åº”ç”¨æ¢¯åº¦ä¸‹é™æ¥å­¦ä¹ æœ€ä¼˜ Q-å‡½æ•°ã€‚
- en: '![](../Images/fc9ef5c014b0479ce017276e9bd2589c.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc9ef5c014b0479ce017276e9bd2589c.png)'
- en: '**Figure 3\.** Modified Mean-Squared Bellman Error. Extracted from **[5]**
    and edited by author'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 3\.** ä¿®æ”¹åçš„å‡æ–¹è´å°”æ›¼è¯¯å·®ã€‚æ‘˜è‡ª**[5]**å¹¶ç”±ä½œè€…ç¼–è¾‘'
- en: Learning of the Policy
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç­–ç•¥å­¦ä¹ 
- en: 'Given that the action space is continuous, and that the Q-Function is differentiable
    with respect to the action, DDPG learns the deterministic policy ***ğœ‡Ï•(s)*** that
    maximizes *QÏ•(s, a)* by applying gradient ascent on the function below with respect
    to the deterministic policyâ€™s parameters:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºåŠ¨ä½œç©ºé—´æ˜¯è¿ç»­çš„ï¼Œå¹¶ä¸” Q-å‡½æ•°å¯¹åŠ¨ä½œæ˜¯å¯å¾®åˆ†çš„ï¼ŒDDPG é€šè¿‡å¯¹ä»¥ä¸‹å‡½æ•°åº”ç”¨æ¢¯åº¦ä¸Šå‡æ¥å­¦ä¹ æœ€å¤§åŒ– *QÏ•(s, a)* çš„ç¡®å®šæ€§ç­–ç•¥***ğœ‡Ï•(s)***ï¼Œè¯¥å‡½æ•°æ˜¯å…³äºç¡®å®šæ€§ç­–ç•¥å‚æ•°çš„ï¼š
- en: '![](../Images/a363a03aeb9fe79da15bf022dff83a3a.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a363a03aeb9fe79da15bf022dff83a3a.png)'
- en: '**Figure 4.** Function to optimize for the deterministic policy learning. Extracted
    from **[5]**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 4.** ç¡®å®šæ€§ç­–ç•¥å­¦ä¹ çš„ä¼˜åŒ–å‡½æ•°ã€‚æ‘˜è‡ª**[5]**'
- en: DDPG Algorithm Flow
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DDPG ç®—æ³•æµç¨‹
- en: The flow of the DDPG algorithm will be presented following the pseudocode below,
    extracted from **[1]**. The DDPG algorithm follows the same steps as other Q-Learning
    algorithms for function approximators, such as the DQN or NAF algorithm.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG ç®—æ³•çš„æµç¨‹å°†æŒ‰ç…§ä»¥ä¸‹ä¼ªä»£ç å±•ç¤ºï¼Œæ‘˜è‡ª**[1]**ã€‚DDPG ç®—æ³•éµå¾ªä¸å…¶ä»–å‡½æ•°é€¼è¿‘ Q-Learning ç®—æ³•ç›¸åŒçš„æ­¥éª¤ï¼Œä¾‹å¦‚ DQN æˆ–
    NAF ç®—æ³•ã€‚
- en: '![](../Images/75b55a12daffd419a15512867063bdfc.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75b55a12daffd419a15512867063bdfc.png)'
- en: DDPG Algorithm Pseudocode. Extracted from **[1]**
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG ç®—æ³•ä¼ªä»£ç ã€‚æ‘˜è‡ª**[1]**
- en: 1\. Initialize Critic, Critic Target, Actor and Actor Target networks
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. åˆå§‹åŒ– Criticã€Critic ç›®æ ‡ã€Actor å’Œ Actor ç›®æ ‡ç½‘ç»œ
- en: Initialize the Actor and Critic neural networks to be used during training.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: åˆå§‹åŒ–åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨çš„ Actor å’Œ Critic ç¥ç»ç½‘ç»œã€‚
- en: The Critic network, *QÏ•(s, a)*, acts as an approximator to the Q-Function.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Critic ç½‘ç»œï¼Œ*QÏ•(s, a)*ï¼Œä½œä¸º Q-å‡½æ•°çš„é€¼è¿‘å™¨ã€‚
- en: The Actor network, *ğœ‡Ï•(s),* acts as an approximator to the deterministic policy
    and is used to obtain the action that maximizes the output of the Critic network.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Actor ç½‘ç»œï¼Œ*ğœ‡Ï•(s)*ï¼Œä½œä¸ºç¡®å®šæ€§ç­–ç•¥çš„é€¼è¿‘å™¨ï¼Œç”¨äºè·å–æœ€å¤§åŒ– Critic ç½‘ç»œè¾“å‡ºçš„åŠ¨ä½œã€‚
- en: Once initialized, the target networks are initialized with the same architecture
    as their corresponding main networks, and the weights of the main networks are
    copied into the target networks.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦åˆå§‹åŒ–ï¼Œç›®æ ‡ç½‘ç»œå°†ä¸å…¶ç›¸åº”çš„ä¸»ç½‘ç»œå…·æœ‰ç›¸åŒçš„æ¶æ„ï¼Œå¹¶ä¸”ä¸»ç½‘ç»œçš„æƒé‡ä¼šè¢«å¤åˆ¶åˆ°ç›®æ ‡ç½‘ç»œä¸­ã€‚
- en: The Critic Target network, *QÏ•targ(s, a),* acts as a delayed Critic network,
    so that the target does not depend on the same parameters to be optimized, as
    explained before.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Critic ç›®æ ‡ç½‘ç»œï¼Œ*QÏ•targ(s, a)*ï¼Œä½œä¸ºä¸€ä¸ªå»¶è¿Ÿçš„ Critic ç½‘ç»œï¼Œä»¥ä¾¿ç›®æ ‡ä¸ä¾èµ–äºéœ€è¦ä¼˜åŒ–çš„ç›¸åŒå‚æ•°ï¼Œæ­£å¦‚ä¹‹å‰æ‰€è§£é‡Šçš„ã€‚
- en: The Actor Target network, *ğœ‡Ï•targ(s),* acts as a delayed Actor network, and
    it used to obtain the action that maximizes the output of the Critic Target network.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Actor ç›®æ ‡ç½‘ç»œï¼Œ*ğœ‡Ï•targ(s)*ï¼Œä½œä¸ºä¸€ä¸ªå»¶è¿Ÿçš„ Actor ç½‘ç»œï¼Œç”¨äºè·å–æœ€å¤§åŒ– Critic ç›®æ ‡ç½‘ç»œè¾“å‡ºçš„åŠ¨ä½œã€‚
- en: 2\. Initialize Replay Buffer
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. åˆå§‹åŒ– Replay Buffer
- en: The Replay Buffer to be used for the training is initialized empty.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºè®­ç»ƒçš„ Replay Buffer è¢«åˆå§‹åŒ–ä¸ºç©ºã€‚
- en: '*For each timestep in an episode, the agent performs the following steps:*'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*åœ¨ä¸€ä¸ªæ—¶é—´æ­¥ä¸­ï¼Œæ™ºèƒ½ä½“æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š*'
- en: 3\. Select action and apply noise
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. é€‰æ‹©åŠ¨ä½œå¹¶æ–½åŠ å™ªå£°
- en: The best action for the current state is obtained from the output of the Actor
    neural network, which approximates the deterministic policy *ğœ‡Ï•(s).* The noise
    extracted from a **Ornstein Uhlenbeck Noise** process **[6]**, or from an **uncorrelated,
    mean-zero Gaussian distribution** **[7]** is then applied to the selected action.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ä» Actor ç¥ç»ç½‘ç»œçš„è¾“å‡ºä¸­è·å¾—å½“å‰çŠ¶æ€çš„æœ€ä½³åŠ¨ä½œï¼Œè¯¥ç½‘ç»œé€¼è¿‘ç¡®å®šæ€§ç­–ç•¥ *ğœ‡Ï•(s).* ä»**Ornstein Uhlenbeck å™ªå£°**è¿‡ç¨‹**[6]**æˆ–**æ— å…³çš„ã€å‡å€¼ä¸ºé›¶çš„é«˜æ–¯åˆ†å¸ƒ**
    **[7]** ä¸­æå–çš„å™ªå£°ç„¶åè¢«åº”ç”¨äºé€‰å®šçš„åŠ¨ä½œã€‚
- en: 4\. Perform the action and store observations in Replay Buffer
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. æ‰§è¡ŒåŠ¨ä½œå¹¶å°†è§‚å¯Ÿç»“æœå­˜å‚¨åœ¨ Replay Buffer ä¸­
- en: The noisy action is performed in the environment. After that, the environment
    returns a **reward** indicating how good the action taken was, the **new state**
    reached after performing the action, and a boolean indicating whether a **terminal
    state** has been reached.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¯å¢ƒä¸­æ‰§è¡Œæœ‰å™ªå£°çš„åŠ¨ä½œã€‚ä¹‹åï¼Œç¯å¢ƒè¿”å›ä¸€ä¸ª**å¥–åŠ±**ï¼ŒæŒ‡ç¤ºé‡‡å–çš„è¡ŒåŠ¨çš„æ•ˆæœï¼Œæ‰§è¡Œè¯¥åŠ¨ä½œåè¾¾åˆ°çš„**æ–°çŠ¶æ€**ï¼Œä»¥åŠä¸€ä¸ªå¸ƒå°”å€¼ï¼ŒæŒ‡ç¤ºæ˜¯å¦å·²è¾¾åˆ°**ç»ˆæ­¢çŠ¶æ€**ã€‚
- en: This information, together with the **current state** and the **action taken**,
    is stored in the Replay Buffer, to be used later to optimize the Critic and Actor
    neural networks.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ä¿¡æ¯è¿åŒ**å½“å‰çŠ¶æ€**å’Œ**é‡‡å–çš„è¡ŒåŠ¨**ä¸€èµ·å­˜å‚¨åœ¨ Replay Buffer ä¸­ï¼Œç¨åç”¨äºä¼˜åŒ– Critic å’Œ Actor ç¥ç»ç½‘ç»œã€‚
- en: 5\. Sample batch of experiences and train Actor and Critic networks
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. é‡‡æ ·ç»éªŒæ‰¹æ¬¡ï¼Œå¹¶è®­ç»ƒ Actor å’Œ Critic ç½‘ç»œ
- en: This step is only performed when the Replay Buffer has enough experiences to
    fill a batch. Once this requirement is met, a batch of experiences is extracted
    from the Replay Buffer for use in training.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: åªæœ‰å½“ Replay Buffer ä¸­æœ‰è¶³å¤Ÿçš„ç»éªŒå¡«æ»¡ä¸€ä¸ªæ‰¹æ¬¡æ—¶ï¼Œè¿™ä¸€æ­¥éª¤æ‰ä¼šæ‰§è¡Œã€‚ä¸€æ—¦æ»¡è¶³æ­¤è¦æ±‚ï¼Œå°±ä¼šä» Replay Buffer ä¸­æå–ä¸€ä¸ªç»éªŒæ‰¹æ¬¡ç”¨äºè®­ç»ƒã€‚
- en: 'With this batch of experiences:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¿™ä¸€æ‰¹ç»éªŒï¼š
- en: The target is calculated and the output of the Critic network (the approximator
    of the Q-Function) is obtained, in order to then apply gradient descent on the
    MSBE error function, as shown in *Figure 3*. This step trains/optimizes the approximator
    to the Q-Function, *QÏ•(s, a).*
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®¡ç®—ç›®æ ‡å€¼ï¼Œå¹¶è·å¾— Critic ç½‘ç»œï¼ˆQ-Function çš„é€¼è¿‘å™¨ï¼‰çš„è¾“å‡ºï¼Œç„¶ååœ¨ MSBE è¯¯å·®å‡½æ•°ä¸Šåº”ç”¨æ¢¯åº¦ä¸‹é™ï¼Œå¦‚*å›¾ 3*æ‰€ç¤ºã€‚è¿™ä¸€æ­¥éª¤è®­ç»ƒ/ä¼˜åŒ–äº†
    Q-Function çš„é€¼è¿‘å™¨ï¼Œ*QÏ•(s, a)ã€‚*
- en: Gradient ascent is performed on the function shown in *Figure 4*, thus optimizing/training
    the approximator to the deterministic policy, *ğœ‡Ï•(s)*.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹*å›¾ 4*æ‰€ç¤ºçš„å‡½æ•°æ‰§è¡Œæ¢¯åº¦ä¸Šå‡ï¼Œä»è€Œä¼˜åŒ–/è®­ç»ƒç¡®å®šæ€§ç­–ç•¥çš„é€¼è¿‘å™¨ï¼Œ*ğœ‡Ï•(s)*ã€‚
- en: 6\. Softly update the Target networks
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
- en: Both the Actor Target and the Critic Target networks are updated every time
    the Actor and Critic networks are updated, by **polyak averaging**, as shown in
    the figure below.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯æ¬¡æ›´æ–° Actor å’Œ Critic ç½‘ç»œæ—¶ï¼ŒActor Target å’Œ Critic Target ç½‘ç»œéƒ½é€šè¿‡**Polyak å¹³å‡**è¿›è¡Œæ›´æ–°ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚
- en: '![](../Images/3a004bfe6f1fb9758bf9750fa17bf710.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a004bfe6f1fb9758bf9750fa17bf710.png)'
- en: '**Figure 5.** Polyak Averaging. Extracted from **[1]**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾ 5.** Polyak å¹³å‡ã€‚æ‘˜è‡ª**[1]**'
- en: '**Tau *Ï„***, the parameter that sets the weights of each element in the polyak
    averaging, is a hyperparameter to be set for the algorithm, which usually takes
    values close to 1.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**Tau *Ï„***ï¼Œè®¾ç½® Polyak å¹³å‡ä¸­æ¯ä¸ªå…ƒç´ æƒé‡çš„å‚æ•°ï¼Œæ˜¯ç®—æ³•çš„ä¸€ä¸ªè¶…å‚æ•°ï¼Œé€šå¸¸å–æ¥è¿‘ 1 çš„å€¼ã€‚'
- en: Conclusion
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: The DDPG algorithm proposed by Lillicrap et al. achieves very good results in
    most continuous environments available in Gym as shown in the paper that presented
    it **[1]**, demonstrating its ability to learn different tasks, regardless of
    their complexity, in a continuous context.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Lillicrap ç­‰äººæå‡ºçš„ DDPG ç®—æ³•åœ¨ Gym ä¸­å¤§å¤šæ•°è¿ç»­ç¯å¢ƒä¸‹å–å¾—äº†éå¸¸å¥½çš„ç»“æœï¼Œå¦‚è®ºæ–‡**[1]**æ‰€ç¤ºï¼Œå±•ç¤ºäº†å®ƒåœ¨è¿ç»­ç¯å¢ƒä¸­å­¦ä¹ ä¸åŒä»»åŠ¡çš„èƒ½åŠ›ï¼Œæ— è®ºè¿™äº›ä»»åŠ¡çš„å¤æ‚æ€§å¦‚ä½•ã€‚
- en: Therefore, this algorithm is still used today to enable an agent to learn an
    optimal policy for a complex task in a continuous environment, such as control
    tasks for manipulator robots, or obstacle avoidance for autonomous vehicles.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œè¯¥ç®—æ³•è‡³ä»Šä»ç”¨äºä½¿æ™ºèƒ½ä½“åœ¨è¿ç»­ç¯å¢ƒä¸­å­¦ä¹ å¤æ‚ä»»åŠ¡çš„æœ€ä¼˜ç­–ç•¥ï¼Œå¦‚æ“ä½œæœºå™¨äººæ§åˆ¶ä»»åŠ¡æˆ–è‡ªä¸»è½¦è¾†çš„é¿éšœä»»åŠ¡ã€‚
- en: REFERENCES
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '**[1]** LILLICRAP, Timothy P., et al. Continuous control with deep reinforcement
    learning. *arXiv preprint arXiv:1509.02971*, 2015.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**[1]** LILLICRAP, Timothy P., ç­‰äººã€‚ä½¿ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ è¿›è¡Œè¿ç»­æ§åˆ¶ã€‚*arXiv é¢„å°æœ¬ arXiv:1509.02971*ï¼Œ2015ã€‚'
- en: '**[2]** MNIH, Volodymyr, et al. Playing atari with deep reinforcement learning.
    *arXiv preprint arXiv:1312.5602*, 2013.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**[2]** MNIH, Volodymyr, ç­‰äººã€‚ä½¿ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ç© Atari æ¸¸æˆã€‚*arXiv é¢„å°æœ¬ arXiv:1312.5602*ï¼Œ2013ã€‚'
- en: '**[3]** GU, Shixiang, et al. Continuous deep q-learning with model-based acceleration.
    En *International conference on machine learning*. PMLR, 2016\. p. 2829â€“2838.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**[3]** GU, Shixiang ç­‰äººã€‚ã€ŠåŸºäºæ¨¡å‹çš„åŠ é€Ÿè¿ç»­æ·±åº¦ Q å­¦ä¹ ã€‹ã€‚å‘è¡¨äº*å›½é™…æœºå™¨å­¦ä¹ å¤§ä¼š*ã€‚PMLRï¼Œ2016å¹´ã€‚ç¬¬ 2829â€“2838
    é¡µã€‚'
- en: '**[4]** SUTTON, Richard S.; BARTO, Andrew G. *Reinforcement learning: An introduction*.
    MIT press, 2018.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**[4]** SUTTON, Richard S.; BARTO, Andrew G. *ã€Šå¼ºåŒ–å­¦ä¹ ï¼šå¯¼è®ºã€‹*ã€‚MITå‡ºç‰ˆç¤¾ï¼Œ2018å¹´ã€‚'
- en: '**[5]** *OpenAI Spinning Up â€” Deep Deterministic Policy Gradient*[https://spinningup.openai.com/en/latest/algorithms/ddpg.html](https://spinningup.openai.com/en/latest/algorithms/ddpg.html)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**[5]** *OpenAI Spinning Up â€” æ·±åº¦ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦*[https://spinningup.openai.com/en/latest/algorithms/ddpg.html](https://spinningup.openai.com/en/latest/algorithms/ddpg.html)'
- en: '**[6]** Uhlenbeck-Ornstein process[https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**[6]** ä¹Œä¼¦è´å…‹-å¥¥æ©æ–¯å¦è¿‡ç¨‹[https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process)'
- en: '**[7]** Normal / Gaussian Distribution'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**[7]** æ­£æ€ / é«˜æ–¯åˆ†å¸ƒ'
- en: '[https://en.wikipedia.org/wiki/Normal_distribution](https://en.wikipedia.org/wiki/Normal_distribution)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Normal_distribution](https://en.wikipedia.org/wiki/Normal_distribution)'
