- en: Big Data File Formats, Explained
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://towardsdatascience.com/big-data-file-formats-explained-275876dc1fc9](https://towardsdatascience.com/big-data-file-formats-explained-275876dc1fc9)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Parquet vs ORC vs AVRO vs JSON. Which one to choose and how to use them?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://mshakhomirov.medium.com/?source=post_page-----275876dc1fc9--------------------------------)[![ðŸ’¡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----275876dc1fc9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----275876dc1fc9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----275876dc1fc9--------------------------------)
    [ðŸ’¡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----275876dc1fc9--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----275876dc1fc9--------------------------------)
    Â·9 min readÂ·Feb 28, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ce5abf8987056d730cd51c36eda44998.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [James Lee](https://unsplash.com/@picsbyjameslee?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Iâ€™m a big fan of data warehouse (DWH) solutions with ELT-designed (Extract-Load-Transform)
    data pipelines. However, at some point, I faced the requirement to *process* raw
    event data in ***Cloud Storage*** and had **to choose the file format** for data
    files.
  prefs: []
  type: TYPE_NORMAL
- en: '*This is a typical scenario when machine learning engineers are tasked to create
    behavior datasets to train models and to generate better product recommendations
    or predict customer churn.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Choosing the right file format for our machine learning pipelines was crucial
    as it might have changed data I/O times significantly and would have definitely
    had a wider impact on our model trainer performance.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://pub.towardsai.net/supercharge-your-data-engineering-skills-with-this-machine-learning-pipeline-b69d159780b7?source=post_page-----275876dc1fc9--------------------------------)
    [## Supercharge Your Data Engineering Skills with This Machine Learning Pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: Data modeling, Python, DAGs, Big Data file formats, costsâ€¦ It covers everything
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: pub.towardsai.net](https://pub.towardsai.net/supercharge-your-data-engineering-skills-with-this-machine-learning-pipeline-b69d159780b7?source=post_page-----275876dc1fc9--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Another thing to consider was the size of the data as we were paying already
    too much for the file storage.
  prefs: []
  type: TYPE_NORMAL
- en: This story aims to consider these important questions and other options to find
    the optimal big data file format for the data pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: '*The requirement was simple, emphasized the idea of using the datalake and
    reducing the costs related to data storage in the data warehouse.*'
  prefs: []
  type: TYPE_NORMAL
- en: I created an **archive** bucket that would be the cheapest storage class and
    prepared to *extract* the data from some really big tables I had in my DWH. I
    have to say, those tables were heavy and had a lot of raw events with user engagement
    data. Therefore, they were the most expensive storage part of the whole data platform.
  prefs: []
  type: TYPE_NORMAL
- en: And then I had to stop at some point to make a decision on which file format
    to use because storage size wasnâ€™t the only consideration.
  prefs: []
  type: TYPE_NORMAL
- en: '*Indeed, in some scenarios read/write time or better schema support might be
    more important*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Parquet vs ORC vs AVRO vs JSON
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the rise of Data Mesh and a considerable number of data processing tools
    available in the **Hadoop** eco-system, it might be more effective **to process
    raw event data in the data lake**.
  prefs: []
  type: TYPE_NORMAL
- en: '*Modern data warehouse solutions are great but some* ***pricing models*** *are
    more expensive than the others and definitely more expensive than datalake tools*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: One of the **greatest benefits** of big data **file formats** is that they carry
    schema information on board. Therefore, it is much easier to load data in, split
    the data to process it more efficiently, etc. JSON canâ€™t offer that feature and
    we would want to define the schema each time we read, load or validate the data.
  prefs: []
  type: TYPE_NORMAL
- en: Columnar ORC and Parquet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Parquet** is a ***column-oriented*** data storage format designed for the
    *Apache Hadoop* ecosystem (backed by Cloudera, in collaboration with Twitter).
    It is very popular among data scientists and data engineers working with **Spark**.
    When working with huge amounts of data, you start to notice the major advantage
    of columnar data, which is achieved when a table has many more rows than columns.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/45fc6ec974c1bc982924ea6d8d9a1976.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Parquet File Layout. Source: [apache.org](http://apache.org) (Apache 2.0 license)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Spark scales well and thatâ€™s why everybody likes it*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In fact, Parquet is a default data file format for Spark. **Parquet** performs
    beautifully while querying and working with analytical workloads.
  prefs: []
  type: TYPE_NORMAL
- en: '*Columnar formats are more suitable for OLAP analytical queries. Specifically,
    we would want to retrieve only the column we need to perform an analytical query.
    That definitely has certain implications on memory and therefore, on performance
    and speed.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**ORC (Optimised Row Columnar)** is also a column-oriented data storage format
    similar to Parquet which carries a schema on board. it means that like Parquet
    it is **self-describing** and we can use it to load data into different disks
    or nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b7301da75334cf83797ce798485045a3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'ORC file layout. Source: [apache.org](http://apache.org) (Apache 2.0 license)'
  prefs: []
  type: TYPE_NORMAL
- en: I did a little test and it seems that both **Parquet and ORC** offer similar
    compression ratios. However, there is an opinion that ORC is more compression
    efficient.
  prefs: []
  type: TYPE_NORMAL
- en: '*10 Mb compressed with* ***SNAPPY*** *algorithm will turn into 2.4Mb in* ***Parquet***'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: However, I have a feeling that **ORC** is supported by a smaller number of Hadoop
    projects than Parquet, i.e. Hive and Pig.
  prefs: []
  type: TYPE_NORMAL
- en: So if we want a wider range of tools to run our OLAP analytics in the data lake
    I would recommend using **Parquet**. It has wider project support and especially
    **Spark**.
  prefs: []
  type: TYPE_NORMAL
- en: '*In reality, PARQUET and ORC have somewhat different columnar architecture.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Both work really well saving you a great deal of disk space.
  prefs: []
  type: TYPE_NORMAL
- en: However, data in **ORC** files are organized into independent *stripes* of data.
    Each has its own separate index, row data, and footer. It enables large, efficient
    reads from HDFS, making this format.
  prefs: []
  type: TYPE_NORMAL
- en: The data is stored as pages in **Parquet**, and each page includes header information,
    definition level information, and repetition level information.
  prefs: []
  type: TYPE_NORMAL
- en: It is very effective when it comes to supporting a complicated **nested** data
    structure and seems to be more efficient at performing IO-type operations on data.
    The great advantage here is that **read time** can be significantly decreased
    if we choose only the columns we need.
  prefs: []
  type: TYPE_NORMAL
- en: My personal experience suggests that selecting just a few columns canâ€¦ read
    the data up to 30 times (!) faster than reading the same file with the complete
    schema.
  prefs: []
  type: TYPE_NORMAL
- en: Read the data 30 times faster (!)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Not bad, hah? Other tests performed by data engineers confirmed that too. Iâ€™ll
    add that link at the bottom.
  prefs: []
  type: TYPE_NORMAL
- en: '*Long story short, choose ORC if you work on HIVE. It is better optimized for
    it. And Choose Parquet if you work with Spark as that would be the default storage
    format for it.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In everything else, both ORC and Parquet share similar architecture and KPIs
    look roughly the same.
  prefs: []
  type: TYPE_NORMAL
- en: AVRO
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**AVRO** is a **row-based** storage format where data is indexed to improve
    query performance.'
  prefs: []
  type: TYPE_NORMAL
- en: It defines data types and schemas using JSON data and stores the data in a binary
    format (condensed) that help with disk space.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6535c558a34c0ed7ca5047e18c4ffd04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'avro file structure. Source: [apache.org](http://apache.org) (Apache 2.0 license)'
  prefs: []
  type: TYPE_NORMAL
- en: Compared to Parquet and ORC it seems that it offers less efficient compression
    but faster **write** speeds.
  prefs: []
  type: TYPE_NORMAL
- en: '*10 Mb* ***Parquet*** *compressed with* ***SNAPPY*** *algorithm will turn into
    2.4Mb*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*10 Mb* ***AVRO*** *compressed with* ***SNAPPY*** *algorithm will turn into
    6.2Mb*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The same 10 Mb* ***AVRO*** *compressed with* ***DEFLATE*** *algorithm will
    require 4Mb of storage*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main advantage of AVRO is a **schema-evolution support**. In other words,
    as data schemas evolve over time, ***AVRO*** enables those changes by tolerating
    fields that have been added, removed, or altered.
  prefs: []
  type: TYPE_NORMAL
- en: '*AVRO works really well with things like added fields and changed fields, missing
    fields.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: AVRO is more efficient while working with ***write-intensive***, big-data operations.
  prefs: []
  type: TYPE_NORMAL
- en: It is a great candidate to store data in `source` layer or a `landing` area
    of our data platform because these *f****iles are usually being read as a whole***
    for further transformation depending on the data pipeline. Any schema evolution
    changes are handled with ease.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----275876dc1fc9--------------------------------)
    [## Data pipeline design patterns'
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right architecture with examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/data-pipeline-design-patterns-100afa4b93e3?source=post_page-----275876dc1fc9--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Even applications written in different languages can share data saved using
    AVRO. Data exchange services might require a **coding generator** to decipher
    data specifications and generate code to access data. AVRO is an ideal contender
    for scripting languages because it [doesnâ€™t require this](https://avro.apache.org/docs/1.11.1/getting-started-python/#serializing-and-deserializing-without-code-generation).
  prefs: []
  type: TYPE_NORMAL
- en: JSON
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For online applications, structured data is frequently serialized and sent using
    the JavaScript Object Notation (JSON) format.
  prefs: []
  type: TYPE_NORMAL
- en: This implies that a significant portion of the big data is gathered and kept
    in a JSON format.
  prefs: []
  type: TYPE_NORMAL
- en: However, because JSON is not highly typed nor schema-enriched, dealing with
    JSON files in big data technologies like Hadoop may be sluggish.
  prefs: []
  type: TYPE_NORMAL
- en: Basically, itâ€™s a no-go for Big Data processing frameworks. This is the main
    reason Parquet and ORC formats were created.
  prefs: []
  type: TYPE_NORMAL
- en: Compression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*How does it work?*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A quick answer is to choose **splittable** compression types if our primary
    objective is to gain data processing performance, and non-splittable like **GZIP,
    DEFLATE** if storage costs deduction is our main objective.
  prefs: []
  type: TYPE_NORMAL
- en: '*What is splittable?*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '***Splittable*** means that Hadoop Mapper can split or divide a large file
    and process it in parallel. Compressing codecs apply compression in blocks and
    when it is applied at the block level mappers can read a single file concurrently
    even if it is a very large file.'
  prefs: []
  type: TYPE_NORMAL
- en: Having said that, *splittable* compression types are **LZO**, **LZ4**, **BZIP2**
    and **SNAPPY** whereas **GZIP** isnâ€™t. These are very fast compressors and very
    fast decompressors. So if we can tolerate bigger storage then it is a better choice
    than **GZIP**.
  prefs: []
  type: TYPE_NORMAL
- en: '**GZIP** on average has a 30% better compression rate and would be a good choice
    for data that doesnâ€™t require frequent access (cold storage).'
  prefs: []
  type: TYPE_NORMAL
- en: External data in lakes and Data warehouse
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Can we query external data storage using a data warehouse?*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Typically, all modern data warehouses offer externally partitioned table features
    so we can run analytics on data lake data.
  prefs: []
  type: TYPE_NORMAL
- en: There is a number of limitation of course but in general, it is enough to connect
    two worlds in order to understand how it works.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in BigQuery we can create an external table in our data warehouse
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You will notice that external tables are much slower in any DWH and are limited
    in many ways, i.e. limited concurrent queries, no DML and wildcard support and
    inconsistency if underlying data was changed during the query run.
  prefs: []
  type: TYPE_NORMAL
- en: 'So we might want to *load* it back like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Native tables are faster.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If there is another data processing tool, then we might want to use a Hive partition
    layout. This is usually required to enable external partitioning. i.e.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Here partition keys have to be always in the same order. And for partitioning,
    we will use `key = value` pairs which will be storage folders at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: We can use it in any other data processing framework that supports *HIVE* layout.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://mydataschool.com/blog/hive-partitioning-layout/?source=post_page-----275876dc1fc9--------------------------------)
    [## Hive Partitioning Layout'
  prefs: []
  type: TYPE_NORMAL
- en: Lakehouse design is one of my favorites because it gives us the best of two
    worlds. In a data warehouse, we can manageâ€¦
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: mydataschool.com](https://mydataschool.com/blog/hive-partitioning-layout/?source=post_page-----275876dc1fc9--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we need support for Spark and a flat columnar format with efficient querying
    of complex nested data structures then we should use Parquet. It is highly integrated
    with Apache Spark and is, actually, a default file format for this framework.
  prefs: []
  type: TYPE_NORMAL
- en: If our data platform architecture relies on data pipelines built with Hive or
    Pig then ORC data format is the better choice. Having all the advantages of columnar
    format it performs beautifully in analytical data lake queries but is better integrated
    with Hive and Pig. Therefore, the performance might be better compared to Parquet.
  prefs: []
  type: TYPE_NORMAL
- en: When we need a higher compression ratio and faster read times then Parquet or
    ORC would suit better. Compared to AVRO read times might be up to 3 times faster.
    It can be improved even further if we reduce the number of selected columns to
    read.
  prefs: []
  type: TYPE_NORMAL
- en: However, we donâ€™t worry to much about the speed of input operations AVRO might
    be a better choice offering a reasonable compression with splittable SNAPPY, schema
    evolution support and way faster write speed (it is row-based remember?). AVRO
    is an absolute leader in *write-intensive*, big-data operations. Therefore, it
    suits better for storing the data in the source layer of our data platform.
  prefs: []
  type: TYPE_NORMAL
- en: '`GZIP`, `DEFLATE` and other non-splittable compression types would work best
    for cold storage where frequent access to data is not required.'
  prefs: []
  type: TYPE_NORMAL
- en: Recommended read
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1\. https://cwiki.apache.org/confluence/display/hive/languagemanual+orc](https://cwiki.apache.org/confluence/display/hive/languagemanual+orc)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2\. https://parquet.apache.org/](https://parquet.apache.org/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3\. https://avro.apache.org/](https://avro.apache.org/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4\. https://avro.apache.org/docs/1.11.1/getting-started-python/#serializing-and-deserializing-without-code-generation](https://avro.apache.org/docs/1.11.1/getting-started-python/#serializing-and-deserializing-without-code-generation)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5\. https://avro.apache.org/docs/1.11.1/specification/](https://avro.apache.org/docs/1.11.1/specification/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[6\. https://stackoverflow.com/questions/14820450/best-splittable-compression-for-hadoop-input-bz2](https://stackoverflow.com/questions/14820450/best-splittable-compression-for-hadoop-input-bz2)'
  prefs: []
  type: TYPE_NORMAL
- en: '[7\. https://stackoverflow.com/questions/35789412/spark-sql-difference-between-gzip-vs-snappy-vs-lzo-compression-formats](https://stackoverflow.com/questions/35789412/spark-sql-difference-between-gzip-vs-snappy-vs-lzo-compression-formats)'
  prefs: []
  type: TYPE_NORMAL
- en: 8\. [https://medium.com/ssense-tech/csv-vs-parquet-vs-avro-choosing-the-right-tool-for-the-right-job-79c9f56914a8](https://medium.com/ssense-tech/csv-vs-parquet-vs-avro-choosing-the-right-tool-for-the-right-job-79c9f56914a8)
  prefs: []
  type: TYPE_NORMAL
