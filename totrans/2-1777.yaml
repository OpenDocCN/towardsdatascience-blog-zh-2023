- en: Why does Regularization actually work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/regularization-996a4967984a](https://towardsdatascience.com/regularization-996a4967984a)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Deconstructing the mathematical concept of regularization in machine learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://guenterroehrich.medium.com/?source=post_page-----996a4967984a--------------------------------)[![Günter
    Röhrich](../Images/31a1d0dc835c7ad31197f8c387023d10.png)](https://guenterroehrich.medium.com/?source=post_page-----996a4967984a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----996a4967984a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----996a4967984a--------------------------------)
    [Günter Röhrich](https://guenterroehrich.medium.com/?source=post_page-----996a4967984a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----996a4967984a--------------------------------)
    ·9 min read·Jan 2, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: 'While there are many good resources that show how regularization is implemented
    in machine learning, this guide is a brief introduction to the question, why adding
    a term actually has beneficial properties for our models. This post will discuss
    the idea of regularization in the context of linear regression and the following
    topics will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Underlying Idea of OLS](#a40e)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Tikhonov Regularization (L2 Regularization)](#cea1)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[L1 Regularization](#ac08)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Specifying Lambda](#3a78)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Regularization in Deep Learning](#32fe)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Conclusion](#877d)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Underlying Idea of OLS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To begin with, lets quickly go over the basic ordinary least squares (OLS)
    equation which plays a crucial role in the course of linear regression. In linear
    regression, our goal is to find a prediction for a value (y-hat) that is very
    close to the true (observed) value. To achieve this, we usually try to find a
    set of coefficients that allow us to draw a straight line through the data points.
    This line is regarded optimal when it **minimizes the distances to the data points**.
    Here is an example using the famous cars dataset in R:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/71919088f3dd9ad31d586bdad3cb08cd.png)'
  prefs: []
  type: TYPE_IMG
- en: A simple Linear Regression demo — Image by Author using “cars” dataset
  prefs: []
  type: TYPE_NORMAL
- en: The red line is an estimate of the response value “*y*”. By the definition of
    an estimate, the value we calculate is very likely close to, **but not exactly
    equal to**, the true *response variable, y*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a86a18d85ac0070a31e585d98c4a8068.png)'
  prefs: []
  type: TYPE_IMG
- en: The estimate (y-hat) which is shown as red line in the chart
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, OLS describes the basic idea of **minimizing the distance between
    the red line (estimate, y-hats) and the true values (response, y), but how to
    actually find this optimal line?** There are several ways to fit the line to the
    data, as an example, we could use the idea of gradient descent to gradually adjust
    (shift, tilt) the red line in a way, so that it ends up in the position shown
    above. In the **regression context, we would rather prefer to use matrix inversion**
    which yields the same result for our estimates.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the gradient descent approach sounds interesting to you, the following step-by-step
    guide might be interesting to read:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/gradient-descent-f7458de38365?source=post_page-----996a4967984a--------------------------------)
    [## A Simple Guide to Gradient Descent — A Linear Regression Example'
  prefs: []
  type: TYPE_NORMAL
- en: When opening a machine learning book it is very unlikely that the first ten
    pages can be survived without encountering…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/gradient-descent-f7458de38365?source=post_page-----996a4967984a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'Our starting point for either case is the loss or objective function which
    can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9ff9656bc547c554ab7f35f55f1d98a7.png)'
  prefs: []
  type: TYPE_IMG
- en: The loss function simply formalizes the idea of “minimizing the distance between
    line and true values”
  prefs: []
  type: TYPE_NORMAL
- en: 'To minimize this function, we are looking for its derivative with respect to
    the set of coefficients (betas). This can be shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4f46c3f3fdb9d10d841024c7ddd113a8.png)'
  prefs: []
  type: TYPE_IMG
- en: The minimum of the function can be obtained by setting the above derivation
    to zero. Further, we are able to solve for the values beta, our model coefficients.
    As you can see in the exponent, we have to invert the matrix of X values — and
    this is where we need to pay attention to linear independence of these columns!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/21720ed80417e5c12c6f20717246fcfb.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the loss function is a convex function so we should get to a local
    minimum, if we were interested *showing this*, we would need to look into the
    second derivative (which is equivalent to the *Hessian matrix*).
  prefs: []
  type: TYPE_NORMAL
- en: '**The key takeaway here is that we can find the estimated coefficients by deriving
    the loss function as demonstrated.**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s now move on with the question “**why and how OLS is related to the idea
    of regularization?**”.
  prefs: []
  type: TYPE_NORMAL
- en: Tikhonov Regularization (Ridge)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the course of regularization you might often be confronted an equation that
    looks like this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/25f43606d1e5bbb0ffecd0fc770adf82.png)'
  prefs: []
  type: TYPE_IMG
- en: Quickly broken down, the equation shows two parts, the OLS part we just analyzed
    above, as well as a new term that adds a sum of squared betas to the overall loss
    function.
  prefs: []
  type: TYPE_NORMAL
- en: I decided to first provide the L2 regularization (“norm squared” as shown above)
    because this is common not only in linear regression, but also quite popular in
    **regularization for weight matrices in deep learning**.
  prefs: []
  type: TYPE_NORMAL
- en: '*But why do we add this new last term to our loss (or objective) function?*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Could we add the same term to the n-th power instead?*'
  prefs: []
  type: TYPE_NORMAL
- en: OLS makes several assumptions about the data. As mentioned, one of these assumptions
    implies that due to linear independence ([to assure we can invert the matrix](https://textbooks.math.gatech.edu/ila/invertible-matrix-thm.html)),
    we are able to find a set of coefficients that provide a unique solution to the
    given problem, however, this may not be the case. There are scenarios where **no
    set** of betas or even **several** sets of betas could be found, resulting in
    a non-unique solution.
  prefs: []
  type: TYPE_NORMAL
- en: '**Why is this a problem?** Regression asks for linearly independent observations,
    and given the fact we use matrix inversion, also requires linearly independent
    columns. This is problematic because we might find ourselves in the position with
    an over- or underdetermined system of equations — meaning there are several non-unique
    or even no solutions at all! This is exactly where and why regularization comes
    into play:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Introducing further constraints to our optimization problem allows us to
    improve the limitation/conditioning of the equation which may further allow us
    to determine a unique solution set.**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/e4aa39534062cce47fb4d4f72dc00d24.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we can see, there is a term accompanied by a Lagrangian multiplier added
    to the equation. This Lagrangian adds the following constraint:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/43cb3b763f3b044fa4cbfb2ae1cc9c80.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that c is a constant and will be omitted when deriving the function leaving
    us with a “beta²” term. What becomes fairly obvious when considering minimizing
    the whole equation is, that many large coefficients contradict with minimizing
    the sum of coefficients. This means, we can expect to see smaller coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: 'This gets us to the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ec0429886a4f9eca51a3f075a7864306.png)'
  prefs: []
  type: TYPE_IMG
- en: In contrast to the OLS derivative, we now introduced a “+ lambda * Identity”
    term
  prefs: []
  type: TYPE_NORMAL
- en: L1 Regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With regard to the previous question, whether we could use different variations
    of the regularization (e.g. power), the simple answer is yes. The most popular
    example is again very closely tied to OLS and is called LASSO regression. LASSO
    adds a constraint that introduces an absolute value penalty as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/83ce4521df5e8a71c7b316ed83af9936.png)'
  prefs: []
  type: TYPE_IMG
- en: Loss under the LASSO L1 constraint
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the Lagrangian term is quite similar to what we saw before for
    Ridge regression, in this case however, we see a change in the norm and power
    for the betas.
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast to the L2 norm (where betas are squared), betas under L1 can be
    negative! For this reason we have to look at a few cases of what values the betas
    could have and obtain the derivation for each scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting with the expansion of the above equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b142154fe899cb076fa0f6637bad57b3.png)'
  prefs: []
  type: TYPE_IMG
- en: the following derivations can be obtained
  prefs: []
  type: TYPE_NORMAL
- en: '**Betas > 0:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/881998a6083c6d7eb49be141836c1aa2.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Betas < 0:**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5d6d15be1183d4e7067b17dfd9fe6cf4.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Betas = 0:** *This corresponds to the standard OLS case*'
  prefs: []
  type: TYPE_NORMAL
- en: LASSO is not only known as regularization method as such, but is also used as
    a technique for variable selection when dealing with a variety of features. The
    below illustration provides a geometric interpretation that shows why L1 norm
    can be used for variable selection tasks as some weights are pushed to 0 (and
    can therefore be omitted).
  prefs: []
  type: TYPE_NORMAL
- en: The L2 penalty (right) can be expressed as circle and the penalty leads to the
    result that the diameter of the circle shrinks. This means, the coefficients or
    betas (named as “weights”, w1 and w2 here) will shrink towards zero.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/62db78bec16065c7e33627f48da4ec90.png)'
  prefs: []
  type: TYPE_IMG
- en: Image from [Wikipedia](https://en.wikipedia.org/wiki/Lasso_(statistics)#/media/File:L1_and_L2_balls.svg)
  prefs: []
  type: TYPE_NORMAL
- en: As you may have noted, a *convex object that is tangent to the constraint boundary
    is likely to encounter a corner under the L1 norm,* meaning one or more betas
    is exactly zero, while for the L2 case, the points at which the convex object
    is tangent to the boundary, are no more likely than all other points on the circle.
  prefs: []
  type: TYPE_NORMAL
- en: As L1 allows coefficients to be zero, the penalty is commonly known as “inducing
    sparsity” on the set of weights (or betas, as we called them in the regression
    context). Coefficients may be shrunk down to 0 which is not likely to happen under
    the quadratic constraint.
  prefs: []
  type: TYPE_NORMAL
- en: Specifying Lambda
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Looking at the regularization examples above, it becomes clear that the choice
    of lambda is crucial to the modeling approach. Lambda plays an important role
    to balance the bias-variance-tradeoff and should therefore be carefully selected.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, **cross validation** is used to find a value of lambda that keeps
    the model error low, while also considering reasonable levels of variance. Feel
    free to read more about this topic in the link below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/cross-validation-7c0163460ea0?source=post_page-----996a4967984a--------------------------------)
    [## Proper Model Selection through Cross Validation'
  prefs: []
  type: TYPE_NORMAL
- en: Cross validation is state of the art. It is nothing close to a fun task, but
    yet vital for everyone dealing with data…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/cross-validation-7c0163460ea0?source=post_page-----996a4967984a--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Regularization in Deep Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far we have looked at L1 and L2 regularization as well as taken a quick reference
    on “how to determine lambda”. Regularization is not only crucial in the course
    of regression, but will also lead to deep learning models that generalize much
    better than “vanilla” implementations without regularization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nevertheless, regularization in deep learning is not entirely re-inventing
    the wheel. Similar to the idea of squared betas (L2) loss, we can easily translate
    this to the weight matrix, as used in a neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3c7e9ce508b466798b364a90b5c888e7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This equation results in a single scalar value, comprising the squared sum
    over the whole weight matrix. This is equivalent to the sum of beta squared, as
    shown earlier. Putting together the regularization loss above as well as the “data
    loss” that is obtained from the objective function (e.g. Softmax or Sigmoid functions)
    and denoted as Li:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e25d64ed873ef587b3f0b642525aa9d4.png)'
  prefs: []
  type: TYPE_IMG
- en: Loss = L(objective function) + lambda*L(Weight Matrix)
  prefs: []
  type: TYPE_NORMAL
- en: 'Having shown the L2 regularization for a neural network, please be aware that
    there are many different ways to achieve regularization or regularization similar
    behavior. The right choice may save you a lot of training time and will also make
    the model more general. Very popular regularization (like) options are:'
  prefs: []
  type: TYPE_NORMAL
- en: L1 and L2 regularization
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dropout
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Batch / Layer / Spatial Normalization](http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43442.pdf)
    (that prevents covariate shifts)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: “Early stopping”
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You may sometimes also find the idea of “early stopping”, which means the training
    is stopped when a certain criteria is met (e.g. the Loss is not improving < epsilon
    for the last x epochs). Although this method is really just looking at the model
    outcome / loss, it often proves to be a great tool to leverage. Early stopping
    may be applied under the term “Callbacks” in deep learning frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Regularization is a great way of incorporating a set of constraints to the underlying
    optimization problem. Adding additional constraints to the loss function can significantly
    help improving model generalization, reduce the risk of overfitting, overcome
    ill-posed problems (like multi-collinearity in linear regression), but also support
    the variable selection process.
  prefs: []
  type: TYPE_NORMAL
- en: '*If you found this post interesting, I would appreciate you hitting the follow
    button!*'
  prefs: []
  type: TYPE_NORMAL
- en: '*{Take care of yourself, and if you can, someone else too}*'
  prefs: []
  type: TYPE_NORMAL
- en: '*— borrowed from Stephen Dubner*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Interesting Reads:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**L1 Regularization**'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://aswani.ieor.berkeley.edu/teaching/SP15/265/lecture_notes/ieor265_lec6.pdf](https://aswani.ieor.berkeley.edu/teaching/SP15/265/lecture_notes/ieor265_lec6.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '**L2 Regularization**'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://cs229.stanford.edu/notes2021fall/lecture10-ridge-regression.pdf](https://cs229.stanford.edu/notes2021fall/lecture10-ridge-regresion.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '**L2 Regularization on weight matrices (deep learning)**'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://cs231n.github.io/linear-classify/?source=post_page-----996a4967984a--------------------------------#multiclass-support-vector-machine-loss)
    [## CS231n Convolutional Neural Networks for Visual Recognition'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table of Contents: In the last section we introduced the problem of Image Classification,
    which is the task of…'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: cs231n.github.io](https://cs231n.github.io/linear-classify/?source=post_page-----996a4967984a--------------------------------#multiclass-support-vector-machine-loss)
    ![](../Images/beaadac02474b27233320e2c07741bfd.png)
  prefs: []
  type: TYPE_NORMAL
- en: Not a constraint boundary — by Christoph Ungerböck
  prefs: []
  type: TYPE_NORMAL
