- en: Introduction to Entropy and Gini Index
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/entropy-and-gini-index-c04b7452efbe](https://towardsdatascience.com/entropy-and-gini-index-c04b7452efbe)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Understanding how these measures help us quantify uncertainty in a dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@gurjinderkaur95?source=post_page-----c04b7452efbe--------------------------------)[![Gurjinder
    Kaur](../Images/d5c6746466025dad06077b1a89a789d1.png)](https://medium.com/@gurjinderkaur95?source=post_page-----c04b7452efbe--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c04b7452efbe--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c04b7452efbe--------------------------------)
    [Gurjinder Kaur](https://medium.com/@gurjinderkaur95?source=post_page-----c04b7452efbe--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c04b7452efbe--------------------------------)
    ·7 min read·Nov 5, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7471e1f27896be8a7c34dd14a251bf09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Can you tell which are the purest and impurest carts? (Source: Image by Author)'
  prefs: []
  type: TYPE_NORMAL
- en: E**ntropy** and **Gini Index** are important machine learning concepts particularly
    helpful in decision tree algorithms to determine the quality of a split. Both
    of these metrics are calculated differently but ultimately used to quantify the
    same thing i.e. uncertainty (or impurity) within a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The higher the Entropy (or Gini Index), the more random (mixed) the data is.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s have an intuitive picture of impurity in a dataset and understand how
    these metrics can help measure it. (Impurity, uncertainty, randomness, heterogeneity
    — all can be used interchangeably in our context and the goal is to ultimately
    reduce them to have better clarity).
  prefs: []
  type: TYPE_NORMAL
- en: '**What is impurity — explained with an example**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Imagine that you go to a supermarket with your friends — *Alice* and *Bob*
    to buy fruit. Each of you grabs a shopping cart because none of you likes sharing
    your fruits. Let’s find out what you guys got (looks like you love apples!!):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/28238e1afb78168cb4d68c8d864abbbf.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the Author
  prefs: []
  type: TYPE_NORMAL
- en: These three carts can be seen as three different data distributions. If we assumed
    that there are two classes (apples and bananas) initially, then the interpretations
    that follow would be incorrect. Rather, think of each cart as a different distribution
    — so the first cart is a data distribution where all data points belong to a single
    class, and the second & third carts are the data distributions with two classes.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the example above, it is easy to identify the carts with the most
    pure or impure data distributions (*class distributions* to be precise). But in
    order to have a mathematical quantification of purity in a dataset so that it
    can be used by an algorithm to make decisions, entropy and Gini Index come to
    rescue.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both of these measures look at the probability of occurrence (or presence)
    of each class in a dataset. In our example, we have a total of 8 data points (fruits)
    in each case, so we can compute our class probabilities for each of the carts
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/df8a3e940e80a9ef2aa0ce78f08cbcff.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the Author
  prefs: []
  type: TYPE_NORMAL
- en: Now we are equipped with everything we need to dive into formal definitions
    of Entropy and Gini Index!
  prefs: []
  type: TYPE_NORMAL
- en: As already discussed, both entropy and gini index are a measure of the degree
    of uncertainty or randomness in data. While they aim to quantify the same fundamental
    concept, each has its own mathematical formulation and interpretation to achieve
    that.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Entropy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given a labeled dataset where each label comes from a set of *n* classes, we
    can compute entropy as follows. Here pi is the probability of randomly picking
    up an element from class i.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1863a937a67ddccd5046ba694b87a7ba.png)'
  prefs: []
  type: TYPE_IMG
- en: To determine the best split in a decision tree, entropy is used to compute information
    gain, and the feature contributing to the maximum information gain is selected
    at a node.
  prefs: []
  type: TYPE_NORMAL
- en: Gini Index
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Gini Index attempts to quantify randomness in a dataset by finding an answer
    to this question — *What is the probability of incorrectly labeling an element
    picked randomly from the given data?*
  prefs: []
  type: TYPE_NORMAL
- en: Given a labeled dataset where each label comes from a set of *n* classes, the
    formula to calculate gini index is given below. Here, pi is the probability of
    randomly picking up an element from class i.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/473c30a64b7d8898db7faf894adff777.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This formula is often reframed as follows as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b20fd9267e7abec313e8dcc3b181110e.png)'
  prefs: []
  type: TYPE_IMG
- en: '(Note: The sum of all class probabilities is 1).'
  prefs: []
  type: TYPE_NORMAL
- en: Gini index is an alternative to information gain that can be used in decision
    tree to determine the quality of split. At a given node, it compares the *difference*
    between the **gini index of the data before split** and **weighted sum of gini
    indices of both branches after split** and chooses the one with the highest difference
    (or *gini gain).* If this is unclear, don’t worry about it for now since it needs
    more context, and the goal of this article is to just have a basic intuition behind
    the meaning of these metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Going back to our example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To make things easier to understand, refer to our shopping cart example, we
    have three datasets — C1, C2, and C3, each of which has 8 records with labels
    coming from two classes — [Apple, Banana]. Using the probabilities calculated
    in the table above, let’s unroll both of these computations for Alice’s cart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ced21c7fff8d8226381fd708d53dcd14.png)![](../Images/3a0c704604f32360454680bf57b8465c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, we can compute these metrics for C1 and C3 as well and will get
    the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/121c8d170df1437f5649ed49acebade5.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the Author
  prefs: []
  type: TYPE_NORMAL
- en: From the above table, we can have some interesting takeaways about the range
    of values both entropy and gini index can have. Let’s call the lowest possible
    value as the *lower bound* and the maximum possible value as the *upper bound*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Lower Bound**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The lower bound of both entropy and gini index is 0 when our data is purely
    homogeneous. Take a look at cart C1 for reference.
  prefs: []
  type: TYPE_NORMAL
- en: Upper Bound
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Entropy and Gini Index are 1 and 0.5 respectively when data has the highest
    uncertainty (take a look at cart C2 for reference as C2 represents an example
    having the highest possible randomness).
  prefs: []
  type: TYPE_NORMAL
- en: One thing to note here is that these values for the upper bound will only hold
    in case of binary classification (because that’s what our two-class apple-banana
    example represents). In the case of **n** classes which are equally likely each
    having probability **(1/n)**, the upper bound will be a function of **n,** as
    shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '**Upper bound for entropy:**'
  prefs: []
  type: TYPE_NORMAL
- en: '**For binary classification**, the upper bound of Entropy is 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**For multi-class classification with each class having same probability**,
    the upper bound of Entropy will be:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/28f6103ab099b939b7147e8d8234eb7d.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Upper bound for Gini Index:**'
  prefs: []
  type: TYPE_NORMAL
- en: '**For binary classification,** the upper bound of Gini Index does not usually
    exceed 0.5'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**For multi-class classification with each class having same probability**,
    the upper bound of Gini Index will be:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/51970f0c49892dbbafb1e8e4313bd928.png)'
  prefs: []
  type: TYPE_IMG
- en: Recap
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Entropy and gini index are used to quantify randomness in a dataset and are
    important to determine the quality of split in a decision tree. We can use the
    terms randomness, uncertainty, impurity, and heterogeneity interchangeably here.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High values of entropy and gini index mean high randomness in the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Entropy**'
  prefs: []
  type: TYPE_NORMAL
- en: Entropy aims to quantify how unpredictable a dataset is.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The formula to calculate entropy is given below. Here pi is the probability
    of choosing an element from a class labeled as *i,* given *n* total classes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/72a6fda8cbdbfbc7478e0a1808cfbf51.png)'
  prefs: []
  type: TYPE_IMG
- en: If the data consists of elements belonging to a single class, it becomes highly
    predictable, so the entropy will be the minimum. And minimum value of entropy
    is 0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the data consists of elements belonging to n classes which are equally
    likely, each having probability = 1/n, entropy will be the maximum.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For binary classification (i.e., data with two classes), the value of entropy
    will never exceed 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For multi-class classification, the maximum value of entropy can be generalized
    as log(n). (Here log is to the base of 2.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gini Index**'
  prefs: []
  type: TYPE_NORMAL
- en: Gini index aims to quantify the probability of incorrectly labeling an element
    chosen randomly from the data.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The formula is given below:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/b20fd9267e7abec313e8dcc3b181110e.png)'
  prefs: []
  type: TYPE_IMG
- en: If the data consists of elements belonging to a single class, the probability
    of incorrectly labeling a randomly chosen element will be zero, hence the gini
    index will be the minimum in this case. So, the minimum possible value of gini
    index is also 0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the data consists of elements belonging to n classes with balanced distribution
    i.e. each class has equal probability 1/n, gini index will be the maximum.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For binary classification (i.e., data with two distinct classes), the maximum
    value of gini index will never exceed 0.5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For multi-class classification, the maximum value of gini index can be generalized
    as 1-(1/n).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
