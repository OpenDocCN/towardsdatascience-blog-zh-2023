["```py\n# listing 1\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import uniform, norm\nfrom scipy.spatial import distance_matrix\nfrom sklearn.neighbors import KNeighborsRegressor\n%matplotlib inline\n\nnum_dims=[2, 3, 9, 15, 50, 100]\nx_list = np.linspace(0.5, 1, 11)\nedge_list = 2*(x_list - 0.5)\nfig, axs = plt.subplots(3, 2, figsize=(14, 19))\nplt.subplots_adjust(hspace=0.3)\n\nfor i, d in enumerate(num_dims):\n    vols = [(edge_list[i+1])**d-(edge_list[i])**d \\\n            for i in range(len(x_list)-1)]\n    axs[i//2, i%2].bar(x_list[1:], vols, width = 0.03)\n    axs[i//2, i%2].set_title(\"d={}\".format(d), fontsize=20)\n    axs[i//2, i%2].set_xlabel('Shell corner $x_1$ coordinate', fontsize=18)\n    axs[i//2, i%2].set_ylabel('Volume', fontsize=18)\n    axs[i//2, i%2].set_xticks(x_list)\n    axs[i//2, i%2].tick_params(axis='both', labelsize=12)\n\nplt.show()\n```", "```py\n# Listing 2\n\nnp.random.seed(0)\nnum_dims=[2,3,9, 15, 50, 100]\nx_list = np.linspace(0.5, 1, 11)\nx_list[-1] = 1.001\nx_list1 = np.linspace(0.5, 0, 11)\nx_list1[-1] = -0.001\nfig, axs = plt.subplots(3, 2, figsize=(14, 19))\nplt.subplots_adjust(hspace=0.3)\n\nfor i, d in enumerate(num_dims):\n    sample = uniform.rvs(size=5000*d).reshape(5000, d)\n    count_list=[]\n    for j in range(len(x_list)-1):\n        inshell_count = (((x_list[j] <= sample) & \\\n                          (sample < x_list[j+1])) | \\\n                          ((x_list1[j+1] < sample) & \\\n                          (sample <= x_list1[j]))).sum(axis=1)\n        outshell_count = ((x_list[j+1] <= sample) | \\\n                          (x_list1[j+1] >= sample)).sum(axis=1)\n        count = ((inshell_count>0) & (outshell_count==0)).sum()\n        count_list.append(count)\n\n    axs[i//2, i%2].bar(x_list[1:], count_list, width = 0.03)\n    axs[i//2, i%2].set_title(\"d={}\".format(d), fontsize=20)\n    axs[i//2, i%2].set_xlabel('Shell corner $x_1$ coordinate', fontsize=18)\n    if i%2==0:\n        axs[i//2, i%2].set_ylabel('Number of data \\npoints within shell',\n                                  fontsize=18)\n    axs[i//2, i%2].set_xticks(np.linspace(0.5, 1, 11))\n    axs[i//2, i%2].tick_params(axis='both', labelsize=12)\n\nplt.show()\n```", "```py\n# Listing 3\n\nnp.random.seed(0)\nnum_dims=[2, 5, 10]\nxi = np.linspace(-3, 3, 500)\nfig, axs = plt.subplots(1, 3, figsize=(19, 5))\nplt.subplots_adjust(wspace=0.25)\n\nfor i, d in enumerate(num_dims):\n    pdf = norm.pdf(xi)**d\n    axs[i].plot(xi, pdf)\n    axs[i].set_xlabel('$x_i$', fontsize=28)\n    axs[i].set_xlim([-3, 3])\n    axs[i].set_title(\"d={}\".format(d), fontsize=26)\n    axs[i].tick_params(axis='both', labelsize=12)\naxs[0].set_ylabel('PDF', fontsize=26)\n\nplt.show()\n```", "```py\n# Listing 4\n\nnp.random.seed(0)\nnum_dims=[2,3,9, 15, 50, 100]\nfig, axs = plt.subplots(2, 3, figsize=(19, 14))\nplt.subplots_adjust(hspace=0.3)\n\nfor i, d in enumerate(num_dims):\n    samples = norm.rvs(size=5000*d).reshape(5000, d)\n    dist = np.linalg.norm(samples, axis=1)\n    axs[i//3, i%3].hist(dist, bins=25)\n    axs[i//3, i%3].set_title(\"d={}\".format(d), fontsize=28)\n    axs[i//3, i%3].set_xlabel('Distance from origin', fontsize=22)\n    if i%3==0:\n        axs[i//3, i%3].set_ylabel('Number of data points', fontsize=22)\n    axs[i//3, i%3].set_xlim([0, 14])\n    axs[i//3, i%3].tick_params(axis='both', labelsize=16)\n\nplt.show()\n```", "```py\n# Listing 5\n\nnp.random.seed(0)\nnum_dims=[2,3,9, 15, 50, 500]\nfig, axs = plt.subplots(2, 3, figsize=(19, 14))\nplt.subplots_adjust(hspace=0.3)\n\nfor i, d in enumerate(num_dims):\n    samples = uniform.rvs(size=500*d).reshape(500, d)\n    dist_amtrix = distance_matrix(samples, samples)\n    dist_arr = dist_amtrix[np.triu_indices(len(dist_amtrix), k = 1)]\n\n    axs[i//3, i%3].hist(dist_arr, bins=30)\n    axs[i//3, i%3].set_title(\"d={}\".format(d), fontsize=28)\n    axs[i//3, i%3].set_xlabel('Pairwise distance', fontsize=22)\n    if i%3==0:\n        axs[i//3, i%3].set_ylabel('Number of data points', fontsize=22)\n    axs[i//3, i%3].set_xlim([0, np.sqrt(d)])\n    axs[i//3, i%3].tick_params(axis='both', labelsize=16)\n\nplt.show()\n```", "```py\n# Listing 6\n\nnp.random.seed(9)\nnum_dims=[2,500]\nfig, axs = plt.subplots(2, 2, figsize=(17, 11))\nplt.subplots_adjust(hspace=0.3)\n\nfor i, d in enumerate(num_dims):\n    samples = uniform.rvs(size=500*d).reshape(500, d)\n    query = samples[0]\n    dist_amtrix = distance_matrix(samples, samples)\n    ind_min = np.argmin(dist_amtrix[0, 1:])+1 \n    ind_max = np.argmax(dist_amtrix[0, 1:])+1\n    nearest = samples[ind_min]\n    farthest = samples[ind_max]\n    query_nearest_comps = np.abs(query - nearest)\n    query_farthest_comps = np.abs(query - farthest)\n    query_nearest_length = np.linalg.norm(query - nearest)\n    query_farthest_length = np.linalg.norm(query - farthest)\n    axs[0,i].bar(np.arange(1, d+1), query_nearest_comps,\n               color=\"blue\", width=0.45, label=\"$x_n-x_q$\")\n    axs[0,i].bar(np.arange(1, d+1), query_farthest_comps,\n               color=\"red\", alpha=0.5, width=0.45, label=\"$x_f-x_q$\")\n    axs[0,i].set_title(\"d={}\".format(d), fontsize=26, pad=20)\n    axs[0,i].set_xlabel('Index of component', fontsize=18)\n    axs[0,i].set_ylabel('Abosulte value of component', fontsize=18)\n    if i==0:\n        axs[0,i].set_xticks(np.arange(1, d+1))\n    axs[0,1].set_ylim([0, 1.2])\n    axs[0,i].tick_params(axis='both', labelsize=16)\n    axs[0,i].legend(loc='best', fontsize=17)\n    axs[1,i].bar([\"$||x_n-x_q||$\", \"$||x_f-x_q||$\"],\n                 [query_nearest_length, query_farthest_length],\n                 width=0.2)\n    axs[1,i].tick_params(axis='both', labelsize=22)\n    axs[1,i].set_ylabel('Length', fontsize=18)\n\nplt.show()\n```", "```py\n# Listing 7\n\nnp.random.seed(2)\nnum_dims = np.arange(1, 11)\nnum_simulations = 1000\n\nvar_list = []\nbias_list = []\nfor d in num_dims:\n    pred_list = []\n    for i in range(num_simulations):\n        X = uniform.rvs(size=1000*d).reshape(1000, d)\n        y = X[:,0]**2\n        knn_model = KNeighborsRegressor(n_neighbors=3)\n        knn_model.fit(X, y)\n        pred_list.append(knn_model.predict(0.5*np.ones((1,d))))\n    var_list.append(np.var(pred_list))\n    bias_list.append((np.mean(pred_list)-0.25)**2)\nerror = np.array(bias_list) + np.array(var_list)\nplt.plot(num_dims, var_list, 'o-', label=\"$Variance$\")\nplt.plot(num_dims, bias_list, 'o-', label=\"$Bias^2$\")\nplt.plot(num_dims, error, 'o-', label=\"$MSE$\")\nplt.xlabel(\"d\", fontsize=16)\nplt.ylabel(\"$MSE, Bias^2, Variance$\", fontsize=16)\nplt.legend(loc='best', fontsize=14)\nplt.xticks(num_dims)\nplt.show()\n```", "```py\n# Listing 8\n\nnp.random.seed(2)\nnum_dims = [2, 10]\nnum_simulations = 1000\nfig, axs = plt.subplots(1, 2, figsize=(10, 5))\nplt.subplots_adjust(wspace=0.3)\n\nfor i, d in enumerate(num_dims):\n    pred_list = []\n    for j in range(num_simulations):\n        X = uniform.rvs(size=1000*d).reshape(1000, d)\n        y = X[:,0]**2\n        knn_model = KNeighborsRegressor(n_neighbors=3)\n        knn_model.fit(X, y)\n        pred_list.append(knn_model.predict(0.5*np.ones((1,d))))\n    axs[i].scatter([0]*num_simulations, pred_list,\n                   color=\"blue\", alpha=0.2, label=\"$\\hat{y}_t$\")\n    axs[i].axhline(y = 0.25, color = 'black',\n                   linestyle='--', label=\"$y_t$\", linewidth=3)\n    axs[i].axhline(y = np.mean(pred_list),\n                   color = 'blue', label=\"$E[\\hat{y}_t]$\")\n    axs[i].set_ylim([0, 0.7])\n    axs[i].set_title(\"d={}\".format(d), fontsize=18)\n    axs[i].set_ylabel('$\\hat{y}_t$', fontsize=18)\n    axs[i].legend(loc='best', fontsize=14)\n    axs[i].set_xticks([])\n\nplt.show()\n```", "```py\n# Listing 9\n\nnp.random.seed(2)\nnum_dims = np.arange(1,11)\nnum_simulations = 1000\n\nvar_list = []\nbias_list = []\nfor d in num_dims:\n    pred_list = []\n    for i in range(num_simulations):\n        X = uniform.rvs(size=1000*d).reshape(1000, d)\n        y = np.exp(-5*np.linalg.norm(X-0.5*np.ones((1,d)), axis=1)**2)\n        knn_model = KNeighborsRegressor(n_neighbors=3)\n        knn_model.fit(X, y)\n        pred_list.append(knn_model.predict(0.5*np.ones((1,d))))\n    var_list.append(np.var(pred_list))\n    bias_list.append((np.mean(pred_list)-1)**2)\nerror = np.array(bias_list) + np.array(var_list)\nplt.plot(num_dims, var_list, 'o-', label=\"$Variance$\")\nplt.plot(num_dims, bias_list, 'o-', label=\"$Bias^2$\")\nplt.plot(num_dims, error, 'o-', label=\"$MSE$\")\nplt.xlabel(\"d\", fontsize=16)\nplt.ylabel(\"$MSE, Bias^2, Variance$\", fontsize=16)\nplt.legend(loc='best', fontsize=14)\nplt.xticks(num_dims)\nplt.show()\n```"]