- en: Data pipeline design patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3](https://towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Choosing the right architecture with examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://mshakhomirov.medium.com/?source=post_page-----100afa4b93e3--------------------------------)[![üí°Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----100afa4b93e3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----100afa4b93e3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----100afa4b93e3--------------------------------)
    [üí°Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----100afa4b93e3--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----100afa4b93e3--------------------------------)
    ¬∑9 min read¬∑Jan 2, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/074dc1570ad17f76947283225050b8db.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [israel palacio](https://unsplash.com/@othentikisra?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Typically data is processed, extracted, and transformed in steps. Therefore,
    a sequence of data processing stages can be referred to as a **data pipeline**.
  prefs: []
  type: TYPE_NORMAL
- en: '*Which design pattern to choose?*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: There are lots of things to consider, i.e. **Which data stack to use? What tools
    to consider? How to design a data pipeline conceptually? ETL or ELT? Maybe ETLT?
    What is Change Data Capture?**
  prefs: []
  type: TYPE_NORMAL
- en: I will try to cover these questions here.
  prefs: []
  type: TYPE_NORMAL
- en: A data pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So it is a sequence of data processing steps. Due to ***logical data flow connections***
    between these stages, each stage generates an **output** that serves as an **input**
    for the following stage.
  prefs: []
  type: TYPE_NORMAL
- en: There is a data pipeline whenever there is data processing between points A
    and B.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A data pipeline‚Äôs three major parts are a **source, a processing step or steps,
    and a destination**. Data extracted from an external API (a source) can then be
    loaded into the data warehouse (destination). This is an example of a most common
    data pipeline where the source and destination are different. However, it is not
    always the case, as *destination-to-destination* pipelines also exist.
  prefs: []
  type: TYPE_NORMAL
- en: For example, data can originate as a `reference` table in the data warehouse
    in the first place and then after some data transformation, it can *land* in a
    new schema, for example, in `analytics` to be used in reporting solutions.
  prefs: []
  type: TYPE_NORMAL
- en: '*There is always a data pipeline when data is processed between source and
    destination.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/02c0d6d925b544a4cc7d82adbc9c1da9.png)'
  prefs: []
  type: TYPE_IMG
- en: Data pipeline. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Event data created by just one `source` at the back-end, an event stream built
    with Kinesis Firehose or Kafka stream, can feed a number of various `destinations`.
  prefs: []
  type: TYPE_NORMAL
- en: Consider ***user engagement* data** from *Google Analytics* as it flows as an
    event stream that can be used in both analytics dashboards for user activity and
    in the Machine learning (ML) pipeline for churn prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Despite using the same data source, both pipelines operate independently and
    must successfully complete before the user can see the results.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, data from two or more `source` locations can be aggregated into
    just one `destination`. For example, data from different payment merchant providers
    can be transformed into a revenue report for the Business Intelligence (BI) dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: Data quality checks, data cleansing, transformation, enrichment, filtering,
    grouping, aggregation, and the application of algorithms to the data are frequent
    steps in data pipelines.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Architecture types and examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data pipeline architecture as a term might mean several things depending on
    the situation. In general, it can be split into **conceptual** (logical) and **platform**
    levels or architecture types.
  prefs: []
  type: TYPE_NORMAL
- en: The **conceptually** logical part describes how a dataset is processed and transformed
    from collection to serving, whereas **platform architecture** focuses on an individual
    set of tools and frameworks used in a given scenario, as well as the functions
    that each of them plays.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a **logical structure** of a data warehouse pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/64499bbb581656e8ea35b7133f50d63d.png)'
  prefs: []
  type: TYPE_IMG
- en: Conceptual data pipeline design. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article I found a way to extract a real-time data from Firebase/Google
    Analytics 4 and load it in BigQuery:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/how-to-extract-real-time-intraday-data-from-google-analytics-4-and-firebase-in-bigquery-65c9b859550c?source=post_page-----100afa4b93e3--------------------------------)
    [## How to extract real-time intraday data from Google Analytics 4 and Firebase
    in BigQuery'
  prefs: []
  type: TYPE_NORMAL
- en: And always have an up-to-date data for your custom reports
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/how-to-extract-real-time-intraday-data-from-google-analytics-4-and-firebase-in-bigquery-65c9b859550c?source=post_page-----100afa4b93e3--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'And this is a conceptual data lake pipeline example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b08c6ce7fe26c3b711572352fe360af2.png)'
  prefs: []
  type: TYPE_IMG
- en: Conceptual data pipeline design. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in this post I previously wrote how to extract data MySQL databases,
    save it in Cloud Storage so later it could be used for analysis later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/mysql-data-connector-for-your-data-warehouse-solution-db0d338b782d?source=post_page-----100afa4b93e3--------------------------------)
    [## MySQL data connector for your data warehouse solution'
  prefs: []
  type: TYPE_NORMAL
- en: How to build one and export millions of rows in chunks, stream, capture real-time
    data changes or extract data and save‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/mysql-data-connector-for-your-data-warehouse-solution-db0d338b782d?source=post_page-----100afa4b93e3--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a platform level architecture example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0a7917159d66471313aedd34549ca170.png)'
  prefs: []
  type: TYPE_IMG
- en: Platform level data pipeline design. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a very common pattern for many lake house architecture solutions. In
    this blog post I created a bespoke data ingestion manager that is triggered by
    new object events when they are created in Cloud Storage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/how-to-handle-data-loading-in-bigquery-with-serverless-ingest-manager-and-node-js-4f99fba92436?source=post_page-----100afa4b93e3--------------------------------)
    [## How to Handle Data Loading in BigQuery with Serverless Ingest Manager and
    Node.js'
  prefs: []
  type: TYPE_NORMAL
- en: File formats, yaml pipe definitions, and transform and event triggers for your
    simple and reliable data ingestion‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/how-to-handle-data-loading-in-bigquery-with-serverless-ingest-manager-and-node-js-4f99fba92436?source=post_page-----100afa4b93e3--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Applications can trigger immediate responses to new data events thanks to stream
    processing.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming is a ‚Äúmust-have‚Äù solution for enterprise data.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Stream processing would gather and process data as it is generated rather than
    aggregating it at a predetermined frequency, as with batch processing. Common
    use cases are **anomaly detection and fraud prevention**, **real-time personalisation
    and marketing** and **internet of things**. Data and events are often produced
    by a ‚Äúpublisher‚Äù or ‚Äúsource‚Äù and transferred to a ‚Äústream processing application,‚Äù
    where the data is processed immediately before being sent to a ‚Äúsubscriber.‚Äù Very
    often, as a `source`, you can meet streaming applications built with **Hadoop,
    Apache Kafka, Amazon Kinesis,** etc. The "**Publisher/subscriber**" pattern is
    often referred to as `pub/sub`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/925417fa8cc74444da6b6fc344e96fe1.png)'
  prefs: []
  type: TYPE_IMG
- en: In this example we can set up an **ELT streaming** data pipeline to **AWS Redshift**.
    AWS Firehose delivery stream can offer this type of seamless integration when
    streaming data will be uploaded directly into the data warehouse table. Then data
    will be transformed to create reports with **AWS Quicksight** as a BI tool.
  prefs: []
  type: TYPE_NORMAL
- en: Batch processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Batch processing is a model where data is gathered according to a predetermined
    threshold or frequency in both micro-batch processing and conventional batch processing,
    and then processing takes place. Historically workloads were primarily batch-oriented
    in data environments. However, modern applications continuously produce enormous
    amounts of data, and a business leans to micro-batch and streaming processing,
    where data is being processed immediately to maintain a competitive advantage.
    Technologies for **micro-batch** loading include **Apache Spark Streaming, Fluentd,
    and Logstash**, and it is very similar to **conventional batch processing**, where
    events are processed on a schedule or in small groups.
  prefs: []
  type: TYPE_NORMAL
- en: It‚Äôs a good choice where the accuracy of your data is not relevant at this moment
    precisely.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/c47854fffb5d5468b0eade33ef679833.png)'
  prefs: []
  type: TYPE_IMG
- en: This data pipeline design pattern works better with smaller datasets that require
    ongoing processing because Athena charges based on the volume of data scanned.
  prefs: []
  type: TYPE_NORMAL
- en: '*Let‚Äôs say, you don‚Äôt want to use* `*change log*` *on MySQL database instance.
    That would be an ideal case because payments dataset is not huge.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Lambda/Kappa architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This architecture combines batch and streaming methodologies. It combines the
    best of both worlds and advises that raw data must be retained, for example, in
    a data lake in case you would want to use it again to build a new pipeline or
    investigate an outage. It has both **batch** and **streaming** (speed) layers
    which helps to respond instantly to shifting business and market conditions. Lambda
    architectures can sometimes very complicated with multiple code repositories to
    maintain.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/85af40b044d54265d18fb38f643a7c49.png)'
  prefs: []
  type: TYPE_IMG
- en: Lambda data pipeline design. Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Transform first then Load?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**ETL** is considered to be a conventional approach and the most widely used
    historically. With the rise of data warehousing **ELT** becomes increasingly popular.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Indeed, why do we need to transform first if we can centralise it for all
    data pipelines in the data warehouse?*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Vurtualisation** is another popular approach for data warehouses where we
    create **views** on data instead of meterialised tables. New requirements to business
    agility put cost-effectiveness on the send plan and data users qould query **views**
    instead of **tables**.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Change Data capture** is another approach to update the data exactly when
    the changes occur. When used typically latent data pipelines, CDC technology can
    recognize data changes as they happen and offer information about those changes.
    Changes are usually pushed to message queue or provided as a stream.'
  prefs: []
  type: TYPE_NORMAL
- en: How to choose the data pipeline architecture?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In recent years, data architecture components such as data pipelines have developed
    to support massive volumes of data. The term ‚ÄúBig Data‚Äù can be described as having
    three traits known as volume, variety and velocity. Big data can open up new opportunities
    in a variety of use cases, including predictive analytics, real-time reporting,
    and alerting. Architects and developers have had to adapt to new ‚Äúbig data‚Äù requirements
    because of the substantially increased volume, diversity, and velocity of data.
    New data processing frameworks emerged and kept emerging. Due to the high velocity
    of modern data streams, we might want to use *streaming* data pipelines. Data
    may then be collected and analysed in real-time, allowing for immediate action.
  prefs: []
  type: TYPE_NORMAL
- en: '*However, streaming data pipeline design pattern is not always the most cost-effective.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For example, in the majority of data warehouse solutions batch data ingestion
    is free. However, streaming, might go with a price. Same statement would be relevant
    for data processing. **Steaming** is the most expensive way to process the data
    in the majority of cases.
  prefs: []
  type: TYPE_NORMAL
- en: Big Data `volumes` require the data pipeline to be able to process events concurrently
    as very often they are sent simultaneously at once. Data solutions must be scalable.
  prefs: []
  type: TYPE_NORMAL
- en: The `variety` implies that data might come through the pipelines in different
    formats, very often unstructured or semi-structured.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture type depends on various factors, i.e. `destination` type and where
    data has to be in the end, **cost considerations** and your team's **development
    stack** and certain data processing skills you and your colleagues already have.
  prefs: []
  type: TYPE_NORMAL
- en: Do your data pipelines have to be managed and cloud-based, or would you rather
    want to deploy it on-premises?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In reality, there are numerous combinations of variables that can help to select
    the best data platform architecture. **What would be the velocity or data flow
    rate in that pipeline? Do you require real-time analytics, or is near-real-time
    sufficient?** This would resolve the question of whether or not you require "streaming"
    pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: For example, there are services that can create and run both **streaming** and
    **batch** data pipelines, i.e. [Google Dataflow](https://cloud.google.com/dataflow/docs/guides/data-pipelines).
    So how is it different from any other pipeline built in the data warehouse solution?
    The choice would depend on existing infrastructure. For example, if you have some
    existing **Hadoop** workloads, then GCP DataFlow would be a wrong choice as it
    will not let you to re-use the code (it is using Apachec Beam). In this case you
    would want to use **GCP Dataproc** which works on **Hadoop/Spark** code.
  prefs: []
  type: TYPE_NORMAL
- en: '*The rule of thumb is that if the processing is dependent on any tools in the
    Hadoop ecosystem, Dataproc should be utilized.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It is basically a Hadoop extension service.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if you are not limited by existing code and would like to
    reliably process ever-increasing amounts of **streaming** data then **Dataflow**
    is the recommended choice. You can check these [Dataflow templates](https://github.com/GoogleCloudPlatform/DataflowTemplates)
    if you like to do things in **JAVA**.
  prefs: []
  type: TYPE_NORMAL
- en: There is a [system design guide from Google](https://cloud.google.com/architecture)
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: BigData posed new challenging data architecture requirements for data developers.
    A constantly increasing variety of data formats and data sources increased the
    importance of data integration without disrupting production applications. Businesses
    are increasingly aiming to automate data integration procedures, process streaming
    data in real-time, and streamline the lifetime of data lakes and warehouses. This
    becomes a challenging task indeed, taking into account the increased data volume,
    velocity and variety of data formats over the last decade. Now data pipeline design
    must be robust and, at the same time, flexible to enable new data pipeline creation
    in a simplified and automated way. The increasing trend of using `data mesh`/
    `data mart` platforms requires data catalogues to be created. To create controlled,
    enterprise-ready data and give data consumers an easy method to find, examine,
    and self-provision data, this process should ideally be automated too. Therefore,
    choosing the right data pipeline architecture can solve these issues effectively.
  prefs: []
  type: TYPE_NORMAL
- en: '*Originally published at* [*https://mydataschool.com*](https://mydataschool.com/blog/data-pipeline-design-patterns/)*.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recommended read:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://cloud.google.com/dataflow/docs/guides/data-pipelines?source=post_page-----100afa4b93e3--------------------------------)
    [## Work with Data Pipelines | Cloud Dataflow | Google Cloud'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can report Dataflow Data Pipelines issues and request new features at Note:
    google-data-pipelines-feedback." You‚Ä¶'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: cloud.google.com](https://cloud.google.com/dataflow/docs/guides/data-pipelines?source=post_page-----100afa4b93e3--------------------------------)
    [](https://cloud.google.com/architecture?source=post_page-----100afa4b93e3--------------------------------)
    [## Cloud Architecture Guidance and Topologies | Cloud Architecture Center | Google
    Cloud
  prefs: []
  type: TYPE_NORMAL
- en: Discover reference architectures, guidance, and best practices for building
    or migrating your workloads on Google‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'cloud.google.com](https://cloud.google.com/architecture?source=post_page-----100afa4b93e3--------------------------------)
    [](https://github.com/GoogleCloudPlatform/DataflowTemplates?source=post_page-----100afa4b93e3--------------------------------)
    [## GitHub - GoogleCloudPlatform/DataflowTemplates: Google-provided Cloud Dataflow
    template pipelines‚Ä¶'
  prefs: []
  type: TYPE_NORMAL
- en: These Dataflow templates are an effort to solve simple, but large, in-Cloud
    data tasks, including data‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/GoogleCloudPlatform/DataflowTemplates?source=post_page-----100afa4b93e3--------------------------------)  [##
    Tutorials
  prefs: []
  type: TYPE_NORMAL
- en: Tutorials - AWS Data Pipeline The following tutorials walk you step-by-step
    through the process of creating and using‚Ä¶
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: docs.aws.amazon.com](https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html?source=post_page-----100afa4b93e3--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: AWS Data Pipeline documentation [[https://docs.aws.amazon.com/data-pipeline](https://docs.aws.amazon.com/data-pipeline/index.html)/]
  prefs: []
  type: TYPE_NORMAL
