["```py\nimport numpy as np\nimport math\n\nclass Network:\n    def __init__(self, layers):\n        self.layers = layers\n        self.activations = self.__init_activations_zero()\n        self.weights = self.__init_weights_random()\n        self.biases = self.__init_biases_zero()\n\n    def __init_activations_zero(self):\n        activations = []\n        for layer in self.layers:\n            activations.append(np.zeros(layer))\n        return activations\n\n    def __init_weights_random(self):\n        weights = []\n        for i in range(0, len(self.layers) - 1):\n            weights.append(np.random.uniform(-1, 1, (self.layers[i+1], self.layers[i])))\n        return weights\n\n    def __init_biases_zero(self):\n        biases = []\n        for i in range(1, len(self.layers)):\n            biases.append(np.zeros(self.layers[i]))\n        return biases\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def feedforward(self, input_layer):\n        self.activations[0] = input_layer\n        for i in range(0, len(self.layers) - 1):\n            self.activations[i+1] = self.sigmoid(np.dot(self.weights[i], self.activations[i]) + self.biases[i])\n```", "```py\nnetwork = Network([10, 3, 3, 2])\n\ninput_layer = np.random.uniform(-1, 1, (10))\n\nnetwork.feedforward(input_layer)\n\nprint(network.activations[-1])\n\n...\n\n[0.29059666 0.5261155 ]\n```", "```py\ndef backpropagation(self, expected_output):\n    # Calculate dcost_dactivations for the output layer\n    dcost_dactivations = 2 * (self.activations[-1] - expected_output)\n\n    # Loop backward through the layers to calculate dcost_dweights and dcost_dbiases\n    for i in range(-1, -len(self.layers), -1):\n        dactivations_dz = self.dsigmoid(np.dot(self.weights[i], self.activations[i-1]) + self.biases[i]) # Sigmoid output layer\n\n        dz_dweights = self.activations[i-1]\n        dz_dbiases = 1\n\n        self.dcost_dweights[i] += dz_dweights[np.newaxis,:] * (dactivations_dz * dcost_dactivations)[:,np.newaxis]\n        self.dcost_dbiases[i] += dz_dbiases * dactivations_dz * dcost_dactivations\n\n        # Calculate dcost_dactivations for hidden layer\n        dz_dactivations = self.weights[i]\n        dcost_dactivations = np.sum(dz_dactivations * (dactivations_dz * dcost_dactivations)[:,np.newaxis], axis=0)\n```", "```py\nimport numpy as np\n\nclass Network:\n    def __init__(self, layers, learning_rate):\n        self.layers = layers\n        self.learning_rate = learning_rate\n        self.activations = self.__init_activations_zero()\n        self.weights = self.__init_weights_random()\n        self.biases = self.__init_biases_zero()\n        self.G_weights = self.__init_G_weights()\n        self.G_biases = self.__init_G_biases()\n        self.dcost_dweights = self.__init_weights_zero()\n        self.dcost_dbiases = self.__init_biases_zero()\n        self.cost = 0\n        self.costs = []\n\n    def __init_activations_zero(self):\n        activations = []\n        for layer in self.layers:\n            activations.append(np.zeros(layer))\n        return activations\n\n    def __init_weights_random(self):\n        weights = []\n        for i in range(0, len(self.layers) - 1):\n            weights.append(np.random.uniform(-1, 1, (self.layers[i+1], self.layers[i])))\n        return weights\n\n    def __init_weights_zero(self):\n        weights = []\n        for i in range(0, len(self.layers) - 1):\n            weights.append(np.zeros((self.layers[i+1], self.layers[i])))\n        return weights\n\n    def __init_biases_zero(self):\n        biases = []\n        for i in range(1, len(self.layers)):\n            biases.append(np.zeros(self.layers[i]))\n        return biases\n\n    def __init_G_weights(self):\n        G_weights = []\n        for i in range(0, len(self.layers) - 1):\n            G_weights.append(np.zeros([len(self.weights[i]), len(self.weights[i][0]), len(self.weights[i][0])]))\n        return G_weights\n\n    def __init_G_biases(self):\n        G_biases = []\n        for i in range(0, len(self.layers) - 1):\n            G_biases.append(np.zeros([len(self.biases[i]), len(self.biases[i])]))\n        return G_biases\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n    def dsigmoid(self, x):\n        sig = self.sigmoid(x)\n        return sig * (1 - sig)\n\n    def calculate_cost(self, expected_output):\n        cost = np.sum((self.activations[-1] - expected_output)**2) # L2 cost function\n        return cost\n\n    def feedforward(self, input_layer):\n        self.activations[0] = input_layer\n        for i in range(0, len(self.layers) - 1):\n            self.activations[i+1] = self.sigmoid(np.dot(self.weights[i], self.activations[i]) + self.biases[i])\n\n    def backpropagation(self, expected_output):\n        # Calculate dcost_dactivations for the output layer\n        dcost_dactivations = 2 * (self.activations[-1] - expected_output)\n\n        # Loop backward through the layers to calculate dcost_dweights and dcost_dbiases\n        for i in range(-1, -len(self.layers), -1):\n            dactivations_dz = self.dsigmoid(np.dot(self.weights[i], self.activations[i-1]) + self.biases[i]) # Sigmoid output layer\n\n            dz_dweights = self.activations[i-1]\n            dz_dbiases = 1\n\n            self.dcost_dweights[i] += dz_dweights[np.newaxis,:] * (dactivations_dz * dcost_dactivations)[:,np.newaxis]\n            self.dcost_dbiases[i] += dz_dbiases * dactivations_dz * dcost_dactivations\n\n            # Calculate dcost_dactivations for hidden layer\n            dz_dactivations = self.weights[i]\n            dcost_dactivations = np.sum(dz_dactivations * (dactivations_dz * dcost_dactivations)[:,np.newaxis], axis=0)\n\n    def average_gradients(self, n):\n        # Calculate the average gradients for a batch containing n samples\n        for i in range(0, len(self.layers) - 1):\n            self.dcost_dweights[i] = self.dcost_dweights[i] / n\n            self.dcost_dbiases[i] = self.dcost_dbiases[i] / n\n\n    def reset_gradients(self):\n        # Reset gradients before starting a new batch\n        self.dcost_dweights = self.__init_weights_zero()\n        self.dcost_dbiases = self.__init_biases_zero()\n\n    def reset_cost(self):\n        self.cost = 0\n\n    def update_G(self):\n        for i in range(0, len(self.layers) - 1):\n            self.G_biases[i] += np.outer(self.dcost_dbiases[i], self.dcost_dbiases[i].T)\n            for j in range(0, len(self.weights[i])):\n                self.G_weights[i][j] += np.outer(self.dcost_dweights[i][j], self.dcost_dweights[i][j].T)\n\n    def update_weights_and_biases(self):\n        # Perform gradient descent step to update weights and biases\n        # Vanilla Gradient Descent\n        # for i in range(0, len(self.layers) - 1):\n        #     self.weights[i] -= (self.learning_rate * self.dcost_dweights[i])\n        #     self.biases[i] -= (self.learning_rate * self.dcost_dbiases[i])\n\n        # AdaGrad Gradient Desecent\n        self.update_G()\n        for i in range(0, len(self.layers) - 1):\n            self.biases[i] -= (self.learning_rate * (np.diag(self.G_biases[i]) + 0.00000001)**(-0.5)) * self.dcost_dbiases[i]\n            for j in range(0, len(self.weights[i])):\n                self.weights[i][j] -= (self.learning_rate * (np.diag(self.G_weights[i][j]) + 0.00000001)**(-0.5)) * self.dcost_dweights[i][j]\n\n    def process_batch(self, batch):\n        for sample in batch:\n            self.feedforward(sample['input_layer'])\n            self.backpropagation(sample['expected_output'])\n            self.cost += self.calculate_cost(sample['expected_output'])\n\n    def train_network(self, n_epochs, batches):\n        for epoch in range(0, n_epochs):\n            print(f\"Epoch: {epoch}\\n\")\n            for batch in batches:\n                self.process_batch(batch)\n                self.costs.append(self.cost / len(batch))\n                self.reset_cost()\n                self.average_gradients(len(batch))\n                self.update_weights_and_biases()\n                self.reset_gradients()\n                print(f\"Cost: {self.costs[-1]}\")\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.datasets import mnist\nfrom network import Network\n\ndef calculate_accuracy(network, x, y):\n    # Calculate network accuracy\n    correct = 0\n    for i in range(0, len(x)):\n        network.feedforward(x[i].flatten() / 255.0)\n        if np.where(network.activations[-1] == max(network.activations[-1]))[0][0] == y[i]:\n            correct += 1\n    print(f\"Accuracy: {correct / len(x)}\")\n\n# Prepare training data\n(train_X, train_y), (test_X, test_y) = mnist.load_data()\n\n# Define n_epochs and set up batches\nn_epochs = 5\nn_batches = 600\nbatches = []\ninput_layer = np.array_split(train_X, n_batches)\nexpected_output = np.array_split(np.eye(10)[train_y], n_batches)\nfor i in range(0, n_batches):\n    batch = []\n    for j in range(0, len(input_layer[i])):\n        batch.append({'input_layer' : input_layer[i][j].flatten() / 255.0, 'expected_output' : expected_output[i][j]})\n    batches.append(batch)\n\n# Setup and train network\nnetwork = Network([784,32,32,10], 0.1)\nnetwork.train_network(n_epochs, batches)\n\n# Calculate accuracy of the network\ncalculate_accuracy(network, test_X, test_y)\n\n...\n\nAccuracy: 0.942\n```"]