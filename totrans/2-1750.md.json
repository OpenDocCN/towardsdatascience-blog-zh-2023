["```py\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\nimport cv2\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport numpy as np # data processing\nimport matplotlib.pyplot as plt # Data visualization\nfrom tqdm import tqdm # Progress bar\n```", "```py\nimport os\nimport pandas as pd \n\nroot_dir = ... # Insert your data here\nsub_folders = [\"Cheetahs\", \"Lions\"] # Insert your classes here\nlabels = [0, 1]\n\ndata = []\n\nfor s, l in zip(sub_folders, labels):\n    for r, d, f in os.walk(root_dir + s):\n        for file in f:\n            if \".jpg\" in file:\n                data.append((os.path.join(s,file), l))\n\ndf = pd.DataFrame(data, columns=['file_name','label'])\n```", "```py\nimport seaborn as sns\nsns.countplot(data = df, x = 'label');\n```", "```py\nfig, ax = plt.subplots(2, 3, figsize=(10, 6))\n\nidx = 0\nfor i in range(2):\n    for j in range(3):\n\n        label = df.label[idx]\n        file_path = os.path.join(root_dir, df.file_name[idx])\n\n        # Read an image with OpenCV\n        image = cv2.imread(file_path)\n\n        # Convert the image to RGB color space.\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        # Resize image\n        image = cv2.resize(image, (256, 256))\n\n        ax[i,j].imshow(image)\n        ax[i,j].set_title(f\"Label: {label} ({'Lion' if label == 1 else 'Cheetah'})\")\n        ax[i,j].axis('off')\n        idx = idx+1\n\nplt.tight_layout()\nplt.show()\n```", "```py\nfrom sklearn.model_selection import train_test_split\n\ntrain_df, test_df = train_test_split(df, \n                                      test_size = 0.1, \n                                      random_state = 42)\n```", "```py\nfrom types import SimpleNamespace\n\ncfg = SimpleNamespace(**{})\n```", "```py\nclass CustomDataset(Dataset):\n    def __init__(self, df):\n        # Initialize anything you need later here ...\n        self.df = df\n        self.X = ...\n        self.y = ...\n        # ...\n\n    # Get the number of rows in the dataset\n    def __len__(self):\n        return len(self.df)\n\n    # Get a sample of the dataset\n    def __getitem__(self, idx):\n        return [self.X[idx], self.y[idx]]\n```", "```py\ncfg.root_dir = ... # Insert your data here\ncfg.image_size = 256\n\nclass CustomDataset(Dataset):\n    def __init__(self, \n                 cfg, \n                 df, \n                 transform=None, \n                mode = \"val\"):\n        self.root_dir = cfg.root_dir\n        self.df = df\n        self.file_names = df['file_name'].values\n        self.labels = df['label'].values\n\n        if transform:\n          self.transform = transform\n        else:\n          self.transform = A.Compose([\n                              A.Resize(cfg.image_size, cfg.image_size), \n                              ToTensorV2(),\n                           ])\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        # Get file_path and label for index\n        label = self.labels[idx]\n        file_path = os.path.join(self.root_dir, self.file_names[idx])\n\n        # Read an image with OpenCV\n        image = cv2.imread(file_path)\n\n        # Convert the image to RGB color space.\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        # Apply augmentations        \n        augmented = self.transform(image=image)\n        image = augmented['image']\n\n        # Normalize because ToTensorV2() doesn't normalize the image\n        image = image/255\n\n        return image, label\n```", "```py\ncfg.batch_size = 32\n\nexample_dataset = CustomDataset(cfg, df)\n\nexample_dataloader = DataLoader(example_dataset, \n                              batch_size = cfg.batch_size, \n                              shuffle = True, \n                              num_workers=0,\n                             )\n```", "```py\nfor (image_batch, label_batch) in example_dataloader:\n    print(image_batch.shape)\n    print(label_batch.shape)\n    break\n```", "```py\ntorch.Size([32, 3, 256, 256])\ntorch.Size([32])\n```", "```py\nX = df\ny = df.label\n\ntrain_df, valid_df, y_train, y_test = train_test_split(X, \n                                                       y, \n                                                       test_size = 0.2, \n                                                       random_state = 42)\n```", "```py\ntrain_dataset = CustomDataset(cfg, train_df)\nvalid_dataset = CustomDataset(cfg, valid_df)\n\ntrain_dataloader = DataLoader(train_dataset, \n                          batch_size = cfg.batch_size, \n                          shuffle = True)\n\nvalid_dataloader = DataLoader(valid_dataset, \n                          batch_size = cfg.batch_size, \n                          shuffle = False)\n```", "```py\nimport timm\n\ncfg.n_classes = 2\ncfg.backbone = 'resnet18'\n\nmodel = timm.create_model(cfg.backbone, \n                          pretrained = True, \n                          num_classes = cfg.n_classes)\n```", "```py\nX = torch.randn(cfg.batch_size, 3, cfg.image_size, cfg.image_size)\ny = model(X)\n```", "```py\ncriterion = nn.CrossEntropyLoss()\n```", "```py\ncfg.learning_rate = 1e-4\n\noptimizer = torch.optim.Adam(\n  model.parameters(), \n  lr = cfg.learning_rate, \n  weight_decay = 0,\n )\n```", "```py\ncfg.lr_min = 1e-5\ncfg.epochs = 5\n\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n  optimizer, \n  T_max = np.ceil(len(train_dataloader.dataset) / cfg.batch_size) * cfg.epochs,\n  eta_min = cfg.lr_min\n)\n```", "```py\nfrom sklearn.metrics import accuracy_score\n\ndef calculate_metric(y, y_pred):\n  metric = accuracy_score(y, y_pred)\n  return metric\n```", "```py\ncfg.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\ndef train_one_epoch(dataloader, model, optimizer, scheduler, cfg):\n    # Training mode\n    model.train()\n\n    # Init lists to store y and y_pred\n    final_y = []\n    final_y_pred = []\n    final_loss = []\n\n    # Iterate over data\n    for step, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n        X = batch[0].to(cfg.device)\n        y = batch[1].to(cfg.device)\n\n        # Zero the parameter gradients\n        optimizer.zero_grad()\n\n        with torch.set_grad_enabled(True):\n            # Forward: Get model outputs\n            y_pred = model(X)\n\n            # Forward: Calculate loss\n            loss = criterion(y_pred, y)\n\n            # Covert y and y_pred to lists\n            y =  y.detach().cpu().numpy().tolist()\n            y_pred =  y_pred.detach().cpu().numpy().tolist()\n\n            # Extend original list\n            final_y.extend(y)\n            final_y_pred.extend(y_pred)\n            final_loss.append(loss.item())\n\n            # Backward: Optimize\n            loss.backward()\n            optimizer.step()\n\n        scheduler.step()\n\n    # Calculate statistics\n    loss = np.mean(final_loss)\n    final_y_pred = np.argmax(final_y_pred, axis=1)\n    metric = calculate_metric(final_y, final_y_pred)\n\n    return metric, loss\n```", "```py\ndef validate_one_epoch(dataloader, model, cfg):\n    # Validation mode\n    model.eval()\n\n    final_y = []\n    final_y_pred = []\n    final_loss = []\n\n    # Iterate over data\n    for step, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n        X = batch[0].to(cfg.device)\n        y = batch[1].to(cfg.device)\n\n        with torch.no_grad():\n            # Forward: Get model outputs\n            y_pred = model(X)\n\n            # Forward: Calculate loss\n            loss = criterion(y_pred, y)  \n\n            # Covert y and y_pred to lists\n            y =  y.detach().cpu().numpy().tolist()\n            y_pred =  y_pred.detach().cpu().numpy().tolist()\n\n            # Extend original list\n            final_y.extend(y)\n            final_y_pred.extend(y_pred)\n            final_loss.append(loss.item())\n\n    # Calculate statistics\n    loss = np.mean(final_loss)\n    final_y_pred = np.argmax(final_y_pred, axis=1)\n    metric = calculate_metric(final_y, final_y_pred)\n\n    return metric, loss\n```", "```py\nfrom sklearn.model_selection import StratifiedKFold\n\ncfg.n_folds = 5\n\n# Create a new column for cross-validation folds\ndf[\"kfold\"] = -1\n\n# Initialize the kfold class\nskf = StratifiedKFold(n_splits=cfg.n_folds)\n\n# Fill the new column\nfor fold, (train_, val_) in enumerate(skf.split(X = df, y = df.label)):\n      df.loc[val_ , \"kfold\"] = fold\n\nfor fold in range(cfg.n_folds):\n    train_df = df[df.kfold != fold].reset_index(drop=True)\n    valid_df = df[df.kfold == fold].reset_index(drop=True)\n```", "```py\ntransform_soft = A.Compose([A.Resize(cfg.image_size, cfg.image_size),\n                             A.Rotate(p=0.6, limit=[-45,45]),\n                             A.HorizontalFlip(p = 0.6),\n                             A.CoarseDropout(max_holes = 1, max_height = 64, max_width = 64, p=0.3),\n                             ToTensorV2()])\n```", "```py\nimport random\n\ndef set_seed(seed=1234):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n\n    # In general seed PyTorch operations\n    torch.manual_seed(seed)\n\n    # If you are using CUDA on 1 GPU, seed it\n    torch.cuda.manual_seed(seed)\n\n    # If you are using CUDA on more than 1 GPU, seed them all\n    torch.cuda.manual_seed_all(cfg.seed)\n\n    # Certain operations in Cudnn are not deterministic, and this line will force them to behave!\n    torch.backends.cudnn.deterministic = True \n\n    # Disable the inbuilt cudnn auto-tuner that finds the best algorithm to use for your hardware.\n    torch.backends.cudnn.benchmark = False\n```", "```py\ncfg.seed = 42\n\ndef fit(model, optimizer, scheduler, cfg, train_dataloader, valid_dataloader=None):\n    acc_list = []\n    loss_list = []\n    val_acc_list = []\n    val_loss_list = []\n\n    for epoch in range(cfg.epochs):\n        print(f\"Epoch {epoch + 1}/{cfg.epochs}\")\n\n        set_seed(cfg.seed + epoch)\n\n        acc, loss = train_one_epoch(train_dataloader, model, optimizer, scheduler, cfg)\n\n        if valid_dataloader:\n            val_acc, val_loss = validate_one_epoch(valid_dataloader, model, cfg)\n\n        print(f'Loss: {loss:.4f} Acc: {acc:.4f}')\n        acc_list.append(acc)\n        loss_list.append(loss)\n\n        if valid_dataloader:\n            print(f'Val Loss: {val_loss:.4f} Val Acc: {val_acc:.4f}')\n            val_acc_list.append(val_acc)\n            val_loss_list.append(val_loss)\n\n    return acc_list, loss_list, val_acc_list, val_loss_list, model\n```", "```py\ndef visualize_history(acc, loss, val_acc, val_loss):\n    fig, ax = plt.subplots(1,2, figsize=(12,4))\n\n    ax[0].plot(range(len(loss)), loss,  color='darkgrey', label = 'train')\n    ax[0].plot(range(len(val_loss)), val_loss,  color='cornflowerblue', label = 'valid')\n    ax[0].set_title('Loss')\n\n    ax[1].plot(range(len(acc)), acc,  color='darkgrey', label = 'train')\n    ax[1].plot(range(len(val_acc)), val_acc,  color='cornflowerblue', label = 'valid')\n    ax[1].set_title('Metric (Accuracy)')\n\n    for i in range(2):\n        ax[i].set_xlabel('Epochs')\n        ax[i].legend(loc=\"upper right\")\n    plt.show()\n```", "```py\nfor fold in range(cfg.n_folds):\n    train_df = df[df.kfold != fold].reset_index(drop=True)\n    valid_df = df[df.kfold == fold].reset_index(drop=True)\n\n    train_dataset = CustomDataset(cfg, train_df, transform = transform_soft)\n    valid_dataset = CustomDataset(cfg, valid_df)\n\n    train_dataloader = DataLoader(train_dataset, \n                              batch_size = cfg.batch_size, \n                              shuffle = True, \n                              num_workers = 0,\n                             )\n    valid_dataloader = DataLoader(valid_dataset, \n                              batch_size = cfg.batch_size, \n                              shuffle = False, \n                              num_workers = 0,\n                             )\n\n    model = timm.create_model(cfg.backbone, \n                              pretrained = True, \n                              num_classes = cfg.n_classes)\n\n    model = model.to(cfg.device)\n\n    criterion = nn.CrossEntropyLoss()\n\n    optimizer = torch.optim.Adam(model.parameters(), \n                                 lr = cfg.learning_rate, \n                                 weight_decay = 0,\n                                )\n\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, \n                                                           T_max= np.ceil(len(train_dataloader.dataset) / cfg.batch_size) * cfg.epochs,\n                                                           eta_min=cfg.lr_min)\n\n    acc, loss, val_acc, val_loss, model, lrs = fit(model, optimizer, scheduler, cfg, train_dataloader, valid_dataloader)\n\n    visualize_history(acc, loss, val_acc, val_loss)\n```", "```py\ntrain_df = df.copy()\n\ntrain_dataset = CustomDataset(cfg, train_df, transform = transform_soft)\n\ntrain_dataloader = DataLoader(train_dataset, \n                          batch_size = cfg.batch_size, \n                          shuffle = True, \n                          num_workers = 0,\n                         )\n```", "```py\nmodel = timm.create_model(cfg.backbone, \n                          pretrained = True, \n                          num_classes = cfg.n_classes)\n\nmodel = model.to(cfg.device)\n\ncriterion = nn.CrossEntropyLoss()\n\noptimizer = torch.optim.Adam(model.parameters(), \n                             lr = cfg.learning_rate, \n                             weight_decay = 0,\n                            )\n\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, \n                                                       T_max= np.ceil(len(train_dataloader.dataset) / cfg.batch_size) * cfg.epochs,\n                                                       eta_min=cfg.lr_min)\n\nacc, loss, val_acc, val_loss, model = fit(model, optimizer, scheduler, cfg, train_dataloader)\n```", "```py\ntest_dataset = CustomDataset(cfg, test_df)\n\ntest_dataloader = DataLoader(test_dataset, \n                          batch_size = cfg.batch_size, \n                          shuffle = False, \n                          num_workers = 0,\n                         )\n\ndataloader = test_dataloader\n\n# Validation mode\nmodel.eval()\n\nfinal_y = []\nfinal_y_pred = []\n\n# Iterate over data\nfor step, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n    X = batch[0].to(cfg.device)\n    y = batch[1].to(cfg.device)\n\n    with torch.no_grad():\n        # Forward: Get model outputs\n        y_pred = model(X)\n\n        # Covert y and y_pred to lists\n        y =  y.detach().cpu().numpy().tolist()\n        y_pred =  y_pred.detach().cpu().numpy().tolist()\n\n        # Extend original list\n        final_y.extend(y)\n        final_y_pred.extend(y_pred)\n\n# Calculate statistics\nfinal_y_pred_argmax = np.argmax(final_y_pred, axis=1)\nmetric = calculate_metric(final_y, final_y_pred_argmax)\n\ntest_df['prediction'] = final_y_pred_argmax\n```"]