["```py\nimport math\nimport torch\nimport torch.nn as nn\nimport torchtext\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndf = pd.read_csv('spam_ham.csv')\ndf_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\nprint(df_train.head())\n\n# Output\n'''\n     Category                                            Message\n1978     spam  Reply to win Â£100 weekly! Where will the 2006 ...\n3989      ham  Hello. Sort of out in town already. That . So ...\n3935      ham   How come guoyang go n tell her? Then u told her?\n4078      ham  Hey sathya till now we dint meet not even a si...\n4086     spam  Orange brings you ringtones from all time Char...\n'''\n```", "```py\nlabels = df_train[\"Category\"].unique()\nnum_labels = len(labels)\nlabel2id, id2label = dict(), dict()\nfor i, label in enumerate(labels):\n    label2id[label] = i\n    id2label[i] = label\n\nprint(id2label)\nprint(label2id)\n\n# Output\n'''\n{0: 'spam', 1: 'ham'}\n{'spam': 0, 'ham': 1}\n'''\n```", "```py\n# Load tokenizer\ntokenizer = get_tokenizer('basic_english')\n\ntext = 'this is text'\nprint(tokenizer(text))\n\n# Output\n'''\n[this, is, text]\n'''\n```", "```py\n# Initialize training data iterator\nclass TextIter(torch.utils.data.Dataset):\n\n  def __init__(self, input_data):\n      self.text = input_data['Message'].values.tolist()\n  def __len__(self):\n      return len(self.text)\n  def __getitem__(self, idx):\n      return self.text[idx]\n\n# Build vocabulary\ndef yield_tokens(data_iter):\n    for text in data_iter:\n        yield tokenizer(text)\n\ndata_iter = TextIter(df_train)\nvocab = build_vocab_from_iterator(yield_tokens(data_iter), specials=[\"<pad>\", \"<unk>\"])\nvocab.set_default_index(vocab[\"<unk>\"])\nprint(vocab.get_stoi())\n\n# Output\n'''\n{'<pad>':0, '<unk>':1,..., 'ny-usa': 7449, ...}\n'''\n```", "```py\ntext_unk = 'this is jkjkj' # jkjkj is an unknown word in our vocab\nseq_unk = [vocab[word] for word in tokenizer(text_unk)]\n\nprint(tokenizer(text_unk))\nprint(seq_unk)\n\n# Output\n'''\n['this', 'is', 'jkjkj']\n[49, 15, 1]\n''' \n```", "```py\n# We will use this example throughout the article\ntext = 'this is text' \nseq = [vocab[word] for word in tokenizer(text)]\n\nprint(tokenizer(text))\nprint(seq)\n\n# Output\n'''\n['this', 'is', 'text']\n[49, 15, 81]\n'''\n```", "```py\nclass Embeddings(nn.Module):\n    def __init__(self, d_model, vocab_size):\n        super(Embeddings, self).__init__()\n        self.emb = nn.Embedding(vocab_size, d_model)\n        self.d_model = d_model\n\n    def forward(self, x):\n        return self.emb(x) * math.sqrt(self.d_model)\n```", "```py\nhidden_size = 4\n\ninput_data = torch.LongTensor(seq).unsqueeze(0)\nemb_model = Embeddings(hidden_size, len(vocab))\ntoken_emb = emb_model(input_data) \nprint(f'Size of token embedding: {token_emb.size()}')\n\n# Output\n'''\nSize of token embedding: torch.Size([1, 3, 4]) [batch, no. seq token, dim]\n'''\n```", "```py\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, vocab_size=5000, dropout=0.1):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(vocab_size, d_model)\n        position = torch.arange(0, vocab_size, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, d_model, 2).float()\n            * (-math.log(10000.0) / d_model)\n        )\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer(\"pe\", pe)\n\n    def forward(self, x):\n        x = x + self.pe[:, : x.size(1), :]\n        return self.dropout(x)\n```", "```py\npe_model = PositionalEncoding(d_model=4, vocab_size=len(vocab))\noutput_pe = pe_model(token_emb)\n\nprint(f'Size of output embedding: {output_pe.size()}')\n\n# Output\n'''\nSize of output embedding: torch.Size([1, 3, 4]) [batch, no. seq token, dim]\n'''\n```", "```py\nclass SingleHeadAttention(nn.Module):\n    def __init__(self, d_model, d_head_size):\n        super().__init__()\n        self.lin_key = nn.Linear(d_model, d_head_size, bias=False)\n        self.lin_query = nn.Linear(d_model, d_head_size, bias=False)\n        self.lin_value = nn.Linear(d_model, d_head_size, bias=False)\n        self.d_model = d_model\n\n    def forward(self, x):\n        query = self.lin_query(x)\n        key = self.lin_key(x)\n        value = self.lin_value(x)\n\n        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.d_model)\n        p_attn = scores.softmax(dim=-1)\n        x = torch.matmul(p_attn, value)\n\n        return x\n```", "```py\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        super().__init__()\n        assert d_model % h == 0\n        d_k = d_model // h\n        self.multi_head = nn.ModuleList([SingleHeadAttention(d_model, d_k) for _ in range(h)])\n        self.lin_agg = nn.Linear(d_model, d_model)\n\n    def forward(self, x):\n        x = torch.cat([head(x) for head in self.multi_head], dim=-1)\n        return self.lin_agg(x)\n```", "```py\nmult_att = MultiHeadAttention(h=2, d_model=4)\noutput_mult_att = mult_att(output_pe)\nprint(f'Size of output embedding after multi-head attention: {output_mult_att.size()}')\n\n# Output\n'''\nSize of output embedding after multi-head attention: torch.Size([1, 3, 4])\n'''\n```", "```py\nclass LayerNorm(nn.Module):\n    def __init__(self, d_model, eps=1e-6):\n        super(LayerNorm, self).__init__()\n        self.a_2 = nn.Parameter(torch.ones(d_model))\n        self.b_2 = nn.Parameter(torch.zeros(d_model))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n\nclass ResidualConnection(nn.Module):\n    def __init__(self, d_model, dropout=0.1):\n        super().__init__()\n        self.norm = LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x1, x2):\n        return self.dropout(self.norm(x1 + x2))\n```", "```py\nres_conn_1 = ResidualConnection(d_model=4)\noutput_res_conn_1 = res_conn_1(output_pe, output_mult_att)\n\nprint(f'Size of output embedding after residual connection: {output_res_conn_1.size()}')\n\n# Output\n'''\nSize of output embedding after residual connection: torch.Size([1, 3, 4])\n''' \n```", "```py\nclass FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super().__init__()\n        self.w_1 = nn.Linear(d_model, d_ff)\n        self.w_2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        return self.w_2(self.dropout(self.w_1(x).relu()))\n```", "```py\nff = FeedForward(d_model=4, d_ff=12)\noutput_ff = ff(output_res_conn_1)\n\nprint(f'Size of output embedding after feed-forward network: {output_ff.size()}')\n\n# Output\n'''\nSize of output embedding after feed-forward network: torch.Size([1, 3, 4])\n''' \n```", "```py\nres_conn_2 = ResidualConnection(d_model=4)\noutput_res_conn_2 = res_conn_2(output_res_conn_1, output_ff)\n\nprint(f'Size of output embedding after second residual: {output_res_conn_2.size()}')\n\n# Output\n'''\nSize of output embedding after second residual: torch.Size([1, 3, 4])\n'''\n```", "```py\nclass SingleEncoder(nn.Module):\n    def __init__(self, d_model, self_attn, feed_forward, dropout):\n        super().__init__()\n        self.self_attn = self_attn\n        self.feed_forward = feed_forward\n        self.res_1 = ResidualConnection(d_model, dropout)\n        self.res_2 = ResidualConnection(d_model, dropout)\n\n        self.d_model = d_model\n\n    def forward(self, x):\n        x_attn = self.self_attn(x)\n        x_res_1 = self.res_1(x, x_attn)\n        x_ff = self.feed_forward(x_res_1)\n        x_res_2 = self.res_2(x_res_1, x_ff)\n\n        return x_res_2\n```", "```py\nclass EncoderBlocks(nn.Module):\n    def __init__(self, layer, N):\n        super().__init__()\n        self.layers = nn.ModuleList([layer for _ in range(N)])\n        self.norm = LayerNorm(layer.d_model)\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        return self.norm(x)\n```", "```py\nclass TransformerEncoderModel(nn.Module):\n    def __init__(self, vocab_size, d_model, nhead, d_ff, N,\n                dropout=0.1):\n        super().__init__()\n        assert d_model % nhead == 0, \"nheads must divide evenly into d_model\"\n\n        self.emb = Embeddings(d_model, vocab_size)\n        self.pos_encoder = PositionalEncoding(d_model=d_model, vocab_size=vocab_size)\n\n        attn = MultiHeadAttention(nhead, d_model)\n        ff = FeedForward(d_model, d_ff, dropout)\n        self.transformer_encoder = EncoderBlocks(SingleEncoder(d_model, attn, ff, dropout), N)\n        self.classifier = nn.Linear(d_model, 2)\n        self.d_model = d_model\n\n    def forward(self, x):\n        x = self.emb(x) * math.sqrt(self.d_model)\n        x = self.pos_encoder(x)\n        x = self.transformer_encoder(x)\n        x = x.mean(dim=1)\n        x = self.classifier(x)\n        return x\n\nmodel = TransformerEncoderModel(len(vocab), d_model=300, nhead=4, d_ff=50, \n                                    N=6, dropout=0.1).to(device)\n```", "```py\nclass TextDataset(torch.utils.data.Dataset):\n\n  def __init__(self, input_data):        \n      self.text = input_data['Message'].values.tolist()\n      self.label = [int(label2id[i]) for i in input_data['Category'].values.tolist()]\n\n  def __len__(self):\n      return len(self.label)\n\n  def get_sequence_token(self, idx):\n      sequence = [vocab[word] for word in tokenizer(self.text[idx])]\n      len_seq = len(sequence)\n      return sequence, len_seq\n\n  def get_labels(self, idx):\n      return self.label[idx]\n\n  def __getitem__(self, idx):\n      sequence, len_seq = self.get_sequence_token(idx)\n      label = self.get_labels(idx)\n      return sequence, label, len_seq\n\ndef collate_fn(batch):\n\n    sequences, labels, lengths = zip(*batch)\n    max_len = max(lengths)\n\n    for i in range(len(batch)):\n        if len(sequences[i]) != max_len:\n          for j in range(len(sequences[i]),max_len):\n            sequences[i].append(0)\n\n    return torch.tensor(sequences, dtype=torch.long), torch.tensor(labels, dtype=torch.long)\n```", "```py\ndef train(model, dataset, epochs, lr, bs):\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam((p for p in model.parameters() \n      if p.requires_grad), lr=lr)\n    train_dataset = TextDataset(dataset)\n    train_dataloader = DataLoader(train_dataset, num_workers=1, batch_size=bs, collate_fn=collate_fn, shuffle=True)\n\n    # Training loop\n    for epoch in range(epochs):\n        total_loss_train = 0\n        total_acc_train = 0   \n        for train_sequence, train_label in tqdm(train_dataloader):\n\n            # Model prediction\n            predictions = model(train_sequence.to(device))\n            labels = train_label.to(device)\n            loss = criterion(predictions, labels)\n\n            # Calculate accuracy and loss per batch\n            correct = predictions.argmax(axis=1) == labels\n            acc = correct.sum().item() / correct.size(0)\n            total_acc_train += correct.sum().item()\n            total_loss_train += loss.item()\n\n            # Backprop\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n            optimizer.step()\n\n        print(f'Epochs: {epoch + 1} | Loss: {total_loss_train / len(train_dataset): .3f} | Accuracy: {total_acc_train / len(train_dataset): .3f}')\n\nepochs = 15\nlr = 1e-4\nbatch_size = 4\ntrain(model, df_train, epochs, lr, batch_size)\n```", "```py\ndef predict(text):\n  sequence = torch.tensor([vocab[word] for word in tokenizer(text)], dtype=torch.long).unsqueeze(0)\n  output = model(sequence.to(device))\n  prediction = id2label[output.argmax(axis=1).item()]\n\n  return prediction\n```", "```py\nidx = 24\ntext = df_test['Message'].values.tolist()[idx]\ngt = df_test['Category'].values.tolist()[idx]\nprediction = predict(text)\n\nprint(f'Text: {text}')\nprint(f'Ground Truth: {gt}')\nprint(f'Prediction: {prediction}')\n\n# Output\n'''\nText: This is the 2nd time we have tried 2 contact u. U have won the Â£750 Pound prize. 2 claim is easy, call 087187272008 NOW1! Only 10p per minute. BT-national-rate.\nGround Truth: spam\nPrediction: spam\n''' \n```", "```py\nidx = 35\ntext = df_test['Message'].values.tolist()[idx]\ngt = df_test['Category'].values.tolist()[idx]\nprediction = predict(text)\n\nprint(f'Text: {text}')\nprint(f'Ground Truth: {gt}')\nprint(f'Prediction: {prediction}')\n\n# Output\n'''\nText: Morning only i can ok.\nGround Truth: ham\nPrediction: ham\n'''\n```"]