- en: 'ü¶úüîó LangChain: Question Answering Agent over Docs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/langchain-question-answering-agent-over-docs-18e5585bdbd3](https://towardsdatascience.com/langchain-question-answering-agent-over-docs-18e5585bdbd3)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/1c8d2ceae9a87852899c61a5f3be5e5c.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Mike Alonzo](https://unsplash.com/@mikezo?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Learn about embeddings and agents to build a QA application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@marcellopoliti?source=post_page-----18e5585bdbd3--------------------------------)[![Marcello
    Politi](../Images/484e44571bd2e75acfe5fef3146ab3c2.png)](https://medium.com/@marcellopoliti?source=post_page-----18e5585bdbd3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----18e5585bdbd3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----18e5585bdbd3--------------------------------)
    [Marcello Politi](https://medium.com/@marcellopoliti?source=post_page-----18e5585bdbd3--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----18e5585bdbd3--------------------------------)
    ¬∑6 min read¬∑Jun 4, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most common use cases in the NLP field is **question-answering related
    to documents**. For example, imagine feeding a pdf or perhaps multiple pdf files
    to the machine and then asking questions related to those files. This can be useful,
    for example, if you have to prepare for a university exam and want to ask the
    machine about things you didn‚Äôt understand. Actually, a more advanced use case
    would be to ask the machine to ask you questions to perform a sort of mock interrogation.
  prefs: []
  type: TYPE_NORMAL
- en: A lot of research has been done to solve this task and many tools have been
    developed, but today we can use the power of Large Language Models (LLMs) such
    as ag example GPT-3 from OpenAI and beyond.
  prefs: []
  type: TYPE_NORMAL
- en: LangChain is a very recent library that allows us to manage and create applications
    based on LLMs. In fact, the **LLM is just one part of a much more complex AI architecture**.
    When we create a system like this we do not only have to make queries to the OpenAI
    models and get a response, but for example save this response, structure the prompt
    properly etc.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we see how to build a simple Question Answering over Docs application
    using LangChain and OpenAI.
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this application, we will make use of a library called ChromaDB. This is
    an open-source library that allows us to save embeddings. Here a question might
    arise: **But what are embeddings?**'
  prefs: []
  type: TYPE_NORMAL
- en: '**An embedding is nothing more than a projection in a vector space of a word**
    (or text).'
  prefs: []
  type: TYPE_NORMAL
- en: 'I try to explain myself in a simpler way. Suppose we have the following words
    available: ‚Äúking,‚Äù ‚Äúqueen,‚Äù ‚Äúman,‚Äù and ‚Äúwoman.‚Äù'
  prefs: []
  type: TYPE_NORMAL
- en: We people from our experience intuitively understand distances between these
    words. For example, man is closer to king conceptually than queen. But machines
    are not intuitive they need data and metrics to work. So what we do is turn these
    words into data on a Cartesian space so that this intuitive concept of distance
    is represented accurately.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bf99a853a4b7a26870f0caba5f8a1656.png)'
  prefs: []
  type: TYPE_IMG
- en: Embedding Example (Image By Author)
  prefs: []
  type: TYPE_NORMAL
- en: In the image above we have an example (dummy) of embeddings. We see that Man
    is much closer to King than to the other words, and the same is true for woman
    and queen.
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting thing is that **the distance between man and king is the
    same as that between woman and queen**. So somehow it seems that this embedding
    has really captured the essence of these words.
  prefs: []
  type: TYPE_NORMAL
- en: Another thing to specify though is that of the metric, i.e. how the **distance**
    is measured. In most cases, this is measured using **cosin similarity** i.e. using
    the cosine of the angle between two embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/80e63e7fcd76cf55f4640b7248e5b062.png)'
  prefs: []
  type: TYPE_IMG
- en: Cosin Similarity (Image By Author)
  prefs: []
  type: TYPE_NORMAL
- en: The embedding in my example has only two dimensions, two axes. But the embeddings
    that are created by modern algorithms like BERT have **hundreds or thousands of
    axes**, so it‚Äôs hard to understand why the algorithm put text at a particular
    point in space.
  prefs: []
  type: TYPE_NORMAL
- en: In this [demo](https://www.cs.cmu.edu/~dst/WordEmbeddingDemo/), you can navigate
    a real embedding space in 3 dimensions and see how the words are near or far from
    each other.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs code!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First of all, we need to install some libraries. Surely we will need Langchain
    and OpenAI to instantiate and manage LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: After that we will go to install ChromaDB and TikToken (the latter is required
    to successfully install ChromaDB)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now we need a text file that we are going to work on. In fact, our purpose is
    to ask questions to the LLM about this file. Downloading a file with Python is
    very simple, it can be done with the following commands.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now we import all the classes we will need.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Obviously to use the OpenAI templates, and to do this you have to enter your
    personal API KEY. If you don‚Äôt know how to do this you can see [my previous article
    about Langchain](https://medium.com/towards-data-science/develop-applications-powered-by-language-models-with-langchain-d2f7a1d1ad1a).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In a real application, you will probably have many text files, and you want
    the LLM to figure out in which of these texts is the answer to your question.
    Instead, in this simple example **we break the single text file into multiple
    parts (chunks) and treat each part as a different document**. The model will have
    to figure out which part contains the answer to our question. We break this text
    into multiple parts by assigning each part a maximum length using the commands
    below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We see that 64 parts have been created from the original text.
  prefs: []
  type: TYPE_NORMAL
- en: We can print the different parts individually since they are contained in a
    list.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now we create an object that we need to save the embeddings of the various parts
    of the created text.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: But we want to **save the embeddings in a DB that is persistent** because recreating
    them every time we open the application would be a resource waste. This is where
    ChromaDB helps us. We can create and save the embeddings using text parts, and
    add metadata for each part. In this, the metadata will be strings that name each
    text part.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Now we want to turn docsearch into a retrieval because that will be its purpose.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We can also see the retriever what distance metric it is using, in this case,
    the default one is similarity as explained in the part on embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we can ask the retriever to take the document that most answers one
    of our queries. The retriever could also take more than one document if necessary.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Let‚Äôs see now how many documents he took and what they contain.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Now what we can do is to create an agent. **An agent is able to perform a series
    of steps to solve the user‚Äôs task on its own**. Our agent will have to go and
    look through the documents available to it where the answer to the question asked
    is and return that document.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: If we want, we can also create a function to post-process the agent‚Äôs output
    so that it is more readable.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Now everything is finally ready, we can use our agent and go and answer our
    queries!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Final Thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we introduced LangChain, ChromaDB and some explanation about
    embeddings. We saw with a simple example how to save embeddings of several documents,
    or parts of a document, into a persistent database and do retrieval of the desired
    part to answer a user query.
  prefs: []
  type: TYPE_NORMAL
- en: If you found this article useful follow me here on Medium! [üòâ](https://emojipedia.org/it/apple/ios-15.4/faccina-che-fa-l-occhiolino/)
  prefs: []
  type: TYPE_NORMAL
- en: The End
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Marcello Politi*'
  prefs: []
  type: TYPE_NORMAL
- en: '[Linkedin](https://www.linkedin.com/in/marcello-politi/), [Twitter](https://twitter.com/_March08_),
    [Website](https://marcello-politi.super.site/)'
  prefs: []
  type: TYPE_NORMAL
