- en: 'ğŸ¦œğŸ”— LangChain: Question Answering Agent over Docs'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ¦œğŸ”— LangChainï¼šæ–‡æ¡£ä¸Šçš„é—®ç­”ä»£ç†
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/langchain-question-answering-agent-over-docs-18e5585bdbd3](https://towardsdatascience.com/langchain-question-answering-agent-over-docs-18e5585bdbd3)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/langchain-question-answering-agent-over-docs-18e5585bdbd3](https://towardsdatascience.com/langchain-question-answering-agent-over-docs-18e5585bdbd3)
- en: '![](../Images/1c8d2ceae9a87852899c61a5f3be5e5c.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c8d2ceae9a87852899c61a5f3be5e5c.png)'
- en: Photo by [Mike Alonzo](https://unsplash.com/@mikezo?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ç”± [Mike Alonzo](https://unsplash.com/@mikezo?utm_source=medium&utm_medium=referral)
    æä¾›çš„ç…§ç‰‡ï¼Œæ¥è‡ª [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Learn about embeddings and agents to build a QA application
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: äº†è§£åµŒå…¥å’Œä»£ç†ä»¥æ„å»ºé—®ç­”åº”ç”¨
- en: '[](https://medium.com/@marcellopoliti?source=post_page-----18e5585bdbd3--------------------------------)[![Marcello
    Politi](../Images/484e44571bd2e75acfe5fef3146ab3c2.png)](https://medium.com/@marcellopoliti?source=post_page-----18e5585bdbd3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----18e5585bdbd3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----18e5585bdbd3--------------------------------)
    [Marcello Politi](https://medium.com/@marcellopoliti?source=post_page-----18e5585bdbd3--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@marcellopoliti?source=post_page-----18e5585bdbd3--------------------------------)[![Marcello
    Politi](../Images/484e44571bd2e75acfe5fef3146ab3c2.png)](https://medium.com/@marcellopoliti?source=post_page-----18e5585bdbd3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----18e5585bdbd3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----18e5585bdbd3--------------------------------)
    [Marcello Politi](https://medium.com/@marcellopoliti?source=post_page-----18e5585bdbd3--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----18e5585bdbd3--------------------------------)
    Â·6 min readÂ·Jun 4, 2023
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----18e5585bdbd3--------------------------------)
    Â·6 åˆ†é’Ÿé˜…è¯»Â·2023å¹´6æœˆ4æ—¥
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»‹ç»
- en: One of the most common use cases in the NLP field is **question-answering related
    to documents**. For example, imagine feeding a pdf or perhaps multiple pdf files
    to the machine and then asking questions related to those files. This can be useful,
    for example, if you have to prepare for a university exam and want to ask the
    machine about things you didnâ€™t understand. Actually, a more advanced use case
    would be to ask the machine to ask you questions to perform a sort of mock interrogation.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼Œæœ€å¸¸è§çš„ç”¨ä¾‹ä¹‹ä¸€æ˜¯**ä¸æ–‡æ¡£ç›¸å…³çš„é—®ç­”**ã€‚ä¾‹å¦‚ï¼Œæƒ³è±¡ä¸€ä¸‹å°†ä¸€ä¸ªæˆ–å¤šä¸ª PDF æ–‡ä»¶è¾“å…¥åˆ°æœºå™¨ä¸­ï¼Œç„¶åè¯¢é—®ä¸è¿™äº›æ–‡ä»¶ç›¸å…³çš„é—®é¢˜ã€‚è¿™å¯èƒ½å¾ˆæœ‰ç”¨ï¼Œæ¯”å¦‚å½“ä½ éœ€è¦ä¸ºå¤§å­¦è€ƒè¯•åšå‡†å¤‡æ—¶ï¼Œå¹¶ä¸”æƒ³è¦å‘æœºå™¨è¯¢é—®ä½ ä¸ç†è§£çš„å†…å®¹ã€‚å®é™…ä¸Šï¼Œä¸€ä¸ªæ›´é«˜çº§çš„ç”¨ä¾‹æ˜¯è®©æœºå™¨å‘ä½ æé—®ï¼Œè¿›è¡Œä¸€ç§æ¨¡æ‹Ÿå®¡é—®ã€‚
- en: A lot of research has been done to solve this task and many tools have been
    developed, but today we can use the power of Large Language Models (LLMs) such
    as ag example GPT-3 from OpenAI and beyond.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è§£å†³è¿™ä¸ªä»»åŠ¡ï¼Œå·²ç»åšäº†å¤§é‡ç ”ç©¶å¹¶å¼€å‘äº†è®¸å¤šå·¥å…·ï¼Œä½†ä»Šå¤©æˆ‘ä»¬å¯ä»¥åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„åŠ›é‡ï¼Œä¾‹å¦‚ OpenAI çš„ GPT-3 åŠæ›´é«˜ç‰ˆæœ¬ã€‚
- en: LangChain is a very recent library that allows us to manage and create applications
    based on LLMs. In fact, the **LLM is just one part of a much more complex AI architecture**.
    When we create a system like this we do not only have to make queries to the OpenAI
    models and get a response, but for example save this response, structure the prompt
    properly etc.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain æ˜¯ä¸€ä¸ªéå¸¸æ–°çš„åº“ï¼Œå®ƒå…è®¸æˆ‘ä»¬ç®¡ç†å’Œåˆ›å»ºåŸºäº LLM çš„åº”ç”¨ç¨‹åºã€‚å®é™…ä¸Šï¼Œ**LLM åªæ˜¯ä¸€ä¸ªæ›´å¤æ‚çš„ AI æ¶æ„ä¸­çš„ä¸€éƒ¨åˆ†**ã€‚å½“æˆ‘ä»¬åˆ›å»ºè¿™æ ·ä¸€ä¸ªç³»ç»Ÿæ—¶ï¼Œæˆ‘ä»¬ä¸ä»…éœ€è¦å‘
    OpenAI æ¨¡å‹æå‡ºæŸ¥è¯¢å¹¶è·å¾—å“åº”ï¼Œè¿˜éœ€è¦ä¾‹å¦‚ä¿å­˜è¿™ä¸ªå“åº”ã€æ­£ç¡®ç»“æ„åŒ–æç¤ºç­‰ã€‚
- en: In this article, we see how to build a simple Question Answering over Docs application
    using LangChain and OpenAI.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•ä½¿ç”¨ LangChain å’Œ OpenAI æ„å»ºä¸€ä¸ªç®€å•çš„æ–‡æ¡£é—®ç­”åº”ç”¨ã€‚
- en: Embeddings
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åµŒå…¥
- en: 'In this application, we will make use of a library called ChromaDB. This is
    an open-source library that allows us to save embeddings. Here a question might
    arise: **But what are embeddings?**'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªåº”ç”¨ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªå«åš ChromaDB çš„åº“ã€‚è¿™æ˜¯ä¸€ä¸ªå¼€æºåº“ï¼Œå…è®¸æˆ‘ä»¬ä¿å­˜åµŒå…¥ã€‚è¿™é‡Œå¯èƒ½ä¼šå‡ºç°ä¸€ä¸ªé—®é¢˜ï¼š**é‚£ä¹ˆä»€ä¹ˆæ˜¯åµŒå…¥ï¼Ÿ**
- en: '**An embedding is nothing more than a projection in a vector space of a word**
    (or text).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**åµŒå…¥ä¸è¿‡æ˜¯è¯ï¼ˆæˆ–æ–‡æœ¬ï¼‰åœ¨å‘é‡ç©ºé—´ä¸­çš„æŠ•å½±**ã€‚'
- en: 'I try to explain myself in a simpler way. Suppose we have the following words
    available: â€œking,â€ â€œqueen,â€ â€œman,â€ and â€œwoman.â€'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°è¯•ç”¨æ›´ç®€å•çš„æ–¹å¼æ¥è§£é‡Šè‡ªå·±ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä»¥ä¸‹å‡ ä¸ªè¯å¯ç”¨ï¼šâ€œkingâ€ï¼Œâ€œqueenâ€ï¼Œâ€œmanâ€å’Œâ€œwomanâ€ã€‚
- en: We people from our experience intuitively understand distances between these
    words. For example, man is closer to king conceptually than queen. But machines
    are not intuitive they need data and metrics to work. So what we do is turn these
    words into data on a Cartesian space so that this intuitive concept of distance
    is represented accurately.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ ¹æ®ç»éªŒç›´è§‚åœ°ç†è§£è¿™äº›è¯ä¹‹é—´çš„è·ç¦»ã€‚ä¾‹å¦‚ï¼Œâ€œmanâ€åœ¨æ¦‚å¿µä¸Šæ¯”â€œqueenâ€æ›´æ¥è¿‘â€œkingâ€ã€‚ä½†æœºå™¨ä¸å…·å¤‡ç›´è§‚èƒ½åŠ›ï¼Œå®ƒä»¬éœ€è¦æ•°æ®å’ŒæŒ‡æ ‡æ¥å·¥ä½œã€‚æ‰€ä»¥æˆ‘ä»¬åšçš„æ˜¯å°†è¿™äº›è¯è½¬åŒ–ä¸ºç¬›å¡å°”ç©ºé—´ä¸­çš„æ•°æ®ï¼Œä»¥ä¾¿å‡†ç¡®è¡¨ç¤ºè¿™ç§ç›´è§‚çš„è·ç¦»æ¦‚å¿µã€‚
- en: '![](../Images/bf99a853a4b7a26870f0caba5f8a1656.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bf99a853a4b7a26870f0caba5f8a1656.png)'
- en: Embedding Example (Image By Author)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: åµŒå…¥ç¤ºä¾‹ï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰
- en: In the image above we have an example (dummy) of embeddings. We see that Man
    is much closer to King than to the other words, and the same is true for woman
    and queen.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šé¢çš„å›¾åƒä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªåµŒå…¥ï¼ˆè™šæ‹Ÿï¼‰çš„ç¤ºä¾‹ã€‚æˆ‘ä»¬çœ‹åˆ°â€œManâ€æ¯”å…¶ä»–å•è¯æ›´æ¥è¿‘â€œKingâ€ï¼ŒåŒæ ·â€œwomanâ€å’Œâ€œqueenâ€ä¹Ÿæ˜¯å¦‚æ­¤ã€‚
- en: Another interesting thing is that **the distance between man and king is the
    same as that between woman and queen**. So somehow it seems that this embedding
    has really captured the essence of these words.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªæœ‰è¶£çš„äº‹æƒ…æ˜¯**â€œmanâ€å’Œâ€œkingâ€ä¹‹é—´çš„è·ç¦»ä¸â€œwomanâ€å’Œâ€œqueenâ€ä¹‹é—´çš„è·ç¦»æ˜¯ç›¸åŒçš„**ã€‚æ‰€ä»¥æŸç§ç¨‹åº¦ä¸Šï¼Œè¿™ä¸ªåµŒå…¥ç¡®å®æ•æ‰åˆ°äº†è¿™äº›è¯çš„æœ¬è´¨ã€‚
- en: Another thing to specify though is that of the metric, i.e. how the **distance**
    is measured. In most cases, this is measured using **cosin similarity** i.e. using
    the cosine of the angle between two embeddings.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªéœ€è¦è¯´æ˜çš„äº‹æƒ…æ˜¯åº¦é‡æ ‡å‡†ï¼Œå³å¦‚ä½•æµ‹é‡**è·ç¦»**ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œè¿™æ˜¯ä½¿ç”¨**ä½™å¼¦ç›¸ä¼¼åº¦**æ¥æµ‹é‡çš„ï¼Œå³ä½¿ç”¨ä¸¤ä¸ªåµŒå…¥ä¹‹é—´è§’åº¦çš„ä½™å¼¦å€¼ã€‚
- en: '![](../Images/80e63e7fcd76cf55f4640b7248e5b062.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/80e63e7fcd76cf55f4640b7248e5b062.png)'
- en: Cosin Similarity (Image By Author)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆå›¾ç‰‡ç”±ä½œè€…æä¾›ï¼‰
- en: The embedding in my example has only two dimensions, two axes. But the embeddings
    that are created by modern algorithms like BERT have **hundreds or thousands of
    axes**, so itâ€™s hard to understand why the algorithm put text at a particular
    point in space.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨ç¤ºä¾‹ä¸­çš„åµŒå…¥åªæœ‰ä¸¤ä¸ªç»´åº¦ï¼Œä¸¤æ¡è½´ã€‚ä½†ç°ä»£ç®—æ³•å¦‚BERTåˆ›å»ºçš„åµŒå…¥æœ‰**æ•°ç™¾æˆ–æ•°åƒæ¡è½´**ï¼Œæ‰€ä»¥å¾ˆéš¾ç†è§£ç®—æ³•ä¸ºä»€ä¹ˆå°†æ–‡æœ¬æ”¾ç½®åœ¨ç©ºé—´ä¸­çš„ç‰¹å®šä½ç½®ã€‚
- en: In this [demo](https://www.cs.cmu.edu/~dst/WordEmbeddingDemo/), you can navigate
    a real embedding space in 3 dimensions and see how the words are near or far from
    each other.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ª[æ¼”ç¤º](https://www.cs.cmu.edu/~dst/WordEmbeddingDemo/)ä¸­ï¼Œä½ å¯ä»¥åœ¨3ç»´ç©ºé—´ä¸­å¯¼èˆªçœŸå®çš„åµŒå…¥ï¼Œå¹¶æŸ¥çœ‹å•è¯å½¼æ­¤ä¹‹é—´çš„è·ç¦»ã€‚
- en: Letâ€™s code!
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¼€å§‹ç¼–ç¨‹å§ï¼
- en: First of all, we need to install some libraries. Surely we will need Langchain
    and OpenAI to instantiate and manage LLMs.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å®‰è£…ä¸€äº›åº“ã€‚æˆ‘ä»¬è‚¯å®šéœ€è¦Langchainå’ŒOpenAIæ¥å®ä¾‹åŒ–å’Œç®¡ç†LLMã€‚
- en: After that we will go to install ChromaDB and TikToken (the latter is required
    to successfully install ChromaDB)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å°†å®‰è£…ChromaDBå’ŒTikTokenï¼ˆåè€…æ˜¯æˆåŠŸå®‰è£…ChromaDBæ‰€å¿…éœ€çš„ï¼‰
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now we need a text file that we are going to work on. In fact, our purpose is
    to ask questions to the LLM about this file. Downloading a file with Python is
    very simple, it can be done with the following commands.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬éœ€è¦ä¸€ä¸ªæˆ‘ä»¬è¦å¤„ç†çš„æ–‡æœ¬æ–‡ä»¶ã€‚äº‹å®ä¸Šï¼Œæˆ‘ä»¬çš„ç›®çš„æ˜¯å‘LLMæé—®æœ‰å…³è¿™ä¸ªæ–‡ä»¶çš„å†…å®¹ã€‚ç”¨Pythonä¸‹è½½æ–‡ä»¶éå¸¸ç®€å•ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®Œæˆã€‚
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now we import all the classes we will need.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯¼å…¥æ‰€æœ‰éœ€è¦çš„ç±»ã€‚
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Obviously to use the OpenAI templates, and to do this you have to enter your
    personal API KEY. If you donâ€™t know how to do this you can see [my previous article
    about Langchain](https://medium.com/towards-data-science/develop-applications-powered-by-language-models-with-langchain-d2f7a1d1ad1a).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¾ç„¶ï¼Œè¦ä½¿ç”¨OpenAIçš„æ¨¡æ¿ï¼Œä½ å¿…é¡»è¾“å…¥ä½ çš„ä¸ªäººAPI KEYã€‚å¦‚æœä½ ä¸çŸ¥é“æ€ä¹ˆåšï¼Œå¯ä»¥æŸ¥çœ‹[æˆ‘å…³äºLangchainçš„ä¸Šä¸€ç¯‡æ–‡ç« ](https://medium.com/towards-data-science/develop-applications-powered-by-language-models-with-langchain-d2f7a1d1ad1a)ã€‚
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In a real application, you will probably have many text files, and you want
    the LLM to figure out in which of these texts is the answer to your question.
    Instead, in this simple example **we break the single text file into multiple
    parts (chunks) and treat each part as a different document**. The model will have
    to figure out which part contains the answer to our question. We break this text
    into multiple parts by assigning each part a maximum length using the commands
    below.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®é™…åº”ç”¨ä¸­ï¼Œä½ å¯èƒ½ä¼šæœ‰å¾ˆå¤šæ–‡æœ¬æ–‡ä»¶ï¼Œä½ å¸Œæœ›LLMæ‰¾å‡ºè¿™äº›æ–‡æœ¬ä¸­å“ªä¸ªåŒ…å«ä½ é—®é¢˜çš„ç­”æ¡ˆã€‚åœ¨è¿™ä¸ªç®€å•çš„ä¾‹å­ä¸­ï¼Œ**æˆ‘ä»¬å°†å•ä¸ªæ–‡æœ¬æ–‡ä»¶åˆ†æˆå¤šä¸ªéƒ¨åˆ†ï¼ˆå—ï¼‰ï¼Œå¹¶å°†æ¯ä¸ªéƒ¨åˆ†è§†ä¸ºä¸åŒçš„æ–‡æ¡£**ã€‚æ¨¡å‹éœ€è¦æ‰¾å‡ºå“ªä¸ªéƒ¨åˆ†åŒ…å«æˆ‘ä»¬é—®é¢˜çš„ç­”æ¡ˆã€‚æˆ‘ä»¬é€šè¿‡ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤å°†æ–‡æœ¬åˆ†æˆå¤šä¸ªéƒ¨åˆ†ï¼Œæ¯éƒ¨åˆ†åˆ†é…ä¸€ä¸ªæœ€å¤§é•¿åº¦ã€‚
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We see that 64 parts have been created from the original text.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çœ‹åˆ°ä»åŸå§‹æ–‡æœ¬åˆ›å»ºäº†64ä¸ªéƒ¨åˆ†ã€‚
- en: We can print the different parts individually since they are contained in a
    list.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€ä¸ªæ‰“å°ä¸åŒçš„éƒ¨åˆ†ï¼Œå› ä¸ºå®ƒä»¬è¢«åŒ…å«åœ¨ä¸€ä¸ªåˆ—è¡¨ä¸­ã€‚
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now we create an object that we need to save the embeddings of the various parts
    of the created text.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå¯¹è±¡ï¼Œç”¨äºä¿å­˜åˆ›å»ºçš„æ–‡æœ¬å„ä¸ªéƒ¨åˆ†çš„åµŒå…¥ã€‚
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: But we want to **save the embeddings in a DB that is persistent** because recreating
    them every time we open the application would be a resource waste. This is where
    ChromaDB helps us. We can create and save the embeddings using text parts, and
    add metadata for each part. In this, the metadata will be strings that name each
    text part.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘ä»¬å¸Œæœ›**å°†åµŒå…¥ä¿å­˜åˆ°ä¸€ä¸ªæŒä¹…åŒ–çš„æ•°æ®åº“ä¸­**ï¼Œå› ä¸ºæ¯æ¬¡æ‰“å¼€åº”ç”¨ç¨‹åºæ—¶é‡æ–°åˆ›å»ºå®ƒä»¬ä¼šæµªè´¹èµ„æºã€‚è¿™å°±æ˜¯ ChromaDB å¸®åŠ©æˆ‘ä»¬çš„åœ°æ–¹ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ–‡æœ¬ç‰‡æ®µåˆ›å»ºå’Œä¿å­˜åµŒå…¥ï¼Œå¹¶ä¸ºæ¯ä¸ªç‰‡æ®µæ·»åŠ å…ƒæ•°æ®ã€‚åœ¨è¿™é‡Œï¼Œå…ƒæ•°æ®å°†æ˜¯å‘½åæ¯ä¸ªæ–‡æœ¬ç‰‡æ®µçš„å­—ç¬¦ä¸²ã€‚
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now we want to turn docsearch into a retrieval because that will be its purpose.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¸Œæœ›å°† docsearch è½¬å˜ä¸ºæ£€ç´¢ï¼Œå› ä¸ºè¿™å°†æ˜¯å®ƒçš„ç›®çš„ã€‚
- en: '[PRE10]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We can also see the retriever what distance metric it is using, in this case,
    the default one is similarity as explained in the part on embeddings.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å¯ä»¥æŸ¥çœ‹æ£€ç´¢å™¨ä½¿ç”¨çš„è·ç¦»åº¦é‡ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé»˜è®¤çš„åº¦é‡æ˜¯ç›¸ä¼¼åº¦ï¼Œå¦‚åµŒå…¥éƒ¨åˆ†æ‰€è§£é‡Šçš„ã€‚
- en: '[PRE11]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Finally, we can ask the retriever to take the document that most answers one
    of our queries. The retriever could also take more than one document if necessary.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å¯ä»¥è¦æ±‚æ£€ç´¢å™¨é€‰æ‹©æœ€èƒ½å›ç­”æˆ‘ä»¬æŸ¥è¯¢çš„æ–‡æ¡£ã€‚å¦‚æœæœ‰å¿…è¦ï¼Œæ£€ç´¢å™¨ä¹Ÿå¯ä»¥é€‰æ‹©å¤šäºä¸€ä»½æ–‡æ¡£ã€‚
- en: '[PRE12]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Letâ€™s see now how many documents he took and what they contain.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ä»–æå–äº†å¤šå°‘æ–‡æ¡£ä»¥åŠè¿™äº›æ–‡æ¡£çš„å†…å®¹ã€‚
- en: '[PRE13]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Now what we can do is to create an agent. **An agent is able to perform a series
    of steps to solve the userâ€™s task on its own**. Our agent will have to go and
    look through the documents available to it where the answer to the question asked
    is and return that document.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªä»£ç†ã€‚ **ä»£ç†èƒ½å¤Ÿæ‰§è¡Œä¸€ç³»åˆ—æ­¥éª¤æ¥ç‹¬ç«‹å®Œæˆç”¨æˆ·çš„ä»»åŠ¡**ã€‚æˆ‘ä»¬çš„ä»£ç†éœ€è¦å»æŸ¥çœ‹å¯ç”¨çš„æ–‡æ¡£ï¼Œæ‰¾å‡ºå›ç­”é—®é¢˜çš„æ–‡æ¡£ï¼Œå¹¶è¿”å›è¯¥æ–‡æ¡£ã€‚
- en: '[PRE15]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: If we want, we can also create a function to post-process the agentâ€™s output
    so that it is more readable.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœéœ€è¦ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥åå¤„ç†ä»£ç†çš„è¾“å‡ºï¼Œä»¥ä½¿å…¶æ›´æ˜“è¯»ã€‚
- en: '[PRE16]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now everything is finally ready, we can use our agent and go and answer our
    queries!
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä¸€åˆ‡ç»ˆäºå‡†å¤‡å¥½äº†ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æˆ‘ä»¬çš„ä»£ç†æ¥å›ç­”æˆ‘ä»¬çš„æŸ¥è¯¢ï¼
- en: '[PRE17]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Final Thoughts
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“æŸè¯­
- en: In this article, we introduced LangChain, ChromaDB and some explanation about
    embeddings. We saw with a simple example how to save embeddings of several documents,
    or parts of a document, into a persistent database and do retrieval of the desired
    part to answer a user query.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº† LangChainã€ChromaDB ä»¥åŠä¸€äº›å…³äºåµŒå…¥çš„è§£é‡Šã€‚æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªç®€å•çš„ä¾‹å­å±•ç¤ºäº†å¦‚ä½•å°†å¤šä¸ªæ–‡æ¡£æˆ–æ–‡æ¡£çš„éƒ¨åˆ†çš„åµŒå…¥ä¿å­˜åˆ°æŒä¹…åŒ–æ•°æ®åº“ä¸­ï¼Œå¹¶æ£€ç´¢æ‰€éœ€çš„éƒ¨åˆ†æ¥å›ç­”ç”¨æˆ·æŸ¥è¯¢ã€‚
- en: If you found this article useful follow me here on Medium! [ğŸ˜‰](https://emojipedia.org/it/apple/ios-15.4/faccina-che-fa-l-occhiolino/)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ è§‰å¾—è¿™ç¯‡æ–‡ç« æœ‰ç”¨ï¼Œå¯ä»¥åœ¨ Medium ä¸Šå…³æ³¨æˆ‘ï¼ [ğŸ˜‰](https://emojipedia.org/it/apple/ios-15.4/faccina-che-fa-l-occhiolino/)
- en: The End
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“æŸ
- en: '*Marcello Politi*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*é©¬åˆ‡æ´›Â·æ³¢åˆ©æ*'
- en: '[Linkedin](https://www.linkedin.com/in/marcello-politi/), [Twitter](https://twitter.com/_March08_),
    [Website](https://marcello-politi.super.site/)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[Linkedin](https://www.linkedin.com/in/marcello-politi/)ã€[Twitter](https://twitter.com/_March08_)ã€[Website](https://marcello-politi.super.site/)'
