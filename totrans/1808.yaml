- en: Say Once! Repeating Words Is Not Helping AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/say-once-repeating-words-is-not-helping-ai-58f38035f66e](https://towardsdatascience.com/say-once-repeating-words-is-not-helping-ai-58f38035f66e)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '| ARTIFICIAL INTELLIGENCE | NLP | LLMs'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How and why is repeating tokens harming LLMs? Why is this a problem?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://salvatore-raieli.medium.com/?source=post_page-----58f38035f66e--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----58f38035f66e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----58f38035f66e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----58f38035f66e--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----58f38035f66e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----58f38035f66e--------------------------------)
    ·14 min read·Jun 20, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/40a23dee61acd2fe912d9497b6a07d5f.png)'
  prefs: []
  type: TYPE_IMG
- en: image by [Kristina Flour](https://unsplash.com/it/@tinaflour) on Unsplash
  prefs: []
  type: TYPE_NORMAL
- en: '[Large Language Models (LLMs)](https://en.wikipedia.org/wiki/Large_language_model)
    have shown their capabilities and have taken the world by storm. Every big company
    now has a model with a fancy name. But, under the hood, they are all [transformers](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)).
    **Everyone dreams of the trillion parameters, but is there no limit?**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, we discuss that:'
  prefs: []
  type: TYPE_NORMAL
- en: Is it guaranteed that a bigger model has better performance than a small model?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do we have the data for huge models?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What happens if instead of collecting new data you use the data again?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scaling over the sky: what is hurting the wing?'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/4c30898f3c8683fc888bf2424f1a20f8.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [Sean Pollock](https://unsplash.com/it/@seanpollock) on Unsplash
  prefs: []
  type: TYPE_NORMAL
- en: '[OpenAI has defined the scaling law](https://arxiv.org/abs/2001.08361), stating
    that model performance follows a power law according to how many parameters are
    used and the number of data points. This along with the search for emergent properties
    has created the parameter race: **the bigger the model, the better.**'
  prefs: []
  type: TYPE_NORMAL
- en: Is that true? Are bigger models giving better performance?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Recently, emergent properties have come into crisis. As shown by Stanford researchers,
    the concept of emergent property may not exist.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9?source=post_page-----58f38035f66e--------------------------------)
    [## Emergent Abilities in AI: Are We Chasing a Myth?'
  prefs: []
  type: TYPE_NORMAL
- en: Changing Perspective on Large Language Models emerging properties
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9?source=post_page-----58f38035f66e--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: The scaling law probably assigns much less value to the dataset than is actually
    thought. [DeepMind has shown with Chinchilla](https://www.deepmind.com/publications/an-empirical-analysis-of-compute-optimal-large-language-model-training),
    that one should not only think to scale the parameters but also the data. In fact,
    Chinchilla shows that it is superior in capacity to [Gopher](https://arxiv.org/abs/2112.11446)
    (70 B vs. 280 B parameters)
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b7da5b5a3d95d2a77649d3981baacb1d.png)'
  prefs: []
  type: TYPE_IMG
- en: '“Overlaid predictions. We overlay the predictions from our three different
    approaches, along with projections from Kaplan et al. (2020). We find that all
    three methods predict that current large models should be substantially smaller
    and therefore trained much longer than is currently done.” Image source: [here](https://arxiv.org/pdf/2203.15556.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Recently, the machine learning community got excited about LLaMA not only because
    it is open source, but because the 65 B version of parameters outperformed [OPT
    175 B](https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/).
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/mlearning-ai/metas-llama-a-small-language-model-beating-giants-5065948e0b7f?source=post_page-----58f38035f66e--------------------------------)
    [## META’s LLaMA: A small language model beating giants'
  prefs: []
  type: TYPE_NORMAL
- en: META open-source model will help us to understand how LMs biases arise
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/mlearning-ai/metas-llama-a-small-language-model-beating-giants-5065948e0b7f?source=post_page-----58f38035f66e--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'As DeepMind states in the Chinchilla article, one can estimate how many tokens
    are required to fully train a state-of-the-art LLM. On the other hand, one can
    also estimate how many high-quality tokens exist. Recent research has wondered
    about this topic. [They concluded](https://arxiv.org/pdf/2211.04325.pdf):'
  prefs: []
  type: TYPE_NORMAL
- en: Language datasets have grown exponentially, with a 50% yearly growth in language
    dataset publication (up to 2e12 words by the end of 2022). This is showing the
    research and publication of new language datasets is a very active field.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other hand, the number of words on the internet (stock of words) is growing
    (and the authors estimate it between 7e13 and 7e16 words, so 1.5 -4.5 orders of
    magnitude).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since, however, they try to use a stock of words that is of high quality, actually
    the authors estimate the quality stock to be between n 4.6e12 and 1.7e13 words.
    **The authors state that between 2023–2027 we will have exhausted the number of
    quality words and between 2030 and 2050 the entire stock.**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The stock of images is not much better off either (three to four orders of magnitude)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/b40e2e8fedec8ae3213af29a06693e90.png)'
  prefs: []
  type: TYPE_IMG
- en: 'projection of data usage. image source: [here](https://arxiv.org/pdf/2211.04325.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Why is this happening?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Well, because we humans are not infinite and do not produce text as much as
    ChatGPT. In fact, projections of the number of Internet users (real and predicted)
    speak volumes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/02944a3a9c4df3a096c2b764ee797c18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Real and projected evolution of internet users. image source: [here](https://arxiv.org/pdf/2211.04325.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: In fact, not everyone is happy about texts, code, and other sources being used
    to train artificial intelligence models. In fact, [Wikipedia, Reddit, and other
    sources historically used to train models would like companies to pay to use their
    data](/the-infinite-babel-library-of-llms-90e203b2f6b0). In contrast, companies
    are invoking fair use, and at present the regulation landscape is unclear.
  prefs: []
  type: TYPE_NORMAL
- en: Combining the data together, a trend can be clearly seen. The number of tokens
    required to optimally train an LLM is growing faster than the tokens in stock.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7a7d8cfc715fd75d345833f62d899c07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/pdf/2305.13230.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: According to the scaling law defined by Chinchilla (number of tokens required
    for optimal LLM training), we have already exceeded the limit. From the graph,
    we can see that according to these estimates with [PaLM-540 B](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html),
    we have reached the limit (10.8 trillion tokens required vs. 9 trillion in stock).
  prefs: []
  type: TYPE_NORMAL
- en: '**Some authors have called this problem with the “token crisis.”** Moreover,
    so far we have considered only English-language tokens, but there are seven thousand
    other languages. [Fifty-six percent of the entire web is in English](https://w3techs.com/technologies/overview/content_language),
    and the remaining 44 percent belongs to only 100 other languages. And this is
    reflected in the performance of models in other languages.'
  prefs: []
  type: TYPE_NORMAL
- en: Can we get more data?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/9f98ef16a2dbf8efd323a627e748fd11.png)'
  prefs: []
  type: TYPE_IMG
- en: image by [Karen Vardazaryan](https://unsplash.com/it/@bright) on Unsplash
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen more parameters do not equate to better performance. For better
    performance, we need quality tokens (texts), but these are in short supply. **How
    can we obtain them? Can we help ourselves with artificial intelligence?**
  prefs: []
  type: TYPE_NORMAL
- en: Why we are not using Chat-GPT to produce text?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**If we humans are not producing enough text, why not automate this process?**
    A recent study shows [how this process is not optimal](https://arxiv.org/pdf/2305.15717.pdf).
    Stanford Alpaca was trained using 52,000 examples derived from [GPT-3](https://en.wikipedia.org/wiki/GPT-3),
    but only apparently achieved similar performance. [**In reality, the model learns
    the style of the target model but not its knowledge.**](https://levelup.gitconnected.com/the-imitation-game-taming-the-gap-between-open-source-and-proprietary-models-627374b390e5)'
  prefs: []
  type: TYPE_NORMAL
- en: Why not train longer?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'For both PaLM, Gopher, and LLaMA (also for the other LLMs) it is clearly written
    that the models were trained for a few epochs (one or however few). This is not
    a limitation of the [Transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    because, for example, the [Vision Transformers (ViT)](https://en.wikipedia.org/wiki/Vision_transformer)
    have been trained for 300 epochs on ImageNet (1 million images), as shown in the
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bc5ebefbcba1ba2a15990ccedb965a4a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/pdf/2010.11929.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Because it is beyond expensive. [In the LLaMA article](https://medium.com/mlearning-ai/metas-llama-a-small-language-model-beating-giants-5065948e0b7f),
    the authors trained for only one epoch (and two epochs for only part of the dataset).
    Nevertheless, the authors report:'
  prefs: []
  type: TYPE_NORMAL
- en: When training a 65B-parameter model, our code processes around 380 tokens/sec/GPU
    on 2048 A100 GPU with 80GB of RAM. This means that training over our dataset containing
    1.4T tokens takes approximately 21 days. ([source](https://arxiv.org/pdf/2302.13971.pdf))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Training an LLM for even a few epochs is extremely expensive. As calculated
    by [Dmytro Nikolaiev (Dimid)](https://medium.com/u/97b5279dad26?source=post_page-----58f38035f66e--------------------------------)
    this is meaning [4.0 million dollars](/behind-the-millions-estimating-the-scale-of-large-language-models-97bd7287fb6b)
    if you train a model similar to META’s LLaMA on the Google Cloud Platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'So training for other epochs would lead to an exponential increase in costs.
    **Also, we don’t know if this additional training is really useful: we haven’t
    tested it yet.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently a group of researchers at the University of Singapore studied what
    happens if we train an LLM for multiple epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://arxiv.org/abs/2305.13230?source=post_page-----58f38035f66e--------------------------------)
    [## To Repeat or Not To Repeat: Insights from Scaling LLM under Token-Crisis'
  prefs: []
  type: TYPE_NORMAL
- en: Recent research has highlighted the importance of dataset size in scaling language
    models. However, large language…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: arxiv.org](https://arxiv.org/abs/2305.13230?source=post_page-----58f38035f66e--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Repetita iuvant aut continuata secant
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/bad195651655fa3252688bea96be3549.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [Unseen Studio](https://unsplash.com/it/@craftedbygc) on Unsplash
  prefs: []
  type: TYPE_NORMAL
- en: Until now we know that the performance of a model is derived not only by the
    number of parameters but also by the number of quality tokens used to train. On
    the other hand, these quality tokens are not infinite and we are approaching the
    limit. **If we cannot find enough quality tokens and it is an option to generate
    with AI, what could we do?**
  prefs: []
  type: TYPE_NORMAL
- en: Can we use the same training set and train longer?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: There is a Latin locution that states that repeating things benefits (r*epetita
    iuvant*), but over time someone added “but continuing bores” (*continuata secant*).
  prefs: []
  type: TYPE_NORMAL
- en: 'The same is true for neural networks: increasing the number of epochs improves
    network performance (decrease in loss); at some point, however, while the loss
    in the training set continues to fall, the loss in the validation set begins to
    rise. The neural network went into [overfitting](https://en.wikipedia.org/wiki/Overfitting),
    beginning to consider patterns that are only present in the training set and losing
    the ability to generalize.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/66cd1d33ca987237ac5987a905023782.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Overfitting/overtraining in supervised learning. Image source: [here](https://en.wikipedia.org/wiki/Overfitting)'
  prefs: []
  type: TYPE_NORMAL
- en: Ok, this has been studied extensively for small neural networks, but what about
    huge transformers?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The authors of this study used the [T5 model](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)
    (encoder-decoder model) on the C4 dataset. The authors trained several versions
    of the model, increasing the number of parameters until the larger model outperformed
    the smaller model (indicating that the larger model received a sufficient number
    of tokens, as Chinchilla’s law). The authors noted that there was a linear relationship
    between the number of tokens required and the size of the model (confirming what
    DeepMind saw with Chinchilla).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/00720fa825e60895d6743c561eab6428.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image source: [here](https://arxiv.org/pdf/2305.13230.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The C4 dataset is limited (does not have infinite tokens) so to increase the
    number of parameters the authors found themselves in a tokens-scarcity condition.
    Thus they decided to simulate what happens if an LLM sees repeated data. They
    sampled a certain number of tokens, so the model found itself seeing them again
    in tokens training. This showed:'
  prefs: []
  type: TYPE_NORMAL
- en: Repeated tokens lead to degraded performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Larger models are more susceptible to overfitting under tokens-crisis conditions
    (so even though it theoretically consumes more computational resources this leads
    to degraded performance).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/940085b94c885d0037fba10b6bf27663.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image source: [here](https://arxiv.org/pdf/2305.13230.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, these models are used for downstream tasks. Often an LLM is trained
    unsupervised on a large amount of text and then fine-tuned on a smaller dataset
    for a downstream task. Or it may go through a process called alignment (as in
    the case of ChatGPT).
  prefs: []
  type: TYPE_NORMAL
- en: When an LLM is trained on repeated data even though it is then fine-tuned on
    another dataset, performance is degraded. So the downstream tasks are also impacted.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ac9c68a16e51f162ca1bc64c95b3d321.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image source: [here](https://arxiv.org/pdf/2305.13230.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Why repeated tokens are not a good idea
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/2c4a3f3cca1e4bbf67e2653d55867c12.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by [Brett Jordan](https://unsplash.com/it/@brett_jordan) on Unsplash
  prefs: []
  type: TYPE_NORMAL
- en: We just saw that repeated tokens harm training. But why does this happen?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The authors decided to investigate by keeping the number of repeated tokens
    fixed and increasing the number of total tokens in the dataset. The results show
    that a larger dataset alleviates multi-epoch degradation issues.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/41965687e7b13654ad2741d3ed1689db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Image source: [here](https://arxiv.org/pdf/2305.13230.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Last year Galactica](https://arxiv.org/pdf/2211.09085.pdf) was published (a
    model that was supposed to help scientists but [lasted only three days](https://www.technologyreview.com/2022/11/18/1063487/meta-large-language-model-ai-only-survived-three-days-gpt-3-science/)).
    Apart from the spectacular debacle, the article suggested that part of their results
    was from the quality of the data. According to the authors, data quality reduced
    the risk of overfitting:'
  prefs: []
  type: TYPE_NORMAL
- en: We are able to train on it for multiple epochs without overfitting, where upstream
    and downstream performance improves with use of repeated tokens. ([source](https://arxiv.org/pdf/2211.09085.pdf))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/973eb580593c47877e6f117fff7207e2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/pdf/2211.09085.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: For the authors, the repeated tokens actually not only do not harm the model
    training but actually improved downstream performance.
  prefs: []
  type: TYPE_NORMAL
- en: In this new study, the authors use the Wikipedia dataset which is considered
    a higher quality dataset than C4, and add repeated tokens. The results show that
    there is a similar level of degradation, which is against what is stated in Galactica’s
    article.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c6635e936469851fb038cd398a69c00b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/pdf/2305.13230.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors also tried to investigate whether it was also due to model scaling.
    During the scaling of a model, both the number of parameters and the computational
    cost increase. The authors decided to study these two factors individually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Mixture-of-Experts (MoE)**](https://ai.googleblog.com/2022/11/mixture-of-experts-with-expert-choice.html)
    because although it increases the number of parameters it maintains a similar
    computational cost.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**ParamShare**](https://ai.googleblog.com/2022/11/mixture-of-experts-with-expert-choice.html),
    on the other hand, reduces the number of parameters but maintains the same computational
    cost.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/a1c97c42cba2c96288f2c14b2d4e632d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/pdf/2305.13230.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: The results show that the model with fewer parameters is less affected by repeated
    tokens. In contrast, the MoE model (greater number of parameters) is more prone
    to overfitting. The result is interesting because MoE has been used successfully
    in many AI models, so the authors suggest that although MoE is a useful technique
    when there is enough data, it can hurt performance when there are not enough tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors also explored whether objective training impacts performance degradation.
    In general, there are two training objectives:'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Next token prediction**](https://arxiv.org/abs/2212.11281) (given a sequence
    of tokens predict the next in the sequence).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Masked language modeling**](https://huggingface.co/docs/transformers/main/tasks/masked_language_modeling),
    where one or more tokens are masked and need to predict them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recently, with [PaLM2–2](https://ai.google/discover/palm2/), Google introduced
    UL2 which is a mix between these two training objectives. UL2 has been shown to
    accelerate model training however interestingly, UL2 is more prone to overfitting
    and has greater multi-epoch degradation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/714c09520e9e558a53f6cf1acd00586a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/pdf/2305.13230.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: The authors next explored how they could try to alleviate multi-epoch degradation.
    Since regularization techniques are used precisely to prevent overfitting, the
    authors tested whether these techniques had a beneficial effect here as well.
  prefs: []
  type: TYPE_NORMAL
- en: Dropout shows to be one of the most efficient techniques to alleviate the problem.
    This is not surprising because one of the most efficient regularization techniques,
    it is easily parallelized and used by most of the models.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/819f3826fd343df6ba93c90da456a499.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/pdf/2305.13230.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, it works best for the authors to start without dropout and only at
    a later point in the training to add dropout.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f0cf3be64f13de793f9c2298a8097df7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/pdf/2305.13230.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the authors note that using Dropout in some models, especially
    the larger ones, can lead to a slight reduction in performance. So although it
    may have beneficial effects against overfitting it could lead to unexpected behaviors
    in other contexts. So much that models GPT-3, PaLM, LLaMA, Chinchilla, and Gopher
    do not use it in their architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a005e0d3460b7fb717c34dbbf3e6c013.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/pdf/2305.13230.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'As described in the table below, the authors used for their experiments what
    are now considered almost small models. Thus, it is expensive to test different
    hyperparameters when designing an LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: For instance, in our specific scenario, training T5-XL five times would require
    approximately $37,000 USD for renting Google Cloud TPUs. Considering even larger
    models like PaLM and GPT-4, trained on even larger datasets, this cost becomes
    unmanageable ([source](https://arxiv.org/pdf/2305.13230.pdf))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/eac5745319c497f6992c6d1f88ec8d78.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/pdf/2305.13230.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Since in their experiments, a Sparse MoE model approximates the behavior of
    a dense model (which is more computationally expensive), one can use it to search
    for the best hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the authors show that one can test different learning rates for
    the MoE model and it exhibits the same performance as the equivalent dense model.
    So for the authors, one can test different hyperparameters with the MoE model
    and then train with the chosen parameters the dense model, thus saving cost:'
  prefs: []
  type: TYPE_NORMAL
- en: sweeping the MoE Large model incurred an expenditure of approximately 10.6K
    USD on the Google Cloud Platform. Conversely, training the Dense XL model only
    once required 7.4K USD. Consequently, the entire development process, including
    sweeping, amounted to a total cost of 18K USD, which is only 0.48 times the expense
    of directly tuning the Dense XL model ([source](https://arxiv.org/pdf/2305.13230.pdf))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/1d7936a9edc9c0c727a734e00b06da96.png)'
  prefs: []
  type: TYPE_IMG
- en: 'image source: [here](https://arxiv.org/pdf/2305.13230.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Parting thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In recent years there has been a race to have the biggest model. On the one
    hand, this race has been motivated by the fact that at a certain scale, properties
    were emerging that were impossible to predict with smaller models. On the other
    hand, the scaling law of OpenAI stated that performance is a function of the number
    of model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: In the past year this paradigm has come into crisis.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Recently LlaMA has shown the importance of data quality. Also, Chinchilla showed
    a new rule for calculating the number of tokens needed to train a model optimally.
    In fact, a model of a certain number of parameters requires a number of data in
    order to perform optimally.
  prefs: []
  type: TYPE_NORMAL
- en: Subsequent studies have shown that the number of quality tokens is not infinite.
    On the other hand, the number of model parameters grows more than how many tokens
    we humans can generate.
  prefs: []
  type: TYPE_NORMAL
- en: This led to the question of how we can solve the tokens crisis. Recent studies
    show that using LLM to generate tokens is not a viable way. This new work shows
    how using the same tokens for multiple epochs can actually deteriorate performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Work like this is important because although we are training and using LLM
    more and more, there are many even basic aspects that we do not know about. This
    work answers a question that seems basic but which the authors answer with experimental
    data: **what happens when training an LLM for multiple epochs?**'
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, this article is part of a growing slice of literature that shows how
    an uncritical increase in the number of parameters is unnecessary. On the other
    hand, bigger and bigger models are more and more expensive and also consume more
    and more electricity. Considering that [we need to optimize resources](/how-ai-could-fuel-global-warming-8f6e1dda6711),
    this article suggests that training a huge model without enough data is just a
    waste.
  prefs: []
  type: TYPE_NORMAL
- en: This article still shows how we need new architectures that can replace the
    transformer. So it is time to focus research on new ideas instead of continuing
    to scale models.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have found this interesting:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*You can look for my other articles, you can also* [***subscribe***](https://salvatore-raieli.medium.com/subscribe)
    *to get notified when I publish articles, you can* [***become a Medium member***](https://medium.com/@salvatore-raieli/membership)
    *to access all its stories (affiliate links of the platform for which I get small
    revenues without cost to you) and you can also connect or reach me on*[***LinkedIn***](https://www.linkedin.com/in/salvatore-raieli/)***.***'
  prefs: []
  type: TYPE_NORMAL
- en: '*Here is the link to my GitHub repository, where I am planning to collect code
    and many resources related to machine learning, artificial intelligence, and more.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://github.com/SalvatoreRa/tutorial?source=post_page-----58f38035f66e--------------------------------)
    [## GitHub - SalvatoreRa/tutorial: Tutorials on machine learning, artificial intelligence,
    data science…'
  prefs: []
  type: TYPE_NORMAL
- en: Tutorials on machine learning, artificial intelligence, data science with math
    explanation and reusable code (in python…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/SalvatoreRa/tutorial?source=post_page-----58f38035f66e--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*or you may be interested in one of my recent articles:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://salvatore-raieli.medium.com/scaling-isnt-everything-how-bigger-models-fail-harder-d64589be4f04?source=post_page-----58f38035f66e--------------------------------)
    [## Scaling Isn’t Everything: How Bigger Models Fail Harder'
  prefs: []
  type: TYPE_NORMAL
- en: Are Large Language Models really understanding programming languages?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'salvatore-raieli.medium.com](https://salvatore-raieli.medium.com/scaling-isnt-everything-how-bigger-models-fail-harder-d64589be4f04?source=post_page-----58f38035f66e--------------------------------)
    [](https://levelup.gitconnected.com/metas-lima-maria-kondo-s-way-for-llms-training-8411e3907fed?source=post_page-----58f38035f66e--------------------------------)
    [## META’S LIMA: Maria Kondo’s way for LLMs training'
  prefs: []
  type: TYPE_NORMAL
- en: Less and tidy data to create a model capable to rival ChatGPT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'levelup.gitconnected.com](https://levelup.gitconnected.com/metas-lima-maria-kondo-s-way-for-llms-training-8411e3907fed?source=post_page-----58f38035f66e--------------------------------)
    [](https://levelup.gitconnected.com/google-med-palm-2-is-ai-ready-for-medical-residency-e37907115bd0?source=post_page-----58f38035f66e--------------------------------)
    [## Google Med-PaLM 2: is AI ready for medical residency?'
  prefs: []
  type: TYPE_NORMAL
- en: Google's new model achieves impressive results in the medical domain
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'levelup.gitconnected.com](https://levelup.gitconnected.com/google-med-palm-2-is-ai-ready-for-medical-residency-e37907115bd0?source=post_page-----58f38035f66e--------------------------------)
    [](https://levelup.gitconnected.com/to-ai-or-not-to-ai-how-to-survive-f5e853aebd5b?source=post_page-----58f38035f66e--------------------------------)
    [## To AI or not to AI: how to survive?'
  prefs: []
  type: TYPE_NORMAL
- en: With generative AI threatening businesses and side hustles, how you can find
    space?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/to-ai-or-not-to-ai-how-to-survive-f5e853aebd5b?source=post_page-----58f38035f66e--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A list of the principal references consulted for this article:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fuzhao Xue et al, 2023, To Repeat or Not To Repeat: Insights from Scaling LLM
    under Token-Crisis, [link](https://arxiv.org/abs/2305.13230)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Hugo Touvron et all. 2023, LLaMA: Open and Efficient Foundation Language Models.
    [link](https://arxiv.org/abs/2302.13971)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Arnav Gudibande et all, 2023, The False Promise of Imitating Proprietary LLMs.
    [link](https://arxiv.org/abs/2305.15717)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: PaLM 2, google blog, [link](https://ai.google/discover/palm2/)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough
    Performance. Google Blog, [link](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Buck Shlegeris et all, 2022, Language models are better than humans at next-token
    prediction, [link](https://arxiv.org/abs/2212.11281)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pablo Villalobos et. all, 2022, Will we run out of data? An analysis of the
    limits of scaling datasets in Machine Learning. [link](https://arxiv.org/abs/2211.04325)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Susan Zhang et al. 2022, OPT: Open Pre-trained Transformer Language Models.
    [link](https://arxiv.org/abs/2205.01068)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Jordan Hoffmann et all, 2022, An empirical analysis of compute-optimal large
    language model training. [link](https://arxiv.org/abs/2203.15556)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Ross Taylor et al, 2022, Galactica: A Large Language Model for Science, [link](https://arxiv.org/abs/2211.09085)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Zixiang Chen et al, 2022, Towards Understanding Mixture of Experts in Deep Learning,
    [link](https://arxiv.org/abs/2208.02813)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Jared Kaplan et all, 2020, Scaling Laws for Neural Language Models. [link](https://arxiv.org/abs/2001.08361)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How AI could fuel global warming, TDS, [link](/how-ai-could-fuel-global-warming-8f6e1dda6711)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Masked language modeling, HuggingFace blog, [link](https://huggingface.co/docs/transformers/main/tasks/masked_language_modeling)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mixture-of-Experts with Expert Choice Routing, Google Blog, [link](https://ai.googleblog.com/2022/11/mixture-of-experts-with-expert-choice.html)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why Meta’s latest large language model survived only three days online, MIT
    review, [link](https://www.technologyreview.com/2022/11/18/1063487/meta-large-language-model-ai-only-survived-three-days-gpt-3-science/)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Exploring Transfer Learning with T5: the Text-To-Text Transfer Transformer,
    Google Blog, [link](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scaling laws for reward model overoptimization, OpenAI blog, [link](https://openai.com/research/scaling-laws-for-reward-model-overoptimization)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An empirical analysis of compute-optimal large language model training, DeepMind
    blog, [link](https://www.deepmind.com/blog/an-empirical-analysis-of-compute-optimal-large-language-model-training)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Xiaonan Nie et al, 2022, EvoMoE: An Evolutional Mixture-of-Experts Training
    Framework via Dense-To-Sparse Gate. [link](https://arxiv.org/abs/2112.14397)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tianyu Chen et al, 2022, Task-Specific Expert Pruning for Sparse Mixture-of-Experts,
    [link](https://arxiv.org/abs/2206.00277)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bo Li et al, 2022, Sparse Mixture-of-Experts are Domain Generalizable Learners,
    [link](https://arxiv.org/abs/2206.04046)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
