- en: Guide to Successful ML Model Deployment for Data Analysts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŽŸæ–‡ï¼š[https://towardsdatascience.com/guide-to-successful-ml-model-deployment-for-data-analysts-e5b893260926](https://towardsdatascience.com/guide-to-successful-ml-model-deployment-for-data-analysts-e5b893260926)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](../Images/1bbe793cc056570d6ca290c61084871b.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [ray rui](https://unsplash.com/@ray30?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: How data model deployment differs from other analytics projects
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://tanuwidjajaolivia.medium.com/?source=post_page-----e5b893260926--------------------------------)[![Olivia
    Tanuwidjaja](../Images/52a56de28da9b782b57f1c3928655cfb.png)](https://tanuwidjajaolivia.medium.com/?source=post_page-----e5b893260926--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e5b893260926--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e5b893260926--------------------------------)
    [Olivia Tanuwidjaja](https://tanuwidjajaolivia.medium.com/?source=post_page-----e5b893260926--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e5b893260926--------------------------------)
    Â·6 min readÂ·Apr 11, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: So you have created an ML model which seems to have reliable results. Congratulations!
    I know from experience that oftentimes itâ€™s not easy to get reliable ML models,
    especially with limited ML exposure as a Data Analyst. However, once the model
    is complete, the next challenge is to productionize it so that it can be integrated
    into the product.
  prefs: []
  type: TYPE_NORMAL
- en: For many Data Analysts, ML model deployment can be uncharted territory, as **most
    data modeling concepts are only used for analytics purposes and not deployed in
    production**. While some companies have ML Ops teams to help with these deployments,
    it is essential for Analysts to understand the key concepts and caveats involved
    to collaborate effectively with their ML Ops counterparts.
  prefs: []
  type: TYPE_NORMAL
- en: To help bridge this knowledge gap, in this article I will share the critical
    concepts and caveats that Data Analysts need to know when deploying their models
    to production. I will also explain how deploying ML models differs from regular
    analytics projects. With this understanding, Data Analysts can ensure that their
    models are deployed successfully and deliver meaningful results.
  prefs: []
  type: TYPE_NORMAL
- en: ML Model Deployment Considerations for Data Analyst
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As Data Analysts, weâ€™re used to consuming historical data for offline analysis.
    Most of the time, we spend a few minutes querying from an online database, and
    another few hours or weeks processing (analyzing) them. For each project, the
    processing and data queried might differ. It is not surprising to find a one-time
    analysis project being done, ***not requiring reproducibility***.
  prefs: []
  type: TYPE_NORMAL
- en: When weâ€™re talking about a machine learning model for deployment in product
    integration, the equation becomes very different. There are several principles
    to be considered, including
  prefs: []
  type: TYPE_NORMAL
- en: '***Latency*: time needed for the model take to provide output**, from retrieving
    features to delivering prediction results. The longer time it takes to do its
    job, the longer time the output can be used by the customers, which impacts user
    experience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Data availability*: the set of data/features available by the time the model
    is triggered**. Some data might not be existent (or possibly be queried) within
    the latency limit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '***Robustness*: the degree that a modelâ€™s performance changes when using new
    data versus training data**. Ideally, a robust model will have stable performance
    even when faced with new real-life data. It is true, however, that over some time
    model performance might decrease and model retraining is needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These factors are nearly non-existent in the one-time analysis world but are
    very crucial in the machine learning deployment world.
  prefs: []
  type: TYPE_NORMAL
- en: Data and Feature Engineering Pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One distinct differentiator between preparing/handling data for analysis versus
    modeling (deployment) is the data pipeline and subsequent feature engineering.
    As mentioned above, we have latency and data availability constraints to consider
    when developing the data pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'In cases when model output is being incorporated into the product, the latency
    requirement might become very strict. Say youâ€™re building an [ETA Prediction model](https://www.uber.com/en-SG/blog/deepeta-how-uber-predicts-arrival-times/),
    model output is expected within a few seconds to avoid any disruption in user
    experience. Because of this, you can only use features that can be available within
    that duration, and a reliable pipeline to support that. Some common practices
    on this include:'
  prefs: []
  type: TYPE_NORMAL
- en: Use of [**Feature Store**](https://www.kdnuggets.com/2022/03/feature-stores-realtime-ai-machine-learning.html)**.**
    Feature store is a centralized platform used to store features from different
    data sources. It helps with (1) **reproducibility** as the consistent features
    can be used between training and inference and (2) the **reusability** of features
    across several models in production. It can be designed for online serving where
    serving speed is maximized. The feature store and its architecture play important
    roles in determining performance â€” in terms of serving speed and accuracy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/ce8007d3ae32cb174c6fd06b6f15d2e5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Feature Store Illustration (Source: [feast.dev](https://feast.dev/blog/what-is-a-feature-store/))'
  prefs: []
  type: TYPE_NORMAL
- en: '**Assemble preprocessing** steps. Before the input data is inserted into the
    model, usually there are multiple preprocessing steps being done â€” from cleansing
    to imputation and normalization. With numerous steps being done, there are readability
    and debug-ability risks that come with them. Assembling these steps, like using
    [Pipeline in sklearn](/why-you-should-use-scikit-learn-pipelines-8754b4d1e375#:~:text=The%20Scikit%2Dlearn%20pipeline%20is,attain%20results%20with%20less%20code.)
    helps to organize these steps that make it clearer for users and less prone to
    errors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ongoing Monitoring and Continuous Improvement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike working on analysis where our work is done when the insights are delivered,
    in data modeling, **the work is not completely done when the model is deployed
    or integrated**. In some cases even, the work is just getting started ðŸ˜†
  prefs: []
  type: TYPE_NORMAL
- en: Just like any other digital product, you need to maintain and improve data models
    over time. Models only understand the dynamics of the data with which it is being
    trained. Over time as the market and user behavior change, the **model might be
    irrelevant to the prediction/classification as it fails to pick up and adjust
    with these new data distributions**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9b66d4f13b4b7245c9e8ae84f3e9554a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'ML Model performance decays over time. Source: [ML Ops](https://ml-ops.org/content/mlops-principles#monitoring)'
  prefs: []
  type: TYPE_NORMAL
- en: Hence, ongoing monitoring and continuous improvement of the model are needed
    the ensure the quality of model prediction results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Aspects of the ML model to be monitored on an ongoing basis includes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Deviation in the input data distribution.** In most cases, model irrelevancy
    happens because of environment/market changes (ie user behavior) which cause movement
    in data distribution. Consider an e-commerce platform with a fraud detection model
    that uses the total order per customer per day as the main predictor to identify
    abusive customers. As the platform expands its product offerings, customer orders
    increase unexpectedly, which may result in some legitimate customers being incorrectly
    identified as abusive by the model. To avoid that, the **statistical distribution
    of the input features, i.e average or standard deviation needs to be monitored
    over time**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the movement is continuous and the deviation starts to impact performance,
    some improvement measures might need to be considered.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Model performance.** Model performance ([model accuracy, precision, recall](https://www.analyticsvidhya.com/blog/2019/08/11-important-model-evaluation-error-metrics/))
    is **the most important aspect to be monitored continuously, but sometimes it
    is also the trickiest to monitor**. The reason is in some cases we donâ€™t have
    a verified label to compare our modelâ€™s predictions with since the model works
    on new data. To enable performance monitoring, we can use (1) **proxy metrics**
    or (2) **baseline reviews**. Proxy metrics are product metrics that can indirectly
    show model impact like customer click/conversion rate for a recommendation model.
    A baseline review is a review and manual labeling of sampled model input and output,
    like in the data prepared for model training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**System performance.** Deployed data model involves not only the model itself
    but also other components in the data pipeline, including feature store, database,
    and product APIs. Common metrics to be looked at for distributed systems are [traffic
    volume, latency, errors, and saturation](https://sre.google/sre-book/monitoring-distributed-systems/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In more advanced organizations, c[ontinuous model training, integration, and
    deployment](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning#mlops_level_1_ml_pipeline_automation)
    can be considered to cope with rapid changes in data and business environment.
  prefs: []
  type: TYPE_NORMAL
- en: Closing remarks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As Data Analysts upskill themselves to be [Full-Stack Data Analysts](https://medium.com/towards-data-science/why-i-choose-full-stack-data-analytics-as-my-career-path-d7b3986e0285),
    they may find themselves **implementing insights as a data product**, including
    data models.
  prefs: []
  type: TYPE_NORMAL
- en: However, it is crucial to recognize that **delivering (deploying) data models**
    is a different ball game from **delivering (presenting) data insights**.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: There are essential factors to consider in operationalization, including continuous
    monitoring and improvement requirements. By understanding these concepts, Data
    Analysts can ensure that they deliver quality models that serve the needs of the
    product/organization over time.
  prefs: []
  type: TYPE_NORMAL
