# 谷歌对多模态基础模型的最新方法

> 原文：[https://towardsdatascience.com/googles-latest-approaches-to-multimodal-foundational-model-beedaced32f9](https://towardsdatascience.com/googles-latest-approaches-to-multimodal-foundational-model-beedaced32f9)

## 多模态基础模型比大型语言模型更令人兴奋。让我们回顾谷歌研究的最新进展，以窥见前沿技术。

[](https://eileen-code4fun.medium.com/?source=post_page-----beedaced32f9--------------------------------)[![Eileen Pangu](../Images/cbdab572af709b6e6b52cb3a078f220d.png)](https://eileen-code4fun.medium.com/?source=post_page-----beedaced32f9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----beedaced32f9--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----beedaced32f9--------------------------------) [Eileen Pangu](https://eileen-code4fun.medium.com/?source=post_page-----beedaced32f9--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----beedaced32f9--------------------------------) ·7 min read·2023年8月12日

--

![](../Images/4c992bcc9d7d065534e2c5fec5e58b79.png)

图片来源：[https://unsplash.com/photos/U3sOwViXhkY](https://unsplash.com/photos/U3sOwViXhkY)

> **背景**

虽然大型语言模型（LLM）的炒作在业界依旧火热，但领先的研究机构已经将目光转向多模态基础模型——这些模型具有与LLM相同的规模和多功能特性，但可以处理文本之外的数据，如图像、音频、传感器信号等。许多人认为，多模态基础模型是解锁人工智能（AI）下一阶段进步的关键。

在这篇博客文章中，我们将更详细地了解谷歌如何处理多模态基础模型。本文的内容基于谷歌最近论文中的关键方法和见解，我们在文章末尾提供了相关参考。

> **你为什么应该关心**

多模态基础模型令人兴奋，但你为什么应该关心？你可能是：

+   一位希望跟上该领域最新研究进展的AI/ML从业者，但又没有耐心阅读几十篇新论文和数百页的综述。

+   一位当前或即将成为行业领导者的人，想知道大型语言模型之后会发生什么，并考虑如何将你的业务与科技界的新趋势对齐。

+   一位好奇的读者，可能会成为当前或未来多模态AI产品的消费者，并希望对幕后工作的原理有一个直观的了解。

对于以上所有读者，本文将提供一个很好的概述，帮助你启动对多模态基础模型的理解，这是未来更易于访问和有用的AI的基石。

在我们深入探讨之前还有一点需要注意：当人们谈论多模态基础模型时，他们通常指的是输入是多模态的，包括文本、图像、视频、信号等。然而，输出总是只是文本。原因在于文本是最通用的沟通格式。如果我们能输出文本，就能输出代码并将其输入到下游工具中，以生成所需的多媒体格式。更不用说现在有很多AI工具可以将自然语言作为输入，生成图像和声音。

> **从文本开始**

首先需要包含的格式是文本，或者用研究术语来说，是语言。研究人员喜欢称之为语言，因为语言意味着文本来源于具有连贯语义意义的语言系统，无论是任何自然语言、代码、命令、符号等等。通俗来说，我们在这篇博客文章中就称之为文本。

关于构建大型语言模型的材料在线上有很多，所以我们在这里不会深入讨论。只需注意，谷歌最近在LLMs和多模态基础模型上的结果都是建立在PaLM [1]之上的。如果你不熟悉它，可以将其视为类似于ChatGPT的黑箱，尽管它们的内部机制差异很大。

这里需要掌握的基本概念是，输入文本被分解为标记。每个标记随后被映射到一个嵌入空间中。实际上，每个标记变成了一个向量。然后，这些向量的列表被输入到由注意力机制网络层组成的模型中。模型的输出也是一个向量列表，可以被转换回标记，然后拼接形成最终的文本输出。

该模型拥有数百亿个参数，并在大量的文本数据上进行训练，执行一些简单的预测任务，例如给定一个文本前缀，预测下一个标记。

生成的模型将需要经过进一步的微调才能变得更直接有用。例如，谷歌已经将其PaLM模型应用于医学领域，并创建了MedPaLM [2]。你可以参考这篇[文章](/how-to-use-large-language-models-llm-in-your-own-domains-b4dff2d08464)以获取更多关于如何将大型语言模型适应到特定领域的见解。

到目前为止，我们拥有一个大型语言模型。下一步是融入其他媒体格式。

> **融入图像**

为了融合图像，首先要做的是将图像转换为可以被大语言模型处理的格式。因为我们已经有了大语言模型，并且希望多模态信息能够被统一处理，自然的选择是将图像转换为嵌入向量列表。回想一下，文本也被转换成了上述的嵌入向量列表。显然，图像和文本的向量应具有相同的维度。

谷歌在采用Transformer架构进行图像处理方面做了令人印象深刻的工作[3][4]。这被证明非常适合图像到向量的转换。

具体来说，每张图像被调整为224×224的分辨率，然后被拆分成14×14像素的片段。这会产生224×224/14/14=256个片段。每个片段经过相同的可学习线性变换以转换为向量。每个向量与一个可学习的位置嵌入进行拼接。这些拼接的向量然后输入到Transformer模型[5]中。谷歌研究将这一模型架构称为ViT——视觉Transformer。ViT模型的输出也是256个向量。然后，将一个浅层网络附加到ViT模型的输出向量上，并对整个架构进行训练，以进行图像分类、图像描述和其他图像处理任务。有关示意图，请参见图-1。

![](../Images/14594b2ba46100f5bda3da94b55c2aa4.png)

图-1：ViT架构的示意图。图片来自作者。

一旦完成图像处理训练，丢弃顶部的浅层网络，我们可以使用ViT的256个输出向量作为图像的嵌入表示。核心思想是，通过大规模训练，ViT模型已经学会用一系列向量表示输入图像，这些向量可以轻松用作各种图像处理任务的输入特征。

这里有一个特别的说明，即谷歌研究发现ViT在图像处理上的结果优于最先进的CNN。他们将这一优越性能归因于规模（基于Transformer的架构被认为有更好的扩展性）以及注意力机制可以在网络早期直接作用于任何两个输入片段的事实。

在ViT中完成的图像表示学习可以转移到多模态环境中，正如你所猜到的那样。来自预训练ViT的输出向量可以作为大语言模型的输入。

对于输入提示如`what happened between <imgA> and <imgB>`，词语按惯例转换为大语言模型的嵌入向量；每张图像通过ViT转换为256个向量。所有向量一起形成大语言模型的输入。最后，大语言模型在多模态训练数据上进行端到端微调（或冻结其各个部分），最终得到的模型就是多模态基础模型。有关示意图，请参见图-2。

![](../Images/4989f2a7ec72bf29ad5c19ad46caad0e.png)

图-2: 多模态基础模型架构的示意图。图片来自作者。

显然，上述维度数字只是谷歌研究论文使用的其中一种配置[6]。如果你愿意，可以尝试自己的配置。但要注意，ViT 输出向量的维度应与较大的语言模型输入向量的维度匹配。如果不同，你可以调整任一模型，或者在 ViT 输出和大型语言模型输入之间插入一个线性变换，以压缩或扩展 ViT 输出以匹配大型语言模型的输入维度。

> **扩展到其他媒体和领域**

既然我们知道了如何整合图像，我们可以采用类似的方案来包含其他媒体类型。它们不一定要使用 Transformer 架构。谷歌研究在机器人领域[7]中使用了多模态基础模型，通过将状态、场景中的对象和其他输入源转换为相同的向量嵌入空间来实现。这些输入格式中的许多实际上是通过非常简单的变换转换到嵌入空间的。谷歌研究最近还将这一方法适应于医疗领域，并创建了一个用于医疗保健的多模态基础模型[8]。有关它们的用途，请参见图-3。

![](../Images/e157128e75cea13e1cdb7696f81c6cff.png)

图-3: 多模态基础模型在 (a) 机器人技术和 (b) 医疗领域的应用示例。图片来源于参考文献[7]和[8]

> **结论**

在这篇博客文章中，我们窥见了多模态基础模型领域的前沿。核心思想有两个方面：（1）将多媒体编码到相同的嵌入空间，以便可以由 LLM 架构统一处理；（2）将大型预训练模型（它们通常在单模态设置中预训练）结合在一起，并使用多媒体训练数据进行微调。

谷歌研究最近的一些突破确实开始接近人们希望并可以在日常生活中使用的 AI 代理。虽然还有很长的路要走才能让 AI 更加易用和有帮助，但毫无疑问，多模态基础模型是 AI 的拐点，未来的发展将会更加激动人心。

> **参考文献**

[1] PaLM: 使用路径扩展语言建模 [https://arxiv.org/abs/2204.02311](https://arxiv.org/abs/2204.02311)

[2] 大型语言模型编码临床知识 [https://arxiv.org/abs/2212.13138](https://arxiv.org/abs/2212.13138)

[3] 一张图胜过 16x16 个词：用于大规模图像识别的 Transformer [https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929)

[4] 扩展视觉 Transformer [https://arxiv.org/abs/2106.04560](https://arxiv.org/abs/2106.04560)

[5] 注意力机制是你所需要的一切 [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)

[6] PaLI: 联合缩放的多语言语言-图像模型 [https://arxiv.org/abs/2209.06794](https://arxiv.org/abs/2209.06794)

[7] PaLM-E：一种具身的多模态语言模型 [https://arxiv.org/abs/2303.03378](https://arxiv.org/abs/2303.03378)

[8] 朝向通用生物医学人工智能 [https://arxiv.org/abs/2307.14334](https://arxiv.org/abs/2307.14334)
