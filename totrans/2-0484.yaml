- en: Chain of Thought Prompting for LLMs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs 的思维链提示
- en: 原文：[https://towardsdatascience.com/chain-of-thought-prompting-for-llms-33c963eead38](https://towardsdatascience.com/chain-of-thought-prompting-for-llms-33c963eead38)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/chain-of-thought-prompting-for-llms-33c963eead38](https://towardsdatascience.com/chain-of-thought-prompting-for-llms-33c963eead38)
- en: A practical and simple approach for “reasoning” with LLMs
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个实用且简单的“推理”方法与 LLMs
- en: '[](https://wolfecameron.medium.com/?source=post_page-----33c963eead38--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----33c963eead38--------------------------------)[](https://towardsdatascience.com/?source=post_page-----33c963eead38--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----33c963eead38--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----33c963eead38--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wolfecameron.medium.com/?source=post_page-----33c963eead38--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----33c963eead38--------------------------------)[](https://towardsdatascience.com/?source=post_page-----33c963eead38--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----33c963eead38--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----33c963eead38--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----33c963eead38--------------------------------)
    ·16 min read·Jul 24, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----33c963eead38--------------------------------)
    ·阅读时长 16 分钟·2023年7月24日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/29d7dcc5bce6b373e4e07c854001e445.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29d7dcc5bce6b373e4e07c854001e445.png)'
- en: (Photo by [Matthew Lancaster](https://unsplash.com/@matthewelancaster?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/chain?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: （照片由 [Matthew Lancaster](https://unsplash.com/@matthewelancaster?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，来自 [Unsplash](https://unsplash.com/s/photos/chain?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)）
- en: The success of large language models (LLMs) stems from our ability to pre-train
    (using a [language modeling objective](https://cameronrwolfe.substack.com/i/85568430/language-modeling))
    [decoder-only transformer](https://twitter.com/cwolferesearch/status/1640446111348555776?s=20)
    models across massive textual corpora. Given that we pre-train sufficiently large
    models, LLMs are incredibly capable [few-shot learners](https://cameronrwolfe.substack.com/i/88082618/language-models-are-few-shot-learners).
    In other words, this means that we can solve a variety of different problems (e.g.,
    translation, sentence classification, summarization, etc.) by just formulating
    a textual prompt (potentially containing a few example of correct output) and
    having the LLM generate the correct answer.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的成功源于我们能够在大量文本语料库上进行预训练（使用[语言建模目标](https://cameronrwolfe.substack.com/i/85568430/language-modeling)）[仅解码器变换器](https://twitter.com/cwolferesearch/status/1640446111348555776?s=20)模型。考虑到我们对足够大的模型进行预训练，LLMs
    在[少量样本学习](https://cameronrwolfe.substack.com/i/88082618/language-models-are-few-shot-learners)方面表现出色。换句话说，这意味着我们可以通过简单地制定一个文本提示（可能包含一些正确输出的示例）并让
    LLM 生成正确答案来解决各种不同的问题（例如，翻译、句子分类、摘要等）。
- en: Despite the power of LLMs, there are some problems that these models consistently
    struggle to solve. In particular, reasoning problems (e.g., arithmetic or commonsense
    reasoning) are notoriously difficult. Initial attempts to solve this issue explored
    fine-tuning LLMs and task-specific verification modules over a supervised dataset
    of solutions and explanations of various reasoning problems [3, 4]. However, recent
    work has found that few-shot learning can be leveraged for an easier solution.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 LLMs 非常强大，但这些模型仍然存在一些持续难以解决的问题。特别是，推理问题（例如，算术或常识推理）特别困难。最初的尝试是通过对监督数据集中的各种推理问题的解决方案和解释进行微调
    LLMs 和任务特定的验证模块 [3, 4]。然而，最近的研究发现可以利用少量样本学习来找到更简单的解决方案。
- en: “The goal of this paper is to endow language models with the ability to generate
    a chain of thought — a coherent series of intermediate reasoning steps that lead
    to the final answer for a problem.” *— from [1]*
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “本文的目标是赋予语言模型生成思维链的能力——一系列连贯的中间推理步骤，这些步骤将引导到问题的最终答案。” *— 来源于 [1]*
- en: In particular, chain-of-thought (CoT) prompting [1] is a recently-proposed technique
    that improves LLM performance on reasoning-based tasks via few-shot learning.
    Similar to standard prompting techniques, CoT prompting inserts several example
    solutions to reasoning problems into the LLM’s prompt. Then, each example is paired
    with a chain of thought, or a series of intermediate reasoning steps for solving
    the problem. The LLM then learns (in a few-shot manner) to generate similar chains
    of thought when solving reasoning problems. Such an approach uses minimal data
    (i.e., just a few examples for prompting), requires no task-specific fine-tuning,
    and significantly improves LLM performance on reasoning-based benchmarks, especially
    for larger models.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/009564d1f7a50d2b1003254acec03b64.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
- en: (from [1, 11])
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Core Concepts
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To understand CoT prompting, we need a baseline understanding of [LLMs](https://twitter.com/cwolferesearch/status/1639378997627826176?s=20)
    and [how they work](https://twitter.com/cwolferesearch/status/1635693551584522256?s=20).
    Luckily, we have covered this topic extensively in prior overviews:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Language Models [[link](https://cameronrwolfe.substack.com/p/language-models-gpt-and-gpt-2)]
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT-3 and Language Model Scaling Laws [[link](https://cameronrwolfe.substack.com/p/language-model-scaling-laws-and-gpt)]
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modern LLMs [[link](https://cameronrwolfe.substack.com/p/modern-llms-mt-nlg-chinchilla-gopher)]
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specialized LLMs [[link](https://cameronrwolfe.substack.com/p/specialized-llms-chatgpt-lamda-galactica)]
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will not cover the fundamentals of LLMs within this section. Rather, we
    will focus upon developing a better understanding of prompting and few-shot learning
    for LLMs, as well as explore how such techniques may be leveraged to solve a core
    limitation of these models: *their inability to solve reasoning tasks.*'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Prompting and Few-Shot Learning
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After the proposal of language models like [GPT](https://cameronrwolfe.substack.com/i/85568430/improving-language-understanding-by-generative-pre-training-gpt)
    and [GPT-2](https://cameronrwolfe.substack.com/i/85568430/language-models-are-unsupervised-multitask-learners-gpt),
    we knew that pre-training via a [self-supervised](https://cameronrwolfe.substack.com/i/74325854/self-supervised-pre-training)
    next-token prediction (or [language modeling](https://cameronrwolfe.substack.com/i/85568430/language-modeling))
    objective was quite powerful. However, the correct way to adapt these generic
    [foundation models](https://cameronrwolfe.substack.com/i/85568430/creating-foundation-models)
    to solve a specific, downstream task was not as clear. For example, GPT fine-tunes
    the model on downstream tasks, while GPT-2 solves problems in a zero-shot manner;
    see below.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ec97fc06f43ce8440f250d1a61df6c84.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: (from [2])
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: After the proposal of [GPT-3](https://cameronrwolfe.substack.com/i/88082618/language-models-are-few-shot-learners)
    [2], we saw that LLMs of sufficient scale can perform few-shot learning quite
    well. After pre-training via a language modeling objective, GPT-3 (a 175 billion
    parameter LLM), was found to accurately solve a variety of different language
    tasks without any fine-tuning. We can instead replace fine-tuning with a prompting
    approach.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在提出 [GPT-3](https://cameronrwolfe.substack.com/i/88082618/language-models-are-few-shot-learners)
    [2] 后，我们看到足够规模的 LLM 可以非常好地进行少样本学习。在通过语言建模目标进行预训练后，GPT-3（一个 1750 亿参数的 LLM）被发现能够准确地解决各种不同的语言任务，而无需任何微调。我们可以用提示方法代替微调。
- en: 'More specifically, prompting exploits the text-to-text structure of language
    models by providing inputs like the following:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，提示利用了语言模型的文本到文本结构，通过提供如下输入：
- en: '“Translate this sentence to English: `<sentence> =>`”'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “将这个句子翻译成英语：`<sentence> =>`”
- en: '“Summarize the following document: `<document> =>`”.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “总结以下文档：`<document> =>`”。
- en: These task-solving “prompts” enable zero-shot (i.e., without seeing examples
    of correct output; see above) or few-shot (i.e., with a few examples of correct
    output inserted in the prompt; see below) inference with language models. The
    most appropriate output from the language model should solve the task (e.g., summarizing
    a document or completing a reasoning task), meaning that we can solve a variety
    of problems with accurate next-token prediction!
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这些任务解决“提示”使得在语言模型中进行零样本（即，不查看正确输出的示例；见上文）或少样本（即，在提示中插入一些正确输出的示例；见下文）推理成为可能。语言模型的最合适输出应该能够解决任务（例如，总结文档或完成推理任务），这意味着我们可以通过准确的下一个标记预测来解决各种问题！
- en: '![](../Images/fa37850c126ae5d3124abf1297cac3e2.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa37850c126ae5d3124abf1297cac3e2.png)'
- en: (from [1, 2])
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1, 2]）
- en: We can do a lot with prompting. In fact, an entire field of [prompt engineering](https://www.promptingguide.ai/)
    has been recently created to study how the wording or structure of prompts can
    be optimized to improve LLM performance. But, *sensitivity is a huge consideration
    in this developing field*. LLM performance may change massively given small perturbations
    to the input prompt (e.g., permuting few-shot exemplars decreases GPT-3 accuracy
    from 93.4% to 54.3% on [SST-2](https://huggingface.co/datasets/sst2) [13]). Thus,
    in our study of prompting approaches, we aim to find techniques that *i)* perform
    well and *ii)* are not subject to sensitivity.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过提示做很多事情。事实上，最近创建了一个专门的[提示工程](https://www.promptingguide.ai/)领域，研究如何优化提示的措辞或结构以提高
    LLM 性能。但是，*敏感性是这个发展中的领域中的一个重要考虑因素*。LLM 的性能可能会因为输入提示的微小扰动而发生巨大变化（例如，少样本示例的排列导致
    GPT-3 在 [SST-2](https://huggingface.co/datasets/sst2) [13] 上的准确率从 93.4% 降低到 54.3%）。因此，在我们对提示方法的研究中，我们旨在找到*
    i)* 表现良好且 *ii)* 不受敏感性影响的技术。
- en: Can we solve reasoning with scale?
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们能通过扩大规模来解决推理问题吗？
- en: 'As mentioned above, LLM few-shot learning performance improves with scale,
    but large models are not all that we need. Powerful LLMs require a [combination](https://twitter.com/cwolferesearch/status/1603837192346165248?s=20)
    of large models with massive pre-training datasets [14]. With this in mind, we
    might ask ourselves: *what about the performance of LLMs specifically on reasoning-based
    datasets? Do LLMs get better at reasoning as we scale them up?*'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，LLM 的少样本学习性能随着规模的增加而改善，但大模型并不是我们所需要的一切。强大的 LLM 需要与大规模预训练数据集的[组合](https://twitter.com/cwolferesearch/status/1603837192346165248?s=20)
    [14]。考虑到这一点，我们可能会问自己：*LLM 在基于推理的数据集上的表现如何？随着规模的扩大，LLM 是否在推理方面变得更好？*
- en: “Scaling up model size alone has not proved sufficient for achieving high performance
    on challenging tasks such as arithmetic, commonsense, and symbolic reasoning”
    *— from [1]*
  id: totrans-35
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “单独扩大模型规模并不足以在如算术、常识和符号推理等具有挑战性的任务上取得高性能”*— 来源于 [1]*
- en: Interestingly, we tend to see that using larger models and pre-training datasets
    does not improve LLM reasoning abilities (e.g., see analysis from [Gopher](https://cameronrwolfe.substack.com/i/91134599/scaling-language-models-methods-analysis-and-insights-from-training-gopher)
    [15]). In fact, these models have been heavily criticized for their inability
    to solve basic reasoning tasks. As a result, many researchers claim that LLMs
    are simply [regurgitating training data](https://twitter.com/cwolferesearch/status/1643388671456886788?s=20)
    rather than performing any complex reasoning or analysis. Either way, this overview
    will focus on prompting techniques that try to solve this problem and enable LLMs
    to more easily solve basic reasoning tasks.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，我们发现使用更大的模型和预训练数据集并不会改善LLM的推理能力（例如，参见[Gopher](https://cameronrwolfe.substack.com/i/91134599/scaling-language-models-methods-analysis-and-insights-from-training-gopher)
    [15]的分析）。事实上，这些模型因无法解决基本推理任务而受到严厉批评。因此，许多研究人员声称LLM只是[重复训练数据](https://twitter.com/cwolferesearch/status/1643388671456886788?s=20)，而没有进行任何复杂的推理或分析。无论如何，本概述将重点关注试图解决这一问题并使LLM更容易解决基本推理任务的提示技术。
- en: '![](../Images/f1f2f30f301504780df460d970c1debc.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1f2f30f301504780df460d970c1debc.png)'
- en: (from [4])
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: （来自[4]）
- en: '**prior approaches.** Before learning more about how we can help LLMs to solve
    reasoning problems, it’s useful to understand prior approaches in this space.
    Baseline techniques for arithmetic, commonsense, and symbolic reasoning tasks
    perform task-specific fine-tuning, meaning that the model is trained over supervised
    examples of each reasoning problem being solved. Going further, the best approaches
    train a supplemental “verification” module that can judge the correctness of LLM
    output on reasoning tasks [4]. At test time, this verifier can deduce the best-possible
    output after generating several answers to a problem; see above.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**先前的方法。** 在进一步了解我们如何帮助LLM解决推理问题之前，了解这个领域中的先前方法是有用的。算术、常识和符号推理任务的基线技术执行任务特定的微调，即模型在每个推理问题的监督示例上进行训练。更进一步，最佳方法是训练一个补充的“验证”模块，该模块可以判断LLM在推理任务中的输出正确性[4]。在测试时，这个验证器可以在生成多个问题答案后推断出最佳可能的输出；参见上述内容。'
- en: 'Although these techniques might work relatively well in some cases, they are
    limited for a few reasons:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些技术在某些情况下可能效果相对较好，但由于一些原因，它们有其局限性：
- en: Task-specific fine-tuning is required.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 需要进行任务特定的微调。
- en: The model architecture must be adapted (i.e., via the verification model) for
    each task.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型架构必须针对每个任务进行适应（即，通过验证模型）。
- en: Large amounts of supervised data must be collected.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 必须收集大量的监督数据。
- en: With these limitations in mind, it becomes clear that using a prompting-only
    approach (e.g., CoT prompting) to solving reasoning tasks would be much simpler.
    We could avoid fine-tuning, maintain the same model architecture, collect less
    data, and solve many tasks with the a single pre-trained model checkpoint.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些限制，显而易见，使用仅基于提示的方法（例如，CoT提示）来解决推理任务将会简单得多。我们可以避免微调，保持相同的模型架构，收集更少的数据，并用一个单独的预训练模型检查点解决许多任务。
- en: Some notable LLMs…
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一些显著的LLM…
- en: CoT prompting is a prompting technique that is applied to improve the few-shot
    learning performance of pre-trained LLMs. In [1], a specific group of LLMs are
    used for evaluation, which are listed and explained below.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: CoT提示是一种提示技术，旨在提高预训练LLM的少样本学习性能。在[1]中，使用了一个特定组的LLM进行评估，下面列出了这些模型并进行了说明。
- en: '[GPT-3](https://cameronrwolfe.substack.com/i/88082618/language-models-are-few-shot-learners)
    [2]: 175 billion parameter LLM that is pre-trained using a standard language modeling
    objective.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GPT-3](https://cameronrwolfe.substack.com/i/88082618/language-models-are-few-shot-learners)
    [2]：一个拥有1750亿参数的预训练LLM，使用标准语言建模目标进行训练。'
- en: '[LaMDA](https://cameronrwolfe.substack.com/i/93578656/lamda-language-modeling-for-dialog-applications)
    [5]: an LLM-based dialogue model that is pre-trained using a language modeling
    objective, then fine-tuned over dialogue data and human-provided feedback (models
    with sizes 422M, 2B, 8B, 68B, and 137B exist).'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LaMDA](https://cameronrwolfe.substack.com/i/93578656/lamda-language-modeling-for-dialog-applications)
    [5]：一个基于LLM的对话模型，使用语言建模目标进行预训练，然后在对话数据和人工反馈上进行微调（存在422M、2B、8B、68B和137B等不同规模的模型）。'
- en: '[PaLM](https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive)
    [6]: an LLM that is pre-trained using a standard language modeling objective,
    Google’s [Pathways](https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/)
    framework, and a massive textual corpus (models with sizes 8B, 62B, and 540B exist).'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Codex](https://cameronrwolfe.substack.com/i/93578656/evaluating-large-language-models-trained-on-code)
    [7]: a 12 billion parameter LLM that is pre-trained using a standard language
    modeling objective, then fine-tuned on publicly-available Python code from GitHub.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'UL2–20B [8]: an LLM that is pre-trained using the [Mixture-of-Denoisers (MoD)](https://syncedreview.com/2022/05/13/googles-universal-pretraining-framework-unifies-language-learning-paradigms/)
    objective, a unified objective that performs well across many datasets and setups.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beyond these LLMs, we have learned about other models (e.g., [LLaMA](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone),
    [LLaMA extensions](https://cameronrwolfe.substack.com/p/llama-llms-for-everyone),
    and [T5](https://cameronrwolfe.substack.com/p/t5-text-to-text-transformers-part-354))
    in recent overviews as well.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Chain of Thought Prompting
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/5dd3cd159808e0d9e338447fb55aa399.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
- en: (from [1])
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Although we might understand the idea of prompting in general, *what is CoT
    prompting?* CoT simply refers to a specific prompting technique that inserts a
    chain of thought (i.e., a series of intermediate reasoning steps) into an LLM’s
    prompt; see above. For sufficiently large models (>100 billion parameters), this
    approach significantly improves the complex reasoning capabilities of LLMs on
    arithmetic, commonsense and symbolic reasoning tasks.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '**where does CoT prompting come from?** Prior to the proposal of CoT prompting,
    we were already aware that few-shot learning was incredibly powerful for LLMs;
    see below. Instead of [fine-tuning](https://cameronrwolfe.substack.com/i/85568430/improving-language-understanding-by-generative-pre-training-gpt)
    an LLM to perform a task, we just “prompt” a generic model with a few examples
    of the correct output before generating a final answer. Such an approach is [incredibly
    successful](https://cameronrwolfe.substack.com/i/88082618/language-models-are-few-shot-learners)
    for a range of tasks.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d131a84fac4687188c586210a6455ac.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
- en: In-context learning with few-shot prompting (from [2])
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Plus, we knew from related work that generating natural language rationales
    that explain how to arrive at a final answer is beneficial for arithmetic reasoning
    tasks. We can train or fine-tune models to generate these rationales [3, 4], but
    this requires creating a dataset of high-quality rationales for different reasoning
    tasks, which is expensive and time consuming!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: “A prompting only approach is important because it does not require a large
    training dataset and a single model can perform many tasks without loss of generality.”
    *— from [1]*
  id: totrans-61
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*CoT prompting combines the strengths of few-shot prompting with the benefit
    of generating natural language rationales*. Instead of performing extra training
    or fine-tuning, we can just insert a few examples of rationales (i.e., chains
    of thought) into the prompt, allowing the LLM to few-shot learn to generate similar
    rationales.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: How does CoT prompting work?
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When we solve reasoning tasks as humans, it is common to break the problem
    down into smaller tasks. For example, I typically do the following when calculating
    how much I want to tip at a restaurant:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'Take the total amount of the bill: $56.00'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Compute 10% of the total: $5.60'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Multiply this value by 2 (yielding a 20% tip): $11.20'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although this example is simple, the idea extends to a variety of mental reasoning
    tasks that we solve as humans. We generate a chain of thought (defined as “a coherent
    series of intermediate reasoning steps that lead to the final answer for a problem”
    in [1]) to solve such tasks. Put simply, *CoT prompting augments LLMs with the
    ability to generate similar chains of thought*.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '“We explore the ability of language models to perform few-shot prompting for
    reasoning tasks, given a prompt that consists of triples: [input, chain of thought,
    output].” *— from [1]*'
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Examples of solutions to reasoning tasks combined with a chain of thought on
    several different problems are shown below.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/82dea93cb3203eb7234f03e06bd43a47.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
- en: (from [1])
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '**learning chains of thought.** To teach LLMs to generate problem-solving rationales,
    we can just insert examples of such rationales into their prompt. Then, the LLMs
    can leverage their few-shot learning capabilities to generate similar chains of
    thought when solving any reasoning problem. As shown below, the prompt usually
    includes several chain of thought examples.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/82704108a882e37234b1e68428850b7c.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
- en: (from [1])
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: 'Authors in [1] find that such a prompting approach leads LLMs to generate similar
    chains of thought when solving problems, which aids reasoning capabilities and
    has several notable benefits:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '*Interpretability*: the LLM’s generated chain of thought can be used to better
    understand the model’s final answer.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Applicability*: CoT prompting can be used for any task that can be solved
    by humans via language.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Prompting*: no training or fine-tuning is required of any LLMs. We can just
    insert a few CoT examples into the prompt!'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plus, LLMs can even allocate more computational resources to complex reasoning
    problems by just generating a chain of thought with more steps. This mimics what
    we would typically do as humans!
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: CoT Prompting is Massively Beneficial
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To evaluate its impact on the ability of LLMs to solve reasoning problems, CoT
    prompting is tested on arithmetic, commonsense and symbolic reasoning benchmarks.
    Evaluation is performed using several different pre-trained LLMs, including [GPT-3](https://cameronrwolfe.substack.com/i/88082618/language-models-are-few-shot-learners)
    [2], [LaMDA](https://cameronrwolfe.substack.com/i/93578656/lamda-language-modeling-for-dialog-applications)
    [5], [PaLM](https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive)
    [6], [Codex](https://cameronrwolfe.substack.com/i/93578656/evaluating-large-language-models-trained-on-code)
    [7], and UL2 [8]. As a baseline, authors in [1] use standard, few-shot prompting,
    as proposed by GPT-3\. All models use [greedy decoding](https://medium.com/nlplanet/two-minutes-nlp-most-used-decoding-methods-for-language-models-9d44b2375612)
    during evaluation, but better results can be achieved by taking a majority vote
    of multiple samples [9].
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估 CoT 提示对 LLM 解决推理问题能力的影响，CoT 提示在算术、常识和符号推理基准上进行了测试。评估使用了几个不同的预训练 LLM，包括
    [GPT-3](https://cameronrwolfe.substack.com/i/88082618/language-models-are-few-shot-learners)
    [2]、[LaMDA](https://cameronrwolfe.substack.com/i/93578656/lamda-language-modeling-for-dialog-applications)
    [5]、[PaLM](https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive)
    [6]、[Codex](https://cameronrwolfe.substack.com/i/93578656/evaluating-large-language-models-trained-on-code)
    [7] 和 UL2 [8]。作为基准，文献 [1] 中的作者使用了标准的少量提示，由 GPT-3 提出的。所有模型在评估过程中使用 [贪婪解码](https://medium.com/nlplanet/two-minutes-nlp-most-used-decoding-methods-for-language-models-9d44b2375612)，但通过对多个样本进行多数投票可以获得更好的结果
    [9]。
- en: '**arithmetic reasoning.** Arithmetic reasoning tasks consist of math word problems.
    Such problems are simple for humans to solve, but LLMs are known to struggle with
    them. An overview of arithmetic reasoning datasets used in [1] is provided below.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**算术推理。** 算术推理任务包括数学应用题。这样的题目对人类来说很简单，但 LLM 常常对此感到困难。下面提供了 [1] 中使用的算术推理数据集的概述。'
- en: '![](../Images/8cd0eb559d36ba28c3b640828c697dd4.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8cd0eb559d36ba28c3b640828c697dd4.png)'
- en: (from [1])
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: For CoT prompting, a set of eight few-shot examples are manually composed (without
    extensive prompt engineering) and used across all datasets except for AQuA, which
    has a multiple choice structure. Results of experiments with CoT prompting on
    arithmetic reasoning datasets with several LLMs are shown below.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 CoT 提示，一组由八个少量示例组成的提示是手动编写的（无需大量提示工程），并且用于所有数据集，除了具有多项选择结构的 AQuA。下面展示了在多个
    LLM 上进行 CoT 提示的算术推理数据集实验结果。
- en: '![](../Images/b98deaf0a7db5981fe5a3169b3573f49.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b98deaf0a7db5981fe5a3169b3573f49.png)'
- en: (from [1])
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: From these experiments, we discover a few notable properties of CoT prompting.
    First, CoT prompting seems to work much better for larger LLMs (i.e., >100B parameters).
    Smaller models are found to produce illogical chains of thought, thus degrading
    performance compared to standard prompting. Additionally, more complicated problems
    (e.g., GSM8K) see a greater benefit from CoT prompting. Compared to prior state-of-the-art
    methods (which perform task-specific fine-tuning), CoT prompting with GPT-3 and
    PaLM-540B achieves comparable or improved performance in all cases.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些实验中，我们发现 CoT 提示有几个显著的特点。首先，CoT 提示似乎在较大的 LLM（即>100B 参数）上效果更好。较小的模型通常产生不合逻辑的思考链，从而使性能低于标准提示。此外，更复杂的问题（例如
    GSM8K）从 CoT 提示中获得的好处更大。与之前的最先进方法（执行任务特定的微调）相比，使用 GPT-3 和 PaLM-540B 的 CoT 提示在所有情况下都达到了相当或更好的性能。
- en: 'When we qualitatively examine the correct and incorrect answers generated by
    CoT prompting, we learn the following:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们定性地检查 CoT 提示生成的正确和错误答案时，我们得到以下结论：
- en: Most correct answers are the result of a logical chain of thought, aside from
    a few cases that coincidentally predict the correct answer from an incorrect chain
    of thought.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数正确答案是逻辑思考链的结果，除了少数偶然从错误的思考链中预测出正确答案的情况。
- en: 46% of incorrect answers are nearly correct, meaning that they contain chains
    of thought with minor mistakes.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 46% 的错误答案几乎是正确的，这意味着它们包含了有轻微错误的思考链。
- en: 56% of incorrect answers are the result of chains of thought with major issues
    in understanding or reasoning.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 56% 的错误答案是由于思考链在理解或推理上存在主要问题造成的。
- en: Authors in [1] also analyze the robustness of CoT prompting to different structures
    (e.g., permuting few-shot examples) and ablate various aspects of CoT prompting,
    finding that CoT yields a consistent and unique benefit to model performance.
    Interestingly, CoT prompting is not very sensitive to small prompt perturbations.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '**commonsense reasoning.** [Commonsense reasoning](https://huggingface.co/datasets/commonsense_qa)
    problems assume a grasp of general background knowledge and require reasoning
    over physical and human interactions. Adopting a similar setup to arithmetic reasoning
    experiments (aside from a few dataset that require manual curation of few-shot
    examples), authors evaluate a variety of pre-trained LLMs on commonsense reasoning
    tasks, yielding the results shown in the figure below.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7789739d62baa4e37dd6dc2eb889ce2f.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
- en: (from [1])
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Put simply, CoT prompting is found to provide a massive benefit on commonsense
    reasoning problems as well. Again, we see that larger models benefit more from
    CoT prompting. However, both standard prompting and CoT prompting performance
    improves with model scale, where CoT prompting tends to achieve slightly improved
    performance.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '**symbolic reasoning.** Authors in [1] also evaluate CoT prompting on symbolic
    reasoning tasks, such as:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '*Last Letter Concatenation*: asks the model to concatenate and output the last
    letters for each word in a sequence.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Coin Flip*: asks the model to determine if a coin is still heads up after
    a sequence of coin flips.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Going further, both in-domain and out-of-domain symbolic reasoning tests are
    considered, where out-of-domain examples are defined as those that require more
    reasoning steps (e.g., more words in last letter concatenation or a longer sequence
    of coin flips) than exemplars seen during training or in the few-shot prompt.
    The results of in-domain and out-of-domain evaluation are shown below.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8b1dc34e576a2366054ec20bebd6d97a.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
- en: (from [1])
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Although these tasks are simplistic, we see above that CoT thought prompting
    *i)* improves performance on both symbolic reasoning tasks and *ii)* enables better
    generalization to out-of-domain problems that require more reasoning steps. Plus,
    we again observe that smaller models are incapable of solving symbolic reasoning
    tasks both with or without CoT prompting. Thus, CoT prompting again seems to be
    a highly beneficial approach for symbolic reasoning.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Variants of CoT Prompting
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After the proposal of CoT prompting in [1], several variants were proposed that
    can improve the reasoning capabilities of LLMs. These different variants provide
    a variety of interesting and practical approaches for eliciting “reasoning” behavior
    in LLMs. A list of notable CoT prompting variants is provided below.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9b12a33c31ba141041c4706197c8b549.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
- en: (from [10])
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '**zero-shot CoT.** Zero-shot CoT prompting [10] is a simple followup to CoT
    prompting [1]. To encourage an LLM to generate a chain of thought, zero-shot CoT
    simply appends the words “*Let’s think step by step.*” to the end of the question
    being asked. By making this simple addition to the LLM’s prompt, we see in [10]
    that LLMs are able to generate chains of thought even without observing any explicit
    examples of such behavior, allowing them to arrive at more accurate answers on
    reasoning tasks. See above for a comparison of zero-shot CoT with other prompting
    approaches.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**零-shot CoT。** 零-shot CoT 提示[10]是对 CoT 提示[1]的简单跟进。为了鼓励 LLM 生成思维链，零-shot CoT
    仅在问题末尾添加了“*让我们一步步思考。*”这句话。通过在 LLM 的提示中加入这一简单的补充，我们在[10]中看到，LLM 即使在没有观察到这种行为的明确示例的情况下，也能生成思维链，从而在推理任务中得到更准确的答案。请参见上文，以比较零-shot
    CoT 与其他提示方法。'
- en: '![](../Images/cbd3a0eeae9448b9bbfd4df43cb7a434.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cbd3a0eeae9448b9bbfd4df43cb7a434.png)'
- en: (from [11])
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [11])
- en: '**self-consistency.** Self-consistency is a variant of CoT prompting that uses
    the LLM to generate multiple chains of thought, then takes the majority vote from
    these generations as the final answer; see above. In cases where CoT prompting
    is ineffective, using self-consistency oftentimes improves results. Put simply,
    self-consistency just replaces the greedy decoding procedure used in [1] with
    a pipeline that generates multiple answers with the LLM and takes the most common
    of these answers.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**自一致性。** 自一致性是 CoT 提示的一种变体，它使用 LLM 生成多个思维链，然后将这些生成的结果中多数票作为最终答案；见上文。在 CoT
    提示无效的情况下，使用自一致性往往会提高结果。简单来说，自一致性只是用一个生成多个答案的 LLM 管道来替代[1]中使用的贪婪解码过程，并选择这些答案中最常见的作为最终答案。'
- en: '![](../Images/ffc5ba26b329f837b1298e377076908d.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ffc5ba26b329f837b1298e377076908d.png)'
- en: (from [12])
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [12])
- en: '**least-to-most prompting.** Least-to-most prompting goes beyond CoT prompting
    by first breaking down a problem into smaller sub-problems, then solving each
    of these sub-problems individually.As each sub-problem is solved, its answers
    are included in the prompt for solving the next sub-problem. Compared to CoT prompting,
    least-to-most prompting yields accuracy improvements on several tasks (e.g., last
    letter concatenation) and improves generalization to out-of-domain problems that
    require more reasoning steps.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**从少到多提示。** 从少到多提示通过首先将问题分解为更小的子问题，然后单独解决每个子问题，从而超越了 CoT 提示。在解决每个子问题时，其答案会被包含在提示中，用于解决下一个子问题。与
    CoT 提示相比，从少到多提示在多个任务（例如，最后一个字母拼接）上提高了准确性，并改善了对需要更多推理步骤的领域外问题的泛化能力。'
- en: '**prompt engineering.** As demonstrated by the examples above (and the idea
    of CoT prompting in general), curating a useful prompt for an LLM is an art. To
    learn more about how to engineer more effective prompts, I strongly recommend
    the course provided on the Learn Prompting website linked [here](https://learnprompting.org/docs/intro).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示工程。** 正如上述示例所示（以及 CoT 提示的想法），为 LLM 制定一个有用的提示是一门艺术。要了解更多关于如何设计更有效提示的信息，我强烈推荐访问[这里](https://learnprompting.org/docs/intro)提供的
    Learn Prompting 网站上的课程。'
- en: Takeaways
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键点
- en: Within this overview, we have seen that standard prompting is not enough to
    get the most out of LLMs. Rather, it seems to provide a sort of “lower bound”
    for LLM performance, especially on more difficult, reasoning-based tasks. CoT
    prompting goes beyond standard prompting techniques by leveraging few-shot learning
    capabilities of LLMs to elicit the generation of coherent, multi-step reasoning
    processes while solving reasoning-based problems. Such an approach is massively
    beneficial to LLM performance, especially for larger models. Some takeaways are
    provided below.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一概述中，我们看到标准提示不足以充分发挥 LLM 的潜力。相反，它似乎提供了一种“下限”来衡量 LLM 的表现，尤其是在更困难的推理任务上。CoT
    提示通过利用 LLM 的少量示例学习能力，超越了标准提示技术，从而在解决基于推理的问题时引导生成连贯的多步骤推理过程。这种方法对 LLM 的表现极为有利，尤其是对于更大的模型。以下是一些关键点。
- en: '**the utility of CoT prompting.** LLMs are bad at handling tasks like commonsense,
    arithmetic, and symbolic reasoning. However, CoT prompting drastically improves
    performance on these tasks. Plus, this approach requires *i)* no fine-tuning and
    *ii)* minimal extra data (i.e., just a set of exemplars for few-shot learning).
    Thus, it is an easy-to-use technique that, given some effort on prompt engineering
    and curation of a few exemplars, can help pre-trained LLMs to solve tasks with
    which they typically struggle.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**CoT 提示的实用性。** LLMs 在处理常识、算术和符号推理等任务时表现不佳。然而，CoT 提示显著提高了这些任务的表现。此外，这种方法不需要
    *i)* 微调和 *ii)* 最少的额外数据（即，仅需一组示例用于少量学习）。因此，这是一种易于使用的技术，经过一些提示工程和少量示例的策划，可以帮助预训练的
    LLMs 解决它们通常面临的困难任务。'
- en: '**reasoning emerges with scale.** Not all models benefit from CoT prompting.
    In fact, larger LLMs see a disproportionate benefit from CoT prompting compared
    to smaller models. In [1], authors observe that the benefits of CoT prompting
    emerge in models with >100 billion parameters. This exact number is likely to
    depend heavily on experimental settings, but the general idea is clear: *the benefit
    of CoT prompting is most noticeable in larger models*.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**推理随着规模的增长而显现。** 并非所有模型都能从 CoT 提示中受益。事实上，相较于较小的模型，较大的 LLMs 从 CoT 提示中获得的好处更加明显。在
    [1] 中，作者观察到 CoT 提示的好处在参数超过 1000 亿的模型中显现。这个具体数字可能会严重依赖于实验设置，但总体思路很明确：*CoT 提示的好处在较大的模型中最为明显*。'
- en: “Chain of thought emulates the thought processes of human reasoners. This does
    not answer whether the neural network is actually reasoning.” *— from [1]*
  id: totrans-122
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “链式思维模拟了人类推理者的思维过程。这并不能回答神经网络是否真的在进行推理。” *— 引自 [1]*
- en: '**do LLMs actually know how to reason?** CoT prompting helps LLMs to solve
    certain reasoning tasks, but this does not necessarily mean that LLMs possess
    complex reasoning capabilities. Authors in [1] even specifically state that analysis
    of CoT prompting does not answer whether LLMs are actually reasoning or not. Rather,
    CoT prompting is an empirical technique that can be used to more accurately solve
    tasks like arithmetic, commonsense, and symbolic reasoning that are typically
    problematic for LLMs. Whether we believe that LLMs are capable of reasoning or
    not, we can agree that this technique is practically useful.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**LLMs 是否真的知道如何推理？** CoT 提示帮助 LLMs 解决某些推理任务，但这并不一定意味着 LLMs 具备复杂的推理能力。[1] 的作者甚至明确指出
    CoT 提示的分析并不能回答 LLMs 是否真正进行推理。相反，CoT 提示是一种经验性技术，可以更准确地解决像算术、常识和符号推理这样通常对 LLMs 成为难题的任务。无论我们是否相信
    LLMs 能够推理，我们都可以同意这项技术在实际应用中非常有用。'
- en: Closing Remarks
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结束语
- en: Thanks so much for reading this article. I am [Cameron R. Wolfe](https://cameronrwolfe.me/),
    Director of AI at [Rebuy](https://www.rebuyengine.com/). I study the empirical
    and theoretical foundations of deep learning. You can also check out my [other
    writings](https://medium.com/@wolfecameron) on medium! If you liked it, please
    follow me on [twitter](https://twitter.com/cwolferesearch) or subscribe to my
    [Deep (Learning) Focus newsletter](https://cameronrwolfe.substack.com/), where
    I help readers build a deeper understanding of topics in AI research via understandable
    overviews of popular papers.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你阅读这篇文章。我是 [Cameron R. Wolfe](https://cameronrwolfe.me/)，[Rebuy](https://www.rebuyengine.com/)
    的人工智能总监。我研究深度学习的经验和理论基础。你还可以查看我在 medium 上的 [其他文章](https://medium.com/@wolfecameron)！如果你喜欢，请在
    [twitter](https://twitter.com/cwolferesearch) 上关注我，或者订阅我的 [Deep (Learning) Focus
    新闻简报](https://cameronrwolfe.substack.com/)，我通过对流行论文的易懂概述帮助读者深入理解人工智能研究中的主题。
- en: Bibliography
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Wei, Jason, et al. “Chain of thought prompting elicits reasoning in large
    language models.” *arXiv preprint arXiv:2201.11903* (2022).'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Wei, Jason, 等. “链式思维提示引发大规模语言模型的推理。” *arXiv 预印本 arXiv:2201.11903* (2022)。'
- en: '[2] Brown, Tom, et al. “Language models are few-shot learners.” *Advances in
    neural information processing systems* 33 (2020): 1877–1901.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Brown, Tom, 等. “语言模型是少量学习者。” *神经信息处理系统进展* 33 (2020): 1877–1901。'
- en: '[3] Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017\. Program
    induction by rationale generation: Learning to solve and explain algebraic word
    problems. ACL.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Wang Ling, Dani Yogatama, Chris Dyer, 和 Phil Blunsom. 2017\. 通过理由生成的程序归纳：学习解决和解释代数文字问题。ACL。'
- en: '[4] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro
    Nakano, Christopher Hesse, and John Schulman. 2021\. Training verifiers to solve
    math word problems. arXiv preprint arXiv:2110.14168.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro
    Nakano, Christopher Hesse 和 John Schulman. 2021\. 训练验证者以解决数学文字问题。arXiv 预印本 arXiv:2110.14168。'
- en: '[5] Thoppilan, Romal, et al. “Lamda: Language models for dialog applications.”
    *arXiv preprint arXiv:2201.08239* (2022).'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Thoppilan, Romal 等人。“Lamda: 对话应用的语言模型。” *arXiv 预印本 arXiv:2201.08239*（2022年）。'
- en: '[6] Chowdhery, Aakanksha, et al. “Palm: Scaling language modeling with pathways.”
    *arXiv preprint arXiv:2204.02311* (2022).'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Chowdhery, Aakanksha 等人。“Palm: 扩展语言建模的途径。” *arXiv 预印本 arXiv:2204.02311*（2022年）。'
- en: '[7] Chen, Mark, et al. “Evaluating large language models trained on code.”
    *arXiv preprint arXiv:2107.03374* (2021).'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Chen, Mark 等人。“评估训练于代码的大型语言模型。” *arXiv 预印本 arXiv:2107.03374*（2021年）。'
- en: '[8] Tay, Yi, et al. “Ul2: Unifying language learning paradigms.” *The Eleventh
    International Conference on Learning Representations*. 2022.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Tay, Yi 等人。“Ul2: 统一语言学习范式。” *第十一届国际学习表征会议*。2022年。'
- en: '[9] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou.
    2022a. Self-consistency improves chain of thought reasoning in language models.
    arXiv preprint arXiv:2203.11171.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi 和 Denny Zhou.
    2022a. 自一致性改进语言模型中的思维链推理。arXiv 预印本 arXiv:2203.11171。'
- en: '[10] Kojima, Takeshi, et al. “Large language models are zero-shot reasoners.”
    *arXiv preprint arXiv:2205.11916* (2022).'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] Kojima, Takeshi 等人。“大型语言模型是零-shot 推理者。” *arXiv 预印本 arXiv:2205.11916*（2022年）。'
- en: '[11] Wang, Xuezhi, et al. “Self-consistency improves chain of thought reasoning
    in language models.” *arXiv preprint arXiv:2203.11171* (2022).'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Wang, Xuezhi 等人。“自一致性改进语言模型中的思维链推理。” *arXiv 预印本 arXiv:2203.11171*（2022年）。'
- en: '[12] Zhou, Denny, et al. “Least-to-most prompting enables complex reasoning
    in large language models.” *arXiv preprint arXiv:2205.10625* (2022).'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] Zhou, Denny 等人。“从最少到最多的提示使大型语言模型能够进行复杂推理。” *arXiv 预印本 arXiv:2205.10625*（2022年）。'
- en: '[13] Zhao, Zihao, et al. “Calibrate before use: Improving few-shot performance
    of language models.” *International Conference on Machine Learning*. PMLR, 2021.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] Zhao, Zihao 等人。“使用前校准: 提升语言模型的少样本性能。” *国际机器学习会议*。PMLR，2021年。'
- en: '[14] Hoffmann, Jordan, et al. “Training compute-optimal large language models.”
    *arXiv preprint arXiv:2203.15556* (2022).'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] Hoffmann, Jordan 等人。“训练计算最优的大型语言模型。” *arXiv 预印本 arXiv:2203.15556*（2022年）。'
- en: '[15] Rae, Jack W., et al. “Scaling language models: Methods, analysis & insights
    from training gopher.” *arXiv preprint arXiv:2112.11446* (2021).'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] Rae, Jack W. 等人。“扩展语言模型: 方法、分析与训练 Gopher 的见解。” *arXiv 预印本 arXiv:2112.11446*（2021年）。'
