["```py\nfrom pyspark.sql.functions import col, sum, when, mean, countDistinct\nfrom pyspark.sql import functions as F\nfrom pyspark.ml.feature import PCA\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.sql.window import Window\n```", "```py\n# Point file path\npath = '/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv'\n\n# Load Data\ndf = spark.read.csv(path, header=True, inferSchema= True)\ndf = df.filter( col('y') < 30)\n```", "```py\n# Get only our selected columns\ncols = [ 'carat', 'table', 'depth']\n\ndf_num = df.select(F.log1p('carat').alias('carat'), \n                   F.log1p('table').alias('table'), \n                   F.log1p('depth').alias('depth'))\n```", "```py\nassembler = VectorAssembler(inputCols= cols, outputCol=\"features\")\nprepared_df = assembler.transform(df_num)\n```", "```py\n# Run PCA it with all the possible components\npca = PCA(k= len(df_num.columns), inputCol=\"features\")\npca.setOutputCol(\"pca_features\")\n\n# Fit\nmodel = pca.fit(prepared_df)\n```", "```py\n# See explained variance of the PCs\ndf_var = ps.DataFrame(model.explainedVariance, columns=['explained_var'])\ndf_var.insert(0, 'Component', \n              value= ['PC'+str(n) for n in range(1,len(df_num.columns)+1)])\n```", "```py\n# Get transformed Output\nmodel.setOutputCol(\"output\")\npca_transformed = model.transform(prepared_df).select('output')\n```", "```py\n# Collect the transformed results\ntemp = spark.createDataFrame(pca_transformed.collect())\n```", "```py\n# Cast data to text\ntemp = temp.select(col('output').cast('string'))\n\n[OUT]\noutput\n[-0.31666817904639416,-2.8476048469770308,5.090764334358518]\n[-0.32446613567339955,-2.833791365368773,5.079981934273924]\n[-0.3312692998472932,-2.777878254082261,5.094343264160385]\n[-0.33271613063654204,-2.830862619169142,5.089337919087877]\n```", "```py\n# split columns\ndf_transformed = (\n    temp\n    .select(\n        F.split('output', ',')[0][2:15].alias('PC1').cast('float'),\n        F.split('output', ',')[1].cast('float').alias('PC2'),\n        F.split('output', ',')[2][0:14].alias('PC3').cast('float')       ) \n        .fillna(0)\n        )\n```", "```py\n# Explained variance array\nexpl_var = model.explainedVariance\n\ndf_transformed = (\n        df_transformed\n        .withColumn('_c0', F.row_number().over(Window.partitionBy().orderBy(F.lit(1))))\n        .withColumn('score', (col('PC1') * expl_var[0]) + (col('PC2') * expl_var[1]) + (col('PC3') * expl_var[2]) )\n        .withColumn('rank', F.dense_rank().over(Window.partitionBy().orderBy('score')) )\n        .sort('_c0')\n        )\n```", "```py\n# Reduce to k components\npca = PCA(k = k, inputCol=\"features\")\n```"]