- en: A Practitioner’s Guide to Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-practitioners-guide-to-reinforcement-learning-1f1e249f8fa5](https://towardsdatascience.com/a-practitioners-guide-to-reinforcement-learning-1f1e249f8fa5)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Reinforcement Learning](https://medium.com/tag/reinforcement-learning)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Take your first steps in writing game-winning AI agents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://dr-robert-kuebler.medium.com/?source=post_page-----1f1e249f8fa5--------------------------------)[![Dr.
    Robert Kübler](../Images/3b8d8b88f76c0c43d9c305e3885e7ab9.png)](https://dr-robert-kuebler.medium.com/?source=post_page-----1f1e249f8fa5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1f1e249f8fa5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1f1e249f8fa5--------------------------------)
    [Dr. Robert Kübler](https://dr-robert-kuebler.medium.com/?source=post_page-----1f1e249f8fa5--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1f1e249f8fa5--------------------------------)
    ·15 min read·Nov 18, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/93f09881c02997c942a1dd69ddc3b8eb.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Vincent Guth](https://unsplash.com/@vingtcent?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning, data scientists primarily navigate the territories of supervised
    and unsupervised learning. However, there is a distinct and interesting subfield
    — **reinforcement learning**!
  prefs: []
  type: TYPE_NORMAL
- en: In reinforcement learning, we try to teach a so-called **agent** how to navigate
    the complexities of *games*, placing it within a simulated environment where it
    explores strategies, receives rewards for successful moves, and faces penalties
    for missteps.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b539adcd0ea6511de8d0f141fb96c4dd.png)'
  prefs: []
  type: TYPE_IMG
- en: The typical reinforcement overview. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: One prominent outcome of the field of reinforcement learning is [AlphaGo](https://en.wikipedia.org/wiki/AlphaGo),
    a model that has beaten the world champions of [Go](https://en.wikipedia.org/wiki/Go_(game)),
    a game more complex than chess.
  prefs: []
  type: TYPE_NORMAL
- en: The great thing about reinforcement learning is that we do not have to tell
    the agent **how** to win. We just need to tell it what winning or losing looks
    like.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In chess, for example, it’s checkmating the opponent’s king, and that’s the
    only guidance we provide. No explicit instructions on the importance of queens
    or the insignificance of pawns — the agent deduces these nuances itself.
  prefs: []
  type: TYPE_NORMAL
- en: And it’s not restricted to *traditional games*; virtually anything can be treated
    as a game. Whether it’s a classic board game, a video game, or a business scenario,
    such as determining the most effective ad for a customer, reinforcement learning
    is at play. In the business scenario, the agent could gain rewards for successful
    customer purchases, lesser rewards for ad clicks, and face penalties when ads
    are ignored by a customer. It becomes a strategic game for the agent, optimizing
    rewards, which, in a business context, translates to revenue.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, I will not go too much into the mathematical theory of reinforcement
    learning. I want to give you the **intuition and working code** to get you started.
    To this end, I will use the great library [gymnasium](https://gymnasium.farama.org/)
    that provides some neat game environments that our agents can learn to master.
  prefs: []
  type: TYPE_NORMAL
- en: '***You can find the code in*** [***my Github***](https://github.com/Garve/towards_data_science/blob/main/A%20Practitioner%E2%80%99s%20Guide%20to%20Reinforcement%20Learning.ipynb)***!***'
  prefs: []
  type: TYPE_NORMAL
- en: Warm Up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we start, let us get used to gymnasium first, to see what it can do for
    us. First, install it via `pip install gymnasium[toy-text] Pillow` . The `toy-text`
    part install [pygame](https://www.pygame.org/news) as well, so we can have some
    nice visuals.
  prefs: []
  type: TYPE_NORMAL
- en: Start a game
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We create a game instance now and print a picture of the game state.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/4b54c861e617c52b0c0ee4006f2586ca.png)'
  prefs: []
  type: TYPE_IMG
- en: A screenshot from the game’s starting state. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: The game is called Frozen Lake, and the goal is to move the character (top left)
    to the present (bottom right) without falling into the frozen lakes. To do so,
    you can move the character up, down, left, or right. Easy, right?
  prefs: []
  type: TYPE_NORMAL
- en: Change the state with an action
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can now take a step down with your character by doing `env.step(1)` . Another
    `Image.fromarray(env.render())` gives us
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fd6b275f6c4aac78bc4ee78b52d9cf70.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are in total four actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**0 —** move left'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**1 —** move down'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2** — move right'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**3** — move up'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`env.step(...)` also yields a tuple `(4, 0.0, False, False, {"prob": 1.0})`
    which is a bit cryptic to read. We will only pay attention to the first three
    entries, these are: (observation, reward, terminated, …). This means that after
    taking a step down, we are in position **4**, we got a reward of **0.0**, and
    the game has **not ended** yet.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/685e43f22381fb63a51f0ac04bd4a653.png)'
  prefs: []
  type: TYPE_IMG
- en: The states (positions). Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: You can take another step right via `env.step(2)` and end up in state 5, get
    no reward again, and the game ended since you fell into a frozen lake. Game over.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/19e8c1b45fcf69fb3f3173535ecc877c.png)'
  prefs: []
  type: TYPE_IMG
- en: Game over. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Winning the game
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you reach the present by picking a correct sequence of actions, you get a
    reward of 1.0 and the game ends. One sequence could be (1, 1, 2, 1, 2, 2).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dbc2ce19c4cbfdce642f4cf1f6c9ced2.png)'
  prefs: []
  type: TYPE_IMG
- en: You win! Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Winning Frozen Lake Using Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this simple game, we could get away without reinforcement learning if we
    wanted. We could just code a [depth-first search](https://en.wikipedia.org/wiki/Depth-first_search)
    from the start position, where the present is the vertex that we want to reach.
    However, this does not work for every game, especially when you deal with **stochastic**
    games, i.e., when the steps you can take result in potentially many new states.
    We will see an example of this later.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, in the following, we will develop an algorithm that can produce **policies**
    to win this, but also many other games. A policy is a kind of **plan** that you
    follow to win the game. It tells you which action you should take depending on
    which state (i.e. the position on the board) you are in.
  prefs: []
  type: TYPE_NORMAL
- en: Q-Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One way to build a policy is by using something called a **Q-learning**, where
    the Q stands for quality. In this approach, you want to build a function *Q* that
    takes a state *s* and an action *a* and outputs a number *Q*(*s*, *a*) that tells
    you how good it is to take action *a* in state *s.*
  prefs: []
  type: TYPE_NORMAL
- en: Q(s, a) = goodness (quality) measure of how good it is to take action a when
    you are in state s
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Note:** You can also view *Q* as a two-dimensional lookup table (dataframe).
    The index is the game state, and the column could be the action, for example.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3e7f0864f870e8d3dc7ce84682a3355d.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, you have *Q*(3, B) = 1.6\. If you have such a Q-function,
    you can determine what the best action to take is like this: you plug in all the
    different actions that you can take and choose the action that maximizes the Q-value.
    This makes sense since *Q* measures the goodness of a state, and you always want
    to be in the best state. In the example above, the best action to take if you
    are in State 1 would be Action A. If you are in State 2 or 3, you should take
    Action B. The big question is:'
  prefs: []
  type: TYPE_NORMAL
- en: How to construct this table with meaningful values?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'I take a ***HUGE*** shortcut and tell you a breakthrough in the field of reinforcement
    learning by Watkins from 1989: If you want to approximate the function *Q* that
    gives you the **best** policy, you can initialize the values of *Q*(*s*, *a*)
    with some values — often zero — and then update it according to the following
    rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/46bad550d09e40ccc041ba08bc4f269c.png)'
  prefs: []
  type: TYPE_IMG
- en: The magic of Q-learning. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is quite a handful, and in this article, I cannot go into detail. But
    let me break it down a bit:'
  prefs: []
  type: TYPE_NORMAL
- en: We start with some initial values, let’s say zero for all values *Q*(*s*, *a*).
    We will update these values iteratively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assume that in our game, we are in some state *s*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We take an action *a*. This can be done randomly, but also following some other
    logic.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After doing so, we get an immediate reward *R*(*s*, *a*) from the game (can
    be zero), and we also find ourselves in a new state *s*’ because of our action.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then have to compute what the best action with the highest Q-value is when
    we are in that new state *s’ —* this is the max term.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So, we know what *Q*(*s*, *a*), *R*(*s*, *a*), and max *Q*(*s*’, *a*) are. Then,
    we update our old value of *Q*(*s*, *a*) with the formula above. Here, *α* is
    the learning rate and *γ* is another hyperparameter called *discount*. A high
    discount means that future rewards are very important to the algorithm. A low
    encourages a policy that is greedy and wants to have rewards immediately, even
    if it might not be good in the long run. Usually, you can take something a bit
    smaller than 1, but it depends on the problem.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us implement everything in Python now. First, let us write a tedious helper
    function. This is needed because the states and actions within gymnasium are sometimes
    not that easy to handle.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can initialize the hyperparameters *α* and *γ*, and also initialize
    the most important ingredient: the Q-table!'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The Q-table looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We can see the state numbered from 0 (top left field) to 15 (bottom right field),
    as well as four actions per state. Everything is initialized to zero, but this
    is not a must.
  prefs: []
  type: TYPE_NORMAL
- en: 'The actual training — the heart of everything — works like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The code should finish after a few seconds. We have now created a Q-table and
    take a look at it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You can derive the following best policy from this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: or graphically
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/72b575aa3d9766384f02f7e10e086550.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: The best action on the fields without arrows is actually undefined since the
    game ends in these states. But it is randomly “Left” since it is the first entry
    in the action dictionaryíes.
  prefs: []
  type: TYPE_NORMAL
- en: Play the game
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can use this trained table to win the game now.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: You should see our agent win the game!
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dbc2ce19c4cbfdce642f4cf1f6c9ced2.png)'
  prefs: []
  type: TYPE_IMG
- en: Winning again, but NOT hardcoded! Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: How often do we win?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can also simulate many games to see how often our algorithm wins.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'As mentioned before, if we win, we get a reward of 1.0, otherwise 0.0\. Let
    us count what we got:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: So, we have won every game. This makes sense because the game as well as our
    policy is **completely deterministic**. We always start in the same spot and have
    to reach the goal in the same spot. The field also always looks the same, and
    we behave the same in the same situation. It follows that we either always win,
    or always lose.
  prefs: []
  type: TYPE_NORMAL
- en: Winning Other Games Without Code Changes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You might wonder why we have put so much effort into winning this simple game.
    Well, the great thing about what we have achieved is that it **does not care about
    the type of game we play** as long as it has a **discrete state and action space**.
    Let us try it out!
  prefs: []
  type: TYPE_NORMAL
- en: Taxi
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Execute the same code as before, just with the new game environment
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'It is a game that is about picking up a person and bringing them to a hotel.
    In order to win, you have to:'
  prefs: []
  type: TYPE_NORMAL
- en: Drive to the person.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pick them up.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Drive to the hotel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Drop them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The action space is going left, down, right, up, pick up and drop. You can
    see how to play this game in the following animation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f2148c45d82d217bab5b1d48e9988e2c.png)'
  prefs: []
  type: TYPE_IMG
- en: The AI playing like a pro. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: You can see how our agent manages to win easily after the training!
  prefs: []
  type: TYPE_NORMAL
- en: Blackjack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Blackjack, in short, is about drawing cards as long as you don’t exceed 21\.
    However, you have an opponent: the sum on your hand has to be higher than the
    sum of the dealer. For learning this game, start with'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: and after training you get
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ea0f757056fcc202263a71bba9381152.png)'
  prefs: []
  type: TYPE_IMG
- en: The AI making good decisions. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Here, you can see that the AI™ makes the good decision of drawing another card
    when it is at 14 but stopping at 20.
  prefs: []
  type: TYPE_NORMAL
- en: Why was it good to draw another card at 14? Because it is likely that the dealer,
    who currently has a 6, gets a 10 as the next card (10, Jack, Queen, King). So
    he would likely have 16 and win if you stop at 14.
  prefs: []
  type: TYPE_NORMAL
- en: Slippery Frozen Lake
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The rules are the same as in Frozen Lake, but with a twist: **you don’t always
    get the direction you want**. You can get it via'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'As an example, if you want to go right, there is a 1/3 chance of going right,
    1/3 of going up, and 1/3 of going down. In general, you have a 1/3 chance of going
    anywhere but backward. This makes things much harder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/565aeac3f90bb339a6eb4378b578a23b.png)'
  prefs: []
  type: TYPE_IMG
- en: Winning is not that easy anymore. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: The reason is that being between the two lakes is a dangerous position to be
    in. If you want to go up, you might go left or right by accident, falling into
    the lake with a probability of 2/3\. The same of you go down. So, the best strategy
    would be to move left or right since then you might end up going up or down with
    a probability of 2/3, and on crash into the ice with a probability of 1/3.
  prefs: []
  type: TYPE_NORMAL
- en: 'My agent learned the following policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/696201257dcffb4ec8e0c34e219e34cc.png)'
  prefs: []
  type: TYPE_IMG
- en: The best policy in the slippery case. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: You can clearly see how the agent tries to avoid the lakes. If it is next to
    a lake, it walks away from it to make it impossible to fall inside. The only danger
    lies between the two lakes. The agent wants to go right — which is an optimal
    decision — but might end up falling into the right lake.
  prefs: []
  type: TYPE_NORMAL
- en: 'When I let my agent play, it won in **55%** of the cases. In comparison: if
    it walks randomly, it wins in about 1.5% of the cases, and if we use the **best
    policy from the non-slippery case, it is surprisingly about 1.5%, too**.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6bde5ed08bda4ea21fa1672cf6949971.png)'
  prefs: []
  type: TYPE_IMG
- en: Sliding with style. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bonus: Multi-armed bandit'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a classic among the reinforcement learning introductory examples. Imagine
    you have *n* slot machines, and you try to find the one that gives you the highest
    rewards. You can always play on one machine and observe a reward. There is only
    **a single state** and *n* actions. In our example, *n* = 10.
  prefs: []
  type: TYPE_NORMAL
- en: Gymnasium doesn’t contain this game, but there is a third-party library that
    you can install that integrates with gymnasium.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'You can then create the following environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: and do the Q-learning steps again. I would use only a single episode since the
    game never ends. Just increase the number of `max_steps` a bit. You will see that
    the highest entry in the Q-table will be for the correct slot machine.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e2ee96d07ab9427921d78c95ee48f307.png)'
  prefs: []
  type: TYPE_IMG
- en: No nice animations to see here. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this article, we have seen how we can teach an agent to play simple games
    with a discrete and finite state and action space. This is the setting where a
    simple form of Q-learning can shine: it starts by initializing a state-action
    table and updating it depending on whether the agent’s action in a certain state
    was working towards winning or not. The great thing about it is that we **do not
    have to craft any strategy to help our agent win**. If we do it right, the agent
    can **learn the best way to achieve the goal on its own**, and this is true magic.'
  prefs: []
  type: TYPE_NORMAL
- en: Be careful what you wish for
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes it is tempting to reward some intermediate action that is not winning
    the game because you think it is good to do so. Be aware that the agent might
    abuse this and do things that you did not have in mind. For example, in Frozen
    Lakes you could give the agent a reward when it is close to the goal.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/100ef1b26bc2b34ba06d95eaedd20623.png)'
  prefs: []
  type: TYPE_IMG
- en: An intermediate reward right before the goal. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: If you do this, what would a rational agent do? Let us see.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0334531f9139092781af32bb48057fc4.png)'
  prefs: []
  type: TYPE_IMG
- en: 200 IQ. Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: It just steps on the +1 field right before the present, and then steps down
    for all eternity. Abusing the system like this, the agent gets an infinite amount
    of rewards (always a +1), while stepping on the present — what we actually want
    — ends the game with only 2 points.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous games
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have not talked about games with continuous state or action spaces yet.
    Making a table for infinitely many states or actions seems… challenging. So we
    need other approaches. One way to handle that is to **discretize** the spaces.
    For example, if your observation space consists of real numbers between 0 and
    1, then you can split this interval into 10 buckets, namely:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Bucket 0: [0, 0.1),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bucket 1: [0.1, 0.2),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: …
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bucket 9: [0.9, 1.0].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your Q-table will then have 10 rows for the observations, and during training,
    you always have to map an observation like 0.92341 to bucket number 9.
  prefs: []
  type: TYPE_NORMAL
- en: However, the finer you discretize, the bigger the Q-table gets, until at some
    point you cannot handle it anymore memory-wise.
  prefs: []
  type: TYPE_NORMAL
- en: Another variation of Q-learning is trying to replace the Q-table with a **neural
    network** that maps the state to several outputs, one for each action**.** We
    call this approach **deep Q-learning**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/96a386ac8b010daa9a4dcf5c92c792d5.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can read about it in my other article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/hands-on-deep-q-learning-9073040ce841?source=post_page-----1f1e249f8fa5--------------------------------)
    [## Hands-On Deep Q-Learning'
  prefs: []
  type: TYPE_NORMAL
- en: Level up your agent to win more difficult games!
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/hands-on-deep-q-learning-9073040ce841?source=post_page-----1f1e249f8fa5--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: I hope that you learned something new, interesting, and valuable today. Thanks
    for reading!
  prefs: []
  type: TYPE_NORMAL
- en: If you have any questions, write me on [LinkedIn](https://www.linkedin.com/in/dr-robert-k%C3%BCbler-983859150/)!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: And if you want to dive deeper into the world of algorithms, give my new publication
    All About Algorithms a try! I’m still searching for writers!
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://allaboutalgorithms.com/?source=post_page-----1f1e249f8fa5--------------------------------)
    [## All About Algorithms'
  prefs: []
  type: TYPE_NORMAL
- en: From intuitive explanations to in-depth analysis, algorithms come to life with
    examples, code, and awesome…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: allaboutalgorithms.com](https://allaboutalgorithms.com/?source=post_page-----1f1e249f8fa5--------------------------------)
  prefs: []
  type: TYPE_NORMAL
