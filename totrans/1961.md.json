["```py\nlearning_rate = 0.2\ndiscount_factor = 0.9\nnum_episodes = int(1e7)\nepsilon = 1.0  # Exploration rate\nepsilon_min = 0.01\nepsilon_decay = 0.999\n```", "```py\nfor episode in range(num_episodes):\n    state = [0] * 9  # Starting state - empty board\n```", "```py\n # If Player 2 starts, make a random move\n    if current_player == 2:\n        actions = get_possible_actions(state)\n        random_action = random.choice(actions)\n        state = update_state(state, random_action, 2)\n        current_player = 1  # Switch to Player 1\n```", "```py\nif random.uniform(0, 1) < epsilon:\n    # Explore: choose a random action\n    action = random.choice(actions)\nelse:\n    # Exploit: choose the best action based on Q-table\n    action = max(actions, key=lambda x: Q_table[state_str][x])\n```", "```py\n# Take action and observe new state and reward\nnew_state, reward = get_next_state_and_reward(state, action)\n```", "```py\ndef get_next_state_and_reward(state, action):\n    new_state = update_state(state, action, 1)  # Player 1's move\n    if is_winner(new_state, 1):\n        return (new_state, 1)  # Reward for winning\n    elif 0 not in new_state:\n        return (new_state, 0.1)  # Draw\n    else:\n        # Player 2 (random) makes a move\n        actions = get_possible_actions(new_state)\n        random_action = random.choice(actions)\n        new_state = update_state(new_state, random_action, 2)\n        if is_winner(new_state, 2):\n            return (new_state, -1)  # Penalty for losing\n        else:\n            return (new_state, 0)  # No immediate reward or penalty\n```", "```py\nQ_table[state_str][action] += learning_rate * (\n            reward + discount_factor * max(Q_table[new_state_str]) - Q_table[state_str][action])\n```", "```py\nepsilon = max(epsilon_min, epsilon_decay * epsilon)\n```"]