- en: Naive Bayes Classifier from Scratch, with Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/naive-bayes-classifier-from-scratch-with-python-942708211470](https://towardsdatascience.com/naive-bayes-classifier-from-scratch-with-python-942708211470)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: From theory to practice with Bayes Theorem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://piero-paialunga.medium.com/?source=post_page-----942708211470--------------------------------)[![Piero
    Paialunga](../Images/de2185596a49484698733e85114dd1ff.png)](https://piero-paialunga.medium.com/?source=post_page-----942708211470--------------------------------)[](https://towardsdatascience.com/?source=post_page-----942708211470--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----942708211470--------------------------------)
    [Piero Paialunga](https://piero-paialunga.medium.com/?source=post_page-----942708211470--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----942708211470--------------------------------)
    ·10 min read·Jan 4, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0861ef7d4e87fc5bedc5552305cb25bc.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Joel Abraham](https://unsplash.com/@joe_27?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/8RRYJg26Wr4?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: 'Math and Physics are full of theorems, equations, principles, axioms, and corollaries.
    When I started studying Physics I remembered I got to the point where all the
    courses I studied had the same structures:'
  prefs: []
  type: TYPE_NORMAL
- en: A. Defining the **fundamental assumptions**
  prefs: []
  type: TYPE_NORMAL
- en: B. Using math to build the next “**brick of the wall**”
  prefs: []
  type: TYPE_NORMAL
- en: C. **Stacking one brick on top of the other** until the whole pieces come together
    into an elegant, beautiful, model of a portion of the world
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take the first course I ever did in physics: **calculus.**'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. You start with the fundamental assumptions of **sets** and **numbers**.
    You start defining natural, integer, real, and complex numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. From there you start defining **functions** that are nothing but a **map**
    from space A (let’s say N-dimensional real space) to space B (let’s say a 1D real
    space).
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Then you start with the **study** of the functions. So you start analyzing
    their minima, maxima, and saddle points. You accidentally (*ops!*) get to know
    the concept of **“derivative**”.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Then you see how you can **integrate** a function, that is the opposite
    of the **derivative**.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Then you combine these things with **differential equations.**
  prefs: []
  type: TYPE_NORMAL
- en: 'Don’t get me wrong: that process is **amazing**. I loved to see how far human
    logic can bring you. I loved to see that very complex natural events can be derived
    by starting from very simple concepts that evolve into way deeper implications.'
  prefs: []
  type: TYPE_NORMAL
- en: Another valuable science lesson is the fact that a theorem that is originally
    applied to **“A”** can also be applied to “B”, “C”, “D” and “E”.
  prefs: []
  type: TYPE_NORMAL
- en: The fascinating aspect is that the field “A” and the other fields (B, C, D,
    and E) don’t have to be related.
  prefs: []
  type: TYPE_NORMAL
- en: Possibly the **greatest example** of that is **Bayes’ Theorem**.
  prefs: []
  type: TYPE_NORMAL
- en: Bayes’ theorem is something that is, in a certain way, an elementary and obvious
    concept. The incredibly cool thing is that we can build some very interesting
    algorithms by stressing the idea behind Bayes’ Theorem and applying it to Machine
    Learning. This tool is called the **Naive Bayes Classifier**
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article:'
  prefs: []
  type: TYPE_NORMAL
- en: We will give a brief introduction to **Bayes’ theorem**. We will explain what
    it is, why it is important, and how it can be applied to **Machine Learning**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will see an application of the Bayes theorem in a made-up **classification
    task.**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will see a leveled-up version of the Bayes theorem using the so-called **Gaussian
    Naive Bayes classifier.**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I’m so excited. Let’s start this!
  prefs: []
  type: TYPE_NORMAL
- en: 1\. About the Bayes Theorem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There was one definition of the Bayes Theorem that has really stuck in my brain:'
  prefs: []
  type: TYPE_NORMAL
- en: “The Bayes Theorem is that theorem that proves that just because your car is
    blue it doesn’t mean that all the cars of the world are blue **but** if all the
    cars of the world are blue **then** your car must be blue”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you read this [article](https://medium.com/towards-data-science/from-theory-to-practice-with-bayesian-neural-network-using-python-9262b611b825)
    about Bayesian Neural Networks you probably recognize it is the same definition
    but I swear I didn’t use the same definition twice **because I’m lazy!** I mean…
    I **am lazy** but that is not the reason.
  prefs: []
  type: TYPE_NORMAL
- en: I use that definition because it helps us understand that **event A given a
    condition of event B** does not have the same probability of happening as **event
    B given the condition of event A**.
  prefs: []
  type: TYPE_NORMAL
- en: Let me do another example to overcome my laziness.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say we have two bowls
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2bf3a6d955208812a1c85bdc730b0ca1.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, let’s say that these two bowls are full of **basketball** and **football**
    (it’s called **football**, not *soccer*) **balls**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/afcf95d180c1f284ac39be7064deb4aa.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s say that I ask you that question:'
  prefs: []
  type: TYPE_NORMAL
- en: “ **KNOWING** that I picked a ball from the **blue** bowl, what is the probability
    that I picked a **football** ball ”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Well, the answer is very simple. The probability is **1,** because, on the
    **blue bowl**, I only have football balls. Now let’s say that I pick a bowl randomly
    with equal probability (0.5 probability of picking the white bowl and 0.5 probability
    of picking the blue bowl). Let me ask you this question:'
  prefs: []
  type: TYPE_NORMAL
- en: “ KNOWING that I picked a football ball, what is the probability that I picked
    that ball from the blue bowl? ”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As you can see, the question is kind of the same but event A **(I picked a football
    ball)** and event B **(I picked the blue bowl)** have the opposite order.
  prefs: []
  type: TYPE_NORMAL
- en: Now, I think that you kind of guessed that the answer is not the same, because
    I could pick the football ball from the white bowl rather than the blue one. I
    also think you kind of guessed that this probability is still high though because
    I only have one football ball in the white bowl.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s do the experiment in **Python**.
  prefs: []
  type: TYPE_NORMAL
- en: 'I can go into the line-by-line detail of it but I’d find that pretty boring.
    What I’m doing here is nothing but setting the situation of bowls and balls like
    we did before and then running the probabilistic experiment **N** times. At the
    end of these N iterations, we will have N results. Then we will have to compute
    our probability as a **frequentist** probability. In this case, we will compute:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f459332f8822862dff2f032a5591fc02.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Now we know that if we run N = infinite times in the same experiments we will
    converge to the analytical result, so we will iteratively increase N and see if
    it converges to whatever result.
  prefs: []
  type: TYPE_NORMAL
- en: As you see, we did 20,50,70,…, 1M iterations.
  prefs: []
  type: TYPE_NORMAL
- en: What is the result of this?
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4acb4532e62a463ddd147c98b424ee0f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Mhhh, so it seems like there is indeed an analytical result right? Let’s find
    out.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, Bayes’ theorem tells us that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fa169e5da4f72824d0578b42efb59a8c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: So the probability that an extracted football **comes from the** **blue bowl**
    is equal to the probability of extracting a football **from the blue bowl (***notice
    the difference! this is the probability of extracting a football GIVEN THE FACT
    THAT YOU PICKED THE BLUE BOWL***)** multiplied by the probability of extracting
    from a **blue bowl** divided by the probability of extracting a **football.**
  prefs: []
  type: TYPE_NORMAL
- en: 'We know that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/892214e9bd65def6210abb0c518d9a89.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Cause there are only the football balls in the blue bowl. We also assume that
    we are picking one of the two bowls with equal probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'So we had:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b85721a4bdb0281539ddc20e47419030.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the probability of picking the football is the sum of picking the football
    from the white bowl and picking the football from the blue bowl. Like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/275360dfbea41ddf2cf3724a1f3ba07b.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'So we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5a1be32fe9e16aca21b91b0fb4c868e5.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Because:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9bb8fb6dd963118c53306a973f8fdbdc.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'So we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d766e8f77c3353591b0f6a136adec5f0.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'And if we plot 5/6 in the previous plot we made we get that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e0e6ea52c1030a0858d9948170b1778f.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: We did the math right 😄
  prefs: []
  type: TYPE_NORMAL
- en: 'Ok so at this point I am positive you are all thinking:'
  prefs: []
  type: TYPE_NORMAL
- en: “What does it have to do with Machine Learning at all?”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let’s find out 😏
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Naive Bayes Classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Naive Bayes Classifier is the **Naive** application of the **Bayes** theorem
    to a Machine Learning **classifier:** as simple as that.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say we have a certain binary classification problem (class 1 and class
    2). You have N instances and each instance has its label Y.
  prefs: []
  type: TYPE_NORMAL
- en: 'The so-called **prior probability** is defined as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/460ebdeba045701844437988f87531ad.png)![](../Images/b18459c6bed99eb5547dc9465b76c857.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'Now what we really want to know is: given a certain instance, what is the probability
    that **that** instance belongs to class 1? What is the probability that belongs
    to class 2?'
  prefs: []
  type: TYPE_NORMAL
- en: 'So what we are interested in is, given an instance x:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b2b4c09fb6a56faf101fa56aaa206b1.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: 'In a binary dataset, we have that the sum of these two quantities is 1, so
    we actually only need one of these two quantities. Now this quantity might look
    a mystery, but the other one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1bdd6071c51410a6d69c978afbdc80cb.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: can be computed very easily (remember the bowl/balls example!).
  prefs: []
  type: TYPE_NORMAL
- en: It is also very easy to compute the probability of P(x), in the same way, that
    we computed P(class 1) and P(class 2).
  prefs: []
  type: TYPE_NORMAL
- en: 'So we can actually compute:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b2b0804ee2a11f067702b9768a38167c.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: All of this looks good. Now I think you might have an understanding of why we
    called it **naive**. It is Naive because it is nothing more than counting the
    occurrences and using Bayesian logic to infer the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s apply it to a toy dataset. We have these two classes and two features
    generated using [sklearn.datasets](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html)
    library.
  prefs: []
  type: TYPE_NORMAL
- en: Fantastic.
  prefs: []
  type: TYPE_NORMAL
- en: 'So:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9797c30b7c7147eed0a7fd9ebc110595.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: Because we have two dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: '**Given the class 0**, we can easily compute what is the probability of a given
    area (a small area around a 2D point).'
  prefs: []
  type: TYPE_NORMAL
- en: 'So:'
  prefs: []
  type: TYPE_NORMAL
- en: We know how to compute the **posterior** probability, that is P(Y|X)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We have our set of Y and X
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can apply the rule to the **training set**, and we can **predict** our test
    set results
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s do it!
  prefs: []
  type: TYPE_NORMAL
- en: '**Train-Test split**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '2\. **Importing** and **fitting the Naive Bayes Classifier**:'
  prefs: []
  type: TYPE_NORMAL
- en: Note that we are adding a bias to our data (x). This is because the Bayes classifier
    we are using considers our x_1 feature as a **categorical** feature and we consider
    these features as numerical ones. A more rigorous approach is using the *LabelEncoder()*feature,
    but in this specific case, it is exactly equivalent.
  prefs: []
  type: TYPE_NORMAL
- en: '3\. **Testing the performance**:'
  prefs: []
  type: TYPE_NORMAL
- en: The performance, in this very simple toy example, is obviously (almost) perfect.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Gaussian Naive Bayes Classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You probably have had enough of me talking, I am sorry for that.
  prefs: []
  type: TYPE_NORMAL
- en: I just want to close this article by talking about the Gaussian Naive Bayes.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous example, we consider the features to be **categorical.** But
    what happens when they are not?
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, we have to assume a certain distribution of the likelihood: **P(Y|X)**.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8522e72ad44166c55c56dd2c7b450e92.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by author
  prefs: []
  type: TYPE_NORMAL
- en: So it is called **gaussian** Naive Bayes because the likelihood is considered
    to be gaussian, as you see above. The mean (mu_y) and variance (sigma_y squared)
    are computed by computing the mean and variance of the classes.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that now we don’t need to add the Bias to our data anymore.
  prefs: []
  type: TYPE_NORMAL
- en: These are the results
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Wrapping it up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this article we:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Acknowledge the existence of Bayes’ Theorem**. We had an intuitive introduction
    to it and we did a very simple controlled case to see how it works.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Saw how Bayes’ Theorem can be applied to Machine Learning**. What is Y, what
    is X, and how we can put them into the Bayes’ Formula to get some predictions
    in a **classification task**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We made up our own dataset and we used the **Categorial Naive Bayes** class
    of **sklearn** to do a toy classification task. It worked almost perfectly but
    had the problem of working only with categorical features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We saw how to extend the Categorical Naive Bayes into **Gaussian Naive Bayes.**
    The Gaussian Naive Bayes, as we saw above, uses a Gaussian Likelihood to work.
    This Gaussian Likelihood works with noncategorical features as well.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Bayes Theorem is applied in Machine Learning in a lot of other ways, like
    the Bayesian Neural Networks or the Bayesian Ridge Regression. I feel that this
    is a very cool introductive example of what the application of the Bayes Theorem
    can do in a classification problem. I had a lot of fun writing it and I really
    hope you loved it! 🥰
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you liked the article and you want to know more about machine learning,
    or you just want to ask me something, you can:'
  prefs: []
  type: TYPE_NORMAL
- en: A. Follow me on [**Linkedin**](https://www.linkedin.com/in/pieropaialunga/),
    where I publish all my stories
  prefs: []
  type: TYPE_NORMAL
- en: B. Subscribe to my [**newsletter**](https://piero-paialunga.medium.com/subscribe).
    It will keep you updated about new stories and give you the chance to text me
    to receive all the corrections or doubts you may have.
  prefs: []
  type: TYPE_NORMAL
- en: C. Become a [**referred member**](https://piero-paialunga.medium.com/membership),
    so you won’t have any “maximum number of stories for the month” and you can read
    whatever I (and thousands of other Machine Learning and Data Science top writers)
    write about the newest technology available.
  prefs: []
  type: TYPE_NORMAL
