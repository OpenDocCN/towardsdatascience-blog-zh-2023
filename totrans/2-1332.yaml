- en: Intermediate Deep Learning with Transfer Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/intermediate-deep-learning-with-transfer-learning-f1aba5a814f](https://towardsdatascience.com/intermediate-deep-learning-with-transfer-learning-f1aba5a814f)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A practical guide for fine-tuning Deep Learning models for computer vision and
    natural language processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@iamleonie?source=post_page-----f1aba5a814f--------------------------------)[![Leonie
    Monigatti](../Images/4044b1685ada53a30160b03dc78f9626.png)](https://medium.com/@iamleonie?source=post_page-----f1aba5a814f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f1aba5a814f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f1aba5a814f--------------------------------)
    [Leonie Monigatti](https://medium.com/@iamleonie?source=post_page-----f1aba5a814f--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f1aba5a814f--------------------------------)
    ·11 min read·Feb 22, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/77c0a9277a5199f8f3fbdf7450e3992a.png)'
  prefs: []
  type: TYPE_IMG
- en: Image by the author
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Deep Learning is easy. You can have a neural network setup
    and training within just a few lines of code. But it can become overwhelming when
    you go from a beginner to an intermediate level. You are confronted with many
    new terms like “EfficientNet” or “DeBERTa”. What are all these terms? What do
    they have to do with neural networks?
  prefs: []
  type: TYPE_NORMAL
- en: The beginner tutorials don’t tell you that only a few people train neural networks
    from scratch in practice. This is because neural networks take a lot of time and
    computational resources to train on a sufficiently large dataset to perform well.
    Hence, re-using a pre-trained model as a starting point is a common practice.
    This practice is called Transfer Learning [2].
  prefs: []
  type: TYPE_NORMAL
- en: The beginner tutorials don’t tell you that only a few people train neural networks
    from scratch in practice.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Prerequisites
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article will guide you from a beginner to an intermediate level in the
    field of Deep Learning. Thus, it assumes you already have some basic understanding
    of Deep Learning concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Disclaimer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article is **inspired by the following four resources** on best practices
    in Deep Learning I have recently come across and summarizes their key points into
    an article format. None of the original ideas in this article are mine — instead,
    view this article as study notes.
  prefs: []
  type: TYPE_NORMAL
- en: 'V. Godbole, G. E. Dahl, J. Gilmer, C. J. Shallue and Z. Nado: [Deep Learning
    Tuning Playbook](https://github.com/google-research/tuning_playbook) [4] (Format:
    Git repository)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'S. Bhutani with H20.ai: [Best Practises for Training ML Models](https://www.youtube.com/watch?v=_mzrfMA8Qx4)
    [1] (Video format)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'P. Singer and Y. Babakhin: [Practical Tips for Deep Transfer Learning](https://drive.google.com/drive/folders/1VtJF-zPbXc-V-UDl2bDgWJp05DnKZpQH)
    presented at Kaggle Days Paris in November 2022\. [11] (Presentation format)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'D. Kłeczek with Munich NLP: [Ten Proven Techniques to Improve Your NLP Model
    Training](https://www.youtube.com/watch?v=UbL1QMwDpec) [9] (Video format)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Brief Introduction to Transfer Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transfer Learning describes the practice of re-using a pre-trained neural network
    instead of training one from scratch to save time and computational resources.
    Thus, were are transferring the pre-learned weights and knowledge from one task
    to another. The pre-trained models are also called **backbones**.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a27087d9ae9dee6d870e82b63acb2ebf.png)'
  prefs: []
  type: TYPE_IMG
- en: Transfer Learning (Image by the author)
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning is practical when your dataset is small or similar to the
    one your backbone was trained on [2]. But it never hurts to use transfer learning
    [8], even if your dataset is sufficiently large or your task is different from
    the one your backbone was trained on, because usually, the first few layers contain
    generic information.
  prefs: []
  type: TYPE_NORMAL
- en: While you can keep the transferred model weights and only retrain the classifier,
    it is common to **fine-tune** the model weights of the whole model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some popular backbones for computer vision (CV) problems currently are:'
  prefs: []
  type: TYPE_NORMAL
- en: ResNet (Residual Network) [6]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DenseNet [7]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EfficientNet [12]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Respectively, some popular backbones for natural language processing (NLP)
    problems are:'
  prefs: []
  type: TYPE_NORMAL
- en: BERT (Bidirectional Encoder Representations from Transformers) [3]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RoBERTa (Robustly Optimized BERT Pretraining Approach) [10]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DeBERTa(Decoding-enhanced BERT with disentangled attention) [5]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, Transfer Learning is only possible due to researchers sharing their
    model checkpoints for the benefit of others [2].
  prefs: []
  type: TYPE_NORMAL
- en: 'The main steps of approaching a CV or NLP problem with Deep Learning are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Building a baseline](#4e16)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Increasing complexity and improving the performance in small increments](#d76d)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Squeezing the last bits of performance](#bd54)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Step 1: Building a Baseline'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As a first step, you should set up a baseline to which you will compare any
    experiments you run in the second step. You should not spend too much time on
    this step. Just make sure you have a good enough working baseline.
  prefs: []
  type: TYPE_NORMAL
- en: Backbone
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The backbone describes the pre-trained model you are using as your starting
    point. Don’t spend too much time choosing the perfect backbone for your baseline.
    You should switch out the backbone in the second step and experiment with others
    anyways (see [Model complexity](#d117)) — so just pick one. **How about EfficientNet
    for CV and DeBERTa for (English) NLP problems** [1, 9, 11]?
  prefs: []
  type: TYPE_NORMAL
- en: Batch size
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Decide on a batch size at this stage of the model development. Don’t change
    the batch size unless necessary because it requires starting the tuning process
    all over again [4, 11].
  prefs: []
  type: TYPE_NORMAL
- en: Usually, increasing the batch size will increase the training speed. Thus, it
    is common practice to use the biggest batch size in powers of two (e.g., 16, 32,
    64, etc.) that fits in the available memory [4].
  prefs: []
  type: TYPE_NORMAL
- en: Number of training steps (epochs)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Number of epochs** — You don’t require many additional training steps for
    fine-tuning a pre-trained model. Usually, **5 epochs is a good starting point**
    for the initial baseline [11].'
  prefs: []
  type: TYPE_NORMAL
- en: '**Early stopping** — While early stopping has been popular in the past [5],
    it is not so much anymore [1, 4, 11]. Early stopping is a technique that stops
    training when the validation metric does not longer improve to avoid overfitting
    to the training data. However, this technique can cause you to overfit to and
    leak information from the validation set [1, 11].'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, early stopping always requires a validation set. Thus, you won’t
    be able to retrain the final model on the whole dataset for an extra performance
    boost [1] (see [Retraining on the full dataset](#f816)). Thus, **early stopping
    is not recommended**.
  prefs: []
  type: TYPE_NORMAL
- en: '**Best checkpoint picking** — There is no common understanding of the best
    practices regarding best checkpoint picking. Best checkpoint picking is the practice
    of training the neural network for a fixed number of runs (without early stopping)
    and then selecting the run with the best validation metric. While the [Deep Learning
    Tuning Playbook](https://github.com/google-research/tuning_playbook) [4] recommends
    using best checkpoint picking, Kaggle Grandmasters don’t recommend it because
    it requires the use of a validation set [1, 11] (see [Retraining on the full dataset](#f816))'
  prefs: []
  type: TYPE_NORMAL
- en: Optimizer and its learning rate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Optimizer** — The [Deep Learning Tuning Playbook](https://github.com/google-research/tuning_playbook)
    [4] recommends starting with either…'
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic gradient descent (SGD) or
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adam.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, Kaggle Grandmasters shared that SGD requires more tuning efforts to
    achieve similar performance as with Adam and thus recommend Adam [1]. They specifically
    recommend **AdamW as an optimizer** [1].
  prefs: []
  type: TYPE_NORMAL
- en: '**Learning rate** — While you would need to start with a larger learning rate
    when training a neural network from scratch, for fine-tuning, we can set a **smaller
    initial learning rate of, e.g., 1e-3** [11]. This is because we assume that the
    weights of the pre-trained model are already good and should not be changed too
    much too quickly [2].'
  prefs: []
  type: TYPE_NORMAL
- en: If you are unsure what learning rate to choose, you could also check the paper
    of the model architecture you are using to find out what learning rate they used
    [9].
  prefs: []
  type: TYPE_NORMAL
- en: '[**Learning rate scheduler**](/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863)
    — While it is recommended to start with a constant learning rate for the initial
    baseline when training a neural network from scratch [4], you should use a [learning
    rate scheduler](/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863)
    for your initial baseline when fine-tuning [1, 4, 11].'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly to the optimizer, people like to argue about the best [learning rate
    scheduler](/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863).
    While it was popular to reduce the learning rate when the validation metric reached
    a plateau, this approach is dependent on a validation set [1]. Kaggle Grandmasters
    **recommend the cosine annealing learning rate scheduler** [1, 11], which is shown
    below.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2bf822f9e00d4b52db4707986c46da7b.png)'
  prefs: []
  type: TYPE_IMG
- en: Cosine Decay/Annealing Learning Rate Scheduler (Image by the author via [“A
    Visual Guide to Learning Rate Schedulers in PyTorch”](https://medium.com/towards-data-science/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863))
  prefs: []
  type: TYPE_NORMAL
- en: '**For NLP,** you could keep your learning rate constant when you are not using
    many epochs to fine-tune, and your initial learning rate is already small [1].'
  prefs: []
  type: TYPE_NORMAL
- en: Bells and whistles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Every month, there is a fancy new technique you can apply to your training pipeline.
    But for the sake of the baseline, it is recommended to keep it as simple as possible
    and add fancy features later [4].
  prefs: []
  type: TYPE_NORMAL
- en: Regularizing vs. overfitting to the validation set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A few years ago, early stopping and reducing the learning rate on a plateau
    were considered regularization techniques to avoid overfitting of the model to
    the training set [8]. Today, these techniques are considered to be leaky in terms
    of **overfitting to the validation set** [1].
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, any technique dependent on a validation set prevents us from re-training
    the model on our final configuration on the entire dataset, which can help increase
    the model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Increasing Complexity and Improving the Performance in Small Increments'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You should spend most of your time in this second step. In this step, you will
    run many experiments with adjustments made to your baseline model. You can also
    add all the bells, whistles, and whatever technique is hot that month to your
    model. Just add them in small increments to be able to evaluate their impact.
  prefs: []
  type: TYPE_NORMAL
- en: For this step, it is recommended to have some sort of [experiment tracking system](https://medium.com/@iamleonie/intro-to-mlops-experiment-tracking-for-machine-learning-858e432bd133)
    in place, whether with pen and paper or an experiment tracking tool.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Theoretically, you could maximize your model’s performance by running an automated
    hyperparameter optimization algorithm over the entire search space of possible
    hyperparameters. But this is not practical. Thus, you should first define a search
    space by running a few initial experiments [4].
  prefs: []
  type: TYPE_NORMAL
- en: 'The most important hyperparameters to tune are the learning rate and the number
    of epochs. Kaggle Grandmasters recommend the following ranges as starting points
    [11]:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning rate: 1e-4 to 1e-3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Epochs: 2 to 10'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**For NLP,** you will need only a few epochs because the models are better
    pre-trained than in CV.'
  prefs: []
  type: TYPE_NORMAL
- en: Because we already have reduced ranges for the hyperparameter search spaces,
    we can use an algorithm to [automate hyperparameter tuning](https://medium.com/@iamleonie/intro-to-mlops-hyperparameter-tuning-9a938d21f894)
    [4, 8]. It is recommended to prefer random search or bayesian optimization over
    grid search [4, 8].
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Deep Learning model’s performance heavily relies on the amount of data. Thus,
    increasing the amount of data with augmented data can help improve your model’s
    performance [1, 8, 11].
  prefs: []
  type: TYPE_NORMAL
- en: 'Common data augmentation in CV are:'
  prefs: []
  type: TYPE_NORMAL
- en: Horizontal or vertical flipping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rotating
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resizing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random cropping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shifting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mixup
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cutout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cutmix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/a82ff7e165b1304c857801c948f3acfe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Data Augmentation Techniques: Mixup, Cutout, Cutmix (Image by the author)'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the implementations of cutout, mixup, and cutmix in PyTorch in
    this article:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/cutout-mixup-and-cutmix-implementing-modern-image-augmentations-in-pytorch-a9d7db3074ad?source=post_page-----f1aba5a814f--------------------------------)
    [## Cutout, Mixup, and Cutmix: Implementing Modern Image Augmentations in PyTorch'
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation techniques for Computer Vision implemented in Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/cutout-mixup-and-cutmix-implementing-modern-image-augmentations-in-pytorch-a9d7db3074ad?source=post_page-----f1aba5a814f--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '**For NLP,** data augmentations that work on CV seem to work on text data as
    well, like random cropping, resizing, or cutout [9]. But not all CV data augmentation
    techniques can be translated to text data directly and thus require their own
    augmentation techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Back translation: Translating text to another language and then back to the
    original language'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Masked Entity Language Modeling (MELM) [13]: Randomly masking a percentage
    of tokens in a sentence (similar to cutout in CV [9])'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Replacing: Replacing words in a sentence with synonyms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model complexity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The best suited backbone will be different for every problem. Thus, you should
    try a few different backbones [1, 11].
  prefs: []
  type: TYPE_NORMAL
- en: Garbage in, garbage out
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Any ML model stands or falls with the quality of data you feed it. Thus, it
    is essential to experiment with different configurations for your model’s training
    pipeline and review the training data. A good approach is to **review** which
    samples your model is predicting well and which it is performing poorly on.
  prefs: []
  type: TYPE_NORMAL
- en: If you **visualize** these samples, you will make your life much easier [9].
    This will give you a hint about possible issues in the data or training pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Squeezing the Last Bits of Performance'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you reach the end of experimentation (e.g., deadline or satisfactory performance),
    you can add some finishing touches and squeeze out the last bits of performance
    of your model.
  prefs: []
  type: TYPE_NORMAL
- en: Retraining on the full dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep Learning models are data-hungry. That’s why you can boost your model’s
    performance if you retrain your model with the final training configuration on
    the full dataset [1, 11].
  prefs: []
  type: TYPE_NORMAL
- en: Ensembles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While a few years ago, big ensembles were fashionable, today, smaller ensembles
    of up to three models are en vogue [1]. When ensembling models, you could try
    the following strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: Combine models with the same training configuration but with different seeds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combine different models with a lot of diversity (e.g., different backbones)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding more bells and whistles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you are still unsatisfied with your results, try pseudo-labeling [9] or test
    time augmentations [11] and see if you can squeeze that little bit of performance.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have just finished your “Introduction to Deep Learning” course and want
    to level up your skills, here is the high-level recipe to approach any Deep Learning
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start with a simple baseline:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Backbone:** Just one to start with — you should experiment with others anyways:
    EfficientNet for CV and DeBERTa for NLP'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch size (fixed):** Check what is the biggest batch size in powers of two
    (e.g., 16, 32, 64, etc.) to fit in the available memory, and then don’t change
    it unless really necessary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of training steps:** 5 epochs without early stopping'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimizer (fixed):** AdamW'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning rate:** 1e-3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning rate scheduler (fixed):** Cosine Annealing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Set up an [experiment tracking system](https://medium.com/@iamleonie/intro-to-mlops-experiment-tracking-for-machine-learning-858e432bd133)
    in place and get cracking:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hyperparameter tuning:** Start with tuning of the learning rate (0.0001–0.001)
    and epochs (2–10). Use random search or bayesian optimization for [automated hyperparameter
    tuning](https://medium.com/@iamleonie/intro-to-mlops-hyperparameter-tuning-9a938d21f894).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data augmentation:** The more, the merrier. Mixup, cutout, cutmix are especially
    popular because of their effectiveness.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Backbone:** Try different backbones and model complexities'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bells and whistles:** In this step, you can go wild with trying every hot
    new trend in Deep Learning and checking if it will improve your model''s performance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To squeeze out the last bit of performance, you should select up to three of
    your best training configurations, retrain a model on the entire dataset, and
    ensemble them.
  prefs: []
  type: TYPE_NORMAL
- en: Enjoyed This Story?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[*Subscribe for free*](https://medium.com/subscribe/@iamleonie) *to get notified
    when I publish a new story.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@iamleonie/subscribe?source=post_page-----f1aba5a814f--------------------------------)
    [## Get an email whenever Leonie Monigatti publishes.'
  prefs: []
  type: TYPE_NORMAL
- en: Get an email whenever Leonie Monigatti publishes. By signing up, you will create
    a Medium account if you don’t already…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@iamleonie/subscribe?source=post_page-----f1aba5a814f--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '*Find me on* [*LinkedIn*](https://www.linkedin.com/in/804250ab/),[*Twitter*](https://twitter.com/helloiamleonie)*,
    and* [*Kaggle*](https://www.kaggle.com/iamleonie)*!*'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] S. Bhutani with H20.ai (2023). [Best Practises for Training ML Models |
    @ChaiTimeDataScience #160](https://www.youtube.com/watch?v=_mzrfMA8Qx4) presented
    on YouTube in January 2023.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] CS231n Convolutional Neural Networks for Visual Recognition (2023). [Transfer
    Learning](https://cs231n.github.io/transfer-learning/) (accessed February 3rd,
    2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] J. Devlin, M. M. Chang,K. Lee & K. Toutanova (2018). Bert: Pre-training
    of deep bidirectional transformers for language understanding. *arXiv preprint
    arXiv:1810.04805*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[4,] V. Godbole, G. E. Dahl, J. Gilmer, C. J. Shallue and Z. Nado (2023). [Deep
    Learning Tuning Playbook](https://github.com/google-research/tuning_playbook)
    (Version 1.0) (accessed February 3rd, 2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] P. He, X. Liu, J. Gao, & W. Chen (2020). Deberta: Decoding-enhanced bert
    with disentangled attention. *arXiv preprint arXiv:2006.03654*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] K. He, X. Zhang, S. Ren, & J. Sun (2016). Deep residual learning for image
    recognition. In *Proceedings of the IEEE conference on computer vision and pattern
    recognition* (pp. 770–778).'
  prefs: []
  type: TYPE_NORMAL
- en: '[7] G. Huang, Z. Liu, L. Van Der Maaten & K. Q. Weinberger (2017). Densely
    connected convolutional networks. In *Proceedings of the IEEE conference on computer
    vision and pattern recognition* (pp. 4700–4708).'
  prefs: []
  type: TYPE_NORMAL
- en: '[8] A. Karpathy (2019). [A Recipe for Training Neural Networks](http://karpathy.github.io/2019/04/25/recipe/)
    (accessed February 3rd, 2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '[9] D. Kłeczek with Munich NLP (2023). [Ten Proven Techniques to Improve Your
    NLP Model Training](https://www.youtube.com/watch?v=UbL1QMwDpec) (accessed February
    9th, 2023)'
  prefs: []
  type: TYPE_NORMAL
- en: '[10] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen & V. Stoyanov (2019).
    Roberta: A robustly optimized bert pretraining approach. *arXiv preprint arXiv:1907.11692*.'
  prefs: []
  type: TYPE_NORMAL
- en: '[11] P. Singer and Y. Babakhin (2022). [Practical Tips for Deep Transfer Learning](https://drive.google.com/drive/folders/1VtJF-zPbXc-V-UDl2bDgWJp05DnKZpQH)
    presented at Kaggle Days Paris in November 2022.'
  prefs: []
  type: TYPE_NORMAL
- en: '[12] M. Tan, & Q. Le (2019). Efficientnet: Rethinking model scaling for convolutional
    neural networks. In *International conference on machine learning* (pp. 6105–6114).
    PMLR'
  prefs: []
  type: TYPE_NORMAL
- en: '[13] R. Zhou, X. Li, R. He, L. Bing, E. Cambria, L. Si, & C. Miao (2021). MELM:
    Data augmentation with masked entity language modeling for low-resource NER. *arXiv
    preprint arXiv:2108.13655*.'
  prefs: []
  type: TYPE_NORMAL
