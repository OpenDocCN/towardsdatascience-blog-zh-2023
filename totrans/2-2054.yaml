- en: 'The Power of Retrieval Augmented Generation: A Comparison between Base and
    RAG LLMs with Llama2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**检索增强生成的力量：Base LLM 与 RAG LLMs 的比较，基于 Llama2**'
- en: 原文：[https://towardsdatascience.com/the-power-of-retrieval-augmented-generation-a-comparison-between-base-and-rag-llms-with-llama2-368865762c0d](https://towardsdatascience.com/the-power-of-retrieval-augmented-generation-a-comparison-between-base-and-rag-llms-with-llama2-368865762c0d)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-power-of-retrieval-augmented-generation-a-comparison-between-base-and-rag-llms-with-llama2-368865762c0d](https://towardsdatascience.com/the-power-of-retrieval-augmented-generation-a-comparison-between-base-and-rag-llms-with-llama2-368865762c0d)
- en: A deep dive into tailoring pre-trained LLMs for custom use cases using a RAG
    approach, featuring LangChain and Hugging Face integration
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入探讨使用 RAG 方法定制预训练 LLM 以适应特定使用案例，涉及 LangChain 和 Hugging Face 集成
- en: '[](https://medium.com/@luisroque?source=post_page-----368865762c0d--------------------------------)[![Luís
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----368865762c0d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----368865762c0d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----368865762c0d--------------------------------)
    [Luís Roque](https://medium.com/@luisroque?source=post_page-----368865762c0d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@luisroque?source=post_page-----368865762c0d--------------------------------)[![Luís
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----368865762c0d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----368865762c0d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----368865762c0d--------------------------------)
    [Luís Roque](https://medium.com/@luisroque?source=post_page-----368865762c0d--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----368865762c0d--------------------------------)
    ·12 min read·Nov 29, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----368865762c0d--------------------------------)
    ·阅读时间 12 分钟·2023年11月29日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '*This post was co-authored with Rafael Guedes.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*本文由 Rafael Guedes 共同撰写。*'
- en: Introduction
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Since the release of ChatGPT in November of 2022, Large Language Models (LLMs)
    have been the hot topic in the AI community for their capabilities in understanding
    and generating human-like text, pushing the boundaries of what was previously
    possible in natural language processing (NLP).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 自 2022 年 11 月 ChatGPT 发布以来，大型语言模型（LLMs）因其理解和生成类似人类文本的能力，成为 AI 社区的热门话题，推动了自然语言处理（NLP）领域的边界。
- en: LLMs have been shown to be versatile by tackling different use cases in various
    industries since they are not limited to a specific task. They can be adapted
    to several domains, making them attractive for organizations and the research
    community. Several applications have been explored using LLMs such as content
    generation, chatbots, code generation, creative writing, virtual assistants, and
    many more.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 已被证明具有多样性，通过处理不同行业的不同使用案例，因为它们不局限于特定任务。它们可以适应多个领域，这使得它们对组织和研究社区具有吸引力。已经探索了许多使用
    LLMs 的应用程序，例如内容生成、聊天机器人、代码生成、创意写作、虚拟助手等。
- en: Another characteristic that makes LLMs so attractive is the fact that there
    are open-source options. Companies like Meta made their pre-trained LLM (Llama2
    🦙) available in repositories like Hugging Face 🤗. Are these pre-trained LLMs good
    enough for each company’s specific use case? Certainly not.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 使 LLMs 非常吸引人的另一个特征是有开源选项。像 Meta 这样的公司将其预训练的 LLM（Llama2 🦙）在 Hugging Face 🤗 等存储库中提供。这些预训练的
    LLM 对于每个公司的特定使用案例足够好吗？显然不够。
- en: 'Organizations could train an LLM from scratch with their own data. But the
    vast majority of them (almost all of them) wouldn’t have either the data or the
    computing capacity required for the task. It requires datasets with trillions
    of tokens, thousands of GPUs, and several months. Another option is to use a pre-trained
    LLM and tailor it for a specific use case. There are two main approaches to follow:
    fine-tuning and **RAGs (Retrieval Augmented Generation)**.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 组织可以用自己的数据从头开始训练 LLM。但绝大多数组织（几乎所有组织）既没有所需的数据，也没有完成任务所需的计算能力。这需要拥有数万亿个标记的数据集，成千上万的
    GPU，以及几个月的时间。另一个选择是使用预训练的 LLM，并将其调整为特定的使用案例。有两种主要的方法：微调和**RAGs（检索增强生成）**。
- en: In this article, we will compare the performance of an isolated pre-trained
    Llama2 with a pre-trained LLama2 integrated in a RAG system to answer questions
    about the latest news regarding OpenAI. We will start by explaining how RAGs work
    and the architecture of their sub-modules (the retriever and the generator). We
    finish with a step-by-step implementation of how we can build a RAG system for
    any use case using LangChain **🦜️** and Hugging Face.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将比较单独预训练的Llama2与集成在RAG系统中的预训练LLama2在回答关于OpenAI最新新闻的问题上的表现。我们将从解释RAG的工作原理及其子模块（检索器和生成器）的架构开始。最后，我们将逐步实现如何使用LangChain
    **🦜️** 和Hugging Face构建一个适用于任何用例的RAG系统。
- en: '![](../Images/684645ae6f864ac647968b569236e55b.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/684645ae6f864ac647968b569236e55b.png)'
- en: 'Figure 1: Llamas are getting more powerful with the RAG approach (image by
    author)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：通过RAG方法，Llamas变得越来越强大（图像来源：作者）
- en: As always, the code is available on our [Github](https://github.com/zaai-ai/large-language-models).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，代码可以在我们的[Github](https://github.com/zaai-ai/large-language-models)上找到。
- en: What is Retrieval Augmented Generation (RAG)?
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是检索增强生成（RAG）？
- en: Retrieval Augmented Generation (RAG) is a technique that combines a retriever
    (a non-parametric memory like vector databases or feature store) and a generator
    (a parametric memory like a pre-trained *seq2seq* transformer). They are used
    to improve the prediction quality of an LLM [1]. It uses the retriever during
    inference time to build a richer prompt by adding context/knowledge based on the
    most relevant documents for the user query.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）是一种结合了检索器（如向量数据库或特征存储的非参数记忆）和生成器（如预训练的*seq2seq*变换器的参数记忆）技术的方法。它们用于提高LLM
    [1] 的预测质量。它在推理时使用检索器，通过添加基于最相关文档的上下文/知识来构建更丰富的提示，以响应用户查询。
- en: 'The advantages of this kind of architecture over the traditional LLM are:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构相对于传统LLM的优势是：
- en: We can easily update its knowledge by replacing or adding more documents/information
    to the non-parametric memory. Thus, it does not require retraining the model.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过替换或添加更多文档/信息到非参数记忆中，轻松更新其知识。因此，它不需要重新训练模型。
- en: It provides explainability over predictions because it allows the user to check
    which documents were retrieved to provide context, something we cannot get from
    traditional LLMs.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它提供了对预测的可解释性，因为它允许用户检查哪些文档被检索以提供上下文，而这是我们从传统LLM中无法获得的。
- en: It reduces the well-known problem of *‘hallucinations’* by providing more accurate
    and up-to-date information through the documents provided by the retriever.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它通过提供更准确和最新的信息，减少了*“幻觉”*的著名问题，这些信息通过检索器提供的文档获取。
- en: '![](../Images/547b6b71f7ad85370f0aaab3c1299483.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/547b6b71f7ad85370f0aaab3c1299483.png)'
- en: 'Figure 2: Schematic view of how Retrieval Augmented Generation (RAG) can be
    set up (image by author)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：检索增强生成（RAG）设置的示意图（图像来源：作者）
- en: Retriever — what is it and how it works?
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检索器——它是什么以及如何工作？
- en: Retrievers were developed to solve the problem of Question-Answering (QA), where
    we expect a system to answer questions like *“What is Retrieval Augmented Generation?”.*
    It does it by accessing a database of documents that contain information about
    the topic.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 检索器的开发旨在解决问答（QA）问题，我们期望系统能够回答类似*“什么是检索增强生成？”*的问题。它通过访问包含有关主题信息的文档数据库来实现。
- en: The database is populated by splitting our documents into *passages* of equal
    lengths, where each passage is represented as a sequence of tokens. Given a question,
    the system needs to span the database to find *‘the passage’* that can better
    answer the question.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库通过将我们的文档拆分成等长的*段落*来填充，每个段落被表示为一系列标记。给定一个问题，系统需要遍历数据库，以找到可以更好回答问题的*“段落”*。
- en: For these systems to work in several domains, their databases need to be populated
    with millions or billions of documents. Therefore, to be able to span the database
    searching for the right *passage,* the **retriever** needs to be very efficient
    in selecting a set of candidate *passages.*
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这些系统在多个领域中有效工作，它们的数据库需要填充数百万或数十亿的文档。因此，为了能够遍历数据库寻找合适的*段落*，**检索器**需要在选择一组候选*段落*时非常高效。
- en: '**Dense Passage Retriever (DPR)** [2] is the retriever used by the authors
    in [1]. Its goal is to index millions of passages into a low-dimensional and continuous
    space to efficiently retrieve the top *k* passages that are the most relevant
    for a specific question.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**密集段落检索器 (DPR)** [2] 是作者在 [1] 中使用的检索器。它的目标是将数百万个段落索引到一个低维的连续空间，以高效地检索与特定问题最相关的前
    *k* 个段落。'
- en: DPR uses two **Dense Encoders:**
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: DPR 使用两个 **密集编码器**：
- en: The **Passage Encoder** converts each passage into a d-dimensional vector and
    indexes them using **FAISS** [3]. FAISS is an open-source library for similarity
    search of dense vectors which can be applied to billions of vectors.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**段落编码器**将每个段落转换为一个 d 维向量，并使用 **FAISS** [3] 对它们进行索引。FAISS 是一个用于密集向量相似性搜索的开源库，可以应用于数十亿个向量。'
- en: The **Question Encoder** converts the input question to a d-dimensional vector
    and then uses **FAISS** to retrieve the **k** passages that have the closest vector
    to the question vector. The similarity between vectors can be computed by using
    the dot product between them.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**问题编码器**将输入问题转换为一个 d 维向量，然后使用 **FAISS** 检索与问题向量最接近的 **k** 个段落。向量之间的相似性可以通过它们之间的点积来计算。'
- en: The architecture for the encoder used by DPR is a BERT [4] network which converts
    the input into a high dimensional vector. However, we can use any architecture
    as long as it fits our use case.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DPR 使用的编码器架构是一个 BERT [4] 网络，它将输入转换为高维向量。然而，只要符合我们的用例，我们可以使用任何架构。
- en: '![](../Images/a52940dd39b60aefb3c1047fbbc1dd37.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a52940dd39b60aefb3c1047fbbc1dd37.png)'
- en: 'Figure 3: Overview of the RAG process with a pre-trained retriever that combines
    a query encoder, document index, and a pre-trained generator (seq2seq model) to
    predict the output in a free-text form ([source](https://arxiv.org/pdf/2005.11401.pdf)).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：RAG 过程的概述，使用一个预训练的检索器，该检索器结合了查询编码器、文档索引和一个预训练生成器（seq2seq 模型）以预测自由文本形式的输出
    ([source](https://arxiv.org/pdf/2005.11401.pdf))。
- en: Generator — what is it and how does it work?
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成器 — 它是什么，如何工作？
- en: The generator is an LLM responsible for generating text given a certain input,
    well known as a prompt.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器是一个 LLM，负责根据特定输入生成文本，通常被称为提示。
- en: 'LLMs are transformer models which are composed mainly of 2 types of layers
    [5]:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 是主要由两种层组成的变换器模型 [5]：
- en: A **fully connected feed-forward network (FFN)** maps one embedding vector to
    a new embedding vector through linear and non-linear transformations.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**全连接前馈网络 (FFN)** 通过线性和非线性变换将一个嵌入向量映射到一个新的嵌入向量。'
- en: The **attention layer** aims to select which parts of the input embedding are
    more useful for the task at hand, producing a new embedding vector.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**注意力层**旨在选择哪些输入嵌入部分对当前任务更有用，产生一个新的嵌入向量。'
- en: 'BART [6] was the LLM chosen by the authors in [1] for the Generator, which
    is a sequence-to-sequence model with the following architecture [7]:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: BART [6] 是作者在 [1] 中为生成器选择的 LLM，它是一个序列到序列模型，具有以下架构 [7]：
- en: The **encoder** receives the input embedding and produces a 512-dimensional
    vector as output for the decoder through its six layers with two sub-layers (multi-head
    self-attention mechanism and FFN).
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码器**接收输入嵌入，并通过其六层（包括两个子层：多头自注意力机制和 FFN）生成一个 512 维的向量作为解码器的输出。'
- en: The **decoder** follows the same logic as the encoder with six layers and two
    sub-layers for the previously generated outputs. It has an additional 3rd sub-layer,
    which performs multi-head attention over the output of the encoder.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解码器**遵循与编码器相同的逻辑，具有六层和两个子层，用于之前生成的输出。它还有一个额外的第三个子层，执行对编码器输出的多头注意力机制。'
- en: The decoder output is then passed to a linear layer followed by a softmax layer
    that will predict the likelihood of the next word.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器输出接着传递到一个线性层，然后是一个 softmax 层，该层将预测下一个词的可能性。
- en: As mentioned in the previous section, BART is not required to be used as the
    generator. With the advancements in this field, mainly since November of 2022
    with the release of chatGPT, we can use any architecture that fits our needs.
    For example, one can use open-source approaches like [Llama](https://medium.com/towards-data-science/leveraging-llama-2-features-in-real-world-applications-building-scalable-chatbots-with-fastapi-406f1cbeb935)2
    or [Falcon](https://medium.com/towards-data-science/harnessing-the-falcon-40b-model-the-most-powerful-open-source-llm-f70010bc8a10).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所述，BART不需要作为生成器使用。随着该领域的发展，特别是自2022年11月chatGPT发布以来，我们可以使用任何符合我们需求的架构。例如，可以使用开源方法如[Llama](https://medium.com/towards-data-science/leveraging-llama-2-features-in-real-world-applications-building-scalable-chatbots-with-fastapi-406f1cbeb935)2或[Falcon](https://medium.com/towards-data-science/harnessing-the-falcon-40b-model-the-most-powerful-open-source-llm-f70010bc8a10)。
- en: '![](../Images/f3730669abcb2a77ffae2369dfc0fcba.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3730669abcb2a77ffae2369dfc0fcba.png)'
- en: 'Figure 4: The general architecture of a transformer. It only differs from BART’s
    architecture on the activation functions, which are GeLUs rather than ReLUs ([source](https://arxiv.org/pdf/1706.03762.pdf)).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：变压器的一般架构。它只在激活函数上与BART的架构不同，激活函数是GeLUs而不是ReLUs ([source](https://arxiv.org/pdf/1706.03762.pdf))。
- en: How to implement a RAG using LangChain 🦜️ and HuggingFace 🤗?
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用LangChain 🦜️和HuggingFace 🤗实现RAG？
- en: This section describes how you can create your RAG using LangChain. LangChain
    is a framework to easily develop applications powered by LLMs, while HuggingFace
    is a platform that provides open-source LLMs and datasets for research and commercial
    usage.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了如何使用LangChain创建您的RAG。LangChain是一个框架，用于轻松开发由LLMs驱动的应用程序，而HuggingFace是一个提供开源LLMs和数据集用于研究和商业用途的平台。
- en: In our case, and as stated in the introduction, we created an RAG where the
    generator is a Llama2 model to compare the quality of its output with a base Llama2\.
    We will make Llama2 answer the question *“What happened to the CEO of OpenAI?”.*
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，正如介绍中所述，我们创建了一个RAG，其中生成器是一个Llama2模型，以便将其输出的质量与基础Llama2进行比较。我们将使Llama2回答问题*“OpenAI的CEO发生了什么？”*。
- en: The process starts with loading a dataset of news from HuggingFace ([cnn_dailymail](https://huggingface.co/datasets/cnn_dailymail)
    — apache 2.0 license) and supplementing it with more recent news about OpenAI,
    based on Luis’s latest posts on X/Twitter regarding the subject, including the
    resignation of its CEO. We then preprocess it by creating a list of ***documents***
    (the expected format from LangChain) to fill our vector database.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程从加载HuggingFace的数据集新闻（[cnn_dailymail](https://huggingface.co/datasets/cnn_dailymail)
    — apache 2.0许可证）开始，并通过Luis最近在X/Twitter上关于该主题的帖子，包括CEO辞职，补充了有关OpenAI的最新新闻。然后，我们通过创建一个***文档***列表（LangChain期望的格式）来预处理它，以填充我们的向量数据库。
- en: '[PRE0]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next, we are ready to create the 2 modules for our RAG.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们准备为我们的RAG创建这两个模块。
- en: Retriever
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检索器
- en: 'In the retriever, we have two sub-modules: the encoder and the retriever.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在检索器中，我们有两个子模块：编码器和检索器。
- en: The **encoder** converts the passages into a d-dimensional embedding vector.
    For that, we import `HuggingFaceEmbeddings` from `langchain.embeddings` and we
    select the model we want to use to create the embeddings.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**编码器**将段落转换为d维的嵌入向量。为此，我们从`langchain.embeddings`导入`HuggingFaceEmbeddings`，并选择我们想要使用的模型来创建嵌入。'
- en: In our case, we have chosen `sentence-transformers/all-MiniLM-l6-v2` because
    it creates 384-dimensional vectors with good quality to calculate the similarity
    between them. It is memory efficient and fast. You can check more details about
    this and other models [here](https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models/).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们选择了`sentence-transformers/all-MiniLM-l6-v2`，因为它创建了384维的向量，具有良好的质量用于计算它们之间的相似度。它在内存使用上高效且快速。您可以在[这里](https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models/)查看有关此模型及其他模型的更多详细信息。
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The **retriever** splits the documents into passages of a certain length using
    `CharacterTextSplitter` from `langchain.text_splitter` .
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**检索器**使用`langchain.text_splitter`中的`CharacterTextSplitter`将文档拆分为一定长度的段落。'
- en: In our case, we chose a length of 1000\. We started with 100, as stated in the
    paper [1], but through some preliminary experiments, we found out that 1000 gives
    better results in our use case.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们选择了1000的长度。我们开始时选择了100，如文献[1]中所述，但通过一些初步实验，我们发现1000在我们的使用案例中能取得更好的结果。
- en: We then use the **encoder** to convert the passages into embeddings. Finally,
    we can store them in a vector store such as `FAISS` from `langchain.vectorstores`
    . From those, we can later retrieve the top *k* documents more similar to the
    question.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用**编码器**将段落转换为嵌入。最后，我们可以将它们存储在如`FAISS`的向量存储中。从这些存储中，我们可以稍后检索出与问题最相似的前*k*个文档。
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Generator
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成器
- en: As mentioned previously, the LLM for text generation is Llama2\. It uses *quantization,*
    a technique to reduce the precision of how the weights are represented to minimize
    the memory required to use the model. Notice that since no [free lunches exist](https://en.wikipedia.org/wiki/No_free_lunch_theorem),
    we are trading off memory size with accuracy.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，生成文本的LLM是Llama2。它使用*量化*技术，这是一种降低权重表示精度的技术，以最小化使用模型所需的内存。请注意，由于不存在[免费的午餐](https://en.wikipedia.org/wiki/No_free_lunch_theorem)，我们在内存大小和准确性之间进行了权衡。
- en: This process brings advantages like the possibility of running LLMs with less
    resources but, also, disadvantages such as a reduction in performance due to *quantization.*
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程带来了如运行LLM时资源需求减少等优点，但也有如*量化*导致性能降低等缺点。
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Once we have our LLM, it is time to set the **Prompt Template.** Prompt Engineering
    is relevant when interacting with LLMs since it can significantly impact its output.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们拥有了LLM，就该设置**提示模板**了。提示工程在与LLM交互时是相关的，因为它可以显著影响输出。
- en: When we find a prompt that produces the desired output for the use case, we
    can create a template. LangChain offers a simple solution to create a **Prompt
    Template**. We start by defining the structure of the prompt and adding the dynamic
    variables in a dictionary format based on the user’s query. In our case, the `{context}`
    is given by the retriever and the user’s `{question}`.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们找到一个能为使用案例生成所需输出的提示时，我们可以创建一个模板。LangChain提供了一个简单的解决方案来创建**提示模板**。我们首先定义提示的结构，并根据用户的查询以字典格式添加动态变量。在我们的案例中，`{context}`由检索器提供，用户的`{question}`则由用户提供。
- en: '[PRE4]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now that we have defined our LLM and Prompt Template, we create an `LLMChain`
    from `langchain.chains` . It allows us to combine multiple components to create
    a coherent application.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了我们的LLM和提示模板，我们从`langchain.chains`中创建一个`LLMChain`。它允许我们结合多个组件以创建一个连贯的应用程序。
- en: In our case, we can create a chain that takes the user’s question and the context,
    formats it with a Prompt Template, and then passes the formatted response to the
    LLM to get a response.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们可以创建一个链条，获取用户的问题和上下文，用提示模板格式化它，然后将格式化的响应传递给LLM以获得答案。
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We are now ready to use our model and compare it with a base Llama2 model.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备使用我们的模型，并将其与基础Llama2模型进行比较。
- en: 'Results: Base Llama2 vs RAG Llama2'
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果：基础Llama2 vs RAG Llama2
- en: “What happened to the CEO of OpenAI?”
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: “OpenAI的CEO发生了什么事？”
- en: Considering the question, *“What happened to the CEO of OpenAI?”* Our goal is
    to show that without fine-tuning an LLM and using an RAG system, we can easily
    improve the prompt with relevant information which will make the LLM be more accurate
    and more updated in its answer. On the contrary, we expect the Base Llama2 to
    provide an accurate answer but is outdated since its training data did not contain
    information from last week.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到问题，*“OpenAI的CEO发生了什么事？”* 我们的目标是展示在不对LLM进行微调并使用RAG系统的情况下，我们可以通过相关信息轻松改进提示，使LLM在回答时更加准确和更新。相反，我们期望基础的Llama2提供准确的答案，但由于其训练数据未包含上周的信息，因此可能过时。
- en: '**Base Llama2:**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**基础 Llama2:**'
- en: OpenAI is a non-profit artificial intelligence research organization based in
    San Francisco, California, that focuses on developing friendly AI and ensuring
    its safe use. The CEO of OpenAI is Sam Altman, who has been at the helm since
    2010 when he co-founded the company with Elon Musk, Greg Brockman, Ilya Sutskever,
    and Vince Conitzer. In terms of recent news, there haven’t been any major developments
    regarding the CEO or OpenAI specifically, but it is known that OpenAI has recently
    expanded its team by hiring an additional 15 employees to work on various projects
    related to AI, including the development of new models and tools for machine learning.
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: OpenAI是一个非营利的人工智能研究组织，总部位于加利福尼亚州旧金山，专注于开发友好的AI并确保其安全使用。OpenAI的CEO是Sam Altman，自2010年与Elon
    Musk、Greg Brockman、Ilya Sutskever和Vince Conitzer共同创立公司以来，他一直担任此职务。就最近的新闻而言，关于CEO或OpenAI本身没有重大进展，但已知OpenAI最近通过招聘额外的15名员工扩大了团队，以从事与AI相关的各种项目，包括新模型和机器学习工具的开发。
- en: '**RAG Llama2:**'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**RAG Llama2:**'
- en: Sam Altman stepped down from his role as CEO of OpenAI on November 20th, 2023\.
    He was replaced by Mira Murati who assumed the position of interim CEO. However,
    Sam Altman returned to the company a few days later as CEO with a new initial
    board consisting of Bret Taylor (Chair), Larry Summers and Adam D’Angelo.
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Sam Altman 于2023年11月20日辞去了 OpenAI 首席执行官职务。他被 Mira Murati 取代，后者担任了临时首席执行官。然而，Sam
    Altman 在几天后以首席执行官身份回到公司，新的初始董事会成员包括 Bret Taylor（主席）、Larry Summers 和 Adam D’Angelo。
- en: As we can see from the examples above, the RAG Llama managed to provide an answer
    with updated information without any additional fine-tuning process which would
    be expensive and time-consuming.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们从上述示例中看到的，RAG Llama 能够提供带有更新信息的答案，而无需任何额外的微调过程，这种过程既昂贵又耗时。
- en: Conclusion
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: RAGs opened the possibility of deploying LLMs faster and in a more affordable
    way for organizations than fine-tuning LLMs for every single use case.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: RAGs 为组织提供了比为每一个使用案例微调 LLMs 更快且更实惠的部署 LLMs 的可能性。
- en: As we saw in our use case, adding just twelve documents about the scandal regarding
    OpenAI and its CEO from last week to the set of 10k news that we fetched from
    HuggingFace was enough. Our retriever was able to create enough context for our
    generator to produce a more accurate and updated answer about the topic.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在使用案例中看到的，将关于 OpenAI 及其首席执行官上周丑闻的十二篇文档添加到我们从 HuggingFace 获取的 10k 新闻集合中就足够了。我们的检索器能够为我们的生成器创建足够的上下文，从而生成关于该主题的更准确且更新的答案。
- en: When it comes to accessing external information RAGs are a very good option
    because they augment LLMs capabilities by retrieving relevant information from
    knowledge sources before generating a response. Nevertheless, when it comes to
    adjusting the LLM behavior to tailor its responses for a specific writing style
    with uncommon words or expressions, a combination of both might be more suitable.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在访问外部信息方面，RAGs 是一个很好的选择，因为它们通过在生成响应之前从知识来源中检索相关信息来增强 LLMs 的能力。然而，当涉及到调整 LLM
    行为以适应特定的写作风格时，使用不常见的单词或表达式，结合使用可能更为合适。
- en: About me
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于我
- en: Serial entrepreneur and leader in the AI space. I develop AI products for businesses
    and invest in AI-focused startups.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 连续创业者和 AI 领域的领导者。我开发 AI 产品以服务于企业，并投资于以 AI 为重点的初创公司。
- en: '[Founder @ ZAAI](http://zaai.ai) | [LinkedIn](https://www.linkedin.com/in/luisbrasroque/)
    | [X/Twitter](https://x.com/luisbrasroque)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[创始人 @ ZAAI](http://zaai.ai) | [LinkedIn](https://www.linkedin.com/in/luisbrasroque/)
    | [X/Twitter](https://x.com/luisbrasroque)'
- en: References
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
    Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel,
    Sebastian Riedel, Douwe Kiela. Retrieval-Augmented Generation for Knowledge-Intensive
    NLP Tasks. arXiv:2005.11401, 2021'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
    Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel,
    Sebastian Riedel, Douwe Kiela. 适用于知识密集型 NLP 任务的检索增强生成。arXiv:2005.11401, 2021'
- en: '[2] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi
    Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering.arXiv:2004.04906,
    2020'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi
    Chen, 和 Wen-tau Yih. 用于开放域问答的密集通道检索。arXiv:2004.04906, 2020'
- en: '[3] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity
    search with GPUs. arXiv:1702.08734, 2017'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Jeff Johnson, Matthijs Douze, 和 Hervé Jégou. 基于 GPU 的亿规模相似性搜索。arXiv:1702.08734,
    2017'
- en: '[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019\.
    BERT: Pre-training of deep bidirectional transformers for language understanding.
    arXiv:1810.04805, 2019'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova. 2019\.
    BERT: 深度双向变换器的预训练用于语言理解。arXiv:1810.04805, 2019'
- en: '[5] Michael R. Douglas. Large Language Models. arXiv:2307.05782, 2023.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Michael R. Douglas. 大型语言模型。arXiv:2307.05782, 2023.'
- en: '[6] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman
    Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer. BART: Denoising Sequence-to-Sequence
    Pre-training for Natural Language Generation, Translation, and Comprehension.
    arXiv:1910.13461, 2019'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman
    Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer. BART: 用于自然语言生成、翻译和理解的去噪序列到序列预训练。arXiv:1910.13461,
    2019'
- en: '[7] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. Attention Is All You Need. arXiv:1706.03762,
    2017.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. 注意力机制是唯一需要的。arXiv:1706.03762,
    2017.'
