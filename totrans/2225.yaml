- en: Unlocking the Power of Text Data with LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/unlocking-the-power-of-text-data-with-llms-3ddcd063274a](https://towardsdatascience.com/unlocking-the-power-of-text-data-with-llms-3ddcd063274a)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: DATA SCIENCE LAB
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Learn how to handle text data with LLMs: a step-by-step guide for newbies'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@sofia-rosa?source=post_page-----3ddcd063274a--------------------------------)[![Sofia
    Rosa](../Images/fe74364da94c392f0eb99f7d528dba66.png)](https://medium.com/@sofia-rosa?source=post_page-----3ddcd063274a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3ddcd063274a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3ddcd063274a--------------------------------)
    [Sofia Rosa](https://medium.com/@sofia-rosa?source=post_page-----3ddcd063274a--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3ddcd063274a--------------------------------)
    ¬∑11 min read¬∑Oct 23, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/68220afffe74dde11a7fa11e5b938296.png)'
  prefs: []
  type: TYPE_IMG
- en: Image generated by the author using Midjourney
  prefs: []
  type: TYPE_NORMAL
- en: Customer reviews, employee surveys, and social media posts can be incredibly
    powerful in **revealing people‚Äôs attitudes** toward a specific product or service.
    However, most data analysts do very little with this type of data. *Why, you ask?*
    Generating insights from text data is **no easy task** and can leave even the
    most experienced data analysts scratching their heads for days.
  prefs: []
  type: TYPE_NORMAL
- en: This is where Large Language Models (LLMs) come to the rescue. They can help
    carry out tasks such as translation, summarization, sentiment analysis, and much
    more. **But what is an LLM, exactly?** To simplify things, you can think of an
    LLM as a *parrot*. Just like a parrot repeats what it hears at home, an LLM imitates
    human language. A key difference is that LLMs have been trained on a huge volume
    of data ‚Äî far beyond what a parrot would learn in its cage! This is why LLMs have
    the ability to generate coherent and contextually relevant text without the occasional
    nonsense of a parrot. ü¶ú
  prefs: []
  type: TYPE_NORMAL
- en: In this article, we‚Äôll explore how LLMs work and how they make it **easier than
    ever** for data analysts to extract insights from text data. There are multiple
    LLMs now available via APIs, each with different capabilities and price points.
    We‚Äôll be using GPT-3 via OpenAI API. At the time of writing, OpenAI charges for
    API usage based on the number of requests made and the number of tokens generated.
    The total cost for this tutorial amounted to $0.2.
  prefs: []
  type: TYPE_NORMAL
- en: Time to dig in!
  prefs: []
  type: TYPE_NORMAL
- en: Table of Contents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '‚ñπ [Step 1: Downloading the Data](#9da9)'
  prefs: []
  type: TYPE_NORMAL
- en: '‚ñπ [Step 2: Reading the Data](#4faf)'
  prefs: []
  type: TYPE_NORMAL
- en: '‚ñπ [Step 3: Data Pre-Processing](#6da6)'
  prefs: []
  type: TYPE_NORMAL
- en: '‚ñπ [Step 3a: Dealing with NaN Values](#185d)'
  prefs: []
  type: TYPE_NORMAL
- en: '‚ñπ [Step 3b: Transforming Text for GPT-3](#3fc1)'
  prefs: []
  type: TYPE_NORMAL
- en: '‚ñπ [Step 3c: Counting Tokens](#dcd3)'
  prefs: []
  type: TYPE_NORMAL
- en: '‚ñπ [Step 4: Setting Up an OpenAI Account](#078a)'
  prefs: []
  type: TYPE_NORMAL
- en: '‚ñπ [Step 5: Working with GPT-3](#d5e3)'
  prefs: []
  type: TYPE_NORMAL
- en: '‚ñπ [Step 6: Summarizing the Results](#7746)'
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To follow along in this tutorial, you will need to have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Working knowledge of Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 3 environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI API key (*see step 4*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 1: Downloading the Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dataset we‚Äôll use is an industry-wide survey conducted by [Kaggle](https://www.kaggle.com/datasets/kaggle/kaggle-survey-2017)
    in 2017 aimed at uncovering new trends in machine learning and data science. For
    this tutorial, we‚Äôll only be using the **freeformResponses** csv file, which contains
    open-ended answers to Kaggle‚Äôs questions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/41572fcfc0635ecc0dcf149d13da712e.png)'
  prefs: []
  type: TYPE_IMG
- en: Snippet of the freeformResponses csv file
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Reading the Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next, we‚Äôll read the csv file into a dataframe and focus on the column ‚Äú**PersonalProjectsChallengeFreeForm**‚Äù.
    This column contains challenges people face when using public datasets for their
    personal projects. Kaggle, as a platform for data science and machine learning,
    can use these insights to improve its services (e.g., by developing relevant content,
    tutorials, and resources that specifically address these challenges).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/67c1d6d1a364e3c77d43d5429f108e74.png)'
  prefs: []
  type: TYPE_IMG
- en: Output
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Data Pre-Processing'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data pre-processing involves a series of steps to clean and prepare the data
    for analysis. GPT-3 can handle relatively clean and structured text data without
    the need for extensive pre-processing. However, for complex or non-standard data,
    some extra pre-processing may be necessary to ensure the best results when leveraging
    GPT-3\. This is something to keep in mind if your text contains multiple languages,
    spelling errors, or domain-specific terms.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3a: Dealing with NaN Values**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We‚Äôll start by dealing with NaN (Not A Number) values. NaN values represent
    missing or undefined values with very distinct properties, making it important
    to detect them early on using the `isna()` function. Once identified, we can take
    appropriate measures to handle them effectively.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/875549df74f51394a4397baefe276963.png)'
  prefs: []
  type: TYPE_IMG
- en: Output
  prefs: []
  type: TYPE_NORMAL
- en: There are 13,214 NaN values (80% of all responses!), meaning that these people
    did not provide an answer to the question. The simplest approach is to remove
    all the entries that contain NaN values using the `dropna()` function. However,
    depending on your specific use case, you might prefer to handle NaN values differently,
    such as by replacing them with specific values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/0a874a59394c34dcfad442f6f3ce3cc6.png)'
  prefs: []
  type: TYPE_IMG
- en: Output
  prefs: []
  type: TYPE_NORMAL
- en: For demo purposes, we‚Äôll work with only the first 500 (non-null) responses from
    the survey.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Step 3b: Transforming Text for GPT-3**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we‚Äôll transform the text data into a format suitable for GPT-3\. We‚Äôll
    extract all the values from the ‚Äú**PersonalProjectsChallengeFreeForm**‚Äù column
    and store them in the ‚Äú**challenges**‚Äù list. This transformation begins with the
    use of the `squeeze()` function, which converts the dataframe into a pandas series.
    Subsequently, the `tolist()` function converts this series into a list.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a171aaf6a16d20293a9be13953e7a3d2.png)'
  prefs: []
  type: TYPE_IMG
- en: Output
  prefs: []
  type: TYPE_NORMAL
- en: In this example, ‚Äú**challenges**‚Äù is a list where each element represents a
    response from the original survey. We‚Äôll provide this text as input to GPT-3.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3c: Counting Tokens'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our text is almost ready for GPT-3\. Before we proceed, it‚Äôs important that
    we understand how GPT-3 works with text. Initially, it performs **tokenization**,
    which involves splitting the text into smaller units known as *tokens*. Tokens
    are units of text, such as sentences, words, numbers, or even punctuation marks.
    For example, the phrase ‚Äú**hello friend!**‚Äù can be split into three tokens: ‚Äú**hello**‚Äù,
    ‚Äú **friend**‚Äù and ‚Äú**!**‚Äù.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5d87ea48d166eb2b3e14498d29a7bc3c.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of tokenization
  prefs: []
  type: TYPE_NORMAL
- en: 'After tokenization, GPT-3 proceeds to **encoding**, which means it converts
    these tokens into token numbers. In our example, the three tokens ‚Äúhello‚Äù, ‚Äú friend‚Äù
    and ‚Äú!‚Äù can be converted into three token numbers: ‚Äú**15339**‚Äù, ‚Äú **4333**‚Äù and
    ‚Äú**0**‚Äù.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/41e738312bfeae289cab399a8f7ded85.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of encoding
  prefs: []
  type: TYPE_NORMAL
- en: By determining the number of tokens in our text, we‚Äôll know whether the text
    is too long for the model to process as well as how much an OpenAI API call will
    cost (as API calls are billed based on the number of tokens sent in your input
    plus the number of tokens that GPT returns in the output).
  prefs: []
  type: TYPE_NORMAL
- en: To do this, we‚Äôll install a library called `tiktoken` and import the necessary
    module `encoding_for_model`. Since different LLMs use different methods for encoding
    text, we‚Äôll need to specify the model we‚Äôll be using, which is ‚Äú**gpt-3.5-turbo-16k**‚Äù.
    For each sentence, we‚Äôll then tokenize and encode the text.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/2b234c7df476c123b2c699fe72675fa6.png)'
  prefs: []
  type: TYPE_IMG
- en: Output
  prefs: []
  type: TYPE_NORMAL
- en: The last step is to count the tokens, which can be accomplished by determining
    the length of the list ‚Äú**num_tokens**‚Äù.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/33a8968b910758874a33ed159bbed6ea.png)'
  prefs: []
  type: TYPE_IMG
- en: Output
  prefs: []
  type: TYPE_NORMAL
- en: To estimate the total cost based on our input, we can refer to [the pricing
    documentation](https://openai.com/pricing). In our case, 4629 tokens would translate
    to a cost of $0.01.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Setting Up an OpenAI Account'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our text is finally ready for GPT-3 (we‚Äôre getting closer to the good stuff!).
    To work with GPT-3, we‚Äôll be using the OpenAI API. Make sure that you have an
    OpenAI account set up to access the OpenAI API. If you don‚Äôt already have an account,
    follow the steps below to create one.
  prefs: []
  type: TYPE_NORMAL
- en: To kick things off, head to the [OpenAI](http://platform.openai.com) website
    and click on the ‚Äú**Sign Up**‚Äù button in the top right corner of the page. Fill
    in the form with your email address, create a password, and provide any other
    necessary info. Then, hit the ‚Äú**Create Account**‚Äù button. Keep an eye on your
    inbox as you‚Äôll receive a confirmation email. Click the link in the email to verify
    your account. Once that‚Äôs done, you‚Äôre all set to log in.
  prefs: []
  type: TYPE_NORMAL
- en: With your account created, the next step is funding it. Remember, as you use
    the API, you‚Äôll be billed for your usage. Simply go to ‚Äú**Manage Account**‚Äù and
    find the ‚Äú**Billing**‚Äù tab. There, you can add your payment card details and specify
    the initial amount you want to put in your account.
  prefs: []
  type: TYPE_NORMAL
- en: The final important step is to generate your API Key, which serves as a private
    access key to the API. You can create it in the ‚Äú**API Keys**‚Äù tab. Keep this
    key safe because it can‚Äôt be recovered if lost. However, if it slips through the
    cracks, you do have the option to create a new one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 5: Working with GPT-3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have access to GPT-3 through the OpenAI API, we can send a request
    containing the input and API key. In return, we‚Äôll get a response containing the
    GPT-3 output.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/82f40db52b0c5e1a5f1156a2861255ee.png)'
  prefs: []
  type: TYPE_IMG
- en: Using GPT-3 via the OpenAI API
  prefs: []
  type: TYPE_NORMAL
- en: First, we‚Äôll install a library called `openai`. Then, we‚Äôll set up the API key
    to authenticate our requests.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We‚Äôll send our text to GPT-3 and ask it to summarise the main topics, which
    are then stored in the ‚Äú**response**‚Äù variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'üí° **Note**: *This code is a simplified example, and you can adapt it for various
    tasks by adjusting the user message and system message according to your specific
    needs.*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let‚Äôs go through the code step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: '`response = ai.ChatCompletion.create(`: This line initiates a request to GPT-3
    and assigns the response to the variable ‚Äú**response**‚Äù.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model = ''gpt-3.5-turbo-16k''`: This parameter specifies which GPT-3 model
    to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`messages = [ ... ]`: This section defines a list of messages for which GPT-3
    will create a response. Each message has a role (e.g., system or user) and content.
    The system message helps set the *behavior* of GPT-3\. For example, we can say:
    ‚ÄúYou‚Äôre a helpful assistant. Your task is to analyze a set of reviews‚Äù. The user
    message, on the other hand, provides *instructions* for the task. For example,
    we can say: ‚ÄúBelow is a set of reviews. Please, identify the main topics mentioned
    in these comments‚Äù.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`temperature = 0`: This parameter influences the randomness of the responses.
    You can think of it as a way to control how creative and unpredictable the responses
    are. Setting it to 0 means that you‚Äôll get the same output every time you ask,
    almost like a broken record. On the other hand, setting it to a higher value (e.g.,
    0.8) means that you‚Äôll get a fresh output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_tokens = 6000`: This parameter specifies the maximum number of tokens
    the response can contain. Setting it to 6000 ensures that the response doesn''t
    exceed this length. If the response exceeds this limit, it will be truncated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After receiving a response from GPT-3, we‚Äôll return the content (excluding any
    additional meta-information).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'GPT-3 returned five topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '‚Äú**1\. Data cleaning and preparation**: Many reviews mention the challenge
    of cleaning and preparing the data for analysis. This includes dealing with missing
    values, formatting issues, unstructured data, and the need for data wrangling.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**2\. Data quality and documentation**: Several reviews highlight the poor
    quality of the data, including lack of documentation, incorrect documentation,
    and unreliable data. Issues with data completeness, accuracy, and reliability
    are also mentioned.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**3\. Finding and accessing relevant datasets**: Many reviewers express difficulties
    in finding the right datasets for their projects. This includes challenges in
    finding datasets that match specific requirements, lack of availability, limited
    size or relevance of public datasets, and the need to collect personal data.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**4\. Connectivity and data fusion**: Some reviews mention challenges related
    to data connectivity and fusion, such as integrating data from different sources,
    dealing with inconsistent formats, and merging datasets.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**5\. Computing power and scalability**: A few reviews mention challenges related
    to computing power and scalability, particularly when working with large datasets
    or when processing data on a single machine.‚Äô,'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ‚ÄòThese topics reflect common challenges faced by individuals when working with
    data, including issues related to data quality, data preparation, dataset availability,
    and technical limitations.‚Äù
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'üí° **Note**: *While GPT-3 is powerful as it is, you can often achieve better
    results by fine-tuning the model with your training data.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 6: Summarizing the Results'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These topics reflect common challenges faced by individuals when working with
    data, including issues related to data preparation, data quality, reliability,
    and scalability. A company like Kaggle can leverage these insights to develop
    educational material that specifically addresses these challenges, thereby providing
    valuable support for their community.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we‚Äôve explored the significant potential of LLMs in extracting
    insights from text data. We‚Äôve discussed how LLMs work and how they can be a game-changer
    for data analysts dealing with text data. You now have the knowledge to apply
    these concepts to your own text analysis tasks.
  prefs: []
  type: TYPE_NORMAL
- en: I hope you found this article helpful. If you have any questions or thoughts,
    I‚Äôll be happy to read them in the comments!
  prefs: []
  type: TYPE_NORMAL
