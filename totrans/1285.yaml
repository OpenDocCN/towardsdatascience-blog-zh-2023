- en: Hyperparameter Optimization With Hyperopt — Intro & Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/hyperparameter-optimization-with-hyperopt-intro-implementation-dfc1c54d0ba7](https://towardsdatascience.com/hyperparameter-optimization-with-hyperopt-intro-implementation-dfc1c54d0ba7)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Improving machine learning models’ performance with hyperparameter optimization.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@fmnobar?source=post_page-----dfc1c54d0ba7--------------------------------)[![Farzad
    Mahmoodinobar](../Images/2d75209693b712300e6f0796bd2487d0.png)](https://medium.com/@fmnobar?source=post_page-----dfc1c54d0ba7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dfc1c54d0ba7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dfc1c54d0ba7--------------------------------)
    [Farzad Mahmoodinobar](https://medium.com/@fmnobar?source=post_page-----dfc1c54d0ba7--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dfc1c54d0ba7--------------------------------)
    ·11 min read·Jun 5, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ba34fe1074371d14f74e66344336a40b.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Te NGuyen](https://unsplash.com/@tenguyen?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/Wt7XT1R6sjU?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: '[Hyperopt](https://github.com/hyperopt/hyperopt) is an open-source hyperparameter
    optimization tool that I personally use to improve my machine learning projects
    and have found it to be quite easy to implement. Hyperparameter optimization,
    is the process of identifying the best combination of hyperparameters for a machine
    learning model to satisfy an objective function (this is usually defined as “minimizing”
    the objective function for consistency). To use a different analogy, each machine
    learning model comes with various knobs and levers that we can tune, until we
    get the outcome that we have been looking for from the model. The act of finding
    the right combination of hyperparameters that results in the outcome that we have
    been looking for is called hyperparameter optimization. Some examples of such
    parameters are: learning rate, architecture of a neural network (e.g. number of
    hidden layers), choice of the optimizer, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are interested in exploring other hyperparameter optimization strategies,
    such as grid search, random search and bayesian optimization, check out the post
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[](/hyperparameter-optimization-intro-and-implementation-of-grid-search-random-search-and-bayesian-b2f16c00578a?source=post_page-----dfc1c54d0ba7--------------------------------)
    [## Hyperparameter Optimization — Intro and Implementation of Grid Search, Random
    Search and Bayesian…'
  prefs: []
  type: TYPE_NORMAL
- en: Most common hyperparameter optimization methodologies to boost machine learning
    outcomes.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/hyperparameter-optimization-intro-and-implementation-of-grid-search-random-search-and-bayesian-b2f16c00578a?source=post_page-----dfc1c54d0ba7--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@fmnobar/membership?source=post_page-----dfc1c54d0ba7--------------------------------)
    [## Join Medium with my referral link'
  prefs: []
  type: TYPE_NORMAL
- en: Read every story from Farzad (and other writers on Medium). Your membership
    fee directly supports Farzad and other…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@fmnobar/membership?source=post_page-----dfc1c54d0ba7--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 1.1\. Concepts and Installation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s first define some relevant concepts for using [Hyperopt](https://github.com/hyperopt/hyperopt).
  prefs: []
  type: TYPE_NORMAL
- en: '**Objective Function:** This is the function that hyperparameter optimization
    tries to minimize. More specifically, objective function accepts a combination
    of hyperparameters as input and returns the error level of the model (a.k.a. loss),
    given those accepted hyperparameters. The goal of hyperparameter optimization
    is to find the combination of hyperparameters that minimize this error/loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Search Space:** The range of input values (i.e. parameters) that the Objective
    Function accepts as arguments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimization Algorithm:** As the name suggests, this is the algorithm used
    to minimize the Objective Function. Hyperopt utilizes different search algorithms,
    such as random search and Tree of Parzen Estimators (TPE) ([documentation](http://hyperopt.github.io/hyperopt/#algorithms)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that we are familiar with the concepts, let’s install [Hyperopt](https://github.com/hyperopt/hyperopt)
    by running the command below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have the library installed, we will first walk through a very simple
    example to get a handle on how Hyperopt works. After that, we will move on to
    more interesting and complicated examples.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2\. Simple Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s start with a very simple one to help us understand how the overall process
    of hyperparameter optimization using Hyperopt works. We will start with a quadratic
    function of `f(x) = (x — 1)²`. This function’s optimized point is at `x = 1` and
    therefore we know what to expect. As it’s been a while since any of us took a
    calculus course, let’s look at the plot of the function, which helps us better
    understand how that point minimizes the function. Code block below will return
    the plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bb99a1173530a776353c596c75cda85c.png)'
  prefs: []
  type: TYPE_IMG
- en: Plot of f(x) = (x-1)²
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the minimum point happens where `x=1`. Let’s implement this using
    Hyperopt and see how it works.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to do so, we will take the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Import necessary libraries and packages
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the objective function and the search space
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the optimization process
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print the results (i.e. the optimized point that we expect to be `x = 1`)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Code block below, follows the above steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fad78aa76c9fe042b4cfd6230556f462.png)'
  prefs: []
  type: TYPE_IMG
- en: “best” returns the best combination of hyperparameters that the model was able
    to find and in this case it is almost equal to `x = 1`, as we expected! The process
    to implement Hyperopt is generally the same and now that we have walked through
    a simple one, let’s move to a more advanced example.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Hyperopt Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will implement two separate examples as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A classification with Support Vector Machine
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A regression with Random Forest Regressor
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will walk through the details of each of these two examples.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1\. Support Vector Machines and Iris Data Set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In a previous [post](/hyperparameter-optimization-intro-and-implementation-of-grid-search-random-search-and-bayesian-b2f16c00578a)
    I used Grid Search, Random Search and Bayesian Optimization for hyperparameter
    optimization using the [Iris data set provided by scikit-learn](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html).
    Iris data set includes 3 different irises petal and sepal lengths and is a commonly-used
    data set for classification exercises. In this post, we will use the same data
    set but we will use a Support Vector Machine (SVM) as a model with two parameters
    that we can optimize as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`C`: Regularization parameter, which trades off misclassification of training
    examples against simplicity of the decision surface.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gamma`: Kernel coefficient, which defines how much influence a single training
    example has. The larger gamma is, the closer other examples must be to be affected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the goal of this exercise is to go through the hyperparameter optimization,
    I will not go deeper into what SVMs do but if you are interested, I find [this](https://scikit-learn.org/stable/modules/svm.html)
    scikit-learn post helpful.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will generally follow the same steps that we used in the simple example
    earlier but will also visualize the process at the end:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Import necessary libraries and packages
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Define the objective function and the search space
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Run the optimization process
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Visualize the optimization
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1\. Step 1 — Import Libraries and Packages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s import the libraries and packages and then load the data set.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 2.1.2\. Step 2 — Define Objective Function and Search Space
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s first start with defining the objective function, which will train an
    SVM and returns the negative of the cross-validation score — that is what we want
    to minimize. Note that we are minimizing the negative of cross-validation score
    to be consistent with the general goal of “minimizing” the objective function
    (instead of “maximizing” the cross-validation score).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Next we will define the search space, which consists of the values that our
    parameters of `C` and `gamma` can take. Note that we will use Hyperopt’s `hp.uniform(label,
    low, high)`, which returns a value uniformly between “low” and “high” ([source](http://hyperopt.github.io/hyperopt/getting-started/search_spaces/)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 2.1.3\. Run Optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Same as the simple example earlier, we will use a TPE algorithm and store the
    results in a `Trials` object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bd5ea3f29056654e842c44cb30b39bb8.png)'
  prefs: []
  type: TYPE_IMG
- en: 2.1.4\. Visualize Optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we remember from the simple example, “best” includes the selected set of
    hyperparameters that Hyperopt found based on the implemented optimization strategy.
    Let’s look at the results!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/8e1681e9afd70df805341e84a54f927d.png)'
  prefs: []
  type: TYPE_IMG
- en: As expected, now we have a combination of hyperparameters that minimize the
    optimization function, using Hyperopt.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s visually look at how the objective function values change as the hyperparameters
    change. We will start with defining a function named `plot_obj_vs_hp()` that accomplishes
    this visualization. And then use that function to visualize the results. Make
    sure to look for the red dot — that one indicates the best combination of hyperparameters,
    according to our hyperparameter optimization!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1d88ed76eee51130d11432d4d197d12e.png)'
  prefs: []
  type: TYPE_IMG
- en: Subplots of Loss vs. Hyperparameters of C and gamma
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that since `C` and `gamma` are not really related to each other, we are
    showing them separately versus changes of the objective function. Since we want
    the objective function to be minimized, then we’re looking for the furthest bottom
    side of the plots above and based on the results of the hyperparameter optimization,
    we know that what we are looking for is where `{‘C’: 5.164418859504847, ‘gamma’:
    0.07084064498886927}`, which results in an objective function loss of around -0.986
    and is indicated by a red dot.'
  prefs: []
  type: TYPE_NORMAL
- en: I was also curious to look at these plots in a three-dimensional manner so I
    created the function below to accomplish that. Let’s look at the plot.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8fab4f3c2de7119c4952281873bbacaa.png)'
  prefs: []
  type: TYPE_IMG
- en: 3D Representation of Loss Function vs. Hyperparameters of C and gamma
  prefs: []
  type: TYPE_NORMAL
- en: Admittedly, this is not very easy to read but let’s give it a shot. We are looking
    for the lowest loss, which is the darkest dots on the plot (and the red dot is
    almost hidden by one of the dark dots). Visually it aligns with the two-dimensional
    plots that we had generated before.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s focus on a regression example.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Random Forest and Diabetes Data Set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This example focuses on a regression model that attempts at measuring the progression
    of the disease, one year after baseline. This data set is also taken from [scikit-learn](https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset)
    but the difference is that the data set is mainly used for regression (instead
    of classification that we looked at with the Iris example).
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in learning more about the differences between regression
    and classification, check out the post below.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://medium.com/@fmnobar/classification-vs-regression-in-machine-learning-which-one-should-i-use-f6b24d251c46?source=post_page-----dfc1c54d0ba7--------------------------------)
    [## Classification vs. Regression in Machine Learning — Which One Should I Use?'
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: medium.com](https://medium.com/@fmnobar/classification-vs-regression-in-machine-learning-which-one-should-i-use-f6b24d251c46?source=post_page-----dfc1c54d0ba7--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use a [Random Forest Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)
    model for this example and will optimize the objective function for two hyperparameters
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`n_estimators`: Number of trees in the random forest'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_depth`: Maximum depth of trees in the random forest'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The overall process of optimization is the same as what we have done so far.
    So, let’s break it down into our four usual steps!
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1\. Step 1 — Import Libraries and Packages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will start with importing the libraries and packages and then loading the
    data set.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 2.2.2\. Step 2 — Define Objective Function and Search Space
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to last time, let’s first start with defining the objective function,
    which will train our Random Forest Regressor and return the negative of the cross-validation
    score.
  prefs: []
  type: TYPE_NORMAL
- en: Next we will define the search space, which consists of the values that our
    parameters of `n_estimators` and `max_depth` can take. Note that we will use Hyperopt’s
    `hp.choice(label, options)`, which takes a value for the hyperparameter (i.e.
    `label`) and the possible values (i.e. `options`) for that hyperparameter ([source](http://hyperopt.github.io/hyperopt/getting-started/search_spaces/)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 2.2.3\. Run Optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Same as the examples earlier, we will use a TPE algorithm and store the results
    in a `Trials` object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2c540458ef2e3d1459e353463455f8a8.png)'
  prefs: []
  type: TYPE_IMG
- en: 2.2.4\. Visualize Optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we remember from the simple example, “best” includes the selected set of
    hyperparameters that Hyperopt found based on the implemented optimization strategy.
    Let’s look at the results!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c88625fea1d6b8d0f5e1bd9c76ac864f.png)'
  prefs: []
  type: TYPE_IMG
- en: As expected, now we have a combination of hyperparameters that minimize the
    optimization function, using Hyperopt. Let’s visually look at how the objective
    function values change as the hyperparameters change, using the functions that
    we had defined before to create the plots in two and three dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5119e66d0173c56a30c3a4a776666070.png)'
  prefs: []
  type: TYPE_IMG
- en: Subplots of Loss vs. Hyperparameters of n_estimators and max_depth
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/84eeaca7c41ae4b0d984d03e79352c4b.png)'
  prefs: []
  type: TYPE_IMG
- en: 3D Representation of Loss Function vs. Hyperparameters of n_estimators and max_depth
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this post, we introduced [Hyperopt](https://github.com/hyperopt/hyperopt)
    — a powerful and open-source hyperparameter optimization tool and then walked
    through examples of implementation in the context of classification via a Support
    Vector Machine and regression via a Random Forest Regressor. Then we looked the
    best combination of hyperparameters found through these processes and visualized
    the results in two and three dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for Reading!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you found this post helpful, please [follow me on Medium](https://medium.com/@fmnobar)
    and subscribe to receive my latest posts!
  prefs: []
  type: TYPE_NORMAL
- en: '*(All images, unless otherwise noted, are by the author.)*'
  prefs: []
  type: TYPE_NORMAL
