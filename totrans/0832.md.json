["```py\nfrom src.utils import *\nfrom src.dgp import dgp_promotional_email\ndgp = dgp_promotional_email(n=500)\ndf = dgp.generate_data()\ndf.head()\n```", "```py\nY = 'sales'\nW = 'mail'\nX = ['age', 'sales_old', 'new']\n```", "```py\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LogisticRegressionCV\n\nmodel_tau = RandomForestRegressor(max_depth=2)\nmodel_y = RandomForestRegressor(max_depth=2)\nmodel_e = LogisticRegressionCV()\n```", "```py\nfrom src.learners_utils import *\nfrom econml.metalearners import SLearner, TLearner, XLearner\nfrom econml.dml import NonParamDML\nfrom econml.dr import DRLearner\n\nS_learner = SLearner(overall_model=model_y)\nT_learner = TLearner(models=clone(model_y))\nX_learner = XLearner(models=model_y, propensity_model=model_e, cate_models=model_tau)\nR_learner = NonParamDML(model_y=model_y, model_t=model_e, model_final=model_tau, discrete_treatment=True)\nDR_learner = DRLearner(model_regression=model_y, model_propensity=model_e, model_final=model_tau)\n```", "```py\nnames = ['SL', 'TL', 'XL', 'RL', 'DRL']\nlearners = [S_learner, T_learner, X_learner, R_learner, DR_learner]\nfor learner in learners:\n    learner.fit(df[Y], df[W], X=df[X])\n```", "```py\ndef loss_oracle_mse(data, learner):\n    tau = learner.effect(data[X])\n    return np.mean((tau - data['effect_on_sales'])**2)\n```", "```py\ndef compare_methods(learners, names, loss, title=None, subtitle='lower is better'):\n    data = dgp.generate_data(seed_data=1, seed_assignment=1, keep_po=True)\n    results = pd.DataFrame({\n        'learner': names,\n        'loss': [loss(data.copy(), learner) for learner in learners]\n    })\n    fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n    sns.barplot(data=results, x=\"learner\", y='loss').set(ylabel='')\n    plt.suptitle(title, y=1.02)\n    plt.title(subtitle, fontsize=12, fontweight=None, y=0.94)\n    return results\n\nresults = compare_methods(learners, names, loss_oracle_mse, title='Oracle MSE Loss')\n```", "```py\ncost = 0.01\n```", "```py\ndef gain_oracle_policy(data, learner):\n    tau_hat = learner.effect(data[X])\n    return np.sum((data['effect_on_sales'] - cost) * (tau_hat > cost))\n\nresults = compare_methods(learners, names, gain_oracle_policy, title='Oracle Policy Gain', subtitle='higher is better')\n```", "```py\ndef loss_pred(data, learner):\n    tau = learner.effect(data[X])\n    learner2 = copy.deepcopy(learner).fit(data[Y], data[W], X=data[X])\n    tau2 = learner2.effect(data[X])\n    return np.mean((tau - tau2)**2)\nresults = compare_methods(learners, names, loss_pred, 'Prediction to Prediction Loss')\n```", "```py\nfrom dcor import energy_distance\n\ndef loss_dist(data, learner):\n    tau = learner.effect(data[X])\n    data.loc[data.mail==1, 'sales'] -= tau[data.mail==1]\n    return energy_distance(data.loc[data.mail==0, [Y] + X], data.loc[data.mail==1, [Y] + X], exponent=2)\nresults = compare_methods(learners, names, loss_dist, 'Distribution Loss')\n```", "```py\nfrom statsmodels.formula.api import ols \n\ndef loss_ab(data, learner):\n    tau = learner.effect(data[X]) + np.random.normal(0, 1e-8, len(data))\n    data['above_median'] = tau >= np.median(tau)\n    param = ols('sales ~ mail * above_median', data=data).fit().params[-1]\n    return param\nresults = compare_methods(learners, names, loss_ab, title='Above-below Median Difference', subtitle='higher is better')\n```", "```py\ndef generate_uplift_curve(df):\n    Q = 20\n    df_q = pd.DataFrame()\n    data = dgp.generate_data(seed_data=1, seed_assignment=1, keep_po=True)\n    ate = np.mean(data[Y][data[W]==1]) - np.mean(data[Y][data[W]==0])\n    for learner, name in zip(learners, names):\n        data['tau_hat'] = learner.effect(data[X])\n        data['q'] = pd.qcut(-data.tau_hat + np.random.normal(0, 1e-8, len(data)), q=Q, labels=False)\n        for q in range(Q):\n            temp = data[data.q <= q]\n            uplift = (np.mean(temp[Y][temp[W]==1]) - np.mean(temp[Y][temp[W]==0])) * q / (Q-1)\n            df_q = pd.concat([df_q, pd.DataFrame({'q': [q], 'uplift': [uplift], 'learner': [name]})], ignore_index=True)\n\n    fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n    sns.lineplot(x=range(Q), y=ate*range(Q)/(Q-1), color='k', ls='--', lw=3)\n    sns.lineplot(x='q', y='uplift', hue='learner', data=df_q);\n    plt.suptitle('Uplift Curve', y=1.02, fontsize=28, fontweight='bold')\n    plt.title('higher is better', fontsize=14, fontweight=None, y=0.96)\n\ngenerate_uplift_curve(df)\n```", "```py\nfrom scipy.spatial import KDTree\n\ndef loss_nn(data, learner):\n    tau_hat = learner.effect(data[X])\n    nn0 = KDTree(data.loc[data[W]==0, X].values)\n    control_index = nn0.query(data.loc[data[W]==1, X], k=1)[-1]\n    tau_nn = data.loc[data[W]==1, Y].values - data.iloc[control_index, :][Y].values\n    return np.mean((tau_hat[data[W]==1] - tau_nn)**2)\n\nresults = compare_methods(learners, names, loss_nn, title='Nearest Neighbor Loss')\n```", "```py\ndef loss_ipw(data, learner):\n    tau_hat = learner.effect(data[X])\n    e_hat = clone(model_e).fit(data[X], data[W]).predict_proba(data[X])[:,1]\n    tau_gg = data[Y] * (data[W] - e_hat) / (e_hat * (1 - e_hat))\n    return np.mean((tau_hat - tau_gg)**2)\n\nresults = compare_methods(learners, names, loss_ipw, title='IPW Loss')\n```", "```py\ndef loss_r(data, learner):\n    tau_hat = learner.effect(data[X])\n    y_hat = clone(model_y).fit(df[X + [W]], df[Y]).predict(data[X + [W]])\n    e_hat = clone(model_e).fit(df[X], df[W]).predict_proba(data[X])[:,1]\n    tau_nw = (data[Y] - y_hat) / (data[W] - e_hat)\n    return np.mean((tau_hat - tau_nw)**2)\n\ncompare_methods(learners, names, loss_r, title='R Loss')\n```", "```py\ndef loss_dr(data, learner):\n    tau_hat = learner.effect(data[X])\n    y_hat = clone(model_y).fit(df[X + [W]], df[Y]).predict(data[X + [W]])\n    mu1 = clone(model_y).fit(df[X + [W]], df[Y]).predict(data[X + [W]].assign(mail=1))\n    mu0 = clone(model_y).fit(df[X + [W]], df[Y]).predict(data[X + [W]].assign(mail=0))\n    e_hat = clone(model_e).fit(df[X], df[W]).predict_proba(data[X])[:,1]\n    tau_nw = mu1 - mu0 + (data[Y] - y_hat) * (data[W] - e_hat) / (e_hat * (1 - e_hat))\n    return np.mean((tau_hat - tau_nw)**2)\n\nresults = compare_methods(learners, names, loss_dr, title='DR Loss')\n```", "```py\ndef gain_policy(data, learner):\n    tau_hat = learner.effect(data[X])\n    e_hat = clone(model_e).fit(df[X], df[W]).predict_proba(data[X])[:,1]\n    d = tau_hat > cost\n    return np.sum((d * data[W] * (data[Y] - cost) / e_hat + (1-d) * (1-data[W]) * data[Y] / (1-e_hat)))\n\nresults = compare_methods(learners, names, gain_policy, title='Empirical Policy Gain', subtitle='higher is better')\n```"]