# 如何选择最佳的分类问题评价指标

> 原文：[https://towardsdatascience.com/how-to-choose-the-best-evaluation-metric-for-classification-problems-638e845da334](https://towardsdatascience.com/how-to-choose-the-best-evaluation-metric-for-classification-problems-638e845da334)

## 一份涵盖最常用的监督分类评价指标及其在不同场景下效用的综合指南

[](https://thomasdorfer.medium.com/?source=post_page-----638e845da334--------------------------------)[![Thomas A Dorfer](../Images/9258a1735cee805f1d9b02e2adf01096.png)](https://thomasdorfer.medium.com/?source=post_page-----638e845da334--------------------------------)[](https://towardsdatascience.com/?source=post_page-----638e845da334--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----638e845da334--------------------------------) [Thomas A Dorfer](https://thomasdorfer.medium.com/?source=post_page-----638e845da334--------------------------------)

·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----638e845da334--------------------------------) ·9分钟阅读·2023年4月17日

--

![](../Images/c6a92a3e2d07ecf632a0435a02146a6d.png)

图片由作者提供。

为了正确评估分类模型，重要的是要仔细考虑哪个评价指标最为合适。

这篇文章将涵盖分类任务中最常用的评价指标，包括相关示例案例，并为你提供选择这些指标所需的信息。

# 分类

分类问题的特点是根据给定观察的特征预测其类别或类别。选择最合适的评价指标将取决于用户希望优化的模型性能方面。

想象一个预测模型旨在诊断某种特定疾病。如果这个模型未能检测到疾病，可能会导致严重后果，如治疗延误和患者受害。另一方面，如果模型错误地诊断出健康患者，那也会导致不必要的检查和治疗，产生高昂的费用。

最终，选择减少哪种错误将取决于具体的使用案例及其相关成本。让我们来深入了解一些最常用的指标，以便对此有更清晰的了解。

# 评价指标

## 准确率

当数据集中的类别是平衡的——即每个类别中样本的数量大致相等时——准确率可以作为评估模型性能的简单直观的指标。

> 简单来说，**准确率**衡量模型所做的正确预测的比例。

为了说明这一点，我们来看以下表格，展示了实际类和预测类：

![](../Images/7c70ffa6575d1b09fd17e7c862da1b11.png)

绿色阴影的列表示正确预测。表格由作者提供。

在这个例子中，我们总共有10个样本，其中6个被正确预测（绿色阴影）。

因此，我们的准确度可以计算如下：

![](../Images/a01277b7429e87ede8b69763a0afd30e.png)

为了准备好接下来要讨论的指标，值得注意的是*正确预测*是*真阳性*和*真负例*的总和。

> **真阳性 (TP)** 发生在模型正确预测正类时。
> 
> **真负例 (TN)** 发生在模型正确预测负类时。

在我们的例子中，真阳性是一个结果，其中实际类和预测类都为1。

![](../Images/e005e302187378cf1ce6a813c9c1911e.png)

绿色阴影的列表示真阳性。表格由作者提供。

同样，当实际类和预测类都为0时，发生**真负例**。

![](../Images/3814599cd210b37642bc1f601488daa0.png)

绿色阴影的列表示真负例。表格由作者提供。

因此，你可能会看到准确度的公式有时写作如下：

![](../Images/d49ce17a8d2df106c52432843f682fb7.png)

***示例：*** 人脸检测。为了检测图像中是否有脸，准确度可以作为一个合适的指标，因为假阳性（将非人脸识别为人脸）和假阴性（未能识别出人脸）的成本大致相等。*注意：* 数据集中的类标签分布应该是平衡的，才能使准确度成为一个合适的衡量标准。

## 精确度

精确度指标适用于衡量正确的正预测比例。

> 换句话说，**精确度**提供了模型正确识别真阳性样本的能力的衡量标准。

因此，当目标是最小化假阳性时，它通常被使用，例如在信用卡欺诈检测或疾病诊断等领域。

> **假阳性 (FP)** 发生在模型错误预测正类时，表示某个条件存在，而实际情况并非如此。

在我们的例子中，假阳性是一个结果，其中预测的类应为0，但实际上是1。

![](../Images/9faf63ad891151da3fd7683a3bed65b6.png)

红色阴影的列表示假阳性。表格由作者提供。

由于精确度衡量的是实际为正类的正预测比例，因此其计算方法如下：

![](../Images/5b14eb56c075ada4798df1c93cabc7d0.png)

***示例:*** 异常检测。例如，在欺诈检测中，精确率可能是一个合适的评估指标，特别是当假阳性成本较高时。将非欺诈活动识别为欺诈活动不仅会导致额外的调查费用，还可能导致客户不满和流失率增加。

## 召回率

当预测任务的目标是最小化假阴性时，召回率作为合适的评估指标。

> **召回率**衡量模型正确识别的真正例比例。

在假阴性比假阳性更昂贵的情况下，它尤其有用。

> **假阴性（FN）**发生在模型错误预测负类，表明给定条件不存在，而实际情况是存在。

在我们的示例中，假阴性是指实际应该为1但预测为0的结果。

![](../Images/7330595e075597530bbf669ac9926949.png)

红色列表示假阴性。表格由作者提供。

召回率计算方法如下：

![](../Images/91a0b0a0437343ea9a6ab9356fa0cf7d.png)

***示例:*** 疾病诊断。例如，在COVID-19检测中，召回率是一个不错的选择，当目标是检测尽可能多的阳性病例时。在这种情况下，容忍更多的假阳性，因为优先目标是最小化假阴性，以防止疾病传播。可以说，漏掉阳性病例的成本远高于将阴性病例误分类为阳性。

## F1分数

在假阳性和假阴性都需要考虑的重要情况下，例如垃圾邮件检测，F1分数是一个有用的指标。

> **F1分数**是精确率和召回率的调和均值，通过考虑假阳性和假阴性来提供模型性能的平衡度量。

计算方法如下：

![](../Images/711f3107f60623ae534fa8212966bf4f.png)

***示例:*** 文档分类。例如，在垃圾邮件检测中，F1分数是一个合适的评估指标，因为目标是在精确率和召回率之间取得平衡。垃圾邮件分类器应尽可能准确地分类垃圾邮件（召回率），同时避免将合法邮件错误分类为垃圾邮件（精确率）。

## ROC曲线下面积（AUC）

接收者操作特征曲线，或[ROC曲线](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)，是一个图表，展示了二分类器在所有分类阈值下的性能。

> ROC曲线下面积，或**AUC**，衡量二分类器在不同阈值下区分正负类的能力。

当假阳性和假阴性的成本不同时时，这是一项特别有用的度量。这是因为它考虑了不同阈值下真正例率（敏感度）和假阳性率（1-特异性）之间的权衡。通过调整阈值，我们可以获得一个优先考虑敏感度或特异性的分类器，具体取决于特定问题的假阳性和假阴性的成本。

> **真正例率 (TPR)**，或**敏感度**，衡量模型正确识别的实际正例的比例。它与召回率完全相同。

计算方法如下：

![](../Images/7ca469388208b10fe61e2441ea3fb123.png)

> **假阳性率 (FPR)**，或**1-特异性**，衡量模型错误地将实际负例分类为正例的比例。

计算方法如下：

![](../Images/438366a0d2b086ce9b7c798eaa950304.png)

通过将分类阈值从 0 变化到 1，并计算每个这些阈值的 TPR 和 FPR，可以生成 ROC 曲线及其对应的 AUC 值。对角线表示随机分类器的表现——即一个对每个样本的类别标签进行随机猜测的分类器。

![](../Images/0aa2053330b313a4b0eb751b23d7af0d.png)

图像由作者提供。

ROC 曲线越接近左上角，分类器的表现越好。对应的 AUC 为 1 表示完美分类，而 AUC 为 0.5 表示随机分类性能。

***示例：*** 排名问题。当任务是按样本属于某个类别的可能性对其进行排序时，AUC 是一个合适的度量，因为它反映了模型正确排序样本的能力，而不仅仅是分类。例如，它可以用于在线广告，因为它评估了模型按用户点击广告的可能性正确排序的能力，而不仅仅是预测二分类点击/未点击结果。

## Log Loss

对数损失，也称为 log loss 或交叉熵损失，是分类问题中评估重要性概率估计的有用指标。

> **Log Loss** 衡量预测的类别概率与实际类别标签之间的差异。

当目标是惩罚模型对错误分类过于自信时，这是一项特别有用的度量。该度量也被用作逻辑回归和神经网络训练中的损失函数。

对于单个样本，其中*y*表示真实标签，*p*表示概率估计，log loss 计算方法如下：

![](../Images/4f4fea9355413799f1d8e8822cb14c60.png)

当真实标签为 1 时，log loss 作为预测概率的函数如下：

![](../Images/8f7bf57f84d7d980d0905a47e718cff2.png)

图像由作者提供。

可以明显看出，日志损失随着分类器对正确标签为 1 的确定性增加而减少。

日志损失也可以推广到多类别分类问题。对于单个样本，其中 *k* 表示类别标签，*K* 代表总类别数，可以按如下方式计算：

![](../Images/523414c7cddf07dc23e0286461a1c7bd.png)

在二分类和多分类问题中，日志损失是一个有用的指标，它衡量了预测概率与真实类别标签的匹配程度。

***示例：*** 信用风险评估。例如，日志损失可以用来评估预测借款人违约可能性的信用风险模型的性能。假阴性的成本（将可靠的借款人预测为不可靠）可能远高于假阳性的成本（将不可靠的借款人预测为可靠）。因此，在这种情况下，最小化日志损失有助于降低贷款的财务风险。

# 结论

为了准确评估分类器的性能并根据其预测做出明智决策，选择适当的评估指标至关重要。在大多数情况下，这一选择将高度依赖于具体问题。需要考虑的重要因素包括数据集中类别的平衡、是否更重要的是最小化假阳性、假阴性或两者，以及排名和概率估计的重要性。

## 喜欢这篇文章吗？

让我们保持联系！你可以在[Twitter](https://twitter.com/ThomasADorfer)、[LinkedIn](https://www.linkedin.com/in/thomasdorfer/)和[Substack](https://thomasdorfer.substack.com/)找到我。

如果你愿意支持我的写作，你可以通过[Medium Membership](https://thomasdorfer.medium.com/membership)来实现，这将使你能够访问我所有的故事以及 Medium 上其他成千上万位作者的作品。

[](https://medium.com/@thomasdorfer/membership?source=post_page-----638e845da334--------------------------------) [## 使用我的推荐链接加入 Medium - Thomas A Dorfer

### 阅读 Thomas A Dorfer（以及 Medium 上其他成千上万位作者）的每一篇故事。你的会员费用直接支持……

[medium.com](https://medium.com/@thomasdorfer/membership?source=post_page-----638e845da334--------------------------------)
