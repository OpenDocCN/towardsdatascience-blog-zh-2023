- en: Pre-Training Context is All You Need
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/pre-training-context-is-all-you-need-f457ffa8a358](https://towardsdatascience.com/pre-training-context-is-all-you-need-f457ffa8a358)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The driving force behind modern transformer models stems to a large extent from
    its pertaining data, allowing for strong in-context learning capabilities.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@benjamin.thuerer?source=post_page-----f457ffa8a358--------------------------------)[![Benjamin
    Thürer](../Images/b4c49698c7270c592bf992fc47f75765.png)](https://medium.com/@benjamin.thuerer?source=post_page-----f457ffa8a358--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f457ffa8a358--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f457ffa8a358--------------------------------)
    [Benjamin Thürer](https://medium.com/@benjamin.thuerer?source=post_page-----f457ffa8a358--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f457ffa8a358--------------------------------)
    ·6 min read·Nov 27, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Generative Artificial Intelligence and its popular transformer models are advertised
    everywhere these days and new models are being released every hour (see [the inflation
    of AI](https://medium.com/towards-data-science/the-inflation-of-ai-is-more-always-better-8ea1be75e0aa)).
    In this rapidly evolving field of AI, the possibilities of values these models
    could bring seem to be endless. Large Language Models (LLM) like [chatGPT](http://chat.openai.com)
    already made it into every Engineers' pile of resources, writers use them to support
    their articles, and designers create the first visuals or seek inspiration from
    the outcome of computer vision models.
  prefs: []
  type: TYPE_NORMAL
- en: If it is not magic, what really powers these impressive transformer models?
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'However, even though the achievements and usefulness are great and generative
    AI enhances productivity, it is important to recall that modern Machine Learning
    models (like LLMs or VisionTransformers) are not performing any magic at all (similar
    to the fact that ML, or statistical models in general, never have been magical).
    Even though the remarkable abilities of models might be perceived as *magic-like*
    and some experts in the field even talk about things like *hallucinations* of
    models, still, the foundation of every model is just math and statistical probabilities
    (sometimes complex, but still math). This leads to the fundamental question: If
    it is not magic, what really powers these impressive transformer models?'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/034acf90a6d7f81e53b4a668d97bd141.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Showcasing that ChatGPT (using GPT4) points towards its “advanced
    technology” and “extensive training” as the main performance drivers.'
  prefs: []
  type: TYPE_NORMAL
- en: The Fundament of Every Model is Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As with any model (statistical or ML), it is the training data that has the
    largest impact on the later model performance. If you don’t have a high volume
    of quality data reflecting the relationships you would like the model to learn,
    there is *nothing* to train on and the resulting model will perform poorly (the
    famous GIGO principle: Garbage In Garbage Out). This fundamental principle of
    data modeling has not changed at all over the years. Behind every revolutionary
    new transformer model stands first of all just one thing: **data**. It is the
    **amount**, **quality**, and **context** of that data that will drive the subsequent
    performance of the model. Recent studies (see further below) support this by showcasing
    that the latest generative AI models generalize well when the provided context
    is part of the training distribution but poorly for out-of-distribution learning.'
  prefs: []
  type: TYPE_NORMAL
- en: In-Distribution vs. Out-Of-Distribution Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is important to keep in mind that a model is nothing else than a huge network,
    tree, or graph of relationships. What an ML model basically learns is how to transform
    a given input into a desired output (see Figure 2).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9181ca0727d831aefc6a5acce38a704b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Schema of a super simple neural network predicting foot traffic based
    on weather and other context. On the left-hand side is the input during training
    (features) while on the right-hand side is the output (target). In between can
    be several transformations (layers) which learn a complex input-output relationship.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When the model is trained (or in other words: when those relationships are
    updated), the context of the input and its informativeness of the output will
    define what the model is good at. Similar to humans being good at responding to
    questions in their native language, ML models are good at responding to input
    data they have seen a lot. That is called **in-distribution** Learning. If, during
    training, the model has been provided with large amounts of rich context, it can
    rely on this acquired knowledge later and the resulting predictions show an accurate
    performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Out-of-distribution** learning, however, describes the situation where a
    model is supposed to predict based on context that it has *not* seen before. You
    can picture a human who never learned Norwegian suddenly responding to a question
    asked in Norwegian. Please inspect Figure 3 for an overview of in- and out-of-distribution
    learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9271f018d29e44fd241a22c72b9e8a15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Showing in-distribution (left) vs. out-of-distribution (right) Learning.
    The model on the left-hand side performs poorly for new context that was **not**
    part of the original training data (in this case “Politics”), while the model
    on the right-hand side does perform well for unseen context. ML models generally
    fall under the left category and perform poorly for out-of-distribution learning.'
  prefs: []
  type: TYPE_NORMAL
- en: The impressive performance of modern LLMs and other ML models comes from a vast
    volume and amount of context in the original training data. Due to this extensive
    pre-training of models, the range of questions that can fall inside of in-distribution
    learning is huge. That allows a model to have answers to a variety of questions
    and that might appear to a user as *magical* or as *human-level intelligence,*
    but it is not. Similarly, a wrong or unexpected answer by the model is also not
    a true *hallucination*, it basically highlights context gaps in the original training
    data and, thus, leads to out-of-distribution learning. In general, machine learning
    models are very limited in their out-of-distribution learning capabilities, requiring
    extensive training for foundational models.
  prefs: []
  type: TYPE_NORMAL
- en: The Power of Pretraining in Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a recent paper by Google DeepMind members, the authors strengthen the argument
    that the in-context learning performance of modern LLMs is mostly derived from
    their pre-training distribution. The paper “[Pretraining Data Mixtures Enable
    Narrow Model Selection Capabilities in Transformer Models](https://doi.org/10.48550/arXiv.2311.00871)”
    by Steve Yadlowsky, Lyric Doshi, and Nilesh Tripuraneni (2023) focuses on how
    modern transformer models, acquire their impressive in-context learning abilities
    (their abilities to have answers for any context prompted to them).
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://doi.org/10.48550/arXiv.2311.00871?source=post_page-----f457ffa8a358--------------------------------)
    [## Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer
    Models'
  prefs: []
  type: TYPE_NORMAL
- en: Transformer models, notably large language models (LLMs), have the remarkable
    ability to perform in-context learning…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: doi.org](https://doi.org/10.48550/arXiv.2311.00871?source=post_page-----f457ffa8a358--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: The findings are very insightful. When transformer models are pre-trained on
    data covering a wide range of contexts, they demonstrate an impressive performance
    in learning new tasks that fall within the pre-training context. This capability
    is near-optimal, showcasing an impressive degree of generalization and adaptability
    within the training distribution. However, when these models encounter context
    outside of their pre-training domain, the performance is limited and failures
    occur. This showcases a reduced generalization and clear limitations for out-of-distribution
    contexts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Vision Transformers: A Case Study in Scale'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In another study (also by Google DeepMind in 2023) with the title: “[ConvNets
    Match Vision Transformers at Scale](https://doi.org/10.48550/arXiv.2310.16764)”
    the authors Samuel L. Smith, Andrew Brock, Leonard Berrada, and Soham De challenge
    a widespread belief for computer vision that, at scale, modern Vision Transformer
    models outperform traditional models like Convolutional Neural Networks (CNNs).
    The study trains both CNNs and Vision Transformers with a similar computing budget
    and compares their performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://doi.org/10.48550/arXiv.2310.16764?source=post_page-----f457ffa8a358--------------------------------)
    [## ConvNets Match Vision Transformers at Scale'
  prefs: []
  type: TYPE_NORMAL
- en: Many researchers believe that ConvNets perform well on small or moderately sized
    datasets, but are not competitive with…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: doi.org](https://doi.org/10.48550/arXiv.2310.16764?source=post_page-----f457ffa8a358--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: The results indicate a scaling law between the compute budget used for pretraining
    and the subsequent performance. After fine-tuning on ImageNet, the pre-trained
    CNNs matched the performance of Vision Transformers at comparable budgets.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Together, these two studies show an interesting picture of the impressive performance
    of modern Transformer models. First, the performance is not just driven by the
    model architecture, it is more driven by the amount of pre-training conducted.
    Second, when the pre-training context covers a wide range, the resulting model
    will also show a wide range of in-context learning capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: These studies underscore the critical principle that volume, quality, and context
    of the training data is the most essential part of any foundational ML model.
    Without knowing the context covered by the pre-training it is hard to determine
    upfront the areas in which a model will perform well. Benchmark tests can help
    indicate potential context caps. Those tests do not showcase how good a model
    performs in general, they mostly showcase which context has been part of the model's
    training distribution.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, as it is the age of AI and the number of Data Scientists and
    Engineers developing ML models is increasing, it is becoming even more evident
    that pre-training with a wide range of contexts isn’t just a part of the process;
    in many ways, it’s all you need.
  prefs: []
  type: TYPE_NORMAL
- en: '*All images, unless otherwise noted, are by the author.*'
  prefs: []
  type: TYPE_NORMAL
- en: '***Please check out*** [***my profile page***](https://medium.com/@benjamin.thuerer/about)***,
    follow me, or*** [***subscribe to my email list***](https://medium.com/@benjamin.thuerer/subscribe)
    ***if you would like to know what I write about or if you want to be updated on
    new stories.***'
  prefs: []
  type: TYPE_NORMAL
