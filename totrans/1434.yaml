- en: Understanding LLM Hallucinations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/llm-hallucinations-ec831dcd7786](https://towardsdatascience.com/llm-hallucinations-ec831dcd7786)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Opinion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How LLMs can make stuff up and what to do about it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://franklyai.medium.com/?source=post_page-----ec831dcd7786--------------------------------)[![Frank
    Neugebauer](../Images/0da70d082d0f9c7ad8ccf574ed215df2.png)](https://franklyai.medium.com/?source=post_page-----ec831dcd7786--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ec831dcd7786--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ec831dcd7786--------------------------------)
    [Frank Neugebauer](https://franklyai.medium.com/?source=post_page-----ec831dcd7786--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ec831dcd7786--------------------------------)
    ·6 min read·May 8, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/95f952aadd9b08c63fbd537fcf27100e.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Ahmad Dirini](https://unsplash.com/@ahmadirini?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: Main Objectives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Working with large language models is not without risks including responses
    based on what’s called a LLM “hallucination.” Hallucinations can be a serious
    problem for LLMs because they can lead to the spread of misinformation, expose
    confidential information, and create unrealistic expectations about what LLMs
    can do. Understanding hallucinations and being critical of the information that
    they generate helps explain and mitigate problems such hallucinations can cause.
  prefs: []
  type: TYPE_NORMAL
- en: What’s a LLM Hallucination?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLMs are a type of artificial intelligence (AI) that are trained on massive
    datasets of text and code. They can generate text, translate languages, write
    different kinds of creative content, and answer questions in informative ways.
    However, LLMs are also prone to “hallucinating,” which means that they can generate
    text that is factually incorrect or nonsensical. As has been spoken about regularly,
    “LLMs can be confidently full of sh**.” Such hallucinations happen because LLMs
    are trained on data that is often incomplete or contradictory. As a result, they
    may learn to associate certain words or phrases with certain concepts, even if
    those associations are not accurate or are unintentionally “overly accurate” (by
    this I mean they can make up things that are true but not meant to be shared).
    This can lead to LLMs generating text that is factually incorrect, inadvertently
    overly indulgent, or simply nonsensical.
  prefs: []
  type: TYPE_NORMAL
- en: Types of Hallucinations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lies! Lies! Lies!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LLMs can sometimes generate text that is factually incorrect. Here’s an example,
    some of which is correct, but highlighting the part that is simply untrue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: To suggest segregation and discrimination no longer exists is factually untrue.
    For me to call it a “lie” is also technical untrue because the models don’t understand
    truth or lie, just how to assemble words. No matter the reason, LLMs can still
    generate factually untrue content. This doesn’t happen infrequently and it’s to
    fact check everything.
  prefs: []
  type: TYPE_NORMAL
- en: Nonsense
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At a very high level, LLMs are using probabilities to arrange words. While the
    range of words and their probabilities is likely to result in something that makes
    sense, that’s not always the case; LLMs can also generate text that is nonsensical.
    For example, if you ask an LLM to write a poem, it might generate something that
    is grammatically correct but does not make any sense.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: I can see how the creatives may say that it’s perfectly fine to say that a fish
    could lay in the sand in a poem (poetic license and all) but I can also argue
    the model is making up some nonsense, which is what I’m going with in this case.
    The ideas drifted from the logical to illogical. However, note that you’ll see
    more serious examples if you work with LLMs for any length of time. Again, check
    the model output and make corrections where necessary. In the poem example, I’d
    probably simply change “…we lay down on the beach…” to “…we laid down by the coral
    reef…” or maybe I’d just remove that line since humans can’t really take a nap
    underwater.
  prefs: []
  type: TYPE_NORMAL
- en: Source Conflation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs can sometimes conflate different sources of information, which can lead
    to them generating text that is inaccurate or misleading. For example, if you
    ask an LLM to write a news article about a current event, it might combine information
    from different news sources, even if those sources contradict each other. Note
    also that combining text that includes inferences made from historic information
    combined with (for example) something like LangChain, can really conflate sources
    (and formats) of information.
  prefs: []
  type: TYPE_NORMAL
- en: Here is an example of how conflation can lead to factually untrue (or minimally
    very misleading) information.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The response accurately opens in the first paragraph, describing the race held
    on May 7, 2023\. It then appears the model conflated the 2022 results thereafter.
    Leclerc qualified seventh and did not lead the first 20 laps in 2023, but he did
    finish second in 2022 and may have led the first 20 laps of that race. (Sergio
    Perez finished second in 2023.) It’s possible, in this case, that the conflation
    happened in whatever (e.g., LangChain) was used to combine current events and
    the LLM text, but the same hallucination by conflation idea holds true.
  prefs: []
  type: TYPE_NORMAL
- en: Overindulgence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given that LLMs can put some fairly significant text together using probabilities,
    and they can conflate information, it’s statistically possible for an LLM to make
    up information that “accidentally” discloses confidential information.
  prefs: []
  type: TYPE_NORMAL
- en: For the sake of protecting confidential information, I can’t provide details
    about the situation I recently ran into where an LLM did just that. However, I
    asked an LLM about a particular topic that I knew should not be disclosed and
    the model guessed at a logically correct but overindulgent reply. While the situation
    I ran into was not a matter of national security, it can be very serious under
    the right circumstances.
  prefs: []
  type: TYPE_NORMAL
- en: How to Manage Hallucinations?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are some tips for managing hallucinations:'
  prefs: []
  type: TYPE_NORMAL
- en: Test different settings for things like temperature and TopK (how probabilities
    are managed by the model). This is one of the most important ways to manage model
    output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don’t trust the output — fact check (don’t worry, you’re still saving massive
    amounts of time!).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generally treat LLM output as a drafting mechanism — for example, an LLM created
    the basic layout and a little of the content for this article. But I edited it
    significantly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tune the model you’re using. Depending on how you’re using model output, you
    may want to tune the model — there are many ways to do this, including prompt
    engineering, parameter efficient tuning (PET), and full model tuning. There’s
    quite a bit of nuance and complexity in that simple list but if you know how do
    it, such tuning can reduce hallucinations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accept the reality that models hallucinate. Unlike humans (in most cases), LLM
    hallucination is usually an unintended consequence and I believe generally the
    positives well outweigh the negatives. Accept this and acknowledge / communicate
    the possibility hallucinations can happen.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EXPLORE! While this article provided an overview of LLM hallucinations, what
    this means for you and your application can vary significantly. Additionally,
    your perception of these words may not exactly align with reality. The only way
    to truly understand and appreciate how LLM hallucination affects what you’re trying
    to do is to explore LLMs extensively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More to be Revealed
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The widespread use of LLMs is really in its infancy and the pros/cons have yet
    to be enumerated with any accuracy. In my opinion, an open mind is the best technique
    to understand all the dimensions of an LLM, including hallucinations. Enjoy this
    ride and explore as much as you can because such rapid evolution seldom happens
    (in my experience) and those who embrace the ride gain the most from it.
  prefs: []
  type: TYPE_NORMAL
