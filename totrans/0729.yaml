- en: Deterministic vs. Probabilistic Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/deterministic-vs-probabilistic-deep-learning-5325769dc758](https://towardsdatascience.com/deterministic-vs-probabilistic-deep-learning-5325769dc758)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Probabilistic Deep Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@luisroque?source=post_page-----5325769dc758--------------------------------)[![Luís
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----5325769dc758--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5325769dc758--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5325769dc758--------------------------------)
    [Luís Roque](https://medium.com/@luisroque?source=post_page-----5325769dc758--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5325769dc758--------------------------------)
    ·9 min read·Jan 11, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article belongs to the series “Probabilistic Deep Learning”. This weekly
    series covers probabilistic approaches to deep learning. The main goal is to extend
    deep learning models to quantify uncertainty, i.e., know what they do not know.
  prefs: []
  type: TYPE_NORMAL
- en: This article covers the main differences between Deterministic and Probabilistic
    deep learning. Deterministic deep learning models are trained to optimize a scalar-valued
    loss function, while probabilistic deep learning models are trained to optimize
    a probabilistic objective function. Deterministic models provide a single prediction
    for each input, while probabilistic models provide a probabilistic characterization
    of the uncertainty in their predictions, as well as the ability to generate new
    samples from the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Articles published so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Gentle Introduction to TensorFlow Probability: Distribution Objects](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-distribution-objects-1bb6165abee1)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Gentle Introduction to TensorFlow Probability: Trainable Parameters](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-trainable-parameters-5098ea4fed15)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Maximum Likelihood Estimation from scratch in TensorFlow Probability](/maximum-likelihood-estimation-from-scratch-in-tensorflow-probability-2fc0eefdbfc2)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Probabilistic Linear Regression from scratch in TensorFlow](/probabilistic-linear-regression-from-scratch-in-tensorflow-2eb633fffc00)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Probabilistic vs. Deterministic Regression with Tensorflow](https://medium.com/towards-data-science/probabilistic-vs-deterministic-regression-with-tensorflow-85ef791beeef)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Frequentist vs. Bayesian Statistics with Tensorflow](https://medium.com/towards-data-science/frequentist-vs-bayesian-statistics-with-tensorflow-fbba2c6c9ae5)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deterministic vs. Probabilistic Deep Learning
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](../Images/71c0028b4a604a1f23ee10072d79519e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The motto for today: we are always uncertain about things; why should
    our models be any different? ([source](https://unsplash.com/photos/Vkp9wg-VAsQ))'
  prefs: []
  type: TYPE_NORMAL
- en: We develop our models using TensorFlow and TensorFlow Probability. TensorFlow
    Probability is a Python library built on top of TensorFlow. We will start with
    the basic objects we can find in TensorFlow Probability and understand how we
    can manipulate them. We will increase complexity incrementally over the following
    weeks and combine our probabilistic models with deep learning on modern hardware
    (e.g. GPU).
  prefs: []
  type: TYPE_NORMAL
- en: As usual, the code is available on my [GitHub](https://github.com/luisroque/probabilistic_deep_learning_with_TFP).
  prefs: []
  type: TYPE_NORMAL
- en: Deterministic vs. Probabilistic Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning has become the dominant approach for a wide range of machine learning
    tasks, such as image and speech recognition, natural language processing, and
    reinforcement learning. A key feature of deep learning is the ability to learn
    complex, non-linear functions from large amounts of data. However, traditional
    deep learning models are deterministic, meaning they make the same predictions
    given the same inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Deterministic deep learning models, such as feedforward neural networks, are
    trained to optimize a scalar-valued loss function, such as mean squared error
    or cross-entropy. The trained model produces a single prediction for each input,
    but does not provide any information about the uncertainty of the prediction.
    In contrast, probabilistic deep learning models, such as variational autoencoders
    (VAEs) and generative adversarial networks (GANs), are trained to optimize a probabilistic
    objective function, such as the negative log-likelihood of the data or an approximate
    posterior distribution. These models provide a probabilistic characterization
    of the uncertainty in their predictions.
  prefs: []
  type: TYPE_NORMAL
- en: One of the main advantages of probabilistic deep learning models is the ability
    to generate new samples from the model. For example, a VAE can be used to generate
    new images that are similar to the training data, while a GAN can be used to generate
    new images that are different from the training data. Additionally, probabilistic
    deep learning models are often used for unsupervised learning, where the goal
    is to learn a compact representation of the data without any labels.
  prefs: []
  type: TYPE_NORMAL
- en: Data and Approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we employ the MNIST dataset and its corrupted counterpart to
    evaluate our approach. The corrupted version of the MNIST dataset introduces an
    added level of complexity through grey spatters superimposed on the original images,
    making classifying the handwritten digits more challenging.
  prefs: []
  type: TYPE_NORMAL
- en: Our objective is to construct a convolutional neural network (CNN) that effectively
    classifies the images of handwritten digits into 10 distinct classes. To this
    end, we make use of the aforementioned datasets, in order to provide a comprehensive
    evaluation of the performance of both the deterministic and the probabilistic
    CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/98fbc67bcdc61423c1180bbbb62c181e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Random examples from the MNIST dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/216a793a81c0b023de312311daadf6cb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Random examples from the corrupted version of the MNIST dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Deterministic Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We begin by formally introducing the deterministic model, which is a convolutional
    neural network (CNN) classifier comprised of several key architectural components.
    Specifically, this model consists of:'
  prefs: []
  type: TYPE_NORMAL
- en: A convolutional layer, in which the convolutional operation is performed by
    a set of 8 filters with a kernel size of 5x5 and ‘VALID’ padding, and the output
    is passed through a rectified linear unit (ReLU) activation function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A max-pooling layer, in which the maximum value within non-overlapping windows
    of size 6x6 is taken, reducing the spatial dimensionality of the feature maps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A flatten layer, in which the pooled feature maps are collapsed into a single
    vector, allowing for the final dense layer to have fully connected computations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A dense layer, which is also known as fully-connected layer, has 10 units and
    applies a softmax activation function to obtain the final probability distribution
    over the class labels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This CNN classifier architecture is designed to efficiently extract discriminative
    features and perform classification on high-dimensional image data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We have implemented the architecture discussed above, so now we are ready to
    start the training procedure.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we can check the accuracy on both datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Probabilistic Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In contrast to the deterministic model previously discussed, our probabilistic
    model introduces a new approach to outputting a distribution object, specifically
    a One-Hot Categorical distribution. This enables the modeling of aleatoric uncertainty
    on the image labels, allowing for a more comprehensive characterization of the
    model’s predictions.
  prefs: []
  type: TYPE_NORMAL
- en: The One-Hot Categorical distribution is a discrete probability distribution
    over one-hot bit vectors, with the event dimension equal to K, the number of classes.
    It is mathematically equivalent to the Categorical distribution, which is a discrete
    probability distribution over positive integers, with the key distinction being
    that Categorical has an empty event dimension, whereas One-Hot Categorical has
    event dimension equal to K.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our proposed probabilistic CNN architecture consists of:'
  prefs: []
  type: TYPE_NORMAL
- en: A convolutional layer, in which the convolutional operation is performed by
    a set of 8 filters with a kernel size of 5x5 and ‘VALID’ padding, and the output
    is passed through a rectified linear unit (ReLU) activation function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A max-pooling layer, in which the maximum value within non-overlapping windows
    of size 6x6 is taken, reducing the spatial dimensionality of the feature maps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A flatten layer, in which the pooled feature maps are collapsed into a single
    vector, allowing for the final dense layer to have fully connected computations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A dense layer, which is also known as fully-connected layer, with the number
    of units required to parameterize the probabilistic layer that follows.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A OneHotCategorical distribution layer, with an event shape equal to 10, corresponding
    to the 10 classes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This novel architecture, incorporating One-Hot Categorical output distribution,
    enables the modeling of aleatoric uncertainty on the image labels, and allows
    for a more comprehensive characterization of the model’s predictions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Our implementation of the model is done. Take a moment to compare the implementation
    above with the one that we did earlier that implements the corresponding deterministic
    architecture. If you have any doubts about specific components that we defined
    for the probabilistic version of the model, e.g. the loss function, please refer
    to the earlier articles in this series. Once again, we can now start the training
    procedure.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we can check the accuracy of this version of the model. Note that the
    test accuracy of the probabilistic model is identical to the deterministic one.
    This is because the model architectures for both are equivalent; the only difference
    being that the probabilistic model returns a distribution object.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Results and Discussion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Accuracy is an important metric to assess the performance of the model. Nonetheless,
    it is sometimes shallow since it does not provide information about the uncertainty
    of the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we go beyond the predicted label and provide a visual representation
    of the uncertainty in the predictions. To accomplish this, we sample the predictive
    distribution of the model and calculate the percentiles of the resulting samples.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The model is very confident that the first image is a 7, which is correct. For
    the second image, the model struggles, assigning nonzero probabilities to many
    different classes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/b04c8d0904a01b294ab8603b1e14abcf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Predictions from the probabilistic model for the MNIST dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Once again, the model is confident about its prediction of the first image.
    Despite the spatters, the number is still easy to identify. The second number
    is significantly harder to identify. The model still does a good job of predicting
    the right number and showing uncertainty about that choice.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/20f19ec19e798f6683ece0420dd5a761.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Predictions from the probabilistic model for the corrupted version
    of the MNIST dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we have discussed the key differences between deterministic
    and probabilistic deep learning models, focusing on using these models for image
    classification tasks. By analyzing the performance of a CNN on the MNIST dataset
    and on the corrupted version of the same dataset, we have shown that probabilistic
    deep learning models can achieve similar accuracy levels to deterministic models
    and provide a probabilistic characterization of the uncertainty in their predictions.
  prefs: []
  type: TYPE_NORMAL
- en: One of the main advantages of probabilistic deep learning models is the ability
    to generate new samples from the model. This can be useful for tasks such as image
    synthesis or data augmentation, where the goal is to create new, realistic images
    from a given dataset. Additionally, probabilistic deep learning models can be
    used for unsupervised learning, where the goal is to learn a compact representation
    of the data without any labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keep in touch: [LinkedIn](https://www.linkedin.com/in/luisbrasroque/)'
  prefs: []
  type: TYPE_NORMAL
- en: References and Materials
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] — [Coursera: Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] — [Coursera: TensorFlow 2 for Deep Learning](https://www.coursera.org/specializations/tensorflow2-deeplearning)
    Specialization'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] — [TensorFlow Probability Guides and Tutorials](https://www.tensorflow.org/probability/overview)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] — [TensorFlow Probability Posts in TensorFlow Blog](https://blog.tensorflow.org/search?label=TensorFlow+Probability&max-results=20)'
  prefs: []
  type: TYPE_NORMAL
