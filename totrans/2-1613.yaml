- en: 'Our MLOps story: Production-Grade Machine Learning for Twelve Brands'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们的 MLOps 故事：为十二个品牌提供生产级机器学习
- en: 原文：[https://towardsdatascience.com/our-mlops-story-production-grade-machine-learning-or-twelve-brands-a8727fd56c94](https://towardsdatascience.com/our-mlops-story-production-grade-machine-learning-or-twelve-brands-a8727fd56c94)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/our-mlops-story-production-grade-machine-learning-or-twelve-brands-a8727fd56c94](https://towardsdatascience.com/our-mlops-story-production-grade-machine-learning-or-twelve-brands-a8727fd56c94)
- en: Things we learned building an MLOps platform with limited means at DPG Media
    in the Netherlands
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在荷兰 DPG Media 以有限资源构建 MLOps 平台的过程中，我们学到的东西
- en: '[](https://medium.com/@jeffluppes?source=post_page-----a8727fd56c94--------------------------------)[![Jeffrey
    Luppes](../Images/f55d5e76fcf7e992582b45096500c215.png)](https://medium.com/@jeffluppes?source=post_page-----a8727fd56c94--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a8727fd56c94--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a8727fd56c94--------------------------------)
    [Jeffrey Luppes](https://medium.com/@jeffluppes?source=post_page-----a8727fd56c94--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jeffluppes?source=post_page-----a8727fd56c94--------------------------------)[![Jeffrey
    Luppes](../Images/f55d5e76fcf7e992582b45096500c215.png)](https://medium.com/@jeffluppes?source=post_page-----a8727fd56c94--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a8727fd56c94--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a8727fd56c94--------------------------------)
    [Jeffrey Luppes](https://medium.com/@jeffluppes?source=post_page-----a8727fd56c94--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a8727fd56c94--------------------------------)
    ·12 min read·Jun 5, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a8727fd56c94--------------------------------)
    ·阅读时间 12 分钟·2023 年 6 月 5 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Deploying a machine learning model once is a straightforward task; repeatedly
    bringing machine learning models into production is much harder. To address this
    complex process, the concept of MLOps (Machine Learning Operations) emerged. MLOps
    represents the convergence of DevOps, machine learning, and software engineering
    practices. There are several nuances needed here, but a better definition of what
    exactly MLOps entails is an open discussion and a battleground for vendors to
    pitch their products. For brevity’s sake, I’d rather move on to our MLOps story.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 部署一个机器学习模型一次是一个直接的任务；重复将机器学习模型投入生产则要困难得多。为了应对这一复杂过程，MLOps（机器学习运维）的概念应运而生。MLOps
    代表了 DevOps、机器学习和软件工程实践的融合。这里有几个细微之处，但 MLOps 具体包含什么的更好定义仍在讨论中，也是供应商推销其产品的战场。为了简洁起见，我宁愿继续讲述我们的
    MLOps 故事。
- en: 'Our MLOps journey started around September 2021\. Our team had only been created
    six months earlier, and we started with a handful of inherited projects. Our team’s
    goal was simple on paper: we were to provide a data / ML platform for *Online
    Services,* a part of DPG Media that focuses on websites and communities. Our “portfolio”
    consisted of a dozen brands that included amongst others: a popular tech news
    website and its’ community, two job portals, a community for expecting parents,
    several websites where one could buy second-hand cars, and a couple more with
    similar audiences. This is relevant for the rest of the post.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 MLOps 之旅始于 2021 年 9 月。我们的团队仅在六个月前成立，我们开始时接手了少量继承的项目。我们团队的目标在纸面上很简单：我们要为*在线服务*提供数据/机器学习平台，这是
    DPG Media 的一部分，专注于网站和社区。我们的“投资组合”包括十多个品牌，其中包括：一个受欢迎的科技新闻网站及其社区、两个招聘门户网站、一个期待父母的社区、几个可以购买二手车的网站，以及几个具有类似受众的网站。这一点对本文的其余部分很重要。
- en: '![](../Images/8961d6ab97f05ce9d2ee41a878ce0f6d.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8961d6ab97f05ce9d2ee41a878ce0f6d.png)'
- en: Back when we started building the platform, we intended to support these twelve
    (Dutch) brands
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始构建平台时，我们计划支持这十二个（荷兰）品牌。
- en: At the time, the ML part of the team was just a Data Scientist and me, a Machine
    Learning Engineer. Both of us were hired two weeks apart, completely new to the
    organisation that we were expected to roll in.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当时，机器学习团队只有一名数据科学家和我，一名机器学习工程师。我们两人相隔两周被聘用，对我们被期望投入的组织都是全新的。
- en: 'Back then, our ML production landscape (I use this term leniently) looked as
    follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当时，我们的机器学习生产环境（我宽松地使用这个术语）如下所示：
- en: 'A recommendation job to provide suggestions to users, running in Spark via
    Airflow for brand #1.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '一个建议工作，为用户提供建议，通过 Airflow 在 Spark 上运行，适用于品牌 #1。'
- en: 'A pricing suggestions model giving suggestions for cars, also running from
    Airflow as a batch job, for brand #2.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '一个给汽车定价建议的模型，同时作为 Airflow 的批处理作业运行，用于品牌 #2。'
- en: 'A model to target the right audiences in Databricks as a scheduled notebook,
    yet again running as a batch job, for brand #3.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '一个针对品牌 #3 的 Databricks 目标受众的模型，作为计划好的笔记本再次作为批处理作业运行。'
- en: A single flask application on EC2 that loaded various models into memory to
    facilitate demos and showcases of projects for various brands.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个在 EC2 上运行的单个 Flask 应用程序，将各种模型加载到内存中，以便演示和展示各个品牌的项目。
- en: With twelve brands, each potentially on their own systems and clouds, we could
    not make strong assumptions on how anything we developed would be used.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 面对十二个品牌，每个品牌可能在自己的系统和云上，我们不能对我们开发的任何东西如何被使用做出强烈假设。
- en: Two Data Scientists, now gone, had done all previous projects. Most of their
    projects lived in Jupyter Notebooks and the Flask application that served them
    was hosted on EC2 — and went down frequently. A single MLflow server was running
    on EC2, but it was outdated, and there were some security concerns with it. The
    Databricks project was just a single notebook, not even under version control.
    We didn’t even know who paid for the cluster on which we had the job scheduled.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 两位现已离职的数据科学家完成了所有先前的项目。他们的大多数项目都存在于 Jupyter Notebooks 中，服务它们的 Flask 应用程序托管在
    EC2 上 — 且经常宕机。一个单一的 MLflow 服务器在 EC2 上运行，但已过时，并且存在一些安全问题。Databricks 项目只是一个单一的笔记本，甚至没有版本控制。我们甚至不知道谁为我们安排作业的集群付费。
- en: Our team needed to scale, take on more projects, and spend only a little time
    supporting old ones. We could not afford to continue as things were, especially
    if we wanted to run real-time inference. Good opportunities for machine learning
    started coming in, and our clients demanded live models. The case for a structured
    approach towards MLOps was clear.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的团队需要扩展，承担更多项目，并仅花少量时间支持旧项目。我们无法继续以原有方式运作，特别是当我们希望进行实时推理时。机器学习的良机开始出现，我们的客户要求实时模型。对
    MLOps 的结构化方法的必要性变得明确。
- en: Formulating a plan
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 制定计划
- en: We could rely on some help from Data Engineers (our team had four dedicated
    data engineers) Architects, and System Engineers across DPG, but it was clear
    that we should own the platform ourselves. Our data engineers in particular, were
    stretched thin and had their own deadlines and migrations In the future we needed
    to be the ones managing and updating our systems. We had to own it ourselves.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以依赖数据工程师（我们的团队有四名专职数据工程师）、架构师和 DPG 的系统工程师的帮助，但显然我们应该自己掌握平台。特别是我们的数据工程师已经捉襟见肘，面临着自己的截止日期和迁移。未来我们需要自己管理和更新我们的系统。我们必须亲自掌控。
- en: We organised a few brainstorming sessions within our team and interviewed a
    couple of architects across the org. Slowly but surely, things became more set
    in stone and less scribbled on a white board in some dark corner on the fifth
    floor of our office. We wanted to give MLflow another chance and try to keep as
    much on AWS as possible. After all, it was clear that AWS SageMaker was evolving
    towards a more mature platform. We were urged by one of the leads of another team
    to adapt Terraform (or some form of IaC) early on. Since our data engineers were
    already using Terraform, this became a cornerstone of our platform.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在团队内部组织了几次头脑风暴会议，并采访了组织中的几位架构师。事情逐渐变得更为确定，而不是像之前那样在我们办公室五楼的一个阴暗角落的白板上涂涂画画。我们想给
    MLflow 另一个机会，并尽可能地将所有内容保留在 AWS 上。毕竟，AWS SageMaker 显然在向更成熟的平台发展。我们的一位团队负责人敦促我们尽早适应
    Terraform（或某种形式的 IaC）。由于我们的数据工程师已经在使用 Terraform，这成为了我们平台的基石。
- en: '![](../Images/e53c3ec23ed64b36063b7f4c1b9f2b0d.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e53c3ec23ed64b36063b7f4c1b9f2b0d.png)'
- en: '*slaps whiteboard* this fella can fit just so many features in it. Image by
    author.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*拍打白板* 这个家伙只能容纳这么多功能。图片由作者提供。'
- en: I joined the MLOps community to learn more about the tools and had a couple
    of conversations with vendors. Started going to meetups. Luckily, we had an AWS
    enterprise support team also available to help us since we’re a large user of
    AWS already. Lots of ideas, perhaps too many, started pouring in. The MLOps scene
    was (still is) a bit of a mess and [that is to be expected](https://www.mihaileric.com/posts/mlops-is-a-mess/);
    what was important for us was moving fast and deciding what the things were that
    mattered most to us.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我加入了 MLOps 社区以了解更多工具，并与供应商进行了几次对话。开始参加聚会。幸运的是，我们还有一个 AWS 企业支持团队可以帮助我们，因为我们已经是
    AWS 的大用户。很多想法，可能太多了，开始涌入。MLOps 场景（现在仍然是）有点混乱，[这在预期之中](https://www.mihaileric.com/posts/mlops-is-a-mess/)；对我们来说，重要的是快速行动并决定最重要的事情。
- en: 'We eventually decided that the principles of the platform would be more or
    less the following:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终决定平台的原则大致如下：
- en: Do everything in Terraform, *unless we couldn’t..*
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用 Terraform 做所有事情，*除非我们不能..*
- en: Try to adhere to data mesh (which our org adapted in 2021), *unless..*
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽量遵循数据网格（我们组织在 2021 年采纳了），*除非..*
- en: Use managed services where possible, *unless..*
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽可能使用托管服务，*除非..*
- en: Go serverless, *unless..*
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转向无服务器，*除非..*
- en: Avoid Kubernetes for as long as possible
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽可能避免使用 Kubernetes
- en: Build *as we go* and migrate old projects when we revisit them
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*随着进展* 建设，并在我们重新访问旧项目时迁移它们'
- en: Model Artefacts and Experiment Tracking and our first MLOOPS
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型工件和实验跟踪以及我们的第一个 MLOOPS
- en: Despite the intention to migrate only when it was needed, one of the first things
    we tried was to migrate the MLflow server to AWS Fargate and have the database
    on AWS RDS, instead of running on the same EC2 instance as the server. Per business
    unit we decided to host one instance, with separate servers for test and prod.
    With four BUs, this would mean 4 x 2 almost identical set-ups
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有意图仅在需要时迁移，但我们尝试的首件事之一是将 MLflow 服务器迁移到 AWS Fargate，并将数据库放在 AWS RDS 上，而不是与服务器运行在同一个
    EC2 实例上。根据业务单元，我们决定托管一个实例，测试和生产用不同的服务器。这样，四个业务单元意味着 4 x 2 几乎相同的设置。
- en: '![](../Images/8c6cfcfcc90f52a3f22b9c67ea7b46d1.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c6cfcfcc90f52a3f22b9c67ea7b46d1.png)'
- en: Example architecture for MLflow on AWS. Taken from [https://aws.amazon.com/blogs/machine-learning/managing-your-machine-learning-lifecycle-with-mlflow-and-amazon-sagemaker/](https://aws.amazon.com/blogs/machine-learning/managing-your-machine-learning-lifecycle-with-mlflow-and-amazon-sagemaker/)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: MLflow 在 AWS 上的示例架构。取自 [https://aws.amazon.com/blogs/machine-learning/managing-your-machine-learning-lifecycle-with-mlflow-and-amazon-sagemaker/](https://aws.amazon.com/blogs/machine-learning/managing-your-machine-learning-lifecycle-with-mlflow-and-amazon-sagemaker/)
- en: This turned out to be a fairly expensive idea (8 copies of this set up, all
    with their own load balancers and databases!) and the amount of projects we were
    doing didn’t warrant it yet by far. We would later bring this down to two instances.
    Starting out big like this meant also figuring out terraform, elastic load balancers,
    IAM, and route 53 head on. The learning curve was pretty steep, and a lot of time
    was spent getting familiar with all the different parts of AWS.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这证明是一个相当昂贵的想法（8 份这个设置，每个都有自己的负载均衡器和数据库！），而我们正在做的项目数量远远不足以支撑它。后来我们将其减少到两个实例。像这样大规模启动也意味着要直接解决
    terraform、弹性负载均衡器、IAM 和 route 53。学习曲线相当陡峭，花了很多时间熟悉 AWS 的各个部分。
- en: Our first ML-OOPS was trying to set-up a ton of (expensive) instances and maintain
    them, way before they were needed.
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们的第一个 ML-OOPS 尝试在需要之前设置大量（昂贵的）实例并维护它们。
- en: For experimentation (AKA Jupyter notebooks) we settled on using AWS SageMaker’s
    Notebook instances. The notebook instances were supplied with a lifecycle script
    that would set the correct MLflow uri as a environment variable. We created some
    scripts to update these in bulk as well as monitor the notebooks themselves, such
    as throwing an alert in our slack channels if someone left their instance on outside
    office hours.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于实验（即 Jupyter 笔记本），我们决定使用 AWS SageMaker 的 Notebook 实例。这些笔记本实例配备了一个生命周期脚本，会将正确的
    MLflow uri 设置为环境变量。我们创建了一些脚本来批量更新这些实例，以及监控笔记本本身，比如在有人将实例留在办公时间外时在我们的 Slack 频道中发出警报。
- en: Model Deployment and training
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型部署和训练
- en: It soon became time to deploy our first models. We used SageMaker’s model endpoints
    and settled for a Lambda and API gateway set-up. SageMaker can essentially be
    a one stop shop for deployment, although there is a premium you pay over spinning
    up your own EC2 instances and hosting your own models. It is more expensive over
    trying to run everything yourself on a Kubernetes cluster you manage yourself.
    You get a lot back for the premium you pay, though. SageMaker handles various
    deployment strategies, autoscaling, and allows us to select GPU types when needed.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 很快就到了部署我们的第一个模型的时候。我们使用了SageMaker的模型端点，并决定使用Lambda和API网关的设置。SageMaker本质上可以作为一个一站式部署服务，尽管相比于自己启动EC2实例并托管模型，费用更高。相比于自己管理的Kubernetes集群，费用也更高。不过，你会得到很多回报。SageMaker处理各种部署策略、自动扩展，并在需要时允许我们选择GPU类型。
- en: Our models were easily deployed via the SageMaker SDK from both local and cloud
    environments (e.g. Jupyter notebook or Airflow). The AWS Lambda gave us extra
    control over in- and output to the model, and the API Gateway provided a restful
    interface that we could use with API keys for our users. Apart from only the model
    itself, all elements were deployed via terraform and adding new models was simply
    adding a terraform module, mainly specifying a name and where to pull the lambda’s
    code from. This also meant we could improve the (pre-)processing to the model
    without changing the SageMaker endpoint, and have CI/CD for the lambdas set up
    separately.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过SageMaker SDK轻松地从本地和云环境（例如Jupyter notebook或Airflow）部署模型。AWS Lambda为模型的输入和输出提供了额外的控制，而API
    Gateway提供了一个可以用API密钥供用户使用的restful接口。除了模型本身，所有元素都通过terraform进行部署，添加新模型只需添加一个terraform模块，主要指定名称和从哪里提取Lambda的代码。这也意味着我们可以改进模型的（前）处理，而无需更改SageMaker端点，并为lambdas单独设置CI/CD。
- en: '![](../Images/9935ad5113dc3ad6d40d32cee590968d.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9935ad5113dc3ad6d40d32cee590968d.png)'
- en: Typical deployment on AWS. Image by author.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: AWS上的典型部署。图像由作者提供。
- en: Training jobs were also delegated to SageMaker. It took considerable effort
    to create a template for our first few ML training jobs (LSTM models for text
    categorization made with TensorFlow) and log them to MLflow from *inside* the
    training job’s container. We made a generic template for training jobs. SageMaker
    is fairly opinionated on how it wants to receive training jobs, which means you
    need to adhere to certain conventions by the platform — even when using TensorFlow.
    Luckily, under the hood SageMaker’s model serving still uses TensorFlow Extended
    for TensorFlow Models, so there is some intuitive operability with SavedModels.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 训练任务也委托给了SageMaker。为我们的前几个ML训练任务（使用TensorFlow构建的文本分类LSTM模型）创建模板并从*训练任务容器内部*将其记录到MLflow上，花费了相当大的精力。我们为训练任务制作了一个通用模板。SageMaker对接收训练任务有相当明确的要求，这意味着你需要遵循平台的某些约定——即使在使用TensorFlow时也是如此。幸运的是，在后台，SageMaker的模型服务仍然使用TensorFlow
    Extended处理TensorFlow模型，因此与SavedModels有一定的直观操作性。
- en: We orchestrated our training jobs from Airflow, and explicitly did not include
    retraining on any code merges. Some of our models are fairly expensive, some are
    not, but almost all have needs for large compute or storage. If needed before
    scheduled to run, we can simply trigger the dag and run the pipeline.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过Airflow协调训练任务，并明确不包括任何代码合并后的再训练。我们的一些模型相当昂贵，一些则不然，但几乎所有模型都需要大量计算或存储。如果在计划运行之前需要，我们可以简单地触发dag并运行管道。
- en: Monitoring and Alerting
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控和警报
- en: The last item on our AWS shopping list was monitoring and alerting. We first
    tried out Amazon Managed Prometheus and Amazon Managed Grafana, hoping we could
    somehow get the data in there that we were seeing in CloudWatch and save on our
    CloudWatch costs. It turned out this was possible with exporter tools. We set
    our sights on YACE ([yet-another-cloudwatch-exporter](https://github.com/nerdswords/yet-another-cloudwatch-exporter))
    but that had to live somewhere. That somewhere would soon be EC2, and later ECS.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们AWS购物清单上的最后一项是监控和警报。我们首先尝试了Amazon Managed Prometheus和Amazon Managed Grafana，希望能够将CloudWatch中的数据导入这些工具中，从而节省CloudWatch的成本。事实证明，这可以通过exporter工具实现。我们瞄准了YACE（[yet-another-cloudwatch-exporter](https://github.com/nerdswords/yet-another-cloudwatch-exporter)），但它需要存放在某个地方。这个地方很快就会是EC2，然后是ECS。
- en: We also had some metrics coming in from one of our business units that we needed
    to track ourselves. This meant that we needed some kind of interface for them
    to interact with. This first seemed possible with Managed Prometheus’s Remote
    Write capability, but we needed more control and were setting up YACE (which was
    to be scraped every five minutes by Prometheus) anyway. We opted to move YACE
    and Prometheus to an ECS cluster and set up Remote Write plus a pushgateway to
    receive metrics from outside our environment. Finally, we did away with Amazon
    Managed Prometheus.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要跟踪来自一个业务单位的一些指标。这意味着我们需要某种接口来与之互动。起初，这似乎可以通过 Managed Prometheus 的 Remote
    Write 功能实现，但我们需要更多控制，因此我们还是设置了 YACE（Prometheus 每五分钟抓取一次）。我们决定将 YACE 和 Prometheus
    移动到 ECS 集群，并设置 Remote Write 加 pushgateway 来接收来自我们环境之外的指标。最后，我们放弃了 Amazon Managed
    Prometheus。
- en: Unfortunately, YACE did not support all of the AWS services we used. We lacked
    exporting for SageMaker and were totally in the dark regarding our model endpoints.
    Luckily the Amazon Managed Grafana instance also pulls stats from CloudWatch,
    again at a slight premium.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，YACE 并不支持我们使用的所有 AWS 服务。我们缺乏 SageMaker 的导出功能，对我们的模型端点一无所知。幸运的是，Amazon Managed
    Grafana 实例也从 CloudWatch 中提取统计数据，虽然这需要支付额外的费用。
- en: In Amazon Managed Grafana we made a generic dashboard that we converted into
    a template by taking the json model and parameterizing it. Once that was done,
    we rolled out dashboards for every model via terraform. Unfortunately, Amazon
    Managed Grafana demands an API key, for our terraform and ci/cd to function, which
    has a max life of 30 days. We set up a key-rotation Lambda to destroy and re-create
    a key every 29 days and store it in an AWS secret we can request inside our terraform
    code. The impact of this is that when deploying a model we could now automatically
    generate an API, monitoring and logging, and a custom dashboard, within a few
    seconds.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Amazon Managed Grafana 中，我们创建了一个通用仪表板，通过将 json 模型参数化，将其转换为模板。一旦完成，我们通过 terraform
    推出了每个模型的仪表板。不幸的是，Amazon Managed Grafana 需要一个 API 密钥，以便我们的 terraform 和 ci/cd 正常运行，该密钥的最大有效期为
    30 天。我们设置了一个密钥轮换 Lambda，每 29 天销毁并重新创建一个密钥，并将其存储在一个 AWS 秘密中，我们可以在 terraform 代码中请求它。这样，当部署模型时，我们现在可以在几秒钟内自动生成一个
    API、监控和日志记录以及一个自定义仪表板。
- en: Since Grafana could also be set to send alerts when metrics pass a certain threshold,
    this set up also allows us to trigger alerts upon issues and forward them to slack
    or OpsGenie. Since we were still with two, we made an on-call schedule with each
    of us taking a week of on-call at a time. The trick is never defining an alert
    with a high priority.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Grafana 也可以设置在指标超过某个阈值时发送警报，这种设置还允许我们在出现问题时触发警报，并将其转发到 Slack 或 OpsGenie。由于我们仍然只有两个人，我们制定了一个值班计划，每人轮流值班一周。诀窍是从不定义高优先级的警报。
- en: Results
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: '![](../Images/89a0c416f54cdc88e1ff0c9df4919f77.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89a0c416f54cdc88e1ff0c9df4919f77.png)'
- en: The most important services that make up the “deployment” platform at a glance.
    Image by author.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 从一瞥来看，组成“部署”平台的最重要服务。图片来源：作者。
- en: Our resulting “framework” is fairly lightweight and the keen reader will have
    noticed it doesn’t actually strive to fully automate anything end-to-end. We currently
    have about 15 models deployed for real-time inference, a year down the line with
    a team of two. Above is an AWS-centric view of the platform, while the below blueprint,
    using the template by [the AI Infrastructure Alliance](https://github.com/ai-infrastructure-alliance/blueprints),
    provides an overview of the features of the stack so far.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终的“框架”相当轻量级，细心的读者会发现它实际上并没有努力实现完全的端到端自动化。目前我们有大约 15 个模型部署用于实时推断，一年下来仍由两个人组成的团队维持。上面的图片是
    AWS 为中心的视图，而下面的蓝图，使用了 [AI 基础设施联盟](https://github.com/ai-infrastructure-alliance/blueprints)
    的模板，提供了迄今为止堆栈功能的概述。
- en: '![](../Images/b30e139ec5d7fb5c8f6721fb659c38e8.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b30e139ec5d7fb5c8f6721fb659c38e8.png)'
- en: Our MLOps stack. Image by author.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 MLOps 堆栈。图片来源：作者。
- en: We wanted to be flexible, and felt that going in with a heavier “do everything
    for us” framework might be more restricting and more costly. We try to make no
    strong assumptions. For instance, not every project has fresh data coming in or
    has access to feedback from production. We might not be allowed to store predictions
    all the time. Not every model is deployed on SageMaker (some can live in the Lambda
    perfectly fine!).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望保持灵活性，觉得使用一个更重的“为我们做所有事情”的框架可能会更具限制性和更昂贵。我们尽量不做强假设。例如，并不是每个项目都有新数据进入或有来自生产的反馈。我们可能不被允许随时存储预测结果。并不是每个模型都部署在SageMaker上（有些可以很好地运行在Lambda中！）。
- en: Like Lak Lakshmanan’s recent post (triggeringly titled “[No, you don’t need
    MLOps](https://becominghuman.ai/no-you-dont-need-mlops-5e1ce9fdaa4b)”) and the
    now-famous [MLOps without much Ops](https://towardsdatascience.com/tagged/mlops-without-much-ops)
    blog series (“[You don’t need a bigger boat](https://github.com/jacopotagliabue/you-dont-need-a-bigger-boat)”)
    we have a platform that strives to keep things simple.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 就像Lak Lakshmanan最近的帖子（标题引发关注的“[不，你不需要MLOps](https://becominghuman.ai/no-you-dont-need-mlops-5e1ce9fdaa4b)”）以及现在著名的[MLOps
    without much Ops](https://towardsdatascience.com/tagged/mlops-without-much-ops)博客系列（“[你不需要更大的船](https://github.com/jacopotagliabue/you-dont-need-a-bigger-boat)”），我们有一个努力保持简单的平台。
- en: '![](../Images/913d53d2829a671ad035fcc0bf44c8bf.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/913d53d2829a671ad035fcc0bf44c8bf.png)'
- en: That is, if you want to remain flexible, save time, budget or complexity. Meme
    by author.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，如果你想保持灵活性、节省时间、预算或复杂性。
- en: That said, we did have a lot of needs when building the platform. A high level-diagram
    of the ML lifecycle with our needs mapped out shows we now cover most of them.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，在构建平台时我们确实有很多需求。展示了我们需求映射的ML生命周期的高层次图表显示，我们现在覆盖了大部分需求。
- en: '![](../Images/5283f2feacac1d59540f207a530193ac.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5283f2feacac1d59540f207a530193ac.png)'
- en: A visualization of features we wanted in the platform and those that we covered.
    Note that we did not discuss all features in this blog post.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望在平台中拥有的功能以及我们已覆盖的功能的可视化。请注意，我们在这篇博客文章中没有讨论所有功能。
- en: Thinking forward
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向前思考
- en: So now we’ve talked a lot about what we’ve built and how it fits into the larger
    picture. What’s next for us?
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了我们构建了什么以及它如何融入更大的图景。接下来我们要做什么？
- en: The platform still has a blind spot when it comes to testing and validation.
    We have no formal framework apart from the odd test here and there. It’s relatively
    hard to build these up front and make sure that you’re not prematurely optimising,
    especially under pressure. At the same thing it’s something we really can’t do
    without from a software development perspective.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 该平台在测试和验证方面仍然存在盲点。除了偶尔的测试外，我们没有正式的框架。提前构建这些框架并确保不进行过早优化相对困难，尤其是在压力下。同时，从软件开发的角度来看，这是我们确实无法缺少的。
- en: Being a media company, we have models that work on text, images, click streams,
    graphs, vectors, and tabular data. Some models consume multiple data types. Models
    can include anything from XGBoost and Random Forests to Transformers and various
    Recurrent Neural Networks and Convolutional Neural Nets. How could we even hope
    to build or buy something to test all data types and all models?
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一家媒体公司，我们有处理文本、图像、点击流、图形、向量和表格数据的模型。一些模型处理多种数据类型。模型可以包括从XGBoost和随机森林到Transformer及各种递归神经网络和卷积神经网络的任何东西。我们如何希望构建或购买能够测试所有数据类型和模型的东西呢？
- en: Another issue to be addressed is reducing the cold start times for our Lambda
    functions. Lambda functions are functions as a service that run when invoked.
    After the first invocation the lambda stays alive for about 15 minutes, unless
    another invocation follows, up to a max lifespan of about two hours. When allocating
    the resources and image for the lambda the first time around there is a cold start.
    Sometimes this is a few seconds, but toss in a TensorFlow import into a Lambda
    and you shouldn’t be surprised to see your APIs time out.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要解决的问题是减少Lambda函数的冷启动时间。Lambda函数是在被调用时运行的服务函数。第一次调用后，Lambda会保持活动状态约15分钟，除非有其他调用跟随，最长寿命约为两小时。在首次分配资源和镜像时会出现冷启动。有时这只是几秒钟，但如果在Lambda中添加TensorFlow导入，你可能会看到你的API超时。
- en: It’s a fact of life and inherent to using lambdas — but it means that we, or
    the user of an API needs to have error handling to deal with it if they go on
    for too long. While it’s recommended to not use lambdas for low volumes of traffic
    it’s by far the cheapest option we have and it helps to off-set the cost premium
    we have for SageMaker. Furthermore, they’re incredibly easy to maintain. Whether
    the cold start is actually a problem, though, is something that entirely depends
    on the business context and request volume.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用 Lambda 的生活事实——但这意味着我们或 API 的用户需要具备错误处理机制，以应对运行时间过长的问题。虽然建议不要在低流量的情况下使用
    Lambda，但它仍然是我们所拥有的最便宜的选择，并且有助于抵消 SageMaker 的成本溢价。此外，它们非常容易维护。然而，冷启动是否真正成为问题，完全取决于业务背景和请求量。
- en: I’m hoping to move away from our self-hosted MLflow at some point. It has no
    role-based access. This means that every user can see (and delete) every model,
    and users will need to scroll through potentially hundreds of models in order
    to find the one they’re looking for. There’s also a cognitive load for using it;
    any Data Scientist using it will have to actively pay attention to setting their
    experiment and calling things like mlflow.log or mlflow.autolog. Since we don’t
    use the option to deploy from MLflow to SageMaker we can switch to one of the
    many other tools in this niche. We literally only use MLflow as a way to keep
    track of past model runs.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望将来能离开我们自托管的 MLflow。它没有基于角色的访问控制。这意味着每个用户都可以查看（和删除）每个模型，并且用户需要滚动浏览可能上百个模型才能找到他们需要的。使用它还需要认知负担；任何使用它的数据科学家都必须主动注意设置实验和调用诸如
    mlflow.log 或 mlflow.autolog 的函数。由于我们没有使用从 MLflow 部署到 SageMaker 的选项，我们可以切换到这个领域中的其他工具之一。我们实际上只是将
    MLflow 用作跟踪过去模型运行的一种方式。
- en: Closing thoughts
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结束语
- en: 'In summary, in this post I’ve gone over our process in creating a MLOps stack
    that suited our needs. We used AWS services and open source tools to select a
    suite of tools to allow us to handle the majority of our use cases. It will continue
    to evolve over time as our team evolves. Our main takeaways are:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本文介绍了我们创建适合我们需求的 MLOps 堆栈的过程。我们使用了 AWS 服务和开源工具来选择一套工具，以处理我们大部分的用例。随着团队的发展，这个堆栈将继续演变。我们的主要收获是：
- en: '**Go for managed services and don’t look back.** If your team is relatively
    small it’s a great idea togo for managed services so you do not have the same
    overhead.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**选择托管服务，别回头看。** 如果你的团队相对较小，选择托管服务是个很好的主意，这样你就不会有同样的管理开销。'
- en: '**Get Sagemaker.** Sagemakeris great for small teams and excels in deployment
    (not updates, but thats for another time), but it takes a while to get comfortable
    with it.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**获取 Sagemaker。** Sagemaker 非常适合小型团队，并且在部署方面表现出色（虽然更新方面表现一般，这个以后再说），但熟悉它需要一些时间。'
- en: '**Go with the flow.** Airflow and MLFlow are great tools in a ML stack because
    they allow orchestration and ML-bookkeeping, which again allow you to focus on
    the work that matters most.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**顺其自然。** Airflow 和 MLFlow 是机器学习堆栈中的好工具，因为它们允许编排和机器学习记账，这使你能够专注于最重要的工作。'
- en: '**Infrastructure as code is the cloud 10x multiplier.** No, seriously. It’s
    insane how much work Terraform has saved us from doing.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基础设施即代码是云计算的 10 倍放大器。** 说真的，Terraform 为我们节省了大量工作，真是太疯狂了。'
- en: I hope this fully open discussion allows other teams to decide on their stack
    and evaluate their needs. Hopefully, in the future the platforms and tools we
    used become more mature and integrate better.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这种完全开放的讨论能帮助其他团队决定他们的技术栈并评估他们的需求。希望未来我们使用的平台和工具能够变得更加成熟，并且更好地集成。
- en: This post benefitted from help by [Gido Schoenmacker](https://www.linkedin.com/in/g-schoenmacker/),
    [Joost de Wit](https://www.linkedin.com/in/jjdewit/), [Kim Sterenborg](https://www.linkedin.com/in/kimsterenborg/),
    and [Amine Ben Slama](https://www.linkedin.com/in/amineslama/).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 本文得到了 [Gido Schoenmacker](https://www.linkedin.com/in/g-schoenmacker/)、[Joost
    de Wit](https://www.linkedin.com/in/jjdewit/)、[Kim Sterenborg](https://www.linkedin.com/in/kimsterenborg/)
    和 [Amine Ben Slama](https://www.linkedin.com/in/amineslama/) 的帮助。
- en: '[*Jeffrey Luppes*](https://www.linkedin.com/in/jeffluppes/) *is a Machine Learning
    Engineer at* [*DPG Media*](https://www.dpgmediagroup.com/en-NL) *in Amsterdam,
    the Netherlands.*'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[*Jeffrey Luppes*](https://www.linkedin.com/in/jeffluppes/) *是位于荷兰阿姆斯特丹的* [*DPG
    Media*](https://www.dpgmediagroup.com/en-NL) *的机器学习工程师。*'
