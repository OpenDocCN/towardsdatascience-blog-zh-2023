- en: Non-Negative Matrix Factorization (NMF) for Dimensionality Reduction in Image
    Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非负矩阵分解（NMF）用于图像数据的降维
- en: 原文：[https://towardsdatascience.com/non-negative-matrix-factorization-nmf-for-dimensionality-reduction-in-image-data-8450f4cae8fa](https://towardsdatascience.com/non-negative-matrix-factorization-nmf-for-dimensionality-reduction-in-image-data-8450f4cae8fa)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/non-negative-matrix-factorization-nmf-for-dimensionality-reduction-in-image-data-8450f4cae8fa](https://towardsdatascience.com/non-negative-matrix-factorization-nmf-for-dimensionality-reduction-in-image-data-8450f4cae8fa)
- en: Discussing theory and implementation with Python and Scikit-learn
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Python 和 Scikit-learn 讨论理论和实现
- en: '[](https://rukshanpramoditha.medium.com/?source=post_page-----8450f4cae8fa--------------------------------)[![Rukshan
    Pramoditha](../Images/b80426aff64ff186cb915795644590b1.png)](https://rukshanpramoditha.medium.com/?source=post_page-----8450f4cae8fa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8450f4cae8fa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8450f4cae8fa--------------------------------)
    [Rukshan Pramoditha](https://rukshanpramoditha.medium.com/?source=post_page-----8450f4cae8fa--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://rukshanpramoditha.medium.com/?source=post_page-----8450f4cae8fa--------------------------------)[![Rukshan
    Pramoditha](../Images/b80426aff64ff186cb915795644590b1.png)](https://rukshanpramoditha.medium.com/?source=post_page-----8450f4cae8fa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8450f4cae8fa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8450f4cae8fa--------------------------------)
    [Rukshan Pramoditha](https://rukshanpramoditha.medium.com/?source=post_page-----8450f4cae8fa--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8450f4cae8fa--------------------------------)
    ·9 min read·May 6, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8450f4cae8fa--------------------------------)
    ·9 分钟阅读·2023年5月6日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/85529f41284031529617ac661c1a911b.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/85529f41284031529617ac661c1a911b.png)'
- en: Original image by [an_photos](https://pixabay.com/users/an_photos-3160435/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=4503287)
    from [Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=4503287)
    (Slightly edited by author)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 原图来自 [an_photos](https://pixabay.com/users/an_photos-3160435/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=4503287)
    从 [Pixabay](https://pixabay.com//?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=4503287)（作者稍作编辑）
- en: I have already discussed different types of dimensionality reduction techniques
    in detail.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经详细讨论了不同类型的降维技术。
- en: '**Principal Component Analysis (PCA)**, **Factor Analysis (FA)**, **Linear
    Discriminant Analysis (LDA)**, **Autoencoders (AEs)**, and **Kernel PCA** are
    the most popular ones.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**主成分分析（PCA）**、**因子分析（FA）**、**线性判别分析（LDA）**、**自编码器（AEs）**和**核主成分分析**是最受欢迎的几种。'
- en: Non-Negative Matrix Factorization (NMF or NNMF) is also a *linear* dimensionality
    reduction technique that can be used to reduce the dimensionality of the feature
    matrix.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 非负矩阵分解（NMF 或 NNMF）也是一种*线性*降维技术，可以用于减少特征矩阵的维度。
- en: All dimensionality reduction techniques fall under the category of unsupervised
    machine learning in which we can reveal hidden patterns and important relationships
    in the data without requiring labels.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 所有降维技术都属于无监督机器学习的范畴，通过这种方法，我们可以揭示数据中隐藏的模式和重要的关系，而不需要标签。
- en: So, dimensionality reduction algorithms deal with unlabeled data. When training
    such an algorithm, the **fit()** method only needs the feature matrix, **X** as
    the input and it does not require the label column, **y**.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，降维算法处理的是无标签的数据。在训练这种算法时，**fit()** 方法只需要特征矩阵 **X** 作为输入，不需要标签列 **y**。
- en: As its name implies, non-negative matrix factorization (NMF) needs the feature
    matrix to be non-negative.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名称所示，非负矩阵分解（NMF）需要特征矩阵为非负值。
- en: Because of this non-negativity constraint, the usage of NMF is limited to data
    with non-negative values such as image data (pixel values always lie between 0
    and 255, hence there are no negative values in image data!).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这种非负性约束，NMF 的使用范围被限制在非负值的数据上，例如图像数据（像素值总是介于 0 和 255 之间，因此图像数据中没有负值！）。
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The maths behind non-negative matrix factorization (NMF)
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非负矩阵分解（NMF）的数学原理
- en: Non-negative matrix factorization comes from linear algebra. In simple words,
    it is the process of decomposing a matrix into the product of two small matrices.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 非负矩阵分解来源于线性代数。简单来说，它是将一个矩阵分解为两个小矩阵的乘积的过程。
- en: To be more precise,
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 更准确地说，
- en: Non-negative matrix factorization (NMF) is the process of decomposing a non-negative
    feature matrix, V (nxp) into a product of two non-negative matrices called W (nxd)
    and H (dxp). All three matrices should contain non-negative elements.
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 非负矩阵分解（NMF）是将一个非负特征矩阵 V (nxp) 分解为两个非负矩阵 W (nxd) 和 H (dxp) 的乘积的过程。这三个矩阵都应包含非负元素。
- en: '![](../Images/358168c09c0d66c5abebf748a85dccbc.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/358168c09c0d66c5abebf748a85dccbc.png)'
- en: '**Non-negative matrix factorization equation** (Image by author)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**非负矩阵分解方程**（作者图片）'
- en: The product of **W** and **H** matrices only gives an approximation to the matrix
    **V**. So, you should expect some information loss when applying NMF.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**W** 和 **H** 矩阵的乘积仅能给出矩阵 **V** 的近似值。因此，在应用 NMF 时应预期会有一些信息损失。'
- en: '**V (*n x p*):** Represents the **feature matrix** where ***n*** is the number
    of observations (samples) and ***p*** is the number of features (variables). This
    is the data matrix we decompose.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**V (*n x p*):** 表示**特征矩阵**，其中 ***n*** 是观察（样本）的数量，***p*** 是特征（变量）的数量。这是我们要分解的数据矩阵。'
- en: '**W (*n x d*):** Represents the **transformed data matrix** after applying
    NMF. We can use this transformed matrix in place of the original feature matrix,
    **V**. So, **W** is the most important output of NMF. It is obtained by calling
    Scikit-learn NMF’s ***fit_transform()*** method. ***n*** is the number of observations
    (samples) and ***d*** is the number of latent factors or components. In other
    words, ***d*** describes the amount of dimensionality that we want to keep. It
    is actually a hyperparameter that we need to specify in the Scikit-learn NMF’s
    ***n_components*** argument. This is an integer value that should be less than
    the number of features, ***p*** and greater than 0\. Selecting the right value
    for ***d*** is a real challenge when performing NMF. We need to consider the balance
    between the amount of information and the number of components that we want to
    keep.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**W (*n x d*):** 表示**应用 NMF 后的转化数据矩阵**。我们可以用这个转化后的矩阵代替原始特征矩阵**V**。因此，**W**
    是 NMF 的最重要输出。它通过调用 Scikit-learn NMF 的 ***fit_transform()*** 方法获得。***n*** 是观察（样本）的数量，***d***
    是潜在因素或组件的数量。换句话说，***d*** 描述了我们希望保留的维度量。实际上，这是一个超参数，我们需要在 Scikit-learn NMF 的 ***n_components***
    参数中指定。这个整数值应该小于特征数量 ***p***，且大于 0。选择合适的 ***d*** 值是执行 NMF 时的一个真正挑战。我们需要考虑信息量与我们希望保留的组件数量之间的平衡。'
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**H (*d x p*):** Represents the **factorization matrix**. ***d***and***p***have
    the same definitions above. This matrix is not very important. However, this can
    be obtained by calling the Scikit-learn NMF’s ***components_*** attribute.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**H (*d x p*):** 表示**分解矩阵**。***d*** 和 ***p*** 的定义如上所述。这个矩阵不是特别重要。然而，可以通过调用
    Scikit-learn NMF 的 ***components_*** 属性来获得。'
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Python implementation of non-negative matrix factorization (NMF)
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非负矩阵分解（NMF）的 Python 实现
- en: In Python, NMF is implemented by using Scikit-learn’s **NMF()** class. As you
    already know, Scikit-learn is the Python machine learning library.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，NMF 通过使用 Scikit-learn 的 **NMF()** 类来实现。如你所知，Scikit-learn 是 Python
    的机器学习库。
- en: All you need to do is to import the **NMF()** class and create an instance of
    it by specifying the required arguments.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你只需导入 **NMF()** 类，并通过指定所需的参数来创建其实例。
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Important arguments of NMF() class
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NMF() 类的重要参数
- en: '**n_components:** An integer that defines the number of components or latent
    factors or the amount of dimensionality that we want to keep. The most important
    hyperparameter! The value is less than the number of original features and greater
    than 0.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**n_components:** 定义组件或潜在因素的数量或我们希望保留的维度量的整数值。最重要的超参数！该值小于原始特征数量，并且大于 0。'
- en: '**init:** A method of the initialization process. The output returned by the
    NMF model will significantly vary depending on the ***init*** method you choose.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**init:** 初始化过程的一种方法。NMF 模型返回的结果会因所选择的 ***init*** 方法而显著不同。'
- en: '**random_state:** Used when the initialization method is*‘nndsvdar’* or *‘random’*.
    Use an integer to get the same results across different executions.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**random_state:** 在初始化方法为 *‘nndsvdar’* 或 *‘random’* 时使用。使用一个整数以确保不同执行之间结果的一致性。'
- en: '**Note:** There are many arguments in the **NMF()** class. If we do not specify
    them, they take their default values when calling the **NMF()** function. To learn
    more about those arguments, refer to the Scikit-learn documentation.'
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** **NMF()** 类中有许多参数。如果我们没有指定它们，调用**NMF()**函数时将采用默认值。要了解更多关于这些参数的信息，请参阅Scikit-learn文档。'
- en: Important methods of NMF() class
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NMF() 类的重要方法
- en: '**fit(V):** Learns an NMF model from the feature matrix, **V**. No transformation
    is applied here.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**fit(V):** 从特征矩阵**V**中学习NMF模型。这里不应用任何转换。'
- en: '**fit_transform(V):** Learns an NMF model from the feature matrix, **V** and
    returns the transformed data matrix, **W**.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**fit_transform(V):** 从特征矩阵**V**中学习NMF模型，并返回转换后的数据矩阵**W**。'
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**transform(V):** Returns the transformed data matrix, **W** after fitting
    the model.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**transform(V):** 返回经过拟合模型后的转换数据矩阵**W**。'
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**inverse_transform(W):** Transforms (recovers) the data matrix, **W** back
    to the original space. Very useful for visualization purposes!'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**inverse_transform(W):** 将数据矩阵**W**转换（恢复）回原始空间。对可视化非常有用！'
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Important attributes of NMF() class
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NMF() 类的重要属性
- en: '**components_:** Returns the factorization matrix, **H**. This matrix is not
    very important.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**components_:** 返回分解矩阵**H**。这个矩阵不是非常重要。'
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**reconstruction_err_:** Returns the beta divergence as a float that measures
    the distance between **V** and the product of **WH**. The solver tries to minimize
    this error during the training process. Analyzing this error by setting different
    values for **n_components** is a great way of choosing the right number of components,
    ***d***.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**reconstruction_err_:** 返回一个浮点数表示的beta散度，该值衡量**V**与**WH**的乘积之间的距离。求解器在训练过程中尝试最小化此误差。通过设置不同的**n_components**值来分析此误差是选择正确的组件数量的一个好方法，***d***。'
- en: Reducing dimensionality in image data using non-negative matrix factorization
    (NMF)
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用非负矩阵分解（NMF）减少图像数据的维度
- en: We’ll use the MNIST digits dataset for this task. We’ll perform NMF on MNIST
    data to reduce dimensionality by choosing different numbers of components and
    then we compare each output with the original one.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用MNIST数字数据集来完成这个任务。我们将对MNIST数据进行NMF处理，通过选择不同数量的组件来降低维度，然后将每个输出与原始数据进行比较。
- en: 'Step 1: Load the MNIST dataset using Scikit-learn'
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1步：使用Scikit-learn加载MNIST数据集
- en: The MNIST digits dataset can be loaded as follows using Scikit-learn.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST数字数据集可以使用Scikit-learn按如下方式加载。
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/2f44f63943b088f8f64b391b263cc7b0.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f44f63943b088f8f64b391b263cc7b0.png)'
- en: (Image by author)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: （图片由作者提供）
- en: The dataset is loaded as a Pandas data frame. The shape is (70000, 784). There
    are 70000 observations (images) in the dataset. Each observation has 784 features
    (pixel values). The size of an image is 28 x 28\. When loading the MNIST dataset
    in this way, each image is represented as a 1D array that contains 784 (28 x 28)
    elements. This is the format we need for this task and no further modification
    is required for the dataset.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集被加载为Pandas数据框。形状为（70000, 784）。数据集中有70000个观测值（图像）。每个观测值有784个特征（像素值）。图像的大小为28
    x 28。以这种方式加载MNIST数据集时，每张图像被表示为一个包含784（28 x 28）元素的一维数组。这是我们完成此任务所需的格式，数据集无需进一步修改。
- en: Alternatively, you can load the MNIST dataset using Keras. There, you’ll get
    a 28 x 28 2D array for each image instead of a 1D array. You can learn more [here](https://medium.com/data-science-365/acquire-understand-and-prepare-the-mnist-dataset-3d71a84e07e7).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你也可以使用Keras加载MNIST数据集。那样的话，你将获得每张图像的28 x 28二维数组，而不是一维数组。你可以在[这里](https://medium.com/data-science-365/acquire-understand-and-prepare-the-mnist-dataset-3d71a84e07e7)了解更多信息。
- en: 'Step 2: Visualize a sample of the original images'
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2步：可视化原始图像的样本
- en: Now, we’ll visualize a sample of the first five images in the MNIST dataset.
    This sample can be used to compare with the outputs of the NMF model.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将可视化MNIST数据集中前五张图像的样本。这个样本可以用来与NMF模型的输出进行比较。
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/c357bf92bfaeb8b4b60e96e3cb71fc3b.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c357bf92bfaeb8b4b60e96e3cb71fc3b.png)'
- en: '**A sample of the original MNIST digits** (Image by author)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**原始MNIST数字的样本**（图片由作者提供）'
- en: 'Step 3: Apply NMF with 9 components (d = 9)'
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3步：应用具有9个组件的NMF（d = 9）
- en: '[PRE10]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/096b088205f681e74188f35702370083.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/096b088205f681e74188f35702370083.png)'
- en: (Image by author)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: （图片由作者提供）
- en: Now, the new dimensionality is 9\. The original dimensionality was 784\. Therefore,
    the dimensionality has been significantly reduced!
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，新的维度是9。原始维度为784。因此，维度已经显著降低！
- en: To get the shape of **V**, **W** and **H** matrices, we can run the following
    code.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取**V**、**W**和**H**矩阵的形状，我们可以运行以下代码。
- en: '[PRE11]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/51e05bace4cf767e10c2a85a36655106.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51e05bace4cf767e10c2a85a36655106.png)'
- en: (Image by author)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: （图片由作者提供）
- en: To get the reconstruction error or beta divergence between **V** and the product
    of **WH**, we can run the following code.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取重建误差或**V**与**WH**乘积之间的β散度，我们可以运行以下代码。
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](../Images/9ab242413756e1abfbae0d230cb07702.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ab242413756e1abfbae0d230cb07702.png)'
- en: (Image by author)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: （图片由作者提供）
- en: The reconstruction error is very high. This is because we have selected only
    9 components out of 784\. We can verify this by visualizing the output.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 重建误差非常高。这是因为我们只选择了784中的9个组件。我们可以通过可视化输出来验证这一点。
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '**NMF output: 9 components or d = 9**'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**NMF输出：9个组件或d = 9**'
- en: '![](../Images/29926404b025345a3b801d18f0945c92.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29926404b025345a3b801d18f0945c92.png)'
- en: '**NMF output: 9 components or d = 9** (Image by author)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**NMF输出：9个组件或d = 9**（图片由作者提供）'
- en: The digits are not clear. You can compare this output with the sample of the
    original images.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 数字不清晰。你可以将此输出与原始图像的样本进行比较。
- en: I have run the NMF algorithm by choosing 100, 225 and 784 components. Here are
    the results.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我运行了NMF算法，选择了100、225和784个组件。以下是结果。
- en: '**NMF output: 100 components or d = 100**'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**NMF输出：100个组件或d = 100**'
- en: '![](../Images/402b335ec56660d1248012423af4fc9c.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/402b335ec56660d1248012423af4fc9c.png)'
- en: '**NMF output: 100 components or d = 100** (Image by author)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**NMF输出：100个组件或d = 100**（图片由作者提供）'
- en: The reconstruction error is 174524.20.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 重建误差为174524.20。
- en: '**NMF output: 225 components or d = 225**'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**NMF输出：225个组件或d = 225**'
- en: '![](../Images/d021e74fcabce9a614b98ff2ffa141ae.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d021e74fcabce9a614b98ff2ffa141ae.png)'
- en: '**NMF output: 225 components or d = 225** (Image by author)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**NMF输出：225个组件或d = 225**（图片由作者提供）'
- en: The reconstruction error is 104024.62.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 重建误差为104024.62。
- en: '**NMF output: 784 components or d = 784 (all components)**'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**NMF输出：784个组件或d = 784（所有组件）**'
- en: '![](../Images/63e27a47d157922b94d3d8dab6489bfc.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63e27a47d157922b94d3d8dab6489bfc.png)'
- en: '**NMF output: 784 components or d = 784** (Image by author)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**NMF输出：784个组件或d = 784**（图片由作者提供）'
- en: The reconstruction error is 23349.67.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 重建误差为23349.67。
- en: Conclusions
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: When the number of components is increased when running non-negative matrix
    factorization (NMF), the images are getting clear and the reconstruction error
    is getting low.
  id: totrans-96
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当运行非负矩阵分解（NMF）时，组件数量增加，图像变得更清晰，重建误差变得更低。
- en: By looking at the output and reconstruction error, a right value for ***d***
    can be chosen. For this, you need to run the NMF algorithm a few times which may
    be time-consuming depending on the computer resources you have.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看输出和重建误差，可以选择一个合适的***d***值。为此，你需要多次运行NMF算法，这可能会根据你计算机的资源而耗时。
- en: With d = 784 (all components), you will ***still*** get a reconstruction error
    of 23349.67 instead of zero.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 使用d = 784（所有组件），你***仍然***会得到23349.67的重建误差，而不是零。
- en: It is obvious that the product of W and H matrices only gives a non-negative
    matrix approximation to the feature matrix, V.
  id: totrans-99
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 显然，W和H矩阵的乘积仅仅给出了特征矩阵V的非负矩阵近似。
- en: '***Can we run NMF with a negative matrix?***'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '***我们能在负矩阵上运行NMF吗？***'
- en: The answer is ***no***. If you try to use NMF with a feature matrix that has
    negative values, you will get the following **ValueError!**
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是***不***。如果你尝试使用含有负值的特征矩阵进行NMF，你将得到以下**ValueError!**
- en: '[PRE14]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](../Images/578974f3013d9327d3dbcbdc476501b9.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/578974f3013d9327d3dbcbdc476501b9.png)'
- en: '**ValueError!** (Image by author)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**ValueError!**（图片由作者提供）'
- en: You can’t break the non-negativity constraint when running non-negative matrix
    factorization (NMF). The feature matrix should always contain non-negative elements.
  id: totrans-105
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在运行非负矩阵分解（NMF）时，不能打破非负性约束。特征矩阵应始终包含非负元素。
- en: This is the end of today’s article.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这是今天文章的结束。
- en: '**Please let me know if you’ve any questions or feedback.**'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果你有任何问题或反馈，请告诉我。**'
- en: Other matrix decomposition methods you might be interested in
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你可能感兴趣的其他矩阵分解方法
- en: '[**Eigendecomposition**](https://medium.com/data-science-365/eigendecomposition-of-a-covariance-matrix-with-numpy-c953334c965d)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**特征分解**](https://medium.com/data-science-365/eigendecomposition-of-a-covariance-matrix-with-numpy-c953334c965d)'
- en: '[**Singular value decomposition**](/singular-value-decomposition-vs-eigendecomposition-for-dimensionality-reduction-fc0d9ac24a8e)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**奇异值分解**](/singular-value-decomposition-vs-eigendecomposition-for-dimensionality-reduction-fc0d9ac24a8e)'
- en: Read next (Recommended)
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 阅读下一篇（推荐）
- en: '[**PCA and Dimensionality Reduction Special Collection**](https://rukshanpramoditha.medium.com/list/pca-and-dimensionality-reduction-special-collection-146045a5acb5)'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**PCA与降维专题合集**](https://rukshanpramoditha.medium.com/list/pca-and-dimensionality-reduction-special-collection-146045a5acb5)'
- en: '[**How RGB and Grayscale Images Are Represented in NumPy Arrays**](/exploring-the-mnist-digits-dataset-7ff62631766a)'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**RGB与灰度图像在NumPy数组中的表示**](/exploring-the-mnist-digits-dataset-7ff62631766a)'
- en: '[**Acquire, Understand and Prepare the MNIST Dataset**](https://medium.com/data-science-365/acquire-understand-and-prepare-the-mnist-dataset-3d71a84e07e7)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**获取、理解与准备MNIST数据集**](https://medium.com/data-science-365/acquire-understand-and-prepare-the-mnist-dataset-3d71a84e07e7)'
- en: How about an AI course?
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 来一门AI课程怎么样？
- en: '[**Neural Networks and Deep Learning Course**](https://rukshanpramoditha.medium.com/list/neural-networks-and-deep-learning-course-a2779b9c3f75)'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**神经网络与深度学习课程**](https://rukshanpramoditha.medium.com/list/neural-networks-and-deep-learning-course-a2779b9c3f75)'
- en: Join my private list of emails
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入我的私人邮件列表
- en: '*Never miss a great story from me again. By* [***subscribing to my email list***](https://rukshanpramoditha.medium.com/subscribe)*,
    you will directly receive my stories as soon as I publish them.*'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*永远不要再错过我的精彩故事。通过* [***订阅我的邮件列表***](https://rukshanpramoditha.medium.com/subscribe)*，你将直接收到我发布的故事。*'
- en: Thank you so much for your continuous support! See you in the next article.
    Happy learning to everyone!
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢你们的持续支持！下篇文章见。祝大家学习愉快！
- en: MNIST dataset info
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MNIST数据集信息
- en: '**Citation:** Deng, L., 2012\. The mnist database of handwritten digit images
    for machine learning research. **IEEE Signal Processing Magazine**, 29(6), pp.
    141–142.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**引用：** Deng, L., 2012\. The mnist database of handwritten digit images for
    machine learning research. **IEEE Signal Processing Magazine**, 29(6), pp. 141–142.'
- en: '**Source:** [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**来源：** [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)'
- en: '**License:** *Yann LeCun* (Courant Institute, NYU) and *Corinna Cortes* (Google
    Labs, New York) hold the copyright of the MNIST dataset which is available under
    the *Creative Commons Attribution-ShareAlike 4.0 International License* ([**CC
    BY-SA**](https://creativecommons.org/licenses/by-sa/4.0/)). You can learn more
    about different dataset license types [here](https://rukshanpramoditha.medium.com/dataset-and-software-license-types-you-need-to-consider-d20965ca43dc#6ade).'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**许可证：** *Yann LeCun*（纽约大学Courant研究所）和*Corinna Cortes*（谷歌实验室，纽约）持有MNIST数据集的版权，该数据集在*Creative
    Commons Attribution-ShareAlike 4.0 International License*（[**CC BY-SA**](https://creativecommons.org/licenses/by-sa/4.0/)）下提供。你可以在[这里](https://rukshanpramoditha.medium.com/dataset-and-software-license-types-you-need-to-consider-d20965ca43dc#6ade)了解更多不同的数据集许可证类型。'
- en: '**Designed and written by:**'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**设计与撰写：**'
- en: '[Rukshan Pramoditha](https://medium.com/u/f90a3bb1d400?source=post_page-----8450f4cae8fa--------------------------------)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[Rukshan Pramoditha](https://medium.com/u/f90a3bb1d400?source=post_page-----8450f4cae8fa--------------------------------)'
- en: '**2023–05–06**'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**2023–05–06**'
