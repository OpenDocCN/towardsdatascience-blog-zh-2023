# 通过随机森林找到我们的路

> 原文：[https://towardsdatascience.com/finding-our-way-through-a-random-forest-5ff6c1382572](https://towardsdatascience.com/finding-our-way-through-a-random-forest-5ff6c1382572)

## 或者在一个被僵尸困扰的假想世界里，决策树如何能决定你是否能够脱身

[](https://medium.com/@manfred.james?source=post_page-----5ff6c1382572--------------------------------)[![Diego Manfre](../Images/2189d8e63df449a869526bf8b6c50440.png)](https://medium.com/@manfred.james?source=post_page-----5ff6c1382572--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5ff6c1382572--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5ff6c1382572--------------------------------) [Diego Manfre](https://medium.com/@manfred.james?source=post_page-----5ff6c1382572--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5ff6c1382572--------------------------------) ·阅读时长17分钟·2023年4月19日

--

![](../Images/42de49ee660a66ca0f4cef619e194d77.png)

图片由作者使用 Midjourney 制作

*车库外面，咆哮和咆哮声不断。他简直不敢相信自己在系列剧和电影中多次看到的僵尸末日最终出现在了自己门前。他可以在车库里藏一段时间，但最终还是得出来。他该带斧头还是仅用步枪就够了？他可以试着找些食物，但应该一个人去吗？他试图回忆起所有看过的僵尸电影，却无法达成一致的策略。如果他能记住每个角色被僵尸杀死的场景，这是否能增加他的生存几率？如果他有一个决策指南，一切都会更简单……*

# 介绍

你是否看过那些僵尸末日电影，其中总有一个角色似乎总是知道僵尸藏在哪里，或者知道是该战斗还是逃跑？这个人真的知道接下来会发生什么吗？有人事先告诉过他/她吗？也许这并没有什么神奇的地方。也许这个人读过很多关于僵尸的漫画，并且非常擅长根据每种情况做出正确的决策，学习别人的错误。找到使用过去事件作为我们决策指南的最佳方式是多么重要！这个指南，也称为决策树，是一种广泛使用的监督学习算法。本文是关于决策树的入门讨论，讲述了如何构建它们以及为什么许多决策树会创建一个随机森林。

# 一个简单的决策

你处在僵尸混乱之中，你想知道如何提高生存的机会。在这一点上，你只有15个朋友的信息。你知道每一个朋友是否孤身一人，是否有车辆或武器，或者是否经过战斗训练。最重要的是，你知道他们是否能够生存下来。你如何利用这些信息来为自己争取优势？

表1总结了你15个朋友的结果和特征。你想成为最终幸存的那3个人中的一员。这3个朋友有什么共同点？简单检查表格会告诉我们这三位幸存者有这些共同点：他们并不孤单，他们经过了战斗训练，他们有车辆和武器。那么，如果你拥有这四样东西，你能幸存吗？过去的经验告诉我们，你有可能！如果你必须决定带上什么以及是否独自一人，至少现在你有一些历史数据来支持你的决定。

![](../Images/e946a0470d4ed9093000bc63ba4b35cc.png)

*表1\. 15个个体的特征及其在僵尸末日中的最终结果示例1（表由作者制作）。*

# 更困难的决定

僵尸末日永远不像它看起来那么简单。假设这次你有以下这些朋友，而不是前面例子中的15个朋友：

![](../Images/1d866c4ed9a2ae935240193408c2876b.png)

表2\. 15个个体的特征及其在僵尸末日中的最终结果示例2 *(表由作者制作)。*

这一次，仅通过视觉检查得出结论并不那么简单。我们唯一可以确定的是，如果你想要生存，最好身边有一个人。幸存的5个人并不是孤身一人（图1）。除此之外，很难看出是否存在某种特定的组合可以让你存活下来。有些人虽然孤身一人仍然能够生存。他们是怎么做到的？如果你知道自己会孤身一人，还有什么可以做的来增加生存的机会？是否存在类似决策路线图的东西？

![](../Images/c9c5ff10bafc7a9f30f9053c72a669a1.png)

图1\. 各特征对示例2中15个人群体最终结果的影响 *(图由作者制作)。*

# 决策树

我们可以在[决策树](https://en.wikipedia.org/wiki/Decision_tree_learning)中找到一些对前面问题的答案。决策树是一个根据我们做出的决定来预测结果的模型。这个模型是通过之前的经验构建的。在我们的例子中，我们可以利用15个朋友的特征和他们的结果来构建决策树。决策树由多个决策节点或分支组成。在这些节点中，我们做出一个决策，决策将引导我们到下一个节点，直到我们得到一个结果。

## 构建决策树

如果有人让你画一个家谱，你可能会从你的祖父母或曾祖父母开始。从那里，家谱将通过你的父母、叔叔和表亲逐渐展开，直到到达你。类似地，要构建决策树，你总是从一个能够最好地分离数据的节点开始。从这一点开始，树将根据最佳分割数据的特征开始生长。有许多[算法](https://en.wikipedia.org/wiki/Decision_tree_learning#Metrics)可以用来构建决策树。本文解释了如何使用[信息增益](https://en.wikipedia.org/wiki/Information_gain_in_decision_trees)和[香农熵](https://en.wikipedia.org/wiki/Entropy_(information_theory))。

让我们关注表2\. 我们可以看到有 5 人幸存，10 人死亡。这意味着幸存的概率是 5/15 = ⅓，死亡的概率是 ⅔。利用这些信息，我们可以计算这种分布的熵。在这种情况下，熵指的是这种分布的平均惊讶或不确定性水平。为了计算熵，我们使用以下方程：

![](../Images/078fa06d576037b251bd37f05462f00f.png)

注意，这个方程也可以用其中一个概率来表示，因为 *p(surv)*+*p(die)*=1。如果我们绘制这个函数，你会看到当 *p(surv)* 和 *p(die)* 都等于 0.5 时，熵的值达到 1 的最高值。相反，如果整个分布对应于所有人都生存或所有人都死亡的情况，则熵为零。因此，熵越高，*不确定性*越高。熵越低，分布越均匀，我们对结果的*惊讶*程度就越小。

![](../Images/454fc1395d6c1b5c4bb4519ab9fb3e8d.png)

图2\. 熵作为每个事件概率的函数的图示。绿色曲线代表死亡的概率，橙色曲线代表生存的概率，蓝色曲线是这两种概率的总和，因为 p(surv)+p(die)=1 *(由作者制作的图像)。*

在我们的案例中，幸存者的数量不到总人口的一半。合理的推测是大多数人没有幸存于僵尸末日。在这种情况下，熵为 0.92，这就是你在图2的蓝色曲线中搜索 x=⅓ 或 ⅔ 时得到的值，或者当你应用以下方程时得到的值：

![](../Images/fcb683d3e34c60d007b7423cc2c49769.png)

现在我们知道了整个分布的熵或不确定性的程度，我们应该怎么做？下一步是找到如何划分数据，以保持这种不确定性水平。

信息增益的前提是选择能够最小化前一节点熵水平的决策节点。在这个阶段，我们正试图找出数据的最佳首次分离点。是我们单独存在、知道如何战斗，还是拥有交通工具或武器？为了知道答案，我们可以计算每个选择的信息增益，然后决定哪一个具有最大的增益。记住，我们试图最小化熵的变化，即结果分布中的异质性或惊讶程度。

## 你受过战斗训练吗？

这是你在这种情况下应该问自己的第一个问题吗？这个问题会最小化结果分布中熵的变化吗？为了知道这一点，让我们计算这两个案例的熵：我们知道如何战斗和我们不知道如何战斗。图3显示，在9名知道如何战斗的人中，只有5名幸存了。相反，所有6名没有接受战斗训练的人都没有生存下来。

![](../Images/2068323863de0c8549517166433366e1.png)

图3\. 根据战斗能力的示例2结果（*图像由作者制作*）。

要计算之前案例的熵，我们可以应用之前使用的相同公式。图3显示，经过战斗训练的情况下，熵为0.99，而在其他情况下，熵为零。请记住，熵为零意味着没有惊讶，分布均匀，这实际上是因为所有没有接受战斗训练的人都没有生存下来。在此阶段，重要的是要注意，在第二种情况下的熵计算包含一个未定义的计算，因为我们最终会得到一个零的对数。在这些情况下，你可以始终应用[洛必达法则](https://en.wikipedia.org/wiki/L%27H%C3%B4pital%27s_rule)，正如在这篇[文章](https://sefiks.com/2018/08/25/indeterminate-forms-and-lhospitals-rule-in-decision-trees/)中所解释的那样。

我们现在需要计算这个决策的信息增益。这与问如果我决定根据这个问题划分所有结果，所有决策的不确定性会改变多少是一样的。信息增益通过从主要节点的熵中减去每个决策的熵来计算。一个重要的事情是注意到，这个操作根据每个决策的个体数量加权。因此，即使熵很大，如果做出该决策的人数量较少，也可能对信息增益计算的影响较小。对于这个例子，战斗能力的信息增益为0.32，如图3所示。

## 你能单独幸存下来吗？

我们可以对单独生存或与他人一起生存的可能性进行类似的分析。图4展示了计算过程。在这种情况下，信息增益为0.52。注意到在这种情况下，单独一人从未导致生存，而在不孤单的情况下，7个案例中有5个幸存下来。

![](../Images/cc2775684cc573d759716b0ad3129970.png)

图4\. 根据是否独自一人，示例2的结果*（图像由作者制作）。*

## 拥有车辆或武器如何？

对于这两种情况，我们可以像之前一样计算信息增益（图5）。你可以看到这些信息增益比之前计算的要小。这意味着，此时根据前两个特征来划分数据比按照这两个特征划分要更好。记住，最大的增益对应于熵减少最小的特征。一旦我们计算了所有特征的信息增益，就可以决定决策树的第一个节点是什么。

![](../Images/3abf9611a00f9fe133e37fa18b493716.png)

图4\. 根据是否拥有车辆或武器，示例2的结果*（图像由作者制作）。*

## 第一个节点

表3显示了每个特征的信息增益。最大的增益对应于是否独自一人或有伴侣。这一节点将我们带到决策树中的第一个决策：你将无法单独生存。8个独自一人的人无论是否有武器、汽车或接受过战斗训练都无法生存。因此，这是我们从分析中可以得出的第一个结论，这也支持了我们仅通过检查数据得出的结论。

![](../Images/46786788bacafa94bc6ef937a9a6398f.png)

表3\. 第一个节点分析后每个特征的信息增益*（表格由作者制作）。*

此时，决策树的样子如图5所示。我们知道如果我们独自一人是没有生存的可能性（考虑到我们拥有的数据）。如果我们不独自一人，那么我们可能会生存下来，但并非所有情况下都如此。由于我们可以计算图5中右侧节点的熵，即0.86（该计算过程在图4中展示），我们也可以计算其他三个特征的信息增益，并决定下一个决策节点是什么。

![](../Images/9119add05e8c42cfc5e8b35438f29889.png)

图5\. 第一个决策节点后的决策树*（图像由作者制作）。*

## 第二个节点

图5显示，此时最大的增益来自于武器特征，因此这是下一个决策节点，如图6所示。注意到所有未独自一人且拥有武器的人都幸存下来，这就是为什么武器节点的左侧以生存决策结束。

![](../Images/eec0d3452a656a0a25ac9ab861f9663e.png)

图6\. 第二个决策节点后的决策树*（图像由作者制作）。*

## 树是完整的

还有3个人没有单独存在且没有武器，我们需要对他们进行分类。如果我们遵循之前解释的相同过程，我们会发现下一个信息增益最大的特征是车辆。因此，我们可以在树上添加一个额外的节点，询问某个人是否拥有车辆。这将把剩下的3个人分成一个有2人拥有车辆但未幸存的组和一个没有车辆但幸存的单独个体。最终的决策树如图7所示。

![](../Images/9bdac9825f161f8090edf8ec1ebfb3a1.png)

图7\. 示例2的最终决策树*(作者制作的图像)。*

## 决策树的问题

如你所见，决策树是一个基于以往经验构建的模型。根据数据中的特征数量，你会遇到多个问题，这些问题将引导你到最终答案。值得注意的是，在这种情况下，某些特征在决策树中并未体现。战斗能力从未被选择为决策节点，因为其他特征总是有更大的信息增益。这意味着，根据输入数据，训练战斗能力对于在僵尸 apocalypse 中生存并不重要。然而，这也可能意味着我们没有足够的样本来确定战斗能力是否重要。关键在于记住，决策树的好坏取决于我们用于构建它的输入数据。在这种情况下，15个人的样本可能不足以对战斗训练的重要性进行准确估计。这是决策树的一个问题。

与其他监督学习方法一样，决策树并不完美。一方面，它们非常依赖输入数据。这意味着输入数据的微小变化可能会导致最终树的重要变化。决策树并不擅长于概括。另一方面，它们往往存在过拟合问题。换句话说，我们可能会得到一个与输入数据完全匹配但在测试集上表现极差的复杂决策树。如果我们使用带有连续变量的决策树而非像示例中那样的分类变量，这也可能影响结果。

提高决策树效率的一种方法是修剪它们。这意味着在达到纯节点之前停止算法，就像我们在示例中所达到的那样。这可能导致删除那些对决策树准确性没有改善的分支。修剪赋予决策树更多的概括能力。然而，如果我们决定修剪决策树，那么我们可能会开始提出额外的问题，比如：何时是停止算法的正确时机？我们应该在达到最小样本数后停止？还是在预定义的节点数后停止？如何确定这些数字？修剪确实可以帮助我们避免过拟合，但它可能带来一些不易回答的额外问题。

# 那么，如何看待整个森林而不是单棵树呢？

如果我们不是使用单一的决策树，而是多个决策树呢？它们将根据所处理的输入数据部分、读取的特征以及修剪特性而变化。最终我们将得到许多决策树和不同的答案，但在分类任务中我们可以选择多数结果，或在回归任务中取平均。这可以帮助我们更好地概括数据的分布。我们可能会认为某棵决策树存在误分类，但如果我们找到10或20棵树得出相同的结论，那么这可能表明实际上没有误分类。基本上，我们是让多数决定，而不是依赖单棵决策树。这种方法称为[随机森林](https://en.wikipedia.org/wiki/Random_forest)。

随机森林的概念通常与[袋装法](https://www.ibm.com/topics/bagging)相关，袋装法是一个从训练集中随机抽取数据样本的过程，并且有放回地选择。这意味着个别数据点可以被选择多次。在随机森林方法中，我们可以选择一个随机数量的点，建立一棵决策树，然后重复这一过程直到得到多棵树。最后的决策将来自所有树的答案。

随机森林是一种著名的集成方法，用于分类和回归问题。该方法已在金融、医疗保健和电子商务等多个行业中应用[1]。虽然随机森林的原始思想是由许多研究人员逐步发展的，但莱奥·布雷曼通常被认为是这一方法的创始人[2]。他的[个人网页](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm)中包含了随机森林的详细描述和其工作原理的广泛解释。这是篇较长但值得阅读的文章。

关于随机森林，一个重要的理解点是它如何处理数据集的特征。在每个节点，随机森林会随机选择一个预定义的特征数量，而不是所有特征，来决定如何分裂每个节点。记住在之前的例子中，我们分析了决策树每个层级的每个特征的信息增益。相反，随机森林会在每个节点只分析特征子集的信息增益。因此，随机森林将Bagging与每个节点的随机变量选择混合在一起。

# 一个真正的僵尸末日

回到僵尸的问题！之前的例子非常简单，我们有15个人的数据，每个人只有4个特征。让我们把难度加大！假设现在我们有一个超过一千条记录的数据集，每条记录有10个特征。这个数据集是在Excel中随机生成的，不属于任何商业或私人库，你可以从这个[GitHub页面](https://github.com/manfrezord/MediumArticles/tree/main/RF)访问它。

对于这些类型的方法论，通常建议将整个数据集分成训练集和测试集。我们将使用训练集来构建决策树和随机森林模型，然后使用测试集来评估它们。为此，我们将使用[scikit-learn库](https://scikit-learn.org/stable/)。这个[Jupyter Notebook](https://github.com/manfrezord/MediumArticles/blob/main/RF/Random_Forests_Zombie_Apocalypse.ipynb)包含了数据集的详细说明、如何加载数据以及如何使用该库构建模型。

整个数据集包含1024条记录，其中212条（21%）对应幸存者，812条（79%）对应死亡。我们将数据集分为一个训练集，占数据的80%（819条记录），以及一个包含205条记录的测试集。图8显示了在所有数据集中幸存者与死亡者之间的关系是如何保持的。

![](../Images/cb94acf47c63441fd40234e370b55774.png)

图8. 示例3中数据集的训练集和测试集分离*(图片由作者制作)。*

关于特征，这次我们为每个个体增加了6个额外的特征：

+   你有收音机吗？

+   你有食物吗？

+   你上过户外生存课程吗？

+   你上过急救课程吗？

+   你以前遇到过僵尸吗？

+   你有GPS吗？

这 6 个特征加上我们已有的 4 个特征，共同代表了每个个体或条目的 10 个不同特征。有了这些信息，我们可以按照之前解释的步骤构建决策树。[Jupyter Notebook](https://github.com/manfrezord/MediumArticles/blob/main/RF/Random_Forests_Zombie_Apocalypse.ipynb) 使用 DecisionTreeClassifier 函数生成决策树。请注意，此函数不适用于分类变量。在这种情况下，我们已将每个类别的所有答案转换为 -1 或 +1。这意味着每次我们在结果中看到 -1 时，表示“否”，而 +1 表示“是”。这一点在[Jupyter Notebook](https://github.com/manfrezord/MediumArticles/blob/main/RF/Random_Forests_Zombie_Apocalypse.ipynb)中有更详细的说明。

[笔记本](https://github.com/manfrezord/MediumArticles/blob/main/RF/Random_Forests_Zombie_Apocalypse.ipynb) 解释了如何加载数据、调用决策树函数并绘制结果。图 9 显示了使用 819 个训练集条目构建的决策树（点击[这里](https://raw.githubusercontent.com/manfrezord/MediumArticles/main/RF/Decision_Tree_Big.png)查看更大图）。深蓝色框对应于最终的决策节点，其中答案为幸存，而深橙色框表示最终答案为未幸存。你可以看到，第一个决策节点对应于车辆，从那里开始，树根据不同的特征开始生长。

![](../Images/bcca9dc17f11ea61f069296f72acd20a.png)

图 9\. 示例 3 的训练集决策树（[这里](https://raw.githubusercontent.com/manfrezord/MediumArticles/main/RF/Decision_Tree_Big.png)是作者制作的可缩放版本）。

我们可以评估这棵树的好坏，如果我们使用测试集输入预测最终类别，然后将这些结果与原始结果进行比较。表 4 显示了一个[混淆矩阵](https://en.wikipedia.org/wiki/Confusion_matrix)，其中列出了决策树将条目误分类的次数。我们可以看到，测试集中有 40 个代表幸存的案例，而决策树仅正确分类了其中的 25 个。另一方面，在 165 个未幸存的案例中，决策树误分类了 11 个。正确分类与 205 个点的整个数据集的关系为 0.87，这通常被称为预测准确率得分。

![](../Images/73c39df49bcc4ee87e3beea94ed94313.png)

表 4\. 决策树运行后的测试数据集混淆矩阵*（图片由作者制作）。*

87%的准确率看起来还不错，但我们能通过随机森林提高这个结果吗？下一部分的[Jupyter Notebook](https://github.com/manfrezord/MediumArticles/blob/main/RF/Random_Forests_Zombie_Apocalypse.ipynb)包含了使用sklearn函数RandomForestClassifier实现的随机森林。这棵随机森林将包含10棵决策树，这些决策树使用所有条目进行构建，但每次分裂时只考虑3个特征。随机森林中的每棵决策树考虑682个条目，这些条目代表了完整训练集的84%。因此，明确来说，随机森林过程将：

1.  从训练集中随机抽取682个条目

1.  构建一个决策树，在每个节点考虑3个随机选择的特征

1.  重复上述步骤9次

1.  预测将对应于10棵决策树中的多数投票

表 5 显示了来自随机森林的结果的混淆矩阵。我们可以看到，这些结果比我们之前用单棵决策树得到的要好。这个随机森林误分类了11个条目，预测准确率为0.95，高于决策树。

![](../Images/ea782a068d65ae2e0ff9bcbc7eaf38f3.png)

表 5\. 运行随机森林后测试数据集的混淆矩阵 *(图像由作者制作)。*

需要注意的是，随机森林方法不仅取决于我们拥有的输入数据的质量，还取决于我们使用的参数选择。我们构建的决策树数量以及每次分裂时分析的参数数量会对最终结果产生重要影响。因此，与许多其他监督学习算法一样，有必要花时间调整参数，直到找到最佳结果。

# 结论

阅读这篇文章就像电影里那个成功逃脱追赶他的僵尸的家伙，因为一个树枝恰好掉在了僵尸的头上！这并不是他遇到的唯一僵尸，他显然还没有脱离困境！关于随机森林和决策树的许多内容在这篇文章中并未提及。然而，了解这种方法的使用和适用性已经足够。目前，有多个库和程序能够在几秒钟内构建这些模型。所以你可能不需要再重新计算熵和信息增益。不过，理解幕后发生的事情以及如何正确解读结果仍然很重要。在“机器学习”、“集成方法”和“数据分析”等话题日益普及的今天，清楚了解这些方法及其应用于日常问题是很重要的。与僵尸末日求生电影不同，准备好不是偶然发生的。

# 参考文献

1.  IBM. [什么是随机森林？](https://www.ibm.com/topics/random-forest)

1.  Louppe, Gilles (2014). 《理解随机森林》。博士论文。列日大学
