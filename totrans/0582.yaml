- en: Cracking Open the OpenAI (Python) API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/cracking-open-the-openai-python-api-230e4cae7971](https://towardsdatascience.com/cracking-open-the-openai-python-api-230e4cae7971)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**A complete beginner-friendly introduction with example code**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://shawhin.medium.com/?source=post_page-----230e4cae7971--------------------------------)[![Shaw
    Talebi](../Images/1449cc7c08890e2078f9e5d07897e3df.png)](https://shawhin.medium.com/?source=post_page-----230e4cae7971--------------------------------)[](https://towardsdatascience.com/?source=post_page-----230e4cae7971--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----230e4cae7971--------------------------------)
    [Shaw Talebi](https://shawhin.medium.com/?source=post_page-----230e4cae7971--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----230e4cae7971--------------------------------)
    ·12 min read·Jul 21, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2c856f26ccf574ede1e5d700f75668e9.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Martin Sanchez](https://unsplash.com/@martinsanchez?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: This is the 2nd article in a [series](/a-practical-introduction-to-llms-65194dda1148)
    on using Large Language Models (LLMs) in practice. Here I present a beginner-friendly
    introduction to the OpenAI API. This allows you to go beyond restrictive chat
    interfaces like ChatGPT and to get more out of LLMs for your unique use cases.
    Python example code is provided below and at the [GitHub repository](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/openai-api).
  prefs: []
  type: TYPE_NORMAL
- en: 'Table of Contents:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What’s an API?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: OpenAI’s (Python) API
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Getting Started (4 Steps)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Example Code
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the [first article](/a-practical-introduction-to-llms-65194dda1148) of this
    series, I described [**Prompt Engineering**](/cracking-open-the-openai-python-api-230e4cae7971)
    as the **most accessible way to use LLMs** in practice. The easiest (and most
    popular) way to do this is via tools like ChatGPT, which provide an intuitive,
    no-cost, and no-code way to interact with an LLM.
  prefs: []
  type: TYPE_NORMAL
- en: '[](/a-practical-introduction-to-llms-65194dda1148?source=post_page-----230e4cae7971--------------------------------)
    [## A Practical Introduction to LLMs'
  prefs: []
  type: TYPE_NORMAL
- en: 3 levels of using LLMs in practice
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/a-practical-introduction-to-llms-65194dda1148?source=post_page-----230e4cae7971--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: However, this **ease of use comes at a cost**. Namely, the chat UI is restrictive
    and does not translate well to many practical use cases e.g. building your own
    customer support bot, real-time sentiment analysis of customer reviews, etc.
  prefs: []
  type: TYPE_NORMAL
- en: In these cases, we can take Prompt Engineering one step further and interact
    with LLMs *programmatically*. One way we can do this is via an API.
  prefs: []
  type: TYPE_NORMAL
- en: '**1) What’s an API?**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An **application programming interface (API)** allows you to interact with a
    remote application programmatically. While this might sound technical and scary,
    the idea is super simple. Consider the following analogy.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you have an intense craving for the [pupusas](https://en.wikipedia.org/wiki/Pupusa)
    you ate during that summer in El Salvador. Unfortunately, you’re back at home
    and don’t know where to find good Salvadoran food. Lucky for you, however, you
    have a super-foodie friend that knows every restaurant in town.
  prefs: []
  type: TYPE_NORMAL
- en: So, you send your friend the text.
  prefs: []
  type: TYPE_NORMAL
- en: “Any good pupusa spots in town?”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Then, a couple of minutes later, you get the response.
  prefs: []
  type: TYPE_NORMAL
- en: “Yes! Flavors of El Salvador has the best pupusas!”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: While this may seem irrelevant to APIs, this is essentially how they work. You
    send a **request** to a remote application i.e. text your super-foodie friend.
    Then, the remote application sends back a **response** i.e. the text back from
    your friend.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d55d67b38e544c96a386e920f92da133.png)'
  prefs: []
  type: TYPE_IMG
- en: A visual analogy of how APIs work. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: The difference between an API and the above analogy is instead of sending the
    request with your phone’s texting app, you use your favorite programming language
    e.g. Python, JavaScript, Ruby, Java, etc. This is great if you are developing
    software where some external information is required because the information retrieval
    can be automated.
  prefs: []
  type: TYPE_NORMAL
- en: '**2) OpenAI’s (Python) API**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can use APIs to interact with Large Language Models. A popular one is OpenAI’s
    API, where instead of typing prompts into the ChatGPT web interface, you can send
    them to and from OpenAI using Python.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/49422df67ac7d25c8846a1ee770af4ad.png)'
  prefs: []
  type: TYPE_IMG
- en: Visualization of how API calls to OpenAI works. Image by author.
  prefs: []
  type: TYPE_NORMAL
- en: This gives virtually anyone access to state-of-the-art LLMs (and other ML models)
    without having to provision the computational resources needed to run them. The
    downside, of course, is OpenAI doesn’t do this as a charity. Each API call costs
    money, but more on that in a bit.
  prefs: []
  type: TYPE_NORMAL
- en: Some **notable features** of the API (not available with ChatGPT) are listed
    below.
  prefs: []
  type: TYPE_NORMAL
- en: '**Customizable system message** (this is set to something like “*I am ChatGPT,
    a large language model trained by OpenAI, based on the GPT-3.5 architecture. My
    knowledge is based on information available up until September 2021\. Today’s
    date is July 13, 2023.*” for ChatGPT)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adjust input parameters** such as maximum response length, number of responses,
    and temperature (i.e. the “randomness” of the response).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Include **images** and **other file types** in prompts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extract helpful word **embeddings** for downstream tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Input audio** for transcription or translation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model **fine-tuning** functionality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The OpenAI API has [several models](https://platform.openai.com/docs/models)
    from which to choose. The *best* model to pick will depend on your particular
    use case. Below is a list of the current models available [[1](https://platform.openai.com/docs/models)].
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/98ce3333b2824ace053715e0867ed296.png)'
  prefs: []
  type: TYPE_IMG
- en: List of available models via the OpenAI API as of Jul 2023\. Image by author.
    [[1](https://platform.openai.com/docs/models)]
  prefs: []
  type: TYPE_NORMAL
- en: '***Note****: Each item listed above is accompanied by a set of models which
    vary in size and cost. Check* [*documentation*](https://platform.openai.com/docs/models)
    *for the most recent information.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pricing & Tokens**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the OpenAI API gives developers easy access to SOTA ML models, one obvious
    downside is that it **costs money**. Pricing is done on a per-token basis (no,
    I don’t mean NFTs or something you use at the arcade).
  prefs: []
  type: TYPE_NORMAL
- en: '**Tokens**, in the context of LLMs, are essentially **a set of numbers representing
    a set of words and characters**. For example, “The” could be a token, “ end” (with
    the space) could be another, and “.” another.'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the text “The End.” would consist of 3 tokens say (73, 102, 6).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1f13c26b8c5b663dd2903f54a9249d13.png)'
  prefs: []
  type: TYPE_IMG
- en: Toy example showing one possible token mapping between text and integers. Image
    by author.
  prefs: []
  type: TYPE_NORMAL
- en: This is a critical step because **LLMs (i.e. neural networks) do not “understand”
    text directly**. The text must be converted into a numerical representation so
    that the model can perform mathematical operations on the input. Hence, the tokenization
    step.
  prefs: []
  type: TYPE_NORMAL
- en: The price of an API call depends on the number of tokens used in the prompt
    and the model being prompted. The price per model is available on [OpenAI’s website](https://openai.com/pricing).
  prefs: []
  type: TYPE_NORMAL
- en: '**3) Getting Started (4 Steps)**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a basic understanding of the OpenAI API let’s see how to use
    it. Before we can start coding, we need to set up four things.
  prefs: []
  type: TYPE_NORMAL
- en: '**3.1) Make an Account (you get a $5 API credit for 1st three months)**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To make an account go to the [OpenAI API Overview page](https://platform.openai.com/overview),
    and click “Sign Up” in the top right corner
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Note* — If you’ve used ChatGPT before, then you probably already have an OpenAI
    account. If so, click “Log in”'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**3.2) Add Payment Method**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If your account is more than 3 months old or the free $5 API credit is not enough
    for you, you will need to add a payment method before making API calls.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click your profile image and select the manage account option.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then add a payment method by clicking the “Billing” tab and then “Payment methods”.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**3.3) Set Usage Limits**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, I recommend setting usage limits so that you **avoid being billed more
    than you budget for**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To do this, go to the “Usage limits” under the “Billing” tab. Here you can set
    a “Soft” and “Hard” limit.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you hit your monthly **soft limit,** OpenAI will send you an **email notification**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you hit your **hard limit,** any additional API **requests will be denied**
    (thus, you won’t be charged more than this).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**3.4) Get API Secret Key**'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Click on “View API keys”
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If this is your first time, you will need to make a new secret key. To do this,
    click “Create new secret key”
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, you can give your key a custom name. Here I used “my-first-key”.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, click “Create secret key”
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**4) Example Code: Chat Completion API**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With all the setup done, we are (finally) ready to make our first API call.
    Here we will use the [openai Python library](https://github.com/openai/openai-python),
    which makes integrating OpenAI’s models into your Python code super easy. You
    can download the package via [pip](https://pypi.org/project/openai/)*.* The below
    example code (and bonus code) is available on the [GitHub repo](https://github.com/ShawhinT/YouTube-Blog/tree/main/LLMs/openai-api)
    for this article.
  prefs: []
  type: TYPE_NORMAL
- en: '***A quick note on Completions API deprecations* —** OpenAI is moving away
    from the freeform prompt paradigm and toward chat-based API calls. According to
    a blog from OpenAI, the chat-based paradigm provides better responses, given its
    structured prompt interface, compared to the previous paradigm [[2](https://openai.com/blog/gpt-4-api-general-availability)].'
  prefs: []
  type: TYPE_NORMAL
- en: While older OpenAI (GPT-3) models are still available via the “freeform” paradigm,
    the more recent (and powerful) models (i.e. GPT-3.5-turbo and GPT-4) are only
    available via chat-based calls.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with a super simple API call. Here we will pass **two inputs** into
    the ***openai.ChatCompletions.create()*** method i.e. **model** and **messages**.
  prefs: []
  type: TYPE_NORMAL
- en: '**model** — defines the name of the language model we want to use (we can choose
    from the models listed earlier in the article.)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**messages** — sets the “preceding” chat dialogue as a list of dictionaries.
    The dictionaries have two key-value pairs (e.g. {“role”: “user”, “content”: “Listen
    to your”}.) **First**, “role” defines *who is talking* (e.g. “role”:”user”). This
    can either be the “user”, “assistant”, or “system”. **Second**, “content” defines
    *what the role is saying* (e.g. “content”: “Listen to your”). While this may feel
    more restrictive than a freeform prompt interface, we can get creative with input
    messages to optimize responses for a particular use case (more on this later).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is what our first API call looks like in Python.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The API response is stored in the *chat_completion* variable. Printing *chat_completion*,
    we see that it is like a dictionary consisting of 6 key-value pairs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The meaning of each field is listed below.
  prefs: []
  type: TYPE_NORMAL
- en: '**‘Id’** = unique ID for the API response'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**‘Object’** = name of API object that sent the response'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**‘Created’** = unix timestamp of when the API request was processed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**‘Model’** = name of the model used'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**‘Choices’** = model response formatted in JSON (i.e. dictionary-like)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**‘Usage’** = token count meta-data formatted in JSON (i.e. dictionary-like)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, the main thing we care about here is the ‘**Choices**’ field since
    this is **where the model response is stored**. In this case, we see the “assistant”
    role responds with the message *“****heart.”***
  prefs: []
  type: TYPE_NORMAL
- en: Yay! We made our 1st API call. Now let’s start playing with the model input
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: max_tokens
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we can set the **maximum number of tokens** allowed in the model response
    using the *max_tokens* input parameter. This can be helpful for many reasons depending
    on the use case. In this case, I just want a one-word response, so I’ll set it
    to 1 token.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: n
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we can set the **number of responses** we would like to receive from the
    model. Again, this can be helpful for many reasons depending on the use case.
    For example, if we want to generate a set of responses from which we can select
    the one we like best.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Notice that **not all the completions are identical**. This may be a good thing
    or a bad thing based on the use case (e.g. creative use cases vs. process automation
    use cases). Therefore, it can be advantageous to adjust the *diversity* of chat
    completions for a given prompt.
  prefs: []
  type: TYPE_NORMAL
- en: temperature
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It turns out we can do this by tuning the **temperature** parameter. Put simply,
    this **adjusts the “randomness” of chat completions**.Values for this parameter
    **range from 0 to 2**, where 0 makes completions more predictable, and 2 makes
    them less predictable [[3](https://platform.openai.com/docs/api-reference/chat/create)].
  prefs: []
  type: TYPE_NORMAL
- en: Conceptually, we can think of temp=0 will default to the most likely next word
    while temp=2 will enable completions that are relatively unlikely. Let’s see what
    this looks like.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As expected, when temp=0, all 5 completions are identical and produce something
    “very likely.” Now let’s see what happens when we **turn up the temperature**.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Again, as expected, the chat completions with temp=2 were much more diverse
    and “out of pocket.”
  prefs: []
  type: TYPE_NORMAL
- en: 'messages roles: Lyric Completion Assistant'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, we can leverage the different roles in this chat-based prompting paradigm
    to adjust the language model responses even further.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall from earlier that we can include content from 3 different roles in our
    prompts: **system**, **user**, and **assistant**. The **system** message **sets
    the context (or task) for model completions** *e.g. “You are a friendly chatbot
    that does not want to destroy all humans” or “Summarize user prompts in max 10
    words”.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**User** and **assistant** messages can be used in at least two ways. **One**,
    to generate examples for **in-context learning**, and **two**, to store and update
    **conversation history** for a real-time chatbot. Here we will use both ways to
    create a lyric completion assistant.'
  prefs: []
  type: TYPE_NORMAL
- en: We start by making the **system message** *“I am Roxette lyric completion assistant.
    When given a line from a song, I will provide the next line in the song.”* Then,
    provide **two examples of** **user and assistant messages**. Followed by the same
    **user prompt** used in the preceding examples i.e.*“Listen to your”.*
  prefs: []
  type: TYPE_NORMAL
- en: Here’s what that looks like in code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Comparing the output to the [actual lyrics](https://www.azlyrics.com/lyrics/roxette/listentoyourheart.html)
    to the hit Roxette song, we see they are an exact match. This is due to the combination
    of all the different inputs we provided to the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see what this looks like when we “*crank the temperature*,” check out the
    bonus code on [GitHub](https://github.com/ShawhinT/YouTube-Blog/blob/main/LLMs/openai-api/openai-api-demo.ipynb).
    (Warning: it gets weird)'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here I gave a beginner-friendly guide to the OpenAI Python API with example
    code. The biggest upside of using OpenAI’s API is you can work with powerful LLMs
    without worrying about provisioning computational resources. The **downsides**,
    however, are **API calls cost money** and potential **security concerns** of sharing
    some types of data with a 3rd party (OpenAI).
  prefs: []
  type: TYPE_NORMAL
- en: To avoid these downsides, we can turn to open-source LLM solutions. This will
    be the focus of the [next article](https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)
    in this series, where we’ll explore the Hugging Face Transformers library.
  prefs: []
  type: TYPE_NORMAL
- en: '👉 **More on LLMs**: [Introduction](/a-practical-introduction-to-llms-65194dda1148)
    | [Hugging Face Transformers](https://medium.com/towards-data-science/cracking-open-the-hugging-face-transformers-library-350aa0ef0161)
    | [Prompt Engineering](https://medium.com/towards-data-science/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f)
    | [Fine-tuning](https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91)
    | [Build an LLM](/how-to-build-an-llm-from-scratch-8c477768f1f9) | [QLoRA](/qlora-how-to-fine-tune-an-llm-on-a-single-gpu-4e44d6b5be32)
    | [RAG](https://medium.com/towards-data-science/how-to-improve-llms-with-rag-abdc132f76ac)
    | [Text Embeddings](/text-embeddings-classification-and-semantic-search-8291746220be)'
  prefs: []
  type: TYPE_NORMAL
- en: '![Shaw Talebi](../Images/02eefb458c6eeff7cd29d40c212e3b22.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Shaw Talebi](https://shawhin.medium.com/?source=post_page-----230e4cae7971--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models (LLMs)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[View list](https://shawhin.medium.com/list/large-language-models-llms-8e009ae3054c?source=post_page-----230e4cae7971--------------------------------)13
    stories![](../Images/82e865594c68f5307e75665842d197bb.png)![](../Images/b9436354721f807e0390b5e301be2119.png)![](../Images/59c8db581de77a908457dec8981f3c37.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Connect**: [My website](https://shawhintalebi.com/) | [Book a call](https://calendly.com/shawhintalebi)
    | [Ask me anything](https://shawhintalebi.com/contact/)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Socials**: [YouTube 🎥](https://www.youtube.com/channel/UCa9gErQ9AE5jT2DZLjXBIdA)
    | [LinkedIn](https://www.linkedin.com/in/shawhintalebi/) | [Twitter](https://twitter.com/ShawhinT)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Support**: [Buy me a coffee](https://www.buymeacoffee.com/shawhint) ☕️'
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://shawhin.medium.com/subscribe?source=post_page-----230e4cae7971--------------------------------)
    [## Get FREE access to every new story I write'
  prefs: []
  type: TYPE_NORMAL
- en: Get FREE access to every new story I write P.S. I do not share your email with
    anyone By signing up, you will create a…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: shawhin.medium.com](https://shawhin.medium.com/subscribe?source=post_page-----230e4cae7971--------------------------------)
  prefs: []
  type: TYPE_NORMAL
- en: '[1] [OpenAI Models documentation](https://platform.openai.com/docs/models)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] [GPT-4 Availability & Completions API Deprecation](https://openai.com/blog/gpt-4-api-general-availability)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] Temperature definition from [API reference](https://platform.openai.com/docs/api-reference/chat/create)'
  prefs: []
  type: TYPE_NORMAL
