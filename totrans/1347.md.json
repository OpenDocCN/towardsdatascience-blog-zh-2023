["```py\nimport tensorflow_datasets as tfds\n\ndata = tfds.load(\"movielens/1m-ratings\")\n```", "```py\ndf = tfds.as_dataframe(data[\"train\"])\nprint(df.head(5))\n```", "```py\nfiltered_data = (\n    df\n    .filter([\"timestamp\", \"user_id\", \"movie_id\", \"user_rating\"])\n    .sort_values(\"timestamp\")\n    .astype({\"user_id\": int, \"movie_id\": int, \"user_rating\": int}) # nicer types\n    .drop(columns=[\"timestamp\"]) # don't need the timestamp anymore\n)\n\ntrain = filtered_data.iloc[:900000] # chronologically first 90% of the dataset\ntest = filtered_data.iloc[900000:]  # chronologically last 10% of the dataset\n```", "```py\nprint(train.query(\"user_id == 1\").shape[0])\nprint(test.query(\"user_id == 1\").shape[0])\n\n# Output:\n# 0\n# 53\n```", "```py\nX_train = train.drop(columns=[\"user_rating\"])\ny_train = train[\"user_rating\"]\n\nX_test = test.drop(columns=[\"user_rating\"])\ny_test = test[\"user_rating\"]\n```", "```py\n# BAD BAD BAD BAD BAD BAD BAD BAD BAD BAD BAD BAD BAD BAD BAD\n\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nhgb = HistGradientBoostingRegressor(random_state=0)\nhgb.fit(X_train, y_train)\nprint(hgb.score(X_test, y_test), mean_absolute_error(y_test, hgb.predict(X_test)))\n\n# Output:\n# 0.07018701410615702 0.8508620798953698\n\n# BAD BAD BAD BAD BAD BAD BAD BAD BAD BAD BAD BAD BAD BAD BAD\n```", "```py\nall_users = train[\"user_id\"].unique()\nall_movies = train[\"movie_id\"].unique()\n```", "```py\nimport tensorflow as tf\n\n# user pipeline\nuser_input = tf.keras.layers.Input(shape=(1,), name=\"user\")\nuser_as_integer = tf.keras.layers.IntegerLookup(vocabulary=all_users)(user_input)\nuser_embedding = tf.keras.layers.Embedding(input_dim=len(all_users)+1, output_dim=32)(user_as_integer)\n\n# movie pipeline\nmovie_input = tf.keras.layers.Input(shape=(1,), name=\"movie\")\nmovie_as_integer = tf.keras.layers.IntegerLookup(vocabulary=all_movies)(movie_input)\nmovie_embedding = tf.keras.layers.Embedding(input_dim=len(all_movies)+1, output_dim=32)(movie_as_integer)\n\n# dot product\ndot = tf.keras.layers.Dot(axes=2)([user_embedding, movie_embedding])\nflatten = tf.keras.layers.Flatten()(dot)\n\n# model input/output definition\nmodel = tf.keras.Model(inputs=[user_input, movie_input], outputs=flatten)\n\nmodel.compile(loss=\"mse\", metrics=[tf.keras.metrics.MeanAbsoluteError()])\n```", "```py\nmodel.fit(\n    x={\n        \"user\": X_train[\"user_id\"],\n        \"movie\": X_train[\"movie_id\"]\n    },\n    y=y_train.values,\n    batch_size=256,\n    epochs=100,\n    validation_split=0.1, # for early stopping\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(patience=1, restore_best_weights=True)\n    ],\n)\n\n# Output (for me):\n# ...\n# Epoch 18/100\n# 3165/3165 [==============================] - 8s 3ms/step - loss: 0.7357 - mean_absolute_error: 0.6595 - val_loss: 11.4699 - val_mean_absolute_error: 2.9923\n```", "```py\nuser_input = tf.keras.layers.Input(shape=(1,), name=\"user\")\nuser_as_integer = tf.keras.layers.IntegerLookup(vocabulary=all_users)(user_input)\nuser_embedding = tf.keras.layers.Embedding(input_dim=len(all_users) + 1, output_dim=32)(user_as_integer)\n\nmovie_input = tf.keras.layers.Input(shape=(1,), name=\"movie\")\nmovie_as_integer = tf.keras.layers.IntegerLookup(vocabulary=all_movies)(movie_input)\nmovie_embedding = tf.keras.layers.Embedding(input_dim=len(all_movies) + 1, output_dim=32)(movie_as_integer)\n\ndot = tf.keras.layers.Dot(axes=2)([user_embedding, movie_embedding])\nflatten = tf.keras.layers.Flatten()(dot)\n\n# this is new!\nsquash = tf.keras.layers.Lambda(lambda x: 4*tf.nn.sigmoid(x) + 1)(flatten) \n\nmodel = tf.keras.Model(inputs=[user_input, movie_input], outputs=squash)\n\nmodel.compile(loss=\"mse\", metrics=[tf.keras.metrics.MeanAbsoluteError()])\n```", "```py\nmodel.evaluate(\n    x={\"user\": X_test[\"user_id\"], \"movie\": X_test[\"movie_id\"]},\n    y=y_test\n)\n\n# Output:\n# [...] loss: 0.9701 - mean_absolute_error: 0.7683\n```", "```py\nfrom sklearn.metrics import r2_score\n\nr2_score(\n    y_test,\n    model.predict(\n        {\"user\": X_test[\"user_id\"], \"movie\": X_test[\"movie_id\"]}\n    ).ravel()\n)\n# Output:\n# 0.1767611765807019 \n```", "```py\nuser_input = tf.keras.layers.Input(shape=(1,), name=\"user\")\nuser_as_integer = tf.keras.layers.IntegerLookup(vocabulary=all_users)(user_input)\nuser_embedding = tf.keras.layers.Embedding(input_dim=len(all_users) + 1, output_dim=32)(user_as_integer)\nuser_bias = tf.keras.layers.Embedding(input_dim=len(all_users) + 1, output_dim=1)(user_as_integer)\n\nmovie_input = tf.keras.layers.Input(shape=(1,), name=\"movie\")\nmovie_as_integer = tf.keras.layers.IntegerLookup(vocabulary=all_movies)(movie_input)\nmovie_embedding = tf.keras.layers.Embedding(input_dim=len(all_movies) + 1, output_dim=32)(movie_as_integer)\nmovie_bias = tf.keras.layers.Embedding(input_dim=len(all_movies) + 1, output_dim=1)(movie_as_integer)\n\ndot = tf.keras.layers.Dot(axes=2)([user_embedding, movie_embedding])\nadd = tf.keras.layers.Add()([dot, user_bias, movie_bias])\nflatten = tf.keras.layers.Flatten()(add)\nsquash = tf.keras.layers.Lambda(lambda x: 4 * tf.nn.sigmoid(x) + 1)(flatten)\n\nmodel = tf.keras.Model(inputs=[user_input, movie_input], outputs=squash)\n\nmodel.compile(loss=\"mse\", metrics=[tf.keras.metrics.MeanAbsoluteError()])\n```", "```py\nmodel.predict({\"user\": tf.constant([[1], [1]]), \"movie\": tf.constant([[2], [3]])})\n\n# Output:\n# array([[3.0344076],\n#        [2.9140234]], dtype=float32)\n```", "```py\nmodel.predict({\n    \"user\": tf.ones_like(all_movies.reshape(-1, 1)), # fill user 1 in many times\n    \"movie\": all_movies.reshape(-1, 1)\n})\n```"]