- en: The Data-centric AI Concepts in Segment Anything
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/the-data-centric-ai-concepts-in-segment-anything-8eea556ac9d](https://towardsdatascience.com/the-data-centric-ai-concepts-in-segment-anything-8eea556ac9d)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Unpacking the data-centric AI concepts used in Segment Anything, the first foundation
    model for image segmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@a0987284901?source=post_page-----8eea556ac9d--------------------------------)[![Henry
    Lai](../Images/eaa1b4eb6f6cebc131f4cf0cfdd4cda7.png)](https://medium.com/@a0987284901?source=post_page-----8eea556ac9d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8eea556ac9d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8eea556ac9d--------------------------------)
    [Henry Lai](https://medium.com/@a0987284901?source=post_page-----8eea556ac9d--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8eea556ac9d--------------------------------)
    ·7 min read·May 31, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/48aef715050a81cdac17d39e62afc269.png)'
  prefs: []
  type: TYPE_IMG
- en: Segment Anything dataset construction. Image from the paper [https://arxiv.org/pdf/2304.02643.pdf](https://arxiv.org/pdf/2304.02643.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: Artificial Intelligence (AI) has made remarkable progress, especially in developing
    foundation models, which are trained with a large quantity of data and can be
    adapted to a wide range of downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: A notable success of the foundation models is [Large Language Models (LLMs)](https://medium.com/towards-data-science/what-are-the-data-centric-ai-concepts-behind-gpt-models-a590071bb727).
    These models can perform complex tasks with great precision, such as language
    translation, text summarization, and question-answering.
  prefs: []
  type: TYPE_NORMAL
- en: Foundation models are also starting to change the game in Computer Vision. Meta’s
    Segment Anything is a recent development that’s causing a stir.
  prefs: []
  type: TYPE_NORMAL
- en: The success of Segment Anything can be attributed to its large labeled dataset,
    which has played a crucial role in enabling its remarkable performance. The model
    architecture, as described in the [Segment Anything paper](https://arxiv.org/pdf/2304.02643.pdf),
    is surprisingly simple and lightweight.
  prefs: []
  type: TYPE_NORMAL
- en: In this article, drawing upon insights from our recent survey papers [1,2],
    we will take a closer look at Segment Anything through the lens of [data-centric
    AI](https://github.com/daochenzha/data-centric-AI), a growing concept in the data
    science community.
  prefs: []
  type: TYPE_NORMAL
- en: What Can Segment Anything Do?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a nutshell, the image segmentation task is to predict a mask to separate
    the areas of interest in an image, such as an object, a person, etc. Segmentation
    is a very important task in Computer Visual, making the image more meaningful
    and easier to analyze.
  prefs: []
  type: TYPE_NORMAL
- en: The difference between Segment Anything and other image segmentation approaches
    lies in introducing prompts to specify the segmentation location. Prompts can
    be vague, such as a point, a box, etc.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/24a3ffd16299a8fdbe0f9bfbe0c5cb23.png)'
  prefs: []
  type: TYPE_IMG
- en: The image is a screenshot from [https://segment-anything.com/](https://segment-anything.com/)
    by uploading the image taken by the author.
  prefs: []
  type: TYPE_NORMAL
- en: What is Data-centric AI?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/71b0afd9544c0075ce9be397d7884629.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparison between data-centric AI and model-centric AI. [https://arxiv.org/abs/2301.04819](https://arxiv.org/abs/2301.04819)
    Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: '[Data-centric AI](https://github.com/daochenzha/data-centric-AI) is a novel
    approach to AI system development, which has been gaining traction and is being
    promoted by AI pioneer Andrew Ng.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Data-centric AI is the discipline of systematically engineering the data used
    to build an AI system. — Andrew Ng*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Previously, our primary focus was on developing better models using data that
    remained largely unchanged — this was referred to as model-centric AI. However,
    this approach can be problematic in real-world scenarios since it fails to account
    for issues that may arise in the data, including inaccurate labels, duplicates,
    and biases. Consequently, overfitting a dataset may not necessarily result in
    improved model behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Data-centric AI, on the other hand, prioritizes enhancing the quality and quantity
    of data utilized in creating AI systems. The focus is on the data itself, with
    relatively fixed models. Adopting a data-centric approach in developing AI systems
    has more promise in real-world applications since the maximum capability of a
    model is determined by the data used for training.
  prefs: []
  type: TYPE_NORMAL
- en: It’s crucial to distinguish between “data-centric” and “data-driven” approaches.
    “Data-driven” methods only rely on data to steer AI development, but the focus
    remains on creating models instead of engineering data, making it fundamentally
    different from “data-centric” approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 'The [data-centric AI framework](https://github.com/daochenzha/data-centric-AI)
    encompasses three main objectives:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training data development** entails gathering and generating high-quality,
    diverse data to facilitate the training of machine learning models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inference data development** involves constructing innovative evaluation
    sets that offer detailed insights into the model or unlock specific capabilities
    of the model through engineered data inputs, such as prompt engineering.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data maintenance** aims to ensure the quality and dependability of data in
    a constantly changing environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/f44d3f3a407f142178b652b3cea13289.png)'
  prefs: []
  type: TYPE_IMG
- en: Data-centric AI framework. [https://arxiv.org/abs/2303.10158](https://arxiv.org/abs/2303.10158).
    Image by the author.
  prefs: []
  type: TYPE_NORMAL
- en: The Model used in Segment Anything
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/2cde1db92dc417c8cef702a27ad69fc2.png)'
  prefs: []
  type: TYPE_IMG
- en: Segment Anything Model. Image from the paper [https://arxiv.org/pdf/2304.02643.pdf](https://arxiv.org/pdf/2304.02643.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: 'The model design is surprisingly simple. The model mainly consists of three
    parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prompt encoder:** This part is used to obtain the representation of the prompt,
    either through positional encoding or convolution.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Image encoder:** This part directly uses the Vision Transformer (ViT) without
    any special modifications.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Lightweight mask decoder:** This part mainly fuses prompt embedding and image
    embedding, using mechanisms such as attention. It is called lightweight because
    it has only a few layers.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The lightweight mask decoder is interesting, as it allows the model to be easily
    deployed, even with just CPUs. Below is the comment provided by the authors of
    Segment Anything.
  prefs: []
  type: TYPE_NORMAL
- en: 'Surprisingly, we find that a simple design satisfies all three constraints:
    a powerful image encoder computes an image embedding, a prompt encoder embeds
    prompts, and then the two information sources are combined in a lightweight mask
    decoder that predicts segmentation masks.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Therefore, the secret of Segment Anything’s strong performance is very likely
    not the model design, as it is very simple and lightweight.
  prefs: []
  type: TYPE_NORMAL
- en: Data-centric AI Concepts in Segment Anything
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The core of training Segment Anything lies in a large annotated dataset containing
    more than a billion masks, which is 400 times larger than existing segmentation
    datasets. How did they achieve this? The authors used a data engine to perform
    the annotation, which can be broadly divided into three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Assisted-manual annotation:** This step can be understood as an active learning
    process. First, an initial model is trained on public datasets. Next, annotators
    modify the predicted masks. Finally, the model is trained with the newly annotated
    data. These three steps were repeated six times, ultimately resulting in 4.3 million
    mask annotations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Semi-automatic annotation:** The goal of this step is to increase the diversity
    of masks, which can also be understood as an active learning process. In simple
    terms, if the model can automatically generate good masks, human annotators don’t
    need to label them, and human efforts can focus on masks where the model is not
    confident enough. The method used to find confident masks is quite interesting,
    involving object detection on masks from the first step. For example, suppose
    there are 20 possible masks in an image. We first use the current model for segmentation,
    but this will probably only annotate a portion of the masks, with some masks not
    being well-annotated. We now need to identify which masks are good (confident)
    automatically. This paper’s approach is to perform object detection on the predicted
    masks to see if objects can be detected in the image. If objects are detected,
    we consider the corresponding mask to be confident. Suppose this process identifies
    eight confident masks; the annotator then labels the remaining 12, saving human
    effort. The above process was repeated five times, adding another 5.9 million
    mask annotations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Fully-automatic annotation:** Simply put, this step uses the model trained
    in the previous step to annotate data. Some strategies were used to improve annotation
    quality, including:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**(1) filtering out less confident masks** based on predicted Intersection
    over Union (IoU) values (the model has a head to predict IoU).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**(2) only considering stable masks**, meaning that if the threshold is adjusted
    slightly above or below 0.5, the masks remain mostly unchanged. Specifically,
    for each pixel, the model outputs a value between 0 and 1\. We typically use 0.5
    as the threshold to decide whether a pixel is masked. Stability means that when
    the threshold is adjusted to a certain extent around 0.5 (e.g., 0.45 to 0.55),
    the corresponding mask remains largely unchanged, indicating that the model’s
    predictions are significantly different on either side of the boundary.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**(3) deduplication was performed** with non-maximal suppression (NMS).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This step annotated 11 billion masks (an increase of more than 100 times in
    quantity).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Does this process sound familiar? That’s right, the Reinforcement Learning from
    Human Feedback (RLHF) used in ChatGPT is quite similar to the process described
    above. The commonality between the two approaches is that instead of directly
    relying on humans to annotate data, a model is first trained by human inputs and
    then used to annotate data. In RLHF, a reward model is trained to give rewards
    for reinforcement learning, while in Segment Anything, the model is trained for
    direct image annotation.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The core contribution of Segment Anything lies in its large annotated data,
    demonstrating the crucial importance of the data-centric AI concept. The success
    of foundation models in the computer vision field can be considered an inevitable
    event, but surprisingly, it happened so quickly. Going forward, I believe other
    AI subfields, and even non-AI and non-computer-related fields, will see the emergence
    of foundation models in due course.
  prefs: []
  type: TYPE_NORMAL
- en: No matter how technology evolves, improving data quality and quantity will always
    be an effective way to enhance AI performance, making the concept of data-centric
    AI increasingly important.
  prefs: []
  type: TYPE_NORMAL
- en: 'I hope this article can inspire you in your own work. You can learn more about
    the data-centric AI framework in the following papers/resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[1] [Data-centric Artificial Intelligence: A Survey](https://arxiv.org/abs/2303.10158)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] [Data-centric AI: Perspectives and Challenges](https://arxiv.org/abs/2301.04819)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] [Awesome Data Centric-AI Resources](https://github.com/daochenzha/data-centric-AI)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you found this article interesting, you may also want to check out my previous
    article: [What Are the Data-Centric AI Concepts behind GPT Models?](https://medium.com/towards-data-science/what-are-the-data-centric-ai-concepts-behind-gpt-models-a590071bb727)'
  prefs: []
  type: TYPE_NORMAL
- en: Stay tuned!
  prefs: []
  type: TYPE_NORMAL
