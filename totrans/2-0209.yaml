- en: A Little Pandas Hack to Handle Large Datasets with Limited Memory
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/a-little-pandas-hack-to-handle-large-datasets-with-limited-memory-6745140f473b](https://towardsdatascience.com/a-little-pandas-hack-to-handle-large-datasets-with-limited-memory-6745140f473b)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Pandas defaults aren’t optimal. A tiny configuration can compress your dataframe
    to fit in your memory.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://thuwarakesh.medium.com/?source=post_page-----6745140f473b--------------------------------)[![Thuwarakesh
    Murallie](../Images/44f1a14a899426592bbd8c7f73ce169d.png)](https://thuwarakesh.medium.com/?source=post_page-----6745140f473b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6745140f473b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6745140f473b--------------------------------)
    [Thuwarakesh Murallie](https://thuwarakesh.medium.com/?source=post_page-----6745140f473b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6745140f473b--------------------------------)
    ·8 min read·Jan 19, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c4b1299e06f03069b94110ff87456e73.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
- en: You can compress a huge Pandas dataframe without losing its properties, just
    like squeezing a burger. Save memory and work faster with this little trick. —
    Photo by [Leonardo Luz](https://www.pexels.com/photo/photo-of-a-burger-between-a-person-s-hands-14001304/)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: I never thought my code needed improvement. I’ve always complained that I don’t
    have enough memory or the dataset was too big to handle.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: My go-to solution was to put them on a Postgres DB and write SQL queries. After
    all, that is an acceptable way to handle large-scale datasets. I kept doing this
    whenever I got a large dataset.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: But I don’t get the complete flexibility I get in Python. For this reason, I
    had to combine them and alternate between them. For instance, I load the dataset
    on a SQL database and write a Python script to run SQL queries, often using Sqlalchemy.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: While it gives me the flexibility of both worlds, I have issues sharing my code
    with other team members. Other members should have the knowledge and setup for
    relational databases.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: The classic solution for this problem is increasing the memory and running tasks
    in parallel. On the infrastructure part, moving to the cloud was my favorite solution.
    I can pay for the high-performance resources only for their usage. And on the
    parallel execution side, technologies like [Dask](https://www.dask.org/) cover
    me.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Yet, I recently discovered code optimization is the first and usually the most
    rewarding solution. If your code doesn’t do well on a single thread, how can we
    guarantee that parallelization could boost the performance? If your code doesn’t
    take the full benefit of your local computer, what’s the point in moving to the
    cloud?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Among the many Pandas optimization techniques I’ve learned, one was profound.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: The trick is …
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Save memory with correct data types.
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pandas infer dtypes based on the values present in each column. But it has no
    context. But you have!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: If all values in a column are whole numbers, Pandas would usually assign int64
    as that column’s data type. Yet, there are more int variants you can use. int8,
    int16, etc.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: The defaults are fine for most cases. But sometimes, it’s a too-large placeholder
    for smaller values. Pull up a notebook and type the following code and notice
    its output.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Int64 can hold values from -9223372036854775808 to 9223372036854775807\. I don’t
    even know how we call these numbers.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: But in most real-life cases, you can infer the possible column values. For example,
    in a hotel reservation system, the number of guests in a room can almost always
    be in a single digit. If it’s a dormitory, perhaps in two digits.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '[](/how-to-detect-memory-leakage-in-your-python-application-f83ae1ad897d?source=post_page-----6745140f473b--------------------------------)
    [## How to Detect Memory Leakage in Your Python Application'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Standard Python libraries that could tell the memory usage and execution time
    of every line
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/how-to-detect-memory-leakage-in-your-python-application-f83ae1ad897d?source=post_page-----6745140f473b--------------------------------)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Why don’t we use int8 instead? It can hold values from -128 to +128\. This smaller
    placeholder can take less space compared to the wasteful int64.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Let’s test this on a demo dataset.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: The following script generates a random dataset. In real life, you won’t be
    generating datasets, though. Notice its data types and memory usage.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This generated dataset has one million records and takes about 180 MB of memory.
    The room_rate variable has a range of 152.17–430.43\. But it has been assigned
    float64 data type. For this, float16 is more than sufficient. Likewise, let’s
    convert the number_of_guests to an integer and channel to a category.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, booking status is a boolean represented categorically. For analysis
    purposes, we can change it to a boolean. But one may argue to keep it as a category
    instead.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Here’s how we convert the data types to more desirable ones and how much memory
    it takes now.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We reduced our 180MB dataset by converting the data types to a 5MB. **That’s
    a ~96% saving.**
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: It’s a good idea to compress the dataset and store it in an optimal storage
    format for later usage.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '[](/best-file-format-to-store-large-data-dfa47701929f?source=post_page-----6745140f473b--------------------------------)
    [## CSVs Are Overrated! I Give up Some of Its Benefits to Gain More.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: What I use instead for to have a small file size and better performance.
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: towardsdatascience.com](/best-file-format-to-store-large-data-dfa47701929f?source=post_page-----6745140f473b--------------------------------)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: How to pick the data type for each column?
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pandas' default data types are almost always not optimal. As we’ve seen in the
    previous section, integers, regardless of their range, are assigned int64\. For
    smaller datasets, we don’t have to worry about this. Performance impact is usually
    insignificant, even at hundreds of thousands of lines.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas 的默认数据类型几乎总是不最优的。正如我们在上一节中看到的，整数无论范围如何，都被分配为 int64。对于较小的数据集，我们不需要担心这个问题。即使在数十万行数据中，性能影响通常也不显著。
- en: But we should take this more seriously when our computations get complex, and
    the dataset size is also bigger. The challenge is to assign the smallest possible
    placeholder without losing information.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 但是当我们的计算变得复杂且数据集规模更大时，我们应该更认真对待这个问题。挑战在于在不丢失信息的情况下，分配尽可能小的占位符。
- en: The first step is to get a description of your dataset. It can tell a lot of
    information all in one place. Check out the output of describe in our example
    above.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是获取数据集的描述。它可以在一个地方提供大量的信息。查看我们上面示例中的 `describe` 输出。
- en: For non-numeric variables, you’ll get the number of unique values, the most
    frequent value, and its count. Looking at this, we can guess that channel must
    be a category and booking_status may be a boolean. Because, out of our 1M records,
    only 4 unique channels are repeating.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非数值变量，你将获得唯一值的数量、最频繁的值及其计数。看着这些数据，我们可以猜测 `channel` 可能是一个类别，而 `booking_status`
    可能是布尔值。因为在我们1M的记录中，只有4个独特的渠道在重复出现。
- en: For numerical columns, we get the minimum and maximum values. By also looking
    at the quartile values, you can come to a conclusion if this column needs to be
    an integer or a floating point. In each case, you can also decide the size of
    the as well.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数值列，我们得到最小值和最大值。通过查看四分位数值，你可以得出这个列是否需要是整数或浮点数。在每种情况下，你也可以决定其大小。
- en: Here’s a table of numerical data types and their value ranges.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个数值数据类型及其值范围的表格。
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Observe your dataset carefully and pick the smallest that can work.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细观察你的数据集，并选择能够工作的最小值。
- en: 'For non-numeric datasets, Pandas usually assign ‘object’ as its type. The object
    data type is complex and inefficient in most cases. Thus, if the column values
    are not free-form, you can assign ‘category’ as its data type. Here is how much
    memory it saves:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非数值数据集，Pandas 通常将其类型指定为 ‘object’。对象数据类型在大多数情况下是复杂且低效的。因此，如果列值不是自由格式的，你可以将其数据类型指定为
    ‘category’。以下是它节省了多少内存：
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can achieve about 8X memory saving by simply converting to a category.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 通过简单地转换为类别数据类型，你可以节省大约 8 倍的内存。
- en: '[](/how-to-speed-up-python-data-pipelines-up-to-91x-80d7accfe7ec?source=post_page-----6745140f473b--------------------------------)
    [## How to Speed up Python Data Pipelines up to 91X?'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/how-to-speed-up-python-data-pipelines-up-to-91x-80d7accfe7ec?source=post_page-----6745140f473b--------------------------------)
    [## 如何将 Python 数据管道的速度提高到 91 倍？'
- en: A 5-minute tutorial that could save months of your big-data projects.
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个 5 分钟的教程，可能会为你的大数据项目节省数月的时间。
- en: towardsdatascience.com](/how-to-speed-up-python-data-pipelines-up-to-91x-80d7accfe7ec?source=post_page-----6745140f473b--------------------------------)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/how-to-speed-up-python-data-pipelines-up-to-91x-80d7accfe7ec?source=post_page-----6745140f473b--------------------------------)
- en: Converting data types needs more domain expertise.
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换数据类型需要更多的领域专业知识。
- en: Our demo was too easy because we generated the dataset. But datasets in the
    real world are usually complex. So do type conversions.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的演示过于简单，因为我们生成了数据集。但是现实世界中的数据集通常是复杂的，类型转换也是如此。
- en: We could use the .astype method on any dataframe to convert types directly.
    But in my example, I’ve used the assign method instead. That’s because we often
    need to modify the column before we type conversions.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在任何数据框上使用 `.astype` 方法直接转换类型。但在我的例子中，我使用了 `assign` 方法。因为我们通常需要在进行类型转换之前修改列。
- en: Imagine a situation when the number_of_guests column has null values. We can’t
    convert this column to int8 without filling the gap with some reasonable default.
    Zero may not make sense as there can’t be a booking with no person. We may think
    `1` is a good default. But if most bookings in the hotel have two people, then
    `2` is better. We can also bring in the channel variable to carefully pick defaults
    for each channel.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个情况，当 `number_of_guests` 列有空值时。我们不能在没有填补合理默认值的情况下将该列转换为 int8。零可能没有意义，因为不可能有一个没有人的预订。我们可能认为
    `1` 是一个好的默认值。但是，如果大多数酒店的预订有两个人，那么 `2` 更好。我们还可以引入 `channel` 变量来仔细选择每个渠道的默认值。
- en: The following example is a conditional way of assigning defaults. Note that
    we convert them to the correct data types only after filling in the appropriate
    missing values.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例是条件分配默认值的方式。注意，我们仅在填补适当的缺失值后才将其转换为正确的数据类型。
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Likewise, picking the default requires domain knowledge not present in the dataset.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，选择默认值需要领域知识，这在数据集中并不存在。
- en: Another challenge is future values. If you analyze offline, you can pick the
    data types only by looking at the dataset. But if your code lives inside a data
    pipeline, you should also think about future values.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个挑战是未来的值。如果你进行离线分析，你可以仅通过查看数据集来选择数据类型。但如果你的代码在数据管道中运行，你还应考虑未来的值。
- en: '[](/the-prefect-way-to-automate-orchestrate-data-pipelines-d4465638bac2?source=post_page-----6745140f473b--------------------------------)
    [## The Prefect Way to Automate & Orchestrate Data Pipelines'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 完美方法自动化和协调数据管道](https://towardsdatascience.com/the-prefect-way-to-automate-orchestrate-data-pipelines-d4465638bac2?source=post_page-----6745140f473b--------------------------------)'
- en: I am migrating all my ETL work from Airflow to this super-cool framework
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我正在将我所有的 ETL 工作从 Airflow 迁移到这个超级酷的框架。
- en: towardsdatascience.com](/the-prefect-way-to-automate-orchestrate-data-pipelines-d4465638bac2?source=post_page-----6745140f473b--------------------------------)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[完美方法自动化和协调数据管道](https://towardsdatascience.com/the-prefect-way-to-automate-orchestrate-data-pipelines-d4465638bac2?source=post_page-----6745140f473b--------------------------------)'
- en: When you get an incompatible value for a column, you may get an error or a wrong
    value instead. See the following example.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当你获得不兼容的列值时，你可能会得到错误或错误的值。请参见以下示例。
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that any values smaller than 128 have been correctly converted. But values
    128 and above are converted incorrectly.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，任何小于 128 的值已正确转换。但 128 及以上的值转换不正确。
- en: Perhaps you should validate the dataset before you pass it to transformation.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 或许你应该在将数据集传递给转换之前进行验证。
- en: '[](/data-validation-for-pandas-b24613959364?source=post_page-----6745140f473b--------------------------------)
    [## High-Quality Data Comes With High-Quality Validations'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 高质量的数据伴随高质量的验证](https://towardsdatascience.com/data-validation-for-pandas-b24613959364?source=post_page-----6745140f473b--------------------------------)'
- en: Here’s how you can ensure the quality of Pandas dataframes in every stage of
    your data pipeline
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 这里是你如何确保 Pandas 数据框在数据管道每个阶段的质量
- en: towardsdatascience.com](/data-validation-for-pandas-b24613959364?source=post_page-----6745140f473b--------------------------------)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[高质量的数据需要高质量的验证](https://towardsdatascience.com/data-validation-for-pandas-b24613959364?source=post_page-----6745140f473b--------------------------------)'
- en: Final thoughts
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结语
- en: As data professionals, we work with massive datasets. And our top-of-the-mind
    solution to performance issues is to stack up more resources and parallel execution.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据专业人士，我们处理的是大规模的数据集。我们对性能问题的终极解决方案是增加更多资源和并行执行。
- en: But I’ve found that some optimization could let us work with larger dataframes
    on smaller computers. The trick is to assign the correct data type.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现一些优化可以让我们在较小的计算机上处理更大的数据框。诀窍是分配正确的数据类型。
- en: Picking the correct data type needs domain knowledge. Also, if you’re building
    a data pipeline, you should think about future values as well. You don’t need
    to worry about it in offline analysis.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 选择正确的数据类型需要领域知识。此外，如果你正在构建数据管道，你还应考虑未来的值。在离线分析中你不需要担心这一点。
- en: I hope this helps.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这对你有所帮助。
- en: Thanks for reading, friend! Say Hi to me on [**LinkedIn**](https://www.linkedin.com/in/thuwarakesh/),
    [**Twitter**](https://twitter.com/Thuwarakesh), and [**Medium**](https://thuwarakesh.medium.com/).
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 谢谢你的阅读，朋友！在 [**LinkedIn**](https://www.linkedin.com/in/thuwarakesh/)、[**Twitter**](https://twitter.com/Thuwarakesh)
    和 [**Medium**](https://thuwarakesh.medium.com/) 上跟我打招呼吧。
- en: ''
  id: totrans-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Not a Medium member yet? Please use this link to [**become a member**](https://thuwarakesh.medium.com/membership)
    because, at no extra cost for you, I earn a small commission for referring you.
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 还不是 Medium 会员？请使用这个链接 [**成为会员**](https://thuwarakesh.medium.com/membership)，因为你没有额外费用，我会因为推荐你而获得小额佣金。
