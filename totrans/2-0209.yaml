- en: A Little Pandas Hack to Handle Large Datasets with Limited Memory
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理大数据集的小技巧，适用于有限内存
- en: 原文：[https://towardsdatascience.com/a-little-pandas-hack-to-handle-large-datasets-with-limited-memory-6745140f473b](https://towardsdatascience.com/a-little-pandas-hack-to-handle-large-datasets-with-limited-memory-6745140f473b)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-little-pandas-hack-to-handle-large-datasets-with-limited-memory-6745140f473b](https://towardsdatascience.com/a-little-pandas-hack-to-handle-large-datasets-with-limited-memory-6745140f473b)
- en: The Pandas defaults aren’t optimal. A tiny configuration can compress your dataframe
    to fit in your memory.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pandas 默认设置并不理想。一个小小的配置就可以将你的数据框压缩到适合你的内存。
- en: '[](https://thuwarakesh.medium.com/?source=post_page-----6745140f473b--------------------------------)[![Thuwarakesh
    Murallie](../Images/44f1a14a899426592bbd8c7f73ce169d.png)](https://thuwarakesh.medium.com/?source=post_page-----6745140f473b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6745140f473b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6745140f473b--------------------------------)
    [Thuwarakesh Murallie](https://thuwarakesh.medium.com/?source=post_page-----6745140f473b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://thuwarakesh.medium.com/?source=post_page-----6745140f473b--------------------------------)[![Thuwarakesh
    Murallie](../Images/44f1a14a899426592bbd8c7f73ce169d.png)](https://thuwarakesh.medium.com/?source=post_page-----6745140f473b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6745140f473b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6745140f473b--------------------------------)
    [Thuwarakesh Murallie](https://thuwarakesh.medium.com/?source=post_page-----6745140f473b--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6745140f473b--------------------------------)
    ·8 min read·Jan 19, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6745140f473b--------------------------------)
    ·阅读时间 8 分钟·2023年1月19日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/c4b1299e06f03069b94110ff87456e73.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4b1299e06f03069b94110ff87456e73.png)'
- en: You can compress a huge Pandas dataframe without losing its properties, just
    like squeezing a burger. Save memory and work faster with this little trick. —
    Photo by [Leonardo Luz](https://www.pexels.com/photo/photo-of-a-burger-between-a-person-s-hands-14001304/)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在不丢失属性的情况下压缩一个巨大的 Pandas 数据框，就像挤压一个汉堡一样。通过这个小技巧节省内存，并提高工作效率。— 图片由 [Leonardo
    Luz](https://www.pexels.com/photo/photo-of-a-burger-between-a-person-s-hands-14001304/)
    提供
- en: I never thought my code needed improvement. I’ve always complained that I don’t
    have enough memory or the dataset was too big to handle.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我从未认为我的代码需要改进。我总是抱怨内存不足或数据集过大无法处理。
- en: My go-to solution was to put them on a Postgres DB and write SQL queries. After
    all, that is an acceptable way to handle large-scale datasets. I kept doing this
    whenever I got a large dataset.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我常用的解决方案是将数据放在 Postgres 数据库中，并编写 SQL 查询。毕竟，这是一种处理大规模数据集的可接受方式。每当我获得一个大数据集时，我都会这样做。
- en: But I don’t get the complete flexibility I get in Python. For this reason, I
    had to combine them and alternate between them. For instance, I load the dataset
    on a SQL database and write a Python script to run SQL queries, often using Sqlalchemy.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 但我无法获得 Python 中的完全灵活性。因此，我不得不将两者结合使用并交替进行。例如，我将数据集加载到 SQL 数据库中，并编写一个 Python
    脚本来运行 SQL 查询，通常使用 Sqlalchemy。
- en: While it gives me the flexibility of both worlds, I have issues sharing my code
    with other team members. Other members should have the knowledge and setup for
    relational databases.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这让我在两者之间拥有灵活性，但我在与其他团队成员共享代码时遇到了问题。其他成员应该具备关系数据库的知识和设置。
- en: The classic solution for this problem is increasing the memory and running tasks
    in parallel. On the infrastructure part, moving to the cloud was my favorite solution.
    I can pay for the high-performance resources only for their usage. And on the
    parallel execution side, technologies like [Dask](https://www.dask.org/) cover
    me.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的经典解决方案是增加内存并进行并行任务。在基础设施方面，迁移到云端是我最喜欢的解决方案。我可以只为高性能资源的使用付费。而在并行执行方面，像 [Dask](https://www.dask.org/)
    这样的技术可以为我提供支持。
- en: Yet, I recently discovered code optimization is the first and usually the most
    rewarding solution. If your code doesn’t do well on a single thread, how can we
    guarantee that parallelization could boost the performance? If your code doesn’t
    take the full benefit of your local computer, what’s the point in moving to the
    cloud?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我最近发现代码优化是首选且通常是最有成效的解决方案。如果你的代码在单线程下表现不佳，我们如何保证并行化能够提升性能？如果你的代码没有充分利用本地计算机的资源，那迁移到云端又有什么意义呢？
- en: Among the many Pandas optimization techniques I’ve learned, one was profound.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在我学到的许多 Pandas 优化技术中，有一种特别深刻。
- en: The trick is …
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 诀窍是…
- en: Save memory with correct data types.
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用正确的数据类型来节省内存。
- en: Pandas infer dtypes based on the values present in each column. But it has no
    context. But you have!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas 根据每列中存在的值来推断数据类型。但它没有上下文。而你有！
- en: If all values in a column are whole numbers, Pandas would usually assign int64
    as that column’s data type. Yet, there are more int variants you can use. int8,
    int16, etc.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果列中的所有值都是整数，Pandas 通常会将 int64 分配为该列的数据类型。然而，还有更多的 int 变体可供使用，例如 int8、int16
    等。
- en: The defaults are fine for most cases. But sometimes, it’s a too-large placeholder
    for smaller values. Pull up a notebook and type the following code and notice
    its output.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数情况，默认设置是可以的。但有时，对于较小的值，这个占位符过大。打开一个笔记本，输入以下代码并查看其输出。
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Int64 can hold values from -9223372036854775808 to 9223372036854775807\. I don’t
    even know how we call these numbers.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Int64 可以容纳从 -9223372036854775808 到 9223372036854775807 的值。我甚至不知道这些数字怎么称呼。
- en: But in most real-life cases, you can infer the possible column values. For example,
    in a hotel reservation system, the number of guests in a room can almost always
    be in a single digit. If it’s a dormitory, perhaps in two digits.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 但在大多数实际情况下，你可以推测可能的列值。例如，在酒店预订系统中，房间中的客人数几乎总是个位数。如果是宿舍，可能是两位数。
- en: '[](/how-to-detect-memory-leakage-in-your-python-application-f83ae1ad897d?source=post_page-----6745140f473b--------------------------------)
    [## How to Detect Memory Leakage in Your Python Application'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 如何检测 Python 应用中的内存泄漏](https://example.org/how-to-detect-memory-leakage-in-your-python-application-f83ae1ad897d?source=post_page-----6745140f473b--------------------------------)'
- en: Standard Python libraries that could tell the memory usage and execution time
    of every line
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标准的 Python 库可以告诉你每一行的内存使用情况和执行时间。
- en: towardsdatascience.com](/how-to-detect-memory-leakage-in-your-python-application-f83ae1ad897d?source=post_page-----6745140f473b--------------------------------)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](https://example.org/how-to-detect-memory-leakage-in-your-python-application-f83ae1ad897d?source=post_page-----6745140f473b--------------------------------)'
- en: Why don’t we use int8 instead? It can hold values from -128 to +128\. This smaller
    placeholder can take less space compared to the wasteful int64.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么不使用 int8 呢？它可以容纳从 -128 到 +128 的值。这个更小的占位符相比浪费的 int64 占用更少的空间。
- en: Let’s test this on a demo dataset.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在一个示例数据集上测试一下这个方法。
- en: The following script generates a random dataset. In real life, you won’t be
    generating datasets, though. Notice its data types and memory usage.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以下脚本生成了一个随机数据集。在实际生活中，你不会生成数据集。注意它的数据类型和内存使用情况。
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This generated dataset has one million records and takes about 180 MB of memory.
    The room_rate variable has a range of 152.17–430.43\. But it has been assigned
    float64 data type. For this, float16 is more than sufficient. Likewise, let’s
    convert the number_of_guests to an integer and channel to a category.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的数据集有一百万条记录，占用约 180 MB 的内存。room_rate 变量的范围是 152.17–430.43\。但它被分配了 float64
    数据类型。实际上，float16 已经足够了。同样，将 number_of_guests 转换为整数，将 channel 转换为类别。
- en: Interestingly, booking status is a boolean represented categorically. For analysis
    purposes, we can change it to a boolean. But one may argue to keep it as a category
    instead.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，预订状态是以分类方式表示的布尔值。为了分析目的，我们可以将其更改为布尔值。但也有人可能会争辩说还是将其保留为类别更好。
- en: Here’s how we convert the data types to more desirable ones and how much memory
    it takes now.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们如何将数据类型转换为更理想的类型，以及现在需要多少内存。
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We reduced our 180MB dataset by converting the data types to a 5MB. **That’s
    a ~96% saving.**
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将数据类型转换将 180MB 的数据集减少到 5MB。**这大约节省了 96%**。
- en: It’s a good idea to compress the dataset and store it in an optimal storage
    format for later usage.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩数据集并将其存储为最佳格式以备后用是个好主意。
- en: '[](/best-file-format-to-store-large-data-dfa47701929f?source=post_page-----6745140f473b--------------------------------)
    [## CSVs Are Overrated! I Give up Some of Its Benefits to Gain More.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[## CSV 文件被高估了！我放弃了一些它的优点以获得更多的好处。](https://example.org/best-file-format-to-store-large-data-dfa47701929f?source=post_page-----6745140f473b--------------------------------)'
- en: What I use instead for to have a small file size and better performance.
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我使用了什么来获得更小的文件大小和更好的性能。
- en: towardsdatascience.com](/best-file-format-to-store-large-data-dfa47701929f?source=post_page-----6745140f473b--------------------------------)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](https://example.org/best-file-format-to-store-large-data-dfa47701929f?source=post_page-----6745140f473b--------------------------------)'
- en: How to pick the data type for each column?
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何为每一列选择数据类型？
- en: Pandas' default data types are almost always not optimal. As we’ve seen in the
    previous section, integers, regardless of their range, are assigned int64\. For
    smaller datasets, we don’t have to worry about this. Performance impact is usually
    insignificant, even at hundreds of thousands of lines.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas 的默认数据类型几乎总是不最优的。正如我们在上一节中看到的，整数无论范围如何，都被分配为 int64。对于较小的数据集，我们不需要担心这个问题。即使在数十万行数据中，性能影响通常也不显著。
- en: But we should take this more seriously when our computations get complex, and
    the dataset size is also bigger. The challenge is to assign the smallest possible
    placeholder without losing information.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 但是当我们的计算变得复杂且数据集规模更大时，我们应该更认真对待这个问题。挑战在于在不丢失信息的情况下，分配尽可能小的占位符。
- en: The first step is to get a description of your dataset. It can tell a lot of
    information all in one place. Check out the output of describe in our example
    above.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是获取数据集的描述。它可以在一个地方提供大量的信息。查看我们上面示例中的 `describe` 输出。
- en: For non-numeric variables, you’ll get the number of unique values, the most
    frequent value, and its count. Looking at this, we can guess that channel must
    be a category and booking_status may be a boolean. Because, out of our 1M records,
    only 4 unique channels are repeating.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非数值变量，你将获得唯一值的数量、最频繁的值及其计数。看着这些数据，我们可以猜测 `channel` 可能是一个类别，而 `booking_status`
    可能是布尔值。因为在我们1M的记录中，只有4个独特的渠道在重复出现。
- en: For numerical columns, we get the minimum and maximum values. By also looking
    at the quartile values, you can come to a conclusion if this column needs to be
    an integer or a floating point. In each case, you can also decide the size of
    the as well.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数值列，我们得到最小值和最大值。通过查看四分位数值，你可以得出这个列是否需要是整数或浮点数。在每种情况下，你也可以决定其大小。
- en: Here’s a table of numerical data types and their value ranges.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个数值数据类型及其值范围的表格。
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Observe your dataset carefully and pick the smallest that can work.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细观察你的数据集，并选择能够工作的最小值。
- en: 'For non-numeric datasets, Pandas usually assign ‘object’ as its type. The object
    data type is complex and inefficient in most cases. Thus, if the column values
    are not free-form, you can assign ‘category’ as its data type. Here is how much
    memory it saves:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非数值数据集，Pandas 通常将其类型指定为 ‘object’。对象数据类型在大多数情况下是复杂且低效的。因此，如果列值不是自由格式的，你可以将其数据类型指定为
    ‘category’。以下是它节省了多少内存：
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can achieve about 8X memory saving by simply converting to a category.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 通过简单地转换为类别数据类型，你可以节省大约 8 倍的内存。
- en: '[](/how-to-speed-up-python-data-pipelines-up-to-91x-80d7accfe7ec?source=post_page-----6745140f473b--------------------------------)
    [## How to Speed up Python Data Pipelines up to 91X?'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/how-to-speed-up-python-data-pipelines-up-to-91x-80d7accfe7ec?source=post_page-----6745140f473b--------------------------------)
    [## 如何将 Python 数据管道的速度提高到 91 倍？'
- en: A 5-minute tutorial that could save months of your big-data projects.
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个 5 分钟的教程，可能会为你的大数据项目节省数月的时间。
- en: towardsdatascience.com](/how-to-speed-up-python-data-pipelines-up-to-91x-80d7accfe7ec?source=post_page-----6745140f473b--------------------------------)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/how-to-speed-up-python-data-pipelines-up-to-91x-80d7accfe7ec?source=post_page-----6745140f473b--------------------------------)
- en: Converting data types needs more domain expertise.
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换数据类型需要更多的领域专业知识。
- en: Our demo was too easy because we generated the dataset. But datasets in the
    real world are usually complex. So do type conversions.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的演示过于简单，因为我们生成了数据集。但是现实世界中的数据集通常是复杂的，类型转换也是如此。
- en: We could use the .astype method on any dataframe to convert types directly.
    But in my example, I’ve used the assign method instead. That’s because we often
    need to modify the column before we type conversions.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在任何数据框上使用 `.astype` 方法直接转换类型。但在我的例子中，我使用了 `assign` 方法。因为我们通常需要在进行类型转换之前修改列。
- en: Imagine a situation when the number_of_guests column has null values. We can’t
    convert this column to int8 without filling the gap with some reasonable default.
    Zero may not make sense as there can’t be a booking with no person. We may think
    `1` is a good default. But if most bookings in the hotel have two people, then
    `2` is better. We can also bring in the channel variable to carefully pick defaults
    for each channel.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个情况，当 `number_of_guests` 列有空值时。我们不能在没有填补合理默认值的情况下将该列转换为 int8。零可能没有意义，因为不可能有一个没有人的预订。我们可能认为
    `1` 是一个好的默认值。但是，如果大多数酒店的预订有两个人，那么 `2` 更好。我们还可以引入 `channel` 变量来仔细选择每个渠道的默认值。
- en: The following example is a conditional way of assigning defaults. Note that
    we convert them to the correct data types only after filling in the appropriate
    missing values.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例是条件分配默认值的方式。注意，我们仅在填补适当的缺失值后才将其转换为正确的数据类型。
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Likewise, picking the default requires domain knowledge not present in the dataset.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，选择默认值需要领域知识，这在数据集中并不存在。
- en: Another challenge is future values. If you analyze offline, you can pick the
    data types only by looking at the dataset. But if your code lives inside a data
    pipeline, you should also think about future values.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个挑战是未来的值。如果你进行离线分析，你可以仅通过查看数据集来选择数据类型。但如果你的代码在数据管道中运行，你还应考虑未来的值。
- en: '[](/the-prefect-way-to-automate-orchestrate-data-pipelines-d4465638bac2?source=post_page-----6745140f473b--------------------------------)
    [## The Prefect Way to Automate & Orchestrate Data Pipelines'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 完美方法自动化和协调数据管道](https://towardsdatascience.com/the-prefect-way-to-automate-orchestrate-data-pipelines-d4465638bac2?source=post_page-----6745140f473b--------------------------------)'
- en: I am migrating all my ETL work from Airflow to this super-cool framework
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我正在将我所有的 ETL 工作从 Airflow 迁移到这个超级酷的框架。
- en: towardsdatascience.com](/the-prefect-way-to-automate-orchestrate-data-pipelines-d4465638bac2?source=post_page-----6745140f473b--------------------------------)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[完美方法自动化和协调数据管道](https://towardsdatascience.com/the-prefect-way-to-automate-orchestrate-data-pipelines-d4465638bac2?source=post_page-----6745140f473b--------------------------------)'
- en: When you get an incompatible value for a column, you may get an error or a wrong
    value instead. See the following example.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当你获得不兼容的列值时，你可能会得到错误或错误的值。请参见以下示例。
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that any values smaller than 128 have been correctly converted. But values
    128 and above are converted incorrectly.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，任何小于 128 的值已正确转换。但 128 及以上的值转换不正确。
- en: Perhaps you should validate the dataset before you pass it to transformation.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 或许你应该在将数据集传递给转换之前进行验证。
- en: '[](/data-validation-for-pandas-b24613959364?source=post_page-----6745140f473b--------------------------------)
    [## High-Quality Data Comes With High-Quality Validations'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 高质量的数据伴随高质量的验证](https://towardsdatascience.com/data-validation-for-pandas-b24613959364?source=post_page-----6745140f473b--------------------------------)'
- en: Here’s how you can ensure the quality of Pandas dataframes in every stage of
    your data pipeline
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 这里是你如何确保 Pandas 数据框在数据管道每个阶段的质量
- en: towardsdatascience.com](/data-validation-for-pandas-b24613959364?source=post_page-----6745140f473b--------------------------------)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[高质量的数据需要高质量的验证](https://towardsdatascience.com/data-validation-for-pandas-b24613959364?source=post_page-----6745140f473b--------------------------------)'
- en: Final thoughts
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结语
- en: As data professionals, we work with massive datasets. And our top-of-the-mind
    solution to performance issues is to stack up more resources and parallel execution.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据专业人士，我们处理的是大规模的数据集。我们对性能问题的终极解决方案是增加更多资源和并行执行。
- en: But I’ve found that some optimization could let us work with larger dataframes
    on smaller computers. The trick is to assign the correct data type.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现一些优化可以让我们在较小的计算机上处理更大的数据框。诀窍是分配正确的数据类型。
- en: Picking the correct data type needs domain knowledge. Also, if you’re building
    a data pipeline, you should think about future values as well. You don’t need
    to worry about it in offline analysis.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 选择正确的数据类型需要领域知识。此外，如果你正在构建数据管道，你还应考虑未来的值。在离线分析中你不需要担心这一点。
- en: I hope this helps.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这对你有所帮助。
- en: Thanks for reading, friend! Say Hi to me on [**LinkedIn**](https://www.linkedin.com/in/thuwarakesh/),
    [**Twitter**](https://twitter.com/Thuwarakesh), and [**Medium**](https://thuwarakesh.medium.com/).
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 谢谢你的阅读，朋友！在 [**LinkedIn**](https://www.linkedin.com/in/thuwarakesh/)、[**Twitter**](https://twitter.com/Thuwarakesh)
    和 [**Medium**](https://thuwarakesh.medium.com/) 上跟我打招呼吧。
- en: ''
  id: totrans-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Not a Medium member yet? Please use this link to [**become a member**](https://thuwarakesh.medium.com/membership)
    because, at no extra cost for you, I earn a small commission for referring you.
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 还不是 Medium 会员？请使用这个链接 [**成为会员**](https://thuwarakesh.medium.com/membership)，因为你没有额外费用，我会因为推荐你而获得小额佣金。
