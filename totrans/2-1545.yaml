- en: 'Multimodal Chain of Thoughts: Solving Problems in a Multimodal World'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多模态思维链：在多模态世界中解决问题
- en: 原文：[https://towardsdatascience.com/multimodal-chain-of-thoughts-solving-problems-in-a-multimodal-world-961a8ab9d0fa](https://towardsdatascience.com/multimodal-chain-of-thoughts-solving-problems-in-a-multimodal-world-961a8ab9d0fa)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/multimodal-chain-of-thoughts-solving-problems-in-a-multimodal-world-961a8ab9d0fa](https://towardsdatascience.com/multimodal-chain-of-thoughts-solving-problems-in-a-multimodal-world-961a8ab9d0fa)
- en: NLP | MULTIMODALITY | CHAIN OF THOUGHTS |
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NLP | 多模态性 | 思维链 |
- en: 'The world is not only text: How to extend the chain of thoughts to image and
    text?'
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 世界不仅仅是文字：如何将思维链扩展到图像和文字？
- en: '[](https://salvatore-raieli.medium.com/?source=post_page-----961a8ab9d0fa--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----961a8ab9d0fa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----961a8ab9d0fa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----961a8ab9d0fa--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----961a8ab9d0fa--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://salvatore-raieli.medium.com/?source=post_page-----961a8ab9d0fa--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----961a8ab9d0fa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----961a8ab9d0fa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----961a8ab9d0fa--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----961a8ab9d0fa--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----961a8ab9d0fa--------------------------------)
    ·14 min read·Mar 13, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----961a8ab9d0fa--------------------------------)
    ·14分钟阅读·2023年3月13日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/868bf8d7fafd276e2175a475fa6cc822.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/868bf8d7fafd276e2175a475fa6cc822.png)'
- en: photo by [Giulio Magnifico](https://unsplash.com/it/@giuliomagnifico) on Unsplash
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Giulio Magnifico](https://unsplash.com/it/@giuliomagnifico) 提供，来源于 Unsplash
- en: Sometimes getting to the answer is not easy, especially when the question requires
    reasoning. A model does not always have the answer hidden in its parameters but
    can get there with the right context and approach. What is the chain of thoughts?
    Why does this approach make it possible to solve multi-step reasoning tasks? Can
    it be extended to multimodal problems (i.e., problems with images and text)? Are
    only large models capable of this?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 有时得出答案并不容易，特别是当问题需要推理时。模型并不总是在其参数中隐藏答案，但可以通过正确的上下文和方法得出答案。什么是思维链？为什么这种方法使得解决多步骤推理任务成为可能？它可以扩展到多模态问题（即包含图像和文字的问题）吗？只有大型模型才能做到这一点吗？
- en: '**This article discusses how to answer these questions.**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**本文讨论了如何回答这些问题。**'
- en: 'Chain of thought (CoT): what is it?'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 思维链（CoT）：它是什么？
- en: '![](../Images/118e0e262bad7b95096ebc2f40b9c948.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/118e0e262bad7b95096ebc2f40b9c948.png)'
- en: photo by [Todd Cravens](https://unsplash.com/it/@toddcravens) on [Unsplash](https://unsplash.com/)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Todd Cravens](https://unsplash.com/it/@toddcravens) 提供，来源于 [Unsplash](https://unsplash.com/)
- en: 'In recent years we have seen the number of model parameters grow (to well over
    100 B of parameters). This has been motivated by the scaling law: as the number
    of parameters increased, the error decreased.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，我们见证了模型参数数量的增长（超过1000亿个参数）。这受到扩展法则的推动：随着参数数量的增加，误差减少。
- en: '[](/unsupervised-data-pruning-less-data-to-learn-better-30cd2bfbd855?source=post_page-----961a8ab9d0fa--------------------------------)
    [## Unsupervised data pruning: less data to learn better'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/unsupervised-data-pruning-less-data-to-learn-better-30cd2bfbd855?source=post_page-----961a8ab9d0fa--------------------------------)
    [## 无监督数据剪枝：用更少的数据更好地学习'
- en: Not always more data is meaning a more accurate model, but how to choose your
    data?
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多的数据并不总是意味着更准确的模型，但如何选择你的数据呢？
- en: towardsdatascience.com](/unsupervised-data-pruning-less-data-to-learn-better-30cd2bfbd855?source=post_page-----961a8ab9d0fa--------------------------------)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/unsupervised-data-pruning-less-data-to-learn-better-30cd2bfbd855?source=post_page-----961a8ab9d0fa--------------------------------)
- en: While this is true for tasks such as [sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis)
    and machine translation (even in the case of [zero-shot](https://en.wikipedia.org/wiki/Zero-shot_learning)
    or [few-shot learning](https://paperswithcode.com/task/few-shot-learning)), even
    models with billions of parameters struggle with tasks that require multi-step
    reasoning (e.g., math problems or commonsense reasoning).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这对于如[情感分析](https://en.wikipedia.org/wiki/Sentiment_analysis)和机器翻译等任务是正确的（即使在[零-shot](https://en.wikipedia.org/wiki/Zero-shot_learning)或[少-shot学习](https://paperswithcode.com/task/few-shot-learning)的情况下），即使是拥有数十亿参数的模型在需要多步推理的任务中（例如数学问题或常识推理）也会遇到困难。
- en: How to allow a model to succeed in these tasks?
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如何让模型在这些任务中取得成功？
- en: 'Large models can be fine-tuned for a specific task, and this was the first
    system that was attempted. [As the authors of this idea explain](https://arxiv.org/pdf/2006.06609.pdf)
    if you ask a model if a whale has a belly button the model will incorrectly answer
    no. This is because the model does not have this information stored in its parameters.
    The authors suggest that one can provide help to the model, providing it with
    a hint of implicit knowledge: “A whale is a mammal”.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 大型模型可以针对特定任务进行微调，这也是最初尝试的系统。[正如这个想法的作者解释的那样](https://arxiv.org/pdf/2006.06609.pdf)，如果你问一个模型鲸鱼是否有肚脐，模型将错误地回答“没有”。这是因为模型的参数中没有存储这条信息。作者建议可以通过提供隐性知识的提示来帮助模型：“鲸鱼是哺乳动物”。
- en: '![](../Images/80dc45de359daf253e1afdba6a7637c1.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/80dc45de359daf253e1afdba6a7637c1.png)'
- en: 'source: [here](https://arxiv.org/pdf/2006.06609.pdf)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[这里](https://arxiv.org/pdf/2006.06609.pdf)
- en: 'The idea of providing implicit knowledge has paved the way for the possibility
    that systems can improve themselves by interacting with users. The user can identify
    an error and provide the information to the model allowing it to correct itself.
    Or more precisely as defined by the authors:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 提供隐性知识的想法为系统通过与用户交互来改进自己铺平了道路。用户可以识别错误并向模型提供信息，允许其自我纠正。或者更准确地说，正如作者所定义的：
- en: This can be viewed as a form of “one-shot learning” that improves the model
    on-the-fly without further training, unlike most current work that relies on data
    collection and re-training for fixing model errors.
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这可以视为一种“单次学习”的形式，可以在不进行进一步训练的情况下即时改进模型，这与大多数当前依赖于数据收集和重新训练来修正模型错误的工作方式不同。
- en: So conceptually the idea is that a model can solve a problem whose exact answer
    it does not directly know by exploiting intermediate steps.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，这个想法是一个模型可以通过利用中间步骤来解决其确切答案并不直接知道的问题。
- en: '[As noted by Google](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html),
    prompting is allowing in-context few-shot learning. In other words, instead of
    fine-tuning an LM on a particular task, one can prompt the LM with a few input-output
    exemplars demonstrating the task. This method has proven to be extremely functional,
    especially for question answering. Also, as demonstrated in context learning it
    is [particularly effective for large models](https://arxiv.org/abs/2005.14165).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[正如谷歌所指出的](https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html)，提示使得上下文少样本学习成为可能。换句话说，除了对特定任务进行微调外，可以通过一些输入-输出示例来提示语言模型。这种方法被证明非常有效，尤其在问答系统中。此外，正如在上下文学习中所示，它对于[大型模型特别有效](https://arxiv.org/abs/2005.14165)。'
- en: '![](../Images/51a81dd2261eeecbc3a6dfdb574ecda2.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51a81dd2261eeecbc3a6dfdb574ecda2.png)'
- en: '“Aggregate performance for all 42 accuracy-denominated benchmarks While zero-shot
    performance improves steadily with model size, few-shot performance increases
    more rapidly, demonstrating that larger models are more proficient at in-context
    learning.” source: [here](https://arxiv.org/pdf/2005.14165.pdf)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: “对于所有42个准确度基准的整体表现，虽然零-shot性能随着模型规模的增加而稳步提升，但少-shot性能则更快增加，证明了更大的模型在上下文学习方面更为高效。”
    来源：[这里](https://arxiv.org/pdf/2005.14165.pdf)
- en: 'Google then proposed that one can allow the model to solve multi-step reasoning
    problems by including a few examples of the chain of thought via prompting only.
    For better understanding, here is an example of what changes between a classic
    prompt and a chain of thought prompt:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌随后提出，通过仅包含几个思维链的示例，可以让模型解决多步推理问题。为了更好地理解，以下是经典提示和思维链提示之间的变化示例：
- en: '![](../Images/fe105e20281bcdedc89613b6e0cc665f.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fe105e20281bcdedc89613b6e0cc665f.png)'
- en: '“Chain-of-thought prompting enables large language models to tackle complex
    arithmetic, commonsense, and symbolic reasoning tasks. Chain-of-thought reasoning
    processes are highlighted.” source: [here](https://arxiv.org/pdf/2201.11903.pdf)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of this method is that it requires neither changing LM’s weights
    nor a large training dataset.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: In short, we can say that the idea is that a complex problem can be decomposed
    into a series of intermediate steps that can be solved separately.
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It may seem like a small thing, but it actually means that this method can be
    applied to any problem you can solve using language.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 'The Google authors say that this is an emergent property of the model and that
    it emerges with a certain size of model capacity (they estimate about 100 B parameters).
    The authors have evaluated increasing models for solving math problems:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e080fc3f70908bcd1f8de1eb59a93f03.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
- en: '“: Chain-of-thought prompting enables large language models to solve challenging
    math problems”. source: [here](https://arxiv.org/pdf/2201.11903.pdf)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the authors note that model improvement does not come from increasing
    parameters, but by using a “chain of thought prompting, increasing model scale
    leads to improved performance that substantially outperforms standard prompting
    for large model sizes.”
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: This is also true for [commonsense reasoning](https://en.wikipedia.org/wiki/Commonsense_reasoning)
    (“reasoning about physical and human interactions under the presumption of general
    background knowledge”).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6eaadf276534702b66a03cafc02d8ab7.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
- en: '“: Examples of input, chain of thought, output triples for arithmetic, commonsense,
    and symbolic reasoning benchmark”. source: [here](https://arxiv.org/pdf/2201.11903.pdf)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 'In this, too, the model showed the same behavior: “performance improved with
    model scale, and employing a chain of thought prompting led to additional small
    improvements.” The greatest improvement was seen in the area of sports understanding
    (surprisingly).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'So in general we have seen that there are two techniques for CoT, fine-tuning
    or using prompting (in context learning). Regarding this second paradigm, we can
    further subdivide into:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '**Zero-Shot-CoT**. Kojima showed that LMs are decent zero-shot COT (simply
    adding “Let’s think step by step”) and this is enough to meaningfully improve
    zero-shot LLM for complex reasoning.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Few-Shot-CoT.** A few step-by-step reasoning demonstrations are used for
    model conditioning in inference. Each demonstration presents both a question and
    a reasoning chain that explains to the model how to arrive at the final answer
    (these demonstrations can be either hand-built or using automatic generation).'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/124ec295b0a57cc2601f62677c11d3ee.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: 'source: [here](https://arxiv.org/pdf/2205.11916.pdf)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Few-shot-CoT has since been shown to be more efficient and with better results
    (provided the demonstrations are well written). Therefore, most subsequent studies
    focused on this method.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4f31f9d790f23ee524f7a0b7b3087c5b.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f31f9d790f23ee524f7a0b7b3087c5b.png)'
- en: '“Typical CoT techniques (FT: fine-tuning; KD: knowledge distillation). Segment
    1: in-context learning techniques; Segment 2: fine-tuning techniques. To the best
    of our knowledge, our work is the first to study CoT reasoning in different modalities.
    Besides, we focus on 1B-models, without relying on the outputs of LLMs”. source:
    [here](https://arxiv.org/pdf/2302.00923.pdf)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '“典型的 CoT 技术（FT: 微调；KD: 知识蒸馏）。第 1 节：上下文学习技术；第 2 节：微调技术。根据我们所知，我们的工作是首次研究不同模态下的
    CoT 推理。此外，我们专注于 1B 模型，不依赖于 LLMs 的输出。” 来源：[这里](https://arxiv.org/pdf/2302.00923.pdf)'
- en: Multimodal Chain of Thought is challenging
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多模态链式思维具有挑战性
- en: '![](../Images/acf1b79a008b36938e2f394e6876ae94.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/acf1b79a008b36938e2f394e6876ae94.png)'
- en: image by [airfocus](https://unsplash.com/fr/@airfocus) on Unsplash
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 [airfocus](https://unsplash.com/fr/@airfocus) 在 Unsplash 上
- en: As we saw above, the chain of thought (CoT) has proven very useful for problems
    requiring complex reasoning. Many of the problems are not only textual but multimodal.
    For example, to solve a problem we may need to look at a picture. As we said CoT
    works only for problems that can be expressed in textual form. **How can we do
    it for multimodal problems?**
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，链式思维（CoT）在需要复杂推理的问题上证明了非常有用。许多问题不仅是文本的，还有多模态的。例如，解决一个问题我们可能需要查看图片。正如我们所说，CoT
    仅适用于可以用文本形式表达的问题。**我们如何处理多模态问题？**
- en: Imagine reading a textbook with no figures or tables. Our ability to knowledge
    acquisition is greatly strengthened by jointly modeling diverse data modalities,
    such as vision, language, and audio. ([source](https://arxiv.org/pdf/2302.00923.pdf))
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 想象一下阅读一本没有图形或表格的教科书。我们通过联合建模不同的数据模态（如视觉、语言和音频）来极大地增强知识获取能力。 ([来源](https://arxiv.org/pdf/2302.00923.pdf))
- en: 'A recent article posed exactly this problem and tried to extend CoT to multimodal
    problems as well:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 最近一篇文章正好提出了这个问题，并试图将 CoT 扩展到多模态问题中：
- en: '[](https://arxiv.org/abs/2302.00923?source=post_page-----961a8ab9d0fa--------------------------------)
    [## Multimodal Chain-of-Thought Reasoning in Language Models'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://arxiv.org/abs/2302.00923?source=post_page-----961a8ab9d0fa--------------------------------)
    [## 多模态链式思维推理在语言模型中的应用]'
- en: Large language models (LLMs) have shown impressive performance on complex reasoning
    by leveraging chain-of-thought…
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）通过利用链式思维在复杂推理上表现出色…
- en: arxiv.org](https://arxiv.org/abs/2302.00923?source=post_page-----961a8ab9d0fa--------------------------------)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[arxiv.org](https://arxiv.org/abs/2302.00923?source=post_page-----961a8ab9d0fa--------------------------------)'
- en: As noted earlier, [models under 100 billion parameters tend to produce illogical
    CoTs](https://arxiv.org/pdf/2206.07682.pdf), thus leading to incorrect answers.
    A [multi-modal model](https://en.wikipedia.org/wiki/Multimodal_learning) must
    not only handle textual input but also other modalities. This makes it difficult
    to create a model smaller than 100 B of parameters.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，[参数少于 100 亿的模型往往会产生不合逻辑的 CoT](https://arxiv.org/pdf/2206.07682.pdf)，从而导致错误的答案。一个
    [多模态模型](https://en.wikipedia.org/wiki/Multimodal_learning) 不仅要处理文本输入，还要处理其他模态。这使得创建一个参数少于
    100 亿的模型变得困难。
- en: On the other hand, META’s LLaMA showed that models trained with fewer 100 B
    parameters can achieve comparable results to much larger models.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，META 的 LLaMA 显示出，参数少于 100 亿的模型可以达到与更大模型相当的结果。
- en: '[](https://medium.com/mlearning-ai/metas-llama-a-small-language-model-beating-giants-5065948e0b7f?source=post_page-----961a8ab9d0fa--------------------------------)
    [## META’s LLaMA: A small language model beating giants'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/mlearning-ai/metas-llama-a-small-language-model-beating-giants-5065948e0b7f?source=post_page-----961a8ab9d0fa--------------------------------)
    [## META 的 LLaMA：一个击败巨头的小型语言模型]'
- en: META open-source model will help us to understand how LMs biases arise
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: META 开源模型将帮助我们理解语言模型的偏见如何产生
- en: medium.com](https://medium.com/mlearning-ai/metas-llama-a-small-language-model-beating-giants-5065948e0b7f?source=post_page-----961a8ab9d0fa--------------------------------)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/mlearning-ai/metas-llama-a-small-language-model-beating-giants-5065948e0b7f?source=post_page-----961a8ab9d0fa--------------------------------)
- en: In addition, as other studies have shown, a textual model did not see pictures
    during training and thus has no information about visual elements or how to exploit
    visual features.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，正如其他研究所示，文本模型在训练过程中没有看到图片，因此没有关于视觉元素或如何利用视觉特征的信息。
- en: 'CoT reasoning in a multimodal context requires the model to take into account
    the different modalities: given the inputs in different modalities, a model decomposes
    a multi-step problem into a series of intermediate stems and can then infer the
    answer.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在多模态环境中进行 CoT 推理要求模型考虑不同的模态：给定不同模态的输入，模型将一个多步骤的问题分解为一系列中间步骤，然后推断出答案。
- en: '![](../Images/3d2df89b5ca5751d1b77424fdce30e20.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d2df89b5ca5751d1b77424fdce30e20.png)'
- en: 'Example of the multimodal CoT task. source: [here](https://arxiv.org/pdf/2302.00923.pdf)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '多模态 CoT 任务示例。 来源: [这里](https://arxiv.org/pdf/2302.00923.pdf)'
- en: The most immediate way to perform Multimodal-CoT is to transform the input of
    different modalities into one modality and prompt LLMs to perform CoT. ([source](https://arxiv.org/pdf/2302.00923.pdf))
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 执行多模态 CoT 的最直接方法是将不同模态的输入转换为一种模态，并提示大型语言模型执行 CoT。 ([source](https://arxiv.org/pdf/2302.00923.pdf))
- en: For example, one could take an image and use it as input for a captioning model.
    Once the caption is obtained one could then use the obtained caption and join
    it to the textual prompt and then provide it to a large LM.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，可以取一张图像，并将其作为输入用于字幕生成模型。一旦获得字幕，就可以将其与文本提示结合起来，然后提供给大型语言模型。
- en: However, this approach has a serious drawback, the caption as opposed to the
    visual features loses a lot of information, so the mutual synergy between the
    information contained in the different modalities is lost.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法有一个严重的缺陷，即字幕与视觉特征相比丢失了大量信息，因此不同模态间的信息协同作用丧失了。
- en: In addition, it has been shown in previous studies that cross-modal alignment
    of pre-trained uni-modal models is not easy. For example, in BLIP-2 to allow a
    vision transformer and a language model to talk to each other they needed an additional
    transformer in between.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，之前的研究表明，预训练的单模态模型的跨模态对齐并不容易。例如，在 BLIP-2 中，为了使视觉变换器和语言模型能够互相交流，他们需要在两者之间增加一个额外的变换器。
- en: '[](https://levelup.gitconnected.com/blip-2-when-chatgpt-meets-images-463582b541e0?source=post_page-----961a8ab9d0fa--------------------------------)
    [## BLIP-2: when ChatGPT meets images'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[## BLIP-2: when ChatGPT meets images](https://levelup.gitconnected.com/blip-2-when-chatgpt-meets-images-463582b541e0?source=post_page-----961a8ab9d0fa--------------------------------)'
- en: BLIP-2, a new visual language model capable to dialogue about images
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: BLIP-2 是一种新的视觉语言模型，能够进行关于图像的对话。
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/blip-2-when-chatgpt-meets-images-463582b541e0?source=post_page-----961a8ab9d0fa--------------------------------)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[levelup.gitconnected.com](https://levelup.gitconnected.com/blip-2-when-chatgpt-meets-images-463582b541e0?source=post_page-----961a8ab9d0fa--------------------------------)'
- en: Considering these challenges, the authors decided to investigate whether it
    is possible to train a 1 B model of parameters for multimodal CoT.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些挑战，作者决定研究是否可以训练一个具有 10 亿参数的多模态 CoT 模型。
- en: This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade
    GPUs (e.g., 32G memory). In this section, we will investigate why 1B-models fail
    at CoT reasoning and study how to design an effective approach to overcome the
    challenge. ([source](https://arxiv.org/pdf/2302.00923.pdf))
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这项工作集中在 10 亿模型上，因为它们可以用消费级 GPU（例如 32G 内存）进行微调和部署。在这一部分，我们将研究为什么 10 亿模型在 CoT
    推理中失败，并研究如何设计有效的方法以克服这一挑战。 ([source](https://arxiv.org/pdf/2302.00923.pdf))
- en: Why do small models fail in CoT and how to design to overcome the challenge?
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么小模型在 CoT 中失败，如何设计以克服这一挑战？
- en: '![](../Images/0d7373870be1ed4a86f40f8f61dfe6d2.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0d7373870be1ed4a86f40f8f61dfe6d2.png)'
- en: image by [Jason Leung](https://unsplash.com/fr/@ninjason) on Unsplash all
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Jason Leung](https://unsplash.com/fr/@ninjason) 提供，来源于 Unsplash
- en: Actually, an approach to train small models to reason had already been tried.
    However, previous attempts had used a large model as the teacher and a small model
    as a student.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，训练小模型进行推理的方法已经被尝试过。然而，之前的尝试中使用了一个大型模型作为教师和一个小型模型作为学生。
- en: For example, the authors provided, the teacher model with a prompt and used
    the “Let’s think step by step” method to get answers that explained the reasoning.
    The prompt plus demonstration was then provided to the smaller model.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，作者为教师模型提供了一个提示，并使用了“让我们一步一步思考”的方法来获得解释推理的答案。然后，将提示加上演示提供给较小的模型。
- en: '![](../Images/84f4aedada0c72c493bcc5a5224755ac.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84f4aedada0c72c493bcc5a5224755ac.png)'
- en: “We consider a method consisting of multiple stages. First, a large teacher
    model is prompted to answer questions using multi-step reasoning, without relying
    on correct examples. That is, the teacher employs zero-shot chain-of-thought reasoning
    to generate output. We then use the resulting reasoning samples (consisting of
    the question and teacher output) for fine-tuning a much smaller student model.”
    image source ([here](https://arxiv.org/pdf/2212.10071.pdf))
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: “我们考虑一种由多个阶段组成的方法。首先，使用多步骤推理提示一个大型教师模型回答问题，而不依赖于正确的示例。也就是说，教师采用零-shot链式思维推理生成输出。然后，我们使用生成的推理样本（包括问题和教师输出）对更小的学生模型进行微调。”
    图像来源 ([here](https://arxiv.org/pdf/2212.10071.pdf))
- en: This approach, however, still requires the use of large LMs with all its drawbacks.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法仍然需要使用大型语言模型及其所有缺点。
- en: 'The authors instead decided to explore the possibility that a small model could
    be fine-tuned for multimode-CoT. In short, fusing multimodal features allows the
    model architecture to be able to be adjusted more flexibly (with respect to prompting).
    However, the main problem remains: “*The key challenge is that language models
    under 100 billion parameters tend to generate hallucinated rationales that mislead
    the answer inference*”.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 作者决定探索小型模型可以针对多模态-CoT进行微调的可能性。简而言之，融合多模态特征允许模型架构更加灵活地调整（与提示相关）。然而，主要问题仍然存在：“*关键挑战在于，参数少于100亿的语言模型往往生成幻觉理由，从而误导答案推断*。”
- en: First, why small models hallucinate with CoT?
  id: totrans-87
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 首先，为什么小模型在CoT推理中会出现幻觉？
- en: 'And it is the same question the authors asked themselves: to investigate why
    a 1-B model fails at CoT reasoning. Once this is understood study an effective
    approach.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提出了相同的问题：调查为什么1-B模型在CoT推理中失败。一旦理解了这一点，研究有效的方法。
- en: The authors started with fine-tuning a text-only baseline model for CoT reasoning.
    In this case, the problem is modeled as a text generation problem. The baseline
    is having the question (Q), context (C), and multiple options (O) the model must
    predict the answer (A). The authors compared the baseline with predicting the
    rationale (R) before the answer (QCM→RA) and the rationale is used for explaining
    the answer (QCM→AR).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 作者首先对文本-only基线模型进行了CoT推理的微调。在这种情况下，问题被建模为文本生成问题。基线包括问题（Q）、上下文（C）和多个选项（O），模型必须预测答案（A）。作者将基线与在答案前预测理由（R）（QCM→RA）以及理由用于解释答案（QCM→AR）进行了比较。
- en: '![](../Images/2146d70a184c982d35f2f28141176e52.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2146d70a184c982d35f2f28141176e52.png)'
- en: '[(source)](https://arxiv.org/pdf/2302.00923.pdf)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[(source)](https://arxiv.org/pdf/2302.00923.pdf)'
- en: 'The result is surprising, there is more than a 10 % accuracy decrease if the
    model predicts the rational first: “*The results imply that the rationales might
    not necessarily contribute to predicting the right answer.*” In other words, it
    almost seems that reasoning harms the answer.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 结果令人惊讶，如果模型首先预测理由，准确率下降超过10%：“*结果表明，理由可能不一定有助于预测正确答案*。”换句话说，似乎推理反而对答案有害。
- en: But why?
  id: totrans-93
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 但为什么呢？
- en: To try to understand this, the authors decided to separate the problem into
    two stages. First, generate the rationale and then use that to answer the question
    as well. The model succeeds in generating a quality rationale ([RougeL](https://en.wikipedia.org/wiki/ROUGE_(metric))
    is a metric used for automatic summarization and machine translation) but at the
    same time, it seems to harm the accuracy inference (the answer to the question).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这一点，作者决定将问题分成两个阶段。首先，生成理由，然后利用这些理由回答问题。模型在生成高质量理由方面成功了（[RougeL](https://en.wikipedia.org/wiki/ROUGE_(metric))是一种用于自动摘要和机器翻译的度量），但同时，似乎对准确性推断（问题的答案）产生了不利影响。
- en: '![](../Images/51cbf7e909c1943b99abc033957ab8db.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51cbf7e909c1943b99abc033957ab8db.png)'
- en: '[(source)](https://arxiv.org/pdf/2302.00923.pdf)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[(source)](https://arxiv.org/pdf/2302.00923.pdf)'
- en: The rationale does not help to improve answer accuracy. So the authors selected
    50 random error cases and inspected them manually. They saw that the model when
    generating rational often hallucinated because it lacked reference to the visual
    content.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 理由并没有帮助提高答案的准确性。因此，作者选择了50个随机错误案例并手动检查。发现模型在生成理由时经常出现幻觉，因为缺乏对视觉内容的参考。
- en: '![](../Images/c06139df299f4b78e12e963813f31e7a.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c06139df299f4b78e12e963813f31e7a.png)'
- en: '[(source)](https://arxiv.org/pdf/2302.00923.pdf)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[(source)](https://arxiv.org/pdf/2302.00923.pdf)'
- en: This was the most common error, more than 60 percent of the errors were attributable
    to this factor.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b3633d5eba9fad4d0720544d6c9f3718.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
- en: '[(source)](https://arxiv.org/pdf/2302.00923.pdf)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '**So why not provide them with information about what is inside the image?**
    The authors used a pipeline to generate captions and provide them to the model
    (append the captions to the input). However, this resulted in an increase in marginal
    accuracy (0.59 percent, in Table 3).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: The authors then tested another approach, took the image, and used it as input
    to the [DETR](https://github.com/facebookresearch/detr) model with the aim of
    extracting vision features. They then combined these vision features with the
    encoded language representation. In other words, the text is encoded by the LM
    encoder, the image is encoded by the vision model. These two outputs are combined
    and become the input to the LM’s decoder.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: The result shows (Table 3, above) that it improves not only the generation of
    the rationale but also the accuracy of the response. In other words, with a better
    rationale “the phenomenon of hallucination is mitigated.” Vision features are
    beneficial for better response, but probably this useful information is lost in
    the process of captioning.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Having understood why the model hallucinated, what framework can we use for
    efficient multimodal-CoT?
  id: totrans-106
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The authors propose that of incorporating language (the text) and vision (the
    images) modalities into a two-stage framework: in which the rationale is generated
    first and the response is generated later.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: The model architecture is the same for both steps; however, the inputs and outputs
    change. In the first step, the model is given language and vision inputs to generate
    rationales. In the second step of the second model, you provide d the original
    language input which is appended to the rationale generated from the first stage.
    This is passing by the encoder of the second model, then you add the vision features
    and use the decoder to get the final answer
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8d57433fa30ff894cf4d070962bf2bad.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
- en: '“Overview of our Multimodal-CoT framework. Multimodal-CoT consists of two stages:
    (i) rationale generation and (ii) answer inference. Both stages share the same
    model architecture but differ in the input and output. In the first stage, we
    feed the model with language and vision inputs to generate rationales. In the
    second stage, we append the original language input with the rationale generated
    from the first stage. Then, we feed the updated language input with the original
    vision input to the model to infer the answer.” [(source)](https://arxiv.org/pdf/2302.00923.pdf)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Can a small model be competitive?
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](../Images/535bf546836d7acf7591f1219306871d.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: photo by [Steven Lelham](https://unsplash.com/it/@slelham) on Unsplash
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: We have seen why small models hallucinate during CoT, how to solve the problem,
    it remains to be understood whether this approach is competitive compared to larger
    models and other approaches.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了为什么小模型在CoT过程中会产生幻觉，如何解决这个问题，现在还需要了解这种方法是否在与大型模型和其他方法相比时具有竞争力。
- en: 'The authors decided to use the ScienceQA benchmark:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 作者决定使用ScienceQA基准：
- en: ScienceQA is the first large-scale multimodal science question dataset that
    annotates the answers with detailed lectures and explanations. It contains 21k
    multimodal multiple choice questions with rich domain diversity across 3 subjects,
    26 topics, 127 categories, and 379 skills. [(source)](https://arxiv.org/pdf/2302.00923.pdf)
  id: totrans-116
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ScienceQA是首个大规模的多模态科学问题数据集，注释了详细的讲解和解释。它包含21,000个多模态选择题，涵盖了3个学科、26个主题、127个类别和379项技能的丰富领域多样性。
    [(来源)](https://arxiv.org/pdf/2302.00923.pdf)
- en: In order to use vision features, they needed a model that uses an encoder-decoder
    so they chose [T5](https://huggingface.co/docs/transformers/model_doc/t5). In
    addition, to better study whether the approach generalizes with other models they
    also chose [FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5).
    They also decided to compare it with a number of models and with humans.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用视觉特征，他们需要一个使用编码器-解码器的模型，因此选择了[T5](https://huggingface.co/docs/transformers/model_doc/t5)。此外，为了更好地研究该方法是否对其他模型具有通用性，他们还选择了[FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5)。他们还决定将其与多个模型和人类进行比较。
- en: The result shows that their approach outperforms GPT-3.5 and also outperforms
    humans (both on average and in the various classes of questions). UnifiedQA and
    GPT-3.5 use captions, the result shows that vision features are more effective.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，他们的方法优于GPT-3.5，并且在各种问题类别中也超越了人类（无论是平均水平还是在不同类别中）。UnifiedQA和GPT-3.5使用了字幕，结果表明视觉特征更为有效。
- en: '![](../Images/1929fc5a46b1dfcc2564668c653efea7.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1929fc5a46b1dfcc2564668c653efea7.png)'
- en: '[(source)](https://arxiv.org/pdf/2302.00923.pdf)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[(来源)](https://arxiv.org/pdf/2302.00923.pdf)'
- en: Ablation studies show that using a two-stages approach leverages the best from
    the vision features.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 消融研究表明，使用两阶段方法能够充分发挥视觉特征的优势。
- en: '![](../Images/45f9f2d4bfd4427b0d97bd7931d2b47f.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/45f9f2d4bfd4427b0d97bd7931d2b47f.png)'
- en: '[(source)](https://arxiv.org/pdf/2302.00923.pdf)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[(来源)](https://arxiv.org/pdf/2302.00923.pdf)'
- en: In addition, the authors note that multimodality boosts convergence. Practically,
    the two-stage model achieves higher accuracy from the beginning of training.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，作者指出，多模态性能提升收敛性。实际上，这种两阶段模型从训练开始时就能实现更高的准确性。
- en: '![](../Images/c2ea5f39f79dcf7d6ba95d44b25c8a1b.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c2ea5f39f79dcf7d6ba95d44b25c8a1b.png)'
- en: “Accuracy curve of the No-CoT baseline and MultimodalCoT variants across epochs.”
    [(source)](https://arxiv.org/pdf/2302.00923.pdf)
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: “No-CoT基准和MultimodalCoT变体在各个时期的准确性曲线。” [(来源)](https://arxiv.org/pdf/2302.00923.pdf)
- en: The authors say that the approach is generalizable with different models to
    extract vision features, they then chose DETR because it gave the best accuracy.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 作者表示，该方法具有广泛的模型通用性来提取视觉特征，他们选择了DETR，因为它提供了最佳的准确性。
- en: '![](../Images/eb8903811f64cc692608c9018b9d2eb9.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eb8903811f64cc692608c9018b9d2eb9.png)'
- en: '[(source)](https://arxiv.org/pdf/2302.00923.pdf)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[(来源)](https://arxiv.org/pdf/2302.00923.pdf)'
- en: And the textual model that is chosen is also generalizable. That is, the approach
    works even with a different LM model.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 选择的文本模型也是具有通用性的。也就是说，该方法即使在使用不同的语言模型时也能有效。
- en: '![](../Images/f7ae7075d0f2ac3fe50e42b801924860.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f7ae7075d0f2ac3fe50e42b801924860.png)'
- en: '[(source)](https://arxiv.org/pdf/2302.00923.pdf)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[(来源)](https://arxiv.org/pdf/2302.00923.pdf)'
- en: The authors then inspected 50 examples for which the answer was correct and
    50 for which the answer was incorrect instead, to better understand the mechanism.
    The result shows that the CoT is not always beneficial for the answer, but the
    model is robust and in some cases is able to answer correctly even if the rationale
    is wrong. Moreover, when the answer is incorrect most of the errors are due to
    commonsense mistakes.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，作者检查了50个答案正确的例子和50个答案错误的例子，以更好地理解机制。结果表明，CoT并不总是对答案有利，但模型非常稳健，在某些情况下即使推理错误也能正确回答。此外，当答案错误时，大多数错误是由于常识性错误。
- en: '![](../Images/c2c13be5e6be851b4efd753c8fd8cc4e.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c2c13be5e6be851b4efd753c8fd8cc4e.png)'
- en: '[(source)](https://arxiv.org/pdf/2302.00923.pdf)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[(来源)](https://arxiv.org/pdf/2302.00923.pdf)'
- en: 'The model in most makes commonsense errors when the question requires commonsense
    knowledge: for example, understanding a map or counting numbers in the image,
    or using the alphabet. An example of an error:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 当问题需要常识知识时，该模型在大多数情况下会出现常识错误：例如，理解地图或计算图像中的数字，或使用字母表。错误示例：
- en: '![](../Images/9d0c25aae5e3728a937c8fac70f1ea22.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d0c25aae5e3728a937c8fac70f1ea22.png)'
- en: '[(source)](https://arxiv.org/pdf/2302.00923.pdf)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[(source)](https://arxiv.org/pdf/2302.00923.pdf)'
- en: 'The authors state that the results of this one are a cue to modify the model
    prospectively:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 作者表示，这些结果为模型的未来修改提供了线索：
- en: It is possible to improve MultimodalCoT by (i) incorporating more informative
    vision features and improving language-vision interaction to be capable of understanding
    maps and counting numbers; (ii) injecting commonsense knowledge; (iii) applying
    a filtering mechanism, e.g., using only the effective CoT to infer the answer
    and get rid of irrelevant CoT. [(source)](https://arxiv.org/pdf/2302.00923.pdf)
  id: totrans-140
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 可以通过以下方式改进MultimodalCoT：（i）结合更有信息量的视觉特征，改善语言-视觉交互，以便理解地图和计数；（ii）注入常识知识；（iii）应用过滤机制，例如，只使用有效的CoT来推断答案，并去除无关的CoT。[(source)](https://arxiv.org/pdf/2302.00923.pdf)
- en: 'The authors have made the model, both code and dataset available on GitHub
    for those who want to test it or learn more about it:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 作者已将模型、代码和数据集上传到GitHub，供希望测试或了解更多的人使用：
- en: '[](https://github.com/amazon-science/mm-cot?source=post_page-----961a8ab9d0fa--------------------------------)
    [## GitHub - amazon-science/mm-cot: Official implementation for "Multimodal Chain-of-Thought
    Reasoning…'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/amazon-science/mm-cot?source=post_page-----961a8ab9d0fa--------------------------------)
    [## GitHub - amazon-science/mm-cot: “Multimodal Chain-of-Thought Reasoning”的官方实现…]'
- en: '"Imagine learning a textbook without figures or tables." Multimodal-CoT incorporates
    vision features in a decoupled…'
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: “想象一下学习一本没有图表的教科书。”Multimodal-CoT在解耦的方式中融入了视觉特征…
- en: github.com](https://github.com/amazon-science/mm-cot?source=post_page-----961a8ab9d0fa--------------------------------)
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/amazon-science/mm-cot?source=post_page-----961a8ab9d0fa--------------------------------)
- en: '**Parting thoughts**'
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**告别思考**'
- en: The authors in this study formally studied multimodal CoT. They analyzed why
    a small model hallucinates during CoT and showed that a small model is capable
    of outperforming large models in multimodal CoT (even outperforming human performance).
    The key is to be able to best combine textual and visual modalities.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究中的作者正式研究了多模态CoT。他们分析了为什么小模型在CoT期间会产生幻觉，并展示了小模型在多模态CoT中能够超越大型模型（甚至超越人类表现）的能力。关键是能够最佳地结合文本和视觉模态。
- en: This is achieved by using a two-stage approach, in the first the visual features
    are used to create the rationale and then exploit this best rationale to be able
    to get the answer. The analysis then conducted by the authors gives suggestions
    on how to get even better models.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过使用两阶段方法实现的，第一阶段使用视觉特征创建推理，然后利用这一最佳推理来获得答案。作者进行的分析给出了如何获得更好模型的建议。
- en: In short, the results of this paper show that even a small model can solve complex
    problems. Moreover, providing the right multimodal features is essential for the
    model. One does not need a large LM with billions of parameters, because captioning
    works worse than a small model that is aware of vision features.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，这篇论文的结果表明，即使是一个小模型也能解决复杂问题。此外，为模型提供正确的多模态特征是至关重要的。无需一个拥有数十亿参数的大型语言模型，因为对视觉特征有了解的小模型在图像描述方面表现得更好。
- en: 'If you have found this interesting:'
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如果你觉得这些内容有趣：
- en: You can look for my other articles, you can also [**subscribe**](https://salvatore-raieli.medium.com/subscribe)
    to get notified when I publish articles, and you can also connect or reach me
    on[**LinkedIn**](https://www.linkedin.com/in/salvatore-raieli/)**.**
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以查看我的其他文章，也可以[**订阅**](https://salvatore-raieli.medium.com/subscribe)以便在我发布新文章时获得通知，你还可以在[**LinkedIn**](https://www.linkedin.com/in/salvatore-raieli/)**上联系我**。
- en: Here is the link to my GitHub repository, where I am planning to collect code
    and many resources related to machine learning, artificial intelligence, and more.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我的GitHub仓库的链接，我计划在这里收集与机器学习、人工智能等相关的代码和许多资源。
- en: '[](https://github.com/SalvatoreRa/tutorial?source=post_page-----961a8ab9d0fa--------------------------------)
    [## GitHub - SalvatoreRa/tutorial: Tutorials on machine learning, artificial intelligence,
    data science…'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/SalvatoreRa/tutorial?source=post_page-----961a8ab9d0fa--------------------------------)
    [## GitHub - SalvatoreRa/tutorial: 机器学习、人工智能、数据科学教程…]'
- en: Tutorials on machine learning, artificial intelligence, data science with math
    explanation and reusable code (in python…
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: github.com](https://github.com/SalvatoreRa/tutorial?source=post_page-----961a8ab9d0fa--------------------------------)
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'or you may be interested in one of my recent articles:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://pub.towardsai.net/pca-bioinformaticians-favorite-tool-can-be-misleading-fe139262a576?source=post_page-----961a8ab9d0fa--------------------------------)
    [## PCA: Bioinformatician’s Favorite Tool Can Be Misleading'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: A new study assesses how a most used technique can be problematic
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'pub.towardsai.net](https://pub.towardsai.net/pca-bioinformaticians-favorite-tool-can-be-misleading-fe139262a576?source=post_page-----961a8ab9d0fa--------------------------------)
    [](https://levelup.gitconnected.com/stable-diffusion-and-the-brain-how-ai-can-read-our-minds-45398b395ea9?source=post_page-----961a8ab9d0fa--------------------------------)
    [## Stable diffusion and the brain: how AI can read our minds'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Researchers were able to reconstruct images using fMRI data
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'levelup.gitconnected.com](https://levelup.gitconnected.com/stable-diffusion-and-the-brain-how-ai-can-read-our-minds-45398b395ea9?source=post_page-----961a8ab9d0fa--------------------------------)
    [](https://levelup.gitconnected.com/microsoft-biogpt-towards-the-chatgpt-of-life-science-56e251536af6?source=post_page-----961a8ab9d0fa--------------------------------)
    [## Microsoft BioGPT: Towards the ChatGPT of life science?'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: BioGPT achieves the SOTA in different biomedical NLP tasks
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/microsoft-biogpt-towards-the-chatgpt-of-life-science-56e251536af6?source=post_page-----961a8ab9d0fa--------------------------------)
    [](https://levelup.gitconnected.com/stable-diffusion-to-fill-gaps-in-medical-image-data-b78a2a7d6c9d?source=post_page-----961a8ab9d0fa--------------------------------)
    [## Stable diffusion to fill gaps in medical image data
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: A new study shows that stable diffusion could help with medical image analysis
    and rare diseases. How?
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/stable-diffusion-to-fill-gaps-in-medical-image-data-b78a2a7d6c9d?source=post_page-----961a8ab9d0fa--------------------------------)
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
