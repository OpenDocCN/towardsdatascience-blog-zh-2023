["```py\nfrom base64 import b64encode\nimport cv2\nimport mediapipe as mp\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nimport numpy as np\nfrom natsort import natsorted\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.animation import FuncAnimation\nfrom IPython.display import clear_output\n%matplotlib inline\nimport pandas as pd\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom IPython.display import HTML, display\nimport ipywidgets as widgets\nfrom typing import List # I don't think I need this!\n\n# Custom imports\nfrom pose_tracking_utils import *\n\nmp_drawing = mp.solutions.drawing_utils\nmp_drawing_styles = mp.solutions.drawing_styles\nmp_pose = mp.solutions.pose\n\ndef create_pose_tracking_video(video_path):\n    # For webcam input:\n    cap = cv2.VideoCapture(video_path)\n    frame_width = int(cap.get(3))\n    frame_height = int(cap.get(4))\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    output_path = pathlib.Path(video_path).stem + \"_pose.mp4\" \n    out = cv2.VideoWriter(output_path, fourcc, 30.0, (frame_width, frame_height))\n    with mp_pose.Pose(min_detection_confidence=0.5,\n                      min_tracking_confidence=0.5) as pose:\n        while cap.isOpened():\n            success, image = cap.read()\n            if not success:\n                print(\"Ignoring empty camera frame.\")\n                break\n            # To improve performance, optinally mark the iamge as \n            # not writeable to pass by reference.\n            image.flags.writeable = False\n            image= cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            results = pose.process(image)\n            # Draw the annotation on the image.\n            image.flags.writeable = True\n            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n            mp_drawing.draw_landmarks(image, results.pose_landmarks,\n                                      mp_pose.POSE_CONNECTIONS,\n            landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())\n\n            # Flip the image horizontally for a self-view display.\n            out.write(cv2.flip(image, 1))\n            if cv2.waitKey(5) & 0xFF == 27:\n                break\n\n    cap.release()\n    out.release()\n    print(\"Pose video created!\")\n\n    return output_path\n```", "```py\nVIDEO_PATH = \"./videos/clip_training_session_1.mp4\"\n# Initialize MediaPipe Pose model\nbody_part_index = 32\npose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n\n# Initialize OpenCV VideoCapture object to capture video from the camera\ncap = cv2.VideoCapture(VIDEO_PATH)\n\n# Create an empty list to store the trace of the right elbow\ntrace = []\n\n# Create empty lists to store the x, y, z coordinates of the right elbow\nx_vals = []\ny_vals = []\nz_vals = []\n\n# Create a Matplotlib figure and subplot for the real-time updating plot\n# fig, ax = plt.subplots()\n# plt.title('Time Lapse of the X Coordinate')\n# plt.xlabel('Frames')\n# plt.ylabel('Coordinate Value')\n# plt.xlim(0,1)\n# plt.ylim(0,1)\n# plt.ion()\n# plt.show()\nframe_num = 0\n\nwhile True:\n    # Read a frame from the video capture\n    success, image = cap.read()\n    if not success:\n        break\n    # Convert the frame to RGB format\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    # Process the frame with MediaPipe Pose model\n    results = pose.process(image)\n\n    # Check if any body parts are detected\n\n    if results.pose_landmarks:\n        # Get the x,y,z coordinates of the right elbow\n        x, y, z = results.pose_landmarks.landmark[body_part_index].x, results.pose_landmarks.landmark[body_part_index].y, results.pose_landmarks.landmark[body_part_index].z\n\n        # Append the x, y, z values to the corresponding lists\n        x_vals.append(x)\n        y_vals.append(y)\n        z_vals.append(z)\n\n        # # Add the (x, y) coordinates to the trace list\n        trace.append((int(x * image.shape[1]), int(y * image.shape[0])))\n\n        # Draw the trace on the image\n        for i in range(len(trace)-1):\n            cv2.line(image, trace[i], trace[i+1], (255, 0, 0), thickness=2)\n\n        plt.title('Time Lapse of the Y Coordinate')\n        plt.xlabel('Frames')\n        plt.ylabel('Coordinate Value')\n        plt.xlim(0,len(pose_coords))\n        plt.ylim(0,1)\n        plt.plot(y_vals);\n        # Clear the plot and update with the new x, y, z coordinate values\n        #ax.clear()\n        # ax.plot(range(0, frame_num + 1), x_vals, 'r.', label='x')\n        # ax.plot(range(0, frame_num + 1), y_vals, 'g.', label='y')\n        # ax.plot(range(0, frame_num + 1), z_vals, 'b.', label='z')\n        # ax.legend(loc='upper left')\n        # plt.draw()\n        plt.pause(0.00000000001)\n        clear_output(wait=True)\n        frame_num += 1\n\n    # Convert the image back to BGR format for display\n    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n\n    # Display the image\n    cv2.imshow('Pose Tracking', image)\n\n    # Wait for user input to exit\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\n# Release the video capture, close all windows, and clear the plot\ncap.release()\ncv2.destroyAllWindows()\nplt.close()\n```", "```py\nplt.figure(figsize=(15,7))\nplt.subplot(3,1,1)\nplt.title('Time Lapse of the x Coordinate')\nplt.xlabel('Frames')\nplt.ylabel('Coordinate Value')\nplt.xlim(0,len(pose_coords))\nplt.ylim(0,1)\nplt.plot(x_vals)\n\nplt.subplot(3,1,2)\nplt.title('Time Lapse of the y Coordinate')\nplt.xlabel('Frames')\nplt.ylabel('Coordinate Value')\nplt.xlim(0,len(pose_coords))\nplt.ylim(0,1.1)\nplt.plot(y_vals)\n\nplt.subplot(3,1,3)\nplt.title('Time Lapse of the z Coordinate')\nplt.xlabel('Frames')\nplt.ylabel('Coordinate Value')\nplt.xlim(0,len(pose_coords))\nplt.ylim(-1,1)\nplt.plot(z_vals)\n\nplt.tight_layout();\n```", "```py\ndef create_joint_trace_video(video_path,body_part_index=32, color_rgb=(255,0,0)):\n    \"\"\"\n    This function creates a trace of the body part being tracked.\n    body_part_index: The index of the body part being tracked.\n    video_path: The path to the video being analysed.\n    \"\"\"\n    # Initialize MediaPipe Pose modelpose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n\n    # Initialize OpenCV VideoCapture object to capture video from the camera\n    cap = cv2.VideoCapture(video_path)\n    frame_width = int(cap.get(3))\n    frame_height = int(cap.get(4))\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    output_path = pathlib.Path(video_path).stem + \"_trace.mp4\" \n    out = cv2.VideoWriter(output_path, fourcc, 30.0, (frame_width, frame_height))\n\n    # Create an empty list to store the trace of the body part being tracked\n    trace = []\n\n    with mp_pose.Pose(min_detection_confidence=0.5,\n                        min_tracking_confidence=0.5) as pose:\n        while cap.isOpened():\n            success, image = cap.read()\n            if not success:\n                print(\"Ignoring empty camera frame.\")\n                break\n\n            # Convert the frame to RGB format\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n            # Process the frame with MediaPipe Pose model\n            results = pose.process(image)\n\n            # Check if any body parts are detected\n            if results.pose_landmarks:\n                # Get the x,y coordinates of the body part being tracked (in this case, the right elbow)\n                x, y = int(results.pose_landmarks.landmark[body_part_index].x * image.shape[1]), int(results.pose_landmarks.landmark[body_part_index].y * image.shape[0])\n\n                # Add the coordinates to the trace list\n                trace.append((x, y))\n\n                # Draw the trace on the image\n                for i in range(len(trace)-1):\n                    cv2.line(image, trace[i], trace[i+1], color_rgb, thickness=2)\n\n            # Convert the image back to BGR format for display\n            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n\n            # Display the image\n            out.write(image)\n            if cv2.waitKey(5) & 0xFF == 27:\n                break\n\n    cap.release()\n    out.release()\n    print(\"Joint Trace video created!\")\n```", "```py\ndef get_joint_trace_data(video_path, body_part_index,xmin=300,xmax=1000,\n                             ymin=200,ymax=800):\n    \"\"\"\n    Creates a graph with the tracing of a particular body part,\n    while executing a certain movement.\n    \"\"\"\n    cap = cv2.VideoCapture(video_path)\n    frame_width = int(cap.get(3))\n    frame_height = int(cap.get(4))\n\n    # Create an empty list to store the trace of the body part being tracked\n    trace = []\n    i = 0\n    with mp_pose.Pose(min_detection_confidence=0.5,\n                    min_tracking_confidence=0.5) as pose:\n        while cap.isOpened():\n            success, image = cap.read()\n            if not success:\n                print(\"Ignoring empty camera frame.\")\n                break\n\n            # Convert the frame to RGB format\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n            # Process the frame with MediaPipe Pose model\n            results = pose.process(image)\n\n            # Check if any body parts are detected\n            if results.pose_landmarks:\n                # Get the x,y coordinates of the body part being tracked (in this case, the right elbow)\n                x, y = int(results.pose_landmarks.landmark[body_part_index].x * image.shape[1]), int(results.pose_landmarks.landmark[body_part_index].y * image.shape[0])\n\n                # Add the coordinates to the trace list\n                trace.append((x, y))\n\n                # Plot the trace on the graph\n                fig, ax = plt.subplots()\n                #ax.imshow(image)\n                ax.set_xlim(xmin,xmax)\n                ax.set_ylim(ymin,ymax)\n                ax.invert_yaxis()\n                ax.plot(np.array(trace)[:, 0], np.array(trace)[:, 1], color='r')\n                # plt.savefig(f'joint_trace{i}.png')\n                # plt.close()\n                i+=1\n                plt.pause(0.00000000001)\n                clear_output(wait=True)\n                # Display the graph\n                #plt.show()\n\n            if cv2.waitKey(5) & 0xFF == 27:\n                break\n\n        cap.release()\n\n        return trace\n\nvideo_path = \"./videos/clip_training_session_2.mp4\"\nbody_part_index = 31\nfoot_trace = get_joint_trace_data(video_path, body_part_index)\n\nvideo_path = \"./videos/uchimata_wall.mp4\"\nbody_part_index = 31\nfoot_trace_reference = get_joint_trace_data(video_path, body_part_index,xmin=0,ymin=0,xmax=1300)\n\nfoot_trace_clip = foot_trace[:len(foot_trace_reference)]\nplt.subplot(1,2,1)\nplt.plot(np.array(foot_trace_clip)[:, 0], np.array(foot_trace_clip)[:, 1], color='r')\nplt.gca().invert_yaxis();\n\nplt.subplot(1,2,2)\nplt.plot(np.array(foot_trace_reference)[:, 0], np.array(foot_trace_reference)[:, 1], color='g')\nplt.gca().invert_yaxis();\n```", "```py\nfrom fastdtw import fastdtw\nfrom scipy.spatial.distance import euclidean\n\nmax_x = max(max(foot_trace_clip, key=lambda x: x[0])[0], max(foot_trace_reference, key=lambda x: x[0])[0])\nmax_y = max(max(foot_trace_clip, key=lambda x: x[1])[1], max(foot_trace_reference, key=lambda x: x[1])[1])\n\nfoot_trace_clip_norm = [(x/max_x, y/max_y) for (x, y) in foot_trace_clip]\nfoot_trace_reference_norm = [(x/max_x, y/max_y) for (x, y) in foot_trace_reference]\n\ndistance, path = fastdtw(foot_trace_clip_norm, foot_trace_reference_norm, dist=euclidean)\n```", "```py\nfoot_trace_reference_norm_mapped = [foot_trace_reference_norm[path[i][1]] for i in range(len(path))]\nfoot_trace_clip_norm_mapped = [foot_trace_clip_norm[path[i][1]] for i in range(len(path))]\n\nplt.subplot(1,2,1)\nplt.plot(np.array(foot_trace_reference_norm_mapped)[:, 0], np.array(foot_trace_reference_norm_mapped)[:, 1], color='g')\nplt.gca().invert_yaxis();\n\nplt.subplot(1,2,2)\nplt.plot(np.array(foot_trace_clip_norm_mapped)[:, 0], np.array(foot_trace_clip_norm_mapped)[:, 1], color='r')\nplt.gca().invert_yaxis();\nplt.show()\n```", "```py\ndef find_individual_traces(trace,window_size=60, color_plot=\"r\"):\n    \"\"\"\n    Function that takes in a liste of tuples containing x,y coordinates\n    and plots them as different clips with varying sizes to allow the user to find\n    the point where a full repetition has been completed\n    \"\"\"\n\n    clip_size = 0\n    for i in range(len(trace)//window_size):\n        plt.plot(np.array(trace[clip_size:clip_size+window_size])[:, 0], np.array(trace[clip_size:clip_size+window_size])[:, 1], color=color_plot)\n        plt.gca().invert_yaxis()\n        plt.title(f\"Trace, clip size = {clip_size}\")\n        plt.show()\n        clip_size+=window_size\n\ndef get_individual_traces(trace, clip_size):\n    num_clips = len(trace)//clip_size\n    trace_clips = []\n    i = 0\n    for clip in range(num_clips):\n        trace_clips.append(trace[i:i+clip_size])\n        i+=clip_size\n\n    return trace_clips\n\nfind_individual_traces(foot_trace_clip_norm)\n```", "```py\nfind_individual_traces(foot_trace_reference_norm, window_size=45,color_plot=\"g\")\n```", "```py\nvideo_path = \"./videos/clip_training_session_3.mp4\"\nbody_part_index = 31\nfoot_trace_clip = get_joint_trace_data(video_path, body_part_index)\n\nvideo_path = \"./videos/uchimata_wall.mp4\"\nbody_part_index = 31\nfoot_trace_reference = get_joint_trace_data(video_path, body_part_index,xmin=0,ymin=0,xmax=1300)\n\n# Showing a plot with the tracings from the training session\nplt.plot(np.array(foot_trace_clip)[:, 0], np.array(foot_trace_clip)[:, 1], color='r')\nplt.gca().invert_yaxis();\n```", "```py\nmax_x = max(max(foot_trace_clip, key=lambda x: x[0])[0], max(foot_trace_reference, key=lambda x: x[0])[0])\nmax_y = max(max(foot_trace_clip, key=lambda x: x[1])[1], max(foot_trace_reference, key=lambda x: x[1])[1])\n\nfoot_trace_clip_norm = [(x/max_x, y/max_y) for (x, y) in foot_trace_clip]\nfoot_trace_reference_norm = [(x/max_x, y/max_y) for (x, y) in foot_trace_reference]\n```", "```py\ntraces = get_individual_traces(foot_trace_clip_norm, clip_size=67)\ntraces_ref = get_individual_traces(foot_trace_reference_norm, clip_size=60)\n```", "```py\n# Here I show an example trace from the new clip\nindex = 0\ncolor_plot = \"black\"\nplt.plot(np.array(traces[index])[:, 0], np.array(traces[index])[:, 1], color=color_plot)\nplt.gca().invert_yaxis()\nplt.title(f\"Trace {index}\")\nplt.show()\n```", "```py\ntrace_ref = traces[2]\ntrace_scores = []\n\nfor trace in traces:\n    distance, path = fastdtw(trace, trace_ref, dist=euclidean)\n    trace_scores.append(distance)\n\nplt.plot(trace_scores, color=\"black\")\nplt.title(\"Trace Scores with DTW\")\nplt.xlabel(\"Trace Index\")\nplt.ylabel(\"Euclidean Distance Score\")\nplt.show()\n```"]