- en: Public Benchmarks for Medical Natural Language Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/public-benchmarks-for-medical-natural-language-processing-c7c794ab4d9](https://towardsdatascience.com/public-benchmarks-for-medical-natural-language-processing-c7c794ab4d9)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A general introduction to a list of canonical tasks and corresponding datasets
    to measure your medical natural language processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://eileen-code4fun.medium.com/?source=post_page-----c7c794ab4d9--------------------------------)[![Eileen
    Pangu](../Images/cbdab572af709b6e6b52cb3a078f220d.png)](https://eileen-code4fun.medium.com/?source=post_page-----c7c794ab4d9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c7c794ab4d9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c7c794ab4d9--------------------------------)
    [Eileen Pangu](https://eileen-code4fun.medium.com/?source=post_page-----c7c794ab4d9--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c7c794ab4d9--------------------------------)
    ·6 min read·Mar 29, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e7f7329d67dd349b27bbe3c0f57628f7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [https://unsplash.com/](https://unsplash.com/)'
  prefs: []
  type: TYPE_NORMAL
- en: The field of natural language processing (NLP) has evolved really fast in recent
    years. Breakthroughs like transformer, BERT, GPT have emerged one after another.
    Practitioners of all industries are exploring how to leverage the exciting development
    of NLP in their specific business domains and workflows [1]. One such industry
    that stands to benefit greatly from the improvement of NLP is healthcare. The
    vast amount of free text medical notes carry incredible data insights, which can
    inform better care provision, cost optimization, and healthcare innovation. To
    measure the efficacy of applying NLP to the medical field, we need good benchmarks.
    This blog post lists the canonical public benchmarks for the common tasks in medical
    natural language processing. The goal is to provide a starting point for healthcare
    machine learning practitioners to measure their NLP endeavours.
  prefs: []
  type: TYPE_NORMAL
- en: '**Entity/Relation Recognition**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The task of entity/relation recognition is to detect and categorize the medical
    concepts in free text and their relations. It is a crucial step in gaining better
    understanding of actionable insights from clinical notes and reports. The canonical
    dataset for this is Informatics for Integrating Biology and the Bedside (i2b2)
    [2]. The dataset contains de-identified patient reports from a few partnered medical
    organizations with 394 training reports, 477 test reports. The labeled medical
    concepts are of type `problems`, `treatments`, and `tests`. The labeled relations
    include things like `treatment improves problem`, `test reveals problem`, `problem
    indicates another problem`, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a concrete example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Only a full recognition is considered correct. That means for an entity, both
    the start and end word indices of the entity need to be accurate; and for a relation,
    the left entity, the right entity, and the relation all need to be accurate. The
    final evaluation metrics are based on precision, recall, and F1 score.
  prefs: []
  type: TYPE_NORMAL
- en: '**Semantics Similarity**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Semantics similarity evaluates the semantic equivalence between two snippets
    of medical text. Clinical Semantic Textual Similarity (ClinicalSTS) [3] is a canonical
    dataset for this task. It contains 1642 training and 412 test de-identified sentence
    pairs. The equivalence is measured by an ordinal scale of 0 to 5, with 0 indicating
    complete dissimilarity and 5 suggesting complete semantic equivalence. The final
    performance is measured by the Pearson correlation between the predicted similarity
    scores `Y’` and human judgement `Y`, and is calculated by the formula below (the
    higher the result, the better):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f14ab84df247d893a0c5efddd14ac295.png)'
  prefs: []
  type: TYPE_IMG
- en: Pearson Correlation Formula
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are two concrete examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Natural language inference**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Natural language inference evaluates how well a medical hypothesis can be derived
    from a medical premise. MedNLI [4] is such a dataset. It contains de-identified
    medical history notes from a group of deceased patients. The notes are segmented
    into snippets, and human experts were asked to write 3 hypotheses based on each
    snippet. The 3 hypotheses are
  prefs: []
  type: TYPE_NORMAL
- en: a clearly true description
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a clearly false description and
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a description might be true or false,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'representing 3 relations of the premise-hypothesis: `entailment`, `contradiction`,
    and `neural`. The dataset contains 11232 training pairs, 1395 development pairs,
    and 1422 test pairs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a concrete example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The final performance can be measured by the classification accuracy of the
    relations given the premise-hypothesis pairs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Medical question choice-answering**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Medical question choice-answering emulates the choice-answer medical exams.
    MedQA [5] is the canonical dataset for this purpose. Its questions are collected
    from medical board exams in the US and China where human doctors are evaluated
    by picking the right answer. It contains 61097 questions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a concrete example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Mechanically, this task can be treated as a scoring system where the input is
    the `question+answer_i`, and the output is a numeric score. The `answer_i` with
    the highest score will be the final answer. The performance can be measured by
    accuracy on a 80/10/10 split of the dataset. This creates a comparable benchmark
    for the model and human expert performance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Medical question answering**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Medical question answering is the most complex form of medical NLP task. It
    requires the model to generate long form free text answers to the given medical
    question. emrQA [6] is a canonical dataset for this purpose. It has 400k question-answer
    pairs. Such a dataset would be very expensive to acquire relying only on human
    experts’ manual efforts. Therefore, emrQA is actually semi-automatically generated
    by
  prefs: []
  type: TYPE_NORMAL
- en: first polling medical experts on the frequently asked questions,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: then replacing the medical concepts in those questions with placeholder and
    thus creating templates of questions,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: and finally using annotated entity-relation (such as i2b2) dataset to establish
    the clinical context, fill in the questions, and generate the answers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is a concrete example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Mechanically, this task can be seen as a language generation task where the
    input is the `context+question`, and the output is the `answer`. Final performance
    can typically be measured on a 80/20 split of the dataset, and by the exact match
    and F1 score. Exact match measures the percentage of prediction that matches the
    exact ground truth. F1 score measures the “overlap” between the prediction and
    ground truth. In this setting, both the prediction and the ground truth are treated
    as a bag of tokens where true/false positive/negative can be calculated.
  prefs: []
  type: TYPE_NORMAL
- en: '**Conclusion**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Researchers and practitioners continue to vigorously apply natural language
    processing (NLP) in the medical space. While it’s exciting to see the enthusiasm,
    it’s important to have public and reproducible benchmarks to measure the performance
    of such applications. This blog post lists the typical tasks, corresponding public
    datasets, and applicable metrics for this purpose, which can serve to quantify
    the potential improvement of new medical NLP applications.
  prefs: []
  type: TYPE_NORMAL
- en: '**References**'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[1] How to Use Large Language Models (LLM) in Your Own Domains [https://towardsdatascience.com/how-to-use-large-language-models-llm-in-your-own-domains-b4dff2d08464](/how-to-use-large-language-models-llm-in-your-own-domains-b4dff2d08464)'
  prefs: []
  type: TYPE_NORMAL
- en: '[2] 2010 i2b2/VA challenge on concepts, assertions, and relations in clinical
    text [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3168320/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3168320/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[3] The 2019 n2c2/OHNLP Track on Clinical Semantic Textual Similarity: Overview
    [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7732706/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7732706/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[4] MedNLI — A Natural Language Inference Dataset For The Clinical Domain [https://physionet.org/content/mednli/1.0.0/](https://physionet.org/content/mednli/1.0.0/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5] What Disease does this Patient Have? A Large-scale Open Domain Question
    Answering Dataset from Medical Exams [https://arxiv.org/abs/2009.13081](https://arxiv.org/abs/2009.13081)'
  prefs: []
  type: TYPE_NORMAL
- en: '[6] emrQA: A Large Corpus for Question Answering on Electronic Medical Records
    [https://arxiv.org/abs/1809.00732](https://arxiv.org/abs/1809.00732)'
  prefs: []
  type: TYPE_NORMAL
