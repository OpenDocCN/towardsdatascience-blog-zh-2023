- en: Linear Regression In Depth (Part 1)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/linear-regression-in-depth-part-1-485f997fd611](https://towardsdatascience.com/linear-regression-in-depth-part-1-485f997fd611)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Deep dive into the theory and implementation of linear regression models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://medium.com/@roiyeho?source=post_page-----485f997fd611--------------------------------)[![Dr.
    Roi Yehoshua](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----485f997fd611--------------------------------)[](https://towardsdatascience.com/?source=post_page-----485f997fd611--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----485f997fd611--------------------------------)
    [Dr. Roi Yehoshua](https://medium.com/@roiyeho?source=post_page-----485f997fd611--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----485f997fd611--------------------------------)
    ·13 min read·Apr 18, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8c5541250ea02cc694fa737234963d1d.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Enayet Raheem](https://unsplash.com/@raheemsphoto?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/3RQnQyyzA9c?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression is one of the most basic and commonly used type of predictive
    models. It dates back to 1805, when Legendre and Gauss used linear regression
    to predict the movement of the planets.
  prefs: []
  type: TYPE_NORMAL
- en: The goal in regression problems is to predict the value of one variable based
    on the values of other variables. For example, we can use regression to predict
    the price of a stock based on various economic indicators or the total sales of
    a company based on the amount spent on advertising.
  prefs: []
  type: TYPE_NORMAL
- en: In linear regression, we assume that there is a linear relationship between
    the given input features and the target label, and we are trying to find the exact
    form of that relationship.
  prefs: []
  type: TYPE_NORMAL
- en: This article provides a comprehensive guide to both the theory and implementation
    of linear regression models. In the first part of the article, we will focus mainly
    on **simple linear regression**, where the data set contains only one feature
    (i.e., the data set consists of two-dimensional points). In the [second part of
    the article](https://medium.com/towards-data-science/linear-regression-in-depth-part-2-5d40fd19efd4),
    we will discuss **multiple linear regression**, where the data set may contain
    more than one feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many terms related to regression that data scientists often use interchangeably,
    but they are not always the same, such as: residuals/errors, cost/loss/error function,
    multiple/multivariate regression, squared loss/mean squared error/sum of squared
    residuals, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: Bearing this in mind, I have tried in this article to be as clear as possible
    with regard to the definitions and terminology used.
  prefs: []
  type: TYPE_NORMAL
- en: Formal Definition and Notations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regression Problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In regression problems, we are given a set of *n* labeled examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '*D* = {(**x**₁, *y*₁), (**x**₂, *y*₂), … , (**x***ₙ, yₙ*)}, where **x***ᵢ*
    represents the **features** of example *i* and *yᵢ* represents the **label** of
    that example.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each **x***ᵢ* is a vector that consists of *m* features: **x***ᵢ* = (*xᵢ*₁,
    *xᵢ*₂, …, *xᵢₘ*)*ᵗ*, where *ᵗ* denotes the transpose*.* The variables *xᵢⱼ* are
    called the **independent variables** or the **explanatory variables**.'
  prefs: []
  type: TYPE_NORMAL
- en: The label *y* is a continuous-valued variable (*y* ∈ *R*), which is called the
    **dependent variable** or the **response variable**.
  prefs: []
  type: TYPE_NORMAL
- en: 'We assume that there is a correlation between the label *y* and the input vector
    **x**, which is modeled by some function *f*(**x**) and an **error variable**
    *ϵ*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/97edf202ca9900e304ab74442e52120b.png)'
  prefs: []
  type: TYPE_IMG
- en: The error variable *ϵ* captures all the unmodeled factors that influence the
    label other than the features, such as errors in the measurement or some random
    noise.
  prefs: []
  type: TYPE_NORMAL
- en: Our goal is to find the function *f*(**x**), since knowing this function will
    allow us to predict the labels for any new sample. However, since we have a limited
    number of training samples from which to learn *f*(**x**), we can only obtain
    an estimate of this function.
  prefs: []
  type: TYPE_NORMAL
- en: The function that our model estimates from the given data is called the **model’s
    hypothesis** and is typically denoted by *h*(**x**).
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In **linear regression**, we assume that there is a linear relationship between
    the features and the target label. Therefore, the model’s hypothesis takes the
    following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d4260b35f964e391872cb203df350a90.png)'
  prefs: []
  type: TYPE_IMG
- en: The linear regression model
  prefs: []
  type: TYPE_NORMAL
- en: '*w*₀, …, *wₘ* are called the **parameters** (or **weights**) of the model.
    The parameter *w*₀ is often called the **intercept** (or **bias**), since it represents
    the intersection point of the graph of *h*(**x**) with the *y-*axis (in two dimensions).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To simplify *h*(**x**), we add a constant feature *x*₀ that is always equal
    to 1\. This allows us to write *h*(**x**) as the dot product between the feature
    vector **x** = (*x*₀, …, *xₘ*)*ᵗ* and the weight vector **w** = (*w*₀, …, *wₘ*)*ᵗ*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/372c87498c0cd36087fb587a88b46239.png)'
  prefs: []
  type: TYPE_IMG
- en: Vector form of the linear regression model
  prefs: []
  type: TYPE_NORMAL
- en: Ordinary Least Squares (OLS)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our goal in linear regression is to find the parameters *w*₀, …, *wₘ* that will
    make our model’s predictions *h*(**x**) be as close as possible to the true labels
    *y*. In other words, we would like to find the model’s parameters that best **fit**
    the data set.
  prefs: []
  type: TYPE_NORMAL
- en: To that end, we define a **cost function** (sometimes also called an **error
    function**) that measures how far our model’s predictions are from the true labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by defining the **residual** as the difference between the label of
    a given data point and the value predicted by the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c3c15f946c06af678f7585485e457092.png)'
  prefs: []
  type: TYPE_IMG
- en: Definition of the residual
  prefs: []
  type: TYPE_NORMAL
- en: '**Ordinary least squares** (OLS) regression finds the optimal parameter values
    that minimize the **sum of squared residuals**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d7778dcffed2685985ae3aa0824c6f04.png)'
  prefs: []
  type: TYPE_IMG
- en: The least squares cost function
  prefs: []
  type: TYPE_NORMAL
- en: Note that a **loss function** calculates the error per observation and in OLS
    it is called **the squared loss**, while a **cost function** (typically denoted
    by *J*) calculates the error over the whole data set, and in OLS it is called
    the **sum of squared residuals** (SSR) or **sum of squared errors** (SSE).
  prefs: []
  type: TYPE_NORMAL
- en: Although OLS is the most common type of regression, there are other types of
    regression such as the least absolute deviations regression. We will motivate
    why the least squares function is the preferred cost function in the last section
    of this article.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, except for some special cases (that will be discussed later), the least
    squares cost function is [convex](https://en.wikipedia.org/wiki/Convex_function).
    A function *f*(*x*) is convex if the line segment between any two points on the
    graph of the function lies above the graph. In simpler terms, the graph of the
    function has a cup shape ∪. This means that convex functions have only one minimum,
    which is also the global minimum.
  prefs: []
  type: TYPE_NORMAL
- en: Since *J*(**w**) is convex, finding its minimum points using its first-order
    derivatives is guaranteed to give us a unique solution, and hence the optimal
    one.
  prefs: []
  type: TYPE_NORMAL
- en: Simple Linear Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When the data set has only one feature (i.e., when it consists of two-dimensional
    points (*x*, *y*)), the regression problem is called **simple linear regression**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Geometrically, in simple linear regression, we are trying to find a **straight
    line** that goes as close as possible through all the data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b9401161566cd86cab8ac8546de8d8b4.png)'
  prefs: []
  type: TYPE_IMG
- en: Simple linear regression
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the model’s hypothesis is simply the equation of the line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/01cff3c39af6cbc54cf95c00bc240785.png)'
  prefs: []
  type: TYPE_IMG
- en: The equation of the regression line
  prefs: []
  type: TYPE_NORMAL
- en: where *w*₁ is the slope of the line and *w*₀ is its intersection with the *y-*axis.
    The residuals in this case are the distances between the data points and the fitted
    line.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the least squares cost function takes the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a9d9f5c1388cbcc30192b2dab34360fc.png)'
  prefs: []
  type: TYPE_IMG
- en: The OLS cost function in simple linear regression
  prefs: []
  type: TYPE_NORMAL
- en: The Normal Equations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our objective is to find the parameters *w*₀ and *w*₁ of the line that best
    fits the points, i.e., the line that leads to the minimum cost. To that end, we
    can take the partial derivatives of *J*(*w*₀, *w*₁) with respect to both parameters,
    set them to 0, and then solve the resulting linear system of equations (which
    are called the **normal equations**).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with the partial derivative of *J* with respect to *w*₀:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/380d9a245392afffed5768d9e8924461.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Setting this derivative to 0 yields the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ad23d991f4f01d2d4156b62ce19b4249.png)'
  prefs: []
  type: TYPE_IMG
- en: We have found an expression for *w*₀ in terms of *w*₁ and the data points.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we compute the partial derivative of *J* with respect to *w*₁:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/09e13b488b240dada0cda46e85bb107c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Setting this derivative to 0 yields the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/78e83847283ccd9a2fa2d746afdc0bb7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s substitute the expression for *w*₀ into this equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3769c40d14bf7454efef4b0c915ee270.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, the coefficients of the regression line are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/135388b35ecf0c9b4d29ba1c82701a32.png)'
  prefs: []
  type: TYPE_IMG
- en: Numerical Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s say that we would like to find if there is a linear correlation between
    the height and weight of people. We are given the following 10 examples that represent
    the average heights and weights of American women aged 30–39 (source: *The World
    Almanac and Book of Facts*, 1975).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/06e95f5fdb8bee8031d4183939ad4415.png)'
  prefs: []
  type: TYPE_IMG
- en: The training set
  prefs: []
  type: TYPE_NORMAL
- en: 'To find the regression line manually, we first build the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2b40ded1a110c7c02e258e93e6ddd821.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Based on the totals in the last row of the table, we can compute the coefficients
    of the regression line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bcad643c80991e446bf48e89d2c298ba.png)![](../Images/b471a1bd782c2e1b5a0776d8defeb93e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, the equation of the fitted line is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5a2c5904e1b61d7a3587aaa6f145b9ee.png)'
  prefs: []
  type: TYPE_IMG
- en: Python Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now find the regression line using Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s write a general function to find the parameters of the regression
    line for any given two-dimensional data set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The code above is a direct translation of the normal equations into NumPy functions
    and operators.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s test our function on the same data set from above. We first define our
    data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s plot them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/2f9716fa76ef31cfea062d449e115f9e.png)'
  prefs: []
  type: TYPE_IMG
- en: The training set
  prefs: []
  type: TYPE_NORMAL
- en: 'We now find the parameters of the regression line using the function we have
    just written:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We get the same results that we had with the manual computation, albeit with
    a higher precision.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s write another function to draw the regression line. To that end, we can
    simply take the minimum and maximum *x* values in the input range, compute their
    *y* coordinates on the regression line, and then draw the line that connects the
    two points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, let’s plot the regression line together with the data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/2171b52b3f3940bb12567cdabb87cb3b.png)'
  prefs: []
  type: TYPE_IMG
- en: The regression line
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the relationship between the two variables is very close to
    linear.
  prefs: []
  type: TYPE_NORMAL
- en: As an exercise, download the [heights and weights data set](https://www.kaggle.com/datasets/burnoutminer/heights-and-weights-dataset)
    from Kaggle. This data set contains the height and weight of 25,000 18-years old
    teenagers. Build a linear regression model for predicting the weight of a teenager
    from their height and plot the result.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several evaluation metrics that are used to evaluate the performance
    of regression models. The two most common ones are **RMSE** (Root Mean Squared
    Error) and **R² score**.
  prefs: []
  type: TYPE_NORMAL
- en: Note the difference between an **evaluation metric** and a **cost function**.
    A cost function is used to define the objective of the model’s learning process
    and is computed on the training set. Conversely, an evaluation metric is used
    after the training process to evaluate the model on a holdout data set (a validation
    or a test set).
  prefs: []
  type: TYPE_NORMAL
- en: RMSE (Root Mean Squared Error)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'RMSE is defined as the square root of the mean of the squared errors (the differences
    between the model’s predictions and the true labels):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/68f045cca1cced5dd85b45c10868334b.png)'
  prefs: []
  type: TYPE_IMG
- en: RMSE definition
  prefs: []
  type: TYPE_NORMAL
- en: Note that what we called **residuals** during the model’s training are typically
    called **errors** (or prediction errors) when they are computed over the holdout
    set.
  prefs: []
  type: TYPE_NORMAL
- en: RMSE is always non-negative, and a lower RMSE means the model has a better fit
    to the data (a perfect model has an RMSE of 0).
  prefs: []
  type: TYPE_NORMAL
- en: We can compute the RMSE directly or by using the [sklearn.metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)
    module. This module provides numerous functions for measuring the performance
    of different types of models. Although it does not have an explicit function for
    computing RMSE, we can use the function [mean_squared_error()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error)
    to first find the MSE, and then take its square root to get the RMSE.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of the scoring functions in sklearn.metrics expect to get as parameters
    an array with the true labels (*y_true*) and an array with the model’s predictions
    (*y_pred*). Therefore, we first need to compute our model’s predictions on the
    given data points. This can be easily done by using the equation of the regression
    line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now call the mean_squared_error() function and find the RMSE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The result we get is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Advantages of RMSE:'
  prefs: []
  type: TYPE_NORMAL
- en: Provides a measure for the average magnitude of the model’s errors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the errors are squared before they are averaged, RMSE gives a relatively
    higher weight to large errors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be used to compare the performance of different models on the same data
    set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Disadvantages of RMSE:'
  prefs: []
  type: TYPE_NORMAL
- en: Cannot be used to compare the model’s performance across different data sets,
    because it depends on the scale of the input features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sensitive to outliers, since the effect of each error on the RMSE is proportional
    to the size of the squared error.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R² Score
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The *R*² score (also called the **coefficient of determination**) is a measure
    of the **goodness of fit** of a model. It computes the ratio between the sum of
    squared errors of the regression model and the sum of squared errors of a baseline
    model that always predicts the mean value of *y*, and subtracts this ratio from
    1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bbe99a494e8074bd7ff9b8e13234c3b0.png)'
  prefs: []
  type: TYPE_IMG
- en: '*R*² score definition'
  prefs: []
  type: TYPE_NORMAL
- en: where *ȳ* is the mean of the target labels.
  prefs: []
  type: TYPE_NORMAL
- en: The best possible *R*² score is 1, which indicates that the model predictions
    perfectly fit the data. A constant model that always predicts the mean value of
    *y*, regardless of the input features, has an *R*² score of 0.
  prefs: []
  type: TYPE_NORMAL
- en: '*R*² scores below 0 occur when the model performs worse than the worst possible
    least-square predictor. This typically indicates that a wrong model was chosen.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To compute the *R*² score, we can use the function [r2_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html)
    from sklearn.metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The result we get is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The *R*² score is very close to 1, which means we have an almost perfect model.
    However, note that in this example we are evaluating the model on the training
    set, where the model would normally have a higher score than on a holdout set.
  prefs: []
  type: TYPE_NORMAL
- en: '*R*² score can also be interpreted as the proportion of the variance of the
    dependent variable *y* that is explained by the independent variables in the model
    (the interested reader can find why in this [Wikipedia article](https://en.wikipedia.org/wiki/Coefficient_of_determination)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Advantages of *R*² score:'
  prefs: []
  type: TYPE_NORMAL
- en: Does not depend on the scale of the features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be used to compare the performance of different models across different
    data sets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Disadvantages of *R*² score:'
  prefs: []
  type: TYPE_NORMAL
- en: Does not provide information on the magnitude of the model’s errors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*R*² score is monotonically increasing with the number of features the model
    has, thus it cannot be used to compare models with very different numbers of features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OLS and Maximum Likelihood
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, we will show the correlation between ordinary least squares (OLS) and
    maximum likelihood, which is the main motivation for using OLS to solve regression
    problems. More specifically, we will prove that an OLS estimator is identical
    to the maximum likelihood estimator (MLE) under the assumption that the errors
    are normally distributed with zero mean.
  prefs: []
  type: TYPE_NORMAL
- en: For those unfamiliar with the concept of maximum likelihood, check out my [previous
    article](https://medium.com/@roiyeho/maximum-likelihood-855b6df92c43).
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that in linear regression we assume that the labels are generated by
    a linear function of the features plus some random noise:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1ce56a552d6f0d2986fe320b69038e85.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s assume that the errors are independent and identically distributed (i.i.d.),
    and have a normal distribution with mean 0 and variance *σ*²:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9d97d8f2caff7c561a4845616b0ee028.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, the labels *y* are also normally distributed with a mean of **w***ᵗ***x**
    and variance *σ*² (adding a constant to a normally-distributed variable yields
    a variable that is also normally distributed but whose mean is shifted by that
    constant):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ae00604ce05c6a2035d1632abd5bf4ee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, the probability density function (PDF) of *y* given the inputs **x**
    and the weight vector **w** is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/16428d74129c4ac309662a2513337dd9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Based on the assumption of independence of the errors (and hence the labels),
    we can write the likelihood of the parameters **w** of the model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b523be477119459a86ca9447a9585180.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, the log likelihood is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c51303a6925a1978e86d8f0fabf8b418.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see that the only expression in the log likelihood that depends on the
    parameters **w** is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3812d945d20e95b3817e4b12fe9e1c8d.png)'
  prefs: []
  type: TYPE_IMG
- en: which is exactly the cost function of OLS! Therefore, maximizing the likelihood
    of the modelis identical to minimizing the sum of squared residuals.
  prefs: []
  type: TYPE_NORMAL
- en: Final Notes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All images unless otherwise noted are by the author.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the code examples of this article on my github: [https://github.com/roiyeho/medium/tree/main/simple_linear_regression](https://github.com/roiyeho/medium/tree/main/simple_linear_regression)'
  prefs: []
  type: TYPE_NORMAL
- en: The second part of the article that discusses multiple linear regression can
    be found [here](https://medium.com/towards-data-science/linear-regression-in-depth-part-2-5d40fd19efd4).
  prefs: []
  type: TYPE_NORMAL
- en: Thanks for reading!
  prefs: []
  type: TYPE_NORMAL
