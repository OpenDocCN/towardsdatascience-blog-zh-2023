- en: Linear Regression In Depth (Part 1)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性回归深入探讨（第1部分）
- en: 原文：[https://towardsdatascience.com/linear-regression-in-depth-part-1-485f997fd611](https://towardsdatascience.com/linear-regression-in-depth-part-1-485f997fd611)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/linear-regression-in-depth-part-1-485f997fd611](https://towardsdatascience.com/linear-regression-in-depth-part-1-485f997fd611)
- en: Deep dive into the theory and implementation of linear regression models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入探讨线性回归模型的理论和实现
- en: '[](https://medium.com/@roiyeho?source=post_page-----485f997fd611--------------------------------)[![Dr.
    Roi Yehoshua](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----485f997fd611--------------------------------)[](https://towardsdatascience.com/?source=post_page-----485f997fd611--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----485f997fd611--------------------------------)
    [Dr. Roi Yehoshua](https://medium.com/@roiyeho?source=post_page-----485f997fd611--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@roiyeho?source=post_page-----485f997fd611--------------------------------)[![Dr.
    Roi Yehoshua](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----485f997fd611--------------------------------)[](https://towardsdatascience.com/?source=post_page-----485f997fd611--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----485f997fd611--------------------------------)
    [Dr. Roi Yehoshua](https://medium.com/@roiyeho?source=post_page-----485f997fd611--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----485f997fd611--------------------------------)
    ·13 min read·Apr 18, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----485f997fd611--------------------------------)
    ·阅读时间13分钟·2023年4月18日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/8c5541250ea02cc694fa737234963d1d.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c5541250ea02cc694fa737234963d1d.png)'
- en: Photo by [Enayet Raheem](https://unsplash.com/@raheemsphoto?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/3RQnQyyzA9c?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[Enayet Raheem](https://unsplash.com/@raheemsphoto?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)拍摄于[Unsplash](https://unsplash.com/photos/3RQnQyyzA9c?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)'
- en: Linear regression is one of the most basic and commonly used type of predictive
    models. It dates back to 1805, when Legendre and Gauss used linear regression
    to predict the movement of the planets.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归是最基础且最常用的预测模型之一。它可以追溯到1805年，当时勒让德和高斯使用线性回归来预测行星的运动。
- en: The goal in regression problems is to predict the value of one variable based
    on the values of other variables. For example, we can use regression to predict
    the price of a stock based on various economic indicators or the total sales of
    a company based on the amount spent on advertising.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 回归问题的目标是根据其他变量的值预测一个变量的值。例如，我们可以使用回归预测股票价格，基于各种经济指标，或者根据广告支出的金额预测公司的总销售额。
- en: In linear regression, we assume that there is a linear relationship between
    the given input features and the target label, and we are trying to find the exact
    form of that relationship.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性回归中，我们假设给定的输入特征与目标标签之间存在线性关系，并且我们试图找到这种关系的确切形式。
- en: This article provides a comprehensive guide to both the theory and implementation
    of linear regression models. In the first part of the article, we will focus mainly
    on **simple linear regression**, where the data set contains only one feature
    (i.e., the data set consists of two-dimensional points). In the [second part of
    the article](https://medium.com/towards-data-science/linear-regression-in-depth-part-2-5d40fd19efd4),
    we will discuss **multiple linear regression**, where the data set may contain
    more than one feature.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提供了关于线性回归模型理论和实现的全面指南。在文章的第一部分，我们将主要关注**简单线性回归**，其中数据集仅包含一个特征（即数据集由二维点组成）。在[文章的第二部分](https://medium.com/towards-data-science/linear-regression-in-depth-part-2-5d40fd19efd4)，我们将讨论**多重线性回归**，其中数据集可能包含多个特征。
- en: 'There are many terms related to regression that data scientists often use interchangeably,
    but they are not always the same, such as: residuals/errors, cost/loss/error function,
    multiple/multivariate regression, squared loss/mean squared error/sum of squared
    residuals, etc.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多与回归相关的术语，数据科学家经常将它们混用，但它们并不总是相同，例如：残差/误差，成本/损失/误差函数，多重/多变量回归，平方损失/均方误差/平方残差和等。
- en: Bearing this in mind, I have tried in this article to be as clear as possible
    with regard to the definitions and terminology used.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于此，我在本文中尽力对所使用的定义和术语进行尽可能清晰的阐述。
- en: Formal Definition and Notations
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正式定义和符号
- en: Regression Problems
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归问题
- en: 'In regression problems, we are given a set of *n* labeled examples:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归问题中，我们得到一组 *n* 个标记示例：
- en: '*D* = {(**x**₁, *y*₁), (**x**₂, *y*₂), … , (**x***ₙ, yₙ*)}, where **x***ᵢ*
    represents the **features** of example *i* and *yᵢ* represents the **label** of
    that example.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*D* = {(**x**₁, *y*₁), (**x**₂, *y*₂), … , (**x***ₙ, yₙ*)}，其中 **x***ᵢ* 表示示例
    *i* 的**特征**，*yᵢ* 表示该示例的**标签**。'
- en: 'Each **x***ᵢ* is a vector that consists of *m* features: **x***ᵢ* = (*xᵢ*₁,
    *xᵢ*₂, …, *xᵢₘ*)*ᵗ*, where *ᵗ* denotes the transpose*.* The variables *xᵢⱼ* are
    called the **independent variables** or the **explanatory variables**.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 每个**x***ᵢ* 是一个包含 *m* 个特征的向量：**x***ᵢ* = (*xᵢ*₁, *xᵢ*₂, …, *xᵢₘ*)*ᵗ*，其中 *ᵗ* 表示转置。变量
    *xᵢⱼ* 被称为**独立变量**或**解释变量**。
- en: The label *y* is a continuous-valued variable (*y* ∈ *R*), which is called the
    **dependent variable** or the **response variable**.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 标签 *y* 是一个连续值变量 (*y* ∈ *R*)，称为**因变量**或**响应变量**。
- en: 'We assume that there is a correlation between the label *y* and the input vector
    **x**, which is modeled by some function *f*(**x**) and an **error variable**
    *ϵ*:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设标签 *y* 和输入向量 **x** 之间存在某种相关性，这种相关性由某个函数 *f*(**x**) 和一个**误差变量** *ϵ* 建模：
- en: '![](../Images/97edf202ca9900e304ab74442e52120b.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97edf202ca9900e304ab74442e52120b.png)'
- en: The error variable *ϵ* captures all the unmodeled factors that influence the
    label other than the features, such as errors in the measurement or some random
    noise.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 误差变量*ϵ*捕捉了所有未建模的因素，这些因素影响标签除了特征外，例如测量误差或一些随机噪声。
- en: Our goal is to find the function *f*(**x**), since knowing this function will
    allow us to predict the labels for any new sample. However, since we have a limited
    number of training samples from which to learn *f*(**x**), we can only obtain
    an estimate of this function.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是找到函数 *f*(**x**)，因为知道这个函数将使我们能够预测任何新样本的标签。然而，由于我们只能从有限数量的训练样本中学习 *f*(**x**)，因此我们只能获得这个函数的估计值。
- en: The function that our model estimates from the given data is called the **model’s
    hypothesis** and is typically denoted by *h*(**x**).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型从给定数据中估算的函数称为**模型的假设**，通常表示为* h*(**x**）。
- en: Linear Regression
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性回归
- en: 'In **linear regression**, we assume that there is a linear relationship between
    the features and the target label. Therefore, the model’s hypothesis takes the
    following form:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在**线性回归**中，我们假设特征和目标标签之间存在线性关系。因此，模型的假设具有以下形式：
- en: '![](../Images/d4260b35f964e391872cb203df350a90.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d4260b35f964e391872cb203df350a90.png)'
- en: The linear regression model
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归模型
- en: '*w*₀, …, *wₘ* are called the **parameters** (or **weights**) of the model.
    The parameter *w*₀ is often called the **intercept** (or **bias**), since it represents
    the intersection point of the graph of *h*(**x**) with the *y-*axis (in two dimensions).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*w*₀, …, *wₘ* 称为模型的**参数**（或**权重**）。参数 *w*₀ 通常称为**截距**（或**偏置**），因为它表示 *h*(**x**)
    的图与 *y-*轴（在二维情况下）的交点。'
- en: 'To simplify *h*(**x**), we add a constant feature *x*₀ that is always equal
    to 1\. This allows us to write *h*(**x**) as the dot product between the feature
    vector **x** = (*x*₀, …, *xₘ*)*ᵗ* and the weight vector **w** = (*w*₀, …, *wₘ*)*ᵗ*:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化 *h*(**x**)，我们添加了一个常数特征 *x*₀，总是等于 1。这样我们可以将 *h*(**x**) 写成特征向量 **x** = (*x*₀,
    …, *xₘ*)*ᵗ* 和权重向量 **w** = (*w*₀, …, *wₘ*)*ᵗ* 的点积：
- en: '![](../Images/372c87498c0cd36087fb587a88b46239.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/372c87498c0cd36087fb587a88b46239.png)'
- en: Vector form of the linear regression model
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归模型的向量形式
- en: Ordinary Least Squares (OLS)
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 普通最小二乘法（OLS）
- en: Our goal in linear regression is to find the parameters *w*₀, …, *wₘ* that will
    make our model’s predictions *h*(**x**) be as close as possible to the true labels
    *y*. In other words, we would like to find the model’s parameters that best **fit**
    the data set.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性回归中，我们的目标是找到参数 *w*₀, …, *wₘ*，使得我们模型的预测 *h*(**x**) 尽可能接近真实标签 *y*。换句话说，我们希望找到最能**拟合**数据集的模型参数。
- en: To that end, we define a **cost function** (sometimes also called an **error
    function**) that measures how far our model’s predictions are from the true labels.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们定义了一个**成本函数**（有时也称为**误差函数**），该函数衡量我们模型的预测与真实标签之间的距离。
- en: 'We start by defining the **residual** as the difference between the label of
    a given data point and the value predicted by the model:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从定义**残差**开始，残差是给定数据点的标签与模型预测值之间的差异：
- en: '![](../Images/c3c15f946c06af678f7585485e457092.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c3c15f946c06af678f7585485e457092.png)'
- en: Definition of the residual
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 残差的定义
- en: '**Ordinary least squares** (OLS) regression finds the optimal parameter values
    that minimize the **sum of squared residuals**:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**普通最小二乘**（OLS）回归找到使**平方残差和**最小的最佳参数值：'
- en: '![](../Images/d7778dcffed2685985ae3aa0824c6f04.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d7778dcffed2685985ae3aa0824c6f04.png)'
- en: The least squares cost function
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 最小二乘成本函数
- en: Note that a **loss function** calculates the error per observation and in OLS
    it is called **the squared loss**, while a **cost function** (typically denoted
    by *J*) calculates the error over the whole data set, and in OLS it is called
    the **sum of squared residuals** (SSR) or **sum of squared errors** (SSE).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，**损失函数**计算每个观察值的误差，在 OLS 中称为**平方损失**，而**成本函数**（通常用 *J* 表示）计算整个数据集的误差，在 OLS
    中称为**平方残差和**（SSR）或**平方误差和**（SSE）。
- en: Although OLS is the most common type of regression, there are other types of
    regression such as the least absolute deviations regression. We will motivate
    why the least squares function is the preferred cost function in the last section
    of this article.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 OLS 是最常见的回归类型，但还有其他类型的回归，例如最小绝对偏差回归。我们将在本文的最后一部分解释为什么最小二乘函数是首选的成本函数。
- en: Luckily, except for some special cases (that will be discussed later), the least
    squares cost function is [convex](https://en.wikipedia.org/wiki/Convex_function).
    A function *f*(*x*) is convex if the line segment between any two points on the
    graph of the function lies above the graph. In simpler terms, the graph of the
    function has a cup shape ∪. This means that convex functions have only one minimum,
    which is also the global minimum.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，除了某些特殊情况（稍后将讨论），最小二乘成本函数是[凸函数](https://en.wikipedia.org/wiki/Convex_function)。一个函数
    *f*(*x*) 是凸的，如果图形上任意两点之间的线段都位于图形之上。简单来说，函数图形呈杯形 ∪。这意味着凸函数只有一个最小值，即全局最小值。
- en: Since *J*(**w**) is convex, finding its minimum points using its first-order
    derivatives is guaranteed to give us a unique solution, and hence the optimal
    one.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 *J*(**w**) 是凸的，使用一阶导数找到其最小值点可以保证得到唯一解，因此也是最优解。
- en: Simple Linear Regression
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单线性回归
- en: When the data set has only one feature (i.e., when it consists of two-dimensional
    points (*x*, *y*)), the regression problem is called **simple linear regression**.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据集只有一个特征（即由二维点 (*x*, *y*) 组成）时，回归问题称为**简单线性回归**。
- en: 'Geometrically, in simple linear regression, we are trying to find a **straight
    line** that goes as close as possible through all the data points:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 从几何上讲，在简单线性回归中，我们尝试找到一条**直线**，使其尽可能接近所有数据点：
- en: '![](../Images/b9401161566cd86cab8ac8546de8d8b4.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b9401161566cd86cab8ac8546de8d8b4.png)'
- en: Simple linear regression
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 简单线性回归
- en: 'In this case, the model’s hypothesis is simply the equation of the line:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，模型的假设只是直线的方程：
- en: '![](../Images/01cff3c39af6cbc54cf95c00bc240785.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01cff3c39af6cbc54cf95c00bc240785.png)'
- en: The equation of the regression line
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 回归线的方程
- en: where *w*₁ is the slope of the line and *w*₀ is its intersection with the *y-*axis.
    The residuals in this case are the distances between the data points and the fitted
    line.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *w*₁ 是直线的斜率，*w*₀ 是直线与 *y-* 轴的交点。在这种情况下，残差是数据点与拟合直线之间的距离。
- en: 'In this case, the least squares cost function takes the following form:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，最小二乘成本函数的形式如下：
- en: '![](../Images/a9d9f5c1388cbcc30192b2dab34360fc.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9d9f5c1388cbcc30192b2dab34360fc.png)'
- en: The OLS cost function in simple linear regression
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: OLS 成本函数在简单线性回归中的表现
- en: The Normal Equations
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正规方程
- en: Our objective is to find the parameters *w*₀ and *w*₁ of the line that best
    fits the points, i.e., the line that leads to the minimum cost. To that end, we
    can take the partial derivatives of *J*(*w*₀, *w*₁) with respect to both parameters,
    set them to 0, and then solve the resulting linear system of equations (which
    are called the **normal equations**).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是找到使得线最符合数据点的参数 *w*₀ 和 *w*₁，即使得成本最小的直线。为此，我们可以对 *J*(*w*₀, *w*₁) 关于两个参数分别求偏导数，将其设为
    0，然后解这个线性方程组（这些方程被称为**正规方程**）。
- en: 'Let’s start with the partial derivative of *J* with respect to *w*₀:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 *J* 关于 *w*₀ 的偏导数开始：
- en: '![](../Images/380d9a245392afffed5768d9e8924461.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/380d9a245392afffed5768d9e8924461.png)'
- en: 'Setting this derivative to 0 yields the following:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个导数设为 0 可以得到以下结果：
- en: '![](../Images/ad23d991f4f01d2d4156b62ce19b4249.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad23d991f4f01d2d4156b62ce19b4249.png)'
- en: We have found an expression for *w*₀ in terms of *w*₁ and the data points.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经找到了关于 *w*₀ 的表达式，该表达式以 *w*₁ 和数据点为变量。
- en: 'Next, we compute the partial derivative of *J* with respect to *w*₁:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算*J*对*w*₁的偏导数：
- en: '![](../Images/09e13b488b240dada0cda46e85bb107c.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09e13b488b240dada0cda46e85bb107c.png)'
- en: 'Setting this derivative to 0 yields the following:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个导数设置为0得到以下结果：
- en: '![](../Images/78e83847283ccd9a2fa2d746afdc0bb7.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/78e83847283ccd9a2fa2d746afdc0bb7.png)'
- en: 'Let’s substitute the expression for *w*₀ into this equation:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将表达式* w*₀代入这个方程中：
- en: '![](../Images/3769c40d14bf7454efef4b0c915ee270.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3769c40d14bf7454efef4b0c915ee270.png)'
- en: 'Therefore, the coefficients of the regression line are:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，回归线的系数为：
- en: '![](../Images/135388b35ecf0c9b4d29ba1c82701a32.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/135388b35ecf0c9b4d29ba1c82701a32.png)'
- en: Numerical Example
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数值示例
- en: 'Let’s say that we would like to find if there is a linear correlation between
    the height and weight of people. We are given the following 10 examples that represent
    the average heights and weights of American women aged 30–39 (source: *The World
    Almanac and Book of Facts*, 1975).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想找出身高和体重之间是否存在线性相关性。我们得到以下10个示例，代表了30至39岁美国女性的平均身高和体重（来源：*世界年鉴与事实手册*，1975年）。
- en: '![](../Images/06e95f5fdb8bee8031d4183939ad4415.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06e95f5fdb8bee8031d4183939ad4415.png)'
- en: The training set
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集
- en: 'To find the regression line manually, we first build the following table:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了手动找到回归线，我们首先构建以下表格：
- en: '![](../Images/2b40ded1a110c7c02e258e93e6ddd821.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b40ded1a110c7c02e258e93e6ddd821.png)'
- en: 'Based on the totals in the last row of the table, we can compute the coefficients
    of the regression line:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 根据表格最后一行的总计，我们可以计算回归线的系数：
- en: '![](../Images/bcad643c80991e446bf48e89d2c298ba.png)![](../Images/b471a1bd782c2e1b5a0776d8defeb93e.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bcad643c80991e446bf48e89d2c298ba.png)![](../Images/b471a1bd782c2e1b5a0776d8defeb93e.png)'
- en: 'Therefore, the equation of the fitted line is:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，拟合线的方程为：
- en: '![](../Images/5a2c5904e1b61d7a3587aaa6f145b9ee.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5a2c5904e1b61d7a3587aaa6f145b9ee.png)'
- en: Python Implementation
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python 实现
- en: We will now find the regression line using Python.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用Python找到回归线。
- en: 'First, let’s write a general function to find the parameters of the regression
    line for any given two-dimensional data set:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们编写一个通用函数，用于找到任何给定二维数据集的回归线参数：
- en: '[PRE0]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The code above is a direct translation of the normal equations into NumPy functions
    and operators.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码是将正规方程直接转换为NumPy函数和运算符。
- en: 'Let’s test our function on the same data set from above. We first define our
    data points:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在上面的数据集上测试我们的函数。我们首先定义我们的数据点：
- en: '[PRE1]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let’s plot them:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制它们：
- en: '[PRE2]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/2f9716fa76ef31cfea062d449e115f9e.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f9716fa76ef31cfea062d449e115f9e.png)'
- en: The training set
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集
- en: 'We now find the parameters of the regression line using the function we have
    just written:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在使用刚刚编写的函数来寻找回归线的参数：
- en: '[PRE4]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We get the same results that we had with the manual computation, albeit with
    a higher precision.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了与手动计算相同的结果，但精度更高。
- en: 'Let’s write another function to draw the regression line. To that end, we can
    simply take the minimum and maximum *x* values in the input range, compute their
    *y* coordinates on the regression line, and then draw the line that connects the
    two points:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写另一个函数来绘制回归线。为此，我们可以简单地取输入范围内的最小和最大*x*值，计算它们在回归线上的*y*坐标，然后绘制连接这两点的线：
- en: '[PRE6]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Lastly, let’s plot the regression line together with the data points:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们将回归线与数据点一起绘制：
- en: '[PRE7]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/2171b52b3f3940bb12567cdabb87cb3b.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2171b52b3f3940bb12567cdabb87cb3b.png)'
- en: The regression line
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 回归线
- en: We can see that the relationship between the two variables is very close to
    linear.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到这两个变量之间的关系非常接近线性。
- en: As an exercise, download the [heights and weights data set](https://www.kaggle.com/datasets/burnoutminer/heights-and-weights-dataset)
    from Kaggle. This data set contains the height and weight of 25,000 18-years old
    teenagers. Build a linear regression model for predicting the weight of a teenager
    from their height and plot the result.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，从Kaggle下载[身高和体重数据集](https://www.kaggle.com/datasets/burnoutminer/heights-and-weights-dataset)。该数据集包含了25,000名18岁青少年的身高和体重。建立一个线性回归模型，用于预测青少年的体重，并绘制结果。
- en: Evaluation Metrics
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估指标
- en: There are several evaluation metrics that are used to evaluate the performance
    of regression models. The two most common ones are **RMSE** (Root Mean Squared
    Error) and **R² score**.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种评估指标用于评估回归模型的性能。最常见的两个指标是**RMSE**（均方根误差）和**R²得分**。
- en: Note the difference between an **evaluation metric** and a **cost function**.
    A cost function is used to define the objective of the model’s learning process
    and is computed on the training set. Conversely, an evaluation metric is used
    after the training process to evaluate the model on a holdout data set (a validation
    or a test set).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 注意**评估指标**和**成本函数**之间的区别。成本函数用于定义模型学习过程的目标，并在训练集上计算。相反，评估指标用于训练过程后评估模型在保留数据集（验证集或测试集）上的表现。
- en: RMSE (Root Mean Squared Error)
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RMSE（均方根误差）
- en: 'RMSE is defined as the square root of the mean of the squared errors (the differences
    between the model’s predictions and the true labels):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: RMSE 定义为平方误差的均值的平方根（模型预测与真实标签之间的差异）：
- en: '![](../Images/68f045cca1cced5dd85b45c10868334b.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/68f045cca1cced5dd85b45c10868334b.png)'
- en: RMSE definition
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: RMSE 定义
- en: Note that what we called **residuals** during the model’s training are typically
    called **errors** (or prediction errors) when they are computed over the holdout
    set.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们在模型训练过程中称为**残差**的东西，通常在计算保留集上的时候称为**误差**（或预测误差）。
- en: RMSE is always non-negative, and a lower RMSE means the model has a better fit
    to the data (a perfect model has an RMSE of 0).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: RMSE 总是非负的，较低的 RMSE 表明模型对数据的拟合更好（完美的模型 RMSE 为 0）。
- en: We can compute the RMSE directly or by using the [sklearn.metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)
    module. This module provides numerous functions for measuring the performance
    of different types of models. Although it does not have an explicit function for
    computing RMSE, we can use the function [mean_squared_error()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error)
    to first find the MSE, and then take its square root to get the RMSE.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接计算 RMSE 或通过使用 [sklearn.metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)
    模块来计算。这个模块提供了许多用于测量不同类型模型性能的函数。虽然它没有明确计算 RMSE 的函数，但我们可以使用 [mean_squared_error()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error)
    函数首先计算 MSE，然后取其平方根得到 RMSE。
- en: 'Most of the scoring functions in sklearn.metrics expect to get as parameters
    an array with the true labels (*y_true*) and an array with the model’s predictions
    (*y_pred*). Therefore, we first need to compute our model’s predictions on the
    given data points. This can be easily done by using the equation of the regression
    line:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: sklearn.metrics 中的大多数评分函数期望得到一个包含真实标签 (*y_true*) 和模型预测 (*y_pred*) 的数组。因此，我们首先需要计算模型在给定数据点上的预测。这可以通过使用回归线的方程轻松完成：
- en: '[PRE8]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can now call the mean_squared_error() function and find the RMSE:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以调用 mean_squared_error() 函数并找到 RMSE：
- en: '[PRE9]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The result we get is:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的结果是：
- en: '[PRE10]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Advantages of RMSE:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: RMSE 的优点：
- en: Provides a measure for the average magnitude of the model’s errors.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供了模型误差的平均幅度的度量。
- en: Since the errors are squared before they are averaged, RMSE gives a relatively
    higher weight to large errors.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于误差在平均之前被平方，RMSE 对大误差赋予了相对较高的权重。
- en: Can be used to compare the performance of different models on the same data
    set.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用于比较不同模型在同一数据集上的性能。
- en: 'Disadvantages of RMSE:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: RMSE 的缺点：
- en: Cannot be used to compare the model’s performance across different data sets,
    because it depends on the scale of the input features.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不能用于比较模型在不同数据集上的表现，因为它依赖于输入特征的尺度。
- en: Sensitive to outliers, since the effect of each error on the RMSE is proportional
    to the size of the squared error.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对离群值敏感，因为每个误差对 RMSE 的影响与平方误差的大小成正比。
- en: R² Score
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: R² 分数
- en: 'The *R*² score (also called the **coefficient of determination**) is a measure
    of the **goodness of fit** of a model. It computes the ratio between the sum of
    squared errors of the regression model and the sum of squared errors of a baseline
    model that always predicts the mean value of *y*, and subtracts this ratio from
    1:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '*R*² 分数（也称为**决定系数**）是衡量模型**拟合优度**的指标。它计算回归模型的平方误差之和与始终预测 *y* 的均值的基线模型的平方误差之和之间的比率，并从
    1 中减去该比率：'
- en: '![](../Images/bbe99a494e8074bd7ff9b8e13234c3b0.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bbe99a494e8074bd7ff9b8e13234c3b0.png)'
- en: '*R*² score definition'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '*R*² 分数定义'
- en: where *ȳ* is the mean of the target labels.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *ȳ* 是目标标签的均值。
- en: The best possible *R*² score is 1, which indicates that the model predictions
    perfectly fit the data. A constant model that always predicts the mean value of
    *y*, regardless of the input features, has an *R*² score of 0.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳的*R*²评分为1，这表明模型预测与数据完全吻合。一个始终预测* y*均值的常量模型，无论输入特征如何，其*R*²评分为0。
- en: '*R*² scores below 0 occur when the model performs worse than the worst possible
    least-square predictor. This typically indicates that a wrong model was chosen.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '*R*²评分低于0表示模型的表现比最差的最小二乘预测器还要差。这通常表明选择了错误的模型。'
- en: 'To compute the *R*² score, we can use the function [r2_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html)
    from sklearn.metrics:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算*R*²评分，我们可以使用sklearn.metrics中的[r2_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html)函数：
- en: '[PRE11]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The result we get is:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的结果是：
- en: '[PRE12]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The *R*² score is very close to 1, which means we have an almost perfect model.
    However, note that in this example we are evaluating the model on the training
    set, where the model would normally have a higher score than on a holdout set.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*R*²评分非常接近1，这意味着我们有一个几乎完美的模型。然而，请注意，在这个例子中，我们是在训练集上评估模型，在训练集上模型通常会有比在保留集上更高的评分。'
- en: '*R*² score can also be interpreted as the proportion of the variance of the
    dependent variable *y* that is explained by the independent variables in the model
    (the interested reader can find why in this [Wikipedia article](https://en.wikipedia.org/wiki/Coefficient_of_determination)).'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '*R*²评分也可以解释为模型中自变量对因变量* y*方差的解释比例（感兴趣的读者可以在这个[维基百科文章](https://en.wikipedia.org/wiki/Coefficient_of_determination)中找到原因）。'
- en: 'Advantages of *R*² score:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '*R*²评分的优点：'
- en: Does not depend on the scale of the features.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不依赖于特征的尺度。
- en: Can be used to compare the performance of different models across different
    data sets.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以用来比较不同模型在不同数据集上的表现。
- en: 'Disadvantages of *R*² score:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '*R*²评分的缺点：'
- en: Does not provide information on the magnitude of the model’s errors.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不提供有关模型误差大小的信息。
- en: '*R*² score is monotonically increasing with the number of features the model
    has, thus it cannot be used to compare models with very different numbers of features.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*R*²评分随着模型特征数量的增加而单调增加，因此不能用来比较特征数量非常不同的模型。'
- en: OLS and Maximum Likelihood
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OLS 和最大似然
- en: Finally, we will show the correlation between ordinary least squares (OLS) and
    maximum likelihood, which is the main motivation for using OLS to solve regression
    problems. More specifically, we will prove that an OLS estimator is identical
    to the maximum likelihood estimator (MLE) under the assumption that the errors
    are normally distributed with zero mean.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将展示普通最小二乘（OLS）和最大似然之间的关联，这也是使用OLS解决回归问题的主要动机。更具体地说，我们将证明在假设误差服从均值为零的正态分布的条件下，OLS估计量与最大似然估计量（MLE）是相同的。
- en: For those unfamiliar with the concept of maximum likelihood, check out my [previous
    article](https://medium.com/@roiyeho/maximum-likelihood-855b6df92c43).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 对最大似然概念不熟悉的人，可以查看我的[上一篇文章](https://medium.com/@roiyeho/maximum-likelihood-855b6df92c43)。
- en: 'Recall that in linear regression we assume that the labels are generated by
    a linear function of the features plus some random noise:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在线性回归中，我们假设标签由特征的线性函数加上一些随机噪声生成：
- en: '![](../Images/1ce56a552d6f0d2986fe320b69038e85.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ce56a552d6f0d2986fe320b69038e85.png)'
- en: 'Let’s assume that the errors are independent and identically distributed (i.i.d.),
    and have a normal distribution with mean 0 and variance *σ*²:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 假设误差是独立同分布（i.i.d.）的，并且具有均值为0、方差为*σ*²的正态分布：
- en: '![](../Images/9d97d8f2caff7c561a4845616b0ee028.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d97d8f2caff7c561a4845616b0ee028.png)'
- en: 'In this case, the labels *y* are also normally distributed with a mean of **w***ᵗ***x**
    and variance *σ*² (adding a constant to a normally-distributed variable yields
    a variable that is also normally distributed but whose mean is shifted by that
    constant):'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，标签* y*也服从均值为**w***ᵗ***x** 和方差为*σ*²的正态分布（向正态分布变量中添加常数将得到一个正态分布的变量，但其均值被该常数平移）：
- en: '![](../Images/ae00604ce05c6a2035d1632abd5bf4ee.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ae00604ce05c6a2035d1632abd5bf4ee.png)'
- en: 'Therefore, the probability density function (PDF) of *y* given the inputs **x**
    and the weight vector **w** is:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，给定输入**x**和权重向量**w**，*y*的概率密度函数（PDF）为：
- en: '![](../Images/16428d74129c4ac309662a2513337dd9.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/16428d74129c4ac309662a2513337dd9.png)'
- en: 'Based on the assumption of independence of the errors (and hence the labels),
    we can write the likelihood of the parameters **w** of the model as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 基于误差（因此标签）的独立假设，我们可以将模型参数**w**的似然表示如下：
- en: '![](../Images/b523be477119459a86ca9447a9585180.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b523be477119459a86ca9447a9585180.png)'
- en: 'Therefore, the log likelihood is:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对数似然是：
- en: '![](../Images/c51303a6925a1978e86d8f0fabf8b418.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c51303a6925a1978e86d8f0fabf8b418.png)'
- en: 'We can see that the only expression in the log likelihood that depends on the
    parameters **w** is:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，对数似然中唯一依赖于参数**w**的表达式是：
- en: '![](../Images/3812d945d20e95b3817e4b12fe9e1c8d.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3812d945d20e95b3817e4b12fe9e1c8d.png)'
- en: which is exactly the cost function of OLS! Therefore, maximizing the likelihood
    of the modelis identical to minimizing the sum of squared residuals.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是普通最小二乘法（OLS）的成本函数！因此，最大化模型的似然等同于最小化平方残差和。
- en: Final Notes
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最后的说明
- en: All images unless otherwise noted are by the author.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，否则所有图片均由作者提供。
- en: 'You can find the code examples of this article on my github: [https://github.com/roiyeho/medium/tree/main/simple_linear_regression](https://github.com/roiyeho/medium/tree/main/simple_linear_regression)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在我的 GitHub 上找到本文的代码示例：[https://github.com/roiyeho/medium/tree/main/simple_linear_regression](https://github.com/roiyeho/medium/tree/main/simple_linear_regression)
- en: The second part of the article that discusses multiple linear regression can
    be found [here](https://medium.com/towards-data-science/linear-regression-in-depth-part-2-5d40fd19efd4).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论多元线性回归的文章第二部分可以在[这里](https://medium.com/towards-data-science/linear-regression-in-depth-part-2-5d40fd19efd4)找到。
- en: Thanks for reading!
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
