- en: Exploring TensorFlow Model Prediction Issues
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索TensorFlow模型预测问题
- en: 原文：[https://towardsdatascience.com/exploring-tensorflow-model-prediction-issues-38092d0cdcc3](https://towardsdatascience.com/exploring-tensorflow-model-prediction-issues-38092d0cdcc3)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/exploring-tensorflow-model-prediction-issues-38092d0cdcc3](https://towardsdatascience.com/exploring-tensorflow-model-prediction-issues-38092d0cdcc3)
- en: Steps to debug BERT’s (and other LLMs’) slow prediction times on a personal
    computer
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在个人电脑上调试BERT（以及其他LLM）慢预测时间的步骤
- en: '[](https://adam1brownell.medium.com/?source=post_page-----38092d0cdcc3--------------------------------)[![Adam
    Brownell](../Images/49b224e4c9f838e907af6c139d666ee3.png)](https://adam1brownell.medium.com/?source=post_page-----38092d0cdcc3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----38092d0cdcc3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----38092d0cdcc3--------------------------------)
    [Adam Brownell](https://adam1brownell.medium.com/?source=post_page-----38092d0cdcc3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://adam1brownell.medium.com/?source=post_page-----38092d0cdcc3--------------------------------)[![Adam
    Brownell](../Images/49b224e4c9f838e907af6c139d666ee3.png)](https://adam1brownell.medium.com/?source=post_page-----38092d0cdcc3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----38092d0cdcc3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----38092d0cdcc3--------------------------------)
    [Adam Brownell](https://adam1brownell.medium.com/?source=post_page-----38092d0cdcc3--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----38092d0cdcc3--------------------------------)
    ·7 min read·Feb 2, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----38092d0cdcc3--------------------------------)
    ·7分钟阅读·2023年2月2日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'This all started when I was playing around with BERT models, and I got the
    ominous message all Data Scientists hope to avoid:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 一切始于我玩弄BERT模型时，收到了一条所有数据科学家都希望避免的凶兆消息：
- en: '![](../Images/a40d7372b0c9206a3e531750beabb71d.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a40d7372b0c9206a3e531750beabb71d.png)'
- en: The dreaded “Kernel Died” message 💀
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 令人恐惧的“内核崩溃”消息 💀
- en: This happened to me while I was running my TensorFlow BERT model on my Jupyter
    Notebook. Training large language models (LLMs) notoriously takes a large amount
    of data and compute, so it could make sense for my comparably puny laptop to crash
    here…
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当我在Jupyter Notebook上运行我的TensorFlow BERT模型时，这种情况发生了。训练大型语言模型（LLMs）通常需要大量的数据和计算，因此我的相对微不足道的笔记本电脑在这里崩溃是有道理的……
- en: … except this crash occurred during *prediction*, rather than *training*, which
    was strange given my assumption that more memory was used during training than
    prediction.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: …只是这次崩溃发生在*预测*期间，而不是*训练*期间，这很奇怪，因为我认为训练时使用的内存比预测时多。
- en: The “Kernel Died” error provided is unfortunately not very descriptive, and
    debugging line-by-line through the TensorFlow sounded like a daunting exercise.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: “Kernel Died”错误提供的信息不够具体，而逐行调试TensorFlow听起来像是一项艰巨的任务。
- en: A few quick searches around Stack Overflow did not completely answer my outstanding
    questions either. But I still needed a path forward.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一些在Stack Overflow上的快速搜索也没有完全回答我悬而未决的问题。但我仍然需要一个前进的方向。
- en: This is my exploration of the Kernel dying problem and how I found a solution.
    🚀
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我对内核崩溃问题的探索以及我找到解决方案的过程。 🚀
- en: Digging Deeper
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入探索
- en: Given the only thing I knew about my issue was that the kernel died, I had to
    gather more context. From a [few other threads](https://stackoverflow.com/questions/39328658/how-to-debug-dying-jupyter-python3-kernel),
    it seemed clear that the reason for the kernel dying was the my model prediction
    required more RAM than my CPU could provide (8GB), even during prediction.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我对问题的唯一了解是内核崩溃，我需要收集更多的背景信息。从[其他几个线程](https://stackoverflow.com/questions/39328658/how-to-debug-dying-jupyter-python3-kernel)来看，似乎内核崩溃的原因是我的模型预测需要的内存超出了我的CPU可以提供的（8GB），即使在预测期间也是如此。
- en: Now, a very direct solution (which most everyone would assume) is to simply
    get or rent a GPU via [Google Colab](https://colab.research.google.com/) or something
    like that. And I think that is certainly a viable solution.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，一个非常直接的解决方案（大多数人会假设）是通过[Google Colab](https://colab.research.google.com/)或类似的服务获取或租用GPU。我认为这确实是一个可行的解决方案。
- en: But I wanted to know how far could I push my CPU on local ML projects before
    RAM became a problem. And with that in mind, we’ll need to explore a few aspects
    of the model and system itself.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 但我想知道在RAM成为问题之前，我能在本地机器学习项目中将CPU推到多远。考虑到这一点，我们需要探索模型和系统本身的几个方面。
- en: Batch Size
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批量大小
- en: Given it was a RAM issue, I figured batch size had a major role to play, so
    I wanted to stress-test this hyperparameter.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这是一个RAM问题，我认为批量大小发挥了重要作用，所以我想对这个超参数进行压力测试。
- en: 'First, I wrote three simplified version of BERT, changing only the size of
    the batches the model was using. I ran three versions:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我写了三个简化版本的BERT，仅改变模型使用的批量大小。我运行了这三种版本：
- en: '**FULL**: BERT predicting on the entire input at once'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FULL**：BERT一次性对整个输入进行预测'
- en: '**SINGLE**: BERT predicting on a single input at a time'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SINGLE**：BERT一次对一个输入进行预测'
- en: '**BATCH (100)**: BERT predicting in batches of 100 inputs at a time'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BATCH (100)**：BERT以每次100个输入的批量进行预测'
- en: 'Code for this below:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是相关代码：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: I then ran each of these models through the same test cases, with increasing
    input size. *I used the* [*classic imdb dataset*](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)
    *for this.*
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我将这些模型通过相同的测试用例运行，逐步增加输入大小。*我使用了* [*经典 imdb 数据集*](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)
    *来进行测试。*
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'And graphing the output produced an interesting trend:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制输出图表显示出有趣的趋势：
- en: '![](../Images/f199ff54e24353720ed7cb52ad7534e6.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f199ff54e24353720ed7cb52ad7534e6.png)'
- en: BATCH outperforming SINGLE made sense, because most Machine Learning models
    and packages like Tensorflow are designed to take advantage of vectorization.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: BATCH优于SINGLE是有道理的，因为大多数机器学习模型和像Tensorflow这样的包设计用来利用向量化。
- en: But what was surprising was **how much worse FULL performed against BATCH.**
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 但令人惊讶的是**FULL与BATCH的差距有多大**。
- en: I had assumed that FULL would perform the best due to vectorization up until
    it crashed the kernel from memory constraints, but in fact ***the memory constrains
    for even a few thousands examples was so extensive on my laptop that it exponentially
    increased prediction time.***
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我曾假设FULL会因为向量化表现最佳，直到它因内存限制崩溃，但实际上***即使是几千个样本的内存限制在我的笔记本上也极大地增加了预测时间。***
- en: FULL actually performed *worse* than processing one input at a time without
    any vectorization on larger inputs. 🤯
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理较大的输入时，FULL的表现实际上*更差*，比起逐个输入处理而不进行向量化。🤯
- en: At about 2,000 examples these RAM requirements start to take a toll on my CPU.
    And what’s amazing is that prior to hitting that 2K, the difference between BATCH
    and FULL is not that different.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在大约2,000个样本时，这些RAM要求开始对我的CPU造成负担。令人惊讶的是，在达到2K之前，BATCH和FULL之间的差异并不大。
- en: Based on the above chart, I assumed using a batch size of 2,000 would yield
    the best results.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上面的图表，我假设使用2,000的批量大小会产生最佳结果。
- en: I was wrong.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我错了。
- en: '**It seems the best batch size is closer to 1K**, because prediction time starting
    to creep up if we use a 2K batch size:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**似乎最佳的批量大小更接近1K**，因为如果使用2K的批量大小，预测时间开始上升：'
- en: '![](../Images/a2c8d06153e517c8199fda54935a6ea3.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a2c8d06153e517c8199fda54935a6ea3.png)'
- en: batch size impact on prediction time for 4K inputs
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 批量大小对4K输入的预测时间的影响
- en: Tokenizer
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Tokenizer
- en: 'The next piece of code I explored was the Tokenizer. Given how many hyperparameters
    the line contained, I figured it would be a place to optimze as well:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我接下来探索的代码是Tokenizer。鉴于这一行包含了许多超参数，我认为这也是一个优化的地方：
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: However, when I time-checked my FULL Model performance, both on 1K inputs where
    it performed on-par with BATCH, and on 4K where it performed significantly worse,
    **Tokenizer performance time was an irrelevant fraction of total time:**
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当我计时检查我的FULL模型性能时，在1K输入下它与BATCH表现相当，而在4K输入下表现显著较差，**Tokenizer性能时间是总时间的微不足道的一部分：**
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: While Tokenizer time increase did slightly outpace input size increase (Quadrupling
    the input size led to Tokenizer time 4.8x) prediction time increased an astounding
    **13.8x**!
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Tokenizer时间的增加确实略微超过了输入大小的增加（输入大小增加四倍导致Tokenizer时间增加4.8倍），但预测时间却惊人地增加了**13.8倍**！
- en: '**Clearly, the problem is in the** `**.predict()**` **portion of my pipeline.**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**显然，问题出在** `**.predict()**` **管道的部分。**'
- en: Tensorflow Version
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Tensorflow 版本
- en: Based on the Stack Overflow thread already referenced above, the most upvoted
    solution was to downgrade Tensorflow to speed up prediction.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上面已提到的Stack Overflow线程，最受欢迎的解决方案是将Tensorflow降级以加快预测速度。
- en: I thought this was a questionable solution, as I assumed that upgraded versions
    would have more optimizations and better runtimes, not worse. But I still tried
    it out.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这是一个值得怀疑的解决方案，因为我假设升级版本会有更多的优化和更好的运行时间，而不是更差。但我还是尝试了。
- en: 'Going to the [tensorflow Pypi page](https://pypi.org/project/tensorflow/#history),
    we can see older versions of the package. Choosing packages released approximately
    one year apart, we get the following package versions:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 访问 [tensorflow Pypi 页面](https://pypi.org/project/tensorflow/#history)，我们可以看到包的旧版本。选择发布大约相隔一年的包，我们得到以下包版本：
- en: '`2.10.0`, released Sept 2022'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`2.10.0`，发布于 2022 年 9 月'
- en: '`2.6.1`, released Nov 2021'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`2.6.1`，发布于 2021 年 11 月'
- en: '`1.15.4`, released Sept 2020'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1.15.4`，发布于 2020 年 9 月'
- en: '`1.15.0`, released Oct 2019'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1.15.0`，发布于 2019 年 10 月'
- en: 'To iteratively install different versions of the same package, we need to utilize
    the `os` package, allowing us to run terminal commands from python code:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 要迭代安装同一包的不同版本，我们需要利用 `os` 包，使我们能够从 Python 代码中运行终端命令：
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `try/except` clause is in there because we don’t know if these functions
    existed in earlier versions of the package. *Luckily, they all do*
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`try/except` 语句存在是因为我们不知道这些函数是否存在于包的早期版本中。*幸运的是，它们都存在*'
- en: '`import` statements within a loop looks wrong but is necessary since we need
    to re-import the functions once the correct package version has been installed'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在循环中的 `import` 语句看起来不对，但这是必要的，因为我们需要在安装正确的包版本后重新导入这些函数
- en: After iterating through each version, we find out that **downgrading tensorflow
    improves runtime by as much as 15%!**
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 经过每个版本的迭代，我们发现**降级 TensorFlow 可以将运行时间提高多达 15%！**
- en: '![](../Images/cf122c9f782c3b77ee13f09e5fb0715e.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf122c9f782c3b77ee13f09e5fb0715e.png)'
- en: My pet theory as to why this is the case is because newer versions of tensorflow
    are built assuming heavy GPU usage, which means it is optimized for this particular
    use case at the cost of local CPU performance.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我个人的理论是，之所以出现这种情况，是因为较新的 TensorFlow 版本假定重度使用 GPU，这意味着它针对这种特定的使用场景进行了优化，但牺牲了本地
    CPU 性能。
- en: '*If anyone has the real answer as to why older versions of TF run faster, please
    let me know!*'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果有人知道为什么旧版本的 TensorFlow 运行更快的真实原因，请告诉我！*'
- en: Conclusion & Putting it all Together
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论与总结
- en: 'With the following insights about Tensorflow runtime:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 TensorFlow 运行时的以下见解：
- en: '**The optimal prediction batch-size is about 1,000**'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最佳预测批量大小约为 1,000**'
- en: '**Tokenizer parameters do play a large role in prediction time**'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分词器参数确实在预测时间中起着重要作用**'
- en: '**Tensorflow 1.X.X has a 15% boost in prediction time**'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorFlow 1.X.X 在预测时间上提升了 15%**'
- en: 'We can put all these together, and see how it performs against our original
    batch-size experiment:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这些信息综合起来，看看它与我们最初的批量大小实验的表现如何：
- en: '![](../Images/a80018713ea66fb7bd6bc271eaa93300.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a80018713ea66fb7bd6bc271eaa93300.png)'
- en: In the largest case tested, our optimal run beats the Batch(100) by 20% and
    Single by 57%!
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在测试的最大案例中，我们的最佳运行比 Batch(100) 快 20%，比 Single 快 57%!
- en: Overall, this exercise was a simple and enjoyable expression of what it means
    to be a Data Scientist. You need to identify a problem, establish a hypothesis,
    develop a rigourous test, and analyze your results. In this case, it was my Tensorflow
    runtime. In the future, I’m sure you will find perplexing data/issues/problems
    in your own work.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，这个过程是对数据科学家身份的一种简单而愉快的表达。你需要识别问题，建立假设，制定严格的测试，并分析结果。在这个案例中，就是我的 TensorFlow
    运行时。将来，我相信你会在自己的工作中发现令人困惑的数据/问题/难题。
- en: And next time, I hope rather than check Stack Overflow and give up if the answer
    isn’t there, you roll up your sleeves and explore the problem space yourself.
    You never know what you might learn 💡
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 下次，希望你不要仅仅是查看 Stack Overflow，如果没有找到答案就放弃，而是卷起袖子自己探索问题空间。你永远不知道你可能会学到什么 💡
- en: Hope this was helpful in debugging your Tensorflow prediction time issues! 🎉
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这对调试你的 TensorFlow 预测时间问题有所帮助！ 🎉
- en: '*All images, unless otherwise noted, are by the author*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*所有图像，除非另有说明，均由作者提供*'
