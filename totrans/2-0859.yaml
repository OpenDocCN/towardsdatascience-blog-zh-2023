- en: Exploring TensorFlow Model Prediction Issues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://towardsdatascience.com/exploring-tensorflow-model-prediction-issues-38092d0cdcc3](https://towardsdatascience.com/exploring-tensorflow-model-prediction-issues-38092d0cdcc3)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Steps to debug BERT‚Äôs (and other LLMs‚Äô) slow prediction times on a personal
    computer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://adam1brownell.medium.com/?source=post_page-----38092d0cdcc3--------------------------------)[![Adam
    Brownell](../Images/49b224e4c9f838e907af6c139d666ee3.png)](https://adam1brownell.medium.com/?source=post_page-----38092d0cdcc3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----38092d0cdcc3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----38092d0cdcc3--------------------------------)
    [Adam Brownell](https://adam1brownell.medium.com/?source=post_page-----38092d0cdcc3--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ¬∑Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----38092d0cdcc3--------------------------------)
    ¬∑7 min read¬∑Feb 2, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: 'This all started when I was playing around with BERT models, and I got the
    ominous message all Data Scientists hope to avoid:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a40d7372b0c9206a3e531750beabb71d.png)'
  prefs: []
  type: TYPE_IMG
- en: The dreaded ‚ÄúKernel Died‚Äù message üíÄ
  prefs: []
  type: TYPE_NORMAL
- en: This happened to me while I was running my TensorFlow BERT model on my Jupyter
    Notebook. Training large language models (LLMs) notoriously takes a large amount
    of data and compute, so it could make sense for my comparably puny laptop to crash
    here‚Ä¶
  prefs: []
  type: TYPE_NORMAL
- en: ‚Ä¶ except this crash occurred during *prediction*, rather than *training*, which
    was strange given my assumption that more memory was used during training than
    prediction.
  prefs: []
  type: TYPE_NORMAL
- en: The ‚ÄúKernel Died‚Äù error provided is unfortunately not very descriptive, and
    debugging line-by-line through the TensorFlow sounded like a daunting exercise.
  prefs: []
  type: TYPE_NORMAL
- en: A few quick searches around Stack Overflow did not completely answer my outstanding
    questions either. But I still needed a path forward.
  prefs: []
  type: TYPE_NORMAL
- en: This is my exploration of the Kernel dying problem and how I found a solution.
    üöÄ
  prefs: []
  type: TYPE_NORMAL
- en: Digging Deeper
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given the only thing I knew about my issue was that the kernel died, I had to
    gather more context. From a [few other threads](https://stackoverflow.com/questions/39328658/how-to-debug-dying-jupyter-python3-kernel),
    it seemed clear that the reason for the kernel dying was the my model prediction
    required more RAM than my CPU could provide (8GB), even during prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Now, a very direct solution (which most everyone would assume) is to simply
    get or rent a GPU via [Google Colab](https://colab.research.google.com/) or something
    like that. And I think that is certainly a viable solution.
  prefs: []
  type: TYPE_NORMAL
- en: But I wanted to know how far could I push my CPU on local ML projects before
    RAM became a problem. And with that in mind, we‚Äôll need to explore a few aspects
    of the model and system itself.
  prefs: []
  type: TYPE_NORMAL
- en: Batch Size
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given it was a RAM issue, I figured batch size had a major role to play, so
    I wanted to stress-test this hyperparameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, I wrote three simplified version of BERT, changing only the size of
    the batches the model was using. I ran three versions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**FULL**: BERT predicting on the entire input at once'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SINGLE**: BERT predicting on a single input at a time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BATCH (100)**: BERT predicting in batches of 100 inputs at a time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Code for this below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: I then ran each of these models through the same test cases, with increasing
    input size. *I used the* [*classic imdb dataset*](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)
    *for this.*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'And graphing the output produced an interesting trend:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f199ff54e24353720ed7cb52ad7534e6.png)'
  prefs: []
  type: TYPE_IMG
- en: BATCH outperforming SINGLE made sense, because most Machine Learning models
    and packages like Tensorflow are designed to take advantage of vectorization.
  prefs: []
  type: TYPE_NORMAL
- en: But what was surprising was **how much worse FULL performed against BATCH.**
  prefs: []
  type: TYPE_NORMAL
- en: I had assumed that FULL would perform the best due to vectorization up until
    it crashed the kernel from memory constraints, but in fact ***the memory constrains
    for even a few thousands examples was so extensive on my laptop that it exponentially
    increased prediction time.***
  prefs: []
  type: TYPE_NORMAL
- en: FULL actually performed *worse* than processing one input at a time without
    any vectorization on larger inputs. ü§Ø
  prefs: []
  type: TYPE_NORMAL
- en: At about 2,000 examples these RAM requirements start to take a toll on my CPU.
    And what‚Äôs amazing is that prior to hitting that 2K, the difference between BATCH
    and FULL is not that different.
  prefs: []
  type: TYPE_NORMAL
- en: Based on the above chart, I assumed using a batch size of 2,000 would yield
    the best results.
  prefs: []
  type: TYPE_NORMAL
- en: I was wrong.
  prefs: []
  type: TYPE_NORMAL
- en: '**It seems the best batch size is closer to 1K**, because prediction time starting
    to creep up if we use a 2K batch size:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a2c8d06153e517c8199fda54935a6ea3.png)'
  prefs: []
  type: TYPE_IMG
- en: batch size impact on prediction time for 4K inputs
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next piece of code I explored was the Tokenizer. Given how many hyperparameters
    the line contained, I figured it would be a place to optimze as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: However, when I time-checked my FULL Model performance, both on 1K inputs where
    it performed on-par with BATCH, and on 4K where it performed significantly worse,
    **Tokenizer performance time was an irrelevant fraction of total time:**
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: While Tokenizer time increase did slightly outpace input size increase (Quadrupling
    the input size led to Tokenizer time 4.8x) prediction time increased an astounding
    **13.8x**!
  prefs: []
  type: TYPE_NORMAL
- en: '**Clearly, the problem is in the** `**.predict()**` **portion of my pipeline.**'
  prefs: []
  type: TYPE_NORMAL
- en: Tensorflow Version
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Based on the Stack Overflow thread already referenced above, the most upvoted
    solution was to downgrade Tensorflow to speed up prediction.
  prefs: []
  type: TYPE_NORMAL
- en: I thought this was a questionable solution, as I assumed that upgraded versions
    would have more optimizations and better runtimes, not worse. But I still tried
    it out.
  prefs: []
  type: TYPE_NORMAL
- en: 'Going to the [tensorflow Pypi page](https://pypi.org/project/tensorflow/#history),
    we can see older versions of the package. Choosing packages released approximately
    one year apart, we get the following package versions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`2.10.0`, released Sept 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`2.6.1`, released Nov 2021'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1.15.4`, released Sept 2020'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1.15.0`, released Oct 2019'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To iteratively install different versions of the same package, we need to utilize
    the `os` package, allowing us to run terminal commands from python code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `try/except` clause is in there because we don‚Äôt know if these functions
    existed in earlier versions of the package. *Luckily, they all do*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`import` statements within a loop looks wrong but is necessary since we need
    to re-import the functions once the correct package version has been installed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After iterating through each version, we find out that **downgrading tensorflow
    improves runtime by as much as 15%!**
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cf122c9f782c3b77ee13f09e5fb0715e.png)'
  prefs: []
  type: TYPE_IMG
- en: My pet theory as to why this is the case is because newer versions of tensorflow
    are built assuming heavy GPU usage, which means it is optimized for this particular
    use case at the cost of local CPU performance.
  prefs: []
  type: TYPE_NORMAL
- en: '*If anyone has the real answer as to why older versions of TF run faster, please
    let me know!*'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion & Putting it all Together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With the following insights about Tensorflow runtime:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The optimal prediction batch-size is about 1,000**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tokenizer parameters do play a large role in prediction time**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tensorflow 1.X.X has a 15% boost in prediction time**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can put all these together, and see how it performs against our original
    batch-size experiment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a80018713ea66fb7bd6bc271eaa93300.png)'
  prefs: []
  type: TYPE_IMG
- en: In the largest case tested, our optimal run beats the Batch(100) by 20% and
    Single by 57%!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Overall, this exercise was a simple and enjoyable expression of what it means
    to be a Data Scientist. You need to identify a problem, establish a hypothesis,
    develop a rigourous test, and analyze your results. In this case, it was my Tensorflow
    runtime. In the future, I‚Äôm sure you will find perplexing data/issues/problems
    in your own work.
  prefs: []
  type: TYPE_NORMAL
- en: And next time, I hope rather than check Stack Overflow and give up if the answer
    isn‚Äôt there, you roll up your sleeves and explore the problem space yourself.
    You never know what you might learn üí°
  prefs: []
  type: TYPE_NORMAL
- en: Hope this was helpful in debugging your Tensorflow prediction time issues! üéâ
  prefs: []
  type: TYPE_NORMAL
- en: '*All images, unless otherwise noted, are by the author*'
  prefs: []
  type: TYPE_NORMAL
