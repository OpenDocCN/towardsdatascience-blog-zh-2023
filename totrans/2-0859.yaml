- en: Exploring TensorFlow Model Prediction Issues
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¢ç´¢TensorFlowæ¨¡å‹é¢„æµ‹é—®é¢˜
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/exploring-tensorflow-model-prediction-issues-38092d0cdcc3](https://towardsdatascience.com/exploring-tensorflow-model-prediction-issues-38092d0cdcc3)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/exploring-tensorflow-model-prediction-issues-38092d0cdcc3](https://towardsdatascience.com/exploring-tensorflow-model-prediction-issues-38092d0cdcc3)
- en: Steps to debug BERTâ€™s (and other LLMsâ€™) slow prediction times on a personal
    computer
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åœ¨ä¸ªäººç”µè„‘ä¸Šè°ƒè¯•BERTï¼ˆä»¥åŠå…¶ä»–LLMï¼‰æ…¢é¢„æµ‹æ—¶é—´çš„æ­¥éª¤
- en: '[](https://adam1brownell.medium.com/?source=post_page-----38092d0cdcc3--------------------------------)[![Adam
    Brownell](../Images/49b224e4c9f838e907af6c139d666ee3.png)](https://adam1brownell.medium.com/?source=post_page-----38092d0cdcc3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----38092d0cdcc3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----38092d0cdcc3--------------------------------)
    [Adam Brownell](https://adam1brownell.medium.com/?source=post_page-----38092d0cdcc3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://adam1brownell.medium.com/?source=post_page-----38092d0cdcc3--------------------------------)[![Adam
    Brownell](../Images/49b224e4c9f838e907af6c139d666ee3.png)](https://adam1brownell.medium.com/?source=post_page-----38092d0cdcc3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----38092d0cdcc3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----38092d0cdcc3--------------------------------)
    [Adam Brownell](https://adam1brownell.medium.com/?source=post_page-----38092d0cdcc3--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----38092d0cdcc3--------------------------------)
    Â·7 min readÂ·Feb 2, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page-----38092d0cdcc3--------------------------------)
    Â·7åˆ†é’Ÿé˜…è¯»Â·2023å¹´2æœˆ2æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'This all started when I was playing around with BERT models, and I got the
    ominous message all Data Scientists hope to avoid:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€åˆ‡å§‹äºæˆ‘ç©å¼„BERTæ¨¡å‹æ—¶ï¼Œæ”¶åˆ°äº†ä¸€æ¡æ‰€æœ‰æ•°æ®ç§‘å­¦å®¶éƒ½å¸Œæœ›é¿å…çš„å‡¶å…†æ¶ˆæ¯ï¼š
- en: '![](../Images/a40d7372b0c9206a3e531750beabb71d.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a40d7372b0c9206a3e531750beabb71d.png)'
- en: The dreaded â€œKernel Diedâ€ message ğŸ’€
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¤äººææƒ§çš„â€œå†…æ ¸å´©æºƒâ€æ¶ˆæ¯ ğŸ’€
- en: This happened to me while I was running my TensorFlow BERT model on my Jupyter
    Notebook. Training large language models (LLMs) notoriously takes a large amount
    of data and compute, so it could make sense for my comparably puny laptop to crash
    hereâ€¦
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘åœ¨Jupyter Notebookä¸Šè¿è¡Œæˆ‘çš„TensorFlow BERTæ¨¡å‹æ—¶ï¼Œè¿™ç§æƒ…å†µå‘ç”Ÿäº†ã€‚è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šå¸¸éœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—ï¼Œå› æ­¤æˆ‘çš„ç›¸å¯¹å¾®ä¸è¶³é“çš„ç¬”è®°æœ¬ç”µè„‘åœ¨è¿™é‡Œå´©æºƒæ˜¯æœ‰é“ç†çš„â€¦â€¦
- en: â€¦ except this crash occurred during *prediction*, rather than *training*, which
    was strange given my assumption that more memory was used during training than
    prediction.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: â€¦åªæ˜¯è¿™æ¬¡å´©æºƒå‘ç”Ÿåœ¨*é¢„æµ‹*æœŸé—´ï¼Œè€Œä¸æ˜¯*è®­ç»ƒ*æœŸé—´ï¼Œè¿™å¾ˆå¥‡æ€ªï¼Œå› ä¸ºæˆ‘è®¤ä¸ºè®­ç»ƒæ—¶ä½¿ç”¨çš„å†…å­˜æ¯”é¢„æµ‹æ—¶å¤šã€‚
- en: The â€œKernel Diedâ€ error provided is unfortunately not very descriptive, and
    debugging line-by-line through the TensorFlow sounded like a daunting exercise.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: â€œKernel Diedâ€é”™è¯¯æä¾›çš„ä¿¡æ¯ä¸å¤Ÿå…·ä½“ï¼Œè€Œé€è¡Œè°ƒè¯•TensorFlowå¬èµ·æ¥åƒæ˜¯ä¸€é¡¹è‰°å·¨çš„ä»»åŠ¡ã€‚
- en: A few quick searches around Stack Overflow did not completely answer my outstanding
    questions either. But I still needed a path forward.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€äº›åœ¨Stack Overflowä¸Šçš„å¿«é€Ÿæœç´¢ä¹Ÿæ²¡æœ‰å®Œå…¨å›ç­”æˆ‘æ‚¬è€Œæœªå†³çš„é—®é¢˜ã€‚ä½†æˆ‘ä»ç„¶éœ€è¦ä¸€ä¸ªå‰è¿›çš„æ–¹å‘ã€‚
- en: This is my exploration of the Kernel dying problem and how I found a solution.
    ğŸš€
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘å¯¹å†…æ ¸å´©æºƒé—®é¢˜çš„æ¢ç´¢ä»¥åŠæˆ‘æ‰¾åˆ°è§£å†³æ–¹æ¡ˆçš„è¿‡ç¨‹ã€‚ ğŸš€
- en: Digging Deeper
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ·±å…¥æ¢ç´¢
- en: Given the only thing I knew about my issue was that the kernel died, I had to
    gather more context. From a [few other threads](https://stackoverflow.com/questions/39328658/how-to-debug-dying-jupyter-python3-kernel),
    it seemed clear that the reason for the kernel dying was the my model prediction
    required more RAM than my CPU could provide (8GB), even during prediction.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæˆ‘å¯¹é—®é¢˜çš„å”¯ä¸€äº†è§£æ˜¯å†…æ ¸å´©æºƒï¼Œæˆ‘éœ€è¦æ”¶é›†æ›´å¤šçš„èƒŒæ™¯ä¿¡æ¯ã€‚ä»[å…¶ä»–å‡ ä¸ªçº¿ç¨‹](https://stackoverflow.com/questions/39328658/how-to-debug-dying-jupyter-python3-kernel)æ¥çœ‹ï¼Œä¼¼ä¹å†…æ ¸å´©æºƒçš„åŸå› æ˜¯æˆ‘çš„æ¨¡å‹é¢„æµ‹éœ€è¦çš„å†…å­˜è¶…å‡ºäº†æˆ‘çš„CPUå¯ä»¥æä¾›çš„ï¼ˆ8GBï¼‰ï¼Œå³ä½¿åœ¨é¢„æµ‹æœŸé—´ä¹Ÿæ˜¯å¦‚æ­¤ã€‚
- en: Now, a very direct solution (which most everyone would assume) is to simply
    get or rent a GPU via [Google Colab](https://colab.research.google.com/) or something
    like that. And I think that is certainly a viable solution.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œä¸€ä¸ªéå¸¸ç›´æ¥çš„è§£å†³æ–¹æ¡ˆï¼ˆå¤§å¤šæ•°äººä¼šå‡è®¾ï¼‰æ˜¯é€šè¿‡[Google Colab](https://colab.research.google.com/)æˆ–ç±»ä¼¼çš„æœåŠ¡è·å–æˆ–ç§Ÿç”¨GPUã€‚æˆ‘è®¤ä¸ºè¿™ç¡®å®æ˜¯ä¸€ä¸ªå¯è¡Œçš„è§£å†³æ–¹æ¡ˆã€‚
- en: But I wanted to know how far could I push my CPU on local ML projects before
    RAM became a problem. And with that in mind, weâ€™ll need to explore a few aspects
    of the model and system itself.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘æƒ³çŸ¥é“åœ¨RAMæˆä¸ºé—®é¢˜ä¹‹å‰ï¼Œæˆ‘èƒ½åœ¨æœ¬åœ°æœºå™¨å­¦ä¹ é¡¹ç›®ä¸­å°†CPUæ¨åˆ°å¤šè¿œã€‚è€ƒè™‘åˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬éœ€è¦æ¢ç´¢æ¨¡å‹å’Œç³»ç»Ÿæœ¬èº«çš„å‡ ä¸ªæ–¹é¢ã€‚
- en: Batch Size
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ‰¹é‡å¤§å°
- en: Given it was a RAM issue, I figured batch size had a major role to play, so
    I wanted to stress-test this hyperparameter.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: é‰´äºè¿™æ˜¯ä¸€ä¸ªRAMé—®é¢˜ï¼Œæˆ‘è®¤ä¸ºæ‰¹é‡å¤§å°å‘æŒ¥äº†é‡è¦ä½œç”¨ï¼Œæ‰€ä»¥æˆ‘æƒ³å¯¹è¿™ä¸ªè¶…å‚æ•°è¿›è¡Œå‹åŠ›æµ‹è¯•ã€‚
- en: 'First, I wrote three simplified version of BERT, changing only the size of
    the batches the model was using. I ran three versions:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘å†™äº†ä¸‰ä¸ªç®€åŒ–ç‰ˆæœ¬çš„BERTï¼Œä»…æ”¹å˜æ¨¡å‹ä½¿ç”¨çš„æ‰¹é‡å¤§å°ã€‚æˆ‘è¿è¡Œäº†è¿™ä¸‰ç§ç‰ˆæœ¬ï¼š
- en: '**FULL**: BERT predicting on the entire input at once'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FULL**ï¼šBERTä¸€æ¬¡æ€§å¯¹æ•´ä¸ªè¾“å…¥è¿›è¡Œé¢„æµ‹'
- en: '**SINGLE**: BERT predicting on a single input at a time'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SINGLE**ï¼šBERTä¸€æ¬¡å¯¹ä¸€ä¸ªè¾“å…¥è¿›è¡Œé¢„æµ‹'
- en: '**BATCH (100)**: BERT predicting in batches of 100 inputs at a time'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BATCH (100)**ï¼šBERTä»¥æ¯æ¬¡100ä¸ªè¾“å…¥çš„æ‰¹é‡è¿›è¡Œé¢„æµ‹'
- en: 'Code for this below:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ç›¸å…³ä»£ç ï¼š
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: I then ran each of these models through the same test cases, with increasing
    input size. *I used the* [*classic imdb dataset*](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)
    *for this.*
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘å°†è¿™äº›æ¨¡å‹é€šè¿‡ç›¸åŒçš„æµ‹è¯•ç”¨ä¾‹è¿è¡Œï¼Œé€æ­¥å¢åŠ è¾“å…¥å¤§å°ã€‚*æˆ‘ä½¿ç”¨äº†* [*ç»å…¸ imdb æ•°æ®é›†*](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)
    *æ¥è¿›è¡Œæµ‹è¯•ã€‚*
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'And graphing the output produced an interesting trend:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ç»˜åˆ¶è¾“å‡ºå›¾è¡¨æ˜¾ç¤ºå‡ºæœ‰è¶£çš„è¶‹åŠ¿ï¼š
- en: '![](../Images/f199ff54e24353720ed7cb52ad7534e6.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f199ff54e24353720ed7cb52ad7534e6.png)'
- en: BATCH outperforming SINGLE made sense, because most Machine Learning models
    and packages like Tensorflow are designed to take advantage of vectorization.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: BATCHä¼˜äºSINGLEæ˜¯æœ‰é“ç†çš„ï¼Œå› ä¸ºå¤§å¤šæ•°æœºå™¨å­¦ä¹ æ¨¡å‹å’ŒåƒTensorflowè¿™æ ·çš„åŒ…è®¾è®¡ç”¨æ¥åˆ©ç”¨å‘é‡åŒ–ã€‚
- en: But what was surprising was **how much worse FULL performed against BATCH.**
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†ä»¤äººæƒŠè®¶çš„æ˜¯**FULLä¸BATCHçš„å·®è·æœ‰å¤šå¤§**ã€‚
- en: I had assumed that FULL would perform the best due to vectorization up until
    it crashed the kernel from memory constraints, but in fact ***the memory constrains
    for even a few thousands examples was so extensive on my laptop that it exponentially
    increased prediction time.***
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ›¾å‡è®¾FULLä¼šå› ä¸ºå‘é‡åŒ–è¡¨ç°æœ€ä½³ï¼Œç›´åˆ°å®ƒå› å†…å­˜é™åˆ¶å´©æºƒï¼Œä½†å®é™…ä¸Š***å³ä½¿æ˜¯å‡ åƒä¸ªæ ·æœ¬çš„å†…å­˜é™åˆ¶åœ¨æˆ‘çš„ç¬”è®°æœ¬ä¸Šä¹Ÿæå¤§åœ°å¢åŠ äº†é¢„æµ‹æ—¶é—´ã€‚***
- en: FULL actually performed *worse* than processing one input at a time without
    any vectorization on larger inputs. ğŸ¤¯
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¤„ç†è¾ƒå¤§çš„è¾“å…¥æ—¶ï¼ŒFULLçš„è¡¨ç°å®é™…ä¸Š*æ›´å·®*ï¼Œæ¯”èµ·é€ä¸ªè¾“å…¥å¤„ç†è€Œä¸è¿›è¡Œå‘é‡åŒ–ã€‚ğŸ¤¯
- en: At about 2,000 examples these RAM requirements start to take a toll on my CPU.
    And whatâ€™s amazing is that prior to hitting that 2K, the difference between BATCH
    and FULL is not that different.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¤§çº¦2,000ä¸ªæ ·æœ¬æ—¶ï¼Œè¿™äº›RAMè¦æ±‚å¼€å§‹å¯¹æˆ‘çš„CPUé€ æˆè´Ÿæ‹…ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œåœ¨è¾¾åˆ°2Kä¹‹å‰ï¼ŒBATCHå’ŒFULLä¹‹é—´çš„å·®å¼‚å¹¶ä¸å¤§ã€‚
- en: Based on the above chart, I assumed using a batch size of 2,000 would yield
    the best results.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®ä¸Šé¢çš„å›¾è¡¨ï¼Œæˆ‘å‡è®¾ä½¿ç”¨2,000çš„æ‰¹é‡å¤§å°ä¼šäº§ç”Ÿæœ€ä½³ç»“æœã€‚
- en: I was wrong.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘é”™äº†ã€‚
- en: '**It seems the best batch size is closer to 1K**, because prediction time starting
    to creep up if we use a 2K batch size:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä¼¼ä¹æœ€ä½³çš„æ‰¹é‡å¤§å°æ›´æ¥è¿‘1K**ï¼Œå› ä¸ºå¦‚æœä½¿ç”¨2Kçš„æ‰¹é‡å¤§å°ï¼Œé¢„æµ‹æ—¶é—´å¼€å§‹ä¸Šå‡ï¼š'
- en: '![](../Images/a2c8d06153e517c8199fda54935a6ea3.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a2c8d06153e517c8199fda54935a6ea3.png)'
- en: batch size impact on prediction time for 4K inputs
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰¹é‡å¤§å°å¯¹4Kè¾“å…¥çš„é¢„æµ‹æ—¶é—´çš„å½±å“
- en: Tokenizer
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Tokenizer
- en: 'The next piece of code I explored was the Tokenizer. Given how many hyperparameters
    the line contained, I figured it would be a place to optimze as well:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ¥ä¸‹æ¥æ¢ç´¢çš„ä»£ç æ˜¯Tokenizerã€‚é‰´äºè¿™ä¸€è¡ŒåŒ…å«äº†è®¸å¤šè¶…å‚æ•°ï¼Œæˆ‘è®¤ä¸ºè¿™ä¹Ÿæ˜¯ä¸€ä¸ªä¼˜åŒ–çš„åœ°æ–¹ï¼š
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: However, when I time-checked my FULL Model performance, both on 1K inputs where
    it performed on-par with BATCH, and on 4K where it performed significantly worse,
    **Tokenizer performance time was an irrelevant fraction of total time:**
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œå½“æˆ‘è®¡æ—¶æ£€æŸ¥æˆ‘çš„FULLæ¨¡å‹æ€§èƒ½æ—¶ï¼Œåœ¨1Kè¾“å…¥ä¸‹å®ƒä¸BATCHè¡¨ç°ç›¸å½“ï¼Œè€Œåœ¨4Kè¾“å…¥ä¸‹è¡¨ç°æ˜¾è‘—è¾ƒå·®ï¼Œ**Tokenizeræ€§èƒ½æ—¶é—´æ˜¯æ€»æ—¶é—´çš„å¾®ä¸è¶³é“çš„ä¸€éƒ¨åˆ†ï¼š**
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: While Tokenizer time increase did slightly outpace input size increase (Quadrupling
    the input size led to Tokenizer time 4.8x) prediction time increased an astounding
    **13.8x**!
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶Tokenizeræ—¶é—´çš„å¢åŠ ç¡®å®ç•¥å¾®è¶…è¿‡äº†è¾“å…¥å¤§å°çš„å¢åŠ ï¼ˆè¾“å…¥å¤§å°å¢åŠ å››å€å¯¼è‡´Tokenizeræ—¶é—´å¢åŠ 4.8å€ï¼‰ï¼Œä½†é¢„æµ‹æ—¶é—´å´æƒŠäººåœ°å¢åŠ äº†**13.8å€**ï¼
- en: '**Clearly, the problem is in the** `**.predict()**` **portion of my pipeline.**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ˜¾ç„¶ï¼Œé—®é¢˜å‡ºåœ¨** `**.predict()**` **ç®¡é“çš„éƒ¨åˆ†ã€‚**'
- en: Tensorflow Version
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Tensorflow ç‰ˆæœ¬
- en: Based on the Stack Overflow thread already referenced above, the most upvoted
    solution was to downgrade Tensorflow to speed up prediction.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®ä¸Šé¢å·²æåˆ°çš„Stack Overflowçº¿ç¨‹ï¼Œæœ€å—æ¬¢è¿çš„è§£å†³æ–¹æ¡ˆæ˜¯å°†Tensorflowé™çº§ä»¥åŠ å¿«é¢„æµ‹é€Ÿåº¦ã€‚
- en: I thought this was a questionable solution, as I assumed that upgraded versions
    would have more optimizations and better runtimes, not worse. But I still tried
    it out.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªå€¼å¾—æ€€ç–‘çš„è§£å†³æ–¹æ¡ˆï¼Œå› ä¸ºæˆ‘å‡è®¾å‡çº§ç‰ˆæœ¬ä¼šæœ‰æ›´å¤šçš„ä¼˜åŒ–å’Œæ›´å¥½çš„è¿è¡Œæ—¶é—´ï¼Œè€Œä¸æ˜¯æ›´å·®ã€‚ä½†æˆ‘è¿˜æ˜¯å°è¯•äº†ã€‚
- en: 'Going to the [tensorflow Pypi page](https://pypi.org/project/tensorflow/#history),
    we can see older versions of the package. Choosing packages released approximately
    one year apart, we get the following package versions:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: è®¿é—® [tensorflow Pypi é¡µé¢](https://pypi.org/project/tensorflow/#history)ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°åŒ…çš„æ—§ç‰ˆæœ¬ã€‚é€‰æ‹©å‘å¸ƒå¤§çº¦ç›¸éš”ä¸€å¹´çš„åŒ…ï¼Œæˆ‘ä»¬å¾—åˆ°ä»¥ä¸‹åŒ…ç‰ˆæœ¬ï¼š
- en: '`2.10.0`, released Sept 2022'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`2.10.0`ï¼Œå‘å¸ƒäº 2022 å¹´ 9 æœˆ'
- en: '`2.6.1`, released Nov 2021'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`2.6.1`ï¼Œå‘å¸ƒäº 2021 å¹´ 11 æœˆ'
- en: '`1.15.4`, released Sept 2020'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1.15.4`ï¼Œå‘å¸ƒäº 2020 å¹´ 9 æœˆ'
- en: '`1.15.0`, released Oct 2019'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1.15.0`ï¼Œå‘å¸ƒäº 2019 å¹´ 10 æœˆ'
- en: 'To iteratively install different versions of the same package, we need to utilize
    the `os` package, allowing us to run terminal commands from python code:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è¿­ä»£å®‰è£…åŒä¸€åŒ…çš„ä¸åŒç‰ˆæœ¬ï¼Œæˆ‘ä»¬éœ€è¦åˆ©ç”¨ `os` åŒ…ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿä» Python ä»£ç ä¸­è¿è¡Œç»ˆç«¯å‘½ä»¤ï¼š
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `try/except` clause is in there because we donâ€™t know if these functions
    existed in earlier versions of the package. *Luckily, they all do*
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`try/except` è¯­å¥å­˜åœ¨æ˜¯å› ä¸ºæˆ‘ä»¬ä¸çŸ¥é“è¿™äº›å‡½æ•°æ˜¯å¦å­˜åœ¨äºåŒ…çš„æ—©æœŸç‰ˆæœ¬ä¸­ã€‚*å¹¸è¿çš„æ˜¯ï¼Œå®ƒä»¬éƒ½å­˜åœ¨*'
- en: '`import` statements within a loop looks wrong but is necessary since we need
    to re-import the functions once the correct package version has been installed'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨å¾ªç¯ä¸­çš„ `import` è¯­å¥çœ‹èµ·æ¥ä¸å¯¹ï¼Œä½†è¿™æ˜¯å¿…è¦çš„ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦åœ¨å®‰è£…æ­£ç¡®çš„åŒ…ç‰ˆæœ¬åé‡æ–°å¯¼å…¥è¿™äº›å‡½æ•°
- en: After iterating through each version, we find out that **downgrading tensorflow
    improves runtime by as much as 15%!**
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ç»è¿‡æ¯ä¸ªç‰ˆæœ¬çš„è¿­ä»£ï¼Œæˆ‘ä»¬å‘ç°**é™çº§ TensorFlow å¯ä»¥å°†è¿è¡Œæ—¶é—´æé«˜å¤šè¾¾ 15%ï¼**
- en: '![](../Images/cf122c9f782c3b77ee13f09e5fb0715e.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf122c9f782c3b77ee13f09e5fb0715e.png)'
- en: My pet theory as to why this is the case is because newer versions of tensorflow
    are built assuming heavy GPU usage, which means it is optimized for this particular
    use case at the cost of local CPU performance.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¸ªäººçš„ç†è®ºæ˜¯ï¼Œä¹‹æ‰€ä»¥å‡ºç°è¿™ç§æƒ…å†µï¼Œæ˜¯å› ä¸ºè¾ƒæ–°çš„ TensorFlow ç‰ˆæœ¬å‡å®šé‡åº¦ä½¿ç”¨ GPUï¼Œè¿™æ„å‘³ç€å®ƒé’ˆå¯¹è¿™ç§ç‰¹å®šçš„ä½¿ç”¨åœºæ™¯è¿›è¡Œäº†ä¼˜åŒ–ï¼Œä½†ç‰ºç‰²äº†æœ¬åœ°
    CPU æ€§èƒ½ã€‚
- en: '*If anyone has the real answer as to why older versions of TF run faster, please
    let me know!*'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¦‚æœæœ‰äººçŸ¥é“ä¸ºä»€ä¹ˆæ—§ç‰ˆæœ¬çš„ TensorFlow è¿è¡Œæ›´å¿«çš„çœŸå®åŸå› ï¼Œè¯·å‘Šè¯‰æˆ‘ï¼*'
- en: Conclusion & Putting it all Together
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®ºä¸æ€»ç»“
- en: 'With the following insights about Tensorflow runtime:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äº TensorFlow è¿è¡Œæ—¶çš„ä»¥ä¸‹è§è§£ï¼š
- en: '**The optimal prediction batch-size is about 1,000**'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æœ€ä½³é¢„æµ‹æ‰¹é‡å¤§å°çº¦ä¸º 1,000**'
- en: '**Tokenizer parameters do play a large role in prediction time**'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åˆ†è¯å™¨å‚æ•°ç¡®å®åœ¨é¢„æµ‹æ—¶é—´ä¸­èµ·ç€é‡è¦ä½œç”¨**'
- en: '**Tensorflow 1.X.X has a 15% boost in prediction time**'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorFlow 1.X.X åœ¨é¢„æµ‹æ—¶é—´ä¸Šæå‡äº† 15%**'
- en: 'We can put all these together, and see how it performs against our original
    batch-size experiment:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å°†è¿™äº›ä¿¡æ¯ç»¼åˆèµ·æ¥ï¼Œçœ‹çœ‹å®ƒä¸æˆ‘ä»¬æœ€åˆçš„æ‰¹é‡å¤§å°å®éªŒçš„è¡¨ç°å¦‚ä½•ï¼š
- en: '![](../Images/a80018713ea66fb7bd6bc271eaa93300.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a80018713ea66fb7bd6bc271eaa93300.png)'
- en: In the largest case tested, our optimal run beats the Batch(100) by 20% and
    Single by 57%!
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åœ¨æµ‹è¯•çš„æœ€å¤§æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬çš„æœ€ä½³è¿è¡Œæ¯” Batch(100) å¿« 20%ï¼Œæ¯” Single å¿« 57%!
- en: Overall, this exercise was a simple and enjoyable expression of what it means
    to be a Data Scientist. You need to identify a problem, establish a hypothesis,
    develop a rigourous test, and analyze your results. In this case, it was my Tensorflow
    runtime. In the future, Iâ€™m sure you will find perplexing data/issues/problems
    in your own work.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»çš„æ¥è¯´ï¼Œè¿™ä¸ªè¿‡ç¨‹æ˜¯å¯¹æ•°æ®ç§‘å­¦å®¶èº«ä»½çš„ä¸€ç§ç®€å•è€Œæ„‰å¿«çš„è¡¨è¾¾ã€‚ä½ éœ€è¦è¯†åˆ«é—®é¢˜ï¼Œå»ºç«‹å‡è®¾ï¼Œåˆ¶å®šä¸¥æ ¼çš„æµ‹è¯•ï¼Œå¹¶åˆ†æç»“æœã€‚åœ¨è¿™ä¸ªæ¡ˆä¾‹ä¸­ï¼Œå°±æ˜¯æˆ‘çš„ TensorFlow
    è¿è¡Œæ—¶ã€‚å°†æ¥ï¼Œæˆ‘ç›¸ä¿¡ä½ ä¼šåœ¨è‡ªå·±çš„å·¥ä½œä¸­å‘ç°ä»¤äººå›°æƒ‘çš„æ•°æ®/é—®é¢˜/éš¾é¢˜ã€‚
- en: And next time, I hope rather than check Stack Overflow and give up if the answer
    isnâ€™t there, you roll up your sleeves and explore the problem space yourself.
    You never know what you might learn ğŸ’¡
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹æ¬¡ï¼Œå¸Œæœ›ä½ ä¸è¦ä»…ä»…æ˜¯æŸ¥çœ‹ Stack Overflowï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°ç­”æ¡ˆå°±æ”¾å¼ƒï¼Œè€Œæ˜¯å·èµ·è¢–å­è‡ªå·±æ¢ç´¢é—®é¢˜ç©ºé—´ã€‚ä½ æ°¸è¿œä¸çŸ¥é“ä½ å¯èƒ½ä¼šå­¦åˆ°ä»€ä¹ˆ ğŸ’¡
- en: Hope this was helpful in debugging your Tensorflow prediction time issues! ğŸ‰
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: å¸Œæœ›è¿™å¯¹è°ƒè¯•ä½ çš„ TensorFlow é¢„æµ‹æ—¶é—´é—®é¢˜æœ‰æ‰€å¸®åŠ©ï¼ ğŸ‰
- en: '*All images, unless otherwise noted, are by the author*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ‰€æœ‰å›¾åƒï¼Œé™¤éå¦æœ‰è¯´æ˜ï¼Œå‡ç”±ä½œè€…æä¾›*'
