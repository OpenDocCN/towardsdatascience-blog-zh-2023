- en: '4 Ways to Write Data To Parquet With Python: A Comparison'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/4-ways-to-write-data-to-parquet-with-python-a-comparison-3c4f54ee5fec](https://towardsdatascience.com/4-ways-to-write-data-to-parquet-with-python-a-comparison-3c4f54ee5fec)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learn How To Efficiently Write Data To Parquet Format Using Pandas, FastParquet,
    PyArrow or PySpark.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://anbento4.medium.com/?source=post_page-----3c4f54ee5fec--------------------------------)[![Antonello
    Benedetto](../Images/bf802bb46dce03dd3bd4e17c1dffe5b7.png)](https://anbento4.medium.com/?source=post_page-----3c4f54ee5fec--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3c4f54ee5fec--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3c4f54ee5fec--------------------------------)
    [Antonello Benedetto](https://anbento4.medium.com/?source=post_page-----3c4f54ee5fec--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3c4f54ee5fec--------------------------------)
    ·7 min read·Mar 13, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/39041e5a101311da2fe0002dd298f589.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Photo by Dominika Roseclay](https://www.pexels.com/photo/background-of-pile-of-firewood-with-rough-surface-4318196/)'
  prefs: []
  type: TYPE_NORMAL
- en: On-Demand Courses | Recommended
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*A few of my readers have contacted me asking for on-demand courses to learn
    more about* ***Data Engineering with Python & PySpark****. These are 3 great resources
    I would recommend:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[**Data Streaming With Apache Kafka & Apache Spark Nano-Degree (UDACITY)**](https://imp.i115008.net/zaX10r)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Data Engineering Nano-Degree (UDACITY)**](https://imp.i115008.net/zaX10r)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**Spark And Python For Big Data With PySpark (UDEMY)**](https://click.linksynergy.com/deeplink?id=533LxfDBSaM&mid=39197&murl=https%3A%2F%2Fwww.udemy.com%2Fcourse%2Fspark-and-python-for-big-data-with-pyspark%2F)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Not a Medium member yet? Consider signing up with my* [*referral link*](https://anbento4.medium.com/membership)
    *to gain access to everything Medium has to offer for as little as $5 a month!*'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In today’s data-driven world, efficient storage and processing of large datasets
    is a crucial requirement for many businesses and organisations. This is where
    the Parquet file format comes into play.
  prefs: []
  type: TYPE_NORMAL
- en: '[Parquet is a **columnar storage format** that is designed to optimise data
    processing and querying performance while minimising storage space.](https://www.databricks.com/glossary/what-is-parquet)'
  prefs: []
  type: TYPE_NORMAL
- en: It is particularly well-suited for use cases where data needs to be analysed
    quickly and efficiently, such as in data warehousing, big data analytics, and
    machine learning applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this article, I will demonstrate how to write data to Parquet files in Python
    using four different libraries: [Pandas](https://pandas.pydata.org/docs/), [FastParquet](https://fastparquet.readthedocs.io/en/latest/),
    [PyArrow,](https://arrow.apache.org/docs/python/index.html) and [PySpark](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/index.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, you will learn how to:'
  prefs: []
  type: TYPE_NORMAL
- en: '**retrieve data from a database, convert it to a DataFrame, and use each one
    of these libraries to write records to a Parquet file.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**write data to Parquet files in batches, to optimise performance and memory
    usage.**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this article, you’ll have a thorough understanding of how to use
    Python to write Parquet files and unlock the full power of this efficient storage
    format.
  prefs: []
  type: TYPE_NORMAL
- en: The Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dataset used as part of this tutorial, includes mock data about daily account
    balances in different currencies and for different companies.
  prefs: []
  type: TYPE_NORMAL
- en: Data has been generated using a Python recursive function and then inserted
    into a SnowFlake DB table.
  prefs: []
  type: TYPE_NORMAL
- en: Then, a connection to the DB has been established using either the Python `snowflake.connector`
    or the native PySpark connectivity tools (*paired with jars*), to retrieve the
    dataset and convert it to DF format.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code to achieve the steps above is available [here](https://gist.github.com/anbento0490/918b9319109b1860b5b7c3878ba08896),
    whereas the first few rows of the DF, are displayed below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d5074981abd7092017a3dccf3ffeb6ed.png)'
  prefs: []
  type: TYPE_IMG
- en: Mock data generated by the author, fetched and converted to DF.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now describe four different strategies to write this dataset to parquet
    format using Python.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring 4 Methods To Write To Parquet Files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you can see, the common starting point to all methods, is having data already
    converted to either a Pandas or PySpark DF.
  prefs: []
  type: TYPE_NORMAL
- en: This will make the code much more concise, readable and your life easier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Method # 1: Using Plain Pandas'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Probably the simplest way to write dataset to parquet files, is by using the
    `to_parquet()` method in the `pandas` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this case, `engine = 'pyarrow'` has been used as [PyArrow](https://arrow.apache.org/docs/python/index.html)
    is a high-performance Python library for working with Apache Arrow data. It provides
    a fast and memory-efficient implementation of the Parquet file format, which can
    improve the write performance of parquet files.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, PyArrow supports a range of compression algorithms, including `gzip`,
    `snappy` and `LZ4`.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this tutorial, let’s pretend that the goal was to achieve high compression
    ratios (*that deliver smaller Parquet files in size*), even at the cost of slower
    compression and decompression speeds. This is why `compression = 'gzip'` has been
    used.
  prefs: []
  type: TYPE_NORMAL
- en: '**Method # 2: Using Pandas & FastParquet**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another popular way to write datasets to parquet files is by using the `fastparquet`
    package. In the simplest form, its `write()` method accepts a `pandas` DF as an
    input dataset and can compress it using variety of algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'But then, if `fastparquet` works with `pandas` DFs anyway, why shouldn’t **Method
    #1** be used instead?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, there are at least a couple of reasons to use `fastparquet` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '*it is designed to be a lightweight package, known for its fast write performance
    and efficient memory usage.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*it provides a pure Python implementation of the parquet format and then offers
    much more flexibility and options to developers.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For instance, by using the `fp.write()` method, you can specify the option `append
    = True` , something that is not yet possible through the `to_parquet()` method.
  prefs: []
  type: TYPE_NORMAL
- en: This option becomes particularly handy when the source dataset is too large
    to be written in memory in one go, so that, to avoid `OOM` errors you decide to
    write it to the `parquet` files in batches.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code below, is a valid example of how to write large datasets to a `parquet`
    file, in batches, by combining the power of `pandas` and `fastparquet`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The strategy used in the code above is to:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a `batch_size`: in this case of this tutorial it has been set to only
    `250` rows but in production it can easily be increased to *several million rows*,
    depending on memory available on the worker that will perform the job.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fetch the very first batch of rows from the database, using `fetchmany()` instead
    of `fetchall()` and use this dataset to create a `pandas` DF. Write such DF to
    a parquet file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instantiate a `WHILE loop` that will keep fetching data from the DB in batches,
    converting it to `pandans` DFs and eventually append it to the initial `parquet`
    file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `WHILE loop` will keep running until the last row has been written to the
    parquet file. Since the `batch_size` can be a variable updated depending on the
    use case, this should be considered a much more *“controlled”* and memory efficient
    method to write to a files with python.
  prefs: []
  type: TYPE_NORMAL
- en: 'Method # 3: Using Pandas & PyArrow'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier in the tutorial, it has been mentioned that `pyarrow` is an high performance
    Python library that also provides a fast and memory efficient implementation of
    the `parquet` format.
  prefs: []
  type: TYPE_NORMAL
- en: 'Its power can be used indirectly (by setting `engine = ''pyarrow''` like in
    **Method #1**) or directly by using some of its native methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, you could easily replicate the code that writes to a `parquet`
    file in batches (**Method # 2.2**) , by using the `ParquetWriter()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note that, since behind the scenes `pyarrow` takes advantage of the Apache Arrow
    format, the `ParquetWriter` requires a `pyarrow` schema as an argument (*which
    datatypes are fairly intuitive and somewhat similar to their* `pandas` *counterpart*).
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, while using this package, you are not allowed to write `pandas` DF
    directly, but those should be converted to a `pyarrow.Table` first (using the
    `from_pandas()` method), before the preferred dataset can be written to a file
    with the `write_table()` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite **Method #3** is a bit more verbose compared to the others, the Apache
    Arrow format is particularly recommended if declaring a schema and the availability
    of columns statistics is paramount for your use-case.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Method #4: Using PySpark'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last and probably most flexible way to write to a `parquet` file, is by
    using a `pyspark` native `df.write.parquet()` method.
  prefs: []
  type: TYPE_NORMAL
- en: Of course the script below, assumes that you are connected to a DB and managed
    to load data into a DF, as shown [here](https://gist.github.com/anbento0490/918b9319109b1860b5b7c3878ba08896).
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that in this case `mode(''overwrite'')` has been used, but you could easily
    switch it to `mode(''append'')` in case you wished to write data in batches. Also
    `pyspark` allows you to specify a large number of options among which the preferred
    `''compression''` algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Conclusion**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this article, we have explored four different Python libraries that allow
    you to write data to Parquet files, including *Pandas*, *FastParquet*, *PyArrow,*
    and *PySpark*.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, the Parquet file format is an essential tool for businesses and organisations
    that need to process and analyse large datasets quickly and efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: You have also learned how to write data in batches, which can further optimise
    performance and memory usage. By mastering these skills, you will be able to leverage
    the full power of Parquet and take your data processing and analysis to the next
    level.
  prefs: []
  type: TYPE_NORMAL
- en: '**Sources**'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[What Is Parquet? | Databricks Official Documentation](https://www.databricks.com/glossary/what-is-parquet)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Apache Arrow Official Documentation](https://arrow.apache.org/#:~:text=Format,data%20access%20without%20serialization%20overhead.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[File is too big? Make it chunks — By BlueBirz](https://www.bluebirz.net/en/make-it-chunks/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
