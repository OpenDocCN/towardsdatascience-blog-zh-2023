- en: How Decision Trees Split Nodes, from Loss Function Perspective
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-decision-trees-split-nodes-from-loss-function-perspective-60a2f2124b4e](https://towardsdatascience.com/how-decision-trees-split-nodes-from-loss-function-perspective-60a2f2124b4e)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Learn how a decision tree splits nodes only to minimize its loss function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://jasonweiyi.medium.com/?source=post_page-----60a2f2124b4e--------------------------------)[![Wei
    Yi](../Images/24b7a438912082519f24d18e11ac9638.png)](https://jasonweiyi.medium.com/?source=post_page-----60a2f2124b4e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----60a2f2124b4e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----60a2f2124b4e--------------------------------)
    [Wei Yi](https://jasonweiyi.medium.com/?source=post_page-----60a2f2124b4e--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----60a2f2124b4e--------------------------------)
    ·12 min read·May 15, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/de317baa764ffb421a758d3453dbd4f5.png)'
  prefs: []
  type: TYPE_IMG
- en: Photo by [Ernest Brillo](https://unsplash.com/@ernest_brillo?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  prefs: []
  type: TYPE_NORMAL
- en: When talking about decision tree node splitting, I often hear phrases such as
    “variance reduction” and “information gain maximization”. And I’m always terrified
    by them. These are not something I am comfortable with using my every day vocabulary,
    until I realized that these phrases are synonyms for “minimizing the decision
    tree’s loss function”.
  prefs: []
  type: TYPE_NORMAL
- en: Which loss function? Well, every machine learning model needs a loss function.
    The loss function quantifies the *difference* between a model’s predictions and
    the actuals. An alternative term is the *error* in the model’s prediction. The
    decision tree model is of no exception.
  prefs: []
  type: TYPE_NORMAL
- en: To me, understanding the loss function is the most important step in understanding
    a machine learning model. This is because a loss function encodes everything we
    want our model to be good at using a single number. Just like we quantify how
    good a man is by a single number; I recall that is a length measure of some sort,
    right, wink, wink?
  prefs: []
  type: TYPE_NORMAL
- en: Since the loss function measures the difference between a tree’s predictions
    and the actuals, to understand the loss function, we first need to understand
    how a decision tree makes predictions. Let’s start with the regression tree, and
    then move on to the classification tree.
  prefs: []
  type: TYPE_NORMAL
- en: How a regression tree makes predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A regression decision tree makes prediction in the form of a continuous value.
    For example, a regression tree that predicts a person’s salary from his or her
    age and gender. Here is a little dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/649ffed19c626688ad7efad936a08972.png)'
  prefs: []
  type: TYPE_IMG
- en: Example dataset for regression tree, with Salary as target variable, by author
  prefs: []
  type: TYPE_NORMAL
- en: We want our regression tree to predict the *target variable* Salary, which is
    a continuous quantity, hence the name “regression tree” from two *features*, Age,
    which is continuous, and Gender, which is categorical. For Salary, I use *y₁*
    to *y₇* instead of real numbers to highlight the fact that we are not supposed
    to know other people’s salary. Let *sₒ={y₁, y₂, y₃, y₄, y₅, y₆, y₇}* represents
    all the salary data entries.
  prefs: []
  type: TYPE_NORMAL
- en: A regression tree uses some condition tuple (feature, value, comparison) to
    split the data into two parts. Note that decision trees are always doing binary
    splits.
  prefs: []
  type: TYPE_NORMAL
- en: The following tree uses the splitting condition tuple (Age, 41, <) to split
    the full dataset *s₀* into two subsets *s1={y₁, y₃, y₅}* and *s2={y₂, y₄, y₆,
    y₇}*. The tuple (Age, 41, <) means that the tree uses the condition “Age < 41”
    to split all the data points that reaches the current root, which is the full
    dataset at the moment, into two non-overlapping subsets.
  prefs: []
  type: TYPE_NORMAL
- en: I *visualised* this split by placing the subset *s1* on the left branch and
    *s2* on the right branch. Note the splitting condition tuple (Age, 41, <) is for
    illustration purpose, it may not be optimal for the example dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a99c3756d4a82ae0ad3f80d8e490c79d.png)'
  prefs: []
  type: TYPE_IMG
- en: Regression tree with splitting condition Age < 41, by author
  prefs: []
  type: TYPE_NORMAL
- en: Regression tree makes predictions by averaging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This little regression tree with one split, makes the salary prediction for
    a person data point in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If that person is younger than 41 years old, he reaches the left branch, i.e.
    the subset *s1*; the model then predicts the quantity *ŷₛ₁* which is the average
    of the salaries in the subset *s1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/13d5134b826e8011bec819644e396971.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If that person is older or equal than 41 years, he reaches the right branch,
    i.e., the subset *s2*; the model predicts the quantity *ŷₛ₂*, which is the average
    of the salaries in the subset *s2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/463e80026d9b07a4976f9b5108d7b5fa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In other words, for a data point, a regression tree makes prediction by:'
  prefs: []
  type: TYPE_NORMAL
- en: first letting the data point fall through the tree and reaches a leaf node
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: collecting the target variable values for the training data subset that are
    in that leaf node
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: using the average of those target values as the prediction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Loss function for regression trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since the regression tree’s prediction is a continuous quantity, the most obvious
    choice of a loss function that measures the error between this prediction and
    the actual is the quadratic loss, or, equivalently, the mean squared loss (MSE).
    Since there are two branches, we define their loss terms respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the subset *s1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d483275c0143a369d527432d6912afb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For the subset *s2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b36bcbd419fdd7e184553bb8647209b0.png)'
  prefs: []
  type: TYPE_IMG
- en: where *|s1|* denotes the size of the subset *s1*, that is, 3, and *|s2|* the
    size of the subset *s2*, that is 4\. The ⅓ and ¼ normalisation terms facts the
    size of the two subsets into the loss function. So *Lₛ₁* and *Lₛ₂* are averaged
    *per data point* loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'The overall loss for the whole tree is then the sum of these two loss terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0e620014efe55ab107c7839e2355233f.png)'
  prefs: []
  type: TYPE_IMG
- en: The fact that *Lₛ₁* and *Lₛ₂* are averaged per data point loss means we don’t
    need to put coefficients in front of them in the total loss *L* to account for
    the size difference in *s1* and *s2*. The subset size difference is already accounted
    for by the normalization constant ⅓ and ¼.
  prefs: []
  type: TYPE_NORMAL
- en: Finding splitting condition for a decision tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To learn a model from data, or equivalently, optimize a model, we change the
    model parameters such that the model’s predictions makes an error as small as
    possible. In other words, the model’s predictions should minimize the above loss
    function *L*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the regression tree, at each branching, the model parameter is the splitting
    condition tuple, such as (Age, 40, <). The algorithm to decide which tuple to
    split the tree involves the following three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Identify a suitable subset of splitting condition tuples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Looking at features in data and find candidate splitting conditions among all
    possible splitting conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: If a feature is continuous, such as Age, look at the range of this feature and
    decide on a short list of splitting values. For example, the Age in the data range
    from 20 to 70, then the short list of splitting values can be 30, 40, 50, 60\.
    This gives the following four tuples (Age, 30, <), (Age, 40, <), (Age, 50, <)
    and (Age, 60, <). For a continuous feature, there can be infinite number of splitting
    values, the algorithm can only afford to choose a *small subset* using heuristics.
    These subset may not be optimal, but it is a tradeoff between model precision
    and computation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a feature is categorical, such as Gender, use its values as splitting values.
    This gives the two tuples (Gender, Male, =), (Gender, Female, =), which is reduced
    to just one tuple (Gender, Male, =) for this binary feature.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So we have five splitting conditional tuples in total.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Partition data using splitting condition tuples and evaluate loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Partition the dataset according to each of the splitting condition tuple and
    evaluate the loss function *L*. In the example, we have five tuples, so we partition
    the data five times according to each tuple, evaluate the loss function five times,
    each time based on a different partition of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Choose the splitting condition tuple with smallest loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now the algorithm chooses the splitting condition tuple that yields the smallest
    loss and puts the two partitioned data subsets from this condition into the two
    leaf nodes created by the split.
  prefs: []
  type: TYPE_NORMAL
- en: It then treats each leaf node as the root of a new tree, and applies the same
    three steps for each new tree to do further splitting.
  prefs: []
  type: TYPE_NORMAL
- en: Quantifying model’s prediction improvement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: That is how a regression tree splits nodes — it splits nodes to minimize its
    loss function, which is the mean squared loss. After the split, the model can
    hopefully make better predictions in the sense that the predictions produce smaller
    errors. The model can do that because it extracted information from the dataset.
    To quantify how much information has been extracted before and after the split,
    calculate the loss over the full dataset s₀, which is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c02aedc33c091b54b6e8af3cf511bca9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then do a differencing between the two:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3e5c46e4f03eeff12b03379ec00d76e3.png)'
  prefs: []
  type: TYPE_IMG
- en: Variance reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You sometimes hear the phrase “variance reduction” in the discussion of regression
    tree splitting. This is because if we look at the tree’s prediction and the corresponding
    loss, shown here again for just the left branch:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/13d5134b826e8011bec819644e396971.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d483275c0143a369d527432d6912afb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You will realize that for a list of numbers *y₁, y₃, y₅*, the regression tree
    prediction *ŷₛ₁* is the mean of these numbers, which we usually denote as *ȳ*.
    And the mean squared error *Lₛ₁* has the same structure as the formula to compute
    the [variance](https://en.wikipedia.org/wiki/Variance) of this list of numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/eac424ac0616db88959295d9a3ee6236.png)'
  prefs: []
  type: TYPE_IMG
- en: where *n=3* in our case. So the above splitting algorithm minimizes the loss,
    or equivalently, reduces the variance. That’s why we say that regression tree
    splits nodes by reducing the variance.
  prefs: []
  type: TYPE_NORMAL
- en: Now move on to classification tree, which makes predictions in the form a a
    categorical value, not a continuous value. You know the drill. First we show how
    a classification tree makes predictions, then define the loss function, and finally
    see how the above splitting algorithm applies to classification trees the same
    way as to regression trees.
  prefs: []
  type: TYPE_NORMAL
- en: How a classification tree makes predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s first create a dataset for the classification task by swapping the Gender
    and Salary columns from the previous dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bc3f7344feeaed1084dfabcb3eaf7ecf.png)'
  prefs: []
  type: TYPE_IMG
- en: Example dataset for classification tree, with Gender as target variable, by
    author
  prefs: []
  type: TYPE_NORMAL
- en: Here, the target variable to predict is Gender, which is a categorical variable,
    so we need a classification tree. The two features are Age and Salary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we have the following classification tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/577a8fb1f2f3356b294d3122b0d7d017.png)'
  prefs: []
  type: TYPE_IMG
- en: Classification tree with split condition Age<41, by author
  prefs: []
  type: TYPE_NORMAL
- en: This tree needs to classify a data point into the Female class, or the Male
    class. Suppose this tree still use the condition Age < 41 to splits the dataset
    *s₀* into *s1={y₁F, y₃F, y₅M}* and *s2={y₂M, y₄F, y₆M, y₇F}*. I attached the gender
    symbol “F” or “M” to each data point, such as “*y1F”* to make the gender of each
    data point reaching a leaf node more clear.
  prefs: []
  type: TYPE_NORMAL
- en: So after the split, ⅔ of the data points arrived at the left leaf node is female,
    denoted by *P(F)=⅔*, which says the *occurrence probability* of female data points
    in this node is ⅔. Likewise, for male, it is *P(M)=⅓*.
  prefs: []
  type: TYPE_NORMAL
- en: Classification trees make predictions by majority vote
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need a prediction mechanism other than averaging (like in the case of regression
    tree) because ⅔ of a woman plus *⅓* of a man is not a thing, despite the active
    LGBT movement in recent years.
  prefs: []
  type: TYPE_NORMAL
- en: A better choice is *majority vote —* predicting the class with the highest occurrence
    probability. For the left node, since there are more female data points than male
    data points, the model will predict the female class.
  prefs: []
  type: TYPE_NORMAL
- en: What about the right leaf node, where the probability of female and male data
    points are equal, both are ½? In this case where the probabilities for the two
    classes are tied, the model can break the tied randomly, say, by predict the male
    class. In reality, the splitting algorithm discourages the tied situation to happen,
    as we will see later.
  prefs: []
  type: TYPE_NORMAL
- en: Loss function for classification trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s work on the loss function that quantifies the error that the tree makes.
    Intuitively, in a leaf node, if the occurrence probability of the major class
    is larger, the model’s prediction for that node is better:'
  prefs: []
  type: TYPE_NORMAL
- en: Our tree predicts Female for the left node, this prediction is right ⅔ of time,
    and wrong *⅓* of the time*.* In other words, if the distribution of different
    classes in a node is **less uniform**, the prediction is better.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It predicts Male for the right node, this prediction is correct only ½ of the
    time. In other words, if the distribution of different classes in a node is **more
    uniform**, the prediction is worse.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'So we need a measure for uniformity of the class labels in a set of data points.
    Many measures exist. A common choice is entropy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6da67e07df847c987a8bca41fbcdc20b.png)'
  prefs: []
  type: TYPE_IMG
- en: where the sum is over all the classes *c*, Female and Male in our case. *P(c)*
    is the occurrence probability for the class *c*. And *log(P(c))* is the log of
    that occurrence probability.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that entropy is one measure of uniformity; there are other common
    measures, such as Chi-Square and Gini Impurity. Recall that in economics, Gini
    index is used to quantify how wealth is unequally distributed among different
    classes of people in a society — it is a measure of un-uniformity.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to know *why* entropy can measure uniformity, please refer to other
    [resources](https://stats.stackexchange.com/questions/66935/measure-for-the-uniformity-of-a-distribution).
    In this article, it is sufficient to know that entropy measures uniformity. But
    just for fun, we can plug in some number to see whether entropy tells us that
    our left branch is more uniform the the right branch.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the left leaf node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c1831178dade93e44d71cee03b637f82.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For the right leaf node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9daf06f993310e002d1c3cadf860b354.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the loss for the left leaf node *Hₛ₁=0.637* is indeed smaller
    than the loss for the right leaf node *Hₛ*₂=0.693, conforming that the model’s
    prediction is better for data points in the left node.
  prefs: []
  type: TYPE_NORMAL
- en: Now, just as in the case of regression tree, we need to combine *Hₛ₁* and *Hₛ*₂
    together to represent the overall loss for the whole tree.
  prefs: []
  type: TYPE_NORMAL
- en: In the regression tree case, we added *Lₛ₁* and *Lₛ*₂ together because *Lₛ₁*
    and *Lₛ*₂ represents per data point loss, with the sizes of the different subset
    *s1* and *s2* normalized. But in classification tree case, this is not the case
    for the above entropy-based loss terms. If you look at the above definition of
    *Hₛ₁* and *Hₛ*₂, the size of the subset s1 and s2 is not mentioned.
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, when a model makes a prediction error for a large subset of data,
    even if that error is small, the impact to the overall model performance measure
    can be big because of the large size of the data subset.
  prefs: []
  type: TYPE_NORMAL
- en: 'To fact the data subset size in, we define the total loss H as the weighted
    sum of *Hₛ₁* and *Hₛ*₂, with weights belong the data subset proportions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5bac29b5ba8f09b9a75a22db5a77c1f7.png)'
  prefs: []
  type: TYPE_IMG
- en: Finding splitting condition for a classification tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the loss function *H* defined for a classification tree, we can apply the
    same algorithm used for regression tree to split nodes. We just need to use the
    new loss *H* for classification, instead of the loss *L* for regression.
  prefs: []
  type: TYPE_NORMAL
- en: This makes sense because the weighted entropy *H* represents the error that
    our classification tree makes — more uniform, larger error, less uniform, smaller
    error. The above splitting algorithm will chooses a split condition that minimizes
    this error, hence encourages data points reaching every leaf node to be less uniform.
    So the tree can make less mistakes. Also, by encouraging less uniform data subsets
    in split leaf nodes, it is unlikely that a tie between two classes will happen.
  prefs: []
  type: TYPE_NORMAL
- en: How amazing!
  prefs: []
  type: TYPE_NORMAL
- en: Quantifying model’s prediction improvement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just as the case in regression tree, you can measure the information extracted
    from data between and after the split by computing the loss on the root node *Hₛ₀*,
    which is before splitting, and do a differencing from *H*, that is *H-Hₛ₀*.
  prefs: []
  type: TYPE_NORMAL
- en: Information Gain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before I end, I want to mention information gain as you will see it everywhere
    when classification trees are in discussion.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/62a8cde34c07890200b246f66b772a3b.png)'
  prefs: []
  type: TYPE_IMG
- en: You can see information gain is merely the negation of the entropy. So instead
    of finding a split condition that minimizes the entropy loss, we can equivalently
    find a split condition that maximizes the information gain. The constant 1 in
    the information gain formula makes no difference in the splitting algorithm. If
    the algorithm finds a splitting condition that maximizes *-Entropy*, that same
    condition also maximizes information gain, which is *1-Entropy*.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This article explains how decision trees, both regression and classification
    trees, splits node from the loss function point of view. This view makes the notion
    such as reduction of variance, information gain, a lot more intuitive to understand.
  prefs: []
  type: TYPE_NORMAL
