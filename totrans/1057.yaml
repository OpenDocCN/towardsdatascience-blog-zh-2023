- en: How Can AI Make Old Videos Look Smoother?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://towardsdatascience.com/how-can-ai-make-old-videos-look-smoother-5ad8b70fdd64](https://towardsdatascience.com/how-can-ai-make-old-videos-look-smoother-5ad8b70fdd64)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Video frame interpolation has come a long way and AI is taking it in some strange
    directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[](https://mikhailklassen.medium.com/?source=post_page-----5ad8b70fdd64--------------------------------)[![Mikhail
    Klassen](../Images/9c4a6cc856fd4061f682e95a1c145c36.png)](https://mikhailklassen.medium.com/?source=post_page-----5ad8b70fdd64--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5ad8b70fdd64--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5ad8b70fdd64--------------------------------)
    [Mikhail Klassen](https://mikhailklassen.medium.com/?source=post_page-----5ad8b70fdd64--------------------------------)'
  prefs: []
  type: TYPE_NORMAL
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5ad8b70fdd64--------------------------------)
    ·7 min read·Jan 10, 2023
  prefs: []
  type: TYPE_NORMAL
- en: --
  prefs: []
  type: TYPE_NORMAL
- en: It was back in 2015 and I was creating some data visualizations for a meeting
    with my PhD supervisory committee. This was towards the end of my graduate studies
    in astrophysics and I had run some supercomputer simulations of a massive star
    forming inside an interstellar cloud of gas.
  prefs: []
  type: TYPE_NORMAL
- en: 'I figured the best way to show off the results of these simulations was to
    create various animations of the data. My analysis code had chewed through gigabytes
    of simulation data and dumped image files at specific time intervals. So I used
    `ffmpeg` to stitch them together into an animation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/36d672e41b3f0e33b72b15a2cb91d823.png)'
  prefs: []
  type: TYPE_IMG
- en: Video of an astrophysics simulation showing the evolution of a protostellar
    disk around a forming star. Author’s own work. See [Klassen et al. (2016)](https://iopscience.iop.org/article/10.3847/0004-637X/823/1/28/meta).
  prefs: []
  type: TYPE_NORMAL
- en: I always wanted higher frame rates, so I could have smoother and more beautiful
    animation, but I was limited by the data I had from my simulation.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes I would spend a few hours seeing if I could perhaps generate intermediate
    frames to make my movies *appear* smoother when I compiled them. My attempts at
    the time were clumsy and ultimately abandoned, but today there are many techniques
    that work very well. In fact, most modern TVs include the option (on by default)
    to artificially enhance the frame rate of the video.
  prefs: []
  type: TYPE_NORMAL
- en: The formal name for this is **video frame interpolation** (VFI) and it turns
    out there’s quite a body of research in this domain.
  prefs: []
  type: TYPE_NORMAL
- en: What is video frame interpolation?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Video footage is typically assembled from individual still frames. If the frame
    rate is low, the video can appear “choppy”. To make the video appear smoother,
    various techniques have been developed for artificially inserting intermediate
    frames between the original frames of the video.
  prefs: []
  type: TYPE_NORMAL
- en: In the example below, a short anime clip has had its frame rate upscaled 8 times
    using an AI-based technique. The result is a much smoother video.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore a few of these techniques. We’ll start with some approaches that
    don’t require artificial intelligence and end with some of the state-of-the-art
    approaches.
  prefs: []
  type: TYPE_NORMAL
- en: As our source materials, we’ll use a 6-second clip of a train locomotive by
    the filmmaker [Pat Whelen](https://www.pexels.com/@pat-whelen-2913248/), uploaded
    to the free stock photo and video site [Pexels](https://www.pexels.com/video/wheels-and-connecting-rods-of-a-moving-train-5708282/).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/cdcd1f7c859b4108d98f26d0fdb9d5ae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Original 23 fps clip of a train locomotive. Credit: [Pat Whelen, Pexels](https://www.pexels.com/video/wheels-and-connecting-rods-of-a-moving-train-5708282/).
    Free to use.'
  prefs: []
  type: TYPE_NORMAL
- en: The original video is recorded in high resolution at a framerate of 23 frames
    per second. In general, we will aim to double the framerate during our interpolations.
  prefs: []
  type: TYPE_NORMAL
- en: Frame interpolation techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Frame averaging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the simplest ways of adding more frames is to blend two sequential frames
    to create an average of the two. This is easy to do. I used `ffmpeg` to perform
    the interpolation using the `minterpolate` filter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: More details on the filter settings can be [found here](https://ffmpeg.org/ffmpeg-filters.html#minterpolate).
    The result is below, converted to an animated gif.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6081dc36e68a7e4548fcd8201c819ecb.png)'
  prefs: []
  type: TYPE_IMG
- en: Source video frame rate enhanced using video frame blending.
  prefs: []
  type: TYPE_NORMAL
- en: If you look carefully, you can see from the resulting video that artifacts appear
    wherever there is motion. If an object moves too much between frames, then the
    blended intermediate frame very obviously shows the object in both places and
    appearing semi-transparent where the motion is.
  prefs: []
  type: TYPE_NORMAL
- en: To overcome this problem, researchers developed motion estimation techniques,
    which improves the quality of the interpolation.
  prefs: []
  type: TYPE_NORMAL
- en: Motion estimation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To create intermediate frames that don’t look like just the blurred average
    of the two other frames, motion-compensated frame interpolation was developed.
    This approach works by identifying objects in sequential frames from similar visual
    features and calculating the path taken by those objects. This creates a vector
    “motion field” that an interpolation algorithm can exploit to improve the quality
    of the intermediate frames.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/704d3387f1cb81bf74bf1002e0f5d3f7.png)'
  prefs: []
  type: TYPE_IMG
- en: A figure from [Choi & Ko (2010)](https://www.researchgate.net/publication/224145279_Hierarchical_motion_estimation_algorithm_using_reliable_motion_adoption),
    showing the motion field overlaid on a video frame.
  prefs: []
  type: TYPE_NORMAL
- en: 'Several motion estimation algorithms have been implemented into `ffmpeg` over
    the years, and so we’ll use one called “overlapped block motion compensation”
    (OBMC). Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0cee5b25f83f344bdb122cc98829d9ba.png)'
  prefs: []
  type: TYPE_IMG
- en: Source video frame rate enhanced using a motion interpolation technique called
    overlapped block motion compensation.
  prefs: []
  type: TYPE_NORMAL
- en: 'While the quality of the resulting video is much better, it’s still possible
    to get minor artifacts in the video. To achieve the above video, I again relied
    on the `minterpolate` filter in `ffmpeg`, but with a few different settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Frame interpolation using artificial intelligence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The area of computer vision has been making rapid progress with the application
    of new artificial intelligence techniques. Deep neural networks can perform highly
    accurate image segmentation, object detection, and even 3D depth estimation.
  prefs: []
  type: TYPE_NORMAL
- en: The previous technique relied on estimating the motion of objects between frames.
    These objects were detected using visual features. Convolutional neural networks
    (CNNs) are great extracting visual features from an image, so it seems reasonable
    to apply them here.
  prefs: []
  type: TYPE_NORMAL
- en: Next, if we could know a little more about the 3D nature of the world represented
    by the image, then we could better estimate the movement of different parts of
    the image and produce more accurate intermediate frames.
  prefs: []
  type: TYPE_NORMAL
- en: The real game-changer here is adding depth estimation. A neural network trained
    to predict the distance to parts of an image achieves this.
  prefs: []
  type: TYPE_NORMAL
- en: The animation below shows the results from a paper titled “[Digging Into Self-Supervised
    Monocular Depth Estimation](https://arxiv.org/abs/1806.01260)” (ICCV 2019, [Code](https://github.com/nianticlabs/monodepth2)).
    A deep neural network accurately estimates, frame by frame, which parts of the
    image represent objects that are closer vs further away.
  prefs: []
  type: TYPE_NORMAL
- en: That this technique works well shouldn’t come as too much of a surprise. This
    is something that our brains do intuitively, even with when taking away parallax
    information by closing one eye. Using both eyes improves the accuracy even further.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/18dc85e2e2a0d3cfa861590a7793ff1a.png)'
  prefs: []
  type: TYPE_IMG
- en: Depth-aware video frame interpolation techniques, such as the [DAIN algorithm](https://arxiv.org/abs/1904.00830)
    introduced in 2019 by Bao et al., perform better than previous approaches because
    they can account for occlusion, i.e. when objects pass behind each other.
  prefs: []
  type: TYPE_NORMAL
- en: More recently, the “[Real-time Intermediate Flow Estimation](https://arxiv.org/abs/2011.06294)”
    (RIFE) technique by Huang et al. (ECCV 2022) achieved superior benchmark scores
    while also being much faster to run. It probably represents the current state
    of the art at this time of writing.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/774419b7400a15f0ad68f7c20ee8a91b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'An example video clip from the RIFE paper showing 16x interpolation of 2 input
    images: [https://github.com/megvii-research/ECCV2022-RIFE](https://github.com/megvii-research/ECCV2022-RIFE)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4ae318fc18734ebd7074fe11cd537cd1.png)'
  prefs: []
  type: TYPE_IMG
- en: The depth estimation performed by RIFE from the same input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s conclude our exercise by applying the RIFE algorithm to our New York
    City scene. I clone the [GitHub repository](https://github.com/megvii-research/ECCV2022-RIFE)
    to my laptop, downloaded the pretrained model parameters as described in the repo’s
    ReadMe file, and made a few tweaks to the `requirements.txt` file. I ran the code
    on my laptop without the help of a GPU. To process 6 seconds of video footage
    took about 15minutes. Here are the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ed1785fa4a2dec0692e6858fce783966.png)'
  prefs: []
  type: TYPE_IMG
- en: Video interpolation using a novel deep CNN technique, “real-time intermediate
    flow estimation” (RIFE).
  prefs: []
  type: TYPE_NORMAL
- en: It looks even better in the higher-resolution mp4 video that RIFE created. For
    this article, I converted that video to an animated gif.
  prefs: []
  type: TYPE_NORMAL
- en: Final thoughts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This field is moving very quickly. Image generation AIs trained on billions
    of images from the internet can already produce highly consistent, novel imagery
    from a sequence of input images or text-based prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other techniques extend depth estimation approaches to create point clouds
    from multiple camera angles and then synthesize new artificial perspectives using
    a technique called neural rendering. See, for example, [Rückert et al. (2021)](https://arxiv.org/abs/2110.06635)
    and their neural rendering approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d5649da822126e79774534e8c2ec56e8.png)'
  prefs: []
  type: TYPE_IMG
- en: Neural rendering in [Rückert et al. (2021)](https://arxiv.org/abs/2110.06635).
    Images on the left are generated from a sequence of ground-truth (GT) input images
    (bottom right).
  prefs: []
  type: TYPE_NORMAL
- en: As the field continues to advance, we can expect greater accuracy of the intermediate
    frames with fewer artifacts, even as more and more of the frames are essentially
    “dreamt up” or hallucinated by very large pre-trained artificial neural networks.
    Relative few input frames are needed to produce highly believable, smooth transitions
    between them
  prefs: []
  type: TYPE_NORMAL
- en: This opens up a lot of creative possibilities, such as the restoration of old
    archival videos, animating still images, and creating virtual reality worlds from
    a handful of 2D photographs.
  prefs: []
  type: TYPE_NORMAL
- en: It may also lead to the flooding of the world with so much synthetic imagery
    that it becomes difficult to distinguish what represents ground truth. In the
    meantime, expect your videos to look really smooth.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c3c4a41ab7c9954339fd12974668fbfe.png)'
  prefs: []
  type: TYPE_IMG
- en: My original research data visualization (see above), passed through the RIFE
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: If you enjoy reading stories like these and want to support me as a writer,
    consider signing up to become a Medium member. It’s $5 a month, which gives you
    access to all my writing and that of thousands of other writers. If you sign up
    using [my link](https://mikhailklassen.medium.com/membership), I’ll earn a small
    commission with no extra cost to you.
  prefs: []
  type: TYPE_NORMAL
- en: '[](https://mikhailklassen.medium.com/membership?source=post_page-----5ad8b70fdd64--------------------------------)
    [## Join Medium with my referral link — Mikhail Klassen'
  prefs: []
  type: TYPE_NORMAL
- en: Read every story from Mikhail Klassen (and thousands of other writers on Medium).
    Your membership fee directly supports…
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: mikhailklassen.medium.com](https://mikhailklassen.medium.com/membership?source=post_page-----5ad8b70fdd64--------------------------------)
  prefs: []
  type: TYPE_NORMAL
