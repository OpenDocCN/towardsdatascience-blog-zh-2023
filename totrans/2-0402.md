# 提升 Spark 联合运算符性能：改进查询速度的优化技巧

> 原文：[https://towardsdatascience.com/boosting-spark-union-operator-performance-optimization-tips-for-improved-query-speed-9123ae6ada80](https://towardsdatascience.com/boosting-spark-union-operator-performance-optimization-tips-for-improved-query-speed-9123ae6ada80)

## 解密 Spark 联合运算符的性能

[](https://chengzhizhao.medium.com/?source=post_page-----9123ae6ada80--------------------------------)[![Chengzhi Zhao](../Images/186bba91822dbcc0f926426e56faf543.png)](https://chengzhizhao.medium.com/?source=post_page-----9123ae6ada80--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9123ae6ada80--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9123ae6ada80--------------------------------) [Chengzhi Zhao](https://chengzhizhao.medium.com/?source=post_page-----9123ae6ada80--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9123ae6ada80--------------------------------) ·阅读时间6分钟·2023年4月20日

--

![](../Images/bd00c06827eb22a6b64bcc2112e3d7d8.png)

图片由 [Fahrul Azmi](https://unsplash.com/@fahrulazmi?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 提供，来源于 [Unsplash](https://unsplash.com/photos/zN4mtLHkHn4?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)

联合运算符是将两个输入数据框合并为一个的集合运算符之一。联合操作在 Apache Spark 中是一个方便的操作，用于合并具有相同列顺序的行。一个常见的用例是应用不同的转换，然后将它们联合在一起。

在 Spark 中使用联合操作的方法常常被广泛讨论。然而，一个较少被讨论的隐藏事实是与联合运算符相关的性能陷阱。如果我们不理解 Spark 中联合运算符的陷阱，可能会陷入将执行时间翻倍的陷阱。

我们将重点讲解 Apache Spark DataFrame 的联合运算符，提供示例，展示物理查询计划，并分享优化技巧。

# Spark 中的 Union Operator 101

像关系型数据库（RDBMS）SQL 一样，联合是一种直接合并行的方式。处理联合运算符时要注意的一点是确保行遵循相同的结构：

+   **列的数量应该相同**。当数据框的列数量不同，联合操作不会静默地工作或用 NULL 填充。

+   **列的数据类型应匹配，并按位置解析列**。列名在每个数据框中应遵循相同的顺序。然而，这不是强制性的。第一个数据框将被选择为列名的默认值。因此，混合顺序可能会导致意外结果。Spark的`unionByName`旨在解决这个问题。

在Spark中，操作`unionAll`是`union`的别名，不会去除重复项。我们需要在执行联合后添加distinct，以进行无重复项的SQL类似联合操作。

我们也可以将多个数据框合并成一个最终的数据框。

[PRE0]

# 联合操作符的性能瓶颈

使用联合操作符的一个典型模式是将单个数据框拆分为多个数据框，然后应用不同的转换，最后将它们合并成最终的数据框。

这里有一个示例：我们有两个需要连接的大表（事实表），最好的连接方式是在Spark中使用SortMerged连接。一旦得到SortMerged数据框，我们将其拆分成四个子集。每个子集使用不同的转换，最终将这四个数据框合并成一个最终的数据框。

![](../Images/aedfcc3042c197c4580de6e8324241f9.png)

Spark中的联合操作 | 图片由作者提供

Spark数据框利用Catalyst优化器，它会对你编写的数据框代码进行代码分析、逻辑优化、物理规划和代码生成。Catalyst尝试创建一个执行Spark作业的**最优**计划。

近年来，Spark在Catalyst上进行了大量优化，以提升Spark连接操作的性能。连接操作的应用场景比联合操作更多，因此对联合操作的优化投入较少。

如果用户不在完全不同的数据源上使用联合操作，联合操作符将面临潜在的性能瓶颈——Catalyst并不“聪明”到能识别共享的数据框以进行重用。

在这种情况下，Spark会将每个数据框视为独立的分支，然后从根节点多次执行所有操作。在我们的示例中，我们将对两个大表进行四次连接！这是一个巨大的瓶颈。

# 在Spark中设置一个使用联合操作符的示例

在Spark中重现一个未优化的物理查询计划对于联合操作符来说非常简单。我们将执行以下操作

1.  创建两个数据框，从1到1000000。我们称它们为`df1`和`df2`

1.  对`df1`和`df2`进行内连接

1.  将连接结果拆分成两个数据框：一个仅包含奇数，另一个包含偶数。

1.  添加一个名为`magic_value`的转换字段，该字段由两个虚拟转换生成。

1.  将奇数和偶数数据框进行联合

[PRE1]

这是DAG的高级视图。如果从底部向上查看，显著的一个点是连接操作发生了两次，而且上游几乎看起来一模一样。

我们已经看到Spark需要广泛优化联合操作符，如果数据源可以重用，大量时间将被浪费在不必要的重新计算上。

![](../Images/add8c79c2672760e195efa5889b57e5f.png)

未优化查询计划的DAG | 图片由作者提供

这是一个物理计划，其中有50个阶段，启用了AQE。我们可以看到id 13和27。Spark确实在每个分支上执行了两次连接并重新计算了它的分支。

![](../Images/f22a40b040403b4d2deb7056b6503f78.png)

未优化的联合操作物理查询计划 | 图片由作者提供

# 如何提高联合操作的性能

现在我们可以看到这个潜在的瓶颈。我们该如何解决这个问题？一个选项是将执行器的数量翻倍，以运行更多的并发任务。但有一个更好的方法是提示Catalyst并让它重用内存中的连接数据框。

为了解决Spark联合操作的性能问题，**我们可以显式调用** `**cache**` **来将连接的数据框保留在内存中。** 这样Catalyst就知道获取数据的快捷方式，而不是返回数据源。

应该在哪里添加 `cache()`？推荐的位置是在过滤之前和连接完成之后的数据框。

让我们看看它的实际效果：

[PRE2]

这是查询计划：**InMemoryTableScan** 存在，因此我们可以重用数据框以节省其他计算。

![](../Images/321a65c28576e4c8594712fac1b6d0ba.png)

优化查询计划的DAG | 图片由作者提供

现在物理计划减少到仅32个阶段，如果我们检查id 1和15都利用了**InMemoryTableScan。** 如果我们将原始数据框分割成较小的数据集，然后将它们联合，这可以节省更多时间。

![](../Images/95230850a6a7f3d0904d3f591ab9bb8a.png)

优化的联合操作物理查询计划 | 图片由作者提供

# 最终思考

我希望这个故事能够提供一些见解，解释为什么有时联合操作会成为Spark性能的瓶颈。由于Catalyst对Spark中联合操作符缺乏优化，用户需要了解这些警示，以更有效地开发Spark代码。

添加缓存可以节省我们例子中的时间，但如果联合操作是在两个完全不同的数据源上进行，并且没有共享的地方来执行缓存，这将无济于事。

石崎和明的演讲激发了这个故事——[告别Spark SQL中的联合地狱](https://www.youtube.com/watch?v=c25eT-2dwAg)，以及我在项目中处理类似问题的经验。

[告别Spark SQL中的联合地狱](https://www.youtube.com/watch?v=c25eT-2dwAg)

ps：如果你对如何处理Spark性能中的数据倾斜感兴趣，我在TDS上有另一个相关的故事。

[](/deep-dive-into-handling-apache-spark-data-skew-57ce0d94ee38?source=post_page-----9123ae6ada80--------------------------------) [## 深入了解处理Apache Spark数据倾斜

### 处理分布式计算中的数据倾斜的终极指南

[深入探讨处理 Apache Spark 数据倾斜](https://towardsdatascience.com/deep-dive-into-handling-apache-spark-data-skew-57ce0d94ee38?source=post_page-----9123ae6ada80--------------------------------)
