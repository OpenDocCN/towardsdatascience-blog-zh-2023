["```py\n# I use tensorflow 2.9.1 version to train model\n!pip install tensorflow==2.9.1\n\nimport pandas as pd\nimport pickle\nfrom PIL import Image\nfrom google.colab import drive\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport tensorflow as tf\nimport numpy as np\nimport zipfile\nimport os\nimport warnings\nimport shutil\nimport seaborn as sns\nimport random\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing\nwarnings.filterwarnings(\"ignore\")\ndrive.mount('/content/gdrive')\nprint(tf.__version__)\n\ndef unzip_data(filename):\n  \"\"\"\n  Unzips filename into the current working directory.\n  Args:\n    filename (str): a filepath to a target zip folder to be unzipped.\n  \"\"\"\n  zip_ref = zipfile.ZipFile(filename, \"r\")\n  zip_ref.extractall()\n  zip_ref.close()\n\ndef plot_value_count(df, column):\n \"\"\"\n plots value count of dataframe column\n \"\"\"\n sns.set(style='darkgrid')\n plt.figure(figsize= (20,10))\n sns.countplot(x=column, data=df, order = df[column].value_counts().index)\n plt.xticks(rotation=90)\n plt.show()\n\nunzip_data(\"/content/gdrive/MyDrive/dog-breed-identification/dog-breed-identification.zip\")\n```", "```py\npython -m venv /path/to/new/virtual/environment\n```", "```py\nlabels_df=pd.read_csv('labels.csv')\n\ntrain_images = os.listdir(\"/content/train/\")\ntest_image = os.listdir(\"/content/test/\")\n\n# how many dog breed? \ntarget_labels = sorted(labels_df['breed'].unique().tolist())\n\nprint('train label shape:', labels_df.shape)\nprint('train dataset and test dataset sizes:',len(train_images), len(test_image))\nprint('total number of dog breeds: ',len(target_labels))\n# lets plot distribution of dog breeds in training data\nplot_value_count(labels_df, 'breed')\n```", "```py\n# 1.create class folders\nparent_folder = \"data\"\ndatasets = [\"all_data\"]\nfor i in datasets:\n  for j in target_labels:\n      os.makedirs(parent_folder + \"/\" + i + \"/\" + j,  exist_ok=True)\n\n# all data\nfor _, name, label in labels_df.itertuples():\n  original_path = \"/content/train/\" + name +\".jpg\"\n  dest_path = \"/content/data/all_data/\" + label\n  shutil.copy(original_path, dest_path)\n```", "```py\nall_data_dir = \"/content/data/all_data/\"\n\ndef view_25_random_image(target_dir, target_classes):\n  plt.figure(figsize=(18, 12))\n  random_images = []\n  for i, target_class in enumerate(target_classes):\n    # setup the target directory\n    target_folder = target_dir + target_class\n\n    # get the random image path\n    random_image = random.sample(os.listdir(target_folder),1)\n    img = mpimg.imread(target_folder + \"/\" + random_image[0])\n    #print(target_folder + \"/\" + random_image[0])\n    #print(random_image)\n    plt.subplot(5,5,i+1)\n    plt.imshow(img)\n    plt.title(f\"{target_class}\")\n    plt.axis(\"off\");\n    random_images.append(img)\n    #print(f\"image shape: {img.shape}\")\n  return random_images\n\nrandom_images = view_25_random_image(target_dir=all_data_dir,\n                                target_classes = random.sample(target_labels, 25))\n```", "```py\ntf.random.set_seed(42)\nIMG_SIZE  = (256,256)\nIMG_SHAPE = IMG_SIZE +(3,)\nBATCH_SIZE = 32\ndata_all = tf.keras.preprocessing.image_dataset_from_directory(all_data_dir,\n                                                              label_mode = \"categorical\",\n                                                              image_size=IMG_SIZE,\n                                                              batch_size=BATCH_SIZE,\n                                                               )\nprint(f\"all_data batches: {len(data_all)}\")\n```", "```py\n#data augmentation\ndata_augmentation_layer = tf.keras.models.Sequential([\n                                                preprocessing.RandomFlip(\"horizontal\"),\n                                                preprocessing.RandomCrop(height=224, width=224),\n                                                preprocessing.RandomRotation(0.2),\n                                                preprocessing.RandomZoom(0.1),\n                                                preprocessing.RandomContrast(factor=0.1),\n                                                ])\n\n# EarlyStopping Callback (stop training in val accuracy not improves)\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", \n                                                  patience=5,\n                                                  restore_best_weights=True)\n\n#ReduceLROnPlateau Callback (Creating learning rate)\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\",  \n                                                 factor=0.2, # multiply the learning rate by 0.2 (reduce by 5x)\n                                                 patience=2,\n                                                 verbose=1, # print out when learning rate goes down \n                                                 min_lr=1e-5)\n\nbaseline_model = tf.keras.applications.EfficientNetB3(include_top=False)\n# trainable freeze\nbaseline_model.trainable = False\ninputs = tf.keras.Input(shape = IMG_SIZE+(3,), name = \"input_layer\")\n#x = data_augmentation_layer(inputs)\nx = baseline_model(inputs, training=False) # weights whhich need to stay frozen, stay frozen\nx = layers.GlobalAveragePooling2D(name=\"global_average_pool_layer\")(x)\nx = layers.Dense(len(data_all.class_names))(x)\noutputs = layers.Activation(\"softmax\", dtype=tf.float32)(x)\nmodel_0 = tf.keras.Model(inputs, outputs)\n\ntrain_size = int(0.9 * len(data_all))\nprint(train_size)\n\nmodel_0.compile(optimizer=tf.keras.optimizers.Adam(),\n                              loss=\"categorical_crossentropy\",\n                              metrics = [\"accuracy\"])\n\nhistory_0 = model_0.fit(data_all.take(train_size),\n                                      epochs=5,\n                                      validation_data = data_all.skip(train_size),\n                                      callbacks = [\n                                                    early_stopping,\n                                                    reduce_lr\n                                                  ]) \n```", "```py\n #save model\nmodel_0.save('/content/gdrive/MyDrive/dog-breed-identification/EfficientNetB3_Model.h5')\n\nbreed_names = data_all.class_names\nbreed_names = [n.replace('_',' ').capitalize() for n in breed_names]\nprint(breed_names[0:3])\n\n#save dog breed names\nwith open('/content/gdrive/MyDrive/dog-breed-identification/class_names', 'wb') as fp:\n    pickle.dump(breed_names, fp)\n```", "```py\nimport streamlit as st\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom tensorflow.keras.preprocessing.image import img_to_array\nimport pickle\nimport numpy as np\nimport pathlib\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nIMG_SIZE  = (256, 256)\nIMG_SHAPE = IMG_SIZE +(3,)\n\n#1\\. load model\n@st.cache_resource\ndef load_model():\n    return tf.keras.models.load_model('pretrained_models/EfficientNetB3_Model.h5')\n\nloaded_model = load_model()\n\n#2\\. load class names\nwith open ('data/class_names', 'rb') as fp:\n    class_names = pickle.load(fp)\n```", "```py\nst.header(\"Dog Identification Application\")\n\nuploaded_file = st.file_uploader(\"Upload an image of your dog and identify its breed...\", type=['jpg', 'jpeg', 'png'])\n```", "```py\n # If an image is uploaded, display it and make a prediction\nif uploaded_file is not None:\n    img = Image.open(uploaded_file).resize(IMG_SIZE)\n    img = np.array(img, dtype=np.float32)\n    img_g = np.expand_dims(img, axis=0)\n    custom_predict = loaded_model.predict(img_g)\n\n    # Display the uploaded image and the predictions\n    # st.write(f\"{class_names[np.argmax(custom_predict[0])]}  ({round(np.max(custom_predict[0]*100))}% confidence)\")\n    if round(np.max(custom_predict[0]*100))>50:\n        st.write(f'<p style=\"font-size:26px; color:green;\"> {class_names[np.argmax(custom_predict[0])]}  ({round(np.max(custom_predict[0]*100))}% confidence)</p> ', unsafe_allow_html=True)\n    else:\n        argsorts = np.argsort(custom_predict[0])\n        sorts = np.sort(custom_predict[0])\n        st.write(f'<p style=\"font-size:26px; color:red;\"> {class_names[argsorts[-1]]}  ({round(sorts[-1]*100)}% confidence) OR {class_names[argsorts[-2]]}  ({round(sorts[-2]*100)}% confidence)</p> ', unsafe_allow_html=True)\n\n    st.image(img/255,  use_column_width=True)\n    st.write('Notes:')\n    st.write('1\\. The model is a first simplest baseline version, the prediction result will be improved in later versions')\n    st.write('2\\. The model can identify up to 120 types of dog breeds')\n    option = st.selectbox(\n    'All Breeds',\n    class_names)\n```", "```py\nstreamlit run Streamlit.py\n```"]