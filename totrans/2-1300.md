# 揭开文本生成 AI 的黑箱

> 原文：[https://towardsdatascience.com/illuminating-the-black-box-of-ai-ddea07e65c35](https://towardsdatascience.com/illuminating-the-black-box-of-ai-ddea07e65c35)

## 需求洞察

[](https://medium.com/@alcarazanthony1?source=post_page-----ddea07e65c35--------------------------------)[![Anthony Alcaraz](../Images/6a71a1752677bd07c384246fb0c7f7e8.png)](https://medium.com/@alcarazanthony1?source=post_page-----ddea07e65c35--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ddea07e65c35--------------------------------)[![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ddea07e65c35--------------------------------) [Anthony Alcaraz](https://medium.com/@alcarazanthony1?source=post_page-----ddea07e65c35--------------------------------)

·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ddea07e65c35--------------------------------) ·8 分钟阅读·2023年12月17日

--

*人工智能软件用于提升本文的语法、流畅性和可读性。*

像 ChatGPT、Claude 3、Gemini 和 Mistral 这样的语言模型以其表达能力和博学引人注目。然而，这些大型语言模型仍然是黑箱，掩盖了驱动其响应的复杂机制。它们生成类似人类的文本的能力超越了我们理解其机器思维如何运作的能力。

但随着人工智能在信任和透明度至关重要的场景中发挥作用，如招聘和风险评估，可解释性现在成为了重点。可解释性不再是复杂系统的可选配件，而是安全推动高影响领域 AI 的必要前提。

为了揭开这些黑箱模型的面纱，生动的可解释 NLP 领域提供了越来越多的工具——从揭示关注模式的注意力可视化，到探查输入的随机部分以量化影响。像 LIME 这样的某些方法创建了模拟关键决策的简化模型。其他方法，如 SHAP，则借鉴了合作博弈论的概念，将“信贷”和“责任”分配到模型输入的不同部分，基于其最终输出。

无论技术如何，所有方法都追求相同的关键目标：阐明语言模型如何利用我们提供的大量文本来编写连贯的段落或进行重要评估。

人工智能已经在影响人类生活的决策中发挥作用——选择性地评估申请者、审查仇恨内容、诊断疾病。

解释不仅仅是附加功能——它们将对监督这些强大的模型在社会中的普及起到关键作用。

随着大型语言模型的不断进步，它们的内部工作机制仍然笼罩在神秘之中。然而，可信的人工智能需要对其在重大决策中的推理过程保持透明。

解释性 NLP 的充满活力的领域提供了两种主要方法来阐明模型逻辑：

1.  **基于扰动的方法**：如 LIME 和 SHAP 的技术通过遮蔽输入组件系统地探测模型，并根据输出变化量化重要性。这些外部视角将模型视为黑箱。

1.  **自我解释**：一种替代范式使模型能够通过生成文本解释自己的推理。例如，突出影响预测的关键输入特征。这依赖于内省的模型意识，而不是强加的解释。

早期分析发现这两种方法都很有前景——LIME 和 SHAP 擅长忠实捕捉模型行为，而自我解释则更符合人类的理性。然而，当前的实践也难以充分评估这两者，建议重新考虑评估策略。

理想情况下，两者之间的协同作用可以结合起来推动进展。例如，自我声明的重要因素可以与扰动实验进行验证。并且归因分数可以增加验证信号，为自由形式的解释提供支撑。

随着模型不断吸收更多的世界知识，阐明它们多方面的推理变得越来越重要。多样化的新兴想法可能对应对这一挑战至关重要。

# 解释 AI 的平衡艺术

构建解释不可避免地需要简化。但过度简化会导致扭曲。以常见的基于注意力的解释为例——它们突出模型 supposedly 关注的输入部分。然而，注意力分数往往与 AI 系统的实际推理过程不一致。

更严格的技术如 **SHAP** 通过系统地遮蔽不同的输入组件并直接测量对输出的影响来避免这一点。通过比较有无每个特征的预测，SHAP 为每个特征分配一个“重要性分数”以表示其影响。这种基于扰动的方法更好地反映了模型的逻辑。

然而，忠实性往往以可理解性为代价。移除单词和子句的组合很快变得认知负担过重。因此，研究社区强调平衡两个关键标准：

**忠实性**：解释多准确地捕捉了模型的实际决策过程？基于遮蔽的扰动方法在这里表现出色。

**可理解性**：解释对目标受众的直观性和易消化程度如何？简化的线性模型有助于理解，但可能会扭曲。

理想情况下，解释应同时展现两者特征。但即便是忠实性高的 SHAP，在模型处理大量文本和不受限制的生成时，也会遇到困难——需要处理的输出组合呈指数级增长。对 10,000 字文章的所有可能遮蔽排列进行计算是不可行的！

这阻碍了对关键应用的进展，如解释作文评分模型或处理文档的问答系统。创建模仿预测的简化模型（如LIME）在复杂文本推理中也变得不可行。需要更具针对性的解决方案来扩展大型语言模型的可解释性。

突出了关键挑战 —— 特别是由长输入和开放输出引入的指数复杂性。如果有任何部分需要更多解释，请告诉我！

# TextGenSHAP：优化语言任务的解释

[](https://arxiv.org/abs/2312.01279?source=post_page-----ddea07e65c35--------------------------------) [## TextGenSHAP：在长文档中生成的可扩展事后解释

### 大型语言模型（LLMs）由于其日益准确的性能而引起了对实际应用的极大关注…

[arxiv.org](https://arxiv.org/abs/2312.01279?source=post_page-----ddea07e65c35--------------------------------)

为了克服复杂语言模型的可解释性障碍，研究人员开发了**TextGenSHAP** —— 在SHAP的基础上，融入了对效率的优化，并考虑了语言结构。

若干创新技术应对了指数级的计算复杂性。**预测解码**首先预测可能的文本输出，避免了浪费的解码尝试。**闪电注意力**简化了内存密集型的注意力计算。**原地重采样**为提高效率预计算了输入编码。

这些加速技术使运行时间从几小时减少到几分钟，实现了实用的周转。作者验证了在不同模型类型和数据集复杂性下的数量级加速。

但仅仅是原始效率是不够的 —— 语言本身的复杂性必须得到体现。TextGenSHAP解决了自然语言处理独有的解释挑战：

**层次结构** —— 除了单个词语外，语言模型还学习句子、段落甚至文档之间的概念联系。TextGenSHAP的层次化归因允许在粗粒度和细粒度层面上分配重要性评分。

**指数输出空间** —— 开放式文本生成产生了巨大的可能输出集，不同于受限的分类任务。通过如Shapley-Shubik指数等重新表述，TextGenSHAP绕过了详尽的枚举来估计特征重要性。

**自回归依赖** —— 生成的标记概率上依赖于前面的标记。TextGenSHAP的适应性解码算法，如预测解码，明确地尊重这些标记间的依赖关系。

这些架构和语言上的进展为TextGenSHAP应对现代自然语言处理的复杂性铺平了道路。现在可以着手解决长期挑战中的可解释性问题，例如文档上的问答。

![](../Images/4082cbd9d8fadca02166150efb0ff90f.png)

作者生成的图像，来源于Dall-E-3.0

# 应用：解释文档中的问答

文档中的问答代表了AI的一个宝贵里程碑——综合散布在段落中的信息以解决复杂的查询。TextGenSHAP现在使解释这些复杂的文本推理工作流程成为可能。

作者在需要从超过10,000个单词的上下文中推导答案的挑战性数据集上评估了TextGenSHAP。令人印象深刻的是，它准确地识别出分布在扩展文本中的关键句子和短语，这些句子和短语对每个答案的形成最有帮助。

通过适当地归因于冗长文档的不同部分，TextGenSHAP使强大的应用成为可能：

**改进文档检索**——通过影响评分对上下文进行排名和筛选，提取了更相关的段落。仅通过根据TextGenSHAP重新排序，作者展示了检索召回率的显著提升——从84%提高到近89%。这有助于更好地为下游推理步骤提供信息。

**提炼证据**——使用重要性评分来挑选每个问题回答的最核心支持段落，在具有多样证据的数据集上准确率从50%提高到70%。确保模型关注简明的解释提取物，以对抗在大型语料库中对虚假模式的过拟合。

**人工监督**——通过揭示最有影响力的文本片段，TextGenSHAP使审计员能够快速验证模型是否使用了适当的支持内容，而不是依赖于非预期的提示。否则，监控复杂的推理过程是不可行的。

对于推理密集型问题回答的成功表明，解释AI能力的社会影响有更广泛的适用性——如评分论文内容和散文或解释医疗诊断。通过揭示语言中的关键联系，TextGenSHAP使我们朝着负责任和可信赖的NLP系统迈进。

[https://anon832098265.github.io/](https://anon832098265.github.io/)

# 调查自我解释

[https://arxiv.org/abs/2310.11207?source=post_page-----ddea07e65c35--------------------------------](https://arxiv.org/abs/2310.11207?source=post_page-----ddea07e65c35--------------------------------) [## 大型语言模型能否自我解释？LLM生成自我解释的研究

### 大型语言模型（LLMs）如ChatGPT在各种自然语言任务中展示了卓越的性能…

[arxiv.org](https://arxiv.org/abs/2310.11207?source=post_page-----ddea07e65c35--------------------------------)

我们讨论了将模型视为黑箱的传统事后方法。一个有趣的替代方案是使系统能够解释自身的推理——**自我解释**。

最近的研究分析了这些用于情感分析，使用ChatGPT。模型突出了输入词汇对其预测的影响。与直接扰动输入的外部技术不同，自我解释依赖于内省模型意识来声明重要因素。

论文系统地比较了不同格式，发现预测然后解释或反之都工作得相当好。模型容易生成所有单词的特征归因分数或仅仅是最重要的高亮部分。但有趣的是，重要性分数通常聚集在“全面的”水平（例如 0.0、0.5、0.75），这更类似于人类判断，而不是离散的机器精度。

尽管自我解释与人类的推理相对一致，但广泛使用的评估实践难以区分质量。依赖于细粒度模型预测变化的指标容易受到欺骗，而这些变化对于 ChatGPT 常常不敏感。研究人员得出结论认为，经典的可解释性流程需要重新考虑以适应大型语言模型。

要充分实现自我解释的潜力，需要新的评估框架，以适应其混合的人机特性。将它们与直接可观察的信号如注意力权重相结合，可能会增强其真实性。构建模块化的推理/解释组件也可能使得自我解释更加纯粹。

通过精心协同设计，以适应其新兴特性，自我解释有可能开启前所未有的模型透明度——将黑箱转换为“玻璃箱”，使系统不仅展示其内部工作原理，还讨论其内在机制。

TextGenSHAP 方法专注于为文本生成模型提供高效的 Shapley 值归因。它在量化特征重要性方面取得了进展，适用于长文档问答任务。

然而，TextGenSHAP 仍然依赖外部视角，扰动输入并观察输出变化，而不是让模型自我反思其推理。这为与自我解释方法的整合留下了空间。

自我解释可以提供更为定性、直观的理解，以补充来自 TextGenSHAP 的定量归因分数。例如，TextGenSHAP 可能会识别文档中的关键段落，并将某些句子突出为回答问题时最具影响力的部分。自我解释可以通过讨论聚焦于这些领域的逻辑来丰富这些信息。

相反，目前的自我解释通常以自由生成的形式出现，缺乏基础。与将模型推理综合为标记重要性排名的归因分数结合起来，可能有助于验证和增强自我解释的意义。

在架构上，TextGenSHAP 模块可以首先处理文档和问题，生成注意力分布和段落排名。然后，自我解释模块可以利用这些定量信号生成自由形式的推理，讨论评估内容，并通过归因分数引导解释。

联合评估还可以评估自我声明的解释因素是否与扰动基础评分指定为有影响的输入组件一致。

本质上，自我解释提供了模型理解的“是什么”，而归因分数则提供了“为什么”。它们的共生关系可能使丰富的解释性融合定量和定性见解成为可能。

# 前进的道路：通过透明度实现信任

TextGenSHAP提供了一个关键的进展——窥视大型语言模型在处理大量文本时的复杂工作。通过创建高效准确的解释，它绕过了现有解释方法仅限于少量语言片段的障碍。

然而，单纯的语言流畅性并不能保证可信赖的人工智能。语言的掌握——推动ChatGPT口才进步的标志——必须与阐明的掌握相结合。

阐明不仅仅是抛出几个关键词那么简单——它需要复制复杂的推理链条，从而得出最终评估。像TextGenSHAP这样的进展将这一必要的透明度更接近现实。

随着模型不断吸收更多世界知识，其内部表征的复杂性也大大增加。通过简化的注意力分数或小扰动样本尝试监督只会混淆视听，而非阐明。尊重结构和逻辑依赖的更全面的方法，如TextGenSHAP，将证明至关重要。

学习缺乏透明度将导致权力没有责任。观察缺乏阐明将导致缺乏严谨的橡皮图章。神经网络的显著复兴必须伴随揭示其复杂性的技术。

在这一领域的进展仍处于初期阶段——但重要的种子已经扎根。通过努力完善理解性与忠实性的混合，无论是通过高效的近似方法还是天生可解释的架构，也许未来的系统可以巧妙地解释其掌握，从而彻底揭开黑箱神秘的面纱。
