["```py\n# Data wrangling\nimport pandas as pd\n\n# Reading the data \nd = pd.read_csv('input/Tweets.csv', header=None)\n\n# Adding the columns \nd.columns = ['INDEX', 'GAME', \"SENTIMENT\", 'TEXT']\n\n# Leaving only the positive and the negative sentiments \nd = d[d['SENTIMENT'].isin(['Positive', 'Negative'])]\n\n# Encoding the sentiments that the negative will be 1 and the positive 0\nd['SENTIMENT'] = d['SENTIMENT'].apply(lambda x: 0 if x == 'Positive' else 1)\n\n# Dropping missing values\nd = d.dropna()\n```", "```py\ndef create_word_index(\n    x: str, \n    shift_for_padding: bool = False, \n    char_level: bool = False) -> Tuple[dict, dict]: \n    \"\"\"\n    Function that scans a given text and creates two dictionaries:\n    - word2idx: dictionary mapping words to integers\n    - idx2word: dictionary mapping integers to words\n\n    Args:\n        x (str): text to scan\n        shift_for_padding (bool, optional): If True, the function will add 1 to all the indexes.\n            This is done to reserve the 0 index for padding. Defaults to False.\n        char_level (bool, optional): If True, the function will create a character level dictionary.\n\n    Returns:\n        Tuple[dict, dict]: word2idx and idx2word dictionaries\n    \"\"\"\n    # Ensuring that the text is a string\n    if not isinstance(x, str):\n        try: \n            x = str(x)\n        except:\n            raise Exception('The text must be a string or a string convertible object')\n\n    # Spliting the text into words\n    words = []\n    if char_level:\n        # The list() function of a string will return a list of characters\n        words = list(x)\n    else:\n        # Spliting the text into words by spaces\n        words = x.split(' ')\n\n    # Creating the word2idx dictionary \n    word2idx = {}\n    for word in words: \n        if word not in word2idx: \n            # The len(word2idx) will always ensure that the \n            # new index is 1 + the length of the dictionary so far\n            word2idx[word] = len(word2idx)\n\n    # Adding the <UNK> token to the dictionary; This token will be used \n    # on new texts that were not seen during training.\n    # It will have the last index. \n    word2idx['<UNK>'] = len(word2idx)\n\n    if shift_for_padding:\n        # Adding 1 to all the indexes; \n        # The 0 index will be reserved for padding\n        word2idx = {k: v + 1 for k, v in word2idx.items()}\n\n    # Reversing the above dictionary and creating the idx2word dictionary\n    idx2word = {idx: word for word, idx in word2idx.items()}\n\n    # Returns the dictionaries\n    return word2idx, idx2word\n```", "```py\n# Spliting to train test \ntrain, test = train_test_split(d, test_size=0.2, random_state=42)\n\n# Reseting the indexes \ntrain.reset_index(drop=True, inplace=True)\ntest.reset_index(drop=True, inplace=True)\n\nprint(f'Train shape: {train.shape}')\nprint(f'Test shape: {test.shape}')\nTrain shape: (34410, 4)\nTest shape: (8603, 4)\n\n# Joining all the texts into one string\ntext = ' '.join(train['TEXT'].values)\n\n# Creating the word2idx and idx2word dictionaries\nword2idx, idx2word = create_word_index(text, shift_for_padding=True, char_level=True)\n\n# Printing the size of the vocabulary\nprint(f'The size of the vocabulary is: {len(word2idx)}')\nThe size of the vocabulary is: 274\n```", "```py\n{'I': 1,\n ' ': 2,\n 'd': 3,\n 'o': 4,\n 'w': 5,\n 'n': 6,\n 'l': 7,\n 'a': 8,\n 'e': 9,\n 'G': 10\n}\n```", "```py\n# For each row in the train and test set, we will create a list of integers\n# that will represent the words in the text\ntrain['text_int'] = train['TEXT'].apply(lambda x: [word2idx.get(word, word2idx['<UNK>']) for word in list(x)])\ntest['text_int'] = test['TEXT'].apply(lambda x: [word2idx.get(word, word2idx['<UNK>']) for word in list(x)])\n\n# Calculating the length of sequences in the train set \ntrain['seq_len'] = train['text_int'].apply(lambda x: len(x))\n\n# Describing the length of the sequences\ntrain['seq_len'].describe()\ncount    34410.000000\nmean       103.600262\nstd         79.972798\nmin          1.000000\n25%         41.000000\n50%         83.000000\n75%        148.000000\nmax        727.000000\n```", "```py\ndef pad_sequences(x: list, pad_length: int) -> list:\n    \"\"\"\n    Function that pads a given list of integers to a given length\n\n    Args:\n        x (list): list of integers to pad\n        pad_length (int): length to pad\n\n    Returns:\n        list: padded list of integers\n    \"\"\"\n    # Getting the length of the list\n    len_x = len(x)\n\n    # Checking if the length of the list is less than the pad_length\n    if len_x < pad_length: \n        # Padding the list with 0s\n        x = x + [0] * (pad_length - len_x)\n    else: \n        # Truncating the list to the desired length\n        x = x[:pad_length]\n\n    # Returning the padded list\n    return x\n\n# Padding the train and test sequences \ntrain['text_int'] = train['text_int'].apply(lambda x: pad_sequences(x, 200))\ntest['text_int'] = test['text_int'].apply(lambda x: pad_sequences(x, 200))\n```", "```py\ndef sigmoid(x: float) -> float: \n    \"\"\"\n    Function that calculates the sigmoid of a given value\n\n    Args:\n        x (float): value to calculate the sigmoid\n\n    Returns:\n        float: sigmoid of the given value in (0, 1)\n    \"\"\"\n    return 1 / (1 + np.exp(-x))\n\ndef tanh(x: float) -> float: \n    \"\"\"\n    Function that calculates the tanh of a given value\n\n    Args:\n        x (float): value to calculate the tanh\n\n    Returns:\n        float: tanh of the given value in (-1, 1)\n    \"\"\"\n    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n```", "```py\nclass ForgetGate: \n    \"\"\"\n    Class that implements the forget gate of an LSTM cell\n    \"\"\"\n    def __init__(\n            self, \n            w1: float = np.random.normal(), \n            w2: float = np.random.normal(),\n            b1: float = np.random.normal(),\n            long_term_memory: float = np.random.normal(), \n            short_term_memory: float = np.random.normal(), \n            ):\n        \"\"\"\n        Constructor of the class\n\n        Args:\n            long_term_memory (float): long term memory\n            short_term_memory (float): short term memory\n            w1 (float): weight 1\n            w2 (float): weight 2\n            b1 (float): bias term 1\n        \"\"\"\n        # Saving the input\n        self.long_term_memory = long_term_memory\n        self.short_term_memory = short_term_memory\n        self.w1 = w1\n        self.w2 = w2\n        self.b1 = b1\n\n    def forward(self, x: float) -> float: \n        \"\"\"\n        Function that calculates the output of the forget gate\n\n        Args:\n            x (float): input to the forget gate\n\n        Returns:\n            float: output of the forget gate\n        \"\"\"\n        # Calculates the percentage of the long term memory that will be kept\n        percentage_to_keep = sigmoid((self.w1 * x  + self.w2 * self.short_term_memory) + self.b1)\n\n        # Updating the long term memory\n        self.long_term_memory = self.long_term_memory * percentage_to_keep\n\n        # The output of the forget gate is the new long term memory and the short term memory\n        return self.long_term_memory, self.short_term_memory\n```", "```py\n# Initiating \nforget_gate = ForgetGate()\n\nprint(f'Initial long term memory: {forget_gate.long_term_memory}')\nprint(f'Initial short term memory: {forget_gate.short_term_memory}')\n\n# Calculating the output of the forget gate\nlt, st = forget_gate.forward(0.5)\n\nprint(f'Long term memory: {lt}')\nprint(f'Short term memory: {st}')\n\nInitial long term memory: -0.8221542907288696\nInitial short term memory: -0.5617438418718841\nLong term memory: -0.37335827895028\nShort term memory: -0.5617438418718841\n```", "```py\nclass InputGate:\n    def __init__(\n            self, \n            w3: float = np.random.normal(), \n            w4: float = np.random.normal(),\n            w5: float = np.random.normal(),\n            w6: float = np.random.normal(),\n            b2: float = np.random.normal(),\n            b3: float = np.random.normal(),\n            long_term_memory: float = np.random.normal(), \n            short_term_memory: float = np.random.normal(), \n            ):\n        \"\"\"\n        Constructor of the class\n\n        Args:\n            long_term_memory (float): long term memory\n            short_term_memory (float): short term memory\n            w3 (float): weight 3\n            w4 (float): weight 4\n            w5 (float): weight 5\n            w6 (float): weight 6\n            b2 (float): bias 2\n            b3 (float): bias 3\n        \"\"\"\n        # Saving the input\n        self.long_term_memory = long_term_memory\n        self.short_term_memory = short_term_memory\n        self.w3 = w3\n        self.w4 = w4\n        self.w5 = w5\n        self.w6 = w6\n        self.b2 = b2\n        self.b3 = b3\n\n    def forward(self, x: float) -> float:\n        \"\"\"\n        Function that calculates the output of the input gate\n\n        Args:\n            x (float): input to the input gate\n\n        Returns:\n            float: output of the input gate\n        \"\"\"\n        # Calculating the memory signal \n        memory_signal = tanh((self.w3 * x + self.w4 * self.short_term_memory) + self.b2)\n\n        # Calculating the percentage of memory to keep \n        percentage_to_keep = sigmoid((self.w5 * x + self.w6 * self.short_term_memory) + self.b3) \n\n        # Multiplying the memory signal by the percentage to keep\n        memory_signal = memory_signal * percentage_to_keep\n\n        # Updating the long term memory\n        self.long_term_memory = self.long_term_memory + memory_signal\n\n        # The output of the input gate is the new long term memory and the short term memory\n        return self.long_term_memory, self.short_term_memory\n```", "```py\n# Creating the input gate object with the forget gates' output \ninput_gate = InputGate(long_term_memory=lt, short_term_memory=st)\n\n# Forward propagating \nlt, st = input_gate.forward(0.5)\n\nprint(f'Long term memory: {lt}')\nprint(f'Short term memory: {st}')\n\nLong term memory: -1.028998511766425\nShort term memory: -0.5617438418718841\n```", "```py\nclass OutputGate:\n    def __init__(\n            self, \n            w7: float = np.random.normal(), \n            w8: float = np.random.normal(),\n            b4: float = np.random.normal(),\n            long_term_memory: float = np.random.normal(), \n            short_term_memory: float = np.random.normal(), \n            ):\n        \"\"\"\n        Constructor of the class\n\n        Args:\n            long_term_memory (float): long term memory\n            short_term_memory (float): short term memory\n            w7 (float): weight 7\n            w8 (float): weight 8\n            w9 (float): weight 9\n            w10 (float): weight 10\n            b4 (float): bias 4\n            b5 (float): bias 5\n        \"\"\"\n        # Saving the input\n        self.long_term_memory = long_term_memory\n        self.short_term_memory = short_term_memory\n        self.w7 = w7\n        self.w8 = w8\n        self.b4 = b4\n\n    def forward(self, x: float) -> float:\n        \"\"\"\n        Function that calculates the output of the output gate\n\n        Args:\n            x (float): input to the output gate\n\n        Returns:\n            float: output of the output gate\n        \"\"\"\n        # Calculating the short term memory signal \n        short_term_memory_signal = tanh(self.long_term_memory)\n\n        # Calculating the percentage of short term memory to keep \n        percentage_to_keep = sigmoid((self.w7 * x + self.w8 * self.short_term_memory) + self.b4) \n\n        # Multiplying the short term memory signal by the percentage to keep\n        short_term_memory_signal = short_term_memory_signal * percentage_to_keep\n\n        # Updating the short term memory\n        self.short_term_memory = short_term_memory_signal\n\n        # The output of the output gate is the new long term memory and the short term memory\n        return self.long_term_memory, self.short_term_memory\n```", "```py\n# Creating the output gate object\noutput_gate = OutputGate(long_term_memory=lt, short_term_memory=st)\n\n# Forward propagating\nlt, st = output_gate.forward(0.5)\n\nprint(f'Long term memory: {lt}')\nprint(f'Short term memory: {st}')\n\nLong term memory: -1.028998511766425\nShort term memory: -0.7233077589896045\n```", "```py\n# Redefining the forget, input and output gates as functions \ndef forget_gate(x: float, w1: float, w2: float, b1: float, long_term_memory: float, short_term_memory: float) -> Tuple[float, float]:\n    \"\"\"\n    Function that calculates the output of the forget gate\n\n    Args:\n        x (float): input to the forget gate\n        w1 (float): weight 1\n        w2 (float): weight 2\n        b1 (float): bias 1\n        long_term_memory (float): long term memory\n        short_term_memory (float): short term memory\n\n    Returns:\n        Tuple[float, float]: output of the forget gate\n    \"\"\"\n    # Calculates the percentage of the long term memory that will be kept\n    percentage_to_keep = sigmoid((w1 * x  + w2 * short_term_memory) + b1)\n\n    # Updating the long term memory\n    long_term_memory = long_term_memory * percentage_to_keep\n\n    # The output of the forget gate is the new long term memory and the short term memory\n    return long_term_memory, short_term_memory\n\ndef input_gate(x: float, w3: float, w4: float, w5: float, w6: float, b2: float, b3: float, long_term_memory: float, short_term_memory: float) -> Tuple[float, float]:\n    \"\"\"\n    Function that calculates the output of the input gate\n\n    Args:\n        x (float): input to the input gate\n        w3 (float): weight 3\n        w4 (float): weight 4\n        w5 (float): weight 5\n        w6 (float): weight 6\n        b2 (float): bias 2\n        b3 (float): bias 3\n        long_term_memory (float): long term memory\n        short_term_memory (float): short term memory\n\n    Returns:\n        Tuple[float, float]: output of the input gate\n    \"\"\"\n    # Calculating the memory signal \n    memory_signal = tanh((w3 * x + w4 * short_term_memory) + b2)\n\n    # Calculating the percentage of memory to keep \n    percentage_to_keep = sigmoid((w5 * x + w6 * short_term_memory) + b3) \n\n    # Multiplying the memory signal by the percentage to keep\n    memory_signal = memory_signal * percentage_to_keep\n\n    # Updating the long term memory\n    long_term_memory = long_term_memory + memory_signal\n\n    # The output of the input gate is the new long term memory and the short term memory\n    return long_term_memory, short_term_memory\n\ndef output_gate(x: float, w7: float, w8: float, b4: float, long_term_memory: float, short_term_memory: float) -> Tuple[float, float]:\n    \"\"\"\n    Function that calculates the output of the output gate\n\n    Args:\n        x (float): input to the output gate\n        w7 (float): weight 7\n        w8 (float): weight 8\n        b4 (float): bias 4\n        long_term_memory (float): long term memory\n        short_term_memory (float): short term memory\n\n    Returns:\n        Tuple[float, float]: output of the output gate\n    \"\"\"\n    # Calculating the short term memory signal \n    short_term_memory_signal = tanh(long_term_memory)\n\n    # Calculating the percentage of short term memory to keep \n    percentage_to_keep = sigmoid((w7 * x + w8 * short_term_memory) + b4) \n\n    # Multiplying the short term memory signal by the percentage to keep\n    short_term_memory_signal = short_term_memory_signal * percentage_to_keep\n\n    # Updating the short term memory\n    short_term_memory = short_term_memory_signal\n\n    # The output of the output gate is the new long term memory and the short term memory\n    return long_term_memory, short_term_memory \n\nclass simpleLSTM: \n    def __init__(\n            self, \n            w1: float = np.random.normal(),\n            w2: float = np.random.normal(),\n            w3: float = np.random.normal(),\n            w4: float = np.random.normal(),\n            w5: float = np.random.normal(),\n            w6: float = np.random.normal(),\n            w7: float = np.random.normal(),\n            w8: float = np.random.normal(),\n            b1: float = np.random.normal(),\n            b2: float = np.random.normal(),\n            b3: float = np.random.normal(),\n            b4: float = np.random.normal(),\n            long_term_memory: float = np.random.normal(),\n            short_term_memory: float = np.random.normal(),\n            ):\n        \"\"\"\n        Constructor of the class\n\n        Args:\n            long_term_memory (float): long term memory\n            short_term_memory (float): short term memory\n            w1 (float): weight 1\n            w2 (float): weight 2\n            w3 (float): weight 3\n            w4 (float): weight 4\n            w5 (float): weight 5\n            w6 (float): weight 6\n            w7 (float): weight 7\n            w8 (float): weight 8\n            b1 (float): bias 1\n            b2 (float): bias 2\n            b3 (float): bias 3\n            b4 (float): bias 4\n        \"\"\"\n\n        # Saving the input\n        self.long_term_memory = long_term_memory\n        self.short_term_memory = short_term_memory\n        self.w1 = w1\n        self.w2 = w2\n        self.w3 = w3\n        self.w4 = w4\n        self.w5 = w5\n        self.w6 = w6\n        self.w7 = w7\n        self.w8 = w8\n        self.b1 = b1\n        self.b2 = b2\n        self.b3 = b3\n        self.b4 = b4\n\n    def forward(self, x: float) -> float:\n        \"\"\"\n        Function that calculates the output of the simple LSTM cell\n\n        Args:\n            x (float): input to the simple LSTM cell\n\n        Returns:\n            float: output of the simple LSTM cell\n        \"\"\"\n        # Calculating the output of the forget gate\n        lt, st = forget_gate(x, self.w1, self.w2, self.b1, self.long_term_memory, self.short_term_memory)\n\n        # Updating the long term memory\n        self.long_term_memory = lt\n\n        # Calculating the output of the input gate\n        lt, st = input_gate(x, self.w3, self.w4, self.w5, self.w6, self.b2, self.b3, self.long_term_memory, self.short_term_memory)\n\n        # Updating the long term memory\n        self.long_term_memory = lt\n\n        # Calculating the output of the output gate\n        lt, st = output_gate(x, self.w7, self.w8, self.b4, self.long_term_memory, self.short_term_memory)\n\n        # Updating the short term memory\n        self.short_term_memory = st\n\n        # The output of the simple LSTM cell is the new long term memory and the short term memory\n        return self.long_term_memory, self.short_term_memory\n\n    def forward_sequence(self, x: list) -> list:\n        \"\"\"\n        Function that forward propagates a sequence of inputs through the simple LSTM cell\n\n        Args:\n            x (list): sequence of inputs to the simple LSTM cell\n\n        Returns:\n            list: sequence of outputs of the simple LSTM cell\n        \"\"\"\n        # Creating a list to store the outputs\n        outputs = []\n\n        # Forward propagating each input\n        for input in x: \n            # Forward propagating the input\n            _, st = self.forward(input)\n\n            # Appending the output to the list\n            outputs.append(st)\n\n        # Returning the list of outputs\n        return outputs\n```", "```py\n# Creating the simple LSTM cell object\nsimple_lstm = simpleLSTM()\n\n# Creating a sequence of x\nx = [0.5, 0.6, 0.7, 0.8, 0.9]\n\n# Forward propagating the sequence\noutputs = simple_lstm.forward_sequence(x)\n\n# Rounding \noutputs = [round(output, 2) for output in outputs]\n\n# Printing the outputs\nprint(f'The outputs of the simple LSTM cell are: {outputs}')\nThe outputs of the simple LSTM cell are: [0.63, 0.41, 0.33, 0.28, 0.25]\n```", "```py\n# Defining the torch model for sentiment classification \nclass SentimentClassifier(torch.nn.Module):\n    \"\"\"\n    Class that defines the sentiment classifier model\n    \"\"\"\n    def __init__(self, vocab_size, embedding_dim):\n        super(SentimentClassifier, self).__init__()\n\n        self.embedding = nn.Embedding(vocab_size + 1, embedding_dim)\n        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=1, batch_first=True)\n        self.fc = nn.Linear(1, 1)  # Output with a single neuron for binary classification\n        self.sigmoid = nn.Sigmoid()  # Sigmoid activation\n\n    def forward(self, x):\n        x = self.embedding(x)  # Embedding layer\n        output, _ = self.lstm(x)  # RNN layer\n\n        # Use the short term memory from the last time step as the representation of the sequence\n        x = output[:, -1, :]\n\n        # Fully connected layer with a single neuron\n        x = self.fc(x) \n\n        # Converting to probabilities\n        x = self.sigmoid(x)\n\n        # Flattening the output\n        x = x.squeeze()\n\n        return x\n\n# Initiating the model \nmodel = SentimentClassifier(vocab_size=len(word2idx), embedding_dim=16)\n\n# Initiating the criterion and the optimizer\ncriterion = nn.BCELoss() # Binary cross entropy loss\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n```", "```py\n# Defining the data loader \nfrom torch.utils.data import Dataset, DataLoader\n\nclass TextClassificationDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        # The x is named as text_int and the y as airline_sentiment\n        x = self.data.iloc[idx]['text_int']\n        y = self.data.iloc[idx]['SENTIMENT']\n\n        # Converting the x and y to torch tensors\n        x = torch.tensor(x)\n        y = torch.tensor(y)\n\n        # Converting the y variable to float \n        y = y.float()\n\n        # Returning the x and y\n        return x, y\n\n# Creating the train and test loaders\ntrain_loader = DataLoader(TextClassificationDataset(train), batch_size=32, shuffle=True)\ntest_loader = DataLoader(TextClassificationDataset(test), batch_size=32, shuffle=True)\n```", "```py\n# Defining the number of epochs\nepochs = 100\n\n# Setting the model to train mode\nmodel.train()\n\n# Saving of the loss values\nlosses = []\n\n# Iterating through epochs\nfor epoch in range(epochs):\n    # Initiating the total loss \n    total_loss = 0\n\n    for batch_idx, (inputs, labels) in enumerate(train_loader):\n        # Zero the gradients\n        optimizer.zero_grad()  # Zero the gradients\n        outputs = model(inputs)  # Forward pass\n\n        loss = criterion(outputs, labels)  # Compute the loss\n        loss.backward()  # Backpropagation\n        optimizer.step()  # Update the model's parameters\n\n        # Adding the loss to the total loss\n        total_loss += loss.item()\n\n    # Calculating the average loss\n    avg_loss = total_loss / len(train_loader)\n\n    # Appending the loss to the list containing the losses\n    losses.append(avg_loss)\n\n    # Printing the loss every n epochs\n    if epoch % 20 == 0:\n        print(f'Epoch: {epoch}, Loss: {avg_loss}')\n\nEpoch: 0, Loss: 0.6951859079329055\nEpoch: 20, Loss: 0.6478807757224292\nEpoch: 40, Loss: 0.6398377026877882\nEpoch: 60, Loss: 0.6353290403144067\nEpoch: 80, Loss: 0.6312290856884758\n```", "```py\n# Setting the model to eval model\nmodel.eval()\n\n# List to track the test acc \ntotal_correct = 0\ntotal_obs = 0\n\n# Iterating over the test set\nfor batch_idx, (inputs, labels) in enumerate(test_loader):\n    outputs = model(inputs)  # Forward pass\n\n    # Getting the number of correct predictions \n    correct = ((outputs > 0.5).float() == labels).float().sum()\n\n    # Getting the total number of predictions\n    total = labels.size(0)\n\n    # Updating the total correct and total observations\n    total_correct += correct\n    total_obs += total\n\nprint(f'The test accuracy is: {total_correct / total_obs}')\nThe test accuracy is: 0.6447750926017761\n```"]