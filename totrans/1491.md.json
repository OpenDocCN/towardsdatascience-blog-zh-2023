["```py\n# Regex \nimport re\n\ndef preprocess_text(x: str) -> str:\n    \"\"\"\n    Function that preprocess the text before tokenization\n\n    Args:\n        x (str): text to preprocess\n\n    Returns:\n        str: preprocessed text\n    \"\"\" \n    # Create whitespaces around punctuation\n    x = re.sub(r'([.,!?;:])', r' \\1 ', x)\n\n    # Returns the text \n    return x\n\n# Original text \ntext = \"NLP in Python is fun and very well documented. Let's get started!\"\n\n# Applying the function \ntext_preprocessed = preprocess_text(text)\n\n# Printing the results \nprint(text)\n```", "```py\n“NLP in Python is fun and very well documented . Let’s get started !”\n```", "```py\n# Typehinting\nfrom typing import Tuple\n\n# Defining the function \ndef create_word_index(x: str) -> Tuple[dict, dict]: \n    \"\"\"\n    Function that scans a given text and creates two dictionaries:\n    - word2idx: dictionary mapping words to integers\n    - idx2word: dictionary mapping integers to words\n\n    Args:\n        x (str): text to scan\n\n    Returns:\n        Tuple[dict, dict]: word2idx and idx2word dictionaries\n    \"\"\"\n    # Spliting the text into words\n    words = x.split()\n\n    # Creating the word2idx dictionary \n    word2idx = {}\n    for word in words: \n        if word not in word2idx: \n            # The len(word2idx) will always ensure that the \n            # new index is 1 + the length of the dictionary so far\n            word2idx[word] = len(word2idx)\n\n    # Adding the <UNK> token to the dictionary; This token will be used \n    # on new texts that were not seen during training.\n    # It will have the last index. \n    word2idx['<UNK>'] = len(word2idx)\n\n    # Reversing the above dictionary and creating the idx2word dictionary\n    idx2word = {idx: word for word, idx in word2idx.items()}\n\n    # Returns the dictionaries\n    return word2idx, idx2word\n\n# Applying the function \nword2idx, idx2word = create_word_index(text_preprocessed)\n```", "```py\n{\n  'NLP': 0,\n  'in': 1,\n  'Python': 2,\n  'is': 3,\n  'fun': 4,\n  'and': 5,\n  'very': 6,\n  'well': 7,\n  'documented': 8,\n  '.': 9,\n  \"Let's\": 10,\n  'get': 11,\n  'started': 12,\n  '!': 13,\n  '<UNK>': 14\n}\n```", "```py\n{\n  0: 'NLP',\n  1: 'in',\n  2: 'Python',\n  3: 'is',\n  4: 'fun',\n  5: 'and',\n  6: 'very',\n  7: 'well',\n  8: 'documented',\n  9: '.',\n  10: \"Let's\",\n  11: 'get',\n  12: 'started',\n  13: '!',\n  14: '<UNK>'\n}\n```", "```py\n# Defining a function that splits a text into tokens \ndef text2tokens(x: str, word2idx: dict) -> list: \n    \"\"\"\n    Function that tokenizes a text\n\n    Args:\n        x (str): text to tokenize\n        word2idx (dict): word2idx dictionary\n\n    Returns:\n        list: list of tokens\n    \"\"\"\n    # Spliting the text into words\n    words = x.split()\n\n    # Creating the list of tokens\n    tokens = []\n    for word in words: \n        # The bellow line searches for the word in the word2idx dictionary\n        # and returns the index of the word. If the word is not found,\n        # it returns the index of the <UNK> token\n        tokens.append(word2idx.get(word, word2idx['<UNK>']))\n\n    # Returns the list of tokens\n    return tokens\n\n# Defining a function that converts the tokens to text \ndef tokens2text(x: list, idx2word: dict) -> str:\n    \"\"\"\n    Function that converts tokens to text\n\n    Args:\n        x (list): list of tokens\n        idx2word (dict): idx2word dictionary\n\n    Returns:\n        str: text\n    \"\"\"\n    # Creating the list of words\n    words = []\n    for idx in x: \n        # The bellow line searches for the index in the idx2word dictionary\n        # and returns the word. If the index is not found,\n        # it returns the <UNK> token\n        words.append(idx2word.get(idx, '<UNK>'))\n\n    # Returns the text\n    return ' '.join(words)\n\n# Applying the text2tokens function to the text\ntokens_seq = text2tokens(text_preprocessed, word2idx)\n\n# Transforming the tokens back to text\ntext_seq = tokens2text(tokens_seq, idx2word)\n```", "```py\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n```", "```py\n\"NLP in Python is fun and very well documented . Let's get started !\"\n```", "```py\n# Putting everything together with a new text\ntext = \"As I said, Python is a very good tool for NLP\"\n\n# Preprocessing the text\ntext_preprocessed = preprocess_text(text)\n\n# Applying the text2tokens function to the text\ntokens_seq = text2tokens(text_preprocessed, word2idx)\n\n# Transforming the tokens back to text\ntext_seq = tokens2text(tokens_seq, idx2word)\n\nprint(f\"Original text:\\n {text_preprocessed}\")\nprint(f\"Tokens:\\n {tokens_seq}\")\nprint(f\"Text:\\n {text_seq}\")\n```", "```py\nOriginal text:\n As I said ,  Python is a very good tool for NLP\nTokens:\n [14, 14, 14, 14, 2, 3, 14, 6, 14, 14, 14, 0]\nText:\n <UNK> <UNK> <UNK> <UNK> Python is <UNK> very <UNK> <UNK> <UNK> NLP\n```", "```py\n# Bellow are examples of valid embeddings\n'dog' = [-0.25, 0.35, 0.87]\n'cat' = [0.98, 0.21, -0.78, 0.77]\n'person' = [1.25, 3.75]\n```", "```py\n# Defining a function that creates the embedding vector \n# Defining a function that creates the embedding vector \ndef create_embedding_vector(\n        n_dim: int = 16, \n        mean: float = 0.0, \n        variance: float = 1.0,\n        precision: int = None\n        ) -> np.array: \n    \"\"\"\n    Function that creates a random embedding vector\n\n    Args:\n        n_dim (int, optional): embedding dimension. Defaults to 16.\n        mean (float, optional): mean of the normal distribution. Defaults to 0.0.\n        variance (float, optional): variance of the normal distribution. Defaults to 1.0.\n        precision (int, optional): precision of each of the gotten coordinate. Defaults to None.\n\n    Returns:\n        np.array: embedding vector\n    \"\"\"\n    # Creating a random normal distribution \n    X = np.random.normal(mean, variance, (n_dim, ))\n\n    # Rounding the coordinates to the given precision\n    if precision: \n        X = np.round(X, precision)\n\n    # Returns the embedding vector \n    return X\n```", "```py\n# To recap, the idx2word dictionary is: \n{\n  0: 'NLP',\n  1: 'in',\n  2: 'Python',\n  3: 'is',\n  4: 'fun',\n  5: 'and',\n  6: 'very',\n  7: 'well',\n  8: 'documented',\n  9: '.',\n  10: \"Let's\",\n  11: 'get',\n  12: 'started',\n  13: '!',\n  14: '<UNK>'\n}\n\n# Initiating the embedding dictionary \nidx2embeddings = {}\n\n# Iterating over the idx2word dictionary\nfor idx in range(len(idx2word)): \n    # Creating the embedding vector \n    X = create_embedding_vector(n_dim=6, precision=3)\n\n    # Adding the embedding vector to the embedding dictionary\n    idx2embeddings[idx] = X\n\n# Creating the word2embeddings dictionary\nword2embeddings = {word: idx2embeddings[idx] for word, idx in word2idx.items()}\n```", "```py\n# idx2embeddings\n{\n  0: array([ 0.308, -1.003, -0.36 ,  0.57 , -1.106, -0.997]),\n  1: array([-1.283,  0.709,  0.812,  0.201,  0.339,  1.264]),\n  2: array([ 1.095, -0.666,  1.32 , -0.668, -0.705,  0.311]),\n  3: array([ 0.417,  1.088,  1.242,  0.905, -0.061,  1.316]),\n  4: array([ 1.432, -0.072,  0.622, -0.077,  0.597,  0.722]),\n  5: array([-0.724, -0.496, -1.652,  1.118, -2.108, -0.996]),\n  6: array([ 0.733,  0.021,  0.972,  0.363,  0.074, -0.661]),\n  7: array([-0.284,  1.453,  2.522,  1.027, -1.484,  0.301]),\n  8: array([ 0.921,  0.19 ,  0.068,  0.517, -0.767,  0.225]),\n  9: array([-0.317,  0.691, -1.281,  0.624,  2.004,  1.377]),\n  10: array([ 0.171,  0.607, -1.064, -0.064, -0.091,  0.748]),\n  11: array([-0.151,  1.137,  0.783, -0.689, -1.473, -0.753]),\n  12: array([1.699, 0.021, 1.422, 0.316, 0.317, 0.064]),\n  13: array([-0.804,  0.156, -0.298,  0.15 , -0.686,  0.752]),\n  14: array([ 0.538,  0.405,  0.501, -0.245, -1.946,  0.282])\n}\n\n# word2embeddings\n{\n  'NLP': array([ 0.308, -1.003, -0.36 ,  0.57 , -1.106, -0.997]),\n  'in': array([-1.283,  0.709,  0.812,  0.201,  0.339,  1.264]),\n  'Python': array([ 1.095, -0.666,  1.32 , -0.668, -0.705,  0.311]),\n  'is': array([ 0.417,  1.088,  1.242,  0.905, -0.061,  1.316]),\n  'fun': array([ 1.432, -0.072,  0.622, -0.077,  0.597,  0.722]),\n  'and': array([-0.724, -0.496, -1.652,  1.118, -2.108, -0.996]),\n  'very': array([ 0.733,  0.021,  0.972,  0.363,  0.074, -0.661]),\n  'well': array([-0.284,  1.453,  2.522,  1.027, -1.484,  0.301]),\n  'documented': array([ 0.921,  0.19 ,  0.068,  0.517, -0.767,  0.225]),\n  '.': array([-0.317,  0.691, -1.281,  0.624,  2.004,  1.377]),\n  \"Let's\": array([ 0.171,  0.607, -1.064, -0.064, -0.091,  0.748]),\n  'get': array([-0.151,  1.137,  0.783, -0.689, -1.473, -0.753]),\n  'started': array([1.699, 0.021, 1.422, 0.316, 0.317, 0.064]),\n  '!': array([-0.804,  0.156, -0.298,  0.15 , -0.686,  0.752]),\n  '<UNK>': array([ 0.538,  0.405,  0.501, -0.245, -1.946,  0.282])\n}\n```", "```py\n# Creating the embedding matrix\nembedding_matrix = np.array([idx2embeddings[idx] for idx in idx2embeddings])\n\n# Printing the matrix\nprint(embedding_matrix)\n\n# The resulting embedding matrix of shape 15 x 6\n[[ 0.308 -1.003 -0.36   0.57  -1.106 -0.997]\n [-1.283  0.709  0.812  0.201  0.339  1.264]\n [ 1.095 -0.666  1.32  -0.668 -0.705  0.311]\n [ 0.417  1.088  1.242  0.905 -0.061  1.316]\n [ 1.432 -0.072  0.622 -0.077  0.597  0.722]\n [-0.724 -0.496 -1.652  1.118 -2.108 -0.996]\n [ 0.733  0.021  0.972  0.363  0.074 -0.661]\n [-0.284  1.453  2.522  1.027 -1.484  0.301]\n [ 0.921  0.19   0.068  0.517 -0.767  0.225]\n [-0.317  0.691 -1.281  0.624  2.004  1.377]\n [ 0.171  0.607 -1.064 -0.064 -0.091  0.748]\n [-0.151  1.137  0.783 -0.689 -1.473 -0.753]\n [ 1.699  0.021  1.422  0.316  0.317  0.064]\n [-0.804  0.156 -0.298  0.15  -0.686  0.752]\n [ 0.538  0.405  0.501 -0.245 -1.946  0.282]]\n```", "```py\n# Text\n\"NLP in Python is fun and very well documented . Let's get started !\"\n\n# The token sequence \n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n```", "```py\n# 5 random texts\ntexts = [\n  \"I love my dog\", \n  \"The sun is rising\", \n  \"Bacon lettuce tomatoe yum\", \n  \"I love my mother\", \n  \"The frogs are gray\"\n]\n```", "```py\n\"I\" -> \"love\" -> \"my\" -> \"dog\"\n\"The\" -> \"sun\" -> \"is\" -> \"rising\"\n\"Bacon\" -> \"lettuce\" -> \"tomatoe\" -> \"yum\" \n\"I\" -> \"love\" -> \"my\" -> \"mother\" \n\"The\" -> \"frogs\" -> \"are\" -> \"gray\"\n```", "```py\n{\n  \"The\": [0.01, 0.14, 0.71, 0.69],\n  \"frogs\": [-1.25, 0.69, -0.54, 0.19],\n  \"are\": [2.10, -1.13, -0.15, -0.13], \n  \"gray\": [-0.22, 0.55, 1.12, 0.25],\n}\n```", "```py\nBCE(w) = -0.5 [(1 * log(0.88) + (1 - 1) log(1 - 0.88)) + (0 * log(0.12) + (1 - 0) log(1 - 0.12))]\nBCE(w) = 0.056\n```", "```py\nBCE(w) = -0.5 [(1 * log(0.95) + (1 - 1) log(1 - 0.95)) + (0 * log(0.08) + (1 - 0) log(1 - 0.08))]\nBCE(w) = 0.029\n```", "```py\nimport pandas as pd \n\n# Reading the data \nd = pd.read_csv('input/Tweets.csv', header=None)\n\n# Adding the columns \nd.columns = ['INDEX', 'GAME', \"SENTIMENT\", 'TEXT']\n\n# Leaving only the positive and the negative sentiments \nd = d[d['SENTIMENT'].isin(['Positive', 'Negative'])]\n\n# Encoding the sentiments that the negative will be 1 and the positive 0\nd['SENTIMENT'] = d['SENTIMENT'].apply(lambda x: 0 if x == 'Positive' else 1)\n\n# Dropping missing values\nd = d.dropna()\n\n# Spliting to train and test sets \ntrain, test = train_test_split(d, test_size=0.2, random_state=42)\n\n# Reseting the indexes \ntrain.reset_index(drop=True, inplace=True)\ntest.reset_index(drop=True, inplace=True)\n```", "```py\ndef create_word_index(x: str, shift_for_padding: bool = False) -> Tuple[dict, dict]: \n    \"\"\"\n    Function that scans a given text and creates two dictionaries:\n    - word2idx: dictionary mapping words to integers\n    - idx2word: dictionary mapping integers to words\n\n    Args:\n        x (str): text to scan\n        shift_for_padding (bool, optional): If True, the function will add 1 to all the indexes.\n\n    Returns:\n        Tuple[dict, dict]: word2idx and idx2word dictionaries\n    \"\"\"\n    # Spliting the text into words\n    words = x.split()\n\n    # Creating the word2idx dictionary \n    word2idx = {}\n    for word in words: \n        if word not in word2idx: \n            # The len(word2idx) will always ensure that the \n            # new index is 1 + the length of the dictionary so far\n            word2idx[word] = len(word2idx)\n\n    # Adding the <UNK> token to the dictionary; This token will be used \n    # on new texts that were not seen during training.\n    # It will have the last index. \n    word2idx['<UNK>'] = len(word2idx)\n\n    if shift_for_padding:\n        # Adding 1 to all the indexes; \n        # The 0 index will be reserved for padding\n        word2idx = {k: v + 1 for k, v in word2idx.items()}\n\n    # Reversing the above dictionary and creating the idx2word dictionary\n    idx2word = {idx: word for word, idx in word2idx.items()}\n\n    # Returns the dictionaries\n    return word2idx, idx2word\n```", "```py\n# Joining all the texts into one string\ntext = ' '.join(train['TEXT'].values)\n\n# Creating the word2idx and idx2word dictionaries\nword2idx, idx2word = create_word_index(text, shift_for_padding=True)\n\n# Printing the size of the vocabulary\nprint(f'The size of the vocabulary is: {len(word2idx)}')\nThe size of the vocabulary is: 29568\n```", "```py\n# For each row in the train and test set, we will create a list of integers\n# that will represent the words in the text\ntrain['text_int'] = train['TEXT'].apply(lambda x: [word2idx.get(word, word2idx['<UNK>']) for word in x.split()])\ntest['text_int'] = test['TEXT'].apply(lambda x: [word2idx.get(word, word2idx['<UNK>']) for word in x.split()])\n\n# Calculating the length of sequences in the train set \ntrain['seq_len'] = train['text_int'].apply(lambda x: len(x))\n\n# Describing the length of the sequences\ntrain['seq_len'].describe()\ncount    34410.000000\nmean        21.825574\nstd         17.707891\nmin          0.000000\n25%          9.000000\n50%         17.000000\n75%         31.000000\nmax        312.000000\n```", "```py\ndef pad_sequences(x: list, pad_length: int) -> list:\n    \"\"\"\n    Function that pads a given list of integers to a given length\n\n    Args:\n        x (list): list of integers to pad\n        pad_length (int): length to pad\n\n    Returns:\n        list: padded list of integers\n    \"\"\"\n    # Getting the length of the list\n    len_x = len(x)\n\n    # Checking if the length of the list is less than the pad_length\n    if len_x < pad_length: \n        # Padding the list with 0s\n        x = x + [0] * (pad_length - len_x)\n    else: \n        # Truncating the list to the desired length\n        x = x[:pad_length]\n\n    # Returning the padded list\n    return x\n```", "```py\n# Padding the train and test sequences \ntrain['text_int'] = train['text_int'].apply(lambda x: pad_sequences(x, 30))\ntest['text_int'] = test['text_int'].apply(lambda x: pad_sequences(x, 30))\n```", "```py\n# Importing the needed packages\nimport torch \nfrom torch import nn\n\n# Defining the torch model for sentiment classification \nclass SentimentClassifier(torch.nn.Module):\n    \"\"\"\n    Class that defines the sentiment classifier model\n    \"\"\"\n    def __init__(self, vocab_size, embedding_dim):\n        super(SentimentClassifier, self).__init__()\n\n        self.embedding = nn.Embedding(vocab_size + 1, embedding_dim)\n        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=1, batch_first=True)\n        self.fc = nn.Linear(1, 1)  # Output with a single neuron for binary classification\n        self.sigmoid = nn.Sigmoid()  # Sigmoid activation\n\n    def forward(self, x):\n        x = self.embedding(x)  # Embedding layer\n        output, _ = self.rnn(x)  # RNN layer\n\n        # Leaving only the last step of the RNN sequence\n        x = output[:, -1, :]\n\n        # Fully connected layer with a single neuron\n        x = self.fc(x) \n\n        # Converting to probabilities\n        x = self.sigmoid(x)\n\n        # Flattening the output\n        x = x.squeeze()\n\n        return x\n\n# Initiating the model \nmodel = SentimentClassifier(vocab_size=len(word2idx), embedding_dim=16)\n\n# Initiating the criterion and the optimizer\ncriterion = nn.BCELoss() # Binary cross entropy loss\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n```", "```py\n# Defining the data loader \nfrom torch.utils.data import Dataset, DataLoader\n\nclass TextClassificationDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        # The x is named as text_int and the y as airline_sentiment\n        x = self.data.iloc[idx]['text_int']\n        y = self.data.iloc[idx]['SENTIMENT']\n\n        # Converting the x and y to torch tensors\n        x = torch.tensor(x)\n        y = torch.tensor(y)\n\n        # Converting the y variable to float \n        y = y.float()\n\n        # Returning the x and y\n        return x, y\n\n# Creating the train and test loaders\ntrain_loader = DataLoader(TextClassificationDataset(train), batch_size=32, shuffle=True)\ntest_loader = DataLoader(TextClassificationDataset(test), batch_size=32, shuffle=True)\n```", "```py\n# Defining the number of epochs\nepochs = 100\n\n# Setting the model to train mode\nmodel.train()\n\n# Saving of the loss values\nlosses = []\n\n# Iterating through epochs\nfor epoch in range(epochs):\n    # Initiating the total loss \n    total_loss = 0\n\n    for batch_idx, (inputs, labels) in enumerate(train_loader):\n        # Zero the gradients\n        optimizer.zero_grad()  # Zero the gradients\n        outputs = model(inputs)  # Forward pass\n\n        loss = criterion(outputs, labels)  # Compute the loss\n        loss.backward()  # Backpropagation\n        optimizer.step()  # Update the model's parameters\n\n        # Adding the loss to the total loss\n        total_loss += loss.item()\n\n    # Calculating the average loss\n    avg_loss = total_loss / len(train_loader)\n\n    # Appending the loss to the list containing the losses\n    losses.append(avg_loss)\n\n    # Printing the loss every n epochs\n    if epoch % 20 == 0:\n        print(f'Epoch: {epoch}, Loss: {avg_loss}')\n```", "```py\n# Ploting the loss \nimport matplotlib.pyplot as plt\n\nplt.plot(losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Loss vs. Epoch')\nplt.show()\n```", "```py\n# Setting the model to eval model\nmodel.eval()\n\n# List to track the test acc \ntotal_correct = 0\ntotal_obs = 0\n\n# Iterating over the test set\nfor batch_idx, (inputs, labels) in enumerate(test_loader):\n    outputs = model(inputs)  # Forward pass\n\n    # Getting the number of correct predictions \n    correct = ((outputs > 0.5).float() == labels).float().sum()\n\n    # Getting the total number of predictions\n    total = labels.size(0)\n\n    # Updating the total correct and total observations\n    total_correct += correct\n    total_obs += total\n\nprint(f'The test accuracy is: {total_correct / total_obs}')\nThe test accuracy is: 0.7420667409896851\n```"]