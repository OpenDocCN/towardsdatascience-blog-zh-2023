- en: Program-Aided Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 程序辅助语言模型
- en: 原文：[https://towardsdatascience.com/program-aided-language-models-93d226c7d9a0](https://towardsdatascience.com/program-aided-language-models-93d226c7d9a0)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/program-aided-language-models-93d226c7d9a0](https://towardsdatascience.com/program-aided-language-models-93d226c7d9a0)
- en: LLMs can write code, but what if they can execute programs?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）可以编写代码，但如果它们能执行程序呢？
- en: '[](https://wolfecameron.medium.com/?source=post_page-----93d226c7d9a0--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----93d226c7d9a0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----93d226c7d9a0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----93d226c7d9a0--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----93d226c7d9a0--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wolfecameron.medium.com/?source=post_page-----93d226c7d9a0--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----93d226c7d9a0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----93d226c7d9a0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----93d226c7d9a0--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----93d226c7d9a0--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----93d226c7d9a0--------------------------------)
    ·18 min read·Aug 22, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----93d226c7d9a0--------------------------------)
    ·18分钟阅读·2023年8月22日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/3947104a20288bfb49a2b532e471fde9.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3947104a20288bfb49a2b532e471fde9.png)'
- en: (Photo by [Florian Olivo](https://unsplash.com/@florianolv?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/4hbJ-eymZ1o?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: (照片由 [Florian Olivo](https://unsplash.com/@florianolv?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供于 [Unsplash](https://unsplash.com/photos/4hbJ-eymZ1o?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
- en: Although Large Language Models (LLMs) are used for a variety of applications,
    they have typically struggled to solve reasoning-based tasks. This issue was significantly
    diminished with the advent of prompting techniques like [Chain of Thought](https://cameronrwolfe.substack.com/p/chain-of-thought-prompting-for-llms)
    and [Least-to-Most prompting](https://cameronrwolfe.substack.com/i/116166267/variants-of-cot-prompting).
    At a high level, these techniques encourage reasoning behavior in LLMs by providing
    examples of problem-solving rationales within the model’s prompt. Then, the model
    can learn to output such rationales and produce a step-by-step solution to the
    underlying problem. Notably, this is a prompting-only approach that requires no
    fine-tuning, revealing that LLMs are capable of reasoning given a prompt with
    sufficient context.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大型语言模型（LLMs）被用于多种应用，但它们通常在解决基于推理的任务时遇到困难。随着[链式思维](https://cameronrwolfe.substack.com/p/chain-of-thought-prompting-for-llms)和[从少到多提示](https://cameronrwolfe.substack.com/i/116166267/variants-of-cot-prompting)等提示技术的出现，这一问题显著减轻了。从高层次来看，这些技术通过在模型的提示中提供问题解决的例子来鼓励LLMs进行推理行为。然后，模型可以学会输出这些推理过程，并逐步解决潜在的问题。值得注意的是，这是一种仅依赖提示的方法，无需微调，显示了LLMs在给定足够上下文的提示时具备推理能力。
- en: Despite the effectiveness of techniques like chain of thought prompting, the
    LLM is expected to produce both a problem-solving chain of thought and a final
    answer. Interestingly, such an approach leads to peculiar failure cases in which
    the LLM may produce an accurate rationale for solving a problem but still generate
    an answer that is incorrect. Usually, such errors are due to simple mistakes (e.g.,
    poor arithmetic). To solve this problem, recent research has explored a programatic
    approach that encourages the LLM to generate chains of thought with both natural
    language and code components. Then, the LLM can run this code via an external
    interpreter to obtain needed outputs.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管链式思维提示等技术效果显著，但LLM预计需要生成问题解决的思维链和最终答案。有趣的是，这种方法可能导致一些特殊的失败情况，其中LLM可能生成准确的解决问题的推理，但仍会给出错误的答案。通常，这些错误是由于简单的错误（例如，计算错误）造成的。为了解决这个问题，最近的研究探讨了一种程序化的方法，鼓励LLM生成包含自然语言和代码组件的思维链。然后，LLM可以通过外部解释器运行这些代码，以获得所需的输出。
- en: To understand why such an approach would be useful, we should note that many
    issues with which LLMs struggle (e.g., arithmetic errors, inability to evaluate
    complex expressions, etc.) can be easily expressed and solved inside of a program.
    As a result, using chain of thought-style prompts on LLMs with coding abilities
    (e.g., Codex) allows us to merge the benefits of LLMs with the computational capabilities
    of an arbitrary Python program! More specifically, the LLM can be encouraged to
    generate a problem-solving rationale that contains both natural language and code
    components, producing a script that can be run by an external interpreter to compute
    the final output for a problem. Such an approach, which we will explore in this
    overview, is massively beneficial to the accuracy and reliability of LLMs in solving
    reasoning-based tasks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这种方法为什么有用，我们应该注意到许多LLM难以解决的问题（例如，算术错误、无法评估复杂表达式等）可以在程序内部轻松表达和解决。因此，使用链式思维风格的提示在具有编码能力的LLM（例如，Codex）上，可以将LLM的优势与任意Python程序的计算能力相结合！更具体地说，LLM可以被鼓励生成包含自然语言和代码组件的问题解决理由，生成一个可以由外部解释器运行的脚本，以计算问题的最终输出。我们将在本概述中探讨这种方法，这对LLM在解决推理任务中的准确性和可靠性大有裨益。
- en: '![](../Images/dd2c8071bf42e824eb7422a2b56fc2cb.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd2c8071bf42e824eb7422a2b56fc2cb.png)'
- en: (from [1, 2])
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1, 2]）
- en: Background Information
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景信息
- en: '![](../Images/2eda2e32fc3f346f8a7776e7925ddcf8.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2eda2e32fc3f346f8a7776e7925ddcf8.png)'
- en: Pre-training an LLM
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练一个大语言模型（LLM）
- en: Despite the incredible abilities of modern LLMs, these models are all based
    upon a simple pre-training procedure that performs [next-token prediction](https://cameronrwolfe.substack.com/i/85568430/language-modeling)
    over a large amount of unlabeled textual data. Although we can tweak the details
    of this procedure (e.g., the [type](https://arxiv.org/abs/2211.09085) or [mixture](https://arxiv.org/abs/2305.10429)
    of data being used), the fundamental pre-training approach of most LLMs remains
    fixed. We just simply *i)* sample some text from the pre-training corpus and *ii)*
    teach the model to accurately predict the next word/token in the corpus. *That’s
    it!* This simple and profound approach lays the foundation for all of modern language
    modeling.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管现代大语言模型具有令人难以置信的能力，但这些模型都基于一个简单的预训练程序，该程序对大量未标记的文本数据执行[下一个词预测](https://cameronrwolfe.substack.com/i/85568430/language-modeling)。虽然我们可以调整这个过程的细节（例如，[数据的类型](https://arxiv.org/abs/2211.09085)或[混合](https://arxiv.org/abs/2305.10429)），但大多数LLM的基本预训练方法保持不变。我们只需*
    i)* 从预训练语料库中采样一些文本，* ii)* 教会模型准确预测语料库中的下一个词/标记。*就是这样！* 这个简单而深刻的方法为现代语言建模奠定了基础。
- en: 'But…there are a few more tricks and lessons learned from years of research
    that allow us to make language models as powerful as [ChatGPT](https://openai.com/blog/chatgpt)
    or [GPT-4](https://openai.com/research/gpt-4). Most models use the same [decoder-only
    architecture](https://twitter.com/cwolferesearch/status/1649476511248818182?s=20),
    but creating a high-performing language model cannot be done via pre-training
    alone. We need:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 但……还有一些从多年的研究中学到的技巧和经验，使我们能够让语言模型变得像[ChatGPT](https://openai.com/blog/chatgpt)或[GPT-4](https://openai.com/research/gpt-4)一样强大。大多数模型使用相同的[仅解码器架构](https://twitter.com/cwolferesearch/status/1649476511248818182?s=20)，但仅仅通过预训练无法创建高性能的语言模型。我们需要：
- en: Sufficient scale (i.e., large model and pre-training dataset).
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 足够的规模（即，大模型和预训练数据集）。
- en: Behavioral refinement via [supervised fine-tuning (SFT)](https://cameronrwolfe.substack.com/i/93578656/lamda-language-modeling-for-dialog-applications)
    and [reinforcement learning from human feedback (RLHF)](https://cameronrwolfe.substack.com/i/93578656/training-language-models-to-follow-instructions-with-human-feedback)
    [11, 12].
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过[监督微调（SFT）](https://cameronrwolfe.substack.com/i/93578656/lamda-language-modeling-for-dialog-applications)和[来自人类反馈的强化学习（RLHF）](https://cameronrwolfe.substack.com/i/93578656/training-language-models-to-follow-instructions-with-human-feedback)进行行为调整[11,
    12]。
- en: '*[Optional]* Domain Specialization (i.e., [fine-tuning the model](https://cameronrwolfe.substack.com/i/93578656/refining-llm-behavior)
    on a specific type of data, such as code or dialogue).'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*[可选]* 领域专业化（即，[在特定类型的数据上微调模型](https://cameronrwolfe.substack.com/i/93578656/refining-llm-behavior)，例如代码或对话）。'
- en: If we perform all of these steps correctly, we can create a powerful [foundation
    model](https://crfm.stanford.edu/) that is capable of solving a variety of tasks
    via textual prompts. Notably, most of a language model’s knowledge and information
    is learned via pre-training (see “Training process” section [here](https://openai.com/research/gpt-4)),
    but these extra refinement steps taken after pre-training make LLMs much more
    [steerable](https://www.marktechpost.com/2023/04/14/how-does-gpt-4s-steerable-nature-set-it-apart-from-the-previous-large-language-models-llms/)
    and interesting; see below.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们正确执行所有这些步骤，我们可以创建一个强大的[基础模型](https://crfm.stanford.edu/)，它能够通过文本提示解决各种任务。值得注意的是，大多数语言模型的知识和信息都是通过预训练获得的（见“训练过程”部分
    [这里](https://openai.com/research/gpt-4)），但在预训练之后进行的这些额外的精细化步骤使LLMs变得更具[可引导性](https://www.marktechpost.com/2023/04/14/how-does-gpt-4s-steerable-nature-set-it-apart-from-the-previous-large-language-models-llms/)和更有趣；见下文。
- en: '![](../Images/8fb50c37ca0347a287d145b3dfcc43a0.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8fb50c37ca0347a287d145b3dfcc43a0.png)'
- en: (from [11])
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [11])
- en: '**What are LLMs bad at?** Language models achieve impressive performance in
    a variety of different applications, but they are not perfect. These models have
    known limitations, such as:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**LLMs在什么方面表现不佳？** 语言模型在各种不同的应用中取得了令人印象深刻的表现，但它们并不完美。这些模型有已知的局限性，例如：'
- en: Difficulty adding large numbers
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加大数的困难
- en: Inability to evaluate/solve complex equations
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无法评估/解决复杂方程
- en: Trouble with reasoning over iterative processes
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对迭代过程的推理困难
- en: For example, if we prompt an LLM with a description of the [Fibonacci sequence](https://www.mathsisfun.com/numbers/fibonacci-sequence.html)
    then ask it to compute the 100th number, there is a high likelihood that it will
    fail! *Why is this the case?* Well, we know that LLMs struggle with performing
    arithmetic, and solving the Fibonacci sequence (unless the model uses brute force
    memorization) requires many, iterative additions between two numbers. If the model
    has a 95% chance of performing this addition correctly during each iteration,
    then the 100th Fibonacci number has a <1% chance of being correct!
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们给大型语言模型（LLM）提供一个关于[Fibonacci 数列](https://www.mathsisfun.com/numbers/fibonacci-sequence.html)的描述，然后要求它计算第100个数字，那么它很可能会失败！*为什么会这样？*
    好吧，我们知道LLMs在进行算术运算时表现不佳，而解决Fibonacci数列（除非模型使用暴力记忆）需要在两个数字之间进行多次迭代加法。如果模型在每次迭代中有95%的概率正确执行加法，那么第100个Fibonacci数正确的概率不到1%！
- en: '**Quick disclaimer.** The recent release of GPT-4 has made claims about LLM
    limitations more difficult to make. For example, GPT-4 is completely capable of
    solving for the 100th Fibonacci number and can even evaluate some (relatively)
    complex equations with minimal prompting effort; see below.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**快速免责声明。** 最近发布的GPT-4使得关于LLM局限性的声明变得更加困难。例如，GPT-4完全能够解决第100个Fibonacci数，甚至可以在最小提示努力下评估一些（相对）复杂的方程；见下文。'
- en: '![](../Images/42e7bc2437f4990d89f19d157899d625.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/42e7bc2437f4990d89f19d157899d625.png)'
- en: (from ChatGPT Plus)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 ChatGPT Plus)
- en: With this in mind, any statement about LLM capabilities needs to be taken with
    a grain of salt. This space is rapidly evolving and models are becoming more and
    more capable and impressive (literally) every day.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于此，对LLM能力的任何声明都需要保持一定的怀疑态度。这个领域迅速发展，模型每天都变得越来越强大和令人印象深刻（字面意义上）。
- en: Teaching LLMs how to code
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 教授LLMs如何编码
- en: As mentioned above, one (optional) part of creating a high-performing LLM is
    [domain specialization](https://cameronrwolfe.substack.com/p/specialized-llms-chatgpt-lamda-galactica).
    After pre-training, LLMs are quite generic and capable of only a single task —
    *next-token prediction*. If we want an LLM that is specialized within a certain
    domain or great at performing a specific task (e.g., [information-seeking dialogue](https://www.deepmind.com/blog/building-safer-dialogue-agents)
    or [writing screenplays](https://deepmind.github.io/dramatron/)), we need to fine-tune
    the model on a lot of data that demonstrates correct behavior for this task. One
    of the most successful applications of this technique, which is especially relevant
    to this overview, is for creating language models that can write code.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，创建高性能LLM的一个（可选）部分是[领域专业化](https://cameronrwolfe.substack.com/p/specialized-llms-chatgpt-lamda-galactica)。在预训练之后，LLMs相当通用，仅能完成单一任务——*下一个标记预测*。如果我们想要一个在某个特定领域专业化或擅长执行特定任务（例如，[信息检索对话](https://www.deepmind.com/blog/building-safer-dialogue-agents)或[编写剧本](https://deepmind.github.io/dramatron/)）的LLM，我们需要在大量展示该任务正确行为的数据上进行微调。这个技术的一个最成功的应用，特别是与此概述相关，是创建可以编写代码的语言模型。
- en: '![](../Images/28a6655f0a238e3b578179e19d7b6ac5.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/28a6655f0a238e3b578179e19d7b6ac5.png)'
- en: (from [4])
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [4])
- en: Similarly to how large amounts of textual data can be downloaded from the internet
    for pre-training a language model normally, we can download large amounts code
    from public sources (e.g., GitHub) for training LLMs, which makes coding a particularly
    perfect application of specialized LLMs. One notable example of such a model is
    [Codex](https://cameronrwolfe.substack.com/i/93578656/evaluating-large-language-models-trained-on-code)
    [4], which is trained using a combination of unlabeled textual data and code downloaded
    from the internet. Given a Python docstring, Codex is tasked with generating a
    working Python function that performs the task outlined in the docstring; see
    above.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于如何从互联网下载大量文本数据用于预训练语言模型，我们可以从公共来源（例如GitHub）下载大量代码用于训练LLMs，这使得编码成为专门化LLMs的一个特别完美的应用。例如，[Codex](https://cameronrwolfe.substack.com/i/93578656/evaluating-large-language-models-trained-on-code)
    [4]就是一个显著的模型，它使用从互联网下载的未标记文本数据和代码的组合进行训练。给定一个Python文档字符串，Codex的任务是生成一个有效的Python函数，以执行文档字符串中概述的任务；见上文。
- en: '![](../Images/1c95ea948206fa8858f91e62b685afa5.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c95ea948206fa8858f91e62b685afa5.png)'
- en: (from [4])
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [4])
- en: Codex performs incredibly well on human-curated coding tasks (see above) and
    is even used to power the [GitHub Copilot](https://github.com/features/copilot)
    coding assistant, revealing that LLMs can be applied to more than just natural
    language! *We can also apply them to many other problems that follow a similar
    structure*. In this case, we use further language model pre-training over a code
    dataset to adapt a pre-trained LLM to a new domain. Notably, Codex is capable
    of generating both code and natural language-based output, making it a particular
    versatile and useful LLM. Plus, creating this domain-specific model is relatively
    simple — *we just need a lot of code for training.*
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Codex在人工策划的编码任务上表现极佳（见上文），甚至被用来驱动[GitHub Copilot](https://github.com/features/copilot)编码助手，揭示了LLMs不仅仅可以应用于自然语言！*我们也可以将它们应用于许多其他具有类似结构的问题*。在这种情况下，我们使用进一步的语言模型预训练来适应预训练LLM到新领域。值得注意的是，Codex能够生成代码和自然语言输出，使其成为一个特别多用途且有用的LLM。而且，创建这种领域特定的模型相对简单——*我们只需要大量的代码进行训练。*
- en: Chain of Thought (CoT) Prompting
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 思维链 (CoT) 提示
- en: Beyond the limitations outlined previously, LLMs were initially criticized for
    their inability to solve reasoning tasks. However, research in this area has led
    to breakthrough techniques like [CoT prompting](https://cameronrwolfe.substack.com/p/chain-of-thought-prompting-for-llms)
    [3] that enable LLMs to solve reasoning-based tasks quite accurately. The idea
    behind CoT prompting is simple. We just use [few-shot learning](https://cameronrwolfe.substack.com/i/117151147/few-shot-learning)
    to teach the LLM how to output a problem-solving rationale that explains its answer
    — in detail — for any reasoning task; see below. Such an approach is incredibly
    practical because we only have to generate a few examples of problem-solving rationales
    to include in the prompt, whereas prior work curated entire datasets of such rationales
    for [fine-tuning](https://cameronrwolfe.substack.com/i/116166267/can-we-solve-reasoning-with-scale).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 超越之前概述的限制，LLMs最初因无法解决推理任务而受到批评。然而，该领域的研究带来了突破性的技术，如[CoT提示](https://cameronrwolfe.substack.com/p/chain-of-thought-prompting-for-llms)
    [3]，使LLMs能够相当准确地解决基于推理的任务。CoT提示的理念很简单。我们只需使用[少量样本学习](https://cameronrwolfe.substack.com/i/117151147/few-shot-learning)来教LLM如何输出详细解释其答案的解决方案——适用于任何推理任务；见下文。这种方法极其实用，因为我们只需要生成少量解决方案示例来包含在提示中，而之前的工作则编纂了整个数据集用于[微调](https://cameronrwolfe.substack.com/i/116166267/can-we-solve-reasoning-with-scale)。
- en: '![](../Images/1d38f42c67fdb0620ca7d375afadf394.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d38f42c67fdb0620ca7d375afadf394.png)'
- en: (from [3])
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [3])
- en: Unlike teaching an LLM how to code, we see with CoT prompting that such models
    are capable of solving reasoning tasks without any fine-tuning! Instead, we just
    need to adopt a better prompting approach that “unlocks” the LLM’s ability to
    solve complex reasoning tasks.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 与教LLM如何编码不同，我们通过CoT提示发现，这些模型能够在无需任何微调的情况下解决推理任务！相反，我们只需采用一种更好的提示方法来“解锁”LLM解决复杂推理任务的能力。
- en: “Large pretrained language models have built in reasoning capabilities, but
    they need specific prompts to unleash their power.” *— from [13]*
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “大型预训练语言模型具备内置的推理能力，但它们需要特定的提示才能释放其威力。” *— 来自 [13]*
- en: 'Given that we have learned a lot about CoT prompting and its [many variants](https://cameronrwolfe.substack.com/i/116166267/variants-of-cot-prompting)
    in prior overviews, I will not extensively explore the idea here. However, there
    is one notable aspect of CoT prompting that we should notice — the LLM is expected
    to both *i)* generate a chain of thought and *ii)* extract the final answer from
    this chain of thought. Although CoT prompting is effective, we might begin to
    wonder: *is relying on the LLM to accurately solve both of these steps actually
    a good idea?*'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们在之前的概述中已经了解了很多关于CoT提示及其[许多变体](https://cameronrwolfe.substack.com/i/116166267/variants-of-cot-prompting)，我不会在这里深入探讨这个概念。然而，有一个显著的方面我们应该注意——语言模型被期望同时*i)*
    生成思维链和*ii)* 从这个思维链中提取最终答案。尽管CoT提示是有效的，但我们可能会开始怀疑：*依赖语言模型准确解决这两个步骤是否真的是一个好主意？*
- en: Decoupling Reasoning and Computation within LLMs
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在语言模型中解耦推理和计算
- en: We know that language models are capable (given a correct prompting approach)
    of providing an accurate problem-solving rationale or detailed explanation of
    their output. However, generating a correct rationale does not mean the LLM will
    solve a problem correctly! *What if the LLM makes a small arithmetic error when
    producing its final answer?* Given fundamental limitations of LLMs, techniques
    like CoT prompting commonly encounter frustrating failure cases in which the model
    produces an accurate rationale but outputs an incorrect, final answer. Errors
    of this kind are typically referred to as the compositionality gap for LLMs.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，语言模型（在正确的提示方法下）能够提供准确的问题解决理由或详细的输出解释。然而，生成正确的理由并不意味着语言模型会正确解决问题！*如果语言模型在给出最终答案时出现一个小的算术错误怎么办？*
    由于语言模型的基本局限性，像CoT提示这样的技术通常会遇到令人沮丧的失败案例，其中模型生成了准确的理由，但输出了错误的最终答案。这类错误通常被称为语言模型的组合性差距。
- en: “We measure how often models can correctly answer all subproblems but not generate
    the overall solution, a ratio we call the compositionality gap.” *— from [16]*
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我们衡量模型可以正确回答所有子问题但未能生成整体解决方案的频率，这一比率称为组合性差距。” *—— 引自 [16]*
- en: Within this section, we will explore recent research that attempts to solve
    this issue by leveraging the unique skills of LLMs that have been trained on code
    (e.g., [Codex](https://cameronrwolfe.substack.com/i/93578656/evaluating-large-language-models-trained-on-code)
    [4]) to write coherent and functional programs. We can rely on LLMs to generate
    a problem-solving rationale. But, instead of asking the LLM to produce an actual
    answer, we just prompt the model to generate a program associated with the rationale
    that, when executed using a separate code interpreter, can generate the final
    answer. Thus, our rationale becomes a hybrid between code and language — *basically
    a Python script with informative comments*!
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨最近的研究，这些研究尝试通过利用已在代码上进行训练的语言模型的独特技能（例如，[Codex](https://cameronrwolfe.substack.com/i/93578656/evaluating-large-language-models-trained-on-code)
    [4]）来编写连贯且功能性强的程序来解决这个问题。我们可以依靠语言模型生成问题解决的理由。但是，我们不是要求语言模型给出实际的答案，而是提示模型生成一个与理由相关的程序，这个程序在使用单独的代码解释器执行时，可以生成最终答案。因此，我们的理由变成了代码和语言的混合体——*基本上是一个带有说明性评论的Python脚本*！
- en: Program-Aided Language Models (PaL)
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 程序辅助语言模型（PaL）
- en: '![](../Images/c733aa48f01f10f726cf1ffa395e4ebb.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c733aa48f01f10f726cf1ffa395e4ebb.png)'
- en: (from [1])
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: In [1], authors propose a CoT-inspired technique, called Program-Aided Language
    Models (PaL), that uses an LLM to decompose reasoning-based problems into a step-by-step,
    problem solving rationale. However, this rationale contains both natural language
    and (Python-based) programatic components. After such a hybrid rationale has been
    generated, we can solve the problem by executing the program-based portion of
    the prompt via a Python interpreter. The goal of such an approach is to eliminated
    instances where the LLM generates a correct reasoning chain but still produces
    an incorrect final answer.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [1] 中，作者提出了一种受CoT启发的技术，称为程序辅助语言模型（PaL），该技术使用语言模型将基于推理的问题分解为逐步的问题解决理由。然而，这种理由包含了自然语言和（基于Python的）编程组件。生成这种混合理由后，我们可以通过Python解释器执行程序化部分来解决问题。这种方法的目标是消除语言模型生成正确推理链但仍产生错误最终答案的情况。
- en: “This bridges an important gap in chain-of-thought-like methods, where reasoning
    chains can be correct but produce an incorrect answer.” *— from [1]*
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “这弥合了链式思维方法中的一个重要差距，即推理链可能是正确的，但产生了错误的答案。” *—— 引自 [1]*
- en: With PaL, we can use the LLM to generate a problem-solving rationale, but the
    process of computing the final solution (i.e., this is the part with which models
    typically struggle!) is delegated to a code interpreter, eliminating the potential
    for arithmetic or logical mistakes. As a result, the LLM only needs to learn how
    to generate a problem-solving rationale — *the solution is derived programatically*.
    We can teach the LLM to generate such a hybrid rational via few-shot learning.
    For this to work, however, we need an LLM that has been pre-trained on both natural
    language and code (e.g., [Codex](https://cameronrwolfe.substack.com/i/93578656/evaluating-large-language-models-trained-on-code)
    [4]).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 PaL，我们可以利用 LLM 生成解决问题的推理，但计算最终解决方案的过程（即，模型通常在这一部分挣扎的地方！）被委托给代码解释器，从而消除了算术或逻辑错误的潜在可能性。因此，LLM
    只需学习如何生成解决问题的推理——*解决方案是程序化得出的*。我们可以通过少量学习教导 LLM 生成这种混合推理。然而，为了实现这一点，我们需要一个在自然语言和代码上都经过预训练的
    LLM（例如，[Codex](https://cameronrwolfe.substack.com/i/93578656/evaluating-large-language-models-trained-on-code)
    [4]）。
- en: '**Understanding PaL.** At a high level, the approach adopted by PaL is quite
    similar to CoT prompting. We use a few-shot prompting approach that provides several
    examples of problems being decomposed into an associated rationale. The main difference
    between CoT and PaL is that the rationales used within PaL are comprised of interleaved
    natural language and programatic statements; see below.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**理解 PaL。** 从高层次来看，PaL 采用的方法与 CoT 提示非常相似。我们使用一种少量提示的方法，提供几个将问题分解为相关推理的示例。CoT
    和 PaL 之间的主要区别在于，PaL 使用的推理是由交错的自然语言和程序语句组成的；见下文。'
- en: '![](../Images/709cab7730ad1126eba8cdb27d518a8e.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/709cab7730ad1126eba8cdb27d518a8e.png)'
- en: (from [1])
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: Each step in the reasoning process with PaL is augmented with a programatic
    statement. Then, when these programatic statements are synthesized, they can be
    executed via a separate Python interpreter to generate a final answer (i.e., done
    via a single, post-hoc execution). PaL is teaching the LLM (via few-shot learning)
    to generate a program that solves the desired question in a step-by-step manner.
    Interestingly, authors in [1] encourage the LLM to generate natural language-based
    intermediate steps by exploiting the Python comment syntax (i.e., the `#` character),
    which enables language-based components to be inserted into a generated program.
    In other words, we are teaching the LLM to solve reasoning task via a step-by-step
    program with informative comments!
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: PaL 中的每一步推理过程都附加了程序语句。然后，当这些程序语句被综合时，它们可以通过单独的 Python 解释器执行，以生成最终答案（即，通过单次、事后的执行完成）。PaL
    正在通过少量学习教导 LLM 生成一个逐步解决所需问题的程序。有趣的是，[1]中的作者鼓励 LLM 通过利用 Python 注释语法（即 `#` 字符）生成基于自然语言的中间步骤，这使得语言组件能够插入到生成的程序中。换句话说，我们正在教导
    LLM 通过逐步的程序和信息性注释来解决推理任务！
- en: '![](../Images/217f17c7239da3f0e14491f23e04099c.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/217f17c7239da3f0e14491f23e04099c.png)'
- en: (from [1])
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: Unlike CoT prompting, few-shot examples used by PaL contain no final solution.
    Rather, exemplars are just programs with interleaved natural language statements
    (and nothing else!). The generation of the final solution is delegated to the
    Python interpreter, so the LLM never needs to learn how to perform this step;
    see above.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 与 CoT 提示不同，PaL 使用的少量示例不包含最终解决方案。相反，示例仅仅是交错了自然语言语句的程序（没有其他东西！）。最终解决方案的生成委托给 Python
    解释器，因此 LLM 不需要学习如何执行这一步骤；见上文。
- en: Going further, authors in [1] observe that providing meaningful names to variables
    used within the program is beneficial. Such a finding indicates that the reasoning
    process proposed by PaL is a true hybrid approach that merges language and program-based
    components. Forming symbolic links between entities within programming and language
    modalities is important; see below.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 更进一步，[1]中的作者观察到，为程序中使用的变量提供有意义的名称是有益的。这一发现表明，PaL提出的推理过程是一种真正的混合方法，它融合了语言和程序组件。在编程和语言模式之间形成符号链接是重要的；见下文。
- en: '![](../Images/aba657282f9928b07086041c539d0827.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aba657282f9928b07086041c539d0827.png)'
- en: (from [1])
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: '**Does this work well?** PaL is evaluated on a variety of symbolic, mathematical,
    and algorithmic reasoning tasks, where it is shown to mitigate many of the common
    problems encountered with CoT prompting. The proposed method is compared to both
    standard [few-shot learning](https://cameronrwolfe.substack.com/i/117151147/few-shot-learning)
    (called “direct” prompting in [1]) and CoT prompting. On mathematical reasoning
    tasks, PaL with [Codex](https://cameronrwolfe.substack.com/i/93578656/evaluating-large-language-models-trained-on-code)
    [4] easily outperforms prior prompting approaches with a variety of different
    models. Notably, PaL even outperforms [Minerva](https://ai.googleblog.com/2022/06/minerva-solving-quantitative-reasoning.html)
    [5], an LLM that was explicitly fine-tuned over a large amount of quantitative
    reasoning data; see below.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**这效果好吗？** PaL 在各种符号、数学和算法推理任务中进行了评估，结果显示它能够减轻许多与 CoT 提示相关的常见问题。所提出的方法与标准的
    [少量学习](https://cameronrwolfe.substack.com/i/117151147/few-shot-learning)（在 [1]
    中称为“直接”提示）以及 CoT 提示进行了比较。在数学推理任务中，PaL 与 [Codex](https://cameronrwolfe.substack.com/i/93578656/evaluating-large-language-models-trained-on-code)
    [4] 相结合，轻松超越了之前的提示方法，适用于各种不同的模型。值得注意的是，PaL 甚至超越了 [Minerva](https://ai.googleblog.com/2022/06/minerva-solving-quantitative-reasoning.html)
    [5]，这是一种专门针对大量定量推理数据进行微调的 LLM；见下文。'
- en: '![](../Images/739fd3a2acc9dc2edd4174b32c4152f2.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/739fd3a2acc9dc2edd4174b32c4152f2.png)'
- en: (from [1])
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: From the table above, we should also notice that PaL using Codex achieves state-of-the-art
    performance on GSM8K, surpassing the performance of [PaLM-540B](https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive)
    (i.e., much larger model!) with CoT by 15% absolute top-1 accuracy. Interestingly,
    authors in [1] point out that `GSM8K` is mostly focused upon math word problems
    with smaller numbers (i.e., 50% of numbers are between 0-8) and propose `GSM-Hard`—a
    version of this dataset with larger numbers. On the harder dataset, PaL achieves
    a nearly 40% improvement in absolute top-1 accuracy relative to PaLM with CoT
    prompting, revealing that program-aided prompting is superior for problems that
    require complex arithmetic with large numbers.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 从上表中，我们还应注意到，使用 Codex 的 PaL 在 GSM8K 上达到了最先进的性能，超越了 [PaLM-540B](https://cameronrwolfe.substack.com/p/palm-efficiently-training-massive)（即更大的模型！）的
    CoT 性能 15% 的绝对 top-1 准确率。有趣的是，[1] 中的作者指出，`GSM8K` 主要集中在较小数字的数学词题上（即，50% 的数字在 0-8
    之间），并提出了 `GSM-Hard`——这是一个包含更大数字的数据集版本。在更困难的数据集上，PaL 相较于 PaLM 的 CoT 提示在绝对 top-1
    准确率上提高了近 40%，揭示了程序辅助提示对于需要复杂算术运算的大数字问题更具优势。
- en: '![](../Images/394d04336e150b5de4dcab65cc4549e1.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/394d04336e150b5de4dcab65cc4549e1.png)'
- en: (from [1])
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: On symbolic and algorithmic reasoning tasks, PaL again provides a significant
    benefit; see above. In fact, PaL comes close to completely solving four of the
    five datasets in this category, achieving accuracy >90%. Furthermore, PaL seems
    to maintain consistent performance as the complexity of questions increases; see
    below. Here, we see that added complexity in the form of large numbers or more
    objects within a reasoning task is simple to handle programmatically, though handling
    such complexity directly with an LLM might cause issues.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在符号和算法推理任务中，PaL 再次提供了显著的好处；见上文。实际上，PaL 在这个类别中的五个数据集中接近完全解决四个，达到了 >90% 的准确率。此外，PaL
    似乎随着问题复杂性的增加而保持一致的表现；见下文。在这里，我们可以看到，大量数字或推理任务中的更多对象所带来的复杂性在程序处理上很简单，尽管直接用 LLM
    处理这样的复杂性可能会引发问题。
- en: '![](../Images/a432ec723c109d42f8a56a7fa34e6c76.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a432ec723c109d42f8a56a7fa34e6c76.png)'
- en: (from [1])
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: Program of Thoughts (PoT) Prompting
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 思维程序（PoT）提示
- en: 'As previously mentioned, the reasoning process with CoT prompting has two distinct
    steps:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，CoT 提示的推理过程有两个不同的步骤：
- en: Generating a language (or program)-based solution rationale
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成基于语言（或程序）的解决方案 rationale
- en: Computing the final answer according to this rationale
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据这一 rationale 计算最终答案
- en: LLMs excel at performing the first step outlined above, but they may struggle
    with computing the final answer. Oftentimes, this issue occurs due to an arithmetic
    error or inability to evaluate a complex expression. Put simply, *LLMs struggle
    with complex numerical tasks*. In [2], authors aim to leverage a code-augmented
    prompting approach, called Program of Thoughts (PoT) prompting, to mitigate this
    issue and enable LLMs to accurately solve complex numerical tasks.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 擅长执行上述第一步，但可能在计算最终答案时遇到困难。通常，这个问题是由于算术错误或无法评估复杂表达式所致。简单来说，*LLM 在处理复杂的数字任务时会遇到困难*。在
    [2] 中，作者旨在利用一种称为思维程序（PoT）提示的代码增强提示方法来缓解这个问题，并使 LLM 能够准确地解决复杂的数字任务。
- en: “In PoT, the computation can be delegated to a program interpreter, which is
    used to execute the generated program, thus decoupling complex computation from
    reasoning and language understanding.” *— from [2]*
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “在 PoT 中，计算可以委托给程序解释器，该解释器用于执行生成的程序，从而将复杂的计算与推理和语言理解解耦。” *— 摘自 [2]*
- en: As we might suspect, PoT prompting is quite similar to PaL. Both techniques
    use code-augmented prompting techniques to solve complex reasoning tasks and delegate
    necessary portions of the reasoning process to a code interpreter. More specifically,
    PoT prompting leverages few-shot learning with code-based LLMs (e.g., [Codex](https://cameronrwolfe.substack.com/i/93578656/evaluating-large-language-models-trained-on-code)
    [4]) to generate hybrid rationales that contain both natural language statements
    and code (in Python). Then, the code portion of the output is offloaded to an
    interpreter for evaluation, thus decoupling reasoning and computation.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所猜测的那样，PoT 提示与 PaL 非常相似。这两种技术都使用代码增强的提示技术来解决复杂的推理任务，并将推理过程的必要部分委托给代码解释器。更具体地说，PoT
    提示利用基于代码的 LLM 的少量学习（例如，[Codex](https://cameronrwolfe.substack.com/i/93578656/evaluating-large-language-models-trained-on-code)
    [4]）生成包含自然语言声明和代码（用 Python 编写）的混合推理。然后，将输出的代码部分卸载到解释器进行评估，从而将推理和计算解耦。
- en: '![](../Images/8b37e6ec626c3cf4e03b825657c167e7.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8b37e6ec626c3cf4e03b825657c167e7.png)'
- en: (from [2])
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: （摘自 [2]）
- en: 'In contrast, CoT prompting performs both reasoning and computation directly
    with the LLM. This is a problem because LLMs struggle to:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，CoT 提示直接在 LLM 上进行推理和计算。这是一个问题，因为 LLM 在以下方面存在困难：
- en: Perform basic arithmetic (especially with larger numbers)
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行基本的算术运算（特别是大数运算）
- en: Evaluate complex mathematical expressions (e.g., polynomial or differential
    equations)
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估复杂的数学表达式（例如，多项式或微分方程）
- en: Solve problems that require iteration
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解决需要迭代的问题
- en: Such issues are demonstrated by the figure above, where an LLM with CoT prompting
    fails to evaluate a simple, cubic equation or reason over an iterative computation
    of the Fibonacci sequence. Luckily, we can solve these problems pretty easily
    with a program! For example, we can compute the Fibonacci sequence with a for
    loop, and a cubic equation can be easily expressed in Python syntax. Then, we
    can just run this program to generate the correct output, thus removing unneeded
    dependencies upon the LLM.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题通过上图展示，其中一个使用 CoT 提示的 LLM 无法评估一个简单的立方方程或在斐波那契序列的迭代计算中进行推理。幸运的是，我们可以用程序轻松解决这些问题！例如，我们可以使用
    for 循环计算斐波那契序列，而立方方程可以轻松地用 Python 语法表示。然后，我们可以运行这个程序来生成正确的输出，从而消除对 LLM 的不必要依赖。
- en: '![](../Images/37ddacc4bdb86a21eae472812a8791fa.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/37ddacc4bdb86a21eae472812a8791fa.png)'
- en: (from [2])
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: （摘自 [2]）
- en: '**Details of PoT.** Similarly to PaL, PoT prompting generates problem-solving
    rationales that contain both language and code components. The LLM is taught how
    to generate such rationales via a series of few-shot exemplars that contain pairs
    of questions with an associated “program of thoughts” (i.e., a multi-step program
    with interleaved natural language statements that explain the computation); see
    above.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**PoT 的详细信息。** 类似于 PaL，PoT 提示生成的解决问题的推理包含语言和代码组件。LLM 通过一系列包含问题对及相关“思维程序”（即包含解释计算的多步骤程序和自然语言声明）的少量示例来学习生成这种推理；见上文。'
- en: '![](../Images/4694a4c5d88ada05432af424e9e371ba.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4694a4c5d88ada05432af424e9e371ba.png)'
- en: Symbolic math programming example with SymPy
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 SymPy 的符号数学编程示例
- en: 'Unlike PaL, code written by PoT relies upon a symbolic math library called
    [SymPy](https://github.com/sympy/sympy). This package allows the user to define
    mathematical “symbols”, which can then be combined together to form complex expressions.
    To evaluate these expressions, we can simply pass them into SymPy’s `solve` function;
    see above. For more details, check out the tutorial [here](https://docs.sympy.org/latest/tutorials/intro-tutorial/intro.html#what-is-symbolic-computation).
    Despite its use of symbolic math, PoT prompting is different from trying to directly
    generate mathematical equations with an LLM, which has been shown by prior work
    to be quite difficult [3]. This is because PoT prompting:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 与PaL不同，PoT编写的代码依赖于一个名为[SymPy](https://github.com/sympy/sympy)的符号数学库。这个包允许用户定义数学“符号”，然后将它们组合在一起形成复杂的表达式。为了评估这些表达式，我们可以将它们传递到SymPy的`solve`函数中；见上文。更多细节，请查看[这里](https://docs.sympy.org/latest/tutorials/intro-tutorial/intro.html#what-is-symbolic-computation)的教程。尽管使用了符号数学，PoT提示与尝试直接生成数学方程的LLM不同，之前的工作表明这非常困难[3]。这是因为PoT提示：
- en: Generates symbolic equations via a multi-step, rationale-driven process.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过一个多步骤、基于理由的过程生成符号方程。
- en: Associates symbolic variables with semantically-meaningful names.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将符号变量与语义上有意义的名称关联起来。
- en: Similarly to PaL, authors in [2] note that assigning meaningful names to the
    variables in a program does measurably impact the LLM’s performance.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 与PaL类似，[2]中的作者指出，为程序中的变量分配有意义的名称确实对LLM的性能产生了可测量的影响。
- en: '**The results.** PoT prompting is evaluated using both Codex [4] and GPT-3
    [7] on several math word problem and financial question answering datasets (e.g.,
    FinQA [8] and ConvFinQA [9]). Several different LLMs are adopted as baselines
    using both few-shot learning and CoT prompting (including a CoT prompting variant
    that is given access to an external calculator). As shown in the table below,
    PoT prompting outperforms baselines by a significant margin in all cases, *which
    emphasizes the value of decoupling reasoning from computation*.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**结果。** PoT提示在多个数学文字问题和金融问答数据集（例如，FinQA [8]和ConvFinQA [9]）上使用Codex [4]和GPT-3
    [7]进行了评估。使用少量学习和CoT提示（包括一个可以访问外部计算器的CoT提示变体）的多个不同LLM被用作基准。如下表所示，PoT提示在所有情况下都显著优于基准，*这强调了将推理与计算分离的价值*。'
- en: '![](../Images/c7a084fbef8d2134732f38293de3ea7e.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7a084fbef8d2134732f38293de3ea7e.png)'
- en: (from [2])
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: （来自[2]）
- en: Interestingly, authors in [2] also find that zero-shot PoT prompting (i.e.,
    similar to [zero-shot CoT prompting](https://cameronrwolfe.substack.com/i/116166267/variants-of-cot-prompting)
    [10]) works quite well. Even without curating several examples of program-infused
    rationales for the LLM, we can achieve reasonable performance with an LLM on numerical
    tasks via PoT prompting. Additionally, the authors make an interesting practical
    note about using PoT prompting. To avoid generating fully language-based rationales
    (i.e., a program with all comments), they have to manually suppress the probability
    of the `#` token. Although this is a small detail, it is important to keep in
    mind—*we don’t want our generated program to be nothing but comments*! Plus, it
    demonstrates that making such techniques work in practice is oftentimes brittle
    and difficult.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，[2]中的作者还发现零-shot PoT提示（即类似于[零-shot CoT提示](https://cameronrwolfe.substack.com/i/116166267/variants-of-cot-prompting)
    [10]）效果非常好。即使没有为LLM策划几个程序融入的理由，我们也可以通过PoT提示在数值任务中实现合理的性能。此外，作者对使用PoT提示提出了一个有趣的实际注意事项。为了避免生成完全基于语言的理由（即一个带有所有注释的程序），他们不得不手动抑制`#`符号的概率。虽然这是一个小细节，但值得记住——*我们不希望生成的程序仅仅是注释*！此外，这也表明，使这种技术在实践中有效往往是脆弱和困难的。
- en: Can we do better?
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们能做得更好吗？
- en: '![](../Images/f5278b5a9c8ee9a7b1c7d30e037ae9e0.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f5278b5a9c8ee9a7b1c7d30e037ae9e0.png)'
- en: (from [14])
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: （来自[14]）
- en: Both PaL and PoT adopt a [greedy decoding](https://twitter.com/cwolferesearch/status/1659608476455256078?s=20)
    strategy in a majority of experiments, meaning that the LLM will produce an output
    sequence by iteratively selecting the next token with the highest probability.
    However, there are a variety of better decoding strategies that we can use! One
    notable (and super simple) strategy is Self-Consistency [14]. This techniques
    uses the same LLM and prompt to produce multiple different outputs for a problem.
    Then, the final answer is derived by taking a majority vote over all outputs that
    are generated; see above.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: PaL 和 PoT 在大多数实验中采用了[贪婪解码](https://twitter.com/cwolferesearch/status/1659608476455256078?s=20)策略，这意味着
    LLM 会通过迭代选择下一个概率最高的 token 来生成输出序列。然而，我们可以使用各种更好的解码策略！一个值得注意的（且超级简单的）策略是自一致性[14]。该技术使用相同的
    LLM 和提示来为一个问题生成多个不同的输出。然后，通过对生成的所有输出进行多数投票来得出最终答案；见上文。
- en: '![](../Images/c43c9351cb566fe1ec7e77a0cf0b590c.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c43c9351cb566fe1ec7e77a0cf0b590c.png)'
- en: (from [2])
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [2])
- en: When Self-Consistency is applied to PoT prompting, we see an immediate and significant
    benefit! As shown above, PoT with Self-Consistency achieves new state-of-the-art
    performance on nearly every dataset considered within [2]. Similarly, PaL [1]
    benefits from the use of Self-Consistency and is even used to explore more complex
    decoding/prompting strategies, such as [least-to-most prompting](https://learnprompting.org/docs/intermediate/least_to_most)
    [15] (i.e., a variant of CoT prompting that explicitly solves reasoning tasks
    one step at a time). When combined with this more sophisticated prompting style,
    PaL becomes even more effective; see below.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当将自一致性应用于 PoT 提示时，我们会看到立即且显著的好处！如上所示，带有自一致性的 PoT 在几乎所有考虑的数据集中都达到了新的最先进性能。同样，PaL
    [1] 也受益于自一致性的使用，甚至用于探索更复杂的解码/提示策略，如[最少到最多提示](https://learnprompting.org/docs/intermediate/least_to_most)
    [15]（即 CoT 提示的一种变体，显式地逐步解决推理任务）。与这种更复杂的提示风格结合时，PaL 变得更加有效；见下文。
- en: '![](../Images/9b1079664f673f6b516b90f8169dfeb7.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b1079664f673f6b516b90f8169dfeb7.png)'
- en: (from [1])
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [1])
- en: Although PaL and PoT work quite well, we can make them a little bit better with
    some easy-to-implement additions to their prompting technique. Such findings inspire
    further experimentation. Maybe we can get added performance benefits by leveraging
    other useful techniques, such as [prompt ensembles](https://cameronrwolfe.substack.com/p/prompt-ensembles-make-llms-more-reliable).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 PaL 和 PoT 表现得相当好，我们可以通过对其提示技术进行一些易于实现的补充来使其更上一层楼。这些发现激发了进一步的实验。也许我们可以通过利用其他有用的技术，如[提示集成](https://cameronrwolfe.substack.com/p/prompt-ensembles-make-llms-more-reliable)，来获得额外的性能提升。
- en: Takeaways
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主要结论
- en: Although LLMs are useful by themselves, we see in this overview that they can
    be a lot cooler when given access to useful tools. In particular, we have learned
    that connecting an LLM to an external code interpreter can be incredibly beneficial
    to performance on reasoning tasks. However, we need access to an LLM that is capable
    of writing code for this to work well. Some takeaways are outlined below.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 LLM 本身非常有用，但在本综述中我们看到，当 LLM 能够访问有用的工具时，它们可以变得更加出色。特别是，我们了解到将 LLM 连接到外部代码解释器对于推理任务的性能极其有利。然而，为了使其效果良好，我们需要访问能够编写代码的
    LLM。以下是一些主要结论。
- en: '**Why does this work?** The effectiveness of PaL and PoT stems from the fact
    that LLMs are capable of generating accurate problem-solving rationales but tend
    to struggle with simple tasks like arithmetic and iteration. Luckily, such concepts
    can be easily modeled within a program, making the connection of LLMs to an external
    code interpreter an intuitive and powerful technique for solving reasoning problems.
    Put simply, we gain a lot by relying on LLMs for what they are good at and delegating
    remaining problem-solving components to a code interpreter than can more reliably
    produce a solution.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么会有效？** PaL 和 PoT 的有效性源于 LLM 能够生成准确的解决问题的理由，但往往在简单任务如算术和迭代中遇到困难。幸运的是，这些概念可以轻松地在程序中建模，使得将
    LLM 连接到外部代码解释器成为一种直观且强大的解决推理问题的技术。简而言之，我们通过依赖 LLM 擅长的领域，并将剩余的解决问题的组件委托给能够更可靠地生成解决方案的代码解释器，获得了很多收益。'
- en: '**How should we solve LLM weaknesses?** As briefly mentioned in this post,
    many of the known shortcomings of LLMs are being solved as more powerful models,
    such as GPT-4, are released. However, we see in this overview that alternative
    methods of solving such problems exist that might even be more reliable. In particular,
    relying upon an external code interpreter can solve issues experienced due to
    an LLM’s limitations in solving reasoning-based tasks. Giving a model the ability
    to execute code undoubtedly increases the scope of its abilities, which inspires
    us to think about other [tools](https://arxiv.org/abs/2302.04761) that may be
    useful to an LLM.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们应该如何解决 LLM 的弱点？** 正如这篇文章简要提到的那样，许多已知的 LLM 缺点正随着更强大的模型（如 GPT-4）的发布而得到解决。然而，我们在这份概述中看到，解决这些问题的替代方法可能更加可靠。特别是，依靠外部代码解释器可以解决由于
    LLM 在解决基于推理的任务时所遇到的局限性问题。赋予模型执行代码的能力无疑扩大了其能力范围，这激发我们思考其他可能对 LLM 有用的 [工具](https://arxiv.org/abs/2302.04761)。'
- en: '**Expressing thoughts as a program.** This work really highlights the fact
    that programs can be interpreted as a structured language for expressing one’s
    thoughts. Compared to natural language, programming languages are more constrained,
    which gives them the ability to easily express iteration, model complex equations,
    and more. However, the formal nature of programs also limits expressivity — *writing
    a poem is much easier in natural language than in a Python script (assuming no
    calls to the GPT-4 API)!* Considering the differences between natural language
    and code is, in my opinion, pretty interesting. We see here that combining them
    together can draw upon the strengths of both.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**将思想表达为程序。** 这项工作真正突显了程序可以被解释为表达个人思想的结构化语言这一事实。与自然语言相比，编程语言的约束更多，这使得它们能够轻松表达迭代、建模复杂方程等。然而，程序的形式化性质也限制了表达能力——*用自然语言写诗要比在
    Python 脚本中写诗容易得多（假设没有调用 GPT-4 API）！* 在我看来，考虑自然语言和代码之间的差异是相当有趣的。我们在这里看到，将它们结合在一起可以发挥两者的优势。'
- en: Closing Remarks
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结束语
- en: Thanks so much for reading this article. I am [Cameron R. Wolfe](https://cameronrwolfe.me/),
    Director of AI at [Rebuy](https://www.rebuyengine.com/). I study the empirical
    and theoretical foundations of deep learning. You can also check out my [other
    writings](https://medium.com/@wolfecameron) on medium! If you liked it, please
    follow me on [twitter](https://twitter.com/cwolferesearch) or subscribe to my
    [Deep (Learning) Focus newsletter](https://cameronrwolfe.substack.com/), where
    I help readers build a deeper understanding of topics in AI research via understandable
    overviews of popular papers.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢你阅读这篇文章。我是 [Cameron R. Wolfe](https://cameronrwolfe.me/)，[Rebuy](https://www.rebuyengine.com/)
    的人工智能总监。我研究深度学习的实证和理论基础。你还可以查看我在 medium 上的 [其他写作](https://medium.com/@wolfecameron)！如果你喜欢这篇文章，请在
    [twitter](https://twitter.com/cwolferesearch) 上关注我，或者订阅我的 [Deep (Learning) Focus
    newsletter](https://cameronrwolfe.substack.com/)，在这里我通过对流行论文的易懂概述帮助读者建立对人工智能研究主题的更深入理解。
- en: Bibliography
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Gao, Luyu, et al. “PAL: Program-aided Language Models.” *arXiv preprint
    arXiv:2211.10435* (2022).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Gao, Luyu 等. “PAL：程序辅助语言模型。” *arXiv 预印本 arXiv:2211.10435* (2022)。'
- en: '[2] Chen, Wenhu, et al. “Program of thoughts prompting: Disentangling computation
    from reasoning for numerical reasoning tasks.” *arXiv preprint arXiv:2211.12588*
    (2022).'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Chen, Wenhu 等. “思维程序提示：将计算与推理解耦以进行数值推理任务。” *arXiv 预印本 arXiv:2211.12588*
    (2022)。'
- en: '[3] Wei, Jason, et al. “Chain of thought prompting elicits reasoning in large
    language models.” *arXiv preprint arXiv:2201.11903* (2022).'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Wei, Jason 等. “思维链提示引发大语言模型的推理。” *arXiv 预印本 arXiv:2201.11903* (2022)。'
- en: '[4] Chen, Mark, et al. “Evaluating large language models trained on code.”
    *arXiv preprint arXiv:2107.03374* (2021).'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Chen, Mark 等. “评估训练有素的代码大型语言模型。” *arXiv 预印本 arXiv:2107.03374* (2021)。'
- en: '[5] Lewkowycz, Aitor, et al. “Solving quantitative reasoning problems with
    language models.” *arXiv preprint arXiv:2206.14858* (2022).'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Lewkowycz, Aitor 等. “用语言模型解决定量推理问题。” *arXiv 预印本 arXiv:2206.14858* (2022)。'
- en: '[6] Chen, Wenhu. “Large language models are few (1)-shot table reasoners.”
    *arXiv preprint arXiv:2210.06710* (2022).'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Chen, Wenhu. “大型语言模型是少量（1）次表格推理者。” *arXiv 预印本 arXiv:2210.06710* (2022)。'
- en: '[7] Brown, Tom, et al. “Language models are few-shot learners.” *Advances in
    neural information processing systems* 33 (2020): 1877–1901.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Brown, Tom 等. “语言模型是少量学习者。” *神经信息处理系统进展* 33 (2020): 1877–1901。'
- en: '[8] Chen, Zhiyu, et al. “Finqa: A dataset of numerical reasoning over financial
    data.” *arXiv preprint arXiv:2109.00122* (2021).'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] 陈志宇 等. “Finqa：一个关于金融数据的数字推理数据集。” *arXiv 预印本 arXiv:2109.00122* (2021)。'
- en: '[9] Chen, Zhiyu, et al. “Convfinqa: Exploring the chain of numerical reasoning
    in conversational finance question answering.” *arXiv preprint arXiv:2210.03849*
    (2022).'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] 陈志宇 等. “Convfinqa：探索对话金融问答中的数字推理链。” *arXiv 预印本 arXiv:2210.03849* (2022)。'
- en: '[10] Kojima, Takeshi, et al. “Large language models are zero-shot reasoners.”
    *arXiv preprint arXiv:2205.11916* (2022).'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] 小岛武志 等. “大型语言模型是零-shot 推理器。” *arXiv 预印本 arXiv:2205.11916* (2022)。'
- en: '[11] Ouyang, Long, et al. “Training language models to follow instructions
    with human feedback.” *Advances in Neural Information Processing Systems* 35 (2022):
    27730–27744.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] 欧阳龙 等. “通过人工反馈训练语言模型以遵循指令。” *神经信息处理系统进展* 35 (2022): 27730–27744。'
- en: '[12] Thoppilan, Romal, et al. “Lamda: Language models for dialog applications.”
    *arXiv preprint arXiv:2201.08239* (2022).'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] 托皮兰·罗马尔 等. “Lamda：用于对话应用的语言模型。” *arXiv 预印本 arXiv:2201.08239* (2022)。'
- en: '[13] Li, Yifei, et al. “On the advance of making language models better reasoners.”
    *arXiv preprint arXiv:2206.02336* (2022).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] 李一飞 等. “关于提升语言模型推理能力的进展。” *arXiv 预印本 arXiv:2206.02336* (2022)。'
- en: '[14] Wang, Xuezhi, et al. “Self-consistency improves chain of thought reasoning
    in language models.” *arXiv preprint arXiv:2203.11171* (2022).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] 王学智 等. “自我一致性提高语言模型中的思维链推理。” *arXiv 预印本 arXiv:2203.11171* (2022)。'
- en: '[15] Zhou, Denny, et al. “Least-to-most prompting enables complex reasoning
    in large language models.” *arXiv preprint arXiv:2205.10625* (2022).'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] 周登嵘 等. “从最少到最多的提示使大型语言模型能够进行复杂推理。” *arXiv 预印本 arXiv:2205.10625* (2022)。'
- en: '[16] Press, Ofir, et al. “Measuring and Narrowing the Compositionality Gap
    in Language Models.” *arXiv preprint arXiv:2210.03350* (2022).'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] 普雷斯·奥菲尔 等. “测量并缩小语言模型中的组合性差距。” *arXiv 预印本 arXiv:2210.03350* (2022)。'
