- en: Introducing a Dataset to Detect GPT-Generated Text
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍一个用于检测 GPT 生成文本的数据集
- en: 原文：[https://towardsdatascience.com/introducing-a-dataset-to-detect-gpt-generated-text-96bb76dd2ed2?source=collection_archive---------7-----------------------#2023-02-08](https://towardsdatascience.com/introducing-a-dataset-to-detect-gpt-generated-text-96bb76dd2ed2?source=collection_archive---------7-----------------------#2023-02-08)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/introducing-a-dataset-to-detect-gpt-generated-text-96bb76dd2ed2?source=collection_archive---------7-----------------------#2023-02-08](https://towardsdatascience.com/introducing-a-dataset-to-detect-gpt-generated-text-96bb76dd2ed2?source=collection_archive---------7-----------------------#2023-02-08)
- en: How to create datasets for ChatGPT detection models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何为 ChatGPT 检测模型创建数据集
- en: '[](https://medium.com/@aadityaubhat?source=post_page-----96bb76dd2ed2--------------------------------)[![Aaditya
    Bhat](../Images/4ae4a03d798d4a3fbec02d81c9c87146.png)](https://medium.com/@aadityaubhat?source=post_page-----96bb76dd2ed2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----96bb76dd2ed2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----96bb76dd2ed2--------------------------------)
    [Aaditya Bhat](https://medium.com/@aadityaubhat?source=post_page-----96bb76dd2ed2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@aadityaubhat?source=post_page-----96bb76dd2ed2--------------------------------)[![Aaditya
    Bhat](../Images/4ae4a03d798d4a3fbec02d81c9c87146.png)](https://medium.com/@aadityaubhat?source=post_page-----96bb76dd2ed2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----96bb76dd2ed2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----96bb76dd2ed2--------------------------------)
    [Aaditya Bhat](https://medium.com/@aadityaubhat?source=post_page-----96bb76dd2ed2--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Feff870d7210e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroducing-a-dataset-to-detect-gpt-generated-text-96bb76dd2ed2&user=Aaditya+Bhat&userId=eff870d7210e&source=post_page-eff870d7210e----96bb76dd2ed2---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----96bb76dd2ed2--------------------------------)
    ·4 min read·Feb 8, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F96bb76dd2ed2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroducing-a-dataset-to-detect-gpt-generated-text-96bb76dd2ed2&user=Aaditya+Bhat&userId=eff870d7210e&source=-----96bb76dd2ed2---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Feff870d7210e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroducing-a-dataset-to-detect-gpt-generated-text-96bb76dd2ed2&user=Aaditya+Bhat&userId=eff870d7210e&source=post_page-eff870d7210e----96bb76dd2ed2---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----96bb76dd2ed2--------------------------------)
    ·4 min read·2023年2月8日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F96bb76dd2ed2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroducing-a-dataset-to-detect-gpt-generated-text-96bb76dd2ed2&user=Aaditya+Bhat&userId=eff870d7210e&source=-----96bb76dd2ed2---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F96bb76dd2ed2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroducing-a-dataset-to-detect-gpt-generated-text-96bb76dd2ed2&source=-----96bb76dd2ed2---------------------bookmark_footer-----------)![](../Images/f19e3823d8d934d82e0773690e2b7474.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F96bb76dd2ed2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintroducing-a-dataset-to-detect-gpt-generated-text-96bb76dd2ed2&source=-----96bb76dd2ed2---------------------bookmark_footer-----------)![](../Images/f19e3823d8d934d82e0773690e2b7474.png)'
- en: Photo by [Markus Spiske](https://unsplash.com/@markusspiske?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/iar-afB0QQw?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Markus Spiske](https://unsplash.com/@markusspiske?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，来源于 [Unsplash](https://unsplash.com/photos/iar-afB0QQw?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: With the breakthrough success of large language models like ChatGPT, people
    are finding innovative ways to use these models in their daily lives. However,
    this has also led to unintended consequences such as cheating on homework and
    tests by students, the use of ChatGPT to publish research papers, and even scammers
    using these models to trick people. To address these issues, there is a growing
    need for models that can detect text generated by GPT models. One of the crucial
    requirements for building robust models for detecting GPT-generated text is access
    to a large dataset of human-written and GPT-generated responses. This article
    introduces such a dataset, consisting of 150k human-written and GPT-generated
    responses to Wikipedia topics and outlines a framework for generating similar
    datasets in the future.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着像 ChatGPT 这样的大型语言模型取得突破性成功，人们正在寻找将这些模型应用于日常生活的创新方式。然而，这也导致了一些意外后果，如学生在作业和考试中作弊、使用
    ChatGPT 发布研究论文，甚至诈骗者利用这些模型来欺骗他人。为了解决这些问题，对能够检测 GPT 模型生成文本的模型的需求越来越大。构建强大 GPT 生成文本检测模型的一个关键要求是访问一个包含大量人工编写和
    GPT 生成响应的数据集。本文介绍了这样一个数据集，由 150k 人工编写和 GPT 生成的维基百科主题回应组成，并概述了未来生成类似数据集的框架。
- en: Dataset
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集
- en: '[GPT-wiki-intro Dataset](https://huggingface.co/datasets/aadityaubhat/GPT-wiki-intro)
    is available on Hugging Face. This dataset has human written and GPT (Curie) generated
    introductions for 150,000 Wikipedia topics. The prompt used to generate GPT response
    is as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[GPT-wiki-intro 数据集](https://huggingface.co/datasets/aadityaubhat/GPT-wiki-intro)
    在 Hugging Face 上提供。该数据集包含 150,000 个维基百科主题的人工编写和 GPT（Curie）生成的介绍。用于生成 GPT 响应的提示如下：'
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Where the `title` is the title of Wikipedia page, and `starter_text` is the
    first 7 words from the introduction paragraph. The dataset also has useful metadata
    such as title_len, wiki_intro_len, generated_intro_len, prompt_tokens, generated_text_tokens,
    etc. The schema of the dataset is as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 `title` 是维基百科页面的标题，`starter_text` 是介绍段落中的前 7 个词。该数据集还具有有用的元数据，如 title_len、wiki_intro_len、generated_intro_len、prompt_tokens、generated_text_tokens
    等。数据集的架构如下：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This dataset is shared under Creative Commons license and can be used to distribute,
    remix, adapt, and build upon. The code to generate this dataset can be found [here](https://github.com/aadityaubhat/wiki_gpt).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集以 Creative Commons 许可证共享，可以用于分发、混合、调整和构建。生成该数据集的代码可以在[这里](https://github.com/aadityaubhat/wiki_gpt)找到。
- en: Framework
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 框架
- en: 'This dataset is a good starting point for a general use-case of detecting GPT
    generated text. But if you have a specific use-case, for example, detecting ChatGPT
    generated answers to test/homework questions, detecting whether a message is sent
    by a human or chatbot, or if you need a larger dataset in a specific domain you
    can use the framework to generate your own dataset. The dataset generation process
    involves three main steps:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集是检测 GPT 生成文本的一般用途的良好起点。但如果你有特定的用例，例如检测 ChatGPT 生成的测试/作业问题的回答、识别消息是否由人类还是聊天机器人发送，或者你需要特定领域的大型数据集，你可以使用该框架生成自己的数据集。数据集生成过程涉及三个主要步骤：
- en: Get the anchor dataset
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取锚数据集
- en: Clean the anchor dataset
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 清理锚数据集
- en: Augment the dataset with human written/ GPT generated data
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用人工编写/GPT 生成的数据扩充数据集
- en: '**Get the anchor dataset**'
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**获取锚数据集**'
- en: In this step, we acquire the anchor dataset. This would be the existing data
    which is readily available for the specific use-case. For [GPT-wiki-intro dataset](https://huggingface.co/datasets/aadityaubhat/GPT-wiki-intro),
    the anchor dataset was [wikipedia dataset](https://huggingface.co/datasets/wikipedia#licensing-information),
    which contains cleaned articles of all languages. For detecting cheating on tests
    and homework, the anchor dataset could be the question answer pairs submitted
    by previous students. If you don’t have well defined anchor dataset, you can explore
    various open source datasets on Hugging Face and Kaggle which align well with
    the use-case. The anchor dataset doesn’t have to be human written, we can also
    use GPT generated data as the anchor data. For example, we can use data from [ChatGPT
    prompt response library](https://www.emergentmind.com/).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，我们获取锚数据集。这将是为特定用例现成的现有数据。对于[GPT-wiki-intro数据集](https://huggingface.co/datasets/aadityaubhat/GPT-wiki-intro)，锚数据集是[wikipedia数据集](https://huggingface.co/datasets/wikipedia#licensing-information)，该数据集包含各种语言的清理文章。对于检测考试和作业作弊，锚数据集可以是之前学生提交的问答对。如果你没有明确定义的锚数据集，可以探索Hugging
    Face和Kaggle上与用例匹配的各种开源数据集。锚数据集不必是人工编写的，我们也可以使用GPT生成的数据作为锚数据。例如，我们可以使用[ChatGPT提示响应库](https://www.emergentmind.com/)的数据。
- en: Clean the anchor dataset
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 清理锚数据集
- en: Once we have the anchor dataset, we need to clean the data to keep the most
    relevant information. ChatGPT detection models are sensitive to the length of
    text. These models perform poorly on smaller texts. We can set a threshold and
    filter out any responses shorter than the threshold. For example, in GPT-wiki-intro
    dataset, we filter out all rows where length of introduction is less than 150
    words or greater than 350 words. We also filter out all rows where title is more
    than three words. At this step we also need to decide the total size of the dataset.
    As augmenting the data with either human written or GPT generated responses is
    going to be expensive, we need to figure out what’s the minimum required size
    of dataset for our use-case.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们拥有锚数据集，我们需要清理数据以保留最相关的信息。ChatGPT检测模型对文本长度非常敏感。这些模型在较小的文本上表现不佳。我们可以设定一个阈值，并过滤掉任何短于该阈值的响应。例如，在GPT-wiki-intro数据集中，我们过滤掉所有引言长度少于150字或超过350字的行。我们还过滤掉标题超过三个词的所有行。在这一步中，我们还需要决定数据集的总大小。由于使用人工编写或GPT生成的响应来增强数据将会很昂贵，我们需要确定我们用例所需的最小数据集大小。
- en: Augment the dataset with human written/ GPT generated data
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 增强数据集，加入人工编写或GPT生成的数据
- en: This is the final step of dataset generation. At this step, we augment the anchor
    dataset with human written or GPT generated data. The most important part in this
    step is coming up with the **prompt** used for generating the response from GPT
    or **question** that will be answered by humans. For finalizing the prompt we
    can leverage [OpenAI Playground](https://platform.openai.com/playground) to test
    out different prompts with different models, temperature, frequency penalty and
    presence penalty. To increase the diversity of the dataset, we can finalize n
    prompts and use them uniformly to get the responses. In case of human responses,
    we would want to finalize the question by giving different variations of the questions
    to small survey population and then checking the results to finalize the n questions.
    Once the prompts or the questions are finalized, we can use the OpenAI API to
    generate the GPT generated responses or use service such as Mechanical Turk to
    get the human written responses.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这是数据集生成的最终步骤。在此步骤中，我们通过人工编写或GPT生成的数据来增强锚数据集。这一步中最重要的是确定用于生成GPT响应的**提示**或由人类回答的**问题**。为了最终确定提示，我们可以利用[OpenAI
    Playground](https://platform.openai.com/playground)测试不同的提示、模型、温度、频率惩罚和存在惩罚。为了增加数据集的多样性，我们可以最终确定n个提示，并统一使用这些提示以获取响应。在人类回应的情况下，我们需要通过向小型调查人群提供不同的问题变体来最终确定问题，然后检查结果以最终确定n个问题。一旦提示或问题确定，我们可以使用OpenAI
    API生成GPT生成的响应，或使用像Mechanical Turk这样的服务获取人工编写的响应。
- en: Conclusion
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In conclusion, with the widespread use of large language models like ChatGPT,
    there is a growing need for models that can detect text generated by these models.
    This article introduced [GPT-wiki-intro dataset](https://huggingface.co/datasets/aadityaubhat/GPT-wiki-intro)
    and outlined a framework for generating similar datasets. The availability of
    such datasets will play a critical role in developing robust models for detecting
    GPT-generated text and address the unintended consequences of the use of these
    models.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，随着像ChatGPT这样的巨大语言模型的广泛使用，对能够检测这些模型生成的文本的模型的需求也在不断增加。本文介绍了[GPT-wiki-intro
    数据集](https://huggingface.co/datasets/aadityaubhat/GPT-wiki-intro)并概述了生成类似数据集的框架。这些数据集的可用性将在开发用于检测GPT生成文本的强大模型以及应对这些模型使用的不良后果方面发挥关键作用。
- en: Citation
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引用
- en: 'If you find this work helpful, please consider citing:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您觉得这项工作有帮助，请考虑引用：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
