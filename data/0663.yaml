- en: Batched Bandit Problems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批量化赌博机问题
- en: 原文：[https://towardsdatascience.com/batched-bandit-problems-ea73dba5da7a?source=collection_archive---------8-----------------------#2023-02-17](https://towardsdatascience.com/batched-bandit-problems-ea73dba5da7a?source=collection_archive---------8-----------------------#2023-02-17)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/batched-bandit-problems-ea73dba5da7a?source=collection_archive---------8-----------------------#2023-02-17](https://towardsdatascience.com/batched-bandit-problems-ea73dba5da7a?source=collection_archive---------8-----------------------#2023-02-17)
- en: Multi-Armed Bandits with delayed rewards in successive trials
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多臂赌博机中的延迟奖励
- en: '[](https://medium.com/@smsmith714?source=post_page-----ea73dba5da7a--------------------------------)[![Sean
    Smith](../Images/611395d113b10ec4bbfaf781301139c7.png)](https://medium.com/@smsmith714?source=post_page-----ea73dba5da7a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ea73dba5da7a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ea73dba5da7a--------------------------------)
    [Sean Smith](https://medium.com/@smsmith714?source=post_page-----ea73dba5da7a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@smsmith714?source=post_page-----ea73dba5da7a--------------------------------)[![Sean
    Smith](../Images/611395d113b10ec4bbfaf781301139c7.png)](https://medium.com/@smsmith714?source=post_page-----ea73dba5da7a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ea73dba5da7a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ea73dba5da7a--------------------------------)
    [Sean Smith](https://medium.com/@smsmith714?source=post_page-----ea73dba5da7a--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6957f6523097&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbatched-bandit-problems-ea73dba5da7a&user=Sean+Smith&userId=6957f6523097&source=post_page-6957f6523097----ea73dba5da7a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ea73dba5da7a--------------------------------)
    ·11 min read·Feb 17, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fea73dba5da7a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbatched-bandit-problems-ea73dba5da7a&user=Sean+Smith&userId=6957f6523097&source=-----ea73dba5da7a---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6957f6523097&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbatched-bandit-problems-ea73dba5da7a&user=Sean+Smith&userId=6957f6523097&source=post_page-6957f6523097----ea73dba5da7a---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ea73dba5da7a--------------------------------)
    ·11分钟阅读·2023年2月17日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fea73dba5da7a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbatched-bandit-problems-ea73dba5da7a&user=Sean+Smith&userId=6957f6523097&source=-----ea73dba5da7a---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fea73dba5da7a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbatched-bandit-problems-ea73dba5da7a&source=-----ea73dba5da7a---------------------bookmark_footer-----------)![](../Images/390d82fb35fa2b239f2be68c33aee022.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fea73dba5da7a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbatched-bandit-problems-ea73dba5da7a&source=-----ea73dba5da7a---------------------bookmark_footer-----------)![](../Images/390d82fb35fa2b239f2be68c33aee022.png)'
- en: Photo by [Erik Mclean](https://unsplash.com/@introspectivedsgn?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Erik Mclean](https://unsplash.com/@introspectivedsgn?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Experiments are vital to any business operation. While AB tests are the defacto
    standard, you’d be surprised to find many practitioners are not running proper
    randomized experiments. A more likely scenario involves seasoned experimenters
    using their judgement of when to pull a treatment, overriding the sacred power
    analysis that would allow for a formal statistical inference. At it’s heart though,
    this solves the problem of regret so many randomized experiments struggle with.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 实验对任何业务操作至关重要。尽管AB测试是事实上的标准，但你会惊讶地发现许多从业者并没有进行适当的随机实验。更常见的情况是经验丰富的实验者会根据他们的判断来决定何时执行处理，从而覆盖了允许进行正式统计推断的神圣功效分析。尽管如此，这从根本上解决了许多随机实验所面临的遗憾问题。
- en: 'The savvy reader has noticed I teed up a great intro to dive into a conversation
    about bandits, but this conversation is slightly different from other articles.
    Here we focus on a class of bandit problems that receives little attention outside
    of academia: batched bandit problems. In the typical bandit scenario, an agent
    executes an action and instantly enjoys a reward. This paradigm is often sufficient
    in the world of digital marketing and website optimization, but what about when
    rewards are not instantaneous?'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 精明的读者已经注意到我为深入讨论多臂老虎机问题铺垫了很好的引言，但这个讨论与其他文章略有不同。这里我们关注的是一个在学术界以外几乎没有受到关注的多臂老虎机问题类别：批次多臂老虎机问题。在典型的多臂老虎机场景中，代理执行一个动作并立即获得奖励。这种模式在数字营销和网站优化领域通常是足够的，但当奖励不是即时的呢？
- en: Take this example, you’re running an email campaign, but no one in the marketing
    department can decide which of 5 pieces of copy will be the most effective. Your
    key KPI will be click through rate and is **measured if the user clicks through
    the ad at any point during 5 days after receiving the email.**For now, assume
    all users respond equivalently. The window to run the campaign is small. Given
    this information, **how will you quickly decide which creative works the best
    and maximize the CTR over the course of the campaign?**
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，你在进行一场电子邮件营销活动，但营销部门没有人能决定5条广告文案中哪一条最有效。你的关键KPI将是点击率，**如果用户在收到邮件后的5天内点击广告，就会被测量**。现在假设所有用户的反应是一样的。进行活动的时间窗口很小。根据这些信息，**你将如何快速决定哪个创意效果最好，并在活动期间最大化CTR？**
- en: This article is broken up into a few sections. First we discuss the concept
    of the *grid*, the configuration of how many subjects receive the treatment during
    each batch. We move on to look at how to format an epsilon-greedy bandit algorithm
    as a batched bandit problem, and extend that to any bandit algorithm. We then
    study Batched Successive Elimination (BaSE) and it’s similarity to the radomized
    experiment (AB Test). Finally, we look at some experiments to study the effect
    of number of batches and number of arms and how those influence regret. We’ll
    see how those algorithms compare to a randomized experiment with the same grid
    configurations. A discussion around the experimental results is provided, which
    is followed by general guidelines concluded from the simulations.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本文分为几个部分。首先我们讨论*网格*的概念，即在每批次中有多少受试者接受治疗的配置。接着我们查看如何将epsilon-greedy多臂老虎机算法格式化为一个批次多臂老虎机问题，并将其扩展到任何多臂老虎机算法。然后，我们研究批次顺序消除（BaSE）及其与随机实验（AB测试）的相似性。最后，我们查看一些实验以研究批次数量和臂数量的影响，以及这些因素如何影响遗憾。我们将看到这些算法如何与具有相同网格配置的随机实验进行比较。文中提供了实验结果的讨论，并总结了从模拟中得出的通用指南。
- en: Recap the Bandit Problem
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回顾多臂老虎机问题
- en: 'Suppose we have a set of Actions **A** each corresponding to an *arm,* each
    with it’s own associated reward **R**. In our case, **R** will be a Bernoulli
    distributed random variable with a true mean equal to the CTR for our ad copy.
    In code this looks like:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个动作集合**A**，每个动作对应一个*臂*，每个臂都有自己的奖励**R**。在我们的案例中，**R**将是一个伯努利分布的随机变量，其真实均值等于我们广告文案的CTR。代码中如下所示：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The goal of the multi-bandit problem is given the set **A,** we want to learn
    which arm has the maximum payoff, or the highest true mean value. We define a
    *policy* as the function that tells us which arm is the best to pull. The catch
    here is that while we are learning, or *exploring* the action space, we are playing
    sub-optimal actions. Therefore, the point of bandit algorithms is to balance exploring
    the possible actions and then *exploiting* actions that appear promising.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 多臂老虎机问题的目标是在给定的集合**A**中，我们希望了解哪个臂的回报最大，或哪个臂的真实均值最高。我们将*策略*定义为告诉我们哪个臂是最值得拉的函数。这里的关键是，当我们在学习或*探索*动作空间时，我们是在执行次优动作。因此，多臂老虎机算法的关键在于平衡探索可能的动作和*利用*那些看起来有前景的动作。
- en: This article assumes readers will be familiar with the Multi-Armed Bandit problem
    and the epsilon-greedy approach to the explore-exploit problem. For those who
    are not, this [article](/solving-the-multi-armed-bandit-problem-b72de40db97c)
    gives a surface level overview. For a comprehensive overview, I recommend Sutton
    and Barto [1] Chapter 2 as a Reference.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本文假设读者对多臂老虎机问题和epsilon-greedy方法有一定了解。对于那些不熟悉的人，这篇[文章](/solving-the-multi-armed-bandit-problem-b72de40db97c)提供了一个表面层次的概述。对于全面了解，我推荐Sutton和Barto的[1]第2章作为参考。
- en: Introducing the Grid
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍网格
- en: As alluded to above, in the batched bandit problem, we do not receive instantaneous
    rewards. Therefore we need to be strategic about how we select actions and update
    our agent’s *policy.* This introduces the concept of the *grid*, how many users
    to sample on each batch such that the agent is able to learn optimally. Perchet
    et al [2] introduced the Batched Bandit Problem in their paper and also introduced
    the grid. To formalize the grid, I use the notation from Gao et al [3].
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，在批处理赌徒问题中，我们无法获得即时奖励。因此，我们需要策略性地选择行动并更新代理的*策略*。这引入了*网格*的概念，即每个批次采样多少用户，以便代理能够最佳地学习。Perchet等人[2]在他们的论文中介绍了批处理赌徒问题并介绍了网格。为了形式化网格，我使用了Gao等人[3]的符号。
- en: The first grid provided is the arithmetic grid, which is quite trivial. This
    grid evenly divides the time horizon T into M equal batches. When M=T, this is
    equivalent to the traditional bandit problem with instant rewards.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 提供的第一个网格是算术网格，这相当简单。这个网格将时间范围T均匀地划分为M个相等的批次。当M=T时，这等同于传统的即时奖励赌徒问题。
- en: 'The second grid we use is the minimax grid, which aims to minimize the maximum
    regret. The equation for the ith term is:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的第二个网格是最小最大网格，其目的是最小化最大遗憾。第i项的方程式为：
- en: 'The third grid provided is the geometric grid, which optimizes the regret with
    respect to a maximum regret bound. The equation for the ith terms is:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 提供的第三个网格是几何网格，它优化了相对于最大遗憾界限的遗憾。第i项的方程式为：
- en: For more information about the intuition behind the grid origins, I recommend
    the discussion in [2].
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 关于网格来源直观理解的更多信息，我推荐在[2]中的讨论。
- en: Extending Traditional Bandit Problems to Batched Bandit Problems
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将传统赌徒问题扩展到批处理赌徒问题
- en: The epsilon-greedy algorithm is the usual introduction to the explore-exploit
    tradeoff fundamental to reinforcement learning. For this reason, I’m opting to
    start with converting the epsilon-greedy algorithm to the batched framework, and
    then show how this can be extended to any bandit algorithm.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: epsilon-贪婪算法通常是介绍强化学习中探索与利用权衡的起点。因此，我选择从将epsilon-贪婪算法转换为批处理框架开始，然后展示如何将其扩展到任何赌徒算法。
- en: 'The difference between the batched bandit problem and the regular bandit problem
    is simply when the agent is allowed to update the policy. A typical bandit algorithm
    might look something like this:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理赌徒问题和常规赌徒问题的区别在于代理更新策略的时间。一个典型的赌徒算法可能如下所示：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'However in the batched bandit, we do not get rewards in real time and must
    wait until the end of the batch to update the agent’s policy. One key point is
    on the last batch the experiment is over. Exploring on the last batch provides
    no utility, since there are no future batches, so we opt to go fully greedy on
    the last batch. Here’s an example of what this might look like as an adaptation
    to our code above:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在批处理赌徒中，我们不能实时获得奖励，必须等到批次结束后才更新代理的策略。一个关键点是，在最后一个批次中实验已经结束。在最后一个批次进行探索没有用处，因为没有未来的批次，所以我们选择在最后一个批次完全贪婪。以下是这可能如何适应我们上述代码的示例：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We can trivially extend this to any class of bandit algorithm. By replacing
    when the bandit is allowed to update, any algorithm can be used with any grid
    in the batched bandit framework.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将此简单地扩展到任何类别的赌徒算法。通过更换允许更新的时机，任何算法都可以在批处理赌徒框架中与任何网格一起使用。
- en: In the context of an email campaign, we may decide to target 5000 users (T=5000).
    Depending on the time frame available, we can pick a number of batches (M = num_days_available
    / num_days_response). Let’s say we need to launch the campaign in 30 days and
    it takes 5 days for a response, then the number of batches we can run is 6\. We
    want to explore in the first 5 batches, but on the last batch our campaign is
    over, so on this batch we commit to the best action.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在电子邮件营销的背景下，我们可能决定针对5000名用户（T=5000）。根据可用的时间框架，我们可以选择一些批次（M = num_days_available
    / num_days_response）。假设我们需要在30天内启动活动，而响应需要5天，那么我们可以运行的批次数为6。我们希望在前5个批次中进行探索，但在最后一个批次中，我们的活动已经结束，所以在这个批次中我们会承诺采取最佳行动。
- en: Batched Successive Elimination (BaSE)
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批处理连续消除（BaSE）
- en: As shown above, it’s easy to extend any bandit algorithm to the batched framework.
    Gao et al [3] showed this with their adaptation of Successive Elimination to the
    batched setting. Successive Elimination (SE) works by pruning less promising arms
    out of the candidate set as early — yet responsibly — as possible. To do this
    we sample each arm randomly during the batch. At the end of the batch we construct
    a confidence threshold as follows
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，任何强盗算法都很容易扩展到批量框架。Gao等人[3]通过将逐次淘汰法（SE）适应到批量环境中来展示了这一点。逐次淘汰（SE）通过尽可能早地修剪掉候选集中的较不有前途的臂来工作。为此，我们在批量过程中随机抽样每个臂。在批量结束时，我们构造置信阈值如下
- en: Confidence threshold for pruning arms
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 修剪臂的置信阈值
- en: where gamma is a scaling factor, T is the time horizon, K is the number of arms,
    and tau_m is the number of observations that have been sampled up to that point.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 其中gamma是缩放因子，T是时间范围，K是臂的数量，tau_m是截至目前已抽样的观测次数。
- en: To decide if an arm stays in the candidate set, we take the difference between
    the cumulative rewards for the max arm and the cumulative rewards for each other
    arm. If the difference between the average value and the current arm is greater
    than our confidence threshold, then that arm is removed from the set of Arms being
    explored. Below is the pseudo code for the algorithm provided by the authors.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了决定一个臂是否留在候选集，我们取最大臂的累计奖励与每个其他臂的累计奖励之间的差异。如果平均值与当前臂之间的差异大于我们的置信阈值，则该臂将从被探索的臂集移除。下面是作者提供的算法伪代码。
- en: '![](../Images/9f5acd84822ada21d073c8ba70eb5bd4.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9f5acd84822ada21d073c8ba70eb5bd4.png)'
- en: Algorithm For BaSE. Image taken from [Gao et al. [3]](https://arxiv.org/abs/1904.01763)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: BaSE算法。图像取自 [Gao et al. [3]](https://arxiv.org/abs/1904.01763)
- en: An interesting note for the BaSE agent is how similar it is to a randomized
    experiment. Noticing step (a), we randomly sample each arm in the set A and observe
    the reward, exactly as we would in the randomized experiment. The difference is
    in the pruning step (b), where we successively attempt to remove a candidate arm
    from the current Arm set based on available information. As alluded to at the
    beginning of the article, most practitioners do not run proper AB tests, but rather
    opt to manually review and prune less promising arms manually. BaSE mimics this
    process by introducing an automatic heuristic that could remove the need for human
    intervention.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于BaSE代理，一个有趣的观察是它与随机实验的相似性。注意步骤(a)，我们随机抽样每个臂集A中的臂并观察奖励，就像在随机实验中一样。不同之处在于修剪步骤(b)，我们根据可用信息逐步尝试从当前臂集移除候选臂。如文章开头所述，大多数从业者并不进行正规的AB测试，而是选择手动审查和修剪较不有前途的臂。BaSE通过引入自动启发式来模拟这一过程，从而可能减少对人工干预的需求。
- en: Experiments
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验
- en: To understand the algorithms in the batched environment we’ll look at a couple
    of experiments with number of batches and the number of arms. Each algorithm was
    run for 100 trials with T=5000 for each grid.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解批量环境中的算法，我们将查看几个实验，涉及批次数和臂的数量。每个算法都进行了100次试验，每个网格的T=5000。
- en: To test the effect of number of batches, an experiment was conducted using a
    fixed set of arms with means 0.5, 0.51, and 0.6\. The number of batches was tested
    at values of 2, 4, 6, and 8\. Each agent was run for each of the three grids presented
    above. To keep the discussion simple, the best performing bandit and grid combination
    was selected. Results are shown in the figure below.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试批次数的效果，进行了一项实验，使用均值为0.5、0.51和0.6的固定臂集。批次数测试了2、4、6和8的值。每个代理都在上述三个网格中运行。为了简化讨论，选择了表现最佳的强盗算法和网格组合。结果如下面的图所示。
- en: '![](../Images/8c64302727f0e78a3341b4ea928e08e5.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c64302727f0e78a3341b4ea928e08e5.png)'
- en: Distributions for each M for each agent during batch experiment
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 每个代理在批量实验中的每个M的分布
- en: To test the effect of the number of arms, an experiment was conducted looking
    at performance on different sets of arms. Each arm set contained an optimal arm
    with mean 0.6 and for each point of the experiment an arm was added to the arm
    set with mean around 0.5\. This was repeated to generate arm sets with cardinality
    between 2 and 6\. The batch size for this experiment was fixed to M=6\. The results
    of the experiment are below.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试臂的数量对效果的影响，进行了一项实验，考察了不同臂集的表现。每个臂集包含一个均值为0.6的最佳臂，对于实验的每个点，臂集里会添加一个均值约为0.5的臂。这一过程重复进行，以生成臂集的基数在2到6之间。该实验的批量大小固定为M=6。实验结果如下。
- en: '![](../Images/039755479ce8efd20f916abacde5061e.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/039755479ce8efd20f916abacde5061e.png)'
- en: Distributions for each K for each agent during arm experiment
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 每个代理在手臂实验中的每个K的分布
- en: Full results from the experiments can be found in the repo in [this](https://github.com/sms1097/batched_bandits/blob/main/tutorial.ipynb)
    notebook.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 实验的完整结果可以在[这个](https://github.com/sms1097/batched_bandits/blob/main/tutorial.ipynb)笔记本中的仓库里找到。
- en: Analysis of Experiments
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验分析
- en: As noticed from the experiments, all agents performed the best on the geometric
    grid for almost all settings of the experiment. The intuition behind this comes
    from the number of sampled individuals before the final batch.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 从实验中可以看出，所有代理在几乎所有实验设置下，在几何网格上的表现最佳。这一现象的直观解释来自于最终批次之前的样本数量。
- en: The figure below shows the cumulative number of subjects treated during each
    batch for each grid in a scenario where M=6 and T=5000\. The drastic difference
    here is that the geometric grid treats substantially less subjects than the arithmetic
    or the minimax grid before the final batch, where the agent opts for a full greedy
    strategy. This means that the agents on the geometric grid are able to exploit
    more subjects than those on the minimax or arithmetic grid, which leads to better
    performance. These findings are supported by the ideas found in Bayati et al [4]
    where the authors do a deep analysis as to why greedy algorithms achieve surprisingly
    low regret compared with other algorithms.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了在M=6和T=5000的情况下，每个网格每个批次的累计处理样本数量。显著的差异在于，几何网格在最终批次之前处理的样本数量远少于算术网格或最小最大网格，其中代理选择了完全贪婪策略。这意味着在几何网格上的代理能够利用比在最小最大网格或算术网格上的更多样本，从而表现更好。这些发现得到了Bayati等人[4]的理论支持，作者对贪婪算法为何相比其他算法能够获得意外低的遗憾进行了深入分析。
- en: '![](../Images/43eafbbb1cbe12595e8a6176d0a4849d.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/43eafbbb1cbe12595e8a6176d0a4849d.png)'
- en: Cumulative number of subjects sampled per batch for each grid.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 每个网格每批次的累计样本数量。
- en: This trend however does not generalize to grids with smaller batch numbers.
    For the case where M=2 the number of samples in the first batch of the geometric
    grid is quite small. In this scenario, the better alternative is to consider either
    a minimax grid or in the case of sparse rewards (ie an extremely small mean for
    the arm’s rewards) an arithmetic grid might be appropriate.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这一趋势并不能推广到批次数量较小的网格。在M=2的情况下，几何网格中第一个批次的样本数量相当少。在这种情况下，更好的选择是考虑最小最大网格，或者在稀疏奖励的情况下（即手臂奖励的平均值极小）算术网格可能更合适。
- en: During both experiments the randomized agent (AB Agent) and BaSE agent display
    very similar performance. This is true only for the geometric grid for the advantages
    in exploration discussed above. While BaSE does introduce a confidence interval
    for pruning arms early, this confidence interval isn’t always triggered before
    the final batch and results in the similar performance to the randomized trial.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在两个实验中，随机代理（AB代理）和BaSE代理表现非常相似。这仅在几何网格上成立，因为前面讨论了探索方面的优势。虽然BaSE确实引入了一个置信区间用于提前剪枝，但这个置信区间并不总是在最终批次之前被触发，因此结果与随机试验相似。
- en: This problem of triggering the confidence threshold highlights the problem of
    hyperparameters in experimentation systems. BaSE and Epsilon-Greedy both suffer
    from this problem. Looking at the Epsilon-Greedy agent we can see that this agent
    has extreme variability between trials. This is due to the static hyperparameters
    used between trials. When using an agent like BaSE or Epsilon-Greedy, it is important
    to pick hyperparameters appropriate for you problem. These parameters can be determined
    by a simulation before your experiment.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 触发置信阈值的问题突显了实验系统中超参数的问题。BaSE和Epsilon-Greedy都存在这个问题。观察Epsilon-Greedy代理，我们可以看到该代理在试验之间有极大的变异性。这是由于在试验之间使用的静态超参数。当使用像BaSE或Epsilon-Greedy这样的代理时，选择适合问题的超参数是很重要的。这些参数可以通过实验前的模拟来确定。
- en: A surprising note of the experiment comes from the Thompson Sampling Agent (TS
    Agent) that had relatively stable performance between trials. The TS Agent does
    not suffer from the hyperparameter problem previously discussed, but does require
    some prior knowledge. To use the TS Agent, an implementation must know the prior
    distribution and support a derivation of the posterior distribution to update
    the agent. In the case of CTR, this is easy since we can sample results from a
    Beta distribution and the posterior of the Beta distribution is the Beta distribution.
    If the reward signal you are working with is continuous this becomes trickier
    to make sure your prior is correct. If you’re curious about learning more about
    Thompson Sampling, this [article](/thompson-sampling-fc28817eacb8) provides a
    good surface level introduction and Russo et al. [5] provides a comprehensive
    overview.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 实验中的一个令人惊讶的结果来自Thompson采样代理（TS代理），它在试验之间表现相对稳定。TS代理不会受到之前讨论的超参数问题的影响，但确实需要一些先验知识。使用TS代理时，实施必须知道先验分布，并支持后验分布的推导以更新代理。在CTR的情况下，这是容易的，因为我们可以从Beta分布中采样结果，并且Beta分布的后验分布仍然是Beta分布。如果你使用的奖励信号是连续的，那么确保你的先验正确会变得更棘手。如果你对了解更多关于Thompson采样的内容感兴趣，[这篇文章](/thompson-sampling-fc28817eacb8)提供了一个良好的表面级介绍，而Russo等人[5]提供了全面的概述。
- en: 'The following seem to be some safe guidelines for general practice based on
    the simulation:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模拟，以下是一些通用实践的安全指南：
- en: If the experiment is going to be managed (human interaction between batches),
    then the BaSE agent with human judgement on key KPIs is a good option to determine
    when to enter the exploit phase
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果实验需要管理（批次之间有人互动），那么带有人类对关键KPI判断的BaSE代理是一个不错的选择，以确定何时进入开发阶段。
- en: If the underlying distribution is known and the experiment will be fully automated,
    then Thompson Sampling is a good option
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果已知基础分布且实验将完全自动化，那么Thompson采样是一个不错的选择。
- en: If the underlying distribution is not known or has a complicated posterior and
    the experiment is fully automated, then carefully considered parameters for an
    epsilon greedy agent or a BaSE agent are good options
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果基础分布未知或具有复杂的后验分布且实验完全自动化，那么为epsilon贪婪代理或BaSE代理仔细考虑的参数是不错的选择。
- en: It’s important to note these takeaways apply generally to these arm distributions.
    Depending on your scenario these agents could respond differently. For this reason,
    it’s advised to run your own simulations to gauge how your experiment should be
    constructed, based on loose expectations of the rewards from your Arms.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，这些总结通常适用于这些臂分布。根据你的情况，这些代理可能会有不同的响应。因此，建议进行你自己的模拟，以评估实验的构建方式，这应基于你对臂的奖励的宽松预期。
- en: Conclusion
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: This was a brief introduction to some ideas on how to convert bandit algorithms
    to batched bandit algorithms. All of these algorithms have been implemented and
    are available for you to access in the repo. For further study I would suggest
    looking at the algorithms proposed in [2] and [3], which provide deep intuition
    into the batched bandit problem and some of the underlying proofs for regret bounds.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是对将多臂老虎机算法转换为批处理多臂老虎机算法的一些想法的简要介绍。所有这些算法都已实现，并可以在仓库中访问。为了进一步学习，我建议查看[2]和[3]中的算法，这些算法提供了对批处理多臂老虎机问题的深刻直觉以及一些关于遗憾界限的基础证明。
- en: You can access the Repo [Here](https://github.com/sms1097/batched_bandits.git)!
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过 [这里](https://github.com/sms1097/batched_bandits.git) 访问这个仓库！
- en: '*All images unless otherwise noted are by the author.*'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有说明，所有图片均由作者提供。*'
- en: '[1] Sutton, R. S., & Barto, A. G. (2018). *Reinforcement learning: An introduction*.
    MIT press.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Sutton, R. S., & Barto, A. G. (2018). *强化学习：入门*. MIT出版社。'
- en: '[2] Vianney Perchet, Philippe Rigollet, Sylvain Chassang, & Erik Snowberg (2016).
    Batched bandit problems*. The Annals of Statistics, 44(2).*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Vianney Perchet, Philippe Rigollet, Sylvain Chassang, & Erik Snowberg (2016).
    *批处理多臂老虎机问题*. 《统计年刊》，44(2)。'
- en: '[3] Gao, Z., Han, Y., Ren, Z., & Zhou, Z.. (2019). Batched Multi-armed Bandits
    Problem.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Gao, Z., Han, Y., Ren, Z., & Zhou, Z.. (2019). *批处理多臂老虎机问题*。'
- en: '[4] Bayati, M., Hamidi, N., Johari, R., & Khosravi, K.. (2020). The Unreasonable
    Effectiveness of Greedy Algorithms in Multi-Armed Bandit with Many Arms.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Bayati, M., Hamidi, N., Johari, R., & Khosravi, K.. (2020). 《在多臂老虎机中贪婪算法的非凡有效性》。'
- en: '[5] Russo, D., Van Roy, B., Kazerouni, A., Osband, I., & Wen, Z. (2017). A
    Tutorial on Thompson Sampling*.*'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Russo, D., Van Roy, B., Kazerouni, A., Osband, I., & Wen, Z. (2017). *Thompson采样教程*。'
- en: '*Thanks for reading the article! My area of focus is in personalization and
    experimentation. If you’re interested in keeping up to date on my work, please
    follow me on* [*Medium*](https://medium.com/@smsmith714)*! I also post more frequent
    updates on* [*LinkedIn*](https://www.linkedin.com/in/sms714/)*, if you’re interested
    in those feel free to follow me there as well.*'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*感谢阅读这篇文章！我的专注领域是个性化和实验。如果你有兴趣了解我的最新工作，请在* [*Medium*](https://medium.com/@smsmith714)*上关注我！我也会在*
    [*LinkedIn*](https://www.linkedin.com/in/sms714/)*上发布更频繁的更新，如果你对此感兴趣，也可以在那里关注我。*'
