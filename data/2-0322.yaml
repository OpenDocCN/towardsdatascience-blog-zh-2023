- en: 'Applied Reinforcement Learning VI: Deep Deterministic Policy Gradients (DDPG)
    for Continuous Control'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用强化学习 VI：用于连续控制的深度确定性策略梯度（DDPG）
- en: 原文：[https://towardsdatascience.com/applied-reinforcement-learning-vi-deep-deterministic-policy-gradients-ddpg-for-continuous-dad372f6cb1d](https://towardsdatascience.com/applied-reinforcement-learning-vi-deep-deterministic-policy-gradients-ddpg-for-continuous-dad372f6cb1d)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/applied-reinforcement-learning-vi-deep-deterministic-policy-gradients-ddpg-for-continuous-dad372f6cb1d](https://towardsdatascience.com/applied-reinforcement-learning-vi-deep-deterministic-policy-gradients-ddpg-for-continuous-dad372f6cb1d)
- en: Introduction and theoretical explanation of the DDPG algorithm, which has many
    applications in the field of continuous control
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DDPG算法的介绍和理论解释，这在连续控制领域有许多应用。
- en: '[](https://medium.com/@JavierMtz5?source=post_page-----dad372f6cb1d--------------------------------)[![Javier
    Martínez Ojeda](../Images/5b5df4220fa64c13232c29de9b4177af.png)](https://medium.com/@JavierMtz5?source=post_page-----dad372f6cb1d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dad372f6cb1d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dad372f6cb1d--------------------------------)
    [Javier Martínez Ojeda](https://medium.com/@JavierMtz5?source=post_page-----dad372f6cb1d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@JavierMtz5?source=post_page-----dad372f6cb1d--------------------------------)[![Javier
    Martínez Ojeda](../Images/5b5df4220fa64c13232c29de9b4177af.png)](https://medium.com/@JavierMtz5?source=post_page-----dad372f6cb1d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----dad372f6cb1d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----dad372f6cb1d--------------------------------)
    [Javier Martínez Ojeda](https://medium.com/@JavierMtz5?source=post_page-----dad372f6cb1d--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dad372f6cb1d--------------------------------)
    ·8 min read·Mar 7, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----dad372f6cb1d--------------------------------)
    ·阅读时间 8 分钟·2023年3月7日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/b37d51029a59d2af9831889b91939bda.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b37d51029a59d2af9831889b91939bda.png)'
- en: Photo by [Eyosias G](https://unsplash.com/pt-br/@yozz_t?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Eyosias G](https://unsplash.com/pt-br/@yozz_t?utm_source=medium&utm_medium=referral)
    在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: If you want to read this article without a Premium Medium account, you can do
    it from this friend link :)
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你想在没有Premium Medium账户的情况下阅读这篇文章，你可以通过这个朋友链接 :)
- en: '[https://www.learnml.wiki/applied-reinforcement-learning-vi-deep-deterministic-policy-gradients-ddpg-for-continuous-control/](https://www.learnml.wiki/applied-reinforcement-learning-vi-deep-deterministic-policy-gradients-ddpg-for-continuous-control/)'
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://www.learnml.wiki/applied-reinforcement-learning-vi-deep-deterministic-policy-gradients-ddpg-for-continuous-control/](https://www.learnml.wiki/applied-reinforcement-learning-vi-deep-deterministic-policy-gradients-ddpg-for-continuous-control/)'
- en: The **DDPG algorithm**, first presented at ICLR 2016 by Lillicarp et al. **[1]**,
    was a significant breakthrough in terms of Deep Reinforcement Learning algorithms
    for continuous control, because of its improvement over DQN **[2]** (which only
    works with discrete actions), and its very good results and ease of implementation
    (see **[1]**).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**DDPG算法**，由Lillicarp等人在ICLR 2016首次提出 **[1]**，在深度强化学习算法用于连续控制方面取得了重大突破，因为它比DQN
    **[2]**（仅适用于离散动作）有了改进，且其结果优异且易于实现（见**[1]**）。'
- en: As for the **NAF algorithm** **[3]** presented in the [previous article](https://medium.com/towards-data-science/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095),
    DDPG works with continuous action spaces and continuous state spaces, making it
    an equally valid choice for continuous control tasks applicable to fields such
    as Robotics or Autonomous Driving.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 关于**NAF算法** **[3]**，该算法在[上一篇文章](https://medium.com/towards-data-science/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095)中介绍，DDPG适用于连续动作空间和连续状态空间，因此它也是适用于机器人技术或自动驾驶等领域的连续控制任务的有效选择。
- en: '[](/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095?source=post_page-----dad372f6cb1d--------------------------------)
    [## Applied Reinforcement Learning V: Normalized Advantage Function (NAF) for
    Continuous Control'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095?source=post_page-----dad372f6cb1d--------------------------------)
    [## 应用强化学习 VI：用于连续控制的归一化优势函数（NAF）'
- en: Introduction and explanation of the NAF algorithm, widely used in continuous
    control tasks
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NAF算法的介绍和解释，该算法广泛用于连续控制任务
- en: towardsdatascience.com](/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095?source=post_page-----dad372f6cb1d--------------------------------)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095?source=post_page-----dad372f6cb1d--------------------------------)'
- en: Logic of the DDPG algorithm
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DDPG算法的逻辑
- en: 'The DDPG algorithm is an Actor-Critic algorithm, which, as its name suggests,
    is composed of two neural networks: the **Actor** and the **Critic**. The Actor
    is in charge of choosing the best action, and the Critic must evaluate how good
    the chosen action was, and inform the actor how to improve it.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG算法是一种Actor-Critic算法，顾名思义，它由两个神经网络组成：**Actor**和**Critic**。Actor负责选择最佳动作，而Critic则必须评估所选动作的好坏，并告知Actor如何改进。
- en: The Actor is trained by applying policy gradient, while the Critic is trained
    by calculating the Q-Function. For this reason, the DDPG algorithm tries to learn
    an approximator to the optimal Q-function (Critic) and an approximator to the
    optimal policy (Actor) at the same time.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Actor通过应用策略梯度进行训练，而Critic通过计算Q函数进行训练。因此，DDPG算法试图同时学习最优Q函数的近似器（Critic）和最优策略的近似器（Actor）。
- en: '![](../Images/21274c16474ed2ceea16e312ef11fa9e.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/21274c16474ed2ceea16e312ef11fa9e.png)'
- en: Actor-Critic schema. Image extracted from **[4]**
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Actor-Critic模式。图像摘自**[4]**
- en: The optimal Q-Function **Q*(s, a)** gives the expected return for being in state
    **s**, taking action **a**, and then acting following the optimal policy. On the
    other hand, the optimal policy **𝜇*(s)** returns the action which maximizes the
    expected return starting from state **s**. According to these two definitons,
    the optimal action on a given state (i.e. the return of the optimal policy on
    a given state) can be obtained by getting the argmax of Q*(s, a) for a given state,
    as shown below.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最优Q函数**Q*(s, a)**给出了在状态**s**下，采取动作**a**并随后按照最优策略行动的期望回报。另一方面，最优策略**𝜇*(s)**返回在状态**s**下最大化期望回报的动作。根据这两个定义，给定状态下的最优动作（即最优策略在给定状态下的回报）可以通过获取给定状态下**Q*(s,
    a)**的argmax来获得，如下所示。
- en: '![](../Images/fc7660bca9ebc13b1e78fe71ea874636.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc7660bca9ebc13b1e78fe71ea874636.png)'
- en: Q-Function — Policy relation. Image by author
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Q函数与策略的关系。作者提供的图像
- en: The problem is that, for continuous action spaces, obtaining the action **a**
    which maximizes Q is not easy, because it would be impossible to calculate Q for
    every possible value of **a** to check which result is the highest (which is the
    solution for discrete action spaces), since it would have infinite possible values.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，对于连续动作空间，获取最大化Q的动作**a**并不容易，因为计算每一个可能的**a**的Q值以检查哪个结果最高（这是离散动作空间的解决方案）几乎是不可能的，因为可能的值是无限的。
- en: As a solution to this, and assuming that the action space is continuous and
    that the Q-Function is differentiable with respect to the action, the DDPG algorithm
    approximates the calculation of ***maxQ(s, a)*** to ***Q(s, 𝜇(s))***, where ***𝜇(s)***(a
    deterministic policy) can be optimized performing gradient ascent.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 作为解决方案，并假设动作空间是连续的且Q函数对动作是可微的，DDPG算法将***maxQ(s, a)***近似为***Q(s, 𝜇(s))***，其中***𝜇(s)***（一个确定性策略）可以通过执行梯度上升来优化。
- en: In simple terms, DDPG learns an approximator to the optimal Q-Function in order
    to obtain the action that maximises it. Since, as the action space is continuous,
    the result of the Q-Function cannot be obtained for every possible value of the
    action, DDPG also learns an approximator to the optimal policy, in order to directly
    obtain the action that maximises the Q-Function.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，DDPG学习最优Q函数的近似器，以获得最大化该函数的动作。由于动作空间是连续的，Q函数的结果不能针对每一个可能的动作值来获得，DDPG还学习最优策略的近似器，以直接获得最大化Q函数的动作。
- en: The following sections explain how the algorithm learns both the approximator
    to the optimal Q-Function and the approximator to the optimal policy.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的部分解释了算法如何学习最优Q函数的近似器和最优策略的近似器。
- en: Learning of the Q-Function
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q函数的学习
- en: Mean-Squared Bellman Error Function
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 均方贝尔曼误差函数
- en: The learning of the Q-Function is carried out using as base the Bellman equation,
    previously introduced in [the first article of this series](https://medium.com/towards-data-science/applied-reinforcement-learning-i-q-learning-d6086c1f437).
    As in the DDPG algorithm the Q-Function is not calculated directly, but a neural
    network denoted ***Qϕ(s, a)*** is used as an approximator of the Q-Function, instead
    of the Bellman equation a loss function called **Mean Squared Bellman Error (MSBE)**
    is used. This function, shown in *Figure 1*, indicates how well the approximator
    *Qϕ(s, a)*satisfies the Bellman equation.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Q-函数的学习是基于贝尔曼方程的，该方程在[本系列的第一篇文章](https://medium.com/towards-data-science/applied-reinforcement-learning-i-q-learning-d6086c1f437)中已介绍。由于在
    DDPG 算法中 Q-函数不是直接计算的，而是使用了一个神经网络 ***Qϕ(s, a)*** 作为 Q-函数的近似器，因此使用了一个称为 **均方贝尔曼误差
    (MSBE)** 的损失函数。该函数如 *图 1* 所示，指示近似器 *Qϕ(s, a)* 如何满足贝尔曼方程。
- en: '![](../Images/4485e93d6884d97d4ec06e4543ff9ed7.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4485e93d6884d97d4ec06e4543ff9ed7.png)'
- en: '**Figure 1.** Mean-Squared Bellman Error (MSBE). Image extracted from **[5]**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 1.** 均方贝尔曼误差 (MSBE)。图像摘自 **[5]**'
- en: The goal of DDPG is to minimize this error function, which will cause the approximator
    to the Q-Function to satisfy Bellman’s equation, which implies that the approximator
    is optimal.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG 的目标是最小化这个误差函数，这将使 Q-函数的近似器满足贝尔曼方程，这意味着近似器是最优的。
- en: Replay Buffer
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回放缓冲区
- en: 'The data required to minimize the MSBE function (i.e. to train the neural network
    to approximate Q*(s, a)), is extracted from a **Replay Buffer** where the experiences
    lived during the training are stored. This Replay Buffer is represented in *Figure
    1* as ***D***, from which the data required for calculating the loss is obtained:
    state **s**, action **a**, reward **r**, next state **s’** and done **d**. If
    you are not familiar with the Replay Buffer, it was explained in the articles
    about the [DQN algorithm](https://medium.com/towards-data-science/applied-reinforcement-learning-iii-deep-q-networks-dqn-8f0e38196ba9)
    and [NAF algorithm](https://medium.com/towards-data-science/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095),
    and implemented and applied in the article about the [implementation of DQN](https://medium.com/towards-data-science/applied-reinforcement-learning-iv-implementation-of-dqn-7a9cb2c12f97).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最小化 MSBE 函数（即训练神经网络以近似 Q*(s, a)），所需的数据是从 **回放缓冲区** 中提取的，该缓冲区存储了训练过程中经历的经验。这个回放缓冲区在
    *图 1* 中表示为 ***D***，从中获取计算损失所需的数据：状态 **s**，动作 **a**，奖励 **r**，下一个状态 **s’** 和完成 **d**。如果你对回放缓冲区不熟悉，它在关于
    [DQN 算法](https://medium.com/towards-data-science/applied-reinforcement-learning-iii-deep-q-networks-dqn-8f0e38196ba9)
    和 [NAF 算法](https://medium.com/towards-data-science/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095)
    的文章中已有解释，并在关于 [DQN 实现](https://medium.com/towards-data-science/applied-reinforcement-learning-iv-implementation-of-dqn-7a9cb2c12f97)
    的文章中进行了实现和应用。
- en: Target Neural Network
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标神经网络
- en: 'The minimization of the MSBE function consists of making the approximator to
    the Q-Function, *Qϕ(s, a)*, as close as possible to the other term of the function,
    **the target**, which originally has the following form:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: MSBE 函数的最小化包括使 Q-函数的近似器，*Qϕ(s, a)*，尽可能接近函数的另一个项，即 **目标**，其原始形式如下：
- en: '![](../Images/d58029862efbecaec31321f03282b10c.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d58029862efbecaec31321f03282b10c.png)'
- en: '**Figure 2.** Target. Extracted from Figure 1 **[5]**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.** 目标。摘自图 1 **[5]**'
- en: As can be seen, the target depends on the same parameters to be optimized, **ϕ**,
    which makes the minimization unstable. Therefore, as a solution another neural
    network is used, containing the parameters of the main neural network but with
    a certain delay. This second neural network is called **target network, *Qϕtarg(s,
    a)***(see *Figure 3*)**,** and its parameters are denoted ***ϕtarg***.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，目标依赖于待优化的相同参数，**ϕ**，这使得最小化变得不稳定。因此，作为解决方案，使用了另一个神经网络，其中包含主神经网络的参数，但有一定的延迟。这个第二个神经网络被称为
    **目标网络，*Qϕtarg(s, a)*（见*图 3*）**，其参数表示为 ***ϕtarg***。
- en: However, in *Figure 2* it can be seen how, when substituting *Qϕ(s, a)* for
    *Qϕtarg(s, a)*, it is necessary to obtain the action that maximizes the output
    of this target network, which, as explained above, is complicated for continuous
    action space environments. This is solved by making use of a **target policy network,
    *𝜇ϕtarg(s)*** (see *Figure 3*), which approximates the action that maximizes the
    output of the target network. In other words, a target policy network *𝜇ϕtarg(s)*has
    been created to solve the problem of continuous actions for *Qϕtarg(s, a)*, just
    as was done with *𝜇ϕ(s)* and *Qϕ(s, a)*.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在*图 2*中可以看到，当将 *Qϕ(s, a)* 替换为 *Qϕtarg(s, a)* 时，必须获得最大化该目标网络输出的动作。正如上文所述，这对于连续动作空间环境来说是复杂的。这通过利用**目标策略网络，*𝜇ϕtarg(s)*（见*图
    3*）**来解决，该网络逼近了最大化目标网络输出的动作。换句话说，创建了一个目标策略网络 *𝜇ϕtarg(s)* 来解决 *Qϕtarg(s, a)* 的连续动作问题，就像之前对
    *𝜇ϕ(s)* 和 *Qϕ(s, a)* 所做的那样。
- en: Minimize the modified MSBE Function
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最小化修改后的 MSBE 函数
- en: With all this, the learning of the optimal Q-Function by the DDPG algorithm
    is carried out minimizing the modified MSBE function in *Figure 3*, by applying
    gradient descent on it.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这一切，DDPG 算法通过对*图 3*中的修改后的 MSBE 函数应用梯度下降来学习最优 Q-函数。
- en: '![](../Images/fc9ef5c014b0479ce017276e9bd2589c.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc9ef5c014b0479ce017276e9bd2589c.png)'
- en: '**Figure 3\.** Modified Mean-Squared Bellman Error. Extracted from **[5]**
    and edited by author'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 3\.** 修改后的均方贝尔曼误差。摘自**[5]**并由作者编辑'
- en: Learning of the Policy
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略学习
- en: 'Given that the action space is continuous, and that the Q-Function is differentiable
    with respect to the action, DDPG learns the deterministic policy ***𝜇ϕ(s)*** that
    maximizes *Qϕ(s, a)* by applying gradient ascent on the function below with respect
    to the deterministic policy’s parameters:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 由于动作空间是连续的，并且 Q-函数对动作是可微分的，DDPG 通过对以下函数应用梯度上升来学习最大化 *Qϕ(s, a)* 的确定性策略***𝜇ϕ(s)***，该函数是关于确定性策略参数的：
- en: '![](../Images/a363a03aeb9fe79da15bf022dff83a3a.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a363a03aeb9fe79da15bf022dff83a3a.png)'
- en: '**Figure 4.** Function to optimize for the deterministic policy learning. Extracted
    from **[5]**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 4.** 确定性策略学习的优化函数。摘自**[5]**'
- en: DDPG Algorithm Flow
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DDPG 算法流程
- en: The flow of the DDPG algorithm will be presented following the pseudocode below,
    extracted from **[1]**. The DDPG algorithm follows the same steps as other Q-Learning
    algorithms for function approximators, such as the DQN or NAF algorithm.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG 算法的流程将按照以下伪代码展示，摘自**[1]**。DDPG 算法遵循与其他函数逼近 Q-Learning 算法相同的步骤，例如 DQN 或
    NAF 算法。
- en: '![](../Images/75b55a12daffd419a15512867063bdfc.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75b55a12daffd419a15512867063bdfc.png)'
- en: DDPG Algorithm Pseudocode. Extracted from **[1]**
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG 算法伪代码。摘自**[1]**
- en: 1\. Initialize Critic, Critic Target, Actor and Actor Target networks
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 初始化 Critic、Critic 目标、Actor 和 Actor 目标网络
- en: Initialize the Actor and Critic neural networks to be used during training.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化在训练过程中使用的 Actor 和 Critic 神经网络。
- en: The Critic network, *Qϕ(s, a)*, acts as an approximator to the Q-Function.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Critic 网络，*Qϕ(s, a)*，作为 Q-函数的逼近器。
- en: The Actor network, *𝜇ϕ(s),* acts as an approximator to the deterministic policy
    and is used to obtain the action that maximizes the output of the Critic network.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Actor 网络，*𝜇ϕ(s)*，作为确定性策略的逼近器，用于获取最大化 Critic 网络输出的动作。
- en: Once initialized, the target networks are initialized with the same architecture
    as their corresponding main networks, and the weights of the main networks are
    copied into the target networks.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦初始化，目标网络将与其相应的主网络具有相同的架构，并且主网络的权重会被复制到目标网络中。
- en: The Critic Target network, *Qϕtarg(s, a),* acts as a delayed Critic network,
    so that the target does not depend on the same parameters to be optimized, as
    explained before.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Critic 目标网络，*Qϕtarg(s, a)*，作为一个延迟的 Critic 网络，以便目标不依赖于需要优化的相同参数，正如之前所解释的。
- en: The Actor Target network, *𝜇ϕtarg(s),* acts as a delayed Actor network, and
    it used to obtain the action that maximizes the output of the Critic Target network.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Actor 目标网络，*𝜇ϕtarg(s)*，作为一个延迟的 Actor 网络，用于获取最大化 Critic 目标网络输出的动作。
- en: 2\. Initialize Replay Buffer
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 初始化 Replay Buffer
- en: The Replay Buffer to be used for the training is initialized empty.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练的 Replay Buffer 被初始化为空。
- en: '*For each timestep in an episode, the agent performs the following steps:*'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*在一个时间步中，智能体执行以下步骤：*'
- en: 3\. Select action and apply noise
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 选择动作并施加噪声
- en: The best action for the current state is obtained from the output of the Actor
    neural network, which approximates the deterministic policy *𝜇ϕ(s).* The noise
    extracted from a **Ornstein Uhlenbeck Noise** process **[6]**, or from an **uncorrelated,
    mean-zero Gaussian distribution** **[7]** is then applied to the selected action.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Actor 神经网络的输出中获得当前状态的最佳动作，该网络逼近确定性策略 *𝜇ϕ(s).* 从**Ornstein Uhlenbeck 噪声**过程**[6]**或**无关的、均值为零的高斯分布**
    **[7]** 中提取的噪声然后被应用于选定的动作。
- en: 4\. Perform the action and store observations in Replay Buffer
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 执行动作并将观察结果存储在 Replay Buffer 中
- en: The noisy action is performed in the environment. After that, the environment
    returns a **reward** indicating how good the action taken was, the **new state**
    reached after performing the action, and a boolean indicating whether a **terminal
    state** has been reached.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在环境中执行有噪声的动作。之后，环境返回一个**奖励**，指示采取的行动的效果，执行该动作后达到的**新状态**，以及一个布尔值，指示是否已达到**终止状态**。
- en: This information, together with the **current state** and the **action taken**,
    is stored in the Replay Buffer, to be used later to optimize the Critic and Actor
    neural networks.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这些信息连同**当前状态**和**采取的行动**一起存储在 Replay Buffer 中，稍后用于优化 Critic 和 Actor 神经网络。
- en: 5\. Sample batch of experiences and train Actor and Critic networks
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 采样经验批次，并训练 Actor 和 Critic 网络
- en: This step is only performed when the Replay Buffer has enough experiences to
    fill a batch. Once this requirement is met, a batch of experiences is extracted
    from the Replay Buffer for use in training.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 只有当 Replay Buffer 中有足够的经验填满一个批次时，这一步骤才会执行。一旦满足此要求，就会从 Replay Buffer 中提取一个经验批次用于训练。
- en: 'With this batch of experiences:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这一批经验：
- en: The target is calculated and the output of the Critic network (the approximator
    of the Q-Function) is obtained, in order to then apply gradient descent on the
    MSBE error function, as shown in *Figure 3*. This step trains/optimizes the approximator
    to the Q-Function, *Qϕ(s, a).*
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算目标值，并获得 Critic 网络（Q-Function 的逼近器）的输出，然后在 MSBE 误差函数上应用梯度下降，如*图 3*所示。这一步骤训练/优化了
    Q-Function 的逼近器，*Qϕ(s, a)。*
- en: Gradient ascent is performed on the function shown in *Figure 4*, thus optimizing/training
    the approximator to the deterministic policy, *𝜇ϕ(s)*.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对*图 4*所示的函数执行梯度上升，从而优化/训练确定性策略的逼近器，*𝜇ϕ(s)*。
- en: 6\. Softly update the Target networks
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 软更新目标网络
- en: Both the Actor Target and the Critic Target networks are updated every time
    the Actor and Critic networks are updated, by **polyak averaging**, as shown in
    the figure below.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 每次更新 Actor 和 Critic 网络时，Actor Target 和 Critic Target 网络都通过**Polyak 平均**进行更新，如下图所示。
- en: '![](../Images/3a004bfe6f1fb9758bf9750fa17bf710.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a004bfe6f1fb9758bf9750fa17bf710.png)'
- en: '**Figure 5.** Polyak Averaging. Extracted from **[1]**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 5.** Polyak 平均。摘自**[1]**'
- en: '**Tau *τ***, the parameter that sets the weights of each element in the polyak
    averaging, is a hyperparameter to be set for the algorithm, which usually takes
    values close to 1.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**Tau *τ***，设置 Polyak 平均中每个元素权重的参数，是算法的一个超参数，通常取接近 1 的值。'
- en: Conclusion
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The DDPG algorithm proposed by Lillicrap et al. achieves very good results in
    most continuous environments available in Gym as shown in the paper that presented
    it **[1]**, demonstrating its ability to learn different tasks, regardless of
    their complexity, in a continuous context.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Lillicrap 等人提出的 DDPG 算法在 Gym 中大多数连续环境下取得了非常好的结果，如论文**[1]**所示，展示了它在连续环境中学习不同任务的能力，无论这些任务的复杂性如何。
- en: Therefore, this algorithm is still used today to enable an agent to learn an
    optimal policy for a complex task in a continuous environment, such as control
    tasks for manipulator robots, or obstacle avoidance for autonomous vehicles.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，该算法至今仍用于使智能体在连续环境中学习复杂任务的最优策略，如操作机器人控制任务或自主车辆的避障任务。
- en: REFERENCES
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '**[1]** LILLICRAP, Timothy P., et al. Continuous control with deep reinforcement
    learning. *arXiv preprint arXiv:1509.02971*, 2015.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**[1]** LILLICRAP, Timothy P., 等人。使用深度强化学习进行连续控制。*arXiv 预印本 arXiv:1509.02971*，2015。'
- en: '**[2]** MNIH, Volodymyr, et al. Playing atari with deep reinforcement learning.
    *arXiv preprint arXiv:1312.5602*, 2013.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**[2]** MNIH, Volodymyr, 等人。使用深度强化学习玩 Atari 游戏。*arXiv 预印本 arXiv:1312.5602*，2013。'
- en: '**[3]** GU, Shixiang, et al. Continuous deep q-learning with model-based acceleration.
    En *International conference on machine learning*. PMLR, 2016\. p. 2829–2838.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**[3]** GU, Shixiang 等人。《基于模型的加速连续深度 Q 学习》。发表于*国际机器学习大会*。PMLR，2016年。第 2829–2838
    页。'
- en: '**[4]** SUTTON, Richard S.; BARTO, Andrew G. *Reinforcement learning: An introduction*.
    MIT press, 2018.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**[4]** SUTTON, Richard S.; BARTO, Andrew G. *《强化学习：导论》*。MIT出版社，2018年。'
- en: '**[5]** *OpenAI Spinning Up — Deep Deterministic Policy Gradient*[https://spinningup.openai.com/en/latest/algorithms/ddpg.html](https://spinningup.openai.com/en/latest/algorithms/ddpg.html)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**[5]** *OpenAI Spinning Up — 深度确定性策略梯度*[https://spinningup.openai.com/en/latest/algorithms/ddpg.html](https://spinningup.openai.com/en/latest/algorithms/ddpg.html)'
- en: '**[6]** Uhlenbeck-Ornstein process[https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**[6]** 乌伦贝克-奥恩斯坦过程[https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process)'
- en: '**[7]** Normal / Gaussian Distribution'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**[7]** 正态 / 高斯分布'
- en: '[https://en.wikipedia.org/wiki/Normal_distribution](https://en.wikipedia.org/wiki/Normal_distribution)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Normal_distribution](https://en.wikipedia.org/wiki/Normal_distribution)'
