- en: 'T5: Text-to-Text Transformers (Part One)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: T5：文本到文本的变换器（第一部分）
- en: 原文：[https://towardsdatascience.com/t5-text-to-text-transformers-part-one-6b655f27c79a](https://towardsdatascience.com/t5-text-to-text-transformers-part-one-6b655f27c79a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/t5-text-to-text-transformers-part-one-6b655f27c79a](https://towardsdatascience.com/t5-text-to-text-transformers-part-one-6b655f27c79a)
- en: Creating a unified framework for language modeling
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个统一的语言建模框架
- en: '[](https://wolfecameron.medium.com/?source=post_page-----6b655f27c79a--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----6b655f27c79a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6b655f27c79a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6b655f27c79a--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----6b655f27c79a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wolfecameron.medium.com/?source=post_page-----6b655f27c79a--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----6b655f27c79a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6b655f27c79a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6b655f27c79a--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----6b655f27c79a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6b655f27c79a--------------------------------)
    ·14 min read·Jun 27, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----6b655f27c79a--------------------------------)
    ·14分钟阅读·2023年6月27日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/03be7b5832a9d134961e51b08dd7a5a7.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03be7b5832a9d134961e51b08dd7a5a7.png)'
- en: (Photo by [Patrick Tomasso](https://unsplash.com/@impatrickt?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/text?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: （照片由[Patrick Tomasso](https://unsplash.com/@impatrickt?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)拍摄，来源于[Unsplash](https://unsplash.com/s/photos/text?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)）
- en: The transfer learning paradigm is comprised of two main stages. First, we pre-train
    a deep neural network over a bunch of data. Then, we fine-tune this model (i.e.,
    train it some more) over a more specific, downstream dataset. The exact implementation
    of these stages may take many different forms. In computer vision, for example,
    we often pre-train models on the ImageNet dataset using a supervised learning
    objective. Then, these models perform supervised fine-tuning on the downstream
    dataset (i.e., the task that we are actually trying to solve). Alternatively,
    in natural language processing (NLP), we often perform [self-supervised](https://cameronrwolfe.substack.com/i/76273144/self-supervised-learning)
    pre-training over an unlabeled textual corpus.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习范式包括两个主要阶段。首先，我们在一大堆数据上对深度神经网络进行预训练。然后，我们在一个更具体的下游数据集上对这个模型进行微调（即，再训练它一段时间）。这些阶段的具体实现可能有多种形式。例如，在计算机视觉中，我们通常使用有监督学习目标在ImageNet数据集上对模型进行预训练。然后，这些模型在下游数据集上进行有监督微调（即，我们实际尝试解决的任务）。而在自然语言处理（NLP）中，我们通常在未标记的文本语料库上进行[自监督](https://cameronrwolfe.substack.com/i/76273144/self-supervised-learning)预训练。
- en: Combining large, deep neural networks with massive (pre-)training datasets often
    leads to impressive results. This finding was found to be especially true for
    NLP. Given that raw textual data is freely available on the internet, we can simply
    download a massive textual corpus, pre-train a large neural net on this data,
    then fine-tune the model on a variety of downstream tasks (or just use zero/few-shot
    learning techniques). This large-scale transfer learning approach was initially
    explored by BERT [2], which pre-trained a [transformer encoder](https://cameronrwolfe.substack.com/i/76273144/transformer-encoders)
    over unlabeled data using a [masking objective](https://cameronrwolfe.substack.com/i/76273144/training-bert),
    then fine-tuned on downstream language tasks.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 结合大型深度神经网络和庞大的（预）训练数据集通常会产生令人印象深刻的结果。这一发现对自然语言处理（NLP）尤其适用。由于原始文本数据在互联网上可以自由获得，我们可以简单地下载大量文本语料库，先在这些数据上预训练一个大型神经网络，然后在各种下游任务上微调模型（或仅使用零/少样本学习技术）。这一大规模迁移学习方法最初由BERT探索，该方法在未标记的数据上使用[掩蔽目标](https://cameronrwolfe.substack.com/i/76273144/training-bert)预训练了一个[变换器编码器](https://cameronrwolfe.substack.com/i/76273144/transformer-encoders)，然后在下游语言任务上进行微调。
- en: The success of BERT [2] cannot be overstated (i.e., new state-of-the-art performance
    on nearly all language benchmarks). As a result, the NLP community began to heavily
    investigate the topic of transfer learning, leading to the proposal of many new
    extensions and improvements. Due to the rapid development in this field, comparison
    between alternatives was difficult. The text-to-text transformer (T5) model [1]
    proposed a unified framework for studying transfer learning approaches in NLP,
    allowing us to analyze different settings and derive a set of best practices.
    This set of best practices comprise T5, a state-of-the-art model and training
    framework for language understanding tasks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: BERT [2] 的成功不可小觑（即，在几乎所有语言基准上都取得了新的最先进性能）。因此，自然语言处理（NLP）社区开始深入研究迁移学习这一主题，提出了许多新的扩展和改进。由于这一领域的发展迅速，各种方法的比较变得困难。文本到文本转换器（T5）模型
    [1] 提出了一个统一的框架，用于研究NLP中的迁移学习方法，使我们能够分析不同的设置并得出一套最佳实践。这套最佳实践包括T5，这是一种用于语言理解任务的最先进模型和训练框架。
- en: '![](../Images/d13e5a905a474a916926152cb15b52b1.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d13e5a905a474a916926152cb15b52b1.png)'
- en: (from [1])
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: Relevant History and Context
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相关历史和背景
- en: T5 reformulates existing transfer learning techniques into a unified format,
    compares them, and determines best practices to arrive at a high-performing result.
    *But what does this mean? What is transfer learning and why should we care about
    it?* To answer these questions, we will first overview a couple of important ideas,
    including transfer learning and different variants of the transformer architecture,
    that will be pivotal to understanding the analysis in [1]. From here, we will
    provide some historical context by explaining the [BERT](https://cameronrwolfe.substack.com/p/language-understanding-with-bert)
    [2] architecture, which popularized transfer learning for natural language processing
    (NLP) tasks.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: T5 将现有的迁移学习技术重新定义为统一的格式，进行比较，并确定最佳实践以获得高性能结果。*但这意味着什么？迁移学习是什么，为什么我们应该关注它？* 为了回答这些问题，我们将首先概述一些重要的概念，包括迁移学习和不同的Transformer架构变体，这些对于理解
    [1] 中的分析至关重要。从这里开始，我们将通过解释 [BERT](https://cameronrwolfe.substack.com/p/language-understanding-with-bert)
    [2] 架构来提供一些历史背景，这一架构使得迁移学习在自然语言处理（NLP）任务中变得流行。
- en: What is transfer learning?
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是迁移学习？
- en: '![](../Images/8a6c9f6324060d9ceeed88e696852d8e.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8a6c9f6324060d9ceeed88e696852d8e.png)'
- en: Different options for training a neural network (created by author)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络的不同选项（由作者创建）
- en: If we want to train a neural network to solve some task, we have two basic options.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想训练一个神经网络来解决某个任务，我们有两个基本的选择。
- en: '*Training from scratch*: randomly initialize your neural network and train
    it (in a supervised manner) on your target task.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*从头开始训练*：随机初始化你的神经网络，并在你的目标任务上进行训练（以监督方式）。'
- en: '*Transfer learning*: pre-train the network on a separate dataset, then fine-tune
    it (i.e., train it more) on the target task.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*迁移学习*：在一个独立的数据集上进行预训练，然后在目标任务上进行微调（即，进一步训练）。'
- en: Typically, pre-training is performed over a dataset that is much larger than
    the downstream, target dataset. In general, pre-training drastically improves
    data efficiency. The model learns faster during fine-tuning and may even perform
    better. The transfer learning process can take many different forms. In computer
    vision, for example, we might pre-train a model over ImageNet (using [supervised
    learning](https://www.geeksforgeeks.org/supervised-unsupervised-learning/)), then
    fine-tune on a smaller dataset like CIFAR-10/100\. For natural language processing
    (NLP) tasks, the story is a bit different. Typically, we use [self-supervised](https://cameronrwolfe.substack.com/i/76273144/self-supervised-learning)
    pre-training objectives (e.g., [masked language modeling](https://cameronrwolfe.substack.com/i/76273144/training-bert)
    or [causal language modeling](https://cameronrwolfe.substack.com/i/85568430/language-modeling))
    with unlabeled text.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，预训练是在比下游目标数据集大得多的数据集上进行的。一般而言，预训练会大幅提高数据效率。模型在微调期间学习得更快，甚至可能表现更好。迁移学习过程可以有许多不同的形式。例如，在计算机视觉中，我们可能会在ImageNet上进行模型预训练（使用[监督学习](https://www.geeksforgeeks.org/supervised-unsupervised-learning/)），然后在像CIFAR-10/100这样的较小数据集上进行微调。对于自然语言处理（NLP）任务，情况稍有不同。通常，我们使用[自监督](https://cameronrwolfe.substack.com/i/76273144/self-supervised-learning)预训练目标（例如，[掩蔽语言建模](https://cameronrwolfe.substack.com/i/76273144/training-bert)或[因果语言建模](https://cameronrwolfe.substack.com/i/85568430/language-modeling)）与未标记的文本。
- en: Different Transformer Architectures
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不同的 Transformer 架构
- en: '![](../Images/fdc1eba4da062bf23e91d98513163f10.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fdc1eba4da062bf23e91d98513163f10.png)'
- en: (from [6])
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [6]）
- en: The transformer, as originally proposed in [1], uses an encoder-decoder architecture,
    as shown above. For a more in-depth overview of this architecture, check out the
    link [here](https://newsletter.artofsaience.com/p/vision-transformers-from-idea-to).
    However, the encoder-decoder transformer architecture is not our only option!
    BERT uses an [encoder-only architecture](https://cameronrwolfe.substack.com/i/76273144/transformer-encoders),
    while most [modern large language models](https://cameronrwolfe.substack.com/p/modern-llms-mt-nlg-chinchilla-gopher)
    (LLMs) are based upon [decoder-only transformers](https://cameronrwolfe.substack.com/i/85568430/decoder-only-transformers).
    Let’s take a minute to understand the differences between each of these architectural
    variants.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer，如在 [1] 中最初提出的，使用编码器-解码器架构，如上所示。有关此架构的更深入概述，请查看链接 [here](https://newsletter.artofsaience.com/p/vision-transformers-from-idea-to)。然而，编码器-解码器
    Transformer 架构并不是我们唯一的选择！BERT 使用 [仅编码器架构](https://cameronrwolfe.substack.com/i/76273144/transformer-encoders)，而大多数
    [现代大型语言模型](https://cameronrwolfe.substack.com/p/modern-llms-mt-nlg-chinchilla-gopher)（LLMs）基于
    [仅解码器的 Transformers](https://cameronrwolfe.substack.com/i/85568430/decoder-only-transformers)。让我们花一点时间了解这些架构变体之间的区别。
- en: '![](../Images/bf18d1c1baf583d31f843272992dd291.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bf18d1c1baf583d31f843272992dd291.png)'
- en: Bidirectional self-attention in the transformer encoder (created by author)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 编码器中的双向自注意力（由作者创建）
- en: '**a primer on self-attention.** The self-attention operation takes a sequence
    of token vectors as input and produces a new sequence of transformed token vectors
    with the same length as output; see above. Each entry of this new sequence is
    a weighted average of vectors in the input sequence. Specifically, we compute
    each token vector in the output sequence as follows, where `y_i` and `x_j` are
    elements of the output and input sequences, respectively.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**自注意力的简介。** 自注意力操作将一个令牌向量序列作为输入，并生成一个长度相同的新的变换后令牌向量序列作为输出；如上所示。这个新序列的每个条目都是输入序列中向量的加权平均值。具体而言，我们计算输出序列中每个令牌向量的方法如下，其中
    `y_i` 和 `x_j` 分别是输出和输入序列的元素。'
- en: '![](../Images/f92435820a1399bc679e16d6a2208315.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f92435820a1399bc679e16d6a2208315.png)'
- en: (created by author)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: （由作者创建）
- en: The weight `w_{i, j}` above is an attention score that is produced as a function
    of `x_i` and `x_j`. Put simply, this score captures how much the current token
    should “pay attention to” another token in the sequence while computing its new
    representation.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 上述权重 `w_{i, j}` 是一个注意力分数，它是 `x_i` 和 `x_j` 的函数。简单来说，这个分数捕捉了当前令牌在计算其新表示时应该“关注”序列中的其他令牌的程度。
- en: '![](../Images/17a73d59ae9f7000596b19db8e29f207.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/17a73d59ae9f7000596b19db8e29f207.png)'
- en: (from [6])
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [6]）
- en: '**single stack or double stack?** The original transformer architecture uses
    two “stacks” of transformer layers; see above. The first stack (the encoder module)
    is comprised of several blocks that contain bidirectional self-attention and a
    [feed-forward neural network](https://cameronrwolfe.substack.com/i/94634004/feed-forward-neural-networks).
    The second stack (the decoder module) is pretty similar, but it uses [masked self
    attention](https://cameronrwolfe.substack.com/i/76273144/self-attention) and has
    an added “cross attention” mechanism that considers activations within the corresponding
    encoder layer while performing self-attention. The transformer was originally
    used for [sequence-to-sequence](https://en.wikipedia.org/wiki/Seq2seq) tasks (e.g.,
    language translation). For other tasks, single stack transformer models have become
    popular:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**单堆栈还是双堆栈？** 原始 Transformer 架构使用两个“堆栈” 的 Transformer 层；见上文。第一个堆栈（编码器模块）由几个包含双向自注意力和
    [前馈神经网络](https://cameronrwolfe.substack.com/i/94634004/feed-forward-neural-networks)
    的块组成。第二个堆栈（解码器模块）非常相似，但它使用 [掩码自注意力](https://cameronrwolfe.substack.com/i/76273144/self-attention)，并增加了一个“交叉注意力”机制，该机制在执行自注意力时考虑对应编码器层中的激活。Transformer
    最初用于 [序列到序列](https://en.wikipedia.org/wiki/Seq2seq) 任务（例如语言翻译）。对于其他任务，单堆栈 Transformer
    模型变得非常流行：'
- en: Language models use a decoder-only architecture
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言模型使用仅解码器架构
- en: BERT-style models use an encoder-only architecture
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT 风格的模型使用仅编码器架构
- en: '![](../Images/de672c901b1a4867466b3b88fb2fe58b.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de672c901b1a4867466b3b88fb2fe58b.png)'
- en: (from [1])
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: '**attention masks.** Variants of the transformer architecture have one major
    distinction: *the type of masking used in their attention layers*. Here, when
    we say “masking”, we are referring to certain tokens being masked (or ignored)
    during the computation of self-attention. Put simply, certain tokens may look
    only at a select portion of other tokens in the full input sequence. The figure
    above depicts different masking options for self-attention.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意力掩码。** 变压器架构的变体有一个主要区别：*在其注意力层中使用的掩码类型*。在这里，当我们说“掩码”时，我们指的是在自注意力计算过程中某些标记被掩盖（或忽略）。简单来说，某些标记可能仅查看完整输入序列中的一部分其他标记。上图描绘了自注意力的不同掩码选项。'
- en: Encoder-only models leverage bidirectional (or fully-visible) self-attention,
    which considers all tokens within the entire sequence during self-attention. Each
    token representation in self-attention is computed as a weighted average of all
    other tokens in the sequence. In contrast, decoder-only models use causal self-attention,
    where each token only considers tokens that come before it in the sequence.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 仅编码器模型利用双向（或完全可见）自注意力，这在自注意力过程中考虑了整个序列中的所有标记。自注意力中的每个标记表示是通过序列中所有其他标记的加权平均来计算的。相比之下，仅解码器模型使用因果自注意力，其中每个标记仅考虑序列中其之前的标记。
- en: '![](../Images/66c7576f1c9ed62f1dd9d8b45461ad80.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/66c7576f1c9ed62f1dd9d8b45461ad80.png)'
- en: (from [1])
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: (来自 [1])
- en: We can also adopt a hybrid approach by defining a “prefix”. More specifically,
    we can perform bidirectional self-attention for a group of tokens at the beginning
    of the sequence (i.e., a prefix), then perform causal self-attention for the rest
    of the tokens in the sequence; see above. Fully-visible (or bi-directional) self-attention
    is useful for attending over a prefix or performing classification tasks. However,
    certain applications (e.g., language modeling) require causal self-attention during
    training to prevent the transformer from “looking into the future” (i.e., just
    copying the correct token when generating output).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过定义“前缀”来采用混合方法。更具体地说，我们可以对序列开头的一组标记（即前缀）执行双向自注意力，然后对序列中其余的标记执行因果自注意力；见上文。完全可见（或双向）自注意力对于处理前缀或执行分类任务非常有用。然而，某些应用（例如，语言建模）在训练过程中需要因果自注意力，以防止变压器“看到未来”（即在生成输出时仅复制正确的标记）。
- en: '**what does T5 use?** Although the analysis in [1] considers many transformer
    architectures, the primary model used for T5 is a standard encoder-decoder architecture.
    Aside from a few small modifications, this model is quite similar to the transformer
    as it was originally proposed [6]. Encoder-only architectures are not explored
    in [1] because they are designed for token or sequence level classification and
    not generative tasks like translation or summarization. T5 aims to find a unified
    approach (based on transfer learning) to solve many language understanding tasks.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**T5使用什么？** 尽管[1]中的分析考虑了许多变压器架构，但T5主要使用的是标准的编码器-解码器架构。除了少数小修改外，该模型与最初提出的变压器[6]非常相似。由于编码器仅架构设计用于标记或序列级分类，而不是像翻译或总结这样的生成任务，[1]中没有探索它们。T5旨在找到一种统一的方法（基于迁移学习）来解决许多语言理解任务。'
- en: 'BERT: Transfer Learning for NLP'
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BERT：NLP的迁移学习
- en: In the early days, transfer learning in NLP typically used recurrent neural
    networks pre-trained with a [causal language modeling objective](https://cameronrwolfe.substack.com/i/85568430/language-modeling).
    However, everything changed with the proposal of [BERT](https://cameronrwolfe.substack.com/p/language-understanding-with-bert)
    [2], a transformer-based model [6] that is pre-trained using a [self-supervised
    objective](https://cameronrwolfe.substack.com/i/76273144/self-supervised-learning).
    BERT can be pre-trained over large amounts of unlabeled text, then fine-tuned
    to classify sentences (and even individual tokens in a sentence) with incredibly-high
    accuracy. At the time of its proposal, BERT set a new state-of-the-art on nearly
    all NLP tasks that were considered, solidifying transfer learning as the go-to
    approach within NLP.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期，NLP 中的迁移学习通常使用经过 [因果语言建模目标](https://cameronrwolfe.substack.com/i/85568430/language-modeling)
    预训练的递归神经网络。然而，随着 [BERT](https://cameronrwolfe.substack.com/p/language-understanding-with-bert)
    [2] 的提出，一切发生了变化。BERT 是一种基于变换器的模型 [6]，其使用 [自监督目标](https://cameronrwolfe.substack.com/i/76273144/self-supervised-learning)
    进行预训练。BERT 可以在大量未标记的文本上进行预训练，然后微调以对句子（甚至句子中的单个标记）进行高精度分类。在提出时，BERT 在几乎所有被考虑的 NLP
    任务上都设立了新的最先进水平，巩固了迁移学习在 NLP 中的主导地位。
- en: '![](../Images/a8726633ee5ad378ba440080664413e2.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8726633ee5ad378ba440080664413e2.png)'
- en: Performing self-supervised MLM pre-training with BERT (created by author)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 BERT 执行自监督 MLM 预训练（由作者创建）
- en: 'To make this a bit more specific, BERT relies upon a “denoising” objective,
    called [masked language modeling (MLM)](https://cameronrwolfe.substack.com/i/76273144/self-supervised-learning),
    during pre-training; see above. Although this might sound a bit complicated, the
    idea is simple, we just:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这一点更加具体，BERT 在预训练过程中依赖于一种“去噪”目标，称为 [掩码语言建模 (MLM)](https://cameronrwolfe.substack.com/i/76273144/self-supervised-learning)；见上文。虽然这听起来可能有些复杂，但其核心思想很简单，我们只需：
- en: Mask some tokens in the input sequence by replacing them with a special `[MASK]`
    token
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入序列中的一些标记掩盖，用特殊的 `[MASK]` 标记替代
- en: Process the corrupted/modified sequence with BERT
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 BERT 处理这些被破坏/修改过的序列
- en: Train BERT to accurately predict the masked tokens
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练 BERT 以准确预测掩码标记
- en: The exact implementation is a bit more complicated. We select 15% of tokens
    at random, then either replace them with the `[MASK]` token (90% probability)
    or a random token (10% probability). By using this objective over a sufficiently-large
    pre-training corpus, BERT can learn a lot of general linguistic knowledge that
    makes it a highly-effective model for transfer learning.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 精确的实现要复杂一些。我们随机选择 15% 的标记，然后将它们替换为 `[MASK]` 标记（90% 的概率）或随机标记（10% 的概率）。通过在足够大的预训练语料库上使用这一目标，BERT
    可以学习大量的一般语言学知识，使其成为一个高效的迁移学习模型。
- en: '**how is T5 related to BERT?** The proposal of BERT showed that transfer learning
    is a useful approach for solving NLP problems. Many people quickly began using
    BERT, trying new techniques, and proposing improvements. As a result, the field
    was overwhelmed with different options for performing transfer learning with BERT-like
    models. T5 [1] continues in this line of research, but tries to analyze all of
    these different proposals using a unified framework, giving us a much clearer
    picture of best practices for transfer learning in NLP. The final T5 model is
    trained using all of these best practices to reach state-of-the-art performance.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**T5 与 BERT 有什么关系？** BERT 的提出展示了迁移学习是一种解决 NLP 问题的有效方法。许多人很快开始使用 BERT，尝试新技术并提出改进建议。因此，该领域充斥着各种使用类似
    BERT 模型进行迁移学习的选项。T5 [1] 在这一研究方向上继续前进，但试图使用统一的框架来分析所有这些不同的提案，从而为我们提供了关于 NLP 中迁移学习最佳实践的更清晰的视角。最终的
    T5 模型利用这些最佳实践进行训练，以达到最先进的性能。'
- en: '**how does T5 related to LLMs?** Currently, we are seeing a massive revolution
    in the generative AI space, in which [LLMs](https://cameronrwolfe.substack.com/p/specialized-llms-chatgpt-lamda-galactica)
    (based on decoder-only transformer architectures) are being used to solve linguistic
    tasks via language model pre-training followed by [zero/few-shot learning](https://cameronrwolfe.substack.com/i/88082618/language-models-are-few-shot-learners).
    LLMs are great, but T5 exists in a relatively distinct area of tools and research.
    Namely, T5 focuses mostly on models that explicitly process input with an encoder
    before generating output with a separate decoder. Plus, T5 adopts a transfer learning
    approach (i.e., pre-training followed by fine-tuning on each target task) instead
    of zero/few-shot learning.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**T5 与 LLMs 的关系是什么？** 目前，我们正在看到生成 AI 领域的重大革命，其中 [LLMs](https://cameronrwolfe.substack.com/p/specialized-llms-chatgpt-lamda-galactica)（基于仅解码器的变压器架构）被用于通过语言模型预训练解决语言任务，然后进行
    [零/少样本学习](https://cameronrwolfe.substack.com/i/88082618/language-models-are-few-shot-learners)。LLMs
    很出色，但 T5 存在于一个相对独特的工具和研究领域。即，T5 主要关注那些明确通过编码器处理输入，然后通过单独的解码器生成输出的模型。此外，T5 采用迁移学习方法（即，预训练后在每个目标任务上进行微调），而不是零/少样本学习。'
- en: Other Useful Links
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他有用的链接
- en: The transformer architecture [[link](https://cameronrwolfe.substack.com/i/74325854/the-transformer-architecture)]
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变压器架构 [[link](https://cameronrwolfe.substack.com/i/74325854/the-transformer-architecture)]
- en: Self-attention [[link](https://cameronrwolfe.substack.com/i/74325854/self-attention)]
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自注意力 [[link](https://cameronrwolfe.substack.com/i/74325854/self-attention)]
- en: The BERT model [[link](https://cameronrwolfe.substack.com/p/language-understanding-with-bert)]
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT 模型 [[link](https://cameronrwolfe.substack.com/p/language-understanding-with-bert)]
- en: The basics of language models [[link](https://cameronrwolfe.substack.com/p/language-models-gpt-and-gpt-2)]
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言模型的基础 [[link](https://cameronrwolfe.substack.com/p/language-models-gpt-and-gpt-2)]
- en: 'T5: The Unified Text-to-Text Transformer'
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: T5：统一的文本到文本变压器
- en: The contribution of T5 is not a novel architecture or training methodology.
    Rather, the study performed in [1] is based entirely upon existing techniques.
    T5 considers all aspects of the transfer learning pipeline in NLP, such as different
    (unlabeled) datasets, pre-training objectives, benchmarks and fine-tuning methods.
    However, all of these aspects are studied via a unified text-to-text format. The
    goal of T5 is to *i)* analyze transfer learning setups and *ii)* determine the
    most effective approaches.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: T5 的贡献并不是一种新颖的架构或训练方法。相反，[1]中进行的研究完全基于现有技术。T5 考虑了 NLP 领域中迁移学习管道的各个方面，例如不同的（未标记的）数据集、预训练目标、基准测试和微调方法。然而，所有这些方面都是通过统一的文本到文本格式进行研究的。T5
    的目标是 *i)* 分析迁移学习设置和 *ii)* 确定最有效的方法。
- en: Text-to-Text Framework
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本到文本框架
- en: T5 converts all text processing problems into a “text-to-text” format (i.e.,
    take text as input and produce text as output). This generic structure, which
    is also exploited by LLMs with zero/few-shot learning, allows us to model and
    solve a variety of different tasks with a shared approach. *We can apply the same
    model, objective, training procedure and decoding process to every task that we
    consider*! We just adopt a [prompting](https://cameronrwolfe.substack.com/i/91134599/a-primer-on-language-modeling)
    approach and ask our language model to generate the answer in a textual format.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: T5 将所有文本处理问题转换为“文本到文本”格式（即，将文本作为输入并生成文本作为输出）。这种通用结构也被采用于具有零/少样本学习的 LLMs，使我们能够用统一的方法建模和解决各种不同的任务。*我们可以将相同的模型、目标、训练程序和解码过程应用于我们考虑的每个任务*！我们只需采用
    [提示](https://cameronrwolfe.substack.com/i/91134599/a-primer-on-language-modeling)
    方法，并要求我们的语言模型以文本格式生成答案。
- en: '![](../Images/359630186438219138733dce145162a0.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/359630186438219138733dce145162a0.png)'
- en: (from [1])
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: 'To make this a bit more concrete, all tasks being solved by T5 can be converted
    into text-to-text format as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让这一点更加具体，T5 解决的所有任务都可以转换为文本到文本格式，如下所示：
- en: Add a task-specific prefix to the original input sequence
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为原始输入序列添加任务特定的前缀
- en: Feed this sequence to the transformer
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这个序列输入到变压器中
- en: Formulate the model’s target as a textual sequence
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型的目标形式化为文本序列
- en: Using this format, we can easily perform tasks like summarization or translation
    (i.e., the target is naturally a sequence). Plus, we can perform classification
    by just training the model to generate text associated with the correct class.
    This process gets a bit complicated for problems like regression (i.e., we have
    to round real-valued outputs to the nearest decimal and treat it as a classification
    problem), but it tends to work well for a majority of linguistic tasks. Examples
    are shown in the figure above.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种格式，我们可以轻松执行诸如总结或翻译（即目标自然是一个序列）之类的任务。此外，我们可以通过仅训练模型生成与正确类别相关的文本来进行分类。这一过程在回归问题（即我们必须将实值输出舍入到最近的小数并将其视为分类问题）时会变得有些复杂，但对于大多数语言任务，它往往效果很好。示例见上图。
- en: '*“An issue arises if our model outputs text on a text classification task that
    does not correspond to any of the possible labels… In this case, we always count
    the model’s output as wrong, though we never observed this behavior in any of
    our trained models.”* — from [1]'
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“如果我们的模型在文本分类任务中输出的文本与任何可能的标签都不对应，那么会出现问题……在这种情况下，我们总是将模型的输出视为错误，尽管我们在任何训练模型中都没有观察到这种行为。”*
    — 来自[1]'
- en: T5 is fine-tuned on each task that it solves. This is in contrast to both LLMs,
    which use few-show learning, and the NLP decathlon [3], which uses [multi-task
    learning](https://www.ruder.io/multi-task/) to solve many tasks at once.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: T5会针对它解决的每个任务进行微调。这与使用少样本学习的LLM和使用[多任务学习](https://www.ruder.io/multi-task/)一次性解决多个任务的NLP十项全能[3]形成对比。
- en: How is T5 studied?
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: T5是如何研究的？
- en: All analysis performed in [1] uses the unified, text-to-text framework described
    above, as it allows a variety of different language understanding tasks to be
    converted into a shared format. Additionally, analysis of T5 uses the same underlying
    transformer architecture and pre-training dataset.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 所有在[1]中进行的分析都使用了上述统一的文本到文本框架，因为它允许将各种不同的语言理解任务转换为共享格式。此外，T5的分析使用了相同的基础变换器架构和预训练数据集。
- en: '![](../Images/fd7dcc89eca1d8d62e10f7b120b41b80.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd7dcc89eca1d8d62e10f7b120b41b80.png)'
- en: (from [6])
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: （来自[6]）
- en: '**the model.** As discussed previously, the transformer architecture, as it
    was originally proposed in [6], contains both an encoder and a decoder module.
    Recent work on language modeling has explored architectural variants that are
    encoder or decoder-only; e.g., [BERT](https://cameronrwolfe.substack.com/i/76273144/transformer-encoders)
    only uses the encoder [2], while most [(large) language models](https://cameronrwolfe.substack.com/i/85568430/decoder-only-transformers)
    only use the decoder. T5 uses an encoder-decoder architecture that closely resembles
    the original transformer. The differences are:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型。** 如前所述，变换器架构，正如[6]中最初提出的那样，包含编码器和解码器模块。最近对语言建模的研究探讨了只使用编码器或解码器的架构变体；例如，[BERT](https://cameronrwolfe.substack.com/i/76273144/transformer-encoders)只使用编码器[2]，而大多数[(大型)语言模型](https://cameronrwolfe.substack.com/i/85568430/decoder-only-transformers)只使用解码器。T5使用了一个与原始变换器非常相似的编码器-解码器架构。不同之处在于：'
- en: '[LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)
    is applied immediately before each attention and feed forward transformation (i.e.,
    outside of the residual path)'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)在每次注意力和前馈变换之前立即应用（即，位于残差路径之外）'
- en: No additive bias is used for LayerNorm (i.e., see [here](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html);
    we only use scale and eliminate the additive bias)
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于LayerNorm未使用加性偏置（即，见[这里](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)；我们只使用缩放并消除加性偏置）
- en: A simple position embedding scheme is used that adds a scalar to the corresponding
    [logit](https://stackoverflow.com/questions/41455101/what-is-the-meaning-of-the-word-logits-in-tensorflow)
    used to compute attention weights
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用了一个简单的位置嵌入方案，将一个标量添加到计算注意力权重时使用的相应[logit](https://stackoverflow.com/questions/41455101/what-is-the-meaning-of-the-word-logits-in-tensorflow)中。
- en: Dropout is applied throughout the network (e.g., attention weights, feed forward
    network, skip connection, etc.)
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在整个网络中应用了Dropout（例如，注意力权重、前馈网络、跳跃连接等）
- en: These modifications are illustrated in the above figure. Using this model (and
    a few others), T5 can test many different transfer learning settings to derive
    a set of best practices.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这些修改在上图中有所说明。使用该模型（以及其他一些模型），T5可以测试许多不同的迁移学习设置，以得出一套最佳实践。
- en: '**pre-training dataset.** T5 is pre-trained over the Colossal Clean Crawled
    Corpus (C4), a 750Gb corpus of “relatively clean” English text that is created
    in [1]. While a variety of pre-training datasets have been proposed in prior work,
    authors in [1] choose to construct their own due to prior datasets not being publicly
    available, using a limited set of filtering rules, having limited scope (e.g.,
    solely from [Creative Commons](https://creativecommons.org/)), or focusing only
    on parallel data for machine translation (i.e., versions of the same exact sentence
    in multiple, different languages).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**预训练数据集。** T5在Colossal Clean Crawled Corpus（C4）上进行预训练，这是一个750GB的“相对干净”的英文文本数据集，详见[1]。尽管先前的工作中提出了各种预训练数据集，[1]中的作者选择构建自己的数据集，因为先前的数据集不可公开获取，使用的过滤规则有限，范围有限（例如，仅来自[Creative
    Commons](https://creativecommons.org/)），或仅专注于机器翻译的平行数据（即多个不同语言中的相同句子版本）。'
- en: '![](../Images/fbce1267771a2184fc932660981bea0d.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fbce1267771a2184fc932660981bea0d.png)'
- en: (from [4])
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: （来自[4]）
- en: Notably, C4 was later used as a subset of the MassiveText dataset used to pre-train
    [Gopher](https://cameronrwolfe.substack.com/i/91134599/scaling-language-models-methods-analysis-and-insights-from-training-gopher)
    and [Chinchilla](https://cameronrwolfe.substack.com/i/91134599/training-compute-optimal-llms)
    [4, 5]. See the table above for size metrics of this dataset, which provides a
    better understanding of C4’s size relative to pre-training datasets used to train
    modern LLMs. With LLMs, we have seen that pre-training decoder-only models over
    sufficiently large datasets is crucial for their success. The same is true of
    transformers with different architectures, such as T5\. Extensive pre-training
    over a large, unlabeled dataset is conducive to better downstream performance.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，C4后来被用作MassiveText数据集的一个子集，该数据集用于预训练[Gopher](https://cameronrwolfe.substack.com/i/91134599/scaling-language-models-methods-analysis-and-insights-from-training-gopher)和[Chinchilla](https://cameronrwolfe.substack.com/i/91134599/training-compute-optimal-llms)[4,
    5]。请参见上表，以了解该数据集的规模指标，这有助于更好地理解C4与用于训练现代LLM的预训练数据集的相对大小。对于LLM，我们已经看到，预训练仅解码器模型在足够大的数据集上是其成功的关键。不同架构的变换器，如T5，亦是如此。在大型未标记数据集上的广泛预训练有利于更好的下游表现。
- en: '**experimental setup.** T5 is pre-trained over C4 then fine-tuned to solve
    a variety of downstream tasks. However, the exact settings used within this framework
    are variable. Namely, we can change the:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**实验设置。** T5在C4上进行预训练，然后微调以解决各种下游任务。然而，在这个框架中使用的确切设置是可变的。即，我们可以更改：'
- en: Transformer architecture
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变换器架构
- en: Pre-training setup (i.e., task or amount of data)
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练设置（即任务或数据量）
- en: Fine-tuning setup
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调设置
- en: Size/Scale of the model
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的规模/大小
- en: By changing each of these settings one-at-a-time and evaluating the results,
    we can develop a set of best practices for transfer learning in NLP, thus distilling
    the many proposals after BERT into a single, effective pipeline for creating effective
    language understanding models.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 通过逐一更改这些设置并评估结果，我们可以为NLP中的迁移学习开发一套最佳实践，从而将BERT之后的众多提议提炼成一个有效的管道，用于创建有效的语言理解模型。
- en: Takeaways
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 要点
- en: This post has covered all preliminary information related to the T5 model, including
    important background information and the basic experimental framework that is
    used. In the next post, we will cover details of the extensive analysis performed
    in [1], which uncovers best practices for transfer learning in NLP. For now, the
    major takeaways related to T5 are outlined below.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 本文涵盖了与T5模型相关的所有初步信息，包括重要的背景信息和使用的基本实验框架。在下一篇文章中，我们将详细介绍[1]中进行的广泛分析，揭示NLP中迁移学习的最佳实践。目前，T5的主要要点概述如下。
- en: '**transfer learning is powerful.** Transfer learning refers to the process
    of pre-training a deep learning model over some separate dataset, then fine-tuning
    (or further training) this model on a downstream, target dataset (i.e., the task
    we are actually trying to solve). If performed over a sufficiently large and aligned
    (i.e., similar to the downstream task) dataset, pre-training is incredibly effective.
    The model can learn much faster during fine-tuning and even reach a higher accuracy.
    This technique is effective across domains (e.g., computer vision and NLP), but
    the exact approach used for pre-training or fine-tuning might differ.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**迁移学习是强大的。** 迁移学习是指在某些独立数据集上预训练深度学习模型，然后在下游目标数据集（即我们实际要解决的任务）上微调（或进一步训练）该模型。如果在足够大且对齐（即，与下游任务类似）的数据集上进行，预训练是非常有效的。模型在微调期间可以学习得更快，甚至达到更高的准确率。这种技术在不同领域（例如计算机视觉和自然语言处理）中都有效，但用于预训练或微调的确切方法可能会有所不同。'
- en: '*“While we do not explicitly measure improvements in data efficiency in this
    paper, we emphasize that this is one of the primary benefits of the transfer learning
    paradigm.”* — from [1]'
  id: totrans-97
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“虽然我们在本文中没有明确测量数据效率的提升，但我们强调这是迁移学习范式的主要好处之一。”* — 来源于 [1]'
- en: '**what comes after BERT?** The proposal of BERT [2] was a massive breakthrough
    that popularized the use of transfer learning for NLP tasks. In fact, BERT set
    a new state-of-the-art performance on nearly every task that it considered. Due
    to its success, the research community adopted and iterated upon BERT’s approach.
    T5 attempts to unify all of this follow-up work and analysis that came after the
    proposal of BERT, providing a clearer view of the most effective transfer learning
    approaches.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**BERT 之后是什么？** BERT [2] 的提出是一个巨大的突破，普及了迁移学习在自然语言处理任务中的应用。事实上，BERT 在几乎所有涉及的任务上都设置了新的最先进性能。由于其成功，研究社区采纳并迭代了
    BERT 的方法。T5 尝试统一 BERT 提出后的所有后续工作和分析，提供了对最有效迁移学习方法的更清晰视角。'
- en: '**generic task formulation.** In order to create a unified framework according
    to which many different transfer learning approaches can be studied, T5 proposed
    a generic text-to-text framework. Similar to prompting and few-shot learning techniques
    used for LLMs, this text-to-text framework can restructure any language task into
    textual input and output. Specifically, this is done by appending a task-specific
    prefix to the textual input (i.e., so that T5 knows what task it is trying to
    solve) and using the decoder module of T5 to generate text corresponding to the
    desired target (e.g., a label, regression value, or sequence of text).'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**通用任务制定。** 为了创建一个统一的框架，以便研究多种不同的迁移学习方法，T5 提出了一个通用的文本到文本框架。类似于用于大型语言模型（LLM）的提示和少样本学习技术，这个文本到文本框架可以将任何语言任务重组为文本输入和输出。具体来说，这通过在文本输入中附加特定任务的前缀（即，让
    T5 知道它正在解决什么任务）来完成，并使用 T5 的解码器模块生成与期望目标（例如标签、回归值或文本序列）对应的文本。'
- en: Closing Remarks
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结束语
- en: Thanks so much for reading this article. I am [Cameron R. Wolfe](https://cameronrwolfe.me/),
    Director of AI at [Rebuy](https://www.rebuyengine.com/). I study the empirical
    and theoretical foundations of deep learning. You can also check out my [other
    writings](https://medium.com/@wolfecameron) on medium! If you liked it, please
    follow me on [twitter](https://twitter.com/cwolferesearch) or subscribe to my
    [Deep (Learning) Focus newsletter](https://cameronrwolfe.substack.com/), where
    I help readers build a deeper understanding of topics in AI research via understandable
    overviews of popular papers.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读本文。我是 [Cameron R. Wolfe](https://cameronrwolfe.me/)，[Rebuy](https://www.rebuyengine.com/)
    的 AI 总监。我研究深度学习的实证和理论基础。你也可以查看我在 Medium 上的 [其他文章](https://medium.com/@wolfecameron)！如果你喜欢，请在
    [twitter](https://twitter.com/cwolferesearch) 上关注我，或订阅我的 [Deep (Learning) Focus
    电子通讯](https://cameronrwolfe.substack.com/)，我在其中通过易于理解的热门论文概述帮助读者深入了解 AI 研究中的主题。
- en: Bibliography
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Raffel, Colin, et al. “Exploring the limits of transfer learning with a
    unified text-to-text transformer.” *The Journal of Machine Learning Research*
    21.1 (2020): 5485–5551.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Raffel, Colin, et al. “Exploring the limits of transfer learning with a
    unified text-to-text transformer.” *The Journal of Machine Learning Research*
    21.1 (2020): 5485–5551.'
- en: '[2] Devlin, Jacob, et al. “Bert: Pre-training of deep bidirectional transformers
    for language understanding.” *arXiv preprint arXiv:1810.04805* (2018).'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Devlin, Jacob, et al. “Bert: Pre-training of deep bidirectional transformers
    for language understanding.” *arXiv preprint arXiv:1810.04805* (2018).'
- en: '[3] McCann, Bryan, et al. “The natural language decathlon: Multitask learning
    as question answering.” *arXiv preprint arXiv:1806.08730* (2018).'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] McCann, Bryan, 等人。“自然语言十项全能：将多任务学习应用于问答。” *arXiv预印本 arXiv:1806.08730* (2018)。'
- en: '[4] Rae, Jack W., et al. “Scaling language models: Methods, analysis & insights
    from training gopher.” *arXiv preprint arXiv:2112.11446* (2021).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Rae, Jack W., 等人。“语言模型的扩展：训练Gopher的方法、分析与见解。” *arXiv预印本 arXiv:2112.11446*
    (2021)。'
- en: '[5] Hoffmann, Jordan, et al. “Training compute-optimal large language models.”
    *arXiv preprint arXiv:2203.15556* (2022).'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Hoffmann, Jordan, 等人。“训练计算最优的大型语言模型。” *arXiv预印本 arXiv:2203.15556* (2022)。'
- en: '[6] Vaswani, Ashish, et al. “Attention is all you need.” *Advances in neural
    information processing systems* 30 (2017).'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Vaswani, Ashish, 等人。“注意力机制才是你所需要的。” *神经信息处理系统进展* 30 (2017)。'
