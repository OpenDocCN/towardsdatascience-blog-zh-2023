- en: Regularization In Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络中的正则化
- en: 原文：[https://towardsdatascience.com/regularisation-techniques-neural-networks-101-1f746ad45b72](https://towardsdatascience.com/regularisation-techniques-neural-networks-101-1f746ad45b72)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/regularisation-techniques-neural-networks-101-1f746ad45b72](https://towardsdatascience.com/regularisation-techniques-neural-networks-101-1f746ad45b72)
- en: How to avoid overfitting whilst training your neural network
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何在训练神经网络时避免过拟合
- en: '[](https://medium.com/@egorhowell?source=post_page-----1f746ad45b72--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----1f746ad45b72--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1f746ad45b72--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1f746ad45b72--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----1f746ad45b72--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@egorhowell?source=post_page-----1f746ad45b72--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----1f746ad45b72--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1f746ad45b72--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1f746ad45b72--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----1f746ad45b72--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1f746ad45b72--------------------------------)
    ·8 min read·Dec 2, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1f746ad45b72--------------------------------)
    ·阅读时间 8 分钟·2023年12月2日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/e56562ba2f8d215063ee836b97f5018d.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e56562ba2f8d215063ee836b97f5018d.png)'
- en: '[https://www.flaticon.com/free-icons/neural-network](https://www.flaticon.com/free-icons/neural-network).
    title=”neural network icons.” Neural network icons created by Vectors Tank — Flaticon.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.flaticon.com/free-icons/neural-network](https://www.flaticon.com/free-icons/neural-network)
    的标题是“neural network icons.” 神经网络图标由 Vectors Tank 创建 — Flaticon。'
- en: Table of Content
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内容目录
- en: '**Background**'
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**背景**'
- en: '**What is Overfitting?**'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**什么是过拟合？**'
- en: '**Lasso (L1) and Ridge (L2) Regularisation**'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Lasso (L1) 和 Ridge (L2) 正则化**'
- en: '**Early Stopping**'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**早停法**'
- en: '**Dropout**'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Dropout**'
- en: '**Other Methods**'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**其他方法**'
- en: '**Summary**'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**总结**'
- en: Background
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: 'So far in this neural networks 101 series we have discussed two ways to improve
    the performance of neural networks: [**hyperparameter tuning**](/optimise-your-hyperparameter-tuning-with-hyperopt-861573239eb5)
    and faster gradient descent optimisers. You can check those posts below:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在这个神经网络 101 系列中，我们讨论了两种提高神经网络性能的方法：[**超参数调整**](/optimise-your-hyperparameter-tuning-with-hyperopt-861573239eb5)
    和更快的梯度下降优化器。你可以查看下面的帖子：
- en: '[](/hyperparameter-tuning-neural-networks-101-ca1102891b27?source=post_page-----1f746ad45b72--------------------------------)
    [## Hyperparameter Tuning: Neural Networks 101'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/hyperparameter-tuning-neural-networks-101-ca1102891b27?source=post_page-----1f746ad45b72--------------------------------)
    [## 超参数调整：神经网络 101'
- en: How you can improve the “learning” and “training” of neural networks through
    tuning hyperparameters
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何通过调整超参数来改进神经网络的“学习”和“训练”
- en: 'towardsdatascience.com](/hyperparameter-tuning-neural-networks-101-ca1102891b27?source=post_page-----1f746ad45b72--------------------------------)
    [](/optimisation-algorithms-neural-networks-101-256e16a88412?source=post_page-----1f746ad45b72--------------------------------)
    [## Optimisation Algorithms: Neural Networks 101'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/hyperparameter-tuning-neural-networks-101-ca1102891b27?source=post_page-----1f746ad45b72--------------------------------)
    [](/optimisation-algorithms-neural-networks-101-256e16a88412?source=post_page-----1f746ad45b72--------------------------------)
    [## 优化算法：神经网络 101
- en: How to improve training beyond the “vanilla” gradient descent algorithm
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何改进超越“普通”梯度下降算法的训练
- en: towardsdatascience.com](/optimisation-algorithms-neural-networks-101-256e16a88412?source=post_page-----1f746ad45b72--------------------------------)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/optimisation-algorithms-neural-networks-101-256e16a88412?source=post_page-----1f746ad45b72--------------------------------)
- en: There is one other set of techniques that aid in performance and that is regularisation.
    This helps prevent the model from overfitting to the training dataset to have
    more accurate and consistent predictions.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 还有另一组技术有助于提高性能，那就是正则化。这有助于防止模型对训练数据集过拟合，从而获得更准确和一致的预测。
- en: In this article, we will cover a wide range of methods to regularise your neural
    network and how you can do it in PyTorch!
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将涵盖一系列正则化神经网络的方法以及如何在 PyTorch 中实现它！
- en: What is Overfitting?
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是过拟合？
- en: Let’s quickly recap of what we mean by overfitting in machine learning and statistics.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速回顾一下在机器学习和统计中我们所说的过拟合是什么意思。
- en: '[Wikipedia](https://en.wikipedia.org/wiki/Overfitting) describes overfitting
    as:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[维基百科](https://en.wikipedia.org/wiki/Overfitting) 描述了过拟合为：'
- en: “The production of an analysis that corresponds too closely or exactly to a
    particular set of data, and may therefore fail to fit to additional data or predict
    future observations reliably”
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “一种分析的生产，该分析与特定数据集过于密切或完全对应，因此可能无法适应额外数据或可靠预测未来观察结果”
- en: In layman’s terms, this is saying that the model is learning the data it is
    training on, but is failing to generalise. Therefore, it will have poor predictions
    on data it has not seen before.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通俗来说，这就是说模型正在学习它训练的数据，但未能进行泛化。因此，它在未见过的数据上会有较差的预测。
- en: 'Below is a visual example depicting a [***null***](https://en.wikipedia.org/wiki/Null_model)
    model (under fit), a [***proposed***](/saturated-models-deviance-and-the-derivation-of-sum-of-squares-ee6fa040f52)
    model (good generalisation) and a [***saturated***](https://en.wikipedia.org/wiki/Saturated_model#:~:text=In%20mathematical%20logic%2C%20and%20particularly,reasonably%20expected%22%20given%20its%20size.)
    model (overfit):'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个视觉示例，描述了 [***空***](https://en.wikipedia.org/wiki/Null_model) 模型（欠拟合）、[***提出的***](/saturated-models-deviance-and-the-derivation-of-sum-of-squares-ee6fa040f52)
    模型（良好泛化）和 [***饱和***](https://en.wikipedia.org/wiki/Saturated_model#:~:text=In%20mathematical%20logic%2C%20and%20particularly,reasonably%20expected%22%20given%20its%20size.)
    模型（过拟合）：
- en: '![](../Images/6965182c664f2ccb7043a2e417225987.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6965182c664f2ccb7043a2e417225987.png)'
- en: Plots showing, from left to right, Null Model, Proposed Model and the Saturated
    Model. Images produced by author in Python.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图显示了从左到右的空模型、提出的模型和饱和模型。图像由作者用 Python 生成。
- en: Notice that the overfit (saturated) model goes through every data point (its
    ‘connecting-the-dots’), so it fits directly into the data. Whereas the proposed
    model has clearly generalised a lot better even though its line is not going through
    every data point.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到过拟合（饱和）模型通过了每一个数据点（其“连接点”），所以它直接拟合了数据。而提出的模型明显更好地进行了泛化，即使它的线没有通过每一个数据点。
- en: 'The code used to generate the above plot is available on my GitHub here:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成上述图的代码可以在我的 GitHub 上找到：
- en: '[](https://github.com/egorhowell/Medium-Articles/blob/main/Statistics/General/saturated_models.py?source=post_page-----1f746ad45b72--------------------------------)
    [## Medium-Articles/Statistics/General/saturated_models.py at main · egorhowell/Medium-Articles'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/egorhowell/Medium-Articles/blob/main/Statistics/General/saturated_models.py?source=post_page-----1f746ad45b72--------------------------------)
    [## Medium-Articles/Statistics/General/saturated_models.py at main · egorhowell/Medium-Articles'
- en: Code I use in my medium blog/articles. Contribute to egorhowell/Medium-Articles
    development by creating an account on…
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我在我的 Medium 博客/文章中使用的代码。通过创建帐户来为 egorhowell/Medium-Articles 的开发做出贡献…
- en: github.com](https://github.com/egorhowell/Medium-Articles/blob/main/Statistics/General/saturated_models.py?source=post_page-----1f746ad45b72--------------------------------)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/egorhowell/Medium-Articles/blob/main/Statistics/General/saturated_models.py?source=post_page-----1f746ad45b72--------------------------------)'
- en: Lasso & Ridge Regularisation
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 套索与岭回归正则化
- en: '[***Lasso***](https://en.wikipedia.org/wiki/Lasso_%28statistics%29) and [***Ridge***](https://en.wikipedia.org/wiki/Ridge_regression)
    regularisation can be similarly used for neural networks to how they are applied
    to [***linear regression***](/why-gradient-descent-is-so-common-in-data-science-def3e6515c5c).
    They apply an additional penalty term to the loss function to help keep the model
    weights small or sparse to encourage simpler models to reduce the chance of overfitting.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[***套索回归***](https://en.wikipedia.org/wiki/Lasso_%28statistics%29) 和 [***岭回归***](https://en.wikipedia.org/wiki/Ridge_regression)
    正则化可以类似于它们在 [***线性回归***](/why-gradient-descent-is-so-common-in-data-science-def3e6515c5c)
    中的应用，用于神经网络。它们在损失函数中应用额外的惩罚项，以帮助保持模型权重小或稀疏，从而鼓励更简单的模型以减少过拟合的机会。'
- en: '**Lasso**'
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**套索回归**'
- en: 'For Lasso regularisation (***L1***), the penalty term is the sum of the absolute
    weights used in the model:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于套索回归（***L1***），惩罚项是模型中权重的绝对值之和。
- en: '![](../Images/badde2d7728763cf3f5f8991400c82e1.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/badde2d7728763cf3f5f8991400c82e1.png)'
- en: Lasso regression for a neural network. Equation by author in LaTeX.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的套索回归。方程由作者用 LaTeX 表示。
- en: 'Where:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '***λ****: The regularisation parameter*'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***λ****: 正则化参数*'
- en: '***Original Loss:*** *The initial loss without taking into account the regularisation
    terms.*'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***原始损失：*** *未考虑正则化项的初始损失。*'
- en: '***w_i****:*​ *The model’s weights*'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***w_i****:*​ *模型的权重*'
- en: Lasso can cause some weights to become zero, creating a sparser neural network.
    This curtails the complexity of the network.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Lasso可以使一些权重变为零，从而创建一个更稀疏的神经网络。这减少了网络的复杂性。
- en: 'Lasso is not available directly in PyTorch, but we can add it by editing the
    loss function inside the code:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Lasso在PyTorch中没有直接提供，但我们可以通过编辑代码中的损失函数来添加它：
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Ridge
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 岭回归
- en: 'Ridge regularisation adds the square of the weights as the penalty term:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 岭回归通过将权重的平方作为惩罚项来添加正则化：
- en: '![](../Images/88ac7fc23725babae3c7ec53f0a3f003.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/88ac7fc23725babae3c7ec53f0a3f003.png)'
- en: Ridge regression for a neural network. Equation by author in LaTeX.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的岭回归。方程由作者在LaTeX中编写。
- en: The terms in this equation are the same as above for Lasso regularisation.
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这个方程中的术语与上述Lasso正则化相同。
- en: The difference to Lasso regularisation is the squaring of the weights. This
    leads to the weights not being zero but does minimise their value, thus helping
    in overfitting.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 与Lasso正则化的区别在于权重的平方。这导致权重不会变为零，但会最小化其值，从而有助于减轻过拟合。
- en: 'Ridge regularisation is much easier to implement than Lasso inside PyTorch.
    It is done by specifying the `weight_decay` argument which is the regularisation
    strength:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 与Lasso相比，岭回归在PyTorch中的实现要简单得多。它是通过指定`weight_decay`参数来完成的，这是正则化的强度：
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If `weight_decay` is too small, then there will be minimum regularisation. Therefore,
    it must be set initialised correctly. This can be achieved through trial and error
    or using hyperparameter tuning.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`weight_decay`过小，则会导致最小的正则化。因此，必须正确初始化。这可以通过试验和错误或使用超参数调优来实现。
- en: Early Stopping
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 早期停止
- en: Early stopping is probably the best regularisation method for neural networks
    and machine learning in general.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 早期停止可能是神经网络和机器学习中最好的正则化方法。
- en: Early stopping measures the performance on an external validation set while
    the model is “learning.” If the performance on the validation set improves each
    epoch, then the neural network continues learning on the training data.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 早期停止在模型“学习”时通过外部验证集来衡量性能。如果验证集上的性能在每个训练周期都有所提高，那么神经网络将继续在训练数据上学习。
- en: However, if the performance on the validation set doesn’t improve for a certain
    number of epochs, typically referred to as *patience*, then training is terminated
    early.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果验证集上的性能在一定数量的周期内没有改善，通常称为*耐心*，则训练将提前终止。
- en: The validation set allows us to evaluate the model on a hold-out dataset that
    is not used to train the model. This is how early stopping helps with any potential
    overfitting problem.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 验证集使我们能够在未用于训练模型的保留数据集上评估模型。这就是早期停止如何帮助解决潜在的过拟合问题。
- en: Some research shows that a neural network can generalise even if the performance
    on the validation set starts to degrade. This is known as [**double descent**](https://en.wikipedia.org/wiki/Double_descent)or
    [**grokking**](https://arxiv.org/abs/2201.02177), and highly recommend checking
    this out as it is such a fascinating result.
  id: totrans-65
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一些研究表明，即使验证集上的表现开始下降，神经网络仍然可以进行泛化。这被称为[**双重下降**](https://en.wikipedia.org/wiki/Double_descent)或[**grokking**](https://arxiv.org/abs/2201.02177)，强烈推荐查看，因为这是一个非常有趣的结果。
- en: 'Below is an example of how you can implement early stopping on the famous [***iris
    datase***](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html)t
    ([MIT License](https://github.com/olafplacha/Iris-Dataset/blob/master/LICENSE))
    using [***PyTorch***](https://pytorch.org/):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何在著名的[***鸢尾花数据集***](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html)
    ([MIT许可证](https://github.com/olafplacha/Iris-Dataset/blob/master/LICENSE)) 上使用[***PyTorch***](https://pytorch.org/)实现早期停止的示例：
- en: '[PRE2]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/4b1c352311e37dfb03fb1de0556f65fd.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b1c352311e37dfb03fb1de0556f65fd.png)'
- en: Example of early stopping. Plot generated by author in Python.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 早期停止的示例。图表由作者用Python生成。
- en: As you can see, training terminated at ***~260*** epochs, despite setting training
    to ***800*** epochs, as the performance on the validation set didn’t improve for
    ***10*** epochs.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，尽管将训练设置为***800***个周期，但训练在***~260***个周期时终止，因为验证集上的性能在***10***个周期内没有改善。
- en: 'The code for the above plot is available on my GitHub here:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表的代码可以在我的GitHub上找到：
- en: '[## Medium-Articles/Neural Networks/regularisation.py at main · egorhowell/Medium-Articles'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[## Medium-Articles/Neural Networks/regularisation.py at main · egorhowell/Medium-Articles'
- en: Code I use in my medium blog/articles. Contribute to egorhowell/Medium-Articles
    development by creating an account on…
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我在我的中等博客/文章中使用的代码。通过创建一个账户来贡献egorhowell/Medium-Articles的开发…
- en: github.com](https://github.com/egorhowell/Medium-Articles/blob/main/Neural%20Networks/regularisation.py?source=post_page-----1f746ad45b72--------------------------------)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/egorhowell/Medium-Articles/blob/main/Neural%20Networks/regularisation.py?source=post_page-----1f746ad45b72--------------------------------)
- en: Dropout
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dropout
- en: '[***Dropout***](https://en.wikipedia.org/wiki/Dilution_%28neural_networks%29)
    is one of the most famous regularisation techniques introduced by the ‘godfather’
    of deep learning, [***Geoffrey Hinton***](https://en.wikipedia.org/wiki/Geoffrey_Hinton).
    It has been shown to improve the performance of state-of-the-art neural networks
    by a couple of percentage points.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[***Dropout***](https://en.wikipedia.org/wiki/Dilution_%28neural_networks%29)
    是深度学习“教父”[***Geoffrey Hinton***](https://en.wikipedia.org/wiki/Geoffrey_Hinton)
    提出的最著名的正则化技术之一。研究表明，它能使最先进的神经网络的性能提高几个百分点。'
- en: The idea behind is drop is quite simple. At every epoch, each neuron has some
    probability ***p*** of being “dropped” from the learning process and is ignored.
    However, for the next epoch, it may be “active” and continue learning its optimal
    weights and biases.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout 的基本思想非常简单。在每个 epoch 中，每个神经元都有一定的概率 ***p*** 被“丢弃”在学习过程中，并且被忽略。然而，在下一个
    epoch 中，它可能会“激活”，继续学习其最佳的权重和偏置。
- en: 'Note: the output neurons are not considered for dropout.'
  id: totrans-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意：输出神经元不考虑在 Dropout 中。
- en: The probability of dropout happening, ***p,*** is a hyperparameter that can
    and should be hyperparameter-tuned for the network you are considering. In general,
    it ranges from 10–50% depending on the type of neural network you are building.
    Types include [***recurrent***](https://en.wikipedia.org/wiki/Recurrent_neural_network)
    and [***convolutional***](https://en.wikipedia.org/wiki/Convolutional_neural_network)
    neural networks.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout 发生的概率，***p***，是一个超参数，可以且应该为你所考虑的网络进行超参数调整。通常，它的范围在 10% 到 50% 之间，具体取决于你正在构建的神经网络类型。包括
    [***递归神经网络***](https://en.wikipedia.org/wiki/Recurrent_neural_network) 和 [***卷积神经网络***](https://en.wikipedia.org/wiki/Convolutional_neural_network)。
- en: 'The diagram below illustrates the dropout technique for a three-layer network:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图示展示了三层网络的 dropout 技术：
- en: '![](../Images/066c830d15e40b55b6adef5eca59c189.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/066c830d15e40b55b6adef5eca59c189.png)'
- en: Diagram for dropout where two neurons have been “dropped” from traning. Created
    by author.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout 的示意图，其中两个神经元被“丢弃”在训练过程中。由作者创建。
- en: The reason dropout is so effective is that it teaches neurons to be useful on
    their own and not co-adapt with neighboring neurons. This makes them generalise
    better as they consider their inputs more sensitively.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout 之所以如此有效，是因为它教会神经元独立发挥作用，而不是与邻近神经元共同适应。这使得它们能更好地泛化，因为它们对输入的敏感度更高。
- en: 'Another nice way of thinking about it is that dropout leads to us training
    several different neural networks. If our network has ***n*** neurons, then we
    have ***2^n*** permutations of networks as each neuron has two states: “active”
    or “dropped.” Therefore, after 1,000 epochs, we have trained 1,000 neural networks.
    The final model is just an average of all these smaller networks.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种理解方式是，Dropout 导致我们训练多个不同的神经网络。如果我们的网络有 ***n*** 个神经元，那么我们就有 ***2^n*** 种网络排列，因为每个神经元有两种状态：“激活”或“丢弃”。因此，在
    1,000 个 epoch 后，我们训练了 1,000 个神经网络。最终模型只是所有这些较小网络的平均值。
- en: 'Dropout is easily added in PyTorch when declaring the architecture of the network:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch 中，当声明网络的架构时，容易添加 Dropout：
- en: '[PRE3]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Other Methods
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他方法
- en: Architecture
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 架构
- en: You can reduce the number of hidden layers and the number of neurons in these
    layers to reduce complexity, hence curtailing the chance of overfitting.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以减少隐藏层的数量和这些层中的神经元数量，以降低复杂性，从而减少过拟合的可能性。
- en: More Data
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多数据
- en: As always, the more data you have, the better. Having more training rows for
    your model to learn from, leads to the neural network finding the best weights
    and biases much more likely.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，数据越多越好。模型用于学习的训练样本越多，神经网络找到最佳权重和偏置的可能性就越大。
- en: Augment Data
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 增强数据
- en: Particularly for computer vision tasks, you can augment the data using random
    transformations (flip, rotate, sheer, etc.) to increase the pool of training data.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是对于计算机视觉任务，你可以通过随机变换（翻转、旋转、剪切等）来增强数据，从而增加训练数据的池。
- en: Summary & Further Thoughts
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结与进一步思考
- en: Regularisation is an important concept to get right for your neural network
    model to prevent it from overfitting on the training data. The main methods I
    recommend to add to your neural network to regularise it are early stopping and
    dropout. The coupling of these two is very effective in reducing the chance of
    overfitting.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化是确保你的神经网络模型避免在训练数据上过拟合的重要概念。我推荐的主要正则化方法是早停法和dropout。这两者的结合在减少过拟合的可能性方面非常有效。
- en: Another Thing!
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另一个事情！
- en: I have a free newsletter, [**Dishing the Data**](https://dishingthedata.substack.com/),
    where I share weekly tips for becoming a better Data Scientist. There is no “fluff”
    or “clickbait,” just pure actionable insights from a practicing Data Scientist.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我有一个免费的新闻通讯，[**Dishing the Data**](https://dishingthedata.substack.com/)，我在其中每周分享成为更好的数据科学家的技巧。没有“虚
    fluff”或“点击诱饵”，只有来自实践数据科学家的纯粹可操作的见解。
- en: '[](https://newsletter.egorhowell.com/?source=post_page-----1f746ad45b72--------------------------------)
    [## Dishing The Data | Egor Howell | Substack'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://newsletter.egorhowell.com/?source=post_page-----1f746ad45b72--------------------------------)
    [## Dishing The Data | Egor Howell | Substack'
- en: How To Become A Better Data Scientist. Click to read Dishing The Data, by Egor
    Howell, a Substack publication with…
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何成为更好的数据科学家。点击阅读**Egor Howell**的《Dishing The Data》，这是一个Substack出版物，包含…
- en: newsletter.egorhowell.com](https://newsletter.egorhowell.com/?source=post_page-----1f746ad45b72--------------------------------)
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[newsletter.egorhowell.com](https://newsletter.egorhowell.com/?source=post_page-----1f746ad45b72--------------------------------)'
- en: Connect With Me!
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与我联系！
- en: '[**YouTube**](https://www.youtube.com/@egorhowell)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**YouTube**](https://www.youtube.com/@egorhowell)'
- en: '[**LinkedIn**](https://www.linkedin.com/in/egor-howell-092a721b3/)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**LinkedIn**](https://www.linkedin.com/in/egor-howell-092a721b3/)'
- en: '[**Twitter**](https://twitter.com/EgorHowell)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Twitter**](https://twitter.com/EgorHowell)'
- en: '[**GitHub**](https://github.com/egorhowell)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**GitHub**](https://github.com/egorhowell)'
- en: References & Further Reading
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献与进一步阅读
- en: '[*Andrej Karpathy Neural Network Course*](https://www.youtube.com/watch?v=i94OvYb6noo)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*Andrej Karpathy 神经网络课程*](https://www.youtube.com/watch?v=i94OvYb6noo)'
- en: '[*PyTorch site*](https://pytorch.org/)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*PyTorch 网站*](https://pytorch.org/)'
- en: '[*Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition.
    Aurélien Géron. September 2019\. Publisher(s): O’Reilly Media, Inc. ISBN: 9781492032649*](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)*.*'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*《Scikit-Learn、Keras和TensorFlow实战，第2版》. Aurélien Géron. 2019年9月. 出版社：O’Reilly
    Media, Inc. ISBN: 9781492032649*](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)*.*'
- en: '*Dropout paper:* [*https://jmlr.org/papers/v15/srivastava14a.html*](https://jmlr.org/papers/v15/srivastava14a.html)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Dropout 论文：* [*https://jmlr.org/papers/v15/srivastava14a.html*](https://jmlr.org/papers/v15/srivastava14a.html)'
