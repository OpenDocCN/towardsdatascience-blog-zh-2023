- en: 'Hashing in Modern Recommender Systems: A Primer'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现代推荐系统中的哈希：入门
- en: 原文：[https://towardsdatascience.com/hashing-in-modern-recommender-systems-a-primer-9c6b2cf4497a](https://towardsdatascience.com/hashing-in-modern-recommender-systems-a-primer-9c6b2cf4497a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/hashing-in-modern-recommender-systems-a-primer-9c6b2cf4497a](https://towardsdatascience.com/hashing-in-modern-recommender-systems-a-primer-9c6b2cf4497a)
- en: Understanding the most underrated trick in applied Machine Learning
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解应用机器学习中最被低估的技巧
- en: '[](https://medium.com/@samuel.flender?source=post_page-----9c6b2cf4497a--------------------------------)[![Samuel
    Flender](../Images/390d82a673de8a8bb11cef66978269b5.png)](https://medium.com/@samuel.flender?source=post_page-----9c6b2cf4497a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9c6b2cf4497a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9c6b2cf4497a--------------------------------)
    [Samuel Flender](https://medium.com/@samuel.flender?source=post_page-----9c6b2cf4497a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@samuel.flender?source=post_page-----9c6b2cf4497a--------------------------------)[![Samuel
    Flender](../Images/390d82a673de8a8bb11cef66978269b5.png)](https://medium.com/@samuel.flender?source=post_page-----9c6b2cf4497a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9c6b2cf4497a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9c6b2cf4497a--------------------------------)
    [Samuel Flender](https://medium.com/@samuel.flender?source=post_page-----9c6b2cf4497a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9c6b2cf4497a--------------------------------)
    ·6 min read·Mar 28, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----9c6b2cf4497a--------------------------------)
    ·阅读时间6分钟·2023年3月28日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/9c2812cf751beb430b8fbe23eefabfa7.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c2812cf751beb430b8fbe23eefabfa7.png)'
- en: (Midjourney)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: (Midjourney)
- en: Hashing is one of the most common “tricks” used in industrial Machine Learning
    applications, yet it doesn’t get nearly as much attention as it deserves.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 哈希是工业机器学习应用中最常见的“技巧”之一，但它并没有得到应有的关注。
- en: 'The biggest advantage of hashing, especially in modern [recommender systems](https://medium.com/towards-data-science/biases-in-recommender-systems-top-challenges-and-recent-breakthroughs-edcda59d30bf),
    is its finite-memory guarantee: without hashing, it would be highly impractical
    to learn the relevance of billions of videos, news articles, photos, or web pages
    for billions of users without running out of memory.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 哈希的最大优势，特别是在现代[推荐系统](https://medium.com/towards-data-science/biases-in-recommender-systems-top-challenges-and-recent-breakthroughs-edcda59d30bf)中，是其有限内存保证：如果没有哈希，在不耗尽内存的情况下，学习数十亿视频、新闻文章、照片或网页的相关性将是极其不切实际的。
- en: 'But we’re getting ahead of ourselves here. This post is a primer, so let’s
    go back to where it all started: the famous 2009 “hashing trick” paper.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们这里有些超前了。本文是一个入门介绍，让我们回到一切开始的地方：著名的2009年“哈希技巧”论文。
- en: The paper that started it all
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开创一切的论文
- en: The idea of using hashing as a way to process features, as well as the term
    “hashing trick”, were first introduced in a 2009 [paper](https://arxiv.org/abs/0902.2206)
    by a team of researchers from Yahoo, led by Kilian Weinberger, in the context
    of Email spam detection. An email, after all, is a sequence of words, and each
    word can be thought of as a feature. With hashing, the authors explain, we can
    represent emails as vectors (where each index corresponds to a particular word),
    and use these vectors in a spam collaborative filtering model.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 使用哈希作为处理特征的一种方法，以及“哈希技巧”这一术语，首次在2009年由雅虎的研究团队提出，该团队由Kilian Weinberger领导，背景是电子邮件垃圾邮件检测。毕竟，电子邮件是一系列单词，每个单词都可以被视为一个特征。作者解释说，通过哈希，我们可以将电子邮件表示为向量（其中每个索引对应一个特定的单词），并在垃圾邮件协同过滤模型中使用这些向量。
- en: For example, the phrase “Your prescription is ready” in a 20-dimensional hash
    space may be equivalent to
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，“您的处方已准备好”这一短语在20维哈希空间中可能等同于
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: and so on, where the position of the ones depends on the particular choice of
    hash function used.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 以及其他，1的位置取决于所使用的特定哈希函数。
- en: However, different words may have different relevance to different people. The
    word “prescription” may signal spam to one user, but not to another. In order
    to account for this difference in relevance, the authors introduce “personalized”
    tokens, in which they combine the user id with the words themselves. For the phrase
    above, they’d not just hash the tokens
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，不同的词可能对不同的人具有不同的相关性。词“处方”对某些用户可能表示垃圾邮件，而对另一些用户则不然。为了考虑这种相关性的差异，作者引入了“个性化”标记，将用户ID与单词本身结合起来。对于上述短语，他们不仅哈希标记
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: but also
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 还包括
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: and so on. Given their dataset of 40 Million unique words and 400,000 users,
    this personalized tokenization results in a total of 16 Trillion possible features.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 等等。鉴于他们的数据集包含4000万唯一单词和40万用户，这种个性化标记化导致了总共16万亿个可能的特征。
- en: By hashing these personalized features into a hash size of 2²², or roughly 4.2
    Million (a reduction of 7 orders of magnitude!), the authors achieve a total spam
    reduction of 30%, compared to a baseline model with no hashing-powered personalization,
    one of the first clear demonstrations of the usefulness of hashing in ML problems.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将这些个性化特征哈希成2²²的哈希大小，或大约420万（减少了7个数量级！），作者实现了30%的垃圾邮件减少，相比于没有哈希个性化的基线模型，这是哈希在机器学习问题中有用性的首次明确展示之一。
- en: Hashing in modern recommender systems
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 现代推荐系统中的哈希
- en: Fast forward from 2009 to today, and we see that a lot of the ML technology
    has changed (deep neural nets have largely replaced linear collaborative filters),
    but hashing is still there.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 从2009年到今天，虽然许多机器学习技术已经发生了变化（深度神经网络在很大程度上取代了线性协同过滤器），但哈希仍然存在。
- en: Modern recommender systems are usually some variant of a [two-tower neural network](/learning-to-rank-a-primer-40d2ff9960af),
    where one tower learns user embeddings from user ids, and the other tower learns,
    say, video embedding from video ids (for a video recommender system). At training
    time the model is given user/video pairs from historic engagement data, such as
    video impressions with clicks as positives, and those without clicks as negatives,
    resulting in a shared embedding space for users and videos. We can then store
    the embeddings for all of our users and videos in a set of embedding tables, and
    use these tables to make recommendation at serving time, for example using a kNN
    algorithm.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现代推荐系统通常是某种变体的[双塔神经网络](/learning-to-rank-a-primer-40d2ff9960af)，其中一个塔从用户ID学习用户嵌入，另一个塔则从视频ID学习视频嵌入（对于视频推荐系统）。在训练时，模型会从历史互动数据中获得用户/视频对，例如带有点击的观看次数作为正样本，没有点击的观看次数作为负样本，从而形成一个共享的用户和视频嵌入空间。然后，我们可以将所有用户和视频的嵌入存储在一组嵌入表中，并在服务时使用这些表进行推荐，例如使用kNN算法。
- en: So far so good, but where does hashing fit in here?
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切还算顺利，但哈希在这里的作用是什么？
- en: 'Well, consider a model that stores modest 100-dimensional embeddings of 1B
    videos and 1B users: that’s already 800GB of memory. This is a huge memory footprint,
    and the model will be very impractical (if not impossible) and expensive to serve.
    With hashing, we can first reduce the “vocabulary” of videos and users down to,
    say, 10M each, resulting in a more manageable memory footprint of 8GB.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，考虑一个存储1B视频和1B用户的100维嵌入的模型：这已经占用了800GB的内存。这是一个巨大的内存开销，模型将非常不切实际（如果不是不可能的话）且昂贵。通过哈希，我们可以首先将视频和用户的“词汇表”减少到，比如说，1000万，从而使内存开销更易于管理，为8GB。
- en: Hashing, in other words, allows us to scale modern recommender systems to Billions
    of users and Billions of items. Without hashing, the scale of recommender systems
    would be fundamentally limited by the amount of memory we can afford.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，哈希允许我们将现代推荐系统扩展到数十亿用户和数十亿项目。如果没有哈希，推荐系统的规模将被我们能够承受的内存量所根本限制。
- en: Towards collision-free hashing
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 朝着无冲突哈希的方向前进
- en: The downside of hashing is the existence of hash collisions, the fact that multiple
    different ids will eventually be mapped into the same hash, resulting in multiple
    users or items sharing the same embedding in the embedding table. Obviously, this
    “cramping” of information can degrade the resulting recommendations. One of the
    most important research questions in recommender systems today is therefore how
    to make them collision-free.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 哈希的缺点是哈希冲突的存在，即多个不同的ID最终会映射到相同的哈希值，导致嵌入表中多个用户或项目共享相同的嵌入。显然，这种信息的“挤压”会降低推荐的质量。因此，当前推荐系统中最重要的研究问题之一是如何使其无冲突。
- en: One idea is the use of “[Deep Hash Embeddings](https://arxiv.org/abs/2010.10784)”
    (DHE), proposed in 2021 by a team from Google Brain led by Wang-Cheng Kang. Instead
    of a single hash function, they use a large number of hash functions, and concatenate
    all hashes for a single id into a dense vector, which is then fed into a deep
    neural net which learns higher-oder representations of the ids. The key idea is
    that, if the number of hash functions is large enough, then hash collisions become
    statistically impossible.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一种思路是使用“[深度哈希嵌入](https://arxiv.org/abs/2010.10784)”（DHE），这是由谷歌大脑的王成康团队于2021年提出的。他们使用大量的哈希函数，而不是单个哈希函数，并将所有哈希合并为一个稠密向量，然后输入到深度神经网络中，网络学习ID的高阶表示。关键思想是，如果哈希函数的数量足够大，那么哈希冲突在统计上变得不可能。
- en: 'And indeed, their approach shows promise: using DHE with 1024 hash functions,
    the authors see an improvement of 0.25% in AUC over regular hashing on a set of
    public benchmark datasets. One caveat with this approach though is that is doesn’t
    work for multivalent features, only for ids directly. (For example, DHE, as proposed
    here, couldn’t encode a list of movies you’ve watched on Netflix in the past 30
    days, which would be a multivalent feature.)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 的确，他们的方法显示出前景：使用1024个哈希函数的DHE，作者在一组公共基准数据集上看到AUC提高了0.25%。不过，这种方法的一个缺点是它不适用于多值特征，只适用于直接的ID。（例如，所提到的DHE不能编码过去30天你在Netflix上观看的电影列表，这将是一个多值特征。）
- en: Another promising collision-free hashing approach is “[Cuckoo hashing](https://www.brics.dk/RS/01/32/BRICS-RS-01-32.pdf)”,
    first introduced in 2001 by Rasmus Pagh and Flemming Rodler at Arhus University,
    Denmark. Cuckoo hashing is used in Monolith, ByteDance’s online recommendation
    system, which is introduced in their 2022 [paper](https://arxiv.org/abs/2209.07663),
    led by Zhuoran Liu.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种有前景的无冲突哈希方法是“[布谷鸟哈希](https://www.brics.dk/RS/01/32/BRICS-RS-01-32.pdf)”，由丹麦奥胡斯大学的拉斯穆斯·帕赫和弗莱明·罗德勒于2001年首次提出。布谷鸟哈希被用于字节跳动的在线推荐系统Monolith，该系统在他们2022年的[论文](https://arxiv.org/abs/2209.07663)中介绍，由刘卓然领导。
- en: 'In Cuckoo hashing we maintain not one but multiple hash tables: in ByteDance’s
    paper, they use 2\. When we encounter a new id, by default we hash it into the
    first hash table. If the first hash table is already occupied with another id,
    we evict that id (hence the name of the algorithm) and re-hash it into a spot
    in the second hash table. This process is repeated until there are no more evictions,
    and the set of hash tables stabilizes without collisions. Compared to regular
    hashing (with collisions) the authors find an improvement of 0.5% AUC on a public
    benchmark dataset with their collision-free hashing approach.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在布谷鸟哈希中，我们不仅维护一个哈希表，而是多个哈希表：在字节跳动的论文中，他们使用了2个。当遇到新的ID时，默认情况下我们将其哈希到第一个哈希表中。如果第一个哈希表已被另一个ID占据，我们将逐出那个ID（因此算法的名称）并将其重新哈希到第二个哈希表中的一个位置。这个过程会重复直到没有更多逐出，哈希表集合稳定且没有冲突。与常规哈希（有冲突）相比，作者发现他们的无冲突哈希方法在公共基准数据集上提高了0.5%
    AUC。
- en: 'Coda: the most underrated trick in applied Machine Learning'
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代码：应用机器学习中最被低估的技巧
- en: 'Recap for memory:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 内存回顾：
- en: Hashing allows us to scale modern recommender systems to Billions of users and
    Billions of items with a finite-memory guarantee.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哈希可以让我们将现代推荐系统扩展到数十亿用户和数十亿项目，同时提供有限内存保证。
- en: The idea of using hashing in ML application dates back to a 2009 paper from
    Yahoo, which demonstrated its usefulness in an Email spam detection model.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在机器学习应用中使用哈希的想法可以追溯到2009年Yahoo的一篇论文，该论文展示了其在电子邮件垃圾邮件检测模型中的有效性。
- en: Hashing however introduces hash collisions, which can degrade recommendations
    due to multiple users and items sharing the same embedding.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，哈希会引入哈希冲突，这可能由于多个用户和项目共享相同的嵌入而降低推荐质量。
- en: For this reason, recent research in recommender systems focuses on how to make
    hashing collision-free. Notable examples are Deep Hash Embeddings (Google Brain)
    and Cuckoo Hashing (ByteDance).
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，最近推荐系统的研究集中在如何使哈希无冲突上。显著的例子有深度哈希嵌入（谷歌大脑）和布谷鸟哈希（字节跳动）。
- en: For many years, hashing has been among the most underrated tricks in the applied
    ML literature, with much of the attention focusing instead on model architecture,
    data selection, or feature engineering.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，哈希一直是应用机器学习文献中最被低估的技巧之一，大部分关注点集中在模型架构、数据选择或特征工程上。
- en: This may begin to change. As the recent papers from Google Brain, ByteDance,
    and others have shown, optimizing recommender systems for hash collisions can
    enable notable gains in performance. The surprising rise in popularity of TikTok
    (owned by ByteDance) may, at least in part, be explained by better hashing.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能开始发生变化。正如谷歌大脑、字节跳动等最近的论文所示，优化推荐系统以减少哈希碰撞可以显著提升性能。TikTok（由字节跳动拥有）的惊人受欢迎程度，至少部分可以用更好的哈希解释。
- en: 'Watch this space: new breakthroughs are certainly on the horizon.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 请关注这个领域：新的突破确实在即将到来。
- en: '[](https://medium.com/@samuel.flender/subscribe?source=post_page-----9c6b2cf4497a--------------------------------)
    [## Don''t want to rely on Medium''s algorithms? Sign up.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@samuel.flender/subscribe?source=post_page-----9c6b2cf4497a--------------------------------)
    [## 不想依赖 Medium 的算法？注册一下。'
- en: Don't want to rely on Medium's algorithms? Sign up. Make sure you won't miss
    my next article by signing up to my Email…
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 不想依赖 Medium 的算法？注册一下。通过注册我的电子邮件，确保你不会错过我的下一篇文章…
- en: medium.com](https://medium.com/@samuel.flender/subscribe?source=post_page-----9c6b2cf4497a--------------------------------)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@samuel.flender/subscribe?source=post_page-----9c6b2cf4497a--------------------------------)
