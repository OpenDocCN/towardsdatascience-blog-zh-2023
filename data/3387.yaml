- en: Towards Understanding the Mixtures of Experts Model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《深入理解专家混合模型》
- en: 原文：[https://towardsdatascience.com/towards-understanding-the-mixtures-of-experts-model-45d11ee5d50d?source=collection_archive---------9-----------------------#2023-11-14](https://towardsdatascience.com/towards-understanding-the-mixtures-of-experts-model-45d11ee5d50d?source=collection_archive---------9-----------------------#2023-11-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/towards-understanding-the-mixtures-of-experts-model-45d11ee5d50d?source=collection_archive---------9-----------------------#2023-11-14](https://towardsdatascience.com/towards-understanding-the-mixtures-of-experts-model-45d11ee5d50d?source=collection_archive---------9-----------------------#2023-11-14)
- en: New research reveals what happens under the hood when we train MoE models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 新研究揭示了我们训练 MoE 模型时底层发生的情况
- en: '[](https://medium.com/@samuel.flender?source=post_page-----45d11ee5d50d--------------------------------)[![Samuel
    Flender](../Images/390d82a673de8a8bb11cef66978269b5.png)](https://medium.com/@samuel.flender?source=post_page-----45d11ee5d50d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----45d11ee5d50d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----45d11ee5d50d--------------------------------)
    [Samuel Flender](https://medium.com/@samuel.flender?source=post_page-----45d11ee5d50d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@samuel.flender?source=post_page-----45d11ee5d50d--------------------------------)[![Samuel
    Flender](../Images/390d82a673de8a8bb11cef66978269b5.png)](https://medium.com/@samuel.flender?source=post_page-----45d11ee5d50d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----45d11ee5d50d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----45d11ee5d50d--------------------------------)
    [Samuel Flender](https://medium.com/@samuel.flender?source=post_page-----45d11ee5d50d--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fce56d9dcd568&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftowards-understanding-the-mixtures-of-experts-model-45d11ee5d50d&user=Samuel+Flender&userId=ce56d9dcd568&source=post_page-ce56d9dcd568----45d11ee5d50d---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----45d11ee5d50d--------------------------------)
    ·8 min read·Nov 14, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F45d11ee5d50d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftowards-understanding-the-mixtures-of-experts-model-45d11ee5d50d&user=Samuel+Flender&userId=ce56d9dcd568&source=-----45d11ee5d50d---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fce56d9dcd568&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftowards-understanding-the-mixtures-of-experts-model-45d11ee5d50d&user=Samuel+Flender&userId=ce56d9dcd568&source=post_page-ce56d9dcd568----45d11ee5d50d---------------------post_header-----------)
    发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----45d11ee5d50d--------------------------------)
    ·8 分钟阅读·2023 年 11 月 14 日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F45d11ee5d50d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftowards-understanding-the-mixtures-of-experts-model-45d11ee5d50d&user=Samuel+Flender&userId=ce56d9dcd568&source=-----45d11ee5d50d---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F45d11ee5d50d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftowards-understanding-the-mixtures-of-experts-model-45d11ee5d50d&source=-----45d11ee5d50d---------------------bookmark_footer-----------)![](../Images/84a6c24700294b529c1ea0523bea9a2a.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F45d11ee5d50d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftowards-understanding-the-mixtures-of-experts-model-45d11ee5d50d&source=-----45d11ee5d50d---------------------bookmark_footer-----------)![](../Images/84a6c24700294b529c1ea0523bea9a2a.png)'
- en: Image created by the author with Midjourney
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作者用 Midjourney 创建的图像
- en: '[Mixtures of Expert (MoE) models](/machine-learning-with-expert-models-a-primer-6c74585f223f)
    have rapidly become one of the most powerful technologies in modern ML applications,
    enabling breakthroughs such as the Switch Transformer and GPT-4\. Really, we’re
    just starting to see their full impact!'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[专家混合模型（MoE）](/machine-learning-with-expert-models-a-primer-6c74585f223f) 已迅速成为现代机器学习应用中最强大的技术之一，使得像
    Switch Transformer 和 GPT-4 这样的突破成为可能。实际上，我们才刚刚开始看到它们的全部影响！'
- en: However, surprisingly little is known about why exactly MoE works in the first
    place. When does MoE work? Why does the gate not simply send all training examples
    to the same expert? Why does the model not collapse into a state in which all
    experts are identical? How exactly do the experts specialize, and in what? What
    exactly does the gate learn?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，令人惊讶的是，对于MoE为何有效的具体原因知之甚少。MoE在什么情况下有效？为什么门控机制不会简单地将所有训练样本都发送给同一个专家？为什么模型不会陷入所有专家都相同的状态？专家们究竟如何专门化，他们专门化的内容是什么？门控机制究竟学到了什么？
- en: Luckily, research has started to shed some light into these questions. Let’s
    take a look.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，研究已经开始揭示这些问题的答案。我们来看看。
- en: MoE models — a lighting primer
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MoE模型——一个基础入门
- en: '![](../Images/6243d227d3ea0a06796504c80d1766a8.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6243d227d3ea0a06796504c80d1766a8.png)'
- en: 'Image source: [Adaptive Mixtures of Local Experts](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[局部专家的自适应混合](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf)
- en: As a brief reminder, MoE was invented in the 1991 paper “[Adaptive Mixtures
    of Local Experts](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf)”, co-authored
    by none other than the godfather of AI himself, Geoffrey Hinton. The key idea
    in MoE is to model an output y given an input x by combining a number of…
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 简单回顾一下，MoE（Mixtures of Experts）最早在1991年的论文“[局部专家的自适应混合](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf)”中被提出，该论文的共同作者正是人工智能的奠基人**杰弗里·辛顿**。MoE的关键思想是通过结合多个专家来对给定输入x的输出y进行建模…
