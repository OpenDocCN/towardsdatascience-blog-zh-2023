- en: The Best Optimization Algorithm for Your Neural Network
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 适合你神经网络的最佳优化算法
- en: 原文：[https://towardsdatascience.com/the-best-optimization-algorithm-for-your-neural-network-d16d87ef15cb](https://towardsdatascience.com/the-best-optimization-algorithm-for-your-neural-network-d16d87ef15cb)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-best-optimization-algorithm-for-your-neural-network-d16d87ef15cb](https://towardsdatascience.com/the-best-optimization-algorithm-for-your-neural-network-d16d87ef15cb)
- en: How to choose it and minimize your neural network training time.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何选择它并最小化你的神经网络训练时间。
- en: '[](https://medium.com/@riccardo.andreoni?source=post_page-----d16d87ef15cb--------------------------------)[![Riccardo
    Andreoni](../Images/5e22581e419639b373019a809d6e65c1.png)](https://medium.com/@riccardo.andreoni?source=post_page-----d16d87ef15cb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d16d87ef15cb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d16d87ef15cb--------------------------------)
    [Riccardo Andreoni](https://medium.com/@riccardo.andreoni?source=post_page-----d16d87ef15cb--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@riccardo.andreoni?source=post_page-----d16d87ef15cb--------------------------------)[![Riccardo
    Andreoni](../Images/5e22581e419639b373019a809d6e65c1.png)](https://medium.com/@riccardo.andreoni?source=post_page-----d16d87ef15cb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d16d87ef15cb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d16d87ef15cb--------------------------------)
    [Riccardo Andreoni](https://medium.com/@riccardo.andreoni?source=post_page-----d16d87ef15cb--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d16d87ef15cb--------------------------------)
    ·13 min read·Oct 14, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d16d87ef15cb--------------------------------)
    ·13分钟阅读·2023年10月14日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/5d74e7bc343277d4b012a49a8ed2de2f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5d74e7bc343277d4b012a49a8ed2de2f.png)'
- en: 'Image source: [unsplash.com](https://unsplash.com/photos/FfbVFLAVscw).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[unsplash.com](https://unsplash.com/photos/FfbVFLAVscw)。
- en: Developing any machine learning model involves a rigorous experimental process
    that follows the [**idea-experiment-evaluation cycle**](/ai-ml-practicalities-the-cycle-of-experimentation-fd46fc1f3835).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 开发任何机器学习模型都涉及一个严格的实验过程，该过程遵循[**思想-实验-评估循环**](/ai-ml-practicalities-the-cycle-of-experimentation-fd46fc1f3835)。
- en: '![](../Images/89f8be024c84d92865f49b0077b29126.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89f8be024c84d92865f49b0077b29126.png)'
- en: Image by the author.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: The above cycle is repeated multiple times until satisfactory performance levels
    are achieved. The “experiment” phase involves both the coding and the training
    steps of the machine learning model. As **models become more complex** and are
    trained over much **larger datasets**, training time inevitably expands. As a
    consequence, training a large deep neural network can be painfully slow.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 上述循环重复多次，直到达到令人满意的性能水平。“实验”阶段包括机器学习模型的编码和训练步骤。由于**模型变得更加复杂**且在更**大的数据集**上训练，训练时间不可避免地增加。因此，训练大型深度神经网络可能会非常缓慢。
- en: 'Fortunately for data science practitioners, there exist several techniques
    to accelerate the training process, including:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据科学从业者来说，幸运的是，存在几种加速训练过程的技术，包括：
- en: '**Transfer Learning**.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迁移学习**。'
- en: '**Weight Initialization**, as Glorot or He initialization.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**权重初始化**，如Glorot或He初始化。'
- en: '**Batch Normalization** for training data.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量归一化**用于训练数据。'
- en: Picking a **reliable activation function**.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个**可靠的激活函数**。
- en: Use a **faster optimizer**.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**更快的优化器**。
- en: While all the techniques I pointed out are important, in this post I will focus
    deeply on the last point. I will describe multiple algorithm for neural network
    parameters optimization, highlighting both their advantages and limitations.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我指出的所有技术都很重要，但在这篇文章中我将深入关注最后一点。我将描述多种神经网络参数优化算法，强调它们的优点和局限性。
- en: In the last section of this post, I will present a visualization displaying
    the comparison between the discussed optimization algorithms.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章的最后一部分，我将展示一个可视化图，显示讨论过的优化算法之间的比较。
- en: 'For **practical implementation**, all the code used in this article can be
    accessed in this **GitHub repository**:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于**实际实现**，本文中使用的所有代码可以在这个**GitHub 仓库**中访问：
- en: '[## articles/NN-optimizer at main · andreoniriccardo/articles'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[## articles/NN-optimizer at main · andreoniriccardo/articles'
- en: Contribute to andreoniriccardo/articles development by creating an account on
    GitHub.
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过在GitHub上创建一个帐户，参与andreoniriccardo/articles的开发。
- en: github.com](https://github.com/andreoniriccardo/articles/tree/main/NN-optimizer?source=post_page-----d16d87ef15cb--------------------------------)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/andreoniriccardo/articles/tree/main/NN-optimizer?source=post_page-----d16d87ef15cb--------------------------------)'
- en: Batch Gradient Descent
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批量梯度下降
- en: Traditonally, Batch Gradient Descent is considered the **default choice** for
    the optimizer method in neural networks.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，批量梯度下降被认为是神经网络中优化方法的**默认选择**。
- en: After the neural network generates predictions for the entire training set *X*,
    we compare the network’s predictions to the actual labels of each training point.
    This is done to compute a cost function *J(W,b)*, which is a scalar representation
    of the capacity of the model to yield accurate predictions. The Gradient Descent
    optimization algorithm uses the cost function as a guide to tune every network’s
    parameter. This iterative process continues until the cost function is close to
    zero, or it can’t be reduced further. What GD does is to adjust each weight and
    bias of the network in a certain direction. The chosen direction is the one that
    reduces the cost function the most.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络对整个训练集*X*生成预测之后，我们将网络的预测与每个训练点的实际标签进行比较。这是为了计算一个成本函数*J(W,b)*，它是模型生成准确预测能力的标量表示。梯度下降优化算法使用成本函数作为指导来调整每个网络参数。这个迭代过程持续进行，直到成本函数接近零，或无法进一步降低。梯度下降算法的作用是以特定方向调整网络的每个权重和偏置。选择的方向是最能降低成本函数的方向。
- en: '![](../Images/032bf6c09d88228ba0b850a1ae27dcc4.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/032bf6c09d88228ba0b850a1ae27dcc4.png)'
- en: Image by the author.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片。
- en: In visual terms, take the image above for reference. Gradient descent starts
    at the leftmost blue point. By analyzing the gradient of the cost function around
    the starting point, it adjusts the parameter (x-axis value) to the right. This
    process is repeated multiple times until the algorithm ends up with a very good
    approximation of the optimum.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从视觉角度来看，可以参考上面的图像。梯度下降从最左边的蓝点开始。通过分析起始点周围的成本函数梯度，它将参数（x轴值）调整到右侧。这个过程重复多次，直到算法得到了非常好的最优近似。
- en: 'If the cost function has 2 input parameters, the function is not a line but
    instead, it creates a three-dimensional surface. The batch gradient descent steps
    can be displayed considering the level curves of the surface:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果成本函数有2个输入参数，函数不是一条直线，而是创建了一个三维表面。可以考虑该表面的等高线来显示批量梯度下降步骤：
- en: '![](../Images/c005940ab8e986e586912d524d4b1965.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c005940ab8e986e586912d524d4b1965.png)'
- en: Image by the author.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片。
- en: A neural network consists of thousands or millions of parameters that affect
    the cost functions. While visualizing million-dimensional representations is infeasible,
    mathematical equations are useful to understand the gradient descent process.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络由成千上万的参数组成，这些参数会影响成本函数。虽然可视化百万维度的表示是不切实际的，但数学方程对理解梯度下降过程很有帮助。
- en: 'At first, we compute the cost function *J(W,b)*, which is a function of the
    network’s weights *W* and biases *b*. Then, the backpropagation algorithm computes
    the derivative of the cost function with respect to each weight and bias of each
    layer of the network:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们计算成本函数*J(W,b)*，这是网络权重*W*和偏置*b*的函数。然后，反向传播算法计算成本函数相对于网络每层的每个权重和偏置的导数：
- en: '![](../Images/b9fac62a55efc0ded9e5e592526f0797.png)![](../Images/d14e35916ec9cbcb3f90950c9b0f03b8.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b9fac62a55efc0ded9e5e592526f0797.png)![](../Images/d14e35916ec9cbcb3f90950c9b0f03b8.png)'
- en: Knowing the direction towards which to adjust the parameters, we update them.
    The magnitude of each parameter’s update is regulated by the gradient itself and
    by the learning rate alpha. Alpha is a hyperparameter of the optimization algorithm
    and its value is typically maintained constant.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 知道了调整参数的方向后，我们更新它们。每个参数更新的幅度由梯度本身和学习率alpha调节。Alpha是优化算法的一个超参数，其值通常保持不变。
- en: '![](../Images/653c4eb05a3afa4a33499ea2f81524f3.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/653c4eb05a3afa4a33499ea2f81524f3.png)'
- en: 'Batch Gradient Descent offers several advantages:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 批量梯度下降提供了几个优点：
- en: '**Simplicity**: It’s a straightforward and easy-to-understand method.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简洁性**：这是一种直接且易于理解的方法。'
- en: '**Hyperparameter Management**: It requires tuning only one hyperparameter,
    the learning rate.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**超参数管理**：只需要调整一个超参数，即学习率。'
- en: '**Optimality**: For convex cost functions, it reliably reaches the global optimum
    with a reasonable learning rate.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最优性**：对于凸成本函数，它可靠地在合理的学习率下达到全局最优。'
- en: 'The disadvantages of gradient descent are:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降的缺点是：
- en: '**Computational Intensity**: It can be slow, particularly for large training
    sets, as it updates parameters after evaluating all examples.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算强度**：它可能较慢，特别是对于大型训练集，因为在评估所有样本后更新参数。'
- en: '**Local Optima and Saddle Points**: It might get stuck in local optima or saddle
    points, slowing convergence.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**局部最优和鞍点**：它可能会陷入局部最优或鞍点，从而减慢收敛速度。'
- en: '**Gradient Issues**: It is prone to vanishing gradient issues.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度问题**：它容易出现梯度消失问题。'
- en: '**Memory Usage**: It requires storing the entire training data in CPU/GPU memory.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存使用**：它需要将整个训练数据存储在CPU/GPU内存中。'
- en: Mini-Batch Gradient Descent
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 小批量梯度下降
- en: In batch gradient descent algorithm all training examples are used to calculate
    a single improvement step. The step will be the most accurate, as it considers
    all the available information. However, this approach often is impractically slow
    in real-world applications, suggesting the implementation of faster alternatives.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在批量梯度下降算法中，所有训练样本都用于计算单个改进步骤。该步骤将是最准确的，因为它考虑了所有可用的信息。然而，这种方法在实际应用中往往过于缓慢，建议实现更快的替代方案。
- en: The most common solution is called [**Mini-Batch Gradient Descent**](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/),
    and it uses only a **small subset of the training data** in order to update the
    weights’ values.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的解决方案是称为[**小批量梯度下降**](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/)，它只使用**训练数据的小子集**来更新权重值。
- en: 'Consider the whole training set *X*, and the associated labels *Y*:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑整个训练集 *X* 和相关标签 *Y*：
- en: '![](../Images/775b6cf970b2d797aac374f7015b1acb.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/775b6cf970b2d797aac374f7015b1acb.png)'
- en: where *m* represents the number of training examples.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *m* 表示训练样本的数量。
- en: 'Rather than feeding the whole batch to the optimization algorithm, we process
    only a small portion of the training set. Let’s say we feed the algorithm the
    subsets *X^t* and *Y^t*, each containing 512 training examples:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 与其将整个批次喂给优化算法，不如仅处理训练集的一小部分。假设我们将子集 *X^t* 和 *Y^t* 喂给算法，每个子集包含512个训练样本：
- en: '![](../Images/bc8b776451f0b2634dcf04cb13cf92aa.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc8b776451f0b2634dcf04cb13cf92aa.png)'
- en: For instance, if the total training set contains 5,120,000 points, it can be
    divided into 10,000 mini-batches, each containing 512 examples.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果总训练集包含5,120,000个点，则可以将其分成10,000个小批量，每个小批量包含512个样本。
- en: 'For each mini-batch, we perform the classical gradient descent operations:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个小批量，我们执行经典的梯度下降操作：
- en: '**Compute the cost** relative to the mini-batch t, *J^t(W,b)*.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算相对于小批量 t 的成本**，*J^t(W,b)*。'
- en: '**Perform backpropagation** to compute the gradients of *J^t(W,b)* with respect
    to each weight and bias.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**执行反向传播**以计算 *J^t(W,b)* 相对于每个权重和偏差的梯度。'
- en: '**Update the parameters**.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更新参数**。'
- en: The green line in the image below shows a typical optimization path of the mini-batch
    gradient descent. While batch gradient descent follows a more direct path to the
    optimal point, mini-batch gradient descent appears to take several unnecessary
    steps, due to its limited dataset at each iteration.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 下图中的绿色线条显示了小批量梯度下降的典型优化路径。虽然批量梯度下降会沿着更直接的路径到达最优点，但由于每次迭代中数据集的限制，小批量梯度下降似乎会采取几步不必要的路径。
- en: '![](../Images/c9c4d7274702fbc3b4c2dfedc82d47ce.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c9c4d7274702fbc3b4c2dfedc82d47ce.png)'
- en: Image by the author.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: The reason for that is that mini-batch gradient descent, at any time *t,* has
    only a small portion of the training set to compute its decision on. With that
    limitation on the available information, it’s clear that the route followed is
    not the most direct.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 原因在于，小批量梯度下降在任何时间 *t* 只有一小部分训练集用于计算其决策。由于可用信息的限制，显然所走的路线不是最直接的。
- en: However, the significant advantage of mini-batch gradient descent, is that each
    step is extremely fast to be computed since the algorithm needs to evaluate only
    a small portion of the data instead of the whole training set. In our example,
    each step requires the evaluation of only 512 data points instead of 5 million.
    This is also the reason why almost no real application, requiring a large amount
    of data, uses batch gradient descent.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，小批量梯度下降的显著优势在于每一步计算非常快速，因为算法只需评估数据的一小部分，而不是整个训练集。在我们的例子中，每一步只需要评估512个数据点，而不是500万。这也是为什么几乎没有真实应用需要大量数据时使用批量梯度下降的原因。
- en: As the image shows, with mini-batch gradient descent there is no guarantee that
    the cost at iteration *t+1* is lower than the cost at iteration t, but, if the
    problem is well defined, the optimization algorithm reaches an area very close
    to the optimal point really quickly.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，使用小批量梯度下降算法无法保证迭代 *t+1* 的成本低于迭代 t 的成本，但如果问题定义良好，优化算法可以非常快速地达到接近最优点的区域。
- en: The choice of the mini-batch size represents an additional hyperparameter of
    the network’s training process. A batch size equal to the total number of examples
    (*m*) corresponds to the batch gradient descent optimization. Instead, if *m=1*
    we are performing Stochastic Gradient Descent, where each training example is
    a mini-batch.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 小批量大小的选择代表了网络训练过程中的一个附加超参数。批量大小等于总样本数量 (*m*) 对应于批量梯度下降优化。相反，如果 *m=1*，我们正在执行随机梯度下降，每个训练样本就是一个小批量。
- en: If the training set is small, batch gradient descent can be a valid option,
    otherwise, popular mini-batch sizes like 32, 64, 128, 256, and 512 are commonly
    considered. For some reason, a batch size equal to a power of 2 seems to perform
    better.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果训练集很小，批量梯度下降可能是一个有效的选择，否则，像32、64、128、256和512这样的常见小批量大小通常被考虑。出于某种原因，等于2的幂的批量大小似乎表现更好。
- en: While stochastic gradient descent with a batch size of 1 is an option, it tends
    to be extremely noisy, and it can bounce further away from the minima. Moreover,
    stochastic gradient descent is very inefficient if we consider the computation
    time per example, as it cannot exploit the benefits of vectorization.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然批量大小为1的随机梯度下降是一种选择，但它往往非常嘈杂，并且可能远离最小值。此外，考虑到每个样本的计算时间，随机梯度下降效率很低，因为它无法利用向量化的好处。
- en: Gradient Descent with Momentum
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动量梯度下降
- en: 'Recall that in both Batch and Mini-Batch Gradient Descent, the parameter update
    follows a defined formula:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在批量和小批量梯度下降中，参数更新遵循一个定义的公式：
- en: '![](../Images/87fefcb663e3b7faa65b4eaa7be30835.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/87fefcb663e3b7faa65b4eaa7be30835.png)'
- en: Therefore, in this equation, the size of each optimization step is determined
    by the learning rate, a fixed quantity, and the gradient, which is computed at
    a specific point of the cost function.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这个方程中，每一步优化的大小由学习率（一个固定的量）和在成本函数特定点计算出的梯度决定。
- en: When the gradient is computed in a nearly flat section of the cost function,
    it will be very small, leading to a proportionally small step of gradient descent.
    Consider the difference the gradient difference at points A and B of the image
    below.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 当梯度在成本函数的近似平坦区域计算时，它将非常小，从而导致相应的小梯度下降步长。考虑下图中点A和B的梯度差异。
- en: '![](../Images/807b45e443cab6256ff027001474723b.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/807b45e443cab6256ff027001474723b.png)'
- en: Image by the author.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由作者提供。
- en: '[**Momentum Gradient Descent**](/gradient-descent-with-momentum-59420f626c8f)
    overcomes this issue. We can imagine momentum gradient descent as a bowling ball
    rolling down a hill, which shape is defined by the cost function. If the ball
    begins its descent from a steep portion of the slope, its movement begins slowly
    but it will quickly gain velocity and momentum. Due to its momentum, the ball
    maintains great velocity even through a -flat area of the slope.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[**动量梯度下降**](/gradient-descent-with-momentum-59420f626c8f) 解决了这个问题。我们可以把动量梯度下降想象成一个沿着坡道滚下的保龄球，坡道的形状由成本函数定义。如果保龄球从坡道陡峭的部分开始下降，它的运动开始时较慢，但会很快获得速度和动量。由于其动量，即使在坡道的平坦区域，保龄球也能保持很高的速度。'
- en: 'This is the core concept of momentum gradient descent: the algorithm will consider
    previous gradients, and not only the one computed at iteration *t*. Similar to
    the bowling ball analogy, the gradient computed at iteration *t* defines the acceleration
    and not the speed of the ball.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是动量梯度下降的核心概念：算法会考虑以前的梯度，而不仅仅是迭代 *t* 时计算的梯度。类似于保龄球的类比，迭代 *t* 时计算的梯度定义了保龄球的加速度，而不是速度。
- en: 'Both weights and biases’ velocities are computed at each step using the previous
    velocity and the current iteration’s gradient:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一步中，权重和偏置的速度都是使用前一速度和当前迭代的梯度计算的。
- en: '![](../Images/8a438ce7233a86faeebe73227e864497.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8a438ce7233a86faeebe73227e864497.png)'
- en: The parameter *beta*, known as momentum, regulates how much the new velocity
    value is determined from the current slope or from the past velocity value.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 参数 *beta*，即动量，调节新速度值是根据当前斜率还是过去速度值来决定的。
- en: 'Finally, parameters are updated using the computed velocity:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用计算出的速度更新参数：
- en: '![](../Images/48791f6392f67a809a6c5c07ccae0868.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/48791f6392f67a809a6c5c07ccae0868.png)'
- en: Momentum Gradient Descent has proven superior performance compared to Mini-Batch
    Gradient Descent in most applications. The main drawback of Momentum Gradient
    Descent, with respect to standard Gradient Descent, is that it requires an additional
    parameter to tune. However, practice shows how a value of *beta* equal to 0.9
    works effectively.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于迷你批量梯度下降，动量梯度下降在大多数应用中表现出更优越的性能。动量梯度下降相对于标准梯度下降的主要缺点是它需要额外的参数进行调整。然而，实践表明，*beta*
    等于 0.9 的值效果很好。
- en: RMS Prop
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RMS Prop
- en: Consider a cost function shaped similarly to an **elongated bowl**, where the
    minimum point is located in the narrowest part. The function’s contour is described
    by the level curves in the image below.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个成本函数，其形状类似于**长形碗**，其中最小点位于最窄的部分。该函数的轮廓由下图中的水平线条描述。
- en: '![](../Images/c94d07c2aca361a7d27c0a0ee2e2f998.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c94d07c2aca361a7d27c0a0ee2e2f998.png)'
- en: Image by the author.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：作者。
- en: In cases where the starting point is far away from the minimum, Gradient Descent,
    also with the momentum variation, starts by following the steepest slope, which,
    in the picture above, is not the optimal path towards the minimum. The key idea
    of [**RMS Prop**](https://keras.io/api/optimizers/rmsprop) optimization algorithm
    is to correct the direction early and to aim for the global minimum more promptly.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在起始点距离最小值较远的情况下，梯度下降（即使是带有动量的变体）开始时沿着最陡的斜坡前进，在上图中，这不是通向最小值的最佳路径。[**RMS Prop**](https://keras.io/api/optimizers/rmsprop)优化算法的关键思想是早期修正方向，并更迅速地瞄准全局最小值。
- en: Similarly to Momentum Gradient Descent, RMS Prop requires fine-tuning through
    an additional hyperparameter known as the decay rate. Through practical experience,
    it has been proven that setting the decay rate to 0.9 is a good choice for most
    problems.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 与动量梯度下降类似，RMS Prop 需要通过一个额外的超参数（称为衰减率）进行调整。通过实际经验，已经证明将衰减率设置为 0.9 是大多数问题的良好选择。
- en: Adam
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Adam
- en: '[**Adam**](https://keras.io/api/optimizers/adam)and its variations are probably
    the most employed optimization algorithms for neural networks. Adam, which stands
    for **Adaptive Moment Estimation**, is derived from the combinations of Momentum
    Gradient Descent and RMS Prop together.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[**Adam**](https://keras.io/api/optimizers/adam)及其变体可能是神经网络中最常用的优化算法。Adam，即**自适应动量估计**，源自动量梯度下降和
    RMS Prop 的组合。'
- en: 'As a mix of the two optimization methods, Adam needs two additional hyperparameters
    to be tuned (besides the learning rate *alpha*). We call them *beta_1* and *beta_2*
    and they are the hyperparameters used respectively in Momentum and in RMS Prop.
    As happened for the other discussed algorithms, effective default choices for
    *beta_1* and *beta_2* exist: *beta_1* is usually set to 0.9 and *beta_2* to 0.999\.
    In Adam appears also a parameter *epsilon* that works as a smoothing term and
    it is nearly always set to a small value like e-7 or e-8.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 作为两种优化方法的混合，Adam 需要调整两个额外的超参数（除了学习率 *alpha*）。我们称它们为 *beta_1* 和 *beta_2*，它们分别是动量和
    RMS Prop 中使用的超参数。与其他讨论的算法一样，*beta_1* 和 *beta_2* 也有有效的默认值选择：*beta_1* 通常设置为 0.9，*beta_2*
    设置为 0.999。Adam 还包括一个 *epsilon* 参数，它作为平滑项，并且几乎总是设置为像 e-7 或 e-8 这样的小值。
- en: 'Adam optimization algorithm outperforms all the above-mentioned methods in
    most cases. The only exceptions can be very simple problems where simpler methods
    work faster. The efficiency of Adam does come with a trade-off: the need to fine-tune
    two additional parameters. However, in my opinion it is a small price to pay for
    its efficiency.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，Adam 优化算法优于上述所有方法。唯一的例外是一些非常简单的问题，在这些问题中，简单的方法效果更快。Adam 的效率确实有一个权衡：需要调整两个额外的参数。然而，在我看来，这是为其效率付出的微小代价。
- en: Nadam and AdaMax
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Nadam 和 AdaMax
- en: 'It is worth citing two algorithms born as modifications of the widely used
    Adam optimization: **Nadam** and **AdaMax**.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 值得提及的两个算法是作为广泛使用的 Adam 优化算法的修改版：**Nadam** 和 **AdaMax**。
- en: '[Nadam](https://keras.io/api/optimizers/Nadam), short of Nesterov-accelerated
    Adaptive Moment Estimation, enhances Adam by incorporating Nesterov Accelerated
    Gradient (NAG) into its framework. This means that Nadam not only benefits from
    the adaptive learning rates and momentum of Adam, but also from the NAG component,
    a technique that helps the algorithm predict the next step more accurately. Especially
    in high-dimensional spaces, Nadam converges faster and more effectively.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[Nadam](https://keras.io/api/optimizers/Nadam)，即Nesterov加速自适应矩估计，通过将Nesterov加速梯度（NAG）融入其框架来增强Adam。这意味着Nadam不仅受益于Adam的自适应学习率和动量，还受益于NAG组件，这是一种帮助算法更准确预测下一步的技术。尤其在高维空间中，Nadam的收敛速度更快，效果更佳。'
- en: '[AdaMax](https://keras.io/api/optimizers/adamax), instead, adopts a slightly
    different approach. While Adam calculates adaptive learning rates based on both
    the first and second moments of the gradients, AdaMax focuses only on the maximum
    norm of the gradients. AdaMax’s simplicity and efficiency in handling sparse gradients
    make it an appealing choice for training deep neural networks, especially in tasks
    involving sparse data.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[AdaMax](https://keras.io/api/optimizers/adamax)则采用了略微不同的方法。虽然Adam根据梯度的一阶矩和二阶矩计算自适应学习率，但AdaMax仅关注梯度的最大范数。AdaMax在处理稀疏梯度方面的简单性和高效性使其成为训练深度神经网络的一个有吸引力的选择，尤其是在涉及稀疏数据的任务中。'
- en: Optimization Algorithms Comparison
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化算法比较
- en: In order to practically test and visualize the performance of each discussed
    optimization algorithm, I trained a simple deep neural network with each one of
    the above optimizers. The job of the network is to classify fashion items displayed
    in 28x28 pixels images. The dataset is called [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist)
    (MIT licensed) and is composed of 70,000 small grey-scale images of clothing items.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实际测试和可视化每种讨论的优化算法的性能，我使用上述每种优化器训练了一个简单的深度神经网络。网络的任务是对28x28像素的图像中的时尚物品进行分类。数据集名为
    [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist)（MIT许可），由70,000张小型灰度服装图像组成。
- en: The algorithm I used to test different optimizers is presented in the following
    snippet and, more completely in the GitHub repository linked below.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我用来测试不同优化器的算法在下面的代码片段中展示，更详细的信息请参见下方的GitHub仓库。
- en: '[PRE0]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[## articles/NN-optimizer at main · andreoniriccardo/articles'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[## articles/NN-optimizer at main · andreoniriccardo/articles'
- en: Contribute to andreoniriccardo/articles development by creating an account on
    GitHub.
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过在GitHub上创建账户来为andreoniriccardo/articles的开发做贡献。
- en: github.com](https://github.com/andreoniriccardo/articles/tree/main/NN-optimizer?source=post_page-----d16d87ef15cb--------------------------------)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/andreoniriccardo/articles/tree/main/NN-optimizer?source=post_page-----d16d87ef15cb--------------------------------)
- en: 'The resulting plot is visualized in this line chart:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图表在这条折线图中可视化：
- en: '![](../Images/dd630ab1d73df29ca08fc4dd809994a4.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd630ab1d73df29ca08fc4dd809994a4.png)'
- en: Image by the author.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: We can immediately see how the Momentum Gradient Descent (yellow line) is considerably
    faster than the standard Gradient Descent (blue line). Instead, RMS Prop (green
    line) seems to obtain similar results as Momentum Gradient Descent. This may be
    due to several reasons such as not perfectly tuned hyperparameters or a too-simple
    neural network. Finally, Adam optimizer (red line) looks superior, by a margin,
    to all the other methods.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以立即看到动量梯度下降（黄色线）比标准梯度下降（蓝色线）快得多。相反，RMS Prop（绿色线）似乎获得了与动量梯度下降相似的结果。这可能是由于几个原因，如超参数调节不完全或神经网络过于简单。最后，Adam优化器（红色线）在所有其他方法中明显优越。
- en: Learning Rate Decay
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习率衰减
- en: I want to include learning rate decay in this post because, even though it is
    not strictly an optimization algorithm, it is a powerful technique to **accelerate
    the network’s learning process**.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望在这篇文章中包含学习率衰减，因为尽管它不严格算作优化算法，但它是一种**加速网络学习过程**的强大技术。
- en: '![](../Images/aef64cc8a1598dbc30577bad03f3e05b.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aef64cc8a1598dbc30577bad03f3e05b.png)'
- en: 'Image source: [unsplash.com](https://unsplash.com/photos/OyCl7Y4y0Bk).'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[unsplash.com](https://unsplash.com/photos/OyCl7Y4y0Bk)。
- en: Learning rate decay consists of the **reduction of the learning rate** hyperparameter
    *alpha* over the epochs. Theis adjustment is essential because at the initial
    stages of the optimization, the algorithm can afford to take bigger steps but,
    as it approaches the minimum point, we prefer to make smaller steps so that it
    will bounce in an area closer to the minimum.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率衰减指的是**学习率**超参数*alpha*在训练周期中的**减少**。这种调整是必要的，因为在优化的初始阶段，算法可以接受更大的步伐，但当它接近最小点时，我们更愿意采取较小的步伐，以便它能在更接近最小点的区域内跳跃。
- en: 'There exist several methods to decrease the learning rate over iterations.
    One approach is described by the following formula:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以在迭代中减少学习率。一种方法由以下公式描述：
- en: '![](../Images/cc1823be5f8dbec5cdcf155298ad11ed.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cc1823be5f8dbec5cdcf155298ad11ed.png)'
- en: where the decay rate is an additional parameter to tune.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，衰减率是一个额外的调节参数。
- en: 'Alternatively, other possibilities are:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种可能性是：
- en: '![](../Images/c21313ad29ebd06c945998b69722069f.png)![](../Images/ec8ea285de373dc3220d44e361a23551.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c21313ad29ebd06c945998b69722069f.png)![](../Images/ec8ea285de373dc3220d44e361a23551.png)'
- en: The first one is called Exponential Learning Rate Decay, and in the second one,
    parameter *k* is a constant.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个叫做指数学习率衰减，而第二个中，参数*k*是一个常量。
- en: Finally, it is also possible to apply a discrete learning rate decay, such as
    halving it after every *t* iterations, or decreasing it manually.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，也可以应用离散学习率衰减，例如在每*t*次迭代后将其减半，或手动减少它。
- en: '![](../Images/c2ad4436b2a23cca5d7f8e308e0a1c17.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c2ad4436b2a23cca5d7f8e308e0a1c17.png)'
- en: Image by the author.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: Conclusion
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Time is a valuable, but also limited, resource for every data science practitioner.
    For this reason, mastering the tools to speed up a learning algorithm training
    can make a difference.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 时间对每个数据科学从业者来说都是宝贵而有限的资源。因此，掌握加速学习算法训练的工具可以带来差异。
- en: In this article, we have seen how the standard Gradient Descent optimizer is
    an outdated tool, and there exist several alternatives that provide better solutions
    in less time.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们已经看到标准的梯度下降优化器是过时的工具，并且存在一些替代方法，它们在更短的时间内提供更好的解决方案。
- en: Choosing the right optimizer for a given Machine Learning application is not
    always easy. It depends on the task and there is not a clear consensus yet on
    which one is the best. However, as we saw above, Adam optimizer with the proposed
    hyperparameters’ values is a valid choice most of the time, and knowing the functioning
    principle of the most popular ones is an excellent starting point.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 选择适合特定机器学习应用的优化器并不总是容易的。这取决于任务，而且尚未有明确的共识关于哪一种优化器最好。然而，正如我们上面所看到的，Adam优化器在大多数情况下是一个有效的选择，了解最受欢迎的优化器的工作原理是一个很好的起点。
- en: If you liked this story, consider following me to be notified of my upcoming
    projects and articles!
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这个故事，请考虑关注我，以便接收到我即将发布的项目和文章的通知！
- en: 'Here are some of my past projects:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我过去的一些项目：
- en: '[](/social-network-analysis-with-networkx-a-gentle-introduction-6123eddced3?source=post_page-----d16d87ef15cb--------------------------------)
    [## Social Network Analysis with NetworkX: A Gentle Introduction'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[社交网络分析与NetworkX：温和的介绍](https://towardsdatascience.com/social-network-analysis-with-networkx-a-gentle-introduction-6123eddced3?source=post_page-----d16d87ef15cb--------------------------------)'
- en: Learn how companies like Facebook and LinkedIn extract insights from networks
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解像Facebook和LinkedIn这样的公司如何从网络中提取洞察。
- en: 'towardsdatascience.com](/social-network-analysis-with-networkx-a-gentle-introduction-6123eddced3?source=post_page-----d16d87ef15cb--------------------------------)
    [](/use-deep-learning-to-generate-fantasy-character-names-build-a-language-model-from-scratch-792b13629efa?source=post_page-----d16d87ef15cb--------------------------------)
    [## Use Deep Learning to Generate Fantasy Names: Build a Language Model from Scratch'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[深度学习生成幻想名字：从零构建语言模型](https://towardsdatascience.com/social-network-analysis-with-networkx-a-gentle-introduction-6123eddced3?source=post_page-----d16d87ef15cb--------------------------------) '
- en: Can a language model invent unique fantasy character names? Let’s build it from
    scratch
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语言模型能否发明独特的幻想角色名字？让我们从零开始构建它。
- en: 'towardsdatascience.com](/use-deep-learning-to-generate-fantasy-character-names-build-a-language-model-from-scratch-792b13629efa?source=post_page-----d16d87ef15cb--------------------------------)
    [](/support-vector-machine-with-scikit-learn-a-friendly-introduction-a2969f2ff00d?source=post_page-----d16d87ef15cb--------------------------------)
    [## Support Vector Machine with Scikit-Learn: A Friendly Introduction'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[使用深度学习生成幻想角色名称：从头构建语言模型](https://towardsdatascience.com/use-deep-learning-to-generate-fantasy-character-names-build-a-language-model-from-scratch-792b13629efa?source=post_page-----d16d87ef15cb--------------------------------)
    [](/support-vector-machine-with-scikit-learn-a-friendly-introduction-a2969f2ff00d?source=post_page-----d16d87ef15cb--------------------------------)
    [## 使用 Scikit-Learn 的支持向量机：友好的介绍'
- en: Every data scientist should have SVM in their toolbox. Learn how to master this
    versatile model with a hands-on…
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每个数据科学家都应该在工具箱中拥有 SVM。了解如何通过实践掌握这一多用途模型…
- en: towardsdatascience.com](/support-vector-machine-with-scikit-learn-a-friendly-introduction-a2969f2ff00d?source=post_page-----d16d87ef15cb--------------------------------)
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[支持向量机与 Scikit-Learn：友好的介绍](https://towardsdatascience.com/support-vector-machine-with-scikit-learn-a-friendly-introduction-a2969f2ff00d?source=post_page-----d16d87ef15cb--------------------------------)'
- en: References
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[Gradient descent — Wikipedia.org](https://en.wikipedia.org/wiki/Gradient_descent)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[梯度下降 — Wikipedia.org](https://en.wikipedia.org/wiki/Gradient_descent)'
- en: '[Dive into Deep Learning — Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander
    J. Smola](https://d2l.ai/)'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深入学习 — Aston Zhang、Zachary C. Lipton、Mu Li 和 Alexander J. Smola](https://d2l.ai/)'
- en: '[Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet
    Classification — Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun](https://arxiv.org/abs/1502.01852v1)'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深入了解整流器：超越 ImageNet 分类的人类水平表现 — Kaiming He、Xiangyu Zhang、Shaoqing Ren、Jian
    Sun](https://arxiv.org/abs/1502.01852v1)'
- en: '[Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and
    Optimization — Andrew Ng](https://www.coursera.org/learn/deep-neural-network)'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[改善深度神经网络：超参数调整、正则化和优化 — Andrew Ng](https://www.coursera.org/learn/deep-neural-network)'
- en: '[Understanding the difficulty of training deep feedforward neural networks
    — Xavier Glorot, Yoshua Bengio](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[理解深度前馈神经网络训练的难度 — Xavier Glorot、Yoshua Bengio](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)'
- en: '[Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition
    — Aurélien Géron](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[动手机器学习：使用 Scikit-Learn、Keras 和 TensorFlow（第2版） — Aurélien Géron](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)'
- en: '[Keras Python library](https://keras.io/)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Keras Python 库](https://keras.io/)'
