- en: Public Benchmarks for Medical Natural Language Processing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 医学自然语言处理的公共基准
- en: 原文：[https://towardsdatascience.com/public-benchmarks-for-medical-natural-language-processing-c7c794ab4d9](https://towardsdatascience.com/public-benchmarks-for-medical-natural-language-processing-c7c794ab4d9)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/public-benchmarks-for-medical-natural-language-processing-c7c794ab4d9](https://towardsdatascience.com/public-benchmarks-for-medical-natural-language-processing-c7c794ab4d9)
- en: A general introduction to a list of canonical tasks and corresponding datasets
    to measure your medical natural language processing
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对于测量医学自然语言处理的标准任务和相应数据集的一般介绍
- en: '[](https://eileen-code4fun.medium.com/?source=post_page-----c7c794ab4d9--------------------------------)[![Eileen
    Pangu](../Images/cbdab572af709b6e6b52cb3a078f220d.png)](https://eileen-code4fun.medium.com/?source=post_page-----c7c794ab4d9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c7c794ab4d9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c7c794ab4d9--------------------------------)
    [Eileen Pangu](https://eileen-code4fun.medium.com/?source=post_page-----c7c794ab4d9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://eileen-code4fun.medium.com/?source=post_page-----c7c794ab4d9--------------------------------)[![Eileen
    Pangu](../Images/cbdab572af709b6e6b52cb3a078f220d.png)](https://eileen-code4fun.medium.com/?source=post_page-----c7c794ab4d9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c7c794ab4d9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c7c794ab4d9--------------------------------)
    [Eileen Pangu](https://eileen-code4fun.medium.com/?source=post_page-----c7c794ab4d9--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c7c794ab4d9--------------------------------)
    ·6 min read·Mar 29, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c7c794ab4d9--------------------------------)
    ·阅读时间6分钟·2023年3月29日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/e7f7329d67dd349b27bbe3c0f57628f7.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e7f7329d67dd349b27bbe3c0f57628f7.png)'
- en: 'Source: [https://unsplash.com/](https://unsplash.com/)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[https://unsplash.com/](https://unsplash.com/)
- en: The field of natural language processing (NLP) has evolved really fast in recent
    years. Breakthroughs like transformer, BERT, GPT have emerged one after another.
    Practitioners of all industries are exploring how to leverage the exciting development
    of NLP in their specific business domains and workflows [1]. One such industry
    that stands to benefit greatly from the improvement of NLP is healthcare. The
    vast amount of free text medical notes carry incredible data insights, which can
    inform better care provision, cost optimization, and healthcare innovation. To
    measure the efficacy of applying NLP to the medical field, we need good benchmarks.
    This blog post lists the canonical public benchmarks for the common tasks in medical
    natural language processing. The goal is to provide a starting point for healthcare
    machine learning practitioners to measure their NLP endeavours.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）领域近年来发展迅猛。变换器、BERT、GPT等突破性技术相继出现。各行各业的从业者都在探索如何在其特定业务领域和工作流中利用NLP的激动人心的发展[1]。医疗行业就是一个从NLP进步中受益匪浅的行业。大量的自由文本医学笔记包含了难以置信的数据见解，这些见解可以帮助提供更好的护理、优化成本和推动医疗创新。为了衡量将NLP应用于医疗领域的效果，我们需要良好的基准。本文列出了医学自然语言处理常见任务的标准公共基准。目标是为医疗机器学习从业者提供一个起点，以衡量他们的NLP工作。
- en: '**Entity/Relation Recognition**'
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**实体/关系识别**'
- en: The task of entity/relation recognition is to detect and categorize the medical
    concepts in free text and their relations. It is a crucial step in gaining better
    understanding of actionable insights from clinical notes and reports. The canonical
    dataset for this is Informatics for Integrating Biology and the Bedside (i2b2)
    [2]. The dataset contains de-identified patient reports from a few partnered medical
    organizations with 394 training reports, 477 test reports. The labeled medical
    concepts are of type `problems`, `treatments`, and `tests`. The labeled relations
    include things like `treatment improves problem`, `test reveals problem`, `problem
    indicates another problem`, and so on.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 实体/关系识别的任务是检测和分类自由文本中的医学概念及其关系。这是从临床笔记和报告中获取可操作见解的关键步骤。这个任务的标准数据集是生物学与临床实践整合信息数据集（i2b2）[2]。该数据集包含来自几个合作医疗组织的去标识化患者报告，共有394份训练报告和477份测试报告。标记的医学概念包括`问题`、`治疗`和`测试`。标记的关系包括`治疗改善问题`、`测试揭示问题`、`问题指示另一个问题`等。
- en: 'Here is a concrete example:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个具体的例子：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Only a full recognition is considered correct. That means for an entity, both
    the start and end word indices of the entity need to be accurate; and for a relation,
    the left entity, the right entity, and the relation all need to be accurate. The
    final evaluation metrics are based on precision, recall, and F1 score.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 只有完全正确的识别才被认为是正确的。这意味着对于一个实体，实体的开始和结束词索引都需要准确；对于一个关系，左实体、右实体和关系都需要准确。最终评估指标基于精确度、召回率和
    F1 分数。
- en: '**Semantics Similarity**'
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**语义相似性**'
- en: 'Semantics similarity evaluates the semantic equivalence between two snippets
    of medical text. Clinical Semantic Textual Similarity (ClinicalSTS) [3] is a canonical
    dataset for this task. It contains 1642 training and 412 test de-identified sentence
    pairs. The equivalence is measured by an ordinal scale of 0 to 5, with 0 indicating
    complete dissimilarity and 5 suggesting complete semantic equivalence. The final
    performance is measured by the Pearson correlation between the predicted similarity
    scores `Y’` and human judgement `Y`, and is calculated by the formula below (the
    higher the result, the better):'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 语义相似性评估两段医疗文本之间的语义等价性。临床语义文本相似性 (ClinicalSTS) [3] 是此任务的标准数据集。它包含 1642 个训练句子对和
    412 个测试句子对。等价性通过 0 到 5 的有序尺度来衡量，0 表示完全不相似，5 表示完全语义等价。最终表现通过预测相似性分数 `Y’` 和人工判断
    `Y` 之间的 Pearson 相关性来衡量，计算公式如下（结果越高越好）：
- en: '![](../Images/f14ab84df247d893a0c5efddd14ac295.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f14ab84df247d893a0c5efddd14ac295.png)'
- en: Pearson Correlation Formula
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Pearson 相关公式
- en: 'Here are two concrete examples:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两个具体的例子：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Natural language inference**'
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**自然语言推断**'
- en: Natural language inference evaluates how well a medical hypothesis can be derived
    from a medical premise. MedNLI [4] is such a dataset. It contains de-identified
    medical history notes from a group of deceased patients. The notes are segmented
    into snippets, and human experts were asked to write 3 hypotheses based on each
    snippet. The 3 hypotheses are
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言推断评估医疗假设如何从医疗前提中推导出来。MedNLI [4] 是这样一个数据集。它包含来自一组已故患者的去标识化医疗历史记录。这些记录被分割成片段，要求专家根据每个片段编写
    3 个假设。这 3 个假设是
- en: a clearly true description
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个明显正确的描述
- en: a clearly false description and
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个明显错误的描述和
- en: a description might be true or false,
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个描述可能是真的或假的，
- en: 'representing 3 relations of the premise-hypothesis: `entailment`, `contradiction`,
    and `neural`. The dataset contains 11232 training pairs, 1395 development pairs,
    and 1422 test pairs.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 代表前提-假设的 3 种关系：`entailment`、`contradiction` 和 `neural`。数据集包含 11232 个训练对，1395
    个开发对和 1422 个测试对。
- en: 'Here is a concrete example:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个具体的例子：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The final performance can be measured by the classification accuracy of the
    relations given the premise-hypothesis pairs.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的表现可以通过给定前提-假设对的关系分类准确率来衡量。
- en: '**Medical question choice-answering**'
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**医疗问题选择回答**'
- en: Medical question choice-answering emulates the choice-answer medical exams.
    MedQA [5] is the canonical dataset for this purpose. Its questions are collected
    from medical board exams in the US and China where human doctors are evaluated
    by picking the right answer. It contains 61097 questions.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 医疗问题选择回答模拟了选择题形式的医疗考试。MedQA [5] 是这个目的的标准数据集。其问题来自于美国和中国的医学考试，通过选择正确答案来评估医生。数据集中包含
    61097 个问题。
- en: 'Here is a concrete example:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个具体的例子：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Mechanically, this task can be treated as a scoring system where the input is
    the `question+answer_i`, and the output is a numeric score. The `answer_i` with
    the highest score will be the final answer. The performance can be measured by
    accuracy on a 80/10/10 split of the dataset. This creates a comparable benchmark
    for the model and human expert performance.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 从机制上讲，这个任务可以被视为一个评分系统，其中输入是 `question+answer_i`，输出是一个数值分数。得分最高的 `answer_i` 将是最终答案。性能可以通过对数据集的
    80/10/10 划分的准确性来衡量。这为模型和人工专家的表现提供了一个可比的基准。
- en: '**Medical question answering**'
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**医疗问题回答**'
- en: Medical question answering is the most complex form of medical NLP task. It
    requires the model to generate long form free text answers to the given medical
    question. emrQA [6] is a canonical dataset for this purpose. It has 400k question-answer
    pairs. Such a dataset would be very expensive to acquire relying only on human
    experts’ manual efforts. Therefore, emrQA is actually semi-automatically generated
    by
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 医学问答是最复杂的医学NLP任务。它要求模型为给定的医学问题生成长篇自由文本答案。emrQA [6] 是这个目的的典型数据集。它有40万对问答对。仅依靠人工专家的手工工作来获取这样一个数据集会非常昂贵。因此，emrQA
    实际上是半自动生成的
- en: first polling medical experts on the frequently asked questions,
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先对医学专家进行关于常见问题的调查，
- en: then replacing the medical concepts in those questions with placeholder and
    thus creating templates of questions,
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后用占位符替换这些问题中的医学概念，从而创建问题的模板，
- en: and finally using annotated entity-relation (such as i2b2) dataset to establish
    the clinical context, fill in the questions, and generate the answers.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终使用标注的实体-关系（如i2b2）数据集来建立临床背景、填写问题并生成答案。
- en: 'Here is a concrete example:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个具体的例子：
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Mechanically, this task can be seen as a language generation task where the
    input is the `context+question`, and the output is the `answer`. Final performance
    can typically be measured on a 80/20 split of the dataset, and by the exact match
    and F1 score. Exact match measures the percentage of prediction that matches the
    exact ground truth. F1 score measures the “overlap” between the prediction and
    ground truth. In this setting, both the prediction and the ground truth are treated
    as a bag of tokens where true/false positive/negative can be calculated.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 从机械的角度来看，这项任务可以视为一种语言生成任务，其中输入是 `context+question`，输出是 `answer`。最终的表现通常可以通过数据集的
    80/20 拆分来衡量，并通过准确匹配和 F1 分数来评估。准确匹配测量与准确真实值匹配的预测百分比。F1 分数测量预测与真实值之间的“重叠”。在这种设置下，预测和真实值都被视为一个令牌包，其中可以计算真正的/假阳性/假阴性。
- en: '**Conclusion**'
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**结论**'
- en: Researchers and practitioners continue to vigorously apply natural language
    processing (NLP) in the medical space. While it’s exciting to see the enthusiasm,
    it’s important to have public and reproducible benchmarks to measure the performance
    of such applications. This blog post lists the typical tasks, corresponding public
    datasets, and applicable metrics for this purpose, which can serve to quantify
    the potential improvement of new medical NLP applications.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员和从业者继续在医学领域积极应用自然语言处理（NLP）。虽然看到这种热情令人兴奋，但拥有公开且可重复的基准来衡量这些应用的表现非常重要。这篇博客文章列出了典型任务、相应的公开数据集以及适用的度量标准，这些可以用来量化新医学NLP应用的潜在改进。
- en: '**References**'
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: '[1] How to Use Large Language Models (LLM) in Your Own Domains [https://towardsdatascience.com/how-to-use-large-language-models-llm-in-your-own-domains-b4dff2d08464](/how-to-use-large-language-models-llm-in-your-own-domains-b4dff2d08464)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 如何在自己的领域中使用大型语言模型（LLM） [https://towardsdatascience.com/how-to-use-large-language-models-llm-in-your-own-domains-b4dff2d08464](/how-to-use-large-language-models-llm-in-your-own-domains-b4dff2d08464)'
- en: '[2] 2010 i2b2/VA challenge on concepts, assertions, and relations in clinical
    text [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3168320/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3168320/)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] 2010 i2b2/VA 挑战：临床文本中的概念、断言和关系 [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3168320/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3168320/)'
- en: '[3] The 2019 n2c2/OHNLP Track on Clinical Semantic Textual Similarity: Overview
    [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7732706/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7732706/)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] 2019 n2c2/OHNLP 临床语义文本相似性追踪：概述 [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7732706/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7732706/)'
- en: '[4] MedNLI — A Natural Language Inference Dataset For The Clinical Domain [https://physionet.org/content/mednli/1.0.0/](https://physionet.org/content/mednli/1.0.0/)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] MedNLI — 临床领域的自然语言推断数据集 [https://physionet.org/content/mednli/1.0.0/](https://physionet.org/content/mednli/1.0.0/)'
- en: '[5] What Disease does this Patient Have? A Large-scale Open Domain Question
    Answering Dataset from Medical Exams [https://arxiv.org/abs/2009.13081](https://arxiv.org/abs/2009.13081)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] 这个病人得了什么病？来自医学考试的大规模开放域问答数据集 [https://arxiv.org/abs/2009.13081](https://arxiv.org/abs/2009.13081)'
- en: '[6] emrQA: A Large Corpus for Question Answering on Electronic Medical Records
    [https://arxiv.org/abs/1809.00732](https://arxiv.org/abs/1809.00732)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] emrQA：用于电子病历问答的大型语料库 [https://arxiv.org/abs/1809.00732](https://arxiv.org/abs/1809.00732)'
