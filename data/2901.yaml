- en: A Foundation Model for Medical AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 医疗 AI 的基础模型
- en: 原文：[https://towardsdatascience.com/a-foundation-model-for-medical-ai-7b97e3ab3893?source=collection_archive---------2-----------------------#2023-09-19](https://towardsdatascience.com/a-foundation-model-for-medical-ai-7b97e3ab3893?source=collection_archive---------2-----------------------#2023-09-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-foundation-model-for-medical-ai-7b97e3ab3893?source=collection_archive---------2-----------------------#2023-09-19](https://towardsdatascience.com/a-foundation-model-for-medical-ai-7b97e3ab3893?source=collection_archive---------2-----------------------#2023-09-19)
- en: Introducing PLIP, a foundation model for pathology
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍 PLIP，一个病理学基础模型
- en: '[](https://fede-bianchi.medium.com/?source=post_page-----7b97e3ab3893--------------------------------)[![Federico
    Bianchi](../Images/fa38ff2051af04df7803af7d84c5cd4d.png)](https://fede-bianchi.medium.com/?source=post_page-----7b97e3ab3893--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7b97e3ab3893--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7b97e3ab3893--------------------------------)
    [Federico Bianchi](https://fede-bianchi.medium.com/?source=post_page-----7b97e3ab3893--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://fede-bianchi.medium.com/?source=post_page-----7b97e3ab3893--------------------------------)[![Federico
    Bianchi](../Images/fa38ff2051af04df7803af7d84c5cd4d.png)](https://fede-bianchi.medium.com/?source=post_page-----7b97e3ab3893--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7b97e3ab3893--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7b97e3ab3893--------------------------------)
    [Federico Bianchi](https://fede-bianchi.medium.com/?source=post_page-----7b97e3ab3893--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2aff872fe60e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-foundation-model-for-medical-ai-7b97e3ab3893&user=Federico+Bianchi&userId=2aff872fe60e&source=post_page-2aff872fe60e----7b97e3ab3893---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7b97e3ab3893--------------------------------)
    ·10 min read·Sep 19, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7b97e3ab3893&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-foundation-model-for-medical-ai-7b97e3ab3893&user=Federico+Bianchi&userId=2aff872fe60e&source=-----7b97e3ab3893---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2aff872fe60e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-foundation-model-for-medical-ai-7b97e3ab3893&user=Federico+Bianchi&userId=2aff872fe60e&source=post_page-2aff872fe60e----7b97e3ab3893---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7b97e3ab3893--------------------------------)
    ·10 分钟阅读·2023年9月19日'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7b97e3ab3893&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-foundation-model-for-medical-ai-7b97e3ab3893&source=-----7b97e3ab3893---------------------bookmark_footer-----------)![](../Images/c8ec0d1dd469532a43f24deab0ce8690.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7b97e3ab3893&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-foundation-model-for-medical-ai-7b97e3ab3893&source=-----7b97e3ab3893---------------------bookmark_footer-----------)![](../Images/c8ec0d1dd469532a43f24deab0ce8690.png)'
- en: 'Photo by Tara Winstead: [https://www.pexels.com/photo/person-reaching-out-to-a-robot-8386434/](https://www.pexels.com/photo/person-reaching-out-to-a-robot-8386434/)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '图片由 Tara Winstead 提供: [https://www.pexels.com/photo/person-reaching-out-to-a-robot-8386434/](https://www.pexels.com/photo/person-reaching-out-to-a-robot-8386434/)'
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: The ongoing AI revolution is bringing us innovations in all directions. OpenAI
    GPT(s) models are leading the development and showing how much foundation models
    can actually make some of our daily tasks easier. From helping us write better
    to streamlining some of our tasks, every day we see new models being announced.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 正在进行的 AI 革新带来了各个方面的创新。OpenAI 的 GPT 模型正在引领发展，并展示了基础模型如何实际上使我们的一些日常任务变得更轻松。从帮助我们写得更好到简化一些任务，我们每天都会看到新模型的发布。
- en: Many opportunities are opening up in front of us. AI products that can help
    us in our work life are going to be one of the most important tools we are going
    to get in the next years.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机会正在我们面前展开。能够帮助我们工作生活的 AI 产品将成为我们在未来几年中获得的最重要工具之一。
- en: Where are we going to see the most impactful changes? Where can we help people
    accomplish their tasks faster? One of the most exciting avenues for AI models
    is the one that brings us to Medical AI tools.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在哪里看到最具影响力的变化？我们可以在哪里帮助人们更快地完成任务？人工智能模型最令人兴奋的一个方向是将我们引向医疗AI工具。
- en: 'In this blog post, I describe [**PLIP**](https://github.com/PathologyFoundation/plip)
    (Pathology Language and Image Pre-Training) as one of the first foundation models
    for pathology. PLIP is a vision-language model that can be used to embed images
    and text in the same vector space, thus allowing multi-modal applications. PLIP
    is derived from the original [CLIP](https://openai.com/research/clip) model proposed
    by OpenAI in 2021 and has been recently published in Nature Medicine:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我将[**PLIP**](https://github.com/PathologyFoundation/plip)（病理语言和图像预训练）描述为病理学的第一个基础模型之一。PLIP是一个视觉-语言模型，可以用于将图像和文本嵌入到同一向量空间中，从而实现多模态应用。PLIP源自2021年OpenAI提出的原始[CLIP](https://openai.com/research/clip)模型，并已在《Nature
    Medicine》上发表：
- en: Huang, Z., Bianchi, F., Yuksekgonul, M., Montine, T., Zou, J., [**A visual–language
    foundation model for pathology image analysis using medical Twitter.**](https://rdcu.be/djLSK)2023,
    Nature Medicine.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Huang, Z., Bianchi, F., Yuksekgonul, M., Montine, T., Zou, J., [**一种用于病理图像分析的视觉-语言基础模型，通过医疗Twitter。**](https://rdcu.be/djLSK)2023,
    Nature Medicine.
- en: 'Some useful links before starting our adventure:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始我们的冒险之前，一些有用的链接：
- en: '[HuggingFace Weights](https://huggingface.co/vinid/plip);'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[HuggingFace Weights](https://huggingface.co/vinid/plip);'
- en: '[Nature Medicine Paper](https://rdcu.be/djLSK);'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Nature Medicine Paper](https://rdcu.be/djLSK);'
- en: '[GitHub Repository](https://github.com/PathologyFoundation/pathology-language-image-pretraining/).'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[GitHub Repository](https://github.com/PathologyFoundation/pathology-language-image-pretraining/).'
- en: All images, unless otherwise noted, are by the author.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图像均由作者提供。
- en: Contrastive Pre-Training 101
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对比预训练101
- en: We show that, through the use of data collection on social media and with some
    additional tricks, we can build a model that can be used in Medical AI pathology
    tasks with good results — and without the need for annotated data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了通过社交媒体上的数据收集以及一些额外的技巧，我们可以构建一个可以在医疗AI病理任务中取得良好结果的模型——而无需标注数据。
- en: While introducing CLIP (the model from which PLIP is derived) and its contrastive
    loss is a bit out of the scope of this blog post, it is still good to get a first
    intro/refresher. The very simple idea behind CLIP is that we can build a model
    that puts images and text in a vector space in which “images and their descriptions
    are going to be close together”.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然介绍CLIP（PLIP衍生的模型）及其对比损失超出了这篇博客文章的范围，但了解一下还是很有帮助。CLIP背后的非常简单的想法是，我们可以构建一个将图像和文本放入一个向量空间的模型，其中“图像及其描述将会彼此接近”。
- en: '![](../Images/3291eac40c88216823339f8f74b79690.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3291eac40c88216823339f8f74b79690.png)'
- en: A contrastive model — like PLIP/CLIP — puts images and text in the same vector
    space to be compared. The description in the yellow box matches the image in the
    yellow box and thus they are also close in the vector space.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对比模型——如PLIP/CLIP——将图像和文本置于同一向量空间进行比较。黄色框中的描述与黄色框中的图像匹配，因此它们在向量空间中也非常接近。
- en: 'The GIF above also shows an example of how a model that embeds images and text
    in the same vector space can be used for classification: by putting everything
    in the same vector space we can associate each image with one or more labels by
    considering the distance in the vector space: the closer the description is to
    the image, the better. We expect the closest label to be the real label of the
    image.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的GIF还展示了一个示例，说明了如何将图像和文本嵌入同一向量空间的模型用于分类：通过将所有内容置于同一向量空间，我们可以通过考虑向量空间中的距离，将每个图像与一个或多个标签关联起来：描述与图像越接近，效果越好。我们期望最接近的标签是图像的真实标签。
- en: 'To be clear: Once CLIP is trained you can embed **any** image or **any** text
    you have. Consider that this GIF shows a 2D space, but in general, the spaces
    used in CLIP are of much higher dimensionality.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 明确来说：一旦CLIP训练完成，你可以嵌入**任何**图像或**任何**文本。请注意，这个GIF展示的是二维空间，但一般而言，CLIP使用的空间维度要高得多。
- en: 'This means that once images and text are in the same vector spaces, there are
    many things we can do: from *zero-shot classification* (**find which text label
    is more similar to an image**) to *retrieval* (**find which image is more similar
    to a given description**).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，一旦图像和文本处于相同的向量空间中，我们可以做很多事情：从*零样本分类*（**找到哪个文本标签与图像更相似**）到*检索*（**找到哪个图像与给定描述更相似**）。
- en: How do we train CLIP? To put it simply, the model is fed with **MANY** image-text
    pairs and tries to put similar matching items close together (as in the image
    above) and all the rest far away. The more image-text pairs you have, the better
    the representation you are going to learn.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何训练CLIP？简单来说，模型会接收**大量**的图像-文本对，并尝试将相似的匹配项放在一起（如上图所示），而将其他项远离。图像-文本对越多，你学到的表示就越好。
- en: We will stop here with the CLIP background, this should be enough to understand
    the rest of this post. I have a more in-depth blog post about CLIP on Towards
    Data Science.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里结束对CLIP背景的介绍，这应该足以理解本文的其余部分。我在Towards Data Science上有一篇关于CLIP的更深入的博客文章。
- en: '[](/how-to-train-your-clip-45a451dcd303?source=post_page-----7b97e3ab3893--------------------------------)
    [## How to Train your CLIP'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/how-to-train-your-clip-45a451dcd303?source=post_page-----7b97e3ab3893--------------------------------)
    [## 如何训练你的CLIP'
- en: Introduction to CLIP and to how we fine-tuned it for the Italian Language during
    the HuggingFace Community Week.
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 介绍CLIP及其在HuggingFace社区周期间如何为意大利语进行微调。
- en: towardsdatascience.com](/how-to-train-your-clip-45a451dcd303?source=post_page-----7b97e3ab3893--------------------------------)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/how-to-train-your-clip-45a451dcd303?source=post_page-----7b97e3ab3893--------------------------------)
- en: CLIP has been trained to be a very general image-text model, but it does not
    work as well for specific use cases (e.g., [Fashion](https://www.nature.com/articles/s41598-022-23052-9)
    (Chia et al., 2022)) and there are also cases in which CLIP underperforms and
    domain-specific implementations perform better (Zhang et al., 2023).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP已被训练成为一个非常通用的图像-文本模型，但对于特定的使用案例（例如，[时尚](https://www.nature.com/articles/s41598-022-23052-9)（Chia等，2022年））效果不佳，而且在某些情况下，CLIP表现不佳，而领域特定的实现效果更好（Zhang等，2023年）。
- en: Pathology Language and Image Pre-Training (PLIP)
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 病理学语言和图像预训练（PLIP）
- en: We now describe how we built PLIP, our fine-tuned version of the original CLIP
    model that is specifically designed for Pathology.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在描述如何构建PLIP，这是我们对原始CLIP模型进行微调的版本，专门针对病理学设计。
- en: Building a Dataset for Pathology Language and Image Pre-Training
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为病理学语言和图像预训练构建数据集
- en: We need data, and this data has to be good enough to be used to train a model.
    **The question is how do we find these data?** What we need is images with relevant
    descriptions — like the one we saw in the GIF above.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要数据，而这些数据必须足够好，以用于训练模型。**问题是我们如何找到这些数据？** 我们需要的是具有相关描述的图像——就像我们在上面的GIF中看到的那样。
- en: Although there is a significant amount of pathology data available on the web,
    it is often lacking annotations and it may be in non-standard formats such as
    PDF files, slides, or YouTube videos.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管网络上有大量的病理数据，但这些数据通常缺乏注释，并且可能以非标准格式存在，如PDF文件、幻灯片或YouTube视频。
- en: We need to look somewhere else, and this somewhere else is going to be social
    media. By leveraging social media platforms, we can potentially access a wealth
    of pathology-related content. Pathologists use social media to share their own
    research online and to ask questions to their fellow colleagues (see Isom et al.,
    2017, for a discussion on how pathologists use social media). There is also a
    [set](https://www.symplur.com/healthcare-hashtags/ontology/pathology/) of generally
    recommended **Twitter hashtags** that pathologists can use to communicate.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要换个地方寻找，而这个地方就是社交媒体。通过利用社交媒体平台，我们有可能接触到大量与病理学相关的内容。病理学家使用社交媒体在线分享自己的研究，并向同事提问（请参见Isom等，2017年，讨论了病理学家如何使用社交媒体）。此外，还有一组一般推荐的**Twitter标签**，病理学家可以用来进行沟通。[点击这里](https://www.symplur.com/healthcare-hashtags/ontology/pathology/)
    查看这些标签。
- en: In addition to Twitter data, we also collect a subset of images from the **LAION
    dataset** (Schuhmann et al., 2022), a vast collection of 5B image-text pairs.
    LAION has been collected by scraping the web and it is the dataset that was used
    to train many of the popular OpenCLIP models.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 除了Twitter数据外，我们还收集了**LAION数据集**（Schuhmann等，2022年）中的一个子集，LAION是一个包含50亿图像-文本对的大型数据集。LAION通过爬取网络收集而来，也是许多流行的OpenCLIP模型训练所使用的数据集。
- en: '**Pathology Twitter**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**病理学Twitter**'
- en: We collect more than 100K tweets using pathology Twitter hashtags. The process
    is rather simple, we use the API to collect tweets that relate to a set of specific
    hashtags. We remove tweets that contain a question mark because these tweets often
    contain requests for other pathologies (e.g., “Which kind of tumor is this?”)
    and not information we might actually need to build our model.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用病理学Twitter标签收集了超过10万条推文。过程相当简单，我们使用API收集与特定标签相关的推文。我们去除包含问号的推文，因为这些推文通常包含对其他病理（例如，“这是什么肿瘤？”）的请求，而不是我们实际需要的信息来构建模型。
- en: '![](../Images/6fa4d4137808c45ca72028b3096e3bba.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6fa4d4137808c45ca72028b3096e3bba.png)'
- en: We extract tweets with specific keywords and we remove sensitive content. In
    addition to this, we also remove all the tweets that contain question marks, which
    appear in tweets used by pathologists to ask questions to their colleagues about
    some possible rare cases.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提取包含特定关键词的推文，并去除敏感内容。此外，我们还去除了所有包含问号的推文，这些推文通常是病理学家向同事询问一些可能罕见的病例的提问。
- en: '**Sampling from LAION**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**从LAION采样**'
- en: 'LAION contains 5B image-text pairs, and our plan to collect our data is going
    to be as follows: we can use our own images that come from Twitter and find similar
    images in this large corpus; in this way, we should be able to get reasonably
    similar images and hopefully, these similar images are also pathology images.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: LAION包含50亿图像-文本对，我们收集数据的计划如下：我们可以使用来自Twitter的图像，并在这个大型语料库中查找相似的图像；这样，我们应该能够获得相当相似的图像，并且这些相似的图像也可能是病理图像。
- en: Now, doing this manually would be infeasible, embedding and searching over 5B
    embeddings is a very time-consuming task. Luckily there are pre-computed vector
    indexes for LAION that we can query with actual images using APIs! We thus simply
    embed our images and use K-NN search to find similar images in LAION. Remember,
    each of these images comes with a caption, something that is perfect for our use
    case.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，手动进行这些操作是不可行的，嵌入和搜索50亿个嵌入是一个非常耗时的任务。幸运的是，LAION有预计算的向量索引，我们可以通过API用实际图像查询！因此，我们简单地嵌入我们的图像并使用K-NN搜索在LAION中查找相似图像。请记住，这些图像每个都有一个标题，非常适合我们的用例。
- en: '![](../Images/e1ad8d3c652c24965be1ac321ad7e7bb.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e1ad8d3c652c24965be1ac321ad7e7bb.png)'
- en: Very simple setup of how we extend our dataset by using K-NN search on the LAION
    dataset. We start with our own image from our original corpus and then search
    for similar images on the LAION dataset. Each of the images we get comes with
    an actual caption.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过在LAION数据集上使用K-NN搜索扩展数据集的设置非常简单。我们从原始语料库中的图像开始，然后在LAION数据集中搜索相似的图像。我们得到的每张图像都有一个实际标题。
- en: '**Ensuring Data Quality**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**确保数据质量**'
- en: Not all the images we collect are good. For example, from Twitter, we collected
    lots of group photos from Medical conferences. From LAION, we sometimes got some
    fractal-like images that could vaguely resemble some pathology pattern.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有我们收集的图像都是好的。例如，我们从Twitter收集了大量医疗会议的合影。从LAION，我们有时会得到一些类似分形的图像，这些图像可能模糊地类似于某些病理模式。
- en: 'What we did was very simple: we trained a classifier by using some pathology
    data as positive class data and ImageNet data as negative class data. This kind
    of classifier has an incredibly high precision (it’s actually easy to distinguish
    pathology images from random images on the web).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们做的事情非常简单：我们使用一些病理数据作为正类数据，用ImageNet数据作为负类数据来训练分类器。这种分类器具有极高的精度（实际上，区分病理图像和网络上的随机图像很容易）。
- en: In addition to this, for LAION data we apply an English language classifier
    to remove examples that are not in English.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于LAION数据，我们应用了英语语言分类器来去除非英语的示例。
- en: Training Pathology Language and Image Pre-Training
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练病理语言和图像预训练
- en: Data collection was the hardest part. Once that is done and we trust our data,
    we can start training.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 数据收集是最困难的部分。一旦完成且我们信任我们的数据，就可以开始训练。
- en: To train PLIP we used the original OpenAI code to do training — we implemented
    the training loop, added a cosine annealing for the loss, and a couple of tweaks
    here and there to make everything ran smoothly and in a verifiable way (e.g. [Comet
    ML](https://www.comet.com/) tracking).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练PLIP，我们使用了原始的OpenAI代码进行训练——我们实现了训练循环，添加了损失的余弦退火，并做了一些调整，以确保一切顺利进行并且可验证（例如，[Comet
    ML](https://www.comet.com/) 跟踪）。
- en: We trained many different models (hundreds) and compared parameters and optimization
    techniques, Eventually, we were able to come up with a model we were pleased with.
    There are more details in the paper, but one of the most important components
    when building this kind of contrastive model is making sure that the batch size
    is as large as possible during training, this allows the model to learn to distinguish
    as many elements as possible.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练了许多不同的模型（数百个），并比较了参数和优化技术。最终，我们得出了一个令人满意的模型。详细信息请参见论文，但在构建这种对比模型时，最重要的组成部分之一是确保在训练过程中批量大小尽可能大，这可以使模型学会区分尽可能多的元素。
- en: Pathology Language and Image Pre-Training for Medical AI
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 医学 AI 的病理语言和图像预训练
- en: It is now time to put our PLIP to the test. Is this foundation model good on
    standard benchmarks?
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是测试我们的 PLIP 模型的时候了。这个基础模型在标准基准测试上表现如何？
- en: We run different tests to evaluate the performance of our PLIP model. The three
    most interesting ones are zero-shot classification, linear probing, and retrieval,
    but I’ll mainly focus on the first two here. I’ll ignore experimental configuration
    for the sake of brevity, but these are all available in the manuscript.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了不同的测试来评估 PLIP 模型的性能。其中最有趣的三个是零样本分类、线性探测和检索，但我主要关注前两个。在这里为了简洁，我将忽略实验配置，但这些都可以在手稿中找到。
- en: '**PLIP as a Zero-Shot Classifier**'
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**PLIP 作为零样本分类器**'
- en: The GIF below illustrates how to do zero-shot classification with a model like
    PLIP. We use the dot product as a measure of similarity in the vector space (the
    higher, the more similar).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的 GIF 演示了如何使用类似 PLIP 的模型进行零样本分类。我们使用点积作为向量空间中的相似性度量（点积越高，相似度越高）。
- en: '![](../Images/fc2a79c5e939d3e789458d61ed11663f.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc2a79c5e939d3e789458d61ed11663f.png)'
- en: The process to do zero-shot classification. We embed an image and all the labels
    and find which labels are closer to the image in the vector space.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 进行零样本分类的过程。我们将图像和所有标签进行嵌入，然后在向量空间中找出与图像最接近的标签。
- en: In the following plot, you can see a quick comparison of PLIP vs CLIP on one
    of the dataset we used for zero-shot classification. There is a significant gain
    in terms of performance when using PLIP to replace CLIP.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，你可以看到 PLIP 与 CLIP 在我们用于零样本分类的一个数据集上的快速比较。使用 PLIP 替代 CLIP 能显著提升性能。
- en: '![](../Images/c595c8c56829e0fbef4c21836279d2b4.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c595c8c56829e0fbef4c21836279d2b4.png)'
- en: PLIP vs CLIP performance (Weighted Macro F1) on two datasets for zero-shot classification.
    Note that y-axis stops at around 0.6 and not 1.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: PLIP 与 CLIP 在两个数据集上的零样本分类性能（加权宏 F1）。注意 y 轴在约 0.6 处停止，而不是 1。
- en: '**PLIP as a Feature Extractor for Linear Probing**'
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**PLIP 作为线性探测的特征提取器**'
- en: Another way to use PLIP is as a feature extractor for pathology images. During
    training, PLIP sees many pathology images and learns to build vector embeddings
    for them.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 PLIP 的另一种方法是作为病理图像的特征提取器。在训练过程中，PLIP 处理了许多病理图像，并学习为这些图像构建向量嵌入。
- en: Let’s say you have some annotated data and you want to train a new pathology
    classifier. You can extract image embeddings with PLIP and then train a logistic
    regression (or any kind of regressor you like) on top of these embeddings. This
    is an easy and effective way to perform a classification task.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一些标注数据，想要训练一个新的病理分类器。你可以使用 PLIP 提取图像嵌入，然后在这些嵌入上训练一个逻辑回归（或你喜欢的任何回归器）。这是一种简单有效的分类任务方法。
- en: Why does this work? The idea is that to train a classifier PLIP embeddings,
    being pathology-specific, should be better than CLIP embeddings, which are general
    purpose.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这样有效？这个想法是，PLIP 嵌入具有病理特异性，应该比 CLIP 嵌入（通用目的）更好。
- en: '![](../Images/fde1892750497b46a5c647b4034eed2e.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fde1892750497b46a5c647b4034eed2e.png)'
- en: PLIP Image Encoder allows us to extract a vector for each image and train an
    image classifier on top of it.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: PLIP 图像编码器允许我们为每个图像提取一个向量，并在其上训练一个图像分类器。
- en: Here is an example of the comparison between the performance of CLIP and PLIP
    on two datasets. While CLIP gets good performance, the results we get using PLIP
    are much higher.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是 CLIP 和 PLIP 在两个数据集上的性能比较示例。虽然 CLIP 的表现不错，但我们使用 PLIP 得到的结果要高得多。
- en: '![](../Images/8b2401d70041e726c5e708a3596f37fa.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8b2401d70041e726c5e708a3596f37fa.png)'
- en: PLIP vs CLIP performance (Macro F1) on two datasets for linear probing. Note
    that y-axis starts from 0.65 and not 0.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: PLIP 与 CLIP 在两个数据集上进行线性探测的表现（宏 F1）。注意 y 轴从 0.65 开始，而不是 0。
- en: Using Pathology Language and Image Pre-Training
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用病理语言和图像预训练
- en: How to use PLIP? here are some examples of how to use PLIP in Python and a Streamlit
    demo you can use to play a bit with the mode.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如何使用PLIP？以下是一些使用PLIP的Python示例，以及一个你可以用来稍微玩一下模型的Streamlit演示。
- en: 'Code: APIs to Use PLIP'
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代码：使用PLIP的API
- en: 'Our GitHub repository offers a couple of additional examples you can follow.
    We have built an API that allows you to interact with the model easily:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的GitHub仓库提供了一些额外的示例，你可以参考。我们已经构建了一个API，允许你轻松地与模型进行交互：
- en: '[PRE0]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can also use the more standard HF API to load and use the model:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用更标准的HF API来加载和使用模型：
- en: '[PRE1]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Demo: PLIP as an Educational Tool'
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 演示：PLIP作为教育工具
- en: 'We also believe PLIP and future models can be effectively used as educational
    tools for Medical AI. PLIP allows users to do zero-shot retrieval: a user can
    search for specific keywords and PLIP will try to find the most similar/matching
    image. We built a simple web app in Streamlit that you can find [here](https://huggingface.co/spaces/vinid/webplip).'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还相信PLIP和未来的模型可以作为医学AI的有效教育工具。PLIP允许用户进行零样本检索：用户可以搜索特定的关键词，PLIP将尝试找到最相似/匹配的图像。我们在Streamlit中构建了一个简单的Web应用，你可以在[这里](https://huggingface.co/spaces/vinid/webplip)找到它。
- en: Conclusions
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Thanks for reading all of this! We are excited about the possible future evolutions
    of this technology.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读这些内容！我们对这项技术未来可能的发展感到兴奋。
- en: I will close this blog post by discussing some very important limitations of
    PLIP and by suggesting some additional things I have written that might be of
    interest.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我将通过讨论PLIP的一些非常重要的局限性以及建议一些可能感兴趣的附加内容来结束这篇博客文章。
- en: Limitations
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局限性
- en: While our results are interesting, PLIP comes with lots of different limitations.
    Data is not enough to learn all the complex aspects of pathology. We have built
    data filters to ensure data quality, but we need better evaluation metrics to
    understand what the model is getting right and what the model is getting wrong.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的结果很有趣，但PLIP存在许多不同的局限性。数据不足以学习病理学所有复杂的方面。我们已经构建了数据过滤器以确保数据质量，但我们需要更好的评估指标来理解模型的正确与错误。
- en: More importantly, PLIP does not solve the current challenges of pathology; PLIP
    is not a perfect tool and can make many errors that require investigation. The
    results we see are definitely promising and they open up a range of possibilities
    for future models in pathology that combine vision and language. However, there
    is still lots of work to do before we can see these tools used in everyday medicine.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是，PLIP并没有解决病理学当前的挑战；PLIP不是一个完美的工具，可能会犯很多需要调查的错误。我们看到的结果无疑是有前景的，它们为未来在病理学中结合视觉和语言的模型打开了许多可能性。然而，在我们能看到这些工具在日常医学中应用之前，还有很多工作要做。
- en: Miscellanea
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 杂项
- en: 'I have a couple of other blog posts regarding CLIP modeling and CLIP limitations.
    For example:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我还有一些关于CLIP建模和CLIP局限性的博客文章。例如：
- en: '[](/teaching-clip-some-fashion-3005ac3fdcc3?source=post_page-----7b97e3ab3893--------------------------------)
    [## Teaching CLIP Some Fashion'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/teaching-clip-some-fashion-3005ac3fdcc3?source=post_page-----7b97e3ab3893--------------------------------)
    [## 教授CLIP一些时尚知识'
- en: Training FashionCLIP, a domain-specific CLIP model for Fashion
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练FashionCLIP，一个特定领域的CLIP模型用于时尚
- en: towardsdatascience.com](/teaching-clip-some-fashion-3005ac3fdcc3?source=post_page-----7b97e3ab3893--------------------------------)
    [](/your-vision-language-model-might-be-a-bag-of-words-30b1beaef7f8?source=post_page-----7b97e3ab3893--------------------------------)
    [## Your Vision-Language Model Might Be a Bag of Words
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/teaching-clip-some-fashion-3005ac3fdcc3?source=post_page-----7b97e3ab3893--------------------------------)
    [](/your-vision-language-model-might-be-a-bag-of-words-30b1beaef7f8?source=post_page-----7b97e3ab3893--------------------------------)
    [## 你的视觉-语言模型可能是一个词袋
- en: We explore the limits of what vision-language models get about language in our
    Oral Paper at ICLR 2023
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们在ICLR 2023的口头报告中探讨了视觉-语言模型在语言方面的局限性
- en: towardsdatascience.com](/your-vision-language-model-might-be-a-bag-of-words-30b1beaef7f8?source=post_page-----7b97e3ab3893--------------------------------)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/your-vision-language-model-might-be-a-bag-of-words-30b1beaef7f8?source=post_page-----7b97e3ab3893--------------------------------)
- en: References
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Chia, P.J., Attanasio, G., Bianchi, F., Terragni, S., Magalhães, A.R., Gonçalves,
    D., Greco, C., & Tagliabue, J. (2022). Contrastive language and vision learning
    of general fashion concepts. *Scientific Reports, 12*.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Chia, P.J., Attanasio, G., Bianchi, F., Terragni, S., Magalhães, A.R., Gonçalves,
    D., Greco, C., & Tagliabue, J. (2022). 一般时尚概念的对比语言与视觉学习。*Scientific Reports, 12*。
- en: 'Isom, J.A., Walsh, M., & Gardner, J.M. (2017). Social Media and Pathology:
    Where Are We Now and Why Does it Matter? *Advances in Anatomic Pathology*.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Isom, J.A., Walsh, M., & Gardner, J.M. (2017). 社交媒体与病理学：我们现在处于何处以及为何重要？*Advances
    in Anatomic Pathology*。
- en: 'Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M.,
    Coombes, T., Katta, A., Mullis, C., Wortsman, M., Schramowski, P., Kundurthy,
    S., Crowson, K., Schmidt, L., Kaczmarczyk, R., & Jitsev, J. (2022). LAION-5B:
    An open large-scale dataset for training next generation image-text models. *ArXiv,
    abs/2210.08402*.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M.,
    Coombes, T., Katta, A., Mullis, C., Wortsman, M., Schramowski, P., Kundurthy,
    S., Crowson, K., Schmidt, L., Kaczmarczyk, R., & Jitsev, J. (2022). LAION-5B：一个用于训练下一代图像-文本模型的开放大规模数据集。*ArXiv,
    abs/2210.08402*。
- en: Zhang, S., Xu, Y., Usuyama, N., Bagga, J.K., Tinn, R., Preston, S., Rao, R.N.,
    Wei, M., Valluri, N., Wong, C., Lungren, M.P., Naumann, T., & Poon, H. (2023).
    Large-Scale Domain-Specific Pretraining for Biomedical Vision-Language Processing.
    *ArXiv, abs/2303.00915*.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Zhang, S., Xu, Y., Usuyama, N., Bagga, J.K., Tinn, R., Preston, S., Rao, R.N.,
    Wei, M., Valluri, N., Wong, C., Lungren, M.P., Naumann, T., & Poon, H. (2023).
    大规模领域特定预训练用于生物医学视觉-语言处理。*ArXiv, abs/2303.00915*。
