- en: 'From Data to Clusters: When is Your Clustering Good Enough?'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从数据到聚类：你的聚类何时足够好？
- en: 原文：[https://towardsdatascience.com/from-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a](https://towardsdatascience.com/from-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/from-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a](https://towardsdatascience.com/from-data-to-clusters-when-is-your-clustering-good-enough-5895440a978a)
- en: Hidden gems can be found using clustering approaches but you need the right
    clustering method and evaluation approach to make sensible clusters. Learn how
    to find them in four steps.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过聚类方法可以发现隐藏的宝石，但你需要正确的聚类方法和评估方法来创建合理的聚类。学习如何在四个步骤中找到它们。
- en: '[](https://erdogant.medium.com/?source=post_page-----5895440a978a--------------------------------)[![Erdogan
    Taskesen](../Images/8e62cdae0502687710d8ae4bbcd8966e.png)](https://erdogant.medium.com/?source=post_page-----5895440a978a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5895440a978a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5895440a978a--------------------------------)
    [Erdogan Taskesen](https://erdogant.medium.com/?source=post_page-----5895440a978a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://erdogant.medium.com/?source=post_page-----5895440a978a--------------------------------)[![Erdogan
    Taskesen](../Images/8e62cdae0502687710d8ae4bbcd8966e.png)](https://erdogant.medium.com/?source=post_page-----5895440a978a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5895440a978a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5895440a978a--------------------------------)
    [Erdogan Taskesen](https://erdogant.medium.com/?source=post_page-----5895440a978a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5895440a978a--------------------------------)
    ·17 min read·Apr 26, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----5895440a978a--------------------------------)
    ·阅读时间17分钟·2023年4月26日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/4760f872473a60131b462ef1392c5bbe.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4760f872473a60131b462ef1392c5bbe.png)'
- en: Photo by [Shubham Dhage](https://unsplash.com/@illustratiions?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/3nwYFtexa4o?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Shubham Dhage](https://unsplash.com/@illustratiions?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)在[Unsplash](https://unsplash.com/photos/3nwYFtexa4o?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: With unsupervised cluster analysis, we can group observations with similar patterns,
    and reveal (hidden) trends in the data. The use of cluster evaluation methods
    helps to determine the clustering tendency, quality, and optimal number of clusters.
    **In this blog, we will dive into cluster evaluation methods, learn how to interpret
    the methods and select the appropriate clustering method for your use case.**
    We will start by delving into the fundamentals of clustering and evaluation methods
    that are used to assess the quality of clusters, including popular techniques
    like the *Silhouette score*, the *Davies-Bouldin index*, and the *Derivative method*.
    With the use of toy example data sets, we will investigate the strengths and limitations
    of each evaluation method, providing practical insights on how to interpret their
    results. For all analyses, the [*clusteval library*](https://erdogant.github.io/clusteval/)is
    used.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 通过无监督聚类分析，我们可以对具有相似模式的观察数据进行分组，揭示数据中的（隐藏）趋势。使用聚类评估方法有助于确定聚类倾向、质量和最佳聚类数。**在本博客中，我们将深入探讨聚类评估方法，学习如何解释这些方法，并选择适合你用例的聚类方法。**
    我们将从探讨聚类和评估方法的基础知识开始，这些方法用于评估聚类的质量，包括流行的技术，如*轮廓系数*、*Davies-Bouldin指数*和*导数法*。通过使用玩具示例数据集，我们将研究每种评估方法的优缺点，提供关于如何解释其结果的实际见解。所有分析都使用[*clusteval库*](https://erdogant.github.io/clusteval/)。
- en: Unsupervised Clustering.
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督聚类。
- en: With unsupervised clustering, we aim to determine “*natural*” or “*data-driven*”
    groups in the data without using apriori knowledge about labels or categories.
    The challenge of using different unsupervised clustering methods is that it will
    result in different partitioning of the samples and thus different groupings since
    each method implicitly impose a structure on the data. Thus the question arises;
    *What is a “good” clustering?* Figure 1A depicts a set set of samples in a 2-dimensional
    space. *How many clusters do you see?* I would state that there are two clusters
    without using any label information. *Why?* Because of the small distances between
    the dots, and the relatively larger “*gap*” between the cluttered dots.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 使用无监督聚类，我们的目标是确定数据中的“*自然*”或“*数据驱动*”的组，而不使用有关标签或类别的先验知识。使用不同的无监督聚类方法的挑战在于，它会导致样本的不同划分，因此会有不同的分组，因为每种方法隐式地对数据施加了结构。因此，问题出现了；*什么是“好的”聚类？*
    图 1A 展示了一组在二维空间中的样本。*你看到多少个簇？* 我会说有两个簇而不使用任何标签信息。*为什么？* 因为点之间的距离很小，而杂乱点之间的“*间隙*”相对较大。
- en: '![](../Images/d02888b3fc2b82b9d581e873046985bd.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d02888b3fc2b82b9d581e873046985bd.png)'
- en: Figure 1\. Image by the author
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1。图像由作者提供
- en: With this in mind, we can convert our intuition of “*clusters*” into a mathematical
    statement such as; the variance of samples within a so-called-cluster should be
    small (***within variance*** σW***, red and blue***), and at the same time, the
    variance between the clusters should be large (***between variance,*** σB) as
    depicted in Figure 1B. The distance between samples (or the intrinsic relationships)
    can be measured with ***a distance metric*** (e.g., *Euclidean* distance), and
    stored in a so-called *dissimilarity matrix*. The distance between groups of samples
    can then be computed using a ***linkage type*** (for hierarchical clustering).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，我们可以将对“*簇*”的直观理解转化为数学陈述；所谓簇内的样本方差应该很小（***簇内方差*** σW***，红色和蓝色***），而同时簇之间的方差应该很大（***簇间方差***，σB），如图
    1B 所示。样本之间的距离（或内在关系）可以用***距离度量***（例如，*欧几里得*距离）来测量，并存储在所谓的*不相似矩阵*中。然后，可以使用***连接类型***计算样本组之间的距离（用于层次聚类）。
- en: Distance metrics
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 距离度量
- en: The most well-known distance metric is the *Euclidean distance*. Although it
    is set as the default metric in many methods, it is not always the best choice.
    A schematic overview of various distance metrics is depicted in Figure 2.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最著名的距离度量是*欧几里得距离*。尽管它在许多方法中被设置为默认度量，但它并不总是最佳选择。各种距离度量的示意图见图 2。
- en: '*Understand the mathematical properties of the distance metrics so that it
    fits to the data and aligns with the research question.*'
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*理解距离度量的数学性质，以便它适合数据并与研究问题对齐。*'
- en: '![](../Images/d6aeb16a4aa3edb061f15e0af6484c68.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6aeb16a4aa3edb061f15e0af6484c68.png)'
- en: 'Figure 2: Schematic overview of the most popular distance metrics. Image by
    the author.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：最受欢迎的距离度量的示意图。图像由作者提供。
- en: Linkage types.
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 连接类型。
- en: The process of *hierarchical* clustering involves an approach of grouping samples
    into a larger cluster. In this process, the distances between two sub-clusters
    need to be computed for which the different types of linkages describe how the
    clusters are connected (Figure 3).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*层次*聚类的过程涉及将样本分组到一个更大的簇中的方法。在这个过程中，需要计算两个子簇之间的距离，不同类型的连接描述了簇之间的连接方式（图 3）。'
- en: '![](../Images/61da71da6c25175de6519d67fa336a96.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/61da71da6c25175de6519d67fa336a96.png)'
- en: 'Figure 3: Linkage types. Image by the author.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：连接类型。图像由作者提供。
- en: Briefly, the ***Single linkage*** between two clusters is the proximity between
    their two closest samples. It produces a long chain and is therefore ideal for
    clustering for outlier detection or *snake-like-clusters*. The ***complete linkage***
    between two clusters is the proximity between their two most distant samples.
    Intuitively, the two most distant samples cannot be much more dissimilar than
    other quite dissimilar pairs. It forces clusters to be spherical and have often
    “*compact*” contours by their borders, but they are not necessarily compact inside.
    The ***average linkage*** between two clusters is the arithmetic mean of all the
    proximities between the objects of one, on one side, and the objects of the other,
    on the other side. The ***Centroid* linkage** is the proximity between the geometric
    centroids of the clusters. In other words, centroid linkage connects clusters
    based on the center of mass of the data points, while average linkage takes into
    account all the distances between the objects in the two clusters. Choose the
    *metric* and *linkage type* carefully because it directly affects the final clustering
    results.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，两个簇之间的***单链接***是它们两个最接近样本之间的距离。它产生一个长链，因此适用于异常检测或*蛇形簇*的聚类。两个簇之间的***完全链接***是它们两个最远样本之间的距离。直观地说，两个最远的样本之间的相似度不可能比其他相似度较低的样本对更大。这迫使簇变得球形，并且通常在边界处有“*紧凑*”的轮廓，但内部不一定紧凑。两个簇之间的***平均链接***是一个簇中所有对象与另一个簇中所有对象之间距离的算术平均值。***质心链接***是簇的几何质心之间的距离。换句话说，质心链接基于数据点的质心连接簇，而平均链接则考虑了两个簇中所有对象之间的距离。选择*度量标准*和*链接类型*时要小心，因为它直接影响最终的聚类结果。
- en: From Data to clusters in 4 steps.
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从数据到簇的4个步骤。
- en: To determine data-driven clusters, that may behold new or unknown information,
    we carefully need to perform four constitutive steps to go from the input data
    set to sensible clusters. Libraries such as ***distfit*** and ***clusteval***
    can help in this process.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定数据驱动的簇，这些簇可能包含新的或未知的信息，我们需要仔细地执行四个构成步骤，从输入数据集到合理的簇。像***distfit***和***clusteval***这样的库可以帮助完成这个过程。
- en: Coming up with a sensible grouping of samples requires more than blindly running
    a clustering algorithm.
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 提出合理的样本分组需要的不仅仅是盲目运行聚类算法。
- en: 'Step 1: Investigate the underlying distribution of the data.'
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一步：调查数据的基础分布。
- en: 'Investigating the underlying distribution of the data is an important step
    because clustering algorithms rely on the statistical properties of the data to
    identify patterns and group similar data points together. By understanding the
    distribution of the data, such as its mean, variance, skewness, and kurtosis,
    we can make informed decisions about which clustering algorithm to use and how
    to set its parameters to achieve optimal results. Furthermore, investigating the
    data distribution can provide insights into the appropriate normalization or scaling
    techniques to be applied before clustering. ***While supervised approaches, like
    tree models, have the capability to handle mixed data sets, clustering algorithms
    on the other hand are designed to work with homogeneous data***. ***This means
    that all variables should have similar types or units of measurement.*** Normalization
    or scaling is thus an important step as clustering algorithms group data points
    based on their similarity using a metric. More details about investigating the
    underlying data distribution can be read here [[1]](/how-to-find-the-best-theoretical-distribution-for-your-data-a26e5673b4bd):'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 调查数据的基础分布是一个重要的步骤，因为聚类算法依赖于数据的统计特性来识别模式并将相似的数据点分组在一起。通过理解数据的分布，如其均值、方差、偏度和峰度，我们可以做出明智的决定，选择合适的聚类算法以及如何设置其参数以实现最佳结果。此外，调查数据分布可以提供有关在聚类之前应用适当的归一化或缩放技术的见解。***虽然监督方法，如树模型，能够处理混合数据集，但聚类算法则设计用于处理同质数据***。***这意味着所有变量应该具有类似的类型或测量单位。***
    归一化或缩放是一个重要的步骤，因为聚类算法基于相似性使用度量标准对数据点进行分组。有关调查基础数据分布的更多细节可以在这里阅读 [[1]](/how-to-find-the-best-theoretical-distribution-for-your-data-a26e5673b4bd)：
- en: '[](/how-to-find-the-best-theoretical-distribution-for-your-data-a26e5673b4bd?source=post_page-----5895440a978a--------------------------------)
    [## How to Find the Best Theoretical Distribution for Your Data'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/how-to-find-the-best-theoretical-distribution-for-your-data-a26e5673b4bd?source=post_page-----5895440a978a--------------------------------)
    [## 如何找到适合数据的最佳理论分布'
- en: Knowing the underlying data distribution is an essential step for data modeling
    and has many applications, such as anomaly detection…
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解潜在的数据分布是数据建模的关键步骤，并且有许多应用，例如异常检测……
- en: towardsdatascience.com](/how-to-find-the-best-theoretical-distribution-for-your-data-a26e5673b4bd?source=post_page-----5895440a978a--------------------------------)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/how-to-find-the-best-theoretical-distribution-for-your-data-a26e5673b4bd?source=post_page-----5895440a978a--------------------------------)'
- en: 'Step 2: Make an educated guess of the cluster density and the expected cluster
    sizes.'
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 2 步：对簇的密度和期望簇大小做出有根据的猜测。
- en: Setting expectations about cluster densities, shape, and the number of clusters
    will help to select the appropriate clustering algorithm and parameter settings
    to achieve the desired outcome. In addition, setting expectations can provide
    more confidence and validity of the results when interpreting and communicating
    the clustering results to stakeholders in a meaningful way. For example, if the
    aim was to identify rare anomalies in a dataset, and the clustering results produce
    a small cluster with very low density, it could potentially indicate the presence
    of such anomalies. However, it is not always possible to set expectations about
    the number of clusters or the density. Then we need to select clustering method(s)
    solely on the mathematical properties that match with the statistical properties
    of the data and the research aim.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 设定对簇密度、形状和簇数量的期望将有助于选择合适的聚类算法和参数设置，以实现期望的结果。此外，设定期望可以在解释和向利益相关者传达聚类结果时提供更多的信心和有效性。例如，如果目标是识别数据集中稀有的异常，并且聚类结果产生了一个非常低密度的小簇，这可能表明存在这样的异常。然而，并非总是可以设定关于簇数量或密度的期望。此时，我们需要根据与数据的统计属性和研究目标相匹配的数学属性选择聚类方法。
- en: 'Step 3: Select the Clustering Method.'
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 3 步：选择聚类方法。
- en: The selection of a clustering method depends on steps 1 to 4 but we should also
    include factors such as scalability, robustness, and ease of use. For example,
    in a production setting, we may need different properties than for experimental
    use cases. There are several popular clustering methods, such as *K-means, Hierarchical,
    and Density-based clustering algorithms*, each with its own assumptions, advantages,
    and limitations (see a summary below). After the selection of the clustering method,
    we can start clustering the data and evaluate the performance.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 选择聚类方法取决于第 1 到第 4 步，但我们还应该考虑可扩展性、鲁棒性和易用性等因素。例如，在生产环境中，我们可能需要与实验用例不同的属性。有几种流行的聚类方法，如
    *K-means、层次聚类和基于密度的聚类算法*，每种方法都有其自身的假设、优点和局限性（见下文总结）。选择聚类方法后，我们可以开始对数据进行聚类并评估其性能。
- en: '**K-means** assumes that clusters are spherical, equally sized, and have similar
    densities. It requires specifying the number of clusters in advance (k). Note
    the detection for the optimal number of clusters is automatically detected in
    the clusteval library.'
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**K-means** 假设簇是球形的、大小相等的，并且具有相似的密度。它需要事先指定簇的数量（k）。注意，最佳簇数量的检测在 clusteval 库中会自动进行。'
- en: ''
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Hierarchical clustering** builds a tree-like structure of clusters by recursively
    merging clusters based on distance or similarity metrics. It is agglomerative
    (bottom-up) and does not require specifying the number of clusters in advance.'
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**层次聚类**通过基于距离或相似性度量递归地合并簇来构建树状结构的簇。它是聚合性的（自底向上）且不需要事先指定簇的数量。'
- en: ''
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Density-based clustering algorithms**, such as DBSCAN (Density-Based Spatial
    Clustering of Applications with Noise) group together data points that are densely
    packed and have lower densities in between clusters. They do not assume any specific
    shape or size of clusters and can identify clusters of arbitrary shapes and sizes.
    Density-based clustering is particularly useful for identifying clusters of varying
    densities and detecting outliers as noise points. However, they require tuning
    of hyperparameters, such as density threshold, and are sensitive to the choice
    of parameters.'
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**基于密度的聚类算法**，例如 DBSCAN（基于密度的空间聚类应用于噪声），将密集的点组在一起，并且在簇之间具有较低的密度。它们不假设簇的任何特定形状或大小，并且可以识别任意形状和大小的簇。基于密度的聚类特别适用于识别不同密度的簇和将异常点检测为噪声。然而，它们需要调整超参数，例如密度阈值，并且对参数选择敏感。'
- en: Different clustering methods can result in different partitioning of the samples
    and thus different groupings since each method implicitly impose a structure on
    the data.
  id: totrans-41
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 不同的聚类方法可能会导致样本的不同划分，从而产生不同的分组，因为每种方法都隐含地对数据施加了一种结构。
- en: 'Step 4: Cluster Evaluation.'
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第4步：簇评估。
- en: Cluster evaluation is to assess the *clustering tendency, quality, and the optimal
    number of clusters*. There are various cluster evaluation methods for which the
    most popular are incorporated in the ***clusteval*** library, i.e., *Silhouette
    score, Davies-Bouldin index, and the derivative (or Elbow) method*. The difficulty
    in using such techniques is that the clustering step and its evaluation are often
    intertwined for which the *clusteval library* internally handles this. In the
    next section, I will dive into the various methods and test their performances.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 簇评估是为了评估*聚类倾向、质量和最佳簇数*。有各种聚类评估方法，其中最流行的是***clusteval***库中包含的，即*Silhouette分数、Davies-Bouldin指数和导数（或肘部）方法*。使用这些技术的难点在于，聚类步骤及其评估通常是交织在一起的，*clusteval库*内部处理了这一点。在下一节中，我将深入探讨各种方法并测试它们的表现。
- en: Cluster evaluation should be performed during the clustering process to assign
    scores to each clustering and enable meaningful comparisons between the results.
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在聚类过程中应进行簇评估，以为每个聚类分配分数，并实现结果之间的有意义比较。
- en: 'The Next Step: From Clusters to Insights.'
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下一步：从簇到洞察。
- en: 'The step after finding the optimal number of clusters is to determine the driving
    features behind the clusters. A great manner to do this is by using enrichment
    analysis with methods such as [*HNET*](/explore-and-understand-your-data-with-a-network-of-significant-associations-9a03cf79d254)[2],
    where we can use the hypergeometric test and Mann-withney U test to test for significant
    associations between the cluster labels and the features. More details can be
    found here:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 找到最佳簇数后的步骤是确定簇背后的驱动特征。一个很好的方法是使用[**HNET**](/explore-and-understand-your-data-with-a-network-of-significant-associations-9a03cf79d254)[2]进行富集分析，我们可以使用超几何检验和Mann-Whitney
    U检验来测试簇标签与特征之间的显著关联。更多细节可以在这里找到：
- en: '[](/explore-and-understand-your-data-with-a-network-of-significant-associations-9a03cf79d254?source=post_page-----5895440a978a--------------------------------)
    [## Explore and understand your data with a network of significant associations.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/explore-and-understand-your-data-with-a-network-of-significant-associations-9a03cf79d254?source=post_page-----5895440a978a--------------------------------)
    [## 通过显著关联网络探索和理解数据。'
- en: Explore to understand your data can make the difference between an unsuccessful
    project or finishing successfully!
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索以理解您的数据可以决定一个项目的成功与否！
- en: towardsdatascience.com](/explore-and-understand-your-data-with-a-network-of-significant-associations-9a03cf79d254?source=post_page-----5895440a978a--------------------------------)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/explore-and-understand-your-data-with-a-network-of-significant-associations-9a03cf79d254?source=post_page-----5895440a978a--------------------------------)
- en: The Clusteval Library.
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Clusteval库。
- en: A few words about the [*clusteval*](https://erdogant.github.io/clusteval/)library
    that is used for all the analysis. The [*clusteval library*](https://erdogant.github.io/clusteval/)
    is designed to tackle the challenges of evaluating clusters and creating meaningful
    plots. In a [*previous blog*](https://medium.com/towards-data-science/a-step-by-step-guide-for-clustering-images-4b45f9906128)
    [3], I demonstrated its strength in image recognition for the clustering of images.
    The *clusteval* library contains the most popular evaluation methods, the *Silhouette
    score, the Davies-Bouldin index, and the derivative (or Elbow) method* to evaluate
    the clustering of *Agglomerative, K-means, DBSCAN, and HDBSCAN*. It can handle
    all data types, such as continuous, categorical, and discrete data sets. Just
    make sure that 1\. The data is correctly normalized. 2\. All variables have similar
    types or units of measurement. 3\. you choose the appropriate distance metric
    and evaluation method.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 关于[*clusteval*](https://erdogant.github.io/clusteval/)库的几句说明，该库用于所有分析。[*clusteval库*](https://erdogant.github.io/clusteval/)旨在应对评估簇和创建有意义图表的挑战。在[*之前的博客*](https://medium.com/towards-data-science/a-step-by-step-guide-for-clustering-images-4b45f9906128)[3]中，我展示了它在图像识别中的强大功能，用于图像的聚类。*clusteval*库包含了最受欢迎的评估方法，即*Silhouette分数、Davies-Bouldin指数和导数（或肘部）方法*，用于评估*层次聚类、K-means、DBSCAN和HDBSCAN*的聚类。它可以处理所有数据类型，例如连续数据、分类数据和离散数据集。只需确保1.
    数据正确归一化。2. 所有变量具有相似的类型或测量单位。3. 选择适当的距离度量和评估方法。
- en: '***Clusteval functionalities***'
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '***Clusteval功能***'
- en: '*It contains the most-wanted cluster evaluation pipelines for the Silhouette
    scores, Davies-Bouldin index, and the Derivative method to evaluate the clustering
    of Agglomerative, K-means, DBSCAN, and HDBSCAN.*'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*它包含了针对轮廓系数、Davies-Bouldin指数以及用于评估层次聚类、K均值、DBSCAN和HDBSCAN的导数方法的最受欢迎的集群评估管道。*'
- en: '*It contains functionalities to create meaningful plots; the optimal number
    of clusters, a Silhouette plot with its coefficients, scatterplots, and dendrograms.*'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*它包含创建有意义图形的功能；集群的最佳数量、带系数的轮廓图、散点图和树状图。*'
- en: '*It contains functionality to determine the driving features behind the clusters.*'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*它包含了确定集群背后驱动特征的功能。*'
- en: '*It can handle all data types, such as continuous, categorical, and discrete
    data sets. Just make sure that all variables have similar types or units of measurement.*'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*它可以处理所有数据类型，如连续型、类别型和离散数据集。只需确保所有变量具有相似的类型或测量单位。*'
- en: '[*Open-source*](https://erdogant.github.io/clusteval/)'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*开源*](https://erdogant.github.io/clusteval/)'
- en: '[*Documentation page*](https://erdogant.github.io/clusteval/)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*文档页面*](https://erdogant.github.io/clusteval/)'
- en: The Choice of a Clustering Method Matters.
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类方法的选择很重要。
- en: It may not surprise you that different cluster techniques can result in different
    groupings as they respond differently to varying densities and group sizes. The
    detection of the correct number of clusters can therefore be challenging, especially
    when it is high-dimensional where visual inspections are not possible. With ***clustering
    evaluation*** methods we can investigate and do some backtesting to determine
    the optimal number of clusters and describe the clustering tendency. To demonstrate
    how well the number of clusters can be determined, I created seven toy example
    data sets (inspired by [scikit-learn](https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html))
    with varying densities, shapes, and sample sizes, i.e., *snake clusters*, *clusters
    with different densities and sizes*, and *uniformly distributed samples* (Figure
    4). Each toy example contains 1000 samples and two features. As such, it can give
    an intuition when working in low dimensional space, such as after a feature extraction
    step e.g., *PCA, t-SNE, or UMAP* step.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的集群技术可以导致不同的分组，因为它们对不同的密度和组大小的响应各异。因此，确定正确的集群数量可能具有挑战性，尤其是在高维空间中，当视觉检查不可行时。通过***集群评估***方法，我们可以进行调查并进行一些回测，以确定最佳的集群数量并描述集群趋势。为了展示如何确定集群数量，我创建了七个玩具示例数据集（灵感来源于[scikit-learn](https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html)），这些数据集具有不同的密度、形状和样本大小，即*蛇形集群*、*不同密度和大小的集群*以及*均匀分布的样本*（图4）。每个玩具示例包含1000个样本和两个特征。因此，它可以在低维空间中工作时提供直观感受，例如在特征提取步骤之后，例如*PCA、t-SNE或UMAP*步骤。
- en: '![](../Images/cadecdd0a883e84d146c5f547ebd4bfd.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cadecdd0a883e84d146c5f547ebd4bfd.png)'
- en: Figure 4\. Seven toy example data sets with different densities, shapes, and
    sizes. Samples are colored on their ground truth labels. Image by the author.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. 七个玩具示例数据集，具有不同的密度、形状和大小。样本根据其实际标签进行着色。图片由作者提供。
- en: For these seven toy examples, we will investigate the clustering using the evaluation
    methods that are available in *clusteval*; *the Silhouette score, the Davies-Bouldin
    index, and the derivative (or Elbow) method*. The clustering itself is performed
    using *K-means, Hierarchical clustering (single, complete, and ward distance),
    and Density-based clustering algorithms (DBSCAN)*.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这七个示例数据，我们将使用在*clusteval*中可用的评估方法进行集群分析；*轮廓系数、Davies-Bouldin指数和导数（或肘部）方法*。集群本身使用*K均值、层次聚类（单一、完全和Ward距离）和基于密度的聚类算法（DBSCAN）*进行。
- en: '*If you find this article helpful, use my* [*referral link*](https://medium.com/@erdogant/membership)
    *to continue learning without limits and sign up for a Medium membership. Plus,*
    [*follow me*](http://erdogant.medium.com/) *to stay up-to-date with my latest
    content!*'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你觉得这篇文章对你有帮助，请使用我的* [*推荐链接*](https://medium.com/@erdogant/membership) *继续无缝学习并注册Medium会员。此外，*
    [*关注我*](http://erdogant.medium.com/) *以保持更新我的最新内容！*'
- en: Cluster Evaluation using Silhouette scores.
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用轮廓系数进行集群评估。
- en: '***The Silhouette score method*** is a measure of how similar a sample is to
    its own cluster (cohesion) compared to other clusters (separation). The scores
    range between [-1, 1], where a high value indicates that the sample or observation
    is well-matched to its own cluster and poorly matched to neighboring clusters.
    It is a sample-wise approach, which means that for each sample, a *Silhouette*
    score is computed, and if most samples have a high value, then the clustering
    configuration is considered appropriate. If many points have a low or negative
    value, then the clustering configuration may have too many or too few clusters.
    *The Silhouette score method is independent of the distance metric which makes
    it an attractive and versatile method to use.*'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '***轮廓系数法***是一种衡量样本与自身簇（内聚性）相比于其他簇（分离性）的相似度的指标。得分范围在[-1, 1]之间，高值表示样本或观察值与自身簇匹配良好，而与相邻簇匹配较差。这是一种样本级的方法，这意味着对于每个样本，都会计算一个*轮廓系数*，如果大多数样本的值很高，则认为聚类配置是合适的。如果许多点的值较低或为负，则可能是聚类配置的簇数过多或过少。*轮廓系数法与距离度量无关，这使得它成为一种有吸引力且通用的方法。*'
- en: Let’s see how well it can group samples with similar densities and sizes for
    an easy data set, namely the ***blobs data set***. In the underneath code section,
    we will initialize the *DBSCAN* method with the *Silhouette* evaluation method.
    Note that we could have used any other clustering method as demonstrated in the
    next section. For *DBSCAN*, A search across the epsilon parameter is performed
    and the number of clusters with the highest *Silhouette score* is retrieved. The
    *DBSCAN* with the *Silhouette score* correctly detects the 4 clusters, and the
    *sample-wise Silhouette coefficients* (Figure 5 middle) look stable (high scores
    with uniform shape for the clusters). This is expected as the samples in the clusters
    are far apart from other clusters and close to the center of their own cluster.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它如何在一个简单的数据集上对具有相似密度和大小的样本进行分组，即***blob数据集***。在下面的代码部分，我们将初始化*DBSCAN*方法与*轮廓系数*评估方法。请注意，我们可以使用任何其他聚类方法，如下一节所示。对于*DBSCAN*，在epsilon参数上进行搜索，并检索具有最高*轮廓系数*的簇数量。具有*轮廓系数*的*DBSCAN*正确检测出4个簇，并且*样本级轮廓系数*（图5中间）看起来稳定（簇的高分且形状均匀）。这是预期的，因为簇中的样本远离其他簇，并且接近各自簇的中心。
- en: '[PRE0]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/9ce1b14279bf38b55c39d8b15f314d49.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ce1b14279bf38b55c39d8b15f314d49.png)'
- en: 'Figure 5\. Left: Optimizing the Epsilon parameter for the detection of the
    optimal number of clusters. Middle: Silhouette scores per sample. Right: Determined
    cluster labels. Image by the author.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图5\. 左侧：优化Epsilon参数以检测最佳簇数量。中间：每个样本的轮廓系数。右侧：确定的簇标签。图片由作者提供。
- en: This was just one data set and one clustering approach (*DBSCAN*) that we evaluated.
    Let’s iterate across the seven toy example data sets and five cluster methods
    and determine the optimal number of clusters using the *Silhouette* method. The
    results are shown in Figure 6 for which the top part shows the scatter plot colored
    on the optimal number of clusters, and the bottom part shows the *Silhouette*
    plots among a varying number of clusters.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个数据集和一种聚类方法（*DBSCAN*）的评估。让我们在七个玩具示例数据集和五种聚类方法中进行迭代，使用*轮廓系数*方法确定最佳簇的数量。结果如图6所示，其中上半部分显示了按最佳簇数量着色的散点图，下半部分显示了不同簇数量的*轮廓系数*图。
- en: The conclusion for Figure 6 is that **if** we choose the appropriate clustering
    method for the data set, **then** we can very well detect the correct number of
    clusters. This may feel like a trivial remark but choosing the appropriate clustering
    method can be challenging as you need to carefully perform the steps 1 to 4 as
    described in the previous section.
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 图6的结论是**如果**我们选择适合数据集的聚类方法，**那么**我们可以很好地检测出正确的簇的数量。这可能看起来是一个微不足道的评论，但选择合适的聚类方法可能具有挑战性，因为你需要仔细执行前面章节中描述的第1到第4步。
- en: '***In case we can not visually inspect the results because of high dimensionality,
    we can still get an intuition of the sample distribution and the formation of
    clusters.*** If the groups are clearly separated, such as *snake-like clusters*,
    and *blobs*, we can expect a strong peak in the *Silhouette* scores for a specific
    number of clusters with the right clustering method. However, when the data becomes
    noisier, such as for the *globular* data set, multiple peaks can start to appear
    and the slope of the Silhouette score can gradually increase without clear peaks.
    This usually hints that the clusters are not strongly formed. With the parameters
    `min_clust` and/or `max_clust` in *clusteval,* we can bound the search space and
    determine relatively high peaks. This manual optimization step of inspecting and
    bounding is what we call backtesting. An interesting observation in Figure 6 is
    that the scores of the *DBSCAN* clustering method show an increase in turbulence
    when the data becomes noisy (see Figure 6, second column). The reasoning is that
    many outliers are detected and start forming smaller clusters, and later on, can
    become part again of larger clusters.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '***如果我们由于高维度无法直观地检查结果，我们仍然可以获取样本分布和簇形成的直觉。*** 如果各组明显分离，如*蛇形簇*和*斑点簇*，我们可以期待在特定簇数上出现*轮廓系数*的强峰值，并且使用正确的聚类方法。但当数据变得更嘈杂时，如*球形*数据集，多重峰值可能会出现，且轮廓系数的斜率可能会逐渐增加而没有明显的峰值。这通常暗示簇的形成不够强烈。通过*clusteval*中的`min_clust`和/或`max_clust`参数，我们可以限定搜索空间并确定相对较高的峰值。这个检查和限定的手动优化步骤被称为回测。图6中的一个有趣观察是，当数据变得嘈杂时，*DBSCAN*聚类方法的得分显示出涡流增加（见图6，第二列）。原因是检测到许多离群点并开始形成更小的簇，后来这些离群点可能再次成为较大簇的一部分。'
- en: '![](../Images/c17caf4d8610e553ab68511dc86d6085.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c17caf4d8610e553ab68511dc86d6085.png)'
- en: Figure 6\. Detection of the optimal number of clusters using the *Silhouette*
    score method for five clustering methods and seven toy example data sets. Image
    by the author.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图6\. 使用*轮廓系数*方法检测五种聚类方法和七个玩具示例数据集的最佳簇数。图片由作者提供。
- en: Cluster Evaluation using the ***Davies-Bouldin index***.
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用***戴维斯-鲍尔丁指数***进行聚类评估。
- en: '***The Davies-Bouldin index*** (*DBindex)* can be intuitively described as
    a measure of the ratio between within-cluster distances and between-cluster distances.
    The score is bounded between [0, 1] for which lower values hint towards tighter
    clusters and better separation between clusters. *Lower scores are thus considered
    better.* In contradiction to the *Silhouette score method*, the *Davies-Bouldin
    index* is ***not*** a sample-wise measure which makes it faster in usage. A disadvantage
    of *DBindex* is that it frequently overshoots as lower values are more common
    with a growing number of clusters. This can be clearly seen by the gradually decreasing
    slope of the scores without a distinct low score.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '***戴维斯-鲍尔丁指数***（*DBindex*）可以直观地描述为簇内距离与簇间距离的比率度量。得分在[0, 1]之间，其中较低的值表示簇之间更紧密和更好的分离。*较低的得分因此被认为更好。*
    与*轮廓系数方法*不同，*戴维斯-鲍尔丁指数****不是***逐样本度量，因此使用速度更快。*DBindex*的一个缺点是它常常过度预估，因为随着簇数增加，较低的值更为常见。这可以通过得分逐渐下降的斜率没有明显低得分来清晰地看到。'
- en: Let’s see how well we can cluster samples with similar densities and sizes to
    get some intuition of the performance of the *Davies-Bouldin index*. In the underneath
    code section, we will initialize the *Agglomerative* clustering method with the
    *Davies-Bouldin index*evaluation method. In case we limit the search space in
    ***clusteval*** using `max_clust=10`, it correctly detects the 4 clusters (Figure
    7 left panel). However, if we do not limit the search space there is a gradually
    decreasing slope of the scores which leads to the detection of the wrong number
    of clusters (Figure 7 right panel).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何对具有相似密度和大小的样本进行聚类，以获取*戴维斯-鲍尔丁指数*性能的一些直觉。在下面的代码部分，我们将用*戴维斯-鲍尔丁指数*评估方法初始化*凝聚*聚类方法。如果我们在***clusteval***中使用`max_clust=10`限制搜索空间，它能正确检测到4个簇（图7左侧面板）。然而，如果我们不限制搜索空间，得分的斜率将逐渐下降，从而导致检测到错误的簇数（图7右侧面板）。
- en: '[PRE1]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/a5cff6602068b80746a6bcbee38c2edd.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a5cff6602068b80746a6bcbee38c2edd.png)'
- en: 'Figure 7\. *Davies-Bouldin index* scores for varying numbers of clusters. Left
    panel: With a maximum limit of 10 clusters. Right panel: With a maximum limit
    of 20 clusters. Image by the author.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图7\. *Davies-Bouldin指数* 对不同数量簇的评分。左侧面板：最多10个簇。右侧面板：最多20个簇。图片由作者提供。
- en: For the *Davies-Bouldin index* method, we will also iterate across the seven
    toy example data sets but I will only include the *Agglomeration* cluster methods
    for single, complete, and ward linkage. Note that *DBSCAN* is not supported in
    combination with the *Davies-Bouldin index*. In all examples, there is a limit
    (`max_clust=10`) in the search space to prevent overshooting (Figure 8). Even
    though we limited the search space, there is still a gradually decreasing slope
    that leads to an incorrect number of clusters in many example data sets. This
    is especially seen in the *snake-like* and *anisotropic* clusters (top three rows).
    A solution is to further lower the maximum number of clusters (`max_clust`). For
    the *Blobs, Globular, and Density* toy examples, the correct cluster tendency
    is detected as can be seen from the scatter plot but it overshoots frequently
    for the optimal number of clusters. Here again, if we could further limit the
    maximum number of clusters to 6 or 7, the results would improve.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*Davies-Bouldin指数*方法，我们也将遍历这七个示例数据集，但我将仅包括*Agglomeration*簇方法的单一、完整和Ward连接。请注意，*DBSCAN*
    与*Davies-Bouldin指数*不兼容。在所有示例中，搜索空间有一个限制（`max_clust=10`），以防止过度估计（图8）。尽管我们限制了搜索空间，但仍然存在一个逐渐下降的斜率，在许多示例数据集中导致不正确的簇数量。这在*蛇形*和*各向异性*簇（前三行）中尤为明显。一种解决方案是进一步降低最大簇数量（`max_clust`）。对于*Blobs,
    Globular, 和 Density*示例数据集，可以从散点图中看出正确的簇趋势，但最佳簇数量经常过度估计。在这里，如果我们能够进一步将最大簇数量限制为6或7，结果会有所改善。
- en: '![](../Images/c4505c12d525f7f7ffe8f6237ddf789b.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4505c12d525f7f7ffe8f6237ddf789b.png)'
- en: Figure 8\. Detection of the optimal number of clusters using the *Davies-Bouldin
    index method* for the seven toy example data sets. Image by the author.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图8\. 使用*Davies-Bouldin指数方法*检测七个示例数据集的最佳簇数量。图片由作者提供。
- en: Cluster Evaluation using ***the derivative method***.
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用***导数方法***进行簇评估。
- en: '***The derivative method*** compares the height of each cluster merge to the
    average and normalizes it by the standard deviation calculated over the depth
    of previous levels. Finally, the derivative method returns the cluster labels
    for the optimal cut-off based on the chosen hierarchical clustering method (red
    vertical line). Or in other words, a strong peak in the orange line can be seen
    when there is a sharp “*elbow*” in the blue line. Let’s see how well the *Derivative
    method* can cluster samples with similar densities and sizes. In the underneath
    code section, we will initialize the *Agglomerative* clustering method with the
    *derivative* evaluation method. In comparison to the *Davies-Bouldin index,* we
    do not need to limit the search space (Figure 9) and can easily detect the 4 clusters.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '***导数方法*** 比较每个簇合并的高度与平均值，并通过计算前几个层级深度的标准差进行归一化。最后，导数方法返回基于所选层次聚类方法（红色垂直线）的最佳截断的簇标签。换句话说，当蓝色线中的“*肘部*”很明显时，可以在橙色线中看到一个强峰值。让我们看看*导数方法*在聚类具有类似密度和大小的样本方面的效果。在下面的代码部分，我们将初始化*Agglomerative*聚类方法，并使用*导数*评估方法。与*Davies-Bouldin指数*相比，我们不需要限制搜索空间（图9），可以轻松检测到4个簇。'
- en: '![](../Images/b1ad873d3d73519694841b9ec53311aa.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b1ad873d3d73519694841b9ec53311aa.png)'
- en: Figure 9\. *Derivative method* scores for varying numbers of clusters. Image
    by the author.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图9\. *导数方法* 对不同数量簇的评分。图片由作者提供。
- en: '[PRE2]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Let’s iterate across the seven toy example data sets and determine the optimal
    number of clusters using the *Derivative* method (Figure 8). It can be seen that
    the *Derivative* approach frequently leads to an incorrect number of clusters.
    If we look closer at the *Derivative* line (orange), it shows a small peak across
    many data sets that point toward a weak elbow and thus likely noisy data. This
    hints that the estimated number of clusters may be unreliable.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将遍历这七个示例数据集，并使用*导数*方法（图8）确定最佳簇数量。可以看出，*导数*方法经常导致不正确的簇数量。如果我们仔细观察*导数*线（橙色），它在许多数据集中显示出一个小峰值，指向一个较弱的肘部，从而可能表示数据噪声。这表明估计的簇数量可能不可靠。
- en: '![](../Images/155446ac31004d51ca3260a4c0058da8.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/155446ac31004d51ca3260a4c0058da8.png)'
- en: Figure 9\. Detection of the optimal number of clusters using the *Derivative
    method* for seven toy example data sets. Image by the author.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9\. 使用*导数法*检测七个玩具示例数据集的最佳聚类数量。图片由作者提供。
- en: '*Be aware that the cluster evaluation approaches can easily be fooled as scores
    can gradually improve by an increasing number of clusters.*'
  id: totrans-93
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*请注意，聚类评估方法很容易被欺骗，因为分数可能会随着聚类数量的增加而逐渐提高。*'
- en: Final words.
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结束语。
- en: I demonstrated how different clustering methods respond to different data sets
    and that it takes multiple steps to determine the optimal number of clusters.
    Also, different clustering methods will yield different grouping since they implicitly
    impose a structure on the data, and thus the partitioning of the samples. The
    ***clusteval package*** can help in cluster investigation and backtesting for
    which the results can be explored by various plotting functionalities.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我展示了不同的聚类方法如何响应不同的数据集，以及确定最佳聚类数量需要多个步骤。此外，不同的聚类方法将产生不同的分组，因为它们隐式地对数据施加了结构，从而影响样本的划分。***clusteval包***
    可以帮助进行聚类调查和回测，其结果可以通过各种绘图功能进行探索。
- en: In general, *K-means* clustering works pretty well, especially for *spherical*,
    balanced, and *globular* clusters. However, it is not well suited for detecting
    clusters with different *densities* or *snake-like* clusters. *DBSCAN* on the
    other hand is a great allrounder that detects the correct number of clusters in
    most scenarios. Just be aware that the clustering tendency can be correct but
    the exact number of detected clusters may not. This is especially the case in
    *globular* clusters as many (groups of) samples can be flagged as outliers. A
    disadvantage of *DBSCAN* is that it can be computationally intensive, especially
    when optimizing the epsilon parameter. Instead, use agglomerative clustering with
    single linkage for *snake-like* clusters or for the detection of outliers. Alternatively,
    ward or complete linkage is suited for noisy, *globular* clusters, and *K-means*
    for *spherical*, balanced, and *globular* clusters.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，*K-means* 聚类效果很好，特别是对于*球形*、平衡的和*球状*聚类。然而，它不适合检测具有不同*密度*或*蛇形*聚类的情况。另一方面，*DBSCAN*
    是一个很好的全能选手，可以在大多数情况下检测到正确的聚类数量。只是要注意，聚类倾向可能是正确的，但检测到的确切聚类数量可能不对。这在*球状*聚类中尤其如此，因为许多（组）样本可能被标记为离群点。*DBSCAN*
    的一个缺点是计算量大，特别是在优化 epsilon 参数时。相反，对于*蛇形*聚类或离群点检测，使用单链聚类；对于噪声*球状*聚类，使用ward或完全链路，*K-means*
    适用于*球形*、平衡的和*球状*聚类。
- en: For the cluster evaluation methods, the *Silhouette score* has many advantages.
    It is a sample-wise measure that can be used with any distance metric, making
    it an attractive and versatile method to use. The *Davies-Bouldin index* and the
    *Derivative method* on the other hand need a more iterative process of backtesting
    and investigation as it often overshoots for the correct number of clusters. The
    *Davies-Bouldin index* is described to be more robust when dealing with datasets
    with overlapping clusters and is more computationally efficient as it does not
    require computing the pairwise distances between all data points.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 对于聚类评估方法，*轮廓系数*有很多优势。它是一种逐样本的度量，可以与任何距离度量一起使用，使其成为一种有吸引力且多用途的方法。另一方面，*Davies-Bouldin指数*和*导数法*需要更多的迭代过程进行回测和调查，因为它们往往超出正确的聚类数量。*Davies-Bouldin指数*在处理具有重叠聚类的数据集时被描述为更具鲁棒性，并且计算效率更高，因为它不需要计算所有数据点之间的成对距离。
- en: To summarize, the choice of cluster evaluation methods depends on the specific
    characteristics of your dataset, the goals of your analysis, and the requirements
    of your application. You can always perform a backtesting approach using multiple
    methods and compare their results to make an informed decision based on the specific
    context of your problem.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，聚类评估方法的选择取决于数据集的特定特征、分析目标和应用要求。你可以使用多种方法进行回测，并比较其结果，以便根据问题的具体背景做出明智的决定。
- en: '*Be Safe. Stay Frosty.*'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*保持安全，保持警觉。*'
- en: '***Cheers E.***'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '***干杯 E.***'
- en: '*If you found this article helpful, use my* [*referral link*](https://medium.com/@erdogant/membership)
    *to continue learning without limits and sign up for a Medium membership. Plus,*
    [*follow me*](http://erdogant.medium.com/) *to stay up-to-date with my latest
    content!*'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你觉得这篇文章对你有帮助，请使用我的* [*推荐链接*](https://medium.com/@erdogant/membership) *继续无限制地学习，并注册Medium会员。此外，*
    [*关注我*](http://erdogant.medium.com/) *以保持最新内容！*'
- en: Software
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 软件
- en: '[clusteval Colab notebook](https://erdogant.github.io/clusteval/pages/html/Documentation.html)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[clusteval Colab 笔记本](https://erdogant.github.io/clusteval/pages/html/Documentation.html)'
- en: '[clusteval Github/Documentation](https://erdogant.github.io/clusteval)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[clusteval Github/文档](https://erdogant.github.io/clusteval)'
- en: Let’s connect!
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我们联系！
- en: '[Let’s connect on LinkedIn](https://www.linkedin.com/in/erdogant/)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在 LinkedIn 上与我联系](https://www.linkedin.com/in/erdogant/)'
- en: '[Follow me on Github](https://github.com/erdogant)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在 Github 上关注我](https://github.com/erdogant)'
- en: '[Follow me on Medium](https://erdogant.medium.com/)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在 Medium 上关注我](https://erdogant.medium.com/)'
- en: References
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: E. Taskesen, [*How to Find the Best Theoretical Distribution for Your Data*](/how-to-find-the-best-theoretical-distribution-for-your-data-a26e5673b4bd),
    Febr. 2023, *Towards Data Science, Medium*.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: E. Taskesen，[*如何找到最适合你数据的理论分布*](/how-to-find-the-best-theoretical-distribution-for-your-data-a26e5673b4bd)，2023年2月，*Towards
    Data Science, Medium*。
- en: E. Taskesen, [*Explore and understand your data with a network of significant
    associations*](/explore-and-understand-your-data-with-a-network-of-significant-associations-9a03cf79d254),
    Aug. 2021, *Towards Data Science, Medium*
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: E. Taskesen，[*通过显著关联网络探索和理解你的数据*](/explore-and-understand-your-data-with-a-network-of-significant-associations-9a03cf79d254)，2021年8月，*Towards
    Data Science, Medium*
- en: '*E. Taskesen,* [*A step-by-step guide for clustering images*](https://medium.com/towards-data-science/a-step-by-step-guide-for-clustering-images-4b45f9906128)*,
    Dec. 2021, Towards Data Science, Medium.*'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*E. Taskesen,* [*图像聚类的逐步指南*](https://medium.com/towards-data-science/a-step-by-step-guide-for-clustering-images-4b45f9906128)*，2021年12月，Towards
    Data Science, Medium。*'
