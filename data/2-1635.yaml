- en: 'Parquet Best Practices: Discover your Data without loading it'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Parquet 最佳实践：在不加载数据的情况下发现你的数据
- en: 原文：[https://towardsdatascience.com/parquet-best-practices-discover-your-data-without-loading-them-f854c57a45b6](https://towardsdatascience.com/parquet-best-practices-discover-your-data-without-loading-them-f854c57a45b6)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/parquet-best-practices-discover-your-data-without-loading-them-f854c57a45b6](https://towardsdatascience.com/parquet-best-practices-discover-your-data-without-loading-them-f854c57a45b6)
- en: Metadata, Statistics on Row Groups, Partitions discovery, and Repartitioning
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 元数据、行组统计、分区发现和重新分区
- en: '[](https://medium.com/@arli94?source=post_page-----f854c57a45b6--------------------------------)[![Arli](../Images/7027413407fa83ce2f9b3d7e9cb008e8.png)](https://medium.com/@arli94?source=post_page-----f854c57a45b6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f854c57a45b6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f854c57a45b6--------------------------------)
    [Arli](https://medium.com/@arli94?source=post_page-----f854c57a45b6--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@arli94?source=post_page-----f854c57a45b6--------------------------------)[![Arli](../Images/7027413407fa83ce2f9b3d7e9cb008e8.png)](https://medium.com/@arli94?source=post_page-----f854c57a45b6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f854c57a45b6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f854c57a45b6--------------------------------)
    [Arli](https://medium.com/@arli94?source=post_page-----f854c57a45b6--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f854c57a45b6--------------------------------)
    ·8 min read·Jan 3, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f854c57a45b6--------------------------------)
    ·8分钟阅读·2023年1月3日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '*If you like to experience Medium yourself, consider supporting me and thousands
    of other writers by* [***signing up for a membership***](https://medium.com/@arli94/membership)*.
    It only costs $5 per month, it supports us, writers, greatly, and you get to access
    all the amazing stories on Medium.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你想亲自体验 Medium，可以考虑通过* [***注册会员***](https://medium.com/@arli94/membership)
    *来支持我和其他成千上万的写作者。这只需每月$5，它对我们写作者的支持巨大，而且你可以访问 Medium 上所有精彩的故事。*'
- en: '![](../Images/c510a1ca6e02384122e5f50eb2674840.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c510a1ca6e02384122e5f50eb2674840.png)'
- en: Photo by [Jakarta Parquet](https://unsplash.com/@lantai_kayu?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由 [Jakarta Parquet](https://unsplash.com/@lantai_kayu?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) 提供的照片
- en: '*This article is the next article from a series of articles on Parquet. You
    should check the previous* [*Parquet article*](https://medium.com/towards-data-science/easy-parquet-tutorial-best-practices-237955e46cb7)
    *before reading this one if you don’t have any Parquet knowledge, but it is also
    a great reminder for people more advanced. If you want to reproduce the input
    data for this article, the code is at the end.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*这篇文章是关于 Parquet 系列文章中的下一篇。如果你没有 Parquet 知识，应该先查看之前的* [*Parquet 文章*](https://medium.com/towards-data-science/easy-parquet-tutorial-best-practices-237955e46cb7)
    *，但它也是对更高级用户的很好的提醒。如果你想重现这篇文章的输入数据，代码在文末。*'
- en: '*Apache Parquet* is a columnar storage format for big data frameworks, such
    as *Apache Hadoop* and *Apache Spark*. It is designed to improve the performance
    of big data processing by using a *columnar storage format*, which stores data
    in a *compressed* and *efficient* way.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*Apache Parquet* 是一种用于大数据框架的列式存储格式，如*Apache Hadoop*和*Apache Spark*。它旨在通过使用*列式存储格式*以*压缩*和*高效*的方式存储数据，从而提高大数据处理的性能。'
- en: '*Parquet* adoption continues to increase as more and more organizations turn
    to big data technologies to process and analyze large datasets. With this continuous
    development, it is important that everyone learns some best practices and how
    to navigate through *Parquet* files.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*Parquet* 的采用持续增加，因为越来越多的组织转向大数据技术来处理和分析大型数据集。随着这种持续发展，学习一些最佳实践以及如何浏览*Parquet*文件变得至关重要。'
- en: In this tutorial, we will show you how to gain maximum insight into your *Parquet*
    data as a *Parquet* user without resorting to the common brute force of loading
    it for understanding.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将展示如何作为*Parquet*用户在不依赖常见的暴力加载方式的情况下，深入洞察你的*Parquet*数据。
- en: The case study
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究
- en: To do this, we provide you with a case study in which a *Data Engineer* has
    provided you with *loan applicants'* data and you need to create predictive models
    with that data. But first, you need to *“technically”* discover the data. And
    it is huge data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们提供了一个案例研究，其中一个*数据工程师*给你提供了*贷款申请者*的数据，你需要使用这些数据创建预测模型。但首先，你需要*“技术性地”*发现数据。数据量非常大。
- en: Indeed, the *Data Engineer* that prepared the data, tells you that the Parquet
    folder is *1TB* large *(for educational purposes only, this is not the case in
    our example)*, so if you try to load everything, you will run through Memory Error
    on your machine.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，准备数据的*数据工程师*告诉你，Parquet文件夹的大小为*1TB*（*仅用于教育目的，这在我们的示例中并非如此*），因此如果你尝试加载所有内容，你的机器将会遇到内存错误。
- en: Don’t worry, we will provide you with the most efficient way to understand the
    big *Parquet* data without even loading the *Parquet* data in memory.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 不用担心，我们会提供最有效的方式来理解大型*Parquet*数据，甚至不需要将*Parquet*数据加载到内存中。
- en: 'That means answering the questions below:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着需要回答以下问题：
- en: What do the *Parquet* files in this folder look like?
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个文件夹中的*Parquet*文件是什么样的？
- en: What *variables* are inside? With what *types*? Some *statistics*?
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*变量*里面有哪些？*类型*是什么？一些*统计数据*？'
- en: how is the data partitioned?
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据是如何分区的？
- en: We will also teach you how to reformat *partitions* if you notice that something
    is wrong with the way the data has been partitioned.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还会教你如何重新格式化*分区*，如果你发现数据分区的方式有问题的话。
- en: Reading the first Parquet file
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 阅读第一个Parquet文件
- en: 'The import that you will need for this tutorial:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你在这个教程中需要的导入：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: First of all, we want to get an idea of what the folder *‘APPLICATIONS_PARTITIONED’*
    contains, this is where the data is stored.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们想了解文件夹*‘APPLICATIONS_PARTITIONED’*包含什么，这里存储了数据。
- en: Since you don’t know how the data is *partitioned*, you cannot just load the
    whole folder blindly because you’ll be loading all the *Parquet* files and that’s
    not what you want to do (remember 1TB size), but rather you want to get an overview
    of your data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你不知道数据是如何*分区*的，因此不能盲目地加载整个文件夹，因为你将会加载所有的*Parquet*文件，这不是你想做的（记住1TB的大小），而是你需要对数据进行概览。
- en: Here, I give you a function `get_first_parquet_from_path()` that will return
    the first *Parquet* file that is in a directory. The function will scan through
    each directory and subdirectory until it finds a *Parquet* file and will return
    the complete path of this single file.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我给你一个函数`get_first_parquet_from_path()`，它会返回目录中的第一个*Parquet*文件。该函数将扫描每个目录和子目录，直到找到一个*Parquet*文件，并返回该单个文件的完整路径。
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Looks like a cool function, let’s put it into practice.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来是个很酷的函数，让我们把它付诸实践。
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We notice from the path that this is partitioned by *NAME_INCOME_TYPE* and *CODE_GENDER*,
    good to know.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 从路径中我们可以注意到，这里按*NAME_INCOME_TYPE*和*CODE_GENDER*进行分区，知道这一点很重要。
- en: 'To read this path now to get the number of rows and columns and also the precious
    *Schema* here is what you can do:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在要读取这个路径以获取行数和列数，以及宝贵的*Schema*，你可以这样做：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/28fe5e017ccb8531099e2f2908ff3087.png)![](../Images/574909a9cf4377be5dd29afccbf17a8f.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/28fe5e017ccb8531099e2f2908ff3087.png)![](../Images/574909a9cf4377be5dd29afccbf17a8f.png)'
- en: It took less than 1 second to run, the reason is that the `read_table()` function
    reads a *Parquet* file and returns a *PyArrow Table* object, which represents
    your data as an optimized data structure developed by *Apache Arrow*.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 执行时间不到1秒，原因是`read_table()`函数读取一个*Parquet*文件并返回一个*PyArrow Table*对象，该对象代表你的数据，作为一个由*Apache
    Arrow*开发的优化数据结构。
- en: Now, we know that there are 637800 rows and 17 columns (+2 coming from the path),
    and have an overview of the variables and their types.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们知道有637800行和17列（+2来自路径），并对变量及其类型有了概览。
- en: Wait, I told you before that we won’t need to load anything in memory to discover
    the data. So here is a method to do it without reading any table.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 等等，我之前告诉过你，我们不需要在内存中加载任何东西来发现数据。所以这里有一个方法，可以在不读取任何表的情况下做到这一点。
- en: '**Metadata**'
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**元数据**'
- en: I am partially tricking you because we won’t be loading any data but we will
    be loading something called **metadata**.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我在部分欺骗你，因为我们不会加载任何数据，而是会加载所谓的**元数据**。
- en: In the context of the *Parquet* file format, **metadata** refers to data that
    describes the *structure* and *characteristics* of the data stored in the file.
    This includes information such as the data types of each column, the names of
    the columns, the number of rows in the table, and the schema.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Parquet*文件格式的上下文中，**metadata**指的是描述文件中存储的数据的*结构*和*特征*的数据。这包括每列的数据类型、列的名称、表中的行数和模式等信息。
- en: 'Let’s use both `read_metadata()` and `read_schema()` function from *pyarrow.parquet*:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用*pyarrow.parquet*中的`read_metadata()`和`read_schema()`函数：
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/5036cc5bf641ce4f28e2b9e1c462cdfe.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5036cc5bf641ce4f28e2b9e1c462cdfe.png)'
- en: This is giving you the same output as with `read_table()`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这会给你与`read_table()`相同的输出。
- en: However, we notice that in the execution times there is a big difference because
    here it is close to instantaneous. And this is not a surprise because reading
    the **metadata** is like reading a very small part of the *Parquet* file that
    contains everything you need to have an overview of the data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们注意到执行时间上有很大差异，因为这里接近瞬时。这并不奇怪，因为读取**metadata**就像是读取*Parquet*文件中一个非常小的部分，它包含了你需要的所有信息来概述数据。
- en: Statistics
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 统计数据
- en: Now let’s say I want to know a little more about the columns what can I do?
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我想多了解一下列，我该怎么办？
- en: You could read the statistics from the first *Row Group* of a file.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从文件的第一个*Row Group*读取统计数据。
- en: A *Row Group* in the *Parquet* file format is a collection of rows that are
    stored together as a unit and divided into smaller chunks for efficient querying
    and processing.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Parquet*文件格式中，*Row Group*是将行作为一个单位存储在一起的集合，并分成更小的块以便于查询和处理。
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This code above will give you an ugly output, here is some code to format it
    into a beautiful DataFrame:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码会给你一个不太美观的输出，这里有一些代码可以将其格式化为一个漂亮的DataFrame：
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/bc143c03768b4a4c06daabdcd3fc1434.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc143c03768b4a4c06daabdcd3fc1434.png)'
- en: 'On the DataFrame you have the type of the columns, the minimum, the maximum,
    and the compressed size. Few learnings from this file:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在DataFrame中，你可以看到列的类型、最小值、最大值和压缩大小。从这个文件中得到的一些学习点：
- en: Strings columns were converted to *BYTE_ARRAY*.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字符串列被转换为*BYTE_ARRAY*。
- en: Min and Max for String columns are sorted alphabetically.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字符串列的最小值和最大值按字母顺序排序。
- en: The compression size of Boolean is not so much better than the *BYTE_ARRAY*.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 布尔型的压缩大小不比*BYTE_ARRAY*好多少。
- en: The youngest applicant is 21 years old, and the oldest is 68 years old.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最年轻的申请者21岁，最年长的是68岁。
- en: Becare to not generalize the statistics, it is just from the first *parquet*
    file!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 要注意不要将统计数据泛化，这只是来自第一个*parquet*文件！
- en: Great, now we have a good understanding of the data such as info on columns,
    types, schemas, and even statistics, but don’t we miss something?
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，现在我们对数据有了很好的理解，包括列的信息、类型、模式，甚至统计数据，但我们是否遗漏了什么？
- en: Partitions
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分区
- en: 'Yes, we don’t know the *partitions* of the data! As said before, we could guess
    at least the partitioning columns from the file path:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，我们不知道数据的*分区*！如前所述，我们可以从文件路径中至少猜测到分区列：
- en: '![](../Images/d6c24006151705c154c2edf0a52a3fc2.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6c24006151705c154c2edf0a52a3fc2.png)'
- en: the data is partitioned on *NAME_INCOME_TYPE* and *CODE_GENDER*. But we don’t
    know the other partition values. Suppose that we want to look at other *NAME_INCOME_TYPE*?
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 数据按*NAME_INCOME_TYPE*和*CODE_GENDER*分区。但我们不知道其他分区值。假设我们想查看其他*NAME_INCOME_TYPE*？
- en: 'But I’ll provide you a code so you can get the *partitions* in a more systemic
    way but also all the values possible for the *partitions*:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 但我会提供你一段代码，这样你可以以更系统的方式获取*分区*，以及所有可能的*分区*值：
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Let’s run this function that returns a dictionary with the keys corresponding
    to the *partitions columns,* and values are the associated *partitions values*
    to each partition column.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行这个函数，它返回一个字典，其中键对应于*分区列*，值是与每个分区列关联的*分区值*。
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We know now that the Data Engineer partitioned the data first by *Income_Type*
    and then by *Gender*. And all the values for the partition columns are listed
    below:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在知道数据工程师首先按*Income_Type*分区，然后按*Gender*分区。所有分区列的值如下：
- en: '![](../Images/20b37f7d962052bb684ae8055d32f3c8.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/20b37f7d962052bb684ae8055d32f3c8.png)'
- en: Now that we have the partition columns and values knowledge, we can read another
    partition of our interest.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经了解了分区列和分区值的知识，我们可以读取另一个感兴趣的分区。
- en: Suppose that we want to read all the data of *‘Pensioner’* regardless of *Gender*.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想读取所有*‘Pensioner’*的数据，无论*Gender*是什么。
- en: From the last tutorial, we know that we can do that by reading the Parquet folder
    *‘APPLICATIONS_PARTITIONED/NAME_INCOME_TYPE=Pensioner’*
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 从上一个教程中，我们知道我们可以通过读取 Parquet 文件夹*‘APPLICATIONS_PARTITIONED/NAME_INCOME_TYPE=Pensioner’*来做到这一点
- en: '[PRE9]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Reformat the partitions
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新格式化分区
- en: Actually, we have no interest in splitting the data by gender and the size of
    the data allows us to read data from both genders without an excessive runtime.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们不打算按性别拆分数据，而且数据的大小允许我们在没有过多运行时间的情况下读取两个性别的数据。
- en: It is important to not over-partition your data because, in general, the execution
    time is increased by the number of partitions within the folder. So you have to
    keep in mind that there is a potential downside to partitions, even if it makes
    the data more functionally readable. (From the [official documentation](https://parquet.apache.org/docs/file-format/configurations/)
    512MB — 1GB is the optimal size for a partition).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 不要过度分区数据，因为通常，执行时间会随着文件夹中的分区数量增加而增加。因此，你必须记住，分区即使使数据在功能上更易读，也可能有潜在的缺点。（来自[官方文档](https://parquet.apache.org/docs/file-format/configurations/)
    512MB — 1GB 是分区的最佳大小）。
- en: 'Here, let’s say we assume that the subfolders of the genders are small enough
    after checking the data, and we find that the functional division of the genders
    is not useful. We decide to reformat the dataset to be partitioned only by *NAME_INCOME_TYPE*:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，假设在检查数据后，我们认为性别的子文件夹足够小，并且发现性别的功能划分没有用处。我们决定将数据集重新格式化，仅按*NAME_INCOME_TYPE*进行分区：
- en: '[PRE10]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We just read the data in *PyArrow Table* object then we wrote a *Parquet* filepartitioned
    only on *NAME_INCOME_TYPE* and no more per *Gender*. If we run now the function
    `get_all_partitions()` with the values:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚在*PyArrow Table* 对象中读取了数据，然后我们写了一个*Parquet* 文件，仅按*NAME_INCOME_TYPE*分区，不再按*性别*分区。如果我们现在运行`get_all_partitions()`函数，值为：
- en: '[PRE11]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/b2f89baa600c8886b3f42982505e126c.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2f89baa600c8886b3f42982505e126c.png)'
- en: We notice that we don’t have the partitioning by *Gender* anymore.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，我们不再按*性别*进行分区。
- en: 'In conclusion, you have just seen how to navigate through Parquet files to
    know everything about the data before loading it: like column names, size, schema,
    statistics and how to get partition names and values. You also found out how to
    reformat the partitions to be more technically and functionally correct.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，你刚刚了解了如何浏览 Parquet 文件，了解有关数据的一切：例如列名、大小、模式、统计信息以及如何获取分区名称和值。你还发现了如何重新格式化分区以使其在技术上和功能上更为正确。
- en: Thanks for reading and see you in the next story!
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读，下次故事见！
- en: 'The full code to generate the input data we used:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 生成我们所用输入数据的完整代码：
- en: '*Continue your learning with my other Parquet articles:*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*继续阅读我其他的 Parquet 文章：*'
- en: '[](/easy-parquet-tutorial-best-practices-237955e46cb7?source=post_page-----f854c57a45b6--------------------------------)
    [## Simple Parquet Tutorial and Best Practices'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/easy-parquet-tutorial-best-practices-237955e46cb7?source=post_page-----f854c57a45b6--------------------------------)
    [## 简单的 Parquet 教程和最佳实践'
- en: Hands-on tutorial for starting your Parquet learning
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实用教程，开始你的 Parquet 学习
- en: 'towardsdatascience.com](/easy-parquet-tutorial-best-practices-237955e46cb7?source=post_page-----f854c57a45b6--------------------------------)
    [](https://pub.towardsai.net/parquet-best-practices-the-art-of-filtering-d729357e441d?source=post_page-----f854c57a45b6--------------------------------)
    [## Parquet Best Practices: The Art of Filtering'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/easy-parquet-tutorial-best-practices-237955e46cb7?source=post_page-----f854c57a45b6--------------------------------)
    [](https://pub.towardsai.net/parquet-best-practices-the-art-of-filtering-d729357e441d?source=post_page-----f854c57a45b6--------------------------------)
    [## Parquet 最佳实践：筛选的艺术
- en: Understanding how to filter Parquet files
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解如何筛选 Parquet 文件
- en: pub.towardsai.net](https://pub.towardsai.net/parquet-best-practices-the-art-of-filtering-d729357e441d?source=post_page-----f854c57a45b6--------------------------------)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: pub.towardsai.net](https://pub.towardsai.net/parquet-best-practices-the-art-of-filtering-d729357e441d?source=post_page-----f854c57a45b6--------------------------------)
- en: '*With no extra costs, you can subscribe to Medium via my referral link.*'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*通过我的推荐链接，你可以无额外费用地订阅 Medium。*'
- en: '[](https://medium.com/@arli94/membership?source=post_page-----f854c57a45b6--------------------------------)
    [## Join Medium with my referral link — Arli'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@arli94/membership?source=post_page-----f854c57a45b6--------------------------------)
    [## 使用我的推荐链接加入 Medium — Arli'
- en: Read every story from Arli and thousands of other writers on Medium. Your membership
    fee directly supports Arli and…
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阅读 Arli 和成千上万其他 Medium 作者的每一个故事。你的会员费用直接支持 Arli 和…
- en: medium.com](https://medium.com/@arli94/membership?source=post_page-----f854c57a45b6--------------------------------)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/@arli94/membership?source=post_page-----f854c57a45b6--------------------------------)'
