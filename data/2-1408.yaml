- en: Learning Transformers Code First Part 2 — GPT Up Close and Personal
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习变压器代码第一部分第2部分——GPT亲密接触
- en: 原文：[https://towardsdatascience.com/learning-transformers-code-first-part-2-gpt-up-close-and-personal-1635b52ae0d7](https://towardsdatascience.com/learning-transformers-code-first-part-2-gpt-up-close-and-personal-1635b52ae0d7)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/learning-transformers-code-first-part-2-gpt-up-close-and-personal-1635b52ae0d7](https://towardsdatascience.com/learning-transformers-code-first-part-2-gpt-up-close-and-personal-1635b52ae0d7)
- en: Digging into Generative Pre-Trained Transformers via nanoGPT
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入研究通过nanoGPT生成预训练变压器
- en: '[](https://medium.com/@oaguy1?source=post_page-----1635b52ae0d7--------------------------------)[![Lily
    Hughes-Robinson](../Images/b610721a40e274e7fb81418395314ae3.png)](https://medium.com/@oaguy1?source=post_page-----1635b52ae0d7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1635b52ae0d7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1635b52ae0d7--------------------------------)
    [Lily Hughes-Robinson](https://medium.com/@oaguy1?source=post_page-----1635b52ae0d7--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[Lily Hughes-Robinson](https://medium.com/@oaguy1?source=post_page-----1635b52ae0d7--------------------------------)[![Lily
    Hughes-Robinson](../Images/b610721a40e274e7fb81418395314ae3.png)](https://medium.com/@oaguy1?source=post_page-----1635b52ae0d7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1635b52ae0d7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1635b52ae0d7--------------------------------)
    [Lily Hughes-Robinson](https://medium.com/@oaguy1?source=post_page-----1635b52ae0d7--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1635b52ae0d7--------------------------------)
    ·13 min read·Jul 13, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1635b52ae0d7--------------------------------)
    ·13分钟阅读·2023年7月13日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f90478c1b96dad6cd2667bfcf0da1f03.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f90478c1b96dad6cd2667bfcf0da1f03.png)'
- en: Photo by [Luca Onniboni](https://unsplash.com/it/@lucaonniboni?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Luca Onniboni](https://unsplash.com/it/@lucaonniboni?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Welcome to the second part of my project, where I delve into the intricacies
    of transformer and GPT-based models using the [TinyStories dataset](https://huggingface.co/datasets/roneneldan/TinyStories)
    and [nanoGPT](https://github.com/karpathy/nanoGPT/tree/master) all trained on
    an aging gaming laptop. In the first part, I prepared the dataset for input into
    a character-level generative model. You can find a link to part one below.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到我的项目的第二部分，在这里我将深入研究变压器和基于GPT的模型的复杂性，使用 [TinyStories数据集](https://huggingface.co/datasets/roneneldan/TinyStories)
    和 [nanoGPT](https://github.com/karpathy/nanoGPT/tree/master)，所有这些都在一台老旧的游戏笔记本电脑上进行训练。在第一部分中，我准备了输入到字符级生成模型的数据集。你可以在下面找到第一部分的链接。
- en: '[](/nanogpt-learning-transformers-code-first-part-1-f2044cf5bca0?source=post_page-----1635b52ae0d7--------------------------------)
    [## Learning Transformers Code First Part 1'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[链接](https://towardsdatascience.com/nanogpt-learning-transformers-code-first-part-1-f2044cf5bca0?source=post_page-----1635b52ae0d7--------------------------------)
    [## 学习变压器代码第一部分'
- en: Part 1 of a new series where I endeavor to learn transformers code first using
    nanoGPT
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 新系列的第一部分，我努力使用nanoGPT学习变压器代码
- en: towardsdatascience.com](/nanogpt-learning-transformers-code-first-part-1-f2044cf5bca0?source=post_page-----1635b52ae0d7--------------------------------)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[链接](https://towardsdatascience.com/nanogpt-learning-transformers-code-first-part-1-f2044cf5bca0?source=post_page-----1635b52ae0d7--------------------------------)'
- en: In this article, I aim to dissect the GPT model, its components, and its implementation
    in nanoGPT. I selected nanoGPT due to its straightforward Python implementation
    of a GPT model, which is approximately 300 lines long, and its similarly digestible
    training script. With the necessary background knowledge, one could quickly comprehend
    GPT models from simply reading the source code. To be frank, I lacked this understanding
    when I first examined the code. Some of the material still eludes me. However,
    I hope that with all I’ve learned, this explanation will provide a starting point
    for those wishing to gain an intuitive understanding of how GPT-style models function
    internally.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我的目标是剖析GPT模型、其组件及其在nanoGPT中的实现。我选择了nanoGPT，因为它是一个约300行代码的GPT模型的直接Python实现，且其训练脚本也同样易于理解。掌握必要的背景知识后，人们可以通过阅读源代码快速理解GPT模型。坦白说，当我第一次检查代码时，我缺乏这种理解。有些材料仍然让我感到困惑。然而，我希望凭借我所学到的知识，这个解释能够为那些希望直观理解GPT风格模型内部运作的人提供一个起点。
- en: n preparation for this article, I read various papers. Initially, I assumed
    that simply reading the seminal work “[Attention is All You Need](https://arxiv.org/abs/1706.03762)”
    would suffice to bring my understanding up to speed. This was a naive assumption.
    While it’s true that this paper introduced the transformer model, it was subsequent
    papers that adapted it for more advanced tasks such as text generation. “AIAYN”
    was merely an introduction to a broader topic. Undeterred, I recalled an article
    on HackerNews that provided a reading list to fully understand LLMs. After a quick
    search, I found the article [here](https://sebastianraschka.com/blog/2023/llm-reading-list.html).
    I didn’t read everything in sequence, but I intend to revisit this reading list
    to continue my learning journey after completing this series.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在为这篇文章做准备时，我阅读了各种论文。最初，我以为仅仅阅读奠基性论文“[Attention is All You Need](https://arxiv.org/abs/1706.03762)”就足以让我对这个领域有足够的了解。这是一个天真的假设。虽然这篇论文确实介绍了变换器模型，但后续的论文才将其适应于更高级的任务，如文本生成。“AIAYN”
    只是一个更广泛主题的介绍。尽管如此，我想起了一篇 HackerNews 上的文章，其中提供了一个阅读清单以全面理解大语言模型。经过快速搜索，我找到了这篇文章[在这里](https://sebastianraschka.com/blog/2023/llm-reading-list.html)。我没有按顺序阅读所有内容，但我打算在完成这一系列之后重新审视这个阅读清单，继续我的学习之旅。
- en: With that said, let’s dive in. To comprehend GPT models in detail, we must start
    with the transformer. The transformer employs a self-attention mechanism known
    as scaled dot-product attention. The following explanation is derived from this
    [insightful article on scaled dot-product attention,](https://peterbloem.nl/blog/transformers)
    which I recommend for a more in-depth understanding. Essentially, for every element
    of an input sequence (the *i-th* element), we want to multiply the input sequence
    by a weighted average of all the elements in the sequence with the *i-th* element.
    These weights are calculated via taking the dot-product of the vector at the *i-th*
    element with the entire input vector and then applying a softmax to it so the
    weights are values between 0 and 1\. In the original “Attention is All You Need”
    paper, these inputs are named **query (**the entire sequence**), key** (the vector
    at the *i-th* element) and the **value** (also the whole sequence). The weights
    passed to the attention mechanism are initialized to random values and learned
    as more passes occur within a neural network.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，让我们开始吧。要详细理解 GPT 模型，我们必须从变换器开始。变换器使用一种称为缩放点积注意力的自注意力机制。以下解释来源于这篇[关于缩放点积注意力的深刻文章](https://peterbloem.nl/blog/transformers)，我推荐阅读，以便更深入地理解。基本上，对于输入序列的每个元素（第
    *i* 个元素），我们希望将输入序列乘以一个加权平均值，该加权平均值由序列中所有元素与第 *i* 个元素的点积计算得出，然后应用 softmax，以使权重值在
    0 和 1 之间。在原始的“Attention is All You Need”论文中，这些输入被称为**查询**（整个序列）、**键**（第 *i* 个元素的向量）和**值**（也是整个序列）。传递给注意力机制的权重被初始化为随机值，并随着神经网络中更多的迭代而学习。
- en: nanoGPT implements scaled dot-product attention and extends it to multi-head
    attention, meaning multiple attention operations occurring at once. It also implements
    it as a `torch.nn.Module`, which allows it to be composed with other network layers
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: nanoGPT 实现了缩放点积注意力，并将其扩展为多头注意力，这意味着同时进行多个注意力操作。它还将其实现为 `torch.nn.Module`，这使得它可以与其他网络层组合。
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s dissect this code further, starting with the constructor. First, we verify
    that the number of attention heads (`n_heads`) divides the dimensionality of the
    embedding (`n_embed`) evenly. This is crucial because when the embedding is divided
    into sections for each head, we want to cover all of the embedding space without
    any gaps. Next, we initialize two Linear layers, `c_att` and `c_proj`: `c_att`
    is the layer that holds all our working space for the matrices that compose of
    a scaled dot-product attention calculation while `c_proj` stores the finally result
    of the calculations. The embedding dimension is tripled in `c_att` because we
    need to include space for the three major components of attention: **query**,
    **key**, and **value**.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进一步剖析这段代码，从构造函数开始。首先，我们验证注意力头的数量（`n_heads`）是否能整除嵌入的维度（`n_embed`）。这非常重要，因为当嵌入被分成每个头的部分时，我们希望覆盖所有的嵌入空间而没有任何空隙。接下来，我们初始化两个线性层，`c_att`
    和 `c_proj`：`c_att` 是保存所有用于计算缩放点积注意力的矩阵的工作空间的层，而 `c_proj` 存储计算的最终结果。在 `c_att` 中，嵌入维度被放大三倍，因为我们需要为注意力的三个主要组成部分**查询**、**键**和**值**留出空间。
- en: We also have two dropout layers, `attn_dropout`and `resid_dropout`. The dropout
    layers randomly nullify elements of the input matrix based on a given probability.
    According to the [PyTorch docs](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout),
    this serves the purpose of reducing overfitting for the model. The value in `config.dropout`
    is the probability that a given sample will be dropped during a dropout layer.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还设置了两个丢弃层，`attn_dropout`和`resid_dropout`。丢弃层根据给定的概率随机使输入矩阵的元素失效。根据[PyTorch
    文档](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout)，这旨在减少模型的过拟合。`config.dropout`中的值是指定样本在丢弃层中被丢弃的概率。
- en: We finalize the constructor by verifying if the user has access to PyTorch 2.0,
    which boasts an optimized version of the scaled dot-product attention. If available,
    the class utilizes it; otherwise we set up a bias mask. This mask is a component
    of the optional masking feature of the attention mechanism. The [torch.tril](https://pytorch.org/docs/stable/generated/torch.tril.html)
    method yields a matrix with its upper triangular section converted to zeros. When
    combined with the torch.ones method, it effectively generates a mask of 1s and
    0s that the attention mechanism uses to produce anticipated outputs for a given
    sampled input.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过验证用户是否可以访问 PyTorch 2.0 来完成构造函数，PyTorch 2.0 拥有优化版本的缩放点积注意力。如果可用，类将使用它；否则，我们将设置一个偏置掩码。这个掩码是注意力机制的可选掩码功能的一部分。[torch.tril](https://pytorch.org/docs/stable/generated/torch.tril.html)
    方法生成一个将上三角部分转换为零的矩阵。当与 torch.ones 方法结合使用时，它有效地生成一个由1和0组成的掩码，注意力机制使用这个掩码生成给定样本输入的预期输出。
- en: 'Next, we delve into the `forward` method of the class, where the attention
    algorithm is applied. Initially, we determine the sizes of our input matrix and
    divide it into three dimensions: **B**atch size, **T**ime (or number of samples),
    **C**orpus (or embedding size). nanoGPT employs a batched learning process, which
    we will explore in greater detail when examining the transformer model that utilizes
    this attention layer. For now, it’s sufficient to understand that we are dealing
    with the data in batches. We then feed the input `x` into the linear transformation
    layer `c_attn` which expands the dimensionality from `n_embed` to three times
    `n_embed`. The output of that transformation is split it into our `q`, `k`, `v`
    variables which are our inputs to the attention algorithm. Subsequently, the `view`
    method is utilized to reorganize the data in each of these variables into the
    format expected by the PyTorch `scaled_dot_product_attention` function.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们深入研究类的`forward`方法，其中应用了注意力算法。最初，我们确定输入矩阵的大小，并将其分成三个维度：**B**atch大小，**T**ime（或样本数量），**C**orpus（或嵌入大小）。nanoGPT
    采用批处理学习过程，我们将在检查使用此注意力层的变换器模型时详细探讨。现在，了解我们处理的是批量数据就足够了。然后，我们将输入`x`传递到线性变换层`c_attn`，它将维度从`n_embed`扩展到三倍的`n_embed`。该变换的输出被分割成我们的`q`、`k`和`v`变量，这些变量是注意力算法的输入。随后，`view`方法被用来将这些变量中的数据重新组织成
    PyTorch `scaled_dot_product_attention` 函数期望的格式。
- en: When the optimized function isn’t available, the code defaults to a manual implementation
    of scaled dot-product attention. It begins by taking the dot product of the `q`
    and `k` matrices, with `k` transposed to fit the dot product function, and the
    result is scaled by the square root of the size of `k`. We then mask the scaled
    output using the previously created bias buffer, replacing the 0s with negative
    infinity. Next, a [softmax function](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html)
    is applied to the `att` matrix, converting the negative infinities back to 0s
    and ensuring all other values are scaled between 0 and 1\. We then apply a dropout
    layer to avoid overfitting before getting the dot-product of the `att` matrix
    and `v`.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当优化后的函数不可用时，代码会默认使用手动实现的缩放点积注意力。它首先计算`q`和`k`矩阵的点积，其中`k`被转置以适应点积函数，然后结果会按`k`的大小的平方根进行缩放。接着，我们使用先前创建的偏置缓冲区来遮蔽缩放后的输出，将0替换为负无穷。接下来，对`att`矩阵应用[softmax
    函数](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html)，将负无穷转换回0，并确保所有其他值在0和1之间缩放。然后，我们应用一个丢弃层以避免过拟合，然后计算`att`矩阵和`v`的点积。
- en: Regardless of the scaled dot-product implementation used, the multi-head output
    is reorganized side by side before passing it through a final dropout layer and
    then returning the result. This is the complete implementation of the attention
    layer in less than 50 lines of Python/PyTorch. If you don’t fully comprehend the
    above code, I recommend spending some time reviewing it before proceeding with
    the rest of the article.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 无论使用何种缩放点积实现，多头输出都会在通过最终的 dropout 层之前重新组织成并排的形式，然后返回结果。这是在不到 50 行的 Python/PyTorch
    代码中完成的注意力层实现。如果你没有完全理解上述代码，建议花些时间复习它，然后再继续阅读文章的其余部分。
- en: Before we delve into the GPT module, which integrates everything, we require
    two more building blocks. The first is a simple multi-layer perceptron (MLP) —
    referred to in the “Attention is All You Need” paper as a feed-forward network
    — and the attention block, which combines the attention layer with an MLP to complete
    the basic transformer architecture represented in the paper. Both are implemented
    in the following snippet from nanoGPT.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入了解整合所有内容的 GPT 模块之前，我们需要两个额外的构建块。第一个是简单的多层感知机（MLP）——在“Attention is All You
    Need”论文中被称为前馈网络——和注意力块，它将注意力层与 MLP 结合在一起，完成论文中表示的基本 transformer 架构。两者都在 nanoGPT
    的以下代码片段中实现。
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The MLP layer, despite its apparent simplicity in terms of code lines, adds
    an extra layer of complexity to the model. Essentially, the Linear layers link
    each input layer with each element of the output layer, using a linear transformation
    to transfer the values between them. In the aforementioned code, we start with
    the embedding size, `n_embed`, as the number of parameters before quadrupling
    it in the output. The quadrupling here is arbitrary; the purpose of the MLP module
    is to enhance the network’s computation by adding more nodes. As long as the dimensionality
    increase at the beginning of the MLP and decrease at the end of the MLP is equivalent,
    yielding the same initial input/final output dimension, then the scaling number
    is merely another hyper-parameter. Another crucial element to consider is the
    activation function. This MLP implementation consists of two linear layers connected
    with the GELU activation function. The original paper uses a ReLU function, but
    nanoGPT employs GELU to ensure compatibility with GPT2 model checkpoints.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: MLP 层，尽管在代码行数上看似简单，却给模型增加了一层额外的复杂性。实质上，Linear 层通过线性变换将每个输入层的元素与输出层的每个元素连接起来，传递它们之间的值。在上述代码中，我们从嵌入大小`n_embed`开始，作为参数的数量，然后在输出中将其四倍增加。这里的四倍增加是任意的；MLP
    模块的目的是通过增加更多节点来增强网络的计算能力。只要在 MLP 开始时维度增加和在 MLP 结束时维度减少是等效的，得到相同的初始输入/最终输出维度，那么缩放数只是另一个超参数。另一个关键要素是激活函数。这种
    MLP 实现包括两个线性层，使用 GELU 激活函数连接起来。原始论文使用的是 ReLU 函数，但 nanoGPT 使用 GELU 以确保与 GPT2 模型检查点的兼容性。
- en: Next, we examine the Block module. This module finalizes our transformer block
    as outlined in the “Attention” paper. Essentially, it channels the input through
    a normalization layer before passing it to the attention layer, then adds the
    result back to the input. The output of this addition is normalized once more
    before being transferred to the MLP, and then added back to itself. This process
    implements the decoder side of the transformer as described in the “Attention”
    paper. For text generation, it’s common to use only a decoder, as it doesn’t need
    to condition the decoder’s output on anything other than the input sequence. The
    transformer was initially designed for machine translation, which needs to account
    for both the input token encoding and the output token encoding. However, with
    text generation, only a single token encoding is used, eliminating the need for
    cross-attention via an encoder. Andrej Karpathy, the author of nanoGPT, provides
    a comprehensive explanation of this [in his video](https://youtu.be/kCc8FmEb1nY?t=6161)
    linked in the first article in this series.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来研究 Block 模块。这个模块完成了我们在“Attention”论文中概述的变压器块。基本上，它通过一个归一化层将输入传递，然后传递到注意力层，再将结果加回输入中。这个加法的输出被再次归一化，然后传递到
    MLP，并再次加回到自身。这一过程实现了“Attention”论文中描述的变压器解码器部分。在文本生成中，通常只使用解码器，因为它不需要根据输入序列以外的内容来调整解码器的输出。变压器最初设计用于机器翻译，这需要考虑输入标记编码和输出标记编码。然而，在文本生成中，只使用一个标记编码，从而消除了通过编码器进行交叉注意力的需求。nanoGPT
    的作者 Andrei Karpathy 在他的视频中提供了这一点的全面解释，[链接在此](https://youtu.be/kCc8FmEb1nY?t=6161)，视频地址在本系列的第一篇文章中。
- en: 'Finally, we reach the main component: the GPT model. The majority of the approximately
    300-line file is dedicated to the GPT module. It manages beneficial features such
    as model fine-tuning and utilities designed for model training (the topic of the
    next article in this series). Therefore, I present a simplified version of what
    is available in the nanoGPT repository below.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们进入主要组件：GPT 模型。大约 300 行的文件中大部分内容都用于 GPT 模块。它管理一些有益的功能，如模型微调和为模型训练设计的实用工具（这是本系列下一篇文章的主题）。因此，我在下面展示了
    nanoGPT 存储库中可用的简化版本。
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let’s begin with the constructor of the class. The different layers are assembled
    into a PyTorch ModuleDict, which provides some structure. We start with two Embedding
    layers: one for token embedding and one for positional embedding. The `nn.Embedding`
    module is designed to be sparsely populated with values, optimizing it storage
    capabilities over other layer modules. Following this, we have a dropout layer,
    succeeded by `n_layer` number of Block modules that form our attention layers,
    and then another single dropout layer. The `lm_head` Linear layer takes the output
    from the attention Blocks and reduces it to the vocab size, acting as our main
    output for the GPT, apart from the loss value.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从类的构造函数开始。不同的层被组装到 PyTorch 的 ModuleDict 中，这提供了一些结构。我们从两个 Embedding 层开始：一个用于标记嵌入，另一个用于位置嵌入。`nn.Embedding`
    模块旨在稀疏地填充值，相比其他层模块，它优化了存储能力。接下来是一个 dropout 层，然后是 `n_layer` 个 Block 模块，这些模块构成了我们的注意力层，然后是另一个单独的
    dropout 层。`lm_head` Linear 层将注意力 Blocks 的输出减少到词汇表大小，作为 GPT 的主要输出，除了损失值之外。
- en: Once the layers are defined, additional setup is required before we can begin
    training the module. Here, Andrej links the weights of the positional encoding
    to those of the output layer. According to [the paper linked in the code comments](https://paperswithcode.com/method/weight-tying),
    this is done to reduce the model’s final parameters while also improving its performance.
    The constructor also initializes the model’s weights. As these weights will be
    learned during training, they are initialized to Gaussian distribution of random
    numbers and the module biases are set to 0\. Finally, a modification from the
    [GPT-2 paper](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe)
    is utilized where the weights of any residual layers are scaled by square root
    of the number of layers.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦层定义完成，在我们开始训练模块之前，还需要进行额外的设置。在这里，Andrej将位置编码的权重链接到输出层的权重上。根据[代码注释中链接的论文](https://paperswithcode.com/method/weight-tying)，这样做是为了减少模型的最终参数，同时提高其性能。构造函数还会初始化模型的权重。由于这些权重将在训练过程中学习，它们被初始化为高斯分布的随机数，并且模块的偏置被设置为0。最后，利用了来自[GPT-2论文](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe)的修改，其中任何残差层的权重都按层数的平方根进行缩放。
- en: When feeding forward through the network, **b**atch size and the number of samples
    (here `t`) are pulled from the input size. We then create memory on the training
    device for what will become the **pos**itional embedding. Next, we embed the input
    tokens into a token embedding later `wte`. Following this, the positional embedding
    is calculated on the `wpe` layer. These embeddings are added together before being
    passed through a dropout layer. The result is then passed through each of the
    `n_layer` blocks and normalized. The final result is passed to the Linear layer
    `lm_head` which reduces the embedded weights into a probability score for each
    token in a vocab.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在通过网络进行前向传播时，**batch**大小和样本数量（此处为`t`）从输入大小中提取。然后，我们在训练设备上为将成为**位置**嵌入的内容创建内存。接着，我们将输入标记嵌入到后续的标记嵌入中`wte`。随后，在`wpe`层计算位置嵌入。这些嵌入在经过丢弃层之前会被相加。结果会通过每一个`n_layer`块进行处理并进行归一化。最后的结果传递到线性层`lm_head`，该层将嵌入权重转换为每个词汇表中标记的概率评分。
- en: When a loss is being calculated (e.g., during training), we calculate the difference
    between the predicted token and the actual token using cross-entropy. If not,
    loss is `None`. Both the loss and the token probabilities are returned as part
    of the feed forward function.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算损失时（例如，在训练期间），我们通过交叉熵计算预测标记与实际标记之间的差异。如果没有损失，则为`None`。损失和标记概率都会作为前向传播函数的一部分返回。
- en: Unlike the previous modules, the GPT module has additional methods. The most
    relevant to us is the generate function, which will be familiar to anyone who
    has used a generative model before. Given a set of input tokens `idx`, a number
    of `max_new_tokens` and a `temperature`, it generates `max_new_tokens` many tokens.
    Let’s delve into how it accomplishes this. First it trims the input tokens to
    fit within the `block_size` (others call this context length), if necessary, sampling
    from the end of the input first. Next, the tokens are fed to the network and the
    output is scaled for the inputted `temperature`. The higher the temperature, the
    more creative and likely to hallucinate the model is. Higher temperatures also
    result in less predictable output. Next, a softmax is applied to convert the model
    output weights into probabilities between 0 and 1\. A sampling function is used
    to select the next token from the probabilities, and that token is added to the
    input vector that gets fed back into the GPT model for the next character.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的模块不同，GPT模块有额外的方法。对我们最相关的是生成函数，这对于任何使用过生成模型的人来说都很熟悉。给定一组输入标记`idx`、一个`max_new_tokens`数量和一个`temperature`，它会生成`max_new_tokens`个标记。让我们深入了解它是如何完成的。首先，它修剪输入标记以适应`block_size`（其他人称之为上下文长度），如有必要，从输入的末尾开始采样。接下来，这些标记被输入到网络中，输出会根据输入的`temperature`进行缩放。温度越高，模型越具创造性，也更容易产生幻觉。较高的温度还会导致输出更难预测。接着，应用softmax将模型输出权重转换为0到1之间的概率。然后使用采样函数从概率中选择下一个标记，并将该标记添加到输入向量中，该向量会被反馈到GPT模型中进行下一个字符的生成。
- en: Thank you for your patience in reading this comprehensive article. While examining
    annotated source code is a valuable method for understanding the function of a
    code segment, there’s no replacement for personally manipulating various parts
    and parameters of the code. In line with this, I’m providing a link to the complete
    `model.py` source code from the nanoGPT repository
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你耐心阅读这篇详尽的文章。虽然检查注释过的源代码是理解代码片段功能的宝贵方法，但亲自操作代码的各个部分和参数仍是无可替代的。为此，我提供了来自nanoGPT库的完整`model.py`源代码的链接。
- en: '[](https://github.com/karpathy/nanoGPT/blob/master/model.py?source=post_page-----1635b52ae0d7--------------------------------)
    [## nanoGPT/model.py at master · karpathy/nanoGPT'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/karpathy/nanoGPT/blob/master/model.py?source=post_page-----1635b52ae0d7--------------------------------)
    [## nanoGPT/model.py at master · karpathy/nanoGPT'
- en: The simplest, fastest repository for training/finetuning medium-sized GPTs.
    - nanoGPT/model.py at master ·…
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最简单、最快的中型GPT训练/微调库。 - nanoGPT/model.py at master ·…
- en: github.com](https://github.com/karpathy/nanoGPT/blob/master/model.py?source=post_page-----1635b52ae0d7--------------------------------)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/karpathy/nanoGPT/blob/master/model.py?source=post_page-----1635b52ae0d7--------------------------------)
- en: In the upcoming article, we’ll explore the `train.py` script of nanoGPT and
    train a character-level model on the TinyStories dataset. Follow me on Medium
    to ensure you don’t miss out!
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在即将发布的文章中，我们将探索nanoGPT的`train.py`脚本，并在TinyStories数据集上训练一个字符级模型。请在Medium上关注我，以确保不会错过！
- en: I utilized a vast array of resources to create this article, many of which have
    already been linked in this and the previous article. However, I would be neglecting
    my duty if I didn’t share these resources with you for further exploration of
    any topic or for alternative explanations of the concepts.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我利用了大量资源来创建这篇文章，其中许多资源已经在本文及上一篇文章中链接了。然而，如果我不与大家分享这些资源以便进一步探索任何话题或替代解释概念，我就会失职。
- en: '[Let’s Build GPT: from scratch, in code, spelled out](https://www.youtube.com/watch?v=kCc8FmEb1nY)
    — YouTube'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[《让我们从零开始构建GPT：代码实现详解》](https://www.youtube.com/watch?v=kCc8FmEb1nY) — YouTube'
- en: '[LLM Reading List](https://sebastianraschka.com/blog/2023/llm-reading-list.html)
    — Blog'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LLM阅读清单](https://sebastianraschka.com/blog/2023/llm-reading-list.html) — 博客'
- en: '[“Attention is All You Need”](https://arxiv.org/pdf/1706.03762.pdf?) — Paper'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“注意力机制是你需要的一切”](https://arxiv.org/pdf/1706.03762.pdf?) — 论文'
- en: '[“Language Models are Unsupervised Multitask Learners”](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe)
    — GPT-2 Paper'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“语言模型是无监督多任务学习者”](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe)
    — GPT-2论文'
- en: '[Multi-Layer Perceptrons Explained and Illustrated](/multi-layer-perceptrons-8d76972afa2b)
    — Medium'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[多层感知器解释和插图](/multi-layer-perceptrons-8d76972afa2b) — Medium'
- en: '[Weight Tying](https://paperswithcode.com/method/weight-tying) — Papers With
    Code'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[权重绑定](https://paperswithcode.com/method/weight-tying) — Papers With Code'
- en: '[Illustrated Guide to Transformers Neural Network: A step by step explanation](https://www.youtube.com/watch?v=4Bdc55j80l8)
    — YouTube'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[《变换器神经网络的插图指南：逐步解释》](https://www.youtube.com/watch?v=4Bdc55j80l8) — YouTube'
- en: '*Edited using GPT-4 and a custom LangChain script.*'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*使用GPT-4和自定义LangChain脚本编辑。*'
