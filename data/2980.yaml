- en: Retrieval-Augmented Generation (RAG)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）
- en: 原文：[https://towardsdatascience.com/add-your-own-data-to-an-llm-using-retrieval-augmented-generation-rag-b1958bf56a5a?source=collection_archive---------1-----------------------#2023-09-29](https://towardsdatascience.com/add-your-own-data-to-an-llm-using-retrieval-augmented-generation-rag-b1958bf56a5a?source=collection_archive---------1-----------------------#2023-09-29)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/add-your-own-data-to-an-llm-using-retrieval-augmented-generation-rag-b1958bf56a5a?source=collection_archive---------1-----------------------#2023-09-29](https://towardsdatascience.com/add-your-own-data-to-an-llm-using-retrieval-augmented-generation-rag-b1958bf56a5a?source=collection_archive---------1-----------------------#2023-09-29)
- en: Learn how to add your own proprietary data to a pre-trained LLM using a prompt-based
    technique called Retrieval-Augmented Generation
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习如何使用名为检索增强生成的基于提示的技术将自己的专有数据添加到预训练的语言模型
- en: '[](https://medium.com/@bea_684?source=post_page-----b1958bf56a5a--------------------------------)[![Beatriz
    Stollnitz](../Images/63a2a7daeca6d93e26b3ac0556c42aa1.png)](https://medium.com/@bea_684?source=post_page-----b1958bf56a5a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b1958bf56a5a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b1958bf56a5a--------------------------------)
    [Beatriz Stollnitz](https://medium.com/@bea_684?source=post_page-----b1958bf56a5a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bea_684?source=post_page-----b1958bf56a5a--------------------------------)[![Beatriz
    Stollnitz](../Images/63a2a7daeca6d93e26b3ac0556c42aa1.png)](https://medium.com/@bea_684?source=post_page-----b1958bf56a5a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b1958bf56a5a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b1958bf56a5a--------------------------------)
    [Beatriz Stollnitz](https://medium.com/@bea_684?source=post_page-----b1958bf56a5a--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1c8863892480&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadd-your-own-data-to-an-llm-using-retrieval-augmented-generation-rag-b1958bf56a5a&user=Beatriz+Stollnitz&userId=1c8863892480&source=post_page-1c8863892480----b1958bf56a5a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b1958bf56a5a--------------------------------)
    ·21 min read·Sep 29, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb1958bf56a5a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadd-your-own-data-to-an-llm-using-retrieval-augmented-generation-rag-b1958bf56a5a&user=Beatriz+Stollnitz&userId=1c8863892480&source=-----b1958bf56a5a---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1c8863892480&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadd-your-own-data-to-an-llm-using-retrieval-augmented-generation-rag-b1958bf56a5a&user=Beatriz+Stollnitz&userId=1c8863892480&source=post_page-1c8863892480----b1958bf56a5a---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b1958bf56a5a--------------------------------)
    ·21 分钟阅读·2023 年 9 月 29 日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb1958bf56a5a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadd-your-own-data-to-an-llm-using-retrieval-augmented-generation-rag-b1958bf56a5a&user=Beatriz+Stollnitz&userId=1c8863892480&source=-----b1958bf56a5a---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb1958bf56a5a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadd-your-own-data-to-an-llm-using-retrieval-augmented-generation-rag-b1958bf56a5a&source=-----b1958bf56a5a---------------------bookmark_footer-----------)![](../Images/cd0c3b0f21d72762c8a25dcf3ac3c039.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb1958bf56a5a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fadd-your-own-data-to-an-llm-using-retrieval-augmented-generation-rag-b1958bf56a5a&source=-----b1958bf56a5a---------------------bookmark_footer-----------)![](../Images/cd0c3b0f21d72762c8a25dcf3ac3c039.png)'
- en: Photo by [Joshua Sortino](https://unsplash.com/@sortino?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Joshua Sortino](https://unsplash.com/@sortino?utm_source=medium&utm_medium=referral)提供，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '**Introduction**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**简介**'
- en: Large Language Models (LLMs) know a lot about the world, but they don’t know
    everything. Since training these models takes a long time, the data they were
    last trained on can be pretty old. And although LLMs know about general-purpose
    facts available on the internet, they don’t know about your proprietary data,
    which is often the data you need in your AI-based application. So it’s no surprise
    that extending LLMs with new data has been a considerable area of focus lately,
    both in academia and in the industry.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）对世界了解很多，但并不是全部。由于训练这些模型需要很长时间，它们最后一次训练的数据可能已经相当陈旧。尽管LLMs了解互联网上的通用事实，但它们不了解你的专有数据，而这些数据通常是你在基于AI的应用中所需要的。因此，最近在学术界和工业界，扩展LLMs以包含新数据成为了一个重要的关注点也就不足为奇了。
- en: Before this new era of large language models, we would often extend models with
    new data by simply fine-tuning them. But now that our models are much larger and
    have been trained with much more data, fine-tuning is only appropriate for a few
    scenarios. Fine-tuning performs particularly well when we want to make our LLM
    communicate in a different style or tone. One great example of fine-tuning is
    OpenAI’s adaptation of their older completion-style GPT-3.5 models into their
    newer chat-style GPT-3.5-turbo (ChatGPT) models. Their completion-style models,
    if given the input “Can you tell me about…
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型语言模型的新时代之前，我们通常通过简单地微调模型来扩展新数据。但现在我们的模型更大，训练的数据也更多，微调只适用于少数情况。当我们想让LLM以不同的风格或语气进行交流时，微调表现尤其出色。一个很好的微调例子是OpenAI将其较老的完成风格GPT-3.5模型适配为较新的对话风格GPT-3.5-turbo（ChatGPT）模型。如果给完成风格模型输入“你能告诉我关于...
