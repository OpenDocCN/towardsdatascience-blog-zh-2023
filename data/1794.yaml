- en: Word Embeddings, Explained
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词嵌入，解读
- en: 原文：[https://towardsdatascience.com/word-embeddings-explained-c07c5ea44d64?source=collection_archive---------10-----------------------#2023-05-30](https://towardsdatascience.com/word-embeddings-explained-c07c5ea44d64?source=collection_archive---------10-----------------------#2023-05-30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/word-embeddings-explained-c07c5ea44d64?source=collection_archive---------10-----------------------#2023-05-30](https://towardsdatascience.com/word-embeddings-explained-c07c5ea44d64?source=collection_archive---------10-----------------------#2023-05-30)
- en: '[](https://medium.com/@dataemporium?source=post_page-----c07c5ea44d64--------------------------------)[![Ajay
    Halthor](../Images/1be821c8d8ed336b9ecedcf94f960ede.png)](https://medium.com/@dataemporium?source=post_page-----c07c5ea44d64--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c07c5ea44d64--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c07c5ea44d64--------------------------------)
    [Ajay Halthor](https://medium.com/@dataemporium?source=post_page-----c07c5ea44d64--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@dataemporium?source=post_page-----c07c5ea44d64--------------------------------)[![Ajay
    Halthor](../Images/1be821c8d8ed336b9ecedcf94f960ede.png)](https://medium.com/@dataemporium?source=post_page-----c07c5ea44d64--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c07c5ea44d64--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c07c5ea44d64--------------------------------)
    [Ajay Halthor](https://medium.com/@dataemporium?source=post_page-----c07c5ea44d64--------------------------------)'
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb0a3e7e495ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-explained-c07c5ea44d64&user=Ajay+Halthor&userId=b0a3e7e495ca&source=post_page-b0a3e7e495ca----c07c5ea44d64---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c07c5ea44d64--------------------------------)
    ·7 min read·May 30, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc07c5ea44d64&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-explained-c07c5ea44d64&user=Ajay+Halthor&userId=b0a3e7e495ca&source=-----c07c5ea44d64---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb0a3e7e495ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-explained-c07c5ea44d64&user=Ajay+Halthor&userId=b0a3e7e495ca&source=post_page-b0a3e7e495ca----c07c5ea44d64---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c07c5ea44d64--------------------------------)
    ·7分钟阅读·2023年5月30日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc07c5ea44d64&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-explained-c07c5ea44d64&user=Ajay+Halthor&userId=b0a3e7e495ca&source=-----c07c5ea44d64---------------------clap_footer-----------)'
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc07c5ea44d64&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-explained-c07c5ea44d64&source=-----c07c5ea44d64---------------------bookmark_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc07c5ea44d64&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fword-embeddings-explained-c07c5ea44d64&source=-----c07c5ea44d64---------------------bookmark_footer-----------)'
- en: In natural language processing, we work with words. However, computers cannot
    directly understand words, necessitating their conversion into numerical representations.
    These numeric representations, known as vectors or embeddings, comprise numbers
    that can be either interpretable or non-interpretable by humans. In this blog,
    we will delve into the advancements made in learning these word representations
    over time.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理领域，我们处理的是词汇。然而，计算机不能直接理解词汇，因此需要将其转换为数值表示。这些数值表示，称为向量或嵌入，由可以被人类解读或无法解读的数字组成。在这篇博客中，我们将深入探讨这些词汇表示的学习进展。
- en: 1 N-grams
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 N-grams
- en: '![](../Images/b72ba194d24fb7590374f82b5317df2e.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b72ba194d24fb7590374f82b5317df2e.png)'
- en: 'Figure 1: N-gram vector representation of a sentence (image by author)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：句子的N-gram向量表示（图由作者提供）
- en: Let’s take the example of n-grams to understand the process better. Imagine
    we have a sentence that we want the computer to comprehend. To achieve this, we
    convert the sentence into a numeric representation. This representation includes
    various combinations of words, such as unigrams (single words), bigrams (pairs
    of words), trigrams (groups of three words), and even higher-order n-grams. The
    result is a vector that could represent any English sentence.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这个过程，我们以n-grams为例。假设我们有一个句子，希望计算机能够理解。为实现这一目标，我们将句子转换为数字表示。这个表示包括各种单词组合，例如unigrams（单个单词）、bigrams（词对）、trigrams（三词组），以及更高阶的n-grams。最终得到的结果是一个可以表示任何英文句子的向量。
- en: In Figure 1, let’s consider encoding the sentence “This is a good day“. Say
    the first position of the vector represents the number of cases the bigram “good
    day” occurs in the original sentence. Since it occurs once, the numeric representation
    is “1” for this first position. In the same way, we can represent every unigram,
    diagram and trigram with different positions in this vector.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在图1中，我们考虑对句子“This is a good day”进行编码。假设向量的第一个位置表示在原句中bigram“good day”出现的次数。由于它出现了一次，所以该位置的数字表示为“1”。以同样的方式，我们可以用向量中的不同位置表示每一个unigram、bigram和trigram。
