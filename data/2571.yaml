- en: Padding Large Language Models — Examples with Llama 2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 填充大型语言模型 — 使用 Llama 2 的示例
- en: 原文：[https://towardsdatascience.com/padding-large-language-models-examples-with-llama-2-199fb10df8ff?source=collection_archive---------2-----------------------#2023-08-11](https://towardsdatascience.com/padding-large-language-models-examples-with-llama-2-199fb10df8ff?source=collection_archive---------2-----------------------#2023-08-11)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/padding-large-language-models-examples-with-llama-2-199fb10df8ff?source=collection_archive---------2-----------------------#2023-08-11](https://towardsdatascience.com/padding-large-language-models-examples-with-llama-2-199fb10df8ff?source=collection_archive---------2-----------------------#2023-08-11)
- en: Best practices to pad training examples for causal LLMs
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为因果 LLMs 填充训练示例的最佳实践
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----199fb10df8ff--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----199fb10df8ff--------------------------------)[](https://towardsdatascience.com/?source=post_page-----199fb10df8ff--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----199fb10df8ff--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----199fb10df8ff--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bnjmn_marie?source=post_page-----199fb10df8ff--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----199fb10df8ff--------------------------------)[](https://towardsdatascience.com/?source=post_page-----199fb10df8ff--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----199fb10df8ff--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----199fb10df8ff--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpadding-large-language-models-examples-with-llama-2-199fb10df8ff&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----199fb10df8ff---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----199fb10df8ff--------------------------------)
    ·10 min read·Aug 11, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F199fb10df8ff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpadding-large-language-models-examples-with-llama-2-199fb10df8ff&user=Benjamin+Marie&userId=ad2a414578b3&source=-----199fb10df8ff---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpadding-large-language-models-examples-with-llama-2-199fb10df8ff&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----199fb10df8ff---------------------post_header-----------)
    发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----199fb10df8ff--------------------------------)
    · 10分钟阅读·2023年8月11日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F199fb10df8ff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpadding-large-language-models-examples-with-llama-2-199fb10df8ff&user=Benjamin+Marie&userId=ad2a414578b3&source=-----199fb10df8ff---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F199fb10df8ff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpadding-large-language-models-examples-with-llama-2-199fb10df8ff&source=-----199fb10df8ff---------------------bookmark_footer-----------)![](../Images/5dee339a0ba5a939a758499baeec9333.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F199fb10df8ff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpadding-large-language-models-examples-with-llama-2-199fb10df8ff&source=-----199fb10df8ff---------------------bookmark_footer-----------)![](../Images/5dee339a0ba5a939a758499baeec9333.png)'
- en: Image by the author — Based on an image from [Pixabay](https://pixabay.com/vectors/llama-alpaca-animal-mammal-zoo-297668/)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像 — 基于来自[Pixabay](https://pixabay.com/vectors/llama-alpaca-animal-mammal-zoo-297668/)的图像
- en: Padding is one of the most under-documented aspects of large language models
    (LLMs). Why? Simply because LLMs are usually pre-trained without padding.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 填充是大型语言模型（LLMs）中最少被记录的方面之一。为什么？仅仅因为 LLMs 通常在没有填充的情况下进行预训练。
- en: 'Nonetheless, for fine-tuning LLMs on custom datasets, padding is necessary.
    Failing to correctly pad training examples may result in different kinds of unexpected
    behaviors: Null loss or infinity loss during training, over-generation, or empty
    output during inference, are all symptoms of incorrect padding.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，对于在自定义数据集上微调 LLMs，填充是必要的。未能正确填充训练示例可能会导致各种意想不到的行为：训练期间的空损失或无限损失、生成过多或推理期间的空输出，都是填充不正确的症状。
- en: In this article, I first explain what is padding and why it is necessary. Then,
    I show how you can find the correct padding strategy for an LLM pre-trained without
    padding. I propose two different solutions to add padding support to LLMs using
    Hugging Face’s Transformers.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我首先解释什么是填充以及为什么它是必要的。然后，我展示了如何为没有填充的预训练 LLM 找到正确的填充策略。我提出了两种不同的解决方案来为
    LLM 添加填充支持，使用 Hugging Face 的 Transformers。
- en: Toward the end of the article, I also provide examples showing how to pad your
    training examples for Llama 2.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在文章的末尾，我还提供了示例，展示了如何为 Llama 2 填充你的训练示例。
- en: After reading this article, you should be able to figure out by yourself how
    to pad training examples for LLMs without reading their documentation or tutorials.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读完这篇文章后，你应该能够自己搞清楚如何为 LLM 填充训练示例，而无需阅读它们的文档或教程。
- en: Pad and batch
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 填充和批次
- en: What is padding and why do we pad?
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是填充，为什么我们需要填充？
- en: Let’s take one example that we wish to use for fine-tuning an LLM.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来举一个例子，假设我们希望用来微调一个 LLM。
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We have to turn this example into a sequence of tokens. Libraries, such as
    Transformers, usually tokenize following these steps:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将这个示例转换为一个标记序列。像 Transformers 这样的库通常按以下步骤进行分词：
- en: 'Segment the example into subwords according to a given vocabulary:'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据给定的词汇表将示例分割成子词：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Replace words by their index from the vocabulary to obtain a sequence of integers:'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用词汇表中的索引替换单词，以获得一个整数序列：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Add special tokens to the sequence: BOS token, EOS token, UNK token, PAD token,
    etc.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向序列中添加特殊标记：BOS 标记、EOS 标记、UNK 标记、PAD 标记等。
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*Note: For this example, I use Llama 2’s tokenizer. We will see below in detail
    how to do it.*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：在这个例子中，我使用了 Llama 2 的分词器。我们将在下面详细了解如何操作。*'
- en: In this example, only the BOS (begin of sequence) special token has been added.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，仅添加了 BOS（序列开始）特殊标记。
- en: An attention mask is also generated for each training example. This mask tells
    the transformer whether it should give attention to a token (1) or not (0). The
    attention mask for this example is simple since all the tokens should be considered.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 每个训练示例还会生成一个注意力掩码。这个掩码告诉变压器是否应该关注一个标记（1）或不关注（0）。这个示例的注意力掩码很简单，因为所有标记都应该被考虑。
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The next step is to wrap everything into tensors with Pytorch. This wrapping
    is necessary to apply the matrix operations for which CUDA and GPUs are optimized.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将所有内容封装成 Pytorch 张量。这种封装是必要的，以便应用 CUDA 和 GPU 优化的矩阵操作。
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, let’s say that we have not one but two training examples. For the sake
    of simplicity, I’ll just duplicate the one I already have. The new tensors have
    one more row:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设我们有两个训练示例。为了简单起见，我会重复我已有的一个。新的张量多出了一行：
- en: '[PRE6]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Both examples have the same length (of course, since they are identical). Both
    tensors have the same dimensions 2x8 (N x M).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 两个示例具有相同的长度（当然，因为它们是相同的）。这两个张量的维度都是 2x8（N x M）。
- en: Examples are put in the tensors to create batches so that the neural network
    can update its values after seeing N examples. Batching is critical for computing
    efficiency and model performance.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 示例被放入张量中以创建批次，以便神经网络在看到 N 个示例后可以更新其值。批次对于计算效率和模型性能至关重要。
- en: 'Now, let’s introduce a third example that is shorter:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们引入一个更短的第三个示例：
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'After tokenization, we obtain:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在分词后，我们得到：
- en: '[PRE8]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'If you try to add it to our list of examples and create tensors, you will get
    an error. But imagine that we don’t have any errors, we would obtain:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你尝试将其添加到我们的示例列表中并创建张量，你将会遇到错误。但想象一下，如果没有错误，我们将得到：
- en: '[PRE9]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '*Can you see the problem here and why it is not possible to create such tensors?*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*你能看到这里的问题吗，为什么不能创建这样的张量？*'
- en: We have one row of a different length. We can’t apply matrix operations on this.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一行不同长度的数据。我们无法对其应用矩阵操作。
- en: In most datasets, examples don’t have the same length. We have to modify them
    to make sure that examples in the same batch have the same length.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数数据集中，示例的长度不相同。我们必须修改它们，以确保同一批次中的示例具有相同的长度。
- en: This is why we need “padding”.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们需要“填充”的原因。
- en: Padding token and padding side
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 填充标记和填充侧
- en: You can see padding as extending a sequence up to a given length by repeating
    a dummy token.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将填充视为通过重复一个虚拟标记来扩展序列直到给定长度。
- en: This dummy token is a “pad token”.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个虚拟标记是一个“填充标记”。
- en: For example, our first example above has a length of 8 tokens (including the
    BOS token). Let’s say that in our batch we won’t have sequences longer than 8
    tokens. All the sequences must be 8 tokens long.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，上面第一个示例的长度为 8 个标记（包括 BOS 标记）。假设在我们的批处理中，我们不会有超过 8 个标记的序列。所有序列必须为 8 个标记长。
- en: Our second example contains only 5 tokens. So we must add 3 pad tokens.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第二个示例仅包含 5 个标记。所以我们必须添加 3 个填充标记。
- en: '[PRE10]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In practice, we don’t manually add “[PAD]” tokens to the sequences. Most tokenizers
    would split “[PAD]” into subwords. The pad token is usually a special token defined
    inside the tokenizer and automatically added, if necessary, along with the other
    special tokens to the sequence.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们不会手动将“[PAD]”标记添加到序列中。大多数分词器会将“[PAD]”拆分成子词。填充标记通常是分词器内部定义的特殊标记，并且在必要时会与其他特殊标记一起自动添加到序列中。
- en: 'If the pad token has the ID 32000 in the vocabulary, we would obtain:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果填充标记在词汇表中的 ID 是 32000，我们将得到：
- en: '[PRE11]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, we have a sequence with the expected length. But one problem remains:
    We also need to modify the attention mask.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有了一个具有预期长度的序列。但还有一个问题：我们还需要修改注意力掩码。
- en: Remember, the pad tokens are dummy tokens, we don’t want the LLM to give any
    attention to them. We only introduced these tokens to fill sequences and create
    correct tensors.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，填充标记是虚拟标记，我们不希望 LLM 对它们给予任何关注。我们引入这些标记只是为了填充序列并创建正确的张量。
- en: To indicate it to the model, we simply put “0” in the attention mask so that
    the model will ignore them.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将其指示给模型，我们只需在注意力掩码中设置“0”，这样模型将忽略它们。
- en: '[PRE12]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, we can create correct tensors with the padded examples:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用填充的示例创建正确的张量：
- en: '[PRE13]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '*Note: Padding is performed when a sequence is too short given the maximum
    length. But in some cases, a sequence can be too long. In this situation, we have
    to truncate the sequence so that its size matches the maximum length.*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：填充是在序列长度小于最大长度时执行的。但在某些情况下，序列可能会太长。在这种情况下，我们必须截断序列，以使其大小匹配最大长度。*'
- en: Another important parameter of padding is the padding side. In the example above,
    I padded right. If the model has an EOS token, the pad tokens will be added after
    it.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 填充的另一个重要参数是填充侧面。在上面的示例中，我进行了右填充。如果模型有 EOS 标记，填充标记将添加到它之后。
- en: 'We can also pad left. In this situation, the tensors look like this:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以进行左填充。在这种情况下，张量看起来像这样：
- en: '[PRE14]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Pad tokens are added before the BOS token.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 填充标记是在 BOS 标记之前添加的。
- en: Which side to choose mainly depends on the LLM you want to use and your downstream
    tasks. That’s why it’s important to study the model and its tokenizer before taking
    any decision. Below, we will see how to make this decision for Llama 2.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 选择哪个侧面主要取决于你想使用的 LLM 和你的下游任务。这就是为什么在做出任何决定之前，研究模型及其分词器是很重要的。下面，我们将看到如何为 Llama
    2 做出这个决定。
- en: Adding padding support for causal LLM
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为因果 LLM 添加填充支持
- en: As we saw, padding is (almost) always necessary for fine-tuning. Yet, many LLMs
    don’t support padding by default. It means that they don’t have a special pad
    token in their vocabulary.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，填充（几乎）总是对于微调是必要的。然而，许多 LLM 默认不支持填充。这意味着它们的词汇表中没有特殊的填充标记。
- en: Here, I present two solutions to add a pad token.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我展示了两种添加填充标记的解决方案。
- en: The simple solution
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单的解决方案
- en: This solution is the one that you will find in most tutorials.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案是你在大多数教程中会找到的。
- en: 'It simply assigns an existing token to the pad token. For instance, you can
    declare that your pad token will be the EOS token. We would obtain tensors like
    this (right-padded, and where “2” is the ID of the EOS token):'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 它只是将现有标记分配给填充标记。例如，你可以声明你的填充标记将是 EOS 标记。我们将得到像这样（右填充，其中“2”是 EOS 标记的 ID）的张量：
- en: '[PRE15]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The problem with this solution is that the LLM is now confused: Most of the
    time, the EOS token will have “0” in the attention mask. It encourages the LLM
    to ignore the original EOS token. This is not ideal since the EOS token signals
    the LLM to stop generating.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案的问题是 LLM 现在感到困惑：大多数情况下，EOS 标记在注意力掩码中将是“0”。这会促使 LLM 忽略原始的 EOS 标记。这并不理想，因为
    EOS 标记会指示 LLM 停止生成。
- en: Also with this solution, we have to pad right. If you pad left, you would have
    sequences beginning with an EOS token thus early stopping the generation.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种解决方案，我们也必须进行右填充。如果你进行左填充，你会得到以 EOS 标记开头的序列，从而过早停止生成。
- en: '*Note: I read several tutorials for fine-tuning Llama 2 that use the EOS token
    for left padding. If you do that, you will have a 0.0 loss and the training will
    diverge. Try it! It’s interesting to observe it.*'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：我阅读了几个用于微调 Llama 2 的教程，这些教程使用 EOS token 进行左填充。如果这样做，你将得到 0.0 的损失，训练将会发散。试试看吧！观察这个现象很有趣。*'
- en: In my opinion, a better alternative is to use the UNK token as the pad token.
    This token is rarely used. It appears in sequences only when a token is not in
    the vocabulary. Using it for another purpose shouldn’t have a significant impact.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在我看来，更好的替代方案是使用 UNK token 作为 pad token。这个 token 很少被使用。只有当 token 不在词汇表中时，它才会出现在序列中。将它用于其他目的不应产生重大影响。
- en: Meta in its “[Llama recipes](https://github.com/facebookresearch/llama-recipes/blob/main/utils/train_utils.py)”
    uses this alternative. With this solution, the padding side doesn’t matter much.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Meta 在其“[Llama recipes](https://github.com/facebookresearch/llama-recipes/blob/main/utils/train_utils.py)”中使用了这一替代方案。使用此解决方案，填充侧别不那么重要。
- en: 'The alternative solution: Create a pad token from scratch'
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另一种解决方案：从头创建一个 pad token
- en: The UNK token already has a role in the model. Ideally, we want a pad token
    that is used only for padding.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: UNK token 在模型中已经有其角色。理想情况下，我们希望有一个仅用于填充的 pad token。
- en: We have to create from scratch a pad token in the vocabulary if it doesn’t exist.
    [This is the solution recommended by Hugging Face for Llama 2](https://huggingface.co/docs/transformers/main/model_doc/llama2).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果词汇表中不存在，我们必须从头创建一个 pad token。[这是 Hugging Face 推荐的 Llama 2 解决方案](https://huggingface.co/docs/transformers/main/model_doc/llama2)。
- en: With libraries such as transformers, it’s easy to extend a vocabulary.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用如 transformers 等库，扩展词汇表非常简单。
- en: 'If you want to create a pad token, you have to follow these steps:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想创建一个 pad token，你需要按照以下步骤进行：
- en: add the pad token as a special token in the vocabulary of the LLM
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 pad token 作为特殊 token 添加到 LLM 的词汇中。
- en: resize the token embeddings
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整 token embeddings 的大小
- en: retrain the token embeddings (optional)
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新训练 token embeddings（可选）
- en: If you are on a budget and use LoRa for fine-tuning, you may want to skip the
    last step since the token embeddings can weigh several 100 million parameters.
    Moreover, in my experiments, retraining the embeddings always led to worse results,
    indicating that my fine-tuning dataset is not large enough or that good hyperparameters
    are difficult to find.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有预算限制并使用 LoRa 进行微调，你可能会想跳过最后一步，因为 token embeddings 可能会有数亿参数。此外，在我的实验中，重新训练
    embeddings 总是导致更糟的结果，这表明我的微调数据集可能不够大，或者好的超参数很难找到。
- en: 'Case study: padding Llama 2 with Hugging Face’s Transformers'
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究：用 Hugging Face 的 Transformers 填充 Llama 2
- en: In this section, we will enable padding for Llama 2\. To replicate each step,
    you will need access to Llama 2 on Hugging Face. I explained [how to get Llama
    2 in this article](https://kaitchup.substack.com/p/fine-tune-llama-2-on-your-computer).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将为 Llama 2 启用填充。要复制每个步骤，你需要访问 Hugging Face 上的 Llama 2。我在[这篇文章中解释了如何获取
    Llama 2](https://kaitchup.substack.com/p/fine-tune-llama-2-on-your-computer)。
- en: '*Note: I share* [*a notebook (#8) replicating all these steps on The Kaitchup,
    my substack newsletter.*](https://kaitchup.substack.com/publish/post/134749542)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：我分享了* [*一个笔记本 (#8) 在 The Kaitchup，我的 substack 新闻通讯上复制所有这些步骤。*](https://kaitchup.substack.com/publish/post/134749542)'
- en: 'First, install the Transformers library:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，安装 Transformers 库：
- en: '[PRE16]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then, we import transformers and load the tokenizer. Make sure you put your
    Hugging Face’s access token there:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们导入 transformers 并加载 tokenizer。确保你在其中输入了你的 Hugging Face 访问 token：
- en: '[PRE17]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We define two training examples:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了两个训练示例：
- en: '[PRE18]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'If we put in the same batch the prompt1 twice, everything goes well:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将 prompt1 放在同一批次中两次，一切顺利：
- en: '[PRE19]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Output:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE20]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'But if you add prompt2, you will get an error as expected:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果你添加了 prompt2，你将如预期那样遇到错误：
- en: '[PRE21]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Output:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE22]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: It’s clear that the tokenizer didn’t pad the examples.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，tokenizer 并没有填充示例。
- en: 'We can solve this problem by simply using the UNK token as a pad token, as
    follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过简单地使用 UNK token 作为 pad token 来解决这个问题，具体如下：
- en: '[PRE23]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In this example, I asked the tokenizer to pad up to max_length. I set max_length
    to 20\. If your example contains 10 tokens, the tokenizer will add 10 pad tokens.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我要求 tokenizer 填充到 max_length。我将 max_length 设置为 20。如果你的示例包含 10 个 tokens，tokenizer
    将添加 10 个 pad tokens。
- en: '[PRE24]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The alternative is to create a pad token from scratch. With Hugging Face’s transformers,
    we can do this with the method “add_special_tokens”.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是从头创建一个 pad token。使用 Hugging Face 的 transformers，我们可以通过方法“add_special_tokens”来实现这一点。
- en: '[PRE25]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Output:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE26]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Don’t forget to resize the token embeddings of Llama 2 after you added the
    pad token to its vocabulary. I explained how to do it in this article:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 添加了填充标记到Llama 2的词汇表后，不要忘记调整Llama 2的令牌嵌入。我在这篇文章中解释了如何做到这一点：
- en: '[](https://kaitchup.substack.com/p/fine-tune-llama-2-on-your-computer?source=post_page-----199fb10df8ff--------------------------------)
    [## Fine-tune Llama 2 on Your Computer with QLoRa and TRL'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 在你的计算机上使用QLoRa和TRL对Llama 2进行微调](https://kaitchup.substack.com/p/fine-tune-llama-2-on-your-computer?source=post_page-----199fb10df8ff--------------------------------)'
- en: On Guanaco and with the correct padding
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在Guanaco和正确的填充设置下
- en: kaitchup.substack.com](https://kaitchup.substack.com/p/fine-tune-llama-2-on-your-computer?source=post_page-----199fb10df8ff--------------------------------)
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[kaitchup.substack.com](https://kaitchup.substack.com/p/fine-tune-llama-2-on-your-computer?source=post_page-----199fb10df8ff--------------------------------)'
- en: Conclusion
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Once you understand it, padding is very straightforward.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你理解了，填充就非常简单。
- en: Using the UNK token for padding, or creating a pad token from scratch, are very
    safe solutions that will work for almost all causal LLMs. But you should always
    have a look at how the tokenizer works. At least you should be aware of the special
    tokens it already supports. For instance, not all LLMs have a UNK token, some
    LLMs have a pad token that is not explicitly defined as a pad token in the vocabulary,
    etc.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 使用UNK标记进行填充，或者从头创建填充标记，是非常安全的解决方案，几乎适用于所有因果语言模型。但你应该始终查看分词器的工作方式。至少你应该了解它已经支持的特殊标记。例如，并非所有语言模型都有UNK标记，有些语言模型的填充标记在词汇表中没有明确定义为填充标记，等等。
- en: As usual, if you have any questions, please leave drop a comment. I’ll try to
    answer it.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，如果你有任何问题，请留下评论。我会尽量回答。
