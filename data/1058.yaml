- en: 'Gradient-Boosted Trees: To Early Stop or Not to Early Stop?'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度提升树：是早期停止还是不早期停止？
- en: 原文：[https://towardsdatascience.com/gradient-boosting-to-early-stop-or-not-to-early-stop-5ea67ac09d83?source=collection_archive---------3-----------------------#2023-03-23](https://towardsdatascience.com/gradient-boosting-to-early-stop-or-not-to-early-stop-5ea67ac09d83?source=collection_archive---------3-----------------------#2023-03-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/gradient-boosting-to-early-stop-or-not-to-early-stop-5ea67ac09d83?source=collection_archive---------3-----------------------#2023-03-23](https://towardsdatascience.com/gradient-boosting-to-early-stop-or-not-to-early-stop-5ea67ac09d83?source=collection_archive---------3-----------------------#2023-03-23)
- en: Leveraging early stopping for LightGBM, XGBoost, and CatBoost
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用早期停止方法优化 LightGBM、XGBoost 和 CatBoost
- en: '[](https://medium.com/@diogoleitao?source=post_page-----5ea67ac09d83--------------------------------)[![Diogo
    Leitão](../Images/3798f5b2653d3297b1bbae9c32d16586.png)](https://medium.com/@diogoleitao?source=post_page-----5ea67ac09d83--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5ea67ac09d83--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5ea67ac09d83--------------------------------)
    [Diogo Leitão](https://medium.com/@diogoleitao?source=post_page-----5ea67ac09d83--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@diogoleitao?source=post_page-----5ea67ac09d83--------------------------------)[![Diogo
    Leitão](../Images/3798f5b2653d3297b1bbae9c32d16586.png)](https://medium.com/@diogoleitao?source=post_page-----5ea67ac09d83--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5ea67ac09d83--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5ea67ac09d83--------------------------------)
    [Diogo Leitão](https://medium.com/@diogoleitao?source=post_page-----5ea67ac09d83--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7bbc4c70a28d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-boosting-to-early-stop-or-not-to-early-stop-5ea67ac09d83&user=Diogo+Leit%C3%A3o&userId=7bbc4c70a28d&source=post_page-7bbc4c70a28d----5ea67ac09d83---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5ea67ac09d83--------------------------------)
    ·7 min read·Mar 23, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5ea67ac09d83&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-boosting-to-early-stop-or-not-to-early-stop-5ea67ac09d83&user=Diogo+Leit%C3%A3o&userId=7bbc4c70a28d&source=-----5ea67ac09d83---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7bbc4c70a28d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-boosting-to-early-stop-or-not-to-early-stop-5ea67ac09d83&user=Diogo+Leit%C3%A3o&userId=7bbc4c70a28d&source=post_page-7bbc4c70a28d----5ea67ac09d83---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5ea67ac09d83--------------------------------)
    ·7分钟阅读·2023年3月23日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5ea67ac09d83&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-boosting-to-early-stop-or-not-to-early-stop-5ea67ac09d83&user=Diogo+Leit%C3%A3o&userId=7bbc4c70a28d&source=-----5ea67ac09d83---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5ea67ac09d83&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-boosting-to-early-stop-or-not-to-early-stop-5ea67ac09d83&source=-----5ea67ac09d83---------------------bookmark_footer-----------)![](../Images/1d27206f36b52ac44e48743d3b31a241.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5ea67ac09d83&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgradient-boosting-to-early-stop-or-not-to-early-stop-5ea67ac09d83&source=-----5ea67ac09d83---------------------bookmark_footer-----------)![](../Images/1d27206f36b52ac44e48743d3b31a241.png)'
- en: Photo by [Julian Berengar Sölter](https://unsplash.com/photos/MFWHeS6yLAI)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Julian Berengar Sölter](https://unsplash.com/photos/MFWHeS6yLAI)
- en: Gradient-boosted decision trees (GBDTs) currently outperform deep learning in
    tabular-data problems, with popular implementations such as LightGBM, XGBoost,
    and CatBoost dominating Kaggle competitions [[1](https://mlcontests.com/state-of-competitive-machine-learning-2022/)].
    Early stopping **—** a popular technique in deep learning — can also be used when
    training and tuning GBDTs. However, it is common to see practitioners explicitly
    tune the number of trees in GBDT ensembles, instead of using early stopping. In
    this article, I show that **early stopping** can **halve training time**, while
    **maintaining the same performance** as explicitly tuning the number of trees.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升决策树（GBDTs）目前在表格数据问题上超越了深度学习，流行的实现如 LightGBM、XGBoost 和 CatBoost 主导了 Kaggle
    竞赛[[1](https://mlcontests.com/state-of-competitive-machine-learning-2022/)]。早期停止**—**一种在深度学习中常用的技术—也可以在训练和调整
    GBDTs 时使用。然而，常见的做法是明确调整 GBDT 集成中的树的数量，而不是使用早期停止。在本文中，我展示了**早期停止**可以**将训练时间缩短一半**，同时**保持相同的性能**，与明确调整树的数量相媲美。
- en: By reducing training time, early stopping can **lower computational costs**
    and **decrease practitioner downtime** while waiting for models to run. Such savings
    are of utmost value in industries with large-scale GBDT applications, such as
    content recommendation, financial fraud detection, or credit scoring. But how
    does early stopping reduce training time without harming performance? Let’s dive
    in.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 通过减少训练时间，早期停止可以**降低计算成本**并**减少实践者等待模型运行时的空闲时间**。这样的节省在大型 GBDT 应用行业中尤为重要，如内容推荐、金融欺诈检测或信用评分。但早期停止如何在不损害性能的情况下减少训练时间呢？让我们深入探讨。
- en: Gradient-Boosted Decision Trees
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度提升决策树
- en: Gradient-boosted decision trees (GBDTs) currently achieve state-of-the-art performance
    in classification and regression problems based on (heterogeneous) tabular data
    (two-dimensional datasets with diverse column types). Deep learning techniques
    — although performant in natural language processing and computer vision— are
    yet to steal the crown in the tabular data domain [[2](https://proceedings.neurips.cc/paper/2021/file/9d86d83f925f2149e9edb0ac3b49229c-Paper.pdf),
    [3](https://arxiv.org/pdf/2106.03253.pdf), [4](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9998482),
    [5](https://openreview.net/pdf?id=Fp7__phQszn)].
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升决策树（GBDTs）目前在基于（异质）表格数据的分类和回归问题中取得了最先进的表现（具有多种列类型的二维数据集）。虽然深度学习技术在自然语言处理和计算机视觉中表现出色，但尚未在表格数据领域夺得桂冠[[2](https://proceedings.neurips.cc/paper/2021/file/9d86d83f925f2149e9edb0ac3b49229c-Paper.pdf),
    [3](https://arxiv.org/pdf/2106.03253.pdf), [4](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9998482),
    [5](https://openreview.net/pdf?id=Fp7__phQszn)]。
- en: '![](../Images/956cf351e4957becbdef5eb8a54ef1b8.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/956cf351e4957becbdef5eb8a54ef1b8.png)'
- en: Gradient-boosted decision trees (GBDTs).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升决策树（GBDTs）。
- en: GBDTs work by sequentially adding decision trees to an ensemble. Unlike with
    random forests, trees in GBDTs are not independent. Instead, they are trained
    to correct the mistakes of previous trees. As such, given enough trees, a GBDT
    model can achieve perfect performance in the training set. Nevertheless, this
    behavior — referred to as [overfittin](https://en.wikipedia.org/wiki/Overfitting)g
    — is known to harm the model’s ability to generalize to unseen data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: GBDTs 通过顺序地将决策树添加到一个集成中来工作。与随机森林不同，GBDTs 中的树不是独立的。相反，它们被训练来纠正之前树的错误。因此，只要有足够的树，GBDT
    模型在训练集上可以实现完美的表现。然而，这种行为——称为[过拟合](https://en.wikipedia.org/wiki/Overfitting)——已知会损害模型对未见数据的泛化能力。
- en: Hyperparameter Tuning and Early Stopping
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数调整与早期停止
- en: 'To optimize the degree of fitting to the training data, practitioners tune
    several key hyperparameters: the number of trees, the learning rate, the maximum
    depth of each tree, among others. To find the optimal set of values, several configurations
    are tested in a separate validation dataset; the model performing best in the
    holdout data is chosen as the final model.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了优化对训练数据的拟合程度，实践者调整几个关键的超参数：树的数量、学习率、每棵树的最大深度等。为了找到最佳的值集，会在一个单独的验证数据集中测试几个配置；在保留数据中表现最佳的模型被选为最终模型。
- en: Another tool that helps fight overfitting is early stopping. Common in deep
    learning, early stopping is a technique where the **learning process is halted
    if the performance on holdout data is not improving**. In GBDTs, this implies
    not building more trees beyond that point.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有助于对抗过拟合的工具是早期停止。早期停止在深度学习中常见，是一种**如果保留数据上的性能没有改善，则停止学习过程**的技术。在 GBDTs 中，这意味着在这一点之后不再构建更多的树。
- en: '![](../Images/db683b42b0addb26a3cc788ff4fbd8ab.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/db683b42b0addb26a3cc788ff4fbd8ab.png)'
- en: Early stopping halts training at the point where loss in the validation set
    stops to decreasing.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 早停法在验证集中的损失停止减少时中止训练。
- en: Although ubiquitous in deep learning, early stopping is not as popular among
    GBDT users. Instead, it is common to see practitioners tune the number of trees
    through the aforementioned search process. But **what if using early stopping
    amounts to the same as explicitly tuning the number of trees**? After all, both
    mechanisms aim to find the optimal size of the GBDT ensemble, given the learning
    rate and other hyperparameters. If that were the case, it **could mean that the
    same performance could be achieved at greatly reduced search time** by using early
    stopping, since it halts the training of time-consuming, unpromising iterations.
    Let’s test this hypothesis.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在深度学习中很普遍，但早停法在GBDT用户中并不那么流行。相反，常见的是看到从业者通过上述搜索过程调整树木数量。但是**如果使用早停法的效果等同于显式调整树木数量呢**？毕竟，这两种机制的目标都是找到GBDT集合的最佳大小，给定学习率和其他超参数。如果是这样的话，**这可能意味着通过使用早停法可以在大大减少的搜索时间内实现相同的性能**，因为它会停止那些耗时且不具前景的迭代。让我们来测试这个假设。
- en: Experimental Setup
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实验设置
- en: To this end, with the authors’ permission, I use the [public bank-account-fraud
    dataset](https://www.kaggle.com/datasets/sgpjesus/bank-account-fraud-dataset-neurips-2022)
    recently published at NeurIPS ’22 [[6](https://openreview.net/pdf?id=UrAYT2QwOX8)].
    It consists of a synthetic replica of a real fraud-detection dataset, having been
    generated by a privacy-preserving GAN. For an implementation of GBDTs, I opt for
    [LightGBM](https://lightgbm.readthedocs.io/) for its speed and state-of-the-art
    performance [[1](https://mlcontests.com/state-of-competitive-machine-learning-2022/),
    [7](https://proceedings.neurips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf)].
    All the code used in this experiment can be found in [this Kaggle notebook](https://www.kaggle.com/code/diogoleitao/lightgbm-with-early-stopping/).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，在作者许可下，我使用了[公共银行账户欺诈数据集](https://www.kaggle.com/datasets/sgpjesus/bank-account-fraud-dataset-neurips-2022)，该数据集最近在NeurIPS
    ’22上发布[[6](https://openreview.net/pdf?id=UrAYT2QwOX8)]。它由一个真实的欺诈检测数据集的合成副本组成，由隐私保护的GAN生成。对于GBDT的实现，我选择了[LightGBM](https://lightgbm.readthedocs.io/)，因其速度快且性能领先[[1](https://mlcontests.com/state-of-competitive-machine-learning-2022/),
    [7](https://proceedings.neurips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf)]。所有在此实验中使用的代码可以在[这个Kaggle笔记本](https://www.kaggle.com/code/diogoleitao/lightgbm-with-early-stopping/)中找到。
- en: As mentioned above, to find the optimal set of hyperparameters, the most common
    approach is to experiment with several configurations. Ultimately, the model that
    performs best in the validation set is chosen as the final model. I follow this
    approach, randomly sampling hyperparameters from sensible distributions at each
    iteration.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，为了找到最佳的超参数集，最常见的方法是尝试多种配置。最终，在验证集中表现最好的模型被选为最终模型。我遵循这种方法，在每次迭代中从合理的分布中随机抽取超参数。
- en: 'To test my hypothesis, I run two parallel random search processes:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检验我的假设，我运行了两个并行的随机搜索过程：
- en: Without early stopping, the number of trees parameter is tested uniformly between
    10 and 4000.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在没有早停法的情况下，树木数量参数在10到4000之间均匀测试。
- en: With early stopping, the maximum number of trees is set to 4000, but ultimately
    defined by the early stopping criteria. Early stopping monitors cross-entropy
    loss in the validation set. The training process is only halted after 100 non-improving
    iterations (the patience parameter), at which point it is reset to its best version.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用早停法，最大树木数量设置为4000，但最终由早停准则定义。早停法监控验证集中的交叉熵损失。训练过程仅在100次没有改进的迭代后（即耐心参数）被中止，此时将其重置为最佳版本。
- en: 'The following function is used to run each random search trial within an [Optuna](https://optuna.org/#code_examples)
    study *(truncated for clarity; full version in the* [*aforementioned notebook*](https://www.kaggle.com/code/diogoleitao/lightgbm-with-early-stopping/)*)*:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数用于在[Optuna](https://optuna.org/#code_examples)研究中运行每次随机搜索试验（*为清晰起见已截断；完整版见*
    [*上述笔记本*](https://www.kaggle.com/code/diogoleitao/lightgbm-with-early-stopping/)）*：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Performance
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能
- en: Since early stopping monitors performance on the validation set, all models
    are evaluated on an unseen test set, thus avoiding biased results.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 由于早停法监控验证集上的性能，所有模型都在未见过的测试集上进行评估，从而避免了偏见结果。
- en: '![](../Images/1a9617a0efc0e8301502e2a3accef361.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a9617a0efc0e8301502e2a3accef361.png)'
- en: Results on the test set. Bottom 20% of trials removed for visualization clarity.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集上的结果。为了视觉清晰度，底部20%的试验结果被移除。
- en: '**To early stop or not to early stop? Both approaches achieve similar results.**
    This outcome is consistent both when measuring cross-entropy loss — the metric
    monitored by early stopping, and recall at 5% FPR — a binary classification metric
    especially relevant in this dataset’s domain [[6](https://openreview.net/pdf?id=UrAYT2QwOX8)].
    On the first criterion, the no-early-stopping strategy achieves marginally better
    results, whereas on the second criteria, it is the early-stopping strategy that
    has the edge.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**是否要早停？两种方法的结果相似。** 这一结果在测量交叉熵损失（早停监控的指标）和5% FPR下的召回率（一个在此数据集领域特别相关的二分类指标）时都一致[[6](https://openreview.net/pdf?id=UrAYT2QwOX8)]。在第一个标准上，无早停策略取得了略微更好的结果，而在第二个标准上，则是早停策略占优。'
- en: In sum, the results of this experiment fail to reject my hypothesis that there
    is no significant difference between employing early stopping and explicitly tuning
    the number of trees in GBDTs. Naturally, a more robust evaluation would require
    experimenting with several datasets, hyperparameter search spaces and random seeds.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，这次实验的结果未能否定我的假设，即使用早停与显式调节GBDT中的树木数量之间没有显著差异。当然，更为稳健的评估需要在多个数据集、超参数搜索空间和随机种子上进行实验。
- en: Training Time
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练时间
- en: Part of my hypothesis was also that early stopping reduces average training
    time by stopping the addition of unpromising trees. Can a meaningful difference
    be measured?
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我的假设的一部分是早停通过停止添加无前景的树木来减少平均训练时间。是否可以测量出有意义的差异？
- en: '![](../Images/0188300216ddc4aa2b66fb6dcf920060.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0188300216ddc4aa2b66fb6dcf920060.png)'
- en: Distribution of training time in seconds.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 训练时间的分布（以秒为单位）。
- en: 'Results confirm the second part of my hypothesis: **training times are substantially
    inferior when using early stopping**. Using this strategy — even with a high patience
    value of 100 iterations — **halves the average training time**, from 122 seconds
    to 58 seconds. This implies a reduction of total training time from 3 hours and
    23 minutes to 1 hour and 37 minutes.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 结果证实了我的假设的第二部分：**使用早停时的训练时间明显减少**。即使使用耐心值高达100次迭代的策略，**平均训练时间也减少了一半**，从122秒降至58秒。这意味着总训练时间从3小时23分钟减少到1小时37分钟。
- en: This reduction comes in spite of the additional computation required by the
    early stopping mechanism to monitor cross-entropy loss on the validation set,
    which is accounted for in the measurements presented above.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管早停机制需要额外的计算来监控验证集上的交叉熵损失，但这些额外计算已在上述测量中考虑在内。
- en: Conclusion
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: Gradient-boosted decision-trees (GBDTs) are currently state of the art in problems
    involving tabular data. I find that using early stopping in the training of these
    models **halves training times**, while **maintaining the same performance** as
    explicitly tuning the number of trees. This makes popular GBDT implementations
    like LightGBM, XGBoost, and CatBoost that much more powerful for applications
    in large industries, such as Digital Marketing and Finance.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升决策树（GBDTs）在处理表格数据的问题中目前处于最先进水平。我发现，在训练这些模型时使用早停**将训练时间减少了一半**，同时**保持了与显式调节树木数量相同的性能**。这使得像LightGBM、XGBoost和CatBoost这样的流行GBDT实现对大规模行业应用（如数字营销和金融）更具优势。
- en: In the future, it would be important to corroborate the findings presented here
    in other datasets and across other GBDT implementations. Tuning the patience parameter
    could also prove beneficial, although its optimal value will likely vary for each
    dataset.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 未来，验证在其他数据集和不同GBDT实现中的发现将是重要的。调节耐心参数也可能有益，尽管其最佳值可能会因数据集而异。
- en: '*Except where otherwise noted, all images are by the author.*'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有说明，所有图片均由作者提供。*'
- en: References
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] H. Carlens. [The State of Competitive Machine Learning in 2022.](https://mlcontests.com/state-of-competitive-machine-learning-2022/)
    ML Contests, 2023.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] H. Carlens. [2022年竞争性机器学习现状。](https://mlcontests.com/state-of-competitive-machine-learning-2022/)
    ML Contests, 2023.'
- en: '[2] Y. Gorishniy, I. Rubachev, V. Khrulkov, and A. Babenko, [Revisiting Deep
    Learning Models for Tabular Data](https://proceedings.neurips.cc/paper/2021/file/9d86d83f925f2149e9edb0ac3b49229c-Paper.pdf),
    35th Conference on Neural Information Processing Systems (NeurIPS 2021).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Y. Gorishniy, I. Rubachev, V. Khrulkov 和 A. Babenko，[重新审视用于表格数据的深度学习模型](https://proceedings.neurips.cc/paper/2021/file/9d86d83f925f2149e9edb0ac3b49229c-Paper.pdf)，第35届神经信息处理系统会议（NeurIPS
    2021）。'
- en: '[3] R. Shwartz-Ziv, and A. Armon, [Tabular Data: Deep Learning is Not All You
    Need](https://arxiv.org/pdf/2106.03253.pdf), Information Fusion 81 (2022): 84–90.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] R. Shwartz-Ziv 和 A. Armon，[表格数据：深度学习并不是你所需的一切](https://arxiv.org/pdf/2106.03253.pdf)，信息融合
    81 (2022): 84–90。'
- en: '[4] V. Borisov, T. Leemann, K. Seßler, J. Haug, M. Pawelczyk, and G. Kasneci,
    [Deep Neural Networks and Tabular Data: A Survey](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9998482),
    IEEE Transactions on Neural Networks and Learning Systems (2022).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] V. Borisov, T. Leemann, K. Seßler, J. Haug, M. Pawelczyk 和 G. Kasneci，[深度神经网络与表格数据：一项调查](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9998482)，IEEE神经网络与学习系统汇刊（2022）。'
- en: '[5] L. Grinsztajn, E. Oyallon, and G. Varoquaux, [Why do tree-based models
    still outperform deep learning on typical tabular data?](https://openreview.net/pdf?id=Fp7__phQszn),
    36th Conference on Neural Information Processing Systems — Datasets and Benchmarks
    Track (NeurIPS 2022).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] L. Grinsztajn, E. Oyallon 和 G. Varoquaux，[为什么树基模型仍然在典型表格数据上优于深度学习？](https://openreview.net/pdf?id=Fp7__phQszn)，第36届神经信息处理系统会议——数据集和基准追踪（NeurIPS
    2022）。'
- en: '[6] S. Jesus, J. Pombal, D. Alves, A. Cruz, P. Saleiro, R. Ribeiro, J. Gama,
    P. Bizarro, [Turning the Tables: Biased, Imbalanced, Dynamic Tabular Datasets
    for ML Evaluation](https://openreview.net/pdf?id=UrAYT2QwOX8), 36th Conference
    on Neural Information Processing Systems — Datasets and Benchmarks Track (NeurIPS
    2022).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] S. Jesus, J. Pombal, D. Alves, A. Cruz, P. Saleiro, R. Ribeiro, J. Gama
    和 P. Bizarro，[扭转局面：用于机器学习评估的偏倚、不平衡和动态表格数据集](https://openreview.net/pdf?id=UrAYT2QwOX8)，第36届神经信息处理系统会议——数据集和基准追踪（NeurIPS
    2022）。'
- en: '[7] G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye, T. Liu, [LightGBM:
    A Highly Efficient Gradient Boosting Decision Tree](https://proceedings.neurips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf),
    31st Conference on Neural Information Processing Systems (NIPS 2017).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye, T. Liu，[LightGBM：一种高效的梯度提升决策树](https://proceedings.neurips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf)，第31届神经信息处理系统会议（NIPS
    2017）。'
