- en: Uncovering the Pioneering Journey of Word2Vec and the State of AI Science
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 揭示Word2Vec的开创之旅及人工智能科学的现状
- en: 原文：[https://towardsdatascience.com/uncovering-the-pioneering-journey-of-word2vec-and-the-state-of-ai-science-an-in-depth-interview-fbca93d8f4ff?source=collection_archive---------7-----------------------#2023-02-03](https://towardsdatascience.com/uncovering-the-pioneering-journey-of-word2vec-and-the-state-of-ai-science-an-in-depth-interview-fbca93d8f4ff?source=collection_archive---------7-----------------------#2023-02-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/uncovering-the-pioneering-journey-of-word2vec-and-the-state-of-ai-science-an-in-depth-interview-fbca93d8f4ff?source=collection_archive---------7-----------------------#2023-02-03](https://towardsdatascience.com/uncovering-the-pioneering-journey-of-word2vec-and-the-state-of-ai-science-an-in-depth-interview-fbca93d8f4ff?source=collection_archive---------7-----------------------#2023-02-03)
- en: '![](../Images/d6208a43e3d289661fd36719745b6683.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6208a43e3d289661fd36719745b6683.png)'
- en: Photo by [Finding Dan | Dan Grinwis](https://unsplash.com/@finding_dan?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Finding Dan | Dan Grinwis](https://unsplash.com/@finding_dan?utm_source=medium&utm_medium=referral)拍摄，发布在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: An in-depth interview with Dr. Tomas Mikolov
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与Dr. Tomas Mikolov的深入访谈
- en: '[](https://emilrijcken.medium.com/?source=post_page-----fbca93d8f4ff--------------------------------)[![Emil
    Rijcken](../Images/d79e867934f45729e6590a20dcf0a440.png)](https://emilrijcken.medium.com/?source=post_page-----fbca93d8f4ff--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fbca93d8f4ff--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fbca93d8f4ff--------------------------------)
    [Emil Rijcken](https://emilrijcken.medium.com/?source=post_page-----fbca93d8f4ff--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://emilrijcken.medium.com/?source=post_page-----fbca93d8f4ff--------------------------------)[![Emil
    Rijcken](../Images/d79e867934f45729e6590a20dcf0a440.png)](https://emilrijcken.medium.com/?source=post_page-----fbca93d8f4ff--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fbca93d8f4ff--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fbca93d8f4ff--------------------------------)
    [Emil Rijcken](https://emilrijcken.medium.com/?source=post_page-----fbca93d8f4ff--------------------------------)'
- en: ·
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F95ae6f4e7791&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcovering-the-pioneering-journey-of-word2vec-and-the-state-of-ai-science-an-in-depth-interview-fbca93d8f4ff&user=Emil+Rijcken&userId=95ae6f4e7791&source=post_page-95ae6f4e7791----fbca93d8f4ff---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fbca93d8f4ff--------------------------------)
    ·19 min read·Feb 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffbca93d8f4ff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcovering-the-pioneering-journey-of-word2vec-and-the-state-of-ai-science-an-in-depth-interview-fbca93d8f4ff&user=Emil+Rijcken&userId=95ae6f4e7791&source=-----fbca93d8f4ff---------------------clap_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F95ae6f4e7791&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcovering-the-pioneering-journey-of-word2vec-and-the-state-of-ai-science-an-in-depth-interview-fbca93d8f4ff&user=Emil+Rijcken&userId=95ae6f4e7791&source=post_page-95ae6f4e7791----fbca93d8f4ff---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fbca93d8f4ff--------------------------------)
    ·19 min read·2023年2月3日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffbca93d8f4ff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcovering-the-pioneering-journey-of-word2vec-and-the-state-of-ai-science-an-in-depth-interview-fbca93d8f4ff&user=Emil+Rijcken&userId=95ae6f4e7791&source=-----fbca93d8f4ff---------------------clap_footer-----------)'
- en: --
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffbca93d8f4ff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcovering-the-pioneering-journey-of-word2vec-and-the-state-of-ai-science-an-in-depth-interview-fbca93d8f4ff&source=-----fbca93d8f4ff---------------------bookmark_footer-----------)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffbca93d8f4ff&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Funcovering-the-pioneering-journey-of-word2vec-and-the-state-of-ai-science-an-in-depth-interview-fbca93d8f4ff&source=-----fbca93d8f4ff---------------------bookmark_footer-----------)'
- en: '*In 2012, Dr Tomas Mikolov received his PhD in Artificial Intelligence at the
    Brno University of Technology in the Czech Republic with a thesis named ‘Statistical
    Language Models Based on Neural Networks’. Working for Google Research, a year
    later, he published two highly influential papers in which he introduced the Continuous
    Bag of Words (CBOW) and skip-gram algorithms, also known as Word2Vec. As a result,
    words could be numerically represented in a dense continuous space following a
    simple training procedure. This was one of the first numerical approaches that
    effectively captured word semantics and allowed for much larger vocabularies to
    be processed. Many state-of-the-art NLP tasks were outperformed using this technique,
    and Word2Vec successors still play an important role in language models that are
    now considered state-of-the-art. Dr Mikolov believes complex systems might be
    the next step towards intelligent language models. However, to reach such intelligent
    language models, the scientific paradigm needs to change to create an equal level
    playing field and allow for novelty.*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*2012 年，托马斯·米科洛夫博士在捷克共和国的布尔诺技术大学获得了人工智能博士学位，论文题为《基于神经网络的统计语言模型》。在谷歌研究部门工作一年后，他发表了两篇极具影响力的论文，介绍了连续词袋模型（CBOW）和跳字模型，也称为
    Word2Vec。因此，单词可以在一个稠密的连续空间中用数字表示，遵循简单的训练过程。这是最早有效捕捉单词语义的数值方法之一，并允许处理更大的词汇表。许多最先进的自然语言处理任务使用这种技术取得了超越性的成果，而
    Word2Vec 的继承者仍在如今被认为是最先进的语言模型中扮演重要角色。米科洛夫博士认为复杂系统可能是通向智能语言模型的下一步。然而，要实现这样的智能语言模型，科学范式需要改变，以创建一个平等的竞争环境并允许新颖性。*'
- en: '*A decade after having defended his PhD thesis, he has been cited more than
    125 000 times and has an h-index of 49 and an i-10 index of 85, according to Google
    Scholar. In 2014, he moved to Facebook and then returned to the Czech Republic
    in 2020\. He builds his team to develop a system that could gradually evolve into
    strong artificial intelligence at the Czech Institute of Informatics, Robotics
    and Cybernetics.*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*在他为其博士论文辩护后的十年里，他的研究成果被引用超过 125,000 次，h-指数为 49，i-10 指数为 85，依据 Google Scholar
    的数据。2014 年，他移居 Facebook，随后在 2020 年返回捷克共和国。他在捷克信息学、机器人学和网络安全研究所组建了团队，开发一个系统，该系统有望逐渐演变为强人工智能。*'
- en: '**Your Word2Vec algorithm was revolutionary in the natural language processing
    domain. Can you describe the publications leading up to your work?**'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**你的 Word2Vec 算法在自然语言处理领域是革命性的。你能描述一下促成你工作的那些出版物吗？**'
- en: 'A very influential research group, led by psychologist David Rumelhart, was
    working on similar concepts already in the 80s. One of Rumelhart’s students was
    Geoff Hinton, famous for his work on neural networks. In the 80s, they already
    used neural networks and distributed representations to represent words and showed
    interesting properties. In the 90s, Jeff Elman used recurrent neural networks
    to model language. He used artificial data generated from simple grammars written
    by hand. Hence, his work had many simplifications and was not as complex as today;
    it was not even close to our current state-of-the-art. But it was a very inspiring
    and forward-looking approach to representing language. A very influential publication
    in 1991 called: ‘*Finding structure in time’* discusses approaches to represent
    time in connectionist models. This was inspiring work for me when I worked on
    language models as a student. Yoshua Bengio published an influential neural language
    modeling paper around 2002 in which he outperformed standard language modeling
    baselines on small datasets. Later, I published several papers with Yoshua and
    visited his group for half a year. Lastly, the first person I discovered to use
    neural networks for general sequence prediction with state-of-the-art performance
    on challenging benchmarks was Matt Mahoney — his PAQ algorithms were basically
    neural language models used for data compression with amazing performance.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常有影响力的研究小组，由心理学家**大卫·鲁梅哈特**领导，早在80年代就开始研究类似的概念。**鲁梅哈特**的学生之一是**杰夫·辛顿**，他因在神经网络方面的工作而闻名。在80年代，他们已经使用神经网络和分布式表示来表示单词，并展示了有趣的特性。在90年代，**杰夫·艾尔曼**使用递归神经网络来建模语言。他使用了由简单的手工编写的语法生成的人工数据。因此，他的工作有许多简化，并不像今天那样复杂；甚至与我们当前的最先进水平相去甚远。但这是一个非常有启发性和前瞻性的方法来表示语言。1991年的一篇非常有影响力的出版物《*Finding
    structure in time*》讨论了在连接主义模型中表示时间的方法。这对我作为学生在工作语言模型时很有启发性。**约书亚·本吉奥**在2002年左右发表了一篇有影响力的神经语言建模论文，他在小数据集上超越了标准语言建模基准。后来，我和**约书亚**发表了几篇论文，并在他的团队中待了半年。最后，我发现第一个使用神经网络进行通用序列预测并在具有挑战性的基准上取得最先进性能的人是**马特·马洪**——他的PAQ算法基本上是用于数据压缩的神经语言模型，表现惊人。
- en: '**Who influenced you the most?**'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**谁对你影响最大？**'
- en: The most influential person for me was Matt Mahoney in the very beginning, and
    then later Holger Schwenk; I found his papers simpler to read than Yoshua’s. It
    contained quickly implementable methods instead of using unnecessary complex approaches.
    Hence, I was trying to do something similar myself. When I started my master thesis
    in 2006, one of the first models I implemented was a recurrent neural language
    model. I did not know any prior work then, and I was very excited about this recurrent
    network idea I had just invented. But it did not work well at first — it was better
    than the n-gram model but worse than a simple feedforward neural network. At the
    time, it was very challenging to make such models work because we did not know
    how to handle the exploding and vanishing gradients. The ‘ ‘community’s interest
    in the learned memory in recurrent networks was high in the 80s and 90s. However,
    nobody knew whether stochastic gradient descent could work, and some papers claimed
    it could not. Furthermore, while people had limited success on small datasets,
    nobody could successfully train recurrent networks on large datasets, at least
    not without sacrificing most of their performance. Nowadays, we know they can,
    and these stories from the past are hard to grasp.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对我影响最大的人最初是**马特·马洪**，后来是**霍尔格·施温克**；我发现他的论文比**约书亚**的更易读。它包含了可以快速实现的方法，而不是使用不必要的复杂方法。因此，我尝试自己做一些类似的事情。当我在2006年开始我的硕士论文时，我实现的第一个模型是递归神经语言模型。那时我对这种我刚刚发明的递归网络想法感到非常兴奋，但一开始效果不好——虽然比n-gram模型好，但不如简单的前馈神经网络。当时，让这种模型正常工作非常具有挑战性，因为我们不知道如何处理梯度爆炸和消失。在80年代和90年代，“社区”对递归网络中的学习记忆非常感兴趣。然而，没人知道随机梯度下降是否有效，一些论文声称它无效。此外，尽管人们在小数据集上取得了有限的成功，但没有人能成功地在大数据集上训练递归网络，至少没有牺牲大部分性能。现在，我们知道它们可以这样做，这些过去的故事很难理解。
- en: This was an exciting story for me. I came up with the idea to generate text
    from neural language models in the summer of 2007 and compared that to the text
    generated from n-gram models (taking inspiration from the SRILM toolkit). The
    fluency increase was incredibly high, and I knew immediately that this was the
    future. It felt very cool to observe these results while knowing I’m the first
    person to ever see this — like discovering an unknown island full of strange animals.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对我来说，这是一个令人兴奋的故事。我在2007年夏天想到从神经语言模型生成文本的想法，并将其与n-gram模型生成的文本进行比较（灵感来自SRILM工具包）。流畅度的提高非常显著，我立即知道这就是未来。看到这些结果的同时知道我是第一个看到这些结果的人，感觉非常酷——就像发现了一个充满奇怪动物的未知岛屿一样。
- en: When I started working on RNNs, I did not know about vanishing and exploding
    gradients. After some time, I managed to make the RNNs work very well on small
    datasets. This was a challenge on its own — to evaluate various language models
    and compare them, as typically, at the time, all published models were evaluated
    on private data. Also, the code was not released. Luckily, I managed to get one
    dataset during my internship at Johns Hopkins University in the group of Fred
    Jelinek in 2010\. After a few small tweaks, I published it on my website, and
    that’s how the now very well-known Penn Treebank language modeling benchmark came
    into existence. It has nothing to do with the treebanks at all — it was simply
    meant by me to be used for comparing different language modeling techniques while
    being compatible with previously published results by JHU researchers.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当我开始研究RNN时，我不知道梯度消失和梯度爆炸的问题。经过一段时间，我成功地让RNN在小数据集上表现得非常好。这本身就是一个挑战——评估各种语言模型并进行比较，因为当时所有发布的模型通常都在私人数据上进行评估。此外，代码也没有发布。幸运的是，在2010年我在约翰霍普金斯大学的Fred
    Jelinek研究组实习期间，我设法获得了一个数据集。经过一些小的调整，我将其发布在我的网站上，这就是现在非常著名的Penn Treebank语言建模基准的由来。它与树库完全无关——它只是我用来比较不同语言建模技术的，同时与JHU研究人员之前发布的结果兼容。
- en: 'I also published my RNNLM code in 2010, including the text generation part,
    so that other researchers could reproduce my results easily. This was crucial:
    the improvements over n-grams I was obtaining were so high that pretty much nobody
    at the time believed my results could be correct.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我还在2010年发布了我的RNNLM代码，包括文本生成部分，以便其他研究人员可以轻松地复制我的结果。这是至关重要的：我获得的相对于n-grams的改进非常显著，当时几乎没有人相信我的结果是正确的。
- en: However, the chances that my recurrent networks did not converge grew with the
    dataset size. This messy behavior on large datasets was unpredictable. While some
    90% of models trained on Penn Treebank did converge to good performance, on larger
    datasets, it dropped to something like 10%. Because RNNs are difficult to implement
    from scratch, I assumed there must be a bug in my code. I thought I just computed
    the gradients incorrectly or had some numerical issues.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着数据集大小的增加，我的递归网络未能收敛的可能性也在增加。这种在大数据集上混乱的行为是不可预测的。虽然大约90%的在Penn Treebank上训练的模型能够收敛到良好的性能，但在更大的数据集上，这个比例降到了10%左右。由于RNN从头实现很困难，我认为我的代码中一定有错误。我认为我只是计算梯度时出错了，或者遇到了一些数值问题。
- en: I looked for the bug for days. Ultimately, I isolated the place where entropy
    spiked, and things worsened. Some gradients became massive and screwed up the
    training by overwriting the models’ weights.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我找了几天的错误。最终，我找到了熵激增的地方，并且情况变得更糟。一些梯度变得非常大，覆盖了模型的权重，导致训练出现问题。
- en: '**What did you do to resolve this?**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**你做了什么来解决这个问题？**'
- en: 'My solution was crude. I truncated the gradient values if they passed a threshold
    value. Any mathematician seeing this trick would think it is horrible. Still,
    the main thing was that the gradients exploded very rarely, and thus anything
    that prevented the explosions was a good enough solution. This heuristic worked
    and allowed recurrent neural language models to be scaled to larger datasets.
    Nowadays, it is much easier to debug code because you know what results you are
    aiming for with standard models evaluated on standard datasets. But in my time,
    this was different. I was getting new state-of-the-art results but did not know
    how much further one could go. It was exciting; I was climbing a mountain that
    no one had visited before me, and I had no idea how high it was. Eventually, I
    managed to get a perplexity on Penn Treebank around 70, which was about half of
    what n-grams had. This remained the state-of-the-art result for quite some years.
    Although here I can complain that language modeling results became incorrectly
    reported around 2014: with the invention of dropouts, researchers started focusing
    on achieving the best possible results with a single model. But then all my results
    were discarded, which were model ensembles. However, the dropout technique is
    an ensemble in disguise.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我的解决方案很粗糙。我将梯度值截断到一个阈值以上。任何数学家看到这个技巧都会觉得很糟糕。不过，主要问题是梯度很少爆炸，因此任何防止爆炸的方法都是一个足够好的解决方案。这种启发式方法有效地使递归神经语言模型能够扩展到更大的数据集。如今，调试代码要容易得多，因为你知道标准模型在标准数据集上期望的结果。但在我的时代，这情况不同。我获得了新的最先进结果，却不知道还可以走多远。这很令人兴奋；我是在攀登一座无人到达过的山峰，而我不知道它有多高。最终，我在宾夕法尼亚树库上的困惑度达到了大约70，大约是n-grams的一半。这一结果保持了相当多年的最先进水平。虽然在这里我可以抱怨，语言建模结果在2014年左右被错误报告：随着dropouts的发明，研究者们开始专注于用单一模型实现最佳结果。但随后我的所有结果都被丢弃了，这些结果是模型集成的。然而，dropout技术本质上是一种伪装的集成。
- en: Many dedicate the rise in deep learning popularity to the increasing computing
    power and large datasets. But this is not the whole story. The reason that it
    started working is that we figured out how to use the algorithms correctly.
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 许多人将深度学习的流行上升归因于计算能力的提高和大数据集。但这并不是全部故事。真正让它开始有效的是我们弄清楚了如何正确使用这些算法。
- en: Following this experience, I found another inaccuracy in the deep learning narrative.
    In 2014–2016, when deep learning’s popularity skyrocketed, there came explanations
    for why the boom happened at this point and not before. Many dedicate the rise
    in popularity to the increasing computing power and large datasets. But this is
    not the whole story. The reason that it started working is that we* figured out
    how to use the algorithms correctly. For example, you could take my RNNLM code
    and run it on hardware and datasets from the 90s — and you would obtain state-of-the-art
    results by a large margin compared to techniques from this time.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 经过这一经验，我发现了深度学习叙事中的另一个不准确之处。在2014-2016年，深度学习的流行度猛增，出现了关于为什么此时而非之前出现这种热潮的解释。许多人将这一流行的上升归因于计算能力的提高和大数据集。但这并不是全部故事。真正让它开始有效的是我们*弄清楚了如何正确使用这些算法。例如，你可以拿我的RNNLM代码，在90年代的硬件和数据集上运行——你会得到远远超过当时技术的最先进结果。
- en: Obviously, having more computing power never hurts, and it was crucial for the
    adoption by the industry. However, the deciding factor for the popularity in the
    research community was how to use these algorithms correctly; the increasing computational
    power was secondary. I would also rate open sourcing and overall reproducibility
    as very important factors. Deep ‘ ‘learning’s history is much richer and longer
    than many people think nowadays.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，拥有更多的计算能力永远不会有害，这对行业的采用至关重要。然而，研究界对这些算法的正确使用才是决定其受欢迎程度的关键；增加的计算能力是次要的。我还认为开源和整体可重复性也是非常重要的因素。深度‘
    ‘学习的历史比许多人现在认为的要丰富得多。
- en: '**This was, of course, not just me; Alex Krizhevsky made CNNs work for image
    classification, George Dahl, Abdel-rahman Mohamed and others did figure out how
    to use deep neural networks for speech recognition, and also many other PhD students
    of our generation did contribute.*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**当然，这不仅仅是我；亚历克斯·克里热夫斯基让卷积神经网络（CNNs）在图像分类中发挥了作用，乔治·达尔、阿卜杜勒-拉赫曼·穆罕默德和其他人则弄清楚了如何利用深度神经网络进行语音识别，我们这一代的许多博士生也做出了贡献。**'
- en: '**Did you already think about representing words differently during your PhD?**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**你在博士期间是否已经考虑过以不同方式表示词汇？**'
- en: Indeed, I did not come up with Word2Vec when I worked at Google; I already did
    before then. The first thing I did, similar to Word2Vec, was during my master
    thesis in 2006\. I didn’t know much about neural networks at the time. I saw a
    paper from Yoshua Bengio that used a projection and a hidden layer. I didn’t know
    how to work with neural networks with more than one hidden layer, so I decided
    to split the model into two parts. The first was just like Word2vec — it learned
    the word representations from the training set. The second network then used these
    concatenated representations at the input to represent the context and predict
    the next word. Both networks had just one hidden layer, and the results were pretty
    good — similar to Yoshua’s paper.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，当我在谷歌工作时，我并没有想出Word2Vec；我在那之前已经做过类似的工作。我做的第一件事，与Word2Vec类似，是在2006年的硕士论文中完成的。当时我对神经网络了解不多。我看到了一篇Yoshua
    Bengio的论文，它使用了一个投影和一个隐藏层。我不知道如何处理具有多个隐藏层的神经网络，所以我决定把模型分成两部分。第一部分就像Word2vec一样——它从训练集中学习单词表示。第二个网络则使用这些拼接的表示作为输入来表示上下文并预测下一个单词。两个网络都只有一个隐藏层，结果相当不错——与Yoshua的论文相似。
- en: During my PhD, my first paper at an international conference was about this
    model. While it wasn’t all that impressive, I knew that good word vectors could
    be learned using rather simple models. Later, I saw several papers that used more
    complex neural architectures to learn word vectors. It did seem rather silly to
    me — people would train a full neural language model, then throw it away and keep
    just the first weight matrix. But the research community interested in this was
    tiny, and I thought it was not worth publishing anything on this topic. Later,
    when finishing my PhD, I interned at Microsoft Research with Geoff Zweig. He was
    an amazing supervisor, but sometimes he would express doubts about neural networks
    being the future of language modeling — so I was thinking of how to impress him.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的博士期间，我在一次国际会议上发表的第一篇论文就是关于这个模型的。虽然它并不是特别令人印象深刻，但我知道可以通过相当简单的模型来学习好的词向量。后来，我看到几篇论文使用了更复杂的神经网络架构来学习词向量。这在我看来相当愚蠢——人们会训练一个完整的神经语言模型，然后把它扔掉，只保留第一个权重矩阵。但对这个研究领域感兴趣的社区非常小，我认为在这个话题上没有发表任何东西的必要。后来，当我完成博士学业时，我在微软研究院实习，与Geoff
    Zweig合作。他是一个了不起的导师，但有时他会对神经网络是否是语言建模的未来表示怀疑——所以我在考虑如何让他印象深刻。
- en: '**What did you do to convince him?**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**你做了什么来说服他？**'
- en: This is a funny story. I made some calculations and double-checked the outcome
    before approaching him. Then, I asked him whether it would be possible to apply
    simple additions and subtractions to word vectors. I asked him what the closest
    vector would be after subtracting ‘man’ from ‘king’ and adding ‘woman’ (other
    than the input word, or else you would often end where you started).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个有趣的故事。我进行了些计算，并在接触他之前仔细检查了结果。然后，我问他是否可以对词向量应用简单的加法和减法。我问他在从‘king’中减去‘man’并添加‘woman’之后，最接近的向量是什么（除了输入词，否则你经常会回到你开始的地方）。
- en: He told me this was a rather silly idea and that nothing sensible could come
    out of this. So, I immediately took him to my computer and showed him the experiment
    — it returned ‘*queen*’. He was amazed and started playing around. He tried past
    tenses of verbs and plurals, etcetera. Some ideas worked, and some did not. But
    it was much better than random guesses. It is very fascinating to see these analogies
    for the first time. It raises fundamental questions. Why does it show these regularities?
    Why is this fully linear? Why don’t you multiply vectors instead of adding and
    subtracting them?
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 他告诉我这是个相当愚蠢的想法，认为这样没有任何意义。因此，我立即把他带到我的电脑前，展示了实验结果——它返回了‘*queen*’。他非常惊讶，开始尝试各种操作。他尝试了动词的过去时和复数形式等等。有些想法有效，有些则无效。但这比随机猜测要好得多。第一次看到这些类比非常令人着迷。这引发了基本的问题。为什么会出现这些规律？为什么这是完全线性的？为什么不乘以向量，而是相加和相减？
- en: '**Were your Google colleagues as skeptical as your supervisor?**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**你的谷歌同事也像你的导师一样持怀疑态度吗？**'
- en: I wouldn’t call Geoff Zweig skeptical but rather careful. He was actually very
    supportive, and it was easy to convince him that some ideas were worth pursuing.
    I’ve had much more trouble earlier in my career. When I started working on neural
    language models, I received extremely negative reviews from a local linguist at
    the Brno University of Technology. He would go as far as to say that the whole
    idea of using neural networks for modeling language is complete bullshit and that
    my results must be fake. He almost managed to get me kicked out of the PhD program.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会称Geoff Zweig为怀疑，但可以说他非常谨慎。他实际上非常支持，很容易说服他相信某些想法值得追求。我在职业生涯初期遇到过更多麻烦。当我开始研究神经语言模型时，我收到了来自布尔诺理工大学一位当地语言学家的极其负面的评价。他甚至说，使用神经网络来建模语言的整个想法完全是胡扯，而且我的结果一定是假的。他差点儿让我被踢出博士项目。
- en: When I joined Google Brain, several colleagues were already trying to learn
    word representations. However, they tried to train large language models to get
    the word vectors. In the large language models, 99,9% of the training time, you
    update parameters irrelevant to the word vectors. From my master thesis in 2006,
    I knew that if the final task was not language modeling, such large language models
    were unnecessary. Instead, using a simpler model to compute word vectors suffices.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当我加入Google Brain时，一些同事已经在尝试学习词语表示。然而，他们试图训练大型语言模型以获得词向量。在大型语言模型中，99.9%的训练时间，你在更新与词向量无关的参数。从2006年我的硕士论文中，我知道如果最终任务不是语言建模，这样的大型语言模型是不必要的。相反，使用更简单的模型来计算词向量就足够了。
- en: I shared this insight with some colleagues. However, no one really listened.
    Some were following a Stanford paper, which was complicated and contained many
    unnecessities. Having just started at Google Brain, my first goal was to show
    how the problem can be solved efficiently. I started playing around, and it worked
    well within a few weeks. Using an ordinary desktop computer, I could train models
    using hundreds of millions of words in a few hours. My model beat an internal
    Google model that was trained over weeks on many machines.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我将这一见解与一些同事分享了。然而，没有人真正听取。一些人跟随的是一篇斯坦福论文，这篇论文复杂且包含许多不必要的内容。刚刚开始在Google Brain工作时，我的第一个目标是展示如何高效地解决这个问题。我开始尝试，很快就取得了成功。使用普通的台式电脑，我可以在几个小时内训练使用数亿个单词的模型。我的模型击败了一个在许多机器上训练了几周的Google内部模型。
- en: '**What happened then?**'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**那时发生了什么？**'
- en: Yoshua had just organized a new conference, ICLR, and asked if I could submit
    a paper about word analogies as it was quite a surprising result back then. He
    thought this would make a cool paper. He reached out to me halfway through December;
    the deadline was early January. So I spent my Christmas holidays writing the Word2Vec
    paper in California. It was not very well written, but I cared more about the
    implementation and results than the paper. Supported by my colleagues, I submitted
    the paper to ICLR. But unfortunately, the reviews were quite negative (it was
    an open review, so it should still be accessible). One reviewer complained that
    the model did not consider word order. Another reviewer tried to force me into
    citing other papers more extensively, which I already cited, and which were published
    after my papers on the model from my master thesis (which already contained the
    main idea).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Yoshua刚刚组织了一个新的会议，ICLR，并问我是否可以提交一篇关于词语类比的论文，因为那时这是一个相当令人惊讶的结果。他认为这会是一篇很酷的论文。他在12月中旬联系了我；截止日期是在1月初。所以我在加州的圣诞假期中写了Word2Vec论文。论文写得不是很好，但我更关心的是实现和结果，而不是论文。在同事的支持下，我向ICLR提交了论文。但不幸的是，评论非常负面（这是一个公开评审，因此应该仍然可以访问）。一位评审抱怨模型没有考虑词序。另一位评审试图强迫我更多地引用其他论文，而这些论文我已经引用过，并且是在我的硕士论文（其中已经包含了主要想法）之后发表的。
- en: ICLR 2013's acceptance rate was around 70%. But the Word2Vec paper got rejected.
    Today, it is probably more cited than all the accepted papers at ICLR 2013 together.
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ICLR 2013的接受率约为70%。但Word2Vec论文被拒绝了。今天，它可能被引用的次数比ICLR 2013上所有接受的论文加起来还要多。
- en: Here is a funny detail. Although a well-known conference now, this was ICLR’s
    first edition and was small. The acceptance rate was around 70%, so pretty much
    everything that was not totally awful was accepted. But the Word2Vec paper got
    rejected, although today, it is probably more cited than all the accepted papers
    at ICLR 2013 together. Then, I decided to write another paper with several extensions.
    This paper was finally accepted to NIPS.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个有趣的细节。虽然现在是一个著名的会议，但这是ICLR的第一届，规模很小。接受率约为70%，所以几乎所有不是完全糟糕的论文都会被接受。但Word2Vec论文被拒绝了，尽管今天它可能比ICLR
    2013上所有接受的论文加起来的引用次数还要多。于是，我决定写另一篇扩展版的论文。这篇论文最终被接受到NIPS。
- en: '**You have never published your first paper anywhere else, have you?**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**你从未在其他地方发表过你的第一篇论文，对吧？**'
- en: The first paper was accepted for a workshop after it was rejected for the ICLR
    conference. But I don’t think a workshop counts as a publication. Also, it was
    published on Arxiv, and I was happy people could read it. When I published it,
    I knew it was much better than currently available — at least in the aspects I
    cared about. The algorithm was not overly complex and provided very good results
    in practice.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 第一篇论文在被ICLR会议拒绝后被接受到一个研讨会。但我不认为研讨会算作发表。此外，它被发布在Arxiv上，我很高兴人们可以阅读。当我发布它时，我知道它比目前可用的要好——至少在我关心的方面。算法并不复杂，实际上提供了非常好的结果。
- en: '**Did you expect the paper to become this well-cited?**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**你是否预料到这篇论文会被如此广泛引用？**'
- en: 'The neural language modeling community was small when I published the paper.
    However, I was very optimistic and expected that at least fifty people would use
    it within a year. Six months after the paper was published, it still went unnoticed.
    This was because Google did not approve me to open-source the code. Initially,
    they perceived the code as a competitive advantage. However, I kept on pushing
    to open-source it. The senior people around me told me to stop trying, as I would
    never get the approval. Luckily, I knew people at Google Brain with even higher
    positions. They managed to bypass the blockade. Finally, Google approved to open
    source the code around August 2013\. That is also why the code was somewhat over-optimized:
    while waiting for approval, I tweaked the code to make it shorter and faster.
    Once the code was open-sourced, the interest skyrocketed. Many people were interested
    in Google’s machine-learning activities and liked that Google was open-sourcing
    its code. This helped immensely. I was, in fact, surprised how many people started
    using the code and the pre-trained models for all kinds of purposes, in some cases
    even outside the area of modeling words and language.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 神经语言建模社区在我发布这篇论文时还很小。然而，我非常乐观，预期至少会有五十个人在一年内使用它。论文发布六个月后，它仍然未被注意。这是因为谷歌没有批准我开源代码。最初，他们认为代码是竞争优势。然而，我一直在推动开源。周围的前辈告诉我停止尝试，因为我永远无法获得批准。幸运的是，我认识谷歌脑的高层，他们成功绕过了阻碍。最后，谷歌在2013年8月左右批准了开源代码。这也是代码有些过度优化的原因：在等待批准的过程中，我对代码进行了调整，使其更短更快。代码开源后，兴趣激增。许多人对谷歌的机器学习活动感兴趣，并喜欢谷歌开源代码。这帮助极大。我确实很惊讶有这么多人开始使用这段代码和预训练模型，甚至在一些情况下超出了建模词汇和语言的范围。
- en: '**Why are you advocating open source?**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**你为什么倡导开源？**'
- en: As a student, I found it difficult to compare different algorithms from peers
    because it was often impossible. Some fifteen years ago, publishing a language
    modeling paper evaluated on a private dataset without any open-source implementation
    was normal. That was, in my view, the main reason why language modeling research
    did not show much progress over the previous decades. I reached out to several
    researchers asking for their datasets, but without success. Nobody could validate
    published results at some point, and the community was dead. I have found that
    certain people are even cheating when reporting results, for example, by using
    weak baselines or reporting the best results after tuning hyperparameters on the
    test set (or even training models on the test set, which was rare but not unheard
    of). I was inspired by Matt Mahoney’s work in the data compression community and
    wanted to rebuild my interest in statistical language modeling, so I wanted to
    publish both my code and the data whenever possible. Of course, one big aspect
    was that when I started publishing my results with large-scale neural language
    models, my improvements were so big that pretty much the whole research community
    did not believe my results could be correct. But since nobody could find any mistake
    in my code (and many tried — I received a number of emails from people who thought
    they finally found a “bug” in my code), my RNNLM toolkit became used in several
    big companies, and language modeling research finally took off. That was the beginning
    of deep learning in NLP.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 作为学生，我发现很难比较不同算法，因为这通常是不可能的。十五年前，发布在私有数据集上评估的语言建模论文而没有任何开源实现是很正常的。在我看来，这就是语言建模研究在过去几十年中没有取得太大进展的主要原因。我曾联系过几位研究人员，询问他们的数据集，但都没有成功。到了某个阶段，没有人能验证已发表的结果，社区也陷入了停滞。我发现某些人甚至在报告结果时作弊，例如，使用弱基线或在测试集上调整超参数后报告最佳结果（甚至在测试集上训练模型，这虽然罕见但并非闻所未闻）。我受到了Matt
    Mahoney在数据压缩社区工作的启发，想要重建我对统计语言建模的兴趣，因此我希望在可能的情况下发布我的代码和数据。当然，一个重要方面是，当我开始发布我的大规模神经语言模型结果时，我的改进幅度之大，以至于几乎整个研究社区都不相信我的结果可能是正确的。但由于没有人能在我的代码中找到任何错误（许多人尝试过——我收到过很多邮件，表示他们终于找到了我代码中的“bug”），我的RNNLM工具包被几家大公司使用，语言建模研究终于起飞。这就是自然语言处理领域深度学习的开始。
- en: '**Are there downsides to open-sourcing?**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**开源有缺点吗？**'
- en: I think so. When new students join the AI community, they should try to develop
    their own models and discover new ideas. It is, however, very difficult because
    they end up competing against state-of-the-art models which were incrementally
    optimized over the years by many researchers.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为有。当新的学生加入人工智能社区时，他们应该尝试开发自己的模型并发现新想法。然而，这非常困难，因为他们最终要与多年由许多研究人员逐步优化的最先进模型竞争。
- en: Alternatively, a student can download someone else’s code and even pre-trained
    models, which are usually complex, and they probably don’t fully understand it.
    Then they tweak it, make incremental changes and publish the results in a paper.
    This approach is much easier. Meanwhile, this is a dangerous development for science,
    as it locks us in a local optimum. Several dominant ideas are being over-explored,
    and very few people are thinking about novel approaches that can bring new paradigm
    shifts. The open-sourcing and publish-or-perish together helped to create an environment
    where taking risks is not rewarded.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种情况是，学生可以下载别人的代码甚至预训练模型，这些通常很复杂，他们可能并未完全理解。然后他们对其进行调整，做出增量变化，并在论文中发布结果。这种方法要容易得多。然而，这对科学来说是一个危险的发展，因为它将我们锁定在局部最优解中。几个主流观点被过度探索，而很少有人思考可以带来新范式转变的新方法。开源和“发布或死亡”共同促成了一个环境，在这里冒险没有回报。
- en: “The group with the most GPUs has a major advantage over everyone else. This
    discourages people from academia and creates unfair competition. Applying computational
    limits on published papers in certain benchmark tracks would be a simple solution.”
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “拥有最多GPU的团队相对于其他团队有很大优势。这使得学术界的人们感到沮丧，并创造了不公平的竞争。对某些基准测试轨道上的已发表论文施加计算限制将是一个简单的解决方案。”
- en: '**So there are benefits to open-sourcing code. Meanwhile, the adverse effects
    are apparent. Is there an ‘optimal’ approach in the middle?**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**所以开源代码有好处。同时，不利影响也很明显。是否存在中间的‘最佳’方法？**'
- en: Given the importance of computational power, the group with the most GPUs has
    a major advantage over everyone else. This discourages people from academia and
    creates unfair competition; not everyone has the same starting conditions. It
    is as if you were to go to the Olympic games to compete in running. However, during
    the race, you are competing against people on bicycles. Regardless of how good
    you are, you will lose. The same happens when students with limited resources
    compete against tech giants. They might have better ideas but still get their
    papers rejected for not being state-of-the-art. This issue needs to be addressed
    by the community.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于计算能力的重要性，拥有最多GPU的团队相较于其他人具有显著优势。这使得学术界的人们受到挫折，并且造成了不公平的竞争；并不是每个人的起点条件都相同。这就好比你去参加奥运会跑步比赛，但比赛时你却是在与骑自行车的人竞争。不论你多么优秀，你都会输。学生们在资源有限的情况下与科技巨头竞争时也会遇到同样的问题。他们可能有更好的想法，但仍会因为不够前沿而被拒绝。这一问题需要社区来解决。
- en: A simple solution to this problem is applying computational limits allowed for
    publishing a paper in certain benchmark tracks. Following this approach, papers
    should be submitted together with code that can train in *X* hours on a standardized
    machine. Still, one could exhaustively explore the search space and submit code
    with the best hyperparameters, so people with more computing power would still
    have an advantage. But at least the competition would be fairer. By the way, when
    Matt Mahoney proposed his compression challenge, he already did consider this.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一个简单方法是对某些基准测试中的论文发布应用计算限制。按照这种方法，论文应当与能够在*X*小时内在标准化机器上进行训练的代码一起提交。不过，人们仍可以详尽地探索搜索空间，并提交具有最佳超参数的代码，因此拥有更多计算能力的人仍会占有优势。但至少这样竞争会公平些。顺便说一下，当Matt
    Mahoney提出压缩挑战时，他已经考虑到了这一点。
- en: “Many believe good models are complex-looking and full of hyper-parameters with
    little tweaks. Simple ideas are often considered unworthy of publishing as anyone
    could have done it. I consider this mentality completely dumb.”
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “许多人认为好的模型看起来复杂且充满了超参数和微调。简单的想法常常被认为不值得发表，因为任何人都可以做到。我认为这种心态完全是愚蠢的。”
- en: '**What are other issues in the machine learning community?**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习社区还有哪些其他问题？**'
- en: As the AI community doubles in size every few years, dominant scientists easily
    take over the mindsets of junior researchers. However, those dominant scientists
    who post tons of tweets and Facebook posts and have strong opinions on everything
    are not always the ones who made as strong contributions as they like to pretend.
    A community where a few dominant senior scientists are leading masses of junior
    researchers then starts to look like some sort of a cult. That means some ideas,
    techniques or models are blindly pushed forward without real evidence that such
    ideas are worth all the effort. For example, the Generative adversarial networks
    (GANs) seemed incredibly over-hyped. This is not a new phenomenon. When I was
    a student, I remember being confused by the popularity of Latent Dirichlet allocation
    — it also did not seem to work any better than simpler baselines. But nowadays
    I see this as a bigger problem, as information spreads much faster.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 随着AI社区每隔几年就翻倍增长，主导科学家容易左右初级研究者的思维。然而，那些发大量推文和Facebook帖子，对所有事情都有强烈意见的主导科学家，并不总是那些做出强大贡献的人。一个由少数主导的资深科学家领导大量初级研究者的社区看起来就像某种邪教。这意味着一些想法、技术或模型被盲目推动，没有真实证据表明这些想法值得付出所有努力。例如，生成对抗网络（GANs）看起来被过度炒作了。这不是新现象。当我还是学生时，我记得对Latent
    Dirichlet分配的受欢迎程度感到困惑——它似乎也不比简单的基线方法更有效。但如今我认为这是一个更大的问题，因为信息传播得更快。
- en: 'The over-emphasis on results achieved through brute force is illustrative of
    this problem. Many believe good models are complex-looking and full of hyper-parameters
    with little tweaks. If a simple idea that works is proposed, a reviewer often
    argues that anyone could have done it and, thus, it is not worth publishing. I
    have seen this several times and consider it completely dumb. In fact, I believe
    in the opposite: simple ideas that work in practice are the most valuable and
    difficult ones to discover. Sort of like in physics, where scientists are trying
    to develop more and more general theories that would explain as many phenomena
    as possible.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对通过蛮力取得的结果的过度强调体现了这个问题。许多人认为好的模型是看起来复杂且充满超参数的小调整。如果提出一个有效的简单想法，审稿人通常会争辩说任何人都可以做到，因此不值得发表。我已经见过这种情况几次，并且认为这完全愚蠢。实际上，我相信相反的观点：在实践中有效的简单想法是最有价值且最难发现的。就像物理学中，科学家们试图发展越来越通用的理论来解释尽可能多的现象一样。
- en: In fact, this happened to Word2Vec, but also to some of my language modeling
    work. When a poor reviewer sees two papers with similar ideas, but one paper also
    adds a dozen unnecessary tweaks, the poor reviewer will choose the complicated
    paper as the better one, as it seems more work went into it. In reality, the opposite
    is often true — if you can get state-of-the-art results with a simple idea, then
    the idea is probably really good.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这种情况发生在 Word2Vec 上，也发生在我一些语言建模工作上。当一个差劲的审稿人看到两篇有类似想法的论文，但其中一篇还添加了十几个不必要的改动时，这个差劲的审稿人会选择复杂的论文作为更好的那一篇，因为看起来投入的工作更多。实际上，情况往往正好相反——如果你能用一个简单的想法获得最先进的结果，那么这个想法可能真的非常好。
- en: '**How can we get better reviewers?**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们如何才能获得更好的审稿人？**'
- en: Machine learning could take inspiration from physics. Over centuries of research,
    physicists have aimed to create simple theories aiming to explain everything.
    Meanwhile, the opposite happens within machine learning. We should abandon the
    emphasis on state-of-the-art results and complex models and focus on discovering
    interesting new ideas. Of course, this is highly subjective, and it would be better
    if we could turn machine learning into an Olympics discipline with clear rules
    that decide who is better. But as I mentioned before, I don’t think this is easily
    achievable. Today, you can propose an amazing new idea that can potentially be
    the next state-of-the-art and still get discouraged and rejected by the community
    for not being state-of-the-art on some large benchmark. PhD students don’t get
    enough time to develop their own methods and approaches. We should change this
    and start rewarding novelty and simplicity, even if it is not easy to measure.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习可以从物理学中获得启发。在几个世纪的研究中，物理学家们旨在创建简单的理论以解释一切。与此同时，在机器学习领域则正好相反。我们应该放弃对最先进结果和复杂模型的强调，专注于发现有趣的新想法。当然，这高度主观，如果我们能将机器学习变成一个具有明确规则的奥林匹克项目来决定谁更优秀，那将更好。但正如我之前提到的，我认为这并不容易实现。今天，你可以提出一个惊人的新想法，可能成为下一个最先进的成果，但仍然会因为在某些大型基准上不够最先进而受到社区的打击和拒绝。博士生没有足够的时间来发展自己的方法和思路。我们应该改变这种情况，开始奖励新颖性和简洁性，即使这很难衡量。
- en: Perhaps you’ve heard of the reviewing experiment at NIPS. More groups of reviewers
    reviewed papers to see how much the accept/reject decisions correlate. And it
    was found that there was a strong correlation only for the very poor papers. In
    other words, the reviewing system is very random.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 或许你听说过 NIPS 的审稿实验。更多的审稿小组对论文进行评审，以查看接受/拒绝决定之间的相关性。结果发现，只有对非常差的论文才有很强的相关性。换句话说，审稿系统是非常随机的。
- en: We should aim to create a better reviewing system. Currently, we do not have
    quality feedback in the reviewing system; the system allows reviewers to be constantly
    wrong and still be able to review more papers. We should have databases of reviewers
    automatically tracking their performance. Their quality should be computed based
    on their ability to predict successful papers. For example, papers with excellent
    ideas but poor English should be accepted.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该致力于创建一个更好的审稿系统。目前，我们在审稿系统中没有质量反馈；系统允许审稿人持续犯错，并且仍然能够审阅更多的论文。我们应该有审稿人数据库，自动跟踪他们的表现。他们的质量应该根据预测成功论文的能力来计算。例如，拥有优秀想法但英语较差的论文应该被接受。
- en: '**In the IEEE SMC conference plenary talk, you described your focus on complex
    systems as the next step in artificial intelligence. Is this an approach to elegantly
    simplify the rules for computer scientists?**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**在IEEE SMC大会的全体报告中，你提到将复杂系统作为人工智能的下一步发展方向。这是一种优雅地简化计算机科学规则的方法吗？**'
- en: Complex systems are simple systems where complexity arises through emergent/evolutionary
    mechanisms that you don’t even specify. Let’s take ‘The Game of Life’ as an example.
    You start with something simple and then simulate the system until all kinds of
    complex structures emerge. This has always been my view of the universe. Many
    things around us seem complex. However, these complexities can be seen as by-products
    of evolution. Natural intelligence is a product of evolution. If we want to mimic
    this with artificial intelligence, we should follow a similar approach — allow
    AI to evolve and have the potential to increase in its complexity spontaneously.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂系统是简单系统中通过你未指定的涌现/进化机制产生的复杂性。以《生命游戏》为例。你从简单的东西开始，然后模拟系统直到各种复杂的结构出现。这一直是我对宇宙的看法。我们周围的许多事物看起来很复杂。然而，这些复杂性可以被视为进化的副产品。自然智能是进化的产物。如果我们想通过人工智能来模拟这一点，我们应该采用类似的方法——允许人工智能进化，并有潜力自发地增加其复杂性。
- en: '**How does this compare with evolutionary algorithms?**'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**这与进化算法相比如何？**'
- en: One could use evolutionary algorithms to approach this. However, I don’t think
    these algorithms capture evolution well. They perform stochastic optimization.
    If the fitness function improves, then you follow this random direction. Hence,
    the gradients are randomly selected instead of being computed. But this is not
    evolution, in my view — after all, evolutionary algorithms tend to stagnate rather
    quickly. Real evolution can be found in complex systems, even deterministic ones.
    Nothing is stochastic in the Game of life; you don’t have to roll a die. And still,
    you can see novel patterns arising. My goal is to create systems that can evolve
    spontaneously based on the emergence of complexity. I feel that discovering machine
    learning models that can implicitly grow in complexity has the potential to make
    our AI models much more powerful. It could be a way to make machine learning truly
    creative.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用进化算法来接近这一点。然而，我认为这些算法并没有很好地捕捉进化。它们进行随机优化。如果适应度函数有所改进，那么你就沿着这个随机方向前进。因此，梯度是随机选择的，而不是计算得出的。但在我看来，这不是进化——毕竟，进化算法往往很快陷入停滞。真实的进化可以在复杂系统中找到，即使是确定性的系统也是如此。《生命游戏》中没有任何随机性；你不需要掷骰子。即便如此，你仍然可以看到新颖的模式出现。我的目标是创建能够自发进化的系统，基于复杂性的涌现。我觉得发现能够在复杂性上隐式增长的机器学习模型具有使我们的AI模型更强大的潜力。这可能是让机器学习真正具有创造性的一种方式。
- en: '**How will you create such systems?**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**你将如何创建这样的系统？**'
- en: I suspect understanding emergence is needed to solve AI. However, we don’t understand
    this direction well. When I started working on recurrent neural networks, I hoped
    these could be a shortcut towards interesting, complex systems where emergence
    happens within the model’s memory. But typical recurrent network architectures
    have somewhat limited memory capacity. We need to design novel machine-learning
    models, training algorithms and evaluation metrics. I am working on this with
    my students.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我怀疑理解涌现现象是解决AI问题所必需的。然而，我们对这一方向的理解还不够深入。当我开始研究递归神经网络时，我希望这些能够成为通向有趣的复杂系统的捷径，其中涌现发生在模型的记忆中。但典型的递归网络架构具有一定的记忆容量限制。我们需要设计新颖的机器学习模型、训练算法和评估指标。我正与我的学生一起致力于这个工作。
- en: '**How would this contribute to the machine-learning community?**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**这将如何为机器学习社区做出贡献？**'
- en: The community has embodied a herding culture. We are all going in the same direction,
    building on the existing pile. This mindset might be strengthened by all the open-sourcing
    and public benchmarks I advocated for quite heavily. However, the approaches on
    which we are extending might be wrong. Then, everyone builds on flawed assumptions.
    If this is the case, it needs to be fixed. As I mentioned, we should explore different
    ideas and reward novelty within the research community.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 社区已经体现出了一种群体文化。我们都朝着同一个方向前进，建立在现有的基础上。这种心态可能因为我强烈倡导的开源和公共基准测试而得到了强化。然而，我们所扩展的方法可能是错误的。如果是这样的话，每个人都在建立在有缺陷的假设之上。如果是这样的话，就需要修正。正如我提到的，我们应该探索不同的想法，并在研究社区中奖励新颖性。
- en: '**This sounds like a reason not to open-source more.**'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**这听起来像是一个不再开源的理由。**'
- en: Open sourcing is, in my opinion, great, and we should continue. Remember that
    when almost nobody published code and datasets were private, researchers generally
    did not trust each other’s results. The language modeling community was pretty
    much dead.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在我看来，开源是很棒的，我们应该继续这样做。请记住，当几乎没有人发布代码和数据集都是私有的时，研究人员通常不会互相信任对方的结果。语言建模社区几乎已经死去。
- en: 'At the same time, we should avoid the dangers of open-sourcing: too much incremental
    work, minor tweaks that provide tiny improvements (and only sometimes), and discouragement
    of exploring novel ideas.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，我们应该避免开源的危险：过多的增量工作、提供微小改进的细微调整（有时仅仅如此），以及对探索新想法的气馁。
- en: '**This marks the end of the interview, is there a last remark you want to make?**'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**这标志着采访的结束，您还有什么最后的评论吗？**'
- en: We should be more open to originality and new directions. Yet, this is very
    difficult to judge. Do we want to see seemingly insane ideas at conferences? As
    a community, we need to make conferences more interesting so that you don’t just
    see hundreds of modifications of Transformers or their applications to hundreds
    of datasets. Let’s be more ambitious and more exploratory.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该对原创性和新方向更加开放。然而，这很难判断。我们是否希望在会议上看到看似疯狂的想法？作为一个社区，我们需要让会议变得更加有趣，而不仅仅是看到数百种Transformers的修改或它们在数百个数据集上的应用。让我们更有雄心，更具探索性。
- en: '*This interview is conducted on behalf of the* [*BNVKI*](http://www.bnvki.org/)*,
    the Benelux Association for Artificial Intelligence. We bring together AI researchers
    from Belgium, The Netherlands and Luxembourg.*'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*本次采访由* [*BNVKI*](http://www.bnvki.org/)*，即贝尔赫斯人工智能协会，进行。我们汇聚了来自比利时、荷兰和卢森堡的人工智能研究人员。*'
