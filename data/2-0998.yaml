- en: Google’s Latest Approaches to Multimodal Foundational Model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 谷歌对多模态基础模型的最新方法
- en: 原文：[https://towardsdatascience.com/googles-latest-approaches-to-multimodal-foundational-model-beedaced32f9](https://towardsdatascience.com/googles-latest-approaches-to-multimodal-foundational-model-beedaced32f9)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/googles-latest-approaches-to-multimodal-foundational-model-beedaced32f9](https://towardsdatascience.com/googles-latest-approaches-to-multimodal-foundational-model-beedaced32f9)
- en: Multimodal foundational models are even more exciting than large language models.
    Let’s review Google research’s recent progress to have a glimpse of the bleeding
    edge.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多模态基础模型比大型语言模型更令人兴奋。让我们回顾谷歌研究的最新进展，以窥见前沿技术。
- en: '[](https://eileen-code4fun.medium.com/?source=post_page-----beedaced32f9--------------------------------)[![Eileen
    Pangu](../Images/cbdab572af709b6e6b52cb3a078f220d.png)](https://eileen-code4fun.medium.com/?source=post_page-----beedaced32f9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----beedaced32f9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----beedaced32f9--------------------------------)
    [Eileen Pangu](https://eileen-code4fun.medium.com/?source=post_page-----beedaced32f9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://eileen-code4fun.medium.com/?source=post_page-----beedaced32f9--------------------------------)[![Eileen
    Pangu](../Images/cbdab572af709b6e6b52cb3a078f220d.png)](https://eileen-code4fun.medium.com/?source=post_page-----beedaced32f9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----beedaced32f9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----beedaced32f9--------------------------------)
    [Eileen Pangu](https://eileen-code4fun.medium.com/?source=post_page-----beedaced32f9--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----beedaced32f9--------------------------------)
    ·7 min read·Aug 12, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----beedaced32f9--------------------------------)
    ·7 min read·2023年8月12日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/4c992bcc9d7d065534e2c5fec5e58b79.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c992bcc9d7d065534e2c5fec5e58b79.png)'
- en: 'Image source: [https://unsplash.com/photos/U3sOwViXhkY](https://unsplash.com/photos/U3sOwViXhkY)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[https://unsplash.com/photos/U3sOwViXhkY](https://unsplash.com/photos/U3sOwViXhkY)
- en: '**Background**'
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**背景**'
- en: While the hype on large language model (LLM) is still iron hot in the industry,
    the leading research organizations have turned their eyes to multimodal foundational
    models — models that have the same scale and versatility characteristics as LLM
    but can handle data beyond just text, such as images, audio, sensor signals, and
    so on. Multimodal foundational models are believed by many to be the key to unlock
    the next phase of Artificial Intelligence (AI) advance.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大型语言模型（LLM）的炒作在业界依旧火热，但领先的研究机构已经将目光转向多模态基础模型——这些模型具有与LLM相同的规模和多功能特性，但可以处理文本之外的数据，如图像、音频、传感器信号等。许多人认为，多模态基础模型是解锁人工智能（AI）下一阶段进步的关键。
- en: In this blog post, we take a closer look at how Google approaches multimodal
    foundational models. The content covered in this blog post is drawn from the key
    methods and insights of Google’s recent papers, for which we provide references
    at the end of this article.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我们将更详细地了解谷歌如何处理多模态基础模型。本文的内容基于谷歌最近论文中的关键方法和见解，我们在文章末尾提供了相关参考。
- en: '**Why Should You Care**'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**你为什么应该关心**'
- en: 'Multimodal foundational models are exciting, but why should you care? You may
    be:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态基础模型令人兴奋，但你为什么应该关心？你可能是：
- en: an AI/ML practitioner who wants to catch up with the latest research development
    of the field, but you don’t have the patience to go through dozens of new papers
    and hundreds of pages of surveys.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一位希望跟上该领域最新研究进展的AI/ML从业者，但又没有耐心阅读几十篇新论文和数百页的综述。
- en: a current or emerging industry leader who is wondering what’s next after large
    language models, and is thinking about how to align your business with the new
    trends in the tech world.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一位当前或即将成为行业领导者的人，想知道大型语言模型之后会发生什么，并考虑如何将你的业务与科技界的新趋势对齐。
- en: a curious reader who may end up being the consumer of current or future multimodal
    AI products, and wants to get a visual and intuitive understanding of how things
    work behind the scenes.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一位好奇的读者，可能会成为当前或未来多模态AI产品的消费者，并希望对幕后工作的原理有一个直观的了解。
- en: For all the above audiences, this article will provide a good overview to jump-start
    your understanding of multimodal foundational models, which is a corner stone
    for future more accessible and helpful AI.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对于以上所有读者，本文将提供一个很好的概述，帮助你启动对多模态基础模型的理解，这是未来更易于访问和有用的AI的基石。
- en: 'One more thing to note before we dive in: when people talk about multimodal
    foundational models, they often mean the input is multimodal, consisting of text,
    images, videos, signals, etc. The output, however, is always just text. The reason
    is that text is the most generic form of communication format. If we can output
    text, we can output code and feed it to downstream tools to generate whatever
    multimedia formats desired. Not to mention there are many AI tools nowadays that
    take natural language as input and generate images and sounds.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨之前还有一点需要注意：当人们谈论多模态基础模型时，他们通常指的是输入是多模态的，包括文本、图像、视频、信号等。然而，输出总是只是文本。原因在于文本是最通用的沟通格式。如果我们能输出文本，就能输出代码并将其输入到下游工具中，以生成所需的多媒体格式。更不用说现在有很多AI工具可以将自然语言作为输入，生成图像和声音。
- en: '**Start With Just Text**'
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**从文本开始**'
- en: The first format to incorporate is text, or in research terms, language. Researchers
    like to call it language because language implies that the text is drawn from
    a linguistic system with a coherent semantic meaning, be it any natural language,
    code, commands, symbols, and so on. In layman’s terms, we’ll just call it text
    in this blog post.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 首先需要包含的格式是文本，或者用研究术语来说，是语言。研究人员喜欢称之为语言，因为语言意味着文本来源于具有连贯语义意义的语言系统，无论是任何自然语言、代码、命令、符号等等。通俗来说，我们在这篇博客文章中就称之为文本。
- en: There are a lot of materials online about building large language models, so
    we won’t cover it in depth here. Just note that Google’s recent results on LLMs
    and multimodal foundational models are all built on PaLM [1]. If you’re not familiar
    with it, just think of it as a black box similar to ChatGPT, though their internal
    mechanics vary quite a bit.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 关于构建大型语言模型的材料在线上有很多，所以我们在这里不会深入讨论。只需注意，谷歌最近在LLMs和多模态基础模型上的结果都是建立在PaLM [1]之上的。如果你不熟悉它，可以将其视为类似于ChatGPT的黑箱，尽管它们的内部机制差异很大。
- en: The fundamental concept to grasp here is that the input text is broken into
    tokens. Each token is then mapped into an embedding space. Effectively each token
    becomes a vector. Then the list of the vectors are fed into a model consisting
    of layers of attention based networks. The output of the model is also a list
    of vectors, which can be converted back to tokens, and then concatenated to form
    the final text output.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要掌握的基本概念是，输入文本被分解为标记。每个标记随后被映射到一个嵌入空间中。实际上，每个标记变成了一个向量。然后，这些向量的列表被输入到由注意力机制网络层组成的模型中。模型的输出也是一个向量列表，可以被转换回标记，然后拼接形成最终的文本输出。
- en: The model has hundreds of billions of parameters, and is trained on a massive
    amount of text data for some simple prediction tasks such as given a text prefix,
    and predicting the next token.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型拥有数百亿个参数，并在大量的文本数据上进行训练，执行一些简单的预测任务，例如给定一个文本前缀，预测下一个标记。
- en: The resulting model will need to go through further fine-tuning to become more
    directly useful. For example, Google has adapted its PaLM model to the medical
    domain and created MedPaLM [2]. You can reference this [article](/how-to-use-large-language-models-llm-in-your-own-domains-b4dff2d08464)
    for more insights on how to adapt a large language model to your specific domain.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的模型将需要经过进一步的微调才能变得更直接有用。例如，谷歌已经将其PaLM模型应用于医学领域，并创建了MedPaLM [2]。你可以参考这篇[文章](/how-to-use-large-language-models-llm-in-your-own-domains-b4dff2d08464)以获取更多关于如何将大型语言模型适应到特定领域的见解。
- en: At this point, we have a large language model. The next step is to incorporate
    other media formats.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们拥有一个大型语言模型。下一步是融入其他媒体格式。
- en: '**Incorporate Image**'
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**融入图像**'
- en: To incorporate images, the first thing to do is to convert an image into a format
    that can be processed by the large language model. Because we already have the
    large language model and we want multimedia to be processed uniformly, the natural
    choice is to convert an image into a list of embedding vectors. Recall that text
    is also converted into a list of embedding vectors above. Obviously, the vectors
    from image and text should all have the same dimension.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了融合图像，首先要做的是将图像转换为可以被大语言模型处理的格式。因为我们已经有了大语言模型，并且希望多模态信息能够被统一处理，自然的选择是将图像转换为嵌入向量列表。回想一下，文本也被转换成了上述的嵌入向量列表。显然，图像和文本的向量应具有相同的维度。
- en: Google has done impressive work in adopting transformer architecture for image
    processing [3] [4]. That turns out to be a great fit for the image to vectors
    conversion.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌在采用Transformer架构进行图像处理方面做了令人印象深刻的工作[3][4]。这被证明非常适合图像到向量的转换。
- en: Concretely, every image is resized to 224×224 resolution, and then broken into
    14×14 pixels patches. This results in 224×234/14/14=256 patches. Each patch goes
    through the same learnable linear transformation to transform into a vector. Each
    vector is concatenated with a learnable positional embedding. These concatenated
    vectors are then fed into a Transformer model [5]. Google research called this
    model architecture ViT— visual Transformer. The output of the ViT model is also
    256 vectors. A shallow network is then attached to the output vectors from the
    ViT model, and the whole architecture is trained for image classification, image
    captioning, and other image processing tasks. See Figure-1 for an illustration.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，每张图像被调整为224×224的分辨率，然后被拆分成14×14像素的片段。这会产生224×224/14/14=256个片段。每个片段经过相同的可学习线性变换以转换为向量。每个向量与一个可学习的位置嵌入进行拼接。这些拼接的向量然后输入到Transformer模型[5]中。谷歌研究将这一模型架构称为ViT——视觉Transformer。ViT模型的输出也是256个向量。然后，将一个浅层网络附加到ViT模型的输出向量上，并对整个架构进行训练，以进行图像分类、图像描述和其他图像处理任务。有关示意图，请参见图-1。
- en: '![](../Images/14594b2ba46100f5bda3da94b55c2aa4.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14594b2ba46100f5bda3da94b55c2aa4.png)'
- en: 'Figure-1: illustration of ViT architecture. Image from author.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图-1：ViT架构的示意图。图片来自作者。
- en: Once the image processing training is done, throw away the shallow network on
    the top, we can use the 256 output vectors from ViT as the embedding representation
    of the image. The core idea is that through large scale training, the ViT model
    has learned to represent the input image by a list of vectors that can be easily
    used as input features for a variety of image processing tasks.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成图像处理训练，丢弃顶部的浅层网络，我们可以使用ViT的256个输出向量作为图像的嵌入表示。核心思想是，通过大规模训练，ViT模型已经学会用一系列向量表示输入图像，这些向量可以轻松用作各种图像处理任务的输入特征。
- en: One special note here is that Google research found that ViT’s results on image
    processing is superior to state of the art CNN. They attributed the superior performance
    to the scale (Transformer based architecture is known to scale better) and the
    fact that attention can happen directly between any 2 input patches early on in
    the network.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个特别的说明，即谷歌研究发现ViT在图像处理上的结果优于最先进的CNN。他们将这一优越性能归因于规模（基于Transformer的架构被认为有更好的扩展性）以及注意力机制可以在网络早期直接作用于任何两个输入片段的事实。
- en: This image representation learning done in ViT is transferable to multimodal
    settings, as you guessed it. The output vectors from the pretrained ViT can serve
    as input to the large language model.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在ViT中完成的图像表示学习可以转移到多模态环境中，正如你所猜到的那样。来自预训练ViT的输出向量可以作为大语言模型的输入。
- en: For an input prompt such as `what happened between <imgA> and <imgB>`, the words
    are converted into embedding vectors for the large language model as usual; each
    image is converted into 256 vectors using ViT. All the vectors together form the
    input to the large language model. Finally, the large language model is fine-tuned
    end to end (or by freezing its various parts) on multimedia training data, and
    the resulting model is the multimodal foundational model. See Figure-2 for an
    illustration.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输入提示如`what happened between <imgA> and <imgB>`，词语按惯例转换为大语言模型的嵌入向量；每张图像通过ViT转换为256个向量。所有向量一起形成大语言模型的输入。最后，大语言模型在多模态训练数据上进行端到端微调（或冻结其各个部分），最终得到的模型就是多模态基础模型。有关示意图，请参见图-2。
- en: '![](../Images/4989f2a7ec72bf29ad5c19ad46caad0e.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4989f2a7ec72bf29ad5c19ad46caad0e.png)'
- en: 'Figure-2: illustration of multimodal foundational model architecture. Image
    from author.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '图-2: 多模态基础模型架构的示意图。图片来自作者。'
- en: Obviously, the dimension numbers mentioned above are just one configuration
    Google research’s papers used [6]. You can try your own if you want. But be aware
    that the ViT output vector dimension should match the larger language model input
    vector dimension. If different, you can adjust either models or you can insert
    a linear transformation between the ViT output and the large language model input
    to squeeze or stretch the ViT output to match the large language model input dimension.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，上述维度数字只是谷歌研究论文使用的其中一种配置[6]。如果你愿意，可以尝试自己的配置。但要注意，ViT 输出向量的维度应与较大的语言模型输入向量的维度匹配。如果不同，你可以调整任一模型，或者在
    ViT 输出和大型语言模型输入之间插入一个线性变换，以压缩或扩展 ViT 输出以匹配大型语言模型的输入维度。
- en: '**Extend to Other Media and Domain**'
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**扩展到其他媒体和领域**'
- en: Now that we know how to incorporate images, we can employ a similar scheme to
    include other media types. They don’t necessarily have to use a Transformer architecture.
    Google research employed a multimodal foundational model in robotics [7] by converting
    states, objects in scene, and other input sources into the same vector embedding
    space. Many of those input formats are actually converted to the embedding space
    using very simple transformations. Google research has also recently adapted the
    methodology to the medical domain and created a multimodal foundational model
    for healthcare [8]. See Figure-3 for an illustration of their utility.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们知道了如何整合图像，我们可以采用类似的方案来包含其他媒体类型。它们不一定要使用 Transformer 架构。谷歌研究在机器人领域[7]中使用了多模态基础模型，通过将状态、场景中的对象和其他输入源转换为相同的向量嵌入空间来实现。这些输入格式中的许多实际上是通过非常简单的变换转换到嵌入空间的。谷歌研究最近还将这一方法适应于医疗领域，并创建了一个用于医疗保健的多模态基础模型[8]。有关它们的用途，请参见图-3。
- en: '![](../Images/e157128e75cea13e1cdb7696f81c6cff.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e157128e75cea13e1cdb7696f81c6cff.png)'
- en: 'Figure-3: illustration of multimodal foundational model use cases in (a) robotics
    and (b) medical field. Image from papers in references [7] and [8]'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '图-3: 多模态基础模型在 (a) 机器人技术和 (b) 医疗领域的应用示例。图片来源于参考文献[7]和[8]'
- en: '**Conclusion**'
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**结论**'
- en: 'In this blog post, we had a glimpse of where the bleeding edge is in the field
    of multimodal foundational models. The core idea is two-fold: (1) encode multimedia
    in the same embedding space so that they can be processed uniformly by LLM architecture;
    (2) stitch together large pre-trained models (they are often pre-trained in unimodal
    settings) and fine-tune with multimedia training data.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我们窥见了多模态基础模型领域的前沿。核心思想有两个方面：（1）将多媒体编码到相同的嵌入空间，以便可以由 LLM 架构统一处理；（2）将大型预训练模型（它们通常在单模态设置中预训练）结合在一起，并使用多媒体训练数据进行微调。
- en: Some of Google research’s recent breakthrough really starts to resemble an AI
    agent that people would want and could use in day to day life. While there is
    still a long way to go for more accessible and helpful AI, there is no doubt that
    multimodal foundational models are the inflection point of AI, and future development
    is going to be ever more exciting.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌研究最近的一些突破确实开始接近人们希望并可以在日常生活中使用的 AI 代理。虽然还有很长的路要走才能让 AI 更加易用和有帮助，但毫无疑问，多模态基础模型是
    AI 的拐点，未来的发展将会更加激动人心。
- en: '**References**'
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: '[1] PaLM: Scaling Language Modeling with Pathways [https://arxiv.org/abs/2204.02311](https://arxiv.org/abs/2204.02311)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] PaLM: 使用路径扩展语言建模 [https://arxiv.org/abs/2204.02311](https://arxiv.org/abs/2204.02311)'
- en: '[2] Large Language Models Encode Clinical Knowledge [https://arxiv.org/abs/2212.13138](https://arxiv.org/abs/2212.13138)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] 大型语言模型编码临床知识 [https://arxiv.org/abs/2212.13138](https://arxiv.org/abs/2212.13138)'
- en: '[3] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
    [https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] 一张图胜过 16x16 个词：用于大规模图像识别的 Transformer [https://arxiv.org/abs/2010.11929](https://arxiv.org/abs/2010.11929)'
- en: '[4] Scaling Vision Transformers [https://arxiv.org/abs/2106.04560](https://arxiv.org/abs/2106.04560)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] 扩展视觉 Transformer [https://arxiv.org/abs/2106.04560](https://arxiv.org/abs/2106.04560)'
- en: '[5] Attention is All You Need [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] 注意力机制是你所需要的一切 [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
- en: '[6] PaLI: A Jointly-Scaled Multilingual Language-Image Model [https://arxiv.org/abs/2209.06794](https://arxiv.org/abs/2209.06794)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] PaLI: 联合缩放的多语言语言-图像模型 [https://arxiv.org/abs/2209.06794](https://arxiv.org/abs/2209.06794)'
- en: '[7] PaLM-E: An Embodied Multimodal Language Model [https://arxiv.org/abs/2303.03378](https://arxiv.org/abs/2303.03378)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] PaLM-E：一种具身的多模态语言模型 [https://arxiv.org/abs/2303.03378](https://arxiv.org/abs/2303.03378)'
- en: '[8] Towards Generalist Biomedical AI [https://arxiv.org/abs/2307.14334](https://arxiv.org/abs/2307.14334)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] 朝向通用生物医学人工智能 [https://arxiv.org/abs/2307.14334](https://arxiv.org/abs/2307.14334)'
