- en: Accelerating PyTorch Training Workloads with FP8 — Part 1
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 FP8 加速 PyTorch 训练工作负载 — 第 1 部分
- en: 原文：[https://towardsdatascience.com/accelerating-pytorch-training-workloads-with-fp8-5a5123aec7d7?source=collection_archive---------4-----------------------#2023-11-15](https://towardsdatascience.com/accelerating-pytorch-training-workloads-with-fp8-5a5123aec7d7?source=collection_archive---------4-----------------------#2023-11-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/accelerating-pytorch-training-workloads-with-fp8-5a5123aec7d7?source=collection_archive---------4-----------------------#2023-11-15](https://towardsdatascience.com/accelerating-pytorch-training-workloads-with-fp8-5a5123aec7d7?source=collection_archive---------4-----------------------#2023-11-15)
- en: How to make the most of your modern-day GPU
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何最大化利用现代 GPU
- en: '[](https://chaimrand.medium.com/?source=post_page-----5a5123aec7d7--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page-----5a5123aec7d7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5a5123aec7d7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5a5123aec7d7--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page-----5a5123aec7d7--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://chaimrand.medium.com/?source=post_page-----5a5123aec7d7--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page-----5a5123aec7d7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5a5123aec7d7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5a5123aec7d7--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page-----5a5123aec7d7--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9440b37e27fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faccelerating-pytorch-training-workloads-with-fp8-5a5123aec7d7&user=Chaim+Rand&userId=9440b37e27fe&source=post_page-9440b37e27fe----5a5123aec7d7---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5a5123aec7d7--------------------------------)
    ·9 min read·Nov 15, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5a5123aec7d7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faccelerating-pytorch-training-workloads-with-fp8-5a5123aec7d7&user=Chaim+Rand&userId=9440b37e27fe&source=-----5a5123aec7d7---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9440b37e27fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faccelerating-pytorch-training-workloads-with-fp8-5a5123aec7d7&user=Chaim+Rand&userId=9440b37e27fe&source=post_page-9440b37e27fe----5a5123aec7d7---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5a5123aec7d7--------------------------------)
    ·9 分钟阅读·2023 年 11 月 15 日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5a5123aec7d7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faccelerating-pytorch-training-workloads-with-fp8-5a5123aec7d7&user=Chaim+Rand&userId=9440b37e27fe&source=-----5a5123aec7d7---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5a5123aec7d7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faccelerating-pytorch-training-workloads-with-fp8-5a5123aec7d7&source=-----5a5123aec7d7---------------------bookmark_footer-----------)![](../Images/39337db002266767825d449c2f59b599.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5a5123aec7d7&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faccelerating-pytorch-training-workloads-with-fp8-5a5123aec7d7&source=-----5a5123aec7d7---------------------bookmark_footer-----------)![](../Images/39337db002266767825d449c2f59b599.png)'
- en: Photo by [Deva Darshan](https://unsplash.com/@darshan394?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 [Deva Darshan](https://unsplash.com/@darshan394?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: The past few years have seen revolutionary advancements in the field of AI,
    perhaps best exemplified by the recent popularity and proliferation of LLM-based
    applications such as [ChatGPT](https://en.wikipedia.org/wiki/ChatGPT). These breakthroughs
    have been powered by equally exciting developments in the machinery used to train
    AI models. New and innovative architectures, sophisticated tensor processing cores,
    and dedicated HW accelerators have enabled the convergence of AI models of ever-increasing
    sizes, at faster and faster rates. In this post, we will focus on one particular
    advancement in AI-specialized HW — **the inclusion of dedicated 8-bit floating-point
    (FP8) tensor processing cores**. Appearing in the most modern AI HW architectures
    (e.g., [Nvidia Hopper](https://www.nvidia.com/en-eu/data-center/technologies/hopper-architecture/),
    [Nvidia Ada Lovelace](https://www.nvidia.com/en-eu/geforce/ada-lovelace-architecture/),
    and [Habana Gaudi2](https://www.intel.com/content/www/us/en/developer/articles/technical/habana-gaudi2-processor-for-deep-learning.html))
    the FP8 tensor cores enable a significant increase in floating-point operations
    per second (FLOPS), as well as opportunities for memory optimization and energy
    savings for both AI training and inference workloads.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 过去几年在 AI 领域取得了革命性进展，最近最好的例证是基于 LLM 的应用程序（如[ChatGPT](https://en.wikipedia.org/wiki/ChatGPT)）的普及和流行。这些突破是由用于训练
    AI 模型的设备同样令人振奋的发展推动的。新颖而创新的架构、复杂的张量处理核心和专用的 HW 加速器使得 AI 模型的收敛速度越来越快，规模也越来越大。在这篇文章中，我们将专注于
    AI 专用 HW 中的一项特定进展 —— **专用的 8 位浮点（FP8）张量处理核心的加入**。出现在最现代的 AI HW 架构中（例如[Nvidia Hopper](https://www.nvidia.com/en-eu/data-center/technologies/hopper-architecture/)、[Nvidia
    Ada Lovelace](https://www.nvidia.com/en-eu/geforce/ada-lovelace-architecture/)和[Habana
    Gaudi2](https://www.intel.com/content/www/us/en/developer/articles/technical/habana-gaudi2-processor-for-deep-learning.html)），FP8
    张量核心使得每秒浮点运算次数（FLOPS）显著增加，同时为 AI 训练和推断工作负载的内存优化和能耗节约提供了机会。
- en: Taking advantage of the HW-level FP8 capabilities requires appropriate support
    in the SW stack and development framework that we use to build our AI training
    and inference applications. In this post we will describe how to **modify a PyTorch
    training script so as to utilize the built-in support for the FP8 datatype of
    an** [**Nvidia H100 GPU**](https://www.nvidia.com/en-eu/data-center/h100/). We
    will start by providing some motivation for the use of the FP8 datatype. We will
    then review the FP8-specific PyTorch API support exposed by the [Transformer Engine](https://github.com/NVIDIA/TransformerEngine)
    library and show how to integrate them into a simple training script. Although
    we will **not** go into the theory behind the use of FP8 for AI training, we will
    note the potential challenges involved in its use. Last, we will demonstrate the
    significant optimization opportunities of the FP8 datatype.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 利用 HW 级别的 FP8 能力需要我们在构建 AI 训练和推断应用程序时，使用适当的 SW 栈和开发框架提供支持。在这篇文章中，我们将描述如何修改一个
    PyTorch 训练脚本，以利用[**Nvidia H100 GPU**](https://www.nvidia.com/en-eu/data-center/h100/)的内置
    FP8 数据类型支持。我们将从提供使用 FP8 数据类型的动机开始。接着，我们将审查由[Transformer Engine](https://github.com/NVIDIA/TransformerEngine)库提供的
    FP8 特定的 PyTorch API 支持，并展示如何将它们集成到一个简单的训练脚本中。虽然我们不会深入探讨 FP8 在 AI 训练中的理论背景，但我们将指出使用它可能涉及的潜在挑战。最后，我们将展示
    FP8 数据类型的显著优化机会。
- en: Disclaimers
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 免责声明
- en: Please do not interpret our mention of any SW component, methodology, or service
    as an endorsement for its use. The best design for ML development will vary greatly
    based on the specific details of your own AI workload. Please also keep in mind
    that the APIs and behaviors of some of the SW packages and components we will
    mention may change by the time you read this post. You are highly encouraged to
    evaluate any potential design decisions based on the most up to date HW and SW
    available.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 请不要将我们提到的任何 SW 组件、方法或服务解读为对其使用的认可。ML 开发的最佳设计将根据您自己 AI 工作负载的具体细节而大相径庭。请还记住，我们将提及的一些
    SW 包和组件的 API 和行为可能在您阅读本文时已经发生变化。强烈建议您根据最新的 HW 和 SW 来评估任何潜在的设计决策。
- en: Motivation
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动机
- en: As AI models grow more and more sophisticated, so does the machinery required
    to train them. The [**Nvidia H100 GPU**](https://www.nvidia.com/en-eu/data-center/h100/),
    said to support “unprecedented performance and scalability”, is (at the time of
    this writing) Nvidia’s newest and strongest AI accelerator, purposely designed
    with the goal of enabling the AI development of the next generation. With the
    current AI hype in full swing, the demand for these GPUs has been huge (e.g.,
    see [here](https://www.hpcwire.com/2023/08/17/nvidia-h100-are-550000-gpus-enough-for-this-year/)).
    Accordingly, and unsurprisingly, the cost of these GPUs has been extremely high
    — perhaps even **forbidding** for many of our readers. Fortunately, cloud service
    providers such as AWS, GCP, and Microsoft Azure, offer “pay as you go” (per hour/per
    second) access to H100 powered machines thereby opening up the opportunity for
    their use to a much greater community of AI developers.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 AI 模型越来越复杂，训练它们所需的硬件也变得越来越复杂。被称为支持“前所未有的性能和可扩展性”的 [**Nvidia H100 GPU**](https://www.nvidia.com/en-eu/data-center/h100/)
    是（在撰写本文时）Nvidia 最新、最强大的 AI 加速器，专门设计用于支持下一代 AI 开发。随着当前 AI 热潮的全面展开，对这些 GPU 的需求非常巨大（例如，请见
    [这里](https://www.hpcwire.com/2023/08/17/nvidia-h100-are-550000-gpus-enough-for-this-year/)）。因此，不出所料，这些
    GPU 的成本非常高 — 对许多读者而言，甚至可能是**令人望而却步的**。幸运的是，AWS、GCP 和 Microsoft Azure 等云服务提供商提供了“按需付费”（按小时/按秒）使用
    H100 驱动的机器的机会，从而使更广泛的 AI 开发者社区能够使用它们。
- en: In AWS, H100 GPUs are offered as a component of the [recently announced](https://aws.amazon.com/blogs/aws/new-amazon-ec2-p5-instances-powered-by-nvidia-h100-tensor-core-gpus-for-accelerating-generative-ai-and-hpc-applications/)
    [AWS EC2 p5 instance family](https://aws.amazon.com/ec2/instance-types/p5/). These
    instances are claimed to **“accelerate your time to solution by up to 4x compared
    to previous-generation GPU-based EC2 instances and reduce cost to train ML models
    by up to 40%”**.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在 AWS 中，H100 GPU 作为 [最近宣布的](https://aws.amazon.com/blogs/aws/new-amazon-ec2-p5-instances-powered-by-nvidia-h100-tensor-core-gpus-for-accelerating-generative-ai-and-hpc-applications/)
    [AWS EC2 p5 实例系列](https://aws.amazon.com/ec2/instance-types/p5/)的一部分提供。这些实例被宣称能够**“相比于前一代基于
    GPU 的 EC2 实例，将解决问题的时间缩短最多 4 倍，并将训练 ML 模型的成本降低最多 40%”**。
- en: In a [recent post](/instance-selection-for-deep-learning-7463d774cff0) we discussed
    some of the considerations that should go into the choice of an ML training instance.
    We highlighted the fact that the most optimal instance type will be very-much
    dependent on the project at hand. Specifically, when it comes to ML training instances
    — **bigger is *not* always better**. This is particularly true of the p5 instance
    family. True — the p5 will likely out-perform any other instance type — after
    all, the H100 is an undisputed performance beast. But once you factor in the cost
    of the p5 ($98.32 per hour for the 8-GPU p5.48xlarge instance — at the time of
    this writing), you might find other instance types to be more suitable.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [最近的一篇文章](/instance-selection-for-deep-learning-7463d774cff0) 中，我们讨论了选择 ML
    训练实例时应考虑的一些因素。我们强调了最优化的实例类型将非常依赖于具体项目。特别是在 ML 训练实例方面 — **更大并*不*总是更好**。这一点在 p5
    实例系列中尤为明显。确实，p5 实例可能会超越其他任何实例类型 — 毕竟，H100 是无可争议的性能怪兽。但一旦你考虑到 p5 的成本（在撰写本文时，8-GPU
    p5.48xlarge 实例的价格为每小时 $98.32），你可能会发现其他实例类型更为合适。
- en: In the next section we will train a relatively large computer vision model on
    a p5.48xlarge and compare its performance to a [p4d.24xlarge](https://aws.amazon.com/ec2/instance-types/p4/)
    containing 8 [Nvidia A100 GPUs](https://www.nvidia.com/en-eu/data-center/a100/).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将会在 p5.48xlarge 上训练一个相对较大的计算机视觉模型，并将其性能与包含 8 个 [Nvidia A100 GPU](https://www.nvidia.com/en-eu/data-center/a100/)
    的 [p4d.24xlarge](https://aws.amazon.com/ec2/instance-types/p4/) 进行比较。
- en: Toy Model
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 玩具模型
- en: In the code-block below we define a [Vision Transformer](https://en.wikipedia.org/wiki/Vision_transformer)
    (ViT)-backed classification model (using the popular [timm](https://pypi.org/project/timm/)
    Python package version 0.9.10) along with a randomly generated dataset. ViT backbones
    come in many shapes and sizes. Here we have chosen what is often referred to as
    the ViT-Huge configuration — with **632** million parameters — in order to take
    better advantage of the capacity the H100 has for large models.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码块中，我们定义了一个以 [Vision Transformer](https://en.wikipedia.org/wiki/Vision_transformer)
    (ViT) 为基础的分类模型（使用流行的 [timm](https://pypi.org/project/timm/) Python 包版本 0.9.10）以及一个随机生成的数据集。ViT
    骨干网有多种形态和尺寸。这里我们选择了通常称为 ViT-Huge 配置的模型 — 具有 **632** 百万参数 — 以更好地利用 H100 对大模型的容量。
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We trained this model on both the [p5.48xlarge](https://aws.amazon.com/ec2/instance-types/p5/)
    and [p4d.24xlarge](https://aws.amazon.com/ec2/instance-types/p4/) instance types
    using the dedicated PyTorch 2.1 [AWS deep learning container](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)
    (763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:2.1.0-gpu-py310-cu121-ubuntu20.04-ec2).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[p5.48xlarge](https://aws.amazon.com/ec2/instance-types/p5/)和[p4d.24xlarge](https://aws.amazon.com/ec2/instance-types/p4/)实例类型上训练了这个模型，使用的是专用的
    PyTorch 2.1 [AWS 深度学习容器](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)（763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:2.1.0-gpu-py310-cu121-ubuntu20.04-ec2）。
- en: Unsurprisingly, the p5 step-time performance blows away the p4d performance
    — 0.199 seconds per step compared to 0.41 — more than twice as fast!! That would
    mean halving the time to train your large ML models. However, when you take into
    account the difference in cost ($32.77 per-hour for the p4d vs $98.32 per-hour
    for the p5 — as of the time of this writing) a completely different story unfolds.
    The **price-performance of the p5 is ~30% worse than the p4d**!! This is very
    far from the 40% improvement that appeared in the [p5 announcement](https://aws.amazon.com/blogs/aws/new-amazon-ec2-p5-instances-powered-by-nvidia-h100-tensor-core-gpus-for-accelerating-generative-ai-and-hpc-applications/).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 不出所料，p5 的每步时间性能远远超过 p4d 的性能 —— 每步 0.199 秒对比 0.41 秒 —— 快了两倍多！！这意味着训练大型 ML 模型的时间将减少一半。然而，当你考虑到成本差异（p4d
    每小时 32.77 美元 vs p5 每小时 98.32 美元 —— 截至本篇文章撰写时），一个完全不同的故事展现在眼前。**p5 的性价比比 p4d 差
    ~30%**！！这远未达到在[p5 宣布](https://aws.amazon.com/blogs/aws/new-amazon-ec2-p5-instances-powered-by-nvidia-h100-tensor-core-gpus-for-accelerating-generative-ai-and-hpc-applications/)中提到的
    40% 改进。
- en: At this point you might draw one of two possible conclusions. The first possibility
    is that, despite all the hype, the p5 is simply not the right machine for you.
    The second is that the p5 could still be viable, but that adaptations would be
    required to your model in order to take full advantage of its potential. In the
    next sections we will adopt the second approach and demonstrate how using the
    FP8 datatype — unique to the p5 instance type — can completely alter the comparative
    price-performance results.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 此时你可能得出两种可能的结论。第一种可能是，尽管有很多宣传，p5 可能根本不是适合你的机器。第二种可能是 p5 可能仍然具有潜力，但需要对模型进行调整，以充分利用其潜力。在接下来的章节中，我们将采用第二种方法，演示如何使用
    FP8 数据类型 —— p5 实例类型特有 —— 完全改变比较的性价比结果。
- en: Integrating FP8 with Transformer Engine
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将 FP8 与 Transformer Engine 集成
- en: The first thing we should emphasize is that, as of the time of this writing,
    PyTorch (version 2.1) does not include a native 8-bit floating datatype. To program
    our script to use FP8 we will use [Transformer Engine](https://github.com/NVIDIA/TransformerEngine)
    (TE) a dedicated library for accelerating Transformer models on NVIDIA GPUs. TE
    (version 0.12) comes preinstalled in the AWS PyTorch 2.1 DL container.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该强调的是，截至撰写本文时，PyTorch（版本 2.1）不包括本地的 8 位浮点数据类型。为了让我们的脚本使用 FP8，我们将使用[Transformer
    Engine](https://github.com/NVIDIA/TransformerEngine)（TE），这是一个加速 NVIDIA GPU 上 Transformer
    模型的专用库。TE（版本 0.12）已预装在 AWS PyTorch 2.1 DL 容器中。
- en: Although the theory behind the use of FP8 for training is beyond the scope of
    this post (e.g., see [here](https://arxiv.org/pdf/2209.05433.pdf)), it is important
    to be aware that **the mechanics of using FP8 are far more complex than the**
    [**16-bit alternatives**](https://pytorch.org/docs/stable/amp.html) (float16 and
    bfloat16). Fortunately, the TE imlementation hides all of the messy details from
    the user. Please see the official [documentation](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/examples/fp8_primer.html)
    as well as this simple [example](https://github.com/NVIDIA/TransformerEngine/blob/main/examples/pytorch/mnist/main.py)
    for instructions on how to use the TE APIs. To learn more about what is going
    on behind the scenes be sure to see the following two video tutorials.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用 FP8 进行训练的理论超出了本文的范围（例如，见[这里](https://arxiv.org/pdf/2209.05433.pdf)），但重要的是要知道**使用
    FP8 的机制比**[**16 位替代方案**](https://pytorch.org/docs/stable/amp.html)（float16 和 bfloat16）**复杂得多**。幸运的是，TE
    实现隐藏了所有繁琐的细节。有关如何使用 TE API，请参阅官方[文档](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/examples/fp8_primer.html)以及这个简单的[示例](https://github.com/NVIDIA/TransformerEngine/blob/main/examples/pytorch/mnist/main.py)。要了解更多幕后情况，请务必查看以下两个视频教程。
- en: '[](https://www.nvidia.com/en-us/on-demand/session/gtcspring23-s51393/?source=post_page-----5a5123aec7d7--------------------------------)
    [## FP8 Training with Transformer Engine | NVIDIA On-Demand'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://www.nvidia.com/en-us/on-demand/session/gtcspring23-s51393/?source=post_page-----5a5123aec7d7--------------------------------)
    [## 使用Transformer Engine进行FP8训练 | NVIDIA On-Demand'
- en: The session will include an introduction to FP8 and mixed precision, an overview
    of Transformer Engine features, and a…
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本次会话将包括FP8和混合精度介绍，Transformer Engine特性概述以及...
- en: www.nvidia.com](https://www.nvidia.com/en-us/on-demand/session/gtcspring23-s51393/?source=post_page-----5a5123aec7d7--------------------------------)
    [](https://www.nvidia.com/en-us/on-demand/session/gtcspring23-s52166/?source=post_page-----5a5123aec7d7--------------------------------)
    [## FP8 for Deep Learning | NVIDIA On-Demand
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: www.nvidia.com](https://www.nvidia.com/en-us/on-demand/session/gtcspring23-s51393/?source=post_page-----5a5123aec7d7--------------------------------)
    [](https://www.nvidia.com/en-us/on-demand/session/gtcspring23-s52166/?source=post_page-----5a5123aec7d7--------------------------------)
    [## 深度学习的FP8 | NVIDIA On-Demand
- en: FP8 is a natural progression for accelerating deep learning (DL) training beyond
    the 16-bit formats common in modern…
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: FP8是加速深度学习（DL）训练的自然进展，超越了现代常见的16位格式…
- en: www.nvidia.com](https://www.nvidia.com/en-us/on-demand/session/gtcspring23-s52166/?source=post_page-----5a5123aec7d7--------------------------------)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: www.nvidia.com](https://www.nvidia.com/en-us/on-demand/session/gtcspring23-s52166/?source=post_page-----5a5123aec7d7--------------------------------)
- en: To modify our model to use TE, we wrap TE’s specialized Transformer Layer with
    a custom transformer block class that conforms to timm’s [block layer signature](https://github.com/huggingface/pytorch-image-models/blob/v0.9.10/timm/models/vision_transformer.py#L114).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 要修改我们的模型以使用TE，我们将TE的专用Transformer层包装在一个符合timm的[block layer signature](https://github.com/huggingface/pytorch-image-models/blob/v0.9.10/timm/models/vision_transformer.py#L114)的自定义Transformer块类中。
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we modify the VisionTransformer initialization to use our custom block
    layer:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们修改VisionTransformer初始化以使用我们的自定义块层：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Until now we have not made any H100-specific changes — the same code can be
    run on our A100-powered p4d instance type. The last modification is wrapping the
    model forward-pass with a [te.fp8_autocast](https://github.com/NVIDIA/TransformerEngine/blob/release_v0.12/transformer_engine/pytorch/fp8.py#L453)
    context manager. This change requires a GPU that supports FP8:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们没有对H100做任何特定的更改 —— 相同的代码可以在我们的A100-powered p4d实例上运行。最后一个修改是将模型前向传递包装在[te.fp8_autocast](https://github.com/NVIDIA/TransformerEngine/blob/release_v0.12/transformer_engine/pytorch/fp8.py#L453)上下文管理器中。此更改需要支持FP8的GPU：
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: A Few Cautionary Remarks Regarding the Use of FP8
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于使用FP8的一些注意事项
- en: Usage of an 8-bit floating-point representation (as opposed to a 16 or 32-bit
    representation) implies lower precision and a lower dynamic range. These can have
    a meaningful impact on the attainability and/or speed of your model convergence.
    Although the underlying TE FP8 implementation is designed to address this challenge,
    there is no guarantee that this will work for your model. You may need to fiddle
    with the underlying FP8 mechanics (e.g., using the TE [recipe](https://github.com/NVIDIA/TransformerEngine/blob/release_v0.12/transformer_engine/common/recipe.py)
    APIs), tune some of the hyperparameters, and/or limit the application of the FP8
    to subsections of the model. You might find that despite all of your attempts,
    your model is simply not compatible with FP8.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 使用8位浮点表示（而不是16位或32位表示）意味着较低的精度和较小的动态范围。这些因素可能会对模型的收敛速度和/或速度产生显著影响。尽管底层TE FP8实现旨在解决这一挑战，但不能保证适用于您的模型。您可能需要调整底层FP8机制（例如使用TE
    [recipe](https://github.com/NVIDIA/TransformerEngine/blob/release_v0.12/transformer_engine/common/recipe.py)
    API）、调整一些超参数，和/或限制FP8在模型子部分的应用。您可能会发现，尽管尽了最大努力，您的模型仍然无法与FP8兼容。
- en: Results
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: In the table below we summarize the results of our experiments on both p4d.24xlarge
    and p5.48xlarge EC2 instance types, with and without the TE library. For the p5.48xlarge
    experiments we doubled the batch size in order to increase the utilization of
    the 80 GB GPU memory. Using FP8 reduces the GPU memory consumption enabling a
    further increase to the batch size.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在下表中，我们总结了在p4d.24xlarge和p5.48xlarge EC2实例类型上的实验结果，包括使用和不使用TE库的情况。对于p5.48xlarge实验，我们增加了批量大小以增加80
    GB GPU内存的利用率。使用FP8可以减少GPU内存消耗，从而进一步增加批量大小。
- en: '![](../Images/51d33ff0bf6fc87a8a5cd2712726b5e2.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/51d33ff0bf6fc87a8a5cd2712726b5e2.png)'
- en: Experiment Results (By Author)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果（作者提供）
- en: We can see that the use of the TE transformer block increased the price-performance
    on both the p4d (~19%) and the p5 (~32%) instance types. Using FP8 boosts the
    performance on the p5 by an additional ~20%. Following the TE and FP8 optimizations
    **the price performance of the H100-based p5.48large beats that of the A100-based
    p4d.24large** — although not by a very wide margin (~2%). Taking into account
    the 3x increase in training speed, we can safely conclude that the p5 would be
    the better instance type for training our optimized model.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，使用 TE transformer 块提高了 p4d（约 19%）和 p5（约 32%）实例类型的价格性能。使用 FP8 使 p5 的性能额外提升了约
    20%。在 TE 和 FP8 优化后，**基于 H100 的 p5.48large 的价格性能优于基于 A100 的 p4d.24large**——尽管差距不算很大（约
    2%）。考虑到训练速度提高了 3 倍，我们可以安全地得出结论，p5 将是训练我们优化模型的更佳实例类型。
- en: Note that the relatively small increase in price-performance (far lower than
    the 40% mentioned in the p5 announcement) leaves us wishing for additional H100-specific
    optimizations… but those will have to wait for another post :).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，价格性能的相对小幅提升（远低于 p5 公告中提到的 40%）使我们期待更多针对 H100 的特定优化……不过这些还需要等到另一篇文章中讨论 :).
- en: Summary
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this post we have demonstrated how to program a PyTorch training script to
    use 8-bit floating types. We further demonstrated how the use of FP8 can be a
    key factor in getting the best performance out of modern GPUs such as Nvidia H100\.
    Importantly, the viability of FP8 as well as its impact on training performance
    can vary a great deal based on the details of the model.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们展示了如何编写 PyTorch 训练脚本以使用 8 位浮点类型。我们进一步展示了使用 FP8 如何成为发挥现代 GPU（如 Nvidia
    H100）最佳性能的关键因素。重要的是，FP8 的可行性以及对训练性能的影响可能会因模型细节的不同而大相径庭。
- en: This post continues a long series of publications on the topic of optimizing
    machine learning workloads. Be sure to see some of our [other posts](https://chaimrand.medium.com/)
    on this important topic.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 本文继续了关于优化机器学习工作负载的长期系列出版物。务必查看我们关于这一重要主题的[其他文章](https://chaimrand.medium.com/)。
- en: '**Update** (May 21, 2024): Please check out our [sequel post](https://chaimrand.medium.com/pytorch-native-fp8-fedc06f1c9f7)
    which covers the newly released PyTorch-native FP8 data types.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**更新**（2024年5月21日）：请查看我们的[续篇](https://chaimrand.medium.com/pytorch-native-fp8-fedc06f1c9f7)，其中涵盖了新发布的
    PyTorch 原生 FP8 数据类型。'
