- en: 'Integrating Neural Net: Deriving the Normal Distribution CDF'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的应用：推导正态分布的累积分布函数
- en: 原文：[https://towardsdatascience.com/integrating-neural-net-deriving-the-normal-distribution-cdf-ea14574548ea?source=collection_archive---------7-----------------------#2023-05-03](https://towardsdatascience.com/integrating-neural-net-deriving-the-normal-distribution-cdf-ea14574548ea?source=collection_archive---------7-----------------------#2023-05-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/integrating-neural-net-deriving-the-normal-distribution-cdf-ea14574548ea?source=collection_archive---------7-----------------------#2023-05-03](https://towardsdatascience.com/integrating-neural-net-deriving-the-normal-distribution-cdf-ea14574548ea?source=collection_archive---------7-----------------------#2023-05-03)
- en: Integrating a function using a neural network (with code)
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用神经网络进行函数积分（含代码）
- en: '[](https://medium.com/@john_morrow?source=post_page-----ea14574548ea--------------------------------)[![John
    Morrow](../Images/4a8ce62a0b4e1eb1cf77ecaba6b7ddcc.png)](https://medium.com/@john_morrow?source=post_page-----ea14574548ea--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ea14574548ea--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ea14574548ea--------------------------------)
    [John Morrow](https://medium.com/@john_morrow?source=post_page-----ea14574548ea--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@john_morrow?source=post_page-----ea14574548ea--------------------------------)[![John
    Morrow](../Images/4a8ce62a0b4e1eb1cf77ecaba6b7ddcc.png)](https://medium.com/@john_morrow?source=post_page-----ea14574548ea--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ea14574548ea--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ea14574548ea--------------------------------)
    [John Morrow](https://medium.com/@john_morrow?source=post_page-----ea14574548ea--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb4bcd051bb38&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintegrating-neural-net-deriving-the-normal-distribution-cdf-ea14574548ea&user=John+Morrow&userId=b4bcd051bb38&source=post_page-b4bcd051bb38----ea14574548ea---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ea14574548ea--------------------------------)
    ·6 min read·May 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fea14574548ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintegrating-neural-net-deriving-the-normal-distribution-cdf-ea14574548ea&user=John+Morrow&userId=b4bcd051bb38&source=-----ea14574548ea---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb4bcd051bb38&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintegrating-neural-net-deriving-the-normal-distribution-cdf-ea14574548ea&user=John+Morrow&userId=b4bcd051bb38&source=post_page-b4bcd051bb38----ea14574548ea---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ea14574548ea--------------------------------)
    ·6分钟阅读·2023年5月3日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fea14574548ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintegrating-neural-net-deriving-the-normal-distribution-cdf-ea14574548ea&user=John+Morrow&userId=b4bcd051bb38&source=-----ea14574548ea---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fea14574548ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintegrating-neural-net-deriving-the-normal-distribution-cdf-ea14574548ea&source=-----ea14574548ea---------------------bookmark_footer-----------)![](../Images/a4ae015d22dc9fef5466d38897886747.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fea14574548ea&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fintegrating-neural-net-deriving-the-normal-distribution-cdf-ea14574548ea&source=-----ea14574548ea---------------------bookmark_footer-----------)![](../Images/a4ae015d22dc9fef5466d38897886747.png)'
- en: Photo by [Jack Anstey](https://unsplash.com/@jack_anstey?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Jack Anstey](https://unsplash.com/@jack_anstey?utm_source=medium&utm_medium=referral)
    提供，来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '**1\. Introduction**'
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**1\. 引言**'
- en: This article presents a method for training a neural network to derive the integral
    of a function. The technique works not only with analytically-solvable integrals
    but also with integrals that do not have a closed-form solution and are typically
    solved by numerical methods. An example is the normal distribution’s cumulative
    density function (CDF). Equation 1 is this distribution’s probability density
    function (PDF), and Equation 2 is its CDF, the integral of the PDF. Figure 1 is
    a plot of these functions. After being trained, the resulting network can serve
    as a stand-alone function generator that delivers points on the CDF curve given
    points from the domain of the distribution’s PDF.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了一种训练神经网络以推导函数积分的方法。该技术不仅适用于解析解积分，还适用于没有封闭形式解且通常通过数值方法求解的积分。例如，正态分布的累积分布函数（CDF）。方程
    1 是该分布的概率密度函数（PDF），方程 2 是其 CDF，即 PDF 的积分。图 1 是这些函数的图示。训练后，生成的网络可以作为一个独立的函数生成器，提供在
    CDF 曲线上给定 PDF 分布域中的点。
- en: '![](../Images/94fc44bd8cc33bdf637f916c2bdb174d.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94fc44bd8cc33bdf637f916c2bdb174d.png)'
- en: 'Equation 1: **PDF (with u=0, 𝜎=1)**'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 1：**概率密度函数（PDF）（u=0，𝜎=1）**
- en: '![](../Images/edc8ecd01cc6d361ff26ba2a908dc473.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/edc8ecd01cc6d361ff26ba2a908dc473.png)'
- en: 'Equation2: **CDF**'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2：**累积分布函数（CDF）**
- en: '![](../Images/20af856a48703278cd9f0085b2662f4c.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/20af856a48703278cd9f0085b2662f4c.png)'
- en: Figure 1\. **PDF and CDF of the normal distribution**
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. **正态分布的 PDF 和 CDF**
- en: '**2\. Integrating neural network**'
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**2\. 积分神经网络**'
- en: 'An integrating neural network is trained to produce the integral of a function
    *y = f(x)*. Expressed in terms of the network’s input and output:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个积分神经网络，以产生函数 *y = f(x)* 的积分。用网络的输入和输出表示：
- en: '![](../Images/72fa511f99bc616132acfde0459a19f5.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/72fa511f99bc616132acfde0459a19f5.png)'
- en: Equation 3
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 3
- en: where *h* and *x* are the network’s output and input, respectively. For the
    normal distribution, *f(x)* is given by Equation 1, the PDF of the distribution.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *h* 和 *x* 分别是网络的输出和输入。对于正态分布，*f(x)* 由方程 1 给出，即该分布的 PDF。
- en: Integration of the function is accomplished by training the neural network such
    that the *derivative of the network’s output is equal to the function’s output,
    resulting in the network’s output becoming the integral of the function*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 通过训练神经网络实现函数的积分，使得 *网络输出的导数等于函数的输出，从而使网络输出成为函数的积分*。
- en: '**2.1 Neural network training**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**2.1 神经网络训练**'
- en: 'Following are the steps of the training procedure:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 训练程序的步骤如下：
- en: 'Apply a training point, *xᵢ* , to the function *y = f(x)*:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将一个训练点 *xᵢ* 应用到函数 *y = f(x)*：
- en: '![](../Images/0d50d763a59f6dbf18e42ec1cc21d51b.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0d50d763a59f6dbf18e42ec1cc21d51b.png)'
- en: Equation 4
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 4
- en: '2\. Also apply *xᵢ* to the input of the neural network:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 还将 *xᵢ* 应用到神经网络的输入：
- en: (The neural network model comprises a single input, *x*, two hidden layers,
    and a single output, *h*, and is represented by *h(x) = nn_model(x)*.)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: （神经网络模型包括一个输入 *x*，两个隐藏层和一个输出 *h*，表示为 *h(x) = nn_model(x)*。）
- en: '![](../Images/4e9d3afa34ec48afde0aaaf37d792e6a.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4e9d3afa34ec48afde0aaaf37d792e6a.png)'
- en: Equation 5
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 5
- en: '3\. Take the derivative of h*ᵢ* :'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 求取 *hᵢ* 的导数：
- en: '![](../Images/440d0ba5cc93ff27f319bfccece30c4a.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/440d0ba5cc93ff27f319bfccece30c4a.png)'
- en: Equation 6
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 6
- en: (Differentiation is provided in TensorFlow and PyTorch via their automatic differentiation
    function. In this article, the neural network is developed with TensorFlow [GradientTape](https://www.tensorflow.org/guide/autodiff).)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: （在 TensorFlow 和 PyTorch 中，通过其自动微分功能提供微分。在本文中，神经网络使用 TensorFlow [GradientTape](https://www.tensorflow.org/guide/autodiff)
    开发。）
- en: '4\. Train the neural network with a loss function (loss 2 in Section 2.2) that
    forces the following relationship:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 使用损失函数（第 2.2 节中的损失 2）训练神经网络，以强制以下关系：
- en: '![](../Images/c1707bbc428483113966cec2fcf1ff66.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c1707bbc428483113966cec2fcf1ff66.png)'
- en: Equation 7
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 7
- en: 'Then after the neural network is trained, since *g = y*, and *s*ubstituting
    *g* and *y* from Equation 6 and Equation 4, respectively:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在神经网络训练后，由于 *g = y*，并且将方程 6 和方程 4 中的 *g* 和 *y* 代入：
- en: '![](../Images/befd286c620dd7aa2a595614937418da.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/befd286c620dd7aa2a595614937418da.png)'
- en: Equation 8
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 8
- en: 'Integrating both sides of Equation 8 confirms that the neural network’s output
    is the integral of the function *f(x)*:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对方程 8 两边进行积分，确认神经网络的输出是函数 *f(x)* 的积分：
- en: '![](../Images/c55fbd86a0f38112beb5b2c47d8fccf9.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c55fbd86a0f38112beb5b2c47d8fccf9.png)'
- en: Equation 9
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 9
- en: where *C* is the constant of integration.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *C* 是积分常数。
- en: '**2.2 Neural network loss function**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**2.2 神经网络损失函数**'
- en: Typically, a neural network is trained with pairs of known input and output
    data. The training input data is presented to the neural network, and the resulting
    output is compared to the training output data using a loss function. The loss
    returned by this function is used via backpropagation to adjust the network’s
    weights to reduce the loss. An integrating neural network uses a custom loss function
    to constrain the neural network to produce an output that complies with the output
    of the integrated function.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，神经网络是通过已知输入和输出数据对进行训练的。训练输入数据呈现给神经网络，结果输出与训练输出数据通过损失函数进行比较。这个函数返回的损失用于通过反向传播调整网络权重，以减少损失。整合神经网络使用自定义损失函数来约束神经网络产生与积分函数输出一致的输出。
- en: The loss function for the integrating neural network, Figure 2, has three components.
    Loss 2, described in the training procedure above (Section 2.1), forces the output
    of the neural network to comply with the integral of *f(x)*.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 整合神经网络的损失函数，如图2所示，有三个组成部分。损失函数2（在上文训练程序第2.1节中描述）强制神经网络的输出符合*f(x)*的积分。
- en: Loss 3 forces the neural network to comply with the initial condition *h(x_init2)
    = h_init2*. For the CDF model, this condition is *h(−10) = 0*, which sets *C =
    0* (Equation 9). For the purpose of this model, the responses of the PDF and CDF
    at *x = −10* approximate the responses at *x = −∞*.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数3迫使神经网络遵守初始条件*h(x_init2) = h_init2*。对于CDF模型，这一条件为*h(−10) = 0*，这设置了*C = 0*（方程9）。对于该模型，*x
    = −10*时PDF和CDF的响应近似于*x = −∞*时的响应。
- en: 'Setting the initial condition in Loss 3 to *h(−∞) = 0* also simplifies the
    CDF calculation. Expanding the definite integral of Equation 2:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在损失函数3中设置初始条件为*h(−∞) = 0*，也简化了CDF计算。展开方程2的定积分：
- en: '![](../Images/7851d45aed1c315196b1004b80825ecd.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7851d45aed1c315196b1004b80825ecd.png)'
- en: Equation 10
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 方程10
- en: 'The initial condition, *h(−∞) = 0*, means that the second term equals zero,
    and the output of the trained neural network is the value of the CDF for the corresponding
    *x* input:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 初始条件*h(−∞) = 0*，意味着第二项为零，训练后的神经网络输出为对应*x*输入的CDF值：
- en: '![](../Images/93fb2602d50c605008d27656cac01573.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/93fb2602d50c605008d27656cac01573.png)'
- en: Equation 11
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 方程11
- en: Loss 1, with condition *h(10) = 1*, stabilizes the training process for points
    near the right tail of the distribution. For the purpose of this model, the responses
    of the PDF and CDF at *x = 10* approximate the responses at
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数1，条件为*h(10) = 1*，稳定了训练过程，尤其是对分布右尾附近的点。对于该模型，*x = 10*时PDF和CDF的响应近似于
- en: x = ∞.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: x = ∞。
- en: '![](../Images/843aebca813b98102b1a60a320978e94.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/843aebca813b98102b1a60a320978e94.png)'
- en: Figure 2\. **Loss function**
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. **损失函数**
- en: '**3\. Integrating neural network implementation**'
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**3\. 神经网络实现的整合**'
- en: Following is the Python code for the integrating neural network implementation
    of the normal distribution CDF. The complete code is available [here](https://github.com/jmorrow1000/integrating-nn).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是实现正态分布CDF的整合神经网络的Python代码。完整代码可在[这里](https://github.com/jmorrow1000/integrating-nn)获取。
- en: '**3.1 Neural network model definition**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.1 神经网络模型定义**'
- en: The neural network has two fully-connected hidden layers, each with 512 neurons.
    There is a single input for domain points and a single output for the corresponding
    integral values.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络有两个全连接的隐藏层，每个层有512个神经元。输入为域点，输出为对应的积分值。
- en: Listing 1\. **TensorFlow neural network model**
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 列表1\. **TensorFlow神经网络模型**
- en: '**3.2 Initialization**'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.2 初始化**'
- en: The *xᵢ* training points from Section 2.1 for loss 2 are defined on line 9,
    below. The order of these points is randomly shuffled on line 11 to promote stable
    training of the neural network. On line 12 the points are applied to the PDF as
    described in Equation 4.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数2中第2.1节的*xᵢ*训练点在下面的第9行中定义。这些点的顺序在第11行被随机打乱，以促进神经网络的稳定训练。在第12行，这些点被应用于PDF，如方程4所述。
- en: The initial conditions for loss 1 and loss 3 are defined in lines 15-16 and
    lines 19-20, respectively.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数1和损失函数3的初始条件分别在第15-16行和第19-20行中定义。
- en: Listing 2\. **Initialization**
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2\. **初始化**
- en: '**3.3 Batch training step**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.3 批量训练步骤**'
- en: Listing 3 is the training step function applied to each batch of training points.
    The total loss (the sum of loss 1, loss 2, and loss 3) in line 24 is used to update
    the neural network’s weights via gradient descent (lines 26 -30). Each training
    epoch includes multiple batches, which collectively use all the training points
    in model updates.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3 是应用于每个训练批次的训练步骤函数。总损失（损失 1、损失 2 和损失 3 的总和）在第 24 行用于通过梯度下降更新神经网络的权重（第 26
    - 30 行）。每个训练周期包括多个批次，这些批次共同使用所有训练点进行模型更新。
- en: Line 9 produces the neural network’s response to the *x_init* initial condition.
    The response is compared to the corresponding initial condition, *h_init*, producing
    loss 1 (line 10).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 第 9 行生成网络对 *x_init* 初始条件的响应。该响应与相应的初始条件 *h_init* 进行比较，产生损失 1（第 10 行）。
- en: Similarly, Line 13 produces the network’s response to the *x_init2* initial
    condition. The response is compared to the corresponding initial condition, *h_init2*,
    producing loss 3 (line 14).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，第 13 行生成网络对 *x_init2* 初始条件的响应。该响应与相应的初始条件 *h_init2* 进行比较，产生损失 3（第 14 行）。
- en: Line 17 produces the network’s response to training point *xᵢ* (Equation 5).
    Line 18 extracts the gradient of the response (Equation 6), and lines 19–20 compare
    the gradient to *f(xᵢ)* (Equation 7), producing loss 2.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 第 17 行生成网络对训练点 *xᵢ*（方程 5）的响应。第 18 行提取响应的梯度（方程 6），第 19 - 20 行将梯度与 *f(xᵢ)*（方程
    7）进行比较，产生损失 2。
- en: Listing 3\. **Batch training step**
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3\. **批量训练步骤**
- en: 4\. Results
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 结果
- en: Figure 3 shows the CDF response (red trace) from the output of the trained neural
    network. As verification of the accuracy of the results, the CDF response from
    the *norm.cdf* function in the Python [SciPy](https://scipy.org) library is included
    (green dots).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3 展示了训练好的神经网络输出的累积分布函数（CDF）响应（红色轨迹）。为了验证结果的准确性，包含了 Python [SciPy](https://scipy.org)
    库中 *norm.cdf* 函数的CDF响应（绿色点）。
- en: '![](../Images/9168084258ef4ca60bc87b5f9ac90b84.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9168084258ef4ca60bc87b5f9ac90b84.png)'
- en: Figure 3\. **Trained CDF neural network output**
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. **训练后的CDF神经网络输出**
- en: Figure 4 is the loss from the total loss function v.s. epoch logged during the
    training process.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4 是训练过程中记录的总损失函数随 epoch 的变化。 '
- en: '![](../Images/6ef33914bc3c560f3192416eff377c17.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6ef33914bc3c560f3192416eff377c17.png)'
- en: Figure 4\. **Training loss**
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. **训练损失**
- en: 5\. Conclusion
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 结论
- en: This article demonstrates a method for training a neural network to integrate
    a function by using a custom loss function and automatic differentiation. Specifically,
    a neural network is trained to successfully integrate the PDF of the normal distribution
    to produce the CDF.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 本文展示了一种使用自定义损失函数和自动微分训练神经网络积分函数的方法。具体来说，训练神经网络成功地积分正态分布的概率密度函数（PDF）以生成累积分布函数（CDF）。
- en: An upcoming article will present a method for training a neural network to invert
    a function. The inverting network will be used to invert the output of the CDF-trained
    network from this article, then produce samples from the normal distribution.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 即将发表的一篇文章将介绍一种训练神经网络反转函数的方法。反转网络将用于反转本文中训练的CDF网络的输出，然后从正态分布生成样本。
- en: '**A pdf of this article is available for download** [**here**](https://github.com/jmorrow1000/integrating-nn)**.**'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**本文的 pdf 可在此处 [**下载**](https://github.com/jmorrow1000/integrating-nn)**。**'
- en: '*All images, unless otherwise noted, are by the author.*'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有说明，所有图片均由作者提供。*'
