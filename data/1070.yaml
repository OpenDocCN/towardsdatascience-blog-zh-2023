- en: 'XAI for Forecasting: Basis Expansion'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XAI 预测：基础扩展
- en: 原文：[https://towardsdatascience.com/xai-for-forecasting-basis-expansion-17a16655b6e4?source=collection_archive---------6-----------------------#2023-03-24](https://towardsdatascience.com/xai-for-forecasting-basis-expansion-17a16655b6e4?source=collection_archive---------6-----------------------#2023-03-24)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/xai-for-forecasting-basis-expansion-17a16655b6e4?source=collection_archive---------6-----------------------#2023-03-24](https://towardsdatascience.com/xai-for-forecasting-basis-expansion-17a16655b6e4?source=collection_archive---------6-----------------------#2023-03-24)
- en: '![](../Images/fba0ad0a63160f4102e075123ef170a5.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fba0ad0a63160f4102e075123ef170a5.png)'
- en: Photo by [Richard Horvath](https://unsplash.com/@orwhat?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[理查德·霍尔瓦斯](https://unsplash.com/@orwhat?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: NBEATS and other Interpretable Deep Forecasting Models
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NBEATS 和其他可解释的深度预测模型
- en: '[](https://medium.com/@upadhyan?source=post_page-----17a16655b6e4--------------------------------)[![Nakul
    Upadhya](../Images/336cb21272e9b1f098177adbde50e92e.png)](https://medium.com/@upadhyan?source=post_page-----17a16655b6e4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----17a16655b6e4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----17a16655b6e4--------------------------------)
    [Nakul Upadhya](https://medium.com/@upadhyan?source=post_page-----17a16655b6e4--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@upadhyan?source=post_page-----17a16655b6e4--------------------------------)[![Nakul
    Upadhya](../Images/336cb21272e9b1f098177adbde50e92e.png)](https://medium.com/@upadhyan?source=post_page-----17a16655b6e4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----17a16655b6e4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----17a16655b6e4--------------------------------)
    [Nakul Upadhya](https://medium.com/@upadhyan?source=post_page-----17a16655b6e4--------------------------------)'
- en: ·
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4d9dddc62a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxai-for-forecasting-basis-expansion-17a16655b6e4&user=Nakul+Upadhya&userId=4d9dddc62a80&source=post_page-4d9dddc62a80----17a16655b6e4---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----17a16655b6e4--------------------------------)
    ·15 min read·Mar 24, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F17a16655b6e4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxai-for-forecasting-basis-expansion-17a16655b6e4&user=Nakul+Upadhya&userId=4d9dddc62a80&source=-----17a16655b6e4---------------------clap_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4d9dddc62a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxai-for-forecasting-basis-expansion-17a16655b6e4&user=Nakul+Upadhya&userId=4d9dddc62a80&source=post_page-4d9dddc62a80----17a16655b6e4---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----17a16655b6e4--------------------------------)
    ·15分钟阅读·2023年3月24日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F17a16655b6e4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxai-for-forecasting-basis-expansion-17a16655b6e4&user=Nakul+Upadhya&userId=4d9dddc62a80&source=-----17a16655b6e4---------------------clap_footer-----------)'
- en: --
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F17a16655b6e4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxai-for-forecasting-basis-expansion-17a16655b6e4&source=-----17a16655b6e4---------------------bookmark_footer-----------)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F17a16655b6e4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fxai-for-forecasting-basis-expansion-17a16655b6e4&source=-----17a16655b6e4---------------------bookmark_footer-----------)'
- en: Forecasting is a critical aspect of many industries, from finance to supply
    chain management. Over the years, researchers have explored various techniques
    for forecasting, ranging from traditional time-series methods to machine learning-based
    models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 预测是许多行业中的关键方面，从金融到供应链管理。多年来，研究人员探索了各种预测技术，从传统的时间序列方法到基于机器学习的模型。
- en: In recent years, forecasters have turned to deep learning and have gotten promising
    results with models such as Long Short-Term Memory (LSTM) networks and Temporal
    Convolution Networks (CNNs) showing great potential. Before 2019, the primary
    approach to the forecasting problem was combining traditional statistical methods
    (like ARIMA) with deep learning [1]. However, the forecasting literature broke
    away from this in 2020 and diverged in two different directions.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，预测人员转向深度学习，并在如长短期记忆 (LSTM) 网络和时间卷积网络 (CNNs) 等模型中获得了有希望的结果。2019 年之前，解决预测问题的主要方法是将传统统计方法（如
    ARIMA）与深度学习相结合 [1]。然而，预测文献在 2020 年突破了这一点，并朝两个不同的方向发展。
- en: The first direction involves developing means of using the attention mechanism
    and the transformer architecture for forecasting. This approach was pioneered
    with the introduction of the LogSparse Transformer by Li et. al [5] which adjusted
    the traditional self-attention mechanism used in NLP tasks to be more sensitive
    to locality and use less memory [5]. This work was then later extended and improved
    by models such as Autoformer [6], Informer [7], FEDFormer [8], and more. This
    is a thriving field, but recent work has put this approach into question. Zeng
    et. al has recently questioned the effectiveness of the self-attention mechanism
    for the forecasting task and did a slew of experiments to show that the attention
    mechanisms may not be useful for temporal representations [9] (a summary of these
    experiments can be found in [this article](https://medium.com/towards-data-science/transformers-lose-to-linear-models-902164ca5974)).
    Additionally, this approach also falls a bit short from an Explainable AI (XAI)
    perspective. While these models all have attention mechanisms that can be visualized,
    many academics have argued that this may not be explainable and this is an active
    field of debate. Denis Vorotyntsev has made a [great article summarizing the debate](/is-attention-explanation-b609a7b0925c)
    and I highly encourage checking his article out as well [10].
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个方向涉及开发利用注意力机制和变换器架构进行预测的方法。这一方法由 Li 等人首次提出的 LogSparse Transformer 开创，该方法调整了传统的自注意力机制，使其在
    NLP 任务中对局部性更为敏感且使用更少的内存 [5]。此工作后来由 Autoformer [6]、Informer [7]、FEDFormer [8] 等模型进一步扩展和改进。这是一个蓬勃发展的领域，但最近的工作对这一方法提出了质疑。Zeng
    等人最近质疑了自注意力机制在预测任务中的有效性，并进行了大量实验，显示注意力机制可能对时间表示不太有用 [9]（这些实验的总结可以在[这篇文章](https://medium.com/towards-data-science/transformers-lose-to-linear-models-902164ca5974)中找到）。此外，从可解释性
    AI (XAI) 角度来看，这一方法也稍显不足。尽管这些模型都有可视化的注意力机制，但许多学者认为这可能无法解释，这也是一个活跃的辩论领域。Denis Vorotyntsev
    发表了一篇[总结辩论的优秀文章](/is-attention-explanation-b609a7b0925c)，我强烈建议查看他的文章 [10]。
- en: In contrast to the attention-based approach of transformers, the other primary
    direction of tackling the forecasting problem is the neural basis expansion analysis
    approach first proposed by Oreshkin et. al [1] in 2020 with the N-BEATS architecture.
    This methodology involves using multiple stacks of deep fully-connected networks
    to iteratively build the forecast. Instead of directly predicting the time series,
    the networks in each stack instead predict the weights of a basis. This architecture
    allows users to specify the components of the time series they want to extract
    and the reconstructive nature of the forecast adds an extra layer of interpretability.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于注意力的变换器方法相比，解决预测问题的另一主要方向是由 Oreshkin 等人 [1] 在 2020 年提出的神经基础扩展分析方法，该方法使用 N-BEATS
    架构。该方法涉及使用多个堆叠的深度全连接网络来迭代构建预测。网络不是直接预测时间序列，而是预测基础的权重。这种架构允许用户指定他们想要提取的时间序列组件，并且预测的重建性质增加了额外的可解释性。
- en: 'In this article, I aim to summarize the full mechanisms behind the basis analysis
    architecture and also showcase the improvements to the basis expansion approach
    since the original N-BEATS paper. Specifically, I aim to cover how N-BEATS works
    [1] and the two improvements to NBEATS: N-HiTS [2] and DEPTS [4].'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我旨在总结基础分析架构的完整机制，并展示自原始 N-BEATS 论文以来基础扩展方法的改进。具体而言，我将涵盖 N-BEATS 的工作原理 [1]
    以及对 NBEATS 的两个改进：N-HiTS [2] 和 DEPTS [4]。
- en: N-BEATS
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: N-BEATS
- en: '![](../Images/28208184fe703ee750bb312da4c19fdc.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/28208184fe703ee750bb312da4c19fdc.png)'
- en: 'Figure 1: N-BEATS Architecture from Oreshkin et. al 2020 [1]'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：Oreshkin 等人 2020 年的 N-BEATS 架构 [1]
- en: N-BEATS stands for **N**eural **B**asis **E**xpansion **A**nalysis for **T**ime
    **S**eries and is the origin model for the basis expansion architecture. Because
    of this, we will spend a bit more time explaining this architecture compared to
    the others.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: N-BEATS 代表**神经基础扩展分析**（Neural Basis Expansion Analysis）用于**时间序列**（Time Series），是基础扩展架构的原始模型。因此，我们将花更多时间解释这一架构，相较于其他模型。
- en: 'When developing N-BEATS, the author’s goal was to showcase the power of deep
    learning by creating a forecasting model that did not leverage any statistical
    concepts such as maximum likelihood estimation or traditional autoregressive models
    (ex. ARIMA) but still provided some level of interpretability that the traditional
    time-series approaches provided [1]. In striving for these qualities, Oreshkin
    et. al not only met their goals but also proved that their model could do better
    than the state-of-the-art at the time. Some qualities of N-BEATS that make it
    a strong model are:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发 N-BEATS 时，作者的目标是通过创建一个不依赖于任何统计概念（如最大似然估计或传统自回归模型（例如 ARIMA））的预测模型来展示深度学习的力量，同时仍提供一些传统时间序列方法所提供的可解释性[1]。在追求这些特性的过程中，Oreshkin
    等人不仅达成了他们的目标，还证明了他们的模型比当时的最先进技术更优秀。使 N-BEATS 成为强大模型的一些特性包括：
- en: 'Great performance: On the M4 dataset (a dataset containing 100,000 different
    time series), N-BEATS was able to beat the top performer at the time, ES-RNN [11],
    by an average of 3%.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优异的性能：在 M4 数据集（包含 100,000 个不同时间序列的数据集）上，N-BEATS 能够击败当时的顶尖表现者 ES-RNN [11]，平均超出
    3%。
- en: 'Large and Multi-Horizon forecasting: The methods N-BEATS uses to generate its
    forecasts can perform direct multi-step forecasting avoiding the forecast drift
    issues brought about by iterative forecast generation (passing predictions back
    into the model)'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 大规模和多重预测：N-BEATS 用于生成预测的方法可以直接进行多步预测，避免了由于迭代预测生成（将预测结果回传到模型中）所带来的预测漂移问题。
- en: 'Relatively Fast Training: Compared to its Transformer counterparts, N-BEATS
    only contains MLP stacks and does not have any recurrent networks in its architecture,
    making the training much simpler.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 相对较快的训练：与 Transformer 对应模型相比，N-BEATS 仅包含 MLP 堆栈，并且在其架构中没有任何递归网络，使得训练过程更为简单。
- en: 'Interpretability: When using the interpretable version of N-BEATS (N-BEATSi),
    users can see a clear breakdown of time series components such as trend and seasonality.
    other time-series datasets with astounding success.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可解释性：当使用 N-BEATS 的可解释版本（N-BEATSi）时，用户可以清楚地看到时间序列成分的分解，如趋势和季节性。其他时间序列数据集也取得了令人惊叹的成功。
- en: To achieve all these goals, N-BEATS uses a few clever architectural components
    and tricks.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这些目标，N-BEATS 使用了一些巧妙的架构组件和技巧。
- en: Blocks & Basis
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 块与基础
- en: The block is probably the most important piece of the N-BEATS architecture.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 块可能是 N-BEATS 架构中最重要的部分。
- en: 'As showcased in blue in Figure 1 above, the block contains 1 fully-connected
    network stack which passes its output into 2 different linear layers. We can think
    of the fully connected layers as generating an encoding of the input. The two
    linear layers then take in this encoding and project it into 2 sets of weights:
    one set for the forecast basis and one for the backcast basis. Mathematically,
    the set of operations can be described as follows (this has a 4-layer fully connected
    network):'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 1 中蓝色部分所示，块包含 1 个全连接网络堆栈，该堆栈将其输出传递到 2 个不同的线性层。我们可以将全连接层视为生成输入的编码。这两个线性层随后接收这个编码并将其投射到
    2 组权重中：一组用于预测基础，一组用于回溯基础。在数学上，这组操作可以描述如下（这有一个 4 层全连接网络）：
- en: '![](../Images/a7851cca3f7db3cc8771c5b63cb1f937.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a7851cca3f7db3cc8771c5b63cb1f937.png)'
- en: Image from Oreshkin et. al 2022
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：Oreshkin 等人 2022
- en: In this, *x_l* is the input into the *l-th* block and the thetas are the basis
    weights.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*x_l* 是输入到 *l-th* 块中的数据，而 thetas 是基础权重。
- en: '**Basis**'
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**基础**'
- en: Now it's important to note that when we say basis, we mean in the linear algebra
    sense. In other words, a basis is a set of linearly independent vectors that we
    can linearly combine to form any other vector in a vector space. In the case of
    N-BEATS, the weights of the linear combination are defined by the neural network.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在需要注意的是，当我们提到“基础”时，我们指的是线性代数中的意义。换句话说，基础是一组线性独立的向量，我们可以将其线性组合形成向量空间中的任何其他向量。在
    N-BEATS 的情况下，线性组合的权重由神经网络定义。
- en: 'The generic architecture follows this exact definition:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 通用架构遵循这一确切定义：
- en: '![](../Images/b15bb2d679bc5ccfb89d3fd4c5cf4398.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b15bb2d679bc5ccfb89d3fd4c5cf4398.png)'
- en: Image from Oreshkin et. al 2022
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 Oreshkin 等人 2022
- en: Where y is the piece of the time series predicted by block 1\. The end output
    is just the weighted sum of a set of basis vectors defined by the user or set
    as a trainable parameter. However, this version of the basis is not interpretable
    and doesn’t attempt to explicitly capture specific components of a time series.
    However, it is really easy to modify N-BEATS into a more interpretable version
    by changing the basis equations used. The authors call this new configuration
    N-BEATSi [1].
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 y 是由块 1 预测的时间序列部分。最终输出只是由用户定义的或作为可训练参数设置的一组基向量的加权和。然而，这种基向量版本不可解释，也不试图显式捕捉时间序列的特定成分。不过，通过更改使用的基方程，N-BEATS
    可以非常容易地修改成更具解释性的版本。作者将这种新配置称为 N-BEATSi [1]。
- en: The first interpretable basis is the trend basis. This changes the basis to
    be a series of low-power polynomials (ex. at+bt²+ct³) and our network is predicting
    the weights for each polynomial term.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个可解释的基向量是趋势基向量。这将基向量更改为一系列低次幂的多项式（例如 at+bt²+ct³），我们的网络预测每个多项式项的权重。
- en: '![](../Images/0dbc76c64300d01a08c880344e5d9b0f.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0dbc76c64300d01a08c880344e5d9b0f.png)'
- en: Image from Oreshkin et. al 2022
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 Oreshkin 等人 2022
- en: The second interpretable basis is the seasonality basis. Similar to the trend
    basis, the network predicts the weights for a set of equations, but this time
    it is predicting the weights for a Fourier series.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个可解释的基向量是季节性基向量。与趋势基向量类似，网络预测一组方程的权重，但这一次它预测的是傅里叶级数的权重。
- en: '![](../Images/5ba2adb957fea254d43eee653c464e89.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5ba2adb957fea254d43eee653c464e89.png)'
- en: Image from Oreshkin et. al 2022
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 Oreshkin 等人 2022
- en: In the end, the network is effectively providing us with a regression equation
    for our time series, allowing the user to understand the exact workings of the
    time series [1].
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，网络实际上为我们的时间序列提供了一个回归方程，使用户能够理解时间序列的具体工作原理 [1]。
- en: '**Backcasts & Forecasts**'
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**回溯预测与预测**'
- en: Each block does not only produce a forecast, but it also produces “backcasts”
    — a prediction of the input window. The backcast can be thought of as a filtering
    mechanism. When a block produces a forecast, the backcast it produces is the component
    of the time series that has already been analyzed and should not be captured by
    subsequent blocks as the current block has already captured that information [1].
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 每个块不仅产生预测结果，还会产生“回溯预测”——对输入窗口的预测。回溯预测可以被视为一种过滤机制。当一个块产生预测结果时，它产生的回溯预测是时间序列中已经被分析的部分，后续的块不应该再捕捉这些信息，因为当前块已经捕捉到了这些信息
    [1]。
- en: In the generic architecture, the forecast and backcasts will have different
    sets of weights produced by different networks and we rely on the training process
    to make them align with each other. This is unavoidable since the dimensions of
    the basis vectors for the forecast and backcast are different. However, since
    the interpretable version uses equations that are dependent on time steps, it
    is highly recommended to have the backcast and forecast share basis weights when
    using the interpretable basis [1].
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在通用架构中，预测和回溯预测将由不同的网络产生不同的权重，我们依赖训练过程使它们相互对齐。这是不可避免的，因为预测和回溯预测的基向量维度不同。然而，由于可解释版本使用的方程依赖于时间步，因此在使用可解释基向量时，强烈建议让回溯预测和预测共享基向量权重
    [1]。
- en: Doubly Residual Stacking
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 双重残差堆叠
- en: 'N-BEATS is not just made up of 1 block, but rather multiple blocks chained
    together that each interpret a signal of the time series. The key mechanism that
    allows for this behavior is doubly residual stacking. **Simply put: the input
    of the current block is equivalent to the input to the previous block minus the
    backcast of the previous block. Additionally, the final forecast is the sum of
    all the forecasts from all the blocks [1].**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: N-BEATS 不只是由 1 个块组成，而是由多个串联在一起的块组成，每个块解释时间序列的一个信号。允许这种行为的关键机制是双重残差堆叠。**简单来说：当前块的输入等同于前一个块的输入减去前一个块的回溯预测。此外，最终预测是所有块预测的总和
    [1]。**
- en: '![](../Images/fecd120caa6fe309e5261a7b0044995a.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fecd120caa6fe309e5261a7b0044995a.png)'
- en: Image from Oreshkin et. al 2022
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 Oreshkin 等人 2022
- en: In the generic case, this doubly residual stacking mechanism allows for smoother
    gradient flows and faster training. Additionally “the aggregation of meaningful
    partial forecasts” also provides a large degree of interpretability as it helps
    users identify the key signals present in the time series they are trying to forecast[1]
    Additionally, by examining the backcasts of all the blocks, the users can see
    if any signals or patterns were ignored or if any were overfit, helping the debugging
    process greatly.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在通用情况下，这种双重残差堆叠机制允许更平滑的梯度流动和更快的训练。此外，“有意义的部分预测的聚合”也提供了较高的可解释性，因为它帮助用户识别他们尝试预测的时间序列中的关键信号[1]。此外，通过检查所有块的回溯，用户可以查看是否有任何信号或模式被忽略或是否有过拟合，这大大有助于调试过程。
- en: Stacks
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 堆栈
- en: To provide more organization to the network, the architecture also organizes
    the individual blocks into stacks. In each stack, the blocks all share the same
    type of basis (generic, trend, or seasonal). This organization not only helps
    the signal decomposition procedure but also allows for more interpretability as
    now we can simply view the stack-level output instead of examining each block.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地组织网络，架构还将各个块组织成堆栈。在每个堆栈中，所有块都共享相同类型的基础（通用、趋势或季节性）。这种组织不仅有助于信号分解过程，还可以提供更多的可解释性，因为现在我们可以直接查看堆栈级别的输出，而无需检查每个块。
- en: For how these interpretations look, check out [the great example found in the
    PyTorch Forecasting documentation](https://pytorch-forecasting.readthedocs.io/en/stable/tutorials/ar.html#Interpret-model).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解这些解释的具体样子，请查看 [PyTorch Forecasting 文档中的精彩示例](https://pytorch-forecasting.readthedocs.io/en/stable/tutorials/ar.html#Interpret-model)。
- en: Covariates
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 协变量
- en: While the original N-BEATS architecture only worked with univariate time series,
    Challu et. al [3] introduced methods to allow for covariates.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然原始的 N-BEATS 架构仅适用于单变量时间序列，但 Challu 等人[3] 引入了允许协变量的方法。
- en: The first way was to simply flatten and append the covariates to the inputs
    of the fully connected stack in each block. Each backcast and forecast would still
    only be for the target time series, and the residual stacking mechanism would
    only apply to the target, not the covariates.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法是将协变量简单地展平并附加到每个块中的全连接层的输入上。每个回溯和预测仍然仅针对目标时间序列，残差堆叠机制仅适用于目标，而不适用于协变量。
- en: 'The second method was to generate an encoding of the covariates first, and
    then use that as a basis for a block:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法是首先生成协变量的编码，然后将其作为块的基础：
- en: '![](../Images/6e86dde1aab893c7c647b9f329b525bc.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6e86dde1aab893c7c647b9f329b525bc.png)'
- en: Image from Challu et. al 2023
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 Challu 等人 2023
- en: DEPTS
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DEPTS
- en: '![](../Images/f7eccce5889922e30d548f5445b6bdfb.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f7eccce5889922e30d548f5445b6bdfb.png)'
- en: DEPTS Architecture from Fan et. al 2022[4]
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: DEPTS 架构来源于 Fan 等人 2022[4]
- en: 'DEPTS stands for **D**eep **E**xpansion **L**earning for **P**eriodic **T**ime
    **S**eries **F**orecasting and as the name suggests, this is an improvement to
    NBEATS that focuses on upgrading the periodic/seasonality forecasting capabilities
    of the original model [4]. Specifically, DEPTS changes the functionalities of
    NBEATS in two ways:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: DEPTS 代表 **D**eep **E**xpansion **L**earning for **P**eriodic **T**ime **S**eries
    **F**orecasting，顾名思义，这是一种对 NBEATS 的改进，专注于提升原始模型的周期性/季节性预测能力 [4]。具体来说，DEPTS 在两个方面改变了
    NBEATS 的功能：
- en: While NBEATS/NBEATSi could handle periodicity via the Fourier basis, this method
    struggles to take larger periodic behaviors that stretch beyond our lookback window
    into account, something that DEPTS is better equipped to handle [4].
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 虽然 NBEATS/NBEATSi 可以通过傅里叶基来处理周期性，但这种方法难以考虑超出回溯窗口的大周期行为，而 DEPTS 更好地处理这种情况 [4]。
- en: NBEATS/NBEATSi only can take additive seasonality into account (since it builds
    the forecasts additively. DEPTS can handle multiplicative seasonalities as well
    [4].
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NBEATS/NBEATSi 只能考虑加法季节性（因为它是逐步构建预测的）。DEPTS 也能处理乘法季节性 [4]。
- en: Periodicity Module & Periodic Context
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 周期性模块与周期性上下文
- en: 'The first mechanism DEPTS uses to achieve the two features above is its dedicated
    periodicity module that takes in the timesteps for both the lookback window and
    the forecast horizon and generates a seasonality context vector defined by a cosine
    series [4]:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: DEPTS 用来实现上述两个特性的第一个机制是其专用的周期性模块，该模块接受回溯窗口和预测范围的时间步长，并生成由余弦序列定义的季节性上下文向量 [4]：
- en: '![](../Images/26df5a1df036784e47802810236fa19e.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26df5a1df036784e47802810236fa19e.png)'
- en: Image from Fan et. al 2022 [4]
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 Fan 等人 2022[4]
- en: In this module, *A*, *F*, and *P* are all trainable parameters. **By allowing
    the network to learn these parameters and share them across predictions, the model
    can directly learn global periodic patterns instead of having to re-infer the
    patterns based on the input data as an NBEATS model has to do** [4].
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模块中，*A*、*F* 和 *P* 都是可训练的参数。**通过允许网络学习这些参数并在预测中共享它们，模型可以直接学习全局周期模式，而无需像 NBEATS
    模型那样基于输入数据重新推断模式** [4]。
- en: 'One tricky piece of this learning process however is that while attempting
    to learn *A*, *F*, and *P*, the model can easily get stuck in local minima. To
    get around this, a pre-learning optimization is performed to initialize the parameter
    values. Note: in this equation, *phi* represents the set of learnable parameters
    and *M* is a binary vector that gets multiplied with every term in the generator
    function (1 if the term is chosen, 0 else) [4]:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这一学习过程中的一个棘手之处在于，在尝试学习 *A*、*F* 和 *P* 时，模型很容易陷入局部最小值。为了解决这个问题，会进行预学习优化以初始化参数值。注意：在这个方程中，*phi*
    代表可学习参数的集合，*M* 是一个与生成函数中的每一项相乘的二进制向量（如果选择该项则为 1，否则为 0） [4]。
- en: '![](../Images/b00eb1396b0f913808f26b915e971ace.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b00eb1396b0f913808f26b915e971ace.png)'
- en: Image from Fan et. al 2022 [4]
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 Fan 等人 2022 [4]
- en: To put this simply, before training the network do a pre-fit of parameters on
    a training set, then choose the *J* best frequencies (the user chooses *J*). To
    avoid solving a whole optimization problem, the authors suggest performing a discrete
    cosine transform and choosing the *J* terms with the highest amplitudes [4].
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，在训练网络之前，先在训练集上进行参数的预拟合，然后选择 *J* 个最佳频率（用户选择 *J*）。为了避免解决整个优化问题，作者建议进行离散余弦变换，并选择具有最高幅度的
    *J* 项 [4]。
- en: Periodicity Block and Periodicity Residuals
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 周期性块和周期性残差
- en: 'The whole purpose of the periodicity module is to generate the initial periodic
    context vector *z*. These context vectors are passed into periodic blocks [4]:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 周期性模块的全部目的是生成初始周期性上下文向量 *z*。这些上下文向量被传递到周期性块 [4]。
- en: '![](../Images/30e265fb31cfea590f09e645433525b3.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/30e265fb31cfea590f09e645433525b3.png)'
- en: DEPTS Periodic Block Architecture from Fan et. al 2022[4]
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: DEPTS 周期性块架构来自 Fan 等人 2022[4]
- en: Simply put, the periodic block takes in the periodic context vector (which is
    effectively the seasonal components of the time series) and passes it through
    a very small sub-network producing a backcast and a forecast like in the traditional
    NBEATS block. **This takes the additive seasonalities that the periodicity module
    handles and processes them to deal with non-linear seasonalities** [4]**.**
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，周期性块接受周期性上下文向量（实际上是时间序列的季节性成分），并通过一个非常小的子网络，生成类似于传统 NBEATS 块的回溯和预测。**这处理了周期性模块处理的加性季节性，并将其处理为应对非线性季节性**
    [4]**。**
- en: 'How are these periodic backcasts and forecasts used? Diving closer into a layer
    of DEPTS gives us more insights into what''s happening. Note in this figure *x*
    is the target series [4]:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这些周期性回溯和预测是如何使用的？深入探讨 DEPTS 的一层可以给我们更多的见解。注意在这个图中，*x* 是目标序列 [4]。
- en: '![](../Images/a6325dca35ace00df886ec9ec3216812.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a6325dca35ace00df886ec9ec3216812.png)'
- en: DEPTS Layer Architecture from Fan et. al 2022[4]
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: DEPTS 层架构来自 Fan 等人 2022[4]
- en: Once the model generates the forecast vector backcast and forecast, the backcast
    is then subtracted from the input of the local block, and the forecast is added
    to the overall model forecast. The local block is the same as the block in NBEATS,
    performs the same operations, and uses the same doubly residual stacking mechanism.
    DEPTS also uses the doubly residual stacking mechanism for the context vector
    and we remove the explained seasonalities from the periodic context vector and
    both the local block output and periodic context vector as passed on to the next
    layer [4].
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型生成了预测向量回溯和预测，回溯将从局部块的输入中减去，预测则添加到整体模型预测中。局部块与 NBEATS 中的块相同，执行相同的操作，并使用相同的双重残差堆叠机制。DEPTS
    还使用双重残差堆叠机制来处理上下文向量，我们从周期性上下文向量中去除解释的季节性，并将局部块输出和周期性上下文向量传递到下一层 [4]。
- en: All of these mechanisms put together make DEPTS a periodic forecasting beast
    [4].
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些机制结合在一起使 DEPTS 成为一个周期性预测的强大工具 [4]。
- en: Interpretability
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可解释性
- en: Like NBEATS, DEPTS maintains many of the interpretation benefits of NBEATS,
    namely the aggregation of forecasts. However, DEPTS also offers a look into global
    periodic patterns, something NBEATS does not have. It is easy to visualize the
    global periodic patterns by just running the periodicity module. Additionally,
    one can examine the final periodic context vector to see how a local prediction
    may deviate from global patterns by looking at the “leftover” patterns.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 像NBEATS一样，DEPTS保持了NBEATS的许多解释优势，即预测的聚合。然而，DEPTS还提供了对全球周期模式的观察，这是NBEATS所没有的。通过运行周期性模块，可以轻松可视化全球周期模式。此外，通过查看“剩余”模式，可以检查最终的周期上下文向量，以了解局部预测如何可能偏离全球模式。
- en: N-HiTS
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: N-HiTS
- en: '![](../Images/e19bf2949152a6fdb6359ca12abb44cc.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e19bf2949152a6fdb6359ca12abb44cc.png)'
- en: N-HiTS Architecture from Challu et. al 2022 [2]
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: N-HiTS架构来自Challu等人2022[2]
- en: 'The second major improvement from NBEATS is N-HiTS or **N**eural **H**ierarchical
    **I**nterpolation for **T**ime Series **F**orecasting [2]. While DEPTS introduced
    dedicated periodic forecasting mechanisms, N-HiTS focuses on better structuring
    the forecast aggregation process and offers two main benefits:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: NBEATS的第二大改进是N-HiTS或**N**eural **H**ierarchical **I**nterpolation for **T**ime
    Series **F**orecasting [2]。虽然DEPTS引入了专门的周期性预测机制，N-HiTS则专注于更好地构建预测聚合过程，并提供了两个主要优势：
- en: Through a hierarchical method of forecast aggregation, N-HiTS is better at identifying
    and extracting signals than N-BEATS is in most cases [2].
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过层次化的预测聚合方法，N-HiTS在识别和提取信号方面比N-BEATS在大多数情况下更为出色[2]。
- en: Through interpolation mechanisms, N-HiTS’s memory usage is magnitudes smaller
    than NBEATS’s, making it even easier to train [2].
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过插值机制，N-HiTS的内存使用量比NBEATS小得多，使得训练变得更加容易[2]。
- en: 'All of this is possible due to the Hierarchical Interpolation mechanism which
    is made up of 2 parts: Multi-Rate Signal sampling and Interpolation [2].'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切都得益于层次插值机制，它由两个部分组成：多速率信号采样和插值[2]。
- en: Multi-Rate Signal Sampling
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多速率信号采样
- en: '![](../Images/8304f5648b9160236c9d42ce723cad26.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8304f5648b9160236c9d42ce723cad26.png)'
- en: N-HiTS Block Architecture from Challu et. al 2022 [2]
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: N-HiTS块架构来自Challu等人2022[2]
- en: 'The first primary mechanism is Multi-Rate signal sampling. This is extremely
    simple (which makes it even more beautiful): Add a MaxPool layer at the start
    of each block that pre-processes the block input and effectively smooths it [2].
    Why is this useful? Well, w**e can vary the kernel size of the MaxPool layers
    across each of the blocks and effectively capture different magnitudes of signal
    sizes [2].** Additionally, the MaxPool also makes the model less susceptible to
    noise due to the smoothing it provides and by itself is a powerful, but simple
    tool [2].'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个主要机制是多速率信号采样。这非常简单（这使得它更加美丽）：在每个块的开头添加一个MaxPool层，该层预处理块输入并有效地平滑它[2]。这为什么有用？好吧，**我们可以在每个块中改变MaxPool层的内核大小，有效捕捉不同大小的信号[2]。**
    此外，MaxPool还通过提供平滑处理，使模型对噪声的敏感性降低，本身就是一个强大但简单的工具[2]。
- en: Interpolation
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 插值
- en: The multi-rate sampling is powerful, but it works even better when combined
    with an interpolation mechanism.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 多速率采样非常强大，但与插值机制结合使用时效果更佳。
- en: Going back to the original generic N-BEATS block architecture, each block generates
    2 sets of weights, a backcast set, and a forecast set. Normally, the number of
    weights generated is equal to the length of the window *L* (for the backcast)
    or the length of the horizon *H* (for the forecast) [1]. However, this approach
    has a few problems. For one, predicting all the weights can cause the generated
    forecasts to be more volatile and noisy [2]. Additionally, for larger horizons,
    this can cause extremely high memory usage [2].
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 回到原始的通用N-BEATS块架构，每个块生成两组权重：回溯权重集和预测权重集。通常，生成的权重数量等于窗口*L*的长度（用于回溯）或预测*H*的长度[1]。然而，这种方法存在一些问题。首先，预测所有权重可能导致生成的预测更具波动性和噪声[2]。此外，对于较大的预测范围，这可能导致极高的内存使用[2]。
- en: N-HiTS instead chose to apply interpolation where the number of predicted parameters
    was instead equal to *r*H* (and *r*L* for the backcast) where r is the expressiveness
    ratio [2]. A higher expressiveness ratio results in more parameters predicted.
    Then one can fill in the missing weights through any interpolation method. The
    authors test linear interpolation, nearest neighbor interpolation, and cubic interpolation,
    but any custom methods can also be used.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: N-HiTS 选择应用插值，其中预测的参数数量等于 *r*H*（以及回溯的 *r*L*），其中 r 是表现比率 [2]。更高的表现比率会导致更多的参数被预测。然后可以通过任何插值方法填补缺失的权重。作者测试了线性插值、最近邻插值和三次插值，但也可以使用任何自定义方法。
- en: By predicting only a few of the weights, the memory usage of N-HiTS is extremely
    small when compared to N-BEATS, making it a much more lightweight model.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 通过仅预测少量的权重，与 N-BEATS 相比，N-HiTS 的内存使用极其少，使其成为一个更轻量级的模型。
- en: 'NOTE: To my knowledge, the authors have not provided interpolation mechanisms
    for the interpretable basis functions, so N-HiTS is limited to the generic basis.'
  id: totrans-104
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意：据我所知，作者尚未提供可解释基函数的插值机制，因此 N-HiTS 限于通用基函数。
- en: Hierarchical Interpolation
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分层插值
- en: 'The authors then combine this weight interpolation as well as the multi-rate
    signal sampling mentioned above to create the hierarchical interpolation mechanism.
    I**n hierarchical interpolation, both the expressiveness ratio and the MaxPool
    kernel size of the blocks are inversely varied across the blocks** [2]**.** Any
    block with a high expressiveness ratio also uses a small kernel size. Any block
    with a low expressiveness ratio uses a large kernel size. The synchronization
    of these two parameters is really what allows N-HiTS to capture different magnitude
    signals as shown in the figure below [2]:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，作者将这种权重插值与上述多速率信号采样结合起来，创建了分层插值机制。**在分层插值中，块的表现比率和 MaxPool 核心大小在各个块之间是反向变化的**
    [2]**。** 任何具有高表现比率的块也使用小的核心大小。任何具有低表现比率的块使用大的核心大小。这两个参数的同步正是使 N-HiTS 能够捕捉不同幅度信号的原因，如下图所示
    [2]：
- en: '![](../Images/2433c91bab87ade4a1f294f5f3708dcc.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2433c91bab87ade4a1f294f5f3708dcc.png)'
- en: Linear Interpolation (Left) vs. No Interpolation (Right). Figure from Challu
    et. al 2022 [2]
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 线性插值（左）与非插值（右）。图源自 Challu 等人 2022 [2]
- en: 'Intuitively this also makes sense: if aggressive smoothing is applied to a
    function (AKA a large kernel size MaxPool), fewer parameters are needed to capture
    the behavior of the input and vice versa.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 直观上，这也有意义：如果对函数应用了强烈平滑（即大核心大小 MaxPool），则需要更少的参数来捕捉输入的行为，反之亦然。
- en: Hierarchical interpolation also provides more interpretability to the generic
    form of NBEATS. Users can look at a stack output and understand which one is modeling
    larger patterns, and which ones are trying to capture small noise. While N-HiTS
    cannot use the interpretable basis equations, one can make the argument that N-HiTS
    does not need those to still be interpretable.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 分层插值还为 NBEATS 的通用形式提供了更多的可解释性。用户可以查看堆叠输出，并理解哪个在建模更大的模式，哪个在捕捉小的噪声。虽然 N-HiTS 无法使用可解释基方程，但可以认为
    N-HiTS 不需要这些方程仍然可以具有可解释性。
- en: Conclusion
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: When N-BEATS came out, it changed the deep forecasting space and spawned a new
    branch of forecasting. Suddenly we could get highly accurate forecasts from a
    relatively simple model that could be trained end-to-end. N-HiTS and DEPTS were
    then able to build on NBEATS and add even more functionalities that enhance specific
    aspects of basis expansion methods. It's important to note that the hierarchical
    interpolation of N-HiTS and the periodicity modules/blocks/context vectors of
    DEPTS can be used together as well, creating an even more powerful model.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 当 N-BEATS 出现时，它改变了深度预测领域，并催生了一个新的预测分支。突然间，我们可以从一个相对简单的模型中获得高度准确的预测，该模型可以端到端地训练。N-HiTS
    和 DEPTS 然后能够在 NBEATS 的基础上进行扩展，增加了更多功能，增强了基扩展方法的特定方面。值得注意的是，N-HiTS 的分层插值与 DEPTS
    的周期性模块/块/上下文向量也可以一起使用，创建一个更强大的模型。
- en: There is also some work in applying attention to these basis expansion methods,
    mainly the DeepFS model by Jiang et. al [12] which uses an attention layer (adapted
    from [7]) to generate an encoding, and then pass that encoding into a module that
    resembles a periodic N-BEATS block as well as into a feed-forward stack. While
    this can be considered an improvement, I did not include a full detailed description
    of this model in this article as the benefit of attention in time-series forecasting
    is still up for debate as shown by [9].
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些工作将注意力机制应用于这些基础扩展方法，主要是 Jiang 等人提出的 DeepFS 模型 [12]，该模型使用了一个注意力层（改编自 [7]）来生成编码，然后将该编码传递到类似周期性
    N-BEATS 块的模块中，以及传递到一个前馈堆栈中。尽管这可以被视为一种改进，但我在本文中没有详细描述此模型，因为正如 [9] 所示，时间序列预测中注意力机制的益处仍然存在争议。
- en: There is also some debate on whether or not basis expansion is interpretable.
    If in the end, our models produce 40 different equations, is it interpretable?
    Additionally, while we can identify what signals all of these models are identifying,
    the mechanisms for determining these are still relatively black box and we may
    not know why a model chooses to pick up or ignore a given signal. The interpretability
    also reduces when adapting these to use covariate time series.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 关于基础扩展是否可解释也存在一些争论。如果最终我们的模型产生了40个不同的方程，这是否能被解释？此外，尽管我们可以识别这些模型所识别的所有信号，但确定这些信号的机制仍然相对不透明，我们可能不知道为什么模型选择接收或忽略某个信号。当将这些模型适应到使用协变量时间序列时，可解释性也会降低。
- en: Despite these flaws, however, all the models discussed here have consistently
    shown strong performance and have paved the way for more innovation in the forecasting
    space.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些缺陷，但本文讨论的所有模型在表现上始终表现出强大的性能，并为预测领域的更多创新铺平了道路。
- en: Resources and References
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源和参考文献
- en: Resources and References
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源和参考文献
- en: N-BEATS and N-HiTS can be used through [Darts](https://unit8co.github.io/darts/index.html)
    and [Pytorch Forecasting](https://pytorch-forecasting.readthedocs.io/en/stable/models.html),
    two high-quality forecasting packages for Python.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: N-BEATS 和 N-HiTS 可以通过 [Darts](https://unit8co.github.io/darts/index.html) 和
    [Pytorch Forecasting](https://pytorch-forecasting.readthedocs.io/en/stable/models.html)
    这两个高质量的 Python 预测包来使用。
- en: '[Source code for DEPTS](https://github.com/weifantt/DEPTS)'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[DEPTS 的源代码](https://github.com/weifantt/DEPTS)'
- en: '[Source code for N-HiTS](https://github.com/cchallu/n-hits)'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[N-HiTS 的源代码](https://github.com/cchallu/n-hits)'
- en: '[Source code for NBEATSx](https://github.com/cchallu/nbeatsx)'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[NBEATSx 的源代码](https://github.com/cchallu/nbeatsx)'
- en: '[Source code for NBEATS](https://github.com/ServiceNow/N-BEATS)'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[NBEATS 的源代码](https://github.com/ServiceNow/N-BEATS)'
- en: References
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] B.N. Oreshkin, D.Carpov, N. Chapados, Y. Bengio. [N-BEATS: Neural basis
    expansion analysis for interpretable time series forecasting](https://arxiv.org/abs/1905.10437)
    (2020). Eighth International Conference on Learning Representations.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] B.N. Oreshkin, D. Carpow, N. Chapados, Y. Bengio. [N-BEATS: 神经基础扩展分析用于可解释的时间序列预测](https://arxiv.org/abs/1905.10437)
    (2020). 第八届国际学习表征会议。'
- en: '[2] C. Challu, K.G. Olivares, B.N. Oreshkin, F. Garza, M. Mergenthaler-Canseco,
    A. Dubrawski. [N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting](https://arxiv.org/abs/2201.12886)
    (2022). Thirty-Seventh AAAI Conference on Artificial Intelligence'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] C. Challu, K.G. Olivares, B.N. Oreshkin, F. Garza, M. Mergenthaler-Canseco,
    A. Dubrawski. [N-HiTS: 神经层次插值用于时间序列预测](https://arxiv.org/abs/2201.12886) (2022).
    第三十七届 AAAI 人工智能会议。'
- en: '[3] K.G. Olivares, C. Challu, G. Marcjasz, R. Weron, A. Dubrawski. [Neural
    basis expansion analysis with exogenous variables: Forecasting electricity prices
    with NBEATSx](https://www.sciencedirect.com/science/article/pii/S0169207022000413)
    (2023). International Journal of Forecasting.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] K.G. Olivares, C. Challu, G. Marcjasz, R. Weron, A. Dubrawski. [带外生变量的神经基础扩展分析：使用
    NBEATSx 预测电力价格](https://www.sciencedirect.com/science/article/pii/S0169207022000413)
    (2023). 国际预测期刊。'
- en: '[4] W. Fan, S. Zheng, X. Yi, W. Cao, Y. Fu, J. Bian, T-Y. Liu. [DEPTS: Deep
    Expansion Learning for Periodic Time Series Forecasting](https://arxiv.org/abs/2203.07681)
    (2022). Tenth International Conference on Learning Representations.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] W. Fan, S. Zheng, X. Yi, W. Cao, Y. Fu, J. Bian, T-Y. Liu. [DEPTS: 深度扩展学习用于周期时间序列预测](https://arxiv.org/abs/2203.07681)
    (2022). 第十届国际学习表征会议。'
- en: '[5] S. Li, X. Jin, Y. Xuan, X. Zhou, W. Chen, Y. Wang, X. Yan. [Enhancing the
    Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting](https://proceedings.neurips.cc/paper/2019/hash/6775a0635c302542da2c32aa19d86be0-Abstract.html)
    (2019). Advances in Neural Information Processing systems 32.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] S. Li, X. Jin, Y. Xuan, X. Zhou, W. Chen, Y. Wang, X. Yan. [增强变换器在时间序列预测中的局部性并突破记忆瓶颈](https://proceedings.neurips.cc/paper/2019/hash/6775a0635c302542da2c32aa19d86be0-Abstract.html)
    (2019). 神经信息处理系统大会第32届'
- en: '[6] H. Wu, J. Xu, J. Wang, M. Long. [Autoformer: Decomposition Transformers
    with Auto-Correlation for Long-Term Series Forecasting](https://arxiv.org/abs/2106.13008)
    (2021). Advances in Neural Information Processing Systems 34.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] H. Wu, J. Xu, J. Wang, M. Long. [Autoformer：具有自相关的分解变换器用于长期序列预测](https://arxiv.org/abs/2106.13008)
    (2021). 神经信息处理系统大会第34届'
- en: '[7] H. Zhou, S. Zhang, J. Peng, S. Zhang, J. Li, H. Xiong, W. Zhang. [Informer:
    Beyond Efficient Transformer for Long Sequence Time-Series Forecasting](https://arxiv.org/abs/2012.07436)
    (2021). The Thirty-Fifth AAAI Conference on Artificial Intelligence, Virtual Conference.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] H. Zhou, S. Zhang, J. Peng, S. Zhang, J. Li, H. Xiong, W. Zhang. [Informer：超越高效变换器用于长序列时间序列预测](https://arxiv.org/abs/2012.07436)
    (2021). 第三十五届AAAI人工智能大会，虚拟会议'
- en: '[8] T. Zhou, Z. Ma, Q. Wen, X. Wang, L. Sun, R. Jin. F[EDformer: Frequency
    Enhanced Decomposed Transformer for Long-term Series Forecasting](https://arxiv.org/abs/2201.12740)
    (2022). 39th International Conference on Machine Learning.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] T. Zhou, Z. Ma, Q. Wen, X. Wang, L. Sun, R. Jin. [EDformer：用于长期序列预测的频率增强分解变换器](https://arxiv.org/abs/2201.12740)
    (2022). 第39届国际机器学习会议'
- en: '[9] A. Zeng, M. Chen, L. Zhang, Q. Xu. [Are Transformers Effective for Time
    Series Forecasting?](https://arxiv.org/abs/2205.13504) (2022). Thirty-Seventh
    AAAI Conference on Artificial Intelligence.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] A. Zeng, M. Chen, L. Zhang, Q. Xu. [变换器对时间序列预测是否有效？](https://arxiv.org/abs/2205.13504)
    (2022). 第三十七届AAAI人工智能大会'
- en: '[10] D. Vorotyntsev. [Is Attention Explanation?](/is-attention-explanation-b609a7b0925c)
    (2022). Towards Data Science.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] D. Vorotyntsev. [注意力是解释吗？](/is-attention-explanation-b609a7b0925c) (2022).
    数据科学前沿'
- en: '[12] S. Jiang, T. Syed, X. Zhy, J. Levy, B. Aronchik, Y. Sun. [Bridging self-attention
    and time series decomposition for periodic forecasting](https://www.amazon.science/publications/bridging-self-attention-and-time-series-decomposition-for-periodic-forecasting)
    (2022). 31st ACM International Conference on Information and Knowledge Management'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] S. Jiang, T. Syed, X. Zhy, J. Levy, B. Aronchik, Y. Sun. [桥接自注意力和时间序列分解以进行周期性预测](https://www.amazon.science/publications/bridging-self-attention-and-time-series-decomposition-for-periodic-forecasting)
    (2022). 第31届ACM国际信息与知识管理会议'
- en: '[11]S. Smyl, J. Ranganathan, A Pasqua. [M4 Forecasting Competition: Introducing
    a New Hybrid ES-RNN Model](https://eng.uber.com/m4-forecasting-competition/) (2018).
    Uber Research'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[11]S. Smyl, J. Ranganathan, A Pasqua. [M4预测竞赛：介绍一种新的混合ES-RNN模型](https://eng.uber.com/m4-forecasting-competition/)
    (2018). 优步研究'
