- en: 'Perceptrons: The First Neural Network Model'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感知机：第一个神经网络模型
- en: 原文：[https://towardsdatascience.com/perceptrons-the-first-neural-network-model-8b3ee4513757](https://towardsdatascience.com/perceptrons-the-first-neural-network-model-8b3ee4513757)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/perceptrons-the-first-neural-network-model-8b3ee4513757](https://towardsdatascience.com/perceptrons-the-first-neural-network-model-8b3ee4513757)
- en: Overview and implementation in Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述及在 Python 中的实现
- en: '[](https://medium.com/@roiyeho?source=post_page-----8b3ee4513757--------------------------------)[![Dr.
    Roi Yehoshua](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----8b3ee4513757--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8b3ee4513757--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8b3ee4513757--------------------------------)
    [Dr. Roi Yehoshua](https://medium.com/@roiyeho?source=post_page-----8b3ee4513757--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@roiyeho?source=post_page-----8b3ee4513757--------------------------------)[![Dr.
    Roi Yehoshua](../Images/905a512ffc8879069403a87dbcbeb4db.png)](https://medium.com/@roiyeho?source=post_page-----8b3ee4513757--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8b3ee4513757--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8b3ee4513757--------------------------------)
    [Dr. Roi Yehoshua](https://medium.com/@roiyeho?source=post_page-----8b3ee4513757--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8b3ee4513757--------------------------------)
    ·14 min read·Mar 28, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8b3ee4513757--------------------------------)
    ·14 分钟阅读·2023年3月28日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/878b3bf6ae98c62e8e147e6ad2892349.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/878b3bf6ae98c62e8e147e6ad2892349.png)'
- en: Photo by [Hal Gatewood](https://unsplash.com/ja/@halacious?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/OgvqXGL7XO4?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[Hal Gatewood](https://unsplash.com/ja/@halacious?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    在 [Unsplash](https://unsplash.com/photos/OgvqXGL7XO4?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    上的照片'
- en: Perceptrons are one of the earliest computational models of neural networks
    (NNs), and they form the basis for the more complex and deep networks we have
    today. Understanding the perceptron model and its theory will provide you with
    a good basis for understanding many of the key concepts in neural networks in
    general.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机是神经网络（NN）的最早计算模型之一，它们构成了今天更复杂、更深层网络的基础。理解感知机模型及其理论将为你理解神经网络中的许多关键概念提供良好的基础。
- en: 'Background: Biological Neural Networks'
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景：生物神经网络
- en: A biological neural network (such as the one we have in our brain) is composed
    of a large number of nerve cells called **neurons**.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 生物神经网络（例如我们大脑中的网络）由大量称为**神经元**的神经细胞组成。
- en: Each neuron receives electrical signals (impulses) from its neighboring neurons
    via fibers called **dendrites**. When the total sum of its incoming signals exceeds
    some threshold, the neuron “fires” its own signal via long fibers called **axons**
    that are connected to the dendrites of other neurons.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 每个神经元通过称为**树突**的纤维接收来自其邻近神经元的电信号（冲动）。当其接收到的信号总和超过某个阈值时，神经元会通过长纤维称为**轴突**发出自己的信号，这些轴突连接到其他神经元的树突上。
- en: The junction between two neurons is called a **synapse**. On average, each neuron
    is connected to about 7,000 synapses, which demonstrates the high connectivity
    of the network we have in our brain. When we learn new associations between two
    concepts, the synaptic strength between the neurons that represent these concepts
    is strengthened. This phenomenon is known as **Hebb’s rule** (1949) that states
    “Cells that fire together wire together”.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 两个神经元之间的连接称为**突触**。平均而言，每个神经元连接到大约7000个突触，这表明我们大脑网络的高度连通性。当我们学习两个概念之间的新联想时，表示这些概念的神经元之间的突触强度会增强。这种现象被称为**赫布规则**（1949），其表述为“一起发火的细胞会一起连接”。
- en: '![](../Images/ba4ba1b74e63ac1caa391f0d56969023.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba4ba1b74e63ac1caa391f0d56969023.png)'
- en: Biological neuron ([public image](https://en.wikipedia.org/wiki/Neuron#/media/File:Blausen_0657_MultipolarNeuron.png)
    freely licensed under Wikimedia Commons)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 生物神经元 ([公共图像](https://en.wikipedia.org/wiki/Neuron#/media/File:Blausen_0657_MultipolarNeuron.png)
    在维基共享资源下自由授权)
- en: The Perceptron Model
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感知机模型
- en: The perceptron model, introduced by Frank Rosenblatt in 1957, is a simplified
    model of a biological neuron.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机模型由弗兰克·罗森布拉特于1957年提出，是生物神经元的简化模型。
- en: The perceptron has *m* binary inputs denoted by *x*₁, …, *xₘ*, which represent
    the incoming signals from its neighboring neurons, and it outputs a single binary
    value denoted by *o* that indicates if the perceptron is “firing” or not.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机有 *m* 个二进制输入，表示为 *x*₁, …, *xₘ*，这些输入代表来自其邻近神经元的信号，并输出一个二进制值 *o*，表示感知机是否“激发”。
- en: '![](../Images/043749ddc33f1bca4ca04eef5dd60caa.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/043749ddc33f1bca4ca04eef5dd60caa.png)'
- en: The perceptron model
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机模型
- en: Each input neuron *xᵢ* is connected to the perceptron via a link whose strength
    is represented by a weight *wᵢ*. Inputs with higher weights have a larger influence
    on the perceptron’s output.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 每个输入神经元 *xᵢ* 通过一个强度由权重 *wᵢ* 表示的连接与感知机相连。权重较高的输入对感知机的输出有更大的影响。
- en: 'The perceptron first computes the weighted sum of its incoming signals, by
    multiplying each input by its corresponding weight. This weighted sum is often
    called **net input** and denoted by *z*:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机首先计算其输入信号的加权和，通过将每个输入乘以其对应的权重。这个加权和通常称为**净输入**，表示为 *z*：
- en: '![](../Images/f7f2b629f0e4be2dab30d99b80d48746.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f7f2b629f0e4be2dab30d99b80d48746.png)'
- en: The net input of the perceptron
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机的净输入
- en: 'If the net input exceeds some predefined threshold value *θ*, then the perceptron
    fires (its output is 1), otherwise it doesn’t fire (its output is 0). In other
    words, the perceptron fires if and only if:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果净输入超过某个预定义的阈值 *θ*，则感知机激发（输出为1），否则不激发（输出为0）。换句话说，感知机当且仅当：
- en: '![](../Images/7bf847811aec716d437c5eba51b5e35b.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7bf847811aec716d437c5eba51b5e35b.png)'
- en: Our goal is to find the weights *w*₁, …, *wₘ* and the threshold *θ,* such that
    the perceptron will map correctly its inputs *x*₁, …, *xₘ* (representing the features
    in our data) to the desired output *y* (representing the label).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是找到权重 *w*₁, …, *wₘ* 和阈值 *θ*，使得感知机能正确地将其输入 *x*₁, …, *xₘ*（代表数据中的特征）映射到期望的输出
    *y*（代表标签）。
- en: To simplify the learning process, instead of having to learn separately the
    weights and the threshold, we add a special input neuron called **bias neuron**
    that always outputs the value 1\. This neuron is typically denoted by *x*₀ and
    its connection weight is denoted by *b* or *w*₀.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化学习过程，我们添加了一个特殊的输入神经元，称为**偏置神经元**，它总是输出值1。这个神经元通常表示为*x*₀，其连接权重表示为*b*或*w*₀。
- en: 'As a result, the net input of the perceptron becomes:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，感知机的净输入变为：
- en: '![](../Images/84fd07a2f3ead2616c2b01f358b0b5b0.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84fd07a2f3ead2616c2b01f358b0b5b0.png)'
- en: The net input including the bias
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 包括偏置的净输入
- en: This formulation allows us to learn the correct threshold (bias) as if it were
    one of the weights of the incoming signals.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这种公式允许我们像学习输入信号的权重一样学习正确的阈值（偏置）。
- en: 'In vector form, we can write *z* as the **dot product** between the input vector
    **x** = (*x*₁, …, *xₘ*)*ᵗ* and the weight vector **w** = (*w*₁, …, *wₘ*)*ᵗ* plus
    the bias:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在向量形式中，我们可以将 *z* 写作输入向量 **x** = (*x*₁, …, *xₘ*)*ᵗ* 与权重向量 **w** = (*w*₁, …, *wₘ*)*ᵗ*
    的**点积**加上偏置：
- en: '![](../Images/21d462f2bbd5c1df340311876109e3e7.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/21d462f2bbd5c1df340311876109e3e7.png)'
- en: Vector form of the net input
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 净输入的向量形式
- en: And the perceptron fires if and only if the net input is non-negative, i.e.,
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 只有当净输入非负时，感知机才会激发，即：
- en: '![](../Images/0ba200fc4137980148b2d9262f728336.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ba200fc4137980148b2d9262f728336.png)'
- en: 'More generally, the perceptron applies an **activation function** *f*(*z*)
    on the net input that generates its output. The two most common activation functions
    used in perceptrons are:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地，感知机对净输入应用**激活函数** *f*(*z*)，以生成其输出。感知机中最常用的两个激活函数是：
- en: 'The **step function** (also known as the **heaviside function**) is a function
    whose value is 0 for negative inputs and 1 for non-negative inputs:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**阶跃函数**（也称为**海维赛德函数**）是一个对于负输入值为0，对于非负输入值为1的函数：'
- en: '![](../Images/0864fe1dc11939ff7aea0427aa3ce731.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0864fe1dc11939ff7aea0427aa3ce731.png)'
- en: The step function
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 阶跃函数
- en: '2\. The **sign function** is a function whose value is -1 for negative inputs
    and 1 for non-negative inputs:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. **符号函数**是一个对于负输入值为-1，对于非负输入值为1的函数：
- en: '![](../Images/77bfa4cb419bf74b935720955c8c4563.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77bfa4cb419bf74b935720955c8c4563.png)'
- en: The sign function
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 符号函数
- en: Other types of activation functions are used in more complex networks, such
    as [multi-layer perceptrons](https://medium.com/@roiyeho/multi-layer-perceptrons-8d76972afa2b)
    (MLPs). For the rest of this article, I will assume that the perceptron is using
    the step function.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在更复杂的网络中使用其他类型的激活函数，例如[多层感知机](https://medium.com/@roiyeho/multi-layer-perceptrons-8d76972afa2b)（MLPs）。在本文的其余部分，我将假设感知机使用步进函数。
- en: 'To summarize, the computation of the perceptron consists of two steps:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，感知机的计算包括两个步骤：
- en: Multiplication of the input values *x*₁, …, *xₘ* by their corresponding weights
    *w*₁, …, *wₘ*, and adding the bias *b*, which gives us the net input of the perceptron
    *z =* **w***ᵗ***x** + *b***.**
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入值 *x*₁, …, *xₘ* 乘以其对应的权重 *w*₁, …, *wₘ*，然后加上偏置 *b*，得到感知机的净输入 *z =* **w***ᵗ***x**
    + *b***。**
- en: Applying an activation function *f*(*z*) on the net input that generates a binary
    output (0/1 or -1/+1).
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在生成二进制输出（0/1 或 -1/+1）的净输入上应用激活函数 *f*(*z*)。
- en: 'We can write this entire computation in one equation:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将整个计算写成一个方程：
- en: '![](../Images/ba8a1cdc2c7301c95a677ad6bf1e1529.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba8a1cdc2c7301c95a677ad6bf1e1529.png)'
- en: where *f* is the chosen activation function and *o* is the output of the perceptron.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *f* 是选择的激活函数，*o* 是感知机的输出。
- en: Implementing Logic Gates with Perceptrons
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用感知机实现逻辑门
- en: To demonstrate how perceptrons work, let’s try to build perceptrons that compute
    the logical functions AND and OR.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示感知机的工作原理，让我们尝试构建计算逻辑函数“与”和“或”的感知机。
- en: As a reminder, the logical AND function has two binary inputs and returns true
    (1) if both of its inputs are true, otherwise it returns false (0).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，逻辑“与”函数有两个二进制输入，当两个输入都为真时返回真（1），否则返回假（0）。
- en: '![](../Images/94ce5425ed83d8e3d79eb1e44161ed96.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94ce5425ed83d8e3d79eb1e44161ed96.png)'
- en: The truth table of the AND function
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: “与”函数的真值表
- en: A perceptron that implements the AND function has two binary inputs and a bias.
    We want this perceptron to “fire” only when both of its inputs are “firing”. This
    can achieved, for example, by choosing the same weight for both inputs, e.g.,
    *w*₁ = *w*₂ = 1, and then choosing the bias to be within the range [-2, -1). This
    will make sure that when both neurons are firing, the net input 2 + *b* will be
    non-negative, but when only one of them is firing, the net input 1 + *b* will
    be negative (and when none of them is firing the net input *b* is also negative).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 实现“与”函数的感知机有两个二进制输入和一个偏置。我们希望这个感知机只有在两个输入都“激活”时才会“触发”。例如，可以通过为两个输入选择相同的权重来实现这一点，例如
    *w*₁ = *w*₂ = 1，然后选择偏置在[-2, -1)范围内。这样可以确保当两个神经元都激活时，净输入 2 + *b* 将是非负的，但当只有一个神经元激活时，净输入
    1 + *b* 将是负的（当没有神经元激活时，净输入 *b* 也为负）。
- en: '![](../Images/ab290827a4ef76a52203716bf9743d4f.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ab290827a4ef76a52203716bf9743d4f.png)'
- en: A perceptron that computes the logical AND function
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 计算逻辑“与”函数的感知机
- en: 'In a similar fashion, we can build a perceptron that computes the logical OR
    function:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 以类似的方式，我们可以构建一个计算逻辑“或”函数的感知机：
- en: '![](../Images/63edd484905b306da029143caade9c83.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63edd484905b306da029143caade9c83.png)'
- en: A perceptron that computes the logical OR function
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 计算逻辑“或”函数的感知机
- en: Verify that you understand how this perceptron works!
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 验证你是否理解这个感知机是如何工作的！
- en: 'As an exercise, try to build a perceptron for the NAND function, whose truth
    table is shown below:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，尝试构建一个用于 NAND 函数的感知机，其真值表如下所示：
- en: '![](../Images/9e9a4ac7ef6c78105c8408baae08e5ae.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e9a4ac7ef6c78105c8408baae08e5ae.png)'
- en: The truth table of the NAND function
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: NAND 函数的真值表
- en: Perceptrons as Linear Classifiers
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感知机作为线性分类器
- en: The perceptron is a type of a **linear classifier**, since it divides the input
    space into two areas separated by the following hyperplane
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机是一种**线性分类器**，因为它将输入空间划分为由下述超平面分隔的两个区域
- en: '![](../Images/bccef6a6d09686cf3a9fc3c06fc0822e.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bccef6a6d09686cf3a9fc3c06fc0822e.png)'
- en: The equation of the separating hyperplane
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 分隔超平面的方程
- en: The weight vector **w** is orthogonal to this hyperplane, and thus determines
    its orientation, while the bias *b* defines its distance from the origin.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 权重向量**w**与这个超平面正交，从而决定其方向，而偏置 *b* 定义了它离原点的距离。
- en: Every example above the hyperplane (**w***ᵗ***x** + *b*> 0**)** is classified
    by the perceptron as a positive example, while every example below the hyperplane
    (**w***ᵗ***x** + *b*< 0**)** is classified as a negative example.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 每个超平面上方的例子（**w***ᵗ***x** + *b*> 0**））被感知机分类为正例，而每个超平面下方的例子（**w***ᵗ***x** + *b*<
    0**））被分类为负例。
- en: '![](../Images/d62f07c961837097e54a9ed3a8042ebf.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d62f07c961837097e54a9ed3a8042ebf.png)'
- en: Perceptron as a linear classifier
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 作为线性分类器的感知器
- en: Other linear classifiers include logistic regression and linear SVMs (support
    vector machines).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 其他线性分类器包括逻辑回归和线性支持向量机（SVM）。
- en: Linear classifiers are capable of learning only **linearly separable** problems,
    i.e., problems where the decision boundary between the positive and the negative
    examples is a linear surface (a hyperplane).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 线性分类器只能学习**线性可分**的问题，即正例和负例之间的决策边界是一个线性表面（一个超平面）。
- en: 'For example, the following data set is not linearly separable, therefore a
    perceptron cannot classify correctly all the examples in this data set:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以下数据集是非线性可分的，因此感知器无法正确分类此数据集中的所有示例：
- en: '![](../Images/11a30412fa1736f1844350d2d41065e2.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11a30412fa1736f1844350d2d41065e2.png)'
- en: Non-linearly separable data set
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性可分数据集
- en: The Perceptron Learning Rule
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感知器学习规则
- en: The perceptron has a simple learning rule that is guaranteed to find the separating
    hyperplane if the data is linearly separable.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器具有一个简单的学习规则，如果数据是线性可分的，它可以找到分离超平面。
- en: 'For each training sample (**x***ᵢ*, *yᵢ*) that is misclassified by the perceptron
    (i.e., *oᵢ* ≠ *yᵢ*), we apply the following update rule to the weight vector:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每一个被感知器误分类的训练样本（即*oᵢ* ≠ *yᵢ*），我们将以下更新规则应用于权重向量：
- en: '![](../Images/070b24e3a53321310326907cd5fba50c.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/070b24e3a53321310326907cd5fba50c.png)'
- en: The perceptron learning rule
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器学习规则
- en: where *α* is a learning rate (0 < *α* ≤ 1) that controls the size of the weight
    adjustment in each update.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*α*是学习率（0 < *α* ≤ 1），它控制每次更新中权重调整的大小。
- en: In other words, we add to each connection weight *wⱼ* the error of the perceptron
    on this example (the difference between the true label *yᵢ* and the output *oᵢ*)
    multiplied by the value of the corresponding input *xᵢⱼ* and the learning rate.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们将每个连接权重*wⱼ*的感知器在这个例子上的误差（真实标签*yᵢ*和输出*oᵢ*之间的差异）乘以相应输入*xᵢⱼ*的值和学习率。
- en: 'What this learning rule tries to do is to reduce the discrepancy between the
    perceptron’s output *oᵢ* and the true label *yᵢ*. To understand why it works,
    let’s examine the two possible cases of a misclassification by the perceptron:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这个学习规则尝试做的是减少感知器输出*oᵢ*和真实标签*yᵢ*之间的差距。为了理解它为什么有效，让我们检查感知器误分类的两种可能情况：
- en: The true label is *yᵢ* = 1, but the perceptron’s prediction is *oᵢ* =0, i.e.,
    **w***ᵗ***x***ᵢ + b* < 0\. In this case, we would like to **increase** the perceptron’s
    net input so eventually it becomes positive.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 真实标签是*yᵢ* = 1，但感知器的预测是*oᵢ* =0，即**w***ᵗ***x***ᵢ + b* < 0。在这种情况下，我们希望**增加**感知器的净输入，使其最终变为正值。
- en: To that end, we add the quantity (*yᵢ — oᵢ)***x***ᵢ* = **x***ᵢ* to the weight
    vector (multiplied by the learning rate). This increases the weights of the inputs
    with positive values (where *xᵢⱼ* > 0), while decreasing the weights of the inputs
    with negative values (where *xᵢⱼ* < 0). Consequently, the overall net input of
    the perceptron increases.
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为此，我们将量(*yᵢ — oᵢ)***x***ᵢ* = **x***ᵢ*添加到权重向量中（乘以学习率）。这会增加正值输入的权重（其中*xᵢⱼ* > 0），同时减少负值输入的权重（其中*xᵢⱼ*
    < 0）。因此，感知器的整体净输入会增加。
- en: The true label is *yᵢ* = 0, but the perceptron’s prediction is *oᵢ* = 1, i.e.,
    **w***ᵗ***x***ᵢ + b >* 0\. Analogously to the previous case, here we would like
    to **decrease** the perceptron’s net input, so eventually it becomes negative.This
    is achieved by adding the quantity (*yᵢ — oᵢ)***x***ᵢ* = -**x***ᵢ* to the weight
    vector, since this decreases the weights of the inputs with positive values (where
    *xᵢⱼ* > 0) while increasing the weights of the inputs with negative values (where
    *xᵢⱼ* < 0). Consequently, the overall net input of the perceptron decreases.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 真实标签是*yᵢ* = 0，但感知器的预测是*oᵢ* = 1，即**w***ᵗ***x***ᵢ + b >* 0。类似于前面的情况，这里我们希望**减少**感知器的净输入，使其最终变为负值。这是通过将量(*yᵢ
    — oᵢ)***x***ᵢ* = -**x***ᵢ*添加到权重向量中实现的，因为这会减少正值输入的权重（其中*xᵢⱼ* > 0），而增加负值输入的权重（其中*xᵢⱼ*
    < 0）。因此，感知器的整体净输入会减少。
- en: This learning rule is applied to all the training samples sequentially (in an
    arbitrary order). It typically requires more than one iteration over the entire
    training set (called an **epoch**) to find the correct weight vector (i.e., the
    vector of a hyperplane that separates between the positive and the negative examples).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 该学习规则会依次应用于所有训练样本（顺序可以是任意的）。通常需要对整个训练集进行多次迭代（称为**一个周期**），才能找到正确的权重向量（即分隔正负样本的超平面的向量）。
- en: According to the **perceptron convergence theorem**, if the data is linearly
    separable, applying the perceptron learning rule repeatedly will eventually converge
    to the weights of the separating hyperplane (in a finite number of steps). The
    interested reader can find a formal proof of this theorem in this [paper](http://www.cs.columbia.edu/~mcollins/courses/6998-2012/notes/perc.converge.pdf).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 根据**感知器收敛定理**，如果数据是线性可分的，则重复应用感知器学习规则最终将收敛于分隔超平面的权重（在有限的步骤中）。有兴趣的读者可以在这篇[论文](http://www.cs.columbia.edu/~mcollins/courses/6998-2012/notes/perc.converge.pdf)中找到该定理的正式证明。
- en: The Perceptron Learning Algorithm
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感知器学习算法
- en: In practice, it may take long time for the perceptron learning process to converge
    (i.e., reach zero errors on the training set). Furthermore, the data itself may
    not be linearly separable, in which case the algorithm may never terminate. Therefore,
    we need to limit the number of training epochs by some predefined parameter. If
    the perceptron achieves zero errors on the training set before this number is
    reached, we can stop its training earlier.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，感知器学习过程可能需要较长时间才能收敛（即，在训练集上达到零错误）。此外，数据本身可能不是线性可分的，在这种情况下，算法可能永远不会终止。因此，我们需要通过一些预定义的参数来限制训练周期的数量。如果感知器在达到该数量之前在训练集上达到了零错误，我们可以提前停止训练。
- en: 'The perceptron learning algorithm is summarized in the following pseudocode:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器学习算法的总结如下伪代码：
- en: '![](../Images/c4d5d412b765f71cdd637fa62ef5106e.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4d5d412b765f71cdd637fa62ef5106e.png)'
- en: Note that the weights are typically initialized to small random values in order
    to break the symmetry (if all the weights were equal, then the output of the perceptron
    would be constant for every input), while the bias is initialized to zero.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，权重通常初始化为小的随机值以打破对称性（如果所有权重相等，则感知器的输出对于每个输入都是恒定的），而偏置则初始化为零。
- en: 'Example: Learning the Majority Function'
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例：学习多数函数
- en: For example, let’s see how the perceptron learning algorithm can be used to
    learn the **majority function** of three binary inputs. The majority function
    is a function that evaluates to true (1) when half or more of its inputs are true
    and to false (0) otherwise.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们来看看感知器学习算法如何用于学习三个二进制输入的**多数函数**。多数函数是一个在其输入中有一半或更多为真时返回真（1），否则返回假（0）的函数。
- en: 'The training set of the perceptron includes all the 8 possible binary inputs:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器的训练集包括所有8种可能的二进制输入：
- en: '![](../Images/54eb3d76c88eca2dde0d65c5107accf4.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/54eb3d76c88eca2dde0d65c5107accf4.png)'
- en: The training set for the majority function
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 多数函数的训练集
- en: In this example we will assume that the initial weights and bias are 0, and
    the learning rate is *α* = 0.5.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将假设初始权重和偏置为0，学习率为 *α* = 0.5。
- en: 'Let’s track the weight updates during the first epoch of training. The first
    sample presented to the perceptron is **x** = (0, 0, 0)*ᵗ*. The net input of the
    perceptron in this case is: *z* = **w***ᵗ***x** + *b* = 0 × 0 + 0 × 0 + 0 × 0
    + 0 = 0\. Therefore, its output is *o* = 1 (remember that the step function outputs
    1 whenever its input is ≥ 0). However, the target label in this case is *y* =
    0, so the error made by the perceptron is *y* - *o* = -1.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们跟踪第一次训练周期中的权重更新。呈现给感知器的第一个样本是**x** = (0, 0, 0)*ᵗ*。在这种情况下，感知器的净输入为：*z* = **w***ᵗ***x**
    + *b* = 0 × 0 + 0 × 0 + 0 × 0 + 0 = 0。因此，它的输出是 *o* = 1（记住，当输入≥0时，阶跃函数的输出为1）。然而，在这种情况下，目标标签是
    *y* = 0，所以感知器的错误为 *y* - *o* = -1。
- en: According to the perceptron learning rule, we update each weight *wᵢ* by adding
    to it *α*(*y - o*)*xᵢ =* -0.5*xᵢ*. Since all the inputs are 0 in this case, except
    for the bias neuron (*x*₀ = 1), we only update the bias to be -0.5 instead of
    0.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 根据感知器学习规则，我们通过将 *α*(*y - o*)*xᵢ* = -0.5*xᵢ* 加到每个权重 *wᵢ* 上来更新权重。由于在这种情况下所有输入都是0，除了偏置神经元（*x*₀
    = 1），我们只更新偏置为-0.5，而不是0。
- en: 'We repeat the same process for all the other 7 training samples. The following
    table shows the weight updates after every sample:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对其他7个训练样本重复相同的过程。下表显示了每个样本之后的权重更新：
- en: '![](../Images/2a58fd2f787931d2fc1d07daa3ea5a3f.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2a58fd2f787931d2fc1d07daa3ea5a3f.png)'
- en: The first epoch of training
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 第一轮训练
- en: During the first epoch the perceptron has made 4 errors. The weight vector after
    the first epoch is **w** = (0, 0.5, 1)*ᵗ* and the bias is 0.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一轮中，感知器发生了4次错误。第一轮后的权重向量为**w** = (0, 0.5, 1)*ᵗ*，偏置为0。
- en: 'In the second training epoch, we get the following weight updates:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二轮训练中，我们得到以下权重更新：
- en: '![](../Images/d71fd5516712bc06974d56d23775ac08.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d71fd5516712bc06974d56d23775ac08.png)'
- en: Second epoch of training
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 第二轮训练
- en: This time the perceptron has made only three errors. The weight vector after
    the second epoch is **w** = (0.5, 0.5, 1)*ᵗ* and the bias is -0.5.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这次感知器只发生了三次错误。第二轮后的权重向量为**w** = (0.5, 0.5, 1)*ᵗ*，偏置为-0.5。
- en: 'The weight updates in the third epoch are:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 第三轮的权重更新如下：
- en: '![](../Images/c065e9a729618c93fa8e78253d6bfd33.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c065e9a729618c93fa8e78253d6bfd33.png)'
- en: Third epoch of training
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 第三轮训练
- en: 'After the update of the second example in this epoch, the perceptron has converged
    to the weight vector that solves this classification problem:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一轮中，第二个示例更新后，感知器已收敛到解决此分类问题的权重向量：
- en: '**w** = (0.5, 0.5, 0.5)*ᵗ* and *b* = -1\. Since all the weights are equal,
    the perceptron fires only when at least two of the inputs are 1, in which case
    their weighted sum is at least 1, i.e., greater or equal than the absolute value
    of the bias, hence the net input of the perceptron is non-negative.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**w** = (0.5, 0.5, 0.5)*ᵗ* 和 *b* = -1。由于所有权重相等，感知器仅在至少两个输入为1时激发，此时它们的加权和至少为1，即大于或等于偏置的绝对值，因此感知器的净输入是非负的。'
- en: Perceptron Implementation in Python
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python中的感知器实现
- en: Let’s now implement the perceptron learning algorithm in Python.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来实现Python中的感知器学习算法。
- en: We will implement it as a custom Scikit-Learn estimator by extending the [sklearn.base.BaseEstimator](https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html)
    class. This will allow us to use it as any other estimator in Scikit-Learn (e.g.,
    adding it to a [pipeline](https://medium.com/@roiyeho/pipelines-in-scikit-learn-46c61c5c60b2)).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过扩展[sklearn.base.BaseEstimator](https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html)类来实现它作为自定义Scikit-Learn估计器。这将允许我们像使用其他Scikit-Learn估计器一样使用它（例如，将其添加到一个[pipeline](https://medium.com/@roiyeho/pipelines-in-scikit-learn-46c61c5c60b2)中）。
- en: A custom estimator needs to implement the **fit()** and **predict()** methods,
    and set all its hyperparameters in the **__init__()** method.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义估计器需要实现**fit()**和**predict()**方法，并在**__init__()**方法中设置所有超参数。
- en: I will first show the complete code of this class, and then walk through it
    step-by-step.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我会首先展示这个类的完整代码，然后逐步讲解。
- en: '[PRE0]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The constructor of the class initializes the two hyperparameters of the model:
    the learning rate (*alpha*) and the number of training epochs (*n_epochs*).'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 类的构造函数初始化了模型的两个超参数：学习率（*alpha*）和训练轮数（*n_epochs*）。
- en: '[PRE1]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The **fit()** method runs the learning algorithm on a given data set *X* with
    labels *y*. We first find out how many samples and features we have in the data
    set by interrogating the shape of *X*:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**fit()**方法在给定数据集*X*（带标签*y*）上运行学习算法。我们首先通过查询*X*的形状来找出数据集中有多少样本和特征：'
- en: '[PRE2]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '*n* is the number of training samples and *m* is the number of features.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*n* 是训练样本的数量，*m* 是特征的数量。'
- en: 'Next, we initialize the weight vector using the standard normal distribution
    (with mean 0 and standard deviation of 1), and the bias to 0:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用标准正态分布（均值为0，标准差为1）初始化权重向量，将偏置设为0：
- en: '[PRE3]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We now run the training loop for *n_epochs* iterations. In each iteration,
    we go over all the training samples, and for each sample we check if the perceptron
    classifies it correctly by calling the **predict()** method and comparing its
    output to the true label:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在运行训练循环进行*n_epochs*次迭代。在每次迭代中，我们遍历所有训练样本，对于每个样本，我们通过调用**predict()**方法并将其输出与真实标签进行比较，检查感知器是否正确分类：
- en: '[PRE4]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'If the perceptron has misclassified the sample, we apply the weight update
    rule to both the weight vector and the bias, and then increment the number of
    misclassification errors by 1:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果感知器错误地分类了样本，我们将权重更新规则应用于权重向量和偏置，然后将误分类错误的数量增加1：
- en: '[PRE5]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'When the epoch terminates, we report the perceptron’s current accuracy on the
    training set, and if the number of errors was 0, we terminate the training loop:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 当轮次终止时，我们报告感知器在训练集上的当前准确性，如果错误数量为0，我们终止训练循环：
- en: '[PRE6]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The **predict()** method is quite straightforward. We first compute the net
    input of the perceptron as the dot product between the input vector and the weights
    plus the bias:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**predict()** 方法非常直接。我们首先计算感知机的净输入，即输入向量与权重的点积加上偏置：'
- en: '[PRE7]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Finally, we use NumPy’s [heaviside()](https://numpy.org/doc/stable/reference/generated/numpy.heaviside.html)
    function to apply the step function to the net input and return the output:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用 NumPy 的 [heaviside()](https://numpy.org/doc/stable/reference/generated/numpy.heaviside.html)
    函数将步进函数应用于净输入并返回输出：
- en: '[PRE8]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The second parameter of np.heaviside() specifies what should be the value of
    the function for *z* = 0.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: np.heaviside() 的第二个参数指定了函数在 *z* = 0 时的值。
- en: Let’s now test our implementation on a data set generated by the [make_blobs()](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html)
    function from Scikit-Learn.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在由 Scikit-Learn 的 [make_blobs()](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html)
    函数生成的数据集上测试我们的实现。
- en: 'We first generate a data set with 100 random points divided into two groups:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先生成一个包含 100 个随机点的数据集，并将其分为两组：
- en: '[PRE9]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We set *cluster_std* to 0.5 (instead of the default 1) in order to make sure
    that the data is linearly separable.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 *cluster_std* 设置为 0.5（而不是默认的 1），以确保数据是线性可分的。
- en: 'Let’s plot the data set:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制数据集：
- en: '[PRE10]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/2a7752dd9f652ee1d0a99e8df9625416.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2a7752dd9f652ee1d0a99e8df9625416.png)'
- en: The blobs data set
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: blobs 数据集
- en: 'We now instantiate the Perceptron class and fit it to the data set:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们实例化 Perceptron 类并将其拟合到数据集上：
- en: '[PRE11]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output during training is:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程中的输出是：
- en: '[PRE12]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The perceptron has converged after three epochs of training.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机在经过三次训练周期后已经收敛。
- en: 'We can plot the decision boundary found by the perceptron and the two class
    areas using the following function:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下函数绘制感知机找到的决策边界和两个类别区域：
- en: '[PRE13]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](../Images/8b3019ff338ea46ad126f23e3903a074.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8b3019ff338ea46ad126f23e3903a074.png)'
- en: The separating hyperplane found by the perceptron
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机找到的分离超平面
- en: Scikit-Learn provides its own [Perceptron](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html)
    class that implements a similar algorithm, but provides more options such as regularization
    and early stopping.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn 提供了自己的 [Perceptron](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html)
    类，实现了类似的算法，但提供了更多选项，如正则化和提前停止。
- en: Limitations of the Perceptron Model
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感知机模型的局限性
- en: 'Although the perceptron model has shown some initial success, it was quickly
    realized that perceptrons cannot learn some simple functions such as the XOR function:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管感知机模型显示出一些初步的成功，但很快就意识到感知机无法学习一些简单的函数，如 XOR 函数：
- en: '![](../Images/e7015ee88d9d218589d2f6d9fa3d1279.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e7015ee88d9d218589d2f6d9fa3d1279.png)'
- en: The XOR problem cannot be solved by a perceptron
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: XOR 问题无法通过感知机解决
- en: The XOR problem is not linearly separable, therefore linear models such as perceptrons
    cannot solve it.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: XOR 问题不是线性可分的，因此线性模型如感知机无法解决它。
- en: This revelation has caused the field of neural networks to stagnate for many
    years (a period known as “the AI winter”), until it was realized that stacking
    multiple perceptrons in layers can solve more complex and non-linear problems
    such as the XOR problem.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这一发现使得神经网络领域停滞了很多年（这一时期被称为“AI 冬天”），直到意识到通过将多个感知机堆叠在层中可以解决更复杂和非线性的问题，例如 XOR 问题。
- en: Multi-layer perceptrons (MLPs) are covered in [this article](https://medium.com/p/8d76972afa2b).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 多层感知机（MLPs）在 [这篇文章](https://medium.com/p/8d76972afa2b) 中有详细介绍。
- en: Final Notes
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最后的备注
- en: All images unless otherwise noted are by the author.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，所有图片均为作者提供。
- en: 'You can find the code samples of this article on my github: [https://github.com/roiyeho/medium/tree/main/perceptrons](https://github.com/roiyeho/medium/tree/main/perceptrons)'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在我的 GitHub 上找到这篇文章的代码示例：[https://github.com/roiyeho/medium/tree/main/perceptrons](https://github.com/roiyeho/medium/tree/main/perceptrons)
- en: Thanks for reading!
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
