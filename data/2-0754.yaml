- en: Dockerizing Apache Zeppelin and Apache Spark for Easy Deployment
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将 Apache Zeppelin 和 Apache Spark 容器化以便于部署
- en: 原文：[https://towardsdatascience.com/dockerizing-apache-zeppelin-and-apache-spark-for-easy-deployment-1814d0c1c245](https://towardsdatascience.com/dockerizing-apache-zeppelin-and-apache-spark-for-easy-deployment-1814d0c1c245)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/dockerizing-apache-zeppelin-and-apache-spark-for-easy-deployment-1814d0c1c245](https://towardsdatascience.com/dockerizing-apache-zeppelin-and-apache-spark-for-easy-deployment-1814d0c1c245)
- en: Learn How To Build a Portable and Scalable Data Analysis Environment with Docker-Compose
    And Volumes.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习如何使用 Docker-Compose 和卷构建一个可移植和可扩展的数据分析环境。
- en: '[](https://anbento4.medium.com/?source=post_page-----1814d0c1c245--------------------------------)[![Antonello
    Benedetto](../Images/bf802bb46dce03dd3bd4e17c1dffe5b7.png)](https://anbento4.medium.com/?source=post_page-----1814d0c1c245--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1814d0c1c245--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1814d0c1c245--------------------------------)
    [Antonello Benedetto](https://anbento4.medium.com/?source=post_page-----1814d0c1c245--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://anbento4.medium.com/?source=post_page-----1814d0c1c245--------------------------------)[![Antonello
    Benedetto](../Images/bf802bb46dce03dd3bd4e17c1dffe5b7.png)](https://anbento4.medium.com/?source=post_page-----1814d0c1c245--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1814d0c1c245--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1814d0c1c245--------------------------------)
    [Antonello Benedetto](https://anbento4.medium.com/?source=post_page-----1814d0c1c245--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1814d0c1c245--------------------------------)
    ·9 min read·Jan 24, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1814d0c1c245--------------------------------)
    ·阅读时长 9 分钟·2023年1月24日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/dcd662fabf5ce0f263fe35015b17886d.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dcd662fabf5ce0f263fe35015b17886d.png)'
- en: Photo by [Tom Fisk](https://www.pexels.com/@tomfisk/) On [Pexels](https://www.pexels.com/photo/birds-eye-view-photo-of-freight-containers-2226458/)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Tom Fisk](https://www.pexels.com/@tomfisk/) 提供，刊登在 [Pexels](https://www.pexels.com/photo/birds-eye-view-photo-of-freight-containers-2226458/)
    上
- en: On-Demand Courses | Recommended
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 按需课程 | 推荐
- en: '*A few of my readers have contacted me asking for on-demand courses to learn
    more about* ***Apache Spark with Python****. These are 3 great resources I would
    recommend:*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*我的一些读者联系我，询问是否有按需课程可以学习更多关于* ***Apache Spark 与 Python*** 的内容。这是我推荐的三个很棒的资源：*'
- en: '[**Data Streaming With Apache Kafka & Apache Spark Nano-Degree (UDACITY)**](https://imp.i115008.net/zaX10r)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Apache Kafka 和 Apache Spark 数据流处理纳米学位 (UDACITY)**](https://imp.i115008.net/zaX10r)'
- en: '[**Data Engineering Nano-Degree (UDACITY)**](https://imp.i115008.net/zaX10r)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**数据工程纳米学位 (UDACITY)**](https://imp.i115008.net/zaX10r)'
- en: '[**Spark And Python For Big Data With PySpark (UDEMY)**](https://click.linksynergy.com/deeplink?id=533LxfDBSaM&mid=39197&murl=https%3A%2F%2Fwww.udemy.com%2Fcourse%2Fspark-and-python-for-big-data-with-pyspark%2F)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**PySpark 大数据与 Python (UDEMY)**](https://click.linksynergy.com/deeplink?id=533LxfDBSaM&mid=39197&murl=https%3A%2F%2Fwww.udemy.com%2Fcourse%2Fspark-and-python-for-big-data-with-pyspark%2F)'
- en: '*Not a Medium member yet? Consider signing up with my* [*referral link*](https://anbento4.medium.com/membership)
    *to gain access to everything Medium has to offer for as little as $5 a month!*'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*还不是 Medium 会员？考虑使用我的* [*推荐链接*](https://anbento4.medium.com/membership) *注册，您只需每月支付
    5 美元即可访问 Medium 提供的所有内容！*'
- en: Introduction
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Docker has revolutionised the way we deploy and manage our applications and
    data analysis tools. It allows for effortless scaling, and the ability to easily
    customise the services to suit specific needs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 革新了我们部署和管理应用程序及数据分析工具的方式。它允许轻松扩展，并且可以根据特定需求轻松定制服务。
- en: In this tutorial, I will show you how to quickly deploy Apache Zeppelin and
    Apache Spark using a `docker-compose.yaml` file and take advantage of volumes
    to manage data dependencies among services.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我将向你展示如何使用`docker-compose.yaml`文件快速部署 Apache Zeppelin 和 Apache Spark，并利用卷来管理服务之间的数据依赖。
- en: '**The advantage of the solution presented below, is that does not require any
    additional instructions to be provided as part of a** `**Dockerfile**`**.**'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**下面展示的解决方案的优势在于，它不需要将任何额外的指令作为** `**Dockerfile**`**的一部分。**'
- en: Even people with limited Docker experience, will find this template as a valuable
    starting point to deploy *easy-to-maintain* containers.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是经验有限的 Docker 用户，也会发现这个模板是部署*易于维护*容器的宝贵起点。
- en: With this method, you will be able to create a portable and scalable data analysis
    environment that requires less maintenance, and can be customised to use your
    preferred Spark image.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法，你将能够创建一个便携且可扩展的数据分析环境，减少维护需求，并且可以自定义使用你首选的 Spark 镜像。
- en: This makes it an excellent choice particularly for data engineers, data scientists
    and machine learning engineers who need to process large datasets, perform exploratory
    analysis and train their models.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得它特别适合数据工程师、数据科学家和机器学习工程师，他们需要处理大数据集，进行探索性分析和训练模型。
- en: '****Please Note:** In the sections that follow, I assume you have Docker installed
    on your machine (*otherwise you might find* [*the official docs*](https://docs.docker.com/engine/install/)
    *useful*) and are familiar with Docker basic commands to interact with containers
    (*otherwise refer to* [*this article*](/12-essential-docker-commands-you-should-know-c2d5a7751bb5)).**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '****注意：** 在接下来的部分中，我假设你已经在机器上安装了 Docker（*否则你可能会发现* [*官方文档*](https://docs.docker.com/engine/install/)
    *很有用*），并且熟悉与容器交互的 Docker 基本命令（*否则请参考* [*这篇文章*](/12-essential-docker-commands-you-should-know-c2d5a7751bb5)）。**'
- en: 1\. Docker-Compose File
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. Docker-Compose 文件
- en: 'The following code (*also available on* [*GitHub*](https://github.com/anbento0490/docker/blob/master/zeppelin_spark/docker-compose.yaml)),
    is an example of `docker-compose.yaml` file that can be used to run **Apache Zeppelin**
    on top of **Apache Spark** standalone:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码（*也可以在* [*GitHub*](https://github.com/anbento0490/docker/blob/master/zeppelin_spark/docker-compose.yaml)
    *找到），是一个 `docker-compose.yaml` 文件的示例，可以用来在 **Apache Spark** 独立模式下运行 **Apache Zeppelin**：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'By running the command `docker-compose up -d`, this file will start three services:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行命令 `docker-compose up -d`，这个文件将启动三个服务：
- en: A `SPARK-MASTER` ( `version=3.1.1`) responsible for managing the Spark cluster.
    This service will be running at `port 7077` and have a UI available at [http://localhost:8080/](http://localhost:8080/).
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 `SPARK-MASTER`（`version=3.1.1`），负责管理 Spark 集群。这个服务将在 `port 7077` 运行，用户界面可在
    [http://localhost:8080/](http://localhost:8080/) 访问。
- en: A `SPARK-WORKER-1` that is indeed a worker node assigned with 2 cores and 4GB
    of memory, that connects to the `SPARK-MASTER` via port `7077` and has a UI reachable
    at [http://localhost:8081/](http://localhost:8080/).
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 `SPARK-WORKER-1`，实际上是一个分配了 2 个核心和 4GB 内存的工作节点，通过端口 `7077` 连接到 `SPARK-MASTER`，其用户界面可在
    [http://localhost:8081/](http://localhost:8080/) 访问。
- en: The image used for both `SPARK-MASTER` and `SPARK-WORKER` is managed by [bde2020](https://github.com/big-data-europe/docker-spark)
    and downloadable from [DockerHub](https://hub.docker.com/r/bde2020/spark-master/tags).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 用于 `SPARK-MASTER` 和 `SPARK-WORKER` 的镜像由 [bde2020](https://github.com/big-data-europe/docker-spark)
    管理，并可从 [DockerHub](https://hub.docker.com/r/bde2020/spark-master/tags) 下载。
- en: An`ZEPPELIN` web-based notebook ( `version=10.1`) that is set to run code against
    the standalone `SPARK-MASTER`, through `SPARK-WORKER-1` and has a UI available
    at [http://localhost:8082/](http://localhost:8080/).
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个`ZEPPELIN` 基于 web 的笔记本（`version=10.1`），设置为在独立的 `SPARK-MASTER` 上运行代码，通过 `SPARK-WORKER-1`，其用户界面可在
    [http://localhost:8082/](http://localhost:8080/) 访问。
- en: 'To verify that all services are up and running, execute:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 要验证所有服务是否正常运行，请执行：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This command should return the `container ID`, `container name` and `current
    status`:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令应该返回 `container ID`、`container name` 和 `current status`：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 2\. Access Docker Containers
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 访问 Docker 容器
- en: At this point, assuming that all the services in the `docker-compose.yaml` file
    are running, their user interfaces should be accessible at the host addresses
    specified in the ports mapping configuration.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在此阶段，假设 `docker-compose.yaml` 文件中的所有服务都在运行，它们的用户界面应当可以在端口映射配置中指定的主机地址上访问。
- en: However, before checking how the UI looks like, let’s access the `SPARK-MASTER`
    and verify that it is indeed available at `port 7077` and PySpark commands can
    be executed against its cluster.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，在检查用户界面样式之前，让我们访问 `SPARK-MASTER` 并验证它确实在 `port 7077` 上可用，并且 PySpark 命令可以在其集群上执行。
- en: 'To execute commands in the running`SPARK-MASTER` container, via bash shell,
    you can use:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 要在运行中的`SPARK-MASTER`容器内执行命令，通过 bash shell，你可以使用：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This is an extremely useful command to troubleshoot the containers, check logs,
    and make changes to the container’s file system or running processes.
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这是一个极其有用的命令，用于排查容器问题，检查日志，和对容器的文件系统或运行进程进行更改。
- en: 'At this stage, you should be able to launch a PySpark shell that connects to
    the `spark-master` service available at `port 7077` by running:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，你应该能够启动一个 PySpark shell，连接到 `port 7077` 上的 `spark-master` 服务，通过运行：
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'If the everything works without errors, you should see a Python shell prompt
    appearing and you can try running a few commands (*for example create a simple
    DF* ) against the cluster, as shown below:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切正常，你应该会看到一个 Python shell 提示符，并且可以尝试在集群上运行一些命令（*例如创建一个简单的 DF*），如下所示：
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/f512a3ebf623676a56ab68306f74e3a7.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f512a3ebf623676a56ab68306f74e3a7.png)'
- en: Success! With the step above you verified that the `spark-master` service is
    reachable and able to run Spark applications in standalone mode (to exit from
    the interactive Python prompt, type `exit()` and then `CTRL + D`)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 成功！通过上一步，你验证了 `spark-master` 服务可以访问并能够在独立模式下运行 Spark 应用程序（要退出交互式 Python 提示符，请输入
    `exit()` 然后按 `CTRL + D`）。
- en: 'In fact, if you now navigate to host [http://localhost:8080/](http://localhost:8080/),
    you will notice that a `PySparkShell` application has appeared under the `Running
    Applications` section:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，如果你现在访问主机 [http://localhost:8080/](http://localhost:8080/)，你会注意到在 `Running
    Applications` 部分下出现了一个 `PySparkShell` 应用程序：
- en: '![](../Images/f785bea04ef0e015f7733eb3b34eff75.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f785bea04ef0e015f7733eb3b34eff75.png)'
- en: 'The details of the job executed a moment ago to create the DF with PySpark,
    are also summarised by the `spark-worker` at host [http://localhost:8081/](http://localhost:8080/):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 刚刚执行的创建 DF 的 PySpark 任务的详细信息，也由主机上的 `spark-worker` 汇总，主机地址为 [http://localhost:8081/](http://localhost:8080/)：
- en: '![](../Images/67a5ce824f27a639070f9aa363e50d14.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67a5ce824f27a639070f9aa363e50d14.png)'
- en: Hurrah! Spark works!
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 好极了！Spark 正在工作！
- en: However, the next step is for you to make sure that also the`ZEPPELIN` service
    can interact with the`SPARK-MASTER` and run commands against the same standalone
    cluster, built on top of `version=3.1.1`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，下一步是确保 `ZEPPELIN` 服务也能与 `SPARK-MASTER` 进行交互，并在基于 `version=3.1.1` 的相同独立集群上运行命令。
- en: By navigating to host [http://localhost:8082/](http://localhost:8080/) , you
    can verify that `ZEPPELIN` is indeed running and new notebooks can be created
    ( *for instance, I created one named* `PySpark Project 1` ) and Spark commands
    be run.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 通过访问主机 [http://localhost:8082/](http://localhost:8080/)，你可以验证 `ZEPPELIN` 是否确实在运行，并且可以创建新的笔记本（*例如，我创建了一个名为*
    `PySpark Project 1` 的笔记本）并运行 Spark 命令。
- en: '![](../Images/c3bce0c64f1d2fc68b98c7da6bc0b269.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c3bce0c64f1d2fc68b98c7da6bc0b269.png)'
- en: By default, code will be executed *in local* instead of against the standalone
    cluster, using Zeppelin’s standard interpreter.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，代码将 *在本地* 执行，而不是在独立集群上，使用 Zeppelin 的标准解释器。
- en: To change this behaviour, you need to get familiar with the concept of Docker
    *volumes* and *environment variables*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 要改变这种行为，你需要熟悉 Docker 的 *卷* 和 *环境变量* 的概念。
- en: '****Please note:** like with `SPARK-MASTER`, also `ZEPPELIN` container can
    be accessed by running `docker exec -it zeppelin /bin/bash`. The main path you
    might want to explore for `version=10.1` is `/opt/zeppelin/`.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '****请注意：** 与 `SPARK-MASTER` 一样，`ZEPPELIN` 容器也可以通过运行 `docker exec -it zeppelin
    /bin/bash` 进行访问。你可能想要探索的主要路径是 `/opt/zeppelin/`，对于 `version=10.1`。'
- en: 3\. Docker Volumes
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. Docker 卷
- en: Docker volumes are a way to store data outside of a container’s filesystem.
    This allows for data to be persisted even if the container is deleted or recreated.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 卷是一种将数据存储在容器文件系统之外的方式。这允许数据在容器被删除或重建时依然得以保留。
- en: 'Going back to the `docker-compose.yaml` file, you can verify that the following
    volumes got created when containers were run:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 返回到 `docker-compose.yaml` 文件，你可以验证在容器运行时是否创建了以下卷：
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: First of all, I created a so called *“named volume”* `spark_volume` under the
    `volumes` section and mapped it to the `/spark` folder of the `SPARK-MASTER` container’s
    filesystem.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我创建了一个所谓的 *“命名卷”* `spark_volume` 在 `volumes` 部分，并将其映射到 `SPARK-MASTER` 容器文件系统中的
    `/spark` 文件夹。
- en: The same `spark_volume` has been then mapped to the `/opt/zeppelin/spark` directory
    in the `ZEPPELIN` filesystem. This effectively creates a new folder `/spark` with
    all the content available in the original `SPARK-MASTER` filesystem. This folder,
    among the other files, includes the `SPARK-SUBMIT` executables, that are necessary
    to run a Spark application via Zeppelin.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相同的 `spark_volume` 已经映射到 `ZEPPELIN` 文件系统中的 `/opt/zeppelin/spark` 目录。这实际上创建了一个新的
    `/spark` 文件夹，包含了原始 `SPARK-MASTER` 文件系统中的所有内容。这个文件夹中，包括其他文件，还包含运行 Spark 应用程序所需的
    `SPARK-SUBMIT` 可执行文件。
- en: The `/opt/zeppelin/notebook` directory has been mapped to the local `./zeppelin/notebook`
    folder in your local environment. This allows you to create and save notebooks
    in local, to avoid losing them every time the container are removed with `docker-compose
    down` or restarted with `docker restart <service-name>`.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '` /opt/zeppelin/notebook`目录已映射到本地的`./zeppelin/notebook`文件夹。这允许你在本地创建和保存笔记本，以避免在使用`docker-compose
    down`删除容器或使用`docker restart <service-name>`重新启动容器时丢失它们。'
- en: Finally, the `/opt/zeppelin/conf` directory has been mapped to the local `./zeppelin/conf`.
    This is necessary, as in the next step, you will have to set some environment
    variables at this location.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，`/opt/zeppelin/conf`目录已经映射到本地的`./zeppelin/conf`。这是必要的，因为在下一步中，你需要在这个位置设置一些环境变量。
- en: '****Please note**: these are the four fundamental volumes, that are required
    to avoid both application / interpreter errors in Zeppelin notebooks. However
    you can create more if you wish.******'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '****请注意**：这四个基本卷是必需的，以避免Zeppelin笔记本中的应用程序/解释器错误。不过你可以根据需要创建更多的卷。******'
- en: Now, let’s see what environment variable are required and how to set them.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入了解一下所需的环境变量及其设置方法。
- en: 4\. Docker Environment Variables
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. Docker环境变量
- en: 'Environment variables are used to pass configuration options to a container.
    Looking at the `docker-compose.yaml` file, the most relevant environment variables
    to describe are:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 环境变量用于将配置选项传递给容器。查看`docker-compose.yaml`文件，最相关的环境变量如下：
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: First of all, for both `SPARK-WORKER` and `ZEPPELIN` services, the `SPARK_MASTER`
    variable has been set to `spark://spark-master:7077`, that is the indeed the URL
    of the `SPARK-MASTER` service. This allows both services to connect to the standalone
    Spark cluster.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，对于`SPARK-WORKER`和`ZEPPELIN`服务，`SPARK_MASTER`变量已设置为`spark://spark-master:7077`，这确实是`SPARK-MASTER`服务的URL。这使得这两个服务可以连接到独立的Spark集群。
- en: Also, in the `ZEPPELIN` service, the `SPARK_HOME` variable has been set to `/opt/zeppelin/spark`
    which, as you remember from the previous section, is the path to the folder created
    via `spark_volume` (*that contains the* `SPARK-SUBMIT` *executable*). ***Setting
    this variable correctly is key if you wish for a* `zeppelin` *application to run
    on top of Spark standalone.***
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同样，在`ZEPPELIN`服务中，`SPARK_HOME`变量已设置为`/opt/zeppelin/spark`，正如你在前一部分所记得的，这就是通过`spark_volume`创建的文件夹的路径（*包含*
    `SPARK-SUBMIT` *可执行文件*）。 ***正确设置这个变量是关键，如果你希望* `zeppelin` *应用程序在Spark独立模式下运行的话。***
- en: For the `SPARK-WORKER` service, I set `SPARK_WORKER_CORES=2`, which means that
    the Spark Worker will use 2 cores and `SPARK_WORKER_MEMORY=4g`, which means that
    the Spark Worker will have 4GB of memory available. You can of course tweak these
    parameters depending on your specific requirements.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`SPARK-WORKER`服务，我设置了`SPARK_WORKER_CORES=2`，这意味着Spark Worker将使用2个核心，并且`SPARK_WORKER_MEMORY=4g`，这意味着Spark
    Worker将有4GB的内存可用。你当然可以根据具体需求调整这些参数。
- en: As far as services configuration is concerned, this is pretty much all you need
    to do. In fact, you could already go ahead and use Zeppelin to execute code (*written
    in Scala*), against the Spark cluster.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 至于服务配置，这基本上就是你需要做的全部。事实上，你可以立即使用Zeppelin执行代码（*用Scala编写*），对Spark集群进行操作。
- en: However, in order to avoid getting errors related to the PySpark interpreter
    in Zeppelin, you should also export two additional variables as part of the `zeppelin-env.sh`
    file, that for `version=10.1` has to be created and persisted under the `/opt/zeppelin/conf`
    directory.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，为了避免在Zeppelin中出现与PySpark解释器相关的错误，你还应该在`zeppelin-env.sh`文件中导出两个额外的变量，`version=10.1`的文件必须在`/opt/zeppelin/conf`目录下创建和保留。
- en: 'To set the environment variables, first access the container:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 要设置环境变量，首先访问容器：
- en: '[PRE8]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'then run this bash script:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 然后运行这个bash脚本：
- en: '[PRE9]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Once created and populated with the `PYSPARK_PYTHON` and `PYSPARK_DRIVER_PYTHON`
    variables, the presence of a `zeppelin-env.sh` will ensure that the PySpark interpreter
    in Zeppelin uses `python3`, while executing PySpark code and avoid clashes with
    default versions.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建并填充了`PYSPARK_PYTHON`和`PYSPARK_DRIVER_PYTHON`变量，`zeppelin-env.sh`的存在将确保Zeppelin中的PySpark解释器使用`python3`，在执行PySpark代码时避免与默认版本冲突。
- en: 5\. Execute Code In Zeppelin
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. 在Zeppelin中执行代码
- en: With both volumes and and environment variables in place, you can now test that
    Scala and Python commands are executed as expected in the Zeppelin notebooks.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷和环境变量设置完成后，你现在可以测试Scala和Python命令在Zeppelin笔记本中是否按预期执行。
- en: 'After saving recent changes to the `docker-compose` file, restart the `ZEPPELIN`
    container:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在保存了最近对`docker-compose`文件的更改后，重启`ZEPPELIN`容器：
- en: '[PRE10]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Navigate to host [http://localhost:8082/](http://localhost:8080/) and create
    a new notebook or open an existing one ( *I am going to use* `PySpark Project
    1`). Now, try to execute the following code:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 访问主机 [http://localhost:8082/](http://localhost:8080/) 并创建一个新笔记本或打开一个现有笔记本（*我将使用*
    `PySpark Project 1`）。现在，尝试执行以下代码：
- en: '[PRE11]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'At this point you can try to create a simple DF with PySpark, as I did on the
    the shell at Step 2\. The output should look similar to this:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 到此，你可以尝试使用PySpark创建一个简单的数据框，就像我在第2步的shell中做的那样。输出应类似于此：
- en: '![](../Images/147e28d3b40e2b87978210e7157d4d3e.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/147e28d3b40e2b87978210e7157d4d3e.png)'
- en: 'Also, notice how a `ZEPPELIN` application has been allocated resources on the
    Spark Cluster:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，请注意`ZEPPELIN`应用在Spark集群上是如何分配资源的：
- en: '![](../Images/8bf796da98e77dc8ac79ece4992c7b6d.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8bf796da98e77dc8ac79ece4992c7b6d.png)'
- en: Terrific work!
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 做得很好！
- en: Conclusion
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this tutorial, you have learnt how to deploy recent versions of **Apache
    Zeppelin** and **Apache Spark** via `docker-compose` file, without the need of
    any additional instructions usually provided via `Dockerfile`.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，你已经学习了如何通过`docker-compose`文件部署**Apache Zeppelin**和**Apache Spark**的最新版本，而不需要任何额外的说明，通常这些说明是通过`Dockerfile`提供的。
- en: Instead, you took advantage of **volumes** to manage and persist data dependencies
    and **environment variables** to customise the services. This makes it easy to
    create a portable and scalable data analysis environment that requires little
    or no maintenance.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，你利用了**卷**来管理和持久化数据依赖项，并使用**环境变量**来定制服务。这使得创建一个便于移植和可扩展的数据分析环境变得简单，几乎无需维护。
- en: This solution, allows you to quickly spin up a **standalone Spark Cluster**,
    interacting with one or more Spark worker nodes to process large datasets, and
    use Apache Zeppelin as a web-based notebook to perform data exploration and visualisation.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案允许你快速启动一个**独立的Spark集群**，与一个或多个Spark工作节点互动以处理大型数据集，并使用Apache Zeppelin作为基于网页的笔记本来进行数据探索和可视化。
- en: Additionally, you understood how to leverage environment variables to configure
    the services, and how to set up the **PySpark interpreter** in Zeppelin, avoiding
    errors.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你了解了如何利用环境变量来配置服务，以及如何在Zeppelin中设置**PySpark解释器**，以避免错误。
- en: Overall, using `docker-compose` to deploy Apache Zeppelin and Apache Spark is
    an efficient and convenient way to manage data analysis environments. It allows
    for easy scaling, and the ability to customise the services to suit specific needs.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，使用`docker-compose`来部署Apache Zeppelin和Apache Spark是管理数据分析环境的高效便捷方式。它允许轻松扩展，并能够根据具体需求定制服务。
- en: This makes it an excellent choice for **data engineers**, **data scientists**
    and **machine learning engineers** who need to process large datasets, perform
    exploratory analysis and train their models.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得它成为**数据工程师**、**数据科学家**和**机器学习工程师**的绝佳选择，他们需要处理大型数据集、执行探索性分析并训练他们的模型。
- en: Sources
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源
- en: '[Docker Engine Installation Overview](https://docs.docker.com/engine/install/)'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Docker Engine 安装概述](https://docs.docker.com/engine/install/)'
- en: '[Docker Docs | Volumes](https://docs.docker.com/storage/volumes/)'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Docker 文档 | 卷](https://docs.docker.com/storage/volumes/)'
- en: '[Docker Docs | Environment Variables In Compose](https://docs.docker.com/compose/environment-variables/)'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Docker 文档 | Compose中的环境变量](https://docs.docker.com/compose/environment-variables/)'
- en: '[DockerHub | bde2020 Images](https://hub.docker.com/r/bde2020/spark-master/tags)'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[DockerHub | bde2020 镜像](https://hub.docker.com/r/bde2020/spark-master/tags)'
- en: '[BDE Docker-Spark GitHub Repository](https://github.com/big-data-europe/docker-spark)'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[BDE Docker-Spark GitHub 仓库](https://github.com/big-data-europe/docker-spark)'
- en: '[Spark interpreter For Apache Zeppelin (version 10.1)](https://zeppelin.apache.org/docs/latest/interpreter/spark.html)'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Apache Zeppelin的Spark解释器（版本10.1）](https://zeppelin.apache.org/docs/latest/interpreter/spark.html)'
- en: Using PySpark In Zeppelin With python3
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Zeppelin中使用PySpark与python3
- en: '[12 Essential Docker Commands You Should Know](/12-essential-docker-commands-you-should-know-c2d5a7751bb5)'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[你应该知道的12个必备Docker命令](/12-essential-docker-commands-you-should-know-c2d5a7751bb5)'
