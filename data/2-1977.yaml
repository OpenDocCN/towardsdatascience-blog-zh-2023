- en: 'Text Pattern Extraction: Comparing GPT-3 & Human-in-the-Loop Tool'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本模式提取：比较 GPT-3 和 人工在环工具
- en: 原文：[https://towardsdatascience.com/text-pattern-extraction-comparing-gpt-3-human-in-the-loop-tool-f2380fd13cf1](https://towardsdatascience.com/text-pattern-extraction-comparing-gpt-3-human-in-the-loop-tool-f2380fd13cf1)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/text-pattern-extraction-comparing-gpt-3-human-in-the-loop-tool-f2380fd13cf1](https://towardsdatascience.com/text-pattern-extraction-comparing-gpt-3-human-in-the-loop-tool-f2380fd13cf1)
- en: Preliminary experiments and results from comparing LLMs and human-in-the-loop
    tools for text pattern extraction
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较 LLM 和人工在环工具在文本模式提取中的初步实验和结果
- en: '[](https://maeda-han.medium.com/?source=post_page-----f2380fd13cf1--------------------------------)[![Maeda
    Hanafi](../Images/c1ceef15ccbe82a5b8655593d685db74.png)](https://maeda-han.medium.com/?source=post_page-----f2380fd13cf1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f2380fd13cf1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f2380fd13cf1--------------------------------)
    [Maeda Hanafi](https://maeda-han.medium.com/?source=post_page-----f2380fd13cf1--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://maeda-han.medium.com/?source=post_page-----f2380fd13cf1--------------------------------)[![Maeda
    Hanafi](../Images/c1ceef15ccbe82a5b8655593d685db74.png)](https://maeda-han.medium.com/?source=post_page-----f2380fd13cf1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f2380fd13cf1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f2380fd13cf1--------------------------------)
    [Maeda Hanafi](https://maeda-han.medium.com/?source=post_page-----f2380fd13cf1--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f2380fd13cf1--------------------------------)
    ·10 min read·Jan 26, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [数据科学的前沿](https://towardsdatascience.com/?source=post_page-----f2380fd13cf1--------------------------------)
    ·10分钟阅读·2023年1月26日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/3c44a30478df4c48cb757c512b575935.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3c44a30478df4c48cb757c512b575935.png)'
- en: Photo by [Aaron Burden](https://unsplash.com/@aaronburden?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/nDeo4F3Zq28?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 [Aaron Burden](https://unsplash.com/@aaronburden?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    在 [Unsplash](https://unsplash.com/photos/nDeo4F3Zq28?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: In the past years, AI garnered a lot of interest in several industrial applications.
    End-users such as doctors, analysts, and journalists want to build AI models for
    their specific use cases.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，AI 在多个工业应用中引起了广泛关注。医生、分析师和记者等最终用户希望为其特定的使用案例构建 AI 模型。
- en: 'However, the workflow of building AI models requires technical expertise that
    end-users may not necessarily have:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，构建 AI 模型的工作流程需要技术专长，而最终用户可能不一定具备：
- en: Preparing the data, such as extracting, cleaning, and transforming the training
    data.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备数据，例如提取、清理和转换训练数据。
- en: Training the AI model, which includes finetuning parameters and retraining layers
    of the model.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练 AI 模型，包括微调参数和重新训练模型的层。
- en: '![](../Images/5862ab47b5a9c4c07d14c3f881ecef5f.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5862ab47b5a9c4c07d14c3f881ecef5f.png)'
- en: 'Motivation: enabling end users to build AI models. Image by Author.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 动机：使最终用户能够构建 AI 模型。图片来源于作者。
- en: An end-user is likely to apply existing tools ***out-of-the-box***. There are
    several tools applicable for end-users to build AI models out-of-the-box.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最终用户可能会直接使用现有工具***开箱即用***。有几种工具适用于最终用户以***开箱即用***的方式构建 AI 模型。
- en: Recently, a class of tools called human-in-the-loop (HITL) tools aims to lower
    the barrier of entry for building AI models for end-users. The “human-in-the-loop”
    incorporates human knowledge in the process of model building. It is essentially
    a framework for human-computer collaboration for the task at hand. In this framework,
    there is a continuous feedback between model building and user input.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，一类称为人工在环 (HITL) 工具的工具旨在降低最终用户构建 AI 模型的门槛。“人工在环”在模型构建过程中融入了人类知识。它本质上是一个人机协作的框架。在这个框架中，模型构建和用户输入之间有一个持续的反馈过程。
- en: On the other hand, there are the popularly researched models in AI, namely the
    large generative language models such as GPT-3, OPT, BLOOM, etc. Language models
    are trained on large datasets and have incredible language understanding capabilities.
    The scale of these models is in the hundreds of millions of parameters, and they
    have been proven great *few-shot* performance (meaning they have been given a
    handful of inputs) for several natural language processing tasks, e.g. extraction,
    classification, etc.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，AI 领域有一些广泛研究的模型，即大型生成语言模型，如 GPT-3、OPT、BLOOM 等。语言模型在大规模数据集上进行训练，具有惊人的语言理解能力。这些模型的规模达到数亿参数，并且在多个自然语言处理任务（例如提取、分类等）中表现出了*少量样本*的优异性能（即它们只需少量输入）。
- en: '![](../Images/60f6e8dc6bae212b3e9b4343a1d141a5.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/60f6e8dc6bae212b3e9b4343a1d141a5.png)'
- en: Existing work. Image by Author.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现有工作。图像由作者提供。
- en: We want to understand how these popularly researched large language models perform
    against human-in-the-loop tools, in the context of enabling end-users to build
    AI models. *In our experiments, we mimic how an end-user or a business user, someone
    who does not have much technical training, might use these tools to perform text
    pattern extraction tasks.* In this brief blog post, I want to talk about a couple
    of experiments from our work at IBM where we compare human-in-the-loop systems
    with large language models for pattern extraction tasks. We presented this work
    at DaSH@EMNLP 2022, and you can find [the paper here](https://aclanthology.org/2022.dash-1.7/).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想了解这些广泛研究的大型语言模型在帮助终端用户构建 AI 模型的背景下，与人机协作工具的表现如何。*在我们的实验中，我们模拟了一个终端用户或业务用户（即没有太多技术培训的人）如何使用这些工具来执行文本模式提取任务。*
    在这篇简短的博客文章中，我想谈谈我们在 IBM 的几项实验，我们比较了人机协作系统与大型语言模型在模式提取任务中的表现。我们在 DaSH@EMNLP 2022
    上展示了这项工作，你可以在 [这里找到论文](https://aclanthology.org/2022.dash-1.7/)。
- en: 'Specifically, we looked into the following tools:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们研究了以下工具：
- en: '**Pattern Induction** is a HITL tool for text pattern extraction on IBM Watson
    Discovery'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模式归纳** 是 IBM Watson Discovery 上用于文本模式提取的 HITL 工具。'
- en: '**GPT-3** is a popular large generative language model. GPT-3 is one of the
    largest of the large language models with 175 billion parameters.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPT-3** 是一个流行的大型生成语言模型。GPT-3 是最大的语言模型之一，拥有 1750 亿个参数。'
- en: '![](../Images/ccb50b60ff825ecbc2b2a849f987ae4a.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ccb50b60ff825ecbc2b2a849f987ae4a.png)'
- en: The goal of our comparison work. Image by Author.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们比较工作的目标。图像由作者提供。
- en: Text pattern extraction
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本模式提取
- en: What is text pattern extraction? Below, we have a scenario where we have a collection
    of financial press releases and we need to extract the fiscal time periods. We
    want extractions such as “2014 first-quarter” and “fourth-quarter of 2013”. Notice
    that the fiscal time periods can have the year appear at the beginning of the
    extraction or at the end of the extraction, and the quarter varies with it either
    being “first”, “second”, “third”, or “fourth”.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是文本模式提取？下面，我们有一个场景，其中包含一系列财务新闻稿，我们需要提取财政时间段。我们希望的提取结果包括“2014年第一季度”和“2013年第四季度”。注意，财政时间段的年份可以出现在提取结果的开始或结束，而季度则可以是“第一”、“第二”、“第三”或“第四”。
- en: '![](../Images/996c313fa5129d1351e11a99b28389ba.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/996c313fa5129d1351e11a99b28389ba.png)'
- en: Text pattern extraction. Image by Author.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 文本模式提取。图像由作者提供。
- en: Text Pattern Extraction with Pattern Induction
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**文本模式提取与模式归纳**'
- en: Pattern Induction is available on IBM Watson Discovery. It is a HITL tool for
    text pattern extraction. The end-user extracts text patterns by providing feedback.
    The tool does not require any coding and the user does not need to provide a large
    training dataset.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 模式归纳在 IBM Watson Discovery 上可用。它是一个用于文本模式提取的 HITL 工具。终端用户通过提供反馈来提取文本模式。该工具无需任何编码，用户也不需要提供大规模的训练数据集。
- en: 'Pattern Induction supports two user actions:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 模式归纳支持两种用户操作：
- en: End-users highlight examples of text that they wish to extract.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 终端用户高亮他们希望提取的文本示例。
- en: End-users also provide feedback to the system’s extractions. The user accepts
    or rejects these extractions.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 终端用户还会向系统的提取结果提供反馈。用户可以接受或拒绝这些提取结果。
- en: 'For our experiments, we ran a user simulation by simulating the two user actions:
    (1) highlighting example texts, and (2) providing feedback.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们通过模拟两种用户操作来进行用户模拟：(1) 高亮示例文本，和 (2) 提供反馈。
- en: '![](../Images/10eaa83cfdf4b30015c6f5e5b37ba658.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10eaa83cfdf4b30015c6f5e5b37ba658.png)'
- en: Evaluate the HITL tool by simulating the user actions. Image by Author.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 通过模拟用户操作来评估 HITL 工具。图像作者提供。
- en: 'In the backend, Pattern Induction learns *extraction rules*. Think of them
    as something similar to regular expressions: an expression that describes patterns
    over a sequence of tokens, or words.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在后台，模式诱导学习 *提取规则*。可以将它们视为类似于正则表达式的东西：描述一系列标记或单词模式的表达式。
- en: '![](../Images/6a337ac23048b501216414bdb11b006c.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6a337ac23048b501216414bdb11b006c.png)'
- en: Pattern induction flow & underlying rule model. Image by Author.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 模式诱导流程及其基础规则模型。图像作者提供。
- en: Text Pattern Extraction with GPT-3
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT-3 的文本模式提取
- en: While the end-user would provide text highlights and feedback in Pattern Induction,
    completing text extraction tasks in GPT-3 requires crafting an input prompt, and
    GPT-3 outputs a *completion text*, which we expect to contain the extracted texts.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然最终用户会在模式诱导中提供文本高亮和反馈，但在 GPT-3 中完成文本提取任务需要构造输入提示，而 GPT-3 输出一个 *完成文本*，我们期望其中包含提取的文本。
- en: '![](../Images/8aacfbfe400216c1cfbed505e5a4a651.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8aacfbfe400216c1cfbed505e5a4a651.png)'
- en: The naive, basic way of prompting GPT-3 for text pattern extraction. Image by
    Author.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的 GPT-3 文本模式提取提示。图像作者提供。
- en: 'Suppose we need to extract ISO numbers such as “ISO 18788” or “ISO 223000”
    from a dataset of reports. In the above image, we have an input prompt that is
    crafted as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们需要从报告数据集中提取 ISO 编号，如“ISO 18788”或“ISO 223000”。在上面的图像中，我们有一个按照以下方式制作的输入提示：
- en: The first part of the input prompt is the sentence from the dataset we want
    to extract from. We put this sentence in between square brackets.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入提示的第一部分是我们要提取的来自数据集的句子。我们将此句子放在方括号中。
- en: The second part of the prompt contains the example extractions in list form,
    where each example extraction is placed in between the bar character, “|”.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示的第二部分包含以列表形式列出的示例提取，其中每个示例提取都放在竖线字符“|”之间。
- en: The format of the input prompt is used to show and demonstrate to GPT-3 what
    kinds of text we want to extract from the sentence. In the above example, GPT-3
    completes the text with its output, “ISO 9001” from the sentence. The above input
    prompt is a rather naïve approach and it mimics how an end-user might construct
    a prompt. Constructing a prompt requires some level of engineering, and a whole
    sub-field of research is dedicated to prompt engineering.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 输入提示的格式用于向 GPT-3 显示和演示我们希望从句子中提取哪些类型的文本。在上面的例子中，GPT-3 从句子中完成了文本，输出了“ISO 9001”。上述输入提示是一种相当天真的方法，它模仿了最终用户可能构造提示的方式。构造提示需要一定程度的工程技术，整个研究子领域都致力于提示工程。
- en: Prompts
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示
- en: '![](../Images/0d8740fc0ab58ee3243de934e2a65f83.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0d8740fc0ab58ee3243de934e2a65f83.png)'
- en: Experimented with a handful of different input formats. Image by Author.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试了几种不同的输入格式。图像作者提供。
- en: 'There are many ways of formatting the input prompt. We experimented with a
    couple of formattings:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 输入提示的格式有很多种。我们尝试了几种格式：
- en: '**Basic prompts**: mimics how an end-user who may not have much technical expertise
    may craft a prompt.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基本提示**：模仿可能没有太多技术专长的最终用户如何构造提示。'
- en: '**Structured prompts**: instead of listing the extraction examples, each extraction
    example is paired with a sentence.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结构化提示**：不是列出提取示例，而是将每个提取示例与一个句子配对。'
- en: '![](../Images/998c937acea0e78f8b28dc1818128e91.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/998c937acea0e78f8b28dc1818128e91.png)'
- en: Structured Prompts. Image by Author.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化提示。图像作者提供。
- en: '**Structured prompts with additional and negative examples:** In the last type
    of prompt, we added both additional examples and negative examples. In Pattern
    Induction, users were able to accept and reject extractions. In GPT-3 we added
    those types of extractions as additional examples.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**带有附加和负面示例的结构化提示：** 在最后一种提示中，我们添加了附加示例和负面示例。在模式诱导中，用户可以接受和拒绝提取。在 GPT-3 中，我们将这些类型的提取作为附加示例添加。'
- en: '![](../Images/edff1ed0cd87cfc9e4f8c3399a7f2c1f.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/edff1ed0cd87cfc9e4f8c3399a7f2c1f.png)'
- en: Structured prompts with negative examples. Image by Author.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 带有负面示例的结构化提示。图像作者提供。
- en: To read more about the specifics of the input prompt format, refer to the [paper
    here](https://aclanthology.org/2022.dash-1.7/).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解有关输入提示格式的更多细节，请参阅 [论文](https://aclanthology.org/2022.dash-1.7/)。
- en: Post-processing GPT-3’s outputs
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 后处理 GPT-3 的输出
- en: One of the major challenges with using GPT-3 for text pattern extraction was
    how it sometimes got creative with its outputs. This is a common occurrence with
    GPT-3, given its text generation capabilities. GPT-3’s extracted texts are not
    always a part of the document dataset.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GPT-3进行文本模式提取的主要挑战之一是它有时会对输出进行创意性的处理。这是GPT-3常见的现象，考虑到其文本生成能力。GPT-3提取的文本并不总是文档数据集的一部分。
- en: '![](../Images/7d04884bb49171756c4367f9b03e0fcf.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7d04884bb49171756c4367f9b03e0fcf.png)'
- en: GPT-3 outputs creative texts. Image by Author.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3输出的文本有创意。图片由作者提供。
- en: In one of the use cases, we wanted to extract percentages of criminal incidents
    from crime reports, such as percentages of “crimes against property” or percentages
    of “crimes against persons”. But GPT-3 generates percentages about other topics
    such as race or gender, e.g. “0.6 percent were American Indian or Alaska Native”.
    This text did not even appear in the dataset, which negatively affected the precision
    scores. So we post-processed all of the outputs from GPT-3\. The post-processing
    step included cleaning the output text (removing delimiters) and removing outputs
    that were not part of the document dataset.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个用例中，我们希望从犯罪报告中提取犯罪事件的百分比，如“财产犯罪”或“人身犯罪”的百分比。但GPT-3生成了有关其他主题的百分比，如种族或性别，例如“0.6%为美洲印第安人或阿拉斯加土著”。这些文本甚至没有出现在数据集中，这对精确度评分产生了负面影响。因此，我们对GPT-3的所有输出进行了后处理。后处理步骤包括清理输出文本（去除分隔符）和删除那些不属于文档数据集的输出。
- en: Experimental Setup
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验设置
- en: 'Here are the details for our experimental setup for comparing GPT-3 against
    Pattern Induction:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们用于将GPT-3与模式归纳进行比较的实验设置的详细信息：
- en: '![](../Images/29f68d7572a783f93de94c60d43c1e71.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29f68d7572a783f93de94c60d43c1e71.png)'
- en: Experiment Setup with User Simulations. Image by Author.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 用户模拟实验设置。图片由作者提供。
- en: 'The user simulation was run on both Pattern Induction and GPT-3\. Each tool
    was given 7 use case tasks:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 用户模拟在模式归纳和GPT-3上运行。每个工具都进行了7个用例任务：
- en: '![](../Images/12c948f71ff2a5257d2930ba9c07585e.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12c948f71ff2a5257d2930ba9c07585e.png)'
- en: Use case tasks we gave to GPT-3 and Pattern Induction. Image by Author.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们给GPT-3和模式归纳的用例任务。图片由作者提供。
- en: The user simulation recorded the precision and recall of each run.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 用户模拟记录了每次运行的精确度和召回率。
- en: GPT-3 Experiments
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-3实验
- en: '![](../Images/ee285086ec3134b1c8edd8918652a609.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee285086ec3134b1c8edd8918652a609.png)'
- en: Image by Author.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: During our evaluations for GPT-3, we used the same seed examples that the user
    simulation would use as highlighted texts in Pattern Induction. The seed examples
    were taken from the logs of the Pattern Induction user simulations, and then those
    seed examples were used in the input prompt for GPT-3\. This was to make it comparable
    to the Pattern Induction runs. We also split the documents into partials due to
    a limit on the number of tokens the input prompt can have in GPT-3 (around 4K
    tokens).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在对GPT-3的评估过程中，我们使用了用户模拟将作为高亮文本的相同种子示例，种子示例取自模式归纳用户模拟的日志，然后这些种子示例被用作GPT-3的输入提示。这是为了使其与模式归纳的运行结果可比。由于GPT-3的输入提示有令牌数量限制（约4K个令牌），我们还将文档拆分成部分。
- en: 'Results: precision scores'
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果：精确度评分
- en: 'We calculated the average precision, which was an aggregate over the 100 user
    simulation runs for each of the 7 use cases. The results show that GPT-3 precision
    scores are lower than Pattern Induction’s precision scores (green line with the triangular marks):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算了平均精确度，这些精确度是对每个7个用例中的100次用户模拟运行的综合结果。结果显示GPT-3的精确度评分低于模式归纳的精确度评分（绿色线条带三角标记）：
- en: '![](../Images/0d9f6e0c285419e70b2c242a834dca1b.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0d9f6e0c285419e70b2c242a834dca1b.png)'
- en: GPT-3 precision scores are lower than Pattern Induction’s. Image by Author.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3的精确度评分低于模式归纳。图片由作者提供。
- en: There are several lines on the chart for GPT-3, where each one belongs to some
    different variation of formatting the input prompt. While varying formats of input
    prompt improved the precision scores, GPT-3 did not beat the HITL precision scores
    on average. Overall, **precision-wise, Pattern Induction is on average 38.8% better
    than the best GPT-3 model** (structured prompting with additional examples).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图表中有几条GPT-3的线，每条线属于某种不同的输入提示格式变体。虽然不同的输入提示格式提高了精确度评分，但GPT-3的精确度评分平均值并没有超过HITL的精确度评分。总体而言，**在精确度方面，模式归纳的表现比最佳GPT-3模型（结构化提示和额外示例）平均高出38.8%**。
- en: 'Results: recall scores'
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果：召回率评分
- en: We also calculated the average recall, which is the aggregate over the 100 user
    simulation runs for each of the 7 use case tasks.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还计算了平均召回率，即对每个 7 个用例任务的 100 次用户模拟运行的汇总。
- en: '![](../Images/050b41d6ef7406f67112d4c9d1d4ac17.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/050b41d6ef7406f67112d4c9d1d4ac17.png)'
- en: GPT-3 recall scores are similar and higher. Image by Author.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 的召回分数相似且更高。图片来自作者。
- en: 'Overall, **recall-wise, Pattern Induction is on average 4.0% better than the
    best GPT-3 model.** However, when the recall scores are analyzed per use case
    task, we notice that the GPT-3 runs have either similar or even higher recall
    scores than the Pattern Induction runs:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，**在召回率方面，模式诱导的表现比最好的 GPT-3 模型平均好 4.0%。** 然而，当对每个用例任务分析召回分数时，我们注意到 GPT-3
    的运行在召回分数上要么相似，要么甚至高于模式诱导的运行：
- en: 'Tasks U1, U3, U4: In these tasks the expected extractions don’t abide by some
    syntactic pattern. For instance, Task U3, requires extractions regarding different
    crime types, e.g. “crimes against property”, “crimes against persons”. Or, Task
    U4 requires extractions of measuring cups of both integer and fractional types.
    These tasks extract *concepts* and GPT-3 does pretty well with these tasks. GPT-3
    understands that some words refers to concepts and entities such as countries,
    crime types, or integer and fractional quantities. **GPT-3 understands concepts
    in text very well.**'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务 U1、U3、U4：在这些任务中，预期的提取不遵循某些语法模式。例如，任务 U3 需要提取不同类型的犯罪，例如“财产犯罪”、“人身犯罪”。或者，任务
    U4 需要提取整数和分数类型的量杯。这些任务提取的是*概念*，GPT-3 在这些任务中表现得相当好。GPT-3 理解某些词指代概念和实体，例如国家、犯罪类型或整数和分数量。**GPT-3
    对文本中的概念理解得非常好。**
- en: '![](../Images/6721cbcee53eefab344b4f57758ba403.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6721cbcee53eefab344b4f57758ba403.png)'
- en: GPT-3 understands concepts in text very well. Image by Author.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 对文本中的概念理解得非常好。图片来自作者。
- en: 'Tasks U2, U5, U6, U7: On the other hand, we see that tasks with stricter, syntactic
    patterns are handled better with Pattern Induction. For instance, in Task U2,
    we wanted to extract counts of incidents. The literal “incidents” constantly appear
    at the end of a 6-digit integer. This pattern is for all expected extractions.
    Such tasks are more syntactic and **Pattern Induction on average performs better
    for extraction tasks that have stricter, syntactic patterns.** And this makes
    sense since the underlying Pattern Induction model is rule-based.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务 U2、U5、U6、U7：另一方面，我们看到，具有更严格语法模式的任务在模式诱导中处理得更好。例如，在任务 U2 中，我们希望提取事件的计数。字面上的“事件”通常出现在一个
    6 位整数的末尾。这个模式适用于所有预期的提取。这些任务更具语法性，**模式诱导在处理具有更严格语法模式的提取任务时表现更好。** 这很合理，因为底层的模式诱导模型是基于规则的。
- en: '![](../Images/9896e83ce41c7e719b20d2830a158bc0.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9896e83ce41c7e719b20d2830a158bc0.png)'
- en: Pattern Induction learns stricter patterns better. Image by Author.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 模式诱导更好地学习严格模式。图片来自作者。
- en: Improving prompt format improved recall scores
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 改进提示格式提高了召回分数
- en: One other key thing we observed in only the GPT-3 runs is that improving the
    input prompt format improves the recall score.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 GPT-3 运行中观察到的另一个关键点是，改进输入提示格式会提高召回分数。
- en: '![](../Images/4c50c06640cee4d2c87790b5da34a070.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c50c06640cee4d2c87790b5da34a070.png)'
- en: Structured prompting improved recall scores. Image by Author.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化提示提高了召回率。图片来自作者。
- en: From the above chart, moving from basic prompting scores (orange line) to the
    structured prompting scores (yellow line), the **recall score increased on average
    by 59.8 percent**. The key takeaway for this particular chart is that prompt engineering
    and formatting are important for improving recall scores.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的图表可以看出，从基础提示分数（橙色线）到结构化提示分数（黄色线），**召回分数平均提高了 59.8%**。这张图的关键结论是，提示工程和格式化对于提高召回分数非常重要。
- en: Summary of the Comparative Analysis
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较分析总结
- en: '![](../Images/7a42e52aaa29e88daeed149dfac91807.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a42e52aaa29e88daeed149dfac91807.png)'
- en: Image by Author.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自作者。
- en: 'The HITL method for text pattern extraction results in higher precision and
    is great for end-users who do not have much technical background. Moreover, there
    are two aspects that are not found in GPT-3 but are found in HITL:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: HITL 方法用于文本模式提取的结果具有更高的精度，适合那些没有太多技术背景的最终用户。此外，有两个方面在 GPT-3 中找不到，但在 HITL 中存在：
- en: HITL method is able to elicit targeted user feedback and
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: HITL 方法能够引出有针对性的用户反馈，并
- en: It allows for an iterative approach to building the model.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这允许采用迭代方法来构建模型。
- en: However, the HITL method has a lower recall and it works better with syntactic
    text patterns.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，HITL方法的召回率较低，并且更适合处理句法文本模式。
- en: In the large generative model, given a structured prompting and post-processing
    step, GPT-3 gives higher recall scores. GPT-3 is able to contextualize the prompts
    and learn a more general model. However, the downside of GPT-3 for text pattern
    extraction is that it in itself does not perform the extraction tasks as the HITL
    method does. In fact, for us to get comparable results to the HITL model, we had
    to
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型生成模型中，给定结构化的提示和后处理步骤，GPT-3提供了更高的召回率。GPT-3能够对提示进行上下文化并学习更通用的模型。然而，GPT-3在文本模式提取中的缺点是，它本身无法像HITL方法那样执行提取任务。实际上，为了获得与HITL模型相当的结果，我们不得不
- en: Engineer and design the structure of the prompts to leverage GPT-3’s powerful
    language abilities and
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设计并构建提示的结构，以利用GPT-3强大的语言能力和
- en: Post-process the string output from GPT-3.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对GPT-3的字符串输出进行后处理。
- en: However, such steps may not necessarily be taken by an end-user who doesn’t
    have much technical training.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些步骤可能不会被没有太多技术培训的终端用户采取。
- en: Conclusion & Future Work
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论与未来工作
- en: In our preliminary work, we compared HITL and pre-trained large generative language
    models over text pattern extraction tasks. We wanted to understand how an end-user
    might use popularly researched models for text pattern extraction. We found that
    the HITL method performed better precision-wise on average, while GPT-3 had comparable
    or higher recall scores.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的初步工作中，我们比较了HITL和预训练的大型生成语言模型在文本模式提取任务中的表现。我们想了解终端用户如何使用广泛研究的模型进行文本模式提取。我们发现，HITL方法在精确度上平均表现更好，而GPT-3在召回率上具有相当或更高的得分。
- en: Our future work builds on top of these results. **How do we combine the advantages
    of each approach, HITL and pre-trained large generative language models?** How
    can we take advantage of the user inputs to improve the prompt design and in turn
    leverage the large language model’s contextual and language abilities?
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的未来工作在这些结果的基础上进行。**我们如何结合HITL和预训练的大型生成语言模型的优点？** 我们如何利用用户输入来改进提示设计，并进而利用大型语言模型的上下文和语言能力？
