- en: 'Efficient Deep Learning: Unleashing the Power of Model Compression'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é«˜æ•ˆæ·±åº¦å­¦ä¹ ï¼šé‡Šæ”¾æ¨¡å‹å‹ç¼©çš„åŠ›é‡
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/efficient-deep-learning-unleashing-the-power-of-model-compression-7b5ea37d4d06](https://towardsdatascience.com/efficient-deep-learning-unleashing-the-power-of-model-compression-7b5ea37d4d06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/efficient-deep-learning-unleashing-the-power-of-model-compression-7b5ea37d4d06](https://towardsdatascience.com/efficient-deep-learning-unleashing-the-power-of-model-compression-7b5ea37d4d06)
- en: '![](../Images/e3f104a88962263b5a88baa0f644362f.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e3f104a88962263b5a88baa0f644362f.png)'
- en: Image By Author
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºï¼šä½œè€…
- en: Accelerate model inference speed in production
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ é€Ÿç”Ÿäº§ä¸­çš„æ¨¡å‹æ¨ç†é€Ÿåº¦
- en: '[](https://medium.com/@marcellopoliti?source=post_page-----7b5ea37d4d06--------------------------------)[![Marcello
    Politi](../Images/484e44571bd2e75acfe5fef3146ab3c2.png)](https://medium.com/@marcellopoliti?source=post_page-----7b5ea37d4d06--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7b5ea37d4d06--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7b5ea37d4d06--------------------------------)
    [Marcello Politi](https://medium.com/@marcellopoliti?source=post_page-----7b5ea37d4d06--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@marcellopoliti?source=post_page-----7b5ea37d4d06--------------------------------)[![Marcello
    Politi](../Images/484e44571bd2e75acfe5fef3146ab3c2.png)](https://medium.com/@marcellopoliti?source=post_page-----7b5ea37d4d06--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7b5ea37d4d06--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7b5ea37d4d06--------------------------------)
    [Marcello Politi](https://medium.com/@marcellopoliti?source=post_page-----7b5ea37d4d06--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7b5ea37d4d06--------------------------------)
    Â·9 min readÂ·Sep 3, 2023
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page-----7b5ea37d4d06--------------------------------)
    Â·9åˆ†é’Ÿé˜…è¯»Â·2023å¹´9æœˆ3æ—¥
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»‹ç»
- en: When a Machine Learning model is deployed into production there are often requirements
    to be met that are not taken into account in a prototyping phase of the model.
    For example, the model in production will have to handle lots of requests from
    different users running the product. So you will want to optimize for instance
    latency and/o throughput.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æœºå™¨å­¦ä¹ æ¨¡å‹æŠ•å…¥ç”Ÿäº§æ—¶ï¼Œé€šå¸¸ä¼šæœ‰ä¸€äº›åœ¨æ¨¡å‹åŸå‹é˜¶æ®µæœªè€ƒè™‘çš„è¦æ±‚ã€‚ä¾‹å¦‚ï¼Œç”Ÿäº§ä¸­çš„æ¨¡å‹å¿…é¡»å¤„ç†æ¥è‡ªä¸åŒç”¨æˆ·çš„å¤§é‡è¯·æ±‚ã€‚å› æ­¤ï¼Œä½ éœ€è¦ä¼˜åŒ–ä¾‹å¦‚å»¶è¿Ÿå’Œ/æˆ–ååé‡ã€‚
- en: '**Latency**: is the time it takes for a task to get done, like how long it
    takes to load a webpage after you click a link. Itâ€™s the waiting time between
    starting something and seeing the result.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å»¶è¿Ÿ**ï¼šæ˜¯å®Œæˆä»»åŠ¡æ‰€éœ€çš„æ—¶é—´ï¼Œä¾‹å¦‚ç‚¹å‡»é“¾æ¥ååŠ è½½ç½‘é¡µæ‰€éœ€çš„æ—¶é—´ã€‚è¿™æ˜¯ä»å¼€å§‹æŸé¡¹å·¥ä½œåˆ°çœ‹åˆ°ç»“æœçš„ç­‰å¾…æ—¶é—´ã€‚'
- en: '**Throughput**: is how much requests a system can handle in a certain time.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ååé‡**ï¼šæ˜¯ç³»ç»Ÿåœ¨ä¸€å®šæ—¶é—´å†…èƒ½å¤Ÿå¤„ç†çš„è¯·æ±‚æ•°é‡ã€‚'
- en: This means that the Machine Learning model has to be very fast at making its
    predictions, and for this there are various techniques that serve to increase
    the speed of model inference, letâ€™s look at the most important ones in this article.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€æœºå™¨å­¦ä¹ æ¨¡å‹å¿…é¡»éå¸¸å¿«é€Ÿåœ°è¿›è¡Œé¢„æµ‹ï¼Œä¸ºæ­¤æœ‰å„ç§æŠ€æœ¯å¯ä»¥æé«˜æ¨¡å‹æ¨ç†çš„é€Ÿåº¦ï¼Œè®©æˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­æ¢è®¨å…¶ä¸­æœ€é‡è¦çš„å‡ ç§ã€‚
- en: Model Compression
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¨¡å‹å‹ç¼©
- en: There are techniques that aim to make **models smaller**, which is why they
    are called **model compression** techniques, while others that focus on making
    models **faster at inference** and thus fall under the field of **model optimization**.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸€äº›æŠ€æœ¯æ—¨åœ¨ä½¿**æ¨¡å‹æ›´å°**ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆå®ƒä»¬è¢«ç§°ä¸º**æ¨¡å‹å‹ç¼©**æŠ€æœ¯ï¼Œè€Œå…¶ä»–æŠ€æœ¯åˆ™ä¸“æ³¨äºä½¿æ¨¡å‹**æ¨ç†æ›´å¿«**ï¼Œå› æ­¤å±äº**æ¨¡å‹ä¼˜åŒ–**é¢†åŸŸã€‚
- en: But often making models smaller also helps with inference speed, so it is a
    very blurred line that separates these two fields of study.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†é€šå¸¸ä½¿æ¨¡å‹æ›´å°ä¹Ÿæœ‰åŠ©äºæ¨ç†é€Ÿåº¦ï¼Œå› æ­¤è¿™ä¸¤ä¸ªç ”ç©¶é¢†åŸŸä¹‹é—´çš„ç•Œé™éå¸¸æ¨¡ç³Šã€‚
- en: Low Rank Factorization
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½ç§©åˆ†è§£
- en: This is the first method we see, and it is being studied a lot, in fact many
    papers have recently come out concerning it.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘ä»¬çœ‹åˆ°çš„ç¬¬ä¸€ç§æ–¹æ³•ï¼Œå®é™…ä¸Šè¿™æ–¹é¢çš„ç ”ç©¶å¾ˆå¤šï¼Œæœ€è¿‘æœ‰è®¸å¤šç›¸å…³è®ºæ–‡å‘è¡¨ã€‚
- en: '**The basic idea is to replace the matrices of a neural network** (the matrices
    representing the layers of the network) **with matrices that have a lower dimensionality**,
    although it would be more correct to talk about tensors, because we can often
    have matrices of more than 2 dimensions. In this way we will have fewer network
    parameters and faster inference.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**åŸºæœ¬æ€æƒ³æ˜¯å°†ç¥ç»ç½‘ç»œä¸­çš„çŸ©é˜µ**ï¼ˆä»£è¡¨ç½‘ç»œå±‚çš„çŸ©é˜µï¼‰**æ›¿æ¢ä¸ºå…·æœ‰è¾ƒä½ç»´åº¦çš„çŸ©é˜µ**ï¼Œå°½ç®¡æ›´å‡†ç¡®çš„è¯´æ³•æ˜¯å¼ é‡ï¼Œå› ä¸ºæˆ‘ä»¬å¸¸å¸¸ä¼šæœ‰è¶…è¿‡2ç»´çš„çŸ©é˜µã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å°†æ‹¥æœ‰æ›´å°‘çš„ç½‘ç»œå‚æ•°å’Œæ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚'
- en: A trivial case is in a CNN network of replacing 3x3 convolutions with 1x1 convolutions.
    Such techniques are used by networks such as [SqueezeNet](https://arxiv.org/abs/1602.07360).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªç®€å•çš„ä¾‹å­æ˜¯åœ¨CNNç½‘ç»œä¸­ç”¨1x1å·ç§¯æ›¿ä»£3x3å·ç§¯ã€‚è¿™æ ·çš„æŠ€æœ¯è¢«[ SqueezeNet](https://arxiv.org/abs/1602.07360)ç­‰ç½‘ç»œä½¿ç”¨ã€‚
- en: Lately, similar ideas are being applied for other purposes, such as allowing
    fine-tuning of large language models with limited resources.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘ï¼Œç±»ä¼¼çš„æ€æƒ³è¢«åº”ç”¨äºå…¶ä»–ç›®çš„ï¼Œä¾‹å¦‚åœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹å…è®¸å¯¹å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚
- en: When fine-tuning a pretrained model for a downstreamtask, one still has to train
    the model on all the parameters of the pretrained model, which can be very expensive.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒä»¥ç”¨äºä¸‹æ¸¸ä»»åŠ¡æ—¶ï¼Œä»éœ€å¯¹é¢„è®­ç»ƒæ¨¡å‹çš„æ‰€æœ‰å‚æ•°è¿›è¡Œè®­ç»ƒï¼Œè¿™å¯èƒ½ä¼šéå¸¸æ˜‚è´µã€‚
- en: So the idea of the method called â€œ[Low Rank Adaptation Of Large Language Models](https://arxiv.org/abs/2106.09685)â€,
    or LoRA, is to replace matrices from the original model with pairs of smaller
    matrices (using matrix decomposition) that have a smaller size. This way only
    these new matrices need to be retrained to fit the pretrained model to more downstream
    tasks.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œåä¸ºâ€œ[å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„ä½ç§©é€‚åº”](https://arxiv.org/abs/2106.09685)â€æˆ–LoRAçš„æ–¹æ³•çš„æƒ³æ³•æ˜¯å°†åŸå§‹æ¨¡å‹ä¸­çš„çŸ©é˜µæ›¿æ¢ä¸ºä¸€å¯¹å¯¹å°ºå¯¸è¾ƒå°çš„çŸ©é˜µï¼ˆä½¿ç”¨çŸ©é˜µåˆ†è§£ï¼‰ã€‚è¿™æ ·ï¼Œåªæœ‰è¿™äº›æ–°çš„çŸ©é˜µéœ€è¦é‡æ–°è®­ç»ƒï¼Œä»¥ä¾¿å°†é¢„è®­ç»ƒæ¨¡å‹è°ƒæ•´åˆ°æ›´å¤šçš„ä¸‹æ¸¸ä»»åŠ¡ä¸­ã€‚
- en: '![](../Images/b21d32443e3691de45e54500a439b131.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b21d32443e3691de45e54500a439b131.png)'
- en: 'Matric Decomposition in LoRA( src: https://arxiv.org/pdf/2106.09685.pdf)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: LoRAä¸­çš„çŸ©é˜µåˆ†è§£ï¼ˆæ¥æºï¼š https://arxiv.org/pdf/2106.09685.pdfï¼‰
- en: Lets now see how to implement fine tuning using LoRA with the [PEFT](https://huggingface.co/docs/peft/index)
    library from Hugging Face ğŸ¤—.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ä½¿ç”¨Hugging Face ğŸ¤— çš„[PEFT](https://huggingface.co/docs/peft/index)åº“æ¥å®ç°å¾®è°ƒã€‚
- en: Suppose we want to fine-tune the `[bigscience/mt0-large](https://huggingface.co/bigscience/mt0-large)`
    using LoRA. We must first take care of importing what we will need.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬æƒ³ä½¿ç”¨LoRAå¯¹`[bigscience/mt0-large](https://huggingface.co/bigscience/mt0-large)`è¿›è¡Œå¾®è°ƒã€‚æˆ‘ä»¬å¿…é¡»é¦–å…ˆå¤„ç†å¯¼å…¥æˆ‘ä»¬éœ€è¦çš„å†…å®¹ã€‚
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The next step will be to create a configuration for LoRA to be applied during
    fine-tuning.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ­¥å°†æ˜¯åˆ›å»ºä¸€ä¸ªLoRAé…ç½®ï¼Œä»¥ä¾¿åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­åº”ç”¨ã€‚
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We now instantiate the model using the base model of the Transformers library
    and the configuration object we created for LoRA.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨ä½¿ç”¨Transformersåº“çš„åŸºç¡€æ¨¡å‹å’Œæˆ‘ä»¬ä¸ºLoRAåˆ›å»ºçš„é…ç½®å¯¹è±¡æ¥å®ä¾‹åŒ–æ¨¡å‹ã€‚
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Knowledge Distillation
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: çŸ¥è¯†è’¸é¦
- en: This is another method that allows us to put a â€œsmallâ€ and therefore fast model
    into production.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å¦ä¸€ç§æ–¹æ³•ï¼Œå®ƒå…è®¸æˆ‘ä»¬å°†ä¸€ä¸ªâ€œå°å‹â€ä¸”å› æ­¤é€Ÿåº¦æ›´å¿«çš„æ¨¡å‹æŠ•å…¥ç”Ÿäº§ã€‚
- en: The idea is to have a **large model called the teacher**, and a s**maller model
    called the student**, and we will **use the** **teacherâ€™s knowledge to teach the
    student what to predict**. That way we can put only the student into production.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯æœ‰ä¸€ä¸ª**ç§°ä¸ºæ•™å¸ˆçš„å¤§å‹æ¨¡å‹**å’Œä¸€ä¸ª**ç§°ä¸ºå­¦ç”Ÿçš„å°å‹æ¨¡å‹**ï¼Œæˆ‘ä»¬å°†**åˆ©ç”¨æ•™å¸ˆçš„çŸ¥è¯†æ¥æ•™å­¦ç”Ÿå¦‚ä½•è¿›è¡Œé¢„æµ‹**ã€‚è¿™æ ·æˆ‘ä»¬å°±å¯ä»¥åªå°†å­¦ç”Ÿæ¨¡å‹æŠ•å…¥ç”Ÿäº§ã€‚
- en: '![](../Images/e3f862323f2d9b3c00b3c9ae076faab1.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e3f862323f2d9b3c00b3c9ae076faab1.png)'
- en: 'Knowledge Distillation (src: [https://www.analyticsvidhya.com/blog/2022/01/knowledge-distillation-theory-and-end-to-end-case-study/](https://www.analyticsvidhya.com/blog/2022/01/knowledge-distillation-theory-and-end-to-end-case-study/))'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: çŸ¥è¯†è’¸é¦ï¼ˆæ¥æºï¼š[https://www.analyticsvidhya.com/blog/2022/01/knowledge-distillation-theory-and-end-to-end-case-study/](https://www.analyticsvidhya.com/blog/2022/01/knowledge-distillation-theory-and-end-to-end-case-study/)ï¼‰
- en: A classic example of a model developed in this way is [DistillBERT](https://huggingface.co/docs/transformers/model_doc/distilbert),
    which is the student model of [BERT](https://arxiv.org/abs/1810.04805). DistilBERT
    is 40% smaller than BERT, but retains 97% of the language comprehension capabilities
    and is 60% faster in inference.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥è¿™ç§æ–¹å¼å¼€å‘çš„ç»å…¸æ¨¡å‹ç¤ºä¾‹æ˜¯[DistillBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)ï¼Œå®ƒæ˜¯[
    BERT](https://arxiv.org/abs/1810.04805)çš„å­¦ç”Ÿæ¨¡å‹ã€‚DistilBERTæ¯”BERTå°40%ï¼Œä½†ä¿ç•™äº†97%çš„è¯­è¨€ç†è§£èƒ½åŠ›ï¼Œå¹¶ä¸”åœ¨æ¨ç†æ—¶å¿«60%ã€‚
- en: One downside of this method is that you still need to have the large techer
    model available in order to train the student, and you may not have the resources
    to train a model like the teacher.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•çš„ä¸€ä¸ªç¼ºç‚¹æ˜¯ï¼Œä½ ä»ç„¶éœ€è¦æ‹¥æœ‰å¤§å‹æ•™å¸ˆæ¨¡å‹ä»¥è®­ç»ƒå­¦ç”Ÿï¼Œè€Œä½ å¯èƒ½æ²¡æœ‰èµ„æºè®­ç»ƒåƒæ•™å¸ˆé‚£æ ·çš„æ¨¡å‹ã€‚
- en: Letâ€™s look at a simple example of knowledge distillation in Python. A key concept
    to understand is [Kullbackâ€“Leibler divergenc](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)e,
    which is a mathematical concept for understanding the difference between two distributions,
    and in fact in our case we want to understand the difference between the predictions
    of the two models, so the loss function of the training will be based on this
    mathematical concept.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªPythonä¸­çš„çŸ¥è¯†è’¸é¦çš„ç®€å•ç¤ºä¾‹ã€‚ä¸€ä¸ªå…³é”®æ¦‚å¿µæ˜¯[KLæ•£åº¦](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)ï¼Œè¿™æ˜¯ä¸€ä¸ªç†è§£ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´å·®å¼‚çš„æ•°å­¦æ¦‚å¿µï¼Œå®é™…ä¸Šåœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬æƒ³è¦äº†è§£ä¸¤ä¸ªæ¨¡å‹çš„é¢„æµ‹ä¹‹é—´çš„å·®å¼‚ï¼Œå› æ­¤è®­ç»ƒçš„æŸå¤±å‡½æ•°å°†åŸºäºè¿™ä¸ªæ•°å­¦æ¦‚å¿µã€‚
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Pruning
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‰ªæ
- en: 'Pruning is a model compression method that I studied for my graduate thesis,
    and in fact I have previously published an article on how to implement Pruning
    in Julia: [Iterative Pruning Methods for Artificial Neural Networks in Julia](/iterative-pruning-methods-for-artificial-neural-networks-in-julia-c605f547a485).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å‰ªææ˜¯ä¸€ç§æ¨¡å‹å‹ç¼©æ–¹æ³•ï¼Œæˆ‘åœ¨ç ”ç©¶ç”Ÿè®ºæ–‡ä¸­ç ”ç©¶è¿‡è¿™ä¸ªæ–¹æ³•ï¼Œå®é™…ä¸Šæˆ‘ä¹‹å‰å·²ç»å‘å¸ƒäº†ä¸€ç¯‡å…³äºå¦‚ä½•åœ¨Juliaä¸­å®ç°å‰ªæçš„æ–‡ç« ï¼š[Juliaä¸­çš„è¿­ä»£å‰ªææ–¹æ³•](/iterative-pruning-methods-for-artificial-neural-networks-in-julia-c605f547a485)ã€‚
- en: Pruning was born to address overfitting in Decision Trees, in fact branches
    were cut off to decrease tree depth. The concept was later used in Neural Networks
    in which edges and/or nodes in the network are removed ( depending on whether
    unstructured pruning or structured pruning is performed).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å‰ªæè¯ç”Ÿæ˜¯ä¸ºäº†åº”å¯¹å†³ç­–æ ‘ä¸­çš„è¿‡æ‹Ÿåˆï¼Œå®é™…ä¸Šï¼Œå‰ªå»äº†åˆ†æ”¯ä»¥å‡å°‘æ ‘çš„æ·±åº¦ã€‚è¿™ä¸ªæ¦‚å¿µåæ¥è¢«åº”ç”¨äºç¥ç»ç½‘ç»œä¸­ï¼Œåœ¨å…¶ä¸­ç½‘ç»œä¸­çš„è¾¹å’Œ/æˆ–èŠ‚ç‚¹è¢«ç§»é™¤ï¼ˆå–å†³äºæ˜¯è¿›è¡Œéç»“æ„åŒ–å‰ªæè¿˜æ˜¯ç»“æ„åŒ–å‰ªæï¼‰ã€‚
- en: '![](../Images/a5512b117100950116c5961541a85284.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a5512b117100950116c5961541a85284.png)'
- en: 'Neural Network Pruning (src: [https://towardsdatascience.com/pruning-neural-networks-1bb3ab5791f9](/pruning-neural-networks-1bb3ab5791f9))'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ç¥ç»ç½‘ç»œå‰ªæï¼ˆæ¥æºï¼š[https://towardsdatascience.com/pruning-neural-networks-1bb3ab5791f9](/pruning-neural-networks-1bb3ab5791f9)ï¼‰
- en: Suppose to remove entire nodes from the network, the matrices representing the
    layers will become smaller along with your model, and thus also faster.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬ä»ç½‘ç»œä¸­ç§»é™¤æ•´ä¸ªèŠ‚ç‚¹ï¼Œè¡¨ç¤ºå±‚çš„çŸ©é˜µä¼šå˜å¾—æ›´å°ï¼Œæ¨¡å‹ä¹Ÿä¼šå˜å¾—æ›´å¿«ã€‚
- en: In contrast, if we remove individual edges, the size of the matrices will remain
    the same, but we will place zeros in correspondence with the removed edges, and
    thus we will have very sparse matrices. In unstructured pruning therefore the
    advantage lies not in increased speed, but in memory, because saving sparse matrices
    in memory takes up much less space than saving dense matrices.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸åï¼Œå¦‚æœæˆ‘ä»¬ç§»é™¤å•ä¸ªè¾¹ï¼ŒçŸ©é˜µçš„å¤§å°å°†ä¿æŒä¸å˜ï¼Œä½†æˆ‘ä»¬ä¼šåœ¨ä¸è¢«ç§»é™¤çš„è¾¹å¯¹åº”çš„ä½ç½®æ”¾ç½®é›¶ï¼Œå› æ­¤æˆ‘ä»¬å°†æ‹¥æœ‰éå¸¸ç¨€ç–çš„çŸ©é˜µã€‚å› æ­¤ï¼Œåœ¨éç»“æ„åŒ–å‰ªæä¸­ï¼Œä¼˜åŠ¿ä¸åœ¨äºæé«˜é€Ÿåº¦ï¼Œè€Œåœ¨äºèŠ‚çœå†…å­˜ï¼Œå› ä¸ºåœ¨å†…å­˜ä¸­ä¿å­˜ç¨€ç–çŸ©é˜µå ç”¨çš„ç©ºé—´æ¯”ä¿å­˜å¯†é›†çŸ©é˜µå°‘å¾—å¤šã€‚
- en: 'But what are the nodes or edges that we want to prune? The most unnecessary
    onesâ€¦ There are a lot of research about this, and Iâ€™d like to link you to two
    papers in particular:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘ä»¬è¦å‰ªæçš„èŠ‚ç‚¹æˆ–è¾¹æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿæœ€ä¸å¿…è¦çš„é‚£äº›â€¦â€¦å…³äºè¿™ä¸€ç‚¹æœ‰å¾ˆå¤šç ”ç©¶ï¼Œæˆ‘ç‰¹åˆ«æƒ³ç»™ä½ æ¨èä¸¤ç¯‡è®ºæ–‡ï¼š
- en: '[Optimal Brain Damage](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=17c0a7de3c17d31f79589d245852b57d083d386e)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Optimal Brain Damage](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=17c0a7de3c17d31f79589d245852b57d083d386e)'
- en: '[Optimal Brain Surgeon and general network pruning](https://ieeexplore.ieee.org/document/298572)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Optimal Brain Surgeon å’Œé€šç”¨ç½‘ç»œå‰ªæ](https://ieeexplore.ieee.org/document/298572)'
- en: Letâ€™s see a simple Python script on how to implement Pruning for a simple MNIST
    Model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªç®€å•çš„Pythonè„šæœ¬ï¼Œäº†è§£å¦‚ä½•ä¸ºä¸€ä¸ªç®€å•çš„MNISTæ¨¡å‹å®ç°å‰ªæã€‚
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Quantization
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é‡åŒ–
- en: I donâ€™t think I am wrong in saying that quantization is probably the most widely
    used comrpession technique at the moment. Again, the basic idea is simple. **We
    generally represent of parameters of our neural network using 32bit float numbers.
    But what if we used less than that? We could use 16bit, 8, bit, 4 bit, or even
    1bit and have binary networks!**
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¸è®¤ä¸ºæˆ‘è¯´é”™äº†ï¼Œé‡åŒ–å¯èƒ½æ˜¯ç›®å‰ä½¿ç”¨æœ€å¹¿æ³›çš„å‹ç¼©æŠ€æœ¯ã€‚å†ä¸€æ¬¡ï¼ŒåŸºæœ¬æ¦‚å¿µå¾ˆç®€å•ã€‚**æˆ‘ä»¬é€šå¸¸ç”¨32ä½æµ®ç‚¹æ•°è¡¨ç¤ºç¥ç»ç½‘ç»œçš„å‚æ•°ã€‚ä½†å¦‚æœæˆ‘ä»¬ä½¿ç”¨æ›´å°‘çš„ä½æ•°å‘¢ï¼Ÿæˆ‘ä»¬å¯ä»¥ä½¿ç”¨16ä½ã€8ä½ã€4ä½ï¼Œç”šè‡³1ä½ï¼Œç”šè‡³å¯ä»¥æ‹¥æœ‰äºŒè¿›åˆ¶ç½‘ç»œï¼**
- en: What does this imply? By using lower precision numbers, the model will weigh
    less and be smaller, however it will also lose precision giving more approximate
    results than the original model. This is a technique used a lot when we need to
    deploy an on edge devices, on some particular hardware like a smartphone, because
    it allows us to shrink the size of the network a lot. Many frameworks allow to
    easily apply quantization such as [TensorFlow Lite](https://www.tensorflow.org/lite),
    [PyTorch](https://pytorch.org/) or [TensorRT](https://developer.nvidia.com/tensorrt).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€ä»€ä¹ˆï¼Ÿé€šè¿‡ä½¿ç”¨è¾ƒä½ç²¾åº¦çš„æ•°å­—ï¼Œæ¨¡å‹çš„ä½“ç§¯ä¼šæ›´å°ï¼Œä½†ä¹Ÿä¼šå¤±å»ç²¾åº¦ï¼Œç»“æœä¼šæ¯”åŸå§‹æ¨¡å‹æ›´ä¸ºè¿‘ä¼¼ã€‚è¿™æ˜¯ä¸€ç§åœ¨éœ€è¦å°†æ¨¡å‹éƒ¨ç½²åˆ°è¾¹ç¼˜è®¾å¤‡ä¸Šæ—¶å¸¸ç”¨çš„æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯æ™ºèƒ½æ‰‹æœºç­‰ç‰¹å®šç¡¬ä»¶ä¸Šï¼Œå› ä¸ºå®ƒèƒ½å¤§å¹…ç¼©å°ç½‘ç»œçš„å°ºå¯¸ã€‚è®¸å¤šæ¡†æ¶å…è®¸è½»æ¾åº”ç”¨é‡åŒ–ï¼Œä¾‹å¦‚
    [TensorFlow Lite](https://www.tensorflow.org/lite)ã€[PyTorch](https://pytorch.org/)
    æˆ– [TensorRT](https://developer.nvidia.com/tensorrt)ã€‚
- en: Quantization can be applied pre-training, so we directly transect a network
    whose parameters can only take parameters in a certain range, or post-training,
    so at the end we round up the value of the parameters.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: é‡åŒ–å¯ä»¥åœ¨è®­ç»ƒå‰åº”ç”¨ï¼Œå³æˆ‘ä»¬ç›´æ¥æˆªæ–­å‚æ•°åªèƒ½å–ç‰¹å®šèŒƒå›´å†…çš„å€¼ï¼Œæˆ–è€…åœ¨è®­ç»ƒååº”ç”¨ï¼Œå³åœ¨ç»“æŸæ—¶å¯¹å‚æ•°çš„å€¼è¿›è¡Œå››èˆäº”å…¥ã€‚
- en: Here again we quickly see how to apply quantization in Python.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¿«é€Ÿå±•ç¤ºäº†å¦‚ä½•åœ¨ Python ä¸­åº”ç”¨é‡åŒ–ã€‚
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Conclusion
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: In this article, we looked at several model compression methodologies in order
    to speed up the model inference phase, which can be a critical requirement for
    models in production. In particular, we focused on Low Rank Factorization, Knowledge
    Distillation, Pruning and Quantization, explaining the basic ideas and showing
    a simple implementation in Python.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†å‡ ç§æ¨¡å‹å‹ç¼©æ–¹æ³•ï¼Œä»¥åŠ å¿«æ¨¡å‹æ¨ç†é˜¶æ®µï¼Œè¿™å¯¹äºç”Ÿäº§ç¯å¢ƒä¸­çš„æ¨¡å‹å¯èƒ½æ˜¯ä¸€ä¸ªå…³é”®è¦æ±‚ã€‚ç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬å…³æ³¨äº†ä½ç§©åˆ†è§£ã€çŸ¥è¯†è’¸é¦ã€å‰ªæå’Œé‡åŒ–ï¼Œè§£é‡Šäº†åŸºæœ¬æ¦‚å¿µï¼Œå¹¶å±•ç¤ºäº†
    Python ä¸­çš„ç®€å•å®ç°ã€‚
- en: Model compression is also particularly useful for deploying models on particular
    hardware such as smartphones that have few resources (RAM, GPU, etcâ€¦).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹å‹ç¼©å¯¹äºåœ¨èµ„æºæœ‰é™ï¼ˆå¦‚ RAMã€GPU ç­‰ï¼‰çš„ç‰¹å®šç¡¬ä»¶ä¸Šéƒ¨ç½²æ¨¡å‹å°¤å…¶æœ‰ç”¨ï¼Œä¾‹å¦‚æ™ºèƒ½æ‰‹æœºã€‚
- en: One use case that I am very passionate about is also to use model compression
    to deploy models on satellites and spacecraft, which is very useful particularly
    in the field of earth observation, for example to allow the satellite to recognize
    autonomously which data or images to discard so as not to have too much traffic
    when this data is then sent to the ground segment on the for the data analysis.
    I hope this article has been helpful to you in better understanding this topic.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘éå¸¸çƒ­è¡·çš„ä¸€ä¸ªåº”ç”¨æ¡ˆä¾‹æ˜¯ä½¿ç”¨æ¨¡å‹å‹ç¼©æ¥åœ¨å«æ˜Ÿå’Œèˆªå¤©å™¨ä¸Šéƒ¨ç½²æ¨¡å‹ï¼Œè¿™åœ¨åœ°çƒè§‚æµ‹é¢†åŸŸå°¤å…¶æœ‰ç”¨ï¼Œä¾‹å¦‚ä½¿å«æ˜Ÿèƒ½å¤Ÿè‡ªä¸»è¯†åˆ«éœ€è¦ä¸¢å¼ƒçš„æ•°æ®æˆ–å›¾åƒï¼Œä»¥é¿å…åœ¨æ•°æ®ä¼ è¾“åˆ°åœ°é¢æ®µè¿›è¡Œåˆ†ææ—¶äº§ç”Ÿè¿‡å¤šæµé‡ã€‚å¸Œæœ›è¿™ç¯‡æ–‡ç« å¯¹ä½ æ›´å¥½åœ°ç†è§£è¿™ä¸ªè¯é¢˜æœ‰æ‰€å¸®åŠ©ã€‚
- en: If you liked this article, follow me on Medium! ğŸ˜„
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Œè¯·åœ¨ Medium ä¸Šå…³æ³¨æˆ‘ï¼ğŸ˜„
- en: ğŸ’¼ [Linkedin](https://www.linkedin.com/in/marcello-politi/) ï¸| ğŸ¦ [Twitter](https://twitter.com/_March08_)
    | [ğŸ’»](https://emojiterra.com/laptop-computer/) [Website](https://marcello-politi.super.site/)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¼ [Linkedin](https://www.linkedin.com/in/marcello-politi/) ï¸| ğŸ¦ [Twitter](https://twitter.com/_March08_)
    | [ğŸ’»](https://emojiterra.com/laptop-computer/) [ç½‘ç«™](https://marcello-politi.super.site/)
