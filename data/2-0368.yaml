- en: 'BERT vs GPT: Comparing the NLP Giants'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BERT 与 GPT：比较 NLP 巨头
- en: 原文：[https://towardsdatascience.com/bert-vs-gpt-comparing-the-nlp-giants-329d105e34ec](https://towardsdatascience.com/bert-vs-gpt-comparing-the-nlp-giants-329d105e34ec)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/bert-vs-gpt-comparing-the-nlp-giants-329d105e34ec](https://towardsdatascience.com/bert-vs-gpt-comparing-the-nlp-giants-329d105e34ec)
- en: How different are their structure, and how do the differences impact the model’s
    ability?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它们的结构有何不同，这些差异如何影响模型的能力？
- en: '[](https://medium.com/@vuphuongthao9611?source=post_page-----329d105e34ec--------------------------------)[![Thao
    Vu](../Images/9d44a2f199cdc9c29da72d9dc4971561.png)](https://medium.com/@vuphuongthao9611?source=post_page-----329d105e34ec--------------------------------)[](https://towardsdatascience.com/?source=post_page-----329d105e34ec--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----329d105e34ec--------------------------------)
    [Thao Vu](https://medium.com/@vuphuongthao9611?source=post_page-----329d105e34ec--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@vuphuongthao9611?source=post_page-----329d105e34ec--------------------------------)[![Thao
    Vu](../Images/9d44a2f199cdc9c29da72d9dc4971561.png)](https://medium.com/@vuphuongthao9611?source=post_page-----329d105e34ec--------------------------------)[](https://towardsdatascience.com/?source=post_page-----329d105e34ec--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----329d105e34ec--------------------------------)
    [Thao Vu](https://medium.com/@vuphuongthao9611?source=post_page-----329d105e34ec--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----329d105e34ec--------------------------------)
    ·7 min read·Aug 20, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----329d105e34ec--------------------------------)
    ·阅读时长 7 分钟·2023年8月20日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/2a6c1c6d9546726bba7434946b0bcdc0.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2a6c1c6d9546726bba7434946b0bcdc0.png)'
- en: Image generated by the author using Stable Diffusion.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用 Stable Diffusion 生成。
- en: 'In 2018, NLP researchers were all amazed by the BERT paper [1]. The approach
    was simple, yet the result was impressive: it set new benchmarks for 11 NLP tasks.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在2018年，NLP 研究人员对 BERT 论文感到惊讶[1]。这个方法虽然简单，但结果却令人印象深刻：它为11个NLP任务设立了新的基准。
- en: In a little over a year, BERT has become a ubiquitous baseline in Natural Language
    Processing (NLP) experiments counting over 150 research publications analysing
    and improving the model. [2]
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在短短一年多的时间里，BERT 已成为自然语言处理（NLP）实验中的一个普遍基准，超过150篇研究论文分析和改进了该模型。[2]
- en: In 2022, ChatGPT [3] blew up the whole Internet with its ability to generate
    human-like responses. The model can comprehend a wide range of topics and carry
    the conversation naturally for an extended period, which sets it apart from all
    traditional chatbots.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在2022年，ChatGPT [3] 以其生成类人响应的能力引爆了整个互联网。该模型可以理解广泛的话题，并能够自然地进行长时间对话，这使其与所有传统聊天机器人不同。
- en: BERT and ChatGPT are significant breakthroughs in NLP, yet their approaches
    are different. How do their structures differ, and how do they impact the models’
    ability? Let’s dive in!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 和 ChatGPT 是自然语言处理（NLP）领域的重大突破，但它们的方法不同。它们的结构有何不同？这些差异如何影响模型的能力？让我们深入探讨一下！
- en: Attention
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意力
- en: We must first recall the commonly-used attention to understand the model structure
    fully. Attention mechanisms are designed to capture and model relationships between
    tokens in a sequence, which is one of the reasons why they have been so successful
    in NLP tasks.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须首先回顾常用的注意力机制，以便完全理解模型结构。注意力机制旨在捕捉和建模序列中令牌之间的关系，这也是它们在NLP任务中如此成功的原因之一。
- en: An intuitive understanding
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个直观的理解
- en: Imagine you have ***n*** goods stored in boxes ***v1, v2,…,v_n.*** These arecalled
    “values”.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 想象一下你有 ***n*** 件商品存放在箱子 ***v1, v2,…,v_n.*** 这些被称为“值”。
- en: We have query ***q*** whichdemands to take some suitable amount ***w*** of goods
    from each box. Let’s call them ***w_1, w_2,..,w_n*** (this is the “attention weight”)
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有一个查询 ***q***，它要求从每个箱子中取出一些适量的商品 ***w***。我们称这些为 ***w_1, w_2,..,w_n***（这就是“注意力权重”）。
- en: How to determine ***w_1, w_2,.., w_n***? Or, in other words, how to know among
    ***v_1,v_2, ..,v_n,*** which should be taken more than others?
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何确定 ***w_1, w_2,.., w_n***？换句话说，如何知道在 ***v_1,v_2, ..,v_n,*** 哪些应该比其他的多取？
- en: Remember, all the values are stored in boxes we cannot peek into. So we can’t
    directly judge ***v_i*** should be taken less or more.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记住，所有的值都存储在我们无法窥探的箱子里。因此，我们不能直接判断 ***v_i*** 应该取少还是取多。
- en: Luckily, we have a tag on each box, ***k_1, k_2,…,k_n***, which are called “keys”.
    The “keys” represent the characteristic of what is inside the containers.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 幸运的是，我们在每个框上都有一个标签，***k_1, k_2,…,k_n***，这些被称为“keys”。“keys”代表容器内部的特征。
- en: Based on the “similarity” of ***q*** and ***k_i (q*k_i)***, we can then decide
    how important the ***v_i*** is (***w_i***) and how much of ***v_i*** we should
    take(***w_i*v_i***).
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于 ***q*** 和 ***k_i (q*k_i)*** 的“相似性”，我们可以决定 ***v_i*** 的重要性 (***w_i***) 以及我们应该取多少
    ***v_i*** (***w_i*v_i***).
- en: '![](../Images/17f21a23443564ae27c1603cc8a1022c.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/17f21a23443564ae27c1603cc8a1022c.png)'
- en: Base attention mechanism (Image by the author)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 基础注意力机制（图片由作者提供）
- en: Of course, that is a very abstract explanation of attention, but it helps me
    to remember better the meaning behind “query”, “key”, and “value”.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这是一种非常抽象的注意力解释，但它帮助我更好地记住“query”、“key”和“value”背后的含义。
- en: Next, let’s take a deeper look at how the Transformer models use different types
    of attention.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们更深入地了解 Transformer 模型如何使用不同类型的注意力。
- en: 'BERT: Global self-attention and bidirectional encoder'
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BERT：全球自注意力和双向编码器
- en: Global self-attention has the same value for query, key and value. In a sequence
    of tokens, **each token will “attend” all other tokens**, so the information is
    propagated along the sequence. And more important, in parallel.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 全球自注意力对 query、key 和 value 的值是相同的。在一系列词元中，**每个词元将“关注”所有其他词元**，因此信息沿序列传播。而且更重要的是，以并行方式进行。
- en: '![](../Images/259c53bf8367e74ea252299f2cbed0b3.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/259c53bf8367e74ea252299f2cbed0b3.png)'
- en: Global self-attention [4]
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 全球自注意力 [4]
- en: This is significant compared with RNN and CNN.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 与 RNN 和 CNN 相比，这一点非常重要。
- en: For RNN, each “state” is passed through many steps, which may cause the loss
    of information. Besides, RNN passing is sequentially through each token; we can’t
    make use of GPU parallelism.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 RNN，每个“状态”经过许多步骤，这可能导致信息的丢失。此外，RNN 按顺序传递每个词元，我们无法利用 GPU 并行处理。
- en: For CNN, even though it runs in parallel, each token can only attend to a limited
    field, making assumptions about the tokens’ relationship.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 CNN，尽管它是并行运行的，但每个词元只能关注有限的领域，从而对词元的关系做出假设。
- en: The self-attention is the key component of encoders, the building block of BERT
    [1]. The BERT paper’s authors pointed out the limits of left-to-right language
    models as follows.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力是编码器的关键组件，是 BERT 的构建块 [1]。BERT 论文的作者指出了从左到右的语言模型的局限性如下。
- en: Such restrictions are sub-optimal for sentence-level tasks and could be very
    harmful when applying finetuning-based approaches to token-level tasks such as
    question answering, where it is crucial to incorporate context from both directions.
    [1]
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这些限制对于句子级任务是次优的，当应用基于微调的方法于如问答这样的词元级任务时，它可能非常有害，因为在这些任务中，结合来自两个方向的上下文至关重要。[1]
- en: '![](../Images/55d42bc5e549cd57e16db6e008eb0624.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/55d42bc5e549cd57e16db6e008eb0624.png)'
- en: BERT pre-training [1]
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 预训练 [1]
- en: To overcome the shortcoming above, BERT was pre-trained on “masked language
    model” (MLM) and “next sentence prediction” (NSP) tasks.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服上述缺点，BERT 在“掩码语言模型”（MLM）和“下一个句子预测”（NSP）任务上进行了预训练。
- en: For the MLM task, 15% of token positions were selected for prediction. So those
    chosen tokens will have 80% replaced with a ***[MASK]*** token, 10% replaced by
    a random token, and 10% not replaced.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 MLM 任务，15% 的词元位置被选中进行预测。因此，所选择的词元中将有 80% 被替换为 ***[MASK]*** 词元，10% 被随机词元替换，10%
    不被替换。
- en: For the NSP task, given 2 sentences, ***s1*** and ***s2***, the input format
    is “***[CLS]<s1>[SEP]<s2>***”, and the model predicts whether ***s1*** is followed
    by ***s2***. [***CLS]*** and ***[SEP]*** are the special classification and separate
    tokens, respectively.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 NSP 任务，给定 2 个句子，***s1*** 和 ***s2***，输入格式为“***[CLS]<s1>[SEP]<s2>***”，模型预测
    ***s1*** 是否接在 ***s2*** 之后。[***CLS]*** 和 ***[SEP]*** 分别是特殊的分类和分隔符标记。
- en: As we can see, the model can “peek” at both the left and right contexts of each
    token in both tasks.This allows the model to take advantage of the bidirectional
    word representation and gain a deeper understanding.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，模型可以在这两个任务中“窥视”每个词元的左右上下文。这使得模型能够利用双向词表示，并获得更深入的理解。
- en: But the bidirectional encoding comes with a cost. Lacking decoders, BERT may
    not be suitable for text generation. Therefore, the model requires adding extra
    task-specific architecture to adapt to generative tasks.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 但双向编码有其代价。缺乏解码器的 BERT 可能不适合文本生成。因此，该模型需要添加额外的任务特定架构以适应生成任务。
- en: 'GPT: Causal self-attention and text generative'
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT：因果自注意力和文本生成
- en: Compared to global self-attention, causal self-attention allows each token to
    only attend to its left context. This architecture is unsuitable for tasks such
    as textual understanding but makes the model good at text generation.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 与全局自注意力相比，因果自注意力允许每个标记仅关注其左侧上下文。这种架构不适合文本理解等任务，但使得模型在文本生成方面表现优秀。
- en: '![](../Images/1ab61426c1a572cc8f2218f75f3e4566.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1ab61426c1a572cc8f2218f75f3e4566.png)'
- en: Causal self-attention [4]
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 因果自注意力 [4]
- en: Namely, causal self-attention allows the model tolearn the probabilities of
    a series of words, which is the core of a “language model” [8]. Given a sequence
    of symbols x=(s1, s2, …, sn), the model can predict the likelihood of the series
    as follows.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 即，因果自注意力使模型能够学习一系列单词的概率，这是“语言模型” [8] 的核心。给定一个符号序列 x=(s1, s2, …, sn)，模型可以预测该系列的概率如下。
- en: '![](../Images/fafe803ac5306d4d08de022ab1a770aa.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fafe803ac5306d4d08de022ab1a770aa.png)'
- en: Joint probabilities over a sequence of symbols [6]
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一系列符号的联合概率 [6]
- en: Causal self-attention is the critical component of the Transformer decoder block.
    One of the first pre-trained Transformer decoders is GPT [5] by OpenAI. Like BERT,
    the model also aims to utilise the massive corpus of unlabeled text datasets to
    build a pre-trained language model. Pretraining on Book Corpus[7], the model objective
    is to predict the next token. The pre-trained model is then finetuned to adapt
    to downstream tasks.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 因果自注意力是 Transformer 解码器块的关键组成部分。第一个预训练的 Transformer 解码器之一是 OpenAI 的 GPT [5]。与
    BERT 类似，该模型也旨在利用大量未标记的文本数据集来构建预训练语言模型。预训练于 Book Corpus[7] 上，该模型的目标是预测下一个标记。然后对预训练模型进行微调以适应下游任务。
- en: GPT-2 [6] shares the same approach of building universal word representation
    but is more ambitious. It aims to be a “multitask learner”, performing different
    tasks without fine-tuning. GPT only learns the distribution of ***p(output|input),***
    which makes the model lack context on “what task to do”***.*** The authors wanted
    to adapt GPT-2 to multi-tasks by conditioning the prediction on both input and
    task, ***p(output|input, task)***.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2 [6] 采用了相同的构建通用词表示的方法，但更具雄心。它旨在成为一个“多任务学习者”，在不进行微调的情况下执行不同任务。GPT 只学习***p(output|input)***
    的分布，这使得模型在“做什么任务”方面缺乏上下文***。*** 作者希望通过将预测条件化为输入和任务来将 GPT-2 适应多任务，***p(output|input,
    task)***。
- en: Previous approaches have corporated the “task” information at the architectural
    level, but GPT-2 makes it more flexible by “expressing” the task through natural
    language. For example, a translation task’s input can be “*translate to French,
    <English sentence>*”.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的方法在架构层面上结合了“任务”信息，但 GPT-2 通过自然语言“表达”任务，使其更加灵活。例如，翻译任务的输入可以是“*translate to
    French, <English sentence>*”。
- en: Mining a large amount of unlabeled text with explicit “task” information can
    be challenging. However, the authors believed the model could infer the implicit
    “tasks” expression from natural languages. Therefore, they collected a vast and
    diverse dataset which can demonstrate the “task” in varied domains. Namely, the
    model was trained on the WebText dataset[6] containing the text subset of 45 million
    links.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 从大量未标记的文本中提取明确的“任务”信息可能是具有挑战性的。然而，作者认为模型可以从自然语言中推断隐含的“任务”表达。因此，他们收集了一个庞大且多样化的数据集，可以在各种领域展示“任务”。即，模型在包含
    4500 万个链接文本子集的 WebText 数据集[6] 上进行了训练。
- en: Despite the less competitive performance on some benchmarks, GPT-2 has laid
    the ground for many LLM laters, such as GPT-3 [9] and ChatGPT. In particular,
    GPT-3 can comprehend tasks and demonstrations solely through text-based interactions.
    For the SuperGLUE benchmark [10], a set of language understanding tasks, GPT-3,
    without gradient-based update, has shown impressive performance compared to fine-tuned
    BERT.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在一些基准测试中的表现不够出色，但 GPT-2 为许多后来的大型语言模型奠定了基础，如 GPT-3 [9] 和 ChatGPT。特别是，GPT-3
    能够仅通过基于文本的交互来理解任务和示例。对于 SuperGLUE 基准测试 [10]，一组语言理解任务，GPT-3 在没有基于梯度的更新的情况下，相较于微调后的
    BERT 展现了令人印象深刻的表现。
- en: '![](../Images/e1626027cc404fdbc4df62c80f60ea0b.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e1626027cc404fdbc4df62c80f60ea0b.png)'
- en: Performance of GPT-3 and BERT on SuperGLUE [9]
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 和 BERT 在 SuperGLUE 上的表现 [9]
- en: '**Which model to choose?**'
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**选择哪个模型？**'
- en: Based on the models’ structure, we can conclude that BERT excels at understanding
    language and extracting contextual information, making it ideal for tasks like
    sentiment analysis and text classification. In contrast, GPT models are designed
    for generating human-like text, making it a top choice for chatbots and language
    generation tasks.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 根据模型的结构，我们可以得出结论，BERT 在理解语言和提取上下文信息方面表现出色，使其非常适合情感分析和文本分类等任务。相比之下，GPT 模型旨在生成类似人类的文本，使其成为聊天机器人和语言生成任务的首选。
- en: Another important factor is our data resources. We can easily customise recent
    GPT models to specific tasks with only a small amount of data, making them suitable
    for a broader range of applications. On the other hand, BERT finetuning might
    require more effort and data. For finetuning LLM techniques, you can check out
    my post.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要因素是我们的数据资源。我们可以仅用少量数据轻松定制最近的 GPT 模型以完成特定任务，使其适用于更广泛的应用。另一方面，BERT 微调可能需要更多的努力和数据。有关微调
    LLM 技术，你可以查看我的文章。
- en: '[](https://medium.com/mlearning-ai/a-simple-survey-of-fine-tuning-techniques-for-large-language-models-6c7945e6ee34?source=post_page-----329d105e34ec--------------------------------)
    [## A Quick Guide to Fine-tuning Techniques for Large Language Models'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/mlearning-ai/a-simple-survey-of-fine-tuning-techniques-for-large-language-models-6c7945e6ee34?source=post_page-----329d105e34ec--------------------------------)
    [## 大型语言模型微调技术快速指南'
- en: Large language models (LLM) have transformed the field of natural language processing
    (NLP) with their remarkable text…
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大型语言模型 (LLM) 以其出色的文本生成能力，已经彻底改变了自然语言处理 (NLP) 领域...
- en: medium.com](https://medium.com/mlearning-ai/a-simple-survey-of-fine-tuning-techniques-for-large-language-models-6c7945e6ee34?source=post_page-----329d105e34ec--------------------------------)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/mlearning-ai/a-simple-survey-of-fine-tuning-techniques-for-large-language-models-6c7945e6ee34?source=post_page-----329d105e34ec--------------------------------)
- en: Last but not least, we also need to consider our computational resources. Although
    there have been many optimisation efforts, finetuning, storing and serving LLM
    still demands substantial resources compared to BERT.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，我们还需要考虑计算资源。尽管进行了许多优化努力，但相比于 BERT，微调、存储和服务 LLM 仍然需要大量资源。
- en: Or you may enjoy the best of both worlds by incorporating them together. I will
    cover this topic in a future article.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 或者你也可以通过将它们结合起来享受两者的最佳体验。我将在未来的文章中讨论这个话题。
- en: For now, I hope you enjoy the reading :-)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，希望你享受阅读 :-)
- en: References
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Devlin, Jacob, et al. “Bert: Pre-training of deep bidirectional transformers
    for language understanding.” *arXiv preprint arXiv:1810.04805* (2018).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Devlin, Jacob 等. “Bert: 语言理解的深度双向变换器的预训练。” *arXiv 预印本 arXiv:1810.04805*
    (2018).'
- en: '[2] Rogers, Anna, Olga Kovaleva, and Anna Rumshisky. “A primer in BERTology:
    What we know about how BERT works.” Transactions of the Association for Computational
    Linguistics 8 (2021): 842–866.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Rogers, Anna, Olga Kovaleva, 和 Anna Rumshisky. “BERT 学科概述：我们对 BERT 工作原理的了解。”
    计算语言学协会会刊 8 (2021): 842–866.'
- en: '[3] [](https://openai.com/blog/chatgpt) [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [](https://openai.com/blog/chatgpt) [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt)'
- en: '[4] [https://www.tensorflow.org/text/tutorials/transformer](https://www.tensorflow.org/text/tutorials/transformer)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [https://www.tensorflow.org/text/tutorials/transformer](https://www.tensorflow.org/text/tutorials/transformer)'
- en: '[5] Radford, Alec, et al. “Improving language understanding by generative pre-training.”
    (2018).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Radford, Alec 等. “通过生成预训练提高语言理解。” (2018).'
- en: '[6] Radford, Alec, et al. “Language models are unsupervised multitask learners.”
    *OpenAI blog* 1.8 (2019): 9.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Radford, Alec 等. “语言模型是无监督的多任务学习者。” *OpenAI博客* 1.8 (2019): 9.'
- en: '[7] Zhu, Yukun, et al. “Aligning books and movies: Towards story-like visual
    explanations by watching movies and reading books.” *Proceedings of the IEEE international
    conference on computer vision*. 2015.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Zhu, Yukun 等. “对齐书籍和电影：通过观看电影和阅读书籍实现类似故事的视觉解释。” *IEEE国际计算机视觉会议论文集*。2015年。'
- en: '[8] [https://en.wikipedia.org/wiki/Language_model](https://en.wikipedia.org/wiki/Language_model)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] [https://en.wikipedia.org/wiki/Language_model](https://en.wikipedia.org/wiki/Language_model)'
- en: '[9] Brown, Tom, et al. “Language models are few-shot learners.” *Advances in
    neural information processing systems* 33 (2020): 1877–1901.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] Brown, Tom 等. “语言模型是少量学习者。” *神经信息处理系统进展* 33 (2020): 1877–1901.'
- en: '[10] Wang, Alex, et al. “Superglue: A stickier benchmark for general-purpose
    language understanding systems.” *Advances in neural information processing systems*
    32 (2019).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] 王艾利克斯等. “Superglue: 一个更具挑战性的通用语言理解系统基准。” *神经信息处理系统进展* 32 (2019).'
