- en: Data Preprocessing for Machine Translation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器翻译的数据预处理
- en: 原文：[https://towardsdatascience.com/data-preprocessing-for-machine-translation-fcbedef0e26a?source=collection_archive---------5-----------------------#2023-02-25](https://towardsdatascience.com/data-preprocessing-for-machine-translation-fcbedef0e26a?source=collection_archive---------5-----------------------#2023-02-25)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/data-preprocessing-for-machine-translation-fcbedef0e26a?source=collection_archive---------5-----------------------#2023-02-25](https://towardsdatascience.com/data-preprocessing-for-machine-translation-fcbedef0e26a?source=collection_archive---------5-----------------------#2023-02-25)
- en: Clean, normalize, and tokenize
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 清洗、归一化和分词
- en: '[](https://medium.com/@bnjmn_marie?source=post_page-----fcbedef0e26a--------------------------------)[![Benjamin
    Marie](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----fcbedef0e26a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fcbedef0e26a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fcbedef0e26a--------------------------------)
    [Benjamin Marie](https://medium.com/@bnjmn_marie?source=post_page-----fcbedef0e26a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@bnjmn_marie?source=post_page-----fcbedef0e26a--------------------------------)[![本杰明·玛丽](../Images/3ea1ad230cb1e67610418a8e36a5e5dd.png)](https://medium.com/@bnjmn_marie?source=post_page-----fcbedef0e26a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fcbedef0e26a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fcbedef0e26a--------------------------------)
    [本杰明·玛丽](https://medium.com/@bnjmn_marie?source=post_page-----fcbedef0e26a--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-preprocessing-for-machine-translation-fcbedef0e26a&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----fcbedef0e26a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fcbedef0e26a--------------------------------)
    ·14 min read·Feb 25, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffcbedef0e26a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-preprocessing-for-machine-translation-fcbedef0e26a&user=Benjamin+Marie&userId=ad2a414578b3&source=-----fcbedef0e26a---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fad2a414578b3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-preprocessing-for-machine-translation-fcbedef0e26a&user=Benjamin+Marie&userId=ad2a414578b3&source=post_page-ad2a414578b3----fcbedef0e26a---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fcbedef0e26a--------------------------------)
    · 14 min read · 2023年2月25日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffcbedef0e26a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-preprocessing-for-machine-translation-fcbedef0e26a&user=Benjamin+Marie&userId=ad2a414578b3&source=-----fcbedef0e26a---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffcbedef0e26a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-preprocessing-for-machine-translation-fcbedef0e26a&source=-----fcbedef0e26a---------------------bookmark_footer-----------)![](../Images/c636f6f625036e3463e36d30b90eb824.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffcbedef0e26a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-preprocessing-for-machine-translation-fcbedef0e26a&source=-----fcbedef0e26a---------------------bookmark_footer-----------)![](../Images/c636f6f625036e3463e36d30b90eb824.png)'
- en: Image from [Pixabay](https://pixabay.com/photos/coffee-pot-cup-of-coffee-filter-2139481/).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [Pixabay](https://pixabay.com/photos/coffee-pot-cup-of-coffee-filter-2139481/)。
- en: Data preprocessing is a critical step for any machine learning tasks. The data
    must be correct, clean, and in the expected format.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理是任何机器学习任务中的关键步骤。数据必须是正确的、清洁的，并且符合预期的格式。
- en: In this blog article, I explain all the steps that are required to preprocess
    the data used to train, validate, and evaluate machine translation systems.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我解释了预处理用于训练、验证和评估机器翻译系统的数据所需的所有步骤。
- en: I explain each preprocessing step with examples and code snippets to reproduce
    them by yourself.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我通过示例和代码片段解释每一步预处理步骤，以便你可以自行重现。
- en: 'For the preprocessing examples in this article, I use the first 100,000 segments
    from the Spanish-English (Es→En) [ParaCrawl v9](https://paracrawl.eu/) corpus
    (CC0). I directly provide [this dataset here](https://benjaminmarie.com/data/paracrawl100k.en-es.zip)
    (size: 9Mb).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文的预处理示例中，我使用了西班牙语-英语（Es→En）[ParaCrawl v9](https://paracrawl.eu/)语料库的前100,000个段落（CC0）。我直接提供了[这个数据集](https://benjaminmarie.com/data/paracrawl100k.en-es.zip)（大小：9Mb）。
- en: 'If you want to make this corpus by yourself, follow these steps (be patient,
    the original dataset is zipped but still weights 24Gb):'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想自己制作这个语料库，请按照这些步骤操作（耐心点，原始数据集已压缩，但仍重达24Gb）：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In my previous article, I presented all the main characteristics of the machine
    translation datasets used for training, validation, and evaluation:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在我之前的文章中，我介绍了用于训练、验证和评估的机器翻译数据集的所有主要特征：
- en: '[](/datasets-to-train-validate-and-evaluate-machine-translation-d61905d126aa?source=post_page-----fcbedef0e26a--------------------------------)
    [## Datasets to Train, Validate, and Evaluate Machine Translation'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/datasets-to-train-validate-and-evaluate-machine-translation-d61905d126aa?source=post_page-----fcbedef0e26a--------------------------------)
    [## 训练、验证和评估机器翻译的数据集'
- en: Select, check, and split
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择、检查和拆分
- en: towardsdatascience.com](/datasets-to-train-validate-and-evaluate-machine-translation-d61905d126aa?source=post_page-----fcbedef0e26a--------------------------------)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/datasets-to-train-validate-and-evaluate-machine-translation-d61905d126aa?source=post_page-----fcbedef0e26a--------------------------------)'
- en: 'Data formats: TXT, TSV, and TMX'
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据格式：TXT、TSV 和 TMX
- en: When looking for datasets for machine translation, you will often find them
    in different formats that try to best deal with their multilinguality nature.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在寻找机器翻译数据集时，你通常会发现它们以不同的格式出现，这些格式试图最好地处理其多语言性质。
- en: No matter what is the original format, most frameworks for training machine
    translation systems only take as input data in a raw text format.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 无论原始格式是什么，大多数用于训练机器翻译系统的框架只接受原始文本格式的数据。
- en: So you may have to convert the datasets that you got if they are not already
    text files.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果数据集不是文本文件，你可能需要对其进行转换。
- en: 'The most common formats that you may find are:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会发现的最常见格式有：
- en: 'parallel text (.txt): This is ideal. We don’t have to do any conversion. The
    source segments are in one text file and the target segments in another text file.
    Most of the following preprocessing steps will be applied to these two files in
    parallel. In the introduction, we downloaded ParaCrawl in this format.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平行文本（.txt）：这是理想的格式。我们不需要进行任何转换。源语言段落在一个文本文件中，目标语言段落在另一个文本文件中。大多数接下来的预处理步骤将并行应用于这两个文件。在介绍部分，我们下载了这种格式的ParaCrawl数据。
- en: 'tab-separated values (.tsv): This is a single file with each pair of source
    and target segments on the same line separated by a tabulation. Converting it
    into text files is straightforward with the command “cut”:'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 制表符分隔值（.tsv）：这是一个单文件，每对源语言和目标语言段落在同一行中由制表符分隔。用“cut”命令将其转换为文本文件是直接的：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Translation Memory eXchange (.tmx): This is an XML format often used by professional
    translators. It is a very verbose format. That’s why it is rarely used for large
    corpus. Dealing with TMX is slightly more difficult. We can start by stripping
    the XML tags. To do this, I use [a script](https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/strip-xml.perl)
    from the Moses project (LGPL license):'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 翻译记忆交换（.tmx）：这是一种XML格式，专业翻译人员经常使用。这是一种非常详细的格式。这就是为什么它很少用于大型语料库。处理TMX稍微困难一些。我们可以先去除XML标签。为此，我使用了来自Moses项目的[脚本](https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/strip-xml.perl)（LGPL许可）：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Don’t modify the target side of evaluation datasets
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不要修改评估数据集的目标端
- en: 'Before going further, there is a very important rule to follow when preprocessing
    datasets for machine translation:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入之前，预处理机器翻译数据集时有一个非常重要的规则：
- en: '**Never preprocessed the target side of the evaluation data!**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**绝不要对评估数据的目标端进行预处理！**'
- en: These are so-called “reference translations.” Since they are “references,” we
    shouldn’t touch them.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这些被称为“参考翻译”。由于它们是“参考”，我们不应对它们进行修改。
- en: There are several reasons for that. The main one is that **the target side of
    the evaluation data should look like the data you want your system to generate.**
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做有几个原因。其中最主要的原因是**评估数据的目标端应该与您希望系统生成的数据相似。**
- en: For instance, in some of the preprocessing steps, we will remove empty lines
    and tokenize the segments.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在某些预处理步骤中，我们将移除空行并对段落进行分词。
- en: You may want your system to return empty lines, for instance when translating
    empty text, and certainly you don’t want to return tokenized texts as a final
    output of your machine translation systems.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能希望系统返回空行，例如在翻译空文本时，当然你不希望返回标记化文本作为机器翻译系统的最终输出。
- en: If you remove empty lines from the references, you won’t be able to directly
    evaluate the ability of your system in generating empty lines when needed. While
    if you tokenize the references, you will only know how good your system is at
    generating tokenized text. As we will see, tokenized texts are not what you want
    your system to generate.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你从参考中删除空行，你将无法直接评估系统在需要时生成空行的能力。而如果你对参考进行标记化，你只能知道系统生成标记化文本的效果。如我们将看到的，标记化文本不是你希望系统生成的内容。
- en: Moreover, reference translations are used to compute automatic metric scores
    to evaluate the machine translation quality. If we modify these translations,
    we modify the scores. Then, the scores would not be comparable anymore with the
    other published scores for the same reference translations, since we modified
    these references.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，参考翻译用于计算自动度量分数以评估机器翻译质量。如果我们修改这些翻译，就会修改分数。这样，分数将不再与其他已发布的参考翻译分数可比，因为我们修改了这些参考。
- en: So keeping the original reference translation is critical to enable the **reproducibility
    and comparability** of an evaluation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，保留原始参考翻译对于确保**可重复性和可比性**至关重要。
- en: If at some point during the preprocessing the target side of the evaluation
    data is not the same as the original one, it means that something went wrong.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在预处理的某个点，评估数据的目标端与原始数据不同，说明出现了问题。
- en: 'Step 1: Cleaning and Filtering'
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 1：清理和过滤
- en: '**Step to be applied to:** source and target sides of the training and validation
    data.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤适用于：**训练和验证数据的源端和目标端。'
- en: For various reasons, publicly available parallel data may require some cleaning.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 出于各种原因，公开的平行数据可能需要一些清理。
- en: This is especially true if the data has been automatically created from text
    crawled on the Web.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据是从网络上抓取的文本自动创建的，这一点尤其正确。
- en: 'Cleaning often implies removing the following segments (or sentences) from
    the parallel data:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 清理通常意味着从平行数据中删除以下段落（或句子）：
- en: '**Empty** or mostly containing **non-printable characters**.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**空白**或大多包含**不可打印字符**。'
- en: With **invalid UTF8**, i.e., not properly encoded.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有**无效UTF8**，即未正确编码的。
- en: Containing **extremely long tokens** (or “words”) since they are often non-translatable,
    for instance, DNA sequences, digit sequences, nonsense, etc.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含**极长的标记**（或“单词”），因为它们通常无法翻译，例如DNA序列、数字序列、无意义的内容等。
- en: And, sometimes, **duplicates**, i.e., if a segment, or a pair of segments, appear
    more than once in the parallel data, we keep only one instance of it.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时，**重复**，即如果一个段或一对段在平行数据中出现多于一次，我们只保留一个实例。
- en: 'It is not necessary, but I usually remove duplicates of segment pairs in the
    training parallel data, for several reasons:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是必需的，但我通常会删除训练平行数据中的段对重复项，原因有多个：
- en: They are rarely useful for training.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们在训练中很少有用。
- en: They give more weight inside the training data to a particular translation without
    good reasons.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们在训练数据中对某个特定翻译赋予更多权重，且没有充分理由。
- en: They are often the unwanted product of a defective process used to acquire the
    data (e.g., crawling), in other words, these duplicates should never have been
    there in the first place.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们通常是获取数据过程中出现的缺陷产品（例如爬取），换句话说，这些重复本不应该存在。
- en: All these filtering rules are applied to keep only what is useful to train a
    neural model. It also removes segments that may trigger some errors in the following
    steps of the preprocessing.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些过滤规则的应用是为了只保留对训练神经模型有用的内容。它也删除了可能在后续预处理步骤中引发错误的段落。
- en: They also slightly reduce the size of the parallel data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 它们也略微减少了平行数据的大小。
- en: Keep in mind that we are cleaning/filtering **parallel data**. Each filtering
    rule should be applied to both sides of the data, simultaneously. For instance,
    if a source segment is empty and should be removed, the target segment should
    also be removed to preserve the parallelism of the data.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，我们在清理/过滤**平行数据**。每条过滤规则应同时应用于数据的两侧。例如，如果源段为空且应删除，则目标段也应删除，以保持数据的平行性。
- en: 'In addition to the rules I mentioned above, we should filter out the segment
    pairs with:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我上面提到的规则外，我们还应该过滤掉包含以下内容的片段对：
- en: A **very long** segment
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非常长**的片段'
- en: A **very short** segment
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非常短**的片段'
- en: A **high fertility**, i.e., when a segment appear disproportionately longer,
    or shorter, than its counterpart
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高繁殖率**，即当一个片段比其对应的片段出现得不成比例地更长或更短时'
- en: These rules are inherited from the statistical machine translation era during
    which these segments were dramatically increasing the computational cost while
    not being useful to train a translation model.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这些规则继承自统计机器翻译时代，在那个时代，这些片段显著增加了计算成本，而对于训练翻译模型没有用处。
- en: With the neural algorithms used for training today, these rules are not necessary
    anymore. Nonetheless, these segments are still mostly useless to train a translation
    model and can be safely removed from the training data to further reduce its size.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 由于今天使用的神经算法进行训练，这些规则已经不再必要。然而，这些片段在训练翻译模型时仍大多无用，因此可以安全地从训练数据中移除，以进一步减少其大小。
- en: There are many tools to perform this cleaning that are publicly available.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多公共工具可以执行此清理操作。
- en: '[*preprocess*](https://github.com/kpu/preprocess) ([LGPL](https://github.com/kpu/preprocess/blob/master/LICENSE)
    license) is an efficient framework that can do many filtering operations. It is
    used by the [workshop on machine translation](http://www2.statmt.org/wmt23/) to
    prepare the data for the main international machine translation competitions.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[*预处理*](https://github.com/kpu/preprocess)（[LGPL](https://github.com/kpu/preprocess/blob/master/LICENSE)
    许可）是一个高效的框架，可以执行许多过滤操作。它被[机器翻译研讨会](http://www2.statmt.org/wmt23/)用于准备主要国际机器翻译竞赛的数据。'
- en: I usually complement it with homemade scripts and additional frameworks such
    as [Moses scripts](https://github.com/moses-smt/mosesdecoder/tree/master/scripts)
    ([LGPL](https://github.com/moses-smt/mosesdecoder/blob/master/COPYING) license).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我通常会用自制脚本和额外的框架来补充，比如[Moses脚本](https://github.com/moses-smt/mosesdecoder/tree/master/scripts)（[LGPL](https://github.com/moses-smt/mosesdecoder/blob/master/COPYING)
    许可）。
- en: In the following paragraphs, I describe step by step the entire cleaning and
    filtering process that I usually apply to raw parallel data.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的段落中，我将逐步描述我通常应用于原始平行数据的整个清理和过滤过程。
- en: Practice
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实践
- en: We want to clean our Paracrawl Spanish-English parallel data (provided in the
    introduction of this article).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望清理我们的Paracrawl西班牙语-英语平行数据（见本文介绍部分）。
- en: One of the most costly steps, in terms of memory, is the removal of duplicates
    (so-called “deduplication”). Before deduplication, we should remove as many segment
    pairs as possible.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在内存方面，最昂贵的步骤之一是删除重复项（即“去重”）。在去重之前，我们应该尽可能地移除更多的片段对。
- en: 'We can start by applying the [clean-n-corpus.perl](https://github.com/moses-smt/mosesdecoder/blob/master/scripts/training/clean-corpus-n.perl)
    (this script doesn’t require installing Moses), as follows: *Note: This step assumes
    the existence of spaces in both the source and target languages. If one of the
    languages (mostly) doesn’t use spaces, such as Japanese or Chinese, you must first
    tokenize the source and target text files. If it applies to your use case, go
    directly to “Step 2 and 3” and then come back here once it is done.*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过应用[clean-n-corpus.perl](https://github.com/moses-smt/mosesdecoder/blob/master/scripts/training/clean-corpus-n.perl)来开始（这个脚本不需要安装Moses），如下所示：*注意：此步骤假设源语言和目标语言中都存在空格。如果其中一种语言（大多）不使用空格，如日语或中文，你必须首先对源文本和目标文本文件进行分词。如果适用于你的用例，直接跳到“步骤2和3”，然后在完成后再回来这里。*
- en: 'To know how to use clean-n-corpus.perl, call the script without any arguments.
    It should return:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何使用clean-n-corpus.perl，调用脚本而不带任何参数。它应返回：
- en: '[PRE3]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The arguments are:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 参数如下：
- en: '*ratio*: This is the fertility. By default, it is set to 9\. We usually don’t
    need to modify it, and thus don’t use this argument.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*比例*：这是繁殖率。默认情况下，它设置为9。我们通常不需要修改它，因此不使用此参数。'
- en: '*corpus*: This one is the path to the dataset to clean, without the extension.
    The script assumes that you named both source and target file the same, using
    the language ISO codes as extensions, for instance, train.es and train.en in our
    case. If you adopt the same file name convention I used for ParaCrawl, you simply
    have to put there: “train” (assuming that you are in the directory containing
    your data).'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*语料库*：这是清理数据集的路径，不包含扩展名。脚本假设你将源文件和目标文件命名为相同，使用语言ISO代码作为扩展名，例如，在我们的例子中是train.es和train.en。如果你采用了我为ParaCrawl使用的相同文件名约定，你只需在那里输入：“train”（假设你在包含数据的目录中）。'
- en: '*l1*: the extension of one of the parallel files, e.g., “es”.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*l1*: 其中一个平行文件的扩展名，例如“es”。'
- en: '*l2*: the extension of the other parallel file, e.g., “en”.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*l2*: 其他平行文件的扩展名，例如“en”。'
- en: '*clean-corpus*: The name of the files after cleaning. For instance, if you
    enter “train.clean”, the script will save the filtered parallel data into “train.clean.es”
    and “train.clean.en”'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*clean-corpus*: 清理后的文件名。例如，如果输入“train.clean”，脚本将把过滤后的平行数据保存到“train.clean.es”和“train.clean.en”。'
- en: '*min*: The minimum number of tokens under which a segment should be discarded.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*min*: 应丢弃的段落的最小标记数。'
- en: '*max*: The maximum number of tokens above which a segment should be discarded.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*max*: 应丢弃的段落的最大标记数。'
- en: '*max-word-length* (not shown here): The maximum number of characters in one
    token. If a segment pair contains a token longer than max-word-length, it is removed.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*max-word-length*（此处未显示）：一个标记中的最大字符数。如果一个段落对包含的标记长度超过 max-word-length，则会被移除。'
- en: 'To clean our ParaCrawl corpus, the full command to run is:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 要清理我们的 ParaCrawl 语料库，运行的完整命令是：
- en: '[PRE4]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This command removes from train.es and train.en segment pairs with:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令从 train.es 和 train.en 中移除带有以下条件的段落：
- en: An empty segment
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 空段落
- en: A segment longer than 150 words (or tokens)
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超过150个单词（或标记）的段落
- en: A high fertility
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高度的繁殖力
- en: Segments with a word (or token) containing more than 50 characters
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词（或标记）包含超过 50 个字符的段落
- en: And save the results into “train.clean.es” and “train.clean.en”.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 并将结果保存到“train.clean.es”和“train.clean.en”。
- en: 'The script displays the number of segments removed. If you did the same as
    I did, it should remains 99976 segments in the data:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本会显示移除的段落数量。如果你做的和我一样，数据中应该剩下 99976 个段落：
- en: '[PRE5]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Note that for each segment removed, its parallel segment is also removed. train.clean.es
    and train.clean.en should have the same number of lines. You can check it with:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，每次移除一个段落时，其平行段落也会被移除。train.clean.es 和 train.clean.en 应该有相同数量的行。你可以用以下命令检查：
- en: '[PRE6]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Next, we remove segments with *preprocess*.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们用 *preprocess* 移除段落：
- en: 'We need to compile it first (cmake is required):'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要先编译它（需要 cmake）：
- en: '[PRE7]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, we can use *preprocess* to remove lines with:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用 *preprocess* 来移除包含以下内容的行：
- en: Invalid UTF-8
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无效的 UTF-8
- en: Control characters (except tab and newline)
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制字符（除了制表符和换行符）
- en: Too many Common and Inherited Unicode script characters (like numbers)
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过多的常见和继承的 Unicode 脚本字符（如数字）
- en: Too much or too little punctuation
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标点符号过多或过少
- en: Too little in the expected script (to remove for instance Chinese sentences
    in English data)
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 期望脚本中的内容过少（例如移除英文数据中的中文句子）
- en: 'We use the “simple_cleaning” binary for this. It handles parallel data:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用“simple_cleaning”二进制文件来处理平行数据：
- en: '[PRE8]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The filtered data are saved in two new files that I named “train.clean.pp.es”
    and “train.clean.pp.en”.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤后的数据保存在两个新文件中，我将其命名为“train.clean.pp.es”和“train.clean.pp.en”。
- en: 'And it should print:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 并且应该打印：
- en: '[PRE9]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'And finally we can remove duplicates with “dedupe”:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以用“dedupe”来去除重复项：
- en: '[PRE10]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'And it should print:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 并且应该打印：
- en: '[PRE11]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We finished cleaning the data.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了数据清理。
- en: We nearly removed 15% of the segments. It means that eachtraining epoch of neural
    machine translation will be 15% (approximately) faster.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎移除了15%的段落。这意味着神经机器翻译的每个训练周期将快15%（大约）。
- en: 'Step 2: Normalization'
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2步：标准化
- en: '**Step to be applied to**: the source sides of all the datasets, and potentially
    to the target side of the training and validation datasets.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**适用步骤**：所有数据集的源侧，可能还包括训练和验证数据集的目标侧。'
- en: The objective of the normalization is to make sure that the same symbols, such
    as punctuation marks, numbers, and spaces, with the same UTF8 codes, are used
    in all the datasets.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化的目标是确保在所有数据集中使用相同的符号，如标点符号、数字和空格，并且具有相同的 UTF8 编码。
- en: In practice, this step can also **reduce the size of the vocabulary** (the number
    of different token types) by mapping symbols with a similar role or meaning to
    the same symbol.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这一步也可以**减少词汇量**（不同标记类型的数量），通过将具有类似作用或意义的符号映射到相同的符号。
- en: 'For instance, this step can normalize these different quote marks to the same
    one, as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这一步可以将这些不同的引号标准化为相同的引号，如下所示：
- en: ‘ **→** “
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ‘ **→** “
- en: '**« → “**'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**« → “**'
- en: 《 **→ “**
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《 **→ “**
- en: This step can also make sure that your system won’t generate translations with
    different styles of punctuation marks, if you apply it to the target sides of
    your training data.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 此步骤还可以确保您的系统不会生成具有不同标点风格的翻译，如果你将其应用于训练数据的目标侧。
- en: Of course, if we also normalize the target side of the training data, we have
    to make sure that we map to the desired characters.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果我们还规范化训练数据的目标侧，我们必须确保映射到所需的字符。
- en: 'For instance, if you prefer “《” because your target language uses this type
    of quote mark, then you should do a different mapping, as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你偏好使用“《”，因为你的目标语言使用这种类型的引号标记，那么你应该做不同的映射，如下所示：
- en: ‘ **→**《
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ‘ **→**《
- en: '**« →**《'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**« →**《'
- en: '**“ →**《'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**“ →**《'
- en: Since this step is commonly performed when preparing data for machine translation,
    there are various tools to do it.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在准备机器翻译数据时通常会执行此步骤，因此有多种工具可以完成它。
- en: I use [sacremoses](https://github.com/alvations/sacremoses) (MIT license). It
    is an implementation, in Python, of the [normalize-punctuation.perl](https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/normalize-punctuation.perl)
    from the Moses project.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了[sacremoses](https://github.com/alvations/sacremoses)（MIT 许可）。它是[Moses 项目](https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/normalize-punctuation.perl)的[normalize-punctuation.perl](https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/normalize-punctuation.perl)的
    Python 实现。
- en: Practice
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实践
- en: sacremoses [normalizer](https://github.com/alvations/sacremoses/blob/master/sacremoses/normalize.py)
    maps dozens of symbols from many languages. The rules can easily be edited to
    better match your expectations.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: sacremoses [规范化工具](https://github.com/alvations/sacremoses/blob/master/sacremoses/normalize.py)
    映射了来自多种语言的几十种符号。这些规则可以轻松编辑，以更好地符合你的期望。
- en: 'sacremoses can be installed with pip (Python 3 is required):'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: sacremoses 可以通过 pip 安装（需要 Python 3）：
- en: '[PRE12]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then you can normalize your data with the CLI:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你可以使用 CLI 来规范化数据：
- en: '[PRE13]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Examples of differences (obtained with the command “diff”): *Note: I took a
    screenshot instead of copy/paste the sentences in this article because the blog
    editor automatically applies its own normalization rules.*'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 差异示例（使用命令“diff”获得）：*注意：我选择截图而不是复制粘贴这些句子，因为博客编辑器会自动应用其自身的规范化规则。*
- en: '![](../Images/d04f41c9cb7179b7b4f21c8ec3c34e32.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d04f41c9cb7179b7b4f21c8ec3c34e32.png)'
- en: Sentences from ParaCrawl V9 (CC0) before and after normalization. Screenshot
    by the author.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ParaCrawl V9 (CC0) 的句子在规范化前后的对比。截图由作者提供。
- en: 'You can pass to this command several options, for instance, if you want to
    normalize numbers, add the option “-n”. To see all the options run:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以为此命令传递多个选项，例如，如果你想规范化数字，可以添加选项“-n”。要查看所有选项，请运行：
- en: '[PRE14]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Step 3: Tokenization'
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三步：分词
- en: '**Step to be applied to**: the source sides of all the datasets, and to the
    target side of the training and validation datasets.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**适用步骤**：所有数据集的源侧，以及训练和验证数据集的目标侧。'
- en: Traditionally, datasets for machine translation were tokenized with rule-based
    tokenizers. They often simply use spaces to delimitate tokens with the addition
    of rules to handle special cases.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，机器翻译的数据集使用基于规则的分词器进行分词。它们通常仅使用空格来分隔词元，并附加规则来处理特殊情况。
- en: 'Let’s take an example with the following English sentences:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以以下英文句子为例：
- en: '[PRE15]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'After tokenization with sacremoses tokenizer, we obtain:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 sacremoses 分词器进行分词后，我们得到：
- en: '[PRE16]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The differences before and after tokenization are difficult to spot. If you
    don’t see them, watch for the spaces near punctuation marks.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 分词前后的差异很难察觉。如果你看不到它们，可以注意标点符号附近的空格。
- en: For several reasons, these **rule-based tokenizers are not practical for neural
    machine translation**. For instance, they generate far **too many rare tokens**
    that cannot be modeled properly by the neural model.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 出于几个原因，这些**基于规则的分词器对神经机器翻译并不实用**。例如，它们生成了**过多的稀有词元**，这些词元无法被神经模型正确建模。
- en: The data must be “sub-tokenized”. For instance, tokens generated by a traditional
    tokenizer are split into smaller tokens. This is what the [byte-pair encoding
    approach (BPE)](https://aclanthology.org/P16-1162.pdf) does.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 数据必须“子分词”。例如，传统分词器生成的词元被拆分为更小的词元。这就是[字节对编码方法 (BPE)](https://aclanthology.org/P16-1162.pdf)的作用。
- en: Even more simple, the [SentencePiece approach](https://aclanthology.org/D18-2012.pdf)
    doesn’t even require a traditional tokenization. Consequently, we have one less
    tool (the traditional tokenizer) to apply and thus one less source of potential
    errors/mistakes in our preprocessing.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 更简单的是，[SentencePiece 方法](https://aclanthology.org/D18-2012.pdf) 甚至不需要传统的分词。因此，我们少了一个工具（传统分词器），从而减少了预处理中的潜在错误/问题来源。
- en: SentencePiece can be applied directly to **any sequences of characters**. This
    is especially practical for languages for which spaces are rare such as Japanese,
    Chinese, and Thai.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: SentencePiece 可以直接应用于**任何字符序列**。这对于像日语、中文和泰语这样空格稀少的语言尤其实用。
- en: Actually, SentencePiece is currently one of the most used tokenization algorithms
    for large language models such as the ones from the [T5](https://arxiv.org/abs/1910.10683)
    or [FLAN](https://arxiv.org/pdf/2109.01652.pdf) families.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，SentencePiece 目前是大语言模型（如[T5](https://arxiv.org/abs/1910.10683) 或 [FLAN](https://arxiv.org/pdf/2109.01652.pdf)
    系列）中最常用的标记化算法之一。
- en: 'Let’s see how it works by taking the same English sentences. We will obtain:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用相同的英文句子来看一下它是如何工作的。我们将获得：
- en: '[PRE17]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The sentences are much more difficult to read by humans and the tokenization
    isn’t intuitive. Yet, with neural models it works very well.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这些句子对人类来说更难阅读，标记化也不直观。然而，对于神经模型，这种方法效果很好。
- en: To obtain this result, you first need to train a SentencePiece model. The model
    is then used to tokenize the data, as well as all new inputs that will be sent
    to our machine translation system.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得这个结果，你首先需要训练一个 SentencePiece 模型。然后使用该模型对数据进行标记化，以及对所有将发送到我们的机器翻译系统的新输入进行标记化。
- en: Practice
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实践
- en: 'To train this model, we first need to install [SentencePiece](https://github.com/google/sentencepiece)
    with:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练这个模型，我们首先需要通过以下方式安装[SentencePiece](https://github.com/google/sentencepiece)：
- en: '[PRE18]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Then, concatenate the source and target sides of your parallel data in one single
    text file. This will allow us to train **a bilingual tokenization model**, rather
    than training different models for the source and target languages.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将并行数据的源侧和目标侧合并到一个单独的文本文件中。这将允许我们训练**一个双语标记化模型**，而不是为源语言和目标语言训练不同的模型。
- en: '[PRE19]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Before moving on to the training, we must decide on a vocabulary size.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入训练之前，我们必须决定词汇表的大小。
- en: 'This choice is a difficult but important decision. We usually use a rule of
    thumb: **A value between 8,000 and 16,000 works well for most use cases**. You
    may choose a higher value if you have a very large parallel data, or a lower value
    if you have much smaller training data, for instance less than 100,000 segment
    pairs.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这个选择是一个困难但重要的决策。我们通常使用一个经验法则：**8,000 到 16,000 之间的值适用于大多数用例**。如果你有非常大的并行数据，可以选择更高的值；如果你的训练数据较小，例如少于
    100,000 个片段对，则可以选择较低的值。
- en: The rationale is that if you set the vocabulary size too high, your vocabulary
    will contain rarer tokens. If your training parallel data doesn’t contain enough
    instances of these tokens, their embeddings would be poorly estimated.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 原因是如果你设置的词汇表大小过高，你的词汇表将包含更稀有的标记。如果你的训练并行数据中没有足够这些标记的实例，它们的嵌入会被估计得很差。
- en: In contrast, if you set the value too low, the neural model will have to deal
    with smaller tokens which would have to carry more information in their embeddings.
    The model may struggle to compose good translations in this situation.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，如果你将值设置得过低，神经模型将不得不处理更小的标记，这些标记需要在其嵌入中携带更多信息。在这种情况下，模型可能难以生成良好的翻译。
- en: Since our parallel data is quite small, I arbitrarily chose 8,000 for the vocabulary
    size.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的并行数据量较小，我随意选择了 8,000 作为词汇表的大小。
- en: 'To start the training:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始训练：
- en: '[PRE20]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This should be fast (less than 2 minutes).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该很快（少于 2 分钟）。
- en: 'The arguments are the following:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 参数如下：
- en: '*input*: The data used to train the SentencePiece model.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*input*：用于训练 SentencePiece 模型的数据。'
- en: '*model_prefix*: The name of the SentencePiece model.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*model_prefix*：SentencePiece 模型的名称。'
- en: '*vocab_size*: The vocabulary size.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*vocab_size*：词汇表的大小。'
- en: 'Then you have to apply the model to **all the data, except the target side
    of the test set** (remember: We never touch this one).'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你必须将模型应用于**所有数据，除了测试集的目标侧**（记住：我们从不接触这一部分）。
- en: 'For our ParaCrawl corpus, we do:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的 ParaCrawl 语料库，我们做：
- en: '[PRE21]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: And that’s it! Our datasets are all preprocessed. We can now start to train
    a machine translation system.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们的数据集都已预处理完毕。我们现在可以开始训练机器翻译系统了。
- en: '**Optional steps: truecasing and shuffling**'
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**可选步骤：truecasing 和打乱**'
- en: 'There are two more steps that you may find in some preprocessing pipeline for
    machine translation: truecasing and shuffling.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 还有两个步骤，你可能会在一些机器翻译的预处理管道中看到：truecasing 和打乱。
- en: Truecasing is getting deprecated but may yield slightly better translation quality.
    This preprocessing step lowercases characters that are uppercased only due to
    their position in a sentence.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Truecasing 正在被淘汰，但可能会略微提高翻译质量。这个预处理步骤将仅因其在句子中的位置而大写的字符转换为小写。
- en: 'For instance:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: '[PRE22]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'is truecased as:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 以 truecased 形式表示：
- en: '[PRE23]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The “h” in “He” is lowercased since it was uppercased only due to its position
    in the sentence. This is the only difference.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: “He”中的“h”被小写化，因为它只因在句中的位置而被大写。这是唯一的区别。
- en: This step slightly reduces the vocabulary size.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步骤略微减少了词汇表的大小。
- en: sacremoses implements truecasing.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: sacremoses 实现了 truecasing。
- en: As for shuffling segment pairs, it is probably already integrated in the framework
    you’ll use to train your machine translation system.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 至于打乱段落对，它可能已经集成在你用来训练机器翻译系统的框架中。
- en: The training data is often automatically reshuffled for each training epoch.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据通常会在每个训练周期自动重新打乱。
- en: Conclusion
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Filtering and normalization are steps that can significantly decrease the computational
    cost of training neural machine translation.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤和归一化是可以显著降低训练神经机器翻译计算成本的步骤。
- en: These steps may also improve the translation quality, especially if the training
    data was very noisy.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤也可能提高翻译质量，特别是当训练数据非常嘈杂时。
- en: The rules I suggest in this article for filtering and normalization are not
    suitable for all use cases. They will work well for most language pairs, but you
    may have to adapt them depending on your languages, e.g., when dealing with Asian
    languages such as Japanese you may want to change most of the rules for normalization
    to avoid generating English punctuation marks inside Japanese texts.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我在本文中建议的过滤和归一化规则并不适用于所有用例。它们对于大多数语言对都能很好地工作，但你可能需要根据你的语言进行调整，例如，当处理日语等亚洲语言时，你可能需要更改大多数归一化规则，以避免在日语文本中生成英文标点符号。
- en: Tokenization is even more critical. Fortunately, this is also the step that
    is most straightforward. Most preprocessing pipelines for machine translation
    do the same for tokenization, albeit with different hyperparameters.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 分词尤为关键。幸运的是，这也是最直接的步骤。大多数机器翻译的预处理管道对分词的处理是一样的，只是超参数不同。
- en: 'In a next article, I’ll explain all you need to know to train a machine translation
    system using the data you just preprocessed: frameworks, neural architectures,
    and hyperparameters.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一篇文章中，我将解释你需要知道的一切，以使用你刚刚预处理的数据来训练机器翻译系统：框架、神经网络结构和超参数。
- en: All my articles are published in The Kaitchup, my newsletter. Subscribe to receive
    weekly news, tips, and tutorials to run large language models and machine translation
    systems on your computer.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我所有的文章都发布在《The Kaitchup》这本通讯中。订阅以接收每周的新闻、技巧和教程，以便在你的计算机上运行大型语言模型和机器翻译系统。
- en: '[](https://kaitchup.substack.com/?source=post_page-----fcbedef0e26a--------------------------------)
    [## The Kaitchup - AI on a Budget | Benjamin Marie, PhD | Substack'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '[## The Kaitchup - AI on a Budget | Benjamin Marie, PhD | Substack](https://kaitchup.substack.com/?source=post_page-----fcbedef0e26a--------------------------------)'
- en: Subscribe for weekly AI news, tips, and tutorials on fine-tuning, running, and
    serving large language models on your…
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 订阅每周的 AI 新闻、技巧和有关微调、运行和服务大型语言模型的教程……
- en: kaitchup.substack.com](https://kaitchup.substack.com/?source=post_page-----fcbedef0e26a--------------------------------)
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '[kaitchup.substack.com](https://kaitchup.substack.com/?source=post_page-----fcbedef0e26a--------------------------------)'
