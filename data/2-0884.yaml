- en: 'Feature Transformations: A Tutorial on PCA and LDA'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征变换：PCA和LDA教程
- en: 原文：[https://towardsdatascience.com/feature-transformations-a-tutorial-on-pca-and-lda-1ac160088092](https://towardsdatascience.com/feature-transformations-a-tutorial-on-pca-and-lda-1ac160088092)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/feature-transformations-a-tutorial-on-pca-and-lda-1ac160088092](https://towardsdatascience.com/feature-transformations-a-tutorial-on-pca-and-lda-1ac160088092)
- en: Reducing the dimension of a dataset using methods such as PCA
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用PCA等方法减少数据集的维度
- en: '[](https://medium.com/@PadraigC?source=post_page-----1ac160088092--------------------------------)[![Pádraig
    Cunningham](../Images/9521fb3dc64c947edf6adf2ce7b80f0f.png)](https://medium.com/@PadraigC?source=post_page-----1ac160088092--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1ac160088092--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1ac160088092--------------------------------)
    [Pádraig Cunningham](https://medium.com/@PadraigC?source=post_page-----1ac160088092--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@PadraigC?source=post_page-----1ac160088092--------------------------------)[![Pádraig
    Cunningham](../Images/9521fb3dc64c947edf6adf2ce7b80f0f.png)](https://medium.com/@PadraigC?source=post_page-----1ac160088092--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1ac160088092--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1ac160088092--------------------------------)
    [Pádraig Cunningham](https://medium.com/@PadraigC?source=post_page-----1ac160088092--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1ac160088092--------------------------------)
    ·7 min read·Jul 14, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1ac160088092--------------------------------)
    ·阅读时间7分钟·2023年7月14日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/4bb1b05b0d0163b3ecd9ab788f291fdb.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4bb1b05b0d0163b3ecd9ab788f291fdb.png)'
- en: Photo by [Nicole Cagnina](https://unsplash.com/@nicole_c?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/projection?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Nicole Cagnina](https://unsplash.com/@nicole_c?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，来源于 [Unsplash](https://unsplash.com/s/photos/projection?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: When dealing with high-dimension data, it is common to use methods such as Principal
    Component Analysis (PCA) to reduce the dimension of the data. This converts the
    data to a different (lower dimension) set of features. This contrasts with feature
    subset selection which selects a subset of the original features (see [[1]](https://medium.com/towards-data-science/feature-subset-selection-6de1f05822b0)
    for a turorial on feature selection).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理高维数据时，通常使用主成分分析（PCA）等方法来降低数据的维度。这将数据转换为不同的（较低维度的）特征集。这与特征子集选择形成对比，后者选择原始特征的子集（参见[[1]](https://medium.com/towards-data-science/feature-subset-selection-6de1f05822b0)了解特征选择的教程）。
- en: PCA is a linear transformation of the data to a lower dimension space. In this
    article we start off by explaining what a linear transformation is. Then we show
    with Python examples how PCA works. The article concludes with a description of
    Linear Discriminant Analysis (LDA) a *supervised* linear transformation method.
    Python code for the methods presented in that paper is available on [GitHub](https://github.com/PadraigC/FeatTransTutorial).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: PCA是一种将数据线性变换到较低维空间的方法。本文首先解释了什么是线性变换。然后，我们通过Python示例展示了PCA的工作原理。文章最后描述了线性判别分析（LDA），这是一种*监督*的线性变换方法。文中介绍的方法的Python代码可在
    [GitHub](https://github.com/PadraigC/FeatTransTutorial) 上找到。
- en: Linear Transformations
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性变换
- en: 'Imagine that after a holiday Bill owes Mary £5 and $15 that needs to be paid
    in euro (€). The rates of exchange are; £1 = €1.15 and $1 = €0.93\. So the debt
    in € is:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，假设在假期之后，比尔欠玛丽£5和$15，需要用欧元（€）支付。汇率为：£1 = €1.15 和 $1 = €0.93。因此，债务的€金额为：
- en: '![](../Images/c0dd42037c086d8bb90eb484a162cb76.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c0dd42037c086d8bb90eb484a162cb76.png)'
- en: Here we are converting a debt in two dimensions (£,$) to one dimension (€).
    Three examples of this are illustrated in Figure 1, the original (£5, $15) debt
    and two other debts of (£15, $20) and (£20, $35). The green dots are the original
    debts and the red dots are the debts projected into a single dimension. The red
    line is this new dimension.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将两维（£，$）的债务转换为一维（€）。图1中展示了三个例子，包括原始债务（£5，$15）和另外两个债务（£15，$20）以及（£20，$35）。绿色的点是原始债务，红色的点是投影到单一维度的债务。红色的线代表这个新维度。
- en: '![](../Images/cbf16cd8b8dd7f74d2227ab0570161a3.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cbf16cd8b8dd7f74d2227ab0570161a3.png)'
- en: '**Figure 1\.** An illustration of how converting £,$ debts to € is a linear
    transformation. Image by author.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 1。** 一个将英镑和美元债务转换为欧元的线性变换的示例。图片作者提供。'
- en: On the left in the figure we can see how this can be represented as matrix multiplication.
    The original dataset is a 3 by 2 matrix (3 samples, 2 features), the rates of
    exchange form a 1D matrix of two components and the output is a 1D matrix of 3
    components. The exchange rate matrix *is* the transformation; if the exchange
    rates are changed then the transformation changes.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在图的左侧，我们可以看到如何将其表示为矩阵乘法。原始数据集是一个 3 x 2 的矩阵（3 个样本，2 个特征），汇率形成一个 1D 矩阵的两个分量，输出是一个
    1D 矩阵的 3 个分量。汇率矩阵*是*变换；如果汇率改变，则变换也会改变。
- en: We can perform this matrix multiplication in Python using the code below. The
    matrices are represented as numpy arrays; the final line calls the `dot` method
    on the `cur` matrix to perform matrix multiplication (dot product). This will
    return the matrix `[19.7, 35.85, 55.55]`.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用下面的代码在 Python 中执行矩阵乘法。这些矩阵被表示为 numpy 数组；最后一行调用`dot`方法在`cur`矩阵上执行矩阵乘法（点积）。这将返回矩阵`[19.7,
    35.85, 55.55]`。
- en: The general format for such data transformations is shown in Figure 2\. **Y**
    is the original dataset (*n* samples, *d* features); this is reduced to *k* featuresin
    **X’** by multiplying by the transformation matrix **P** which has dimension *d*
    by *k.*
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这种数据变换的一般格式如图 2 所示。**Y** 是原始数据集（*n* 个样本，*d* 个特征）；通过乘以具有 *d* x *k* 维的变换矩阵 **P**，将其减少到
    *k* 个特征的 **X’** 中。
- en: '![](../Images/f5cdbb8952098a66272d10d823b4a790.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f5cdbb8952098a66272d10d823b4a790.png)'
- en: '**Figure 2\.** If we have a dataset **Y** of *n samples described by d features,
    this can be reduced to* k *features (****X’)*** *by multiplying by the transformation
    matrix* ***P*** *of dimension d x k.* Image by author*.*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2。** 如果我们有一个由 *n* 个样本和 *d* 个特征描述的数据集 **Y**，则可以通过乘以维度为 *d* x *k* 的变换矩阵 ***P***
    将其减少到 *k* 个特征（****X’***）。图片作者提供。'
- en: Principal Components Analysis
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主成分分析
- en: 'In general the transformation matrix **P** determines the transformation. In
    the example in Figure 1 the details of the transformation are determined by the
    rates of exchange and these are given. If we wish to use PCA to reduce the dimension
    of a dataset, how do we decide on the nature of the transformation? Well PCA is
    driven by three principles:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，变换矩阵 **P** 决定了变换。在图 1 的示例中，变换的细节由汇率决定，这些汇率是给定的。如果我们希望使用 PCA 来减少数据集的维度，我们如何决定变换的性质？嗯，PCA
    由三个原则驱动：
- en: Select a transformation that preserves the spread in the data, i.e. prefer dimensions
    that preserve distances between data points.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一种保留数据扩展的变换，即选择保留数据点之间距离的维度。
- en: Select dimensions that are orthogonal to each other (no redundancy).
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择彼此正交的维度（无冗余）。
- en: Select *k* dimensions that capture most of the variance in the data (say 90%).
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 *k* 个维度，以捕获数据中大部分的方差（例如 90%）。
- en: These principles are illustrated in Figure 3\. We have a dataset of individuals
    described by two features, waist measurement and weight. These features are correlated
    with each other so the objective is to project the data into different, uncorrelated
    dimensions without losing the ‘spread’ in the data. These new dimensions are the
    principal components that give PCA its name. As an alternative to thinking about
    this as a projection, you can think of it as *rotating* the data cloud to align
    with the red axes in Figure 3\. Either way, the new axes are PC1, the first principal
    component and PC2 which is perpendicular to PC1\. If it were felt that PC1 captured
    enough of the variation in the data then PC2 might be dropped.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这些原理在图 3 中得到了说明。我们有一个由两个特征描述的个体数据集：腰围和体重。这些特征彼此相关，因此目标是将数据投影到不同的、不相关的维度中，而不丢失数据中的‘扩展’。这些新维度就是主成分，赋予了
    PCA 其名称。作为投影的替代思路，你可以将其视为将数据云*旋转*以对齐图 3 中的红色坐标轴。无论哪种方式，新轴都是 PC1，第一个主成分和 PC2，它与
    PC1 垂直。如果认为 PC1 捕获了足够的数据变化，则可以舍弃 PC2。
- en: '![](../Images/30cb90ec4da05433963e882d8ae62a79.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/30cb90ec4da05433963e882d8ae62a79.png)'
- en: '**Figure 3\.** A 2D dataset where the features are weight and waist measurement.
    The first principle component (PC1) should be in the direction that captures most
    of the variance in the data. PC2 should be orthogonal to PC1 so that they are
    independent. Image by author.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**图3.** 一个二维数据集，其中的特征是体重和腰围。第一个主成分（PC1）应在捕获数据方差最多的方向上。PC2应与PC1正交，以便它们独立。图像由作者提供。'
- en: 'The steps to perform PCA on a data matrix **Y** as shown in Figure 2 are as
    follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对数据矩阵**Y**执行PCA的步骤如图2所示：
- en: Calculate the means and standard deviations of the columns of **Y**.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算**Y**的列的均值和标准差。
- en: Subtract the column means from each row of **Y** and divide by the standard
    deviation to create the *normalised centred matrix* **Z**.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从**Y**的每一行中减去列均值，并除以标准差以创建*标准化中心矩阵* **Z**。
- en: Calculate the covariance matrix **C** = 1/(n-1) **Zᵀ Z** where **Zᵀ** is the
    transpose of **Z**.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算协方差矩阵**C** = 1/(n-1) **Zᵀ Z**，其中**Zᵀ**是**Z**的转置。
- en: Calculate the eigenvectors and eigenvalues of the covariance matrix **C**.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算协方差矩阵**C**的特征向量和特征值。
- en: Examine the eigenvalues in descending order to determine the number of dimensions
    *k* to be retained — this is the number of principle components.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查特征值的降序以确定保留的维度数*k* — 这是主成分的数量。
- en: The top *k* eigenvectors make up the columns of the transformation matrix **P**
    which has dimension (*p* × *k*).
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 顶部的*k*特征向量构成了转换矩阵**P**的列，该矩阵的维度为（*p* × *k*）。
- en: The data is transformed by **X′ = ZP** where **X**′ has dimension (*n* × *k*).
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据通过**X′ = ZP**进行转换，其中**X**′的维度为（*n* × *k*）。
- en: In the following examples we will work with the Harry Potter TT dataset that
    is shared on [Github](https://github.com/PadraigC/FeatTransTutorial). The format
    of the data is shown below. There are 22 rows and fve descriptive features. We
    will use PCA to compress this to two dimensions.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们将使用在[Github](https://github.com/PadraigC/FeatTransTutorial)上共享的哈利·波特TT数据集。数据的格式如下所示。共有22行和五个描述性特征。我们将使用PCA将其压缩为两个维度。
- en: Python code to do this is shown below. `Y_df`is a Pandas dataframe with the
    dataset. The eigenvalues and eigenvectors (`ev,evec`) are calculated in line 8\.
    If we examine the eigenvalues they tells us the amount of variance captured in
    each PC; [49%, 31%, 11%, 5%, 4%]. The first two PCs will retain 80% of (49% +
    31%) of the variance in the data so we decide to go with two PCs. `X_dash`contains
    this data projected into two dimensions. The data projected in 2D is shown in
    Figure 4\. It could be argued that PC1 dimension represents competence/incompetence
    and the PC2 dimension represents goodness. Fred & George Weasley (twins) are plotted
    at the same point as they have exactly the same feature values in the original
    dataset.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 下面显示了执行此操作的Python代码。 `Y_df`是包含数据集的Pandas数据框。特征值和特征向量（`ev,evec`）在第8行计算。如果我们检查特征值，它们告诉我们每个主成分（PC）捕获的方差量；[49%，31%，11%，5%，4%]。前两个主成分将保留数据中80%（49%
    + 31%）的方差，因此我们决定使用两个主成分。 `X_dash`包含投影到二维的这些数据。二维投影的数据如图4所示。可以说，PC1维度代表能力/无能力，而PC2维度代表优良程度。弗雷德和乔治·韦斯利（双胞胎）被绘制在相同的点，因为他们在原始数据集中具有完全相同的特征值。
- en: '![](../Images/8140bd9901ba3831fbe1ba6a5950c832.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8140bd9901ba3831fbe1ba6a5950c832.png)'
- en: '**Figure 4\.** The Harry Potter dataset projected into two dimensions (2 PCs).
    Image by author.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**图4.** 哈利·波特数据集投影到二维（2个主成分）。图像由作者提供。'
- en: 'If we use the PCA implementation in scikit learn we can do this in three lines
    of code:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用scikit learn中的PCA实现，可以用三行代码完成这项工作：
- en: In line 4 the data is normalised, the PCA object is set up in line 5 and the
    data transformation is done in line 6\. Again, the code for this is available
    in the notebook on [Github](https://github.com/PadraigC/FeatTransTutorial).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4行中数据被标准化，第5行中设置了PCA对象，第6行中完成了数据转换。再次说明，这些代码可以在[Github](https://github.com/PadraigC/FeatTransTutorial)的笔记本中找到。
- en: Linear Discriminant Analysis
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性判别分析
- en: It should be clear that PCA is inherently an *unsupervised* ML technique in
    that it does not take any class labels into account. In fact, PCA will not necessarily
    be effective in a supervised learning context — this should not be surprising
    given the focus on maintaining the *spread* in the data without consideration
    for class labels. In Figure 4 we see how PCA does on the penguins dataset [2].
    This is a three class dataset described by four features (also available on [GitHub](https://github.com/PadraigC/FeatTransTutorial)).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 应该明确的是，PCA 本质上是一种*无监督*的机器学习技术，因为它不考虑任何类别标签。实际上，在监督学习的背景下，PCA 不一定会有效——考虑到其重点是保持数据的*分布*而不考虑类别标签，这一点并不令人惊讶。在图
    4 中，我们可以看到 PCA 在企鹅数据集 [2] 上的表现。这是一个由四个特征描述的三类数据集（也可在[GitHub](https://github.com/PadraigC/FeatTransTutorial)上找到）。
- en: '![](../Images/e6187650d55b3a67a19b4b44252761e0.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e6187650d55b3a67a19b4b44252761e0.png)'
- en: '**Figure 5\.** These scatter plots compare the performance of PCA and LDA on
    the penguins dataset. PCA does not do so well as it does not take class label
    information into account. Image by author.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 5\.** 这些散点图比较了 PCA 和 LDA 在企鹅数据集上的表现。PCA 由于未考虑类别标签信息，其表现不如 LDA。图片由作者提供。'
- en: 'On the right in Figure 5 we see how Linear Discriminant Analysis (LDA) performs
    on the same dataset. LDA takes the class labels into account and seeks a projection
    that maximises the separation between the classes. The objective is to uncover
    a transformation that will maximise between-class separation and minimise within-class
    separation. These can be calculated in two matrices **S***ᵦ* for between-class
    separation and **S***𝓌* for within-class separation:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 5 右侧，我们可以看到线性判别分析 (LDA) 在相同数据集上的表现。LDA 考虑了类别标签，并寻求一个最大化类别之间分离的投影。目标是揭示一个能够最大化类别间分离并最小化类别内分离的变换。这些可以在两个矩阵中计算：**S***ᵦ*
    代表类别间分离，**S***𝓌* 代表类别内分离：
- en: '![](../Images/5eb6c85583a67a29b962b5f60edf9f40.png)![](../Images/64a85f1afb154f360b51171e931a292e.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5eb6c85583a67a29b962b5f60edf9f40.png)![](../Images/64a85f1afb154f360b51171e931a292e.png)'
- en: 'where *n*𝒸 is the number of objects in class *c*, *μ* is the mean of all examples
    and *μ*𝒸 is the mean of all examples in class *c*:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *n*𝒸 是类别 *c* 中对象的数量，*μ* 是所有示例的均值，*μ*𝒸 是类别 *c* 中所有示例的均值：
- en: '![](../Images/b3fdafe3b26e98a1d1681038917d8179.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3fdafe3b26e98a1d1681038917d8179.png)'
- en: 'The components within these summations *μ, μ*𝒸*, xⱼ* are vectors of dimension
    *p* so **S***ᵦ* and **S***𝓌* are matrices of dimension *p* × *p*. The objectives
    of maximising between-class separation and minimising within-class separation
    can be combined into a single maximisation called the Fisher criterion:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这些求和中的组件 *μ, μ*𝒸*, xⱼ* 是维度为 *p* 的向量，因此 **S***ᵦ* 和 **S***𝓌* 是维度为 *p* × *p* 的矩阵。最大化类别间分离和最小化类别内分离的目标可以结合成一个称为
    Fisher 判别准则的单一最大化：
- en: '![](../Images/ad7cc6716c5572688b96e8ea797df542.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad7cc6716c5572688b96e8ea797df542.png)'
- en: This formulation and the task of finding the best **W***ˡᵈᵃ*matrix is discussed
    in more detail in [3]. For now we just need to recognise that **W***ˡᵈᵃ* has the
    same role as the **P** matrix in PCA. It has dimension *p* × *k* and will project
    the data into a *k* dimension space that maximises between class separation and
    minimises within class separation. The objectives are represented by the two **S**
    matrices. We see on the right in Figure 5 that it does a pretty good job.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这种表述以及找到最佳 **W***ˡᵈᵃ* 矩阵的任务在 [3] 中有更详细的讨论。现在我们只需认识到 **W***ˡᵈᵃ* 的作用与 PCA 中的 **P**
    矩阵相同。它的维度是 *p* × *k*，将数据投影到一个 *k* 维空间中，以最大化类别间分离并最小化类别内分离。目标由两个 **S** 矩阵表示。我们可以看到图
    5 右侧，它做得相当不错。
- en: While the inner workings of LDA might seem complicated, it is very straightforward
    in scikit-learn as can be seen in the code block below. This is very similar to
    the PCA code above; the main difference is that the `y`target variable is considered
    when the LDA is fitted to the data; this is not the case with PCA.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 LDA 的内部工作机制可能看起来复杂，但在 scikit-learn 中它非常简单，如下面的代码块所示。这与上述的 PCA 代码非常相似；主要区别在于在拟合
    LDA 时会考虑 `y` 目标变量；而 PCA 并非如此。
- en: Conclusion
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: The objective for this article was to explain the general principles underlying
    data transformations, to show how PCA and LDA work in scikit-learn and to present
    some examples of these methods in operation.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的目标是解释数据转换的基本原理，展示 PCA 和 LDA 在 scikit-learn 中的工作方式，并展示这些方法的一些实际示例。
- en: The code and data for these examples is available on on [GitHub](https://github.com/PadraigC/FeatTransTutorial)).
    A more in-depth treatment of these methods is presented in this *arXiv* report
    [3].
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这些示例的代码和数据可以在 [GitHub](https://github.com/PadraigC/FeatTransTutorial) 上找到。关于这些方法的更深入处理在此
    *arXiv* 报告 [3] 中进行了介绍。
- en: References
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] P. Cunningham, “Feature Subset Selection”, Towards Data Science, 2022,
    [Online], [https://towardsdatascience.com/feature-subset-selection-6de1f05822b0](/feature-subset-selection-6de1f05822b0)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] P. Cunningham, “特征子集选择”，Towards Data Science，2022年，[在线]，[https://towardsdatascience.com/feature-subset-selection-6de1f05822b0](/feature-subset-selection-6de1f05822b0)'
- en: '[2] A.M. Horst, A.P. Hill, K.B. Gorman KB, palmerpenguins: Palmer Archipelago
    (Antarctica) penguin data, 2020, [doi:10.5281/zenodo.3960218](https://doi.org/10.5281/zenodo.3960218),
    R package version 0.1.0, [https://allisonhorst.github.io/palmerpenguins/](https://allisonhorst.github.io/palmerpenguins/).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] A.M. Horst, A.P. Hill, K.B. Gorman KB, palmerpenguins: Palmer Archipelago（南极洲）企鹅数据，2020年，[doi:10.5281/zenodo.3960218](https://doi.org/10.5281/zenodo.3960218)，R
    包版本 0.1.0，[https://allisonhorst.github.io/palmerpenguins/](https://allisonhorst.github.io/palmerpenguins/)'
- en: '[3] P. Cunningham, B. Kathirgamanathan, & S.J. Delany, Feature selection tutorial
    with python examples, 2021, [*arXiv preprint arXiv:2106.06437*.](https://arxiv.org/abs/2106.06437)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] P. Cunningham, B. Kathirgamanathan, & S.J. Delany, 特征选择教程（带有 Python 示例），2021年，[*arXiv
    预印本 arXiv:2106.06437*](https://arxiv.org/abs/2106.06437)'
