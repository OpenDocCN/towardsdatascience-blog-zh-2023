- en: 'LLM and GNN: How to Improve Reasoning of Both AI Systems on Graph Data'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM 和 GNN：如何提升两种 AI 系统在图数据上的推理能力
- en: 原文：[https://towardsdatascience.com/llm-and-gnn-how-to-improve-reasoning-of-both-ai-systems-on-graph-data-5ebd875eef30](https://towardsdatascience.com/llm-and-gnn-how-to-improve-reasoning-of-both-ai-systems-on-graph-data-5ebd875eef30)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/llm-and-gnn-how-to-improve-reasoning-of-both-ai-systems-on-graph-data-5ebd875eef30](https://towardsdatascience.com/llm-and-gnn-how-to-improve-reasoning-of-both-ai-systems-on-graph-data-5ebd875eef30)
- en: '[](https://medium.com/@alcarazanthony1?source=post_page-----5ebd875eef30--------------------------------)[![Anthony
    Alcaraz](../Images/6a71a1752677bd07c384246fb0c7f7e8.png)](https://medium.com/@alcarazanthony1?source=post_page-----5ebd875eef30--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5ebd875eef30--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5ebd875eef30--------------------------------)
    [Anthony Alcaraz](https://medium.com/@alcarazanthony1?source=post_page-----5ebd875eef30--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@alcarazanthony1?source=post_page-----5ebd875eef30--------------------------------)[![Anthony
    Alcaraz](../Images/6a71a1752677bd07c384246fb0c7f7e8.png)](https://medium.com/@alcarazanthony1?source=post_page-----5ebd875eef30--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5ebd875eef30--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5ebd875eef30--------------------------------)
    [Anthony Alcaraz](https://medium.com/@alcarazanthony1?source=post_page-----5ebd875eef30--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5ebd875eef30--------------------------------)
    ·9 min read·Dec 3, 2023
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5ebd875eef30--------------------------------)
    ·9 分钟阅读·2023年12月3日
- en: --
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '*Artificial intelligence software was used to enhance the grammar, flow, and
    readability of this article’s text.*'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*人工智能软件被用来提升本文文本的语法、流畅性和可读性。*'
- en: Graph neural networks (GNNs) and large language models (LLMs) have emerged as
    two major branches of artificial intelligence, achieving immense success in learning
    from graph-structured and natural language data respectively.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 图神经网络（GNNs）和大型语言模型（LLMs）已成为人工智能的两个主要分支，分别在从图结构数据和自然语言数据中学习方面取得了巨大成功。
- en: As graph-structured and natural language data become increasingly interconnected
    in real-world applications, there is a growing need for artificial intelligence
    systems that can perform multi-modal reasoning.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 随着图结构和自然语言数据在现实应用中越来越相互关联，对能够进行多模态推理的人工智能系统的需求也在增长。
- en: This article explores integrated graph-language architectures that combine the
    complementary strengths of graph neural networks (GNNs) and large language models
    (LLMs) for enhanced analysis.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本文探讨了结合图神经网络（GNNs）和大型语言模型（LLMs）互补优势的集成图语言架构，以提升分析能力。
- en: Real-world scenarios often involve interconnected data with both structural
    and textual modalities. This brings forth the need for integrated architectures
    that can perform multi-faceted reasoning by unifying the complementary strengths
    of GNNs and LLMs.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界场景通常涉及具有结构性和文本性模式的互联数据。这就带来了对能够通过统一 GNNs 和 LLMs 互补优势进行多方面推理的集成架构的需求。
- en: Specifically, while GNNs leverage message passing over graphs to aggregate local
    patterns, node embeddings are limited in their ability to capture rich features.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，虽然 GNNs 利用图上的消息传递来聚合局部模式，但节点嵌入在捕捉丰富特征方面存在局限性。
- en: In contrast, LLMs exhibit exceptional semantic reasoning capabilities but struggle
    with relational reasoning over structured topology inherently understood by GNNs.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，LLMs 展现出卓越的语义推理能力，但在处理 GNNs 自然理解的结构化拓扑关系时表现欠佳。
- en: Fusing the two paradigms allows more contextual, informed analysis.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 融合这两种范式可以实现更具上下文的、有信息的分析。
- en: Recently, integrated graph-language architectures that synergize the complementary
    strengths of GNN encoders and LLM decoders have gained prominence.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，融合了 GNN 编码器和 LLM 解码器互补优势的图语言集成架构越来越受到关注。
- en: 'As summarized in a survey paper (Li et al. 2023), these integrated approaches
    can be categorized based on the role played by LLMs:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 正如综述论文（Li et al. 2023）总结的，这些集成方法可以根据 LLMs 所扮演的角色进行分类。
- en: '[](https://synthical.com/article/3c4812bb-0cde-43fb-a861-4bc7cd978b7b?source=post_page-----5ebd875eef30--------------------------------)
    [## A Survey of Graph Meets Large Language Model: Progress and Future Directions'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://synthical.com/article/3c4812bb-0cde-43fb-a861-4bc7cd978b7b?source=post_page-----5ebd875eef30--------------------------------)
    [## 图与大型语言模型的汇合调研：进展与未来方向'
- en: Graph plays a significant role in representing and analyzing complex relationships
    in real-world applications such as…
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图在表示和分析复杂关系方面发挥了重要作用，如…
- en: synthical.com](https://synthical.com/article/3c4812bb-0cde-43fb-a861-4bc7cd978b7b?source=post_page-----5ebd875eef30--------------------------------)
    ![](../Images/e41a7623e0d61855d91f797115c5d231.png)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: synthical.com](https://synthical.com/article/3c4812bb-0cde-43fb-a861-4bc7cd978b7b?source=post_page-----5ebd875eef30--------------------------------)
    ![](../Images/e41a7623e0d61855d91f797115c5d231.png)
- en: Li et al. 2023
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Li et al. 2023
- en: '**LLM as Enhancer:** LLMs strengthen node embeddings and textual features to
    boost GNN performance on text-attributed graphs. Techniques apply either explanation-based
    enhancement that leverages additional LLM-generated information or directly output
    embedding-based enhancements.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**LLM作为增强器：**LLMs通过强化节点嵌入和文本特征来提升GNN在文本属性图上的性能。技术方法包括基于解释的增强，这种方法利用额外的LLM生成的信息，或直接输出基于嵌入的增强。'
- en: '**LLM as Predictor:** Leverages the generative capabilities of LLMs to make
    predictions on graphs. Strategies either flatten graphs into sequential text descriptions
    or employ GNNs for structure encoding before LLM predictions.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**LLM作为预测器：**利用LLMs的生成能力对图进行预测。策略要么是将图展平成顺序文本描述，要么在LLM预测之前采用GNN进行结构编码。'
- en: '**GNN-LLM Alignment:** Focuses on aligning the vector spaces of GNN and LLM
    encoders for coordinated analyzed. Alignment can be symmetric with equal emphasis
    or asymmetric prioritizing certain modalities.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**GNN-LLM对齐：**专注于对齐GNN和LLM编码器的向量空间以实现协调分析。对齐可以是对称的，强调平等，或是不对称的，优先考虑某些模式。'
- en: The core motivation is effectively fusing the relational modeling strengths
    of graphs with the contextual reasoning capacity of language models. This shows
    great promise in lifting analysis abilities on interconnected data combining both
    structure and semantics.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 核心动机在于有效地将图的关系建模优势与语言模型的上下文推理能力相融合。这在提升对结合了结构和语义的互联数据的分析能力方面显示了巨大的潜力。
- en: '![](../Images/df4864411c0a2d1ee62c13b1eb562f7e.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df4864411c0a2d1ee62c13b1eb562f7e.png)'
- en: Li et al. 2023
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Li et al. 2023
- en: 'Reasoning Challenges :'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推理挑战：
- en: '**For LLMs**'
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**对于LLMs**'
- en: While large language models (LLMs) have achieved impressive performance on a
    wide variety of natural language tasks, their reasoning ability is constrained
    when it comes to graph-structured data.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大型语言模型（LLMs）在各种自然语言任务上取得了令人印象深刻的成绩，但在处理图结构数据时，其推理能力仍然受到限制。
- en: This is because most graphs lack an intrinsic sequential structure that LLMs
    can process. For instance, social networks, molecular data, or knowledge graphs
    define complex relationships between entities that cannot be easily flattened
    into a text description.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为大多数图缺乏LLMs可以处理的固有顺序结构。例如，社交网络、分子数据或知识图谱定义了复杂的实体关系，这些关系无法轻易展平成文本描述。
- en: As such, LLMs struggle to effectively incorporate positional and relational
    dependencies based on the graph topology into their reasoning process. Without
    straightforward conversion of nodes and edges into words/tokens, LLMs cannot perceive
    insights like neighborhood similarities, community structures, or multi-hop connections
    that facilitate graph analysis.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，LLMs在将基于图的拓扑的位置信息和关系依赖有效地融入其推理过程中存在困难。由于无法将节点和边直接转换为单词/标记，LLMs无法感知像邻域相似性、社区结构或多跳连接等有助于图分析的见解。
- en: '**For GNNs**'
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**对于GNNs**'
- en: On the other hand, graph neural networks (GNNs) are designed to aggregate local
    neighborhood information around each node by message passing between connected
    nodes. This allows them to uncover patterns and roles of nodes based on graph
    positions.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，图神经网络（GNNs）旨在通过节点之间的消息传递来聚合每个节点周围的局部邻域信息。这使得它们能够基于图的位置发现节点的模式和角色。
- en: However, reasoning is largely confined to individual nodes and their immediate
    neighbors. Capturing longer range dependencies across far-off nodes remains difficult
    for standard GNN architectures.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，推理在很大程度上被限制在单个节点及其直接邻居。捕捉远离节点的较长距离依赖性对于标准GNN架构仍然很困难。
- en: More importantly, GNNs rely on fixed-sized vector representations of nodes that
    constrain their ability to express complex semantics. Without the ability to process
    rich textual features, the reasoning capacity gets bottlenecked on the graph side
    as well.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是，GNN 依赖于固定大小的节点向量表示，这限制了它们表达复杂语义的能力。在缺乏处理丰富文本特征的能力的情况下，推理能力也会在图形端遇到瓶颈。
- en: I. Enhancer, Predictor or Alignment
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: I. 增强器、预测器或对齐
- en: 1\. LLM as Enhancer
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. LLM 作为增强器
- en: This category of techniques focuses on using the knowledge and contextual understanding
    of large language models (LLMs) to enhance the learning process of graph neural
    networks (GNNs), specifically on text-attributed graphs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这一类技术专注于利用大语言模型（LLMs）的知识和上下文理解来增强图神经网络（GNNs）的学习过程，特别是在文本属性图上。
- en: The core motivation is that while GNNs are specialized for topological analysis,
    they rely on limited textual node embeddings. Augmenting features through external
    semantic knowledge from language models provides a pathway for boosted performance.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 核心动机是，虽然 GNN 专注于拓扑分析，但它们依赖于有限的文本节点嵌入。通过语言模型的外部语义知识增强特征提供了性能提升的途径。
- en: '**Explanation-Based Enhancement**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于解释的增强**'
- en: A class of approaches prompt LLMs to generate additional node explanations,
    descriptors or labels to enrich textual attributes. These supplement the existing
    text data to allow improved embedding. For example, an LLM may output research
    area tags for papers in a citation dataset.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一类方法提示 LLM 生成额外的节点解释、描述符或标签，以丰富文本属性。这些补充现有文本数据，以改进嵌入。例如，LLM 可能会为引用数据集中的论文输出研究领域标签。
- en: '**Embedding-Based Enhancement**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于嵌入的增强**'
- en: Alternatively, LLMs can directly output enhanced text embeddings fed into GNNs
    instead of generic word vectors. Fine-tuning strategies like graph-based training
    or adapter layers allow injecting topology-awareness. By processing text through
    task-tuned LLMs first, the linguistic expressivity for the graph model can be
    massively elevated.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，LLM 可以直接输出增强的文本嵌入，供 GNN 使用，而不是通用词向量。像图基训练或适配器层这样的微调策略允许注入拓扑感知。通过任务调优的 LLM
    处理文本，图模型的语言表达能力可以大大提升。
- en: 2\. LLM as Predictor
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. LLM 作为预测器
- en: This branch focuses on empowering the predictive capability of LLMs by encoding
    graph-structured data such that language models can effectively leverage their
    self-attention to uncover insights.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这一分支专注于通过对图结构数据进行编码来提升 LLM 的预测能力，使语言模型能够有效利用自注意力来揭示洞察。
- en: '**Graph Flattening**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**图形展平**'
- en: A common technique is flattening graphs into sequential node descriptions similar
    to sentences via natural language templates. For example, a paper citation network
    can transform into mentions of research papers with directed “cites” connections.
    The sequential linearization allows direct application of LLM architectures.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的技术是通过自然语言模板将图形展平为类似句子的顺序节点描述。例如，论文引用网络可以转化为带有定向“引用”连接的研究论文提及。顺序线性化允许直接应用
    LLM 架构。
- en: '**GNN Fusion**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**GNN 融合**'
- en: For tighter integration, GNN encoders can be used to extract topological representations
    first, which are then fused with token embeddings within the LLM to leverage both
    modalities. The LLM then makes predictions over the consolidated embedding.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更紧密的集成，GNN 编码器可以先提取拓扑表示，然后将其与 LLM 中的标记嵌入融合，以利用两种模态。然后，LLM 对合并后的嵌入进行预测。
- en: 3\. GNN-LLM Alignment
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. GNN-LLM 对齐
- en: This category focuses specifically on techniques to align the vector spaces
    of GNN and LLM encoders for improved consolidated reasoning, while retaining their
    specialized roles.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这一类别特别关注于对齐 GNN 和 LLM 编码器的向量空间，以改进合并推理，同时保留其专门角色。
- en: '**Symmetric Alignment**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**对称对齐**'
- en: Methods like contrastive representation learning given aligned graph-text pairs
    treat each encoder equally during training.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对齐的图-文本对下的对比表示学习方法在训练过程中对每个编码器进行平等待遇。
- en: '**Asymmetric Alignment**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**非对称对齐**'
- en: Approaches for injecting structural knowledge directly into LLM encoders via
    auxiliary tuned graph layers or distillation asymmetricaly target enhancing language
    reasoning.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通过辅助调优的图层或蒸馏方法直接将结构知识注入 LLM 编码器，非对称地增强语言推理能力。
- en: '**II. Integrating Graph Structure with Text Semantics**'
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**II. 将图结构与文本语义集成**'
- en: To overcome the individual reasoning limitations of both graphs and language
    models, an effective approach is to integrate GNN and LLM modules together into
    an end-to-end trainable architecture.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服图和语言模型各自的推理局限性，一个有效的方法是将 GNN 和 LLM 模块整合到一个端到端可训练的架构中。
- en: The key insight is to allow both components to complement each other instead
    of work in isolation — fusing the topological modeling strengths of graphs with
    the contextual reasoning capacity of language models.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的见解是允许两个组件互补而非孤立工作——将图的拓扑建模优势与语言模型的上下文推理能力融合起来。
- en: This enables enhanced collective reasoning by jointly learning over the two
    modalities rather than using them in a decoupled manner. Specifically, the graph
    encoder leverages message passing to produce representations of structural properties
    like node neighborhoods, communities, roles and positions.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这通过在两种模式上共同学习来实现增强的集体推理，而不是以解耦的方式使用它们。具体来说，图编码器利用消息传递生成节点邻域、社区、角色和位置等结构属性的表示。
- en: Meanwhile, the text decoder utilizes self-attention over sequential tokens along
    with pre-trained knowledge to generate inferences grounded in rich semantic features.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，文本解码器利用对顺序标记的自注意力以及预训练的知识，生成基于丰富语义特征的推断。
- en: '**GNN-LLM Fusion Architecture**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**GNN-LLM 融合架构**'
- en: 'A typical high-level blueprint contains three key components:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的高级蓝图包含三个关键组件：
- en: '**Graph Encoder:** A GNN like Graph Convolutional Networks (GCNs) or Graph
    Attention Networks (GATs) that outputs node embeddings capturing topology.'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**图编码器：** 像图卷积网络（GCNs）或图注意力网络（GATs）这样的 GNN，它输出捕捉拓扑的节点嵌入。'
- en: '**Inter-Modal Projector:** A cross-modal alignment module like contrastive
    learning that maps graph and text vectors into a common embedding space.'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**跨模态投影器：** 像对比学习这样的跨模态对齐模块，将图和文本向量映射到一个共同的嵌入空间。'
- en: '**Language Decoder:** An LLM like BERT that performs token-level reasoning
    on the fused graph-text representations from the projector.'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**语言解码器：** 像 BERT 这样的 LLM，在来自投影器的融合图文表示上进行标记级推理。'
- en: By encoding structure and semantics separately in their native formats, and
    fusing via alignment, the strengths of both graphs and language can be unified
    within an integrated reasoning system. The joint end-to-end learning allows appropriate
    mixing of signals.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分别以其本地格式编码结构和语义，并通过对齐融合，可以将图和语言的优势统一在一个集成的推理系统中。联合的端到端学习允许适当的信号混合。
- en: III. Strategies for Improved Reasoning
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: III. 改进推理的策略
- en: '**Prompt-based Reformulation**'
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**基于提示的重新表述**'
- en: Carefully designing prompts that describe key graph concepts like nodes, edges,
    connectivity, positions etc. in a natural language format allows shifting the
    structural graph domain to one that plays to the strengths of large language models
    (LLMs).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细设计描述图关键概念（如节点、边、连接、位置等）的提示，以自然语言格式使结构图领域转移到发挥大型语言模型（LLMs）优势的领域。
- en: By mapping graph components into words/tokens, complex topology and relationships
    can be translated into sequences that LLM architectures are inherently designed
    to process and reason over. This facilitates transferring of reasoning patterns
    between the two modalities.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将图组件映射到词汇/标记，复杂的拓扑和关系可以转化为 LLM 架构本身设计用来处理和推理的序列。这有助于在两种模式之间转移推理模式。
- en: For instance, a paper citation graph could be portrayed through mentions of
    papers along with “cites” or “cited by” relationships.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，论文引用图可以通过提及论文以及“引用”或“被引用”关系来描绘。
- en: '**Multi-hop Neighbor Description**'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**多跳邻居描述**'
- en: LLMs are limited in their ability to aggregate global graph structure and capture
    long-range dependencies due to sequence length constraints. Describing multi-hop
    neighbors for each node provides additional contextual information about extended
    network positions and roles.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 由于序列长度的限制，LLMs 在聚合全局图结构和捕捉长距离依赖关系方面有限。描述每个节点的多跳邻居提供了有关扩展网络位置和角色的额外上下文信息。
- en: By flexibly increasing the hop limit and recursively integrating further nodes,
    LLMs can learn to mimic the aggregation process of graph neural networks, enabling
    locality as well as globality aware representations.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通过灵活地增加跳数限制并递归地整合进一步的节点，LLMs 可以学习模仿图神经网络的聚合过程，实现局部性和全局性意识的表示。
- en: '**In-Context Learning**'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**上下文学习**'
- en: Demonstrating graph analysis through step-by-step reasoning over examples allows
    steering LLM graph understanding in an interpretable direction. By feeding premises
    and conclusions of tasks alongside explanations, LLM generations can mimic and
    chain such logical processes.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 通过逐步推理示例演示图分析，可以引导LLM图理解朝着可解释的方向发展。通过提供任务的前提和结论以及解释，LLM生成可以模仿并链式处理这些逻辑过程。
- en: This technique of learning from contextual demonstrations allows gaining more
    coherent and sound graph reasoning abilities. Fine-tuning over such data leads
    to stronger inductive bias.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这种从上下文示例中学习的技术可以获得更连贯和可靠的图推理能力。对这些数据进行微调会带来更强的归纳偏置。
- en: '**Interpretable Fine-tuning**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**可解释的微调**'
- en: Strategies like adapter layers and prompt-based tuning allow precisely injecting
    structural knowledge into language models while maintaining overall model interpretability
    due to isolated adaptation. By anchoring customizations to certain layers only,
    reasoning capacity can be lifted without losing linguistic coherence.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 诸如适配器层和基于提示的调优等策略允许将结构知识精确地注入到语言模型中，同时保持模型的整体可解释性，因为这种适应是隔离的。通过仅将定制锚定到某些层，可以提升推理能力而不会丧失语言一致性。
- en: IV. Future Outlook
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: IV. 未来展望
- en: '**Hierarchical Reasoning with LLM Controllers**'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**带有LLM控制器的层次推理**'
- en: Looking beyond simply fusing GNNs and LLMs, formulating language models as meta-controllers
    that can selectively delegate to and coordinate between the most optimal graph,
    text, and other specialized modules promises more sophisticated reasoning.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 超越仅仅融合GNN和LLM，将语言模型制定为能够选择性地委托和协调最优图形、文本及其他专业模块的元控制器，承诺提供更复杂的推理能力。
- en: Based on hierarchical task decomposition, LLMs can plan computational paths
    through available AI components, combining strengths in a dynamic flow. This also
    builds towards more human-like problem solving architectures.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 基于层次任务分解，LLM可以通过可用的AI组件规划计算路径，在动态流中结合各自的优势。这也朝着更具人类解决问题特征的架构发展。
- en: For instance, a recommendation system could use LLMs to break down objectives,
    leverage GNN user encoders, apply vision tools for item analysis and finally fuse
    signals for outcomes.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个推荐系统可以使用LLM来分解目标，利用GNN用户编码器，应用视觉工具进行项目分析，最后融合信号以获得结果。
- en: '**Transferable Graph-Centric Pre-training**'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**可转移的图中心预训练**'
- en: A persistent challenge for graph neural networks is poor generalization across
    domains owing to varying structure and schemas. Pre-training GNN models on large
    corpora of representative graphs before downstream fine-tuning mitigates this
    issue.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图神经网络面临的一个持续挑战是由于结构和模式的变化而导致的领域间泛化能力差。在下游微调之前在大量代表性图数据上预训练GNN模型可以缓解这个问题。
- en: Similarly pre-training strategies customized for graph topological patterns
    and injections into language models also needs exploration for improving reasoning
    transferability across graph tasks.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，需要探索定制的预训练策略，以适应图拓扑模式并注入到语言模型中，以提高跨图任务的推理可转移性。
- en: '**Evaluating LLM Graph Expressivity**'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**评估LLM的图表达能力**'
- en: Given the dominance of LLMs for language domains, analyzing their theoretical
    expressive power w.r.t fundamental graph functions also offers a research direction.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于LLM在语言领域的主导地位，分析其在基本图函数方面的理论表达能力也提供了一个研究方向。
- en: For instance, the 1-WL test is used to evaluate GNN expressivity. Do LLMs enhanced
    with certain structural injections match or exceed such baselines? Element-level
    and network-level evaluations can quantify this.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，1-WL测试用于评估GNN的表达能力。增强的LLM与这种基准匹配或超越了吗？元素级和网络级的评估可以量化这一点。
- en: '**Towards Shared Representations**'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**共享表示的方向**'
- en: Looking beyond model-specific optimizations, creating shared vector spaces allowing
    seamless consolidation of signals from graphical and textual modalities provides
    the most flexible reasoning foundation.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 超越模型特定的优化，创建共享向量空间以便无缝整合来自图形和文本模态的信号提供了最灵活的推理基础。
- en: Balancing specificity and commonality when aligning encoders, and synergizing
    signals during joint analysis offers an intriguing path forward.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在对齐编码器时平衡特异性和共性，以及在联合分析过程中协同信号提供了一个有趣的前进道路。
- en: Conclusion
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: '**The Need for Multi-Paradigm Reasoning**'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**多范式推理的需求**'
- en: To recap, real-world data is increasingly becoming interconnected, with both
    graph-structured representations of relationships between entities as well as
    text-based information associated with nodes. This drives the need for artificial
    intelligence techniques that can perform multi-faceted reasoning spanning both
    topological as well as semantic domains.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，现实世界的数据正日益互联，包括实体之间关系的图结构表示以及与节点相关的基于文本的信息。这推动了对能够在拓扑和语义领域进行多方面推理的人工智能技术的需求。
- en: Neither pure graph-centric or text-centric approaches can fully address the
    complexities of such interconnected data alone. This necessitates integrated architectures
    unifying multiple specialized modalities.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 纯粹的图中心或文本中心的方法都不能单独完全解决这种互联数据的复杂性。这需要统一多种专业化模态的集成架构。
- en: '**Complementary Strengths of GNNs and LLMs**'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**图神经网络与大语言模型的互补优势**'
- en: Among the two most dominant analysis paradigms currently are graph neural networks,
    which excel at computational patterns over graph topologies, and large language
    models that exhibit extreme reasoning capacity over textual concepts.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当前最主导的两种分析范式是图神经网络，这些网络在图拓扑上的计算模式表现出色，以及大语言模型，这些模型在文本概念上展示了极高的推理能力。
- en: Fusing both offers an opportunity to combine topological proficiency with semantic
    expression effectively within a joint model, providing more coherent multi-paradigm
    reasoning.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 将两者融合提供了在联合模型中有效结合拓扑能力和语义表达的机会，从而提供更连贯的多范式推理。
- en: '**Ongoing Exploration of Techniques**'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**技术的持续探索**'
- en: As covered in this article, diverse techniques leveraging large language models
    as enhancers, predictors as well as for alignment with graph neural networks provide
    promising initial routes towards this harmonization goal. Each approach contributes
    unique strategies with their respective strengths and limitations.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本文所述，利用大语言模型作为增强器、预测器以及与图神经网络对齐的多种技术提供了有前景的初步途径来实现这一协调目标。每种方法都贡献了独特的策略，具有各自的优势和局限性。
- en: Hierarchical techniques, optimized pre-training strategies and consolidated
    reasoning over shared representations offer intriguing forward-looking pathways
    as graph-text synergies continue being explored to address interconnected intelligence
    needs.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 层次化技术、优化的预训练策略以及在共享表示上的综合推理提供了有趣的前瞻性路径，因为图与文本的协同效应继续被探索，以满足互联智能的需求。
- en: '![](../Images/d7e9a9ffc786492a1c30e2ac73fb589e.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d7e9a9ffc786492a1c30e2ac73fb589e.png)'
- en: Image by the author
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
