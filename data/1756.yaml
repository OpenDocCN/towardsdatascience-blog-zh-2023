- en: Why are language models everywhere?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么语言模型无处不在？
- en: 原文：[https://towardsdatascience.com/why-are-language-models-everywhere-36d9961dd9e1?source=collection_archive---------6-----------------------#2023-05-26](https://towardsdatascience.com/why-are-language-models-everywhere-36d9961dd9e1?source=collection_archive---------6-----------------------#2023-05-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/why-are-language-models-everywhere-36d9961dd9e1?source=collection_archive---------6-----------------------#2023-05-26](https://towardsdatascience.com/why-are-language-models-everywhere-36d9961dd9e1?source=collection_archive---------6-----------------------#2023-05-26)
- en: The answer lies in the 75 years of NLP history
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 答案在于75年的自然语言处理历史
- en: '[](https://medium.com/@dataemporium?source=post_page-----36d9961dd9e1--------------------------------)[![Ajay
    Halthor](../Images/1be821c8d8ed336b9ecedcf94f960ede.png)](https://medium.com/@dataemporium?source=post_page-----36d9961dd9e1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----36d9961dd9e1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----36d9961dd9e1--------------------------------)
    [Ajay Halthor](https://medium.com/@dataemporium?source=post_page-----36d9961dd9e1--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@dataemporium?source=post_page-----36d9961dd9e1--------------------------------)[![Ajay
    Halthor](../Images/1be821c8d8ed336b9ecedcf94f960ede.png)](https://medium.com/@dataemporium?source=post_page-----36d9961dd9e1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----36d9961dd9e1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----36d9961dd9e1--------------------------------)
    [Ajay Halthor](https://medium.com/@dataemporium?source=post_page-----36d9961dd9e1--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb0a3e7e495ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-are-language-models-everywhere-36d9961dd9e1&user=Ajay+Halthor&userId=b0a3e7e495ca&source=post_page-b0a3e7e495ca----36d9961dd9e1---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----36d9961dd9e1--------------------------------)
    ·6 min read·May 26, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F36d9961dd9e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-are-language-models-everywhere-36d9961dd9e1&user=Ajay+Halthor&userId=b0a3e7e495ca&source=-----36d9961dd9e1---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb0a3e7e495ca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-are-language-models-everywhere-36d9961dd9e1&user=Ajay+Halthor&userId=b0a3e7e495ca&source=post_page-b0a3e7e495ca----36d9961dd9e1---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----36d9961dd9e1--------------------------------)
    · 6 min read · 2023年5月26日'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F36d9961dd9e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-are-language-models-everywhere-36d9961dd9e1&source=-----36d9961dd9e1---------------------bookmark_footer-----------)![](../Images/fcff59340752837048b7aa666ca9d1ec.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F36d9961dd9e1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fwhy-are-language-models-everywhere-36d9961dd9e1&source=-----36d9961dd9e1---------------------bookmark_footer-----------)![](../Images/fcff59340752837048b7aa666ca9d1ec.png)'
- en: Photo by [Romain Vignes](https://unsplash.com/@rvignes?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Romain Vignes](https://unsplash.com/@rvignes?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) 提供
- en: Have you ever wondered about how we got here with [ChatGPT](https://youtu.be/NpmnWgQgcsA)
    and Large Language Models? The answer lies in the development of Natural Language
    Processing (NLP) itself. So let’s talk about it. Don’t worry; the history is more
    interesting than you think! Section 1 will describe the birth of AI and NLP. Section
    2 will talk about the major pillars of the field. Sections 3 through 5 will go
    into detailed timelines for the past 75 years. And for the final section 6, we
    describe the convergence of all these fields into *language modeling* which has
    become so popular today!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否曾经想过我们是如何来到今天的 [ChatGPT](https://youtu.be/NpmnWgQgcsA) 和大型语言模型的？答案在于自然语言处理（NLP）本身的发展。所以让我们来谈谈这个话题。别担心，历史比你想象的更有趣！第一部分将描述人工智能和NLP的诞生。第二部分将讨论该领域的主要支柱。第三至第五部分将详细介绍过去75年的时间线。最后，第六部分，我们将描述这些领域如何汇聚成今天如此流行的*语言建模*！
- en: 1 Genesis
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 起源
- en: In the beginning, there was Alan Turing’s 1950 publication [*Computing Machinery
    and Intelligence*](https://phil415.pbworks.com/f/TuringComputing.pdf) where he
    posits the question “Can machines think”. This paper is often touted as the birth
    of Artificial Intelligence. Although it did not talk about natural language explicitly,
    it laid the groundwork for future research in NLP. This is why the earliest works
    in NLP spring up in the 1950s.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，是艾伦·图灵1950年的发表论文 [*计算机械与智能*](https://phil415.pbworks.com/f/TuringComputing.pdf)，在其中他提出了“机器能思考吗”的问题。这篇论文常被誉为人工智能的诞生。尽管它没有明确讨论自然语言，但它为未来的NLP研究奠定了基础。这也是为什么NLP的最早研究出现在1950年代的原因。
- en: 2 Pillars of NLP
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 NLP的支柱
- en: '**Machine Translation**: This is when an AI takes in a sentence of one language
    and outputs a sentence in another language. For example, Google Translate.'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**机器翻译**：这是指人工智能将一种语言的句子输入，并输出另一种语言的句子。例如，Google翻译。'
- en: '**Speech Processing**: AI takes an audio as input…'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**语音处理**：人工智能将音频作为输入……'
