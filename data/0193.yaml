- en: Cᵥ Topic Coherence Explained
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Cᵥ 话题连贯性解释
- en: 原文：[https://towardsdatascience.com/c%E1%B5%A5-topic-coherence-explained-fc70e2a85227?source=collection_archive---------3-----------------------#2023-01-12](https://towardsdatascience.com/c%E1%B5%A5-topic-coherence-explained-fc70e2a85227?source=collection_archive---------3-----------------------#2023-01-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/c%E1%B5%A5-topic-coherence-explained-fc70e2a85227?source=collection_archive---------3-----------------------#2023-01-12](https://towardsdatascience.com/c%E1%B5%A5-topic-coherence-explained-fc70e2a85227?source=collection_archive---------3-----------------------#2023-01-12)
- en: '![](../Images/b2a235b931d6cc582afd7601f12f3759.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2a235b931d6cc582afd7601f12f3759.png)'
- en: Photo by [Emile Perron](https://unsplash.com/@emilep?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Emile Perron](https://unsplash.com/@emilep?utm_source=medium&utm_medium=referral)
    提供，发布在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Understanding the metric that correlates the highest with humans
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解与人类相关性最高的度量标准
- en: '[](https://emilrijcken.medium.com/?source=post_page-----fc70e2a85227--------------------------------)[![Emil
    Rijcken](../Images/d79e867934f45729e6590a20dcf0a440.png)](https://emilrijcken.medium.com/?source=post_page-----fc70e2a85227--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fc70e2a85227--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fc70e2a85227--------------------------------)
    [Emil Rijcken](https://emilrijcken.medium.com/?source=post_page-----fc70e2a85227--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://emilrijcken.medium.com/?source=post_page-----fc70e2a85227--------------------------------)[![Emil
    Rijcken](../Images/d79e867934f45729e6590a20dcf0a440.png)](https://emilrijcken.medium.com/?source=post_page-----fc70e2a85227--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fc70e2a85227--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fc70e2a85227--------------------------------)
    [Emil Rijcken](https://emilrijcken.medium.com/?source=post_page-----fc70e2a85227--------------------------------)'
- en: ·
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F95ae6f4e7791&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fc%E1%B5%A5-topic-coherence-explained-fc70e2a85227&user=Emil+Rijcken&userId=95ae6f4e7791&source=post_page-95ae6f4e7791----fc70e2a85227---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fc70e2a85227--------------------------------)
    ·7 min read·Jan 12, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffc70e2a85227&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fc%25E1%25B5%25A5-topic-coherence-explained-fc70e2a85227&user=Emil+Rijcken&userId=95ae6f4e7791&source=-----fc70e2a85227---------------------clap_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F95ae6f4e7791&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fc%E1%B5%A5-topic-coherence-explained-fc70e2a85227&user=Emil+Rijcken&userId=95ae6f4e7791&source=post_page-95ae6f4e7791----fc70e2a85227---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fc70e2a85227--------------------------------)
    ·7分钟阅读·2023年1月12日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffc70e2a85227&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fc%25E1%25B5%25A5-topic-coherence-explained-fc70e2a85227&user=Emil+Rijcken&userId=95ae6f4e7791&source=-----fc70e2a85227---------------------clap_footer-----------)'
- en: --
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffc70e2a85227&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fc%25E1%25B5%25A5-topic-coherence-explained-fc70e2a85227&source=-----fc70e2a85227---------------------bookmark_footer-----------)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffc70e2a85227&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fc%25E1%25B5%25A5-topic-coherence-explained-fc70e2a85227&source=-----fc70e2a85227---------------------bookmark_footer-----------)'
- en: In natural language processing (NLP), topic modeling is a popular task. The
    goal is to extract the hidden (*K*) topics in a corpus of documents. Each topic
    is a distribution over words. Typically, the *N* most probable words per topic
    represent that topic. The idea is that if the topic modeling algorithm works well,
    these top-*N* words are semantically related. The difficulty is how to evaluate
    these sets of words. Just as with any machine learning task, model evaluation
    is critical. In a large systematic study (Röder et al., 2015), Cᵥ coherence, which
    was unknown until then, was found to correlate the highest with human interpretation.
    Since then, this measure has been widely used to evaluate topic models and is
    the default setting in Gensim’s Coherencemodel. I couldn’t find an intuitive algorithm
    description online, so I wrote one. Many data scientists work with topic modeling
    to analyse their texts. Understanding the evaluation metrics will help in tuning
    both the model and evaluation metric.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）中，主题建模是一个流行的任务。目标是从文档语料库中提取隐藏的(*K*)主题。每个主题是一个单词的分布。通常，每个主题中*最有可能的N*个单词代表该主题。这个想法是，如果主题建模算法工作良好，这些前*最有可能的N*个单词在语义上是相关的。难点在于如何评估这些单词集合。正如任何机器学习任务一样，模型评估至关重要。在一项大规模系统研究（Röder等，2015）中，直到那时尚不为人知的Cᵥ连贯性被发现与人工解释的相关性最高。从那时起，这一度量被广泛用于评估主题模型，并且是Gensim的Coherencemodel中的默认设置。我在网上找不到直观的算法描述，所以写了一个。许多数据科学家使用主题建模来分析他们的文本。理解评估指标将有助于调整模型和评估指标。
- en: We will start by discussing some background information. This will lead the
    way to the study by Röder et al.. After discussing the study approach, we will
    go through the algorithm mathematically. Then, we will walk through the steps
    with an example (about roses and violets). Lastly, I will share some observations
    and considerations when using theCᵥ coherence.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先讨论一些背景信息。这将为Röder等人的研究铺平道路。在讨论了研究方法后，我们将从数学上讲解算法。然后，我们将通过一个例子（关于玫瑰和紫罗兰）逐步讲解这些步骤。最后，我将分享一些使用Cᵥ连贯性时的观察和考虑因素。
- en: Background
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: 'A long-standing problem with many potential applications far beyond NLP is
    quantifying the coherence of a set of statements. Quantitative approaches in NLP
    are typically inspired by the *distributional hypothesis*. Several researchers
    in the 1950s (Martin Joos, Zellig S. Harris and John Rupert Firth) found that
    synonyms (e.g. oculist and eye-doctor) tended to occur in the same environment
    of words (near words such as ‘eye’ or ‘examined’). Hence, the distributional hypothesis
    states that words in the same contexts tend to have similar meanings. For this
    reason, the word context in a corpus is often used to assess whether a set of
    topic words are coherent. However, there are many ways to obtain information about
    ‘the context’ and reflect this in a single score. E.g.:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一个长期存在的问题，具有远超NLP的潜在应用，是量化一组陈述的连贯性。NLP中的定量方法通常受*分布假设*的启发。1950年代的几位研究人员（Martin
    Joos、Zellig S. Harris和John Rupert Firth）发现，同义词（如oculist和eye-doctor）往往出现在相同的单词环境中（如‘eye’或‘examined’）。因此，分布假设指出，相同上下文中的单词倾向于具有相似的意义。出于这个原因，语料库中的单词上下文通常用于评估一组主题单词是否连贯。然而，有很多方法可以获取‘上下文’的信息并反映在一个分数中。例如：
- en: Do you compare (all) single topic words or sets?
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你是比较（所有）单一主题的单词还是集合？
- en: If you consider co-occurrence, do you consider this for a whole document or
    only a specific window of words? If you choose a window, how many words are in
    the window?
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你考虑共现，你是考虑整个文档还是仅仅是一个特定的单词窗口？如果选择窗口，窗口中有多少个单词？
- en: Given a comparison of words/word sets, how do you indicate how strongly these
    are semantically related?
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定单词/单词集合的比较，你如何指示这些单词在语义上相关的程度？
- en: Having collected the scores for all topics, how do you aggregate these numbers
    to reflect one score only?
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在收集了所有主题的分数之后，你如何聚合这些数字以反映一个唯一的分数？
- en: Exploring the Space of Topic Coherence
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索主题连贯性的空间
- en: 'Recognizing the considerations from above, Röder et al. (2015) define four
    dimensions that span the configuration space of coherence measures. Or in layman''s
    terms, they define coherence scores as a combination of the four considerations
    listed above. They call these dimensions:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到上述因素，Röder等人（2015）定义了四个维度，这些维度涵盖了连贯性度量的配置空间。通俗来说，他们将连贯性得分定义为上述四个考虑因素的组合。他们称这些维度为：
- en: Segmentation of word subsets,
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词子集的分段，
- en: Probability estimation,
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率估计，
- en: Confirmation measure,
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确认度量，
- en: Aggregation.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合。
- en: Then, they define potential settings for each of these dimensions. Based on
    these settings, they can make 237 912 combinations. They calculate the correlation
    between human evaluation scores of given topics and each combination. From this
    systematic study, an unknown measure appeared to have the highest correlation;
    Cᵥ coherence.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，他们为这些维度定义了潜在的设置。基于这些设置，他们可以生成237,912种组合。他们计算了给定主题的人类评估分数与每种组合之间的相关性。通过这一系统化研究，一个未知的度量表现出了最高的相关性；Cᵥ
    连贯性。
- en: Cᵥ Coherence
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Cᵥ 连贯性
- en: In cᵥ coherence, each topic word is compared with the set of all topics. A boolean
    sliding window of size 110 is used to assess whether two words co-occur. Then,
    the confirmation measure consists of direct- and indirect confirmations. For all
    *N* most probable words per topic, a ‘*word vector*’of size *N* is created in
    which each cell contains the Normalized Pointwise Mutual Information (NPMI) between
    that word and word *i, i* in{1,2,…, *N*}. Then, all the word vectors in a topic
    are aggregated into one big topic vector. The average of all the cosine similarities
    between each topic word and its topic vector (this is the segmentation) is used
    to calculate the Cᵥ score.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Cᵥ 连贯性中，每个主题词都与所有主题的集合进行比较。使用大小为110的布尔滑动窗口来评估两个词是否共现。然后，确认度量由直接确认和间接确认组成。对于每个主题的所有
    *N* 个最可能的词，创建一个大小为 *N* 的‘*词向量*’，其中每个单元包含该词与词 *i, i* 属于 {1,2,…, *N*} 之间的归一化点对点互信息
    (NPMI)。然后，将一个主题中的所有词向量聚合成一个大的主题向量。所有主题词与其主题向量之间的余弦相似度的平均值（即分割）用于计算 Cᵥ 分数。
- en: Now comes a formal description. Since Medium does not do so well with mathematical
    notation, I am pasting some notation in here from another document my group and
    I created.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在进行正式描述。由于 Medium 对数学符号的支持不佳，我在此粘贴了我和我的小组创建的另一文档中的一些符号。
- en: '![](../Images/a80e0cc020b266dbb041f94ff7745ff9.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a80e0cc020b266dbb041f94ff7745ff9.png)'
- en: The Cᵥ score is heavily based on the NPMI score, an advanced way to calculate
    the probability of two words co-occurring in a corpus.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Cᵥ 分数在很大程度上依赖于 NPMI 分数，这是一种计算两个词在语料库中共现概率的高级方法。
- en: '![](../Images/855297eb491e86340f06895325b6612a.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/855297eb491e86340f06895325b6612a.png)'
- en: The formula for Normalized Pointwise Mutual Information
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化点对点互信息的公式
- en: The epsilon is a small constant used to avoid a logarithm of zero.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: epsilon 是一个小常数，用于避免对零取对数。
- en: 'This probability is based on a sliding window *s* (*s* = 110 for Cᵥ). With
    *j* being the index of the sliding window in a document, the probabilities in
    the NPMI formula are calculated as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概率基于滑动窗口 *s* (*s* = 110 对于 Cᵥ)。其中 *j* 为文档中滑动窗口的索引，NPMI 公式中的概率计算如下：
- en: '![](../Images/a8e483968223dd7095d4d0069f074696.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8e483968223dd7095d4d0069f074696.png)'
- en: The calculation of probabilities between two words, based on a sliding window
    s
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 基于滑动窗口 s 的两个词之间的概率计算
- en: 'Based on the NPMI, a word vector with length N is created for each topic word
    (direct confirmation):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 NPMI，为每个主题词创建长度为 N 的词向量（直接确认）：
- en: '![](../Images/870c8dbf7e327fb3234c8d3dd7eeed1d.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/870c8dbf7e327fb3234c8d3dd7eeed1d.png)'
- en: The creation of a word vector (direct confirmation measure)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 词向量的创建（直接确认度量）
- en: 'For the segmentation, we have the following:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分割，我们有以下内容：
- en: '![](../Images/e422002e97514051765c0f2315ced5d5.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e422002e97514051765c0f2315ced5d5.png)'
- en: The segmentation of word subsets
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 词子集的分割
- en: 'To compare each word with the topic vector, we create *K* topic vectors as
    the sum of all *N* words per topic (mind you that the *W** refers to the same
    *w** as below:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将每个词与主题向量进行比较，我们创建 *K* 个主题向量作为每个主题中所有 *N* 个词的总和（请注意，*W** 指的是下面相同的 *w**）：
- en: '![](../Images/d4e7fe411c49409fcc77131432100db7.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d4e7fe411c49409fcc77131432100db7.png)'
- en: The topic vector
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 主题向量
- en: 'Based on the topic vector and segmentation, the cosine similarity is calculated
    for each topic word vector with the topic vector. The cosine similarity is calculated
    as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 基于主题向量和分割，计算每个主题词向量与主题向量之间的余弦相似度。余弦相似度计算如下：
- en: '![](../Images/7b70692d75a7648de8c22d8ed9227460.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b70692d75a7648de8c22d8ed9227460.png)'
- en: The cosine similarity
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度
- en: 'Then, the average of all *NxK* cosine similarities is taken to calculate the
    Cᵥ score:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，取所有 *NxK* 余弦相似度的平均值以计算 Cᵥ 分数：
- en: '![](../Images/fbf5008b2e3912c83d2fb498da74f162.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fbf5008b2e3912c83d2fb498da74f162.png)'
- en: The Cᵥ score is the average of all cosine similarities.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Cᵥ 分数是所有余弦相似度的平均值。
- en: The beautiful thing about understanding complex things is that once you understand
    them, they seem not so complex anymore. Having made it to the end of this blog,
    I hope this is the case for you. If not, we will go to plan B. Let’s walk through
    an example.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 理解复杂事物的美妙之处在于，一旦你理解了它们，它们似乎就不那么复杂了。希望在阅读完这篇博客后，你能有这样的体会。如果没有，我们将进入B计划。让我们通过一个例子来说明。
- en: Example
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例
- en: '![](../Images/952fe040185449e0fd4a7f24509b5d32.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/952fe040185449e0fd4a7f24509b5d32.png)'
- en: Furthermore, we have three topics, with two most probable words per topic (hence,
    *K = 3, N = 2):*
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们有三个主题，每个主题有两个最可能的词（因此，*K = 3, N = 2*）：
- en: '![](../Images/e0df271229a04581094e6eedb308609b.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e0df271229a04581094e6eedb308609b.png)'
- en: 'Using a sliding window size 3 (*s = 3*), we have the following windows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 使用滑动窗口大小为3 (*s = 3*)，我们有以下窗口：
- en: (’mary’,’had’,’a’), (’had’,’a’,’lamp’), (’mary’,’has’,’roses’), (’roses’,’are’,’red’),
    (’violets’,’ are’,’light’),(’are’,’light’,’blue’).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: (’mary’,’had’,’a’), (’had’,’a’,’lamp’), (’mary’,’has’,’roses’), (’roses’,’are’,’red’),
    (’violets’,’ are’,’light’),(’are’,’light’,’blue’).
- en: 'Hence, we have:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有：
- en: '![](../Images/769c3e47fcc8862e17b9cfcf920455ef.png)![](../Images/09f7556721628387e48cd75b682e60f1.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/769c3e47fcc8862e17b9cfcf920455ef.png)![](../Images/09f7556721628387e48cd75b682e60f1.png)'
- en: 'Based on these scores, we have:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些得分，我们有：
- en: '![](../Images/d9934e8aae383edff8764d9e45009fdb.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d9934e8aae383edff8764d9e45009fdb.png)'
- en: 'Hence:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 因此：
- en: '![](../Images/9e015ada0ec0917216a8083b8327adf0.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e015ada0ec0917216a8083b8327adf0.png)'
- en: 'And:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 并且：
- en: '![](../Images/c781c88c3ba8b25a616fbb058ead7992.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c781c88c3ba8b25a616fbb058ead7992.png)'
- en: 'This gives us:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们带来：
- en: '![](../Images/1c425bc9d8a02ba16b865419e75b665d.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c425bc9d8a02ba16b865419e75b665d.png)'
- en: 'So that the Cᵥ score for these topics is calculated as:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 因此这些主题的Cᵥ得分计算为：
- en: '![](../Images/1c865146708ef77e4139143465dc16e5.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c865146708ef77e4139143465dc16e5.png)'
- en: Cᵥonsiderations
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Cᵥ考量
- en: Although Cᵥ came out as the best coherence score in Röder et al.’s study, there
    are some issues in practice. The Cᵥ score negatively correlates with other coherence
    measures in [some experiments](https://github.com/dice-group/Palmetto/issues/12).
    However, the author suspects this issue might be caused by the value for epsilon,
    and the error is not further investigated in the thread. Neither am I aware of
    any peer-reviewed work discussing the issue. My conclusion is that until proven
    guilty, Cᵥ remains innocent. But it is good to be aware of this issue. Also, word-embedding-based
    topic models are commonly used now. The author claims these correlate to human
    judgment, similar to the Cᵥ score. However, in [one of my studies](https://ieeexplore.ieee.org/document/9945594),
    my group and I found different (embedding-based-) coherence scores favoured different
    topics. Hence, the last word about topic coherence remains to be spoken.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Cᵥ在Röder等人的研究中表现为最佳的一致性得分，但在实践中存在一些问题。Cᵥ得分与[一些实验](https://github.com/dice-group/Palmetto/issues/12)中的其他一致性测量值负相关。然而，作者怀疑这个问题可能是由epsilon值引起的，线程中没有进一步调查该错误。我也没有看到任何同行评审的工作讨论这个问题。我的结论是，在证明有罪之前，Cᵥ仍然是无辜的。但了解这个问题是有益的。此外，现在常用基于词嵌入的主题模型。作者声称这些模型与人类判断相关，类似于Cᵥ得分。然而，在[我的一项研究](https://ieeexplore.ieee.org/document/9945594)中，我和我的团队发现不同（基于嵌入的）一致性得分偏爱不同的主题。因此，关于主题一致性的最后结论仍需进一步讨论。
- en: If you like this work, you are likely interested in topic modeling. In that
    case, you might be interested in the following as well.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这项工作，你可能对主题建模也感兴趣。在这种情况下，你可能也会对以下内容感兴趣。
- en: '**We have created a new topic modeling algorithm called FLSA-W** (the official
    page is [here](https://ieeexplore.ieee.org/abstract/document/9660139), but you
    can see the paper [here](https://pure.tue.nl/ws/portalfiles/portal/243684581/A_Comparative_Study_of_Fuzzy_Topic_Models_and_LDA_in_terms_of_Interpretability.pdf)).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们创建了一种新的主题建模算法，称为FLSA-W**（官方页面[在这里](https://ieeexplore.ieee.org/abstract/document/9660139)，但你可以在[这里](https://pure.tue.nl/ws/portalfiles/portal/243684581/A_Comparative_Study_of_Fuzzy_Topic_Models_and_LDA_in_terms_of_Interpretability.pdf)查看论文）。'
- en: '[**FLSA-W outperforms other state-of-the-art algorithms (such as LDA, ProdLDA,
    NMF, CTM and more) on several open datasets.**](https://pure.tue.nl/ws/files/222725628/Pure_ExperimentalStudyOfFlsa_wForTopicModeling.pdf)
    **This work has been submitted but is not peer-reviewed yet.**'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[**FLSA-W在多个开放数据集上优于其他最先进的算法（如LDA、ProdLDA、NMF、CTM等）。**](https://pure.tue.nl/ws/files/222725628/Pure_ExperimentalStudyOfFlsa_wForTopicModeling.pdf)
    **这项工作已提交，但尚未经过同行评审。**'
- en: '**If you want to use FLSA-W, you can download** [**the FuzzyTM package**](https://pypi.org/project/FuzzyTM/)
    **or the flsamodel in Gensim.** For citations, [please use this paper](https://ieeexplore.ieee.org/abstract/document/9882661?casa_token=UsYg7SvoSioAAAAA%3A3ltCVZexA9-lPveuGVeRDh5VQW6rw0pVRDxmYk39tXbx13u4OuB2sTEFZzIGJCkdRiZBg0eJ).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果你想使用FLSA-W，你可以下载** [**FuzzyTM包**](https://pypi.org/project/FuzzyTM/) **或Gensim中的flsamodel。**
    有关引用，[请使用这篇论文](https://ieeexplore.ieee.org/abstract/document/9882661?casa_token=UsYg7SvoSioAAAAA%3A3ltCVZexA9-lPveuGVeRDh5VQW6rw0pVRDxmYk39tXbx13u4OuB2sTEFZzIGJCkdRiZBg0eJ)。'
- en: Let me know if you have any questions or remarks.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有任何问题或意见，请告知我。
