- en: Three Fundamental Flaws In Common Reinforcement Learning Algorithms (And How
    To Fix Them)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见强化学习算法的三个根本性缺陷（及其修复方法）
- en: 原文：[https://towardsdatascience.com/three-fundamental-flaws-in-common-reinforcement-learning-algorithms-and-how-to-fix-them-951160b7a207](https://towardsdatascience.com/three-fundamental-flaws-in-common-reinforcement-learning-algorithms-and-how-to-fix-them-951160b7a207)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/three-fundamental-flaws-in-common-reinforcement-learning-algorithms-and-how-to-fix-them-951160b7a207](https://towardsdatascience.com/three-fundamental-flaws-in-common-reinforcement-learning-algorithms-and-how-to-fix-them-951160b7a207)
- en: Harness yourself against these shortcomings encountered in everyday RL algorithms
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对抗在日常强化学习算法中遇到的这些缺陷
- en: '[](https://wvheeswijk.medium.com/?source=post_page-----951160b7a207--------------------------------)[![Wouter
    van Heeswijk, PhD](../Images/9c996bccd6fdfb6d9aa8b50b93338eb9.png)](https://wvheeswijk.medium.com/?source=post_page-----951160b7a207--------------------------------)[](https://towardsdatascience.com/?source=post_page-----951160b7a207--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----951160b7a207--------------------------------)
    [Wouter van Heeswijk, PhD](https://wvheeswijk.medium.com/?source=post_page-----951160b7a207--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wvheeswijk.medium.com/?source=post_page-----951160b7a207--------------------------------)[![Wouter
    van Heeswijk, PhD](../Images/9c996bccd6fdfb6d9aa8b50b93338eb9.png)](https://wvheeswijk.medium.com/?source=post_page-----951160b7a207--------------------------------)[](https://towardsdatascience.com/?source=post_page-----951160b7a207--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----951160b7a207--------------------------------)
    [Wouter van Heeswijk, PhD](https://wvheeswijk.medium.com/?source=post_page-----951160b7a207--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----951160b7a207--------------------------------)
    ·8 min read·Jan 30, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [数据科学前沿](https://towardsdatascience.com/?source=post_page-----951160b7a207--------------------------------)
    ·阅读时间8分钟·2023年1月30日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/9dda11eec29ec887f349772c00f43653.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9dda11eec29ec887f349772c00f43653.png)'
- en: Photo by [Varvara Grabova](https://unsplash.com/@santabarbara77?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Varvara Grabova](https://unsplash.com/@santabarbara77?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Reinforcement Learning algorithms such as Q-learning and REINFORCE have been
    around for decades and their textbook implementations are still widely used. Unfortunately,
    they exhibit some fundamental flaws, which greatly increase the struggle of learning
    a good policy.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 像Q学习和REINFORCE这样的强化学习算法已经存在了几十年，其教科书中的实现仍然被广泛使用。不幸的是，它们存在一些根本性的缺陷，这大大增加了学习良好策略的难度。
- en: This article addresses three major shortcomings of classical Reinforcement Learning
    algorithms, along with solutions to overcome them.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本文讨论了经典强化学习算法的三个主要缺陷，并提出了克服这些缺陷的解决方案。
- en: I. Selecting overvalued actions
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: I. 选择被高估的动作
- en: The problem
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: Most RL algorithms use some notion of value functions to capture downstream
    rewards, many of them based on the well-known Q-learning algorithm. The mechanism
    driving Q-learning is that it **selects the action that yields the highest expected
    value**. Depending on initialization, this mechanism might get stuck at the first
    action that is tried, so we also select random actions with probability ϵ, typically
    set at 0.05 or so.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数强化学习算法使用某种形式的价值函数来捕捉后续奖励，其中许多基于广为人知的Q学习算法。Q学习的机制是**选择带来最高期望值的动作**。根据初始化情况，这一机制可能会在尝试的第一个动作上陷入困境，因此我们还以概率ϵ选择随机动作，通常设定为0.05左右。
- en: In the limit, we would explore each action infinitely often and the Q-values
    would converge to the true values. In practice, however, we work with limited
    samples and biased Q-values. As a result, **Q-learning consistently selects actions
    with overestimated values**!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在极限情况下，我们将无限次地探索每个动作，Q值将收敛于真实值。然而，在实践中，我们处理的是有限样本和有偏的Q值。因此，**Q学习一贯选择具有高估值的动作**！
- en: Imagine a setting in which we play two identical [slot machines](https://medium.com/towards-data-science/a-minimal-working-example-for-deep-q-learning-in-tensorflow-2-0-e0ca8a944d5e).
    Machine A happens to give above-average rewards in the early iteration, so its
    Q-value is higher and we keep playing A. As machine B is only rarely selected,
    it would take a long time before we figure out the Q-values are actually identical.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 想象这样一个场景，我们玩两个完全相同的[老虎机](https://medium.com/towards-data-science/a-minimal-working-example-for-deep-q-learning-in-tensorflow-2-0-e0ca8a944d5e)。机器A在早期迭代中给出的奖励高于平均水平，因此其Q值更高，我们持续玩A。由于机器B仅偶尔被选择，我们需要很长时间才能发现Q值实际上是相同的。
- en: More generally speaking, value functions will always be imperfect, and RL has
    a penchant to perform overvalued actions. In a sense, RL “rewards” poor estimates,
    which is obviously not a desirable property.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地说，价值函数总是会存在不完美之处，而强化学习往往会执行过高估值的动作。从某种意义上说，强化学习“奖励”了糟糕的估计，这显然不是一个理想的特性。
- en: '![](../Images/5151c5cbcfb468c1ae5daf6599a9f596.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5151c5cbcfb468c1ae5daf6599a9f596.png)'
- en: Multi-armed bandit problems clearly demonstrate the impact of selecting overvalued
    actions [Photo by [Bangyu Wang](https://unsplash.com/es/@bangyuwang?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)]
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 多臂老虎机问题清楚地展示了选择过高估值动作的影响[照片由[Bangyu Wang](https://unsplash.com/es/@bangyuwang?utm_source=medium&utm_medium=referral)提供，见[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)]
- en: Solutions
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: The problems of Q-learning can be traced back to the practice of sampling and
    updating with the same observations. We can decouple these steps by sampling with
    one policy and updating another. This is precisely what **Double Q-learning**
    (Van Hasselt, 2010) does.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习的问题可以追溯到使用相同观察进行采样和更新的实践。我们可以通过使用一个策略进行采样，另一个策略进行更新来解耦这些步骤。这正是**双Q学习**（Van
    Hasselt, 2010）所做的。
- en: '![](../Images/26a17cf25f3c9d50f4235a38c3e19f53.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/26a17cf25f3c9d50f4235a38c3e19f53.png)'
- en: 'Double Q-learning samples actions with one network, but updates the Q-value
    with the output of the other network. This procedure decouples sampling and learning
    and combats overestimation. [source: Van Hasselt (2010)]'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 双Q学习使用一个网络来采样动作，但用另一个网络的输出更新Q值。这一过程将采样和学习解耦，并且克服了过度估计。[来源：Van Hasselt (2010)]
- en: More generally speaking, it is good practice to work with [**target networks**](/how-to-model-experience-replay-batch-learning-and-target-networks-c1350db93172).
    The target network is a periodical copy of the policy, used to generate target
    values that we train upon (rather than using exactly the same policy to generate
    both observation and target). This approach reduces the correlation between target
    and observation.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地说，使用[**目标网络**](/how-to-model-experience-replay-batch-learning-and-target-networks-c1350db93172)是一个良好的实践。目标网络是策略的周期性副本，用于生成我们训练的目标值（而不是使用完全相同的策略来生成观察和目标）。这种方法减少了目标与观察之间的相关性。
- en: Another solution angle is to take into account the uncertainty of our Q-value
    estimates. Rather than solely tabulating the expected values of actions, we can
    also keep track of the variance of observations, indicating how for we might be
    off from the true value. Working with [**uncertainty bounds and knowledge gradients**](/seven-exploration-strategies-in-reinforcement-learning-you-should-know-8eca7dec503b)are
    two ways to achieve this purpose. Instead of simply selecting the action with
    the highest expected Q-value, we also consider how much we can *learn* from a
    new observation. This approach favors exploring actions with high uncertainty,
    while still sampling with intelligence.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种解决角度是考虑我们Q值估计的不确定性。与其仅仅记录动作的期望值，我们还可以跟踪观察的方差，以指示我们可能偏离真实值的程度。使用[**不确定性边界和知识梯度**](/seven-exploration-strategies-in-reinforcement-learning-you-should-know-8eca7dec503b)是实现这一目标的两种方法。我们不仅考虑选择具有最高期望Q值的动作，还考虑从新观察中能*学到*多少。这种方法偏向于探索具有高不确定性的动作，同时仍然进行智能采样。
- en: '[](/seven-exploration-strategies-in-reinforcement-learning-you-should-know-8eca7dec503b?source=post_page-----951160b7a207--------------------------------)
    [## Seven Exploration Strategies In Reinforcement Learning You Should Know'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 你应该知道的七种强化学习探索策略](/seven-exploration-strategies-in-reinforcement-learning-you-should-know-8eca7dec503b?source=post_page-----951160b7a207--------------------------------)'
- en: Pure exploration and -exploitation, ϵ-greedy, Boltzmann exploration, optimistic
    initialization, confidence intervals…
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 纯探索和-利用，ϵ-贪婪，玻尔兹曼探索，乐观初始化，置信区间……
- en: towardsdatascience.com](/seven-exploration-strategies-in-reinforcement-learning-you-should-know-8eca7dec503b?source=post_page-----951160b7a207--------------------------------)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/seven-exploration-strategies-in-reinforcement-learning-you-should-know-8eca7dec503b?source=post_page-----951160b7a207--------------------------------)'
- en: II. Poor policy gradient updates
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: II. 糟糕的策略梯度更新
- en: The problem
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: Policy gradient algorithms have been in swing for decades, and are at the root
    of all modern actor-critic models. The vanilla policy gradient algorithms — e.g.,
    REINFORCE — rely on gradients to determine the direction of weight updates. A
    combination of high rewards and high gradients yield a strong update signal.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度算法已经存在了几十年，并且是所有现代演员-评论员模型的根基。普通的策略梯度算法——例如 REINFORCE——依赖梯度来确定权重更新的方向。高奖励和高梯度的组合会产生强烈的更新信号。
- en: '![](../Images/0f452ca259da52230dadd302edcc7084.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0f452ca259da52230dadd302edcc7084.png)'
- en: Traditional policy gradient update function, updating policy weights θ based
    on objective function gradient ∇_θJ(θ) and step size α
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的策略梯度更新函数，通过目标函数梯度 ∇_θJ(θ) 和步长 α 来更新策略权重 θ。
- en: The idea seems natural. If the slope of the reward function is steep, you take
    a big step in that direction. If it is small, there is no sense in performing
    big updates. Compelling as it may seem, this logic is also fundamentally flawed.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法似乎很自然。如果奖励函数的斜率很陡，你就朝那个方向迈出大步。如果斜率很小，那就没有必要进行大幅更新。虽然这个逻辑可能很有说服力，但它也是根本有缺陷的。
- en: '![](../Images/eb76f86045facb568f6e99b2e8037c78.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eb76f86045facb568f6e99b2e8037c78.png)'
- en: 'Left: example of overshooting behavior, performing a large policy update that
    misses the reward peak. Right: example of stalling behavior, being stuck in a
    local optimum with gradients close to 0\. [image by author]'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 左：超调行为的示例，进行了一次大幅策略更新但错过了奖励峰值。右：停滞行为的示例，被困在局部最优点，梯度接近 0。 [图像由作者提供]
- en: A gradient only provides local information. It tells us how steep the slope
    is, but not how far to step in that direction; we might **overshoot**. Furthermore,
    policy gradients do not consider that a lack of gradient signal might get us **stuck
    on a sub-optimal plateau**.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度仅提供局部信息。它告诉我们斜率的陡峭程度，但不告诉我们在该方向上应走多远；我们可能会**超调**。此外，策略梯度并未考虑缺乏梯度信号可能会使我们**陷入次优平稳点**。
- en: To make matters worse, we cannot control this behavior by forcing the weight
    updates to be within a certain parameter region. In the figure below, for example,
    weight updates of the same magnitude have very different effects on the policy.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 更糟的是，我们无法通过强制权重更新在某个参数区域内来控制这种行为。例如，在下面的图中，相同幅度的权重更新对策略产生了非常不同的效果。
- en: '![](../Images/a02d2bc8a6f52052e211ff468ad74b77.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a02d2bc8a6f52052e211ff468ad74b77.png)'
- en: Example of two Gaussian policy updates. Although both updates have the same
    size in terms of parameter space, the policy on the left is clearly affected much
    more than the one on the right [image by author]
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 两个高斯策略更新的示例。尽管这两个更新在参数空间中的大小相同，但左侧的策略显然受到的影响要大得多，而右侧的影响则较小 [图像由作者提供]
- en: Solutions
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: A simple start is to experiment with various learning algorithms. The traditional
    stochastic gradient descent (SGD) algorithm only considers first moments. Modern
    learning algorithms (e.g., **ADAM**) consider second moments, often substantially
    enhancing performance. Although not fully resolving the problem, the performance
    increase may be remarkable.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的开始是尝试各种学习算法。传统的随机梯度下降（SGD）算法只考虑一阶矩。现代学习算法（例如 **ADAM**）考虑了二阶矩，通常会显著提升性能。虽然不能完全解决问题，但性能提升可能会非常显著。
- en: '**Entropy regularization** is a common way to prevent premature convergence
    of vanilla policy gradient algorithms. Loosely stated, entropy in RL is a metric
    for the unpredictability of action selection. Entropy regularization adds a bonus
    for exploring unknown actions, which is higher when we have learned less about
    the system:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**熵正则化**是一种常见的方法来防止普通策略梯度算法的过早收敛。大致来说，强化学习中的熵是衡量动作选择不可预测性的指标。熵正则化为探索未知动作添加了奖励，当我们对系统了解较少时，这种奖励会更高：'
- en: '![](../Images/881d33b567b4fb05c37a9fd5e3dcc53f.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/881d33b567b4fb05c37a9fd5e3dcc53f.png)'
- en: Measure of entropy in Reinforcement Learning
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习中的熵度量
- en: More sophisticated extensions of policy gradient algorithms also consider second-order
    derivatives, which provide information on the local sensitivity of the function.
    At a plateau, we can safely take big steps without consequence. At a steep but
    curved slope, we would prefer cautious steps. Algorithms such as [**natural policy
    gradients**](https://medium.com/towards-data-science/natural-policy-gradients-in-reinforcement-learning-explained-2265864cf43c)**,**
    [**TRPO**](/trust-region-policy-optimization-trpo-explained-4b56bd206fc2) **and**
    [**PPO**](/proximal-policy-optimization-ppo-explained-abed1952457b) take into
    account the sensitivity towards updates, either explicitly or implicitly considering
    second-order derivatives. At the moment, PPO is the go-to policy gradient algorithm,
    striking a fine balance between ease of implementation, speed and performance.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 更复杂的策略梯度算法扩展还考虑了二阶导数，这提供了函数的局部灵敏度信息。在平稳区域，我们可以安全地大步前进而不会产生后果。在陡峭但弯曲的坡度上，我们则会更倾向于谨慎地前进。诸如
    [**自然策略梯度**](https://medium.com/towards-data-science/natural-policy-gradients-in-reinforcement-learning-explained-2265864cf43c)**、**
    [**TRPO**](/trust-region-policy-optimization-trpo-explained-4b56bd206fc2) **和**
    [**PPO**](/proximal-policy-optimization-ppo-explained-abed1952457b) 之类的算法考虑了更新的敏感性，无论是显式还是隐式地考虑了二阶导数。目前，PPO
    是首选的策略梯度算法，在实现的便利性、速度和性能之间取得了良好的平衡。
- en: '![](../Images/4a8fed27fa04ca364cff2ee5ddc88057.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4a8fed27fa04ca364cff2ee5ddc88057.png)'
- en: Weight update scheme for natural policy gradients. The Fischer matrix F(θ) contains
    information about local sensitivity, generating dynamic weight updates.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 自然策略梯度的权重更新方案。F(θ) 的 Fischer 矩阵包含关于局部灵敏度的信息，生成动态权重更新。
- en: '[](/natural-policy-gradients-in-reinforcement-learning-explained-2265864cf43c?source=post_page-----951160b7a207--------------------------------)
    [## Natural Policy Gradients In Reinforcement Learning Explained'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/natural-policy-gradients-in-reinforcement-learning-explained-2265864cf43c?source=post_page-----951160b7a207--------------------------------)
    [## 自然策略梯度在强化学习中的解释'
- en: Traditional policy gradient methods are inherently flawed. Natural gradients
    converge quicker and better, forming the…
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 传统的策略梯度方法固有地存在缺陷。自然梯度收敛更快、更好，形成了…
- en: towardsdatascience.com](/natural-policy-gradients-in-reinforcement-learning-explained-2265864cf43c?source=post_page-----951160b7a207--------------------------------)
    [](/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=post_page-----951160b7a207--------------------------------)
    [## Trust Region Policy Optimization (TRPO) Explained
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/natural-policy-gradients-in-reinforcement-learning-explained-2265864cf43c?source=post_page-----951160b7a207--------------------------------)
    [](/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=post_page-----951160b7a207--------------------------------)
    [## 信赖区域策略优化（TRPO）解释
- en: The Reinforcement Learning algorithm TRPO builds upon natural policy gradient
    algorithms, ensuring updates remain…
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 强化学习算法 TRPO 基于自然策略梯度算法，确保更新保持…
- en: towardsdatascience.com](/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=post_page-----951160b7a207--------------------------------)
    [](/proximal-policy-optimization-ppo-explained-abed1952457b?source=post_page-----951160b7a207--------------------------------)
    [## Proximal Policy Optimization (PPO) Explained
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/trust-region-policy-optimization-trpo-explained-4b56bd206fc2?source=post_page-----951160b7a207--------------------------------)
    [](/proximal-policy-optimization-ppo-explained-abed1952457b?source=post_page-----951160b7a207--------------------------------)
    [## 近端策略优化（PPO）解释
- en: The journey from REINFORCE to the go-to algorithm in continuous control
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从 REINFORCE 到在连续控制中的首选算法的旅程
- en: towardsdatascience.com](/proximal-policy-optimization-ppo-explained-abed1952457b?source=post_page-----951160b7a207--------------------------------)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/proximal-policy-optimization-ppo-explained-abed1952457b?source=post_page-----951160b7a207--------------------------------)'
- en: III. Underperformance off-policy learning
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: III. 性能不足的离策略学习
- en: The problem
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: Certain algorithms (e.g., these rooted in Q-learning) rely on *off-policy learning*,
    meaning that **updates may be performed with a different action than actually
    observed**. Whereas on-policy learning requires a tuple *(s,a,r,s’,a’)* — indeed,
    like its algorithm namesake SARSA— off-policy learning uses the best known action
    *a** instead of *a’.* Consequently, we only store *(s,a,r,s’)* for the weight
    updates and learn the policy independent of the agent’s actions.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 某些算法（例如，基于 Q 学习的算法）依赖于 *离策略学习*，这意味着 **更新可能会使用与实际观察到的不同的动作**。而在策略学习中需要一个元组 *(s,a,r,s’,a’)*
    — 实际上，就像其算法名称 SARSA 一样— 离策略学习使用已知的最佳动作 *a* 而不是 *a’*。因此，我们只存储 *(s,a,r,s’)* 用于权重更新，并且学习策略独立于代理的动作。
- en: Due to its setup, off-policy learning can re-use prior observations by drawing
    them from an **experience replay buffer,** which is particularly convenient when
    **creating observations is (computationally) expensive**. We simply feed state
    *s’* to our policy to obtain an action *a**, using the resulting value to update
    Q-values. The transition dynamics from *s* to *s’* do not need to be recomputed.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其设置，离策略学习可以通过从**经验回放缓冲区**中提取先前观察进行重用，这在**生成观察（计算上）昂贵**时尤其方便。我们只需将状态*s’*输入到我们的策略中以获得动作*a*，使用得到的值来更新
    Q 值。状态从*s*到*s’*的转移动态不需要重新计算。
- en: Unfortunately, even after extensively training an off-policy Reinforcement Learning
    algorithm on a large dataset, it often does not work nearly as well when deployed.
    Why is that?
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，即使在大型数据集上广泛训练离策略强化学习算法，当部署时，它通常表现得远不如预期。这是为什么呢？
- en: The problem boils down to a common statistical caveat. The assumption is that
    the **training set is representative for the true data set.** When this is not
    the case — which it often isn’t, as updated policies generate different state-action
    pairs — the policy is fit to a dataset that does not reflect the environment the
    agent ultimately operates in.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 问题归结为一个常见的统计学警告。假设是**训练集代表真实数据集**。当这种情况不成立时——通常是不成立的，因为更新的策略生成了不同的状态-动作对——策略就会拟合一个不反映代理最终操作环境的数据集。
- en: Solutions
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: True off-policy learning — e.g., learning good policies solely from a static
    dataset — might be fundamentally infeasible in Reinforcement Learning, as updating
    policies inevitably alters the probability of observing state-action pairs. As
    we cannot exhaustively explore the search space, we inevitably **extrapolate values
    towards unseen state-action pairs.**
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 真实的离策略学习——例如，仅从静态数据集中学习良好的策略——在强化学习中可能从根本上是不可行的，因为更新策略不可避免地会改变观察到状态-动作对的概率。由于我们无法彻底探索搜索空间，我们不可避免地**将值外推到未见的状态-动作对**。
- en: The most common solution is to not train on a fully static dataset, but **continuously
    enrich the dataset** with observations generated under the new policy. It may
    help to also **remove older samples**, which no longer represent data generated
    under recent policies.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的解决方案是不要在完全静态的数据集上训练，而是**不断丰富数据集**，添加在新策略下生成的观察。也可以考虑**移除旧的样本**，这些样本不再代表最近策略生成的数据。
- en: Another solution is **importance sampling**, which is essentially reweighs observations
    based on their likelihood of being generated under the present policy. For each
    observation, we can compute its ratio of the probability being generated under
    the original and present policy, making observations stemming from similar policies
    more likely to be drawn.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个解决方案是**重要性采样**，本质上是根据观察在当前策略下生成的可能性重新加权观察。对于每个观察，我们可以计算它在原始和当前策略下生成的概率的比率，使得来自相似策略的观察更可能被抽取。
- en: '![](../Images/ad2e71f9d0df5e643786d6da4cc73b93.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad2e71f9d0df5e643786d6da4cc73b93.png)'
- en: Importance sampling considers the similarity between the original policy and
    the target policy, selecting observations generated under a policy similar to
    the present one with a higher probability.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 重要性采样考虑原始策略和目标策略之间的相似性，以更高的概率选择在与当前策略相似的策略下生成的观察。
- en: If you continue to struggle to get your off-policy algorithm to perform well
    out-of-sample, a switch to an **on-policy algorithm** should be considered. Especially
    when generating new observations is cheap, the loss in sample efficiency might
    be offset by the enhanced policy quality.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你继续在样本外使离策略算法表现不佳，可以考虑切换到**在策略算法**。特别是当生成新观察很便宜时，样本效率的损失可能会被增强的策略质量所弥补。
- en: '[](/how-to-model-experience-replay-batch-learning-and-target-networks-c1350db93172?source=post_page-----951160b7a207--------------------------------)
    [## How To Model Experience Replay, Batch Learning and Target Networks'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/how-to-model-experience-replay-batch-learning-and-target-networks-c1350db93172?source=post_page-----951160b7a207--------------------------------)
    [## 如何建模经验回放、批量学习和目标网络'
- en: A quick tutorial on three essential tricks for stable and successful Deep Q-learning,
    using TensorFlow 2.0
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于使用 TensorFlow 2.0 进行稳定和成功的深度 Q 学习的三种基本技巧的快速教程
- en: towardsdatascience.com](/how-to-model-experience-replay-batch-learning-and-target-networks-c1350db93172?source=post_page-----951160b7a207--------------------------------)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/how-to-model-experience-replay-batch-learning-and-target-networks-c1350db93172?source=post_page-----951160b7a207--------------------------------)'
- en: Summary
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This article addressed three common flaws encountered in traditional RL algorithms,
    along with strategies to address them.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章解决了传统强化学习算法中遇到的三个常见缺陷，以及应对这些缺陷的策略。
- en: I. Overvalued actions
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I. 高估的动作
- en: 'Problem:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：
- en: Algorithms based on value function approximation systematically select actions
    with overestimated values.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于价值函数逼近的算法系统性地选择具有高估值的动作。
- en: 'Solutions:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案：
- en: Use target networks to decrease correlation between target and observation (e.g.,
    as in Double Q-learning).
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用目标网络减少目标和观察之间的相关性（例如，如在双重 Q 学习中）。
- en: Incorporate uncertainty of value estimates in action selection (e.g., uncertainty
    bounds, knowledge gradients.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在动作选择中纳入价值估计的不确定性（例如，不确定性边界、知识梯度）。
- en: '**II. Poor policy gradient updates**'
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**II. 策略梯度更新不良**'
- en: 'Problem:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：
- en: Policy gradient algorithms often perform poor update steps, e.g., minor steps
    when stuck in a local optimum or large ones that overshoot and miss the reward
    peak.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略梯度算法常常执行不佳的更新步骤，例如，当陷入局部最优时采取小步，或当过度步骤时错过奖励峰值。
- en: 'Solutions:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案：
- en: Use learning algorithm that includes, e.g., ADAM — which tracks momentum in
    addition to first-order gradients — instead of the standard Stochastic Gradient
    Descent.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用包括例如 ADAM 的学习算法——它除了跟踪一阶梯度外，还跟踪动量——而不是标准的随机梯度下降。
- en: Add an entropy bonus to the reward signal, encouraging more exploration of unknown
    regions.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向奖励信号中添加熵奖励，鼓励对未知区域进行更多探索。
- en: Deploy algorithms that include second-order derivatives (either explicitly or
    implicitly), such as natural policy gradients, TRPO or PPO.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署包含二阶导数的算法（无论是显式还是隐式），如自然策略梯度、TRPO 或 PPO。
- en: III. Underperformance off-policy learning
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III. 离策略学习的表现不足
- en: 'Problem:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：
- en: The experiences in the replay buffer may not be representative for out-of-sample
    experiences, such that values are incorrectly extrapolated and performance drops.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回放缓冲区中的经验可能无法代表样本外的经验，从而导致值被错误地外推并且性能下降。
- en: 'Solutions:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案：
- en: Update replay buffer, adding new experiences and removing older ones.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新回放缓冲区，添加新的经验并删除旧的经验。
- en: Perform importance sampling to increase probability of selecting experiences
    stemming from policies closer to the target policy.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行重要性采样以增加选择来自更接近目标策略的策略的经验的概率。
- en: Switch to on-policy learning (if sampling observations is cheap).
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 切换到在线学习（如果采样观察很便宜）。
- en: References
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Problem I: Overvalued actions'
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '问题 I: 高估的动作'
- en: Hasselt, H. (2010). Double Q-learning. *Advances in neural information processing
    systems*, *23*.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Hasselt, H. (2010). 双重 Q 学习。*神经信息处理系统进展*，*23*。
- en: Matiisen, Tambet (2015). Demystifying deep reinforcement learning. *Computational
    Neuroscience Lab.* Retrieved from neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Matiisen, Tambet (2015). 解密深度强化学习。*计算神经科学实验室*。取自 neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/
- en: 'Problem II: Poor policy gradient updates'
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '问题 II: 策略梯度更新不良'
- en: Mahmood, A. R., Van Hasselt, H. P., & Sutton, R. S. (2014). Weighted importance
    sampling for off-policy learning with linear function approximation. *Advances
    in Neural Information Processing Systems*, *27*.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Mahmood, A. R., Van Hasselt, H. P., & Sutton, R. S. (2014). 具有线性函数逼近的离策略学习的加权重要性采样。*神经信息处理系统进展*，*27*。
- en: 'Cornell University Computational Optimization Open Textbook. (2021). ADAM.
    URL: [https://optimization.cbe.cornell.edu/index.php?title=Adam](https://optimization.cbe.cornell.edu/index.php?title=Adam)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 康奈尔大学计算优化开放教材。 (2021). ADAM。网址：[https://optimization.cbe.cornell.edu/index.php?title=Adam](https://optimization.cbe.cornell.edu/index.php?title=Adam)
- en: 'Problem III: Underperformance off-policy learning'
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '问题 III: 离策略学习的表现不足'
- en: Fujimoto, S., Meger, D., & Precup, D. (2019, May). Off-policy deep reinforcement
    learning without exploration. In *International conference on machine learning*
    (pp. 2052–2062). PMLR.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Fujimoto, S., Meger, D., & Precup, D. (2019年5月). 无探索的离策略深度强化学习。在 *国际机器学习大会*（第2052–2062页）。PMLR。
