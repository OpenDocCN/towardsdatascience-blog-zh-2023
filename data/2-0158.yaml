- en: A Beginner’s Guide to LLM Fine-Tuning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM微调的初学者指南
- en: 原文：[https://towardsdatascience.com/a-beginners-guide-to-llm-fine-tuning-4bae7d4da672](https://towardsdatascience.com/a-beginners-guide-to-llm-fine-tuning-4bae7d4da672)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-beginners-guide-to-llm-fine-tuning-4bae7d4da672](https://towardsdatascience.com/a-beginners-guide-to-llm-fine-tuning-4bae7d4da672)
- en: How to fine-tune Llama and other LLMs with one tool
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用一个工具微调Llama及其他LLM
- en: '[](https://medium.com/@mlabonne?source=post_page-----4bae7d4da672--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----4bae7d4da672--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4bae7d4da672--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4bae7d4da672--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----4bae7d4da672--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mlabonne?source=post_page-----4bae7d4da672--------------------------------)[![Maxime
    Labonne](../Images/a7efdd305e3cc77d5509bbb1076d57d8.png)](https://medium.com/@mlabonne?source=post_page-----4bae7d4da672--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4bae7d4da672--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4bae7d4da672--------------------------------)
    [Maxime Labonne](https://medium.com/@mlabonne?source=post_page-----4bae7d4da672--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4bae7d4da672--------------------------------)
    ·8 min read·Aug 30, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----4bae7d4da672--------------------------------)
    ·8分钟阅读·2023年8月30日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/3cd56f68c14e07ab9ae3eb624bd064ed.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3cd56f68c14e07ab9ae3eb624bd064ed.png)'
- en: Image by author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: The growing interest in Large Language Models (LLMs) has led to a surge in **tools
    and wrappers designed to streamline their training process**.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对大型语言模型（LLMs）的日益关注导致了**旨在简化其训练过程的工具和封装的激增**。
- en: Popular options include [FastChat](https://github.com/lm-sys/FastChat) from
    LMSYS (used to train [Vicuna](https://huggingface.co/lmsys/vicuna-13b-v1.5)) and
    Hugging Face’s [transformers](https://github.com/huggingface/transformers)/[trl](https://github.com/huggingface/trl)
    libraries (used in [my previous article](/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32)).
    In addition, each big LLM project, like [WizardLM](https://github.com/nlpxucan/WizardLM/tree/main),
    tends to have its own training script, inspired by the original [Alpaca](https://github.com/tatsu-lab/stanford_alpaca)
    implementation.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 流行的选项包括LMSYS的[FastChat](https://github.com/lm-sys/FastChat)（用于训练[Vicuna](https://huggingface.co/lmsys/vicuna-13b-v1.5)）和Hugging
    Face的[transformers](https://github.com/huggingface/transformers)/[trl](https://github.com/huggingface/trl)库（用于[我之前的文章](/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32)）。此外，每个大型LLM项目，如[WizardLM](https://github.com/nlpxucan/WizardLM/tree/main)，通常都有自己独特的训练脚本，灵感来自于最初的[Alpaca](https://github.com/tatsu-lab/stanford_alpaca)实现。
- en: In this article, we will use [**Axolotl**](https://github.com/OpenAccess-AI-Collective/axolotl),
    a tool created by the OpenAccess AI Collective. We will use it to fine-tune a
    [**Code Llama 7b**](https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/examples/llama-2/qlora.yml)
    model on an evol-instruct dataset comprised of 1,000 samples of Python code.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将使用由OpenAccess AI Collective创建的[**Axolotl**](https://github.com/OpenAccess-AI-Collective/axolotl)工具。我们将使用它在一个包含1,000个Python代码样本的evol-instruct数据集上微调一个[**Code
    Llama 7b**](https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/examples/llama-2/qlora.yml)模型。
- en: 🤔 Why Axolotl?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🤔 为什么选择Axolotl？
- en: 'The main appeal of Axolotl is that it provides a one-stop solution, which includes
    numerous features, model architectures, and an active community. Here’s a quick
    list of my favorite things about it:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Axolotl的主要吸引力在于它提供了一站式解决方案，其中包括众多功能、模型架构和一个活跃的社区。以下是我最喜欢的几点：
- en: '**Configuration**: All parameters used to train an LLM are neatly stored in
    a yaml config file. This makes it convenient for sharing and reproducing models.
    You can see an example for Llama 2 [here](https://github.com/OpenAccess-AI-Collective/axolotl/tree/main/examples/llama-2).'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**配置**：用于训练LLM的所有参数都整齐地存储在yaml配置文件中。这使得模型的共享和再现变得方便。你可以在[这里](https://github.com/OpenAccess-AI-Collective/axolotl/tree/main/examples/llama-2)查看Llama
    2的示例。'
- en: '**Dataset Flexibility**: Axolotl allows the specification of multiple datasets
    with varied prompt formats such as alpaca (`{"instruction": "...", "input": "...",
    "output": "..."}`), sharegpt:chat (`{"conversations": [{"from": "...", "value":
    "..."}]}`), and raw completion (`{"text": "..."}`). Combining datasets is seamless,
    and the hassle of unifying the prompt format is eliminated.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集灵活性**：Axolotl允许指定多个数据集，支持各种提示格式，如alpaca（`{"instruction": "...", "input":
    "...", "output": "..."}`），sharegpt:chat（`{"conversations": [{"from": "...", "value":
    "..."}]}`）和原始完成（`{"text": "..."}`）。组合数据集非常顺畅，并且避免了统一提示格式的麻烦。'
- en: '**Features**: Axolotl is packed with SOTA techniques such as FSDP, deepspeed,
    LoRA, QLoRA, ReLoRA, sample packing, GPTQ, FlashAttention, xformers, and rope
    scaling.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**功能**：Axolotl配备了SOTA技术，如FSDP、deepspeed、LoRA、QLoRA、ReLoRA、样本打包、GPTQ、FlashAttention、xformers和rope
    scaling。'
- en: '**Utilities**: There are numerous user-friendly utilities integrated, including
    the addition or alteration of special tokens, or a custom wandb configuration.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工具**：集成了众多用户友好的工具，包括添加或更改特殊token，或自定义wandb配置。'
- en: Some well-known models trained using this tool are [Manticore-13b](https://huggingface.co/openaccess-ai-collective/manticore-13b)
    from the OpenAccess AI Collective and [Samantha-1.11–70b](https://huggingface.co/ehartford/Samantha-1.11-70b)
    from Eric Hartford. Like other wrappers, it is built on top of the transformers
    library and uses many of its features.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一些使用此工具训练的知名模型包括OpenAccess AI Collective的[Manticore-13b](https://huggingface.co/openaccess-ai-collective/manticore-13b)和Eric
    Hartford的[Samantha-1.11–70b](https://huggingface.co/ehartford/Samantha-1.11-70b)。像其他包装器一样，它建立在transformers库之上，并使用了许多其功能。
- en: ⚙️ Create your own config file
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ⚙️ 创建你自己的配置文件
- en: Before anything, we need a configuration file. You can reuse an existing configuration
    from the `[examples](https://github.com/OpenAccess-AI-Collective/axolotl/tree/main/examples)`
    folder. In our case, we will tweak the [QLoRA config](https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/examples/llama-2/qlora.yml)
    for Llama 2 to create our own **Code Llama** model. The model will be trained
    on a subset of 1,000 Python samples from the `[nickrosh/Evol-Instruct-Code-80k-v1](https://huggingface.co/datasets/nickrosh/Evol-Instruct-Code-80k-v1)`
    dataset.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，我们需要一个配置文件。你可以重用`[examples](https://github.com/OpenAccess-AI-Collective/axolotl/tree/main/examples)`文件夹中的现有配置。我们将调整[QLoRA配置](https://github.com/OpenAccess-AI-Collective/axolotl/blob/main/examples/llama-2/qlora.yml)以创建我们自己的**Code
    Llama**模型。该模型将在`[nickrosh/Evol-Instruct-Code-80k-v1](https://huggingface.co/datasets/nickrosh/Evol-Instruct-Code-80k-v1)`数据集中的1,000个Python样本子集上进行训练。
- en: First, we must change the `base_model` and `base_model_config` fields to "codellama/CodeLlama-7b-hf".
    To push our trained adapter to the Hugging Face Hub, let's add a new field `hub_model_id`,
    which corresponds to the name of our model, "EvolCodeLlama-7b". Now, we have to
    update the dataset to `[mlabonne/Evol-Instruct-Python-1k](https://huggingface.co/datasets/mlabonne/Evol-Instruct-Python-1k)`
    and set `type` to "alpaca".
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须将`base_model`和`base_model_config`字段更改为"codellama/CodeLlama-7b-hf"。为了将我们训练的适配器推送到Hugging
    Face Hub，我们需要添加一个新的字段`hub_model_id`，对应我们的模型名称"EvolCodeLlama-7b"。现在，我们必须将数据集更新为`[mlabonne/Evol-Instruct-Python-1k](https://huggingface.co/datasets/mlabonne/Evol-Instruct-Python-1k)`并将`type`设置为"alpaca"。
- en: There's no sample bigger than 2048 tokens in this dataset, so we can reduce
    the `sequence_len` to "2048" and save some VRAM. Talking about VRAM, we’re going
    to use a `micro_batch_size` of 10 and a `gradient_accumulation_steps` of 1 to
    maximize its use. In practice, you try different values until you use >95% of
    the available VRAM.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中没有大于2048个token的样本，因此我们可以将`sequence_len`减少到"2048"以节省一些VRAM。说到VRAM，我们将使用`micro_batch_size`为10和`gradient_accumulation_steps`为1，以最大化其使用。在实际操作中，你可以尝试不同的值，直到使用超过95%的可用VRAM。
- en: For convenience, I'm going to add the name "axolotl" to the `wandb_project`
    field so it's easier to track on my account. I'm also setting the `warmup_steps`
    to "100" (personal preference) and the `eval_steps` to 0.01 so we'll end up with
    100 evaluations.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便，我将"axolotl"添加到`wandb_project`字段中，以便在我的账户上更容易追踪。我还将`warmup_steps`设置为"100"（个人偏好），将`eval_steps`设置为0.01，以便进行100次评估。
- en: 'Here’s how the final config file should look:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的配置文件应该是这样的：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You can also find this config file [here](https://gist.github.com/mlabonne/8055f6335e2b85f082c8c75561321a66)
    as a GitHub gist.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在[这里](https://gist.github.com/mlabonne/8055f6335e2b85f082c8c75561321a66)找到这个配置文件，作为一个GitHub
    gist。
- en: 'Before we start training our model, I want to introduce a few parameters that
    are important to understand:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始训练模型之前，我想介绍几个重要的参数：
- en: '**QLoRA**: We’re using QLoRA for fine-tuning, which is why we’re loading the
    base model in 4-bit precision (NF4 format). You can check [this article](https://medium.com/towards-data-science/qlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b)
    from [Benjamin Marie](https://medium.com/u/ad2a414578b3?source=post_page-----4bae7d4da672--------------------------------)
    to know more about QLoRA.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**QLoRA**：我们使用QLoRA进行微调，这就是为什么我们以4位精度（NF4格式）加载基础模型。你可以查看[这篇文章](https://medium.com/towards-data-science/qlora-fine-tune-a-large-language-model-on-your-gpu-27bed5a03e2b)来了解更多关于QLoRA的内容，作者是[本杰明·玛丽](https://medium.com/u/ad2a414578b3?source=post_page-----4bae7d4da672--------------------------------)。'
- en: '**Gradient checkpointing**: It lowers the VRAM requirements by removing some
    activations that are re-computed on demand during the backward pass. It also slows
    down training by about 20%, according to Hugging Face’s [documentation](https://huggingface.co/docs/transformers/v4.18.0/en/performance).'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度检查点**：通过删除在反向传播过程中按需重新计算的一些激活来降低VRAM需求。根据Hugging Face的[文档](https://huggingface.co/docs/transformers/v4.18.0/en/performance)，它还会使训练速度降低约20%。'
- en: '**FlashAttention**: This implements the [FlashAttention](https://github.com/Dao-AILab/flash-attention)
    mechanism, which improves the speed and memory efficiency of our model thanks
    to a clever fusion of GPU operations (learn more about it in [this article](https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad)
    from [Aleksa Gordić](https://medium.com/u/37f02ae83e8c?source=post_page-----4bae7d4da672--------------------------------)).'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FlashAttention**：这实现了[FlashAttention](https://github.com/Dao-AILab/flash-attention)机制，通过巧妙融合GPU操作来提高模型的速度和内存效率（了解更多内容，请参阅[这篇文章](https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad)，作者是[Aleksa
    Gordić](https://medium.com/u/37f02ae83e8c?source=post_page-----4bae7d4da672--------------------------------)）。'
- en: '**Sample packing**: Smart way of creating batches with as little padding as
    possible, by reorganizing the order of the samples ([bin packing problem](https://en.wikipedia.org/wiki/Bin_packing_problem)).
    As a result, we need fewer batches to train the model on the same dataset. It
    was inspired by the [Multipack Sampler](https://github.com/imoneoi/multipack_sampler/tree/master)
    (see [my note](https://mlabonne.github.io/blog/notes/Large%20Language%20Models/multipack_sampler.html))
    and [Krell et al.](https://arxiv.org/pdf/2107.02027.pdf)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**样本打包**：一种通过重新组织样本顺序来创建尽可能少填充的批次的智能方法（[装箱问题](https://en.wikipedia.org/wiki/Bin_packing_problem)）。因此，我们需要更少的批次来训练相同的数据集。它的灵感来自于[Multipack
    Sampler](https://github.com/imoneoi/multipack_sampler/tree/master)（见[我的笔记](https://mlabonne.github.io/blog/notes/Large%20Language%20Models/multipack_sampler.html)）和[Krell等人](https://arxiv.org/pdf/2107.02027.pdf)。'
- en: You can find FlashAttention in some other tools, but sample packing is relatively
    new. As far as I know, [OpenChat](https://github.com/imoneoi/openchat) was the
    first project to use sample packing during fine-tuning. Thanks to Axolotl, we’ll
    use these techniques for free.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在其他一些工具中找到FlashAttention，但样本打包相对较新。据我所知，[OpenChat](https://github.com/imoneoi/openchat)是第一个在微调过程中使用样本打包的项目。感谢Axolotl，我们可以免费使用这些技术。
- en: 🦙 Fine-tune Code Llama
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 🦙 微调Code Llama
- en: Having the config file ready, it’s time to get our hands dirty with the actual
    fine-tuning. You might consider running the training on a Colab notebook. However,
    for those without access to a high-performance GPU, a more cost-effective solution
    consists of renting **cloud-based GPU services**, like AWS, [Lambda Labs](https://lambdalabs.com/),
    [Vast.ai](https://vast.ai/), [Banana](https://www.banana.dev/), or [RunPod](https://www.runpod.io/).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 配置文件准备好之后，就该开始实际的微调工作了。你可以考虑在Colab笔记本上运行训练。然而，对于那些没有高性能GPU的人来说，一个更具成本效益的解决方案是租用**基于云的GPU服务**，如AWS、[Lambda
    Labs](https://lambdalabs.com/)、[Vast.ai](https://vast.ai/)、[Banana](https://www.banana.dev/)或[RunPod](https://www.runpod.io/)。
- en: Personally, I use RunPod, which is a popular option in the fine-tuning community.
    It’s not the cheapest service but it hits a good tradeoff with a clean UI. You
    can easily replicate the following steps using your favorite service.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 就我个人而言，我使用RunPod，这是微调社区中一个受欢迎的选项。它不是最便宜的服务，但在界面清晰度和成本之间达到了一个良好的折中。你可以使用你喜欢的服务轻松复制以下步骤。
- en: 'When your RunPod account is set up, go to Manage > Templates and click on “New
    Template”. Here is a simple template:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的RunPod账户设置好后，转到“管理”>“模板”并点击“新建模板”。以下是一个简单的模板：
- en: '![](../Images/827c395c5294c1d157914fe4661ef301.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/827c395c5294c1d157914fe4661ef301.png)'
- en: Image by author
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: 'Let’s review the different fields and their corresponding values:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下不同的字段及其对应的值：
- en: '**Template Name**: Axolotl (you can choose whatever you want)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模板名称**：Axolotl（你可以选择任何你想要的名称）'
- en: '**Container Image**: winglian/axolotl-runpod:main-py3.10-cu118–2.0.1'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容器镜像**: winglian/axolotl-runpod:main-py3.10-cu118–2.0.1'
- en: '**Container Disk**: 100 GB'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容器磁盘**: 100 GB'
- en: '**Volume Disk**: 0 GB'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**存储卷**: 0 GB'
- en: '**Volume Mount Path**: /workspace'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷挂载路径**: /workspace'
- en: 'In addition, there are two handy environment variables can include:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有两个有用的环境变量可以包含：
- en: '**HUGGING_FACE_HUB_TOKEN**: you can find your token on [this page](https://huggingface.co/settings/tokens)
    (requires an account)'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HUGGING_FACE_HUB_TOKEN**: 你可以在 [此页面](https://huggingface.co/settings/tokens)
    上找到你的令牌（需要账户）'
- en: '**WANDB_API_KEY**: you can find your key on [this page](https://wandb.ai/authorize)
    (requires an account)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**WANDB_API_KEY**: 你可以在 [此页面](https://wandb.ai/authorize) 上找到你的密钥（需要账户）'
- en: 'Alternatively, you can simply log in the terminal later (using huggingface-cli
    login and wandb login). Once you’re set-up, go to Community Cloud and deploy an
    RTX 3090\. Here you can search for the name of your template and select it as
    follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，你可以稍后在终端登录（使用 huggingface-cli login 和 wandb login）。设置完成后，前往 Community Cloud
    并部署 RTX 3090。你可以在这里搜索你的模板名称并选择它，如下所示：
- en: '![](../Images/93732516ab48d2df1b5b62b9641f1117.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/93732516ab48d2df1b5b62b9641f1117.png)'
- en: Image by author
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: You can click on “Continue” and RunPod will deploy your template. You can see
    the installation in your pod’s logs (Manage > Pods). When the option becomes available,
    click on “Connect”. Here, click on “Start Web Terminal” and then “Connect to Web
    Terminal”. You are now connected to your pod!
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以点击“继续”，RunPod 将部署你的模板。你可以在你的 pod 的日志中查看安装情况（管理 > Pods）。当选项可用时，点击“连接”。在这里，点击“启动
    Web 终端”，然后“连接到 Web 终端”。你现在已连接到你的 pod！
- en: 'The following steps are **the same no matter what service you choose**:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤是**无论你选择哪个服务都相同**的：
- en: 'We install Axolotl and the PEFT library as follows:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们按如下方式安装 Axolotl 和 PEFT 库：
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '2\. Download the config file we created:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 下载我们创建的配置文件：
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '3\. You can now **start fine-tuning the model** with the following command:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 你现在可以使用以下命令 **开始微调模型**：
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: If everything is configured correctly, you should be able to train the model
    in a little more than **one hour** (it took me 1h 11m 44s). If you check the GPU
    memory used, you’ll see almost 100% with this config, which means we’re optimizing
    it pretty nicely. If you’re using a GPU with more VRAM (like an A100), you can
    increase the micro-batch size to make sure you’re fully using it.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切配置正确，你应该能够在 **一个多小时** 内训练模型（我花了 1小时 11分钟 44秒）。如果你检查使用的 GPU 内存，你会发现几乎达到 100%，这意味着我们正在很好地优化它。如果你使用具有更多
    VRAM 的 GPU（如 A100），你可以增加微批量大小以确保充分利用它。
- en: 'In the meantime, feel free to close the web terminal and check your loss on
    Weights & Biases. We’re using tmux so the training won’t stop if you close the
    terminal. Here are my loss curves:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，可以关闭 Web 终端，并在 Weights & Biases 上检查你的损失。我们使用 tmux，所以即使你关闭终端，训练也不会停止。以下是我的损失曲线：
- en: '![](../Images/fab0126f54062cf424f89f11e300d20c.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fab0126f54062cf424f89f11e300d20c.png)'
- en: Image by author
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'We see a steady improvement in the eval loss, which is a good sign. However,
    you can also spot drops in the eval loss that are not correlated with a decrease
    in the quality of the outputs… The best way to evaluate your model is simply by
    using it: you can run it in the terminal with the command `accelerate launch scripts/finetune.py
    EvolCodeLlama-7b.yaml --inference --lora_model_dir="./qlora-out"`.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到评估损失稳步改善，这是一个好兆头。然而，你也可以发现评估损失的下降并未与输出质量的降低相关联… 评估你的模型的最佳方式是直接使用它：你可以在终端中运行命令
    `accelerate launch scripts/finetune.py EvolCodeLlama-7b.yaml --inference --lora_model_dir="./qlora-out"`。
- en: 'The QLoRA adapter should already be uploaded to the Hugging Face Hub. However,
    you can also **merge the base Code Llama model with this adapter and push the
    merged model** there by following these steps:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: QLoRA 适配器应该已经上传到 Hugging Face Hub。然而，你也可以通过以下步骤**将基础 Code Llama 模型与此适配器合并并将合并后的模型推送**到那里：
- en: 'Download [this script](https://gist.github.com/mlabonne/a3542b0519708b8871d0703c938bba9f):'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载 [这个脚本](https://gist.github.com/mlabonne/a3542b0519708b8871d0703c938bba9f)：
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '2\. Execute it with this command:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 使用以下命令执行：
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Congratulations, you should have **your own EvolCodeLlama-7b** on the Hugging
    Face Hub at this point! For reference, you can access my own model trained with
    this process here: `[mlabonne/EvolCodeLlama-7b](https://huggingface.co/mlabonne/EvolCodeLlama-7b)`'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '恭喜，你现在应该在 Hugging Face Hub 上拥有**你自己的 EvolCodeLlama-7b**！作为参考，你可以在这里访问我用这个过程训练的模型:
    `[mlabonne/EvolCodeLlama-7b](https://huggingface.co/mlabonne/EvolCodeLlama-7b)`'
- en: 'Considering that our EvolCodeLlama-7b is a code LLM, it would be interesting
    to compare its performance with other models on **standard benchmarks**, such
    as [HumanEval](https://github.com/openai/human-eval) and [MBPP](https://github.com/google-research/google-research/tree/master/mbpp).
    For reference, you can find a leaderboard at the following address: [Multilingual
    Code Evals](https://huggingface.co/spaces/bigcode/multilingual-code-evals).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们的EvolCodeLlama-7b是一个代码LLM，比较它在**标准基准**上的表现与其他模型会很有趣，例如[HumanEval](https://github.com/openai/human-eval)和[MBPP](https://github.com/google-research/google-research/tree/master/mbpp)。作为参考，你可以在以下地址找到排行榜：[多语言代码评估](https://huggingface.co/spaces/bigcode/multilingual-code-evals)。
- en: If you’re happy with this model, you can **quantize** it with GGML for local
    inference with [this free Google Colab notebook](https://colab.research.google.com/drive/1pL8k7m04mgE5jo2NrjGi8atB0j_37aDD?usp=sharing).
    You can also fine-tune **bigger models** (e.g., 70b parameters) thanks to [deepspeed](https://github.com/microsoft/DeepSpeed),
    which only requires an additional config file.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对这个模型满意，你可以使用GGML进行**量化**以便本地推理，使用[这个免费的Google Colab笔记本](https://colab.research.google.com/drive/1pL8k7m04mgE5jo2NrjGi8atB0j_37aDD?usp=sharing)。你还可以利用[deepspeed](https://github.com/microsoft/DeepSpeed)对**更大的模型**（例如，70b参数）进行微调，只需额外的配置文件即可。
- en: Conclusion
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we’ve covered the essentials of **how to efficiently fine-tune
    LLMs**. We customized parameters to train on our Code Llama model on a small Python
    dataset. Finally, we merged the weights and uploaded the result on Hugging Face.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们覆盖了**如何高效微调LLMs**的基本要点。我们自定义了参数，在一个小的Python数据集上训练了我们的Code Llama模型。最后，我们合并了权重，并将结果上传到Hugging
    Face。
- en: I hope you found this guide useful. I recommend using Axolotl with a cloud-based
    GPU service to get some experience and upload a few models on Hugging Face. Build
    your own datasets, play with the parameters, and break stuff along the way. Like
    with every wrapper, don’t hesitate to check the source code to get a good intuition
    of what it’s actually doing. It will massively help in the long run.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你觉得这份指南有用。我推荐使用Axolotl与基于云的GPU服务来获取一些经验，并在Hugging Face上上传几个模型。构建自己的数据集，调整参数，过程中可能会遇到一些问题。就像使用任何包装器一样，不要犹豫去查看源代码，以便对其实际操作有一个很好的直观了解。这样在长期内会大大受益。
- en: Thanks to the OpenAccess AI Collective and all the contributors!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢OpenAccess AI Collective和所有贡献者！
- en: If you’re interested in more technical content around LLMs, [follow me on Medium](https://medium.com/@mlabonne).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对LLMs的更多技术内容感兴趣，[在Medium上关注我](https://medium.com/@mlabonne)。
- en: Related articles
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相关文章
- en: '[](/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32?source=post_page-----4bae7d4da672--------------------------------)
    [## Fine-Tune Your Own Llama 2 Model in a Colab Notebook'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[在Colab笔记本中微调你自己的Llama 2模型](https://towardsdatascience.com/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32?source=post_page-----4bae7d4da672--------------------------------)'
- en: A practical introduction to LLM fine-tuning
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LLM微调的实用介绍
- en: towardsdatascience.com](/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32?source=post_page-----4bae7d4da672--------------------------------)
    [](/4-bit-quantization-with-gptq-36b0f4f02c34?source=post_page-----4bae7d4da672--------------------------------)
    [## 4-bit Quantization with GPTQ
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[4位量化与GPTQ](https://towardsdatascience.com/fine-tune-your-own-llama-2-model-in-a-colab-notebook-df9823a04a32?source=post_page-----4bae7d4da672--------------------------------) '
- en: Quantize your own LLMs using AutoGPTQ
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用AutoGPTQ对自己的LLMs进行量化
- en: towardsdatascience.com](/4-bit-quantization-with-gptq-36b0f4f02c34?source=post_page-----4bae7d4da672--------------------------------)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[4位量化与GPTQ](https://towardsdatascience.com/4-bit-quantization-with-gptq-36b0f4f02c34?source=post_page-----4bae7d4da672--------------------------------)'
- en: '*Learn more about machine learning and support my work with one click — become
    a Medium member here:*'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*了解更多关于机器学习的内容，并通过一键支持我的工作——在这里成为Medium会员：*'
- en: '[](https://medium.com/@mlabonne/membership?source=post_page-----4bae7d4da672--------------------------------)
    [## Join Medium with my referral link - Maxime Labonne'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[加入Medium，使用我的推荐链接 - Maxime Labonne](https://medium.com/@mlabonne/membership?source=post_page-----4bae7d4da672--------------------------------)'
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story…
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 作为Medium会员，你的会员费用的一部分会给你阅读的作者，同时你可以全面访问每个故事…
- en: medium.com](https://medium.com/@mlabonne/membership?source=post_page-----4bae7d4da672--------------------------------)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[medium.com](https://medium.com/@mlabonne/membership?source=post_page-----4bae7d4da672--------------------------------)'
