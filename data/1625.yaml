- en: How Decision Trees Split Nodes, from Loss Function Perspective
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从损失函数的角度看决策树如何分裂节点
- en: 原文：[https://towardsdatascience.com/how-decision-trees-split-nodes-from-loss-function-perspective-60a2f2124b4e?source=collection_archive---------6-----------------------#2023-05-15](https://towardsdatascience.com/how-decision-trees-split-nodes-from-loss-function-perspective-60a2f2124b4e?source=collection_archive---------6-----------------------#2023-05-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-decision-trees-split-nodes-from-loss-function-perspective-60a2f2124b4e?source=collection_archive---------6-----------------------#2023-05-15](https://towardsdatascience.com/how-decision-trees-split-nodes-from-loss-function-perspective-60a2f2124b4e?source=collection_archive---------6-----------------------#2023-05-15)
- en: Learn how a decision tree splits nodes only to minimize its loss function
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解决策树如何仅通过最小化其损失函数来分裂节点
- en: '[](https://jasonweiyi.medium.com/?source=post_page-----60a2f2124b4e--------------------------------)[![Wei
    Yi](../Images/24b7a438912082519f24d18e11ac9638.png)](https://jasonweiyi.medium.com/?source=post_page-----60a2f2124b4e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----60a2f2124b4e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----60a2f2124b4e--------------------------------)
    [Wei Yi](https://jasonweiyi.medium.com/?source=post_page-----60a2f2124b4e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://jasonweiyi.medium.com/?source=post_page-----60a2f2124b4e--------------------------------)[![Wei
    Yi](../Images/24b7a438912082519f24d18e11ac9638.png)](https://jasonweiyi.medium.com/?source=post_page-----60a2f2124b4e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----60a2f2124b4e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----60a2f2124b4e--------------------------------)
    [Wei Yi](https://jasonweiyi.medium.com/?source=post_page-----60a2f2124b4e--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1b4bd5317a6e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-decision-trees-split-nodes-from-loss-function-perspective-60a2f2124b4e&user=Wei+Yi&userId=1b4bd5317a6e&source=post_page-1b4bd5317a6e----60a2f2124b4e---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----60a2f2124b4e--------------------------------)
    ·12 min read·May 15, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F60a2f2124b4e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-decision-trees-split-nodes-from-loss-function-perspective-60a2f2124b4e&user=Wei+Yi&userId=1b4bd5317a6e&source=-----60a2f2124b4e---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1b4bd5317a6e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-decision-trees-split-nodes-from-loss-function-perspective-60a2f2124b4e&user=Wei+Yi&userId=1b4bd5317a6e&source=post_page-1b4bd5317a6e----60a2f2124b4e---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----60a2f2124b4e--------------------------------)
    ·12分钟阅读·2023年5月15日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F60a2f2124b4e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-decision-trees-split-nodes-from-loss-function-perspective-60a2f2124b4e&user=Wei+Yi&userId=1b4bd5317a6e&source=-----60a2f2124b4e---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F60a2f2124b4e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-decision-trees-split-nodes-from-loss-function-perspective-60a2f2124b4e&source=-----60a2f2124b4e---------------------bookmark_footer-----------)![](../Images/de317baa764ffb421a758d3453dbd4f5.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F60a2f2124b4e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-decision-trees-split-nodes-from-loss-function-perspective-60a2f2124b4e&source=-----60a2f2124b4e---------------------bookmark_footer-----------)![](../Images/de317baa764ffb421a758d3453dbd4f5.png)'
- en: Photo by [Ernest Brillo](https://unsplash.com/@ernest_brillo?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ernest Brillo](https://unsplash.com/@ernest_brillo?utm_source=medium&utm_medium=referral)拍摄的照片，发布在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)上'
- en: When talking about decision tree node splitting, I often hear phrases such as
    “variance reduction” and “information gain maximization”. And I’m always terrified
    by them. These are not something I am comfortable with using my every day vocabulary,
    until I realized that these phrases are synonyms for “minimizing the decision
    tree’s loss function”.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在谈到决策树节点分裂时，我经常听到“方差减少”和“信息增益最大化”等词汇。每次听到这些词汇我都会感到恐惧。这些并不是我日常词汇中常用的术语，直到我意识到这些术语实际上是“最小化决策树损失函数”的同义词。
- en: Which loss function? Well, every machine learning model needs a loss function.
    The loss function quantifies the *difference* between a model’s predictions and
    the actuals. An alternative term is the *error* in the model’s prediction. The
    decision tree model is of no exception.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 哪种损失函数？嗯，每个机器学习模型都需要一个损失函数。损失函数量化了模型预测与实际结果之间的*差异*。另一个术语是模型预测中的*误差*。决策树模型也不例外。
- en: To me, understanding the loss function is the most important step in understanding
    a machine learning model. This is because a loss function encodes everything we
    want our model to be good at using a single number. Just like we quantify how
    good a man is by a single number; I recall that is a length measure of some sort,
    right, wink, wink?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对我来说，理解损失函数是理解机器学习模型的最重要一步。这是因为损失函数用一个单一的数字来编码我们希望模型擅长的所有内容。就像我们用一个单一的数字来量化一个人的优点；我记得那是某种长度的度量，对吧，眨眨眼？
- en: Since the loss function measures the difference between a tree’s predictions
    and the actuals, to understand the loss function, we first need to understand
    how a decision tree makes predictions…
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于损失函数测量的是树的预测与实际结果之间的差异，要理解损失函数，我们首先需要了解决策树如何进行预测……
