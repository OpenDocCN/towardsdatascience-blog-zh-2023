- en: A Quick Guide on Normalization for Your NLP Model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理模型的归一化快速指南
- en: 原文：[https://towardsdatascience.com/a-quick-guide-on-normalization-for-your-nlp-model-2dbd7d2d42a7](https://towardsdatascience.com/a-quick-guide-on-normalization-for-your-nlp-model-2dbd7d2d42a7)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-quick-guide-on-normalization-for-your-nlp-model-2dbd7d2d42a7](https://towardsdatascience.com/a-quick-guide-on-normalization-for-your-nlp-model-2dbd7d2d42a7)
- en: Accelerate your model convergence and stabilize the training process with normalization
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用归一化加速模型收敛并稳定训练过程
- en: '[](https://medium.com/@vuphuongthao9611?source=post_page-----2dbd7d2d42a7--------------------------------)[![Thao
    Vu](../Images/9d44a2f199cdc9c29da72d9dc4971561.png)](https://medium.com/@vuphuongthao9611?source=post_page-----2dbd7d2d42a7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2dbd7d2d42a7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2dbd7d2d42a7--------------------------------)
    [Thao Vu](https://medium.com/@vuphuongthao9611?source=post_page-----2dbd7d2d42a7--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@vuphuongthao9611?source=post_page-----2dbd7d2d42a7--------------------------------)[![Thao
    Vu](../Images/9d44a2f199cdc9c29da72d9dc4971561.png)](https://medium.com/@vuphuongthao9611?source=post_page-----2dbd7d2d42a7--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2dbd7d2d42a7--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2dbd7d2d42a7--------------------------------)
    [Thao Vu](https://medium.com/@vuphuongthao9611?source=post_page-----2dbd7d2d42a7--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2dbd7d2d42a7--------------------------------)
    ·7 min read·Sep 14, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----2dbd7d2d42a7--------------------------------)
    ·阅读时间7分钟·2023年9月14日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/664add61750b752d6992bc44d3645704.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/664add61750b752d6992bc44d3645704.png)'
- en: Photo by [Mattia Bericchia](https://unsplash.com/@mattiabericchia?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由[Mattia Bericchia](https://unsplash.com/@mattiabericchia?utm_source=medium&utm_medium=referral)拍摄，发布于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Efficiently training deep learning models is challenging. The problem becomes
    more difficult with the recent growth of NLP models’ size and architecture complexity.
    To handle billions of parameters, more optimizations are proposed for faster convergence
    and stable training. One of the most remarkable techniques is normalization.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 高效训练深度学习模型是一项具有挑战性的任务。随着近期自然语言处理模型规模和架构复杂性的增长，问题变得更加棘手。为了处理数十亿的参数，提出了更多优化方法以实现更快的收敛和稳定的训练。其中一个最显著的技术是**归一化**。
- en: In this article, we will learn about some normalization techniques, how they
    work, and how they can be used for NLP deep models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将探讨一些归一化技术，它们的工作原理，以及如何将其用于自然语言处理深度模型。
- en: Why not BatchNorm?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么不使用BatchNorm？
- en: BatchNorm [2] is an early normalization technique proposed to solve internal
    covariate shifts.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: BatchNorm [2] 是一种早期的归一化技术，旨在解决内部协变量偏移问题。
- en: To explain in simple terms, an internal covariate shift occurs when there is
    a change in the layer’s input data distribution. When the neural networks are
    forced to fit different data distributions, the gradient update changes dramatically
    between batches. Therefore, the models take longer to adjust, learn the correct
    weights and converge. The problem gets worse as the model size grows.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，内部协变量偏移发生在层的输入数据分布发生变化时。当神经网络被迫适应不同的数据分布时，梯度更新在批次之间发生剧烈变化。因此，模型需要更长时间来调整、学习正确的权重并收敛。随着模型规模的增长，这个问题变得更严重。
- en: Initial solutions include using a small learning rate (so the impact of data
    distribution shifting is minor) and careful weight initialization. BatchNorm solved
    the problem effectively by normalizing the input on the feature dimension.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 初始解决方案包括使用较小的学习率（以减少数据分布变化的影响）和小心的权重初始化。BatchNorm通过在特征维度上对输入进行归一化，有效地解决了这个问题。
- en: '![](../Images/07a089aad68ef457d36037c70e0e8a1f.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07a089aad68ef457d36037c70e0e8a1f.png)'
- en: Batch Norm (Image by the author)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化（图片来源：作者）
- en: 'The technique helps speed up the convergence significantly and allows a higher
    learning rate as the model becomes less sensitive to outliers. However, it still
    has some drawbacks:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 该技术显著加快了收敛速度，并允许更高的学习率，因为模型对异常值的敏感度降低。然而，它仍然有一些缺点：
- en: '**Small batch size:** BatchNorm relies on batch data to compute the feature’s
    mean and standard deviation. When the batch size is small, the mean and variance
    can no longer represent the population. Therefore, online learning is impossible
    with BatchNorm.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**小批量大小：** BatchNorm 依赖于批量数据来计算特征的均值和标准差。当批量大小较小时，均值和方差无法再代表总体。因此，BatchNorm
    无法进行在线学习。'
- en: '**Sequence input:** In BatchNorm, each input sample’s normalization depends
    on other samples from the same batch. This does not work so well with sequence
    data. For example, we have 2 training samples with different lengths ***(a1, a2,..,
    a10)*** and ***(b1,b2,…,b20)***. Is it okay if token ***b11*** is normalized along
    with a padding token ***a11***? At the inference step, if we have a sequence of
    length 30 ***(c1, c2,.., c30)***, how do we get the mean and variance to normalize
    token ***c21***? This is the crucial reason BatchNorm is not suitable for NLP
    tasks.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**序列输入：** 在 BatchNorm 中，每个输入样本的归一化依赖于来自同一批次的其他样本。这对序列数据效果不好。例如，我们有两个训练样本，长度不同
    ***(a1, a2,.., a10)*** 和 ***(b1,b2,…,b20)***。如果 token ***b11*** 与填充 token ***a11***
    一起归一化，这是否合适？在推理步骤中，如果我们有一个长度为 30 的序列 ***(c1, c2,.., c30)***，我们如何获得均值和方差来归一化 token
    ***c21***？这就是 BatchNorm 不适用于 NLP 任务的关键原因。'
- en: '**Parallelization**: It''s difficult to parallelize batch-normalized models.
    Since there is dependence between elements (mean and variance), we need to synchronize
    them between devices. NLP models, such as Transformers, suffer from this setting
    due to their large-scale setup.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并行化：** 批量归一化模型难以并行化。由于元素（均值和方差）之间存在依赖关系，我们需要在设备之间进行同步。NLP 模型，如 Transformers，由于其大规模设置而受到影响。'
- en: This is where LayerNorm [1] came in. Proposed in 2016, LayerNorm has dethroned
    BatchNorm and steadily become the most popular normalization technique among the
    research community.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 LayerNorm [1] 出现的原因。2016 年提出的 LayerNorm 已经取代了 BatchNorm，稳步成为研究界最流行的归一化技术。
- en: '![](../Images/a2faf64e6af8978334d16ffd7b80d04a.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a2faf64e6af8978334d16ffd7b80d04a.png)'
- en: Normalization Technique Usage ([Source](https://paperswithcode.com/method/layer-normalization))
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化技术的使用（[来源](https://paperswithcode.com/method/layer-normalization)）
- en: So what is Layer Norm, and why it’s so effective?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，什么是层归一化，它为什么如此有效？
- en: Why LayerNorm works so well?
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么 LayerNorm 表现如此出色？
- en: In contrast to BatchNorm, LayerNorm normalizes the inputs on each data sample
    dimension. So, in a training batch of n samples ***(x1, x2,.., xn)***, the normalization
    is done on each ***xi*** independently.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 与 BatchNorm 相对，LayerNorm 在每个数据样本维度上进行归一化。因此，在一个包含 n 个样本的训练批次 ***(x1, x2,..,
    xn)*** 中，归一化是在每个 ***xi*** 上独立完成的。
- en: '![](../Images/da3e48d88a2cf730fea695cb0f856f00.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da3e48d88a2cf730fea695cb0f856f00.png)'
- en: Layer Norm (Image by the author)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 层归一化（图片由作者提供）
- en: LayerNorm has been widely used in many state-of-the-art language models, such
    as BERT [5] and BLOOM [6]. Why does it work so well?
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: LayerNorm 已广泛应用于许多最先进的语言模型，如 BERT [5] 和 BLOOM [6]。它为什么表现如此出色？
- en: 'Firstly, we need to mention a few advantages of LayerNorm compared to BatchNorm:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要提到 LayerNorm 相较于 BatchNorm 的几个优点：
- en: '**Sequence data:** The technique introduces no dependency between training
    examples. Therefore, we can safely normalize sequence inputs without worrying
    about the different characteristics between training samples.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**序列数据：** 该技术引入了训练样本之间的独立性。因此，我们可以放心地对序列输入进行归一化，而不必担心训练样本之间的不同特性。'
- en: '**Flexible batch size:** Since the normalization is done on each sample, batch
    size is no longer a problem.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活的批量大小：** 由于归一化是在每个样本上完成的，因此批量大小不再是问题。'
- en: '**Training and Testing:** Unlike BatchNorm, LayerNorm does not need to keep
    the moving mean or variance of the population. Therefore, it performs the exact
    computation at training and inference.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练和测试：** 与 BatchNorm 不同，LayerNorm 不需要保持总体的移动均值或方差。因此，它在训练和推理时执行相同的计算。'
- en: '**Parallelization*:*** There is no dependency between training samples, so
    we can train the model on different devices without the need for synchronization.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并行化：** 没有训练样本之间的依赖关系，因此我们可以在不同的设备上训练模型，而无需同步。'
- en: 'Secondly, we will discuss one of the critical reasons normalization techniques
    help us so much with training stabilization: their invariance under weights and
    input transformation. Invariance means the normalization technique result does
    not get affected by input transformation.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们将讨论归一化技术在训练稳定性方面如此有帮助的一个关键原因：它们在权重和输入变换下的不变性。不变性意味着归一化技术的结果不会受到输入变换的影响。
- en: Common known transformations are re-scaling and re-centering. Re-centering invariance
    makes the model insensitive to random noise in both weights and data. Meanwhile,
    re-scaling invariance keeps the output resilient to arbitrary scaling of inputs
    and weights.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的变换有重新缩放和重新中心化。重新中心化不变性使模型对权重和数据中的随机噪声不敏感。与此同时，重新缩放不变性使输出对输入和权重的任意缩放保持弹性。
- en: It is much easier to understand the invariance characteristic by building a
    layer normalization function and trying it on different transformations to see
    how the mean, variance and result change.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 通过构建一个层归一化函数并在不同的变换上进行尝试，可以更容易理解不变性特征，以查看均值、方差和结果如何变化。
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: I got the following result after re-scaling and re-centering the matrix weight
    (w) and dataset (x).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我在重新缩放和重新中心化矩阵权重（w）和数据集（x）后得到了以下结果。
- en: '![](../Images/deec85212d0042c359fc372672c7324c.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/deec85212d0042c359fc372672c7324c.png)'
- en: Layer Normalization results with different transformations (Image by the author)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 不同变换下的层归一化结果（作者提供的图片）
- en: As we can see, the normalized result remains the same with weight re-scaling
    and re-center. For dataset transformation, the technique is invariant to re-scaling
    but not re-centering.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，归一化结果在权重重新缩放和重新中心化下保持不变。对于数据集变换，该技术对重新缩放具有不变性，但对重新中心化不具备不变性。
- en: It’s pretty amazing how LayerNorm can keep the output resilient to such transformation.
    How can it do that? Let’s delve into the mathematical details to grasp better
    what is happening under the hood.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: LayerNorm 能够使输出对这种变换保持弹性，这非常令人惊讶。它是如何做到的？让我们深入探讨数学细节，以更好地理解其内部发生了什么。
- en: Things get a bit intimidating here, so feel free to skip this section. However,
    I bet this will give you a strong intuition of how normalization works.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的内容有点复杂，因此可以跳过此部分。然而，我敢肯定，这将给你一个强烈的直觉，帮助你更好地理解归一化的工作原理。
- en: Mathematical proof of Layer Norm invariance
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Layer Norm 不变性的数学证明
- en: We have a neural network with weight ***W***, input ***x***, bias ***b*** and
    activation function ***f***. The neural network output is ***y = f(Wx + b)***.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个神经网络，权重为 ***W***，输入为 ***x***，偏置为 ***b*** 和激活函数为 ***f***。神经网络的输出为 ***y =
    f(Wx + b)***。
- en: 'Then LayerNorm can be expressed as:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然后 LayerNorm 可以表示为：
- en: '![](../Images/cafd2765079df98ddcb13375251eddfa.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cafd2765079df98ddcb13375251eddfa.png)'
- en: The mathematical proof of LayerNorm variance can be shown as follows.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: LayerNorm 方差的数学证明如下所示。
- en: Weight matrix re-scaling invariance
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 权重矩阵重新缩放不变性
- en: '![](../Images/053d0d711d617510f3d464b1de7d7fd1.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/053d0d711d617510f3d464b1de7d7fd1.png)'
- en: Weight matrix re-scaling invariance (Image by the author)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 权重矩阵重新缩放不变性（作者提供的图片）
- en: Weight matrix re-centering invariance
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 权重矩阵重新中心化不变性
- en: '![](../Images/00e0fbc3290c2f8133a3992e637422e8.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/00e0fbc3290c2f8133a3992e637422e8.png)'
- en: Weight matrix re-centering invariance (Image by the author)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 权重矩阵重新中心化不变性（作者提供的图片）
- en: Dataset re-scaling invariance
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集重新缩放不变性
- en: '![](../Images/7169c039922d4cab5ff0f6afb0fa6bef.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7169c039922d4cab5ff0f6afb0fa6bef.png)'
- en: Dataset rep-scaling invariance (Image by the author)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集重新缩放不变性（作者提供的图片）
- en: So that’s the proof of LayerNorm invariance to transformations. However, not
    all invariance is necessary. RMSNorm [3] is a younger sibling of LayerNorm with
    only re-scaling invariance characteristics. However, it has become a favoured
    choice for recent LLM architectures like Llama [4].
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 LayerNorm 对变换的不变性的证明。然而，并非所有的不变性都是必要的。RMSNorm [3] 是 LayerNorm 的一个较年轻的兄弟，只具有重新缩放不变性特征。然而，它已成为最近
    LLM 架构如 Llama [4] 的首选。
- en: What makes RMSNorm more superior?
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 是什么使 RMSNorm 更优越？
- en: What is RMSNorm?
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 RMSNorm？
- en: RMSNorm was published in 2019, with RMS stands for “root mean square”. Despite
    LayerNorm accelerating convergence, the authors pointed out that it consumes more
    time for each training step.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: RMSNorm 于 2019 年发布，其中 RMS 代表“均方根”。尽管 LayerNorm 加速了收敛，作者指出它在每个训练步骤中消耗的时间更多。
- en: '![](../Images/56a9cc92261dc4a7cc1e7459850c6826.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56a9cc92261dc4a7cc1e7459850c6826.png)'
- en: Training procedure of a GRU-based RNNSearch [3]
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 GRU 的 RNNSearch 的训练过程 [3]
- en: The author also argued that the mean normalization of LayerNorm has an insignificant
    impact on the final performance and, hence, can be removed for computation efficiency.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 作者还辩称，LayerNorm 的均值归一化对最终性能的影响微乎其微，因此可以为了计算效率而被移除。
- en: Given that hypothesis, RMSNorm is proposed to focus on the re-scaling invariance
    and using root mean square regularization as follows.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个假设，提出了RMSNorm，以关注重新缩放不变性，并使用均方根正则化，如下所示。
- en: '![](../Images/1a7e814d0daf545bf29ef6dee6bd14d0.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a7e814d0daf545bf29ef6dee6bd14d0.png)'
- en: RMSNorm (Image by the author)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: RMSNorm（作者提供的图片）
- en: As we skip the mean computation, the normalization becomes simpler. With such
    elegant optimization, the author observed speedups of 7%-64% across different
    implementations without performance degradation!
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们跳过了均值计算，归一化变得更简单。通过这种优雅的优化，作者观察到在不同实现中速度提升了7%-64%，且性能没有下降！
- en: One of the experiments is the WMT14 English-German translation task with a GRU-based
    RNNSearch using BLEU score metric. LayerNorm and RMSNorm both achieved similar
    scores on the test. But RMSNorm was much faster with 25% less training time.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个实验是WMT14英语-德语翻译任务，使用基于GRU的RNNSearch并采用BLEU分数指标进行评估。LayerNorm和RMSNorm在测试中都取得了相似的分数。但RMSNorm的训练时间快了25%。
- en: '![](../Images/342b4b5a636d4a6a696a1580552c2bb8.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/342b4b5a636d4a6a696a1580552c2bb8.png)'
- en: BLEU score on test14 and test17 [3]
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: test14和test17上的BLEU分数[3]
- en: A closer look at the validation score during training also suggests RMSNorm
    performance is comparable to LayerNorm during all training stages. This supports
    the initial argument that re-centering invariance is insignificant and RMSNorm
    is more efficient.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 更详细地观察训练中的验证分数也表明RMSNorm的表现与LayerNorm在所有训练阶段相当。这支持了最初的观点，即重新中心化不具有显著意义，RMSNorm更加高效。
- en: '![](../Images/933cfe0838f456dd2071ad863223973a.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/933cfe0838f456dd2071ad863223973a.png)'
- en: '*SacreBLEU score on newstest2013 for the RNNSearch [3]*'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*RNNSearch在newstest2013上的SacreBLEU分数[3]*'
- en: Another intriguing metric is hidden vectors’ mean and standard deviation at
    different token positions. While the baseline’s mean and variance vary greatly,
    using LayerNorm and RMSNorm helps remarkably stabilize the distribution output.
    This is a strong proof of how vital normalization is to NLP models.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的指标是不同令牌位置的隐藏向量的均值和标准差。虽然基线的均值和方差变化很大，但使用LayerNorm和RMSNorm显著稳定了分布输出。这是归一化对于NLP模型至关重要的有力证明。
- en: '![](../Images/2844353cab639fc74bc7cb108ce7da46.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2844353cab639fc74bc7cb108ce7da46.png)'
- en: '*Mean (M) and standard deviation (S) statistics estimated for tokens at specific
    positions [3]*'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*特定位置的均值(M)和标准差(S)统计[3]*'
- en: I hope you find this article helpful in understanding each normalization technique’s
    pros and cons for the NLP tasks.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这篇文章能帮助你理解每种归一化技术在NLP任务中的优缺点。
- en: See you in the next post!
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 下篇文章见！
- en: Reference
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization.
    arXiv preprint arXiv:1607.06450, 2016.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Jimmy Lei Ba, Jamie Ryan Kiros 和 Geoffrey E Hinton。“层归一化。” *arXiv预印本 arXiv:1607.06450*
    (2016)。'
- en: '[2] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep
    network training by reducing internal covariate shift. ICML, 2015.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Sergey Ioffe和Christian Szegedy。“批量归一化：通过减少内部协方差偏移加速深度网络训练。” ICML, 2015。'
- en: '[3] Zhang, Biao, and Rico Sennrich. “Root mean square layer normalization.”
    *Advances in Neural Information Processing Systems* 32 (2019).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Zhang, Biao和Rico Sennrich。“均方根层归一化。” *神经信息处理系统进展* 32 (2019)。'
- en: '[4] Touvron, Hugo, et al. “Llama: Open and efficient foundation language models.”
    *arXiv preprint arXiv:2302.13971* (2023).'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Touvron, Hugo等。“Llama: 开放且高效的基础语言模型。” *arXiv预印本 arXiv:2302.13971* (2023)。'
- en: '[5] Devlin, Jacob, et al. “Bert: Pre-training of deep bidirectional transformers
    for language understanding.” *arXiv preprint arXiv:1810.04805* (2018).'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Devlin, Jacob等。“Bert: 深度双向变换器的预训练用于语言理解。” *arXiv预印本 arXiv:1810.04805* (2018)。'
- en: '[6] Scao, Teven Le, et al. “Bloom: A 176b-parameter open-access multilingual
    language model.” *arXiv preprint arXiv:2211.05100* (2022).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Scao, Teven Le等。“Bloom: 一个176b参数的开放访问多语言模型。” *arXiv预印本 arXiv:2211.05100*
    (2022)。'
