- en: Activation Functions For Neural Networks & Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络与深度学习的激活函数
- en: 原文：[https://towardsdatascience.com/activation-functions-non-linearity-neural-networks-101-ab0036a2e701](https://towardsdatascience.com/activation-functions-non-linearity-neural-networks-101-ab0036a2e701)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/activation-functions-non-linearity-neural-networks-101-ab0036a2e701](https://towardsdatascience.com/activation-functions-non-linearity-neural-networks-101-ab0036a2e701)
- en: Explaining why neural networks can learn (nearly) anything and everything
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释神经网络如何学习（几乎）任何和所有事物
- en: '[](https://medium.com/@egorhowell?source=post_page-----ab0036a2e701--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----ab0036a2e701--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ab0036a2e701--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ab0036a2e701--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----ab0036a2e701--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@egorhowell?source=post_page-----ab0036a2e701--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----ab0036a2e701--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ab0036a2e701--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ab0036a2e701--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----ab0036a2e701--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ab0036a2e701--------------------------------)
    ·8 min read·Oct 12, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ab0036a2e701--------------------------------)
    ·8 分钟阅读·2023年10月12日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f20f48d260492d8ecb7fb2bf39ec6862.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f20f48d260492d8ecb7fb2bf39ec6862.png)'
- en: Machine learning icons created by Becris — Flatico. [https://www.flaticon.com/free-icons/machine-learning](https://www.flaticon.com/free-icons/machine-learning)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习图标由 Becris — Flatico 创建。 [https://www.flaticon.com/free-icons/machine-learning](https://www.flaticon.com/free-icons/machine-learning)
- en: Background
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: 'In my previous article, we introduced the [***multi-layer perceptron***](https://en.wikipedia.org/wiki/Multilayer_perceptron)
    ***(MLP)****,* which is just a set of stacked interconnected [***perceptrons***](https://en.wikipedia.org/wiki/Perceptron).
    I highly recommend you check my previous post if you are unfamiliar with the perceptron
    and MLP as will discuss it quite a bit in this article:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在我之前的文章中，我们介绍了 [***多层感知器***](https://en.wikipedia.org/wiki/Multilayer_perceptron)
    ***(MLP)****，它只是堆叠互连的 [***感知器***](https://en.wikipedia.org/wiki/Perceptron) 的集合。如果你不熟悉感知器和
    MLP，我强烈建议你查看我之前的文章，因为这篇文章将会详细讨论：
- en: '[](https://levelup.gitconnected.com/intro-perceptron-architecture-neural-networks-101-2a487062810c?source=post_page-----ab0036a2e701--------------------------------)
    [## Intro, Perceptron & Architecture: Neural Networks 101'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://levelup.gitconnected.com/intro-perceptron-architecture-neural-networks-101-2a487062810c?source=post_page-----ab0036a2e701--------------------------------)
    [## 介绍、感知器与架构：神经网络 101'
- en: An introduction to Neural Networks and their building blocks
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络及其构建模块的介绍
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/intro-perceptron-architecture-neural-networks-101-2a487062810c?source=post_page-----ab0036a2e701--------------------------------)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: levelup.gitconnected.com](https://levelup.gitconnected.com/intro-perceptron-architecture-neural-networks-101-2a487062810c?source=post_page-----ab0036a2e701--------------------------------)
- en: 'An example MLP with two hidden layers is shown below:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有两个隐藏层的示例 MLP 如下所示：
- en: '![](../Images/94cdac9897a5e852d9cd6edd425fcba0.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/94cdac9897a5e852d9cd6edd425fcba0.png)'
- en: A basic two-hidden multi-layer perceptron. Diagram by author.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一个基本的双隐藏层多层感知器。图示由作者提供。
- en: 'However, the problem with the MLP is that it can only fit a [***linear classifier***](https://en.wikipedia.org/wiki/Linear_separability).
    This is because the individual perceptrons have a [***step function***](https://en.wikipedia.org/wiki/Step_function)
    as their [***activation function***](https://en.wikipedia.org/wiki/Activation_function),
    which is linear:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，MLP 的问题在于它只能拟合一个 [***线性分类器***](https://en.wikipedia.org/wiki/Linear_separability)。这是因为各个感知器具有
    [***阶跃函数***](https://en.wikipedia.org/wiki/Step_function) 作为其 [***激活函数***](https://en.wikipedia.org/wiki/Activation_function)，这是一种线性函数：
- en: '![](../Images/aa3d36442d4a95bbdd62ecbac912ad3a.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa3d36442d4a95bbdd62ecbac912ad3a.png)'
- en: The Perceptron, which is the simplest neural network. Diagram by author.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器，这是最简单的神经网络。图示由作者提供。
- en: So despite stacking our perceptrons may look like a modern-day neural network,
    it is still a linear classifier and not that much different from regular linear
    regression!
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管堆叠我们的感知器看起来像是现代神经网络，但它仍然是一个线性分类器，与普通的线性回归没有太大区别！
- en: Another problem is that it is not fully differentiable over the whole domain
    range.
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 另一个问题是，它在整个领域范围内并非完全可微分。
- en: So, what do we do about it?
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们该如何处理呢？
- en: Non-Linear Activation Functions!
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 非线性激活函数！
- en: Why Do We Need Non-Linearity?
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么我们需要非线性？
- en: What is Linearity?
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是线性性？
- en: 'Let’s quickly state what linearity means to build some context. Mathematically,
    a function is considered linear if it satisfies the following condition:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速说明一下线性性的含义，以建立一些背景。数学上，如果一个函数满足以下条件，则认为它是线性的：
- en: '![](../Images/5ff20f8ab0a88e38e57c203934156124.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5ff20f8ab0a88e38e57c203934156124.png)'
- en: Equation by author in LaTeX.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 作者用 LaTeX 编写的方程。
- en: 'There is also another condition:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 还有另一个条件：
- en: '![](../Images/7924532d6bff8fdc08fb33c5d3f79615.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7924532d6bff8fdc08fb33c5d3f79615.png)'
- en: Equation by author in LaTeX.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 作者用 LaTeX 编写的方程。
- en: But, we will work with the previously equation for this demonstration.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们将使用之前的方程进行演示。
- en: 'Take this very simple case:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以这个非常简单的例子为例：
- en: '![](../Images/27e61559c5cf914c7b7fe6a28aeac415.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/27e61559c5cf914c7b7fe6a28aeac415.png)'
- en: Equation by author in LaTeX.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 作者用 LaTeX 编写的方程。
- en: So the function ***f(x) = 10x*** is linear!
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 所以函数 ***f(x) = 10x*** 是线性的！
- en: If we added a bias term to the above equation it’s no longer be a linear function
    but rather an [**affine function**](https://en.wikipedia.org/wiki/Affine_transformation).
    See [this statexchange thread](https://math.stackexchange.com/questions/275310/what-is-the-difference-between-linear-and-affine-function)
    for the difference.
  id: totrans-36
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果我们在上述方程中添加一个偏置项，它将不再是线性函数，而是[**仿射函数**](https://en.wikipedia.org/wiki/Affine_transformation)。请参见[这个
    statexchange 线程](https://math.stackexchange.com/questions/275310/what-is-the-difference-between-linear-and-affine-function)了解其区别。
- en: 'Now, consider this case:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑以下情况：
- en: '![](../Images/856701d94e726496754d19133fe10b32.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/856701d94e726496754d19133fe10b32.png)'
- en: Equation by author in LaTeX.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 作者用 LaTeX 编写的方程。
- en: 'So the function ***f(x) = 10x²*** is *not* linear. As most of us will know,
    this function is quadratic and has a parabola shape:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 所以函数 ***f(x) = 10x²*** 是 *非* 线性的。如我们大多数人所知，这个函数是二次的，具有抛物线形状：
- en: Gist by author.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 作者总结。
- en: '![](../Images/9100d65fd7dc2194a0e23a1f4a431761.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9100d65fd7dc2194a0e23a1f4a431761.png)'
- en: Example parabola. Plot generated by author in Python.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 示例抛物线。图形由作者在 Python 中生成。
- en: Definitely not linear!
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 绝对不是线性的！
- en: Linearity in Neural Networks
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络中的线性性
- en: We have already answered why we need non-linearity in neural networks, but let’s
    really dive in to this concept and what it means to our model.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经回答了为什么神经网络需要非线性，但让我们深入探讨一下这个概念以及它对我们模型的意义。
- en: 'The output of each perceptron can be mathematically written as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 每个感知器的输出可以用以下数学公式表示：
- en: '![](../Images/60e9f3aa07b7135e6c0eadb2b8a184ca.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/60e9f3aa07b7135e6c0eadb2b8a184ca.png)'
- en: Equation by author in LaTeX.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 作者用 LaTeX 编写的方程。
- en: Where ***f*** is the step function (activation function), ***y*** is the output
    of the perceptron, ***b*** is the bias term, ***n*** is the number of features,
    and ***w_i*** and ***x_i*** are the [***weights***](https://en.wikipedia.org/wiki/Weighting)
    and their corresponding input values. This equation is just a mathematical version
    of the perceptron diagram above.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ***f*** 是步进函数（激活函数），***y*** 是感知器的输出，***b*** 是偏置项，***n*** 是特征数量，***w_i***
    和 ***x_i*** 是[***权重***](https://en.wikipedia.org/wiki/Weighting) 及其对应的输入值。这个方程只是上面感知器图示的数学版本。
- en: 'Now, let’s imagine our activation function is the identity function, in other
    words, we have no function. Using this concept, now consider a feed-forward two-hidden
    layer MLP with two neurons in the middle layer (ignoring the bias terms):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设我们的激活函数是恒等函数，换句话说，我们没有任何函数。使用这一概念，现在考虑一个前馈的双隐藏层 MLP 中间层有两个神经元（忽略偏置项）：
- en: '![](../Images/7e743bf6079fe4879b43c69e4dc07d27.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e743bf6079fe4879b43c69e4dc07d27.png)'
- en: Two layer feed-forward neural network.Equation by author in LaTeX.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 两层前馈神经网络。作者用 LaTeX 编写的方程。
- en: You might be wondering what this means.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道这意味着什么。
- en: Well, what we have done here is condense the 2-layer MLP into a single-layer
    MLP! The final equation, in the above derivation, is simply a linear regression
    model with features ***x_1*** and ***x_2*** and their corresponding coefficients***.***
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我们在这里做的就是将 2 层 MLP 压缩成一个单层 MLP！上面推导出的最终方程仅仅是具有特征 ***x_1*** 和 ***x_2*** 及其对应系数的线性回归模型。
- en: So despite the MLP being ‘two layers’ it would to a single layer and become
    the good old linear regression model! This is not good as the neural network won’t
    be able to model or fit complex functions to the data
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，尽管 MLP 是“两层”的，它会变成单层，从而成为传统的线性回归模型！这不好，因为神经网络无法对数据进行建模或拟合复杂函数。
- en: By allowing the MLP to have non-linear activation functions we satisfy something
    called the [***Universal Approximation Theorem***](https://en.wikipedia.org/wiki/Universal_approximation_theorem).
    This theorem basically says that an MLP, or more correctly a neural network, can
    approximate and fit any function. Check out this post [here](http://mcneela.github.io/machine_learning/2017/03/21/Universal-Approximation-Theorem.html)
    if you want to learn more!
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通过允许 MLP 使用非线性激活函数，我们满足了所谓的 [***通用逼近定理***](https://en.wikipedia.org/wiki/Universal_approximation_theorem)。这个定理基本上表示
    MLP，或者更准确地说是神经网络，可以逼近和拟合任何函数。如果你想了解更多，查看 [这里](http://mcneela.github.io/machine_learning/2017/03/21/Universal-Approximation-Theorem.html)！
- en: Being linear is not necessarily a bad thing, it’s more that many real-world
    problems are non-linear, so we need non-linear models if we want to predict these
    phenomena.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 线性并不一定是坏事，更重要的是许多现实世界的问题是非线性的，因此如果我们想预测这些现象，就需要非线性模型。
- en: Another requirement for the activation function is that needs to be differentiable
    to enable [**gradient descent**](/why-gradient-descent-is-so-common-in-data-science-def3e6515c5c).
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 激活函数的另一个要求是需要可微分，以便启用 [**梯度下降**](/why-gradient-descent-is-so-common-in-data-science-def3e6515c5c)。
- en: Activation Functions
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活函数
- en: Let’s now quickly run through some of the most common non-linear activation
    functions.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们快速浏览一些最常见的非线性激活函数。
- en: Sigmoid
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Sigmoid
- en: The [***sigmoid function***](https://en.wikipedia.org/wiki/Sigmoid_function)is
    well known in the industry and derives from the [***binomial distribution***](https://medium.com/towards-artificial-intelligence/decoding-the-binomial-distribution-a-fundamental-concept-for-data-scientists-81c64c7e4580).
    It ‘squashes’ the inputs to an output between 0 and 1, so it can be viewed as
    a probability and has an ‘S-shape.’ However, most importantly it’s non-linear
    as it contains a division and an exponential function.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[***Sigmoid 函数***](https://en.wikipedia.org/wiki/Sigmoid_function) 在业界非常知名，源于
    [***二项分布***](https://medium.com/towards-artificial-intelligence/decoding-the-binomial-distribution-a-fundamental-concept-for-data-scientists-81c64c7e4580)。它将输入“挤压”到
    0 和 1 之间的输出，因此可以被视为概率，并且具有“S 形”曲线。然而，最重要的是它是非线性的，因为它包含了一个除法和一个指数函数。'
- en: 'The sigmoid function mathematically and visually looks like as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 函数在数学上和视觉上看起来如下：
- en: '![](../Images/29fff662bec33adab7258d011d2e9db6.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29fff662bec33adab7258d011d2e9db6.png)'
- en: Equation by author in LaTeX.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 方程由作者在 LaTeX 中生成。
- en: Gist by author.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 作者总结。
- en: '![](../Images/b71b01559c120a8967c3bf80bc04c16f.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b71b01559c120a8967c3bf80bc04c16f.png)'
- en: Sigmoid function. Plot generated by author in Python.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 函数。图表由作者在 Python 中生成。
- en: 'However, the sigmoid is not used in cutting-edge neural networks for the following
    reasons:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Sigmoid 函数在前沿神经网络中并未使用，原因如下：
- en: '[**Vanishing Gradient Problem**](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)
    **—** From the above plot, we can see the gradients at the extreme positive or
    negative values are near zero. This is a problem when training neural networks
    as it slows down learning and convergence.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**消失梯度问题**](https://en.wikipedia.org/wiki/Vanishing_gradient_problem) **—**
    从上述图中可以看到，在极端正值或负值时，梯度接近零。这在训练神经网络时是一个问题，因为它会减缓学习和收敛的速度。'
- en: '**Not Centred On Zero** — The sigmoid is between 0 and 1, so it is always positive.
    This is a problem when using gradient descent-based algorithms, as it will push
    or descend in one direction.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**未以零为中心** — Sigmoid 函数在 0 和 1 之间，因此它始终是正的。这在使用基于梯度下降的算法时是一个问题，因为它会朝一个方向推动或下降。'
- en: '**Computationally Expensive** — Computing exponentials are not efficient, and
    so this function is expensive especially in large neural nets when it is computed
    thousands of times.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算开销大** — 计算指数运算并不高效，因此这个函数特别是在大型神经网络中被计算数千次时，开销很大。'
- en: Tanh
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Tanh
- en: '[***Tanh***](https://en.wikipedia.org/wiki/Hyperbolic_functions) is a common
    trigonometric hyperbolic function that maps inputs to be between -1 and 1\. Therefore,
    it is zero-centred, so is an improvement upon the sigmoid function.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[***Tanh***](https://en.wikipedia.org/wiki/Hyperbolic_functions) 是一个常见的三角超越函数，它将输入映射到
    -1 和 1 之间。因此，它是以零为中心的，相比于 Sigmoid 函数有所改进。'
- en: 'The tanh function mathematically and visually looks like as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Tanh 函数在数学上和视觉上看起来如下：
- en: '![](../Images/fc548876673a33f9076e8494e4315631.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc548876673a33f9076e8494e4315631.png)'
- en: Equation by author in LaTeX.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者用 LaTeX 编写的方程。
- en: Gist by author.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 作者的概要。
- en: '![](../Images/b43c2e3f628349da147ec521cb2ded5b.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b43c2e3f628349da147ec521cb2ded5b.png)'
- en: Tanh function. Plot generated by author in Python.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Tanh 函数。由作者在 Python 中生成的图。
- en: 'Despite being zero-centred, the tanh function suffers from similar issues as
    the sigmoid function:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管以零为中心，tanh 函数也面临与 sigmoid 函数类似的问题：
- en: '**Vanishing Gradient Problem —** Tanh’s gradients are also near zero at the
    extremes of positives and negatives. This is a problem when training neural networks
    as it slows down learning and convergence.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度消失问题 —** Tanh 的梯度在正负极值处也接近零。这在训练神经网络时是一个问题，因为它会减缓学习和收敛速度。'
- en: '**Computationally Expensive** — Tanh has more exponentials to calculate, which
    makes it even less computationally efficient than the sigmoid.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算开销大 —** Tanh 需要计算更多的指数函数，这使得它在计算上比 sigmoid 更低效。'
- en: ReLU
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ReLU
- en: The [***rectified linear unit (ReLU)***](https://en.wikipedia.org/wiki/Rectifier_%28neural_networks%29)
    is the most popular activation function as it is computationally efficient and
    removes the issues with the vanishing gradient problem mentioned above.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[***修正线性单元 (ReLU)***](https://en.wikipedia.org/wiki/Rectifier_%28neural_networks%29)
    是最受欢迎的激活函数，因为它计算高效并且解决了上述的梯度消失问题。'
- en: 'Mathematically and visually it looks like:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学上和视觉上，它看起来像：
- en: '![](../Images/ad1cf21ebe2a1666034bee8102b78d29.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad1cf21ebe2a1666034bee8102b78d29.png)'
- en: Equation by author in LaTeX.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者用 LaTeX 编写的方程。
- en: Gist by author.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 作者的概要。
- en: '![](../Images/cf6409fc078754bcab71802d9bbe477e.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf6409fc078754bcab71802d9bbe477e.png)'
- en: ReLU. Plot generated by author in Python.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU。由作者在 Python 中生成的图。
- en: 'Even though the ReLU activation function has many advantages over the sigmoid
    and tanh functions, it suffers from some new issues:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 ReLU 激活函数相比于 sigmoid 和 tanh 函数有许多优点，但它也存在一些新问题：
- en: '[**Dead Neurons**](https://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks)
    **—** There is a “dying ReLU problem” where the neuron always outputs zero for
    any input due to this zero line for any value less than zero.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**死亡神经元**](https://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks)
    **—** 存在一种“死亡 ReLU 问题”，即由于对任何小于零的值都设定为零，导致神经元对任何输入始终输出零。'
- en: '**Unbounded Value —** The gradient can go to positive infinity. This can cause
    compute problems in extreme cases with such large numbers.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无界值 —** 梯度可以达到正无穷大。这在极端情况下可能导致计算问题。'
- en: '**Not Centred On Zero** — The ReLU is also not symmetrical around zero and
    its mean is positive. This can lead to issues during training.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不以零为中心** — ReLU 也不以零为对称，其均值为正。这可能在训练过程中引发问题。'
- en: Leaky ReLU
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Leaky ReLU
- en: 'We can adjust the ReLU function to produce the ‘Leaky’ ReLU, where the negative
    input is not zero, but rather has some shallow slope with gradient ***α***:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以调整 ReLU 函数以生成‘Leaky’ ReLU，其中负输入不为零，而是有一个浅斜率的梯度 ***α***：
- en: '![](../Images/d5dbcbc68a24c8ab953e5b19704fabfa.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d5dbcbc68a24c8ab953e5b19704fabfa.png)'
- en: Equation by author in LaTeX.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 由作者用 LaTeX 编写的方程。
- en: Gist by author.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 作者的概要。
- en: '![](../Images/4d44ec1c36473a7f01361080256878af.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d44ec1c36473a7f01361080256878af.png)'
- en: Leaky ReLU. Plot generated by author in Python.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Leaky ReLU。由作者在 Python 中生成的图。
- en: 'The Leaky ReLU avoids the dead neuron problem by having a little gradient,
    so the neurons can keep learning. Not to mention avoiding the vanishing gradients
    problem and being computationally efficient. However, like the other functions,
    it also has some flaws:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Leaky ReLU 通过拥有少量的梯度来避免死亡神经元问题，从而使神经元能够继续学习。更不用说它避免了梯度消失问题，并且计算上更高效。然而，与其他函数一样，它也有一些缺陷：
- en: '**Slope Choice**: The gradient of the leaky slope is not prescribed and needs
    to be chosen manually. It can always be hyperparameter tuned though.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**斜率选择**：Leaky 斜率的梯度没有规定，需要手动选择。不过，它可以通过超参数调整进行优化。'
- en: '**Not Centred On Zero** — Like normal ReLU it’s also not centred about zero.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不以零为中心** — 与普通 ReLU 相似，它也不以零为对称。'
- en: '**Unbounded Value —** Like ReLU, its gradient can go to positive infinity and
    is effectively unbounded.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无界值 —** 与 ReLU 类似，其梯度可以达到正无穷大，实际上是无界的。'
- en: Others
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他
- en: 'Here I have listed the four most common activation functions, however there
    are several others such as:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我列出了四种最常见的激活函数，然而还有一些其他函数，例如：
- en: '[**Swish**](https://medium.com/@neuralnets/swish-activation-function-by-google-53e1ea86f820)**:**
    This is a differentiable approximation of the ReLU function developed by Google,
    ***swish(x) = x * sigmoid(x)***.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Swish**](https://medium.com/@neuralnets/swish-activation-function-by-google-53e1ea86f820)**:**
    这是由Google开发的ReLU函数的可微近似，***swish(x) = x * sigmoid(x)***。'
- en: '**Gated Linear Unit (GLU)**: This is used primarily in [**recurrent neural
    networks**](https://en.wikipedia.org/wiki/Recurrent_neural_network).'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**门控线性单元（GLU）**：主要用于[**递归神经网络**](https://en.wikipedia.org/wiki/Recurrent_neural_network)。'
- en: '[**Softmax**](/softmax-activation-function-how-it-actually-works-d292d335bd78#:~:text=Softmax%20is%20an%20activation%20function,all%20possible%20outcomes%20or%20classes.):
    Primarily used for multi-nomial classification problems.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Softmax**](/softmax-activation-function-how-it-actually-works-d292d335bd78#:~:text=Softmax%20is%20an%20activation%20function,all%20possible%20outcomes%20or%20classes.)：主要用于多项分类问题。'
- en: One activation is not better than the other, its always best to try out various
    ones and see which one best works for your model.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 一种激活函数并不一定优于另一种，最好是尝试各种激活函数，看看哪一种最适合你的模型。
- en: Summary & Further Thoughts
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结与进一步思考
- en: To make a neural network satisfy the universal approximation theorem, it needs
    to contain a non-linear activation function, otherwise you can squash into a single
    layer, basically a linear regression model. There are many suitable activation
    function, the one that is most frequently used is the ReLU. However, this is not
    a hard and fast rule, and you should experiment with various functions to find
    the best one for your model.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 要使神经网络满足通用逼近定理，它需要包含一个非线性激活函数，否则你可以将其压缩到单层，基本上就是一个线性回归模型。有很多合适的激活函数，最常用的是ReLU。然而，这并不是一个硬性规则，你应该尝试各种函数以找到最适合你模型的那一个。
- en: 'The full code used in this article is available at my GitHub:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中使用的完整代码可以在我的GitHub上找到：
- en: '[](https://github.com/egorhowell/Medium-Articles/blob/main/Data%20Science%20Basics/activation_functions.py?source=post_page-----ab0036a2e701--------------------------------)
    [## Medium-Articles/Data Science Basics/activation_functions.py at main · egorhowell/Medium-Articles'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/egorhowell/Medium-Articles/blob/main/Data%20Science%20Basics/activation_functions.py?source=post_page-----ab0036a2e701--------------------------------)
    [## Medium-Articles/Data Science Basics/activation_functions.py at main · egorhowell/Medium-Articles'
- en: Code I use in my medium blog/articles. Contribute to egorhowell/Medium-Articles
    development by creating an account on…
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我在Medium博客/文章中使用的代码。通过创建一个账户来贡献于egorhowell/Medium-Articles的开发…
- en: github.com](https://github.com/egorhowell/Medium-Articles/blob/main/Data%20Science%20Basics/activation_functions.py?source=post_page-----ab0036a2e701--------------------------------)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/egorhowell/Medium-Articles/blob/main/Data%20Science%20Basics/activation_functions.py?source=post_page-----ab0036a2e701--------------------------------)
- en: References & Further Reading
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考资料与进一步阅读
- en: '[*Great article on activation functions*](/activation-functions-neural-networks-1cbd9f8d91d6)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*关于激活函数的优秀文章*](/activation-functions-neural-networks-1cbd9f8d91d6)'
- en: '[*Non-linear neural networks*](https://medium.com/ml-cheat-sheet/understanding-non-linear-activation-functions-in-neural-networks-152f5e101eeb)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*非线性神经网络*](https://medium.com/ml-cheat-sheet/understanding-non-linear-activation-functions-in-neural-networks-152f5e101eeb)'
- en: '[*Intro to neural networks*](https://web.pdx.edu/~nauna/week7b-neuralnetwork.pdf)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*神经网络简介*](https://web.pdx.edu/~nauna/week7b-neuralnetwork.pdf)'
- en: Another Thing!
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另一件事！
- en: I have a free newsletter, [**Dishing the Data**](https://dishingthedata.substack.com/),
    where I share weekly tips for becoming a better Data Scientist. There is no “fluff”
    or “clickbait,” just pure actionable insights from a practicing Data Scientist.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我有一个免费的新闻简报，[**Dishing the Data**](https://dishingthedata.substack.com/)，每周分享成为更好的数据科学家的小贴士。没有“废话”或“点击诱饵”，只有来自实际数据科学家的纯粹可操作见解。
- en: '[](https://newsletter.egorhowell.com/?source=post_page-----ab0036a2e701--------------------------------)
    [## Dishing The Data | Egor Howell | Substack'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://newsletter.egorhowell.com/?source=post_page-----ab0036a2e701--------------------------------)
    [## Dishing The Data | Egor Howell | Substack'
- en: How To Become A Better Data Scientist. Click to read Dishing The Data, by Egor
    Howell, a Substack publication with…
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何成为更好的数据科学家。点击阅读Egor Howell的Dishing The Data，这是一个Substack出版物，内容包括…
- en: newsletter.egorhowell.com](https://newsletter.egorhowell.com/?source=post_page-----ab0036a2e701--------------------------------)
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: newsletter.egorhowell.com](https://newsletter.egorhowell.com/?source=post_page-----ab0036a2e701--------------------------------)
- en: Connect With Me!
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与我联系！
- en: '[**YouTube**](https://www.youtube.com/@egorhowell)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**YouTube**](https://www.youtube.com/@egorhowell)'
- en: '[**LinkedIn**](https://www.linkedin.com/in/egor-howell-092a721b3/)'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**LinkedIn**](https://www.linkedin.com/in/egor-howell-092a721b3/)'
- en: '[**Twitter**](https://twitter.com/EgorHowell)'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Twitter**](https://twitter.com/EgorHowell)'
- en: '[**GitHub**](https://github.com/egorhowell)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**GitHub**](https://github.com/egorhowell)'
