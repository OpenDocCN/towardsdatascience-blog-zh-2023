- en: Shape Reconstruction with ONets
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用ONets进行形状重建
- en: 原文：[https://towardsdatascience.com/shape-reconstruction-with-onets-1c1afe89c50](https://towardsdatascience.com/shape-reconstruction-with-onets-1c1afe89c50)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/shape-reconstruction-with-onets-1c1afe89c50](https://towardsdatascience.com/shape-reconstruction-with-onets-1c1afe89c50)
- en: Representing 3D space with learnable functions
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用可学习函数表示3D空间
- en: '[](https://wolfecameron.medium.com/?source=post_page-----1c1afe89c50--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----1c1afe89c50--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1c1afe89c50--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1c1afe89c50--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----1c1afe89c50--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wolfecameron.medium.com/?source=post_page-----1c1afe89c50--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----1c1afe89c50--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1c1afe89c50--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1c1afe89c50--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----1c1afe89c50--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1c1afe89c50--------------------------------)
    ·11 min read·Feb 7, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1c1afe89c50--------------------------------)
    ·11分钟阅读·2023年2月7日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/7b032ee38e423c492d6d1afd44c4febf.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7b032ee38e423c492d6d1afd44c4febf.png)'
- en: (Photo by [Tareq Ajalyakin](https://unsplash.com/@tareq_aj?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/points?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: （照片由 [Tareq Ajalyakin](https://unsplash.com/@tareq_aj?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    拍摄，来源于 [Unsplash](https://unsplash.com/s/photos/points?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)）
- en: The problem of 3D reconstruction aims to produce a high-resolution object representation
    given a noisy view of the object (e.g., partial point cloud, 2D image, etc.) as
    input. Recently, deep neural networks have become a popular approach for 3D reconstruction,
    as they can encode prior information that helps to handle ambiguity. Put simply,
    this means that if the correct way to reconstruct a given object is unclear from
    the input, *a neural network can draw from its experience with other data points
    that it has seen during training to still produce a reasonable output*.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 3D重建的问题旨在根据物体的噪声视图（例如，部分点云、2D图像等）生成高分辨率的物体表示。最近，深度神经网络成为3D重建的热门方法，因为它们可以编码有助于处理模糊的信息。简单来说，这意味着如果从输入中不清楚如何正确重建给定的物体，*神经网络可以借鉴它在训练过程中遇到的其他数据点的经验，仍然生成合理的输出*。
- en: Most 3D reconstruction approaches were originally limited in their ability to
    represent high-resolution objects. Voxels, point clouds, and meshes all fell short
    of modeling high-resolution objects in a memory efficient manner. Compared to
    models like GANs [2] that could generate high-resolution, realistic images, comparable
    approaches for generating 3D geometries were primitive.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数3D重建方法最初在表示高分辨率物体的能力上存在限制。体素、点云和网格在以内存高效的方式建模高分辨率物体方面都存在不足。与如GANs [2] 等可以生成高分辨率、逼真图像的模型相比，用于生成3D几何的可比方法还处于初级阶段。
- en: '![](../Images/d5698249a6c8526e8d37cfb37058c157.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d5698249a6c8526e8d37cfb37058c157.png)'
- en: (from [1])
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: To solve this problem, authors in [1] develop a 3D reconstruction approach that
    models occupancy functions with neural networks. More specifically, we train a
    neural network to predict whether a given point in space is occupied (or not)
    by an object (i.e., an *occupancy function*!). Then, the underlying object is
    represented by the decision boundary (i.e., locations where predictions switch
    from occupied to not occupied) of this neural network; see above. Occupancy networks
    (ONets) can represent and reconstruct 3D shapes with arbitrary precision and reasonable
    memory requirements.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，文献[1]中作者提出了一种3D重建方法，该方法使用神经网络对占据函数进行建模。更具体地说，我们训练一个神经网络来预测空间中给定点是否被物体占据（即，*占据函数*！）。然后，底层物体通过该神经网络的决策边界（即，预测从占据到未占据的切换位置）来表示；见上文。占据网络（ONets）可以以任意精度和合理的内存要求表示和重建3D形状。
- en: '![](../Images/cb91cda5485e5dba89bf19351192e5e7.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb91cda5485e5dba89bf19351192e5e7.png)'
- en: (from [1])
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: Background
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: 'In our prior review of [DeepSDF](https://cameronrwolfe.substack.com/p/3d-generative-modeling-with-deepsdf)
    for 3D shape generation, we covered a few relevant background concepts, including:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前对[DeepSDF](https://cameronrwolfe.substack.com/p/3d-generative-modeling-with-deepsdf)的3D形状生成的回顾中，我们涵盖了一些相关的背景概念，包括：
- en: Point Cloud, Mesh, and Voxel Representations [[link](https://cameronrwolfe.substack.com/i/94634004/representing-d-shapes)]
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点云、网格和体素表示 [[link](https://cameronrwolfe.substack.com/i/94634004/representing-d-shapes)]
- en: Feed-Forward Neural Networks [[link](https://cameronrwolfe.substack.com/i/94634004/feed-forward-neural-networks)]
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前馈神经网络 [[link](https://cameronrwolfe.substack.com/i/94634004/feed-forward-neural-networks)]
- en: Most neural networks in [1] use a simple, feed-forward architecture, which serves
    as an alternative to storing 3D shapes in their canonical mesh, voxel, or point
    cloud formats. To understand why this is preferable, we have to go into a bit
    more detail regarding the limitations of these representations.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[1]中的大多数神经网络使用了简单的前馈架构，这种架构作为一种替代方案，用于存储3D形状的标准网格、体素或点云格式。为了理解为什么这种方法更可取，我们需要更详细地了解这些表示的限制。'
- en: Shortcomings of 3D Shape Representations
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3D 形状表示的缺点
- en: '![](../Images/ce853b48a41956dd0c5351651c38db31.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce853b48a41956dd0c5351651c38db31.png)'
- en: (from [6])
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [6]）
- en: 3D geometries are typically stored or represented as meshes, voxels, or point
    clouds; see above. Given that these representations are available, *why would
    we want to represent shapes using a neural network instead?* The simple answer
    is that *(i)* other representations have some notable limitations and *(ii)* we
    can save a lot of memory this way.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 3D几何形状通常以网格、体素或点云的形式存储或表示；见上文。鉴于这些表示已经存在，*我们为什么还要使用神经网络来表示形状呢？* 简单的答案是 *(i)*
    其他表示方法存在一些显著的限制，以及 *(ii)* 我们可以通过这种方式节省大量内存。
- en: '**voxels are memory inefficient.** In deep learning applications, voxels are
    the most widely-used representations for 3D shapes due to their simplicity — they
    are a straightforward generalization of pixels to 3D space. If we encode a 3D
    shape using voxels, however, the memory footprint of this encoding grows cubically
    with its resolution. If we want a more precise voxel representation, we will need
    to use a lot more memory.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**体素在内存使用上不高效。** 在深度学习应用中，体素是用于3D形状的最广泛使用的表示形式，因为它们简单——它们是像素在3D空间中的直接推广。然而，如果我们使用体素来编码一个3D形状，这种编码的内存占用会随着分辨率的提高而立方增长。如果我们想要更精确的体素表示，我们需要使用更多的内存。'
- en: '**point clouds are disconnected.** Point clouds are similar in format to data
    that we would typically get from sensors like LiDAR, but the resulting geometry
    is disconnected — it is just a bunch of points in 3D space. As a result, extracting
    the actual 3D shape from this data requires expensive post-processing procedures.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**点云是断开的。** 点云的格式类似于我们通常从传感器（如LiDAR）获得的数据，但结果几何形状是断开的——它只是3D空间中的一堆点。因此，从这些数据中提取实际的3D形状需要昂贵的后处理程序。'
- en: '**meshes aren’t the solution.** *If point clouds need post-processing and voxels
    aren’t memory efficient, we should use meshes, right?* Unfortunately, most mesh
    representations are actually based upon deforming a “template” mesh. In practice,
    this means that meshes cannot encode arbitrary topologies, meaning that they are
    limited in their ability to accurately represent certain geometries.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**网格并不是解决方案。** *如果点云需要后处理而体素在内存使用上不高效，我们应该使用网格，对吗？* 不幸的是，大多数网格表示实际上是基于变形的“模板”网格。实际上，这意味着网格无法编码任意的拓扑结构，这使得它们在准确表示某些几何形状时受到限制。'
- en: '**what should we do then?** With these limitations in mind, the approach proposed
    in [1] starts to make a little more sense. Instead of directly storing mesh, voxel,
    or point cloud representations of 3D shapes, we can train a neural network to
    generate output from which the shape can be recovered. This way, we can store
    a bunch of different geometries within the parameters of a single neural network,
    which has a fixed memory cost!'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**那么我们该怎么办？** 考虑到这些限制，[1]中提出的方法开始变得更加合理。我们可以训练一个神经网络来生成可以恢复形状的输出，而不是直接存储3D形状的网格、体素或点云表示。通过这种方式，我们可以在一个具有固定内存成本的神经网络参数中存储大量不同的几何形状！'
- en: Occupancy Functions
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 占用函数
- en: The work in [1] represents occupancy functions with neural networks, but *what
    is an occupancy function*? Put simply, this is just a function that takes a point
    in space as an input (e.g., an `[x, y, z]` coordinate) and returns a binary output,
    representing whether this location is “occupied” by the object in question.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[1]中的工作将占据函数表示为神经网络，但*占据函数是什么*？简单来说，这只是一个将空间中的点（例如，`[x, y, z]` 坐标）作为输入，并返回一个二进制输出的函数，表示该位置是否被目标对象“占据”。'
- en: '![](../Images/ffc36c0f7e4744d5e17edda068594e9c.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ffc36c0f7e4744d5e17edda068594e9c.png)'
- en: Signature of an occupancy function
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 占据函数的特征
- en: Such functions can be approximated with a neural network that is taught to output
    a probability between zero and one given an `[x, y, z]` coordinate as input.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的函数可以通过一个神经网络进行逼近，该网络被训练为在给定 `[x, y, z]` 坐标作为输入时输出零到一之间的概率。
- en: '**extracting an isosurface.** To extract a 3D geometry from an occupancy function,
    we have to find an isosurface. To do this, we just find points in 3D space at
    which the occupancy function is equal to some threshold, which we will call `t`.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**提取等值面。** 要从占据函数中提取3D几何体，我们必须找到一个等值面。为此，我们只需在3D空间中找到占据函数等于某个阈值 `t` 的点。'
- en: '![](../Images/3cb7993edb8ea1fe547ac9e5c1136f34.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3cb7993edb8ea1fe547ac9e5c1136f34.png)'
- en: An isosurface
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 等值面
- en: Here, t is set to some value between zero and one. Thus, the isosurface represents
    the boundary of the occupancy function, where the output switches from zero to
    one (or vice versa) — this just corresponds to the surface of the underlying object!
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，t被设置为零到一之间的某个值。因此，等值面表示占据函数的边界，即输出从零切换到一（或反之）的地方——这对应于基础对象的表面！
- en: Evaluation Metrics
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估指标
- en: The metrics used to judge the quality of 3D shapes is quite similar to the metrics
    that we see in normal computer vision. The main metrics used in [1] are explained
    below.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 用于判断3D形状质量的指标与我们在普通计算机视觉中看到的指标非常相似。[1]中使用的主要指标如下所述。
- en: '**volumetric IoU.** The volume of two shapes’ intersection divided by the volume
    of their union. This metric is the same as [normal IoU](https://learnopencv.com/intersection-over-union-iou-in-object-detection-and-segmentation/),
    but it has been generalized to work in three dimensions.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**体积IoU。** 两个形状交集的体积除以它们并集的体积。该指标与[法线IoU](https://learnopencv.com/intersection-over-union-iou-in-object-detection-and-segmentation/)相同，但它已被推广到三维。'
- en: '**chamfer-L1.** The mean of accuracy and completeness. Accuracy is the mean
    distance of points on an output mesh to the nearest point on the ground truth
    mesh. Completeness is the same, but in the opposite direction.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**Chamfer-L1。** 精度和完整性的均值。精度是输出网格上点到地面真实网格最近点的平均距离。完整性是相同的，但方向相反。'
- en: '**normal consistency.** We take a [face normal](https://adamsturge.github.io/Engine-Blog/mydoc_mesh_normals.html#face-normals)
    (i.e., a vector that’s orthogonal to the plane of one of the mesh’s faces) of
    the predicted mesh, find the face normal of the corresponding nearest neighbor
    in the other mesh, then take the absolute value of these vectors’ [dot product](https://en.wikipedia.org/wiki/Dot_product).
    By repeating this process over all normals in the predicted mesh and taking a
    mean, we get the normal consistency. This one’s a bit tougher to understand, but
    it’s useful for determining whether higher-order information is captured in the
    predicted shape (i.e., whether the faces of the two meshes tend to point in the
    same direction).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**法线一致性。** 我们取预测网格的[面法线](https://adamsturge.github.io/Engine-Blog/mydoc_mesh_normals.html#face-normals)（即一个垂直于网格某一面平面的向量），找到另一个网格中对应最近邻的面法线，然后取这些向量的[点积](https://en.wikipedia.org/wiki/Dot_product)的绝对值。通过对预测网格中的所有法线重复这一过程并取平均值，我们得到法线一致性。这一指标稍显复杂，但它对于确定预测形状是否捕捉到高阶信息（即两个网格的面是否趋向于指向相同方向）非常有用。'
- en: '[Occupancy Networks](https://arxiv.org/abs/1812.03828) [1]'
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[占据网络](https://arxiv.org/abs/1812.03828) [1]'
- en: 'Given our prior overview of DeepSDF, the first question we might ask about
    Occupancy Networks is: *Why model an occupancy function instead of alternatives
    like* [*signed distance functions (SDFs)*](https://cameronrwolfe.substack.com/i/94634004/signed-distance-functions)*?*
    The basic answer is that occupancy functions are much easier to learn.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解DeepSDF的基本概念后，我们可能会问占据网络的第一个问题是：*为什么要建模占据函数，而不是像* [*有符号距离函数（SDFs）*](https://cameronrwolfe.substack.com/i/94634004/signed-distance-functions)*这样的替代方法？*
    基本的答案是，占据函数更容易学习。
- en: '*“[SDFs are] usually much harder to learn compared to occupancy representations
    as the network must reason about distance functions in 3D space instead of merely
    classifying a voxel as occupied or not.” — from [1]*'
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“[SDFs 通常比占据表示更难学习，因为网络必须在 3D 空间中推断距离函数，而不仅仅是将体素分类为占据或未占据。]” — 摘自 [1]*'
- en: '**the network.** To approximate an occupancy function, we use a feed-forward
    neural network that assigns a probability between zero and one to any 3D coordinate.
    The network’s input includes:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**网络。** 为了逼近占据函数，我们使用一个前馈神经网络，该网络将一个介于零和一之间的概率分配给任何 3D 坐标。网络的输入包括：'
- en: A (single) 3D coordinate.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个（单一的）3D 坐标。
- en: Noisy observations of the underlying object.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对底层物体的噪声观察。
- en: The network, which we call an ONet, outputs a scalar probability. The noisy
    observations of the underling object that are passed as input to the model could
    be something like an incomplete point cloud or a coarse voxel grid. We are conditioning
    the neural network on this noisy data, then using it to produce a much more accurate
    representation of the object; see below.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称之为 ONet 的网络输出一个标量概率。作为模型输入的底层物体的噪声观察可能是诸如不完整的点云或粗略的体素网格之类的东西。我们将神经网络以这些噪声数据为条件，然后使用它生成物体的更精确表示；见下文。
- en: '![](../Images/ea83a1ae73aedc9baf434d88801b8960.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea83a1ae73aedc9baf434d88801b8960.png)'
- en: Modeling the occupancy function at each spatial location
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对每个空间位置建模占据函数
- en: We cannot directly pass the observed, noisy object data as input to the feed-forward
    neural network because it many come in many different formats. Instead, authors
    in [1] embed this data (i.e., turn it into a vector) using different neural network-based
    encoders (e.g., ResNets [4] for images or PointNet [5] for point clouds; these
    are just common network architectures for converting images and point clouds to
    vectors).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能直接将观察到的噪声数据作为输入传递给前馈神经网络，因为它们可能有许多不同的格式。相反，[1]中的作者使用不同的基于神经网络的编码器（例如，图像的
    ResNets [4] 或点云的 PointNet [5]；这些只是将图像和点云转换为向量的常见网络架构）将这些数据（即，将其转换为向量）嵌入。
- en: Then, the first layer of the feed-forward neural network uses [conditional batch
    normalization](https://paperswithcode.com/method/conditional-batch-normalization)
    — a class-conditional [batch normalization](https://paperswithcode.com/method/batch-normalization)
    variant — to adapt the network’s input based upon the object embedding. In this
    way, we ensure that the ONet’s input is conditioned upon the data that is already
    available about the 3D geometry we are trying to reconstruct.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，前馈神经网络的第一层使用 [条件批量归一化](https://paperswithcode.com/method/conditional-batch-normalization)
    — 一种条件 [批量归一化](https://paperswithcode.com/method/batch-normalization) 变体 — 根据物体嵌入调整网络的输入。这样，我们确保
    ONet 的输入以我们尝试重建的 3D 几何数据为条件。
- en: '**training.** To train the ONet, we *(i)* consider a padded volume of space
    around an object used for training and *(ii)* uniformly sample `K` occupancy function
    values throughout this space. A mini-batch is formed by applying this sampling
    procedure to several training objects. We train the network normally using mini-batch
    gradient descent with a [cross entropy loss](/cross-entropy-loss-function-f38c4ec8643e)
    that is applied per occupancy function value in the mini-batch.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练。** 为了训练 ONet，我们 *(i)* 考虑一个围绕训练物体的填充空间体积，以及 *(ii)* 在这个空间中均匀采样 `K` 个占据函数值。通过将这种采样过程应用于多个训练物体来形成一个小批量。我们使用带有每个小批量中占据函数值的
    [交叉熵损失](/cross-entropy-loss-function-f38c4ec8643e) 的小批量梯度下降法正常训练网络。'
- en: '**generating objects.** Once the ONet is trained, we can output occupancy function
    values given a spatial location at arbitrary resolution. But, *how do we use this
    to create an actual 3D object (e.g., in mesh format)?* To do this,authors in [1]
    propose a Multi-resolution IsoSurface Extraction (MISE) procedure. This procedure
    is inspired by the idea of an [octree](https://en.wikipedia.org/wiki/Octree),
    a tree data structure that recursively represents 3D space. Each octree node has
    eight children, and we can recursively add children to each node to represent
    a volume with higher resolution.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成对象。** 一旦 ONet 训练完成，我们可以在任意分辨率下输出给定空间位置的占据函数值。但是，*我们如何使用这些值来创建实际的 3D 对象（例如，以网格格式）？*
    为此，[1]中的作者提出了一种多分辨率等值面提取（MISE）过程。该过程受到 [八叉树](https://en.wikipedia.org/wiki/Octree)
    的启发，八叉树是一种递归表示 3D 空间的树形数据结构。每个八叉树节点有八个子节点，我们可以递归地向每个节点添加子节点，以表示更高分辨率的体积。'
- en: 'The basic procedure for MISE is as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: MISE 的基本过程如下：
- en: Discretize space at an initial resolution
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在初始分辨率下离散化空间
- en: Evaluate the ONet at each discrete location in this space
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在该空间的每个离散位置评估 ONet
- en: Mark all voxels that have at least two adjacent voxels with differing occupancy
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标记所有具有至少两个相邻体素且占用情况不同的体素
- en: Divide marked voxels into eight sub-voxels and repeat until the desired resolution
    is reached
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将标记的体素划分为八个子体素，并重复直到达到所需的分辨率
- en: Apply [Marching Cubes](https://graphics.stanford.edu/~mdfisher/MarchingCubes.html#:~:text=Marching%20cubes%20is%20a%20simple,a%20region%20of%20the%20function.)
    to obtain a mesh
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用 [Marching Cubes](https://graphics.stanford.edu/~mdfisher/MarchingCubes.html#:~:text=Marching%20cubes%20is%20a%20simple,a%20region%20of%20the%20function.)
    以获取网格
- en: Thus, MISE obtains a mesh by recursively evaluating the ONet at higher resolutions
    where it is needed (i.e., close to the object boundary). Although we must apply
    a few extra steps to refine this mesh, the overall process is pretty intuitive;
    see below.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，MISE 通过在需要的更高分辨率下递归评估 ONet 来获得网格（即，接近对象边界）。尽管我们必须应用一些额外的步骤来精细化此网格，但整体过程相当直观；见下文。
- en: '![](../Images/a1b7fbb48e4ddce5fed7a3b3b61c3050.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a1b7fbb48e4ddce5fed7a3b3b61c3050.png)'
- en: (from [1])
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: '**experimental results.** ONets are mostly evaluated on the synthetic [ShapeNet](https://shapenet.org/)
    dataset based on their ability to represent complex 3D shapes and/or reconstruct
    them from images, noisy point clouds, and low resolution voxel grids. In these
    experiments, we see that ONets can accurately represent the “Chair” portion of
    ShapeNet. Using an ON, we can encode nearly 5K objects independently of resolution
    using only 6M parameters. In contrast, voxel representations are not as accurate
    and their memory requirements increase with the desired resolution; see below.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**实验结果。** ONets 主要在合成 [ShapeNet](https://shapenet.org/) 数据集上进行评估，基于其表示复杂 3D
    形状和/或从图像、噪声点云和低分辨率体素网格中重建它们的能力。在这些实验中，我们看到 ONets 可以准确地表示 ShapeNet 的“椅子”部分。使用 ONet，我们可以使用仅
    6M 参数独立编码近 5K 对象。相比之下，体素表示不够准确，并且其内存要求随着所需分辨率的增加而增加；见下文。'
- en: '![](../Images/b63da98dfcfdbfd51bafc02356733af8.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b63da98dfcfdbfd51bafc02356733af8.png)'
- en: (from [1])
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: In reconstruction experiments, we continue to see that ONets work quite well.
    They can recover complicated shapes and tend to produce results that are closest
    to the ground truth geometry. Most baseline methodologies tend to suffer from
    limitations, such as producing coarse representations, disconnected objects that
    require post-processing, or objects that lack relevant details. Qualitative examples
    of model output are shown below.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在重建实验中，我们继续看到 ONets 工作得相当好。它们能够恢复复杂的形状，并且往往产生最接近真实几何的结果。大多数基准方法通常存在限制，例如产生粗糙的表示、需要后处理的断开对象或缺乏相关细节的对象。模型输出的定性示例如下所示。
- en: '![](../Images/13a800222705e2c24bd2365064c82782.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13a800222705e2c24bd2365064c82782.png)'
- en: (from [1])
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: When trying to reconstruct geometries from noisy point clouds and coarse voxel
    grids (i.e., instead of images as is done above), we see that the ONet continues
    to perform quite well; see below.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 当尝试从噪声点云和粗糙体素网格（即，而不是像上面那样使用图像）重建几何时，我们看到 ONet 继续表现良好；见下文。
- en: '![](../Images/4445dda408faa42f60b82ff67e372c43.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4445dda408faa42f60b82ff67e372c43.png)'
- en: (from [1])
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: '**other stuff.** ONets are also applied to real-world data by taking images
    from [KITTI](https://www.cvlibs.net/datasets/kitti/) and the [Online Products](https://github.com/rksltnl/Deep-Metric-Learning-CVPR16)
    datasets. Despite being trained on only synthetic data, the ONet seems to generalize
    pretty well to this type of data. Notably, however, the authors do use segmentation
    masks provided by KITTI to extract pixels associated with the desired object.
    Some examples of the reconstructions generated from these datasets are shown below.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**其他内容。** ONets 还通过从 [KITTI](https://www.cvlibs.net/datasets/kitti/) 和 [Online
    Products](https://github.com/rksltnl/Deep-Metric-Learning-CVPR16) 数据集中获取图像应用于现实世界数据。尽管仅在合成数据上训练，ONet
    似乎对这种类型的数据泛化效果很好。然而，值得注意的是，作者确实使用了 KITTI 提供的分割掩码来提取与所需对象相关的像素。以下展示了从这些数据集中生成的重建示例。'
- en: '![](../Images/8718f13257c8a6463488e842783abf78.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8718f13257c8a6463488e842783abf78.png)'
- en: (from [1])
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: Beyond the initial proposal in [1], the authors also propose a generative version
    of the ONet that is trained in an unsupervised fashion over ShapeNet and forms
    a latent space of 3D geometries using approach that is similar to a [Variational
    Auto-Encoder](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/) (VAE).
    Put simply, the authors find that you can create generative ONets that can create
    new meshes and interpolate between meshes. This is useful if we want to focus
    on generative applications instead of 3D reconstruction in particular.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 超越[1]中的初步提议，作者还提出了一种生成版本的ONet，该版本通过对ShapeNet进行无监督训练，并形成一个类似于[变分自编码器](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/)（VAE）的3D几何潜在空间。简单来说，作者发现可以创建生成ONets，这些ONets能够生成新的网格并在网格之间进行插值。如果我们希望关注生成应用而不仅仅是3D重建，这将非常有用。
- en: Takeaways
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 要点
- en: Prior to ONets, existing methods for 3D reconstruction struggled to model high-resolution
    objects while maintaining a reasonable memory footprint. In [1], we learn that
    a smarter representation of 3D geometries can provide a significant benefit. The
    major takeaways from the proposal of ONets are outlined below.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在ONets之前，现有的3D重建方法在保持合理内存占用的同时，难以对高分辨率物体进行建模。在[1]中，我们了解到，更智能的3D几何表示可以带来显著的好处。ONets提案中的主要要点如下。
- en: '**occupancy functions are great.** Common representations of 3D geometries
    (e.g., meshes, point clouds, voxels) tend to use too much memory when representing
    high-resolution objects. Occupancy functions are a more concise representation
    that allow us to model 3D objects with arbitrary precision by encoding a single
    function. Plus, occupancy functions are easier to learn or model compared to alternatives
    like SDFs.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**占据函数非常棒。** 常见的3D几何表示（如网格、点云、体素）在表示高分辨率物体时往往占用过多内存。占据函数是一种更简洁的表示方法，它通过编码单一函数来实现对3D物体的任意精度建模。而且，与像SDF这样的替代方法相比，占据函数更容易学习或建模。'
- en: '**learnable reconstruction.** Sure, occupancy functions are great, but the
    real value in this work is how we represent these functions. Namely, we can use
    neural networks to learn and store occupancy functions for a variety of shapes.
    As a result, we can represent many different shapes at arbitrary precision just
    by *(i)* training an ONet and *(ii)* storing the model’s parameters. This approach
    uses a limited, fixed amount of memory and improves reconstruction quality by
    leveraging prior information!'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**可学习的重建。** 当然，占据函数很棒，但这项工作的真正价值在于我们如何表示这些函数。也就是说，我们可以使用神经网络来学习和存储各种形状的占据函数。因此，我们可以通过*(i)*训练一个ONet和*(ii)*存储模型的参数，以任意精度表示许多不同的形状。这种方法使用有限且固定的内存量，并通过利用先验信息来提高重建质量！'
- en: '**representing 3D space.** Here, we quickly encountered the concept of an octree
    when explaining the MISE approach for generating meshes with ONets. This is an
    important data structure for 3D modeling that allows us to recursively generate
    shapes with varying levels of precision. If we want to get a more precise representation,
    we just continue subdividing voxels. But, we should only do this where it makes
    sense (i.e., when nearby voxels have differing occupancy) to avoid unnecessary
    computation.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**表示3D空间。** 在解释使用ONets生成网格的MISE方法时，我们很快遇到了八叉树的概念。这是3D建模中的一个重要数据结构，它允许我们以不同的精度递归生成形状。如果我们想获得更精确的表示，只需继续细分体素。但我们应仅在有意义的地方（即当附近的体素具有不同的占据情况时）进行此操作，以避免不必要的计算。'
- en: Closing remarks
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结束语
- en: Thanks so much for reading this article. I am [Cameron R. Wolfe](https://cameronrwolfe.me/),
    a research scientist at [Alegion](https://www.alegion.com/) and PhD student at
    Rice University studying the empirical and theoretical foundations of deep learning.
    You can also check out my [other writings](https://medium.com/@wolfecameron) on
    medium! If you liked it, please follow me on [twitter](https://twitter.com/cwolferesearch)
    or subscribe to my [Deep (Learning) Focus newsletter](https://cameronrwolfe.substack.com/),
    where I write series of understandable overviews on important deep learning topics.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢阅读本文。我是[Cameron R. Wolfe](https://cameronrwolfe.me/)，一名在[Alegion](https://www.alegion.com/)工作的研究科学家，同时也是莱斯大学的博士生，研究深度学习的经验和理论基础。你也可以查看我在medium上的[其他写作](https://medium.com/@wolfecameron)！如果你喜欢这篇文章，请在[twitter](https://twitter.com/cwolferesearch)上关注我或订阅我的[Deep
    (Learning) Focus通讯](https://cameronrwolfe.substack.com/)，我在其中撰写了有关深度学习重要主题的易懂概述系列。
- en: Bibliography
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Mescheder, Lars, et al. “Occupancy networks: Learning 3d reconstruction
    in function space.” *Proceedings of the IEEE/CVF conference on computer vision
    and pattern recognition*. 2019.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Mescheder, Lars, 等. “占据网络：在函数空间中学习 3D 重建。” *IEEE/CVF 计算机视觉与模式识别会议论文集*。2019年。'
- en: '[2] Goodfellow, Ian, et al. “Generative adversarial networks.” *Communications
    of the ACM* 63.11 (2020): 139–144.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Goodfellow, Ian, 等. “生成对抗网络。” *ACM 通讯* 63.11 (2020)：139–144。'
- en: '[3] Mildenhall, Ben, et al. “Nerf: Representing scenes as neural radiance fields
    for view synthesis.” *Communications of the ACM* 65.1 (2021): 99–106.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Mildenhall, Ben, 等. “NeRF：将场景表示为神经辐射场以进行视图合成。” *ACM 通讯* 65.1 (2021)：99–106。'
- en: '[4] He, Kaiming, et al. “Deep residual learning for image recognition.” *Proceedings
    of the IEEE conference on computer vision and pattern recognition*. 2016.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] He, Kaiming, 等. “图像识别的深度残差学习。” *IEEE 计算机视觉与模式识别会议论文集*。2016年。'
- en: '[5] Qi, Charles R., et al. “Pointnet: Deep learning on point sets for 3d classification
    and segmentation.” *Proceedings of the IEEE conference on computer vision and
    pattern recognition*. 2017.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Qi, Charles R., 等. “Pointnet：针对 3D 分类和分割的点集深度学习。” *IEEE 计算机视觉与模式识别会议论文集*。2017年。'
- en: '[6] Hoang, Long, et al. “A deep learning method for 3D object classification
    using the wave kernel signature and a center point of the 3D-triangle mesh.” *Electronics*
    8.10 (2019): 1196.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Hoang, Long, 等. “一种使用波形核签名和 3D 三角网中心点的 3D 对象分类深度学习方法。” *电子学* 8.10 (2019)：1196。'
