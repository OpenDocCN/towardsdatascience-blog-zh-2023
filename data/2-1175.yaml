- en: How to Enhance Your Pandas Code — Don’t Wait No More
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何提升你的Pandas代码 — 不要再等待了
- en: 原文：[https://towardsdatascience.com/how-to-enhance-your-pandas-code-dont-wait-no-more-5fb89bc1ece9](https://towardsdatascience.com/how-to-enhance-your-pandas-code-dont-wait-no-more-5fb89bc1ece9)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-enhance-your-pandas-code-dont-wait-no-more-5fb89bc1ece9](https://towardsdatascience.com/how-to-enhance-your-pandas-code-dont-wait-no-more-5fb89bc1ece9)
- en: 6 simple changes to improve your pandas’ code performance
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提升你的pandas代码性能的6个简单改变
- en: '[](https://polmarin.medium.com/?source=post_page-----5fb89bc1ece9--------------------------------)[![Pol
    Marin](../Images/a4f69a96717d453db9791f27b8f85e86.png)](https://polmarin.medium.com/?source=post_page-----5fb89bc1ece9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5fb89bc1ece9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5fb89bc1ece9--------------------------------)
    [Pol Marin](https://polmarin.medium.com/?source=post_page-----5fb89bc1ece9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://polmarin.medium.com/?source=post_page-----5fb89bc1ece9--------------------------------)[![Pol
    Marin](../Images/a4f69a96717d453db9791f27b8f85e86.png)](https://polmarin.medium.com/?source=post_page-----5fb89bc1ece9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5fb89bc1ece9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5fb89bc1ece9--------------------------------)
    [Pol Marin](https://polmarin.medium.com/?source=post_page-----5fb89bc1ece9--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5fb89bc1ece9--------------------------------)
    ·9 min read·Apr 10, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----5fb89bc1ece9--------------------------------)
    ·9分钟阅读·2023年4月10日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/5316eff4e5a960cbcac4b7ab3908130b.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5316eff4e5a960cbcac4b7ab3908130b.png)'
- en: Photo by [Pascal Müller](https://unsplash.com/@millerthachiller?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[Pascal Müller](https://unsplash.com/@millerthachiller?utm_source=medium&utm_medium=referral)拍摄于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)'
- en: '**Pandas** is arguably a requirement for any data scientist’s portfolio. Whether
    you use it or not, mastering it will always be an asset.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**Pandas**可以说是任何数据科学家简历中的必备技能。无论你是否使用它，掌握它始终是一种资产。'
- en: However, like with any other thing, the key is not to know how to use *pandas*
    but to actually produce efficient *pandas* code. That’s where not most excel,
    but after this post, you won’t be part of that crew anymore.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，就像其他任何事情一样，关键不是知道如何使用*pandas*，而是实际编写高效的*pandas*代码。这是大多数人做不到的，但在读完这篇文章后，你不会再是其中的一员。
- en: But, first things first.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 但首先要做的事情是。
- en: What Is Pandas and Why Does Efficiency Matter?
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是Pandas以及为什么效率很重要？
- en: Using the official definition, “**pandas** is a fast, powerful, flexible, and
    easy to use open source data analysis and manipulation tool, built on top of the
    [Python](https://www.python.org/) programming language”[1].
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 根据官方定义，“**pandas**是一个快速、强大、灵活且易于使用的开源数据分析和操作工具，建立在[Python](https://www.python.org/)编程语言之上”[1]。
- en: It’s really powerful and the default go-to for many of us when we want to play
    with data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 它真的很强大，许多人在处理数据时默认使用它。
- en: As an additional comment, it loads the datasets into our RAM, so caring about
    optimizing its space-usage sounds like a great idea.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，它将数据集加载到我们的RAM中，所以关心优化空间使用是个好主意。
- en: 'But we don’t only want to optimize its memory management, we probably care
    more about making our code’s execution faster. Why? As obvious as it sounds, with
    a faster code we save time. And you know what they say: time is money (especially
    if you have your code running on AWS or GCP).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们不仅想优化它的内存管理，我们可能更关心让代码执行得更快。为什么？显而易见，更快的代码可以节省时间。你知道他们怎么说：时间就是金钱（尤其是当你的代码在AWS或GCP上运行时）。
- en: Hardware can improve our program’s performance up to a limit, but that’s half
    of the equation. Having an efficient code is the other half, as important as the
    first one. So mastering pandas is almost a necessity for all of us who use it
    daily.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件可以在一定程度上提高我们程序的性能，但这只是方程式的一半。高效的代码是另一半，和前者一样重要。因此，对于我们这些每天使用它的人来说，掌握pandas几乎是必需的。
- en: However, do not lose focus. The main goal is to make our codes meet the requirements.
    In my opinion, optimization should be left to when your scripts are unnecessarily
    slow or if it’s taking too much memory. That doesn’t mean you shouldn’t code having
    efficiency in mind but don’t make it your priority if it’s not worth the effort.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，请不要分心。主要目标是让我们的代码符合要求。在我看来，优化应当留到你的脚本运行过慢或占用过多内存时。并不是说你不应考虑效率，但如果不值得付出额外的努力，就不要将其作为优先事项。
- en: 'I will try to go beyond the logical efficiency claims, but let’s just briefly
    highlight the two basic-most:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我将尝试超越逻辑效率的说法，但让我们简要突出两个最基本的：
- en: '**Use only what you need** — load the columns/rows that will be useful for
    your analysis and get rid of the rest.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**只使用你需要的** — 加载对你的分析有用的列/行，并去掉其余的。'
- en: '**Choose the right variable type** — As said, pandas loads all the data in
    our system’s RAM so choosing wisely the type of variables we’re going to use is
    worth it. We do not want to use floats when all can be done with integers, for
    example.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**选择正确的变量类型** — 如前所述，pandas 将所有数据加载到系统的 RAM 中，所以明智地选择变量类型是值得的。例如，当所有操作都可以用整数完成时，就不应使用浮点数。'
- en: Load a dataframe and use the `astype()` function to make sure all columns have
    the desired types.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 加载数据框并使用`astype()`函数以确保所有列都具有期望的类型。
- en: 'Before we begin, let me share with you the dataframe I’ll be using, along with
    the `%timeit` magic function to put the performance improvements into numbers:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，让我与大家分享我将使用的数据框，并结合`%timeit`魔法函数来将性能提升量化：
- en: '![](../Images/0e24140bf311cb2938254ad195bb8fbf.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e24140bf311cb2938254ad195bb8fbf.png)'
- en: Randomly created data for this post — Screenshot by the author
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本文随机创建的数据 — 作者截图
- en: 1\. Avoid Loops At All Costs
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 尽量避免使用循环
- en: It won’t always be possible, but getting rid of loops is one of the best optimizations
    you can add to your code.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不总是可能，但去掉循环是你可以为代码添加的最佳优化之一。
- en: Pandas is built on top of NumPy so we better take advantage of vectorization
    instead of using loops to perform operations on columns/rows.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas 构建在 NumPy 之上，所以我们最好利用矢量化，而不是使用循环对列/行进行操作。
- en: 'This example is what would be an approach for many, trying to subtract the
    value of one column from another:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子是许多人会采用的一种方法，试图从一列中减去另一列的值：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Well, that didn’t take a lot of time, 14 seconds for a ~500k row dataframe.
    Not that bad, but what if I told you that this could be done in less than one
    second?
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，这并没有花费太多时间，对于一个 ~500k 行的数据框花费了 14 秒。还不错，但如果我告诉你这可以在不到一秒钟内完成呢？
- en: 'Let’s see a better (and the best) way to do the same:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看做同样事情的更好（甚至是最佳）方法：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: PAM.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: PAM。
- en: The improvement is crazy! We didn’t just simplify our code (it contains three
    fewer rows) but we’ve also saved **a lot** of time here. This is thanks to the
    vectorization feature which does not perform the operation on one data element
    at a time but does them all at the same time. One operation is applied to any
    number of data elements per one moment in time.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 改进效果真是疯狂！我们不仅简化了代码（减少了三行），还节省了**大量**时间。这要归功于矢量化特性，它不是一次操作一个数据元素，而是同时处理所有数据元素。一次操作可以应用于任意数量的数据元素。
- en: This optimization alone provided a 100x improvement in execution time. Worth
    it, right?
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 仅这一优化就带来了 100 倍的执行时间提升。值得吧？
- en: 2\. If You Need Loops, Don’t Use Iterrows()
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 如果需要循环，避免使用Iterrows()
- en: There are several alternatives to iterrows, like using `apply()` or using list
    comprehension. However, I want to provide a comparable alternative to iterrows(),
    using a loop either way.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种替代`iterrows`的方法，比如使用`apply()`或列表推导。然而，我想提供一个与`iterrows()`相当的替代方案，无论如何都需要使用循环。
- en: Using `itertuples()` returns each row as a **namedtuple**, which is one of the
    specialized data types within the collections module. They behave like a tuple
    but have fields accessible by their names, with the dot (**.**) lookup.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`itertuples()`会返回每一行作为**命名元组**，这是 collections 模块中的一种专门数据类型。它们像元组一样工作，但通过名称访问字段，使用点号（**.**）查找。
- en: 'Let’s repeat the previous experiment but now using itertuples():'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重复之前的实验，这次使用`itertuples()`：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The improvement remains huge here. While iterrows takes like 14 seconds to execute,
    the exact same code using itertuples takes less than a second.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 改进效果依然巨大。虽然`iterrows`执行需要大约 14 秒，但使用`itertuples`的完全相同代码则不到一秒钟。
- en: While using vectorization is preferred, we’ve seen that loops can be optimized
    as well.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然使用矢量化是首选，但我们已经看到循环也可以进行优化。
- en: 3\. Append vs Concat
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. Append 与 Concat
- en: 'I’ve found myself several times having to concatenate dataframes and, at one
    point in the past, I always asked the same question to myself: Should I use `append()`
    or `concat()`. I ended up defaulting to concat because I found it easier to use.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我曾多次需要连接数据框，过去曾一直问自己一个问题：我应该使用 `append()` 还是 `concat()`。最后我默认使用 concat，因为我觉得它更易于使用。
- en: But don’t let yourself answer that question like I did, randomly. It ultimately
    depends on how many dataframes you’re willing to concatenate. When it’s just two,
    then don’t bother caring about it, using one or the other won’t probably affect
    much your code’s performance.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 但不要像我一样随意回答这个问题。它的**最终**取决于你愿意连接多少个数据框。当只有两个时，不用太在意，使用其中一个或另一个不会对代码性能产生太大影响。
- en: However, when trying to concatenate more than two DFs, I’d recommend using concat.
    Why? Because `concat()` is a library’s function that allows us to concatenate
    various at the same time while `append()` is a method of the DataFrame class and
    it only allows for two at a time.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当尝试连接超过两个数据框时，我建议使用 concat。为什么？因为 `concat()` 是一个库函数，允许我们同时连接多个数据框，而 `append()`
    是 DataFrame 类的方法，仅允许一次连接两个数据框。
- en: 'Long story short: with `concat()` you can concatenate multiple DFs at the same
    time whereas to do the same with `append()` you’d need a loop.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 长话短说：使用 `concat()` 可以同时连接多个数据框，而使用 `append()` 需要一个循环来完成相同的操作。
- en: It isn’t only less efficient in time but also in memory usage because each call
    to the append function returns a new DF, which is stored in our system’s memory.
    So, if we want to concatenate N DFs, we’ll create N-1 dataframes until we get
    the final one, all of them using valuable memory space.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这不仅在时间上效率较低，而且在内存使用上也不够高效，因为每次调用 append 函数都会返回一个新的数据框，存储在系统内存中。所以，如果我们要连接 N
    个数据框，我们会创建 N-1 个数据框，直到得到最终结果，这些数据框都占用宝贵的内存空间。
- en: Not good.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 不好。
- en: So use `concat()` when appending multiple DFs.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在追加多个数据框时使用 `concat()`。
- en: 4\. Don’t Ruin Your GroupBy’s and Merges
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 不要破坏你的 GroupBy 和合并
- en: Both the `groupby()` and the `merge()` are powerful tools for us, I bet you
    use them daily. Taking care of them is, therefore, crucial too.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`groupby()` 和 `merge()` 都是强大的工具，我敢打赌你每天都在使用它们。因此，妥善处理它们也非常重要。'
- en: '**Filter first**. GroupBy’s and merges are expensive operations so make sure
    you filter out the data you don’t care about before grouping/merging.'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**首先进行过滤**。GroupBy 和合并是代价高昂的操作，因此请确保在分组/合并之前过滤掉不需要的数据。'
- en: '**Try to avoid custom functions on GroupBy**. You’ll be tempted to create your
    own grouping functions when you need something different than the usual sum, mean…
    Chances are, someone has done it before. There might even be built-in functions
    that can take care of those. Use those when possible, because performance slows
    down when you use your own.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**尽量避免在 GroupBy 中使用自定义函数**。当你需要不同于通常的总和、平均值等的分组功能时，你会想创建自己的分组函数。很可能有人已经做过这件事。可能还有内置函数可以处理这些情况。尽可能使用这些内置函数，因为使用自定义函数会降低性能。'
- en: '**Consider using DuckDB**. DuckDB is a strong OLAP system that can really help
    you perform analytical tasks just like these both. I recently wrote a story going
    more in-depth into the topic, so go read it if you’re interested in DuckDB:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**考虑使用 DuckDB**。DuckDB 是一个强大的 OLAP 系统，可以真正帮助你执行像这样分析任务。我最近写了一篇更深入探讨这个话题的文章，如果你对
    DuckDB 感兴趣，可以去读读：'
- en: '[](/forget-about-sqlite-use-duckdb-instead-and-thank-me-later-df76ee9bb777?source=post_page-----5fb89bc1ece9--------------------------------)
    [## Forget about SQLite, Use DuckDB Instead — And Thank Me Later'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 忘记 SQLite，改用 DuckDB——稍后感谢我](https://towardsdatascience.com/forget-about-sqlite-use-duckdb-instead-and-thank-me-later-df76ee9bb777?source=post_page-----5fb89bc1ece9--------------------------------)'
- en: Introduction to DuckDB and its Python integration
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DuckDB 及其 Python 集成介绍
- en: towardsdatascience.com](/forget-about-sqlite-use-duckdb-instead-and-thank-me-later-df76ee9bb777?source=post_page-----5fb89bc1ece9--------------------------------)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/forget-about-sqlite-use-duckdb-instead-and-thank-me-later-df76ee9bb777?source=post_page-----5fb89bc1ece9--------------------------------)'
- en: 5\. Use query() on large datasets
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 在大型数据集上使用 query()
- en: I have to confess I wasn’t aware of this one but, after testing it out a little
    bit, I thought it was worth being added to this post.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我必须承认我之前并没有意识到这一点，但在稍微测试了一下后，我认为值得将其添加到这篇文章中。
- en: Pandas offers a dataframe method called `query()`, with the purpose of querying
    the columns of a DF with a boolean expression[2]. It takes advantage of the **numexpr**
    parser, which is a fast numerical expression evaluator for NumPy[3].
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas 提供了一种名为 `query()` 的数据框方法，目的是用布尔表达式查询数据框的列[2]。它利用了**numexpr** 解析器，这是一个用于
    NumPy 的快速数值表达式求值器[3]。
- en: 'Enough theory, let’s show how `query()` performs with the entire dataframe
    vs the usual filtering. As we have dates in our DF, let’s just randomly choose
    one and try to get the data from that date:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 说够理论了，接下来我们展示一下 `query()` 在整个数据框与通常过滤方法的性能对比。由于我们的数据框中包含日期，我们就随机选择一个日期，并尝试从那个日期获取数据：
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Cool huh? It’s almost a **50% improvement** **in performance** on this ~500k-row
    dataframe. What if we did it to a small subset of 2,279 rows? See this:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 很酷吧？在这个 ~500k 行的数据框上，性能提升了将近**50%**。如果我们对一个包含 2,279 行的小数据子集做同样的操作呢？看看这个：
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Results point toward the opposite direction here — using `query()` is not desired
    on small dataframes, as it really slows down the performance if we compare it
    to the usual pandas filtering method. That’s why I specified “on large datasets”
    in the title of this section.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，相反的方向——在小数据框上使用`query()`并不理想，因为与通常的 pandas 过滤方法相比，它会显著降低性能。这就是为什么我在本节标题中指定了“在大型数据集上”。
- en: 'Pandas developers have studied this method in depth and they have the analysis
    available for everyone online, but one of the main takeaways is this: “You will
    only see the performance benefits of using the `numexpr` engine with `DataFrame.query()`if
    your frame has more than approximately 100,000 rows.”[4]'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas 开发者已经深入研究了这一方法，并且他们在线上提供了相关分析，但主要的结论之一是：“只有当你的数据框有大约 100,000 行或更多时，你才会看到使用
    `numexpr` 引擎与 `DataFrame.query()` 的性能提升。”[4]
- en: 6\. Don’t Default to CSVs
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 不要默认使用 CSV 文件
- en: I used CSVs to load and save my data all the time in the past. I’m not against
    CSVs now and I keep using them depending on the task, but I’ve switched to **parquet**
    files when the data is more relevant and bigger.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我过去总是使用 CSV 文件来加载和保存数据。我现在并不反对 CSV 文件，视任务而定我仍然会使用它们，但当数据更相关和更大时，我已经转向使用**parquet**
    文件。
- en: 'Why parquet? Read the official introduction: “Apache Parquet is an open-source,
    column-oriented data file format designed for efficient data storage and retrieval.
    It provides efficient data compression and encoding schemes with enhanced performance
    to handle complex data in bulk.”[5]'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么选择 parquet？阅读官方介绍：“Apache Parquet 是一种开源的、面向列的数据文件格式，旨在实现高效的数据存储和检索。它提供了高效的数据压缩和编码方案，增强了处理复杂数据的大数据性能。”[5]
- en: We can see parquet files can be more efficient than CSVs when it comes to memory
    usage — they are “designed for efficient data storage and provide efficient data
    compression and encoding schemes” — but how does that translate to time?
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，parquet 文件在内存使用方面比 CSV 文件更高效——它们是“为高效的数据存储设计的，并提供高效的数据压缩和编码方案”——但这如何转化为时间呢？
- en: '[PRE5]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Well, that’s an **above-40% improvement in time.** The difference here is just
    a few milliseconds, but imagine how can this 40% improvement translate to a bigger
    dataframe.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，这就是**超过 40% 的时间提升**。这里的差异只是几毫秒，但想象一下，这 40% 的提升如何转化为更大的数据框。
- en: 'When it comes to loading times, let’s see their difference here:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 说到加载时间，我们来看看它们的差异：
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We see some improvements here again, but bigger this time — **65%**. That’s
    a lot of time we can potentially save when playing with bigger datasets.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们再次看到一些改进，不过这次提升更大——**65%**。这意味着在处理更大的数据集时，我们可以节省大量时间。
- en: 'As a side note: There’s the **Feather format (.ftr)**, which I haven’t personally
    used but I’ve seen really good references around the Internet and there’s impressive
    analysis everywhere showing how it even outperforms parquet, so feel free to give
    it a chance as well.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一句：还有**Feather 格式 (.ftr)**，虽然我个人没有使用过，但在互联网上看到了很多很好的参考资料，并且有令人印象深刻的分析显示它甚至超过了
    parquet，所以也可以试试这个格式。
- en: Conclusion
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: There are a lot of ways to optimize pandas and I just mentioned a few of them,
    the ones that I benefit the most from.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多方法可以优化 pandas，我这里只提到了一些，我从中受益最多的方法。
- en: I hope this post was instructive and helpful. Once you know all this information,
    there’s no excuse to not use it!
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这篇文章对你有启发和帮助。一旦你掌握了这些信息，就没有理由不加以使用！
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'If you’d like to support me further, consider subscribing to Medium’s Membership
    through the link you find below: it won’t cost you any extra penny but it’ll help
    me through this process. Thanks a lot!'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想进一步支持我，可以考虑通过下面的链接订阅Medium的会员：这不会花费你额外的钱，但会帮助我完成这个过程。非常感谢！
- en: '[](https://medium.com/@polmarin/membership?source=post_page-----5fb89bc1ece9--------------------------------)
    [## Join Medium with my referral link - Pol Marin'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@polmarin/membership?source=post_page-----5fb89bc1ece9--------------------------------)
    [## 使用我的推荐链接加入Medium - Pol Marin'
- en: Read every story from Pol Marin (and thousands of other writers on Medium).
    Your membership fee directly supports Pol…
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阅读Pol Marin的每一篇故事（以及Medium上的成千上万其他作者的故事）。你的会员费用直接支持Pol…
- en: medium.com](https://medium.com/@polmarin/membership?source=post_page-----5fb89bc1ece9--------------------------------)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@polmarin/membership?source=post_page-----5fb89bc1ece9--------------------------------)
- en: Resources
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资源
- en: '[1] [pandas — Python Data Analysis Library](https://pandas.pydata.org/)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [pandas — Python数据分析库](https://pandas.pydata.org/)'
- en: '[2] [pandas.DataFrame.query — Documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [pandas.DataFrame.query — 文档](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.query.html)'
- en: '[3] [NumExpr — Github](https://github.com/pydata/numexpr)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [NumExpr — Github](https://github.com/pydata/numexpr)'
- en: '[4] [Indexing and selecting data — Pandas](https://pandas.pydata.org/docs/user_guide/indexing.html#performance-of-query)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [数据的索引和选择 — Pandas](https://pandas.pydata.org/docs/user_guide/indexing.html#performance-of-query)'
- en: '[5] [Apache Parquet](https://parquet.apache.org/)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] [Apache Parquet](https://parquet.apache.org/)'
