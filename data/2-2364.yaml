- en: Why are language models everywhere?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么语言模型无处不在？
- en: 原文：[https://towardsdatascience.com/why-are-language-models-everywhere-36d9961dd9e1](https://towardsdatascience.com/why-are-language-models-everywhere-36d9961dd9e1)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/why-are-language-models-everywhere-36d9961dd9e1](https://towardsdatascience.com/why-are-language-models-everywhere-36d9961dd9e1)
- en: The answer lies in the 75 years of NLP history
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 答案在于75年的NLP历史
- en: '[](https://medium.com/@dataemporium?source=post_page-----36d9961dd9e1--------------------------------)[![Ajay
    Halthor](../Images/1be821c8d8ed336b9ecedcf94f960ede.png)](https://medium.com/@dataemporium?source=post_page-----36d9961dd9e1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----36d9961dd9e1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----36d9961dd9e1--------------------------------)
    [Ajay Halthor](https://medium.com/@dataemporium?source=post_page-----36d9961dd9e1--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@dataemporium?source=post_page-----36d9961dd9e1--------------------------------)[![Ajay
    Halthor](../Images/1be821c8d8ed336b9ecedcf94f960ede.png)](https://medium.com/@dataemporium?source=post_page-----36d9961dd9e1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----36d9961dd9e1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----36d9961dd9e1--------------------------------)
    [Ajay Halthor](https://medium.com/@dataemporium?source=post_page-----36d9961dd9e1--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----36d9961dd9e1--------------------------------)
    ·6 min read·May 26, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----36d9961dd9e1--------------------------------)
    ·6分钟阅读·2023年5月26日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/fcff59340752837048b7aa666ca9d1ec.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fcff59340752837048b7aa666ca9d1ec.png)'
- en: Photo by [Romain Vignes](https://unsplash.com/@rvignes?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Romain Vignes](https://unsplash.com/@rvignes?utm_source=medium&utm_medium=referral)
    提供，来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Have you ever wondered about how we got here with [ChatGPT](https://youtu.be/NpmnWgQgcsA)
    and Large Language Models? The answer lies in the development of Natural Language
    Processing (NLP) itself. So let’s talk about it. Don’t worry; the history is more
    interesting than you think! Section 1 will describe the birth of AI and NLP. Section
    2 will talk about the major pillars of the field. Sections 3 through 5 will go
    into detailed timelines for the past 75 years. And for the final section 6, we
    describe the convergence of all these fields into *language modeling* which has
    become so popular today!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否想过我们是如何走到今天的 [ChatGPT](https://youtu.be/NpmnWgQgcsA) 和大型语言模型的？答案在于自然语言处理（NLP）本身的发展。所以让我们来谈谈吧。别担心；历史比你想象的更有趣！第一部分将描述AI和NLP的诞生。第二部分将讨论该领域的主要支柱。第三到第五部分将详细介绍过去75年的时间线。最后第六部分，我们将描述这些领域如何汇聚到*语言建模*中，今天已经变得如此流行！
- en: 1 Genesis
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 起源
- en: In the beginning, there was Alan Turing’s 1950 publication [*Computing Machinery
    and Intelligence*](https://phil415.pbworks.com/f/TuringComputing.pdf) where he
    posits the question “Can machines think”. This paper is often touted as the birth
    of Artificial Intelligence. Although it did not talk about natural language explicitly,
    it laid the groundwork for future research in NLP. This is why the earliest works
    in NLP spring up in the 1950s.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，有艾伦·图灵1950年的出版物 [*计算机及智能*](https://phil415.pbworks.com/f/TuringComputing.pdf)，他提出了“机器能思考吗？”的问题。该论文常被誉为人工智能的起源。虽然它没有明确讨论自然语言，但为未来的NLP研究奠定了基础。这也是为什么NLP的最早工作出现在1950年代。
- en: 2 Pillars of NLP
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLP的两个支柱
- en: '**Machine Translation**: This is when an AI takes in a sentence of one language
    and outputs a sentence in another language. For example, Google Translate.'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**机器翻译**：这是当AI接收一种语言的句子并输出另一种语言的句子。例如，谷歌翻译。'
- en: '**Speech Processing**: AI takes an audio as input and generates the corresponding
    text as output.'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**语音处理**：AI接收音频作为输入，并生成相应的文本作为输出。'
- en: '**Text Summarization**: AI takes in a story as input and generates a summary
    as an output.'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**文本摘要**：AI接收一个故事作为输入，并生成一个摘要作为输出。'
- en: '**Language Modeling**: AI is given a sequence of words, it will determine the
    next word.'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**语言建模**：AI接收到一系列单词后，将确定下一个单词。'
- en: There are far more than these four. Over time, there has been a convergence
    of each pillar towards using Language Models to accomplish their task. In the
    following sections, let’s talk about each timeline.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是其中的一部分。随着时间的推移，各个领域的支柱逐渐趋向于使用语言模型来完成其任务。在接下来的部分，我们将深入探讨每个时间线。
- en: '![](../Images/7c8c92495d3d8d2e349e0147a8b16604.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7c8c92495d3d8d2e349e0147a8b16604.png)'
- en: 'Figure 1: Timelines of major pillars of NLP (image by author)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：NLP主要支柱的时间线（图像由作者提供）
- en: 3 Machine Translation
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 机器翻译
- en: '***Rule Based Systems***: 1954 saw [Georgetown IBM experiment](https://aclanthology.org/www.mt-archive.info/Garvin-1967.pdf)
    that was used in the Cold War era to translate from Russian to English. The idea
    was that the translation task could be broken down into a set of rules to convert
    one language to the other i.e. a rule based system. Another early rule-based system
    was Yehoshua Bar-Hillel’s “Analytical Engine” for translating Hebrew to English.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '***基于规则的系统***：1954年出现了[乔治城IBM实验](https://aclanthology.org/www.mt-archive.info/Garvin-1967.pdf)，用于冷战时期的俄语到英语翻译。这个想法是将翻译任务分解为一组规则，以便将一种语言转换为另一种语言，即基于规则的系统。另一个早期的基于规则的系统是耶霍舒亚·巴尔-希勒尔的“分析引擎”，用于将希伯来语翻译为英语。'
- en: '***Statistical Approaches***: The problem with Rule based systems is they make
    a ton of assumptions. More complex the problem, more problematic are these assumptions.
    Translation is complex. From the 1980s, as we had access to more bilingual data
    and statistical methods become better established, we started applying these statistical
    models to language translation. A paradigm called Statistical Machine Translation
    (SMT) became popular. SMT paradigms decomposed the problem into 2 sub-problems:
    a translation problem and a language modeling problem.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '***统计方法***：规则基础系统的问题在于它们做了大量假设。问题越复杂，这些假设就越成问题。翻译是复杂的。从1980年代起，随着我们获取了更多的双语数据和统计方法的逐步完善，我们开始将这些统计模型应用于语言翻译。一种被称为统计机器翻译（SMT）的范式变得流行。SMT范式将问题分解为两个子问题：翻译问题和语言建模问题。'
- en: '***Neural Approaches***: Since 2015, SMT has been replaced by [Neural Machine
    Translation](https://arxiv.org/pdf/1409.0473.pdf). These make use of Neural Networks
    to directly learn the task of translation. They include the development of Recurrent
    Neural Networks, and eventually Transformer Models. With the introduction of models
    like GPT, the baseline pretrained model became Language Modeling and it is fine
    tuned with translation.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '***神经方法***：自2015年起，SMT已被[神经机器翻译](https://arxiv.org/pdf/1409.0473.pdf)取代。这些方法利用神经网络直接学习翻译任务。它们包括递归神经网络的发展，最终发展为Transformer模型。随着像GPT这样的模型的引入，基线预训练模型变成了语言建模，并通过翻译进行微调。'
- en: 4 Speech Processing
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 语音处理
- en: '***Rule Based Systems***: The start for speech processing was also back in
    the 1950s & 60s where single digits and words were recognized. For example, Audrey
    by Bell Labs recognized digits, while IBM’s [Shoebox](https://www.ibm.com/ibm/history/exhibits/specialprod1/specialprod1_7.html)
    performed arithmetic on voice command.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '***基于规则的系统***：语音处理的起点也可以追溯到1950年代和60年代，那时能够识别单个数字和单词。例如，贝尔实验室的Audrey识别数字，而IBM的[Shoebox](https://www.ibm.com/ibm/history/exhibits/specialprod1/specialprod1_7.html)则执行语音命令下的算术运算。'
- en: '***Statistical Approaches***: However, converting speech to text is a complex
    problem; there are different dialects, accents, loudness. So breaking this complex
    problem down into subproblems was the move. Around the 70s, after Hidden Markov
    Models were introduced, the complex problem of speech to text could be broken
    down into 3 simpler problems:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '***统计方法***：然而，将语音转换为文本是一个复杂的问题；存在不同的方言、口音、音量。因此，将这个复杂问题分解为子问题是一个解决方法。大约在70年代，隐马尔可夫模型被引入后，语音到文本的复杂问题可以被分解为三个更简单的问题：'
- en: '**Language modeling**: We can determine the sequence of words and sentences.
    These were [n-gram models](https://youtu.be/cUsqFx4Sij8).'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言建模**：我们可以确定单词和句子的顺序。这些是[n-gram模型](https://youtu.be/cUsqFx4Sij8)。'
- en: '**Pronunciation modeling**: This is done to associate words and phones. These
    are essentially simple models or even tables.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**发音建模**：这是为了将单词与音素关联起来。这些本质上是简单模型或甚至是表格。'
- en: '**Acoustic modeling**: We understand the relationship between the speech and
    phones. These were Hidden Markov Models with Gaussian Mixture models'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**声学建模**：我们理解语音与音素之间的关系。这些是带有高斯混合模型的隐马尔可夫模型。'
- en: These 3 parts are trained separately and then used together. But this creates
    its own complexity.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这三部分被单独训练，然后一起使用。但这也带来了自身的复杂性。
- en: '***Neural Approaches***: In the early 2000s, we saw these techniques replaced
    with neural networks. As we saw the advent of large scale text corpora, neural
    networks started outperforming everything. They performed end-to-end speech to
    text. So we could optimize the objective of generating text from the input speech
    directly; this led to better performance. With further development in the field,
    we got into Recurrent Networks, Convolution Neural Networks, and eventually fine
    tuning of pretrained language models.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '***神经网络方法***：在2000年代初，我们看到这些技术被神经网络所取代。随着大规模文本语料库的出现，神经网络开始超越其他所有方法。它们实现了端到端的语音转文本。因此，我们可以直接优化从输入语音生成文本的目标；这带来了更好的性能。随着领域的发展，我们进入了递归网络、卷积神经网络，并最终细化了预训练语言模型。'
- en: 5 Text Summarization
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 文本摘要
- en: '***Rule Based Systems***: Research Started with Luhn’s publication [*The automatic
    creation of literature abstracts*](https://courses.ischool.berkeley.edu/i256/f06/papers/luhn58.pdf)
    in 1958 that ranked the importance of sentences using word frequencies. This method
    selected sentences in the original text to construct a summary; the corresponding
    summary is called an “extraction based summary”. The next significant leap in
    the field came in 1969 with Edmonson’s paper [*New methods of automatic extractive*](https://www.semanticscholar.org/paper/New-Methods-in-Automatic-Extracting-Edmundson/3c493143e49ef3304e85222c16c1cfde2b058dd9).
    He claimed the importance of a sentence not only depended on word frequencies,
    but also on other factors such as location of sentence in the paragraph; whether
    the sentence has certain cue words; or if the sentence has words in the title.
    In the 1980s, we tried summarizing text as a human would without using the original
    sentences. These were “abstractive summaries”. FRUMP (Fast reading and understanding
    memory program) and SUSY were early implementations of such systems. However,
    they too depended on hand crafted rules and the summaries were not high quality.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '***基于规则的系统***：研究始于Luhn在1958年发表的[*The automatic creation of literature abstracts*](https://courses.ischool.berkeley.edu/i256/f06/papers/luhn58.pdf)，该研究通过词频来排名句子的的重要性。这种方法从原文中选择句子来构建总结；相应的总结被称为“基于提取的总结”。该领域的下一个重大突破出现在1969年，Edmonson的论文[*New
    methods of automatic extractive*](https://www.semanticscholar.org/paper/New-Methods-in-Automatic-Extracting-Edmundson/3c493143e49ef3304e85222c16c1cfde2b058dd9)中。他声称句子的重要性不仅取决于词频，还取决于其他因素，如句子在段落中的位置；句子是否包含某些提示词；或者句子是否包含标题中的词。在1980年代，我们尝试不使用原始句子来总结文本。这些是“抽象总结”。FRUMP（快速阅读和理解记忆程序）和SUSY是早期的这种系统的实现。然而，它们也依赖于手工制作的规则，且总结质量不高。'
- en: '***Statistical Approaches***: In the 90s and 2000s, we used statistical approaches
    to build classifiers that determine whether a sentence should be included in a
    summary or not. These classifiers could be a Logistic Regression, Decision Tree,
    SVM, or any other statistical model.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '***统计方法***：在90年代和2000年代，我们使用统计方法构建分类器，以确定一个句子是否应该被包含在总结中。这些分类器可以是逻辑回归、决策树、SVM或任何其他统计模型。'
- en: '***Neural Approaches***: From 2015, Neural Networks saw impact with the introduction
    of [A neural attention model for abstractive sentence summarization.](https://arxiv.org/abs/1509.00685)
    This produced abstractive summaries typically headlines that are very short. However,
    the incorporation of LSTM cells and a [sequence-to-sequence architecture](https://arxiv.org/abs/1804.05685)
    lead to the ability to deal with longer input sequences and also generate proper
    summaries. From there, the field took the same pages as Machine Translation and
    use the pretainining and fine tuning architecture we see today.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '***神经网络方法***：自2015年起，神经网络随着[A neural attention model for abstractive sentence
    summarization.](https://arxiv.org/abs/1509.00685)的引入产生了影响。这种方法生成的抽象总结通常是非常简短的标题。然而，LSTM单元的加入以及[sequence-to-sequence架构](https://arxiv.org/abs/1804.05685)使得处理更长输入序列和生成合适总结成为可能。从那时起，该领域采取了与机器翻译相同的方法，并使用了我们今天看到的预训练和微调架构。'
- en: 6 Putting it all together
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 将一切整合起来
- en: The history of multiple pillars discussed in the previous sections shows some
    common patterns.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 前面讨论的多个支柱的历史展示了一些共同的模式。
- en: Rule based systems dominated in the early days of AI from 1950s and 60s. Around
    the 70s, we saw the introduction of statistical models to solve these problems.
    However, since language is complex, these statistical models would break down
    the complex tasks into sub tasks to solve these problems. With the advent of more
    data and better hardware in the 2000s, neural network approaches were on the rise.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 规则基础系统在1950年代和60年代的早期AI中占据主导地位。到70年代，我们看到了统计模型的引入以解决这些问题。然而，由于语言的复杂性，这些统计模型会将复杂任务拆解为子任务来解决这些问题。随着2000年代更多数据和更好硬件的出现，神经网络方法逐渐兴起。
- en: Neural Networks can learn complex language tasks end to end and hence have better
    performance than statistical approaches. [Transformer Neural Networks](https://youtu.be/TQQlZhbC5ps)
    introduced in 2017 that could effectively learn to solve language tasks. But since
    they required a ton of data to train models effectively, [BERT](https://youtu.be/xI0HHN5XKDo)
    and [GPT](https://youtu.be/3IweGfgytgY) were introduced and used the concept of
    *transfer learning* to learn language tasks. The idea here is that language tasks
    are don’t require much data for systems that have some baseline understanding
    of language itself. GPT for example acquires this “understanding of language”
    by understanding [Language Modeling](https://youtu.be/cUsqFx4Sij8) first and then
    fine tuning on a specific language task. This is why modern NLP has converged
    to using language models at their core.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络可以端到端地学习复杂的语言任务，因此比统计方法表现更好。[Transformer神经网络](https://youtu.be/TQQlZhbC5ps)
    于2017年推出，能够有效地学习解决语言任务。但由于它们需要大量的数据来有效地训练模型，[BERT](https://youtu.be/xI0HHN5XKDo)
    和 [GPT](https://youtu.be/3IweGfgytgY) 被引入并使用了*迁移学习*的概念来学习语言任务。这里的想法是，对于那些对语言本身有一定基础理解的系统，语言任务并不需要大量数据。例如，GPT通过首先理解[语言建模](https://youtu.be/cUsqFx4Sij8)，然后在特定语言任务上进行微调，从而获得这种“语言理解”。这就是为什么现代NLP核心都趋向于使用语言模型的原因。
- en: Hope you now know why Large Language Models like ChatGPT are super important
    and why we see language modeling everywhere. It took the better part of a century
    to get here. For more details on NLP and Language Modeling, check out [this playlist
    of videos](https://www.youtube.com/playlist?list=PLTl9hO2Oobd_bzXUpzKMKA3liq2kj6LfE)
    that delves into different concepts in the field. Happy learning!
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你现在明白为什么像ChatGPT这样的**大型语言模型**如此重要，以及为什么我们到处都能看到语言建模。到达这一点花费了近一个世纪的时间。有关NLP和语言建模的更多详细信息，查看[这个视频播放列表](https://www.youtube.com/playlist?list=PLTl9hO2Oobd_bzXUpzKMKA3liq2kj6LfE)，它*深入探讨*了该领域的不同概念。祝你学习愉快！
