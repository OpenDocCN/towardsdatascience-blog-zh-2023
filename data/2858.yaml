- en: 'LLMOps: Production Prompt Engineering Patterns with Hamilton'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMOpsï¼šä¸ Hamilton ä¸€èµ·è¿›è¡Œç”Ÿäº§çº§æç¤ºå·¥ç¨‹æ¨¡å¼
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/llmops-production-prompt-engineering-patterns-with-hamilton-5c3a20178ad2?source=collection_archive---------1-----------------------#2023-09-13](https://towardsdatascience.com/llmops-production-prompt-engineering-patterns-with-hamilton-5c3a20178ad2?source=collection_archive---------1-----------------------#2023-09-13)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/llmops-production-prompt-engineering-patterns-with-hamilton-5c3a20178ad2?source=collection_archive---------1-----------------------#2023-09-13](https://towardsdatascience.com/llmops-production-prompt-engineering-patterns-with-hamilton-5c3a20178ad2?source=collection_archive---------1-----------------------#2023-09-13)
- en: An overview of the production-grade ways to iterate on prompts with Hamilton
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç”Ÿäº§çº§åˆ«çš„æç¤ºè¿­ä»£æ¦‚è¿°ï¼Œä¸ Hamilton ä¸€èµ·è¿›è¡Œ
- en: '[](https://medium.com/@stefan.krawczyk?source=post_page-----5c3a20178ad2--------------------------------)[![Stefan
    Krawczyk](../Images/150405abaad9590e1dc2589168ed2fa3.png)](https://medium.com/@stefan.krawczyk?source=post_page-----5c3a20178ad2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5c3a20178ad2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5c3a20178ad2--------------------------------)
    [Stefan Krawczyk](https://medium.com/@stefan.krawczyk?source=post_page-----5c3a20178ad2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@stefan.krawczyk?source=post_page-----5c3a20178ad2--------------------------------)[![Stefan
    Krawczyk](../Images/150405abaad9590e1dc2589168ed2fa3.png)](https://medium.com/@stefan.krawczyk?source=post_page-----5c3a20178ad2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5c3a20178ad2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5c3a20178ad2--------------------------------)
    [Stefan Krawczyk](https://medium.com/@stefan.krawczyk?source=post_page-----5c3a20178ad2--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F193628e26f00&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllmops-production-prompt-engineering-patterns-with-hamilton-5c3a20178ad2&user=Stefan+Krawczyk&userId=193628e26f00&source=post_page-193628e26f00----5c3a20178ad2---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5c3a20178ad2--------------------------------)
    Â·13 min readÂ·Sep 13, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5c3a20178ad2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllmops-production-prompt-engineering-patterns-with-hamilton-5c3a20178ad2&user=Stefan+Krawczyk&userId=193628e26f00&source=-----5c3a20178ad2---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F193628e26f00&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllmops-production-prompt-engineering-patterns-with-hamilton-5c3a20178ad2&user=Stefan+Krawczyk&userId=193628e26f00&source=post_page-193628e26f00----5c3a20178ad2---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5c3a20178ad2--------------------------------)
    Â·13 åˆ†é’Ÿé˜…è¯»Â·2023 å¹´ 9 æœˆ 13 æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5c3a20178ad2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllmops-production-prompt-engineering-patterns-with-hamilton-5c3a20178ad2&user=Stefan+Krawczyk&userId=193628e26f00&source=-----5c3a20178ad2---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5c3a20178ad2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllmops-production-prompt-engineering-patterns-with-hamilton-5c3a20178ad2&source=-----5c3a20178ad2---------------------bookmark_footer-----------)![](../Images/e12ddab031956fa56ab4177df69ca46a.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5c3a20178ad2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllmops-production-prompt-engineering-patterns-with-hamilton-5c3a20178ad2&source=-----5c3a20178ad2---------------------bookmark_footer-----------)![](../Images/e12ddab031956fa56ab4177df69ca46a.png)'
- en: Prompts. How do you evolve them in a production context? *This post is based
    on one that originally appeared* [*here*](https://blog.dagworks.io/p/llmops-production-prompt-engineering)*.*
    Image from [pixabay](https://pixabay.com/illustrations/picture-frame-banner-status-badge-3042585/).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æç¤ºã€‚åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œå¦‚ä½•æ¼”å˜è¿™äº›æç¤ºï¼Ÿ*è¿™ç¯‡æ–‡ç« åŸºäºæœ€åˆå‘è¡¨çš„å†…å®¹* [*è¿™é‡Œ*](https://blog.dagworks.io/p/llmops-production-prompt-engineering)*.*
    å›¾ç‰‡æ¥è‡ª [pixabay](https://pixabay.com/illustrations/picture-frame-banner-status-badge-3042585/).
- en: What you send to your large language model (LLM) is quite important. Small variations
    and changes can have large impacts on outputs, so as your product evolves, the
    need to evolve your prompts will too. LLMs are also constantly being developed
    and released, and so as LLMs change, your prompts will also need to change. Therefore
    itâ€™s important to set up an iteration pattern to operationalize how you â€œdeployâ€
    your prompts so you and your team can move efficiently, but also ensure that production
    issues are minimized, if not avoided. In this post, weâ€™ll guide you through the
    best practices of managing prompts with [Hamilton](http://github.com/dagworks-inc/hamilton),
    an open source micro-orchestration framework, making analogies to [MLOps](https://en.wikipedia.org/wiki/MLOps)
    patterns, and discussing trade-offs along the way. The high level takeaways of
    this post are still applicable even if you donâ€™t use Hamilton.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å‘é€ç»™å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å†…å®¹éå¸¸é‡è¦ã€‚å°çš„å˜åŒ–å’Œè°ƒæ•´å¯èƒ½å¯¹è¾“å‡ºäº§ç”Ÿé‡å¤§å½±å“ï¼Œå› æ­¤éšç€ä½ çš„äº§å“å‘å±•ï¼Œä½ çš„æç¤ºä¹Ÿéœ€è¦è¿›åŒ–ã€‚LLMsä¹Ÿåœ¨ä¸æ–­å¼€å‘å’Œå‘å¸ƒï¼Œå› æ­¤éšç€LLMsçš„å˜åŒ–ï¼Œä½ çš„æç¤ºä¹Ÿéœ€è¦å˜åŒ–ã€‚å› æ­¤ï¼Œå»ºç«‹ä¸€ä¸ªè¿­ä»£æ¨¡å¼æ¥æ“ä½œåŒ–ä½ çš„â€œéƒ¨ç½²â€æç¤ºæ˜¯é‡è¦çš„ï¼Œä»¥ä¾¿ä½ å’Œä½ çš„å›¢é˜Ÿå¯ä»¥é«˜æ•ˆåœ°ç§»åŠ¨ï¼ŒåŒæ—¶ç¡®ä¿ç”Ÿäº§é—®é¢˜æœ€å°åŒ–ï¼Œç”šè‡³é¿å…ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†é€šè¿‡[Hamilton](http://github.com/dagworks-inc/hamilton)è¿™ä¸€ä¸ªå¼€æºå¾®è°ƒåº¦æ¡†æ¶æ¥æŒ‡å¯¼ä½ ç®¡ç†æç¤ºçš„æœ€ä½³å®è·µï¼Œå¹¶ç±»æ¯”[MLOps](https://en.wikipedia.org/wiki/MLOps)æ¨¡å¼ï¼Œå¹¶è®¨è®ºå…¶ä¸­çš„æƒè¡¡ã€‚è¿™ç¯‡æ–‡ç« çš„é«˜çº§è¦ç‚¹å³ä½¿ä½ ä¸ä½¿ç”¨Hamiltonä¹ŸåŒæ ·é€‚ç”¨ã€‚
- en: '**A few things before we start:**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**åœ¨æˆ‘ä»¬å¼€å§‹ä¹‹å‰å‡ ç‚¹æ³¨æ„äº‹é¡¹ï¼š**'
- en: I am one of the co-creators of [Hamilton](http://github.com/dagworks-inc/hamilton).
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘æ˜¯[Hamilton](http://github.com/dagworks-inc/hamilton)çš„å…±åŒåˆ›å»ºè€…ä¹‹ä¸€ã€‚
- en: Not familiar with [Hamilton](http://github.com/dagworks-inc/hamilton)? Scroll
    all the way to the bottom for more links.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹[Hamilton](http://github.com/dagworks-inc/hamilton)ä¸ç†Ÿæ‚‰ï¼Ÿè¯·æ»šåŠ¨åˆ°åº•éƒ¨æŸ¥çœ‹æ›´å¤šé“¾æ¥ã€‚
- en: If youâ€™re looking for a post that talks about â€œcontext managementâ€ this isnâ€™t
    that post. But it is the post that will help you with the nuts and bolts on how
    to iterate and create that production grade â€œprompt context managementâ€ iteration
    story.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœä½ åœ¨å¯»æ‰¾è®¨è®ºâ€œä¸Šä¸‹æ–‡ç®¡ç†â€çš„æ–‡ç« ï¼Œè¿™ä¸æ˜¯é‚£ç¯‡æ–‡ç« ã€‚ä½†è¿™ç¯‡æ–‡ç« å°†å¸®åŠ©ä½ äº†è§£å¦‚ä½•è¿­ä»£å’Œåˆ›å»ºç”Ÿäº§çº§åˆ«çš„â€œæç¤ºä¸Šä¸‹æ–‡ç®¡ç†â€è¿­ä»£æ•…äº‹ã€‚
- en: Weâ€™ll use prompt & prompt template interchangeably.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æç¤ºå’Œæç¤ºæ¨¡æ¿äº’æ¢ä½¿ç”¨ã€‚
- en: Weâ€™ll assume an â€œonlineâ€ web-service setting is where these prompts are being
    used.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å‡è®¾è¿™äº›æç¤ºæ˜¯åœ¨â€œåœ¨çº¿â€ç½‘é¡µæœåŠ¡è®¾ç½®ä¸­ä½¿ç”¨çš„ã€‚
- en: Weâ€™ll be using our [Hamiltonâ€™s PDF summarizer example](https://github.com/DAGWorks-Inc/hamilton/tree/main/examples/LLM_Workflows/pdf_summarizer)
    to project our patterns onto.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨æˆ‘ä»¬çš„[Hamiltonçš„PDFæ‘˜è¦ç¤ºä¾‹](https://github.com/DAGWorks-Inc/hamilton/tree/main/examples/LLM_Workflows/pdf_summarizer)æ¥æ˜ å°„æˆ‘ä»¬çš„æ¨¡å¼ã€‚
- en: Whatâ€™s our credibility here? Weâ€™ve spent our careers building self-service data/MLOps
    tooling, most famously for Stitch Fixâ€™s 100+ Data Scientists. So weâ€™ve seen our
    share of outages and approaches play out over time.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ä¿¡èª‰æ˜¯ä»€ä¹ˆï¼Ÿæˆ‘ä»¬åœ¨ä¸ºStitch Fixçš„100å¤šä½æ•°æ®ç§‘å­¦å®¶æ„å»ºè‡ªæœåŠ¡æ•°æ®/MLOpså·¥å…·æ–¹é¢åº¦è¿‡äº†èŒä¸šç”Ÿæ¶¯ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è§è¯äº†å¾ˆå¤šæ•…éšœå’Œæ–¹æ³•çš„æ¼”å˜ã€‚
- en: Prompts are to LLMs what hyper-parameters are to ML models
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æç¤ºå¯¹LLMsçš„ä½œç”¨ç±»ä¼¼äºè¶…å‚æ•°å¯¹MLæ¨¡å‹çš„ä½œç”¨ã€‚
- en: '**Point:** Prompts + LLM APIs are analogous to hyper-parameters + machine learning
    models.'
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**è¦ç‚¹ï¼š** æç¤º+LLM APIsç±»ä¼¼äºè¶…å‚æ•°+æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚'
- en: In terms of â€œOpsâ€ practices, LLMOps is still in its infancy. MLOps is a little
    older, but still neither are widely adopted if youâ€™re comparing it to how widespread
    knowledge is around DevOps practices.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å°±â€œOpsâ€å®è·µè€Œè¨€ï¼ŒLLMOpsä»å¤„äºèµ·æ­¥é˜¶æ®µã€‚MLOpsç¨å¾®æˆç†Ÿä¸€ç‚¹ï¼Œä½†å¦‚æœä¸DevOpså®è·µçš„å¹¿æ³›çŸ¥è¯†ç›¸æ¯”ï¼Œä»ç„¶æ²¡æœ‰è¢«å¹¿æ³›é‡‡ç”¨ã€‚
- en: 'DevOps practices largely concern themselves with how you ship code to production,
    and MLOps practices how to ship code ***& data artifacts*** (e.g., statistical
    models)to production. So what about LLMOps? Personally, I think itâ€™s closer to
    MLOps since you have:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: DevOpså®è·µä¸»è¦å…³æ³¨å¦‚ä½•å°†ä»£ç äº¤ä»˜åˆ°ç”Ÿäº§ç¯å¢ƒï¼Œè€ŒMLOpså®è·µå…³æ³¨å¦‚ä½•å°†ä»£ç ***& æ•°æ®å·¥ä»¶***ï¼ˆä¾‹å¦‚ï¼Œç»Ÿè®¡æ¨¡å‹ï¼‰äº¤ä»˜åˆ°ç”Ÿäº§ç¯å¢ƒã€‚é‚£ä¹ˆLLMOpså‘¢ï¼Ÿä¸ªäººè®¤ä¸ºï¼Œå®ƒæ›´æ¥è¿‘MLOpsï¼Œå› ä¸ºä½ æœ‰ï¼š
- en: your LLM workflow is simply code.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½ çš„LLMå·¥ä½œæµä»…ä»…æ˜¯ä»£ç ã€‚
- en: and an LLM API is a data artifact that can be â€œtweakedâ€ using prompts, similar
    to a machine learning (ML) model and its hyper-parameters.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LLM APIæ˜¯ä¸€ä¸ªæ•°æ®å·¥ä»¶ï¼Œå¯ä»¥ä½¿ç”¨æç¤ºâ€œè°ƒæ•´â€ï¼Œç±»ä¼¼äºæœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ¨¡å‹åŠå…¶è¶…å‚æ•°ã€‚
- en: Therefore, you most likely care about versioning the LLM API + prompts together
    tightly for good production practices. For instance, in MLOps practice, youâ€™d
    want a process in place to validate your ML model still behaves correctly whenever
    its hyper-parameters are changed.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œä½ å¾ˆå¯èƒ½éœ€è¦ç´§å¯†ç‰ˆæœ¬æ§åˆ¶LLM APIå’Œæç¤ºï¼Œä»¥ç¡®ä¿è‰¯å¥½çš„ç”Ÿäº§å®è·µã€‚ä¾‹å¦‚ï¼Œåœ¨MLOpså®è·µä¸­ï¼Œä½ éœ€è¦ä¸€ä¸ªè¿‡ç¨‹æ¥éªŒè¯MLæ¨¡å‹åœ¨å…¶è¶…å‚æ•°æ›´æ”¹æ—¶ä»ç„¶è¡¨ç°æ­£ç¡®ã€‚
- en: How should you think about operationalizing a prompt?
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½ åº”è¯¥å¦‚ä½•è€ƒè™‘æç¤ºçš„æ“ä½œåŒ–ï¼Ÿ
- en: To be clear, the two parts to control for are the *LLM* and the *prompts*. Much
    like MLOps, when the code or the model artifact changes, you want to be able to
    determine which did. For LLMOps, weâ€™ll want the same discernment, separating the
    LLM workflow from the LLM API + prompts. Importantly, we should consider LLMs
    (self-hosted or APIs) to be mostly static since we less frequently update (or
    even control) their internals. So, changing the *prompts* part of LLM API + prompts
    is effectively like creating a new model artifact.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: éœ€è¦æ˜ç¡®çš„æ˜¯ï¼Œæ§åˆ¶çš„ä¸¤ä¸ªéƒ¨åˆ†æ˜¯*LLM*å’Œ*æç¤º*ã€‚ç±»ä¼¼äºMLOpsï¼Œå½“ä»£ç æˆ–æ¨¡å‹å·¥ä»¶å‘ç”Ÿå˜åŒ–æ—¶ï¼Œä½ éœ€è¦èƒ½å¤Ÿç¡®å®šæ˜¯å“ªä¸€éƒ¨åˆ†å‘ç”Ÿäº†å˜åŒ–ã€‚å¯¹äºLLMOpsï¼Œæˆ‘ä»¬ä¹Ÿéœ€è¦ç›¸åŒçš„è¾¨åˆ«èƒ½åŠ›ï¼Œå°†LLMå·¥ä½œæµä¸LLM
    API + æç¤ºåˆ†å¼€ã€‚é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬åº”è¯¥è®¤ä¸ºLLMï¼ˆè‡ªæ‰˜ç®¡æˆ–APIï¼‰å¤§å¤šæ˜¯é™æ€çš„ï¼Œå› ä¸ºæˆ‘ä»¬ä¸ç»å¸¸æ›´æ–°ï¼ˆç”šè‡³æ§åˆ¶ï¼‰å®ƒä»¬çš„å†…éƒ¨ã€‚å› æ­¤ï¼Œæ”¹å˜LLM API + æç¤ºçš„*æç¤º*éƒ¨åˆ†å®é™…ä¸Šå°±åƒæ˜¯åˆ›å»ºä¸€ä¸ªæ–°çš„æ¨¡å‹å·¥ä»¶ã€‚
- en: 'There are two main ways to treat prompts:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å¤„ç†æç¤ºçš„ä¸»è¦æ–¹å¼æœ‰ä¸¤ç§ï¼š
- en: '**Prompts as dynamic runtime variables**. The template used isnâ€™t static to
    a deployment.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æç¤ºä½œä¸ºåŠ¨æ€è¿è¡Œæ—¶å˜é‡**ã€‚æ‰€ä½¿ç”¨çš„æ¨¡æ¿åœ¨éƒ¨ç½²æ—¶ä¸æ˜¯é™æ€çš„ã€‚'
- en: '**Prompts as code.** The prompt template is static/ predetermined given a deployment.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æç¤ºä½œä¸ºä»£ç **ã€‚æç¤ºæ¨¡æ¿åœ¨ç»™å®šçš„éƒ¨ç½²ä¸‹æ˜¯é™æ€çš„/é¢„å®šçš„ã€‚'
- en: The main difference is the amount of moving parts you need to manage to ensure
    a great production story. Below, we dig into how to use Hamilton in the context
    of these two approaches.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸»è¦åŒºåˆ«åœ¨äºä½ éœ€è¦ç®¡ç†çš„ç§»åŠ¨éƒ¨åˆ†çš„æ•°é‡ï¼Œä»¥ç¡®ä¿ä¸€ä¸ªè‰¯å¥½çš„ç”Ÿäº§æ•…äº‹ã€‚ä¸‹é¢ï¼Œæˆ‘ä»¬å°†æ¢è®¨å¦‚ä½•åœ¨è¿™ä¸¤ç§æ–¹æ³•çš„èƒŒæ™¯ä¸‹ä½¿ç”¨Hamiltonã€‚
- en: Prompts as dynamic runtime variables
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æç¤ºä½œä¸ºåŠ¨æ€è¿è¡Œæ—¶å˜é‡
- en: Dynamically Pass/Load Prompts
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŠ¨æ€ä¼ é€’/åŠ è½½æç¤º
- en: Prompts are just strings. Since strings are a primitive type in most languages,
    this means that they are quite easy to pass around. The idea is to abstract your
    code so that at runtime you pass in the prompts required. More concretely, youâ€™d
    â€œload/reloadâ€ prompt templates whenever thereâ€™s an â€œupdatedâ€ one.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æç¤ºåªæ˜¯å­—ç¬¦ä¸²ã€‚ç”±äºå­—ç¬¦ä¸²åœ¨å¤§å¤šæ•°è¯­è¨€ä¸­æ˜¯åŸå§‹ç±»å‹ï¼Œè¿™æ„å‘³ç€å®ƒä»¬éå¸¸å®¹æ˜“ä¼ é€’ã€‚è¿™ä¸ªæƒ³æ³•æ˜¯æŠ½è±¡ä½ çš„ä»£ç ï¼Œä½¿ä½ åœ¨è¿è¡Œæ—¶ä¼ é€’æ‰€éœ€çš„æç¤ºã€‚æ›´å…·ä½“åœ°è¯´ï¼Œä½ ä¼šåœ¨æœ‰â€œæ›´æ–°â€çš„æç¤ºæ¨¡æ¿æ—¶â€œåŠ è½½/é‡æ–°åŠ è½½â€æç¤ºæ¨¡æ¿ã€‚
- en: The MLOps analogy here, would be to auto-reload the ML model artifact (e.g.,
    a pkl file) whenever a new model is available.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œçš„MLOpsç±»æ¯”æ˜¯ï¼Œå½“æœ‰æ–°çš„æ¨¡å‹å¯ç”¨æ—¶ï¼Œè‡ªåŠ¨é‡æ–°åŠ è½½MLæ¨¡å‹å·¥ä»¶ï¼ˆä¾‹å¦‚ï¼Œpklæ–‡ä»¶ï¼‰ã€‚
- en: '![](../Images/9be4a5424bd19e9f1f9b4908676d99d2.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9be4a5424bd19e9f1f9b4908676d99d2.png)'
- en: 'MLOps Analogy: diagram showing how ML model auto reloading would look. Image
    by author.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: MLOpsç±»æ¯”ï¼šå›¾ç¤ºMLæ¨¡å‹è‡ªåŠ¨é‡æ–°åŠ è½½çš„æ•ˆæœã€‚å›¾ç‰‡ä½œè€…ã€‚
- en: '![](../Images/5eba88a24da31dd531ba38a42de38f3e.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5eba88a24da31dd531ba38a42de38f3e.png)'
- en: Diagram showing what dynamically reloading/querying prompts would look like.
    Image by author.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç¤ºåŠ¨æ€é‡æ–°åŠ è½½/æŸ¥è¯¢æç¤ºçš„æ•ˆæœã€‚å›¾ç‰‡ä½œè€…ã€‚
- en: The benefit here is that you can very quickly roll out new prompts because you
    do not need to redeploy your application!
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œçš„å¥½å¤„æ˜¯ä½ å¯ä»¥éå¸¸è¿…é€Ÿåœ°æ¨å‡ºæ–°çš„æç¤ºï¼Œå› ä¸ºä½ ä¸éœ€è¦é‡æ–°éƒ¨ç½²ä½ çš„åº”ç”¨ç¨‹åºï¼
- en: 'The downside to this iteration speed is increased operational burden:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§è¿­ä»£é€Ÿåº¦çš„ç¼ºç‚¹æ˜¯å¢åŠ äº†æ“ä½œè´Ÿæ‹…ï¼š
- en: To someone monitoring your application, itâ€™ll be unclear when the change occurred
    and whether itâ€™s propagated itself through your systems. For example, you just
    pushed a new prompt, and the LLM now returns more tokens per request, causing
    latency to spike; whoever is monitoring will likely be puzzled, unless you have
    a great change log culture.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹äºç›‘æ§ä½ çš„åº”ç”¨ç¨‹åºçš„äººæ¥è¯´ï¼Œä»€ä¹ˆæ—¶å€™å‘ç”Ÿäº†å˜åŒ–ä»¥åŠæ˜¯å¦å·²ç»åœ¨ä½ çš„ç³»ç»Ÿä¸­ä¼ æ’­å¼€æ¥å°†æ˜¯ä¸æ¸…æ¥šçš„ã€‚ä¾‹å¦‚ï¼Œä½ åˆšåˆšæ¨é€äº†ä¸€ä¸ªæ–°çš„æç¤ºï¼Œè€ŒLLMç°åœ¨æ¯ä¸ªè¯·æ±‚è¿”å›æ›´å¤šçš„tokenï¼Œå¯¼è‡´å»¶è¿Ÿæ¿€å¢ï¼›ç›‘æ§çš„äººå¯èƒ½ä¼šæ„Ÿåˆ°å›°æƒ‘ï¼Œé™¤éä½ æœ‰ä¸€ä¸ªä¼˜ç§€çš„å˜æ›´æ—¥å¿—æ–‡åŒ–ã€‚
- en: Rollback semantics involve having to know about *another* system. You canâ€™t
    just rollback a prior deployment to fix things.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å›æ»šè¯­ä¹‰æ¶‰åŠåˆ°éœ€è¦äº†è§£*å¦ä¸€ä¸ª*ç³»ç»Ÿã€‚ä½ ä¸èƒ½ä»…ä»…å›æ»šä¹‹å‰çš„éƒ¨ç½²æ¥ä¿®å¤é—®é¢˜ã€‚
- en: Youâ€™ll need great monitoring to understand what was run and when; e.g., when
    customer service gives you a ticket to investigate, how do you know what prompt
    was in use?
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½ éœ€è¦ä¼˜ç§€çš„ç›‘æ§æ¥äº†è§£è¿è¡Œäº†ä»€ä¹ˆä»¥åŠä½•æ—¶è¿è¡Œï¼›ä¾‹å¦‚ï¼Œå½“å®¢æˆ·æœåŠ¡ç»™ä½ ä¸€ä¸ªè°ƒæŸ¥çš„ç¥¨æ®æ—¶ï¼Œä½ æ€ä¹ˆçŸ¥é“ä½¿ç”¨äº†ä»€ä¹ˆæç¤ºï¼Ÿ
- en: Youâ€™ll need to manage and monitor whatever system youâ€™re using to manage and
    store your prompts. This will be an extra system youâ€™ll need to maintain outside
    of whatever is serving your code.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½ éœ€è¦ç®¡ç†å’Œç›‘æ§ä½ ç”¨æ¥ç®¡ç†å’Œå­˜å‚¨æç¤ºçš„ä»»ä½•ç³»ç»Ÿã€‚è¿™å°†æ˜¯ä¸€ä¸ªä½ éœ€è¦ç»´æŠ¤çš„é¢å¤–ç³»ç»Ÿï¼Œè¶…å‡ºäº†æä¾›ä½ ä»£ç çš„ç³»ç»Ÿä¹‹å¤–ã€‚
- en: Youâ€™ll need to manage two processes, one for updating and pushing the service,
    and one for updating and pushing prompts. Synchronizing these changes will be
    on you. For example, you need to make a code change to your service to handle
    a new prompt. You will need to coordinate changing two systems to make it work,
    which is extra operational overhead to manage.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½ éœ€è¦ç®¡ç†ä¸¤ä¸ªè¿‡ç¨‹ï¼Œä¸€ä¸ªç”¨äºæ›´æ–°å’Œæ¨é€æœåŠ¡ï¼Œå¦ä¸€ä¸ªç”¨äºæ›´æ–°å’Œæ¨é€æç¤ºã€‚åŒæ­¥è¿™äº›æ›´æ”¹å°†ç”±ä½ è´Ÿè´£ã€‚ä¾‹å¦‚ï¼Œä½ éœ€è¦å¯¹æœåŠ¡è¿›è¡Œä»£ç æ›´æ”¹ä»¥å¤„ç†æ–°çš„æç¤ºã€‚ä½ éœ€è¦åè°ƒæ›´æ”¹ä¸¤ä¸ªç³»ç»Ÿä»¥ä½¿å…¶å·¥ä½œï¼Œè¿™å¢åŠ äº†é¢å¤–çš„æ“ä½œå¼€é”€ã€‚
- en: How it would work with Hamilton
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸ Hamilton ä¸€èµ·å·¥ä½œçš„æ–¹å¼
- en: 'Our PDF summarizer flow would look something like this if you remove `summarize_text_from_summaries_prompt`
    and `summarize_chunk_of_text_prompt` function definitions:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ åˆ é™¤`sumarize_text_from_summaries_prompt`å’Œ`sumarize_chunk_of_text_prompt`å‡½æ•°å®šä¹‰ï¼Œæˆ‘ä»¬çš„
    PDF æ€»ç»“å™¨æµç¨‹å¤§è‡´å¦‚ä¸‹ï¼š
- en: '![](../Images/6c2edc36ce6f03b6ef456e4e855bc4bc.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6c2edc36ce6f03b6ef456e4e855bc4bc.png)'
- en: summarization_shortened.py. Note the two inputs `*_prompt` that denote prompts
    that are now required as input to the dataflow to function. With Hamilton youâ€™ll
    be able to determine what inputs should be required for your prompt template by
    just looking at a diagram like this. Diagram created via Hamilton. Image by author.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: summarization_shortened.pyã€‚æ³¨æ„ä¸¤ä¸ªè¾“å…¥`*_prompt`ï¼Œå®ƒä»¬è¡¨ç¤ºç°åœ¨ä½œä¸ºæ•°æ®æµè¾“å…¥æ‰€éœ€çš„æç¤ºã€‚é€šè¿‡ Hamiltonï¼Œä½ å¯ä»¥é€šè¿‡æŸ¥çœ‹åƒè¿™æ ·çš„å›¾è¡¨æ¥ç¡®å®šä½ çš„æç¤ºæ¨¡æ¿æ‰€éœ€çš„è¾“å…¥ã€‚å›¾è¡¨ç”±
    Hamilton åˆ›å»ºã€‚å›¾åƒç”±ä½œè€…æä¾›ã€‚
- en: 'To operate things, youâ€™ll want to either inject the prompts at request time:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æ“ä½œäº‹ç‰©ï¼Œä½ éœ€è¦åœ¨è¯·æ±‚æ—¶æ³¨å…¥æç¤ºï¼š
- en: '[PRE0]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Or you change your code to dynamically load prompts, i.e., add functions to
    retrieve prompts from an external system as part of the Hamilton dataflow. At
    each invocation, they will query for the prompt to use (you can of course cache
    this for performance):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…ä½ å¯ä»¥æ›´æ”¹ä»£ç ä»¥åŠ¨æ€åŠ è½½æç¤ºï¼Œå³å°†å‡½æ•°æ·»åŠ åˆ° Hamilton æ•°æ®æµä¸­ä»¥ä»å¤–éƒ¨ç³»ç»Ÿæ£€ç´¢æç¤ºã€‚åœ¨æ¯æ¬¡è°ƒç”¨æ—¶ï¼Œå®ƒä»¬å°†æŸ¥è¯¢è¦ä½¿ç”¨çš„æç¤ºï¼ˆå½“ç„¶ä½ å¯ä»¥ç¼“å­˜ä»¥æé«˜æ€§èƒ½ï¼‰ï¼š
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Driver code:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Driver ä»£ç ï¼š
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: How would I log prompts used and monitor flows?
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æˆ‘å¦‚ä½•è®°å½•ä½¿ç”¨çš„æç¤ºå¹¶ç›‘æ§æµç¨‹ï¼Ÿ
- en: Here we outline a few ways to monitor what went on.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æ¦‚è¿°äº†å‡ ç§ç›‘æ§å‘ç”Ÿæƒ…å†µçš„æ–¹æ³•ã€‚
- en: Log results of execution. That is run Hamilton, then emit information to wherever
    you want it to go.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®°å½•æ‰§è¡Œç»“æœã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œè¿è¡Œ Hamiltonï¼Œç„¶åå°†ä¿¡æ¯å‘é€åˆ°ä½ å¸Œæœ›å®ƒå»çš„åœ°æ–¹ã€‚
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*Note. In the above, Hamilton allows you to request* **any *intermediate***
    *outputs simply by requesting â€œfunctionsâ€ (i.e. nodes in the diagram) by name.
    If we really want to get all the intermediate outputs of the entire dataflow,
    we can do so and log it wherever we want to!*'
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*æ³¨æ„ã€‚åœ¨ä¸Šè¿°å†…å®¹ä¸­ï¼ŒHamilton å…è®¸ä½ è¯·æ±‚* **ä»»ä½• *ä¸­é—´*** *è¾“å‡ºï¼Œåªéœ€æŒ‰åç§°è¯·æ±‚â€œå‡½æ•°â€ï¼ˆå³å›¾ä¸­çš„èŠ‚ç‚¹ï¼‰ã€‚å¦‚æœæˆ‘ä»¬çœŸçš„æƒ³è·å–æ•´ä¸ªæ•°æ®æµçš„æ‰€æœ‰ä¸­é—´è¾“å‡ºï¼Œæˆ‘ä»¬å¯ä»¥è¿™æ ·åšå¹¶å°†å…¶è®°å½•åˆ°ä»»ä½•æˆ‘ä»¬æƒ³è¦çš„åœ°æ–¹ï¼*'
- en: 'Use loggers inside Hamilton functions (to see the power of this approach, [see
    my old talk on structured logs](https://www.youtube.com/watch?v=4Y3VdS2pLF4)):'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨ Hamilton å‡½æ•°å†…éƒ¨ä½¿ç”¨è®°å½•å™¨ï¼ˆè¦æŸ¥çœ‹è¿™ç§æ–¹æ³•çš„å¼ºå¤§åŠŸèƒ½ï¼Œ[è¯·å‚è§æˆ‘å…³äºç»“æ„åŒ–æ—¥å¿—çš„æ—§è®²åº§](https://www.youtube.com/watch?v=4Y3VdS2pLF4)ï¼‰ï¼š
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Extend Hamilton to emit this information. You use Hamilton to capture information
    from executed functions, i.e. nodes, without needing to insert logging statement
    inside the functionâ€™s body. This promotes reusability since you can toggle logging
    between development and production settings at the Driver level. See [GraphAdapters](https://hamilton.dagworks.io/en/latest/reference/graph-adapters/),
    or write your own [Python decorator](https://realpython.com/primer-on-python-decorators/#simple-decorators)
    to wrap functions for monitoring.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰©å±• Hamilton ä»¥å‘å‡ºè¿™äº›ä¿¡æ¯ã€‚ä½ å¯ä»¥ä½¿ç”¨ Hamilton æ•è·æ‰§è¡Œå‡½æ•°çš„ä¿¡æ¯ï¼Œå³èŠ‚ç‚¹ï¼Œè€Œæ— éœ€åœ¨å‡½æ•°ä½“å†…æ’å…¥æ—¥å¿—è¯­å¥ã€‚è¿™ä¿ƒè¿›äº†é‡ç”¨ï¼Œå› ä¸ºä½ å¯ä»¥åœ¨
    Driver çº§åˆ«åœ¨å¼€å‘å’Œç”Ÿäº§è®¾ç½®ä¹‹é—´åˆ‡æ¢æ—¥å¿—è®°å½•ã€‚å‚è§ [GraphAdapters](https://hamilton.dagworks.io/en/latest/reference/graph-adapters/)ï¼Œæˆ–ç¼–å†™ä½ è‡ªå·±çš„
    [Python è£…é¥°å™¨](https://realpython.com/primer-on-python-decorators/#simple-decorators)
    æ¥åŒ…è£…å‡½æ•°è¿›è¡Œç›‘æ§ã€‚
- en: In any of the above code, you could easily pull in a 3rd party tool to help
    track & monitor the code, as well as the external API call.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šè¿°ä»»ä½•ä»£ç ä¸­ï¼Œä½ éƒ½å¯ä»¥è½»æ¾åœ°å¼•å…¥ç¬¬ä¸‰æ–¹å·¥å…·æ¥å¸®åŠ©è·Ÿè¸ªå’Œç›‘æ§ä»£ç ä»¥åŠå¤–éƒ¨ API è°ƒç”¨ã€‚
- en: Prompts as code
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½œä¸ºä»£ç çš„æç¤º
- en: Prompts as static strings
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½œä¸ºé™æ€å­—ç¬¦ä¸²çš„æç¤º
- en: Since prompts are simply strings, theyâ€™re also very amenable to being stored
    along with your source code. The idea is to store as many prompt versions as you
    like within your code so that at runtime, the set of prompts available is fixed
    and deterministic.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæç¤ºä»…ä»…æ˜¯å­—ç¬¦ä¸²ï¼Œå®ƒä»¬ä¹Ÿéå¸¸é€‚åˆä¸æºä»£ç ä¸€èµ·å­˜å‚¨ã€‚è¿™ä¸ªæƒ³æ³•æ˜¯å°†å°½å¯èƒ½å¤šçš„æç¤ºç‰ˆæœ¬å­˜å‚¨åœ¨ä½ çš„ä»£ç ä¸­ï¼Œä»¥ä¾¿åœ¨è¿è¡Œæ—¶ï¼Œæ‰€ç”¨æç¤ºé›†æ˜¯å›ºå®šä¸”ç¡®å®šçš„ã€‚
- en: The MLOps analogy here is, instead of dynamically reloading models, you instead
    bake the ML model into the container/hard code the reference. Once deployed, your
    app has everything that it needs. The deployment is immutable; nothing changes
    once itâ€™s up. This makes debugging & determining whatâ€™s going on, much simpler.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œçš„ MLOps ç±»æ¯”æ˜¯ï¼Œä¸å…¶åŠ¨æ€é‡æ–°åŠ è½½æ¨¡å‹ï¼Œä¸å¦‚å°† ML æ¨¡å‹åµŒå…¥åˆ°å®¹å™¨ä¸­/ç¡¬ç¼–ç å¼•ç”¨ã€‚ä¸€æ—¦éƒ¨ç½²ï¼Œä½ çš„åº”ç”¨ç¨‹åºæ‹¥æœ‰å®ƒæ‰€éœ€çš„ä¸€åˆ‡ã€‚éƒ¨ç½²æ˜¯ä¸å¯å˜çš„ï¼›ä¸€æ—¦å¯åŠ¨ä¾¿ä¸ä¼šæ›´æ”¹ã€‚è¿™ä½¿å¾—è°ƒè¯•å’Œç¡®å®šé—®é¢˜å˜å¾—æ›´åŠ ç®€å•ã€‚
- en: '![](../Images/a6b4892b68bdd963d6a95375f8129072.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a6b4892b68bdd963d6a95375f8129072.png)'
- en: 'MLOps Analogy: make an immutable deployment by making the model fixed for your
    appâ€™s deployment. Image by author.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps ç±»æ¯”ï¼šé€šè¿‡å°†æ¨¡å‹å›ºå®šä»¥è¿›è¡Œåº”ç”¨ç¨‹åºçš„éƒ¨ç½²ï¼Œä»è€Œåˆ›å»ºä¸€ä¸ªä¸å¯å˜çš„éƒ¨ç½²ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: '![](../Images/f1a0b5cfaa56786c4fa4908b3a0c9b93.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1a0b5cfaa56786c4fa4908b3a0c9b93.png)'
- en: Diagram showing how treating prompts as code enables you to leverage your CI/CD
    and build an immutable deployment for talking to your LLM API. Image by author.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç¤ºå±•ç¤ºäº†å¦‚ä½•å°†æç¤ºè§†ä¸ºä»£ç ï¼Œä½¿ä½ èƒ½å¤Ÿåˆ©ç”¨ CI/CD æ„å»ºä¸€ä¸ªä¸å¯å˜çš„éƒ¨ç½²æ¥ä¸ LLM API è¿›è¡Œäº¤äº’ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: 'This approach has many operational benefits:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•æœ‰è®¸å¤šæ“ä½œä¸Šçš„å¥½å¤„ï¼š
- en: Whenever a new prompt is pushed, it forces a new deployment. Rollback semantics
    are clear if thereâ€™s an issue with a new prompt.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ¯å½“æ¨é€ä¸€ä¸ªæ–°æç¤ºæ—¶ï¼Œéƒ½ä¼šå¼ºåˆ¶æ‰§è¡Œæ–°çš„éƒ¨ç½²ã€‚å¦‚æœæ–°æç¤ºå‡ºç°é—®é¢˜ï¼Œå›æ»šè¯­ä¹‰æ˜¯æ˜ç¡®çš„ã€‚
- en: You can submit a pull request (PR) for the source code and prompts at the same
    time. It becomes simpler to review what the change is, and the downstream dependencies
    of what these prompts will touch/interact with.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åŒæ—¶æäº¤æºä»£ç å’Œæç¤ºçš„æ‹‰å–è¯·æ±‚ï¼ˆPRï¼‰ã€‚è¿™ä½¿å¾—å®¡æŸ¥æ›´æ”¹å˜å¾—æ›´ç®€å•ï¼Œå¹¶ä¸”å¯ä»¥æ¸…æ¥šåœ°äº†è§£è¿™äº›æç¤ºå°†å½±å“/äº¤äº’çš„ä¸‹æ¸¸ä¾èµ–å…³ç³»ã€‚
- en: You can add checks to your CI/CD system to ensure bad prompts donâ€™t make it
    to production.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨ CI/CD ç³»ç»Ÿä¸­æ·»åŠ æ£€æŸ¥ï¼Œä»¥ç¡®ä¿ä¸è‰¯æç¤ºä¸ä¼šè¿›å…¥ç”Ÿäº§ç¯å¢ƒã€‚
- en: Itâ€™s simpler to debug an issue. You just pull the (Docker) container that was
    created and youâ€™ll be able to exactly replicate any customer issue quickly and
    easily.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è°ƒè¯•é—®é¢˜å˜å¾—æ›´ç®€å•ã€‚ä½ åªéœ€æ‹‰å–åˆ›å»ºçš„ï¼ˆDockerï¼‰å®¹å™¨ï¼Œå°±èƒ½è¿…é€Ÿè½»æ¾åœ°ç²¾ç¡®å¤ç°ä»»ä½•å®¢æˆ·é—®é¢˜ã€‚
- en: There is no other â€œprompt systemâ€ to maintain or manage. Simplifying operations.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸éœ€è¦ç»´æŠ¤æˆ–ç®¡ç†å…¶ä»–â€œæç¤ºç³»ç»Ÿâ€ã€‚ç®€åŒ–äº†æ“ä½œã€‚
- en: It doesnâ€™t preclude adding extra monitoring and visibility.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¿™å¹¶ä¸æ’é™¤æ·»åŠ é¢å¤–çš„ç›‘æ§å’Œå¯è§†æ€§ã€‚
- en: How it would work with Hamilton
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¿™åœ¨ Hamilton ä¸­å¦‚ä½•å·¥ä½œ
- en: 'The prompts would be encoded into functions into the dataflow/directed acyclic
    graph (DAG):'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: æç¤ºå°†è¢«ç¼–ç ä¸ºæ•°æ®æµ/æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰ä¸­çš„å‡½æ•°ï¼š
- en: '![](../Images/1e6312bfd0b4ee92760f5999a4e30b8a.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1e6312bfd0b4ee92760f5999a4e30b8a.png)'
- en: What summarization.py in the PDF summarizer example looks like. The prompt templates
    are part of the code. Diagram created via Hamilton. Image by author.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: PDF æ‘˜è¦ç¤ºä¾‹ä¸­çš„ summarization.py æ–‡ä»¶çš„æ ·å­ã€‚æç¤ºæ¨¡æ¿æ˜¯ä»£ç çš„ä¸€éƒ¨åˆ†ã€‚å›¾ç¤ºç”± Hamilton åˆ›å»ºã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: 'Pairing this code with [git](https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control),
    we have a lightweight versioning system for your entire dataflow (i.e. â€œchainâ€),
    so you can always discern what state the world was in, given a git commit SHA.
    If you want to manage and have access to multiple prompts at any given point in
    time, Hamilton has two powerful abstractions to enable you to do so: `@config.when`
    and *Python modules*. This allows you to store and keep available all older prompt
    versions together and specify which one to use via code.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: å°†è¿™æ®µä»£ç ä¸ [git](https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control)
    ç»“åˆä½¿ç”¨ï¼Œæˆ‘ä»¬ä¸ºä½ çš„æ•´ä¸ªæ•°æ®æµï¼ˆå³â€œé“¾â€ï¼‰æä¾›äº†ä¸€ä¸ªè½»é‡çº§çš„ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿï¼Œè¿™æ ·ä½ æ€»æ˜¯å¯ä»¥æ ¹æ® git commit SHA è¾¨åˆ«ä¸–ç•Œçš„çŠ¶æ€ã€‚å¦‚æœä½ æƒ³åœ¨ä»»ä½•æ—¶é—´ç‚¹ç®¡ç†å’Œè®¿é—®å¤šä¸ªæç¤ºï¼ŒHamilton
    æä¾›äº†ä¸¤ä¸ªå¼ºå¤§çš„æŠ½è±¡æ¥å®ç°è¿™ä¸€ç‚¹ï¼š`@config.when` å’Œ *Python æ¨¡å—*ã€‚è¿™å…è®¸ä½ å­˜å‚¨å¹¶ä¿ç•™æ‰€æœ‰æ—§ç‰ˆæœ¬çš„æç¤ºï¼Œå¹¶é€šè¿‡ä»£ç æŒ‡å®šä½¿ç”¨å“ªä¸ªç‰ˆæœ¬ã€‚
- en: '@config.when ([docs](https://hamilton.dagworks.io/en/latest/reference/decorators/config_when/))'
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '@config.when ([æ–‡æ¡£](https://hamilton.dagworks.io/en/latest/reference/decorators/config_when/))'
- en: Hamilton has a concept of decorators, which are just annotations on functions.
    The `@config.when` decorator allows to specify alternative implementations for
    a functions, i.e. â€œnodeâ€, in your dataflow. In this case, we specify alternative
    prompts.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Hamilton æœ‰ä¸€ä¸ªè£…é¥°å™¨çš„æ¦‚å¿µï¼Œå®ƒä»¬åªæ˜¯å‡½æ•°ä¸Šçš„æ³¨è§£ã€‚`@config.when` è£…é¥°å™¨å…è®¸ä¸ºæ•°æ®æµä¸­çš„å‡½æ•°ï¼ˆå³â€œèŠ‚ç‚¹â€ï¼‰æŒ‡å®šæ›¿ä»£å®ç°ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æŒ‡å®šæ›¿ä»£çš„æç¤ºã€‚
- en: '[PRE5]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You can keep adding functions annotated with `@config.when`, allowing you to
    swap between them using configuration passed to the Hamilton `Driver`. When instantiating
    the `Driver`, it will construct the dataflow using the prompt implementation associated
    with the configuration value.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥ç»§ç»­æ·»åŠ å¸¦æœ‰`@config.when`æ³¨è§£çš„å‡½æ•°ï¼Œè¿™æ ·å¯ä»¥é€šè¿‡ä¼ é€’ç»™Hamilton `Driver`çš„é…ç½®åœ¨å®ƒä»¬ä¹‹é—´åˆ‡æ¢ã€‚åœ¨å®ä¾‹åŒ–`Driver`æ—¶ï¼Œå®ƒå°†ä½¿ç”¨ä¸é…ç½®å€¼å…³è”çš„æç¤ºå®ç°æ¥æ„å»ºæ•°æ®æµã€‚
- en: '[PRE6]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Module switching
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¨¡å—åˆ‡æ¢
- en: Alternatively to using `@config.when`, you can instead place your different
    prompt implementations into different Python modules. Then, at `Driver` construction
    time, pass the correct module for the context you want to use.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†ä½¿ç”¨`@config.when`ä¹‹å¤–ï¼Œä½ è¿˜å¯ä»¥å°†ä¸åŒçš„æç¤ºå®ç°æ”¾å…¥ä¸åŒçš„Pythonæ¨¡å—ä¸­ã€‚ç„¶åï¼Œåœ¨`Driver`æ„é€ æ—¶ï¼Œä¼ é€’é€‚åˆä½ æƒ³ä½¿ç”¨çš„ä¸Šä¸‹æ–‡çš„æ­£ç¡®æ¨¡å—ã€‚
- en: 'So here we have one module housing V1 of our prompt:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥åœ¨è¿™é‡Œæˆ‘ä»¬æœ‰ä¸€ä¸ªåŒ…å«V1çš„æ¨¡å—ï¼š
- en: '[PRE7]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here we have one module housing V2 (see how they differ slightly):'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªåŒ…å«V2çš„æ¨¡å—ï¼ˆçœ‹çœ‹å®ƒä»¬ä¹‹é—´çš„ç»†å¾®å·®åˆ«ï¼‰ï¼š
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the driver code below, we choose the right module to use based on some context.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹é¢çš„é©±åŠ¨ä»£ç ä¸­ï¼Œæˆ‘ä»¬æ ¹æ®æŸäº›ä¸Šä¸‹æ–‡é€‰æ‹©è¦ä½¿ç”¨çš„æ­£ç¡®æ¨¡å—ã€‚
- en: '[PRE9]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Using the module approach allows us to encapsulate and version whole sets of
    prompts together. If you want to go back in time (via git), or see what a blessed
    prompt version was, you just need to navigate to the correct commit, and then
    look in the right module.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ¨¡å—æ–¹æ³•å…è®¸æˆ‘ä»¬å°†æ•´ä¸ªæç¤ºé›†å°è£…å’Œç‰ˆæœ¬åŒ–ã€‚å¦‚æœä½ æƒ³å›åˆ°è¿‡å»ï¼ˆé€šè¿‡gitï¼‰ï¼Œæˆ–è€…æŸ¥çœ‹ä¸€ä¸ªè¢«æ‰¹å‡†çš„æç¤ºç‰ˆæœ¬ï¼Œä½ åªéœ€è¦å¯¼èˆªåˆ°æ­£ç¡®çš„æäº¤ï¼Œç„¶åæŸ¥çœ‹æ­£ç¡®çš„æ¨¡å—ã€‚
- en: How would I log prompts used and monitor flows?
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æˆ‘è¯¥å¦‚ä½•è®°å½•ä½¿ç”¨çš„æç¤ºå¹¶ç›‘æ§æµç¨‹ï¼Ÿ
- en: Assuming youâ€™re using git to track your code, you wouldnâ€™t need to record what
    prompts were being used. Instead, youâ€™d just need to know what git commit SHA
    is deployed and youâ€™ll be able to track the version of your code and prompts simultaneously.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾ä½ ä½¿ç”¨gitæ¥è·Ÿè¸ªä½ çš„ä»£ç ï¼Œä½ å°±ä¸éœ€è¦è®°å½•ä½¿ç”¨äº†å“ªäº›æç¤ºã€‚ç›¸åï¼Œä½ åªéœ€çŸ¥é“éƒ¨ç½²çš„gitæäº¤SHAï¼Œä½ å°±èƒ½åŒæ—¶è·Ÿè¸ªä½ çš„ä»£ç å’Œæç¤ºçš„ç‰ˆæœ¬ã€‚
- en: 'To monitor flows, just like the above approach, you have the same monitoring
    hooks available at your disposal, and I wont repeat them here, but they are:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ç›‘æ§æµç¨‹ï¼Œå°±åƒä¸Šè¿°æ–¹æ³•ä¸€æ ·ï¼Œä½ å¯ä»¥ä½¿ç”¨ç›¸åŒçš„ç›‘æ§é’©å­ï¼Œæˆ‘ä¸ä¼šåœ¨è¿™é‡Œé‡å¤ï¼Œä½†å®ƒä»¬æ˜¯ï¼š
- en: Request any intermediate outputs and log them yourself outside of Hamilton.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯·æ±‚ä»»ä½•ä¸­é—´è¾“å‡ºå¹¶åœ¨Hamiltonä¹‹å¤–è®°å½•å®ƒä»¬ã€‚
- en: Log them from within the function yourself, or build a [Python decorator](https://realpython.com/primer-on-python-decorators/#simple-decorators)
    / [GraphAdapter](https://hamilton.dagworks.io/en/latest/reference/graph-adapters/)
    to do it at the framework level.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»å‡½æ•°å†…éƒ¨è®°å½•å®ƒä»¬ï¼Œæˆ–æ„å»ºä¸€ä¸ª[Pythonè£…é¥°å™¨](https://realpython.com/primer-on-python-decorators/#simple-decorators)
    / [GraphAdapter](https://hamilton.dagworks.io/en/latest/reference/graph-adapters/)åœ¨æ¡†æ¶çº§åˆ«è¿›è¡Œè®°å½•ã€‚
- en: Integrate 3rd party tooling for monitoring your code and LLM API calls.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é›†æˆç¬¬ä¸‰æ–¹å·¥å…·æ¥ç›‘æ§ä½ çš„ä»£ç å’ŒLLM APIè°ƒç”¨ã€‚
- en: or all the above!
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ–è€…ä»¥ä¸Šæ‰€æœ‰ï¼
- en: What about A/B testing my prompts?
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é‚£å¦‚ä½•è¿›è¡ŒA/Bæµ‹è¯•æˆ‘çš„æç¤ºï¼Ÿ
- en: With any ML initiative, itâ€™s important to measure business impacts of changes.
    Likewise, with LLMs + prompts, itâ€™ll be important to test and measure changes
    against important business metrics. In the MLOps world, youâ€™d be A/B testing ML
    models to evaluate their business value by dividing traffic between them. To ensure
    the randomness necessary to A/B tests, you wouldnâ€™t know at runtime which model
    to use until a coin is flipped. However, to get those models out, they both would
    have follow a process to qualify them. So for prompts, we should think similarly.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä»»ä½•æœºå™¨å­¦ä¹ é¡¹ç›®ï¼Œæµ‹é‡å˜æ›´çš„ä¸šåŠ¡å½±å“éå¸¸é‡è¦ã€‚åŒæ ·ï¼Œå¯¹äºLLMs + æç¤ºï¼Œæµ‹è¯•å’Œè¡¡é‡å˜æ›´å¯¹é‡è¦ä¸šåŠ¡æŒ‡æ ‡çš„å½±å“ä¹Ÿå¾ˆé‡è¦ã€‚åœ¨MLOpsä¸–ç•Œä¸­ï¼Œä½ ä¼šå¯¹MLæ¨¡å‹è¿›è¡ŒA/Bæµ‹è¯•ï¼Œä»¥é€šè¿‡åœ¨å®ƒä»¬ä¹‹é—´åˆ†é…æµé‡æ¥è¯„ä¼°å®ƒä»¬çš„ä¸šåŠ¡ä»·å€¼ã€‚ä¸ºäº†ç¡®ä¿A/Bæµ‹è¯•æ‰€éœ€çš„éšæœºæ€§ï¼Œä½ ä¸ä¼šåœ¨è¿è¡Œæ—¶çŸ¥é“ä½¿ç”¨å“ªä¸ªæ¨¡å‹ï¼Œç›´åˆ°æ·ç¡¬å¸ã€‚ç„¶è€Œï¼Œè¦è®©è¿™äº›æ¨¡å‹ä¸Šçº¿ï¼Œå®ƒä»¬éƒ½éœ€è¦éµå¾ªä¸€ä¸ªè¿‡ç¨‹ä»¥è¿›è¡Œèµ„æ ¼è®¤è¯ã€‚å› æ­¤ï¼Œå¯¹äºæç¤ºï¼Œæˆ‘ä»¬ä¹Ÿåº”è¯¥è€ƒè™‘ç±»ä¼¼çš„æ–¹å¼ã€‚
- en: The above two prompt engineering patterns donâ€™t preclude you from being able
    to A/B test prompts, but it means you need to manage a process to enable however
    many prompt templates youâ€™re testing in parallel. If youâ€™re also adjusting code
    paths, having them in code will be simpler to discern and debug what is going
    on, and you can make use of the ``@config.when`` decorator / python module swapping
    for this purpose. Versus, having to critically rely on your logging/monitoring/observability
    stack to tell you what prompt was used if youâ€™re dynamically loading/passing them
    in and then having to mentally map which prompts go with which code paths.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°ä¸¤ç§æç¤ºå·¥ç¨‹æ¨¡å¼å¹¶ä¸ä¼šå¦¨ç¢ä½ è¿›è¡Œ A/B æµ‹è¯•æç¤ºï¼Œä½†è¿™æ„å‘³ç€ä½ éœ€è¦ç®¡ç†ä¸€ä¸ªè¿‡ç¨‹ï¼Œä»¥å¯ç”¨ä½ æ­£åœ¨å¹¶è¡Œæµ‹è¯•çš„ä»»ä½•æ•°é‡çš„æç¤ºæ¨¡æ¿ã€‚å¦‚æœä½ è¿˜åœ¨è°ƒæ•´ä»£ç è·¯å¾„ï¼Œå°†å®ƒä»¬æ”¾åœ¨ä»£ç ä¸­å°†æ›´å®¹æ˜“è¾¨åˆ«å’Œè°ƒè¯•å‘ç”Ÿäº†ä»€ä¹ˆï¼Œå¹¶ä¸”ä½ å¯ä»¥åˆ©ç”¨
    ``@config.when`` è£…é¥°å™¨/ Python æ¨¡å—äº¤æ¢æ¥å®ç°è¿™ä¸€ç›®çš„ã€‚ä¸å…¶ä¾èµ–ä½ çš„æ—¥å¿—/ç›‘æ§/å¯è§‚å¯Ÿæ€§æ ˆæ¥å‘Šè¯‰ä½ ä½¿ç”¨äº†å“ªä¸ªæç¤ºï¼Œç‰¹åˆ«æ˜¯å½“ä½ åŠ¨æ€åŠ è½½/ä¼ é€’å®ƒä»¬æ—¶ï¼Œå¹¶ä¸”è¿˜éœ€è¿›è¡Œå¿ƒç†æ˜ å°„å“ªä¸ªæç¤ºå¯¹åº”å“ªä¸ªä»£ç è·¯å¾„ï¼Œå‰è€…å°†æ›´ä¸ºç®€ä¾¿ã€‚
- en: Note, this all gets harder if you start needing to change multiple prompts for
    an A/B test because you have several of them in a flow. For example you have two
    prompts in your workflow and youâ€™re changing LLMs, youâ€™ll want to A/B test the
    change holistically, rather than individually per prompt. Our advice, by putting
    the prompts into code your operational life will be simpler, since youâ€™ll know
    what two prompts belong to what code paths without having to do any mental mapping.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œå¦‚æœä½ å¼€å§‹éœ€è¦æ›´æ”¹å¤šä¸ªæç¤ºè¿›è¡Œ A/B æµ‹è¯•ï¼Œå› ä¸ºä½ åœ¨ä¸€ä¸ªæµç¨‹ä¸­æœ‰å‡ ä¸ªæç¤ºï¼Œé‚£ä¹ˆè¿™ä¼šå˜å¾—æ›´åŠ å›°éš¾ã€‚ä¾‹å¦‚ï¼Œä½ åœ¨å·¥ä½œæµä¸­æœ‰ä¸¤ä¸ªæç¤ºå¹¶ä¸”ä½ æ­£åœ¨æ›´æ¢ LLMï¼Œä½ å°†å¸Œæœ›ä»æ•´ä½“ä¸Šè¿›è¡Œ
    A/B æµ‹è¯•ï¼Œè€Œä¸æ˜¯é€ä¸ªæç¤ºåœ°æµ‹è¯•ã€‚æˆ‘ä»¬çš„å»ºè®®æ˜¯ï¼Œé€šè¿‡å°†æç¤ºæ”¾å…¥ä»£ç ä¸­ï¼Œä½ çš„æ“ä½œç”Ÿæ´»ä¼šæ›´ç®€å•ï¼Œå› ä¸ºä½ å°†çŸ¥é“å“ªäº›æç¤ºå±äºå“ªäº›ä»£ç è·¯å¾„ï¼Œè€Œæ— éœ€è¿›è¡Œä»»ä½•å¿ƒç†æ˜ å°„ã€‚
- en: Summary
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ€»ç»“
- en: In this post, we explained two patterns for managing prompts in a production
    environment with Hamilton. The first approach treats **prompts as** **dynamic
    runtime variables,** while the second, treats **prompts as code** for production
    settings. If you value reducing operational burden, then our advice is to encode
    prompts as code, as it is operationally simpler, unless the speed to change them
    really matters for you.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬è§£é‡Šäº†åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨ Hamilton ç®¡ç†æç¤ºçš„ä¸¤ç§æ¨¡å¼ã€‚ç¬¬ä¸€ç§æ–¹æ³•å°†**æç¤ºè§†ä¸º** **åŠ¨æ€è¿è¡Œæ—¶å˜é‡**ï¼Œè€Œç¬¬äºŒç§æ–¹æ³•å°†**æç¤ºè§†ä¸ºä»£ç **ç”¨äºç”Ÿäº§è®¾ç½®ã€‚å¦‚æœä½ é‡è§†å‡å°‘æ“ä½œè´Ÿæ‹…ï¼Œé‚£ä¹ˆæˆ‘ä»¬çš„å»ºè®®æ˜¯å°†æç¤ºç¼–ç ä¸ºä»£ç ï¼Œå› ä¸ºå®ƒåœ¨æ“ä½œä¸Šæ›´ç®€å•ï¼Œé™¤éæ›´æ”¹å®ƒä»¬çš„é€Ÿåº¦å¯¹ä½ æ¥è¯´çœŸçš„å¾ˆé‡è¦ã€‚
- en: 'To recap:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ç»“ï¼š
- en: '**Prompts as dynamic runtime variables**. Use an external system to pass the
    prompts to your Hamilton dataflows, or use Hamilton to pull them from a DB. For
    debugging & monitoring, itâ€™s important to be able to determine what prompt was
    used for a given invocation. You can integrate open source tools, or use something
    like the DAGWorks Platform to help ensure you know what was used for any invocation
    of your code.'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æç¤ºä½œä¸ºåŠ¨æ€è¿è¡Œæ—¶å˜é‡**ã€‚ä½¿ç”¨å¤–éƒ¨ç³»ç»Ÿå°†æç¤ºä¼ é€’åˆ°ä½ çš„ Hamilton æ•°æ®æµï¼Œæˆ–ä½¿ç”¨ Hamilton ä»æ•°æ®åº“ä¸­æå–æç¤ºã€‚å¯¹äºè°ƒè¯•å’Œç›‘æ§ï¼Œèƒ½å¤Ÿç¡®å®šç»™å®šè°ƒç”¨ä½¿ç”¨äº†å“ªä¸ªæç¤ºæ˜¯å¾ˆé‡è¦çš„ã€‚ä½ å¯ä»¥é›†æˆå¼€æºå·¥å…·ï¼Œæˆ–ä½¿ç”¨åƒ
    DAGWorks å¹³å°è¿™æ ·çš„å·¥å…·æ¥å¸®åŠ©ç¡®ä¿ä½ çŸ¥é“åœ¨ä»»ä½•ä»£ç è°ƒç”¨ä¸­ä½¿ç”¨äº†ä»€ä¹ˆã€‚'
- en: '**Prompts as code.** Encoding the prompts as code allows easy versioning with
    git. Change management can be done via pull requests and CI/CD checks. It works
    well with Hamiltonâ€™s features like `@config.when` and module switching at the
    Driver level because it determines clearly what version of the prompt is used.
    This approach strengthens the use of any tooling you might use to monitor or track,
    like the DAGWorks Platform, as prompts for a deployment are immutable.'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æç¤ºä½œä¸ºä»£ç ã€‚** å°†æç¤ºç¼–ç ä¸ºä»£ç å…è®¸ä½¿ç”¨ git è¿›è¡Œç®€å•çš„ç‰ˆæœ¬æ§åˆ¶ã€‚æ›´æ”¹ç®¡ç†å¯ä»¥é€šè¿‡æ‹‰å–è¯·æ±‚å’Œ CI/CD æ£€æŸ¥æ¥å®Œæˆã€‚å®ƒä¸ Hamilton
    çš„åŠŸèƒ½ï¼Œå¦‚ `@config.when` å’Œ Driver çº§åˆ«çš„æ¨¡å—åˆ‡æ¢ï¼Œå·¥ä½œè‰¯å¥½ï¼Œå› ä¸ºå®ƒæ¸…æ¥šåœ°ç¡®å®šä½¿ç”¨äº†å“ªä¸ªç‰ˆæœ¬çš„æç¤ºã€‚è¿™ç§æ–¹æ³•åŠ å¼ºäº†ä½ å¯èƒ½ä½¿ç”¨çš„ä»»ä½•å·¥å…·ï¼Œå¦‚
    DAGWorks å¹³å°ï¼Œå› ä¸ºéƒ¨ç½²çš„æç¤ºæ˜¯ä¸å¯å˜çš„ã€‚'
- en: We want to hear from you!
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æƒ³å¬å¬ä½ çš„æ„è§ï¼
- en: 'If youâ€™re excited by any of this, or have strong opinions, leave a comment,
    or drop by our Slack channel! Some links to do praise/complain/chat:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å¯¹è¿™äº›å†…å®¹æ„Ÿåˆ°å…´å¥‹æˆ–æœ‰å¼ºçƒˆçš„æ„è§ï¼Œè¯·ç•™ä¸‹è¯„è®ºï¼Œæˆ–è®¿é—®æˆ‘ä»¬çš„ Slack é¢‘é“ï¼ä¸€äº›ç”¨äºè¡¨æ‰¬/æŠ•è¯‰/èŠå¤©çš„é“¾æ¥ï¼š
- en: ğŸ“£ [join our community on Slack](https://join.slack.com/t/hamilton-opensource/shared_invite/zt-1bjs72asx-wcUTgH7q7QX1igiQ5bbdcg)
    â€” weâ€™re more than happy to help answer questions you might have or get you started.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğŸ“£ [åŠ å…¥æˆ‘ä»¬çš„ Slack ç¤¾åŒº](https://join.slack.com/t/hamilton-opensource/shared_invite/zt-1bjs72asx-wcUTgH7q7QX1igiQ5bbdcg)
    â€” æˆ‘ä»¬éå¸¸ä¹æ„å¸®åŠ©è§£ç­”ä½ å¯èƒ½æœ‰çš„é—®é¢˜æˆ–å¸®åŠ©ä½ å…¥é—¨ã€‚
- en: â­ï¸ us on [GitHub](https://github.com/DAGWorks-Inc/hamilton).
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â­ï¸ åœ¨ [GitHub](https://github.com/DAGWorks-Inc/hamilton) ä¸Šå…³æ³¨æˆ‘ä»¬ã€‚
- en: ğŸ“ leave us an [issue](https://github.com/DAGWorks-Inc/hamilton/issues) if you
    find something.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğŸ“ å¦‚æœä½ å‘ç°é—®é¢˜ï¼Œè¯·åœ¨ [issue](https://github.com/DAGWorks-Inc/hamilton/issues) ä¸­å‘Šè¯‰æˆ‘ä»¬ã€‚
- en: ğŸ“š read our [documentation](https://hamilton.dagworks.io/en/latest/).
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğŸ“š é˜…è¯»æˆ‘ä»¬çš„ [æ–‡æ¡£](https://hamilton.dagworks.io/en/latest/)ã€‚
- en: âŒ¨ï¸ interactively [learn about Hamilton in your browser](https://www.tryhamilton.dev/).
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: âŒ¨ï¸ äº’åŠ¨å¼ [åœ¨æµè§ˆå™¨ä¸­äº†è§£ Hamilton](https://www.tryhamilton.dev/)ã€‚
- en: 'Other Hamilton links/posts you might be interested in:'
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å…¶ä»–ä½ å¯èƒ½æ„Ÿå…´è¶£çš„ Hamilton é“¾æ¥/å¸–å­ï¼š
- en: '[tryhamilton.dev](https://www.tryhamilton.dev/) â€” an interactive tutorial in
    your browser!'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[tryhamilton.dev](https://www.tryhamilton.dev/) â€” åœ¨æµè§ˆå™¨ä¸­çš„äº’åŠ¨æ•™ç¨‹ï¼'
- en: '[Hamilton + Lineage in 10 minutes](/lineage-hamilton-in-10-minutes-c2b8a944e2e6)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hamilton + Lineage åœ¨ 10 åˆ†é’Ÿå†…](/lineage-hamilton-in-10-minutes-c2b8a944e2e6)'
- en: '[How to use Hamilton with Pandas in 5 Minutes](/how-to-use-hamilton-with-pandas-in-5-minutes-89f63e5af8f5)'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å¦‚ä½•åœ¨ 5 åˆ†é’Ÿå†…ä¸ Pandas ä¸€èµ·ä½¿ç”¨ Hamilton](/how-to-use-hamilton-with-pandas-in-5-minutes-89f63e5af8f5)'
- en: '[How to use Hamilton with Ray in 5 minutes](/scaling-hamilton-with-ray-in-5-minutes-3beb1755fc09)'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å¦‚ä½•åœ¨ 5 åˆ†é’Ÿå†…ä¸ Ray ä¸€èµ·ä½¿ç”¨ Hamilton](/scaling-hamilton-with-ray-in-5-minutes-3beb1755fc09)'
- en: '[How to use Hamilton in a Notebook environment](/how-to-iterate-with-hamilton-in-a-notebook-8ec0f85851ed)'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[å¦‚ä½•åœ¨ç¬”è®°æœ¬ç¯å¢ƒä¸­ä½¿ç”¨ Hamilton](/how-to-iterate-with-hamilton-in-a-notebook-8ec0f85851ed)'
- en: '[General backstory & introduction on Hamilton](/functions-dags-introducing-hamilton-a-microframework-for-dataframe-generation-more-8e34b84efc1d)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hamilton çš„èƒŒæ™¯æ•…äº‹ä¸ä»‹ç»](/functions-dags-introducing-hamilton-a-microframework-for-dataframe-generation-more-8e34b84efc1d)'
- en: '[The perks of creating dataflows with Hamilton](https://medium.com/@thijean/the-perks-of-creating-dataflows-with-hamilton-36e8c56dd2a)
    (Organic user post on Hamilton!)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ä½¿ç”¨ Hamilton åˆ›å»ºæ•°æ®æµçš„å¥½å¤„](https://medium.com/@thijean/the-perks-of-creating-dataflows-with-hamilton-36e8c56dd2a)ï¼ˆæ¥è‡ª
    Hamilton çš„æœ‰æœºç”¨æˆ·å¸–å­ï¼ï¼‰'
