- en: 'LLMOps: Production Prompt Engineering Patterns with Hamilton'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMOps：与 Hamilton 一起进行生产级提示工程模式
- en: 原文：[https://towardsdatascience.com/llmops-production-prompt-engineering-patterns-with-hamilton-5c3a20178ad2?source=collection_archive---------1-----------------------#2023-09-13](https://towardsdatascience.com/llmops-production-prompt-engineering-patterns-with-hamilton-5c3a20178ad2?source=collection_archive---------1-----------------------#2023-09-13)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/llmops-production-prompt-engineering-patterns-with-hamilton-5c3a20178ad2?source=collection_archive---------1-----------------------#2023-09-13](https://towardsdatascience.com/llmops-production-prompt-engineering-patterns-with-hamilton-5c3a20178ad2?source=collection_archive---------1-----------------------#2023-09-13)
- en: An overview of the production-grade ways to iterate on prompts with Hamilton
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生产级别的提示迭代概述，与 Hamilton 一起进行
- en: '[](https://medium.com/@stefan.krawczyk?source=post_page-----5c3a20178ad2--------------------------------)[![Stefan
    Krawczyk](../Images/150405abaad9590e1dc2589168ed2fa3.png)](https://medium.com/@stefan.krawczyk?source=post_page-----5c3a20178ad2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5c3a20178ad2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5c3a20178ad2--------------------------------)
    [Stefan Krawczyk](https://medium.com/@stefan.krawczyk?source=post_page-----5c3a20178ad2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@stefan.krawczyk?source=post_page-----5c3a20178ad2--------------------------------)[![Stefan
    Krawczyk](../Images/150405abaad9590e1dc2589168ed2fa3.png)](https://medium.com/@stefan.krawczyk?source=post_page-----5c3a20178ad2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5c3a20178ad2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5c3a20178ad2--------------------------------)
    [Stefan Krawczyk](https://medium.com/@stefan.krawczyk?source=post_page-----5c3a20178ad2--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F193628e26f00&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllmops-production-prompt-engineering-patterns-with-hamilton-5c3a20178ad2&user=Stefan+Krawczyk&userId=193628e26f00&source=post_page-193628e26f00----5c3a20178ad2---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5c3a20178ad2--------------------------------)
    ·13 min read·Sep 13, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5c3a20178ad2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllmops-production-prompt-engineering-patterns-with-hamilton-5c3a20178ad2&user=Stefan+Krawczyk&userId=193628e26f00&source=-----5c3a20178ad2---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F193628e26f00&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllmops-production-prompt-engineering-patterns-with-hamilton-5c3a20178ad2&user=Stefan+Krawczyk&userId=193628e26f00&source=post_page-193628e26f00----5c3a20178ad2---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5c3a20178ad2--------------------------------)
    ·13 分钟阅读·2023 年 9 月 13 日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5c3a20178ad2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllmops-production-prompt-engineering-patterns-with-hamilton-5c3a20178ad2&user=Stefan+Krawczyk&userId=193628e26f00&source=-----5c3a20178ad2---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5c3a20178ad2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllmops-production-prompt-engineering-patterns-with-hamilton-5c3a20178ad2&source=-----5c3a20178ad2---------------------bookmark_footer-----------)![](../Images/e12ddab031956fa56ab4177df69ca46a.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5c3a20178ad2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllmops-production-prompt-engineering-patterns-with-hamilton-5c3a20178ad2&source=-----5c3a20178ad2---------------------bookmark_footer-----------)![](../Images/e12ddab031956fa56ab4177df69ca46a.png)'
- en: Prompts. How do you evolve them in a production context? *This post is based
    on one that originally appeared* [*here*](https://blog.dagworks.io/p/llmops-production-prompt-engineering)*.*
    Image from [pixabay](https://pixabay.com/illustrations/picture-frame-banner-status-badge-3042585/).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 提示。在生产环境中，如何演变这些提示？*这篇文章基于最初发表的内容* [*这里*](https://blog.dagworks.io/p/llmops-production-prompt-engineering)*.*
    图片来自 [pixabay](https://pixabay.com/illustrations/picture-frame-banner-status-badge-3042585/).
- en: What you send to your large language model (LLM) is quite important. Small variations
    and changes can have large impacts on outputs, so as your product evolves, the
    need to evolve your prompts will too. LLMs are also constantly being developed
    and released, and so as LLMs change, your prompts will also need to change. Therefore
    it’s important to set up an iteration pattern to operationalize how you “deploy”
    your prompts so you and your team can move efficiently, but also ensure that production
    issues are minimized, if not avoided. In this post, we’ll guide you through the
    best practices of managing prompts with [Hamilton](http://github.com/dagworks-inc/hamilton),
    an open source micro-orchestration framework, making analogies to [MLOps](https://en.wikipedia.org/wiki/MLOps)
    patterns, and discussing trade-offs along the way. The high level takeaways of
    this post are still applicable even if you don’t use Hamilton.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你发送给大型语言模型（LLM）的内容非常重要。小的变化和调整可能对输出产生重大影响，因此随着你的产品发展，你的提示也需要进化。LLMs也在不断开发和发布，因此随着LLMs的变化，你的提示也需要变化。因此，建立一个迭代模式来操作化你的“部署”提示是重要的，以便你和你的团队可以高效地移动，同时确保生产问题最小化，甚至避免。在这篇文章中，我们将通过[Hamilton](http://github.com/dagworks-inc/hamilton)这一个开源微调度框架来指导你管理提示的最佳实践，并类比[MLOps](https://en.wikipedia.org/wiki/MLOps)模式，并讨论其中的权衡。这篇文章的高级要点即使你不使用Hamilton也同样适用。
- en: '**A few things before we start:**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**在我们开始之前几点注意事项：**'
- en: I am one of the co-creators of [Hamilton](http://github.com/dagworks-inc/hamilton).
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我是[Hamilton](http://github.com/dagworks-inc/hamilton)的共同创建者之一。
- en: Not familiar with [Hamilton](http://github.com/dagworks-inc/hamilton)? Scroll
    all the way to the bottom for more links.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对[Hamilton](http://github.com/dagworks-inc/hamilton)不熟悉？请滚动到底部查看更多链接。
- en: If you’re looking for a post that talks about “context management” this isn’t
    that post. But it is the post that will help you with the nuts and bolts on how
    to iterate and create that production grade “prompt context management” iteration
    story.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你在寻找讨论“上下文管理”的文章，这不是那篇文章。但这篇文章将帮助你了解如何迭代和创建生产级别的“提示上下文管理”迭代故事。
- en: We’ll use prompt & prompt template interchangeably.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将提示和提示模板互换使用。
- en: We’ll assume an “online” web-service setting is where these prompts are being
    used.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们假设这些提示是在“在线”网页服务设置中使用的。
- en: We’ll be using our [Hamilton’s PDF summarizer example](https://github.com/DAGWorks-Inc/hamilton/tree/main/examples/LLM_Workflows/pdf_summarizer)
    to project our patterns onto.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用我们的[Hamilton的PDF摘要示例](https://github.com/DAGWorks-Inc/hamilton/tree/main/examples/LLM_Workflows/pdf_summarizer)来映射我们的模式。
- en: What’s our credibility here? We’ve spent our careers building self-service data/MLOps
    tooling, most famously for Stitch Fix’s 100+ Data Scientists. So we’ve seen our
    share of outages and approaches play out over time.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的信誉是什么？我们在为Stitch Fix的100多位数据科学家构建自服务数据/MLOps工具方面度过了职业生涯。因此，我们见证了很多故障和方法的演变。
- en: Prompts are to LLMs what hyper-parameters are to ML models
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示对LLMs的作用类似于超参数对ML模型的作用。
- en: '**Point:** Prompts + LLM APIs are analogous to hyper-parameters + machine learning
    models.'
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**要点：** 提示+LLM APIs类似于超参数+机器学习模型。'
- en: In terms of “Ops” practices, LLMOps is still in its infancy. MLOps is a little
    older, but still neither are widely adopted if you’re comparing it to how widespread
    knowledge is around DevOps practices.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 就“Ops”实践而言，LLMOps仍处于起步阶段。MLOps稍微成熟一点，但如果与DevOps实践的广泛知识相比，仍然没有被广泛采用。
- en: 'DevOps practices largely concern themselves with how you ship code to production,
    and MLOps practices how to ship code ***& data artifacts*** (e.g., statistical
    models)to production. So what about LLMOps? Personally, I think it’s closer to
    MLOps since you have:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: DevOps实践主要关注如何将代码交付到生产环境，而MLOps实践关注如何将代码***& 数据工件***（例如，统计模型）交付到生产环境。那么LLMOps呢？个人认为，它更接近MLOps，因为你有：
- en: your LLM workflow is simply code.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你的LLM工作流仅仅是代码。
- en: and an LLM API is a data artifact that can be “tweaked” using prompts, similar
    to a machine learning (ML) model and its hyper-parameters.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LLM API是一个数据工件，可以使用提示“调整”，类似于机器学习（ML）模型及其超参数。
- en: Therefore, you most likely care about versioning the LLM API + prompts together
    tightly for good production practices. For instance, in MLOps practice, you’d
    want a process in place to validate your ML model still behaves correctly whenever
    its hyper-parameters are changed.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你很可能需要紧密版本控制LLM API和提示，以确保良好的生产实践。例如，在MLOps实践中，你需要一个过程来验证ML模型在其超参数更改时仍然表现正确。
- en: How should you think about operationalizing a prompt?
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你应该如何考虑提示的操作化？
- en: To be clear, the two parts to control for are the *LLM* and the *prompts*. Much
    like MLOps, when the code or the model artifact changes, you want to be able to
    determine which did. For LLMOps, we’ll want the same discernment, separating the
    LLM workflow from the LLM API + prompts. Importantly, we should consider LLMs
    (self-hosted or APIs) to be mostly static since we less frequently update (or
    even control) their internals. So, changing the *prompts* part of LLM API + prompts
    is effectively like creating a new model artifact.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 需要明确的是，控制的两个部分是*LLM*和*提示*。类似于MLOps，当代码或模型工件发生变化时，你需要能够确定是哪一部分发生了变化。对于LLMOps，我们也需要相同的辨别能力，将LLM工作流与LLM
    API + 提示分开。重要的是，我们应该认为LLM（自托管或API）大多是静态的，因为我们不经常更新（甚至控制）它们的内部。因此，改变LLM API + 提示的*提示*部分实际上就像是创建一个新的模型工件。
- en: 'There are two main ways to treat prompts:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 处理提示的主要方式有两种：
- en: '**Prompts as dynamic runtime variables**. The template used isn’t static to
    a deployment.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**提示作为动态运行时变量**。所使用的模板在部署时不是静态的。'
- en: '**Prompts as code.** The prompt template is static/ predetermined given a deployment.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**提示作为代码**。提示模板在给定的部署下是静态的/预定的。'
- en: The main difference is the amount of moving parts you need to manage to ensure
    a great production story. Below, we dig into how to use Hamilton in the context
    of these two approaches.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 主要区别在于你需要管理的移动部分的数量，以确保一个良好的生产故事。下面，我们将探讨如何在这两种方法的背景下使用Hamilton。
- en: Prompts as dynamic runtime variables
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示作为动态运行时变量
- en: Dynamically Pass/Load Prompts
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动态传递/加载提示
- en: Prompts are just strings. Since strings are a primitive type in most languages,
    this means that they are quite easy to pass around. The idea is to abstract your
    code so that at runtime you pass in the prompts required. More concretely, you’d
    “load/reload” prompt templates whenever there’s an “updated” one.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 提示只是字符串。由于字符串在大多数语言中是原始类型，这意味着它们非常容易传递。这个想法是抽象你的代码，使你在运行时传递所需的提示。更具体地说，你会在有“更新”的提示模板时“加载/重新加载”提示模板。
- en: The MLOps analogy here, would be to auto-reload the ML model artifact (e.g.,
    a pkl file) whenever a new model is available.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的MLOps类比是，当有新的模型可用时，自动重新加载ML模型工件（例如，pkl文件）。
- en: '![](../Images/9be4a5424bd19e9f1f9b4908676d99d2.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9be4a5424bd19e9f1f9b4908676d99d2.png)'
- en: 'MLOps Analogy: diagram showing how ML model auto reloading would look. Image
    by author.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps类比：图示ML模型自动重新加载的效果。图片作者。
- en: '![](../Images/5eba88a24da31dd531ba38a42de38f3e.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5eba88a24da31dd531ba38a42de38f3e.png)'
- en: Diagram showing what dynamically reloading/querying prompts would look like.
    Image by author.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图示动态重新加载/查询提示的效果。图片作者。
- en: The benefit here is that you can very quickly roll out new prompts because you
    do not need to redeploy your application!
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的好处是你可以非常迅速地推出新的提示，因为你不需要重新部署你的应用程序！
- en: 'The downside to this iteration speed is increased operational burden:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这种迭代速度的缺点是增加了操作负担：
- en: To someone monitoring your application, it’ll be unclear when the change occurred
    and whether it’s propagated itself through your systems. For example, you just
    pushed a new prompt, and the LLM now returns more tokens per request, causing
    latency to spike; whoever is monitoring will likely be puzzled, unless you have
    a great change log culture.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于监控你的应用程序的人来说，什么时候发生了变化以及是否已经在你的系统中传播开来将是不清楚的。例如，你刚刚推送了一个新的提示，而LLM现在每个请求返回更多的token，导致延迟激增；监控的人可能会感到困惑，除非你有一个优秀的变更日志文化。
- en: Rollback semantics involve having to know about *another* system. You can’t
    just rollback a prior deployment to fix things.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 回滚语义涉及到需要了解*另一个*系统。你不能仅仅回滚之前的部署来修复问题。
- en: You’ll need great monitoring to understand what was run and when; e.g., when
    customer service gives you a ticket to investigate, how do you know what prompt
    was in use?
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你需要优秀的监控来了解运行了什么以及何时运行；例如，当客户服务给你一个调查的票据时，你怎么知道使用了什么提示？
- en: You’ll need to manage and monitor whatever system you’re using to manage and
    store your prompts. This will be an extra system you’ll need to maintain outside
    of whatever is serving your code.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你需要管理和监控你用来管理和存储提示的任何系统。这将是一个你需要维护的额外系统，超出了提供你代码的系统之外。
- en: You’ll need to manage two processes, one for updating and pushing the service,
    and one for updating and pushing prompts. Synchronizing these changes will be
    on you. For example, you need to make a code change to your service to handle
    a new prompt. You will need to coordinate changing two systems to make it work,
    which is extra operational overhead to manage.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你需要管理两个过程，一个用于更新和推送服务，另一个用于更新和推送提示。同步这些更改将由你负责。例如，你需要对服务进行代码更改以处理新的提示。你需要协调更改两个系统以使其工作，这增加了额外的操作开销。
- en: How it would work with Hamilton
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与 Hamilton 一起工作的方式
- en: 'Our PDF summarizer flow would look something like this if you remove `summarize_text_from_summaries_prompt`
    and `summarize_chunk_of_text_prompt` function definitions:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你删除`sumarize_text_from_summaries_prompt`和`sumarize_chunk_of_text_prompt`函数定义，我们的
    PDF 总结器流程大致如下：
- en: '![](../Images/6c2edc36ce6f03b6ef456e4e855bc4bc.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6c2edc36ce6f03b6ef456e4e855bc4bc.png)'
- en: summarization_shortened.py. Note the two inputs `*_prompt` that denote prompts
    that are now required as input to the dataflow to function. With Hamilton you’ll
    be able to determine what inputs should be required for your prompt template by
    just looking at a diagram like this. Diagram created via Hamilton. Image by author.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: summarization_shortened.py。注意两个输入`*_prompt`，它们表示现在作为数据流输入所需的提示。通过 Hamilton，你可以通过查看像这样的图表来确定你的提示模板所需的输入。图表由
    Hamilton 创建。图像由作者提供。
- en: 'To operate things, you’ll want to either inject the prompts at request time:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 要操作事物，你需要在请求时注入提示：
- en: '[PRE0]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Or you change your code to dynamically load prompts, i.e., add functions to
    retrieve prompts from an external system as part of the Hamilton dataflow. At
    each invocation, they will query for the prompt to use (you can of course cache
    this for performance):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 或者你可以更改代码以动态加载提示，即将函数添加到 Hamilton 数据流中以从外部系统检索提示。在每次调用时，它们将查询要使用的提示（当然你可以缓存以提高性能）：
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Driver code:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Driver 代码：
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: How would I log prompts used and monitor flows?
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我如何记录使用的提示并监控流程？
- en: Here we outline a few ways to monitor what went on.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们概述了几种监控发生情况的方法。
- en: Log results of execution. That is run Hamilton, then emit information to wherever
    you want it to go.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录执行结果。也就是说，运行 Hamilton，然后将信息发送到你希望它去的地方。
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*Note. In the above, Hamilton allows you to request* **any *intermediate***
    *outputs simply by requesting “functions” (i.e. nodes in the diagram) by name.
    If we really want to get all the intermediate outputs of the entire dataflow,
    we can do so and log it wherever we want to!*'
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*注意。在上述内容中，Hamilton 允许你请求* **任何 *中间*** *输出，只需按名称请求“函数”（即图中的节点）。如果我们真的想获取整个数据流的所有中间输出，我们可以这样做并将其记录到任何我们想要的地方！*'
- en: 'Use loggers inside Hamilton functions (to see the power of this approach, [see
    my old talk on structured logs](https://www.youtube.com/watch?v=4Y3VdS2pLF4)):'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Hamilton 函数内部使用记录器（要查看这种方法的强大功能，[请参见我关于结构化日志的旧讲座](https://www.youtube.com/watch?v=4Y3VdS2pLF4)）：
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Extend Hamilton to emit this information. You use Hamilton to capture information
    from executed functions, i.e. nodes, without needing to insert logging statement
    inside the function’s body. This promotes reusability since you can toggle logging
    between development and production settings at the Driver level. See [GraphAdapters](https://hamilton.dagworks.io/en/latest/reference/graph-adapters/),
    or write your own [Python decorator](https://realpython.com/primer-on-python-decorators/#simple-decorators)
    to wrap functions for monitoring.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展 Hamilton 以发出这些信息。你可以使用 Hamilton 捕获执行函数的信息，即节点，而无需在函数体内插入日志语句。这促进了重用，因为你可以在
    Driver 级别在开发和生产设置之间切换日志记录。参见 [GraphAdapters](https://hamilton.dagworks.io/en/latest/reference/graph-adapters/)，或编写你自己的
    [Python 装饰器](https://realpython.com/primer-on-python-decorators/#simple-decorators)
    来包装函数进行监控。
- en: In any of the above code, you could easily pull in a 3rd party tool to help
    track & monitor the code, as well as the external API call.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述任何代码中，你都可以轻松地引入第三方工具来帮助跟踪和监控代码以及外部 API 调用。
- en: Prompts as code
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 作为代码的提示
- en: Prompts as static strings
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 作为静态字符串的提示
- en: Since prompts are simply strings, they’re also very amenable to being stored
    along with your source code. The idea is to store as many prompt versions as you
    like within your code so that at runtime, the set of prompts available is fixed
    and deterministic.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 由于提示仅仅是字符串，它们也非常适合与源代码一起存储。这个想法是将尽可能多的提示版本存储在你的代码中，以便在运行时，所用提示集是固定且确定的。
- en: The MLOps analogy here is, instead of dynamically reloading models, you instead
    bake the ML model into the container/hard code the reference. Once deployed, your
    app has everything that it needs. The deployment is immutable; nothing changes
    once it’s up. This makes debugging & determining what’s going on, much simpler.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的 MLOps 类比是，与其动态重新加载模型，不如将 ML 模型嵌入到容器中/硬编码引用。一旦部署，你的应用程序拥有它所需的一切。部署是不可变的；一旦启动便不会更改。这使得调试和确定问题变得更加简单。
- en: '![](../Images/a6b4892b68bdd963d6a95375f8129072.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a6b4892b68bdd963d6a95375f8129072.png)'
- en: 'MLOps Analogy: make an immutable deployment by making the model fixed for your
    app’s deployment. Image by author.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps 类比：通过将模型固定以进行应用程序的部署，从而创建一个不可变的部署。图片由作者提供。
- en: '![](../Images/f1a0b5cfaa56786c4fa4908b3a0c9b93.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1a0b5cfaa56786c4fa4908b3a0c9b93.png)'
- en: Diagram showing how treating prompts as code enables you to leverage your CI/CD
    and build an immutable deployment for talking to your LLM API. Image by author.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图示展示了如何将提示视为代码，使你能够利用 CI/CD 构建一个不可变的部署来与 LLM API 进行交互。图片由作者提供。
- en: 'This approach has many operational benefits:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有许多操作上的好处：
- en: Whenever a new prompt is pushed, it forces a new deployment. Rollback semantics
    are clear if there’s an issue with a new prompt.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每当推送一个新提示时，都会强制执行新的部署。如果新提示出现问题，回滚语义是明确的。
- en: You can submit a pull request (PR) for the source code and prompts at the same
    time. It becomes simpler to review what the change is, and the downstream dependencies
    of what these prompts will touch/interact with.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以同时提交源代码和提示的拉取请求（PR）。这使得审查更改变得更简单，并且可以清楚地了解这些提示将影响/交互的下游依赖关系。
- en: You can add checks to your CI/CD system to ensure bad prompts don’t make it
    to production.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以在 CI/CD 系统中添加检查，以确保不良提示不会进入生产环境。
- en: It’s simpler to debug an issue. You just pull the (Docker) container that was
    created and you’ll be able to exactly replicate any customer issue quickly and
    easily.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调试问题变得更简单。你只需拉取创建的（Docker）容器，就能迅速轻松地精确复现任何客户问题。
- en: There is no other “prompt system” to maintain or manage. Simplifying operations.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不需要维护或管理其他“提示系统”。简化了操作。
- en: It doesn’t preclude adding extra monitoring and visibility.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这并不排除添加额外的监控和可视性。
- en: How it would work with Hamilton
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这在 Hamilton 中如何工作
- en: 'The prompts would be encoded into functions into the dataflow/directed acyclic
    graph (DAG):'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 提示将被编码为数据流/有向无环图（DAG）中的函数：
- en: '![](../Images/1e6312bfd0b4ee92760f5999a4e30b8a.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1e6312bfd0b4ee92760f5999a4e30b8a.png)'
- en: What summarization.py in the PDF summarizer example looks like. The prompt templates
    are part of the code. Diagram created via Hamilton. Image by author.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: PDF 摘要示例中的 summarization.py 文件的样子。提示模板是代码的一部分。图示由 Hamilton 创建。图片由作者提供。
- en: 'Pairing this code with [git](https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control),
    we have a lightweight versioning system for your entire dataflow (i.e. “chain”),
    so you can always discern what state the world was in, given a git commit SHA.
    If you want to manage and have access to multiple prompts at any given point in
    time, Hamilton has two powerful abstractions to enable you to do so: `@config.when`
    and *Python modules*. This allows you to store and keep available all older prompt
    versions together and specify which one to use via code.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 将这段代码与 [git](https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control)
    结合使用，我们为你的整个数据流（即“链”）提供了一个轻量级的版本控制系统，这样你总是可以根据 git commit SHA 辨别世界的状态。如果你想在任何时间点管理和访问多个提示，Hamilton
    提供了两个强大的抽象来实现这一点：`@config.when` 和 *Python 模块*。这允许你存储并保留所有旧版本的提示，并通过代码指定使用哪个版本。
- en: '@config.when ([docs](https://hamilton.dagworks.io/en/latest/reference/decorators/config_when/))'
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '@config.when ([文档](https://hamilton.dagworks.io/en/latest/reference/decorators/config_when/))'
- en: Hamilton has a concept of decorators, which are just annotations on functions.
    The `@config.when` decorator allows to specify alternative implementations for
    a functions, i.e. “node”, in your dataflow. In this case, we specify alternative
    prompts.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Hamilton 有一个装饰器的概念，它们只是函数上的注解。`@config.when` 装饰器允许为数据流中的函数（即“节点”）指定替代实现。在这种情况下，我们指定替代的提示。
- en: '[PRE5]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You can keep adding functions annotated with `@config.when`, allowing you to
    swap between them using configuration passed to the Hamilton `Driver`. When instantiating
    the `Driver`, it will construct the dataflow using the prompt implementation associated
    with the configuration value.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以继续添加带有`@config.when`注解的函数，这样可以通过传递给Hamilton `Driver`的配置在它们之间切换。在实例化`Driver`时，它将使用与配置值关联的提示实现来构建数据流。
- en: '[PRE6]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Module switching
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模块切换
- en: Alternatively to using `@config.when`, you can instead place your different
    prompt implementations into different Python modules. Then, at `Driver` construction
    time, pass the correct module for the context you want to use.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用`@config.when`之外，你还可以将不同的提示实现放入不同的Python模块中。然后，在`Driver`构造时，传递适合你想使用的上下文的正确模块。
- en: 'So here we have one module housing V1 of our prompt:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在这里我们有一个包含V1的模块：
- en: '[PRE7]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here we have one module housing V2 (see how they differ slightly):'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有一个包含V2的模块（看看它们之间的细微差别）：
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the driver code below, we choose the right module to use based on some context.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的驱动代码中，我们根据某些上下文选择要使用的正确模块。
- en: '[PRE9]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Using the module approach allows us to encapsulate and version whole sets of
    prompts together. If you want to go back in time (via git), or see what a blessed
    prompt version was, you just need to navigate to the correct commit, and then
    look in the right module.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 使用模块方法允许我们将整个提示集封装和版本化。如果你想回到过去（通过git），或者查看一个被批准的提示版本，你只需要导航到正确的提交，然后查看正确的模块。
- en: How would I log prompts used and monitor flows?
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我该如何记录使用的提示并监控流程？
- en: Assuming you’re using git to track your code, you wouldn’t need to record what
    prompts were being used. Instead, you’d just need to know what git commit SHA
    is deployed and you’ll be able to track the version of your code and prompts simultaneously.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你使用git来跟踪你的代码，你就不需要记录使用了哪些提示。相反，你只需知道部署的git提交SHA，你就能同时跟踪你的代码和提示的版本。
- en: 'To monitor flows, just like the above approach, you have the same monitoring
    hooks available at your disposal, and I wont repeat them here, but they are:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 要监控流程，就像上述方法一样，你可以使用相同的监控钩子，我不会在这里重复，但它们是：
- en: Request any intermediate outputs and log them yourself outside of Hamilton.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请求任何中间输出并在Hamilton之外记录它们。
- en: Log them from within the function yourself, or build a [Python decorator](https://realpython.com/primer-on-python-decorators/#simple-decorators)
    / [GraphAdapter](https://hamilton.dagworks.io/en/latest/reference/graph-adapters/)
    to do it at the framework level.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从函数内部记录它们，或构建一个[Python装饰器](https://realpython.com/primer-on-python-decorators/#simple-decorators)
    / [GraphAdapter](https://hamilton.dagworks.io/en/latest/reference/graph-adapters/)在框架级别进行记录。
- en: Integrate 3rd party tooling for monitoring your code and LLM API calls.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成第三方工具来监控你的代码和LLM API调用。
- en: or all the above!
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者以上所有！
- en: What about A/B testing my prompts?
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 那如何进行A/B测试我的提示？
- en: With any ML initiative, it’s important to measure business impacts of changes.
    Likewise, with LLMs + prompts, it’ll be important to test and measure changes
    against important business metrics. In the MLOps world, you’d be A/B testing ML
    models to evaluate their business value by dividing traffic between them. To ensure
    the randomness necessary to A/B tests, you wouldn’t know at runtime which model
    to use until a coin is flipped. However, to get those models out, they both would
    have follow a process to qualify them. So for prompts, we should think similarly.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何机器学习项目，测量变更的业务影响非常重要。同样，对于LLMs + 提示，测试和衡量变更对重要业务指标的影响也很重要。在MLOps世界中，你会对ML模型进行A/B测试，以通过在它们之间分配流量来评估它们的业务价值。为了确保A/B测试所需的随机性，你不会在运行时知道使用哪个模型，直到掷硬币。然而，要让这些模型上线，它们都需要遵循一个过程以进行资格认证。因此，对于提示，我们也应该考虑类似的方式。
- en: The above two prompt engineering patterns don’t preclude you from being able
    to A/B test prompts, but it means you need to manage a process to enable however
    many prompt templates you’re testing in parallel. If you’re also adjusting code
    paths, having them in code will be simpler to discern and debug what is going
    on, and you can make use of the ``@config.when`` decorator / python module swapping
    for this purpose. Versus, having to critically rely on your logging/monitoring/observability
    stack to tell you what prompt was used if you’re dynamically loading/passing them
    in and then having to mentally map which prompts go with which code paths.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 上述两种提示工程模式并不会妨碍你进行 A/B 测试提示，但这意味着你需要管理一个过程，以启用你正在并行测试的任何数量的提示模板。如果你还在调整代码路径，将它们放在代码中将更容易辨别和调试发生了什么，并且你可以利用
    ``@config.when`` 装饰器/ Python 模块交换来实现这一目的。与其依赖你的日志/监控/可观察性栈来告诉你使用了哪个提示，特别是当你动态加载/传递它们时，并且还需进行心理映射哪个提示对应哪个代码路径，前者将更为简便。
- en: Note, this all gets harder if you start needing to change multiple prompts for
    an A/B test because you have several of them in a flow. For example you have two
    prompts in your workflow and you’re changing LLMs, you’ll want to A/B test the
    change holistically, rather than individually per prompt. Our advice, by putting
    the prompts into code your operational life will be simpler, since you’ll know
    what two prompts belong to what code paths without having to do any mental mapping.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果你开始需要更改多个提示进行 A/B 测试，因为你在一个流程中有几个提示，那么这会变得更加困难。例如，你在工作流中有两个提示并且你正在更换 LLM，你将希望从整体上进行
    A/B 测试，而不是逐个提示地测试。我们的建议是，通过将提示放入代码中，你的操作生活会更简单，因为你将知道哪些提示属于哪些代码路径，而无需进行任何心理映射。
- en: Summary
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this post, we explained two patterns for managing prompts in a production
    environment with Hamilton. The first approach treats **prompts as** **dynamic
    runtime variables,** while the second, treats **prompts as code** for production
    settings. If you value reducing operational burden, then our advice is to encode
    prompts as code, as it is operationally simpler, unless the speed to change them
    really matters for you.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们解释了在生产环境中使用 Hamilton 管理提示的两种模式。第一种方法将**提示视为** **动态运行时变量**，而第二种方法将**提示视为代码**用于生产设置。如果你重视减少操作负担，那么我们的建议是将提示编码为代码，因为它在操作上更简单，除非更改它们的速度对你来说真的很重要。
- en: 'To recap:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 总结：
- en: '**Prompts as dynamic runtime variables**. Use an external system to pass the
    prompts to your Hamilton dataflows, or use Hamilton to pull them from a DB. For
    debugging & monitoring, it’s important to be able to determine what prompt was
    used for a given invocation. You can integrate open source tools, or use something
    like the DAGWorks Platform to help ensure you know what was used for any invocation
    of your code.'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**提示作为动态运行时变量**。使用外部系统将提示传递到你的 Hamilton 数据流，或使用 Hamilton 从数据库中提取提示。对于调试和监控，能够确定给定调用使用了哪个提示是很重要的。你可以集成开源工具，或使用像
    DAGWorks 平台这样的工具来帮助确保你知道在任何代码调用中使用了什么。'
- en: '**Prompts as code.** Encoding the prompts as code allows easy versioning with
    git. Change management can be done via pull requests and CI/CD checks. It works
    well with Hamilton’s features like `@config.when` and module switching at the
    Driver level because it determines clearly what version of the prompt is used.
    This approach strengthens the use of any tooling you might use to monitor or track,
    like the DAGWorks Platform, as prompts for a deployment are immutable.'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**提示作为代码。** 将提示编码为代码允许使用 git 进行简单的版本控制。更改管理可以通过拉取请求和 CI/CD 检查来完成。它与 Hamilton
    的功能，如 `@config.when` 和 Driver 级别的模块切换，工作良好，因为它清楚地确定使用了哪个版本的提示。这种方法加强了你可能使用的任何工具，如
    DAGWorks 平台，因为部署的提示是不可变的。'
- en: We want to hear from you!
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们想听听你的意见！
- en: 'If you’re excited by any of this, or have strong opinions, leave a comment,
    or drop by our Slack channel! Some links to do praise/complain/chat:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对这些内容感到兴奋或有强烈的意见，请留下评论，或访问我们的 Slack 频道！一些用于表扬/投诉/聊天的链接：
- en: 📣 [join our community on Slack](https://join.slack.com/t/hamilton-opensource/shared_invite/zt-1bjs72asx-wcUTgH7q7QX1igiQ5bbdcg)
    — we’re more than happy to help answer questions you might have or get you started.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 📣 [加入我们的 Slack 社区](https://join.slack.com/t/hamilton-opensource/shared_invite/zt-1bjs72asx-wcUTgH7q7QX1igiQ5bbdcg)
    — 我们非常乐意帮助解答你可能有的问题或帮助你入门。
- en: ⭐️ us on [GitHub](https://github.com/DAGWorks-Inc/hamilton).
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ⭐️ 在 [GitHub](https://github.com/DAGWorks-Inc/hamilton) 上关注我们。
- en: 📝 leave us an [issue](https://github.com/DAGWorks-Inc/hamilton/issues) if you
    find something.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 📝 如果你发现问题，请在 [issue](https://github.com/DAGWorks-Inc/hamilton/issues) 中告诉我们。
- en: 📚 read our [documentation](https://hamilton.dagworks.io/en/latest/).
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 📚 阅读我们的 [文档](https://hamilton.dagworks.io/en/latest/)。
- en: ⌨️ interactively [learn about Hamilton in your browser](https://www.tryhamilton.dev/).
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ⌨️ 互动式 [在浏览器中了解 Hamilton](https://www.tryhamilton.dev/)。
- en: 'Other Hamilton links/posts you might be interested in:'
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他你可能感兴趣的 Hamilton 链接/帖子：
- en: '[tryhamilton.dev](https://www.tryhamilton.dev/) — an interactive tutorial in
    your browser!'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[tryhamilton.dev](https://www.tryhamilton.dev/) — 在浏览器中的互动教程！'
- en: '[Hamilton + Lineage in 10 minutes](/lineage-hamilton-in-10-minutes-c2b8a944e2e6)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hamilton + Lineage 在 10 分钟内](/lineage-hamilton-in-10-minutes-c2b8a944e2e6)'
- en: '[How to use Hamilton with Pandas in 5 Minutes](/how-to-use-hamilton-with-pandas-in-5-minutes-89f63e5af8f5)'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何在 5 分钟内与 Pandas 一起使用 Hamilton](/how-to-use-hamilton-with-pandas-in-5-minutes-89f63e5af8f5)'
- en: '[How to use Hamilton with Ray in 5 minutes](/scaling-hamilton-with-ray-in-5-minutes-3beb1755fc09)'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何在 5 分钟内与 Ray 一起使用 Hamilton](/scaling-hamilton-with-ray-in-5-minutes-3beb1755fc09)'
- en: '[How to use Hamilton in a Notebook environment](/how-to-iterate-with-hamilton-in-a-notebook-8ec0f85851ed)'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何在笔记本环境中使用 Hamilton](/how-to-iterate-with-hamilton-in-a-notebook-8ec0f85851ed)'
- en: '[General backstory & introduction on Hamilton](/functions-dags-introducing-hamilton-a-microframework-for-dataframe-generation-more-8e34b84efc1d)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hamilton 的背景故事与介绍](/functions-dags-introducing-hamilton-a-microframework-for-dataframe-generation-more-8e34b84efc1d)'
- en: '[The perks of creating dataflows with Hamilton](https://medium.com/@thijean/the-perks-of-creating-dataflows-with-hamilton-36e8c56dd2a)
    (Organic user post on Hamilton!)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Hamilton 创建数据流的好处](https://medium.com/@thijean/the-perks-of-creating-dataflows-with-hamilton-36e8c56dd2a)（来自
    Hamilton 的有机用户帖子！）'
