- en: How Does PPO With Clipping Work?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PPO 的剪切如何工作？
- en: 原文：[https://towardsdatascience.com/how-does-ppo-with-clipping-work-eff71a7a974a](https://towardsdatascience.com/how-does-ppo-with-clipping-work-eff71a7a974a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-does-ppo-with-clipping-work-eff71a7a974a](https://towardsdatascience.com/how-does-ppo-with-clipping-work-eff71a7a974a)
- en: Intuition + math + code, for practitioners
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 直观 + 数学 + 代码，针对实践者
- en: '[](https://medium.com/@byjameskoh?source=post_page-----eff71a7a974a--------------------------------)[![James
    Koh, PhD](../Images/8e7af8b567cdcf24805754801683b426.png)](https://medium.com/@byjameskoh?source=post_page-----eff71a7a974a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----eff71a7a974a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----eff71a7a974a--------------------------------)
    [James Koh, PhD](https://medium.com/@byjameskoh?source=post_page-----eff71a7a974a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@byjameskoh?source=post_page-----eff71a7a974a--------------------------------)[![詹姆斯·科，博士](../Images/8e7af8b567cdcf24805754801683b426.png)](https://medium.com/@byjameskoh?source=post_page-----eff71a7a974a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----eff71a7a974a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----eff71a7a974a--------------------------------)
    [詹姆斯·科，博士](https://medium.com/@byjameskoh?source=post_page-----eff71a7a974a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----eff71a7a974a--------------------------------)
    ·9 min read·Oct 7, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----eff71a7a974a--------------------------------)
    ·9 min 阅读·2023年10月7日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/847c27062f8d23472ca40f8a61a45a03.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/847c27062f8d23472ca40f8a61a45a03.png)'
- en: Photo by [Tamanna Rumee](https://unsplash.com/@tamanna_rumee?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Tamanna Rumee](https://unsplash.com/@tamanna_rumee?utm_source=medium&utm_medium=referral)
    提供，[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: In Reinforcement Learning, Proximal Policy Optimization (PPO) is often cited
    as the example for a policy approach, as compared to DQN (a value-based approach)
    and the big family of actor-critic methods which includes TD3 and SAC.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，近端策略优化（PPO）通常被引用作为策略方法的例子，相对于 DQN（基于价值的方法）和包括 TD3 和 SAC 在内的大量 actor-critic
    方法。
- en: I recalled some time ago, when I was learning it for the first time, I was left
    unconvinced. Many teachers adopt a sort of hand-wavy approach. I don’t buy that,
    and neither should you.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我回想起之前第一次学习时，我感到不信服。许多老师采用了一种模糊的方法。我不接受这种做法，你也不应如此。
- en: In this article, I will attempt to explain how PPO works, supporting the math
    with both intuition and code. You can try different scenarios, and see for yourself
    that it works not just in principle but also in practice, and that there is no
    cherry picking.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将尝试解释 PPO 如何工作，通过直观和代码来支持数学。你可以尝试不同的场景，亲自验证它不仅在原则上有效，而且在实践中也有效，并且没有
    cherry picking。
- en: Why Bother?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么要费心？
- en: PPO and the other SOTA models can be implemented in minutes using stable-baselines3
    (sb3). Anyone following the documentation can get it running, without knowledge
    of the underlying model.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: PPO 和其他 SOTA 模型可以在几分钟内使用 stable-baselines3（sb3）实现。任何遵循文档的人都可以运行它，而无需了解底层模型。
- en: However, whether you are a practitioner or a theorist, fundamentals do matter.
    If you simply treat PPO (or any model for that matter) as a black box, how do
    you expect your users to have faith in what you deliver?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，无论你是实践者还是理论家，基础知识确实重要。如果你仅仅把 PPO（或任何模型）当作一个黑箱，你怎么指望你的用户对你提供的结果有信心呢？
- en: I will do a detailed code walkthrough later this month, writing a wrapper such
    that any environment, be it those from Gymnasium or your own, would work with
    any sb3 model, regardless of whether the space is ‘*Discrete*’ or ‘*Box*’. (Last
    month, I showed how Monte Carlo, SARSA, and Q-learning can be [derived from TD(λ)](/a-cornerstone-of-rl-td-λ-and-3-big-names-2e547b37c05?sk=5282ba767da813522872cbf09aaad2b8),
    all with a single set of code.)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在本月晚些时候进行详细的代码讲解，编写一个包装器，使任何环境，无论是来自 Gymnasium 还是你自己的环境，都能与任何 sb3 模型兼容，无论空间是‘*Discrete*’还是‘*Box*’。（上个月，我展示了如何从[TD(λ)](/a-cornerstone-of-rl-td-λ-and-3-big-names-2e547b37c05?sk=5282ba767da813522872cbf09aaad2b8)派生出
    Monte Carlo、SARSA 和 Q-learning，所有这些都是通过一套代码实现的。）
- en: Enough for tomorrow, let’s be here, right now!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 够了，明天再谈，现在就让我们在这里吧！
- en: Predecessor to PPO
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PPO 的前身
- en: Vanilla policy gradient is the most basic case of the policy-based methods,
    where the policy is learnt and updated directly, rather than being derived from
    some value function. The drawback is that suffers from high variance in the policy
    updates, which is problematic for convergence, especially in environments with
    sparse rewards.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Vanilla策略梯度是基于策略方法中的最基本情况，其中策略是直接学习和更新的，而不是从某些价值函数推导出来的。缺点是策略更新的方差很高，这对收敛性是一个问题，特别是在奖励稀疏的环境中。
- en: Math of TRPO
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TRPO的数学
- en: TRPO (Trust Region Policy Optimization) ensures that the new policy (where ‘new’
    refers to after an update) does not deviate too far from the old policy. This
    is done by asserting a constraint that the KL divergence of the new policy with
    respect to the old policy does not exceed some threshold *δ*.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: TRPO（Trust Region Policy Optimization）确保新的策略（其中“新”指的是更新后）不会偏离旧策略太远。这是通过施加一个约束来实现的，即新策略相对于旧策略的KL散度不超过某个阈值*δ*。
- en: '![](../Images/a8a8b139726c6aeecfbc05e68d157059.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8a8b139726c6aeecfbc05e68d157059.png)'
- en: Objective of PPO. Equation typed by author, with reference to [OpenAI Spinning
    Up](https://spinningup.openai.com/en/latest/algorithms/trpo.html).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: PPO的目标。公式由作者输入，参考[OpenAI Spinning Up](https://spinningup.openai.com/en/latest/algorithms/trpo.html)。
- en: Notice that each policy *π*_{*θ*} is, itself, a distribution. *D_{KL}*, with
    a ‘hat’ above, is the (weighted) average of KL divergence over states visited
    under the old policy. KL divergence, itself, is an average of the log of probability
    ratios, weighted according to the first distribution.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 注意每个策略*π*_{*θ*}本身是一个分布。*D_{KL}*，带有“帽子”，是相对于旧策略下访问的状态的KL散度的（加权）平均值。KL散度本身是概率比的对数的平均值，根据第一个分布加权。
- en: '![](../Images/746df19dd62349b2c2c1ac19ad331368.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/746df19dd62349b2c2c1ac19ad331368.png)'
- en: Formula for KL divergence between two discrete distributions *p* and q. For
    a continuous distribution, we would have probability densities and integral in
    place of the summation. Equation by author.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: KL散度公式用于两个离散分布*p*和q之间。对于连续分布，我们将有概率密度和积分替代求和。公式由作者提供。
- en: The objective function is the surrogate advantage, which is a ratio (action
    probability under the new policy divided by that corresponding to the old policy)
    multiplied by the advantage. This advantage is the expected return over some baseline,
    which can simply be a moving average of the respective state value.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 目标函数是替代优势，它是一个比率（新策略下的动作概率除以对应于旧策略的概率）乘以优势。这个优势是相对于某些基准的期望回报，基准可以简单地是相应状态值的移动平均。
- en: We have a constrained optimization problem, and this is solved using Lagrangian
    multipliers and the conjugate gradient method. The objective and constraint are
    linearized using Taylor expansion, hence the solution is approximate. In order
    to ensure the constraint is met, a backtracking line search is employed to adjust
    the step size in the policy update.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个约束优化问题，使用拉格朗日乘子和共轭梯度法来解决。目标和约束通过泰勒展开线性化，因此解决方案是近似的。为了确保满足约束，使用回溯线搜索来调整策略更新中的步长。
- en: (This is where I draw the line, of not delving further away. The above are math
    corresponding to a Level 7xx course, which practitioners can skip; knowing the
    direction as described is good enough in my opinion.)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: （这是我设定的界限，不再深入探讨。上述内容对应于Level 7xx课程，实践者可以跳过；了解描述的方向在我看来已经足够了。）
- en: Intuition of TRPO
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TRPO的直觉
- en: If an action is advantageous, ie. **A > 0**, we want to increase its probability.
    It should be selected more often after updating the policy.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个动作是有利的，即**A > 0**，我们希望增加其概率。更新策略后，它应该被更频繁地选择。
- en: If an action is disadvantageous, ie. **A < 0**, we want to reduce its probability.
    *L* is negative, and we want it to be less negative so as to maximize the objective.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个动作是不利的，即**A < 0**，我们希望降低其概率。*L*是负的，我们希望它更不负，以最大化目标。
- en: A small KL divergence means that the probability of each action under the old
    and new policy remains close to each other. At the extreme, we have log(1) which
    makes the KL divergence zero.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小的KL散度意味着在旧政策和新政策下每个动作的概率保持接近。在极端情况下，我们有log(1)，这使得KL散度为零。
- en: Moving to PPO
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 过渡到PPO
- en: Solving the constrained optimization problem involves the backtracking line
    search (ie. repeated computations with smaller steps each time, until the constraint
    is satisfied) as well as computing the Hessian matrix which comprises second order
    partial derivatives. Can we simplify the process?
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 解决约束优化问题涉及回溯线搜索（即每次用较小的步长重复计算，直到满足约束）以及计算包含二阶偏导数的Hessian矩阵。我们可以简化这一过程吗？
- en: The first author of the TRPO paper in 2015 gave an improved model in 2017, also
    as first author. That is PPO, and it remains widely used even today (year 2023).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: TRPO论文的第一作者在2015年提出了改进模型，并在2017年作为第一作者发表了PPO。即便在今天（2023年），PPO仍然被广泛使用。
- en: Math of PPO
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PPO 的数学
- en: I will be showing less math here, so stay with me! There are different variants
    of PPO, and we will talk about the ‘clip’ version here, which is also the version
    used by sb3.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我将减少数学内容，所以请继续跟随我！有不同的PPO变体，我们将在这里讨论‘clip’版本，这也是sb3使用的版本。
- en: Instead of imposing a constraint for the KL divergence, the objective function
    is modified such that it does not benefit from large changes in the policy. We
    still try to increase the probability of an advantageous action (and reduce the
    probability of a disadvantageous action), but the effects of the probability ratio
    going much greater (or much lower) than 1 is bounded by the clipping.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 不通过对KL散度施加约束，而是修改目标函数，使其不受政策大幅变化的影响。我们仍然尝试增加有利动作的概率（并减少不利动作的概率），但概率比大幅高于（或低于）1的影响被裁剪所限制。
- en: A value of 0.2 is commonly chosen for ϵ. This value is also used in the original
    paper.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ϵ 通常选择0.2。这个值也在原始论文中使用。
- en: '![](../Images/bd3668c4b9feb027f1c03eb210bc7888.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd3668c4b9feb027f1c03eb210bc7888.png)'
- en: Objective of TRPO. Equation typed by author, with reference to [OpenAI Spinning
    Up](https://spinningup.openai.com/en/latest/algorithms/trpo.html).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: TRPO的目标。作者编写的方程，参考 [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/algorithms/trpo.html)。
- en: 'Figure 1 of the [paper](https://arxiv.org/pdf/1707.06347.pdf) (in page 3) shows
    that the factor of (1 + ϵ) or (1 — ϵ), depending on whether the advantage is positive
    or negative, both sets an *upper bound* on the objective function. This is better
    visualized with the following:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[论文](https://arxiv.org/pdf/1707.06347.pdf)的图1（第3页）显示了 (1 + ϵ) 或 (1 — ϵ) 的因子，根据优势是正还是负，二者都对目标函数设置了*上界*。下面的图示更直观：'
- en: '![](../Images/e70c6e67ad467fafbcafe5e795da6786.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e70c6e67ad467fafbcafe5e795da6786.png)'
- en: Simplified equation, depending on value of advantage A.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 简化方程，取决于优势A的值。
- en: Do not get confused by the ‘min’ operator. We want to optimize the objective
    *L* such that it is as high (positive) as possible. The ‘min’ operator is simply
    for (1 + ϵ) or (1 — ϵ) to take effect and form the ceiling.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 不要被‘min’运算符搞混。我们希望优化目标 *L* 使其尽可能高（积极）。‘min’运算符仅用于 (1 + ϵ) 或 (1 — ϵ) 的作用形成上限。
- en: Intuition of PPO
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PPO 的直观理解
- en: If **A > 0**, our objection function *L* is greater (more positive) when the
    probability ratio is *above* 1, ie. the new policy selects the *advantageous*
    action more often. However, there is an **upper bound** due to (1 + ϵ).
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果**A > 0**，当概率比高于1时，我们的目标函数 *L* 较大（更积极），即新策略更频繁地选择*有利*的动作。然而，由于 (1 + ϵ)，存在一个**上界**。
- en: If **A < 0**, our objection function *L* is greater (less negative) when the
    probability ratio is *below* 1, ie. the new policy selects the *disadvantageous*
    action more often. However, there is an **upper bound** due to (1 - ϵ).
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果**A < 0**，当概率比低于1时，我们的目标函数 *L* 较大（较少负值），即新策略更频繁地选择*不利*的动作。然而，由于 (1 - ϵ)，存在一个**上界**。
- en: The gradient update can easily be performed using the usual pytorch or tensorflow.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度更新可以使用常规的pytorch或tensorflow轻松完成。
- en: Code
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码
- en: Let’s first convince everyone that gradient update works, without any clipping
    involved.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们说服大家，梯度更新是有效的，不涉及任何裁剪。
- en: 1 Layer
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 层
- en: We set up a simple neural network with one layer, that takes in a state of dimension
    1, and outputs action probabilities between 3 possible choices.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建立了一个简单的单层神经网络，输入维度为1的状态，输出在3个可能选择之间的动作概率。
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You can have a multi-dimensional state, or even an image, and the concept remains
    — For some given state, the neural network gives you probabilities that sums to
    1.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以有多维状态，甚至是图像，概念依然有效——对于某个给定状态，神经网络给出总和为1的概率。
- en: Without Clip
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无裁剪
- en: Now, let’s prescribe a state. For this exercise, it can be any constant. In
    practice this state is obtained from observations of the environment and entirely
    independent of the parameters θ, ie. treated as a constant when computing the
    gradients.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们指定一个状态。对于本练习，它可以是任何常数。在实际操作中，这个状态是通过对环境的观察获得的，并且完全独立于参数θ，即在计算梯度时将其视为常数。
- en: The advantage can be computed using the generalized advantage estimation (see
    equation 11 of the same paper cited above, by Schulman *et al*. 2017 if interested).
    It combines information from multiple time horizons in an attempt to balance the
    trade-off between bias and variance, similar to TD(λ).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 优势可以通过广义优势估计（如有兴趣，请参见上述引用的Schulman *et al*. 2017年论文的公式11）来计算。它结合了来自多个时间区间的信息，试图平衡偏差和方差之间的权衡，类似于TD(λ)。
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For this exercise, we arbitrarily prescribe a negative advantage for the first
    action, simulating that the first action had been taken which resulted in a negative
    advantage. This is for the purpose of verifying whether the parameters are updated
    such that the probability of the first action decreases.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本练习，我们任意指定了第一行动的负面优势，模拟了第一次行动带来负面优势的情况。这是为了验证参数是否更新，使得第一行动的概率减少。
- en: '![](../Images/6dbfca798835a5183aa9aa2c19d4620a.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6dbfca798835a5183aa9aa2c19d4620a.png)'
- en: Output of the code cell above, where the first action leading to a negative
    advantage was taken
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 上面代码单元的输出，其中第一次行动导致负面收益
- en: Indeed, after the gradient update, the policy outputs a lower probability for
    the first action, and a higher probability for the other actions.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，在梯度更新后，策略输出了较低的第一行动概率和较高的其他行动概率。
- en: With Clip
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Clip
- en: Now, we will implement the clipping aspects. To first check for correctness,
    a single update will be performed. This time, suppose that the second action is
    taken, and the advantage is estimated to be 1.8.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将实现剪切方面。为了首先检查正确性，将进行一次更新。这次，假设第二行动被执行，估计的优势为1.8。
- en: Notice that a negative sign is added to the sum-product, to get the loss (upon
    which the back propagation will be performed). This is because we want to maximise
    the objective function, and are performing gradient descent.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在求和乘积时添加了负号，以得到损失（在此基础上将进行反向传播）。这是因为我们希望最大化目标函数，并进行梯度下降。
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/ea85c186ec8afe6ae32ff0a4901a4737.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea85c186ec8afe6ae32ff0a4901a4737.png)'
- en: Output of the code cell above, where the second action leading to a positive
    advantage was taken
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 上面代码单元的输出，其中第二行动导致了正面收益
- en: So far, so good. When an action leading to a positive advantage was taken, updating
    the policy increases the probability of that action. Now, we will consider the
    net effect of many iterations, where each of the different actions are taken multiple
    times.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切顺利。当采取一种有利的行动时，更新策略会增加该行动的概率。现在，我们将考虑许多迭代的净效果，其中每种不同的行动都被多次执行。
- en: You would say that in reality, the advantage are noisy, and using a constant
    is unfair. And you are right! Let’s add a random noise, via `np.random.randn()`.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会说，在现实中，优势是有噪声的，使用常数是不公平的。你说得对！让我们通过`np.random.randn()`添加随机噪声。
- en: '[PRE3]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: After 500 iterations, with noise included, we have the following action probability
    under the given state. (Of course, we can easily extend this environment to consider
    different states.)
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 经过500次迭代，包括噪声，我们在给定状态下得到了以下行动概率。（当然，我们可以轻松扩展这个环境以考虑不同的状态。）
- en: '![](../Images/82de71f620ef213fdbb4e6181b5b4db7.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/82de71f620ef213fdbb4e6181b5b4db7.png)'
- en: Output of the code cell above, where actions are taken repeatedly, with noise
    added
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 上面代码单元的输出，其中多次执行行动，并添加了噪声
- en: We see that the net effect of all updates lead to the probability of the second
    action increasing, while the probabilities of the other actions decrease. (We
    started with [0.2149, 0.5257, 0.2594]).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，所有更新的净效果使得第二种行动的概率增加，而其他行动的概率减少。（我们开始时的概率为[0.2149, 0.5257, 0.2594]。）
- en: This is in spite of the fact that the last action has a positive mean advantage.
    It is a result of the gradient being dependent on not just the sign, but the magnitude,
    of the advantage. Each time the last action is taken, the probability increases
    after an update, but this is offset by the effects of updates when the second
    action is taken.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管最后一个动作具有正的均值优势，这种情况仍然存在。这是因为梯度不仅依赖于优势的符号，还依赖于其大小。每次执行最后一个动作时，概率在更新后会增加，但在执行第二个动作时，更新的影响会抵消这一增加。
- en: Finally, let’s see the effect of clipping in action. Repeat the cells above
    (where `epsilon` ϵ was set to 0.2), but this time set it to a smaller value, say
    0.02.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看剪切的实际效果。重复上述单元（其中`epsilon` ϵ 设置为0.2），但这次将其设置为更小的值，比如0.02。
- en: '![](../Images/6b975c69e25774c33300e7544bdbfec9.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6b975c69e25774c33300e7544bdbfec9.png)'
- en: Output when the code is re-run with a smaller value of epsilon
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 以较小的epsilon值重新运行代码时的输出
- en: Excellent! We see that the clipping indeed works! By using a smaller ϵ, the
    policy has changed to a smaller extent, albeit in the same desired direction.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 很好！我们看到剪切确实有效！通过使用更小的ϵ，策略的变化幅度确实较小，尽管仍朝着期望的方向变化。
- en: Conclusion
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we looked at how PPO came about. The math is kept to just the
    bare essentials — enough for practitioners to present the ideas to other stakeholders
    without drowning in detailed derivations. We obtained the key intuitions from
    the math, to understand the principles of how PPO works.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们探讨了PPO的起源。数学部分仅保留了最基本的内容——足以让从业者向其他利益相关者展示这些思想，而不会被详细的推导所淹没。我们从数学中获得了关键的直觉，以理解PPO的工作原理。
- en: In addition, you now have the code to prove that the underlying components of
    PPO really delivers its promise. We see that the most advantageous actions has
    its probability increased, even with noise added. Furthermore, we see that by
    tightening the clipping, the resulting policy changes to a smaller extent.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你现在拥有了代码来证明PPO的基本组件确实实现了其承诺。我们看到，即使添加了噪声，最有利的动作的概率也有所增加。此外，我们发现，通过收紧剪切，得到的策略变化的幅度较小。
