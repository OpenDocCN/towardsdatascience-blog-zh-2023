- en: The carbon footprint of GPT-4
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT-4 的碳足迹
- en: 原文：[https://towardsdatascience.com/the-carbon-footprint-of-gpt-4-d6c676eb21ae](https://towardsdatascience.com/the-carbon-footprint-of-gpt-4-d6c676eb21ae)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-carbon-footprint-of-gpt-4-d6c676eb21ae](https://towardsdatascience.com/the-carbon-footprint-of-gpt-4-d6c676eb21ae)
- en: Recently leaked data allows us for the first time to estimate the carbon emissions
    from training OpenAI’s GPT-4
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最近泄露的数据使我们第一次能够估算训练 OpenAI 的 GPT-4 的碳排放
- en: '[](https://kaspergroesludvigsen.medium.com/?source=post_page-----d6c676eb21ae--------------------------------)[![Kasper
    Groes Albin Ludvigsen](../Images/3c31c9e54fae4fd1c8f1c441379d1f10.png)](https://kaspergroesludvigsen.medium.com/?source=post_page-----d6c676eb21ae--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d6c676eb21ae--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d6c676eb21ae--------------------------------)
    [Kasper Groes Albin Ludvigsen](https://kaspergroesludvigsen.medium.com/?source=post_page-----d6c676eb21ae--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://kaspergroesludvigsen.medium.com/?source=post_page-----d6c676eb21ae--------------------------------)[![Kasper
    Groes Albin Ludvigsen](../Images/3c31c9e54fae4fd1c8f1c441379d1f10.png)](https://kaspergroesludvigsen.medium.com/?source=post_page-----d6c676eb21ae--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d6c676eb21ae--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d6c676eb21ae--------------------------------)
    [Kasper Groes Albin Ludvigsen](https://kaspergroesludvigsen.medium.com/?source=post_page-----d6c676eb21ae--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d6c676eb21ae--------------------------------)
    ·7 min read·Jul 18, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d6c676eb21ae--------------------------------)
    ·阅读时间 7 分钟·2023年7月18日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a1cfe5cfdb78e054e76587a5d51276e3.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a1cfe5cfdb78e054e76587a5d51276e3.png)'
- en: Photo by Taylor Vick on Unsplash
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 Taylor Vick 提供，来源于 Unsplash
- en: With recent news alerting us that global average temperatures keep rising [1]
    it’s important to remind ourselves that most human activities have a carbon footprint
    that contributes towards global warming and other climate change. This is also
    true for digital technology in general and AI in particular. This article serves
    as a reminder of this, as it estimates the carbon emissions of training OpenAI’s
    large language model GPT-4.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随着最近的新闻警告我们全球平均温度持续上升 [1]，我们需要提醒自己，大多数人类活动都有碳足迹，这对全球变暖和其他气候变化有贡献。这对于数字技术尤其是
    AI 也是如此。本文作为一个提醒，估算了训练 OpenAI 的大型语言模型 GPT-4 的碳排放。
- en: 'To make such estimates, we need to know:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行这些估算，我们需要了解：
- en: How much electricity was used to train GPT-4
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练 GPT-4 使用了多少电力
- en: The carbon intensity of the electricity, i.e. the carbon footprint of generating
    1 KWh of electricity
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 电力的碳强度，即生成 1 千瓦时电力的碳足迹
- en: Let’s dive right in.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直接进入主题。
- en: '[](https://kaspergroesludvigsen.medium.com/membership?source=post_page-----d6c676eb21ae--------------------------------)
    [## Join Medium with my referral link - Kasper Groes Albin Ludvigsen'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://kaspergroesludvigsen.medium.com/membership?source=post_page-----d6c676eb21ae--------------------------------)
    [## 使用我的推荐链接加入 Medium - Kasper Groes Albin Ludvigsen'
- en: As a Medium member, a portion of your membership fee goes to writers you read,
    and you get full access to every story…
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 作为 Medium 会员，您的一部分会员费用将分配给您阅读的作者，并且您可以完全访问所有故事……
- en: kaspergroesludvigsen.medium.com](https://kaspergroesludvigsen.medium.com/membership?source=post_page-----d6c676eb21ae--------------------------------)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[kaspergroesludvigsen.medium.com](https://kaspergroesludvigsen.medium.com/membership?source=post_page-----d6c676eb21ae--------------------------------)'
- en: The electricity consumption of GPT-4
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT-4 的电力消耗
- en: Let’s first estimate GPT-4's energy consumption. According to unverified information
    leaks, GPT-4 was trained on about 25,000 Nvidia A100 GPUs for 90–100 days [2].
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们估算 GPT-4 的能耗。根据未经验证的信息泄露，GPT-4 在大约 25,000 个 Nvidia A100 GPU 上训练了 90-100
    天 [2]。
- en: Let’s assume the GPUs were installed in Nvidia HGX servers which can host 8
    GPUs each, meaning 25,000 / 8 = 3,125 servers were needed.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 假设这些 GPU 安装在 Nvidia HGX 服务器中，每台服务器可以容纳 8 个 GPU，这意味着需要 25,000 / 8 = 3,125 台服务器。
- en: One way to estimate the electricity consumption from this information, is to
    consider the thermal design power (TDP) of an Nvidia HGX server. TDP, denoted
    in watts, expresses the power consumption of a piece of hardware under maximum
    *theoretical* load [11], ie the actual power consumption may differ.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 估算电力消耗的一种方法是考虑 Nvidia HGX 服务器的热设计功率 (TDP)。TDP 以瓦特为单位，表示硬件在最大*理论*负载下的功耗 [11]，即实际功耗可能有所不同。
- en: Unfortunately, Nvidia does not disclose this information, so let’s instead use
    the TDP of the similar Nvidia DGX server, which is 6.5 kW [3]. So, if an Nvidia
    DGX server runs at full power for 1 hour, it will have consumed 6.5 KWh according
    to the TDP.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，Nvidia 并没有披露这些信息，因此我们改为使用类似 Nvidia DGX 服务器的 TDP，6.5 kW [3]。因此，如果一台 Nvidia
    DGX 服务器以满功率运行 1 小时，它将根据 TDP 消耗 6.5 KWh。
- en: Recall that it’s estimated that it took 90–100 days to train GPT-4\. That’s
    90 or 100 * 24 = 2,160 to 2,600 hours per server. If we assume the servers ran
    at full power constantly, we can multiply the number of hours by 6.5 kW, and we
    get that during training, each server may have consumed 14,040 to 16,900 KWh of
    electricity.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，估计训练 GPT-4 需要 90–100 天。这是 90 或 100 * 24 = 2,160 到 2,600 小时每台服务器。如果我们假设服务器始终以满功率运行，我们可以将小时数乘以
    6.5 kW，因此在训练期间，每台服务器可能消耗了 14,040 到 16,900 KWh 的电力。
- en: 'Let’s multiply that by the 3,125 servers needed to host 25,000 GPU’S: 3,125
    * 14,040 to 16,900 KWh = 43,875,000 to 52,812,500 KWh.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将其乘以 3,125 台服务器所需的 25,000 GPU：3,125 * 14,040 到 16,900 KWh = 43,875,000 到 52,812,500
    KWh。
- en: When calculating the energy consumption of computer hardware, it is customary
    to multiply the energy consumption of the hardware by the so-called power usage
    effectiveness (PUE) of the data center in which the hardware runs (see e.g. [4]).
    PUE is a ratio that describes how efficiently a computer data center uses energy.
    Let’s assume GPT-4 was trained in a Microsoft Azure data center because of OpenAI’s
    partnership with Microsoft. Microsoft Azure data centers have an average PUE of
    1.18 [7], but note that this can vary between data centers.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算计算机硬件的能耗时，通常需要将硬件的能耗乘以硬件运行的数据中心的所谓能效比（PUE）（参见例如 [4]）。PUE 是一个描述计算机数据中心使用能源效率的比率。假设
    GPT-4 是在微软 Azure 数据中心进行训练的，因为 OpenAI 与微软有合作关系。微软 Azure 数据中心的平均 PUE 为 1.18 [7]，但请注意，这在不同数据中心之间可能有所不同。
- en: So, let’s multiply the 43,875,000 to 52,812,500 KWh hardware electricity consumption
    by 1.18\. That gives us 51,772,500 to 62,318,750 KWh. I.e. to train GPT-4 may
    have used between 51,772,500 and 62,318,750 KWh of electricity.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将 43,875,000 到 52,812,500 KWh 的硬件电力消耗乘以 1.18。这样我们得到 51,772,500 到 62,318,750
    KWh。即训练 GPT-4 可能使用了 51,772,500 到 62,318,750 KWh 的电力。
- en: This concludes our estimate of the energy consumption from training GPT-4\.
    Now let’s estimate the carbon footprint of training GPT-4.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们对 GPT-4 训练能耗的估计。现在让我们估算训练 GPT-4 的碳足迹。
- en: '[](/chatgpts-electricity-consumption-7873483feac4?source=post_page-----d6c676eb21ae--------------------------------)
    [## ChatGPT’s Electricity Consumption'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/chatgpts-electricity-consumption-7873483feac4?source=post_page-----d6c676eb21ae--------------------------------)
    [## ChatGPT 的电力消耗'
- en: ChatGPT may have consumed as much electricity as 175,000 people in January 2023.
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ChatGPT 可能消耗了相当于 2023 年 1 月 175,000 人的电力。
- en: towardsdatascience.com](/chatgpts-electricity-consumption-7873483feac4?source=post_page-----d6c676eb21ae--------------------------------)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/chatgpts-electricity-consumption-7873483feac4?source=post_page-----d6c676eb21ae--------------------------------)
- en: The carbon footprint of GPT-4
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT-4 的碳足迹
- en: Above, we estimated GPT-4's training electricity consumption to be between 51,772,500
    and 62,318,750 KWh.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 上述估计 GPT-4 的训练电力消耗在 51,772,500 和 62,318,750 KWh 之间。
- en: To convert that to its carbon footprint, we need to multiply by the carbon intensity
    of the electricity used to power the compute.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要将其转换为碳足迹，我们需要将其乘以用于提供计算电力的电力的碳强度。
- en: I want to stress that we don’t know how the power was generated, so we don’t
    know its carbon intensity, but we can make some assumptions.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我想强调的是，我们不知道电力的生成方式，因此不知道其碳强度，但我们可以做一些假设。
- en: Let’s assume GPT-4 was trained in an Azure data center because of OpenAI’s partnership
    with Microsoft. According to researchers, the lowest carbon intensity of an Azure
    data center in the US is of the one in California called West US [5]. This has
    a carbon footprint of 240.6 gCO2e/KWh meaning that generating 1 KWh of electricity
    in this region emits 240.6 grams CO2e on average.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 GPT-4 是在 Azure 数据中心进行训练的，因为 OpenAI 与微软有合作关系。根据研究人员的说法，美国最低碳强度的 Azure 数据中心位于加利福尼亚州的
    West US [5]。该数据中心的碳足迹为 240.6 gCO2e/KWh，这意味着在该地区生成 1 KWh 的电力平均会排放 240.6 克 CO2e。
- en: Thus, we can estimate the carbon footprint of training GPT-4 to be between 12,456
    and 14,994 metric tons CO2e if the model was trained on “normal” grid electricity
    in California.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果模型是在加利福尼亚州的“普通”电网电力上进行训练的，我们可以估算 GPT-4 的碳足迹在 12,456 和 14,994 公吨 CO2e 之间。
- en: If OpenAI trained the GPT-4 in a data center in the Canada East region – which
    boasts a carbon intensity of just 20 gCO2e/KWh, the lowest of all Azure regions
    – the carbon footprint would be 1,035 to 1,246 metric tons CO2e. The difference
    is shown is Figure 1 below.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果OpenAI在加拿大东部地区的数据中心训练GPT-4，该地区的碳强度仅为20 gCO2e/KWh，是所有Azure地区中最低的，那么碳足迹将为1,035到1,246公吨CO2e。下图1显示了这一差异。
- en: '![](../Images/680050edb55000160dac44311d3a00c7.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/680050edb55000160dac44311d3a00c7.png)'
- en: 'Figure 1: If GPT-4 was trained in the Azure cloud region Canada East its training
    carbon footprint would have been smaller by a factor of 13.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：如果GPT-4是在Azure云区域加拿大东部进行训练的，其训练碳足迹将比原来小13倍。
- en: Discussion
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 讨论
- en: This article estimates that training GPT-4 may have emitted upwards of 15 metric
    tons CO2e. That’s the same as the annual emissions of 938 Americans [8]. Or 0.0000375
    % of global emissions assuming global annual emissions of 40 billion tons [9].
    That may not be a lot, but it pales in comparison to the hardware manufacturing
    emissions and the carbon footprint from serving a model like that to a large user
    base. This is something I’ve written about in my articles [*Environmental impact
    of ubiquitous generative AI*](/environmental-impact-of-ubiquitous-generative-ai-9e061bac6800)
    and [*ChatGPT’s electricity consumption*](/chatgpts-electricity-consumption-7873483feac4)*.*
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 本文估计，训练GPT-4可能排放了超过15公吨的CO2e。这相当于938名美国人的年排放量[8]。或者占全球排放量的0.0000375%，假设全球年排放量为400亿吨[9]。虽然这可能不算多，但与硬件制造排放和为大规模用户群体服务模型的碳足迹相比，就显得微不足道了。这一点我在我的文章中有提到过，[*无处不在的生成性AI的环境影响*](/environmental-impact-of-ubiquitous-generative-ai-9e061bac6800)
    和 [*ChatGPT的电力消耗*](/chatgpts-electricity-consumption-7873483feac4)*.*
- en: Above, I estimated GPT-4's electricity consumption to be between 51,772,500
    and 62,318,750 KWh. For comparison, it’s estimated that training GPT-3 consumed
    1,287,000 KWh [6] as seen in Figure 2\. So, if the estimates made here are in
    the right ballpark, the electricity consumption from training GPT-4 may be around
    40–48 times higher than the electricity needed to train GPT-3 although GPT-4’s
    total parameter count is said to be roughly 10 times that of GPT-3\. Obviously,
    the electricity required to train a model depends on many other factors than the
    model’s parameter count, but this serves as a reminder that we can’t necessarily
    say anything about the energy consumption of training some model based on knowledge
    of the energy consumption of training similar models.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 上述内容中，我估计GPT-4的电力消耗在51,772,500到62,318,750 KWh之间。相比之下，估计训练GPT-3的电力消耗为1,287,000
    KWh[6]，如图2所示。因此，如果这里的估算是正确的，那么训练GPT-4所需的电力可能是训练GPT-3所需电力的40至48倍，尽管GPT-4的总参数数量大约是GPT-3的10倍。显然，训练模型所需的电力还受到许多其他因素的影响，而不仅仅是模型的参数数量，但这提醒我们，不能仅凭类似模型的能耗来推测其他模型的能耗。
- en: '![](../Images/b0929a883697b30047c51422d29697d4.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b0929a883697b30047c51422d29697d4.png)'
- en: 'Figure 2: Comparing the estimated electricity consumption from training GPT-3
    and GPT-4'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：比较训练GPT-3和GPT-4的估计电力消耗
- en: 'The results shown above showed that carbon emissions could differ by a factor
    13 between Azure cloud regions. This clearly shows the huge environmental benefit
    of training your models in regions with greener energy — something I’ve also touched
    upon here:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 上述结果表明，Azure云区域之间的碳排放可能相差13倍。这清楚地显示了在绿色能源地区训练模型的巨大环境效益 —— 这是我在这里也提到的：
- en: '[](/how-to-estimate-and-reduce-the-carbon-footprint-of-machine-learning-models-49f24510880?source=post_page-----d6c676eb21ae--------------------------------)
    [## How to estimate and reduce the carbon footprint of machine learning models'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/how-to-estimate-and-reduce-the-carbon-footprint-of-machine-learning-models-49f24510880?source=post_page-----d6c676eb21ae--------------------------------)
    [## 如何估算和减少机器学习模型的碳足迹]'
- en: Two ways to easily estimate the carbon footprint of machine learning models
    and 17 ideas for how you might reduce it
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 有两种简单的方法来估算机器学习模型的碳足迹，并提供了17个减少碳足迹的想法。
- en: towardsdatascience.com](/how-to-estimate-and-reduce-the-carbon-footprint-of-machine-learning-models-49f24510880?source=post_page-----d6c676eb21ae--------------------------------)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/how-to-estimate-and-reduce-the-carbon-footprint-of-machine-learning-models-49f24510880?source=post_page-----d6c676eb21ae--------------------------------)'
- en: Furthermore, please note that the results obtained here are based on unverified
    information about the number of GPUs used to train GPT-4\. For this reason — and
    because assumptions e.g. regarding PUE, the hardware utilization rate and carbon
    intensity are made — the results obtained here should be considered educated guesswork.
    If the unverified information turns out to be true, I believe the numbers presented
    here are in the right ballpark, but please challenge my assumptions.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意，这里获得的结果基于关于用于训练 GPT-4 的 GPU 数量的未验证信息。因此——以及因为对 PUE、硬件利用率和碳强度等假设——这里获得的结果应被视为有根据的猜测。如果这些未验证的信息被证实为真实，我相信这里提供的数字大致是正确的，但请质疑我的假设。
- en: Conclusion
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: This article estimates that training GPT-4 consumed between 51,772,500 and 62,318,750
    KWh of electricity and emitted 12,456 and 14,994 metric tons CO2e if trained in
    California and 1,035 to 1,246 metric tons CO2e if trained in eastern Canada.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 本文估算训练 GPT-4 消耗了 51,772,500 到 62,318,750 千瓦时的电力，并排放了 12,456 到 14,994 公吨 CO2e（如果在加利福尼亚州训练）和
    1,035 到 1,246 公吨 CO2e（如果在加拿大东部训练）。
- en: Although these numbers may seem small, they pale in comparison to the environmental
    impact from other stages of an AI model’s life cycle, e.g. the deployment stage.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些数字可能看起来很小，但与 AI 模型生命周期其他阶段的环境影响（例如部署阶段）相比，它们显得微不足道。
- en: The carbon footprint estimates presented here can be used by organizations or
    individuals interested in calculating the total global carbon footprint of AI.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这里提供的碳足迹估算可以供有兴趣计算 AI 全球总碳足迹的组织或个人使用。
- en: Another interesting finding is that the estimates clearly show the environmental
    benefit of taking into consideration the carbon intensity of the electricity of
    the cloud region in which models are trained. When comparing the electricity consumption
    of GPT-4 to GPT-3's electricity, we also see that the difference in their electricity
    consumption is much larger than their difference in size.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的发现是，估算结果清楚地显示了考虑到模型训练云区域电力碳强度的环境效益。在比较 GPT-4 和 GPT-3 的电力消耗时，我们还发现它们的电力消耗差异远大于它们的大小差异。
- en: '[](/environmental-impact-of-ubiquitous-generative-ai-9e061bac6800?source=post_page-----d6c676eb21ae--------------------------------)
    [## Environmental impact of ubiquitous generative AI'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/environmental-impact-of-ubiquitous-generative-ai-9e061bac6800?source=post_page-----d6c676eb21ae--------------------------------)
    [## 生成式 AI 的普遍环境影响'
- en: What could happen to our environment if billions of people began to use generative
    AI technology on a daily basis?
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如果数十亿人开始每天使用生成式 AI 技术，我们的环境可能会发生什么变化？
- en: towardsdatascience.com](/environmental-impact-of-ubiquitous-generative-ai-9e061bac6800?source=post_page-----d6c676eb21ae--------------------------------)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/environmental-impact-of-ubiquitous-generative-ai-9e061bac6800?source=post_page-----d6c676eb21ae--------------------------------)'
- en: That’s it! I hope you enjoyed the story. Let me know what you think!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些！希望你喜欢这个故事。让我知道你的想法！
- en: Get the benefits of Medium and support my writing by signing up for a Medium
    membership [HERE](https://kaspergroesludvigsen.medium.com/membership).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 通过注册 Medium 会员[此处](https://kaspergroesludvigsen.medium.com/membership)来获得 Medium
    的福利并支持我的写作。
- en: Follow me for more on AI and sustainability and [subscribe](https://kaspergroesludvigsen.medium.com/subscribe)
    to get my stories via email when I publish.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 关注我以获取更多有关 AI 和可持续性的内容，并[订阅](https://kaspergroesludvigsen.medium.com/subscribe)以在我发布时通过电子邮件接收我的故事。
- en: I also sometimes write about [time series forecasting](/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我有时也会写有关[时间序列预测](/how-to-make-a-pytorch-transformer-for-time-series-forecasting-69e073d4061e)的文章。
- en: And feel free to connect on [LinkedIn](https://www.linkedin.com/in/kaspergroesludvigsen).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以在[LinkedIn](https://www.linkedin.com/in/kaspergroesludvigsen)上随时联系我。
- en: References
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] [https://edition.cnn.com/2023/07/05/world/hottest-day-world-climate-el-nino-intl/index.html](https://edition.cnn.com/2023/07/05/world/hottest-day-world-climate-el-nino-intl/index.html)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [https://edition.cnn.com/2023/07/05/world/hottest-day-world-climate-el-nino-intl/index.html](https://edition.cnn.com/2023/07/05/world/hottest-day-world-climate-el-nino-intl/index.html)'
- en: '[2] [https://archive.md/2RQ8X](https://archive.md/2RQ8X)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [https://archive.md/2RQ8X](https://archive.md/2RQ8X)'
- en: '[3] [https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-dgx-a100-datasheet.pdf](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-dgx-a100-datasheet.pdf)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-dgx-a100-datasheet.pdf](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-dgx-a100-datasheet.pdf)'
- en: '[4] [https://arxiv.org/abs/2211.02001](https://arxiv.org/abs/2211.02001)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [https://arxiv.org/abs/2211.02001](https://arxiv.org/abs/2211.02001)'
- en: '[5] [https://github.com/mlco2/impact/blob/master/data/impact.csv](https://github.com/mlco2/impact/blob/master/data/impact.csv)
    – MIT license – “Permission is hereby granted, free of charge, to any person”
    [10]'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] [https://github.com/mlco2/impact/blob/master/data/impact.csv](https://github.com/mlco2/impact/blob/master/data/impact.csv)
    – MIT 许可证 – “在此特此免费授予任何个人” [10]'
- en: '[6] [https://arxiv.org/ftp/arxiv/papers/2204/2204.05149.pdf](https://arxiv.org/ftp/arxiv/papers/2204/2204.05149.pdf)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] [https://arxiv.org/ftp/arxiv/papers/2204/2204.05149.pdf](https://arxiv.org/ftp/arxiv/papers/2204/2204.05149.pdf)'
- en: '[7] [https://azure.microsoft.com/en-us/blog/how-microsoft-measures-datacenter-water-and-energy-use-to-improve-azure-cloud-sustainability/](https://azure.microsoft.com/en-us/blog/how-microsoft-measures-datacenter-water-and-energy-use-to-improve-azure-cloud-sustainability/)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] [https://azure.microsoft.com/en-us/blog/how-microsoft-measures-datacenter-water-and-energy-use-to-improve-azure-cloud-sustainability/](https://azure.microsoft.com/en-us/blog/how-microsoft-measures-datacenter-water-and-energy-use-to-improve-azure-cloud-sustainability/)'
- en: '[8] https://www.forbes.com/sites/robtoews/2020/06/17/deep-learnings-climate-change-problem/'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] https://www.forbes.com/sites/robtoews/2020/06/17/deep-learnings-climate-change-problem/'
- en: '[9] https://www.iea.org/reports/co2-emissions-in-2022'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] https://www.iea.org/reports/co2-emissions-in-2022'
- en: '[10] https://github.com/mlco2/impact/blob/master/LICENSE'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] https://github.com/mlco2/impact/blob/master/LICENSE'
- en: '[11] https://www.intel.com/content/www/us/en/support/articles/000055611/processors.html'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] https://www.intel.com/content/www/us/en/support/articles/000055611/processors.html'
