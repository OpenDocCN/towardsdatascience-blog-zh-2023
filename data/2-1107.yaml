- en: How to Automate PySpark Pipelines on AWS EMR With Airflow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何在 AWS EMR 上使用 Airflow 自动化 PySpark 管道
- en: 原文：[https://towardsdatascience.com/how-to-automate-pyspark-pipelines-on-aws-emr-with-airflow-a0cbd94c4516](https://towardsdatascience.com/how-to-automate-pyspark-pipelines-on-aws-emr-with-airflow-a0cbd94c4516)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-automate-pyspark-pipelines-on-aws-emr-with-airflow-a0cbd94c4516](https://towardsdatascience.com/how-to-automate-pyspark-pipelines-on-aws-emr-with-airflow-a0cbd94c4516)
- en: Optimising big data workflows orchestration.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化大数据工作流的编排。
- en: '[](https://anbento4.medium.com/?source=post_page-----a0cbd94c4516--------------------------------)[![Antonello
    Benedetto](../Images/bf802bb46dce03dd3bd4e17c1dffe5b7.png)](https://anbento4.medium.com/?source=post_page-----a0cbd94c4516--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a0cbd94c4516--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a0cbd94c4516--------------------------------)
    [Antonello Benedetto](https://anbento4.medium.com/?source=post_page-----a0cbd94c4516--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://anbento4.medium.com/?source=post_page-----a0cbd94c4516--------------------------------)[![Antonello
    Benedetto](../Images/bf802bb46dce03dd3bd4e17c1dffe5b7.png)](https://anbento4.medium.com/?source=post_page-----a0cbd94c4516--------------------------------)[](https://towardsdatascience.com/?source=post_page-----a0cbd94c4516--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----a0cbd94c4516--------------------------------)
    [Antonello Benedetto](https://anbento4.medium.com/?source=post_page-----a0cbd94c4516--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a0cbd94c4516--------------------------------)
    ·8 min read·Aug 23, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----a0cbd94c4516--------------------------------)
    ·8分钟阅读·2023年8月23日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/8d92446cdf5a73adbbd00d940c246dee.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8d92446cdf5a73adbbd00d940c246dee.png)'
- en: Photo By [Tom Fisk](https://www.pexels.com/photo/trees-near-ocean-2246950/)
    On [Pexels](https://www.pexels.com/@tomfisk/)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Tom Fisk](https://www.pexels.com/photo/trees-near-ocean-2246950/) 提供，来自
    [Pexels](https://www.pexels.com/@tomfisk/)
- en: On-Demand Courses | Recommended
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 按需课程 | 推荐
- en: '*A few of my readers have contacted me asking for on-demand courses to help
    you* ***BECOME*** *a solid* ***Data Engineer****. These are 3 great resources
    I would recommend:*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*我的一些读者联系过我，询问是否有按需课程来帮助你* ***成为*** *一名扎实的* ***数据工程师***。这些是我推荐的 3 个优秀资源：'
- en: '[**Data Engineering Nano-Degree (UDACITY)**](https://imp.i115008.net/zaX10r)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**数据工程微学位 (UDACITY)**](https://imp.i115008.net/zaX10r)'
- en: '[**Data Streaming With Apache Kafka & Apache Spark Nano-Degree** **(UDACITY)**](https://imp.i115008.net/zaX10r)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**使用 Apache Kafka 和 Apache Spark 的数据流处理微学位** **(UDACITY)**](https://imp.i115008.net/zaX10r)'
- en: '[**Spark And Python For Big Data With PySpark (UDEMY)**](https://click.linksynergy.com/deeplink?id=533LxfDBSaM&mid=39197&murl=https%3A%2F%2Fwww.udemy.com%2Fcourse%2Fspark-and-python-for-big-data-with-pyspark%2F)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Spark 和 Python 大数据与 PySpark (UDEMY)**](https://click.linksynergy.com/deeplink?id=533LxfDBSaM&mid=39197&murl=https%3A%2F%2Fwww.udemy.com%2Fcourse%2Fspark-and-python-for-big-data-with-pyspark%2F)'
- en: '***Not a Medium member yet?*** *Consider signing up with my* [*referral link*](https://anbento4.medium.com/membership)
    *to gain access to everything Medium has to offer for as little as $5 a month!*'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '***还不是 Medium 会员？*** *考虑使用我的* [*推荐链接*](https://anbento4.medium.com/membership)
    *注册，以每月仅需 $5 的费用访问 Medium 的所有内容！*'
- en: '**Introduction**'
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**简介**'
- en: In the dynamic landscape of data engineering and analytics, building scalable
    and automated pipelines is paramount.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据工程和分析的动态领域中，构建可扩展和自动化的管道至关重要。
- en: 'Spark enthusiasts who have been working with Airflow for a while might be wondering:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对于已经在使用 Airflow 的 Spark 爱好者来说，可能会好奇：
- en: How to execute a Spark job on a remote cluster using Airflow?
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如何使用 Airflow 在远程集群上执行 Spark 作业？
- en: ''
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: How to automate Spark pipelines with AWS EMR and Airflow?
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如何使用 AWS EMR 和 Airflow 自动化 Spark 管道？
- en: 'In this tutorial we are going to integrate these two technologies by showing
    how to:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将通过展示如何集成这两种技术来进行演示：
- en: Configure and fetch essential parameters from the Airflow UI.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置并从 Airflow UI 中获取必要的参数。
- en: Create auxiliary functions to automatically generate the preferred `spark-submit`
    command.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建辅助函数以自动生成首选的 `spark-submit` 命令。
- en: Use Airflow’s `EmrAddStepsOperator()` method to build a task that submits and
    executes a PySpark job to EMR
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Airflow 的 `EmrAddStepsOperator()` 方法构建一个任务，该任务提交并执行一个 PySpark 作业到 EMR。
- en: Use Airflow’s `EmrStepSensor()` method to monitor the script execution.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Airflow 的 `EmrStepSensor()` 方法监控脚本执行。
- en: The code used in this tutorial is available on [GitHub.](https://github.com/anbento0490/projects/tree/main/airflow_emr_spark)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程中使用的代码可在 [GitHub](https://github.com/anbento0490/projects/tree/main/airflow_emr_spark)
    上获取。
- en: Prerequisites
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 先决条件
- en: 'An [AWS account](https://aws.amazon.com/) with a S3 bucket and EMR cluster
    configured **on the same region (** in this case `eu-north-1`**).** The EMR cluster
    should be available and in `WAITING` state. In our case it has been named `emr-cluster-tutorial`:'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 [AWS 账户](https://aws.amazon.com/) 配置了一个 S3 存储桶和 EMR 集群 **在同一区域（** 在这种情况下是
    `eu-north-1`**）。** EMR 集群应处于 `WAITING` 状态。在我们的案例中，它被命名为 `emr-cluster-tutorial`：
- en: '![](../Images/fa41a5326a1368fdd721eab8322a0987.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa41a5326a1368fdd721eab8322a0987.png)'
- en: Photo By the Author (Personal EMR Cluster)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的照片（个人 EMR 集群）
- en: Some mock `balances` data already available in the `S3` bucket under the `src/balances`
    folder. Data can be generated and written to the location using the [data producer](https://github.com/anbento0490/projects/blob/main/airflow_emr_spark/dags/assets/data_producer.ipynb)
    script.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些模拟 `balances` 数据已经在 `S3` 存储桶的 `src/balances` 文件夹中。数据可以使用 [数据生成器](https://github.com/anbento0490/projects/blob/main/airflow_emr_spark/dags/assets/data_producer.ipynb)
    脚本生成并写入该位置。
- en: The required `JARs` should already downloaded from Maven and available in the
    `S3` bucket.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所需的`JARs`应已经从 Maven 下载并保存在 `S3` 存储桶中。
- en: Docker installed and running on the local machine with 4-6 GB of allocated memory.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker 已安装并在本地计算机上运行，分配了 4-6 GB 的内存。
- en: Architecture
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构
- en: 'The goal is to write some mock data in `parquet` format to a `S3`a bucket and
    then build a `DAG` that:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是将一些模拟数据以 `parquet` 格式写入 `S3` 存储桶，然后构建一个 `DAG`，该 `DAG`：
- en: Fetches required configuration from Airflow UI;
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 Airflow UI 获取所需配置；
- en: Uploads a `pyspark` script to the same `S3`a bucket;
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 `pyspark` 脚本上传到同一个 `S3` 存储桶；
- en: Submits a spark job that executes the script just uploaded;
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提交一个 Spark 作业，执行刚刚上传的脚本；
- en: Monitors the execution state via a sensor until it succeeds or fails.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过传感器监控执行状态，直到成功或失败。
- en: '![](../Images/cb0937a803dbb8aab7666d9aba107508.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb0937a803dbb8aab7666d9aba107508.png)'
- en: Photo By The Author ([https://app.diagrams.net/](https://app.diagrams.net/))
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的照片 ([https://app.diagrams.net/](https://app.diagrams.net/))
- en: Configuration
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置
- en: Clone repository
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 克隆仓库
- en: 'The first step is to clone the [repository](https://github.com/anbento0490/projects/tree/main)
    in your desired local folder and display the folder structure:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是将 [仓库](https://github.com/anbento0490/projects/tree/main) 克隆到您选择的本地文件夹，并显示文件夹结构：
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `airflow_emr_spark` folder structure looks like this:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`airflow_emr_spark` 文件夹结构如下：'
- en: The `docker-compose.yml` and `Dockerfile.airflow` files are needed to run the
    Airflow service in local and then visualise / execute the `DAG`;
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`docker-compose.yml` 和 `Dockerfile.airflow` 文件是运行本地 Airflow 服务所需的，然后可视化/执行
    `DAG`；'
- en: The `orchestration` folder hosts the `DAG` itself named `submit_spark_job_to_emr.py`;
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`orchestration` 文件夹包含名为 `submit_spark_job_to_emr.py` 的 `DAG` 本身；'
- en: The `computation` folder includes the `.py` executable that will be submitted
    to the `EMR` cluster;
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`computation` 文件夹包含将提交到 `EMR` 集群的 `.py` 可执行文件；'
- en: Lastly, the `assets` folder includes a copy of the required configuration in
    `JSON` format as well as the `data_producer` notebook.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，`assets` 文件夹包括了所需配置的 `JSON` 格式副本以及 `data_producer` 笔记本。
- en: '![](../Images/576805ef306cc6b546746d8e8c2bee6c.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/576805ef306cc6b546746d8e8c2bee6c.png)'
- en: Folder Structure (Tree)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 文件夹结构（树状图）
- en: Run Airflow On Docker
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Docker 上运行 Airflow
- en: 'Let’s make sure `docker` is running in local, then while in `airflow_emr_spark`
    folder we execute `docker compose up -d`. This command will spin Apache Airflow
    in our container:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 确保本地运行 `docker`，然后在 `airflow_emr_spark` 文件夹中执行 `docker compose up -d`。此命令将在我们的容器中启动
    Apache Airflow：
- en: '![](../Images/b4fa8d3976b70f3852f763ee7307e9c9.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b4fa8d3976b70f3852f763ee7307e9c9.png)'
- en: Screenshot from Author’s Terminal
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 作者终端的截图
- en: 'Now we navigate to [**localhost:8091**](http://localhost:8091/home) and when
    requested for credential, we should simply type `admin` twice. Ad this point the
    `DAG` should be already parsed and look like this:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们导航到 [**localhost:8091**](http://localhost:8091/home)，当请求凭证时，我们只需输入 `admin`
    两次。此时，`DAG` 应该已经解析完毕，样子如下：
- en: '![](../Images/7ae030d73168254a63c6f1a7ff40d444.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ae030d73168254a63c6f1a7ff40d444.png)'
- en: From Airflow Running On Docker
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 从运行在 Docker 上的 Airflow
- en: 'Before exploring the `DAG` code, there are two remaining actions while in Airflow:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索 `DAG` 代码之前，在 Airflow 中还有两个剩余操作：
- en: Navigate to the `Variables` section and create a new variable named `dag_params`
    copying and pasting the content of the `dags/assets/dag_params.json` file. Of
    course, the `bucket_name` should be updated to match the `S3` bucket available
    in our AWS account.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导航到 `Variables` 部分，创建一个名为 `dag_params` 的新变量，将 `dags/assets/dag_params.json`
    文件的内容复制粘贴到其中。当然，`bucket_name` 应更新以匹配我们 AWS 账户中的 `S3` 存储桶。
- en: '![](../Images/85d32219e5d74342494bb1e9941f6eba.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/85d32219e5d74342494bb1e9941f6eba.png)'
- en: The **dag_params** Variable In AF UI
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: AF UI 中的 **dag_params** 变量
- en: 'Navigate to the `Connections` section and create a new `Amazon Web Services`
    connection named `aws_default` that will be used to interact with both `S3` and
    `EMR` cluster via Airflow. The connection parameters should be pasted into the
    `Extra` section in this form:'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导航到 `Connections` 部分，并创建一个名为 `aws_default` 的新 `Amazon Web Services` 连接，用于通过
    Airflow 与 `S3` 和 `EMR` 集群进行交互。连接参数应以如下形式粘贴到 `Extra` 部分：
- en: '[PRE1]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/15e40305b44c74a5a85d438ea97a35b1.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15e40305b44c74a5a85d438ea97a35b1.png)'
- en: The **aws_default** Connection In AF UI
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: AF UI 中的 **aws_default** 连接
- en: Tasks
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任务
- en: A preliminary step implemented as part of the `submit_spark_job_to_emr.py` DAG
    is to retrieve configuration from the `dag_params` variable previously saved in
    Airflow UI.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 初步步骤作为 `submit_spark_job_to_emr.py` DAG 的一部分是从 Airflow UI 中之前保存的 `dag_params`
    变量中检索配置。
- en: If a variable does not exist, the `DAG` will default to the `dag_params.json`
    available in the folder (`default_json`).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果变量不存在，`DAG` 将默认为文件夹中可用的 `dag_params.json`（`default_json`）。
- en: 'Since this tutorial also shows how to make specific `JARS` available to the
    `EMR` cluster, at the beginning we also define a `jar_string` variable to be submitted
    as an argument, later on, while adding a step. However, if no `JARS` are required,
    this variable (*as well* as `spark_jars_conf` *and* `spark_jars_conf_value`*in*
    `dag_params`) can be omitted:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本教程还展示了如何将特定的 `JARS` 提供给 `EMR` 集群，因此一开始我们还定义了一个 `jar_string` 变量以便在稍后添加步骤时作为参数提交。然而，如果不需要
    `JARS`，则可以省略此变量（*以及* `spark_jars_conf` *和* `spark_jars_conf_value` *在* `dag_params`
    中）：
- en: '![](../Images/2b0d75dca561f72f9bc037f4760af39e.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b0d75dca561f72f9bc037f4760af39e.png)'
- en: We should acknowledge that this is not an efficient way to import variables
    in Airflow.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该认识到，这不是在 Airflow 中导入变量的高效方法。
- en: This is because the `Variable.get()` method is called outside of a task, meaning
    that variables are imported *every time* the `airflow scheduler` parses the `dags`
    folder.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为 `Variable.get()` 方法在任务之外被调用，这意味着变量在每次 `airflow scheduler` 解析 `dags` 文件夹时都会被导入
    *每次*。
- en: 'However, for learning purposes, this is easier to implement and not a big deal
    when there is only one pipeline involved. However, in production, we should make
    sure to follow [these](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html#top-level-python-code)
    best practices instead, that in summary suggest to:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了学习目的，这种实现方式更简单，当只有一个管道涉及时并不算大问题。然而，在生产环境中，我们应确保遵循 [这些](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html#top-level-python-code)
    最佳实践，总结来说建议：
- en: '*Import variables as part of a task and only when needed*;'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*将变量作为任务的一部分并仅在需要时导入*；'
- en: '*Exchange variable among tasks using* `XCOMS`.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用* `XCOMS` *在任务之间交换变量*。'
- en: The `DAG` includes four tasks. Let’s describe what they are doing one by one.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`DAG` 包含四个任务。让我们逐一描述它们的功能。'
- en: Fetch Cluster ID
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取集群 ID
- en: 'The first task in order is `fetch_cluster_id`:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个任务是 `fetch_cluster_id`：
- en: '[PRE2]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'What this task is doing is to run the `fetch_cluster_id()` python function
    that:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这个任务的功能是运行 `fetch_cluster_id()` python 函数，该函数：
- en: fetches (*by name*) and returns the `CLUSTER_ID` (*required later on to add
    an EMR step*) and expects for the `cluster_state` to be `WAITING` or `RUNNING`.
    In our case, we passed `"emr-cluster-tutorial"` to the `get_cluster_id_by_name`
    method → in order to work well the `region_name` specified in the connection has
    to match with the region where the `EMR` cluster has been created.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按名称获取并返回 `CLUSTER_ID`（*后续添加 EMR 步骤所需*），并期望 `cluster_state` 为 `WAITING` 或 `RUNNING`。在我们的例子中，我们将
    `"emr-cluster-tutorial"` 传递给 `get_cluster_id_by_name` 方法 → 为了正常工作，连接中指定的 `region_name`
    必须与创建 `EMR` 集群的区域匹配。
- en: 'Displays the value of the imported variables for visibility (***suggestion**:
    *in production, this function should be modified to fetch all the attributes in
    the first place, to avoid inefficient top-level imports*).'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显示导入变量的值以便于查看（***建议**：*在生产中，此函数应修改为首先获取所有属性，以避免低效的顶级导入*）。
- en: '![](../Images/2f67301ca715bc5e062f1ac41ec9c873.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f67301ca715bc5e062f1ac41ec9c873.png)'
- en: Upload Script To S3
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 上传脚本到 S3
- en: 'We use the following task to upload the `read_and_write_back_to_s3.py` executable
    from the local `computation/src/python_scripts` folder to the the designated `s3://bucket_name/src/`
    folder, that will be then submitted to the `EMR` cluster:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下任务将 `read_and_write_back_to_s3.py` 可执行文件从本地 `computation/src/python_scripts`
    文件夹上传到指定的 `s3://bucket_name/src/` 文件夹，然后提交到 `EMR` 集群：
- en: '[PRE3]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The function `upload_script_to_s3` creates a `S3Hook` between Airflow and AWS
    using the `aws_default` connection and makes the script available at the preferred
    `S3` path:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 `upload_script_to_s3` 创建了一个 `S3Hook`，在 Airflow 和 AWS 之间使用 `aws_default` 连接，并使脚本在首选的
    `S3` 路径下可用：
- en: '![](../Images/378ac533bd4a22047e1ff124f78c4c71.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/378ac533bd4a22047e1ff124f78c4c71.png)'
- en: Execute PySpark Script
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行 PySpark 脚本
- en: Now that the python script is available into the `s3` bucket, it’s time to add
    a step to `EMR` using the `EmrAddStepsOperator()`.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 python 脚本已经存在于 `s3` 存储桶中，是时候使用 `EmrAddStepsOperator()` 向 `EMR` 添加一个步骤了。
- en: 'In fact, the `execute_pyspark_script` takes advantage of the auxiliary function
    below, to deploy a spark application to `EMR` in `client` mode (*however in production*
    `cluster` *mode* [*should be preferred*](https://stackoverflow.com/questions/41124428/spark-yarn-cluster-vs-client-how-to-choose-which-one-to-use)
    *instead*) together with a number of other (**optional* ) `spark_conf` parameters
    fetched from `dag_params`:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，`execute_pyspark_script` 利用下面的辅助函数，将一个 spark 应用程序以 `client` 模式部署到 `EMR`（*但在生产环境中*
    `cluster` *模式* [*应当更为推荐*](https://stackoverflow.com/questions/41124428/spark-yarn-cluster-vs-client-how-to-choose-which-one-to-use)
    *），同时使用从 `dag_params` 中获取的其他（**可选**）`spark_conf` 参数：
- en: '![](../Images/c118f04a777da9afce44131d98e05109.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c118f04a777da9afce44131d98e05109.png)'
- en: 'In practice, the `execute_pyspark_script` task instructs `EMR` to execute the
    `read_and_write_back_to_s3.py` script (*on the* `CLUSTER_ID` *fetched by the first
    task and pulled via* `XCOMS`) and also makes a number of parameters available
    to the cluster, in the order:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，`execute_pyspark_script` 任务指示 `EMR` 执行 `read_and_write_back_to_s3.py` 脚本（*在*
    `CLUSTER_ID` *通过第一个任务获取并通过* `XCOMS` *拉取*），并将多个参数提供给集群，顺序如下：
- en: Global parameters like a `jar_strings` or `packages` → need to be passed as
    arguments before the python script itself (*despite being optional*);
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全局参数如 `jar_strings` 或 `packages` → 需要作为参数传递在 python 脚本之前（*尽管是可选的*）；
- en: The function parameter `s3_input`→ specifies the input `balances` dataset `S3`
    path to be passed to the python script;
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数参数 `s3_input` → 指定了传递给 python 脚本的输入 `balances` 数据集 `S3` 路径；
- en: The function parameter `s3_output` → specifies the `S3` path where to store
    the output dataset.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数参数 `s3_output` → 指定了存储输出数据集的 `S3` 路径。
- en: '[PRE4]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Monitor Execution With Sensor
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用传感器监控执行
- en: Last but not least, the `execute_pyspark_script_sensor` task runs an `EmrStepSensor()`
    that periodically *pokes* the cluster itself to verify the health of the application
    and the state of the execution.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，`execute_pyspark_script_sensor` 任务运行一个 `EmrStepSensor()`，该传感器定期 *探测*
    集群本身，以验证应用程序的健康状况和执行状态。
- en: 'It specifically requires two parameters: `JOB_FLOW_ID == CLUSTER_ID` and `STEP_ID`
    that we can both fetch via `XCOMS`:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 它特别要求两个参数：`JOB_FLOW_ID == CLUSTER_ID` 和 `STEP_ID`，我们可以通过 `XCOMS` 获取这两个参数：
- en: '[PRE5]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Code
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码
- en: 'The python executable includes a simple `process_data_and_write_to_s3()` function
    that:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: python 可执行文件包括一个简单的 `process_data_and_write_to_s3()` 函数，该函数：
- en: reads data in `parquet` format from the `s3_input` path to a PySpark SQL view;
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 `s3_input` 路径读取 `parquet` 格式的数据到一个 PySpark SQL 视图中；
- en: filters the initial dataset, by taking advantage of the SQL API and saves the
    results to a `df_filtered` DF;
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用 SQL API 过滤初始数据集，并将结果保存到 `df_filtered` DF；
- en: eventually writes the filtered dataset (*again in* `parquet` *format*) to the
    `s3_output` path.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终将过滤后的数据集（*再次以* `parquet` *格式*）写入 `s3_output` 路径。
- en: '![](../Images/07b7b60f2b0750373b8ae1d852a7d89e.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07b7b60f2b0750373b8ae1d852a7d89e.png)'
- en: 'We can now trigger the `DAG` and wait until it is executed successfully:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以触发 `DAG` 并等待其成功执行：
- en: '![](../Images/1e6ea7f4f0a449da8829baec7e319586.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1e6ea7f4f0a449da8829baec7e319586.png)'
- en: Airflow DAG Graph
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow DAG 图
- en: 'Navigating to the `EMR` cluster `Steps` section, we should be able to verify
    that the script execution is indeed completed:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 导航到 `EMR` 集群的 `Steps` 部分，我们应该能够验证脚本执行确实已完成：
- en: '![](../Images/526d4ca4746a172fc2a744cf13aaf920.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/526d4ca4746a172fc2a744cf13aaf920.png)'
- en: AWS EMR **Steps** Section
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: AWS EMR **步骤** 部分
- en: 'In fact, by selecting the `stout` logs, we should visualise a message similar
    to the following, that tells us that a DF with `~2.5M` rows (*filtered from initial*
    `10M`) has been written to the the `tgt/balances/` folder:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，通过选择 `stout` 日志，我们应该能看到类似于以下的消息，它告诉我们一个包含 `~2.5M` 行（*从初始的* `10M` *中筛选出来*）的
    DF 已经被写入 `tgt/balances/` 文件夹中：
- en: '[PRE6]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Indeed, checking in the `bucket_name/tgt/balances/` folder, we should find
    one or more `parquet` files including data related to `Company_A` (*as specified
    in the filter*):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，检查 `bucket_name/tgt/balances/` 文件夹时，我们应该找到一个或多个包含与 `Company_A` 相关的数据的 `parquet`
    文件（*如筛选条件中指定的*）：
- en: '![](../Images/4e8798a2a8a45f1426bf90917afc18b2.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4e8798a2a8a45f1426bf90917afc18b2.png)'
- en: Content of **tgt/balances** folder
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**tgt/balances** 文件夹的内容'
- en: Conclusion
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this tutorial we learnt how automate PySpark pipelines using Apache Airflow
    and AWS EMR in combination.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们学习了如何使用 Apache Airflow 和 AWS EMR 结合来自动化 PySpark 流水线。
- en: The assumption here, is that **the EMR cluster is always on, in an idle state,
    waiting for a step to be added**. This is not uncommon in a professional environment
    or easily achievable by setting a personal AWS account.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的假设是**EMR 集群始终处于闲置状态，等待添加步骤**。在专业环境中这种情况并不罕见，或者通过设置个人 AWS 账户也能轻松实现。
- en: Another common practice is to fetch configuration directly from AF UI and use
    auxiliary functions to generate more articulated submit-commands that can be updated
    by editing configuration on demand.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的做法是直接从 AF UI 中获取配置，并使用辅助功能生成更复杂的提交命令，这些命令可以通过按需编辑配置来更新。
- en: Ours was a straightforward PySpark script, but we can of course imagine of expanding
    the example to cover much more complex use-cases.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的是一个简单的 PySpark 脚本，但我们当然可以设想扩展这个示例，以涵盖更复杂的使用案例。
- en: 'Once we have completed our work, let’s remember to stop Airflow service on
    docker with the following command:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成工作，请记得使用以下命令停止 docker 上的 Airflow 服务：
- en: '[PRE7]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Sources
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 来源
- en: '[Getting Started With Amazon EMR](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-gs.html)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Amazon EMR 入门](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-gs.html)'
- en: '[How To Submit spark Jobs To EMR Cluster From Airflow](https://www.startdataengineering.com/post/how-to-submit-spark-jobs-to-emr-cluster-from-airflow/)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[如何从 Airflow 提交 Spark 作业到 EMR 集群](https://www.startdataengineering.com/post/how-to-submit-spark-jobs-to-emr-cluster-from-airflow/)'
- en: '[Airflow Best Practices While Creating DAGs](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html#top-level-python-code)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[创建 DAG 时的 Airflow 最佳实践](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html#top-level-python-code)'
- en: '[Spark Yarn Cluster vs Client — How To Choose Which One To Use?](https://stackoverflow.com/questions/41124428/spark-yarn-cluster-vs-client-how-to-choose-which-one-to-use)'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Spark Yarn 集群与客户端 — 如何选择使用哪一个？](https://stackoverflow.com/questions/41124428/spark-yarn-cluster-vs-client-how-to-choose-which-one-to-use)'
