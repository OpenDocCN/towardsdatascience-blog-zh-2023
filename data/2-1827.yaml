- en: Segmenting Text Into Sentences Using NLP
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 NLP 将文本分割成句子
- en: 原文：[https://towardsdatascience.com/segmenting-text-into-sentences-using-nlp-35d8ef55c0fd](https://towardsdatascience.com/segmenting-text-into-sentences-using-nlp-35d8ef55c0fd)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/segmenting-text-into-sentences-using-nlp-35d8ef55c0fd](https://towardsdatascience.com/segmenting-text-into-sentences-using-nlp-35d8ef55c0fd)
- en: '**Feature engineering, statistical model, and learning from feedback**'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**特征工程、统计模型以及从反馈中学习**'
- en: '[](https://jagota-arun.medium.com/?source=post_page-----35d8ef55c0fd--------------------------------)[![Arun
    Jagota](../Images/3c3eb142f671b5fb933c2826d8ed78d9.png)](https://jagota-arun.medium.com/?source=post_page-----35d8ef55c0fd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----35d8ef55c0fd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----35d8ef55c0fd--------------------------------)
    [Arun Jagota](https://jagota-arun.medium.com/?source=post_page-----35d8ef55c0fd--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://jagota-arun.medium.com/?source=post_page-----35d8ef55c0fd--------------------------------)[![Arun
    Jagota](../Images/3c3eb142f671b5fb933c2826d8ed78d9.png)](https://jagota-arun.medium.com/?source=post_page-----35d8ef55c0fd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----35d8ef55c0fd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----35d8ef55c0fd--------------------------------)
    [Arun Jagota](https://jagota-arun.medium.com/?source=post_page-----35d8ef55c0fd--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----35d8ef55c0fd--------------------------------)
    ·10 min read·Jan 30, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----35d8ef55c0fd--------------------------------)
    ·10分钟阅读·2023年1月30日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/b2d3d765f44d8d3e011a1660ef32353e.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b2d3d765f44d8d3e011a1660ef32353e.png)'
- en: Image by [Nile](https://pixabay.com/users/nile-598962/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=623167)
    from [Pixabay](https://pixabay.com/)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Nile](https://pixabay.com/users/nile-598962/?utm_source=link-attribution&utm_medium=referral&utm_campaign=image&utm_content=623167)
    提供，来源于 [Pixabay](https://pixabay.com/)
- en: In NLP, segmenting a text document into its sentences is a useful basic operation.
    It is the first step in many NLP tasks that are more elaborate. Such as detecting
    and correcting errors in the text as it is being written [1], or detecting named
    entities [2].
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）中，将文本文档分割成句子是一个有用的基本操作。这是许多更复杂的NLP任务的第一步。例如，在写作过程中检测和纠正文本中的错误[1]，或者检测命名实体[2]。
- en: In the former, the idea is that common errors don’t cross sentence boundaries.
    This holds for the latter as well. Named entities also tend not to cross sentence
    boundaries.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在前者中，想法是常见错误不会跨越句子边界。这对于后者也是成立的。命名实体也通常不会跨越句子边界。
- en: In either case, this simplifies the problem considerably. For instance, training
    and evaluation can work off a corpus of sentences. Even sentences derived from
    longer documents can be treated independently.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何情况下，这都大大简化了问题。例如，训练和评估可以依赖于一个句子语料库。即使是从更长的文档中提取的句子也可以独立处理。
- en: In this post, we will cover the problem of segmenting text into sentences. We
    will take a “Socratic method” style approach. By this, we mean that we will iteratively
    build up a heuristic predictor from scratch in a sort of “iterative hypothesis
    refinement” style. With suitable speculations and questions driving refinements.
    ChatGPT will be quite helpful during this process.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将讨论将文本分割成句子的问题。我们将采用一种“苏格拉底式方法”的风格。通过这种方法，我们将从头开始迭代地构建一个启发式预测器，采用一种“迭代假设精炼”的风格。适当的猜测和问题推动精炼。在这个过程中，ChatGPT将非常有帮助。
- en: The particular approach we will end up with is similar in spirit to the popular
    Punkt algorithm [5] for this problem.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终采用的特定方法在精神上类似于流行的 Punkt 算法 [5]。
- en: Let’s get started.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: First, let’s be clear upfront that we will start from the raw text. No preprocessing
    such as tokenizing into words, downcasing words, removing stop words, whatever.
    All these risk losing some signal for this prediction task.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们明确一点：我们将从原始文本开始。没有诸如分词、转小写、去除停用词等预处理。这些操作可能会丢失一些对预测任务有用的信号。
- en: We all know that most sentences end at periods. So our simplest predictor is
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们都知道，大多数句子以句号结束。因此，我们最简单的预测器是
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The obvious question is, “where does this fail”? Well, for one, the sentence
    could end in a question mark. Or in an exclamation point. Ok, let’s abstract this
    rule out a bit.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 显而易见的问题是，“这在哪些方面会失败”？例如，句子可能以问号结束。或者以感叹号结束。好吧，让我们稍微抽象一下这个规则。
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We’ll set boundary_marker to {*period* (.), *question-mark* (?), *exclamation
    point* (!)}.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将boundary_marker设置为{*句号* (.), *问号* (?), *感叹号* (!) }。
- en: Okay, so now the same question, “where does **this** fail”? Consider Ph.D. We
    wouldn’t want the first period to be deemed a sentence boundary.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，那么现在同样的问题是，“**这**在哪儿失败”？考虑“Ph.D.”。我们不希望第一个句号被视为句子边界。
- en: Intuitively, if the period is within a word, we are not at a sentence boundary.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 直观上，如果句号在一个单词中，我们就不是在句子边界。
- en: So the following rule should work better.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 所以下面的规则应该会更有效。
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Okay, so this filters out false positives involving periods within words and
    does not seem to lose any recall.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，这样可以过滤掉涉及单词内句号的误报，并且似乎没有丢失任何召回率。
- en: While looking at the sentences in this post, we also notice that the first word
    in the sentence that follows is capitalized. Duh, that’s a basic grammar rule.
    Well, now it’s also a predictor of sentence boundary.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看本文的句子时，我们还注意到紧接着的句子中的第一个词是大写的。呃，这是一个基本的语法规则。好吧，现在它也是句子边界的预测指标。
- en: In view of the above, let’s refine our rule further.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于上述情况，让我们进一步优化规则。
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: where right_context is “white space followed by a capitalized word”.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 其中right_context是“空格后跟大写字母”。
- en: By adding “by a capitalized word” we have tightened the condition. How does
    this impact precision and recall?
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通过添加“大写字母”，我们收紧了条件。这如何影响精确度和召回率？
- en: Precision cannot have dropped. The recall could. Consider the following scenario.
    The text is short and informal and perhaps hastily written–such as in a text message.
    We may miss actual sentence boundaries because the first word in a new sentence
    may not be capitalized.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度不可能下降。召回率可能下降。考虑以下情况。文本短且非正式，也许是匆忙写成的——例如在短信中。我们可能会错过实际的句子边界，因为新句子的第一个词可能没有大写。
- en: Well, actually this makes us ponder whether we want a sentence boundary predictor
    for clean text or for not-necessarily clean text. It seems the complexity of the
    problem changes.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，这实际上让我们思考一下我们是否需要一个用于干净文本的句子边界预测器，还是用于不一定干净的文本。看来问题的复杂性发生了变化。
- en: Should we want to predict sentence boundaries in not-necessarily clean text,
    since the recall can drop, we need to ask if the precision increases when we tighten
    the condition.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想在不一定干净的文本中预测句子边界，因为召回率可能会下降，我们需要问一下，当我们收紧条件时，精确度是否提高。
- en: Consider this example which I managed to cajole out of ChatGPT when I asked
    it the question
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这个例子，我在询问ChatGPT时设法诱导出来的。
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: I am not certain that it understood the question, however, I did find an answer
    buried in its response I could use.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我不确定它是否理解了问题，但我确实在其回答中找到了一些我可以使用的答案。
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The condition “period followed by white space” would flag a sentence boundary
    immediately after *U.S* which is clearly wrong.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: “句号后跟空格”的条件会在*U.S*之后立即标记为句子边界，这显然是错误的。
- en: Okay, let’s keep going. Onto the next pondering. What if we added a left context
    to our condition? The new condition would look like
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我们继续前进。接下来思考一下。如果我们在条件中添加左侧上下文呢？新的条件将是
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: On not-necessarily-clean text, it might. We could drop “capitalized word” from
    the right context. This would help with recall but may reduce precision. We can
    try to increase precision by adding a left context.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在不一定干净的文本中，可能会这样。我们可以从右侧上下文中去掉“大写字母”。这将有助于召回，但可能会降低精确度。我们可以通过添加左侧上下文来尝试提高精确度。
- en: I asked ChatGPT this.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我问了ChatGPT这个问题。
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The first few lines in its response are below.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其回答的前几行如下。
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Okay, seems like it didn’t quite get my question, possibly because it wasn’t
    expressed clearly enough. Nonetheless, these answers are useful, even if very
    limited. Indeed in one-word sentences, these words are good left contexts.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，看来它没有完全理解我的问题，可能是因为表达得不够清楚。尽管如此，这些答案是有用的，即使非常有限。确实，在单词句子中，这些词是很好的左侧上下文。
- en: This also helped me refine my question to ChatGPT. First, I rephrased my question
    as
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这也帮助我完善了对ChatGPT的提问。首先，我将问题重新表述为
- en: '[PRE9]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: That didn’t help. Its response was still comprised of one-word sentences, albeit
    the actual words were good new candidates for being in the left context.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 那没有帮助。它的回答仍然由单词句子组成，尽管实际的单词是很好的新候选词，适合在左侧上下文中使用。
- en: The question below
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下问题
- en: '[PRE10]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: generated an interesting and possibly useful response, albeit not quite what
    I was looking for. So I won’t elaborate.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 生成了一个有趣且可能有用的回答，尽管不是我所寻找的。所以我不会详细阐述。
- en: The question below
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 以下问题
- en: '[PRE11]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: did lead to a more useful response.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 的确导致了更有用的回答。
- en: '[PRE12]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Nice response! From this, we can surmise that adding a left context can help.
    This response also gives the following tangible insights that move us forward.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 很好的响应！从中我们可以推测，添加左上下文可能有帮助。这个响应还提供了以下实际的见解，推动我们前进。
- en: Replace
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 替换
- en: '[PRE13]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: by a less rigid condition. Something like *left_context* features, *right_context*
    features, *boundary_marker*. The point is that left_context and right_context
    expressed in a condition may lead us into a combinatorial explosion. Features
    can be combined more softly for predictive purposes.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 通过较不严格的条件。例如*left_context*特征、*right_context*特征、*boundary_marker*。关键是left_context和right_context在条件中表示可能会导致组合爆炸。特征可以更柔和地结合以用于预测目的。
- en: This can be powerful especially if we have a way to leverage supervised learning,
    that is feedback. The parameters that govern how the collection of features gets
    used for predicting sentence boundaries then become learnable and may alleviate
    the need to introduce new features as new scenarios are encountered.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能很强大，特别是如果我们有办法利用监督学习，即反馈。支配特征集合如何用于预测句边界的参数就变得可学习，并可能减轻在遇到新场景时引入新特征的需要。
- en: Leveraging supervised learning in this setting is not as hard as one might imagine.
    We don’t need to construct a large training set of sentence boundary positives
    and sentence boundary negatives. Rather we could proceed as follows.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种设置中利用监督学习并不像人们想象的那么困难。我们不需要构建大量的句边界正例和句边界负例训练集。相反，我们可以按如下方式进行。
- en: Start with an initial condition, say
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从初始条件开始，例如
- en: '[PRE14]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: which has a high recall. Posit additional left_context and right_context features.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 具有高召回率。额外增加left_context和right_context特征。
- en: Run the initially-configured sentence boundary predictor on one or more sufficiently
    large documents and output pairs of adjacent predicted sentences in an easily
    visualizable and labelable format. Even a CSV file will be adequate. Inspect the
    CSV file and label any false positives you find as such. Even time-boxing this
    work will likely yield an improved sentence boundary predictor.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 运行最初配置的句边界预测器，在一个或多个足够大的文档上，并以易于可视化和标记的格式输出相邻预测句对。即使是CSV文件也足够了。检查CSV文件并将发现的任何假阳性标记出来。即使时间限定这个工作也可能会提升句边界预测器的性能。
- en: Sure, investing more time in labeling data would likely improve the predictive
    accuracy further. a rich labeled data set would likely yield even better results.
    This investment doesn’t need to be done upfront though. It could be incremental.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，投入更多时间进行标记数据可能会进一步提高预测准确性。一个丰富的标记数据集可能会产生更好的结果。不过，这项投资不需要提前完成。可以逐步进行。
- en: Next, let’s discuss specific sensible features to include. These are inspired
    by our investigations thus far. First, to our examples let’s add a few we found
    from [5], as these will help with the feature engineering.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们讨论要包含的具体合理特征。这些特征受到我们迄今为止的调查的启发。首先，我们的示例中添加一些我们从[5]找到的特征，因为这些特征将有助于特征工程。
- en: '[PRE15]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 1\. The number of words that precede the boundary_marker in the current sentence.
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 1\. 当前句子中在boundary_marker之前的单词数。
- en: ''
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 2\. If the boundary_marker is a period then the number of additional periods
    in the current token that precedes it.
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 2\. 如果boundary_marker是句号，则当前标记中在其之前的额外句号数量。
- en: ''
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 3\. The part-of-speech of the previous word.
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 3\. 上一个词的词性。
- en: ''
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 4\. The identity of the word that precedes the boundary_marker.
  id: totrans-75
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 4\. 边界标记符前面单词的身份。
- en: ''
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 5\. The length of the word that precedes the boundary_marker.
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 5\. 边界标记符前面单词的长度。
- en: ''
  id: totrans-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 6\. If the boundary_marker is a period then whether it is embedded within a
    named entity.
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 6\. 如果boundary_marker是句号，那么它是否嵌入在一个命名实体中。
- en: Feature 1) combined with feature 4) is obviously useful for one-word sentences.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 特征 1) 与特征 4) 结合显然对单词句子有用。
- en: Feature 2) is inspired by the example
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 特征 2) 受到示例的启发。
- en: '[PRE16]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: It could help predict that the second period in *U.S.* is not a sentence boundary.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以帮助预测*U.S.*中的第二个句号不是句边界。
- en: Feature 3) is inspired by ChatGPT’s response involving proper nouns.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 特征 3) 受到 ChatGPT 对专有名词的响应的启发。
- en: Feature 5) when combined with feature 2) may be useful in predicting that a
    boundary marker is in fact part of the preceding word, meaning it's a false positive.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 特征 5) 与特征 2) 结合时可能对预测边界标记实际上是前一个单词的一部分有用，也就是说这是一个假阳性。
- en: Feature 6) helps with this example.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 特征 6) 有助于这个示例。
- en: '[PRE17]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '**Supervised Statistical Algorithm**'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**监督统计算法**'
- en: Now we’ll describe what we think is an effective supervised algorithm of a statistical
    nature. Tuned to this problem. Relatively straightforward to implement.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将描述我们认为是一种有效的统计性质的监督算法。针对这个问题进行了调优。相对容易实现。
- en: The algorithm will assume we are at a character that is a boundary_marker. It
    will make a probabilistic inference which we will refer to as equation (1).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 算法将假设我们正处于一个`boundary_marker`字符上。它将进行一个概率推断，我们将其称为等式 (1)。
- en: '*P*(this boundary is real|boundary_marker, left context features, right context
    features)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(该边界真实|`boundary_marker`，左上下文特征，右上下文特征)'
- en: 'Here boundary_marker means the actual value of the marker: *period*, *question
    mark*, or *exclamation point*).'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的`boundary_marker`指的是标记的实际值：*句点*，*问号*，或*感叹号*。
- en: 'While we won’t develop this algorithm in full detail, we’d like to call out
    certain characteristics we have in mind:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们不会详细开发这个算法，但我们希望指出一些我们心中所想的特征：
- en: If we so deem, the contributions of certain features could be specified in advance
    and not allowed to change. For example, if the character that follows boundary_marker
    is not a space and we want to be sure this boundary_marker is predicted to be
    a false positive in this case, we can force eqn (1) to evaluate to zero always.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们认为合适，可以预先指定某些特征的贡献，并且不允许其发生变化。例如，如果紧随`boundary_marker`之后的字符不是空格，而我们希望确保在这种情况下`boundary_marker`被预测为假阳性，我们可以强制使等式
    (1) 始终评估为零。
- en: We do not necessarily want to assume that the features in eqn(1) are conditionally
    independent given the outcome — the boundary is real or not. For instance, the
    features could be connected up by a suitable probabilistic graphical model, whose
    structure is possibly set by the modeler for this particular use case. Or it could
    be in the spirit of a product-of-experts model in which the “experts” are particular
    values of features or combinations of features that strongly predict that the
    boundary marker is a false positive. See [4] for the product-of-experts model.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们不一定要假设等式 (1) 中的特征在结果—边界是否真实—给定的情况下是条件独立的。例如，这些特征可能通过一个合适的概率图模型连接起来，该模型的结构可能由模型设计者为特定用例设定。或者，它可以是一个专家组合模型，其中“专家”是特定的特征值或特征组合，它们强烈预测边界标记是假阳性。有关专家组合模型，请参见
    [4]。
- en: 'We can imagine a variant of eqn (1) that is not entirely probabilistic while
    remaining in the same spirit. Specifically, we can replace eqn(1) with two score
    functions: *score_true*(*boundary_marker*, *left context* features, *right context*
    features) and *score_false*(*boundary_marker*, *left context* features, *right
    context* features). In doing so, we have potentially doubled the number of learnable
    parameters in the model, which makes it richer.'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以想象一个等式 (1) 的变体，它不是完全概率性的，同时保持相同的精神。具体来说，我们可以用两个评分函数替换等式 (1)：*score_true*(`boundary_marker`，*左上下文*特征，*右上下文*特征)
    和 *score_false*(`boundary_marker`，*左上下文*特征，*右上下文*特征)。这样做，我们可能将模型中可学习的参数数量翻倍，从而使模型更为丰富。
- en: '**Learning Illustrated**'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习示例**'
- en: Now let’s illustrate learning in a concrete setting. We’ll go with a product-of-experts-like
    formulation as it both makes intuitive sense in our setting and the learning is
    also convenient to explain.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在具体的设置中说明学习过程。我们将采用类似专家组合的形式，因为它在我们的设置中直观有效，并且学习过程也易于解释。
- en: Say we want to learn that if the boundary marker is a period and the previous
    token has additional periods, then we should predict that this boundary marker
    is not real. For defensive purposes, let’s throw in the previous token’s length
    as an additional predictor.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要学习如果边界标记是句点而前一个标记有额外的句点，则我们应该预测这个边界标记不真实。出于防御目的，我们还将前一个标记的长度作为额外的预测因子。
- en: Our hope is that the second ‘.’ in *U.S.* gets flagged as a false positive boundary_marker.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望第二个‘.’在*美国*中被标记为假阳性`boundary_marker`。
- en: Let’s imagine that we formulate this expert as
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设想将这个专家表述为
- en: '*P*(*boundary_marker* is real| *boundary_marker* is ‘.’, *previous_token’s
    length*, *number of periods in the previous token*).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(`boundary_marker` 真实| `boundary_marker` 是 ‘.’，*前一个标记的长度*，*前一个标记中的句点数量*)。'
- en: This models a particular expert who weighs in only when the boundary_marker
    is a period. In this case, it uses the combination of the previous token’s length
    and the number of periods in it to decide whether to predict that the boundary_marker
    is real or not.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这模拟了一个特定的专家，当`boundary_marker`是句点时会参与其中。在这种情况下，它使用前一个标记的长度和其中句点的数量来决定是否预测`boundary_marker`为真实。
- en: Say initially we set the priors so that
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们最初设定先验为
- en: '*P*(*boundary_marker* is real|*boundary_marker* is ‘.’, *l*, *np*) is 1'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*boundary_marker* 为真实|*boundary_marker* 为‘.’, *l*, *np*) 为 1'
- en: for various sensible combinations of *l* and *np*. Say the initial algorithm
    predicts periods such as the second one in *U.S.* as sentence boundaries. Say
    we provide feedback that these are false positives. If we see enough such examples,
    we should be able to learn that
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 针对各种合理的 *l* 和 *np* 组合。假设初始算法将句号（例如 *U.S.* 中的第二个）预测为句子边界。假设我们提供反馈，指出这些是误报。如果我们看到足够多的这种例子，我们应该能够学习到
- en: '*P*(*boundary_marker* is real|*boundary_marker* is ‘.’, 3, 1) is near-zero'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*boundary_marker* 为真实|*boundary_marker* 为‘.’, 3, 1) 接近零'
- en: We chose *l*=3 and *np*=1 for our illustration because it applies to *U.S.*
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择 *l*=3 和 *np*=1 作为示例，因为它适用于 *U.S.*
- en: Next, imagine a different expert, modeled as
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，设想一个不同的专家，建模为
- en: '*P*(*boundary_marker* is real|the character that follows boundary_marker is
    not space)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*boundary_marker* 为真实|紧随 *boundary_marker* 之后的字符不是空格)'
- en: Again, we may set the priors for this to near 1 so initially, for example, the
    first period in *U.S.* gets predicted as a real sentence boundary. The user then
    provides feedback to label this as a false positive. This feedback can be used
    to drive
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以将先验设定为接近1，这样最初，例如，*U.S.* 中的第一个句号就会被预测为真实的句子边界。用户随后提供反馈，将其标记为假阳性。这些反馈可以用于推动
- en: '*P*(*boundary_marker* is real|the character that follows boundary_marker is
    not space)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*P*(*boundary_marker* 为真实|紧随 *boundary_marker* 之后的字符不是空格)'
- en: towards zero.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 逼近零。
- en: '**Summary**'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结**'
- en: In this post, we covered the problem of segmenting text into sentences. We approached
    this problem as follows. We iteratively built up a sentence boundary predictor
    by starting from the most basic rule and refining it from questions and answers.
    We asked what false positives or false negatives a particular rule would lead
    to. The answers helped us refine the rule. During this process, we solicited help
    from ChatGPT in the manner described earlier.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们讨论了将文本分割成句子的问题。我们以如下方式处理这个问题。我们通过从最基本的规则开始，迭代地构建句子边界预测器，并通过问答来完善它。我们询问某个规则可能导致的假阳性或假阴性。答案帮助我们完善规则。在此过程中，我们按照之前描述的方式请求
    ChatGPT 的帮助。
- en: Somewhere along the way, it became clear that we should pivot from hard rules
    to a feature-based approach. We then developed this approach under a particular
    probabilistic model, brought in specific features that our earlier investigations
    had suggested, framed this in a product-of-experts setting, and also discussed
    learning from feedback in this setting.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在某个阶段，我们意识到我们应该从硬规则转向基于特征的方法。我们在一个特定的概率模型下开发了这种方法，引入了我们早期调查中建议的特定特征，将其框定在专家产品的设置中，并讨论了在这种设置中从反馈中学习。
- en: '**References**'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: '[Text Correction Using NLP. Detecting and correcting common errors… | by Arun
    Jagota | Jan, 2023 | Towards Data Science](/text-correction-using-nlp-b68c7233b86)'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[使用 NLP 进行文本修正。检测和修正常见错误… | 作者 Arun Jagota | 2023年1月 | Towards Data Science](/text-correction-using-nlp-b68c7233b86)'
- en: '[Named Entity Recognition in NLP. Real-world use cases, models, methods…](/named-entity-recognition-in-nlp-be09139fa7b8)'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[NLP中的命名实体识别。真实世界的应用案例、模型、方法…](/named-entity-recognition-in-nlp-be09139fa7b8)'
- en: ChatGPT
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ChatGPT
- en: '[Geoff Hinton — Products of experts](https://www.cs.toronto.edu/~hinton/absps/icann-99.html)'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Geoff Hinton — 专家产品](https://www.cs.toronto.edu/~hinton/absps/icann-99.html)'
- en: '[nltk.tokenize.punkt](https://www.nltk.org/_modules/nltk/tokenize/punkt.html)'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[nltk.tokenize.punkt](https://www.nltk.org/_modules/nltk/tokenize/punkt.html)'
