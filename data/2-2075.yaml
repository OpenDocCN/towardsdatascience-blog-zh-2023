- en: 'The Ultimate Guide to Training BERT from Scratch: Prepare the Dataset'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从头开始训练BERT的终极指南：准备数据集
- en: 原文：[https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-prepare-the-dataset-beaae6febfd5](https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-prepare-the-dataset-beaae6febfd5)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-prepare-the-dataset-beaae6febfd5](https://towardsdatascience.com/the-ultimate-guide-to-training-bert-from-scratch-prepare-the-dataset-beaae6febfd5)
- en: 'Data Preparation: Dive Deeper, Optimize your Process, and Discover How to Attack
    the Most Crucial Step'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据准备：深入挖掘，优化你的过程，发现如何解决最关键的步骤
- en: '[](https://dpoulopoulos.medium.com/?source=post_page-----beaae6febfd5--------------------------------)[![Dimitris
    Poulopoulos](../Images/ce535a1679779f5a2ec8b024e6691e50.png)](https://dpoulopoulos.medium.com/?source=post_page-----beaae6febfd5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----beaae6febfd5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----beaae6febfd5--------------------------------)
    [Dimitris Poulopoulos](https://dpoulopoulos.medium.com/?source=post_page-----beaae6febfd5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://dpoulopoulos.medium.com/?source=post_page-----beaae6febfd5--------------------------------)[![Dimitris
    Poulopoulos](../Images/ce535a1679779f5a2ec8b024e6691e50.png)](https://dpoulopoulos.medium.com/?source=post_page-----beaae6febfd5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----beaae6febfd5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----beaae6febfd5--------------------------------)
    [Dimitris Poulopoulos](https://dpoulopoulos.medium.com/?source=post_page-----beaae6febfd5--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----beaae6febfd5--------------------------------)
    ·13 min read·Sep 14, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----beaae6febfd5--------------------------------)
    ·阅读时间13分钟·2023年9月14日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/34a45dbd982191c4d64c0848ec5fbae1.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/34a45dbd982191c4d64c0848ec5fbae1.png)'
- en: Photo by [Patrick Tomasso](https://unsplash.com/@impatrickt?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Patrick Tomasso](https://unsplash.com/@impatrickt?utm_source=medium&utm_medium=referral)拍摄，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '[Part I](/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f),
    [Part II](/the-ultimate-guide-to-training-bert-from-scratch-the-tokenizer-ddf30f124822),
    and [Part IV](/the-ultimate-guide-to-training-bert-from-scratch-final-act-eab78b0657bb)
    of this story are now live.'
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 本故事的[第一部分](/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f)、[第二部分](/the-ultimate-guide-to-training-bert-from-scratch-the-tokenizer-ddf30f124822)和[第四部分](/the-ultimate-guide-to-training-bert-from-scratch-final-act-eab78b0657bb)现已上线。
- en: 'Imagine investing a full day fine-tuning BERT, only to hit a performance bottleneck
    that leaves you scratching your head. You dig into your code and discover the
    culprit: you just didn’t do a good job preparing your features and labels. Just
    like that, ten hours of precious GPU time evaporates into thin air.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，花整整一天时间来微调BERT，却遇到性能瓶颈让你感到困惑。你深入研究代码，发现问题所在：你没有做好特征和标签的准备。就这样，十小时宝贵的GPU时间一瞬间化为乌有。
- en: 'Let’s face it, *setting up your dataset isn’t just another step — it’s the
    engineering cornerstone of your entire training pipeline*. Some even argue that
    once your dataset is in good shape, the rest is mostly boilerplate: feed your
    model, calculate the loss, perform backpropagation, and update the model weights.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 说实话，*设置数据集不仅仅是另一个步骤——它是整个训练流程的工程基石*。有人甚至认为，一旦你的数据集准备好，剩下的主要是模板化的：喂入模型、计算损失、进行反向传播和更新模型权重。
- en: '![](../Images/b9ce304927937c39d5bde10ec7fd6689.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b9ce304927937c39d5bde10ec7fd6689.png)'
- en: The training pipeline — Image by Author
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 训练流程 — 作者图像
- en: 'In this story, **we’ll get into the process of preparing your data for BERT,
    setting the stage for the ultimate goal: training a BERT model from scratch.**'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个故事中，**我们将深入探讨为BERT准备数据的过程，为最终目标奠定基础：从头开始训练一个BERT模型。**
- en: 'Welcome to the third installment of our comprehensive BERT series! In the first
    chapter, we introduced BERT — breaking down its objectives and demonstrating how
    to fine-tune it for a practical question-answering system:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到我们全面BERT系列的第三部分！在第一章中，我们介绍了BERT——解析其目标并演示如何对其进行微调，以便用于实际的问答系统：
- en: '[](/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f?source=post_page-----beaae6febfd5--------------------------------)
    [## The Ultimate Guide to Training BERT from Scratch: Introduction'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[从零开始训练BERT的终极指南：介绍](https://example.org/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f?source=post_page-----beaae6febfd5--------------------------------)'
- en: 'Demystifying BERT: The definition and various applications of the model that
    changed the NLP landscape.'
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 揭开BERT的神秘面纱：改变NLP领域的模型的定义及其各种应用
- en: towardsdatascience.com](/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f?source=post_page-----beaae6febfd5--------------------------------)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f?source=post_page-----beaae6febfd5--------------------------------)'
- en: 'Then, in the second chapter, we dived deep into the world of tokenizers, exploring
    their mechanics and even creating a custom tokenizer for the Greek language:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在第二章中，我们深入探讨了标记器的世界，探索了它们的机制，甚至为希腊语创建了一个自定义标记器：
- en: '[](/the-ultimate-guide-to-training-bert-from-scratch-the-tokenizer-ddf30f124822?source=post_page-----beaae6febfd5--------------------------------)
    [## The Ultimate Guide to Training BERT from Scratch: The Tokenizer'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[从零开始训练BERT的终极指南：标记器](https://example.org/the-ultimate-guide-to-training-bert-from-scratch-the-tokenizer-ddf30f124822?source=post_page-----beaae6febfd5--------------------------------)'
- en: 'From Text to Tokens: Your Step-by-Step Guide to BERT Tokenization'
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从文本到标记：你的BERT标记化逐步指南
- en: towardsdatascience.com](/the-ultimate-guide-to-training-bert-from-scratch-the-tokenizer-ddf30f124822?source=post_page-----beaae6febfd5--------------------------------)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/the-ultimate-guide-to-training-bert-from-scratch-the-tokenizer-ddf30f124822?source=post_page-----beaae6febfd5--------------------------------)'
- en: 'Now, we’re tackling one of the most pivotal stages of building a high-performing
    BERT model: *dataset preparation*. This guide will be a technical one, providing
    Python snippets and links to the GitHub repositories of popular open-source projects.
    Okay, we’re losing the light; let’s start!'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们正处理构建高性能BERT模型的一个最关键阶段：*数据集准备*。本指南将是技术性的，提供Python代码片段和流行开源项目的GitHub仓库链接。好了，我们失去光线了；让我们开始吧！
- en: '[Learning Rate](https://www.dimpo.me/newsletter?utm_source=medium&utm_medium=article&utm_campaign=bert-dataset)
    is a newsletter for those who are curious about the world of ML and MLOps. If
    you want to learn more about topics like this subscribe [here](https://www.dimpo.me/newsletter?utm_source=medium&utm_medium=article&utm_campaign=bert-dataset).
    You’ll hear from me on the last Sunday of every month with updates and thoughts
    on the latest MLOps news and articles!'
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[学习速递](https://www.dimpo.me/newsletter?utm_source=medium&utm_medium=article&utm_campaign=bert-dataset)是一个针对对机器学习和MLOps世界感到好奇的人的新闻通讯。如果你想了解更多类似的主题，可以[在这里](https://www.dimpo.me/newsletter?utm_source=medium&utm_medium=article&utm_campaign=bert-dataset)订阅。你会在每个月的最后一个星期天收到我的更新和对最新MLOps新闻和文章的思考！'
- en: Download & Explore
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载并探索
- en: First things first, we should choose a dataset. For the purpose of this tutorial,
    we’ll be working with the `[wikitext](https://huggingface.co/datasets/wikitext)`
    dataset, specifically the `wikitext-2-raw-v1` subset.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们应该选择一个数据集。为了本教程的目的，我们将使用`[wikitext](https://huggingface.co/datasets/wikitext)`数据集，具体是`wikitext-2-raw-v1`子集。
- en: 'According to its documentation, this dataset gathers over 100 million tokens
    extracted from Wikipedia’s verified articles. It’s the ideal playground for our
    BERT experiments, and thanks to Hugging Face, accessing it is a breeze:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 根据文档，这个数据集收集了来自维基百科已验证文章的超过1亿个标记。这是我们BERT实验的理想场所，多亏了Hugging Face，访问它非常简单：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `datasets` variable contains three splits: train, validation, and test.
    Let’s focus on the train split first.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`datasets`变量包含三个分割：训练、验证和测试。我们先关注训练分割。'
- en: 'The train split contains 36,718 rows of variable-length text. You’ll find entries
    ranging from empty strings to full-blown paragraphs. Here’s a peek using the Hugging
    Face dataset viewer:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集包含36,718行长度可变的文本。你会发现从空字符串到完整段落的条目。这里用Hugging Face数据集查看器来预览一下：
- en: '![](../Images/455f492ae353ea3571db5ff4918618e8.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/455f492ae353ea3571db5ff4918618e8.png)'
- en: Wikitext dataset — Image by Author
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Wikitext数据集 — 作者提供的图片
- en: '**Note to self: Consistency in token lengths is non-negotiable.** This is crucial
    as our model knows how to handle sequences of specific length during training.
    We should keep that in mind during the data processing phase.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**自我提醒：标记长度的一致性是不可妥协的。** 这很关键，因为我们的模型在训练过程中知道如何处理特定长度的序列。我们应该在数据处理阶段牢记这一点。'
- en: Now, what about the dataset’s features?
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，数据集的特征如何呢？
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output reveals a rather minimalistic structure:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示了一个相当简约的结构：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: That’s right, it’s just a `text` feature. So how do we shape this raw data into
    a robust training set for BERT? Get ready; we’re about to get our hands dirty
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 没错，它只是一个`text`特征。那么我们如何将这些原始数据转化为一个强大的BERT训练集呢？准备好，我们要开始动手了。
- en: Just like Halloween
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 就像万圣节一样
- en: 'To create our dataset preparation blueprint, let’s take a moment to revisit
    BERT’s objectives. BERT training is a two-phase process, but today, we’re focusing
    only on the first phase: pre-training. The goal here is to teach the model what
    language is and how context changes the meaning of words.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建我们的数据集准备蓝图，让我们花一点时间回顾BERT的目标。BERT训练是一个两阶段的过程，但今天我们只关注第一阶段：预训练。这里的目标是教会模型什么是语言以及上下文如何改变单词的意义。
- en: 'The pre-training phase has two key tasks: i) Masked Language Modeling (MLM)
    and ii) Next Sentence Prediction (NSP). During MLM, we purposely mask certain
    tokens in a sequence and train BERT to accurately predict them. With NSP, we present
    BERT with two adjacent sequences, asking it to predict whether the second naturally
    follows the first.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练阶段有两个关键任务：i) 掩码语言建模（MLM）和 ii) 下一个句子预测（NSP）。在MLM中，我们故意掩盖序列中的某些标记，并训练BERT准确预测它们。通过NSP，我们向BERT呈现两个相邻的序列，让它预测第二个序列是否自然跟随第一个序列。
- en: So, our goal is to create a dataset that will support these tasks and teach
    BERT the concepts we want. Let’s begin!
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们的目标是创建一个支持这些任务的数据集，并教会BERT我们想要的概念。让我们开始吧！
- en: The foundations
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基础知识
- en: 'Okay, first things first: we need to do two things before moving on to masking
    a sequence: We must tokenize the sentences and then create token sequences of
    equal length. Do you remember the mental note we made earlier?'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，首先：在进行序列掩码之前，我们需要做两件事：我们必须对句子进行分词，然后创建相同长度的标记序列。你还记得我们之前做的心理笔记吗？
- en: '![](../Images/88ac4835ee226280938f57b71003a539.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/88ac4835ee226280938f57b71003a539.png)'
- en: Image by Author
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'Let’s first load the pre-trained BERT tokenizer:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们加载预训练的BERT分词器：
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, tokenizing a text sentence is really simple: We come up with a function
    that accepts an example and passes it through the tokenizer. Then, we map this
    function across the dataset. This is a common practice when working with Hugging
    Face datasets, and we’ll see this pattern again and again.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，分词一个文本句子非常简单：我们编写一个接受示例并将其通过分词器的函数。然后，我们在数据集上映射这个函数。这是处理Hugging Face数据集时的常见做法，我们会一次次看到这个模式。
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note the `num_proc` and `batched` arguments. These are essential to make the
    process run faster. They will make a huge difference as your datasets are getting
    bigger. For more information about the `map` function, check out the `datasets`
    library [docs](https://huggingface.co/docs/datasets/process#map). To understand
    more about these arguments specifically, take a closer look at the [multiprocessing](https://huggingface.co/docs/datasets/process#multiprocessing)
    and [batch-processing](https://huggingface.co/docs/datasets/process#batch-processing)
    sections.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`num_proc`和`batched`参数。这些参数对加速过程至关重要。随着数据集的增大，它们将产生巨大差异。有关`map`函数的更多信息，请查看`datasets`库的[文档](https://huggingface.co/docs/datasets/process#map)。要具体了解这些参数，请仔细阅读[多处理](https://huggingface.co/docs/datasets/process#multiprocessing)和[批处理](https://huggingface.co/docs/datasets/process#batch-processing)部分。
- en: Also, be mindful that we asked the tokenizer to omit adding any special tokens
    (`add_special_tokens=False`). You’ll see later why.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意我们要求分词器省略添加任何特殊标记（`add_special_tokens=False`）。稍后你会明白为什么。
- en: The next step is to create sequences of equal length. BERT usually accepts sequences
    of `512` tokens. This is mostly defined by the shape of the positional encodings
    matrix in the model’s architecture.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是创建相同长度的序列。BERT通常接受`512`个标记的序列。这主要由模型架构中的位置编码矩阵的形状定义。
- en: 'Thus, in this step, we will create sequences of `255` tokens. Why? Because
    later, we want to concatenate two sentences to create a dataset that supports
    the NSP tasks. This process will create sequences of `510` tokens (`255` + `255`).
    However, we should also add two special tokens: `[CLS]` to indicate the start
    of a sequence and `[SEP]` to mark where the first sentence ends and the second
    one begins.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这一步，我们将创建`255`个标记的序列。为什么？因为稍后我们希望将两个句子连接起来，创建一个支持NSP任务的数据集。这个过程将创建`510`个标记的序列（`255`
    + `255`）。然而，我们还应添加两个特殊标记：`[CLS]`以指示序列的开始，以及`[SEP]`以标记第一个句子结束和第二个句子开始的地方。
- en: 'As always, let’s create a helper function that does that:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，让我们创建一个辅助函数来完成这个任务：
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This function performs two transformations: First, it brings all the sequences
    together and then splits them again into small blocks of `255` tokens. Let’s map
    it over our tokenized dataset:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数执行两个转换：首先，它将所有序列汇集在一起，然后将它们再次拆分成`255`个标记的小块。让我们在我们的标记化数据集上映射它：
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: That concludes the preliminary transformations. Next, let’s concatenate two
    sequences together to create the effect we want for NSP.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了初步的转换。接下来，让我们将两个序列连接在一起，以创建我们想要的NSP效果。
- en: Nest Sentence Prediction
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌套句子预测
- en: As a reminder, the NSP task tries to predict if two given sentences are adjacent
    in the original text or not. Thus, we need a dataset where each entry should comprise
    a pair of sentences along with a label that indicates if they are related.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，NSP任务尝试预测两个给定句子是否在原始文本中相邻。因此，我们需要一个数据集，其中每条记录应包含一对句子以及一个标签，指示它们是否相关。
- en: To create such a dataset, we will, once again, define a helper function. Admittedly,
    this function will be a bit more intricate than what we’ve worked with before,
    but don’t let that intimidate you. The most effective way to tackle complexity
    is often to dive in head first.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建这样的数据集，我们将再次定义一个辅助函数。诚然，这个函数会比我们之前处理的稍微复杂一些，但不要因此感到害怕。处理复杂性最有效的方法往往是全身心投入其中。
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Well, that seems a lot! So, let’s visualize what this function does to make
    the code more digestible. This function aims to create something similar to the
    QnA scheme we saw in the first [introductory blog post](/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，这看起来确实很多！所以，让我们通过可视化这个函数的功能来使代码更易于理解。这个函数旨在创建类似于我们在第一篇[介绍博客文章](/the-ultimate-guide-to-training-bert-from-scratch-introduction-b048682c795f)中看到的QnA方案的东西：
- en: '![](../Images/db03129cde7b08c9322c885d8cd3e0bd.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/db03129cde7b08c9322c885d8cd3e0bd.png)'
- en: BERT QnA Dataset Preparation — Image by Author
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: BERT QnA数据集准备 — 作者图片
- en: 'The `if/else` block works as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`if/else`块的工作原理如下：'
- en: Half of the time, create sequences of subsequent sentences, except if the sequence
    at hand is the last one, in which case, wrap around and create a sequence of two
    random sentences.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一半的时间，创建后续句子的序列，除了当当前序列是最后一个时，这种情况下，回绕并创建两个随机句子的序列。
- en: The other half of the time, create a sequence of two random sentences.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一半时间，创建两个随机句子的序列。
- en: 'Set the next sentence label accordingly: `0` for subsequent sequences, `1`
    if they have no connection.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相应地设置下一个句子的标签：`0`表示后续序列，`1`表示它们没有关联。
- en: Combine the two sentences using the `[CLS]` and `[SEP]` special tokens.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`[CLS]`和`[SEP]`特殊标记将两个句子组合在一起。
- en: Create the `token_type_ids` list to mark the two segments.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建`token_type_ids`列表以标记两个段落。
- en: 'Now, how did I come up with the names `token_type_ids` and `next_sentence_label`?
    Are they random names, and you can use whatever variable name you want? No! If
    you look at the model’s [forward](https://github.com/huggingface/transformers/blob/fa6107c97edf7cf725305a34735a57875b67d85e/src/transformers/models/bert/modeling_bert.py#L1077)
    method signature, you’ll find exactly these names:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我是如何想到`token_type_ids`和`next_sentence_label`这些名称的？这些名称是随机的吗，你可以使用任何变量名吗？不！如果你查看模型的[forward](https://github.com/huggingface/transformers/blob/fa6107c97edf7cf725305a34735a57875b67d85e/src/transformers/models/bert/modeling_bert.py#L1077)方法签名，你会发现正是这些名称：
- en: '[PRE8]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Pay close attention to this fact; you need to use exactly these names. If you
    don’t, the library will remove those features from your dataset before passing
    it to the model. Why? Because this is what it is supposed to do by [default](https://github.com/huggingface/transformers/blob/fa6107c97edf7cf725305a34735a57875b67d85e/src/transformers/trainer.py#L746).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这一点；你需要使用这些确切的名称。如果不使用，库会在将数据集传递给模型之前从中删除这些特性。为什么？因为这是[默认](https://github.com/huggingface/transformers/blob/fa6107c97edf7cf725305a34735a57875b67d85e/src/transformers/trainer.py#L746)行为。
- en: 'Finally, as always, let’s map this method. But first, let’s separate the different
    dataset splits so we have them ready later for training and map the function to
    each one separately:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，像往常一样，让我们映射这个方法。但首先，让我们分开不同的数据集拆分，以便稍后可以准备好进行训练，并将函数分别映射到每一个：
- en: '[PRE9]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'There is a subtle difference in how we map the function now: We are creating
    a lambda function that uses the one we defined, and we set `with_indices=True`
    so that our helper function has a handle on the index of each example that it
    operates on.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在映射函数的方法有一个微妙的区别：我们正在创建一个使用我们定义的函数的lambda函数，并设置`with_indices=True`，以便我们的辅助函数可以处理它操作的每个示例的索引。
- en: Great! We have prepared our dataset to handle the NSP task. What about MLM?
    In the first blog, we’ve seen that we train BERT on the two tasks simultaneously.
    So, let’s tackle the MLM transformation next.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！我们已经准备好了数据集来处理NSP任务。那么MLM呢？在第一篇博客中，我们看到我们同时训练BERT进行这两个任务。因此，让我们接下来处理MLM转换。
- en: Masked Language Modeling
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 掩码语言模型
- en: Luckily, the `transformers` library offers a streamlined solution for the Masked
    Language Modeling (MLM) task. Simply instantiate the `DataCollatorForLanguageModeling`
    class with the appropriate parameters, and voila — most of the work is done for
    you.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，`transformers`库为掩码语言模型（MLM）任务提供了一个简化的解决方案。只需使用适当的参数实例化`DataCollatorForLanguageModeling`类，效果就会非常好——大部分工作都为你完成了。
- en: But wait, you didn’t come here for the easy way out, did you? This guide is
    all about diving deep, so we won’t tolerate any black boxes. We’re going to pull
    back the curtain and explore how exactly to create a custom dataset for MLM.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 但等等，你不是来这里寻找简单的方法的，对吧？本指南的最终目的是深入探讨，因此我们不接受任何黑箱操作。我们将揭开帷幕，探讨如何为MLM创建自定义数据集。
- en: 'Here’s how we’ll tackle it: First, we’ll paint a mental picture by visualizing
    the core concepts. Think of it as a virtual storyboard that brings everything
    into focus. Next, we’ll dissect the code, breaking it down step by step to get
    a grasp of each component.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将如何处理：首先，我们通过可视化核心概念来绘制心理图像。可以将其视为一个虚拟的故事板，使一切变得清晰。接下来，我们将逐步解析代码，以掌握每个组件。
- en: Our first order of business? Crafting labels for our input tokens. this is an
    easy one; all we need to do is clone the input array.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的首要任务？为输入标记制作标签。这很简单；我们只需克隆输入数组即可。
- en: '![](../Images/1b88e658a7544a842588c33718e9fd3d.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b88e658a7544a842588c33718e9fd3d.png)'
- en: Labels generation — Image by Author
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 标签生成——作者图像
- en: Next up, we forge the “mask” array, which mirrors the shape of our inputs and
    labels. Each entry in this mask array represents a probability value, dictating
    the likelihood that a toke at the corresponding position will be masked.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们构建“掩码”数组，该数组反映了输入和标签的形状。此掩码数组中的每个条目表示一个概率值，决定了相应位置的标记被掩码的可能性。
- en: 'For the sake of demonstration, we’ll use a masking probability of `0.5`, even
    though in real-world applications, it’s usually set closer to `0.15`:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示，我们将使用`0.5`的掩码概率，尽管在实际应用中，通常设定更接近`0.15`：
- en: '![](../Images/ff664b4a162b8cd72c35d13362bcc81f.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ff664b4a162b8cd72c35d13362bcc81f.png)'
- en: Mask generation — Image by Author
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码生成——作者图像
- en: 'Now, we know that there are some tokens we surely do not want to mask: the
    special `[CLS]` and `[SEP]` tokens. So, let’s set their corresponding probability
    values in the mask array to zero, effectively ruling out any chance of them being
    masked.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们知道有些标记我们绝对不想掩码：特殊的`[CLS]`和`[SEP]`标记。所以，让我们将掩码数组中它们对应的概率值设置为零，从而有效地排除它们被掩码的可能性。
- en: '![](../Images/f548bd61e099a0fa57fd1f3385c1c1c1.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f548bd61e099a0fa57fd1f3385c1c1c1.png)'
- en: Mask special tokens — Image by Author
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码特殊标记——作者图像
- en: 'Now, for each position in the mask, let’s evaluate the probability and set
    the value to either `1.0` or `0.0`. Value `1.0` means that we will mask the token
    in the corresponding position while `0.0` means we will leave it unchanged:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对于掩码中的每个位置，让我们评估概率并将值设置为`1.0`或`0.0`。值`1.0`表示我们将在相应位置掩码标记，而`0.0`表示我们将保持不变：
- en: '![](../Images/89e70b742cf2428654661772c93b041b.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89e70b742cf2428654661772c93b041b.png)'
- en: Probability evaluation — Image by Author
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 概率评估——作者图像
- en: 'Great, we now have a map that points out which tokens we will mask. But here’s
    a twist: not all masked positions will be replaced by the `[MASK]` token. In fact,
    we’ll employ a more tricky strategy. Here’s how it breaks down: 50% of the masked
    tokens will get the special `[MASK]` token, 25% will be swapped out for a random
    token, and the remaining 25% will be left as is — though BERT will not know this.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，我们现在有了一个指示要掩码哪些标记的地图。但是有个变数：并不是所有被掩码的位置都会被`[MASK]`标记替代。实际上，我们将采用更复杂的策略。具体来说：50%的被掩码标记会得到特殊的`[MASK]`标记，25%会被替换为随机标记，剩下的25%保持不变——尽管BERT对此一无所知。
- en: 'In reality, we use an 80–10–10 scheme, but to make it work with our illustration,
    let’s employ the 50–25–25 strategy:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们使用80–10–10方案，但为了使其适用于我们的示例，我们采用50–25–25策略：
- en: '![](../Images/cc44b4ddc64ad7cfdda675d80469130b.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cc44b4ddc64ad7cfdda675d80469130b.png)'
- en: Mask tokens — Image by Author
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码标记——作者图像
- en: 'Finally, it’s time to treat the labels. The labels at indices that should not
    contribute to the loss — i.e., the unmasked tokens — are set to `-100`. This is
    not a random negative number. It’s the default value for the `ignore_index` property
    in PyTorch’s [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html).
    Thus, Pytorch will ignore these labels when calculating the loss:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，是时候处理标签了。那些不应该影响损失的索引标签——即未掩码的标记——被设置为 `-100`。这不是一个随机的负数，而是 PyTorch 的 [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)
    中 `ignore_index` 属性的默认值。因此，Pytorch 在计算损失时会忽略这些标签：
- en: '![](../Images/63492528b5b2fd0eeb5be3ed01dee92c.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63492528b5b2fd0eeb5be3ed01dee92c.png)'
- en: Process labels— Image by Author
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 过程标签— 作者提供的图像
- en: 'That’s it! Let’s bring everything together in a final summarizing animation:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些了！让我们在最终的总结动画中将所有内容汇总起来：
- en: '![](../Images/041fd60e99943b7c383bf241ea72ceb7.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/041fd60e99943b7c383bf241ea72ceb7.png)'
- en: Masked Language Modeling — Image by Author
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码语言建模 — 作者提供的图像
- en: 'Now that we have a clear mental picture, let’s see the code. This is taken
    directly from `transformers` [library](https://github.com/huggingface/transformers/blob/41aef33758ae166291d72bc381477f2db84159cf/src/transformers/data/data_collator.py#L751):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了清晰的心理图像，让我们看看代码。这直接取自 `transformers` [库](https://github.com/huggingface/transformers/blob/41aef33758ae166291d72bc381477f2db84159cf/src/transformers/data/data_collator.py#L751)：
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We kick off by cloning the inputs to form the labels — old news by now! Next,
    we generate a probability matrix that’s the same shape as the labels and fill
    it with a chosen probability value. We then pinpoint the special tokens in the
    sequence and turn down their masking probabilities to a flat zero. Are you with
    me? Everything should look familiar!
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从克隆输入开始，形成标签——这现在已经是老新闻了！接下来，我们生成一个与标签形状相同的概率矩阵，并用选择的概率值填充它。然后，我们确定序列中的特殊标记，并将它们的掩码概率降低到零。你跟上了吗？一切应该都很熟悉！
- en: 'Here’s where it gets spicy: we evaluate the probability matrix using a Bernoulli
    distribution. Sounds a bit grande, but it’s basically a coin flip determining
    whether each matrix position gets a `1.0` or a `0.0`. Then we translate these
    ones and zeros to true and false values. Any ‘false’ hits in the matrix? We set
    their corresponding label values to `-100`, as we discussed previously.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这就有趣了：我们使用伯努利分布评估概率矩阵。听起来有点复杂，但基本上是一个硬币抛掷决定每个矩阵位置是 `1.0` 还是 `0.0`。然后我们将这些 1
    和 0 转换为真和假值。矩阵中有‘假’的情况吗？我们将它们对应的标签值设置为 `-100`，如前所述。
- en: Finally, we bring into play the 80–10–10 masking scheme we previously touched
    on, using the same Bernoulli logic. This effectively masks, replaces, or leaves
    tokens untouched in the input sequence, offering BERT a varied diet of data to
    chew on.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们运用了之前提到的 80–10–10 掩码方案，使用相同的伯努利逻辑。这有效地掩盖、替换或保留输入序列中的标记，为 BERT 提供了丰富的数据。
- en: 'And that’s all! It’s a one-liner to go through all these steps using the `datasets`
    library. Instantiate a `DataCollatorForLanguageModeling` object and then pass
    it in the `Trainer` (more on the `Trainer` in the next episode):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！使用 `datasets` 库来完成所有这些步骤只需一行代码。实例化一个 `DataCollatorForLanguageModeling` 对象，然后将其传递给
    `Trainer`（有关 `Trainer` 的更多信息将在下一集介绍）：
- en: '[PRE11]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Conclusion
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In the journey to train BERT from scratch, preparing the dataset is often the
    most labor-intensive and difficult step but also one of the most rewarding.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在从头开始训练 BERT 的过程中，准备数据集通常是最耗时且最困难的步骤，但也是最有价值的步骤之一。
- en: If you’ve followed along, you’ve not just learned how to prepare the dataset
    but also the reason behind each decision in dataset preparation. From selecting
    the ideal dataset and breaking down its features to the fine art of masking tokens
    using a varied strategy, we’ve delved into the details that could make or break
    your model’s performance.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你一直跟着，你不仅学会了如何准备数据集，还了解了数据集准备过程中每个决策的原因。从选择理想的数据集和分解其特征，到使用多种策略掩盖标记的精妙艺术，我们已经深入探讨了那些可能影响模型性能的细节。
- en: In the next chapter, we’ll finally touch on the training process. See you there!
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将最终触及训练过程。到时见！
- en: About the Author
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于作者
- en: My name is [Dimitris Poulopoulos](https://www.dimpo.me/?utm_source=medium&utm_medium=article&utm_campaign=bert-dataset),
    and I’m a machine learning engineer working for [HPE](https://www.hpe.com/us/en/home.html).
    I have designed and implemented AI and software solutions for major clients such
    as the European Commission, IMF, the European Central Bank, IKEA, Roblox and others.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我的名字是[Dimitris Poulopoulos](https://www.dimpo.me/?utm_source=medium&utm_medium=article&utm_campaign=bert-dataset)，我是一名机器学习工程师，现为[HPE](https://www.hpe.com/us/en/home.html)工作。我为主要客户如欧盟委员会、国际货币基金组织、欧洲中央银行、宜家、Roblox等设计和实施了人工智能及软件解决方案。
- en: If you are interested in reading more posts about Machine Learning, Deep Learning,
    Data Science, and DataOps, follow me on [Medium](https://towardsdatascience.com/medium.com/@dpoulopoulos/follow),
    [LinkedIn](https://www.linkedin.com/in/dpoulopoulos/), or [@james2pl](https://twitter.com/james2pl)
    on Twitter.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣阅读更多关于机器学习、深度学习、数据科学和数据操作的文章，可以在[Medium](https://towardsdatascience.com/medium.com/@dpoulopoulos/follow)、[LinkedIn](https://www.linkedin.com/in/dpoulopoulos/)或在Twitter上的[@james2pl](https://twitter.com/james2pl)关注我。
- en: Opinions expressed are solely my own and do not express the views or opinions
    of my employer.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 表达的观点仅代表我个人，不代表我的雇主的观点或意见。
