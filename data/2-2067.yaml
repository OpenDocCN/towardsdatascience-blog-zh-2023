- en: The Three Essential Methods to Evaluate a New Language Model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¯„ä¼°æ–°è¯­è¨€æ¨¡å‹çš„ä¸‰ç§åŸºæœ¬æ–¹æ³•
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/the-three-essential-methods-to-evaluate-a-new-language-model-aa5c526bacfd](https://towardsdatascience.com/the-three-essential-methods-to-evaluate-a-new-language-model-aa5c526bacfd)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/the-three-essential-methods-to-evaluate-a-new-language-model-aa5c526bacfd](https://towardsdatascience.com/the-three-essential-methods-to-evaluate-a-new-language-model-aa5c526bacfd)
- en: How to check whether the newest, hottest Large Language Model (LLM) fits your
    needs
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¦‚ä½•æ£€æŸ¥æœ€æ–°ã€æœ€çƒ­é—¨çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯å¦ç¬¦åˆä½ çš„éœ€æ±‚
- en: '[](https://heiko-hotz.medium.com/?source=post_page-----aa5c526bacfd--------------------------------)[![Heiko
    Hotz](../Images/d08394d46d41d5cd9e76557a463be95e.png)](https://heiko-hotz.medium.com/?source=post_page-----aa5c526bacfd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----aa5c526bacfd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----aa5c526bacfd--------------------------------)
    [Heiko Hotz](https://heiko-hotz.medium.com/?source=post_page-----aa5c526bacfd--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://heiko-hotz.medium.com/?source=post_page-----aa5c526bacfd--------------------------------)[![Heiko
    Hotz](../Images/d08394d46d41d5cd9e76557a463be95e.png)](https://heiko-hotz.medium.com/?source=post_page-----aa5c526bacfd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----aa5c526bacfd--------------------------------)[![æ•°æ®ç§‘å­¦ä¹‹é“](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----aa5c526bacfd--------------------------------)
    [Heiko Hotz](https://heiko-hotz.medium.com/?source=post_page-----aa5c526bacfd--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----aa5c526bacfd--------------------------------)
    Â·6 min readÂ·Jul 3, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº[æ•°æ®ç§‘å­¦ä¹‹é“](https://towardsdatascience.com/?source=post_page-----aa5c526bacfd--------------------------------)
    Â·6åˆ†é’Ÿé˜…è¯»Â·2023å¹´7æœˆ3æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/d44df1d02e0e695eedca24b1c6e2d15b.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d44df1d02e0e695eedca24b1c6e2d15b.png)'
- en: Image by author (using Stable Diffusion)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºï¼ˆä½¿ç”¨Stable Diffusionï¼‰
- en: What is this about?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä»€ä¹ˆï¼Ÿ
- en: 'New LLMs are released every week, and if youâ€™re like me, you might ask yourself:
    Does this one finally fit all the use cases I want to utilise an LLM for? In this
    tutorial, I will share the techniques that I use to evaluate new LLMs. Iâ€™ll introduce
    three techniques I use regularly â€” none of them are new (in fact, I will refer
    to blog posts that I have written previously), but by bringing them all together,
    I save a significant amount of time whenever a new LLM is released. I will demonstrate
    examples of testing on the new [OpenChat](https://huggingface.co/openchat/openchat_8192)
    model.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯å‘¨éƒ½ä¼šå‘å¸ƒæ–°çš„LLMï¼Œå¦‚æœä½ åƒæˆ‘ä¸€æ ·ï¼Œå¯èƒ½ä¼šé—®è‡ªå·±ï¼šè¿™ä¸ªæ¨¡å‹æ˜¯å¦ç»ˆäºé€‚åˆæˆ‘æƒ³è¦åˆ©ç”¨LLMçš„æ‰€æœ‰ç”¨ä¾‹ï¼Ÿåœ¨è¿™ä¸ªæ•™ç¨‹ä¸­ï¼Œæˆ‘å°†åˆ†äº«æˆ‘ç”¨æ¥è¯„ä¼°æ–°LLMçš„æŠ€æœ¯ã€‚æˆ‘å°†ä»‹ç»æˆ‘å®šæœŸä½¿ç”¨çš„ä¸‰ç§æŠ€æœ¯â€”â€”å®ƒä»¬æ²¡æœ‰æ–°çš„ï¼ˆå®é™…ä¸Šï¼Œæˆ‘ä¼šæåˆ°æˆ‘ä¹‹å‰å†™çš„åšå®¢æ–‡ç« ï¼‰ï¼Œä½†é€šè¿‡å°†å®ƒä»¬ç»“åˆåœ¨ä¸€èµ·ï¼Œæ¯å½“æœ‰æ–°çš„LLMå‘å¸ƒæ—¶ï¼Œæˆ‘å¯ä»¥èŠ‚çœå¤§é‡æ—¶é—´ã€‚æˆ‘å°†å±•ç¤ºåœ¨æ–°çš„[OpenChat](https://huggingface.co/openchat/openchat_8192)æ¨¡å‹ä¸Šè¿›è¡Œæµ‹è¯•çš„ç¤ºä¾‹ã€‚
- en: Why is this important?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆè¿™å¾ˆé‡è¦ï¼Ÿ
- en: When it comes to new LLMs, itâ€™s important to understand their capabilities and
    limitations. Unfortunately, figuring out how to deploy the model and then systematically
    testing it can be a bit of a drag. This process is often manual and can consume
    a lot of time. However, with a standardised approach, we can iterate much faster
    and quickly determine whether a model is worth investing more time in, or if we
    should discard it. So, letâ€™s get started.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ–°çš„LLMï¼Œäº†è§£å…¶èƒ½åŠ›å’Œé™åˆ¶æ˜¯å¾ˆé‡è¦çš„ã€‚ä¸å¹¸çš„æ˜¯ï¼Œå¼„æ¸…æ¥šå¦‚ä½•éƒ¨ç½²æ¨¡å‹å¹¶ç³»ç»Ÿåœ°æµ‹è¯•å®ƒå¯èƒ½ä¼šæœ‰äº›éº»çƒ¦ã€‚è¿™ä¸ªè¿‡ç¨‹é€šå¸¸æ˜¯æ‰‹åŠ¨çš„ï¼Œå¹¶ä¸”å¯èƒ½æ¶ˆè€—å¤§é‡æ—¶é—´ã€‚ç„¶è€Œï¼Œé€šè¿‡æ ‡å‡†åŒ–çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥æ›´å¿«åœ°è¿­ä»£ï¼Œå¹¶è¿…é€Ÿç¡®å®šä¸€ä¸ªæ¨¡å‹æ˜¯å¦å€¼å¾—æŠ•å…¥æ›´å¤šæ—¶é—´ï¼Œè¿˜æ˜¯åº”è¯¥æ”¾å¼ƒå®ƒã€‚æ‰€ä»¥ï¼Œè®©æˆ‘ä»¬å¼€å§‹å§ã€‚
- en: '**Getting Started**'
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**å¼€å§‹ä½¿ç”¨**'
- en: There are many ways to utilise an LLM, but when we distil the most common uses,
    they often pertain to open-ended tasks (e.g. generating text for a marketing ad),
    chatbot applications, and Retrieval Augmented Generation (RAG). Correspondingly,
    I employ relevant methods to test these capabilities in an LLM.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ©ç”¨LLMçš„æ–¹å¼æœ‰å¾ˆå¤šï¼Œä½†å½“æˆ‘ä»¬æç‚¼å‡ºæœ€å¸¸è§çš„ç”¨é€”æ—¶ï¼Œå®ƒä»¬é€šå¸¸æ¶‰åŠå¼€æ”¾æ€§ä»»åŠ¡ï¼ˆä¾‹å¦‚ï¼Œä¸ºè¥é”€å¹¿å‘Šç”Ÿæˆæ–‡æœ¬ï¼‰ã€èŠå¤©æœºå™¨äººåº”ç”¨å’Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ã€‚ç›¸åº”åœ°ï¼Œæˆ‘ä¼šä½¿ç”¨ç›¸å…³æ–¹æ³•æ¥æµ‹è¯•è¿™äº›èƒ½åŠ›ã€‚
- en: 0\. Deploying the model
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 0\. éƒ¨ç½²æ¨¡å‹
- en: 'Before we get started with the evaluation, we first need to deploy the model.
    I have boilerplate code ready for this, where we can just swap out the model ID
    and the instance to which to deploy (Iâ€™m using Amazon SageMaker for model hosting
    in this example) and weâ€™re good to go:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹è¯„ä¼°ä¹‹å‰ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦éƒ¨ç½²æ¨¡å‹ã€‚æˆ‘å·²ç»å‡†å¤‡å¥½äº†ä¸€äº›æ¨¡æ¿ä»£ç ï¼Œæˆ‘ä»¬åªéœ€æ›´æ¢æ¨¡å‹IDå’Œè¦éƒ¨ç½²çš„å®ä¾‹ï¼ˆåœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä½¿ç”¨çš„æ˜¯Amazon SageMakerè¿›è¡Œæ¨¡å‹æ‰˜ç®¡ï¼‰ï¼Œå°±å¯ä»¥å¼€å§‹äº†ï¼š
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Itâ€™s worth noting that we can utilise the new [Hugging Face LLM Inference Container
    for SageMaker](https://huggingface.co/blog/sagemaker-huggingface-llm), as the
    new OpenChat model is based on the LLAMA architecture, which is supported in this
    container.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨æ–°çš„[Hugging Face LLM Inference Container for SageMaker](https://huggingface.co/blog/sagemaker-huggingface-llm)ï¼Œå› ä¸ºæ–°çš„OpenChatæ¨¡å‹åŸºäºLLAMAæ¶æ„ï¼Œè¯¥æ¶æ„åœ¨è¿™ä¸ªå®¹å™¨ä¸­å—æ”¯æŒã€‚
- en: 1\. Playground
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. æ¸¸ä¹åœº
- en: Using the notebook to test a few prompts can be burdensome, and it may also
    discourage non-technical users from experimenting with the model. A much more
    effective way to familiarise yourself with the model, and to encourage others
    to do the same, involves the construction of a playground. I have previously detailed
    how to easily create such a playground in this [blog post](/create-your-own-large-language-model-playground-in-sagemaker-studio-1be5846c5089).
    With the code from that blog post, we can get a playground up and running quickly.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ç¬”è®°æœ¬æµ‹è¯•ä¸€äº›æç¤ºå¯èƒ½å¾ˆç¹çï¼Œä¹Ÿå¯èƒ½ä¼šè®©éæŠ€æœ¯ç”¨æˆ·ä¸æ„¿å°è¯•æ¨¡å‹ã€‚æ›´æœ‰æ•ˆçš„æ–¹å¼æ˜¯æ„å»ºä¸€ä¸ªæ¸¸ä¹åœºã€‚æˆ‘ä¹‹å‰åœ¨è¿™ç¯‡[åšå®¢æ–‡ç« ](/create-your-own-large-language-model-playground-in-sagemaker-studio-1be5846c5089)ä¸­è¯¦ç»†è¯´æ˜äº†å¦‚ä½•è½»æ¾åˆ›å»ºè¿™æ ·ä¸€ä¸ªæ¸¸ä¹åœºã€‚åˆ©ç”¨é‚£ç¯‡åšå®¢æ–‡ç« ä¸­çš„ä»£ç ï¼Œæˆ‘ä»¬å¯ä»¥å¿«é€Ÿæ­å»ºä¸€ä¸ªæ¸¸ä¹åœºã€‚
- en: 'Once the playground is established, we can introduce some prompts to evaluate
    the modelâ€™s responses. I prefer using open-ended prompts, where I pose a question
    that requires some degree of common sense to answer:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦å»ºç«‹äº†æ¸¸ä¹åœºï¼Œæˆ‘ä»¬å¯ä»¥å¼•å…¥ä¸€äº›æç¤ºæ¥è¯„ä¼°æ¨¡å‹çš„å›åº”ã€‚æˆ‘æ›´å–œæ¬¢ä½¿ç”¨å¼€æ”¾å¼æç¤ºï¼Œå³æå‡ºä¸€ä¸ªéœ€è¦ä¸€å®šå¸¸è¯†æ¥å›ç­”çš„é—®é¢˜ï¼š
- en: '***How can I improve my time management skills?***'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '***æˆ‘å¦‚ä½•æé«˜æˆ‘çš„æ—¶é—´ç®¡ç†æŠ€èƒ½ï¼Ÿ***'
- en: '![](../Images/4c59792642c67fb692d5da39dac0ed64.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c59792642c67fb692d5da39dac0ed64.png)'
- en: Image by author
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡
- en: '***What if the Suez Canal had never been constructed?***'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '***å¦‚æœè‹ä¼Šå£«è¿æ²³ä»æœªè¢«å»ºé€ ä¼šæ€æ ·ï¼Ÿ***'
- en: '![](../Images/257c34f41bfc5060f0add201b72d2ee3.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/257c34f41bfc5060f0add201b72d2ee3.png)'
- en: Image by author
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡
- en: Both responses appear promising, suggesting that it could be worthwhile to invest
    additional time and resources in testing the OpenChat model.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¤ç§å›åº”éƒ½å¾ˆæœ‰å‰æ™¯ï¼Œè¿™è¡¨æ˜æŠ•èµ„é¢å¤–çš„æ—¶é—´å’Œèµ„æºæ¥æµ‹è¯•OpenChatæ¨¡å‹å¯èƒ½æ˜¯å€¼å¾—çš„ã€‚
- en: 2.Chatbot
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2. èŠå¤©æœºå™¨äºº
- en: The second thing we want to explore is a modelâ€™s chatbot capabilities. Unlike
    the playground, where the model is consistently stateless, we want to understand
    its ability to â€œrememberâ€ context within a conversation. In this [blog post](https://medium.com/mlearning-ai/unlocking-the-future-of-chatbots-with-falcon-hugging-face-and-amazon-sagemaker-cf6bd8aeba54),
    I described how to set up a chatbot using the Falcon model. Itâ€™s a simple plug-and-play
    operation, and by changing the SageMaker endpoint, we can direct it towards the
    new OpenChat model.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æƒ³è¦æ¢ç´¢çš„ç¬¬äºŒä»¶äº‹æ˜¯æ¨¡å‹çš„èŠå¤©æœºå™¨äººèƒ½åŠ›ã€‚ä¸æ¸¸ä¹åœºä¸åŒï¼Œåœ¨æ¸¸ä¹åœºä¸­æ¨¡å‹å§‹ç»ˆæ˜¯æ— çŠ¶æ€çš„ï¼Œæˆ‘ä»¬å¸Œæœ›äº†è§£å…¶åœ¨å¯¹è¯ä¸­â€œè®°ä½â€ä¸Šä¸‹æ–‡çš„èƒ½åŠ›ã€‚åœ¨è¿™ç¯‡[åšå®¢æ–‡ç« ](https://medium.com/mlearning-ai/unlocking-the-future-of-chatbots-with-falcon-hugging-face-and-amazon-sagemaker-cf6bd8aeba54)ä¸­ï¼Œæˆ‘æè¿°äº†å¦‚ä½•ä½¿ç”¨Falconæ¨¡å‹è®¾ç½®èŠå¤©æœºå™¨äººã€‚è¿™æ˜¯ä¸€ä¸ªç®€å•çš„å³æ’å³ç”¨æ“ä½œï¼Œé€šè¿‡æ›´æ”¹SageMakerç«¯ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶æŒ‡å‘æ–°çš„OpenChatæ¨¡å‹ã€‚
- en: 'Letâ€™s see how it fares:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹å®ƒçš„è¡¨ç°ï¼š
- en: '![](../Images/d384127f6a6d619277833486c512d7bf.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d384127f6a6d619277833486c512d7bf.png)'
- en: Image by author
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡
- en: The performance as a chatbot is quite impressive. There was an instance, however,
    where Openchat attempted to abruptly terminate the conversation, cutting off in
    mid-sentence. This occurrence is not rare, in fact. We donâ€™t usually observe this
    with other chatbots because they employ specific stop words to compel the AI to
    cease text generation. The occurrence of this issue in my app is probably due
    to the implementation of stop words within my application.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºèŠå¤©æœºå™¨äººçš„è¡¨ç°ç›¸å½“ä»¤äººå°è±¡æ·±åˆ»ã€‚ç„¶è€Œï¼Œæœ‰ä¸€æ¬¡Openchatè¯•å›¾çªç„¶ç»“æŸå¯¹è¯ï¼ŒåŠå¥è¯è¢«åˆ‡æ–­äº†ã€‚å®é™…ä¸Šï¼Œè¿™ç§æƒ…å†µå¹¶ä¸ç½•è§ã€‚æˆ‘ä»¬é€šå¸¸ä¸ä¼šåœ¨å…¶ä»–èŠå¤©æœºå™¨äººä¸­è§‚å¯Ÿåˆ°è¿™ç§æƒ…å†µï¼Œå› ä¸ºå®ƒä»¬ä½¿ç”¨ç‰¹å®šçš„åœæ­¢è¯æ¥è¿«ä½¿AIåœæ­¢æ–‡æœ¬ç”Ÿæˆã€‚è¿™ç§é—®é¢˜åœ¨æˆ‘çš„åº”ç”¨ç¨‹åºä¸­å‘ç”Ÿçš„åŸå› å¯èƒ½æ˜¯ç”±äºæˆ‘çš„åº”ç”¨ç¨‹åºä¸­å®æ–½äº†åœæ­¢è¯ã€‚
- en: Beyond that, OpenChat has the capability to maintain context throughout a conversation,
    as well as to extract crucial information from a document. Impressive. ğŸ˜Š
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤æ­¤ä¹‹å¤–ï¼ŒOpenChatå…·æœ‰åœ¨å¯¹è¯ä¸­ä¿æŒä¸Šä¸‹æ–‡çš„èƒ½åŠ›ï¼Œå¹¶ä¸”èƒ½å¤Ÿä»æ–‡æ¡£ä¸­æå–å…³é”®ä¿¡æ¯ã€‚ä»¤äººå°è±¡æ·±åˆ»ã€‚ğŸ˜Š
- en: 3\. Retrieval Augmented Generation (RAG)
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰
- en: 'The last task we want to test involves using LangChain for some RAG tasks.
    Iâ€™ve found that RAG tasks can be quite challenging for open source models, often
    requiring me to write my own prompts and custom response parsers to achieve functionality.
    However, what Iâ€™d like to see is a model that operates optimally â€œout of the boxâ€
    for standard RAG tasks. This [blog post](https://medium.com/mlearning-ai/supercharging-large-language-models-with-langchain-1cac3c103b52)
    provides a few examples of such tasks. Letâ€™s examine how well it performs. The
    question weâ€™ll be posing is:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æƒ³æµ‹è¯•çš„æœ€åä¸€ä¸ªä»»åŠ¡æ¶‰åŠä½¿ç”¨LangChainè¿›è¡Œä¸€äº›RAGä»»åŠ¡ã€‚æˆ‘å‘ç°RAGä»»åŠ¡å¯¹äºå¼€æºæ¨¡å‹æ¥è¯´å¯èƒ½ç›¸å½“å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œé€šå¸¸éœ€è¦æˆ‘ç¼–å†™è‡ªå·±çš„æç¤ºå’Œè‡ªå®šä¹‰å“åº”è§£æå™¨æ¥å®ç°åŠŸèƒ½ã€‚ç„¶è€Œï¼Œæˆ‘å¸Œæœ›çœ‹åˆ°çš„æ˜¯ä¸€ä¸ªåœ¨æ ‡å‡†RAGä»»åŠ¡ä¸­â€œå¼€ç®±å³ç”¨â€çš„æ¨¡å‹ã€‚è¿™ä¸ª[åšå®¢æ–‡ç« ](https://medium.com/mlearning-ai/supercharging-large-language-models-with-langchain-1cac3c103b52)æä¾›äº†ä¸€äº›è¿™æ ·çš„ä»»åŠ¡ç¤ºä¾‹ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å®ƒçš„è¡¨ç°å¦‚ä½•ã€‚æˆ‘ä»¬è¦æå‡ºçš„é—®é¢˜æ˜¯ï¼š
- en: '***Who is the prime minister of the UK? Where was he or she born? How far is
    their birth place from London?***'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '***è°æ˜¯è‹±å›½çš„é¦–ç›¸ï¼Ÿä»–æˆ–å¥¹å‡ºç”Ÿåœ¨å“ªé‡Œï¼Ÿä»–ä»¬çš„å‡ºç”Ÿåœ°è·ç¦»ä¼¦æ•¦æœ‰å¤šè¿œï¼Ÿ***'
- en: '![](../Images/3aaf72deeacd3075e4d82c4bfdd1f528.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3aaf72deeacd3075e4d82c4bfdd1f528.png)'
- en: Image by author
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡
- en: This is, without a doubt, the best performance Iâ€™ve seen from an open-source
    model using the standard prompt from LangChain. This is probably unsurprising,
    considering OpenChat has been fine-tuned on ChatGPT conversations, and LangChain
    is tailored towards OpenAI models, particularly ChatGPT. Nonetheless, the model
    was capable of retrieving all three facts accurately using the tools at its disposal.
    The only shortcoming was that, in the end, the model failed to recognise that
    it possessed all the necessary information and could answer the userâ€™s question.
    Ideally, it should have stated, â€œI now have the final answer,â€ and provided the
    user with the facts it had gathered.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯‹åº¸ç½®ç–‘ï¼Œè¿™æ˜¯æˆ‘è§è¿‡çš„ä½¿ç”¨LangChainæ ‡å‡†æç¤ºçš„å¼€æºæ¨¡å‹ä¸­è¡¨ç°æœ€å¥½çš„ã€‚è¿™å¯èƒ½å¹¶ä¸ä»¤äººæƒŠè®¶ï¼Œå› ä¸ºOpenChatå·²ç»åœ¨ChatGPTå¯¹è¯ä¸Šè¿›è¡Œäº†å¾®è°ƒï¼Œè€ŒLangChainåˆ™é’ˆå¯¹OpenAIæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯ChatGPTã€‚ç„¶è€Œï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå‡†ç¡®åœ°ä½¿ç”¨å…¶å¯ç”¨çš„å·¥å…·æ£€ç´¢æ‰€æœ‰ä¸‰ä¸ªäº‹å®ã€‚å”¯ä¸€çš„ç¼ºç‚¹æ˜¯ï¼Œæœ€åæ¨¡å‹æœªèƒ½è®¤è¯†åˆ°å®ƒå·²ç»æŒæ¡äº†æ‰€æœ‰å¿…è¦çš„ä¿¡æ¯å¹¶èƒ½å¤Ÿå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œå®ƒåº”è¯¥è¯´ï¼šâ€œæˆ‘ç°åœ¨æœ‰äº†æœ€ç»ˆç­”æ¡ˆï¼Œâ€å¹¶å‘ç”¨æˆ·æä¾›å…¶æ”¶é›†åˆ°çš„äº‹å®ã€‚
- en: '![](../Images/dffb5124caaedb68a80ae033b5ab5478.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dffb5124caaedb68a80ae033b5ab5478.png)'
- en: Image by author
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡
- en: Conclusion
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: In this blog post, Iâ€™ve introduced you to three standard evaluation techniques
    that I use all the time to evaluate LLMs. Weâ€™ve observed that the new OpenChat
    model performs exceptionally well on all these tasks. Surprisingly, it appears
    very promising as the underlying LLM for a RAG application, probably just requiring
    customised prompting to discern when it has arrived at the final answer.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡åšå®¢æ–‡ç« ä¸­ï¼Œæˆ‘å‘ä½ ä»‹ç»äº†æˆ‘ç»å¸¸ä½¿ç”¨çš„ä¸‰ç§æ ‡å‡†è¯„ä¼°æŠ€æœ¯æ¥è¯„ä¼°LLMsã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œæ–°ç‰ˆçš„OpenChatæ¨¡å‹åœ¨æ‰€æœ‰è¿™äº›ä»»åŠ¡ä¸­è¡¨ç°éƒ½éå¸¸å‡ºè‰²ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå®ƒä½œä¸ºRAGåº”ç”¨çš„åº•å±‚LLMéå¸¸æœ‰å‰æ™¯ï¼Œå¯èƒ½åªéœ€å®šåˆ¶åŒ–æç¤ºæ¥ç¡®å®šä½•æ—¶è¾¾åˆ°äº†æœ€ç»ˆç­”æ¡ˆã€‚
- en: Itâ€™s noteworthy that this isnâ€™t a comprehensive evaluation, nor is it intended
    to be. Instead, it offers an indication of whether a particular model is worth
    investing more time in and conducting further, more intensive testing. It seems
    that OpenChat is definitely worth spending time on ğŸ¤—
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸æ˜¯ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°ï¼Œä¹Ÿä¸æ‰“ç®—æˆä¸ºå…¨é¢è¯„ä¼°ã€‚ç›¸åï¼Œå®ƒæä¾›äº†ä¸€ä¸ªå…³äºç‰¹å®šæ¨¡å‹æ˜¯å¦å€¼å¾—æŠ•å…¥æ›´å¤šæ—¶é—´è¿›è¡Œè¿›ä¸€æ­¥ã€æ›´æ·±å…¥æµ‹è¯•çš„æŒ‡ç¤ºã€‚çœ‹æ¥OpenChatç»å¯¹å€¼å¾—èŠ±æ—¶é—´ç ”ç©¶
    ğŸ¤—
- en: Feel free to use all the tools, expand and customise them, and start evaluating
    the LLMs that pique your interest within minutes.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: éšæ„ä½¿ç”¨æ‰€æœ‰å·¥å…·ï¼Œæ‰©å±•å’Œå®šåˆ¶å®ƒä»¬ï¼Œå¹¶åœ¨å‡ åˆ†é’Ÿå†…å¼€å§‹è¯„ä¼°ä½ æ„Ÿå…´è¶£çš„LLMsã€‚
- en: Heiko Hotz
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æµ·ç§‘Â·éœèŒ¨
- en: ğŸ‘‹ Follow me on [Medium](https://heiko-hotz.medium.com/) and [LinkedIn](https://www.linkedin.com/in/heikohotz/)
    to read more about Generative AI, Machine Learning, and Natural Language Processing.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ‘‹ åœ¨[Medium](https://heiko-hotz.medium.com/)å’Œ[LinkedIn](https://www.linkedin.com/in/heikohotz/)ä¸Šå…³æ³¨æˆ‘ï¼Œä»¥äº†è§£æ›´å¤šå…³äºç”Ÿæˆå¼äººå·¥æ™ºèƒ½ã€æœºå™¨å­¦ä¹ å’Œè‡ªç„¶è¯­è¨€å¤„ç†çš„å†…å®¹ã€‚
- en: ğŸ‘¥ If youâ€™re based in London join one of our [NLP London Meetups](https://www.meetup.com/nlp_london/).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ‘¥ å¦‚æœä½ åœ¨ä¼¦æ•¦ï¼Œå¯ä»¥åŠ å…¥æˆ‘ä»¬çš„[NLP London Meetups](https://www.meetup.com/nlp_london/)ã€‚
- en: '![](../Images/e787e5a8febdad2128756ce2a7e75fd4.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e787e5a8febdad2128756ce2a7e75fd4.png)'
- en: Image by author
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡
