- en: Supervised & Unsupervised Approach to Topic Modelling in Python
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python中的监督与非监督主题建模方法
- en: 原文：[https://towardsdatascience.com/supervised-unsupervised-approach-to-topic-modelling-in-python-d03e0b9da1dc](https://towardsdatascience.com/supervised-unsupervised-approach-to-topic-modelling-in-python-d03e0b9da1dc)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/supervised-unsupervised-approach-to-topic-modelling-in-python-d03e0b9da1dc](https://towardsdatascience.com/supervised-unsupervised-approach-to-topic-modelling-in-python-d03e0b9da1dc)
- en: Build a Topic Modelling Pipeline from Scratch in Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从头开始在Python中构建主题建模管道
- en: '[](https://vatsal12-p.medium.com/?source=post_page-----d03e0b9da1dc--------------------------------)[![Vatsal](../Images/f9648ff1f084b5b3361d90caf8c15959.png)](https://vatsal12-p.medium.com/?source=post_page-----d03e0b9da1dc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d03e0b9da1dc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d03e0b9da1dc--------------------------------)
    [Vatsal](https://vatsal12-p.medium.com/?source=post_page-----d03e0b9da1dc--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://vatsal12-p.medium.com/?source=post_page-----d03e0b9da1dc--------------------------------)[![Vatsal](../Images/f9648ff1f084b5b3361d90caf8c15959.png)](https://vatsal12-p.medium.com/?source=post_page-----d03e0b9da1dc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d03e0b9da1dc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d03e0b9da1dc--------------------------------)
    [Vatsal](https://vatsal12-p.medium.com/?source=post_page-----d03e0b9da1dc--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d03e0b9da1dc--------------------------------)
    ·11 min read·Jan 31, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d03e0b9da1dc--------------------------------)
    ·11分钟阅读·2023年1月31日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/0091d080160c43289a310e2c3495bcb5.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0091d080160c43289a310e2c3495bcb5.png)'
- en: Image taken from [Unsplash](https://unsplash.com/photos/c9OfrVeD_tQ) by [v2osk](https://unsplash.com/@v2osk)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 [Unsplash](https://unsplash.com/photos/c9OfrVeD_tQ) 由 [v2osk](https://unsplash.com/@v2osk)
- en: This article will provide a high level intuition behind topic modelling and
    its associated applications. It will do a deep dive into various ways one can
    approach solving a problem which requires topic modelling and how you can solve
    those problems in both a supervised and unsupervised manner. I placed an emphasis
    on restructuring the data and initial problem such that the solution can be executed
    in a variety of methods. The following table breaks down the contents in the article.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将提供关于主题建模及其相关应用的高层次直观理解。将深入探讨解决需要主题建模的问题的各种方法，以及如何在监督和非监督方式下解决这些问题。我强调了数据和初始问题的重构，以便可以通过多种方法执行解决方案。下表详细说明了本文的内容。
- en: '**Table of Contents**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**目录**'
- en: What is Topic Modelling?
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是主题建模？
- en: Applications of Topic Modelling
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主题建模的应用
- en: Supervised vs Unsupervised Learning
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习与非监督学习
- en: Problem Breakdown
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题拆解
- en: Requirements
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需求
- en: Data
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据
- en: '- Load Data'
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 加载数据'
- en: '- Cleaning & Preprocessing'
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 清洗与预处理'
- en: '- Data Statistics'
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 数据统计'
- en: Unsupervised Learning
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非监督学习
- en: '- Train Model'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 训练模型'
- en: '- Visualization'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 可视化'
- en: '- Topic Analysis'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 主题分析'
- en: Supervised Learning
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习
- en: '- Keyword Statistics'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 关键词统计'
- en: '- Generate Labels'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 生成标签'
- en: '- Train Model'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 训练模型'
- en: '- Evaluation'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 评估'
- en: Concluding Remarks
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结论
- en: Resources
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 资源
- en: What is Topic Modelling?
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是主题建模？
- en: Topic modelling is a subsection of natural language processing (NLP) or text
    mining which aims to build models in order to parse various bodies of text with
    the goal of identifying topics mapped to the text. These models assist in identifying
    big picture topics associated with documents at scale. It is a useful tool for
    understanding and organizing large collections of text data, and can help organizations
    make sense of large amounts of unstructured data.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 主题建模是自然语言处理（NLP）或文本挖掘的一个子领域，旨在建立模型以解析各种文本，以识别与文本相关的主题。这些模型有助于识别与文档相关的大致主题，适用于大规模的文档处理。它是理解和组织大量文本数据的有用工具，并能帮助组织理解大量非结构化数据。
- en: Applications of Topic Modelling
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主题建模的应用
- en: Document classification — Categorize documents into various topics
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档分类 — 将文档归类为各种主题
- en: '[Social media analysis](https://medium.com/towards-data-science/mining-modelling-character-networks-part-i-e37e4878c467)
    — Identify the major topics associated to posts by users on social media'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[社交媒体分析](https://medium.com/towards-data-science/mining-modelling-character-networks-part-i-e37e4878c467)
    — 识别用户在社交媒体上发帖的主要话题'
- en: '[Recommender systems](https://medium.com/towards-data-science/recommendation-systems-explained-a42fc60591ed)
    — Recommend products to users based on the topics that they’re interested in.
    A common application is customized advertisement recommendation based on the topics
    the user is interested in. For example, if a user was into cars, they might like
    advertisements from promising car brands like Honda / Toyota.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[推荐系统](https://medium.com/towards-data-science/recommendation-systems-explained-a42fc60591ed)
    — 根据用户感兴趣的主题推荐产品。一个常见的应用是根据用户感兴趣的主题推荐定制广告。例如，如果用户对汽车感兴趣，他们可能会喜欢来自像本田/丰田这样有前景的汽车品牌的广告。'
- en: Supervised vs Unsupervised Learning
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习与非监督学习
- en: There’s a clear distinction between supervised and unsupervised learning. Supervised
    learning is associated with training models given some label to map to the initial
    dataset. On the contrary, unsupervised learning is associated with training models
    given no labelled information present. Topic modelling is generally an unsupervised
    learning approach but this article will cover both a supervised and unsupervised
    learning approach to topic modelling.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习与非监督学习之间有明确的区别。监督学习涉及在给定标签的情况下训练模型以映射到初始数据集。相反，非监督学习涉及在没有标签信息的情况下训练模型。主题建模通常是非监督学习方法，但本文将涵盖监督学习和非监督学习方法的主题建模。
- en: The supervised learning approach will consist of binary classification. Binary
    classification is mapping the input data to exactly 2 targets, whereas multi-class
    classification is mapping the input data to more than 2 targets. The binary classification
    topic models will indicate whether the input article is or is not mapped to a
    topic we’ve labelled or not. The multi-class classification topic models will
    identify the topic this article is most likely to fall under given a set of topics.
    This article will showcase the implementation of the binary classification approach.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习方法将包括二分类。二分类是将输入数据映射到恰好 2 个目标，而多类分类是将输入数据映射到超过 2 个目标。二分类主题模型将指示输入文章是否映射到我们已标记的主题中。多类分类主题模型将识别该文章最有可能归属的主题。本文将展示二分类方法的实现。
- en: Problem Breakdown
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题分析
- en: The problem this article is aiming to solve is to identify the main topics associated
    with research papers given the summary of the paper. Based on the topics identified,
    the user can then infer whether this paper is of interest to them or not. We will
    be using the arXiv database to query and fetch several research papers across
    a variety of domains.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 本文旨在解决的问题是，给定论文的摘要，识别与之相关的主要主题。根据识别出的主题，用户可以推断这篇论文是否对他们感兴趣。我们将使用 arXiv 数据库查询并获取多个领域的研究论文。
- en: Requirements
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 需求
- en: The following are the required modules and versions necessary to follow along
    with this tutorial. The version of Python in my environment is `3.10.0.` If an
    error does occur during the execution process, be mindful of the versions associated
    with the modules you’re referencing as this is commonly a problem in collaboration
    across platforms.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是跟随本教程所需的模块及版本。我的环境中的 Python 版本是 `3.10.0.` 如果执行过程中发生错误，请注意您引用的模块的版本，因为这是跨平台协作中的常见问题。
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you don’t have the gensim package installed, [here](https://pypi.org/project/gensim/)
    is the library documentation to install it through the command line. Similarly,
    you can install the arXiv package in Python with the following instructions [here](https://pypi.org/project/arxiv/).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您尚未安装 gensim 包，[这里](https://pypi.org/project/gensim/) 是通过命令行安装它的库文档。类似地，您可以按照以下说明在
    Python 中安装 arXiv 包，[这里](https://pypi.org/project/arxiv/) 是相关文档。
- en: Data
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据
- en: Based on the arXiv terms of use for their API it is completely free and encouraged
    to use. For more information regarding their terms of use, please reference their
    documentation which you can find [here](https://arxiv.org/help/api/tou).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 arXiv 的 API 使用条款，它是完全免费的，并且鼓励使用。有关使用条款的更多信息，请参考其文档，您可以在[这里](https://arxiv.org/help/api/tou)找到。
- en: In this article, I will show you how to hit the API through Python to collect
    the following information necessary for the models we’re building today. If you
    want to hit this API through other programming languages, or just want more information
    on how to use the API, I highly encourage you to reference their documentation
    which you can find [here](https://arxiv.org/help/api/user-manual).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我将展示如何通过 Python 访问 API 来收集构建今天模型所需的以下信息。如果你想通过其他编程语言访问这个 API，或者想了解如何使用
    API 的更多信息，我强烈建议你参考他们的文档，你可以在[这里](https://arxiv.org/help/api/user-manual)找到。
- en: Load Data
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载数据
- en: The code provided below will consist of importing the required modules, setting
    up constants to be used throughout the project and defining a function to query
    and load data from arXiv given a set of prompts.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 下列代码包括导入所需模块、设置在整个项目中使用的常量以及定义一个函数，通过一组提示从 arXiv 查询和加载数据。
- en: 'That script should yield a resulting pandas DataFrame looking similar to the
    screenshot below:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本应生成一个类似于下方截图的 pandas DataFrame：
- en: '![](../Images/bea47659e5184e1ea335c1b17e122a63.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bea47659e5184e1ea335c1b17e122a63.png)'
- en: Data queried from arXiv. Image provided by the author.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从 arXiv 查询的数据。图像由作者提供。
- en: Cleaning & Preprocessing
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 清理与预处理
- en: Now we can clean and preprocess the summary information associated with each
    article. The cleaning and preprocessing phase of working with text data is essential
    in optimizing the performance of the underlying models. The lower the quality
    of data you feed into the model, the lower the performance of it will be in production
    settings. Furthermore, the amount of data you clean, preprocess and reduce will
    impact the training and inference time associated with the model. This will overall
    improve the experiments you run and performance in production. Topic modelling
    algorithms rely on the frequency of words within a document to identify patterns
    and topics, so any irrelevant information passed in can skew the results.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以清理和预处理与每篇文章相关的摘要信息。处理文本数据的清理和预处理阶段对于优化底层模型的性能至关重要。你喂给模型的数据质量越低，模型在生产环境中的性能也会越低。此外，你清理、预处理和减少的数据量将影响模型的训练和推理时间。这将整体提高你运行的实验和生产中的性能。话题建模算法依赖于文档中词语的频率来识别模式和主题，因此任何传入的无关信息都可能扭曲结果。
- en: 'The text preprocessing we will be doing will consist of the following:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将进行的文本预处理包括以下内容：
- en: Unicode the input data. This is critical when working with data in different
    languages. It will convert `à` into `á` , this will be critical during the cleaning
    phase.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对输入数据进行 Unicode 编码。这在处理不同语言的数据时至关重要。它会将`à`转换为`á`，这在清理阶段非常关键。
- en: Lowering the text such that all upper case characters are now lower case.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文本转换为小写，使所有大写字符都变为小写。
- en: 'The text cleaning we will be doing will consist of the following:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将进行的文本清理包括以下内容：
- en: Removing punctuations
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除标点符号
- en: Removing stop words
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除停用词
- en: When removing stop words, be mindful of the data you’re working with. The reason
    you would want to remove stop words is because they don’t provide any new information
    and it aids in optimizing the performance of the model. The instances where you
    wouldn’t want to remove stop words is when the context around the sentence matters.
    Not removing stop words would be useful for things like sentiment analysis and
    summarization. However, for our use case of topic modelling, we can proceed with
    removal of stop words.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在删除停用词时，请注意你正在处理的数据。你想删除停用词的原因是因为它们不会提供任何新信息，并且有助于优化模型的性能。不想删除停用词的情况是当句子周围的上下文很重要时。不删除停用词对于情感分析和摘要等任务很有用。然而，对于我们的话题建模用例，我们可以继续删除停用词。
- en: The output of the cleaned data should yield a new column called `cleaned_summary`
    . The resulting dataset should look something similar to the image shown below.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 清理后的数据输出应生成一个名为`cleaned_summary`的新列。结果数据集应类似于下图所示的样子。
- en: '![](../Images/669bb78975b4e0893e2b32ee9e2281cf.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/669bb78975b4e0893e2b32ee9e2281cf.png)'
- en: Transformed initial dataset through cleaning and preprocessing of the summary
    column. Image provided by the author.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 通过清理和预处理摘要列转换初始数据集。图像由作者提供。
- en: Data Statistics
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据统计
- en: Now let’s investigate the word count breakdown associated with the cleaned dataset
    and identify the underlying distribution associated with the word count.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们调查与清理后的数据集相关的词频分布，并识别与词频相关的潜在分布。
- en: '![](../Images/fdbeac5399acbb8670f8d5b5b7f5b1fd.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fdbeac5399acbb8670f8d5b5b7f5b1fd.png)'
- en: Word count distribution on the cleaned summaries associated with a sample of
    research papers from arXiv. Image provided by the author.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 基于从 arXiv 中的研究论文样本的清理总结的字数分布。图片由作者提供。
- en: Based on this, it seems that out of the 1548 sample of articles we queried from
    arXiv, ~700 of them have fewer than 100 words. This corresponds to 45.1% of the
    data having fewer than 100 words.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此，我们从 arXiv 查询的 1548 篇文章中，大约有 700 篇文章少于 100 个字。这对应于 45.1% 的数据少于 100 个字。
- en: Unsupervised Learning
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: We will be using LDA as the topic modelling algorithm in Python for the unsupervised
    learning approach associated with identifying the topics of research papers. LDA
    is a common approach to topic modelling and is the same approach large organizations
    like AWS provide as a service when using their `Comprehend` tool. This approach
    will essentially outline the backend code AWS would be using to process documents
    and generate topics for each of them in an unsupervised approach. At least this
    way, you won’t have to pay for it (aside from computing costs — which depends
    on the quantity of data you’re working with).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 LDA 作为 Python 中无监督学习方法的主题建模算法，用于识别研究论文的主题。LDA 是一种常见的主题建模方法，大型组织如 AWS 提供的
    `Comprehend` 工具也使用这种方法。这种方法将基本展示 AWS 用于处理文档并以无监督方式生成每个主题的后台代码。至少这样，你就不必为此付费（除了计算成本——这取决于你处理的数据量）。
- en: Train Model
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练模型
- en: Visualization
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化
- en: Now that we have the model object associated with the data we prepared to train
    in it. We can create a few unique visualizations which would help provide insights
    associated to the topics and keywords the model has identified for each article.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拥有与我们准备训练的数据相关的模型对象。我们可以创建几个独特的可视化，这将有助于提供与模型为每篇文章识别的主题和关键词相关的见解。
- en: We’re going to be using the `pyLDAvis` library for the following visualization.
    Be aware that more recent versions of this library doesn’t support the visualization
    in JupyterNotebooks. I highly encourage this module to be installed with the specific
    version `2.1.2` as outlined in the requirements section of this article. [This](https://stackoverflow.com/questions/66096149/pyldavis-visualization-from-gensim-not-displaying-the-result-in-google-colab)
    thread on Stack Overflow highlights the difficulty of generating this visualization
    on different versions.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `pyLDAvis` 库进行以下可视化。请注意，这个库的较新版本不支持在 JupyterNotebooks 中进行可视化。我强烈建议安装文章要求部分中提到的特定版本
    `2.1.2`。[这个](https://stackoverflow.com/questions/66096149/pyldavis-visualization-from-gensim-not-displaying-the-result-in-google-colab)
    Stack Overflow 线程突出了在不同版本中生成此可视化的难度。
- en: '![](../Images/a6faac50bb440006f13625959d71a76d.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a6faac50bb440006f13625959d71a76d.png)'
- en: LDA topic visualized for top 30 most frequent terms per topic. Image provided
    by the author.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: LDA 主题可视化，显示每个主题的前 30 个最频繁的术语。图片由作者提供。
- en: '![](../Images/cb4ae2e933746d1f181ea0ed7ab7df74.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb4ae2e933746d1f181ea0ed7ab7df74.png)'
- en: Word cloud of the topics identified by the LDA model. Image provided by the
    author.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: LDA 模型识别的主题词云。图片由作者提供。
- en: There are 10 word cloud images created by the script above, but only 2 are showcased
    in this article. As you can see that there is quite a bit of overlap between the
    topics (namely with terms like model and models). Based on this it can be seen
    that further preprocessing and cleaning would be required to take the stem of
    a word, remove further stop words like `use, show, first, also, may, one, number,
    etc...` . The model development process is an iterative one but this highly outlines
    the importance of having high quality data being fed into the model.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 通过上述脚本创建了 10 个词云图像，但本文仅展示了 2 个。如你所见，主题之间存在相当大的重叠（例如模型和模型等术语）。基于此，可以看出需要进一步预处理和清理，例如提取词干，去除像
    `use, show, first, also, may, one, number, etc...` 这样的停用词。模型开发过程是一个迭代过程，但这高度突显了将高质量数据输入模型的重要性。
- en: Regardless of that we can also see that the two topics identified by this approach
    are quite unique. The first topic seems to dive into things centered around quantum
    computing and deep learning whereas the second topic is centered around machine
    learning, automated machine learning and data.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，我们也可以看到，采用这种方法识别的两个主题是相当独特的。第一个主题似乎深入探讨了围绕量子计算和深度学习的内容，而第二个主题则集中于机器学习、自动化机器学习和数据。
- en: Topic Analysis
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主题分析
- en: '![](../Images/963f7a02cd564d5bad536b08e13c9c86.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/963f7a02cd564d5bad536b08e13c9c86.png)'
- en: Frequency of learned topics with the threshold greater than 0.3\. Image provided
    by the author.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 学习到的主题频率，阈值大于0.3。图像由作者提供。
- en: It seems that a majority of the articles which the model was trained on fall
    under the first and fifth topics.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来模型训练所用的大多数文章属于第一个和第五个主题。
- en: '![](../Images/ac19bdc5499848dcb39359d444b5baba.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ac19bdc5499848dcb39359d444b5baba.png)'
- en: Term frequency of the top 30 words. Image provided by the author.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 前30个词的词频。图像由作者提供。
- en: Clearly, based on this it seems that further data cleaning and preprocessing
    can be done. As `data, model and models` are the most frequent terms, we wouldn’t
    want the model to be influenced by these words as they’re not distinctive enough.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，根据这些结果，似乎可以进行进一步的数据清理和预处理。由于`数据、模型和模型`是最常见的术语，我们不希望模型受到这些词的影响，因为它们不够具有区分性。
- en: '![](../Images/00b86ede10472f6df671802522dbdae0.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/00b86ede10472f6df671802522dbdae0.png)'
- en: Top relevant keywords and document count associated with each topic. Image provided
    by the author.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 与每个主题相关的顶级关键词及其文档计数。图像由作者提供。
- en: As corroborated by the two images prior, the most popular topics being predicted
    are 5 and 0\. Both topics use words like `model` and `data` which should be removed
    on further iteration. This was the first iteration of the model development process
    for this approach. The model which goes into production will never be the first
    model you train, it is imperative to take the results of models from previous
    iterations to influence the changes required for models in future iterations.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如前两张图所证实，最常预测的主题是5和0。这两个主题使用了诸如`模型`和`数据`之类的词汇，这些词在进一步迭代中应该被移除。这是该方法的模型开发过程的第一次迭代。投入生产的模型永远不会是你训练的第一个模型，必须利用前几次迭代模型的结果来影响未来迭代中模型所需的变化。
- en: Supervised Learning
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习
- en: The supervised learning approach to topic modelling will consist of generating
    topic labels to train a binary classification model. This can be done by identifying
    the keywords associated to topics we are interested in labelling and predicting.
    I will mainly focus on the three topics of `machine learning, nlp (natural language
    processing) and mathematics` .
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 主题建模的监督学习方法将包括生成主题标签来训练一个二分类模型。这可以通过识别我们感兴趣的标记和预测的主题的相关关键词来完成。我将主要关注`机器学习、自然语言处理（NLP）和数学`这三个主题。
- en: This is the set of keywords I’ve identified per topic. By no means is this list
    exhaustive but it is good enough to begin with.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我为每个主题识别出的关键词集。这个列表绝不是详尽无遗，但足以作为起点。
- en: '[PRE1]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We can parse through the cleaned summary associated with each article and identify
    which summaries had any keywords which we’re interested in and link it back to
    the original topic those keywords were mapped to. This will provide us with labels
    for each of the topics above. We can use TF-IDF to transform the input summary
    into a vector corresponding to the articles being fed into the model.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以解析与每篇文章相关的清理后的摘要，识别出包含我们感兴趣的关键词的摘要，并将其链接回这些关键词所映射的原始主题。这将为上述每个主题提供标签。我们可以使用TF-IDF将输入摘要转换为与输入模型的文章相对应的向量。
- en: Keyword Statistics
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关键词统计
- en: '![](../Images/707c7bb722e05073ec95c4ba44e7b420.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/707c7bb722e05073ec95c4ba44e7b420.png)'
- en: Count of articles with the corresponding keyword counts. Image provided by the
    author.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 具有相应关键词计数的文章计数。图像由作者提供。
- en: '![](../Images/a43d4a9d2d90131d1edd1a6c503aeec3.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a43d4a9d2d90131d1edd1a6c503aeec3.png)'
- en: Frequency of keyword occurrences throughout the articles. Image provided by
    the author.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 文章中关键词出现的频率。图像由作者提供。
- en: '**Generate Labels**'
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**生成标签**'
- en: '![](../Images/5aa80bb712fe013c6a71af2a21e86d4f.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5aa80bb712fe013c6a71af2a21e86d4f.png)'
- en: From the 1548 documents we are working with, given the keywords associated to
    the topics defined above, this is the count of documents which have a positive
    label for the corresponding topic. Image provided by the author.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们处理的1548篇文档中，根据上述定义的主题相关关键词，这是拥有正标签的文档计数。图像由作者提供。
- en: '![](../Images/98637e570aa4bd2378d74e278f1c5722.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/98637e570aa4bd2378d74e278f1c5722.png)'
- en: The corresponding data frame after the label generation. Image provided by the
    author.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 标签生成后的数据框。图像由作者提供。
- en: '**Train Model**'
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**训练模型**'
- en: '![](../Images/73b167a06f40d8ff8ee0898a10107608.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/73b167a06f40d8ff8ee0898a10107608.png)'
- en: The script above will yield the following sklearn pipeline corresponding to
    the cleaned summaries and labels we’ve generated above. Image provided by the
    author.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 上述脚本将生成以下 sklearn 管道，对应于我们上面生成的清理后的摘要和标签。图片由作者提供。
- en: Evaluation
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评价
- en: Since we generated a holdout set during the training phase, we can now pass
    in the trained models against the holdout set to identify the performance of the
    model. Be advised that given that we’re working with a small sample of data with
    a class imbalance, there is a high likelihood that the trained model will be overfit.
    This can be resolved fairly easily (in our case) by increasing the number of articles
    we label and train the model on. All this means is that we should query arXiv
    for a larger dataset and generate better keywords and for labelling the articles.
    This might not be an easy issue for you to resolve if you’re working with a different
    dataset.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在训练阶段生成了一个保留集，我们现在可以将训练好的模型应用于保留集，以识别模型的性能。请注意，由于我们处理的是一个小样本数据且存在类别不平衡，训练模型很可能会过拟合。这可以通过增加我们标记的文章数量并用来训练模型来比较容易解决。这意味着我们应该查询
    arXiv 以获取更大的数据集，并生成更好的关键词和标记文章。如果你处理的是不同的数据集，这可能不是一个容易解决的问题。
- en: I would also highly encourage you to try out multiple classification models
    and not just the gradient boosting classifier. As stated before, iteration is
    an integral part of the machine learning development cycle!
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我也强烈建议你尝试多个分类模型，而不仅仅是梯度提升分类器。如前所述，迭代是机器学习开发周期的一个重要部分！
- en: Concluding Remarks
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: This article aimed to be a tutorial for the reader with the intention of providing
    both a supervised and unsupervised learning approach to topic modelling. I hope
    that I was able to outline the change in mindset when looking at the underlying
    data and how that can impact and broaden the approach to solving a particular
    problem.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章旨在为读者提供一个教程，旨在提供监督学习和无监督学习两种主题建模方法。我希望我能够概述在查看底层数据时思维方式的变化，以及这种变化如何影响和拓宽解决特定问题的方法。
- en: I also hope that this article outlined the importance of iteration in machine
    learning. The model which goes into production will never be the first model you
    train, it is imperative to take the results of models from previous iterations
    to influence the changes required for models in future iterations.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我还希望这篇文章概述了机器学习中迭代的重要性。投入生产的模型永远不会是你训练的第一个模型，重要的是利用之前迭代模型的结果来影响未来迭代中模型所需的变更。
- en: I hope that it is also clear that the results of the unsupervised learning approach
    can influence the supervised learning approach. It could also bring forth a semi-supervised
    learning approach to topic modelling where you train a binary classification model
    on the results of the LDA model.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望也能清楚的是，无监督学习方法的结果可以影响监督学习方法。这也可能引发一种半监督学习方法来进行主题建模，你可以在 LDA 模型的结果上训练一个二分类模型。
- en: If you want to download the jupyter notebook associated with this tutorial,
    I have provided it [here](https://github.com/vatsal220/medium_articles/blob/main/topic_modelling/topic_model.ipynb).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想下载与本教程相关的 jupyter notebook，我已经在[这里](https://github.com/vatsal220/medium_articles/blob/main/topic_modelling/topic_model.ipynb)提供了它。
- en: Resources
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源
- en: '[https://en.wikipedia.org/wiki/Topic_model](https://en.wikipedia.org/wiki/Topic_model)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Topic_model](https://en.wikipedia.org/wiki/Topic_model)'
- en: '[https://docs.aws.amazon.com/comprehend/latest/dg/topic-modeling.html](https://docs.aws.amazon.com/comprehend/latest/dg/topic-modeling.html)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.aws.amazon.com/comprehend/latest/dg/topic-modeling.html](https://docs.aws.amazon.com/comprehend/latest/dg/topic-modeling.html)'
- en: If you enjoyed reading through the article I wrote today, here are a few others
    I’ve written around the topic of natural language processing which you might also
    enjoy!
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢今天我写的文章，这里还有一些我写的关于自然语言处理的其他文章，你可能也会喜欢！
- en: '[](/text-similarity-w-levenshtein-distance-in-python-2f7478986e75?source=post_page-----d03e0b9da1dc--------------------------------)
    [## Text Similarity w/ Levenshtein Distance in Python'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/text-similarity-w-levenshtein-distance-in-python-2f7478986e75?source=post_page-----d03e0b9da1dc--------------------------------)
    [## Python 中的文本相似度与 Levenshtein 距离'
- en: Building a Plagiarism Detection Pipeline in Python
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Python 构建剽窃检测管道
- en: towardsdatascience.com](/text-similarity-w-levenshtein-distance-in-python-2f7478986e75?source=post_page-----d03e0b9da1dc--------------------------------)
    [](/word2vec-explained-49c52b4ccb71?source=post_page-----d03e0b9da1dc--------------------------------)
    [## Word2Vec Explained
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/text-similarity-w-levenshtein-distance-in-python-2f7478986e75?source=post_page-----d03e0b9da1dc--------------------------------)
    [](/word2vec-explained-49c52b4ccb71?source=post_page-----d03e0b9da1dc--------------------------------)
    [## 解释 Word2Vec
- en: Explaining the Intuition of Word2Vec & Implementing it in Python
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解释 Word2Vec 的直觉及其在 Python 中的实现
- en: towardsdatascience.com](/word2vec-explained-49c52b4ccb71?source=post_page-----d03e0b9da1dc--------------------------------)
    [](/text-summarization-in-python-with-jaro-winkler-and-pagerank-72d693da94e8?source=post_page-----d03e0b9da1dc--------------------------------)
    [## Text Summarization in Python with Jaro-Winkler and PageRank
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/word2vec-explained-49c52b4ccb71?source=post_page-----d03e0b9da1dc--------------------------------)
    [](/text-summarization-in-python-with-jaro-winkler-and-pagerank-72d693da94e8?source=post_page-----d03e0b9da1dc--------------------------------)
    [## 使用 Jaro-Winkler 和 PageRank 进行 Python 文本摘要
- en: Building a Text Summarizer with Jaro-Winkler and PageRank
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Jaro-Winkler 和 PageRank 构建文本摘要器
- en: towardsdatascience.com](/text-summarization-in-python-with-jaro-winkler-and-pagerank-72d693da94e8?source=post_page-----d03e0b9da1dc--------------------------------)
    [](/identifying-tweet-sentiment-in-python-7c37162c186b?source=post_page-----d03e0b9da1dc--------------------------------)
    [## Identifying Tweet Sentiment in Python
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/text-summarization-in-python-with-jaro-winkler-and-pagerank-72d693da94e8?source=post_page-----d03e0b9da1dc--------------------------------)
    [](/identifying-tweet-sentiment-in-python-7c37162c186b?source=post_page-----d03e0b9da1dc--------------------------------)
    [## 用 Python 识别推文情感
- en: How to use Tweepy and Textblob to Identify Tweet Sentiment
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何使用 Tweepy 和 Textblob 识别推文情感
- en: towardsdatascience.com](/identifying-tweet-sentiment-in-python-7c37162c186b?source=post_page-----d03e0b9da1dc--------------------------------)
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/identifying-tweet-sentiment-in-python-7c37162c186b?source=post_page-----d03e0b9da1dc--------------------------------)
