- en: The Meaning Behind Logistic Classification, from Physics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从物理学看 *logistic* 分类的意义
- en: 原文：[https://towardsdatascience.com/the-meaning-behind-logistic-classification-from-physics-291774fda579](https://towardsdatascience.com/the-meaning-behind-logistic-classification-from-physics-291774fda579)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-meaning-behind-logistic-classification-from-physics-291774fda579](https://towardsdatascience.com/the-meaning-behind-logistic-classification-from-physics-291774fda579)
- en: In classification problems, why do we use the logistic and softmax functions?
    Thermal physics may have an answer.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在分类问题中，我们为什么使用 *logistic* 和 *softmax* 函数？热物理学也许有答案。
- en: '[](https://tim-lou.medium.com/?source=post_page-----291774fda579--------------------------------)[![Tim
    Lou, PhD](../Images/e4931bb6d59e27730529ceaf00a23822.png)](https://tim-lou.medium.com/?source=post_page-----291774fda579--------------------------------)[](https://towardsdatascience.com/?source=post_page-----291774fda579--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----291774fda579--------------------------------)
    [Tim Lou, PhD](https://tim-lou.medium.com/?source=post_page-----291774fda579--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://tim-lou.medium.com/?source=post_page-----291774fda579--------------------------------)[![Tim
    Lou, PhD](../Images/e4931bb6d59e27730529ceaf00a23822.png)](https://tim-lou.medium.com/?source=post_page-----291774fda579--------------------------------)[](https://towardsdatascience.com/?source=post_page-----291774fda579--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----291774fda579--------------------------------)
    [Tim Lou, PhD](https://tim-lou.medium.com/?source=post_page-----291774fda579--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----291774fda579--------------------------------)
    ·9 min read·Jan 24, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----291774fda579--------------------------------)
    ·阅读时间 9 分钟·2023 年 1 月 24 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/506d63dd0efa1ff62f42cfd27cce22b1.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/506d63dd0efa1ff62f42cfd27cce22b1.png)'
- en: Logistic regression is ubiquitous, but what’s the intuition behind why it works?
    (Image by Author)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*Logistic* 回归无处不在，但它为什么有效的直觉是什么？（图片由作者提供）'
- en: Logistic regression is perhaps the most popular and well-known machine learning
    model. It solves the binary classification problem — for predicting whether a
    data point belongs to a category.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*Logistic* 回归也许是最受欢迎和最知名的机器学习模型。它解决了二分类问题——预测数据点是否属于某个类别。'
- en: 'The key ingredient is the the *logistic* function, where an input is converted
    into a probability, written in the form:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 关键成分是 *logistic* 函数，其中一个输入被转换为概率，形式如下：
- en: '![](../Images/69180f3741a9fd0e59094ceb8a327de0.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/69180f3741a9fd0e59094ceb8a327de0.png)'
- en: But why does the exponential make an appearance? What is the intuition behind
    it, beyond just converting a real number into a probability?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 但是为什么指数函数会出现？它背后的直觉是什么，除了将一个实数转换为概率之外？
- en: It turns out, thermal physics has an answer. But before digging into insights
    from physics, let’s understand the mathematics first.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 结果发现，热物理学有答案。但在深入物理学的见解之前，让我们首先了解数学部分。
- en: Mathematical Quandary
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数学难题
- en: Before tackling the “why” behind the logistic function, let’s understand its
    properties first.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理 *logistic* 函数的“为什么”之前，让我们首先理解它的属性。
- en: '![](../Images/f4ca81396932f382d9d5bf1369094b35.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f4ca81396932f382d9d5bf1369094b35.png)'
- en: Time to get the math out of the way! (Photo by [Michal Matlon](https://unsplash.com/@michalmatlon?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral))
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 该是让数学见鬼去吧的时候了！（图片由 [Michal Matlon](https://unsplash.com/@michalmatlon?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)）
- en: It is helpful to understand the logistic function in a more generalized form
    — one that is applicable for multiple categories — so that the logistic function
    becomes a special two-category example.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 理解 *logistic* 函数的一种更广泛的形式是有帮助的——这种形式适用于多个类别——这样 *logistic* 函数就成为了一个特殊的两类示例。
- en: 'This can be done by introducing multiple extra variables *zᵢ*, one for each
    category. We then arrive at the *softmax* function, where each class *i* is assigned
    a probability:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过引入多个额外变量 *zᵢ* 来完成，每个类别一个。然后我们得到 *softmax* 函数，其中每个类别 *i* 被分配一个概率：
- en: '![](../Images/f33e64283cb7da68dcdb54fa61f49aa9.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f33e64283cb7da68dcdb54fa61f49aa9.png)'
- en: 'For linear models, the *zᵢ’s* are generally linear sums of features. This function
    has a lot of extra utilities beyond linear models:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于线性模型，*zᵢ* 通常是特征的线性和。这个函数在超出线性模型的情况下还有很多额外的用途：
- en: as the final classification layer in a neural network
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为神经网络中的最终分类层
- en: as the attention weights in a transformer
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为变压器中的注意力权重
- en: as the sampling layer for choosing an action in reinforcement learning
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为强化学习中选择行动的采样层
- en: 'But why do we choose this functional form for parameterizing probabilities?
    The common explanation is that it is simply a conversion tool. Indeed, any sets
    of probabilities, as long as none of them are zero, can be written in this form.
    Mathematically, we can always solve for *zᵢ* by simply taking logarithms:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 但为什么我们选择这种函数形式来参数化概率？常见的解释是它仅仅是一个转换工具。事实上，只要其中没有一个概率为零，任何概率集合都可以写成这种形式。从数学上讲，我们可以通过简单地取对数来求解
    *zᵢ*：
- en: '![](../Images/a12e51febb50d569d1cc14cdcb826748.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a12e51febb50d569d1cc14cdcb826748.png)'
- en: 'But is this the only special property of the softmax, as a conversion tool?
    Not quite, because there are infinitely many choices for converting numbers into
    probabilities. In fact we can consider functions of other form:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 但这就是 softmax 作为转换工具的唯一特殊属性吗？其实不是，因为将数字转换为概率的选择是无穷无尽的。实际上，我们可以考虑其他形式的函数：
- en: '![](../Images/311bfbac89d5f24a0af3c88acc89afdc.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/311bfbac89d5f24a0af3c88acc89afdc.png)'
- en: 'To produce a sensible probability function for optimization, we require the
    following criterion on *f*:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了产生一个合理的优化概率函数，我们需要对 *f* 满足以下标准：
- en: Always positive, always increasing, ranges from 0 to ∞, and differentiable
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 始终为正，始终递增，范围从 0 到 ∞，并且可微分
- en: 'So exp(*z*) is just one of infinitely many choices, as can be seen below:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，exp(*z*) 只是无数选择中的一种，下面可以看到：
- en: '![](../Images/4c57e953c55ad9ace7354fe524663244.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c57e953c55ad9ace7354fe524663244.png)'
- en: Alternatives to exp(z) that can also provide reasonable conversions from numbers
    to probabilities (Image by Author)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: exp(z) 的替代方案也可以合理地将数字转换为概率（图像来源：作者）
- en: So is exp(*z*) just a matter of conventions out of a sea of possibilities? Should
    exp(*z*) be preferable? While we could simply justify this choice by model performance,
    we should strive for understanding whenever possible. In this case, there is a
    theoretical motivation, and it comes from thermal physics.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 那么 exp(*z*) 是否只是海量可能性中的一个约定？exp(*z*) 是否应该更可取？虽然我们可以通过模型性能简单地证明这个选择，但我们应尽可能追求理解。在这种情况下，存在理论动机，它来自于热物理学。
- en: A Thermal Physics Link
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个热物理学链接
- en: 'It turns out, these probabilities are ubiquitous in thermal physics, they are
    called the [Boltzmann distribution](https://en.wikipedia.org/wiki/Boltzmann_distribution),
    which describes the probability of a particle (or a system more generally) to
    be in a particular energy state:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，这些概率在热物理学中无处不在，它们被称为 [玻尔兹曼分布](https://en.wikipedia.org/wiki/Boltzmann_distribution)，它描述了一个粒子（或更广泛地说，一个系统）处于特定能量状态的概率：
- en: '![](../Images/93a0186f7ebc8b4f5befd16f8441e39f.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/93a0186f7ebc8b4f5befd16f8441e39f.png)'
- en: Where *Eᵢ* denotes the energy of the state and *T* the temperature. When the
    temperature is non-zero, one can measure energy as multiples of temperature so
    that *T* can be conveniently set to 1.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *Eᵢ* 表示状态的能量，*T* 表示温度。当温度非零时，可以将能量测量为温度的倍数，以便将 *T* 方便地设置为1。
- en: The Boltzmann distribution is a bit confusing though, as they model systems
    that are governed by precise and deterministic equations, so how do probability
    and statistics come into the picture (ignoring quantum physics)?
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管玻尔兹曼分布有点令人困惑，因为它们建模的系统是由精确和确定性的方程支配的，那么概率和统计学是如何融入其中的（忽略量子物理学）？
- en: '![](../Images/7d4e959534b2553a09e2fa27a372e4a0.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7d4e959534b2553a09e2fa27a372e4a0.png)'
- en: Thermal physics uses probability to manage our ignorance in the complex world
    (Photo by [Rainer Gelhot](https://unsplash.com/@rynaehr?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral))
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 热物理学利用概率来管理我们在复杂世界中的无知（照片来源：[Rainer Gelhot](https://unsplash.com/@rynaehr?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)）
- en: The crux is complexity. While we have equations that describe the details of
    a system, they tend to be very complicated. Additionally, these equations often
    exhibit chaotic behaviors, leading to high unpredictability (the [Butterfly effect](https://en.wikipedia.org/wiki/Butterfly_effect)).
    So practically, these detailed deterministic equations are not so useful.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 关键在于复杂性。虽然我们有描述系统细节的方程，它们往往非常复杂。此外，这些方程常常表现出混沌行为，导致高度不可预测性（[蝴蝶效应](https://en.wikipedia.org/wiki/Butterfly_effect)）。因此，实际上，这些详细的确定性方程并不是很有用。
- en: 'How do we understand these complex systems then? Luckily in real life, we rarely
    need to know about the microscopic details of a system, as we cannot measure them
    anyway. It is often sufficient for us to consider macroscopic and emergent quantities
    (like temperature, energy, or entropy). This is where probability theory comes
    in — it is a bridge between the micro and the macro:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们如何理解这些复杂系统呢？幸运的是，在现实生活中，我们很少需要了解系统的微观细节，因为我们无论如何都无法测量它们。通常，我们只需考虑宏观和涌现量（如温度、能量或熵）。这就是概率理论的作用——它是微观与宏观之间的桥梁：
- en: Microscopic details are modeled using probability distributions
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微观细节使用概率分布来建模
- en: Macroscopic quantities are modeled as various averages of these distributions
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 宏观量被建模为这些分布的各种平均值
- en: For an analogy, imagine we want to study all the digits of some irrational numbers,
    say √2, *π* and *e*
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以一个类比来说，假设我们想研究某些无理数的所有数字，比如√2、*π*和*e*。
- en: √2 = 1.41421356237309504880…
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: √2 = 1.41421356237309504880…
- en: '*π =* 3.14159265358979323846…'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*π =* 3.14159265358979323846…'
- en: '*e* = 2.71828182845904523536…'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*e* = 2.71828182845904523536…'
- en: The task seems daunting, as each number arises from a different mathematical
    concept. To get the precise digits would require specific numerical methods to
    compute them. However, if we simply look at *macroscopic* behaviors of the digits,
    we’ll easily find that each of the 10 digits appear roughly 10% of the time. The
    crux is that these digits, like many dynamical systems, tend to explore all the
    possibilities without prejudice. In more technical terms,
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 任务似乎很艰巨，因为每个数字来自不同的数学概念。要获得精确的数字，需要特定的数值方法来计算。然而，如果我们简单地观察这些数字的*宏观*行为，我们会发现每个数字大约出现10%的时间。关键在于，这些数字像许多动态系统一样，倾向于无偏见地探索所有可能性。从更技术的角度来说，
- en: Chaotic systems tend to maximize all the possibilities, or in other words, entropy
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 混沌系统倾向于最大化所有可能性，换句话说，就是熵。
- en: 'What does this have to do with our Boltzmann distribution? Well, mathematically
    the Boltzmann distribution is a *maximum entropy* distribution, under the constraint
    that the statistics it approximates respects a key physical law — the conservation
    of energy. In other words, the Boltzmann distribution is the solution to:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这和我们的玻尔兹曼分布有什么关系呢？数学上，玻尔兹曼分布是一个*最大熵*分布，在约束条件下，这些统计数据符合一个关键的物理法则——能量守恒。换句话说，玻尔兹曼分布是以下方程的解：
- en: '![](../Images/584ef30d5e922bef91c11781bf03ae75.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/584ef30d5e922bef91c11781bf03ae75.png)'
- en: So the specific form of exp(−*E*) comes from energy conservation.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，exp(−*E*)的具体形式来源于能量守恒。
- en: Intuitively, we can think of energy as a sort of budget. Chaotic systems are
    trying to explore all possibilities (to maximize entropy). If a category has a
    high energy need, it will explore that category less so that there are more opportunities
    to explore other categories. Yet, the probability doesn’t drop to zero because
    the system does still want to explore this energy-inefficient category. The exponential
    form is a result of the compromise between efficiency and exploration.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地说，我们可以把能量看作一种预算。混沌系统尝试探索所有可能性（以最大化熵）。如果某个类别需要的能量很高，它将较少探索该类别，从而有更多机会探索其他类别。然而，概率不会降为零，因为系统仍然希望探索这个能量低效的类别。指数形式是效率和探索之间的妥协结果。
- en: Going back to data science and classification, our data are not part of a dynamical
    system, and there is no conservation of energy, so how are these physics insights
    useful?
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 回到数据科学和分类问题，我们的数据不是动态系统的一部分，也没有能量守恒，所以这些物理学见解如何有用呢？
- en: Energies for categories
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 类别的能量
- en: '![](../Images/97469963694fe23ca5faea520a744995.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97469963694fe23ca5faea520a744995.png)'
- en: Energy is the basic unit of all interactions, but does it have a place in classification
    problem? (Photo by [Fré Sonneveld](https://unsplash.com/@fresonneveld?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral))
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 能量是所有互动的基本单位，但它在分类问题中有用吗？（图片来源：[Fré Sonneveld](https://unsplash.com/@fresonneveld?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)）
- en: The crux is that in data science, we are not necessarily interested in literally
    how the data came to be—this is an almost impossible task. Rather, we are trying
    to construct a system that can mimic relevant behaviors in our data. From this
    viewpoint, a model becomes a sort of dynamical system that is molded by data in
    some desirable ways (this was explored in my [article](https://medium.com/towards-data-science/a-physicists-view-of-machine-learning-the-thermodynamics-of-machine-learning-6a3ab00e46f1)).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 关键在于，在数据科学中，我们不一定对数据是如何得来的感兴趣——这是一个几乎不可能完成的任务。相反，我们正在尝试构建一个可以模仿数据中相关行为的系统。从这个角度来看，模型成为一个由数据以某种可取方式塑造的动态系统（这一点在我的[文章](https://medium.com/towards-data-science/a-physicists-view-of-machine-learning-the-thermodynamics-of-machine-learning-6a3ab00e46f1)中探讨过）。
- en: Just like in thermal physics, the Boltzmann distribution is effective in capturing
    rough microscopic details, while faithfully reproducing macroscopic physical quantities
    (temperature, pressure… etc). So it’s at least plausible that it could lend that
    superpower to data science.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在热物理学中，玻尔兹曼分布在捕捉粗略的微观细节方面很有效，同时忠实地再现宏观物理量（温度、压力等）。因此，至少可以认为它可能将这种超能力赋予数据科学。
- en: 'Indeed, rather than looking for some sort of energy conservation laws in our
    data, we could just *enforce* a notion of energy conservation in our classification
    model. This way, analogous to the Boltzmann distribution in thermal physics:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，与其在我们的数据中寻找某种能量守恒定律，我们不如在我们的分类模型中*强制*能量守恒的概念。这样，与热物理学中的玻尔兹曼分布类似：
- en: The softmax function in a model assumes a maximum entropy guess for categories,
    under the assumption that there is a sort of conserved budget for making these
    guesses
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 模型中的 softmax 函数假设类别的最大熵猜测，前提是对这些猜测存在某种保守预算。
- en: 'The maximum entropy can be justified as a sort of maximal likelihood estimate
    (see [my article on entropy](https://medium.com/towards-data-science/what-does-entropy-measure-an-intuitive-explanation-a7f7e5d16421)).
    The remaining question is, why does it make sense to create an artificially fixed
    energy budget? Here are some reasons:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 最大熵可以被视为一种最大似然估计（参见[我的关于熵的文章](https://medium.com/towards-data-science/what-does-entropy-measure-an-intuitive-explanation-a7f7e5d16421)）。剩下的问题是，为什么创建一个人为固定的能量预算是合理的？以下是一些原因：
- en: 'Central Limit Theorem: the energies are often linear sums of features, so they
    have well-defined mean. So it’s not much of a leap to enforce the average of these
    energies over categories to be constant.'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 中心极限定理：能量通常是特征的线性和，因此它们有明确的均值。所以，强制这些能量在类别间的平均值保持恒定并不是一个很大的跳跃。
- en: 'Regularization: this forces the extremes of the probabilities to be limited,
    as a 1 or 0 probability would require some energies to be infinite.'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正则化：这迫使概率的极值受到限制，因为概率为 1 或 0 将要求一些能量为无限大。
- en: 'Variance reduction: by imposing a reasonable constraint, we introduce bias
    while reducing variance in our model (bias/variance trade-off)'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 方差减少：通过施加合理的约束，我们在模型中引入偏差的同时减少方差（偏差/方差权衡）。
- en: Point 1 and 3 are particularly salient when using softmax in deep neural networks.
    Since network layers often have some sort of normalization already, it makes even
    more sense to enforce a fixed energy to ensure good statistical behaviors downstream.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度神经网络中使用 softmax 时，点 1 和 3 尤其显著。由于网络层通常已经有某种规范化，因此强制固定能量以确保下游良好的统计行为更加有意义。
- en: 'So, how do these insights help us understand our models? Well, we can use them
    to explain anecdotal facts regarding models that utilize softmax (i.e. logistic
    regression):'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这些见解如何帮助我们理解我们的模型呢？我们可以用它们来解释有关使用 softmax 的模型的轶事事实（即逻辑回归）：
- en: '**Issues with imbalance**: this is because softmax assumes maximum entropy.
    It is intentionally trying to get the model to explore all possible categories
    without prejudice.'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**不平衡的问题**：这是因为 softmax 假设最大熵。它故意试图让模型在没有偏见的情况下探索所有可能的类别。'
- en: '**Issues with large # of categories**: softmax attempts to assign all possible
    categories, even ones that are impossible (i.e. assign cats as vehicles). For
    data with cleanly separated categories, clustering, nearest-neighbors, [support-vector-machines](https://en.wikipedia.org/wiki/Support_vector_machine)
    and [random forest](https://en.wikipedia.org/wiki/Random_forest) models could
    perform better.'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**大量类别的问题**：softmax 尝试分配所有可能的类别，即使是那些不可能的类别（例如，将猫分配为车辆）。对于类别被干净地分开的数据，聚类、最近邻、[支持向量机](https://en.wikipedia.org/wiki/Support_vector_machine)和[随机森林](https://en.wikipedia.org/wiki/Random_forest)模型可能表现更好。'
- en: Beyond just the model structure, our thermal physics analogy also helps us understand
    the training paradigm, which we turn to next.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 除了模型结构之外，我们的热物理类比还帮助我们理解训练范式，这也是我们接下来要讨论的内容。
- en: Putting on the Training Pressure
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 施加训练压力
- en: '![](../Images/de6e9ad36e97c0c2950d25e74211641d.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de6e9ad36e97c0c2950d25e74211641d.png)'
- en: Boltzmann distribution allows us to compute things like pressure, what’s the
    analogy in classification? (Photo by [NOAA](https://unsplash.com/@noaa?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral))
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Boltzmann 分布允许我们计算类似压力的东西，在分类中有什么类比？（照片由[NOAA](https://unsplash.com/@noaa?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)）
- en: 'The utility of the Boltzmann distribution goes beyond simply categorizing states
    of a system: it enables us to compute other useful emergent quantities — things
    like pressure. This pressure can be the physical pressure we experience, or more
    abstract thermodynamic forces like magnetic fields and chemical potentials.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Boltzmann 分布的实用性不仅仅在于对系统状态进行分类：它使我们能够计算其他有用的突现量——例如压力。这个压力可以是我们所经历的物理压力，也可以是更抽象的热力学力，如磁场和化学势。
- en: 'As an example, the equation for pressure is defined by the change in the Boltzmann
    factor per change in volume:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，压力的方程是通过体积变化引起的 Boltzmann 因子的变化来定义的：
- en: '![](../Images/74698591107419637b4fc0cd6d3019ba.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/74698591107419637b4fc0cd6d3019ba.png)'
- en: More generally, thermodynamic pressures are defined as some sort of change in
    our statistical distributions with respect to some variable.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地说，热力学压力被定义为我们统计分布中相对于某个变量的变化。
- en: 'Jumping back to data science, what are some quantities that look like “pressure”
    in classification? Well, one related quantity is the [Cross Entropy](https://en.wikipedia.org/wiki/Cross_entropy),
    which is the loss function that is typically minimized when training classification
    models. The cross entropy is often estimated via sampling:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 回到数据科学中，分类中的一些量类似于“压力”是什么呢？一个相关的量是[交叉熵](https://en.wikipedia.org/wiki/Cross_entropy)，这是训练分类模型时通常要最小化的损失函数。交叉熵通常通过采样来估计：
- en: '![](../Images/2854f2235033e82f7c95b1bfa85829ea.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2854f2235033e82f7c95b1bfa85829ea.png)'
- en: 'where *l* indicates the proper category a particular data point is in. To optimize
    this we can perform gradient descent: taking the derivative and updating until
    the derivative is zero.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *l* 表示特定数据点所在的正确类别。为了优化它，我们可以进行梯度下降：计算导数并更新，直到导数为零。
- en: Using the physics analogy, we could then view the derivative/gradient as a sort
    of thermodynamic pressure!
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 使用物理类比，我们可以将导数/梯度视为一种热力学压力！
- en: '![](../Images/d768ee68ab9c81e0cc992617625f1b16.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d768ee68ab9c81e0cc992617625f1b16.png)'
- en: What this means then, for model training, is that we are imposing pressure on
    our system (our model) until it equalizes. The model is trained when this internal
    “model pressure” reaches zero—some sort of thermal equilibrium.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 那么对于模型训练来说，这意味着我们在对系统（我们的模型）施加压力，直到它达到平衡。当这种内部“模型压力”达到零时，模型就被训练完成——某种热平衡。
- en: 'While the analogy isn’t 100% exact, it gives us an intuition on how classification
    models work (more specifically, models that utilize softmax and are optimized
    through something like cross-entropy). So we conclude that:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种类比并不是 100% 精确，但它给了我们关于分类模型如何工作的直观认识（更具体地说，利用 softmax 并通过诸如交叉熵的东西进行优化的模型）。所以我们可以得出结论：
- en: Classification models are mock thermodynamic system driven to equilibrium to
    mimic categories in our data
  id: totrans-86
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 分类模型是模拟热力学系统的模型，它们被驱动到平衡状态，以模拟我们数据中的类别。
- en: This viewpoint can perhaps explain why simple logistic regressions are effective,
    even when assumptions regarding linear models are often violated in real life.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这一观点或许可以解释为什么简单的逻辑回归模型是有效的，即使在现实生活中，关于线性模型的假设通常会被违反。
- en: Conclusion
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Hopefully I’ve shown you some intriguing connections between simple logistic
    regressions and thermal physics.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 希望我已经向你展示了一些简单逻辑回归与热力学之间的有趣联系。
- en: Data science is a very broad discipline. Like thermal physics, we can think
    of data science as a high level way of understanding our macroscopic world, while
    properly handling microscopic details of which we may be ignorant. Perhaps it
    is not surprising that many conceptual and mathematical tools in data science
    can be linked or even traced back to physics, as they both share the common objective
    of modelling our world.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学是一个非常广泛的学科。像热力学一样，我们可以将数据科学视为一种高级的理解宏观世界的方式，同时妥善处理我们可能不知晓的微观细节。或许并不奇怪，数据科学中的许多概念和数学工具可以与物理学联系起来，甚至可以追溯到物理学，因为它们都有着建模我们世界的共同目标。
- en: If you like this article, please do leave a comment. Also let me know if you
    would like to see other abstract concepts demystified and properly explained.
    Happy reading 👋.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这篇文章，请留下评论。如果你希望看到其他抽象概念被解密并得到正确解释，也请告诉我。祝阅读愉快👋。
- en: 'If you like my article, you may be interested in my other insight pieces:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢我的文章，你可能对我的其他见解也感兴趣：
- en: '[](/what-does-entropy-measure-an-intuitive-explanation-a7f7e5d16421?source=post_page-----291774fda579--------------------------------)
    [## What does Entropy Measure? An Intuitive Explanation'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/what-does-entropy-measure-an-intuitive-explanation-a7f7e5d16421?source=post_page-----291774fda579--------------------------------)
    [## 熵的测量是什么？一个直观的解释'
- en: Entropy can be thought of as the probability of seeing certain patterns in data.
    Here’s how it works.
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 熵可以被认为是看到数据中某些模式的概率。以下是它是如何工作的。
- en: 'towardsdatascience.com](/what-does-entropy-measure-an-intuitive-explanation-a7f7e5d16421?source=post_page-----291774fda579--------------------------------)
    [](/a-physicists-view-of-machine-learning-the-thermodynamics-of-machine-learning-6a3ab00e46f1?source=post_page-----291774fda579--------------------------------)
    [## A Physicist’s View of Machine Learning: The Thermodynamics of Machine Learning'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/what-does-entropy-measure-an-intuitive-explanation-a7f7e5d16421?source=post_page-----291774fda579--------------------------------)
    [](/a-physicists-view-of-machine-learning-the-thermodynamics-of-machine-learning-6a3ab00e46f1?source=post_page-----291774fda579--------------------------------)
    [## 物理学家的机器学习视角：机器学习的热力学
- en: Complex systems in nature can be successfully studied using thermodynamics.
    What about Machine Learning?
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自然界中的复杂系统可以通过热力学成功研究。那么，机器学习呢？
- en: 'towardsdatascience.com](/a-physicists-view-of-machine-learning-the-thermodynamics-of-machine-learning-6a3ab00e46f1?source=post_page-----291774fda579--------------------------------)
    [](https://medium.com/swlh/entropy-is-not-disorder-a-physicists-perspective-c0dccfea67f1?source=post_page-----291774fda579--------------------------------)
    [## Entropy Is Not Disorder: A Physicist’s Perspective'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/a-physicists-view-of-machine-learning-the-thermodynamics-of-machine-learning-6a3ab00e46f1?source=post_page-----291774fda579--------------------------------)
    [](https://medium.com/swlh/entropy-is-not-disorder-a-physicists-perspective-c0dccfea67f1?source=post_page-----291774fda579--------------------------------)
    [## 熵不是混乱：物理学家的视角
- en: Entropy is often treated as synonymous to chaos. But what is it really? In this
    article, we explore how entropy is more…
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 熵通常被视为混乱的同义词。但它真正是什么呢？在这篇文章中，我们探讨了熵如何更…
- en: medium.com](https://medium.com/swlh/entropy-is-not-disorder-a-physicists-perspective-c0dccfea67f1?source=post_page-----291774fda579--------------------------------)
    [](https://medium.com/physicist-musings/why-we-dont-live-in-a-simulation-a-physicist-s-perspective-1811d65f502d?source=post_page-----291774fda579--------------------------------)
    [## Why We Don’t Live in a Simulation
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/swlh/entropy-is-not-disorder-a-physicists-perspective-c0dccfea67f1?source=post_page-----291774fda579--------------------------------)
    [](https://medium.com/physicist-musings/why-we-dont-live-in-a-simulation-a-physicist-s-perspective-1811d65f502d?source=post_page-----291774fda579--------------------------------)
    [## 为什么我们不生活在模拟中
- en: Describing reality as a simulation vastly understates the complexities of our
    world. Here’s why the simulation…
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将现实描述为一种模拟严重低估了我们世界的复杂性。以下是为什么这种模拟…
- en: medium.com](https://medium.com/physicist-musings/why-we-dont-live-in-a-simulation-a-physicist-s-perspective-1811d65f502d?source=post_page-----291774fda579--------------------------------)
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/physicist-musings/why-we-dont-live-in-a-simulation-a-physicist-s-perspective-1811d65f502d?source=post_page-----291774fda579--------------------------------)
