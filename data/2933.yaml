- en: Quantifying GPT-4’s Hidden Regressions Over Time
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 量化 GPT-4 的隐藏回归
- en: 原文：[https://towardsdatascience.com/quantifying-gpt-4s-hidden-regressions-over-time-368d3a16dca?source=collection_archive---------7-----------------------#2023-09-22](https://towardsdatascience.com/quantifying-gpt-4s-hidden-regressions-over-time-368d3a16dca?source=collection_archive---------7-----------------------#2023-09-22)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/quantifying-gpt-4s-hidden-regressions-over-time-368d3a16dca?source=collection_archive---------7-----------------------#2023-09-22](https://towardsdatascience.com/quantifying-gpt-4s-hidden-regressions-over-time-368d3a16dca?source=collection_archive---------7-----------------------#2023-09-22)
- en: Part 3 of a study on generative AI usage and testing
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 研究生成型 AI 使用和测试的第三部分
- en: '[](https://markopolocheno.medium.com/?source=post_page-----368d3a16dca--------------------------------)[![Mark
    Chen](../Images/2d51d4e7ab451b55733a018a3d10a0a7.png)](https://markopolocheno.medium.com/?source=post_page-----368d3a16dca--------------------------------)[](https://towardsdatascience.com/?source=post_page-----368d3a16dca--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----368d3a16dca--------------------------------)
    [Mark Chen](https://markopolocheno.medium.com/?source=post_page-----368d3a16dca--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://markopolocheno.medium.com/?source=post_page-----368d3a16dca--------------------------------)[![Mark
    Chen](../Images/2d51d4e7ab451b55733a018a3d10a0a7.png)](https://markopolocheno.medium.com/?source=post_page-----368d3a16dca--------------------------------)[](https://towardsdatascience.com/?source=post_page-----368d3a16dca--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----368d3a16dca--------------------------------)
    [Mark Chen](https://markopolocheno.medium.com/?source=post_page-----368d3a16dca--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F377682c0f342&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquantifying-gpt-4s-hidden-regressions-over-time-368d3a16dca&user=Mark+Chen&userId=377682c0f342&source=post_page-377682c0f342----368d3a16dca---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----368d3a16dca--------------------------------)
    ·5 min read·Sep 22, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F368d3a16dca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquantifying-gpt-4s-hidden-regressions-over-time-368d3a16dca&user=Mark+Chen&userId=377682c0f342&source=-----368d3a16dca---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F377682c0f342&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquantifying-gpt-4s-hidden-regressions-over-time-368d3a16dca&user=Mark+Chen&userId=377682c0f342&source=post_page-377682c0f342----368d3a16dca---------------------post_header-----------)
    发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----368d3a16dca--------------------------------)
    ·5分钟阅读·2023年9月22日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F368d3a16dca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquantifying-gpt-4s-hidden-regressions-over-time-368d3a16dca&user=Mark+Chen&userId=377682c0f342&source=-----368d3a16dca---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F368d3a16dca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquantifying-gpt-4s-hidden-regressions-over-time-368d3a16dca&source=-----368d3a16dca---------------------bookmark_footer-----------)![](../Images/071dde74efb851e67523d42b93c503db.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F368d3a16dca&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fquantifying-gpt-4s-hidden-regressions-over-time-368d3a16dca&source=-----368d3a16dca---------------------bookmark_footer-----------)![](../Images/071dde74efb851e67523d42b93c503db.png)'
- en: Photo by [Randy Fath](https://unsplash.com/@randyfath?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由[Randy Fath](https://unsplash.com/@randyfath?utm_source=medium&utm_medium=referral)拍摄，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: GPT-4 is bigger and better than GPT-3\. GPT-4 can draft up eloquent speeches,
    [pass standardized exams](https://openai.com/research/gpt-4#:~:text=among%20test%20takers)-,Exam%20results,-(ordered%20by%20GPT),
    and even [interpret images](https://openai.com/research/gpt-4#:~:text=Visual%20inputs%3A%20VGA%20charger).
    Since its release on March 14, 2023, OpenAI continues to iterate and update GPT-4
    to improve its performance for the millions of queries it receives each day. However,
    **is the latest version of GPT-4 in OpenAI’s API, called “gpt-4”, actually better
    than the initial version** from March, called “gpt-4–0314”?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4比GPT-3更大更好。GPT-4可以撰写优美的演讲，[通过标准化考试](https://openai.com/research/gpt-4#:~:text=among%20test%20takers)-,Exam%20results,-(ordered%20by%20GPT)，甚至[解读图像](https://openai.com/research/gpt-4#:~:text=Visual%20inputs%3A%20VGA%20charger)。自2023年3月14日发布以来，OpenAI不断迭代和更新GPT-4，以提升其对每天接收的数百万个查询的处理能力。然而，**OpenAI的API中最新版本的GPT-4，即“gpt-4”，真的比三月份的初始版本**
    “gpt-4–0314” **更好吗？**
- en: From the perspective of a machine learning engineer at [Kolena](https://www.kolena.io/),
    this article is a [continuation in a series of discussions highlighting a testing
    paradigm for LLMs](/how-to-validate-openai-gpt-model-performance-with-text-summarization-298978fea764),
    comparing the performance of GPT models under different scenarios.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 从[Kolena](https://www.kolena.io/)机器学习工程师的角度来看，这篇文章是[系列讨论的继续，强调了一种针对LLM的测试范式](/how-to-validate-openai-gpt-model-performance-with-text-summarization-298978fea764)，比较了不同场景下GPT模型的表现。
- en: While the overall behavior of “gpt-4” might be better than “gpt-4–0314” through
    the results of various testing benchmarks and metrics, the word “better” is a
    relative term. Users have shared online that they **experienced a recent** [**regression
    in GPT-4 model performance**](https://arxiv.org/pdf/2307.09009.pdf) **in a variety
    of contexts**. One viral instance of GPT-4’s regression over time is that it could
    not figure out that 17077 was a prime number as well as it could before.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管通过各种测试基准和指标，“gpt-4”的整体表现可能比“gpt-4–0314”更好，但“更好”一词是相对的。用户在线分享了他们**最近在不同环境中**
    [**经历了GPT-4模型性能回退**](https://arxiv.org/pdf/2307.09009.pdf)。一个广为流传的GPT-4性能回退实例是，它无法像之前一样识别17077是一个质数。
- en: Naturally, using the most up-to-date model when it continually declines in subjective
    and objective performance is problematic. **What other regressions might secretly
    exist?**
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，使用最先进的模型时，它在主观和客观表现上不断下降，这是一个问题。**可能还存在什么其他隐藏的回归？**
- en: 'We can test for hidden regressions of GPT-4 by using the CoQA ([Conversational
    Question Answering](https://stanfordnlp.github.io/coqa/))** dataset. The CoQA
    dataset contains multiple articles, each having a series of corresponding questions,
    where understanding question *n* is necessary for answering question *n+1*. Given
    an article on sport history as an example, here are some potential questions:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用CoQA（[对话问答](https://stanfordnlp.github.io/coqa/)）**数据集来测试GPT-4的隐藏回归。CoQA数据集包含多篇文章，每篇文章有一系列相关的问题，其中理解问题*n*对于回答问题*n+1*是必要的。以一篇关于体育历史的文章为例，这里有一些潜在的问题：
- en: 1\. Who is the most decorated Olympian?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 谁是获得奖牌最多的奥运选手？
- en: 2\. Which country are they from?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 他们来自哪个国家？
- en: 3\. How many gold medals do they have?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 他们获得了多少枚金牌？
- en: It’s impossible to individually answer these questions because we would not
    know the person of interest without answering the first question.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 无法单独回答这些问题，因为在回答第一个问题之前，我们不知道感兴趣的对象。
- en: Findings
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发现
- en: 'At a high level, **GPT-4 performs better than GPT-3 significantly**, but it’s
    still not perfect:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，**GPT-4的表现比GPT-3显著更好**，但它仍然不完美：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*Note: “gpt-3” is the latest Turbo model of the GPT-3.5 series, and n_correct
    is the count of questions where the average of its* [*BERT_F1*](https://huggingface.co/spaces/evaluate-metric/bertscore)
    *and* [*ROUGE_1*](https://huggingface.co/spaces/evaluate-metric/rouge) *is greater
    than 0.75*'
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*注意：“gpt-3”是GPT-3.5系列的最新Turbo模型，而n_correct是平均值* [*BERT_F1*](https://huggingface.co/spaces/evaluate-metric/bertscore)
    *和* [*ROUGE_1*](https://huggingface.co/spaces/evaluate-metric/rouge) *大于0.75的问题数量*'
- en: From the above, how come **“gpt-4–0314” is worse by metric** (BERT_F1 and ROUGE_1)
    **yet has more correctly answered questions than “gpt-4”**? Maybe both models
    incorrectly answer the same questions, but there is no guarantee that the failure
    sets of “gpt-4” and “gpt-4–0314” are homogeneous. Under the assumption that a
    newer model should be more performant, the reason for this difference or regression
    is not explainable when we observe the metrics. We can dig deeper into understanding
    the potential root causes of failure when we logically break down the data into
    smaller groups.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述内容来看，**为什么“gpt-4–0314”在指标（BERT_F1和ROUGE_1）上表现更差**却有更多正确回答问题的次数？也许这两个模型在回答相同问题时都存在错误，但不能保证“gpt-4”和“gpt-4–0314”的失败集是同质的。在假设新模型应该具有更高性能的情况下，当我们观察指标时，无法解释这种差异或退步的原因。我们可以通过逻辑上将数据分解为更小的组来深入理解潜在的失败根源。
- en: When we stratify the CoQA dataset with respect to the data source of each article,
    we will find that the question-answer **data pertaining to Wikipedia articles
    performed better in the newest GPT-4 model** but worse overall and in every other
    data source.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们根据每篇文章的数据源对CoQA数据集进行分层时，我们会发现**涉及维基百科文章的问题回答数据在最新的GPT-4模型中表现更好**，但整体上以及在所有其他数据源中的表现较差。
- en: '![](../Images/0bcb9c86a241236d376533458b45d1c1.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0bcb9c86a241236d376533458b45d1c1.png)'
- en: A comparison of “gpt-4” and “gpt-4–0314” by BERT_F1, ROUGE_1, and the count
    of correct answers, taken from [Kolena](https://www.kolena.io/)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 通过BERT_F1、ROUGE_1和正确答案数量对“gpt-4”和“gpt-4–0314”进行比较，数据来自[Kolena](https://www.kolena.io/)
- en: The image above shows a comparison between “gpt-4–0314” as a benchmark and “gpt-4”,
    highlighting the differences in the number of correct answers generated with respect
    to an improvement or decline among different data sources. **In terms of the number
    of correct answers, GPT-4's only improvement is from Wikipedia’s datapoints, and
    it declines in performance everywhere else.**
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了“gpt-4–0314”作为基准与“gpt-4”的比较，突出了在不同数据源中正确答案数量的差异，这些差异表明了改进或退步。**在正确答案的数量方面，GPT-4唯一的改进来自于维基百科的数据点，而在其他所有地方表现都在下降。**
- en: Analysis
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析
- en: '**Does this reveal that “gpt-4” is a fine-tuned version of “gpt-4–0314” on
    Wikipedia articles?** Unfortunately, we don’t know.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**这是否揭示了“gpt-4”是“gpt-4–0314”在维基百科文章上经过微调的版本？** 不幸的是，我们并不知道。'
- en: Can we then say that GPT-4 has become worse? By this measure, not necessarily.
    While academia considers Wikipedia to be an unreliable source of information,
    many people still regularly use it for quick and accessible information. If OpenAI
    wants GPT to answer any question in any domain, having **a complete comprehension
    of Wikipedia is more valuable than understanding news articles when users make
    millions of random queries each day**. News articles tend to have common themes
    anyway, and the average person might not ask GPT questions pertaining to news
    articles on topics absent within Wikipedia.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们是否可以说GPT-4变得更糟了？按照这个标准，不一定。尽管学术界认为维基百科是不可靠的信息来源，但许多人仍然经常使用它以获取快速且可访问的信息。如果OpenAI希望GPT能回答任何领域的任何问题，那么**对维基百科的全面理解比理解新闻文章更有价值，因为用户每天进行数百万次随机查询**。新闻文章往往有共同的主题，而普通人可能不会向GPT提问涉及新闻文章的维基百科中不存在的话题。
- en: Prior to stratifying the dataset by the different data sources, there was no
    concrete explanation for why “gpt-4–0314” obtained a greater number of correct
    results compared to “gpt-4”. With just one stratification, we gain one plausible
    explanation as to why and how the models are different.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在对数据集进行不同数据源的分层之前，没有具体的解释说明为什么“gpt-4–0314”相比于“gpt-4”获得了更多正确结果。通过一次分层，我们获得了一个合理的解释，说明模型之间的差异及其原因。
- en: Conclusion
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: '**Over time, GPT-4 has regressed in conversational question answering for multiple
    data sources, but improved in performance for queries involving Wikipedia articles.**'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**随着时间的推移，GPT-4在处理多个数据源的对话式问题回答上有所退步，但在涉及维基百科文章的查询上表现有所改善。**'
- en: Being able to identify hidden regressions should be a priority for all engineers
    before deploying models to production. Finding hidden regressions for LLMs is
    not trivial, but it becomes easier with the right approach. **The best model is
    not necessarily the one with the best overall performance but the one with the
    best results under the scenarios that matter most.**
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 能够识别隐藏的退步应成为所有工程师在将模型部署到生产环境之前的优先事项。寻找LLM的隐藏退步并非易事，但采用正确的方法可以简化这一过程。**最佳模型并不一定是整体性能最好的模型，而是最能在最重要的场景中取得最佳结果的模型。**
- en: We’ll dig deeper into more stratifications of CoQA to further understand how
    GPT-4 has changed over time in a future blog post. Stay tuned!
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在未来的博客文章中深入探讨 CoQA 的更多层次，以进一步了解 GPT-4 随时间的变化。敬请关注！
- en: '**The [CoQA dataset contains data from seven different datasets](https://stanfordnlp.github.io/coqa/#:~:text=Submission%20Tutorial-,License,-CoQA%20contains%20passages)
    having different licenses. In this article, we do not reveal any data within the
    dataset, and only used the data for testing and analysis from these commercially
    available data sources: Gutenberg, CNN, MCTest, and Wikipedia, with a [CC BY-SA
    4.0](https://creativecommons.org/licenses/by-sa/4.0/), [MSR-LA](https://github.com/mcobzarenco/mctest/blob/master/data/MCTest/LICENSE.pdf),
    or [Apache](https://github.com/deepmind/rc-data/blob/master/LICENSE) license.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**[CoQA 数据集包含来自七个不同数据集的数据](https://stanfordnlp.github.io/coqa/#:~:text=Submission%20Tutorial-,License,-CoQA%20contains%20passages)**，这些数据集拥有不同的许可证。在本文中，我们不会透露数据集中的任何数据，仅使用这些商业数据源中的数据进行测试和分析，包括
    Gutenberg、CNN、MCTest 和 Wikipedia，这些数据源具有 [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)、[MSR-LA](https://github.com/mcobzarenco/mctest/blob/master/data/MCTest/LICENSE.pdf)
    或 [Apache](https://github.com/deepmind/rc-data/blob/master/LICENSE) 许可证。'
