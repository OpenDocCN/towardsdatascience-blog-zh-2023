- en: A Little Pandas Hack to Handle Large Datasets with Limited Memory
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个小巧的 Pandas 黑客，用于在内存有限的情况下处理大型数据集
- en: 原文：[https://towardsdatascience.com/a-little-pandas-hack-to-handle-large-datasets-with-limited-memory-6745140f473b?source=collection_archive---------3-----------------------#2023-01-19](https://towardsdatascience.com/a-little-pandas-hack-to-handle-large-datasets-with-limited-memory-6745140f473b?source=collection_archive---------3-----------------------#2023-01-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-little-pandas-hack-to-handle-large-datasets-with-limited-memory-6745140f473b?source=collection_archive---------3-----------------------#2023-01-19](https://towardsdatascience.com/a-little-pandas-hack-to-handle-large-datasets-with-limited-memory-6745140f473b?source=collection_archive---------3-----------------------#2023-01-19)
- en: The Pandas defaults aren’t optimal. A tiny configuration can compress your dataframe
    to fit in your memory.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pandas 的默认设置并不理想。一个小的配置可以将你的数据框压缩到适合你的内存中。
- en: '[](https://thuwarakesh.medium.com/?source=post_page-----6745140f473b--------------------------------)[![Thuwarakesh
    Murallie](../Images/44f1a14a899426592bbd8c7f73ce169d.png)](https://thuwarakesh.medium.com/?source=post_page-----6745140f473b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6745140f473b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6745140f473b--------------------------------)
    [Thuwarakesh Murallie](https://thuwarakesh.medium.com/?source=post_page-----6745140f473b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://thuwarakesh.medium.com/?source=post_page-----6745140f473b--------------------------------)[![Thuwarakesh
    Murallie](../Images/44f1a14a899426592bbd8c7f73ce169d.png)](https://thuwarakesh.medium.com/?source=post_page-----6745140f473b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6745140f473b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6745140f473b--------------------------------)
    [Thuwarakesh Murallie](https://thuwarakesh.medium.com/?source=post_page-----6745140f473b--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F93ce19993bef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-little-pandas-hack-to-handle-large-datasets-with-limited-memory-6745140f473b&user=Thuwarakesh+Murallie&userId=93ce19993bef&source=post_page-93ce19993bef----6745140f473b---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6745140f473b--------------------------------)
    ·8 min read·Jan 19, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6745140f473b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-little-pandas-hack-to-handle-large-datasets-with-limited-memory-6745140f473b&user=Thuwarakesh+Murallie&userId=93ce19993bef&source=-----6745140f473b---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F93ce19993bef&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-little-pandas-hack-to-handle-large-datasets-with-limited-memory-6745140f473b&user=Thuwarakesh+Murallie&userId=93ce19993bef&source=post_page-93ce19993bef----6745140f473b---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6745140f473b--------------------------------)
    ·8分钟阅读·2023年1月19日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F6745140f473b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-little-pandas-hack-to-handle-large-datasets-with-limited-memory-6745140f473b&user=Thuwarakesh+Murallie&userId=93ce19993bef&source=-----6745140f473b---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6745140f473b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-little-pandas-hack-to-handle-large-datasets-with-limited-memory-6745140f473b&source=-----6745140f473b---------------------bookmark_footer-----------)![](../Images/c4b1299e06f03069b94110ff87456e73.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F6745140f473b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fa-little-pandas-hack-to-handle-large-datasets-with-limited-memory-6745140f473b&source=-----6745140f473b---------------------bookmark_footer-----------)![](../Images/c4b1299e06f03069b94110ff87456e73.png)'
- en: You can compress a huge Pandas dataframe without losing its properties, just
    like squeezing a burger. Save memory and work faster with this little trick. —
    Photo by [Leonardo Luz](https://www.pexels.com/photo/photo-of-a-burger-between-a-person-s-hands-14001304/)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在不丧失属性的情况下压缩一个巨大的 Pandas 数据框，就像挤压汉堡一样。通过这个小技巧节省内存，提高工作效率。 — 图片来自 [Leonardo
    Luz](https://www.pexels.com/photo/photo-of-a-burger-between-a-person-s-hands-14001304/)
- en: I never thought my code needed improvement. I’ve always complained that I don’t
    have enough memory or the dataset was too big to handle.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我从未认为我的代码需要改进。我总是抱怨内存不足或数据集太大而无法处理。
- en: My go-to solution was to put them on a Postgres DB and write SQL queries. After
    all, that is an acceptable way to handle large-scale datasets. I kept doing this
    whenever I got a large dataset.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我通常的解决方案是将它们放在 Postgres 数据库中并编写 SQL 查询。毕竟，这是一种处理大规模数据集的可接受方式。每当我得到一个大型数据集时，我就会这样做。
- en: But I don’t get the complete flexibility I get in Python. For this reason, I
    had to combine them and alternate between them. For instance, I load the dataset
    on a SQL database and write a Python script to run SQL queries, often using Sqlalchemy.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 但我无法获得在 Python 中得到的完全灵活性。出于这个原因，我不得不将它们结合起来并交替使用。例如，我将数据集加载到 SQL 数据库中，并编写 Python
    脚本来运行 SQL 查询，通常使用 Sqlalchemy。
- en: While it gives me the flexibility of both worlds, I have issues sharing my code
    with other team members. Other members should have the knowledge and setup for
    relational databases.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种方法让我兼具两方面的灵活性，但我在与其他团队成员共享代码时遇到了问题。其他成员需要具备关系数据库的知识和设置。
- en: The classic solution for this problem is increasing the memory and running tasks
    in parallel. On the infrastructure part, moving to the cloud was my favorite solution.
    I can pay for the high-performance resources only for their usage. And on the
    parallel execution side, technologies like [Dask](https://www.dask.org/) cover
    me.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的经典解决方案是增加内存并并行运行任务。在基础设施方面，迁移到云端是我最喜欢的解决方案。我可以按使用量为高性能资源付费。在并行执行方面，像[Dask](https://www.dask.org/)这样的技术可以满足我的需求。
