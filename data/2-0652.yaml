- en: Data pipeline design patterns
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ•°æ®ç®¡é“è®¾è®¡æ¨¡å¼
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3](https://towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3](https://towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3)
- en: Choosing the right architecture with examples
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é€‰æ‹©åˆé€‚çš„æ¶æ„åŠå…¶ç¤ºä¾‹
- en: '[](https://mshakhomirov.medium.com/?source=post_page-----100afa4b93e3--------------------------------)[![ğŸ’¡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----100afa4b93e3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----100afa4b93e3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----100afa4b93e3--------------------------------)
    [ğŸ’¡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----100afa4b93e3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mshakhomirov.medium.com/?source=post_page-----100afa4b93e3--------------------------------)[![ğŸ’¡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----100afa4b93e3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----100afa4b93e3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----100afa4b93e3--------------------------------)
    [ğŸ’¡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----100afa4b93e3--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----100afa4b93e3--------------------------------)
    Â·9 min readÂ·Jan 2, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----100afa4b93e3--------------------------------)
    Â·9 åˆ†é’Ÿé˜…è¯»Â·2023å¹´1æœˆ2æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/074dc1570ad17f76947283225050b8db.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/074dc1570ad17f76947283225050b8db.png)'
- en: Photo by [israel palacio](https://unsplash.com/@othentikisra?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ç…§ç‰‡ç”± [israel palacio](https://unsplash.com/@othentikisra?utm_source=medium&utm_medium=referral)
    åœ¨ [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) æä¾›
- en: Typically data is processed, extracted, and transformed in steps. Therefore,
    a sequence of data processing stages can be referred to as a **data pipeline**.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸æ•°æ®åœ¨å¤šä¸ªæ­¥éª¤ä¸­è¢«å¤„ç†ã€æå–å’Œè½¬æ¢ã€‚å› æ­¤ï¼Œä¸€ç³»åˆ—æ•°æ®å¤„ç†é˜¶æ®µå¯ä»¥ç§°ä¸º**æ•°æ®ç®¡é“**ã€‚
- en: '*Which design pattern to choose?*'
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*é€‰æ‹©å“ªä¸ªè®¾è®¡æ¨¡å¼ï¼Ÿ*'
- en: There are lots of things to consider, i.e. **Which data stack to use? What tools
    to consider? How to design a data pipeline conceptually? ETL or ELT? Maybe ETLT?
    What is Change Data Capture?**
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: éœ€è¦è€ƒè™‘è®¸å¤šå› ç´ ï¼Œå³**ä½¿ç”¨å“ªä¸ªæ•°æ®æ ˆï¼Ÿè€ƒè™‘å“ªäº›å·¥å…·ï¼Ÿå¦‚ä½•ä»æ¦‚å¿µä¸Šè®¾è®¡æ•°æ®ç®¡é“ï¼ŸETL è¿˜æ˜¯ ELTï¼Ÿä¹Ÿè®¸æ˜¯ ETLTï¼Ÿä»€ä¹ˆæ˜¯å˜æ›´æ•°æ®æ•è·ï¼Ÿ**
- en: I will try to cover these questions here.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†åœ¨è¿™é‡Œå°è¯•æ¶µç›–è¿™äº›é—®é¢˜ã€‚
- en: A data pipeline
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæ•°æ®ç®¡é“
- en: So it is a sequence of data processing steps. Due to ***logical data flow connections***
    between these stages, each stage generates an **output** that serves as an **input**
    for the following stage.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å®ƒæ˜¯ä¸€ä¸ªæ•°æ®å¤„ç†æ­¥éª¤çš„åºåˆ—ã€‚ç”±äºè¿™äº›é˜¶æ®µä¹‹é—´çš„***é€»è¾‘æ•°æ®æµè¿æ¥***ï¼Œæ¯ä¸ªé˜¶æ®µéƒ½ä¼šç”Ÿæˆä¸€ä¸ª**è¾“å‡º**ï¼Œä½œä¸ºä¸‹ä¸€ä¸ªé˜¶æ®µçš„**è¾“å…¥**ã€‚
- en: There is a data pipeline whenever there is data processing between points A
    and B.
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¯å½“åœ¨ A ç‚¹å’Œ B ç‚¹ä¹‹é—´è¿›è¡Œæ•°æ®å¤„ç†æ—¶ï¼Œå°±ä¼šæœ‰ä¸€ä¸ªæ•°æ®ç®¡é“ã€‚
- en: A data pipelineâ€™s three major parts are a **source, a processing step or steps,
    and a destination**. Data extracted from an external API (a source) can then be
    loaded into the data warehouse (destination). This is an example of a most common
    data pipeline where the source and destination are different. However, it is not
    always the case, as *destination-to-destination* pipelines also exist.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®ç®¡é“çš„ä¸‰ä¸ªä¸»è¦éƒ¨åˆ†æ˜¯**æºã€å¤„ç†æ­¥éª¤æˆ–æ­¥éª¤ã€ä»¥åŠç›®æ ‡**ã€‚ä»å¤–éƒ¨ APIï¼ˆæºï¼‰æå–çš„æ•°æ®å¯ä»¥åŠ è½½åˆ°æ•°æ®ä»“åº“ï¼ˆç›®æ ‡ï¼‰ä¸­ã€‚è¿™æ˜¯ä¸€ä¸ªæœ€å¸¸è§çš„æ•°æ®ç®¡é“çš„ä¾‹å­ï¼Œå…¶ä¸­æºå’Œç›®æ ‡æ˜¯ä¸åŒçš„ã€‚ç„¶è€Œï¼Œè¿™å¹¶éæ€»æ˜¯å¦‚æ­¤ï¼Œå› ä¸º*ç›®æ ‡åˆ°ç›®æ ‡*çš„ç®¡é“ä¹Ÿå­˜åœ¨ã€‚
- en: For example, data can originate as a `reference` table in the data warehouse
    in the first place and then after some data transformation, it can *land* in a
    new schema, for example, in `analytics` to be used in reporting solutions.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œæ•°æ®å¯ä»¥æœ€åˆä½œä¸ºæ•°æ®ä»“åº“ä¸­çš„`reference`è¡¨æ ¼å­˜åœ¨ï¼Œç„¶åç»è¿‡ä¸€äº›æ•°æ®è½¬æ¢åï¼Œå¯ä»¥*è½åœ°*åˆ°ä¸€ä¸ªæ–°çš„æ¨¡å¼ä¸­ï¼Œä¾‹å¦‚ï¼Œåœ¨`analytics`ä¸­ç”¨äºæŠ¥å‘Šè§£å†³æ–¹æ¡ˆã€‚
- en: '*There is always a data pipeline when data is processed between source and
    destination.*'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*æ¯å½“åœ¨æºå’Œç›®æ ‡ä¹‹é—´å¤„ç†æ•°æ®æ—¶ï¼Œæ€»ä¼šæœ‰ä¸€ä¸ªæ•°æ®ç®¡é“ã€‚*'
- en: '![](../Images/02c0d6d925b544a4cc7d82adbc9c1da9.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02c0d6d925b544a4cc7d82adbc9c1da9.png)'
- en: Data pipeline. Image by author
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®ç®¡é“ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: Event data created by just one `source` at the back-end, an event stream built
    with Kinesis Firehose or Kafka stream, can feed a number of various `destinations`.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±åç«¯ä»…ä¸€ä¸ª`source`åˆ›å»ºçš„äº‹ä»¶æ•°æ®ï¼Œé€šè¿‡ä½¿ç”¨ Kinesis Firehose æˆ– Kafka æµæ„å»ºçš„äº‹ä»¶æµï¼Œå¯ä»¥æä¾›ç»™å¤šä¸ªä¸åŒçš„`destinations`ã€‚
- en: Consider ***user engagement* data** from *Google Analytics* as it flows as an
    event stream that can be used in both analytics dashboards for user activity and
    in the Machine learning (ML) pipeline for churn prediction.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘æ¥è‡ª*Google Analytics*çš„***ç”¨æˆ·å‚ä¸æ•°æ®***ï¼Œå› ä¸ºå®ƒä½œä¸ºäº‹ä»¶æµæµåŠ¨ï¼Œå¯ä»¥ç”¨äºç”¨æˆ·æ´»åŠ¨çš„åˆ†æä»ªè¡¨æ¿ä»¥åŠç”¨äºæµå¤±é¢„æµ‹çš„æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰ç®¡é“ä¸­ã€‚
- en: Despite using the same data source, both pipelines operate independently and
    must successfully complete before the user can see the results.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡ä½¿ç”¨ç›¸åŒçš„æ•°æ®æºï¼Œä½†ä¸¤ä¸ªç®¡é“ç‹¬ç«‹æ“ä½œï¼Œå¿…é¡»æˆåŠŸå®Œæˆåç”¨æˆ·æ‰èƒ½çœ‹åˆ°ç»“æœã€‚
- en: Alternatively, data from two or more `source` locations can be aggregated into
    just one `destination`. For example, data from different payment merchant providers
    can be transformed into a revenue report for the Business Intelligence (BI) dashboard.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…ï¼Œå¯ä»¥å°†æ¥è‡ªä¸¤ä¸ªæˆ–æ›´å¤š`source`ä½ç½®çš„æ•°æ®æ±‡æ€»åˆ°ä¸€ä¸ª`destination`ä¸­ã€‚ä¾‹å¦‚ï¼Œæ¥è‡ªä¸åŒæ”¯ä»˜å•†æä¾›å•†çš„æ•°æ®å¯ä»¥è½¬æ¢ä¸ºä¸šåŠ¡æ™ºèƒ½ï¼ˆBIï¼‰ä»ªè¡¨æ¿çš„æ”¶å…¥æŠ¥å‘Šã€‚
- en: Data quality checks, data cleansing, transformation, enrichment, filtering,
    grouping, aggregation, and the application of algorithms to the data are frequent
    steps in data pipelines.
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ•°æ®è´¨é‡æ£€æŸ¥ã€æ•°æ®æ¸…æ´—ã€è½¬æ¢ã€ä¸°å¯Œã€è¿‡æ»¤ã€åˆ†ç»„ã€èšåˆä»¥åŠå¯¹æ•°æ®åº”ç”¨ç®—æ³•æ˜¯æ•°æ®ç®¡é“ä¸­çš„å¸¸è§æ­¥éª¤ã€‚
- en: Architecture types and examples
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¶æ„ç±»å‹å’Œç¤ºä¾‹
- en: Data pipeline architecture as a term might mean several things depending on
    the situation. In general, it can be split into **conceptual** (logical) and **platform**
    levels or architecture types.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®ç®¡é“æ¶æ„è¿™ä¸€æœ¯è¯­æ ¹æ®æƒ…å†µå¯èƒ½æœ‰å¤šç§å«ä¹‰ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå®ƒå¯ä»¥åˆ†ä¸º**æ¦‚å¿µæ€§**ï¼ˆé€»è¾‘ï¼‰å’Œ**å¹³å°**çº§åˆ«æˆ–æ¶æ„ç±»å‹ã€‚
- en: The **conceptually** logical part describes how a dataset is processed and transformed
    from collection to serving, whereas **platform architecture** focuses on an individual
    set of tools and frameworks used in a given scenario, as well as the functions
    that each of them plays.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ¦‚å¿µæ€§**é€»è¾‘éƒ¨åˆ†æè¿°äº†æ•°æ®é›†ä»é‡‡é›†åˆ°æœåŠ¡çš„å¤„ç†å’Œè½¬æ¢æ–¹å¼ï¼Œè€Œ**å¹³å°æ¶æ„**åˆ™ä¸“æ³¨äºç‰¹å®šåœºæ™¯ä¸­ä½¿ç”¨çš„ä¸€ç»„å·¥å…·å’Œæ¡†æ¶ï¼Œä»¥åŠå®ƒä»¬å„è‡ªçš„åŠŸèƒ½ã€‚'
- en: 'This is a **logical structure** of a data warehouse pipeline:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æ•°æ®ä»“åº“ç®¡é“çš„**é€»è¾‘ç»“æ„**ï¼š
- en: '![](../Images/64499bbb581656e8ea35b7133f50d63d.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/64499bbb581656e8ea35b7133f50d63d.png)'
- en: Conceptual data pipeline design. Image by author
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æ¦‚å¿µæ€§æ•°æ®ç®¡é“è®¾è®¡ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: 'In this article I found a way to extract a real-time data from Firebase/Google
    Analytics 4 and load it in BigQuery:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘æ‰¾åˆ°äº†ä¸€ç§ä» Firebase/Google Analytics 4 ä¸­æå–å®æ—¶æ•°æ®å¹¶å°†å…¶åŠ è½½åˆ° BigQuery çš„æ–¹æ³•ï¼š
- en: '[](/how-to-extract-real-time-intraday-data-from-google-analytics-4-and-firebase-in-bigquery-65c9b859550c?source=post_page-----100afa4b93e3--------------------------------)
    [## How to extract real-time intraday data from Google Analytics 4 and Firebase
    in BigQuery'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[## å¦‚ä½•ä» Google Analytics 4 å’Œ Firebase ä¸­æå–å®æ—¶çš„æ—¥å†…æ•°æ®å¹¶åŠ è½½åˆ° BigQuery](https://example.org/how-to-extract-real-time-intraday-data-from-google-analytics-4-and-firebase-in-bigquery-65c9b859550c?source=post_page-----100afa4b93e3--------------------------------)'
- en: And always have an up-to-date data for your custom reports
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¹¶ä¸”å§‹ç»ˆä¸ºæ‚¨çš„è‡ªå®šä¹‰æŠ¥å‘Šä¿æŒæœ€æ–°çš„æ•°æ®
- en: towardsdatascience.com](/how-to-extract-real-time-intraday-data-from-google-analytics-4-and-firebase-in-bigquery-65c9b859550c?source=post_page-----100afa4b93e3--------------------------------)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](https://example.org/how-to-extract-real-time-intraday-data-from-google-analytics-4-and-firebase-in-bigquery-65c9b859550c?source=post_page-----100afa4b93e3--------------------------------)'
- en: 'And this is a conceptual data lake pipeline example:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªæ¦‚å¿µæ€§æ•°æ®æ¹–ç®¡é“ç¤ºä¾‹ï¼š
- en: '![](../Images/b08c6ce7fe26c3b711572352fe360af2.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b08c6ce7fe26c3b711572352fe360af2.png)'
- en: Conceptual data pipeline design. Image by author
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æ¦‚å¿µæ€§æ•°æ®ç®¡é“è®¾è®¡ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: 'For example, in this post I previously wrote how to extract data MySQL databases,
    save it in Cloud Storage so later it could be used for analysis later:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œåœ¨ä¹‹å‰çš„å¸–å­ä¸­ï¼Œæˆ‘å†™äº†å¦‚ä½•ä» MySQL æ•°æ®åº“ä¸­æå–æ•°æ®ï¼Œå°†å…¶ä¿å­˜åˆ° Cloud Storageï¼Œä»¥ä¾¿ç¨åè¿›è¡Œåˆ†æï¼š
- en: '[](/mysql-data-connector-for-your-data-warehouse-solution-db0d338b782d?source=post_page-----100afa4b93e3--------------------------------)
    [## MySQL data connector for your data warehouse solution'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[## MySQL æ•°æ®è¿æ¥å™¨ç”¨äºæ‚¨çš„æ•°æ®ä»“åº“è§£å†³æ–¹æ¡ˆ](https://example.org/mysql-data-connector-for-your-data-warehouse-solution-db0d338b782d?source=post_page-----100afa4b93e3--------------------------------)'
- en: How to build one and export millions of rows in chunks, stream, capture real-time
    data changes or extract data and saveâ€¦
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¦‚ä½•æ„å»ºä¸€ä¸ªå¹¶åˆ†å—å¯¼å‡ºæ•°ç™¾ä¸‡è¡Œã€æµå¼ä¼ è¾“ã€æ•è·å®æ—¶æ•°æ®å˜åŒ–æˆ–æå–æ•°æ®å¹¶ä¿å­˜â€¦â€¦
- en: towardsdatascience.com](/mysql-data-connector-for-your-data-warehouse-solution-db0d338b782d?source=post_page-----100afa4b93e3--------------------------------)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](https://example.org/mysql-data-connector-for-your-data-warehouse-solution-db0d338b782d?source=post_page-----100afa4b93e3--------------------------------)'
- en: 'This is a platform level architecture example:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªå¹³å°çº§æ¶æ„ç¤ºä¾‹ï¼š
- en: '![](../Images/0a7917159d66471313aedd34549ca170.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a7917159d66471313aedd34549ca170.png)'
- en: Platform level data pipeline design. Image by author
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å¹³å°çº§æ•°æ®ç®¡é“è®¾è®¡ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: 'This is a very common pattern for many lake house architecture solutions. In
    this blog post I created a bespoke data ingestion manager that is triggered by
    new object events when they are created in Cloud Storage:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯è®¸å¤šæ¹–ä»“æ¶æ„è§£å†³æ–¹æ¡ˆä¸­éå¸¸å¸¸è§çš„æ¨¡å¼ã€‚åœ¨è¿™ç¯‡åšå®¢æ–‡ç« ä¸­ï¼Œæˆ‘åˆ›å»ºäº†ä¸€ä¸ªå®šåˆ¶çš„æ•°æ®æ‘„å–ç®¡ç†å™¨ï¼Œå®ƒåœ¨ Cloud Storage ä¸­åˆ›å»ºæ–°å¯¹è±¡äº‹ä»¶æ—¶è¢«è§¦å‘ï¼š
- en: '[](/how-to-handle-data-loading-in-bigquery-with-serverless-ingest-manager-and-node-js-4f99fba92436?source=post_page-----100afa4b93e3--------------------------------)
    [## How to Handle Data Loading in BigQuery with Serverless Ingest Manager and
    Node.js'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/how-to-handle-data-loading-in-bigquery-with-serverless-ingest-manager-and-node-js-4f99fba92436?source=post_page-----100afa4b93e3--------------------------------)
    [## å¦‚ä½•ä½¿ç”¨æ— æœåŠ¡å™¨æ‘„å–ç®¡ç†å™¨å’Œ Node.js å¤„ç† BigQuery ä¸­çš„æ•°æ®åŠ è½½'
- en: File formats, yaml pipe definitions, and transform and event triggers for your
    simple and reliable data ingestionâ€¦
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ–‡ä»¶æ ¼å¼ã€yaml ç®¡é“å®šä¹‰ä»¥åŠç”¨äºç®€å•å¯é çš„æ•°æ®æ‘„å–çš„è½¬æ¢å’Œäº‹ä»¶è§¦å‘å™¨â€¦
- en: towardsdatascience.com](/how-to-handle-data-loading-in-bigquery-with-serverless-ingest-manager-and-node-js-4f99fba92436?source=post_page-----100afa4b93e3--------------------------------)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/how-to-handle-data-loading-in-bigquery-with-serverless-ingest-manager-and-node-js-4f99fba92436?source=post_page-----100afa4b93e3--------------------------------)
- en: Streaming
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æµå¤„ç†
- en: Applications can trigger immediate responses to new data events thanks to stream
    processing.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: åº”ç”¨ç¨‹åºå¯ä»¥é€šè¿‡æµå¤„ç†å¯¹æ–°æ•°æ®äº‹ä»¶åšå‡ºå³æ—¶å“åº”ã€‚
- en: Streaming is a â€œmust-haveâ€ solution for enterprise data.
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æµå¤„ç†æ˜¯ä¼ä¸šæ•°æ®çš„â€œå¿…å¤‡â€è§£å†³æ–¹æ¡ˆã€‚
- en: Stream processing would gather and process data as it is generated rather than
    aggregating it at a predetermined frequency, as with batch processing. Common
    use cases are **anomaly detection and fraud prevention**, **real-time personalisation
    and marketing** and **internet of things**. Data and events are often produced
    by a â€œpublisherâ€ or â€œsourceâ€ and transferred to a â€œstream processing application,â€
    where the data is processed immediately before being sent to a â€œsubscriber.â€ Very
    often, as a `source`, you can meet streaming applications built with **Hadoop,
    Apache Kafka, Amazon Kinesis,** etc. The "**Publisher/subscriber**" pattern is
    often referred to as `pub/sub`.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æµå¤„ç†ä¼šåœ¨æ•°æ®ç”Ÿæˆæ—¶æ”¶é›†å’Œå¤„ç†æ•°æ®ï¼Œè€Œä¸æ˜¯åƒæ‰¹å¤„ç†é‚£æ ·åœ¨é¢„å®šçš„é¢‘ç‡ä¸‹è¿›è¡Œæ±‡æ€»ã€‚å¸¸è§çš„ä½¿ç”¨æ¡ˆä¾‹åŒ…æ‹¬**å¼‚å¸¸æ£€æµ‹å’Œæ¬ºè¯ˆé¢„é˜²**ã€**å®æ—¶ä¸ªæ€§åŒ–å’Œè¥é”€**ä»¥åŠ**ç‰©è”ç½‘**ã€‚æ•°æ®å’Œäº‹ä»¶é€šå¸¸ç”±â€œå‘å¸ƒè€…â€æˆ–â€œæºâ€ç”Ÿæˆï¼Œå¹¶ä¼ è¾“åˆ°â€œæµå¤„ç†åº”ç”¨ç¨‹åºâ€ï¼Œåœ¨è¿™é‡Œæ•°æ®è¢«ç«‹å³å¤„ç†ï¼Œç„¶åå‘é€åˆ°â€œè®¢é˜…è€…â€ã€‚é€šå¸¸ï¼Œä½œä¸º`æº`ï¼Œä½ å¯ä»¥é‡åˆ°ä½¿ç”¨**Hadoopã€Apache
    Kafkaã€Amazon Kinesis**ç­‰æ„å»ºçš„æµåº”ç”¨ç¨‹åºã€‚â€œ**å‘å¸ƒè€…/è®¢é˜…è€…**â€æ¨¡å¼é€šå¸¸è¢«ç§°ä¸º`pub/sub`ã€‚
- en: '![](../Images/925417fa8cc74444da6b6fc344e96fe1.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/925417fa8cc74444da6b6fc344e96fe1.png)'
- en: In this example we can set up an **ELT streaming** data pipeline to **AWS Redshift**.
    AWS Firehose delivery stream can offer this type of seamless integration when
    streaming data will be uploaded directly into the data warehouse table. Then data
    will be transformed to create reports with **AWS Quicksight** as a BI tool.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥è®¾ç½®ä¸€ä¸ª**ELT æµå¼**æ•°æ®ç®¡é“åˆ°**AWS Redshift**ã€‚AWS Firehose ä¼ è¾“æµå¯ä»¥æä¾›è¿™ç§æ— ç¼é›†æˆï¼Œå½“æµå¼æ•°æ®ç›´æ¥ä¸Šä¼ åˆ°æ•°æ®ä»“åº“è¡¨æ—¶ã€‚ç„¶åï¼Œæ•°æ®å°†è¢«è½¬åŒ–ä¸ºæŠ¥å‘Šï¼Œä½¿ç”¨**AWS
    Quicksight**ä½œä¸ºBIå·¥å…·ã€‚
- en: Batch processing
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ‰¹å¤„ç†
- en: Batch processing is a model where data is gathered according to a predetermined
    threshold or frequency in both micro-batch processing and conventional batch processing,
    and then processing takes place. Historically workloads were primarily batch-oriented
    in data environments. However, modern applications continuously produce enormous
    amounts of data, and a business leans to micro-batch and streaming processing,
    where data is being processed immediately to maintain a competitive advantage.
    Technologies for **micro-batch** loading include **Apache Spark Streaming, Fluentd,
    and Logstash**, and it is very similar to **conventional batch processing**, where
    events are processed on a schedule or in small groups.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰¹å¤„ç†æ˜¯ä¸€ç§æ¨¡å‹ï¼Œå…¶ä¸­æ•°æ®æ ¹æ®é¢„å®šçš„é˜ˆå€¼æˆ–é¢‘ç‡è¢«æ”¶é›†ï¼Œæ— è®ºæ˜¯å¾®æ‰¹å¤„ç†è¿˜æ˜¯ä¼ ç»Ÿæ‰¹å¤„ç†ï¼Œç„¶åè¿›è¡Œå¤„ç†ã€‚å†å²ä¸Šï¼Œæ•°æ®ç¯å¢ƒä¸­çš„å·¥ä½œè´Ÿè½½ä¸»è¦æ˜¯æ‰¹å¤„ç†å¯¼å‘çš„ã€‚ç„¶è€Œï¼Œç°ä»£åº”ç”¨ç¨‹åºæŒç»­ç”Ÿæˆå¤§é‡æ•°æ®ï¼Œä¼ä¸šå€¾å‘äºå¾®æ‰¹å¤„ç†å’Œæµå¤„ç†ï¼Œåœ¨è¿™ç§å¤„ç†æ–¹å¼ä¸­ï¼Œæ•°æ®è¢«ç«‹å³å¤„ç†ä»¥ä¿æŒç«äº‰ä¼˜åŠ¿ã€‚**å¾®æ‰¹å¤„ç†**åŠ è½½çš„æŠ€æœ¯åŒ…æ‹¬**Apache
    Spark Streamingã€Fluentd å’Œ Logstash**ï¼Œè¿™ä¸**ä¼ ç»Ÿæ‰¹å¤„ç†**éå¸¸ç›¸ä¼¼ï¼Œåœ¨ä¼ ç»Ÿæ‰¹å¤„ç†ä¸­ï¼Œäº‹ä»¶æŒ‰è®¡åˆ’æˆ–å°ç»„å¤„ç†ã€‚
- en: Itâ€™s a good choice where the accuracy of your data is not relevant at this moment
    precisely.
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å½“æ•°æ®çš„å‡†ç¡®æ€§æ­¤æ—¶å¹¶ä¸é‡è¦æ—¶ï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é€‰æ‹©ã€‚
- en: '![](../Images/c47854fffb5d5468b0eade33ef679833.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c47854fffb5d5468b0eade33ef679833.png)'
- en: This data pipeline design pattern works better with smaller datasets that require
    ongoing processing because Athena charges based on the volume of data scanned.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ•°æ®ç®¡é“è®¾è®¡æ¨¡å¼åœ¨éœ€è¦æŒç»­å¤„ç†çš„å°å‹æ•°æ®é›†ä¸Šæ•ˆæœæ›´å¥½ï¼Œå› ä¸º Athena æ˜¯æ ¹æ®æ‰«æçš„æ•°æ®é‡æ”¶è´¹çš„ã€‚
- en: '*Letâ€™s say, you donâ€™t want to use* `*change log*` *on MySQL database instance.
    That would be an ideal case because payments dataset is not huge.*'
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*å‡è®¾ä½ ä¸æƒ³åœ¨ MySQL æ•°æ®åº“å®ä¾‹ä¸Šä½¿ç”¨* `*change log*` *ã€‚è¿™å°†æ˜¯ç†æƒ³çš„æƒ…å†µï¼Œå› ä¸ºæ”¯ä»˜æ•°æ®é›†å¹¶ä¸åºå¤§ã€‚*'
- en: Lambda/Kappa architecture
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Lambda/Kappa æ¶æ„
- en: This architecture combines batch and streaming methodologies. It combines the
    best of both worlds and advises that raw data must be retained, for example, in
    a data lake in case you would want to use it again to build a new pipeline or
    investigate an outage. It has both **batch** and **streaming** (speed) layers
    which helps to respond instantly to shifting business and market conditions. Lambda
    architectures can sometimes very complicated with multiple code repositories to
    maintain.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ¶æ„ç»“åˆäº†æ‰¹å¤„ç†å’Œæµå¤„ç†æ–¹æ³•ã€‚å®ƒç»“åˆäº†ä¸¤è€…çš„ä¼˜ç‚¹ï¼Œå¹¶å»ºè®®å¿…é¡»ä¿ç•™åŸå§‹æ•°æ®ï¼Œä¾‹å¦‚ï¼Œä¿å­˜åœ¨æ•°æ®æ¹–ä¸­ï¼Œä»¥ä¾¿åœ¨ä½ æƒ³è¦å†æ¬¡ä½¿ç”¨å®ƒæ¥æ„å»ºæ–°ç®¡é“æˆ–è°ƒæŸ¥æ•…éšœæ—¶ä½¿ç”¨ã€‚å®ƒå…·æœ‰**æ‰¹å¤„ç†**å’Œ**æµå¤„ç†**ï¼ˆé€Ÿåº¦ï¼‰å±‚ï¼Œè¿™æœ‰åŠ©äºå¿«é€Ÿå“åº”å˜åŒ–çš„ä¸šåŠ¡å’Œå¸‚åœºæ¡ä»¶ã€‚Lambda
    æ¶æ„æœ‰æ—¶å¯èƒ½ä¼šéå¸¸å¤æ‚ï¼Œéœ€è¦ç»´æŠ¤å¤šä¸ªä»£ç åº“ã€‚
- en: '![](../Images/85af40b044d54265d18fb38f643a7c49.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/85af40b044d54265d18fb38f643a7c49.png)'
- en: Lambda data pipeline design. Image by author
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda æ•°æ®ç®¡é“è®¾è®¡ã€‚å›¾åƒç”±ä½œè€…æä¾›
- en: Transform first then Load?
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å…ˆè½¬æ¢ç„¶ååŠ è½½ï¼Ÿ
- en: '**ETL** is considered to be a conventional approach and the most widely used
    historically. With the rise of data warehousing **ELT** becomes increasingly popular.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**ETL** è¢«è®¤ä¸ºæ˜¯ä¸€ç§ä¼ ç»Ÿçš„æ–¹æ³•ï¼Œå¹¶ä¸”å†å²ä¸Šæœ€å¹¿æ³›ä½¿ç”¨ã€‚éšç€æ•°æ®ä»“åº“çš„å…´èµ·ï¼Œ**ELT** å˜å¾—è¶Šæ¥è¶Šæµè¡Œã€‚'
- en: '*Indeed, why do we need to transform first if we can centralise it for all
    data pipelines in the data warehouse?*'
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*ç¡®å®ï¼Œå¦‚æœæˆ‘ä»¬å¯ä»¥åœ¨æ•°æ®ä»“åº“ä¸­é›†ä¸­å¤„ç†ï¼Œä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦å…ˆè¿›è¡Œè½¬æ¢å‘¢ï¼Ÿ*'
- en: '**Vurtualisation** is another popular approach for data warehouses where we
    create **views** on data instead of meterialised tables. New requirements to business
    agility put cost-effectiveness on the send plan and data users qould query **views**
    instead of **tables**.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**è™šæ‹ŸåŒ–** æ˜¯å¦ä¸€ç§æ•°æ®ä»“åº“çš„æµè¡Œæ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨æ•°æ®ä¸Šåˆ›å»º**è§†å›¾**è€Œä¸æ˜¯ç‰©åŒ–è¡¨ã€‚å¯¹ä¸šåŠ¡æ•æ·æ€§çš„æ–°å¢è¦æ±‚å°†æˆæœ¬æ•ˆç›Šç½®äºæ¬¡è¦ä½ç½®ï¼Œæ•°æ®ç”¨æˆ·å¯ä»¥æŸ¥è¯¢**è§†å›¾**è€Œä¸æ˜¯**è¡¨**ã€‚'
- en: '**Change Data capture** is another approach to update the data exactly when
    the changes occur. When used typically latent data pipelines, CDC technology can
    recognize data changes as they happen and offer information about those changes.
    Changes are usually pushed to message queue or provided as a stream.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**å˜æ›´æ•°æ®æ•è·** æ˜¯å¦ä¸€ç§åœ¨å˜åŒ–å‘ç”Ÿæ—¶å³æ—¶æ›´æ–°æ•°æ®çš„æ–¹æ³•ã€‚å½“é€šå¸¸ä½¿ç”¨å»¶è¿Ÿæ•°æ®ç®¡é“æ—¶ï¼ŒCDC æŠ€æœ¯å¯ä»¥è¯†åˆ«æ•°æ®å˜åŒ–å¹¶æä¾›æœ‰å…³è¿™äº›å˜åŒ–çš„ä¿¡æ¯ã€‚å˜åŒ–é€šå¸¸ä¼šè¢«æ¨é€åˆ°æ¶ˆæ¯é˜Ÿåˆ—æˆ–ä½œä¸ºæµæä¾›ã€‚'
- en: How to choose the data pipeline architecture?
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦‚ä½•é€‰æ‹©æ•°æ®ç®¡é“æ¶æ„ï¼Ÿ
- en: In recent years, data architecture components such as data pipelines have developed
    to support massive volumes of data. The term â€œBig Dataâ€ can be described as having
    three traits known as volume, variety and velocity. Big data can open up new opportunities
    in a variety of use cases, including predictive analytics, real-time reporting,
    and alerting. Architects and developers have had to adapt to new â€œbig dataâ€ requirements
    because of the substantially increased volume, diversity, and velocity of data.
    New data processing frameworks emerged and kept emerging. Due to the high velocity
    of modern data streams, we might want to use *streaming* data pipelines. Data
    may then be collected and analysed in real-time, allowing for immediate action.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: è¿‘å¹´æ¥ï¼Œæ•°æ®æ¶æ„ç»„ä»¶å¦‚æ•°æ®ç®¡é“å·²ç»å‘å±•ï¼Œä»¥æ”¯æŒå¤§é‡æ•°æ®ã€‚æœ¯è¯­â€œå¤§æ•°æ®â€å¯ä»¥æè¿°ä¸ºå…·æœ‰ä¸‰ä¸ªç‰¹å¾ï¼šä½“é‡ã€ç§ç±»å’Œé€Ÿåº¦ã€‚å¤§æ•°æ®å¯ä»¥åœ¨å„ç§ç”¨ä¾‹ä¸­å¼€å¯æ–°çš„æœºä¼šï¼ŒåŒ…æ‹¬é¢„æµ‹åˆ†æã€å®æ—¶æŠ¥å‘Šå’Œè­¦æŠ¥ã€‚ç”±äºæ•°æ®é‡ã€ç§ç±»å’Œé€Ÿåº¦çš„æ˜¾è‘—å¢åŠ ï¼Œæ¶æ„å¸ˆå’Œå¼€å‘è€…ä¸å¾—ä¸é€‚åº”æ–°çš„â€œå¤§æ•°æ®â€éœ€æ±‚ã€‚æ–°çš„æ•°æ®å¤„ç†æ¡†æ¶ä¸æ–­å‡ºç°ã€‚ç”±äºç°ä»£æ•°æ®æµçš„é«˜é€Ÿï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦ä½¿ç”¨*æµå¼*æ•°æ®ç®¡é“ã€‚æ•°æ®å¯ä»¥å®æ—¶æ”¶é›†å’Œåˆ†æï¼Œä»è€Œå…è®¸ç«‹å³é‡‡å–è¡ŒåŠ¨ã€‚
- en: '*However, streaming data pipeline design pattern is not always the most cost-effective.*'
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*ç„¶è€Œï¼Œæµå¼æ•°æ®ç®¡é“è®¾è®¡æ¨¡å¼å¹¶ä¸æ€»æ˜¯æœ€å…·æˆæœ¬æ•ˆç›Šçš„ã€‚*'
- en: For example, in the majority of data warehouse solutions batch data ingestion
    is free. However, streaming, might go with a price. Same statement would be relevant
    for data processing. **Steaming** is the most expensive way to process the data
    in the majority of cases.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œåœ¨å¤§å¤šæ•°æ•°æ®ä»“åº“è§£å†³æ–¹æ¡ˆä¸­ï¼Œæ‰¹é‡æ•°æ®æ‘„å–æ˜¯å…è´¹çš„ã€‚ç„¶è€Œï¼Œæµå¼å¤„ç†å¯èƒ½ä¼šå¸¦æ¥è´¹ç”¨ã€‚åŒæ ·çš„è¯´æ³•é€‚ç”¨äºæ•°æ®å¤„ç†ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œ**æµå¼**å¤„ç†æ˜¯å¤„ç†æ•°æ®çš„æœ€æ˜‚è´µæ–¹å¼ã€‚
- en: Big Data `volumes` require the data pipeline to be able to process events concurrently
    as very often they are sent simultaneously at once. Data solutions must be scalable.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§æ•°æ®`volumes`è¦æ±‚æ•°æ®ç®¡é“èƒ½å¤ŸåŒæ—¶å¤„ç†äº‹ä»¶ï¼Œå› ä¸ºè¿™äº›äº‹ä»¶é€šå¸¸æ˜¯åŒæ—¶å‘é€çš„ã€‚æ•°æ®è§£å†³æ–¹æ¡ˆå¿…é¡»å…·å¤‡å¯æ‰©å±•æ€§ã€‚
- en: The `variety` implies that data might come through the pipelines in different
    formats, very often unstructured or semi-structured.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`variety`æ„å‘³ç€æ•°æ®å¯èƒ½ä»¥ä¸åŒçš„æ ¼å¼é€šè¿‡ç®¡é“ä¼ è¾“ï¼Œé€šå¸¸æ˜¯éç»“æ„åŒ–æˆ–åŠç»“æ„åŒ–çš„ã€‚'
- en: Architecture type depends on various factors, i.e. `destination` type and where
    data has to be in the end, **cost considerations** and your team's **development
    stack** and certain data processing skills you and your colleagues already have.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: æ¶æ„ç±»å‹å–å†³äºå„ç§å› ç´ ï¼Œå³`destination`ç±»å‹å’Œæ•°æ®æœ€ç»ˆè¦åˆ°è¾¾çš„ä½ç½®ã€**æˆæœ¬è€ƒè™‘**ä»¥åŠä½ å›¢é˜Ÿçš„**å¼€å‘æ ˆ**å’Œä½ ä¸åŒäº‹ä»¬å·²ç»å…·å¤‡çš„æŸäº›æ•°æ®å¤„ç†æŠ€èƒ½ã€‚
- en: Do your data pipelines have to be managed and cloud-based, or would you rather
    want to deploy it on-premises?
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ä½ çš„æ•°æ®ç®¡é“æ˜¯å¦å¿…é¡»æ˜¯å¯ç®¡ç†ä¸”åŸºäºäº‘çš„ï¼Œè¿˜æ˜¯æ›´æ„¿æ„å°†å…¶éƒ¨ç½²åœ¨æœ¬åœ°ï¼Ÿ
- en: In reality, there are numerous combinations of variables that can help to select
    the best data platform architecture. **What would be the velocity or data flow
    rate in that pipeline? Do you require real-time analytics, or is near-real-time
    sufficient?** This would resolve the question of whether or not you require "streaming"
    pipelines.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œæœ‰è®¸å¤šå˜é‡ç»„åˆå¯ä»¥å¸®åŠ©é€‰æ‹©æœ€ä½³çš„æ•°æ®å¹³å°æ¶æ„ã€‚**é‚£ä¸ªç®¡é“ä¸­çš„é€Ÿåº¦æˆ–æ•°æ®æµç‡ä¼šæ˜¯å¤šå°‘ï¼Ÿä½ æ˜¯å¦éœ€è¦å®æ—¶åˆ†æï¼Œè¿˜æ˜¯è¿‘å®æ—¶çš„åˆ†æå°±è¶³å¤Ÿäº†ï¼Ÿ**è¿™å°†è§£å†³ä½ æ˜¯å¦éœ€è¦â€œæµâ€ç®¡é“çš„é—®é¢˜ã€‚
- en: For example, there are services that can create and run both **streaming** and
    **batch** data pipelines, i.e. [Google Dataflow](https://cloud.google.com/dataflow/docs/guides/data-pipelines).
    So how is it different from any other pipeline built in the data warehouse solution?
    The choice would depend on existing infrastructure. For example, if you have some
    existing **Hadoop** workloads, then GCP DataFlow would be a wrong choice as it
    will not let you to re-use the code (it is using Apachec Beam). In this case you
    would want to use **GCP Dataproc** which works on **Hadoop/Spark** code.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œæœ‰äº›æœåŠ¡å¯ä»¥åˆ›å»ºå’Œè¿è¡Œ**æµ**å’Œ**æ‰¹å¤„ç†**æ•°æ®ç®¡é“ï¼Œå³[Google Dataflow](https://cloud.google.com/dataflow/docs/guides/data-pipelines)ã€‚é‚£ä¹ˆå®ƒä¸æ•°æ®ä»“åº“è§£å†³æ–¹æ¡ˆä¸­çš„å…¶ä»–ç®¡é“æœ‰ä½•ä¸åŒï¼Ÿé€‰æ‹©å–å†³äºç°æœ‰çš„åŸºç¡€è®¾æ–½ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æœ‰ä¸€äº›ç°æœ‰çš„**Hadoop**å·¥ä½œè´Ÿè½½ï¼Œé‚£ä¹ˆGCP
    DataFlowå°†æ˜¯ä¸€ä¸ªé”™è¯¯çš„é€‰æ‹©ï¼Œå› ä¸ºå®ƒä¸å…è®¸ä½ é‡ç”¨ä»£ç ï¼ˆå®ƒä½¿ç”¨çš„æ˜¯Apache Beamï¼‰ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ ä¼šå¸Œæœ›ä½¿ç”¨**GCP Dataproc**ï¼Œå®ƒå¯ä»¥å¤„ç†**Hadoop/Spark**ä»£ç ã€‚
- en: '*The rule of thumb is that if the processing is dependent on any tools in the
    Hadoop ecosystem, Dataproc should be utilized.*'
  id: totrans-80
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*ä¸€ä¸ªç»éªŒæ³•åˆ™æ˜¯ï¼Œå¦‚æœå¤„ç†ä¾èµ–äºHadoopç”Ÿæ€ç³»ç»Ÿä¸­çš„ä»»ä½•å·¥å…·ï¼Œåˆ™åº”ä½¿ç”¨Dataprocã€‚*'
- en: It is basically a Hadoop extension service.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒåŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ªHadoopæ‰©å±•æœåŠ¡ã€‚
- en: On the other hand, if you are not limited by existing code and would like to
    reliably process ever-increasing amounts of **streaming** data then **Dataflow**
    is the recommended choice. You can check these [Dataflow templates](https://github.com/GoogleCloudPlatform/DataflowTemplates)
    if you like to do things in **JAVA**.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€æ–¹é¢ï¼Œå¦‚æœä½ ä¸å—ç°æœ‰ä»£ç çš„é™åˆ¶ï¼Œå¹¶ä¸”å¸Œæœ›å¯é åœ°å¤„ç†ä¸æ–­å¢åŠ çš„**æµæ•°æ®**ï¼Œé‚£ä¹ˆ**Dataflow**æ˜¯æ¨èçš„é€‰æ‹©ã€‚å¦‚æœä½ å–œæ¬¢ç”¨**JAVA**åšäº‹ï¼Œå¯ä»¥æŸ¥çœ‹è¿™äº›[Dataflowæ¨¡æ¿](https://github.com/GoogleCloudPlatform/DataflowTemplates)ã€‚
- en: There is a [system design guide from Google](https://cloud.google.com/architecture)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæœ‰ä¸€ä¸ªæ¥è‡ªGoogleçš„[ç³»ç»Ÿè®¾è®¡æŒ‡å—](https://cloud.google.com/architecture)ã€‚
- en: Conclusion
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: BigData posed new challenging data architecture requirements for data developers.
    A constantly increasing variety of data formats and data sources increased the
    importance of data integration without disrupting production applications. Businesses
    are increasingly aiming to automate data integration procedures, process streaming
    data in real-time, and streamline the lifetime of data lakes and warehouses. This
    becomes a challenging task indeed, taking into account the increased data volume,
    velocity and variety of data formats over the last decade. Now data pipeline design
    must be robust and, at the same time, flexible to enable new data pipeline creation
    in a simplified and automated way. The increasing trend of using `data mesh`/
    `data mart` platforms requires data catalogues to be created. To create controlled,
    enterprise-ready data and give data consumers an easy method to find, examine,
    and self-provision data, this process should ideally be automated too. Therefore,
    choosing the right data pipeline architecture can solve these issues effectively.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§æ•°æ®æå‡ºäº†å¯¹æ•°æ®æ¶æ„çš„æ–°æŒ‘æˆ˜è¦æ±‚ã€‚ä¸æ–­å¢åŠ çš„æ•°æ®æ ¼å¼å’Œæ•°æ®æºçš„å¤šæ ·æ€§æé«˜äº†æ•°æ®é›†æˆçš„é‡è¦æ€§ï¼Œè€Œä¸å½±å“ç”Ÿäº§åº”ç”¨ã€‚ä¼ä¸šè¶Šæ¥è¶Šå€¾å‘äºè‡ªåŠ¨åŒ–æ•°æ®é›†æˆç¨‹åºã€å®æ—¶å¤„ç†æµæ•°æ®ï¼Œå¹¶ç®€åŒ–æ•°æ®æ¹–å’Œæ•°æ®ä»“åº“çš„ç”Ÿå‘½å‘¨æœŸã€‚è€ƒè™‘åˆ°è¿‡å»åå¹´æ•°æ®é‡ã€é€Ÿåº¦å’Œæ•°æ®æ ¼å¼çš„å¤šæ ·æ€§ä¸æ–­å¢åŠ ï¼Œè¿™ç¡®å®æˆä¸ºäº†ä¸€é¡¹æŒ‘æˆ˜ã€‚ç°åœ¨ï¼Œæ•°æ®ç®¡é“è®¾è®¡å¿…é¡»æ—¢ç¨³å¥åˆçµæ´»ï¼Œä»¥ç®€åŒ–å’Œè‡ªåŠ¨åŒ–åˆ›å»ºæ–°çš„æ•°æ®ç®¡é“ã€‚æ—¥ç›Šå¢åŠ çš„ä½¿ç”¨
    `data mesh`/ `data mart` å¹³å°çš„è¶‹åŠ¿è¦æ±‚åˆ›å»ºæ•°æ®ç›®å½•ã€‚ä¸ºäº†åˆ›å»ºå—æ§çš„ã€ä¼ä¸šçº§çš„æ•°æ®ï¼Œå¹¶ä¸ºæ•°æ®æ¶ˆè´¹è€…æä¾›ä¾¿æ·çš„æ–¹æ³•æ¥æŸ¥æ‰¾ã€æ£€æŸ¥å’Œè‡ªåŠ©æä¾›æ•°æ®ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¹Ÿåº”å°½å¯èƒ½è‡ªåŠ¨åŒ–ã€‚å› æ­¤ï¼Œé€‰æ‹©åˆé€‚çš„æ•°æ®ç®¡é“æ¶æ„å¯ä»¥æœ‰æ•ˆè§£å†³è¿™äº›é—®é¢˜ã€‚
- en: '*Originally published at* [*https://mydataschool.com*](https://mydataschool.com/blog/data-pipeline-design-patterns/)*.*'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*æœ€åˆå‘å¸ƒäº* [*https://mydataschool.com*](https://mydataschool.com/blog/data-pipeline-design-patterns/)*ã€‚*'
- en: 'Recommended read:'
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨èé˜…è¯»ï¼š
- en: '[](https://cloud.google.com/dataflow/docs/guides/data-pipelines?source=post_page-----100afa4b93e3--------------------------------)
    [## Work with Data Pipelines | Cloud Dataflow | Google Cloud'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://cloud.google.com/dataflow/docs/guides/data-pipelines?source=post_page-----100afa4b93e3--------------------------------)
    [## ä½¿ç”¨æ•°æ®ç®¡é“ | Cloud Dataflow | Google Cloud'
- en: 'You can report Dataflow Data Pipelines issues and request new features at Note:
    google-data-pipelines-feedback." Youâ€¦'
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'ä½ å¯ä»¥åœ¨ [Note: google-data-pipelines-feedback](https://example.com) æŠ¥å‘Š Dataflow
    æ•°æ®ç®¡é“é—®é¢˜å¹¶è¯·æ±‚æ–°åŠŸèƒ½ã€‚ä½ â€¦'
- en: cloud.google.com](https://cloud.google.com/dataflow/docs/guides/data-pipelines?source=post_page-----100afa4b93e3--------------------------------)
    [](https://cloud.google.com/architecture?source=post_page-----100afa4b93e3--------------------------------)
    [## Cloud Architecture Guidance and Topologies | Cloud Architecture Center | Google
    Cloud
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[cloud.google.com](https://cloud.google.com/dataflow/docs/guides/data-pipelines?source=post_page-----100afa4b93e3--------------------------------)
    [](https://cloud.google.com/architecture?source=post_page-----100afa4b93e3--------------------------------)
    [## äº‘æ¶æ„æŒ‡å¯¼å’Œæ‹“æ‰‘ | äº‘æ¶æ„ä¸­å¿ƒ | Google Cloud'
- en: Discover reference architectures, guidance, and best practices for building
    or migrating your workloads on Googleâ€¦
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å‘ç°å‚è€ƒæ¶æ„ã€æŒ‡å¯¼å’Œæœ€ä½³å®è·µï¼Œä»¥å¸®åŠ©ä½ åœ¨ Google ä¸Šæ„å»ºæˆ–è¿ç§»å·¥ä½œè´Ÿè½½â€¦
- en: 'cloud.google.com](https://cloud.google.com/architecture?source=post_page-----100afa4b93e3--------------------------------)
    [](https://github.com/GoogleCloudPlatform/DataflowTemplates?source=post_page-----100afa4b93e3--------------------------------)
    [## GitHub - GoogleCloudPlatform/DataflowTemplates: Google-provided Cloud Dataflow
    template pipelinesâ€¦'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[cloud.google.com](https://cloud.google.com/architecture?source=post_page-----100afa4b93e3--------------------------------)
    [](https://github.com/GoogleCloudPlatform/DataflowTemplates?source=post_page-----100afa4b93e3--------------------------------)
    [## GitHub - GoogleCloudPlatform/DataflowTemplates: Google æä¾›çš„ Cloud Dataflow
    æ¨¡æ¿ç®¡é“â€¦'
- en: These Dataflow templates are an effort to solve simple, but large, in-Cloud
    data tasks, including dataâ€¦
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è¿™äº› Dataflow æ¨¡æ¿æ—¨åœ¨è§£å†³ç®€å•ä½†å¤§è§„æ¨¡çš„äº‘ç«¯æ•°æ®ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ•°æ®â€¦
- en: github.com](https://github.com/GoogleCloudPlatform/DataflowTemplates?source=post_page-----100afa4b93e3--------------------------------)  [##
    Tutorials
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/GoogleCloudPlatform/DataflowTemplates?source=post_page-----100afa4b93e3--------------------------------)
    [## æ•™ç¨‹'
- en: Tutorials - AWS Data Pipeline The following tutorials walk you step-by-step
    through the process of creating and usingâ€¦
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æ•™ç¨‹ - AWS æ•°æ®ç®¡é“ ä»¥ä¸‹æ•™ç¨‹å°†é€æ­¥å¼•å¯¼ä½ åˆ›å»ºå’Œä½¿ç”¨â€¦
- en: docs.aws.amazon.com](https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html?source=post_page-----100afa4b93e3--------------------------------)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[docs.aws.amazon.com](https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html?source=post_page-----100afa4b93e3--------------------------------)'
- en: AWS Data Pipeline documentation [[https://docs.aws.amazon.com/data-pipeline](https://docs.aws.amazon.com/data-pipeline/index.html)/]
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: AWS æ•°æ®ç®¡é“æ–‡æ¡£ [[https://docs.aws.amazon.com/data-pipeline](https://docs.aws.amazon.com/data-pipeline/index.html)/]
