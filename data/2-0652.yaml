- en: Data pipeline design patterns
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据管道设计模式
- en: 原文：[https://towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3](https://towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3](https://towardsdatascience.com/data-pipeline-design-patterns-100afa4b93e3)
- en: Choosing the right architecture with examples
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择合适的架构及其示例
- en: '[](https://mshakhomirov.medium.com/?source=post_page-----100afa4b93e3--------------------------------)[![💡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----100afa4b93e3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----100afa4b93e3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----100afa4b93e3--------------------------------)
    [💡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----100afa4b93e3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mshakhomirov.medium.com/?source=post_page-----100afa4b93e3--------------------------------)[![💡Mike
    Shakhomirov](../Images/bc6895c7face3244d488feb97ba0f68e.png)](https://mshakhomirov.medium.com/?source=post_page-----100afa4b93e3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----100afa4b93e3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----100afa4b93e3--------------------------------)
    [💡Mike Shakhomirov](https://mshakhomirov.medium.com/?source=post_page-----100afa4b93e3--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----100afa4b93e3--------------------------------)
    ·9 min read·Jan 2, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----100afa4b93e3--------------------------------)
    ·9 分钟阅读·2023年1月2日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/074dc1570ad17f76947283225050b8db.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/074dc1570ad17f76947283225050b8db.png)'
- en: Photo by [israel palacio](https://unsplash.com/@othentikisra?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [israel palacio](https://unsplash.com/@othentikisra?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) 提供
- en: Typically data is processed, extracted, and transformed in steps. Therefore,
    a sequence of data processing stages can be referred to as a **data pipeline**.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 通常数据在多个步骤中被处理、提取和转换。因此，一系列数据处理阶段可以称为**数据管道**。
- en: '*Which design pattern to choose?*'
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*选择哪个设计模式？*'
- en: There are lots of things to consider, i.e. **Which data stack to use? What tools
    to consider? How to design a data pipeline conceptually? ETL or ELT? Maybe ETLT?
    What is Change Data Capture?**
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 需要考虑许多因素，即**使用哪个数据栈？考虑哪些工具？如何从概念上设计数据管道？ETL 还是 ELT？也许是 ETLT？什么是变更数据捕获？**
- en: I will try to cover these questions here.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在这里尝试涵盖这些问题。
- en: A data pipeline
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个数据管道
- en: So it is a sequence of data processing steps. Due to ***logical data flow connections***
    between these stages, each stage generates an **output** that serves as an **input**
    for the following stage.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 所以它是一个数据处理步骤的序列。由于这些阶段之间的***逻辑数据流连接***，每个阶段都会生成一个**输出**，作为下一个阶段的**输入**。
- en: There is a data pipeline whenever there is data processing between points A
    and B.
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 每当在 A 点和 B 点之间进行数据处理时，就会有一个数据管道。
- en: A data pipeline’s three major parts are a **source, a processing step or steps,
    and a destination**. Data extracted from an external API (a source) can then be
    loaded into the data warehouse (destination). This is an example of a most common
    data pipeline where the source and destination are different. However, it is not
    always the case, as *destination-to-destination* pipelines also exist.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 数据管道的三个主要部分是**源、处理步骤或步骤、以及目标**。从外部 API（源）提取的数据可以加载到数据仓库（目标）中。这是一个最常见的数据管道的例子，其中源和目标是不同的。然而，这并非总是如此，因为*目标到目标*的管道也存在。
- en: For example, data can originate as a `reference` table in the data warehouse
    in the first place and then after some data transformation, it can *land* in a
    new schema, for example, in `analytics` to be used in reporting solutions.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，数据可以最初作为数据仓库中的`reference`表格存在，然后经过一些数据转换后，可以*落地*到一个新的模式中，例如，在`analytics`中用于报告解决方案。
- en: '*There is always a data pipeline when data is processed between source and
    destination.*'
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*每当在源和目标之间处理数据时，总会有一个数据管道。*'
- en: '![](../Images/02c0d6d925b544a4cc7d82adbc9c1da9.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02c0d6d925b544a4cc7d82adbc9c1da9.png)'
- en: Data pipeline. Image by author
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 数据管道。图片由作者提供
- en: Event data created by just one `source` at the back-end, an event stream built
    with Kinesis Firehose or Kafka stream, can feed a number of various `destinations`.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 由后端仅一个`source`创建的事件数据，通过使用 Kinesis Firehose 或 Kafka 流构建的事件流，可以提供给多个不同的`destinations`。
- en: Consider ***user engagement* data** from *Google Analytics* as it flows as an
    event stream that can be used in both analytics dashboards for user activity and
    in the Machine learning (ML) pipeline for churn prediction.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑来自*Google Analytics*的***用户参与数据***，因为它作为事件流流动，可以用于用户活动的分析仪表板以及用于流失预测的机器学习（ML）管道中。
- en: Despite using the same data source, both pipelines operate independently and
    must successfully complete before the user can see the results.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用相同的数据源，但两个管道独立操作，必须成功完成后用户才能看到结果。
- en: Alternatively, data from two or more `source` locations can be aggregated into
    just one `destination`. For example, data from different payment merchant providers
    can be transformed into a revenue report for the Business Intelligence (BI) dashboard.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，可以将来自两个或更多`source`位置的数据汇总到一个`destination`中。例如，来自不同支付商提供商的数据可以转换为业务智能（BI）仪表板的收入报告。
- en: Data quality checks, data cleansing, transformation, enrichment, filtering,
    grouping, aggregation, and the application of algorithms to the data are frequent
    steps in data pipelines.
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 数据质量检查、数据清洗、转换、丰富、过滤、分组、聚合以及对数据应用算法是数据管道中的常见步骤。
- en: Architecture types and examples
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构类型和示例
- en: Data pipeline architecture as a term might mean several things depending on
    the situation. In general, it can be split into **conceptual** (logical) and **platform**
    levels or architecture types.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 数据管道架构这一术语根据情况可能有多种含义。一般来说，它可以分为**概念性**（逻辑）和**平台**级别或架构类型。
- en: The **conceptually** logical part describes how a dataset is processed and transformed
    from collection to serving, whereas **platform architecture** focuses on an individual
    set of tools and frameworks used in a given scenario, as well as the functions
    that each of them plays.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**概念性**逻辑部分描述了数据集从采集到服务的处理和转换方式，而**平台架构**则专注于特定场景中使用的一组工具和框架，以及它们各自的功能。'
- en: 'This is a **logical structure** of a data warehouse pipeline:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这是数据仓库管道的**逻辑结构**：
- en: '![](../Images/64499bbb581656e8ea35b7133f50d63d.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/64499bbb581656e8ea35b7133f50d63d.png)'
- en: Conceptual data pipeline design. Image by author
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 概念性数据管道设计。图片由作者提供
- en: 'In this article I found a way to extract a real-time data from Firebase/Google
    Analytics 4 and load it in BigQuery:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我找到了一种从 Firebase/Google Analytics 4 中提取实时数据并将其加载到 BigQuery 的方法：
- en: '[](/how-to-extract-real-time-intraday-data-from-google-analytics-4-and-firebase-in-bigquery-65c9b859550c?source=post_page-----100afa4b93e3--------------------------------)
    [## How to extract real-time intraday data from Google Analytics 4 and Firebase
    in BigQuery'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 如何从 Google Analytics 4 和 Firebase 中提取实时的日内数据并加载到 BigQuery](https://example.org/how-to-extract-real-time-intraday-data-from-google-analytics-4-and-firebase-in-bigquery-65c9b859550c?source=post_page-----100afa4b93e3--------------------------------)'
- en: And always have an up-to-date data for your custom reports
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 并且始终为您的自定义报告保持最新的数据
- en: towardsdatascience.com](/how-to-extract-real-time-intraday-data-from-google-analytics-4-and-firebase-in-bigquery-65c9b859550c?source=post_page-----100afa4b93e3--------------------------------)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](https://example.org/how-to-extract-real-time-intraday-data-from-google-analytics-4-and-firebase-in-bigquery-65c9b859550c?source=post_page-----100afa4b93e3--------------------------------)'
- en: 'And this is a conceptual data lake pipeline example:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个概念性数据湖管道示例：
- en: '![](../Images/b08c6ce7fe26c3b711572352fe360af2.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b08c6ce7fe26c3b711572352fe360af2.png)'
- en: Conceptual data pipeline design. Image by author
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 概念性数据管道设计。图片由作者提供
- en: 'For example, in this post I previously wrote how to extract data MySQL databases,
    save it in Cloud Storage so later it could be used for analysis later:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在之前的帖子中，我写了如何从 MySQL 数据库中提取数据，将其保存到 Cloud Storage，以便稍后进行分析：
- en: '[](/mysql-data-connector-for-your-data-warehouse-solution-db0d338b782d?source=post_page-----100afa4b93e3--------------------------------)
    [## MySQL data connector for your data warehouse solution'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[## MySQL 数据连接器用于您的数据仓库解决方案](https://example.org/mysql-data-connector-for-your-data-warehouse-solution-db0d338b782d?source=post_page-----100afa4b93e3--------------------------------)'
- en: How to build one and export millions of rows in chunks, stream, capture real-time
    data changes or extract data and save…
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何构建一个并分块导出数百万行、流式传输、捕获实时数据变化或提取数据并保存……
- en: towardsdatascience.com](/mysql-data-connector-for-your-data-warehouse-solution-db0d338b782d?source=post_page-----100afa4b93e3--------------------------------)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](https://example.org/mysql-data-connector-for-your-data-warehouse-solution-db0d338b782d?source=post_page-----100afa4b93e3--------------------------------)'
- en: 'This is a platform level architecture example:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个平台级架构示例：
- en: '![](../Images/0a7917159d66471313aedd34549ca170.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a7917159d66471313aedd34549ca170.png)'
- en: Platform level data pipeline design. Image by author
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 平台级数据管道设计。图片由作者提供
- en: 'This is a very common pattern for many lake house architecture solutions. In
    this blog post I created a bespoke data ingestion manager that is triggered by
    new object events when they are created in Cloud Storage:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这是许多湖仓架构解决方案中非常常见的模式。在这篇博客文章中，我创建了一个定制的数据摄取管理器，它在 Cloud Storage 中创建新对象事件时被触发：
- en: '[](/how-to-handle-data-loading-in-bigquery-with-serverless-ingest-manager-and-node-js-4f99fba92436?source=post_page-----100afa4b93e3--------------------------------)
    [## How to Handle Data Loading in BigQuery with Serverless Ingest Manager and
    Node.js'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/how-to-handle-data-loading-in-bigquery-with-serverless-ingest-manager-and-node-js-4f99fba92436?source=post_page-----100afa4b93e3--------------------------------)
    [## 如何使用无服务器摄取管理器和 Node.js 处理 BigQuery 中的数据加载'
- en: File formats, yaml pipe definitions, and transform and event triggers for your
    simple and reliable data ingestion…
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文件格式、yaml 管道定义以及用于简单可靠的数据摄取的转换和事件触发器…
- en: towardsdatascience.com](/how-to-handle-data-loading-in-bigquery-with-serverless-ingest-manager-and-node-js-4f99fba92436?source=post_page-----100afa4b93e3--------------------------------)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/how-to-handle-data-loading-in-bigquery-with-serverless-ingest-manager-and-node-js-4f99fba92436?source=post_page-----100afa4b93e3--------------------------------)
- en: Streaming
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流处理
- en: Applications can trigger immediate responses to new data events thanks to stream
    processing.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序可以通过流处理对新数据事件做出即时响应。
- en: Streaming is a “must-have” solution for enterprise data.
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 流处理是企业数据的“必备”解决方案。
- en: Stream processing would gather and process data as it is generated rather than
    aggregating it at a predetermined frequency, as with batch processing. Common
    use cases are **anomaly detection and fraud prevention**, **real-time personalisation
    and marketing** and **internet of things**. Data and events are often produced
    by a “publisher” or “source” and transferred to a “stream processing application,”
    where the data is processed immediately before being sent to a “subscriber.” Very
    often, as a `source`, you can meet streaming applications built with **Hadoop,
    Apache Kafka, Amazon Kinesis,** etc. The "**Publisher/subscriber**" pattern is
    often referred to as `pub/sub`.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理会在数据生成时收集和处理数据，而不是像批处理那样在预定的频率下进行汇总。常见的使用案例包括**异常检测和欺诈预防**、**实时个性化和营销**以及**物联网**。数据和事件通常由“发布者”或“源”生成，并传输到“流处理应用程序”，在这里数据被立即处理，然后发送到“订阅者”。通常，作为`源`，你可以遇到使用**Hadoop、Apache
    Kafka、Amazon Kinesis**等构建的流应用程序。“**发布者/订阅者**”模式通常被称为`pub/sub`。
- en: '![](../Images/925417fa8cc74444da6b6fc344e96fe1.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/925417fa8cc74444da6b6fc344e96fe1.png)'
- en: In this example we can set up an **ELT streaming** data pipeline to **AWS Redshift**.
    AWS Firehose delivery stream can offer this type of seamless integration when
    streaming data will be uploaded directly into the data warehouse table. Then data
    will be transformed to create reports with **AWS Quicksight** as a BI tool.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们可以设置一个**ELT 流式**数据管道到**AWS Redshift**。AWS Firehose 传输流可以提供这种无缝集成，当流式数据直接上传到数据仓库表时。然后，数据将被转化为报告，使用**AWS
    Quicksight**作为BI工具。
- en: Batch processing
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批处理
- en: Batch processing is a model where data is gathered according to a predetermined
    threshold or frequency in both micro-batch processing and conventional batch processing,
    and then processing takes place. Historically workloads were primarily batch-oriented
    in data environments. However, modern applications continuously produce enormous
    amounts of data, and a business leans to micro-batch and streaming processing,
    where data is being processed immediately to maintain a competitive advantage.
    Technologies for **micro-batch** loading include **Apache Spark Streaming, Fluentd,
    and Logstash**, and it is very similar to **conventional batch processing**, where
    events are processed on a schedule or in small groups.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理是一种模型，其中数据根据预定的阈值或频率被收集，无论是微批处理还是传统批处理，然后进行处理。历史上，数据环境中的工作负载主要是批处理导向的。然而，现代应用程序持续生成大量数据，企业倾向于微批处理和流处理，在这种处理方式中，数据被立即处理以保持竞争优势。**微批处理**加载的技术包括**Apache
    Spark Streaming、Fluentd 和 Logstash**，这与**传统批处理**非常相似，在传统批处理中，事件按计划或小组处理。
- en: It’s a good choice where the accuracy of your data is not relevant at this moment
    precisely.
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当数据的准确性此时并不重要时，这是一个很好的选择。
- en: '![](../Images/c47854fffb5d5468b0eade33ef679833.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c47854fffb5d5468b0eade33ef679833.png)'
- en: This data pipeline design pattern works better with smaller datasets that require
    ongoing processing because Athena charges based on the volume of data scanned.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这种数据管道设计模式在需要持续处理的小型数据集上效果更好，因为 Athena 是根据扫描的数据量收费的。
- en: '*Let’s say, you don’t want to use* `*change log*` *on MySQL database instance.
    That would be an ideal case because payments dataset is not huge.*'
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*假设你不想在 MySQL 数据库实例上使用* `*change log*` *。这将是理想的情况，因为支付数据集并不庞大。*'
- en: Lambda/Kappa architecture
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Lambda/Kappa 架构
- en: This architecture combines batch and streaming methodologies. It combines the
    best of both worlds and advises that raw data must be retained, for example, in
    a data lake in case you would want to use it again to build a new pipeline or
    investigate an outage. It has both **batch** and **streaming** (speed) layers
    which helps to respond instantly to shifting business and market conditions. Lambda
    architectures can sometimes very complicated with multiple code repositories to
    maintain.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构结合了批处理和流处理方法。它结合了两者的优点，并建议必须保留原始数据，例如，保存在数据湖中，以便在你想要再次使用它来构建新管道或调查故障时使用。它具有**批处理**和**流处理**（速度）层，这有助于快速响应变化的业务和市场条件。Lambda
    架构有时可能会非常复杂，需要维护多个代码库。
- en: '![](../Images/85af40b044d54265d18fb38f643a7c49.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/85af40b044d54265d18fb38f643a7c49.png)'
- en: Lambda data pipeline design. Image by author
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda 数据管道设计。图像由作者提供
- en: Transform first then Load?
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 先转换然后加载？
- en: '**ETL** is considered to be a conventional approach and the most widely used
    historically. With the rise of data warehousing **ELT** becomes increasingly popular.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**ETL** 被认为是一种传统的方法，并且历史上最广泛使用。随着数据仓库的兴起，**ELT** 变得越来越流行。'
- en: '*Indeed, why do we need to transform first if we can centralise it for all
    data pipelines in the data warehouse?*'
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*确实，如果我们可以在数据仓库中集中处理，为什么我们需要先进行转换呢？*'
- en: '**Vurtualisation** is another popular approach for data warehouses where we
    create **views** on data instead of meterialised tables. New requirements to business
    agility put cost-effectiveness on the send plan and data users qould query **views**
    instead of **tables**.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**虚拟化** 是另一种数据仓库的流行方法，我们在数据上创建**视图**而不是物化表。对业务敏捷性的新增要求将成本效益置于次要位置，数据用户可以查询**视图**而不是**表**。'
- en: '**Change Data capture** is another approach to update the data exactly when
    the changes occur. When used typically latent data pipelines, CDC technology can
    recognize data changes as they happen and offer information about those changes.
    Changes are usually pushed to message queue or provided as a stream.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**变更数据捕获** 是另一种在变化发生时即时更新数据的方法。当通常使用延迟数据管道时，CDC 技术可以识别数据变化并提供有关这些变化的信息。变化通常会被推送到消息队列或作为流提供。'
- en: How to choose the data pipeline architecture?
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何选择数据管道架构？
- en: In recent years, data architecture components such as data pipelines have developed
    to support massive volumes of data. The term “Big Data” can be described as having
    three traits known as volume, variety and velocity. Big data can open up new opportunities
    in a variety of use cases, including predictive analytics, real-time reporting,
    and alerting. Architects and developers have had to adapt to new “big data” requirements
    because of the substantially increased volume, diversity, and velocity of data.
    New data processing frameworks emerged and kept emerging. Due to the high velocity
    of modern data streams, we might want to use *streaming* data pipelines. Data
    may then be collected and analysed in real-time, allowing for immediate action.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，数据架构组件如数据管道已经发展，以支持大量数据。术语“大数据”可以描述为具有三个特征：体量、种类和速度。大数据可以在各种用例中开启新的机会，包括预测分析、实时报告和警报。由于数据量、种类和速度的显著增加，架构师和开发者不得不适应新的“大数据”需求。新的数据处理框架不断出现。由于现代数据流的高速，我们可能需要使用*流式*数据管道。数据可以实时收集和分析，从而允许立即采取行动。
- en: '*However, streaming data pipeline design pattern is not always the most cost-effective.*'
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*然而，流式数据管道设计模式并不总是最具成本效益的。*'
- en: For example, in the majority of data warehouse solutions batch data ingestion
    is free. However, streaming, might go with a price. Same statement would be relevant
    for data processing. **Steaming** is the most expensive way to process the data
    in the majority of cases.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在大多数数据仓库解决方案中，批量数据摄取是免费的。然而，流式处理可能会带来费用。同样的说法适用于数据处理。在大多数情况下，**流式**处理是处理数据的最昂贵方式。
- en: Big Data `volumes` require the data pipeline to be able to process events concurrently
    as very often they are sent simultaneously at once. Data solutions must be scalable.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据`volumes`要求数据管道能够同时处理事件，因为这些事件通常是同时发送的。数据解决方案必须具备可扩展性。
- en: The `variety` implies that data might come through the pipelines in different
    formats, very often unstructured or semi-structured.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`variety`意味着数据可能以不同的格式通过管道传输，通常是非结构化或半结构化的。'
- en: Architecture type depends on various factors, i.e. `destination` type and where
    data has to be in the end, **cost considerations** and your team's **development
    stack** and certain data processing skills you and your colleagues already have.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 架构类型取决于各种因素，即`destination`类型和数据最终要到达的位置、**成本考虑**以及你团队的**开发栈**和你与同事们已经具备的某些数据处理技能。
- en: Do your data pipelines have to be managed and cloud-based, or would you rather
    want to deploy it on-premises?
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你的数据管道是否必须是可管理且基于云的，还是更愿意将其部署在本地？
- en: In reality, there are numerous combinations of variables that can help to select
    the best data platform architecture. **What would be the velocity or data flow
    rate in that pipeline? Do you require real-time analytics, or is near-real-time
    sufficient?** This would resolve the question of whether or not you require "streaming"
    pipelines.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，有许多变量组合可以帮助选择最佳的数据平台架构。**那个管道中的速度或数据流率会是多少？你是否需要实时分析，还是近实时的分析就足够了？**这将解决你是否需要“流”管道的问题。
- en: For example, there are services that can create and run both **streaming** and
    **batch** data pipelines, i.e. [Google Dataflow](https://cloud.google.com/dataflow/docs/guides/data-pipelines).
    So how is it different from any other pipeline built in the data warehouse solution?
    The choice would depend on existing infrastructure. For example, if you have some
    existing **Hadoop** workloads, then GCP DataFlow would be a wrong choice as it
    will not let you to re-use the code (it is using Apachec Beam). In this case you
    would want to use **GCP Dataproc** which works on **Hadoop/Spark** code.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，有些服务可以创建和运行**流**和**批处理**数据管道，即[Google Dataflow](https://cloud.google.com/dataflow/docs/guides/data-pipelines)。那么它与数据仓库解决方案中的其他管道有何不同？选择取决于现有的基础设施。例如，如果你有一些现有的**Hadoop**工作负载，那么GCP
    DataFlow将是一个错误的选择，因为它不允许你重用代码（它使用的是Apache Beam）。在这种情况下，你会希望使用**GCP Dataproc**，它可以处理**Hadoop/Spark**代码。
- en: '*The rule of thumb is that if the processing is dependent on any tools in the
    Hadoop ecosystem, Dataproc should be utilized.*'
  id: totrans-80
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*一个经验法则是，如果处理依赖于Hadoop生态系统中的任何工具，则应使用Dataproc。*'
- en: It is basically a Hadoop extension service.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 它基本上是一个Hadoop扩展服务。
- en: On the other hand, if you are not limited by existing code and would like to
    reliably process ever-increasing amounts of **streaming** data then **Dataflow**
    is the recommended choice. You can check these [Dataflow templates](https://github.com/GoogleCloudPlatform/DataflowTemplates)
    if you like to do things in **JAVA**.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果你不受现有代码的限制，并且希望可靠地处理不断增加的**流数据**，那么**Dataflow**是推荐的选择。如果你喜欢用**JAVA**做事，可以查看这些[Dataflow模板](https://github.com/GoogleCloudPlatform/DataflowTemplates)。
- en: There is a [system design guide from Google](https://cloud.google.com/architecture)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个来自Google的[系统设计指南](https://cloud.google.com/architecture)。
- en: Conclusion
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: BigData posed new challenging data architecture requirements for data developers.
    A constantly increasing variety of data formats and data sources increased the
    importance of data integration without disrupting production applications. Businesses
    are increasingly aiming to automate data integration procedures, process streaming
    data in real-time, and streamline the lifetime of data lakes and warehouses. This
    becomes a challenging task indeed, taking into account the increased data volume,
    velocity and variety of data formats over the last decade. Now data pipeline design
    must be robust and, at the same time, flexible to enable new data pipeline creation
    in a simplified and automated way. The increasing trend of using `data mesh`/
    `data mart` platforms requires data catalogues to be created. To create controlled,
    enterprise-ready data and give data consumers an easy method to find, examine,
    and self-provision data, this process should ideally be automated too. Therefore,
    choosing the right data pipeline architecture can solve these issues effectively.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据提出了对数据架构的新挑战要求。不断增加的数据格式和数据源的多样性提高了数据集成的重要性，而不影响生产应用。企业越来越倾向于自动化数据集成程序、实时处理流数据，并简化数据湖和数据仓库的生命周期。考虑到过去十年数据量、速度和数据格式的多样性不断增加，这确实成为了一项挑战。现在，数据管道设计必须既稳健又灵活，以简化和自动化创建新的数据管道。日益增加的使用
    `data mesh`/ `data mart` 平台的趋势要求创建数据目录。为了创建受控的、企业级的数据，并为数据消费者提供便捷的方法来查找、检查和自助提供数据，这个过程也应尽可能自动化。因此，选择合适的数据管道架构可以有效解决这些问题。
- en: '*Originally published at* [*https://mydataschool.com*](https://mydataschool.com/blog/data-pipeline-design-patterns/)*.*'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*最初发布于* [*https://mydataschool.com*](https://mydataschool.com/blog/data-pipeline-design-patterns/)*。*'
- en: 'Recommended read:'
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推荐阅读：
- en: '[](https://cloud.google.com/dataflow/docs/guides/data-pipelines?source=post_page-----100afa4b93e3--------------------------------)
    [## Work with Data Pipelines | Cloud Dataflow | Google Cloud'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://cloud.google.com/dataflow/docs/guides/data-pipelines?source=post_page-----100afa4b93e3--------------------------------)
    [## 使用数据管道 | Cloud Dataflow | Google Cloud'
- en: 'You can report Dataflow Data Pipelines issues and request new features at Note:
    google-data-pipelines-feedback." You…'
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '你可以在 [Note: google-data-pipelines-feedback](https://example.com) 报告 Dataflow
    数据管道问题并请求新功能。你…'
- en: cloud.google.com](https://cloud.google.com/dataflow/docs/guides/data-pipelines?source=post_page-----100afa4b93e3--------------------------------)
    [](https://cloud.google.com/architecture?source=post_page-----100afa4b93e3--------------------------------)
    [## Cloud Architecture Guidance and Topologies | Cloud Architecture Center | Google
    Cloud
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[cloud.google.com](https://cloud.google.com/dataflow/docs/guides/data-pipelines?source=post_page-----100afa4b93e3--------------------------------)
    [](https://cloud.google.com/architecture?source=post_page-----100afa4b93e3--------------------------------)
    [## 云架构指导和拓扑 | 云架构中心 | Google Cloud'
- en: Discover reference architectures, guidance, and best practices for building
    or migrating your workloads on Google…
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 发现参考架构、指导和最佳实践，以帮助你在 Google 上构建或迁移工作负载…
- en: 'cloud.google.com](https://cloud.google.com/architecture?source=post_page-----100afa4b93e3--------------------------------)
    [](https://github.com/GoogleCloudPlatform/DataflowTemplates?source=post_page-----100afa4b93e3--------------------------------)
    [## GitHub - GoogleCloudPlatform/DataflowTemplates: Google-provided Cloud Dataflow
    template pipelines…'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[cloud.google.com](https://cloud.google.com/architecture?source=post_page-----100afa4b93e3--------------------------------)
    [](https://github.com/GoogleCloudPlatform/DataflowTemplates?source=post_page-----100afa4b93e3--------------------------------)
    [## GitHub - GoogleCloudPlatform/DataflowTemplates: Google 提供的 Cloud Dataflow
    模板管道…'
- en: These Dataflow templates are an effort to solve simple, but large, in-Cloud
    data tasks, including data…
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 这些 Dataflow 模板旨在解决简单但大规模的云端数据任务，包括数据…
- en: github.com](https://github.com/GoogleCloudPlatform/DataflowTemplates?source=post_page-----100afa4b93e3--------------------------------)  [##
    Tutorials
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/GoogleCloudPlatform/DataflowTemplates?source=post_page-----100afa4b93e3--------------------------------)
    [## 教程'
- en: Tutorials - AWS Data Pipeline The following tutorials walk you step-by-step
    through the process of creating and using…
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 教程 - AWS 数据管道 以下教程将逐步引导你创建和使用…
- en: docs.aws.amazon.com](https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html?source=post_page-----100afa4b93e3--------------------------------)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[docs.aws.amazon.com](https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/welcome.html?source=post_page-----100afa4b93e3--------------------------------)'
- en: AWS Data Pipeline documentation [[https://docs.aws.amazon.com/data-pipeline](https://docs.aws.amazon.com/data-pipeline/index.html)/]
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 数据管道文档 [[https://docs.aws.amazon.com/data-pipeline](https://docs.aws.amazon.com/data-pipeline/index.html)/]
