- en: 'Mixed Effects Machine Learning for High-Cardinality Categorical Variables —
    Part I: An Empirical Comparison of Different Methods'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高基数分类变量的混合效应机器学习 — 第I部分：不同方法的实证比较
- en: 原文：[https://towardsdatascience.com/mixed-effects-machine-learning-for-high-cardinality-categorical-variables-part-i-an-empirical-df0a43dce059?source=collection_archive---------11-----------------------#2023-07-07](https://towardsdatascience.com/mixed-effects-machine-learning-for-high-cardinality-categorical-variables-part-i-an-empirical-df0a43dce059?source=collection_archive---------11-----------------------#2023-07-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/mixed-effects-machine-learning-for-high-cardinality-categorical-variables-part-i-an-empirical-df0a43dce059?source=collection_archive---------11-----------------------#2023-07-07](https://towardsdatascience.com/mixed-effects-machine-learning-for-high-cardinality-categorical-variables-part-i-an-empirical-df0a43dce059?source=collection_archive---------11-----------------------#2023-07-07)
- en: Why random effects are useful for machine learning models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么随机效应对机器学习模型有用
- en: '[](https://medium.com/@fabsig?source=post_page-----df0a43dce059--------------------------------)[![Fabio
    Sigrist](../Images/f7bc2adc17255ae1efd0886a19ec202c.png)](https://medium.com/@fabsig?source=post_page-----df0a43dce059--------------------------------)[](https://towardsdatascience.com/?source=post_page-----df0a43dce059--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----df0a43dce059--------------------------------)
    [Fabio Sigrist](https://medium.com/@fabsig?source=post_page-----df0a43dce059--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@fabsig?source=post_page-----df0a43dce059--------------------------------)[![Fabio
    Sigrist](../Images/f7bc2adc17255ae1efd0886a19ec202c.png)](https://medium.com/@fabsig?source=post_page-----df0a43dce059--------------------------------)[](https://towardsdatascience.com/?source=post_page-----df0a43dce059--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----df0a43dce059--------------------------------)
    [Fabio Sigrist](https://medium.com/@fabsig?source=post_page-----df0a43dce059--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb5b503a0c329&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmixed-effects-machine-learning-for-high-cardinality-categorical-variables-part-i-an-empirical-df0a43dce059&user=Fabio+Sigrist&userId=b5b503a0c329&source=post_page-b5b503a0c329----df0a43dce059---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----df0a43dce059--------------------------------)
    ·10 min read·Jul 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fdf0a43dce059&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmixed-effects-machine-learning-for-high-cardinality-categorical-variables-part-i-an-empirical-df0a43dce059&user=Fabio+Sigrist&userId=b5b503a0c329&source=-----df0a43dce059---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb5b503a0c329&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmixed-effects-machine-learning-for-high-cardinality-categorical-variables-part-i-an-empirical-df0a43dce059&user=Fabio+Sigrist&userId=b5b503a0c329&source=post_page-b5b503a0c329----df0a43dce059---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----df0a43dce059--------------------------------)
    ·10分钟阅读·2023年7月7日'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdf0a43dce059&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmixed-effects-machine-learning-for-high-cardinality-categorical-variables-part-i-an-empirical-df0a43dce059&source=-----df0a43dce059---------------------bookmark_footer-----------)![](../Images/e27c9e777e29c1064a414d99ae363e3b.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fdf0a43dce059&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fmixed-effects-machine-learning-for-high-cardinality-categorical-variables-part-i-an-empirical-df0a43dce059&source=-----df0a43dce059---------------------bookmark_footer-----------)![](../Images/e27c9e777e29c1064a414d99ae363e3b.png)'
- en: 'Figure 1: Comparison of tree-boosting with and without random effects (“GPBoost”
    vs. “LogitBoost”) for binary data and a categorical variable with varying cardinality.
    **The higher the cardinality (= lower number of samples per level), the larger
    is the outperformance (=decrease in test error) of a model with random effects**
    — Image by author'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：二元数据和具有不同基数的分类变量中树增强方法（“GPBoost”与“LogitBoost”）的对比。**基数越高（即每个水平的样本数越少），随机效应模型的表现越好（即测试误差的减少）**
    — 图片由作者提供
- en: High-cardinality categorical variables are variables for which the number of
    different levels is large relative to the sample size of a data set, or in other
    words, there are few data points per level of a categorical variable. Machine
    learning methods can have difficulties with high-cardinality variables. In this
    article, we argue that random effects are an effective tool for modeling high-cardinality
    categorical variables in machine learning models. In particular, we empirically
    compare several versions of two of the most successful machine learning methods,
    tree-boosting and deep neural networks, as well as linear mixed effects models
    using multiple tabular data sets with high-cardinality categorical variables.
    Our results show that, first, machine learning models with random effects perform
    better than their counterparts without random effects, and, second, tree-boosting
    with random effects outperforms deep neural networks with random effects.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 高基数分类变量是指不同级别的数量相对于数据集的样本大小很大，换句话说，每个分类变量级别的数据点很少。机器学习方法可能在处理高基数变量时遇到困难。在本文中，我们认为随机效应是机器学习模型中建模高基数分类变量的有效工具。特别是，我们通过对比在多个具有高基数分类变量的表格数据集上，两种最成功的机器学习方法——树提升和深度神经网络——以及线性混合效应模型的不同版本。我们的结果表明，首先，具有随机效应的机器学习模型表现优于没有随机效应的模型，其次，具有随机效应的树提升优于具有随机效应的深度神经网络。
- en: '**Table of contents**'
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**目录**'
- en: · [1 Introduction](#1986)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: · [1 介绍](#1986)
- en: · [2 Random effects for modeling high-cardinality categorical variables](#9ecd)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: · [2 高基数分类变量的随机效应建模](#9ecd)
- en: · [3 Comparison of different methods using real-world data sets](#ab90)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: · [3 使用真实数据集的不同方法比较](#ab90)
- en: · [4 Conclusion](#9bfc)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: · [4 结论](#9bfc)
- en: · [References](#9aca)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: · [参考文献](#9aca)
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 介绍
- en: A simple strategy for dealing with categorical variables is to use one-hot encoding
    or dummy variables. But this approach often does not work well for high-cardinality
    categorical variables due to the reasons described below. For neural networks,
    a frequently adopted solution is entity embeddings [Guo and Berkhahn, 2016] which
    map every level of a categorical variable into a low-dimensional Euclidean space.
    For tree-boosting, a simple approach is to assign a number to every level of a
    categorical variable, and then consider this as a one-dimensional numeric variable.
    An alternative solution implemented in the `LightGBM` boosting library [Ke et
    al., 2017] works by partitioning all levels into two subsets using an approximate
    approach [Fisher, 1958] when finding splits in the tree-building algorithm. Further,
    the `CatBoost` boosting library [Prokhorenkova et al., 2018] implements an approach
    based on ordered target statistics calculated using random partitions of the training
    data for handling categorical predictor variables.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 处理分类变量的一种简单策略是使用独热编码或虚拟变量。但由于下面描述的原因，这种方法对于高基数分类变量通常效果不好。对于神经网络，一个常用的解决方案是实体嵌入
    [Guo and Berkhahn, 2016]，它将分类变量的每个级别映射到低维欧几里得空间。对于树提升，一种简单的方法是给分类变量的每个级别分配一个数字，然后将其视为一维数值变量。在`LightGBM`提升库
    [Ke et al., 2017] 中实现的替代方案通过使用近似方法 [Fisher, 1958] 将所有级别分成两个子集来进行树构建算法中的分裂。进一步地，`CatBoost`提升库
    [Prokhorenkova et al., 2018] 实现了一种基于有序目标统计的方法，该方法通过使用训练数据的随机划分来处理分类预测变量。
- en: 2 Random effects for modeling high-cardinality categorical variables
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 高基数分类变量的随机效应建模
- en: Random effects can be used as an effective tool for modeling high-cardinality
    categorical variables. In the regression case with a single high-cardinality categorical
    variable, a random effects model can be written as
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 随机效应可以作为建模高基数分类变量的有效工具。在具有单一高基数分类变量的回归情况下，随机效应模型可以表示为
- en: '![](../Images/ba900ba4edc62246b80da8f488ab0c40.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba900ba4edc62246b80da8f488ab0c40.png)'
- en: where j=1,…,ni is the sample index within level i with ni being the number of
    samples for which the categorical variable attains level i, and i denotes the
    level with q being the total number of levels of the categorical variable. The
    total number of samples is thus n = n0 + n1 + … + nq. Such a model is also called
    mixed effects model since it contains both fixed effects F(xij) and random effects
    bi. xij are the fixed effects predictor variables or features. Mixed effects models
    can be extended to other response variable distributions (e.g., classification)
    and multiple categorical variables.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 j=1,…,ni 是第 i 层内的样本索引，其中 ni 表示分类变量达到第 i 层的样本数量，而 i 表示层级，q 为分类变量的总层数。因此，总样本数为
    n = n0 + n1 + … + nq。这样的模型也被称为混合效应模型，因为它包含了固定效应 F(xij) 和随机效应 bi。xij 是固定效应预测变量或特征。混合效应模型可以扩展到其他响应变量分布（例如，分类）和多个分类变量。
- en: Traditionally, random effects were used in linear models in which it is assumed
    that F is a linear function. In the last years, linear mixed effects models have
    been extended to non-linear ones using random forest [Hajjem et al., 2014], tree-boosting
    [Sigrist, [2022](https://www.jmlr.org/papers/v23/20-322.html), [2023a](https://ieeexplore.ieee.org/document/9759834)],
    and most recently (in terms of first public preprint) deep neural networks [Simchoni
    and Rosset, [2021](https://proceedings.neurips.cc/paper/2021/hash/d35b05a832e2bb91f110d54e34e2da79-Abstract.html),
    [2023](https://www.jmlr.org/papers/v24/22-0501.html)]. In contrast to classical
    independent machine learning models, the random effects introduce dependence among
    samples.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，随机效应被用于线性模型中，其中假设 F 是一个线性函数。在过去几年中，线性混合效应模型已经扩展到使用随机森林 [Hajjem et al., 2014]、树提升
    [Sigrist, [2022](https://www.jmlr.org/papers/v23/20-322.html), [2023a](https://ieeexplore.ieee.org/document/9759834)]，以及最近（从首次公开预印本的角度来看）深度神经网络
    [Simchoni and Rosset, [2021](https://proceedings.neurips.cc/paper/2021/hash/d35b05a832e2bb91f110d54e34e2da79-Abstract.html),
    [2023](https://www.jmlr.org/papers/v24/22-0501.html)]。与经典的独立机器学习模型相比，随机效应引入了样本之间的依赖性。
- en: '**Why are random effects useful for high-cardinality categorical variables?**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么随机效应对高基数分类变量有用？**'
- en: For high-cardinality categorical variables, there is little data for every level.
    Intuitively, if the response variable has a different (conditional) mean for many
    levels, traditional machine learning models (with, e.g., one-hot encoding, embeddings,
    or simply one-dimensional numeric variables) may have problems with over- or underfitting
    for such data. From the point of view of a classical bias-variance trade-off,
    independent machine learning models may have difficulties balancing this trade-off
    and finding an appropriate amount of regularization. For instance, overfitting
    may occur which means that a model has a low bias but high variance.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于高基数分类变量，每个层级的数据很少。从直观上讲，如果响应变量在许多层级上具有不同的（条件）均值，传统的机器学习模型（例如，独热编码、嵌入或简单的一维数值变量）可能会在此类数据上出现过拟合或欠拟合问题。从经典的偏差-方差权衡的角度来看，独立的机器学习模型可能难以平衡这种权衡并找到适当的正则化量。例如，可能会发生过拟合，这意味着模型具有低偏差但高方差。
- en: Broadly speaking, random effects act as a prior, or regularizer, which models
    the difficult part of a function, i.e., the part whose “dimension” is similar
    to the total sample size, and, in doing so, provide an effective way for finding
    a balance between over- and underfitting or bias and variance. For instance, for
    a single categorical variable, random effects models will shrink estimates of
    group intercept effects towards the global mean. This process is sometimes also
    called “information pooling”. It represents a trade-off between completely ignoring
    the categorical variable (= underfitting / high bias and low variance) and giving
    every level in the categorical variable “complete freedom” in estimation (= overfitting
    / low bias and high variance). Importantly, the amount of regularization, which
    is determined by the variance parameters of the model, is learned from the data.
    Specifically, in the above single-level random effects model, a (point) prediction
    for the response variable for a sample with predictor variables xp and categorical
    variable having level i is given by
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 从广义上讲，随机效应作为先验或正则化器，建模函数的难点部分，即其“维度”类似于总样本大小，并且通过这种方式提供了一种在过拟合与欠拟合或偏差与方差之间找到平衡的有效方法。例如，对于单一的分类变量，随机效应模型将会将组拦截效应的估计值收缩向全局均值。这一过程有时也称为“信息汇聚”。它代表了完全忽略分类变量（=
    欠拟合 / 高偏差和低方差）与在估计中给予分类变量的每个级别“完全自由”（= 过拟合 / 低偏差和高方差）之间的权衡。重要的是，正则化的程度，由模型的方差参数决定，是从数据中学习得到的。具体来说，在上述单级随机效应模型中，对于具有预测变量xp和分类变量级别i的样本，响应变量的（点）预测值由下式给出
- en: '![](../Images/a222d8c7f356c47b699dc508e821c912.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a222d8c7f356c47b699dc508e821c912.png)'
- en: where F(xp) is the trained function evaluated at xp, σ²_1 and σ² are variance
    estimates, and yi and Fi are sample means of yij and F(xij), respectively, for
    level i. Ignoring the categorical variable would give the prediction yp = F(xp),
    and a fully flexible model without regularization gives yp = F(xp) + ( yi — Fi).
    I.e., the difference between these two extreme cases and the random effects model
    is the shrinkage factor σ²_1 / (σ²/ni + σ²_1) (which goes to zero if the number
    of samples ni for level i is large). Related to this, random effects models allow
    for more efficient (i.e., lower variance) estimation of the fixed effects function
    F(.) [Sigrist, 2022].
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 其中F(xp)是评估在xp处的训练函数，σ²_1和σ²是方差估计值，而yi和Fi分别是级别i下yij和F(xij)的样本均值。忽略分类变量会得到预测值yp
    = F(xp)，而一个完全灵活的无正则化模型给出的预测值是yp = F(xp) + (yi — Fi)。即，这两种极端情况与随机效应模型之间的差异是收缩因子σ²_1
    / (σ²/ni + σ²_1)（如果级别i的样本数量ni很大，这个因子趋近于零）。与此相关，随机效应模型允许更高效（即更低方差）的固定效应函数F(.)的估计[Sigrist,
    2022]。
- en: In line with this argumentation, Sigrist [2023a, Section 4.1] find in empirical
    experiments that tree-boosting combined with random effects (“GPBoost”) outperforms
    classical independent tree-boosting (“LogitBoost”) more, the lower the number
    of samples per level of a categorical variable, i.e., the higher the cardinality
    of a categorical variable. The results are reproduced above in Figure 1\. These
    results are obtained by simulating binary classification data with 5000 samples,
    a non-linear predictor function, and a categorical variable with successively
    more levels, i.e., fewer samples per level; see Sigrist [2023a] for more details.
    The results show that the difference in the test error of GPBoost and LogitBoost
    is the larger, the fewer samples there are per level of the categorical variable
    (= the higher the number of levels).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这一论点，Sigrist [2023a，第4.1节]在实证实验中发现，树提升与随机效应结合（“GPBoost”）的表现优于经典的独立树提升（“LogitBoost”），尤其是在分类变量的每个级别样本数量较少，即分类变量的基数较高时。上述结果在图1中进行了再现。这些结果是通过模拟具有5000个样本的二分类数据、一个非线性预测函数和一个逐步增加级别的分类变量（即每个级别样本更少）获得的；有关更多细节，请参见Sigrist
    [2023a]。结果表明，GPBoost和LogitBoost的测试误差差异越大，当每个级别的样本数量越少（即级别数量越多）时。
- en: 3 Comparison of different methods using real-world data sets
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 使用真实数据集的不同方法比较
- en: In the following, we compare several methods using multiple real-world data
    sets with high-cardinality categorical variables. We use all the publicly available
    tabular data sets from Simchoni and Rosset [2021, 2023] and also the same experimental
    setting as in Simchoni and Rosset [2021, 2023]. In addition, we include the Wages
    data set analyzed in Sigrist [2022].
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用多个具有高卡方类别变量的真实数据集比较几种方法。我们使用Simchoni和Rosset [2021, 2023]提供的所有公开表格数据集，以及与Simchoni和Rosset
    [2021, 2023]相同的实验设置。此外，我们还包括Sigrist [2022]分析的Wages数据集。
- en: 'We consider the following methods:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑以下方法：
- en: '`Linear’: linear mixed effects models'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Linear’：线性混合效应模型'
- en: '`NN Embed’: deep neural networks with embeddings'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NN Embed’：具有嵌入的深度神经网络'
- en: '`LMMNN’: combining deep neural networks and random effects [Simchoni and Rosset,
    2021, 2023]'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LMMNN’：结合深度神经网络和随机效应 [Simchoni和Rosset, 2021, 2023]'
- en: '`LGBM_Num’: tree-boosting by assigning a number to every level of categorical
    variables and considering these as one-dimensional numeric variables'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LGBM_Num’：通过为每个类别变量的每个水平分配一个数字，并将这些数字视为一维数值变量的树提升'
- en: '`LGBM_Cat’: tree-boosting with the approach of `LightGBM` [Ke et al., 2017]
    for categorical variables'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LGBM_Cat’：使用`LightGBM` [Ke et al., 2017]的方法进行树提升'
- en: '`CatBoost’: tree-boosting with the approach of `CatBoost` [Prokhorenkova et
    al., 2018] for categorical variables'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CatBoost’：使用`CatBoost` [Prokhorenkova et al., 2018]的方法进行树提升'
- en: '`GPBoost’: combining tree-boosting and random effects [Sigrist, 2022, 2023a]'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GPBoost’：结合树提升和随机效应 [Sigrist, 2022, 2023a]'
- en: Note that, recently (version 1.6 and later), the `XGBoost` library [Chen and
    Guestrin, 2016] has also implemented the same approach as `LightGBM` for handling
    categorical variables. We do not consider this as a separate approach here.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，最近（版本1.6及以后），`XGBoost`库 [Chen和Guestrin, 2016] 也实现了与`LightGBM`相同的处理类别变量的方法。我们在这里不将其视为单独的方法。
- en: 'We use the following data sets:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下数据集：
- en: '![](../Images/788213f92c9b8a0af74ff7efe79da785.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/788213f92c9b8a0af74ff7efe79da785.png)'
- en: 'Table 1: Summary of data sets. n is the number of samples, p is the number
    of predictor variables (excl. high-cardinality categorical variables), K is the
    number of high-cardinality categorical variables, and ‘Cat. var.’ describes the
    categorical variable(s) — Image by author'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：数据集总结。n是样本数量，p是预测变量的数量（不包括高卡方类别变量），K是高卡方类别变量的数量，‘Cat. var.’描述类别变量 — 作者提供的图像
- en: For all methods with random effects, we include random effects for every categorical
    variable mentioned in Table 1 with no prior correlation among random effects.
    The Rossmann, AUImport, and Wages data sets are longitudinal data sets. For these,
    we also include linear and quadratic random slopes; see [Part III](https://medium.com/towards-data-science/mixed-effects-machine-learning-for-longitudinal-panel-data-with-gpboost-part-iii-523bb38effc)
    of this series. See Simchoni and Rosset [2021, 2023] and Sigrist [2023b] for more
    details on the data sets.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有随机效应方法，我们包括表1中提到的每个类别变量的随机效应，随机效应之间没有先验相关性。Rossmann、AUImport和Wages数据集是纵向数据集。对于这些数据集，我们还包括线性和二次随机斜率；请参见本系列的[第三部分](https://medium.com/towards-data-science/mixed-effects-machine-learning-for-longitudinal-panel-data-with-gpboost-part-iii-523bb38effc)。有关数据集的更多详细信息，请参见Simchoni和Rosset
    [2021, 2023]以及Sigrist [2023b]。
- en: We perform 5-fold cross-validation (CV) on every data set with the test mean
    squared error (MSE) to measure prediction accuracy. See Sigrist [2023b] for detailed
    information on the experimental setting. Code for pre-processing the data with
    instructions on how to download the data and code for running the experiments
    can be found [here](https://github.com/fabsig/Compare_ML_HighCardinality_Categorical_Variables).
    Pre-processed data for modeling can also be found on the above webpage for data
    sets for which the license of the original source permits it.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对每个数据集进行5折交叉验证（CV），使用测试均方误差（MSE）来衡量预测准确性。有关实验设置的详细信息，请参见Sigrist [2023b]。数据预处理的代码以及下载数据和运行实验的代码可以在[这里](https://github.com/fabsig/Compare_ML_HighCardinality_Categorical_Variables)找到。对于许可证允许的原始数据源，建模所用的预处理数据也可以在上述网页找到。
- en: '![](../Images/e70115790a2114b16bdc770634c2be48.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e70115790a2114b16bdc770634c2be48.png)'
- en: 'Figure 2: Average relative difference to the lowest test MSE. The Wages data
    set is not included for calculating this since not all methods were run on it
    — Image by author'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：相对最低测试MSE的平均差异。由于不是所有方法都在Wages数据集上运行，因此该数据集未用于计算 — 作者提供的图像
- en: The results are summarized in Figure 2 which shows average relative differences
    to the lowest test MSE. This is obtained by first calculating the relative difference
    of a test MSE of a method compared to the lowest MSE for every data set, and then
    taking the average over all data sets. Detailed results can be found in [Sigrist
    [2023b]](https://arxiv.org/abs/2307.02071). We observe that combined tree-boosting
    and random effects (GPBoost) has the highest prediction accuracy with an average
    relative difference to the best results of approx. 7%. The second best results
    are obtained by the categorical variables approach of `LightGBM` (LGMB_Cat) and
    neural networks with random effects (LMMNN) both having an average relative difference
    to the best method of approx. 17%. CatBoost and linear mixed effects models perform
    substantially worse having an average relative difference to the best method of
    almost 50%. Given that [CatBoost is specifically designed to handle categorical
    features](https://proceedings.neurips.cc/paper/2018/hash/14491b756b3a51daac41c24863285549-Abstract.html),
    this is somewhat sobering. Overall worst perform neural networks with embeddings
    having an average relative difference to the best result of more than 150%. Tree-boosting
    with the categorical variables transformed to one-dimensional numeric variables
    (LGBM_Num) performs slightly better with an average relative difference to the
    best result of approximately 100%. In their online documentation, `LightGBM` recommends
    [*“For a categorical feature with high cardinality, it often works best to treat
    the feature as numeric” (as of July 6, 2023)*](https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html#categorical-feature-support).
    We clearly come to a different conclusion.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 结果汇总在图2中，该图显示了与最低测试均方误差（MSE）的平均相对差异。这是通过首先计算每个数据集的测试MSE与最低MSE的相对差异，然后对所有数据集进行平均得到的。详细结果可以在
    [Sigrist [2023b]](https://arxiv.org/abs/2307.02071) 中找到。我们观察到，结合树提升和随机效应（GPBoost）具有最高的预测准确性，与最佳结果的平均相对差异约为7%。第二好的结果是由`LightGBM`（LGMB_Cat）的分类变量方法和具有随机效应的神经网络（LMMNN）获得，它们的平均相对差异约为17%。CatBoost和线性混合效应模型的表现则明显较差，与最佳方法的平均相对差异接近50%。鉴于
    [CatBoost 特别设计用来处理分类特征](https://proceedings.neurips.cc/paper/2018/hash/14491b756b3a51daac41c24863285549-Abstract.html)，这令人有些失望。总体上，表现最差的是具有嵌入的神经网络，其与最佳结果的平均相对差异超过150%。将分类变量转换为一维数值变量的树提升（LGBM_Num）的表现略好一些，与最佳结果的平均相对差异约为100%。在其在线文档中，`LightGBM`
    推荐 [*“对于高基数的分类特征，通常将特征视为数值特征效果最佳”（截至2023年7月6日）*](https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html#categorical-feature-support)。我们显然得出了不同的结论。
- en: 4 Conclusion
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 结论
- en: We have empirically compared several methods on tabular data with high-cardinality
    categorical variables. Our results show that, first, machine learning models with
    random effects perform better than their counterparts without random effects,
    and, second, tree-boosting with random effects outperforms deep neural networks
    with random effects. While there may be several possible reasons for the latter
    finding, this is in line with the recent work of Grinsztajn et al. [2022] who
    find that tree-boosting outperforms deep neural networks (and also random forest)
    on tabular data without high-cardinality categorical variables. Similarly, Shwartz-Ziv
    and Armon [2022] conclude that tree-boosting “outperforms deep models on tabular
    data.”
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在具有高基数分类变量的表格数据上对几种方法进行了实证比较。我们的结果显示，首先，具有随机效应的机器学习模型的表现优于没有随机效应的模型；其次，具有随机效应的树提升优于具有随机效应的深度神经网络。虽然可能有几种原因可以解释这一发现，但这与Grinsztajn等人
    [2022] 的最新工作一致，他们发现树提升在没有高基数分类变量的表格数据上优于深度神经网络（以及随机森林）。类似地，Shwartz-Ziv和Armon [2022]
    也得出结论，树提升“在表格数据上优于深度模型。”
- en: In [Part II](https://medium.com/towards-data-science/mixed-effects-machine-learning-for-high-cardinality-categorical-variables-part-ii-gpboost-3bdd9ef74492)
    of this series, we will show how to apply the `[GPBoost](https://github.com/fabsig/GPBoost)`
    [library](https://github.com/fabsig/GPBoost) with a demo using on one of the above-mentioned
    real-world data sets. In [Part III](https://medium.com/towards-data-science/mixed-effects-machine-learning-for-longitudinal-panel-data-with-gpboost-part-iii-523bb38effc),
    we will show how longitudinal, aka panel, data can be modeled with the `GPBoost`
    library.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [Part II](https://medium.com/towards-data-science/mixed-effects-machine-learning-for-high-cardinality-categorical-variables-part-ii-gpboost-3bdd9ef74492)
    中，我们将展示如何使用 `[GPBoost](https://github.com/fabsig/GPBoost)` [库](https://github.com/fabsig/GPBoost)
    进行演示，使用上述提到的真实数据集之一。在 [Part III](https://medium.com/towards-data-science/mixed-effects-machine-learning-for-longitudinal-panel-data-with-gpboost-part-iii-523bb38effc)
    中，我们将展示如何使用 `GPBoost` 库来建模纵向数据，即面板数据。
- en: References
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'T. Chen and C. Guestrin. XGBoost: A scalable tree boosting system. In Proceedings
    of the 22nd acm sigkdd international conference on knowledge discovery and data
    mining, pages 785–794\. ACM, 2016.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: T. Chen 和 C. Guestrin. XGBoost：一个可扩展的树提升系统。发表于第22届 ACM SIGKDD 国际知识发现与数据挖掘会议，页码
    785–794。ACM，2016年。
- en: W. D. Fisher. On grouping for maximum homogeneity. Journal of the American statistical
    Association, 53(284):789–798, 1958.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: W. D. Fisher. 论最大同质性分组。美国统计协会期刊，53(284):789–798，1958年。
- en: L. Grinsztajn, E. Oyallon, and G. Varoquaux. Why do tree-based models still
    outperform deep learning on typical tabular data? In S. Koyejo, S. Mohamed, A.
    Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information
    Processing Systems, volume 35, pages 507–520\. Curran Associates, Inc., 2022.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L. Grinsztajn, E. Oyallon 和 G. Varoquaux. 为什么基于树的模型在典型的表格数据上仍然优于深度学习？在 S. Koyejo,
    S. Mohamed, A. Agarwal, D. Belgrave, K. Cho 和 A. Oh 编辑的《神经信息处理系统进展》卷35，页码 507–520。Curran
    Associates, Inc., 2022年。
- en: C. Guo and F. Berkhahn. Entity embeddings of categorical variables. arXiv preprint
    arXiv:1604.06737, 2016.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: C. Guo 和 F. Berkhahn. 分类变量的实体嵌入。arXiv 预印本 arXiv:1604.06737，2016年。
- en: A. Hajjem, F. Bellavance, and D. Larocque. Mixed-effects random forest for clustered
    data. Journal of Statistical Computation and Simulation, 84(6):1313–1328, 2014.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: A. Hajjem, F. Bellavance 和 D. Larocque. 用于聚类数据的混合效应随机森林。统计计算与模拟期刊，84(6):1313–1328，2014年。
- en: 'G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye, and T.-Y. Liu. LightGBM:
    A highly efficient gradient boosting decision tree. In Advances in Neural Information
    Processing Systems, pages 3149–3157, 2017.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye 和 T.-Y. Liu. LightGBM：一种高效的梯度提升决策树。发表于神经信息处理系统进展，页码
    3149–3157，2017年。
- en: 'L. Prokhorenkova, G. Gusev, A. Vorobev, A. V. Dorogush, and A. Gulin. CatBoost:
    unbiased boosting with categorical features. In Advances in Neural Information
    Processing Systems, pages 6638–6648, 2018.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L. Prokhorenkova, G. Gusev, A. Vorobev, A. V. Dorogush 和 A. Gulin. CatBoost：具有分类特征的无偏提升。发表于神经信息处理系统进展，页码
    6638–6648，2018年。
- en: 'R. Shwartz-Ziv and A. Armon. Tabular data: Deep learning is not all you need.
    Information Fusion, 81:84–90, 2022.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R. Shwartz-Ziv 和 A. Armon. 表格数据：深度学习并不是你所需要的一切。信息融合，81:84–90，2022年。
- en: F. Sigrist. Gaussian Process Boosting. The Journal of Machine Learning Research,
    23(1):10565–10610, 2022.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: F. Sigrist. 高斯过程提升。机器学习研究期刊，23(1):10565–10610，2022年。
- en: F. Sigrist. Latent Gaussian Model Boosting. IEEE Transactions on Pattern Analysis
    and Machine Intelligence, 45(2):1894–1905, 2023a.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: F. Sigrist. 潜在高斯模型提升。IEEE 模式分析与机器智能学报，45(2):1894–1905，2023a年。
- en: F. Sigrist. A Comparison of Machine Learning Methods for Data with High-Cardinality
    Categorical Variables. arXiv preprint arXiv::2307.02071 2023b.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: F. Sigrist. 针对高维分类变量的数据的机器学习方法比较。arXiv 预印本 arXiv::2307.02071 2023b。
- en: G. Simchoni and S. Rosset. Using random effects to account for high-cardinality
    categorical features and repeated measures in deep neural networks. Advances in
    Neural Information Processing Systems, 34:25111–25122, 2021.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: G. Simchoni 和 S. Rosset. 使用随机效应来考虑深度神经网络中的高维分类特征和重复测量。神经信息处理系统进展，34:25111–25122，2021年。
- en: G. Simchoni and S. Rosset. Integrating Random Effects in Deep Neural Networks.
    Journal of Machine Learning Research, 24(156):1–57, 2023.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: G. Simchoni 和 S. Rosset. 深度神经网络中的随机效应整合。机器学习研究期刊，24(156):1–57，2023年。
- en: '[https://github.com/fabsig/Compare_ML_HighCardinality_Categorical_Variables](https://github.com/fabsig/Compare_ML_HighCardinality_Categorical_Variables)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/fabsig/Compare_ML_HighCardinality_Categorical_Variables](https://github.com/fabsig/Compare_ML_HighCardinality_Categorical_Variables)'
