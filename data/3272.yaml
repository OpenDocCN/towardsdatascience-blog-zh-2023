- en: Variable Importance in Random Forests
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林中的变量重要性
- en: 原文：[https://towardsdatascience.com/variable-importance-in-random-forests-20c6690e44e0?source=collection_archive---------2-----------------------#2023-11-03](https://towardsdatascience.com/variable-importance-in-random-forests-20c6690e44e0?source=collection_archive---------2-----------------------#2023-11-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/variable-importance-in-random-forests-20c6690e44e0?source=collection_archive---------2-----------------------#2023-11-03](https://towardsdatascience.com/variable-importance-in-random-forests-20c6690e44e0?source=collection_archive---------2-----------------------#2023-11-03)
- en: Traditional Methods and New Developments
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 传统方法与新发展
- en: '[](https://medium.com/@jeffrey_85949?source=post_page-----20c6690e44e0--------------------------------)[![Jeffrey
    Näf](../Images/0ce6db85501192cdebeeb910eb81a688.png)](https://medium.com/@jeffrey_85949?source=post_page-----20c6690e44e0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----20c6690e44e0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----20c6690e44e0--------------------------------)
    [Jeffrey Näf](https://medium.com/@jeffrey_85949?source=post_page-----20c6690e44e0--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jeffrey_85949?source=post_page-----20c6690e44e0--------------------------------)[![Jeffrey
    Näf](../Images/0ce6db85501192cdebeeb910eb81a688.png)](https://medium.com/@jeffrey_85949?source=post_page-----20c6690e44e0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----20c6690e44e0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----20c6690e44e0--------------------------------)
    [Jeffrey Näf](https://medium.com/@jeffrey_85949?source=post_page-----20c6690e44e0--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fca780798011a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvariable-importance-in-random-forests-20c6690e44e0&user=Jeffrey+N%C3%A4f&userId=ca780798011a&source=post_page-ca780798011a----20c6690e44e0---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----20c6690e44e0--------------------------------)
    ·15 min read·Nov 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F20c6690e44e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvariable-importance-in-random-forests-20c6690e44e0&user=Jeffrey+N%C3%A4f&userId=ca780798011a&source=-----20c6690e44e0---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fca780798011a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvariable-importance-in-random-forests-20c6690e44e0&user=Jeffrey+N%C3%A4f&userId=ca780798011a&source=post_page-ca780798011a----20c6690e44e0---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----20c6690e44e0--------------------------------)
    ·15分钟阅读·2023年11月3日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F20c6690e44e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvariable-importance-in-random-forests-20c6690e44e0&user=Jeffrey+N%C3%A4f&userId=ca780798011a&source=-----20c6690e44e0---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F20c6690e44e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvariable-importance-in-random-forests-20c6690e44e0&source=-----20c6690e44e0---------------------bookmark_footer-----------)![](../Images/67a49d96a841b077c6462f8c9063a193.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F20c6690e44e0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fvariable-importance-in-random-forests-20c6690e44e0&source=-----20c6690e44e0---------------------bookmark_footer-----------)![](../Images/67a49d96a841b077c6462f8c9063a193.png)'
- en: 'Features of (Distributional) Random Forests. In this article: The ability to
    produce variable importance. Source: Author.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: （分布式）随机森林的特点。本文中：产生变量重要性的能力。来源：作者。
- en: 'Random Forest and generalizations (in particular, Generalized Random Forests
    (GRF) and Distributional Random Forests (DRF) ) are powerful and easy-to-use machine
    learning methods that should not be absent in the toolbox of any data scientist.
    They not only show robust performance over a large range of datasets without the
    need for tuning, but can also easily handle [missing values](https://medium.com/towards-data-science/random-forests-and-missing-values-3daaea103db0),
    and even provide [confidence intervals](https://medium.com/towards-data-science/inference-for-distributional-random-forests-64610bbb3927).
    In this article, we focus on another feature they are able to provide: notions
    of feature importance. In particular, we focus on:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林及其推广（特别是广义随机森林（GRF）和分布随机森林（DRF））是强大且易于使用的机器学习方法，任何数据科学家都不应缺少它们。这些方法不仅在大量数据集上表现稳健而无需调优，还能轻松处理[缺失值](https://medium.com/towards-data-science/random-forests-and-missing-values-3daaea103db0)，甚至提供[置信区间](https://medium.com/towards-data-science/inference-for-distributional-random-forests-64610bbb3927)。本文重点讨论它们能够提供的另一个特性：特征重要性的概念。具体来说，我们关注：
- en: Traditional Random Forest (RF), which is used to predict the conditional expectation
    of a variable *Y* given p predictors ***X***.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 传统的随机森林（RF）用于预测给定 p 个预测变量时变量 *Y* 的条件期望 ***X***。
- en: The [Distributional Random Forest](https://medium.com/towards-data-science/drf-a-random-forest-for-almost-everything-625fa5c3bcb8),
    which is used to predict the whole conditional distribution of a d-variate ***Y***
    given *p* predictors ***X***.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[分布随机森林](https://medium.com/towards-data-science/drf-a-random-forest-for-almost-everything-625fa5c3bcb8)，用于预测给定
    *p* 个预测变量 ***X*** 时 d-变量 ***Y*** 的整个条件分布。'
- en: 'Unfortunately, like many modern machine learning methods, both forests lack
    interpretability. That is, there are so many operations involved, it seems impossible
    to determine what the functional relationship between the predictors and *Y* actually
    is. A common way to tackle this problem is to define Variable Importance measures
    (VIMP), that at least help decide which predictors are important. Generally, this
    has two different objectives:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，与许多现代机器学习方法一样，这些森林缺乏可解释性。即，涉及的操作过多，似乎无法确定预测变量与 *Y* 之间的实际功能关系。解决此问题的一种常见方法是定义变量重要性度量（VIMP），至少可以帮助决定哪些预测变量是重要的。通常，这有两个不同的目标：
- en: (1) finding a small number of variables with maximal accuracy,
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 寻找少量具有最大准确度的变量，
- en: (2) detecting and ranking all influential variables to focus on for further
    exploration.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 检测和排名所有影响变量，以便进一步探索。
- en: The difference between (1) and (2) matters as soon as there is dependence between
    the elements in ***X*** (so pretty much always). For example, if two variables
    are highly correlated together and with *Y*, one of the two inputs can be removed
    without hurting accuracy for objective (1), since both variables convey the same
    information. However, both should be included for objective (2), since these two
    variables may have different meanings in practice for domain experts.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 和 (2) 之间的差异很重要，只要 ***X*** 中的元素之间存在依赖关系（几乎总是如此）。例如，如果两个变量与 *Y* 高度相关且彼此相关，则可以在目标
    (1) 中移除其中一个输入，而不会影响准确度，因为这两个变量传达了相同的信息。然而，对于目标 (2)，两个变量都应包含在内，因为这两个变量在实际应用中可能对领域专家有不同的含义。
- en: Today we focus on (1) and try to find a smaller number of predictors that display
    more or less the same predictive accuracy. For instance, in the wage example below,
    we are able to reduce the number of predictors from 79 to about 20, with only
    a small reduction in accuracy. These most important predictors contain variables
    such as age and education which are well-known to influence wages. There are also
    many great articles on medium about (2), using Shapley values such as [this one](/from-shapley-to-shap-understanding-the-math-e7155414213b)
    or [this one](/understand-the-working-of-shap-based-on-shapley-values-used-in-xai-in-the-most-simple-way-d61e4947aa4e).
    There is also very recent and exciting [academic literature](https://hal.science/hal-03232621/document)
    on how to efficiently calculate Shapley values with Random Forest. But this is
    material for a second article.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 今天我们专注于（1），并试图找到较少数量的预测因子，这些因子展示了或多或少相同的预测准确性。例如，在下面的工资示例中，我们能够将预测因子的数量从79减少到大约20，仅有少量准确性损失。这些最重要的预测因子包含了如年龄和教育等众所周知影响工资的变量。此外，还有许多关于（2）的精彩文章，使用Shapley值，例如[这篇](/from-shapley-to-shap-understanding-the-math-e7155414213b)或[这篇](/understand-the-working-of-shap-based-on-shapley-values-used-in-xai-in-the-most-simple-way-d61e4947aa4e)。也有非常近期且令人兴奋的[学术文献](https://hal.science/hal-03232621/document)讨论如何高效计算与随机森林结合的Shapley值。这些是第二篇文章的材料。
- en: The two measures we look at today are actually more general variable importance
    measures that can be used for any method, based on the drop-and-relearn principle
    which we will look at below. We focus exclusively on tree-based methods here,
    however. Moreover, we don’t go into great detail explaining the methods, but rather
    try to focus on their applications and why newer versions are preferable to the
    more traditional ones.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 今天我们关注的两个度量实际上是更通用的变量重要性度量，可以用于任何方法，基于我们将要讨论的丢弃重学原则。然而，我们在此仅专注于树基方法。此外，我们不会详细解释这些方法，而是专注于它们的应用及为何新版本优于传统版本。
- en: '![](../Images/3acf6bdba23d5be159289e2dd040c7a8.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3acf6bdba23d5be159289e2dd040c7a8.png)'
- en: 'Overview of Variable Importance Measures for Random Forests. Mean Decrease
    Impurity (MDI) and Mean Decrease Accuracy (MDA) were both postulated by Breiman.
    Due to their empirical nature, however, several problems remained, which were
    recently addressed by Sobol-MDA. Source: Author'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林变量重要性度量概述。均值减少纯度（MDI）和均值减少准确性（MDA）都是由Breiman提出的。然而，由于其经验性，本方法仍存在若干问题，最近由Sobol-MDA解决。来源：作者
- en: The Beginnings
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 起始
- en: 'Variable importance measures for RFs are in fact as old as RF itself. The first
    accuracy the **Mean Decrease Accuracy (MDA)** was proposed by Breiman in his seminal
    Random Forest paper [1]. The idea is simple: For every dimension *j=1,…,p*, one
    compares the accuracy of the full prediction with the accuracy of the prediction
    when *X_j* is randomly permuted. The idea of this is to break the relationship
    between *X_j* and *Y* and compare the accuracy when *X_j* is not helping to predict
    *Y* by design, to the case when it is potentially of use.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: RF的变量重要性度量实际上与RF本身一样古老。**均值减少准确性（MDA）**这一方法最早由Breiman在其开创性的随机森林论文中提出[1]。其原理很简单：对于每一个维度
    *j=1,…,p*，将完整预测的准确性与当 *X_j* 被随机置换时的预测准确性进行比较。这一想法是打破 *X_j* 和 *Y* 之间的关系，并将 *X_j*
    不参与预测 *Y* 的准确性与它可能有用的情况进行比较。
- en: 'There are various different versions of MDA implemented in R and Python:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: MDA在R和Python中有各种不同的版本实现：
- en: '![](../Images/6a9877cc70937cb5664d9cf9d41986cd.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6a9877cc70937cb5664d9cf9d41986cd.png)'
- en: 'Different Versions of MDA, implemented in different packages. Source: Table
    1 in [3]'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: MDA的不同版本，在不同包中实现。来源：[3]中的表1
- en: Unfortunately, permuting variable *X_j* in this way not only breaks its relationship
    to *Y*, but also to the other variables in ***X***. This is not a problem if *X_j*
    is independent from all other variables, but it becomes a problem once there is
    dependence. Consequently, [3] is able to show that as soon as there is dependence
    in ***X***, the MDA converges to something nonsensical. In particular, MDA can
    give high importance to a variable *X_j* that is not important to predict *Y*,
    but is highly correlated with another variable, say *X_l*, that is actually important
    for predicting *Y* (as demonstrated in the example below). At the same time, it
    can fail to detect variables that are actually relevant, as demonstrated by a
    long list of papers in [3, Section 2.1]. Intuitively, what we would want to measure
    is the performance of the model if *X_j* is not included, and instead, we measure
    the performance of a model with a permuted *X_j* variable.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，以这种方式排列变量 *X_j* 不仅打破了它与 *Y* 的关系，也打破了它与 ***X*** 中其他变量的关系。如果 *X_j* 与所有其他变量独立，这不是问题，但一旦存在依赖关系，就会成为问题。因此，[3]
    证明了只要 ***X*** 中存在依赖关系，MDA 就会收敛到一些荒谬的结果。特别是，MDA 可能会给予一个对预测 *Y* 并不重要的变量 *X_j* 高重要性，但这个变量与另一个对预测
    *Y* 实际上很重要的变量，比如 *X_l*，高度相关（如下例所示）。同时，它可能无法检测到实际相关的变量，这在 [3, Section 2.1] 的大量文献中得到了证明。从直觉上讲，我们希望测量的是如果不包括
    *X_j* 模型的性能，而我们测量的是使用排列的 *X_j* 变量的模型性能。
- en: The second traditional accuracy measure is **Mean Decrease Impurity (MDI)**,
    which sums the weighted decreases of impurity over all nodes that split on a given
    covariate, averaged over all trees in the forest. Unfortunately, MDI is ill-defined
    from the start (it's not clear what it should measure) and several papers highlight
    the practical problem of this approach (e.g. [5]) As such, we will not go into
    detail about MDI, as MDA is often the preferred choice.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种传统的准确性度量是 **均值减少不纯度（MDI）**，它是对所有分裂在给定协变量上的节点的不纯度加权减少的总和，平均到森林中的所有树。不幸的是，MDI
    从一开始就定义不明确（不清楚它应该度量什么），且几篇论文强调了这种方法的实际问题（例如 [5]）。因此，我们不会详细讨论 MDI，因为 MDA 通常是首选。
- en: 'Modern Developments I: Sobol-MDA'
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '现代发展 I: Sobol-MDA'
- en: 'For the longest time, I thought these somewhat informal measures were the best
    we could do. One paper that changed that, came out only very recently. In this
    paper, the authors demonstrate theoretically that the popular measures above are
    actually quite flawed and do not measure what we want to measure. So the first
    question might be: What do we actually want to measure? One potential answer:
    The Sobol-index (originally proposed in the computer science literature):'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 长时间以来，我认为这些稍显非正式的度量是我们能做到的最好方法。一篇改变了这一点的论文最近才发表。在这篇论文中，作者理论上证明了上述流行度量实际上存在很大缺陷，并未测量我们想要测量的内容。因此，第一个问题可能是：我们实际上想要测量什么？一个潜在的答案是：Sobol
    指数（最早在计算机科学文献中提出）：
- en: '![](../Images/02d351a195fc2410668b427c48916e28.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02d351a195fc2410668b427c48916e28.png)'
- en: Let’s unpack this. First, *tau(****X****)=E[ Y |* ***X****]* is the conditional
    expectation function we would like to estimate. This is a random variable because
    it is a function of the random ***X***. Now ***X****^{(-j)}* is the *p-1* vector
    with covariate *j* removed. Thus *ST^{(j)}* is the reduction in output explained
    variance if the *j*th output variable is removed.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来详细了解一下。首先，*tau(****X****)=E[ Y |* ***X****]* 是我们希望估计的条件期望函数。由于它是随机 ***X***
    的函数，因此这是一个随机变量。现在 ***X****^{(-j)}* 是移除协变量 *j* 的 *p-1* 向量。因此，*ST^{(j)}* 是移除第 *j*
    个输出变量后解释方差的减少量。
- en: 'The above is the more traditional way of writing the measure. However, for
    me writing:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 以上是更传统的度量方式。然而，对我来说，写作：
- en: '![](../Images/9c77e4e33b3e4720e609e9aa5edd97e1.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c77e4e33b3e4720e609e9aa5edd97e1.png)'
- en: is much more intuitive. Here *d* is a distance between two random vectors and
    for the *ST^{(j)}* above, this distance is simply the usual Euclidean distance.
    Thus the upper part of *ST^{(j)}* is simply measuring the average squared distance
    between what we want (*tau(****X****)*) and what we get without variable *j*.
    The latter is
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 更直观。在这里，*d* 是两个随机向量之间的距离，对于上述的 *ST^{(j)}*，这个距离就是通常的欧几里得距离。因此，*ST^{(j)}* 的上半部分只是测量了我们想要的（*tau(****X****)*)
    和没有变量 *j* 时得到的之间的平均平方距离。
- en: '![](../Images/45dc0fc966fb263863a38d32c9aa5289.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/45dc0fc966fb263863a38d32c9aa5289.png)'
- en: 'The question becomes how to estimate this efficiently. It turns out that the
    intuitive drop-and-relearn principle would be enough: Simply estimating *tau(****X****)*
    using RF and then dropping *X_j* and refitting the RF to obtain an estimate of
    *tau(****X^{(-j)}****),* one obtains the consistent estimator*:*'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 问题变成了如何有效地估计这个距离。事实证明，直观的“丢弃和重新学习”原则就足够了：简单地使用随机森林估计 *tau(****X****)*，然后丢弃 *X_j*
    并重新拟合随机森林以获得 *tau(****X^{(-j)}****)* 的估计值，就可以得到一致的估计量：
- en: '![](../Images/7a7852dcae89d65509a4d43a4d4b2a63.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a7852dcae89d65509a4d43a4d4b2a63.png)'
- en: where *tau_n(****X****_i)* is the RF estimate for a test point ***X****_i* using
    all *p* predictors and similarly *tau_n(****X****_i^{(-j)})* is the refitted forest
    using only *p-1* predictors.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *tau_n(****X****_i)* 是对测试点 ***X****_i* 使用所有 *p* 个预测变量的随机森林估计值，同样 *tau_n(****X****_i^{(-j)})*
    是仅使用 *p-1* 个预测变量的重新拟合森林。
- en: However, this means the forest needs to be refitted *p* times, not very efficient
    when *p* is large! As such the authors in [3] develop what they call the **Sobol-MDA**.
    Instead of refitting the forest each time, the forest is only fitted once. Then
    test points are dropped down the same forest and the resulting prediction is “projected”
    to form the measure in (1). That is, splits on *X_j* are simply ignored (remember
    the goal is to obtain an estimate without *X_j*). The authors are able to show
    that calculating (1) above with this projected approach also results in a consistent
    estimator! This is a beautiful idea indeed and renders the algorithm applicable
    even in high dimensions.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这意味着森林需要重新拟合 *p* 次，当 *p* 很大时效率不高！因此，[3] 中的作者开发了他们所称的 **Sobol-MDA**。与每次重新拟合森林不同，该方法仅拟合一次森林。然后，将测试点丢弃到同一森林中，结果预测“投影”以形成公式
    (1) 中的度量。也就是说，*X_j* 上的分裂会被简单忽略（记住目标是获得没有 *X_j* 的估计）。作者能够表明，使用这种投影方法计算上述公式 (1)
    也能得到一致的估计量！这确实是一个很棒的想法，使得该算法即使在高维情况下也能适用。
- en: '![](../Images/ded296de922c3d43a2d81d508473bfb6.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ded296de922c3d43a2d81d508473bfb6.png)'
- en: 'Illustration of the projection approach. On the left the division of the two-dimensional
    space by RF. On the right the projection approach ignores splits in X^(2), thereby
    removing it when making predictions. As can be seen the point X gets projected
    onto X^{(-j)} on the right using this principle. Source: Figure 1 in [3]'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 投影方法的示意图。左侧是 RF 对二维空间的划分。右侧的投影方法忽略了 X^(2) 中的分裂，从而在进行预测时将其移除。可以看出，点 X 被投影到右侧的
    X^{(-j)} 上。来源：[3] 中的图 1
- en: The method is implemented in R in the [soboldMDA](https://gitlab.com/drti/sobolmda)
    package, based on the very fast [ranger](https://cran.r-project.org/web/packages/ranger/ranger.pdf)
    package.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法在 R 语言中实现于 [soboldMDA](https://gitlab.com/drti/sobolmda) 包中，基于非常快速的 [ranger](https://cran.r-project.org/web/packages/ranger/ranger.pdf)
    包。
- en: 'Modern Developments II: MMD-based sensitivity index'
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 现代发展 II：基于 MMD 的敏感性指数
- en: Looking at the formulation using the distance *d*, a natural question is to
    ask whether different distances could be used to get variable importance measures
    for more difficult problems. One such recent example is to use the MMD distance
    as *d:*
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用距离 *d* 的公式，产生了一个自然的问题：是否可以使用不同的距离来获得更复杂问题的变量重要性度量。其中一个近期的例子是使用 MMD 距离作为
    *d*：
- en: '![](../Images/317b256761f69b6c00357facb2981d7a.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/317b256761f69b6c00357facb2981d7a.png)'
- en: 'The MMD distance is a wonderful tool, that allows to quite easily build a distance
    between distributions using a kernel *k* (such as the Gaussian kernel):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: MMD 距离是一个很棒的工具，它允许通过使用核 *k*（如高斯核）相对轻松地构建分布之间的距离：
- en: '![](../Images/1aa92f737ec9e3ff4e3dbda69e531d56.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1aa92f737ec9e3ff4e3dbda69e531d56.png)'
- en: 'For the moment I leave the details to further articles. The most important
    takeaway is simply that *I^{(j)}* considers a more general target than the conditional
    expectation. It recognizes a variable *X_j* as important, as soon as it influences
    the distribution of *Y* in any way. It might be that *X_j* only changes the variance
    or the quantiles and leaves the conditional mean of *Y* untouched (see example
    below). In this case, the Sobol-MDA would not recognize *X_j* as important, but
    the MMD method would. This doesn’t necessarily make it better, it is simply a
    different tool: If one is interested in predicting the conditional expectation,
    *ST^{(j)}* is the right measure. However, if one is interested in predicting other
    aspects of the distribution, especially quantiles, *I^{(j)}* would be better suited.
    Again *I^{(j)}* can be consistently estimated using the drop-and-relearn principle
    (refitting DRF for *j=1,…,p* eacht time with variable $j$ removed), or the same
    projection approach as for Sobol-MDA can be used. A drop-and-relearn-based implementation
    is attached at the end of this article. We refer to this method here as **MMD-MDA**.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 目前我将细节留给后续文章。最重要的结论是*I^{(j)}*考虑了比条件期望更一般的目标。它将变量*X_j*视为重要，只要它以任何方式影响*Y*的分布。可能是*X_j*仅改变方差或分位数，而条件均值*Y*保持不变（见下面的例子）。在这种情况下，Sobol-MDA将不会识别*X_j*为重要，但MMD方法会。这并不一定意味着它更好，它只是另一种工具：如果你关注预测条件期望，*ST^{(j)}*是合适的度量。然而，如果你关注预测分布的其他方面，尤其是分位数，*I^{(j)}*会更适合。再次强调，*I^{(j)}*可以通过使用丢弃重学原则（每次去掉变量$j$时重新拟合DRF），或者使用与Sobol-MDA相同的投影方法来一致地估计。基于丢弃重学的实现附在本文末尾。我们在这里将这种方法称为**MMD-MDA**。
- en: Simulated Data
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模拟数据
- en: 'We now illustrate these two modern measures on a simple simulated example:
    We first download and install the Sobol-MDA package from [Gitlab](https://gitlab.com/drti/sobolmda)
    and then load all the packages necessary for this example:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们用一个简单的模拟例子来说明这两种现代度量方法：我们首先从[Gitlab](https://gitlab.com/drti/sobolmda)下载并安装Sobol-MDA包，然后加载这个例子所需的所有包：
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then we simulate from this simple example: We take *X_1, X_2, X_4, …, X_10*
    independently uniform between (-1,1) and create dependence between *X_1* and *X_3*
    by taking *X_3=X_1 + uniform error*. Then we simulate *Y* as'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们从这个简单的例子中进行模拟：我们将*X_1, X_2, X_4, …, X_10*独立地在（-1,1）之间均匀分布，并通过取*X_3=X_1 +
    均匀误差*来创建*X_1*和*X_3*之间的依赖关系。然后我们模拟*Y*为
- en: '![](../Images/db8b95d3d53203e739c5509eb1aa992e.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/db8b95d3d53203e739c5509eb1aa992e.png)'
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We then analyze the Sobol-MDA approach to estimate the conditional expectation
    of *Y* given ***X***:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们分析Sobol-MDA方法以估计给定***X***的*Y*的条件期望：
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As can be seen, it correctly identifies that *X_1* is the most important variable,
    while the others are ranked equally (un)important. This makes sense because the
    conditional expectation of *Y* is only changed by *X_1*. Crucially, the measure
    manages to do this despite the dependence between *X_1* and *X_3*. Thus we successfully
    pursued goal (1), as explained above, in this example. On the other hand, we can
    also have a look at the traditional MDA:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如所见，它正确识别出*X_1*是最重要的变量，而其他变量则被同等排序（不）重要。这是合理的，因为*Y*的条件期望仅由*X_1*改变。关键是，尽管存在*X_1*和*X_3*之间的依赖关系，该度量仍然能够做到这一点。因此，我们在这个例子中成功实现了上述目标（1）。另一方面，我们也可以看看传统的MDA：
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In this case, while it correctly identifies *X_1* as the most important variable,
    it also places *X_3* in second place, with a value that seems quite a bit higher
    than the remaining variables. This despite the fact, that *X_3* is just as unimportant
    as *X_2, X_4,…, X_10*!
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，尽管它正确识别了*X_1*作为最重要的变量，但它还将*X_3*排在第二位，其值似乎明显高于其他变量。这尽管*X_3*与*X_2, X_4,…,
    X_10*一样不重要！
- en: 'But what if we are interested in predicting the distribution of *Y* more generally,
    say for estimating quantiles? In this case, we need a measure that is able to
    recognize the influence of *X_2* on the conditional variance of *Y*. Here the
    MMD variable importance measure comes into play:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们更普遍地关注预测*Y*的分布，例如估计分位数呢？在这种情况下，我们需要一个能够识别*X_2*对*Y*条件方差影响的度量方法。在这里，MMD变量重要性度量方法发挥作用：
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Again the measure is able to correctly identify what matters: *X_1* and *X_2*
    are the two most important variables. And again, it does this despite the dependence
    between *X_1* and *X_3*. Interestingly, it also gives the variance shift from
    *X_2* a higher importance than the expectation shift from *X_1*.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，该度量能够正确识别重要的内容：*X_1* 和 *X_2* 是两个最重要的变量。尽管 *X_1* 和 *X_3* 之间存在依赖关系，它仍然能够做到这一点。有趣的是，它还给予
    *X_2* 的方差变化比 *X_1* 的期望变化更高的重要性。
- en: Real Data
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实际数据
- en: Finally, I present a real data application to demonstrate the variable importance
    measure. Note that with DRF, we could look even at multivariate ***Y*** but to
    keep things more simple, we focus on a univariate setting and consider the US
    wage data from the 2018 American Community Survey by the US Census Bureau. In
    the first [DRF paper](https://www.jmlr.org/papers/v23/21-0585.html), we obtained
    data on approximately 1 million full-time employees from the 2018 American Community
    Survey by the US Census Bureau from which we extracted the salary information
    and all covariates that might be relevant for salaries. This wealth of data is
    ideal to experiment with a method like DRF (in fact we will only use a tiny subset
    for this analysis). The data we load can be found [here](https://github.com/lorismichel/drf/blob/master/applications/wage_data/data/datasets/wage_benchmark.Rdata).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我展示了一个实际数据应用来演示变量重要性度量。注意，使用 DRF，我们甚至可以查看多变量 ***Y***，但为了简化，我们专注于单变量设置，并考虑来自美国人口普查局的
    2018 年美国社区调查的工资数据。在第一篇 [DRF 论文](https://www.jmlr.org/papers/v23/21-0585.html)
    中，我们从美国人口普查局的 2018 年美国社区调查中获得了约 100 万名全职员工的数据，从中提取了工资信息及所有可能与工资相关的协变量。这些数据非常适合用来试验像
    DRF 这样的方法（实际上我们只会使用其中的一小部分进行分析）。我们加载的数据可以在 [这里](https://github.com/lorismichel/drf/blob/master/applications/wage_data/data/datasets/wage_benchmark.Rdata)
    找到。
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We now calculate both variable importance measures (this will take a while
    as only the drop-and-relearn method is implemented for DRF):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在计算两个变量重要性度量（这会花费一些时间，因为仅实现了丢弃再学习方法用于 DRF）：
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In this case, the two variable importance measures agree quite a bit on which
    variables are important. While this is not a causal analysis, it is also nice
    that variables that are known to be important to predict wages, specifically “age”,
    “education_level” and “gender”, are indeed seen as very important by the two measures.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，这两种变量重要性度量在哪些变量重要上有相当一致的意见。虽然这不是因果分析，但变量 “年龄”、“教育水平”和“性别” 确实被这两种度量视为非常重要的，这一点也很令人欣慰。
- en: To obtain a small set of predictive variables, one could now for *j=1,…p-1,*
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得一小组预测变量，现在可以对 *j=1,…p-1* 进行操作，
- en: (I) Remove the least important variable
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: (I) 移除最不重要的变量
- en: (II) Calculate the loss (e.g. mean squared error) on a test set
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: (II) 在测试集上计算损失（例如均方误差）
- en: (III) Recalculate the variable importance for the remaining variable
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: (III) 重新计算剩余变量的变量重要性
- en: (IV) Repeat until a certain stopping criterion is met
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: (IV) 重复直到满足某个停止标准
- en: 'One could stop, for instance, if the loss increased by more than 5%. To make
    my life easier in this article, I just use the same variable importance values
    saved in “SobolMDA” and “MMDVimp” above. That is, I ignore step (III) and only
    consider (I), (II) and (IV). When the goal of estimation is the full conditional
    distribution, step (II) is also not entirely clear. We use what we refer to as
    MMD loss, described in more detail in our paper ([4]). This loss considers the
    error we are making in the prediction of the distribution. For the conditional
    mean, we simply use the mean-squared error. This is done in the function “evalall”
    found below:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 比如，如果损失增加超过 5%，可以停止。为了简化本文，我仅使用上面保存的相同变量重要性值，即“SobolMDA”和“MMDVimp”。也就是说，我忽略了步骤（III），只考虑（I）、（II）和（IV）。当估计目标是完整的条件分布时，步骤（II）也不是完全清楚的。我们使用称为
    MMD 损失的度量，这在我们的论文中有更详细的描述 ([4])。这种损失考虑了我们在预测分布时所犯的错误。对于条件均值，我们简单地使用均方误差。这在下面找到的函数“evalall”中完成：
- en: '[PRE9]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This results in the following two pictures:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下两张图片：
- en: '![](../Images/3267a58fc9905842a0a742c1a5a2b645.png)![](../Images/0700067c7c45b844f3d63d0257a74368.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3267a58fc9905842a0a742c1a5a2b645.png)![](../Images/0700067c7c45b844f3d63d0257a74368.png)'
- en: Notice that both have somewhat wiggly lines, which is first due to the fact
    that I did not recalculate the importance measure, e.g., left out step (III),
    and second due to the randomness of the forests. Aside from this, the graphs nicely
    show how the errors successively increase with each variable that is removed.
    This increase is first slow for the least important variables and then gets quicker
    for the most important ones, exactly as one would expect. In particular, the loss
    in both cases remains virtually unchanged if one removes the 50 least important
    variables! In fact, one could remove about 70 variables in both cases without
    increasing the loss by more than 6%. One has to note though that many predictors
    are part of one-hot encoded categorical variables and thus one needs to be somewhat
    careful when removing predictors, as they correspond to levels of one categorical
    variable. However, in an actual application, this might still be desirable.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到这两者都有些许波动线条，这首先是由于我没有重新计算重要性测量，例如，省略了步骤(III)，其次是森林的随机性。除此之外，图形很好地展示了每次移除变量后错误如何逐渐增加。这种增加对于最不重要的变量开始时很慢，然后对于最重要的变量加速，正如预期的那样。特别是，如果移除50个最不重要的变量，损失几乎保持不变！实际上，在这两种情况下，移除约70个变量而损失增加不超过6%是可能的。不过，需要注意的是，许多预测变量是单独编码的分类变量的一部分，因此在移除预测变量时需要小心，因为它们对应于一个分类变量的水平。然而，在实际应用中，这可能仍然是可取的。
- en: Conclusion
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we looked at modern approaches to variable importance in Random
    Forests, with the goal of obtaining a small set of predictors or covariates, both
    with respect to the conditional expectation and for the conditional distribution
    more generally. We have seen in the wage data example, that this can lead to a
    substantial reduction in predictors with virtually the same accuracy.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们探讨了随机森林中变量重要性的现代方法，目标是获得一个小的预测变量或协变量集合，既考虑条件期望，也更一般地考虑条件分布。我们在工资数据示例中看到，这可以显著减少预测变量，同时保持几乎相同的准确度。
- en: As noted above the measures presented are not strictly constrained to Random
    Forest, but can be used more generally in principle. However, forests allow for
    the elegant projection approach that allows for the calculation of the importance
    measure for all variables *j*, without having to refit the forest each time (!)
    This is described in both [3] and [4].
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，所呈现的测量方法并不严格限制于随机森林，但原则上可以更一般地使用。然而，森林允许优雅的投影方法，使得可以计算所有变量的*重要性测量*，而无需每次都重新拟合森林（！）这在[3]和[4]中都有描述。
- en: Literature
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文献
- en: '[1] Breiman, L. (2001). Random forests. Machine learning, 45(1):5–32.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Breiman, L. (2001). 随机森林。机器学习, 45(1):5–32。'
- en: '[2] Breiman, L. (2003a). Setting up, using, and understanding random forests
    v3.1\. Technical report, UC Berkeley, Department of Statistics'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Breiman, L. (2003a). 设置、使用和理解随机森林 v3.1。技术报告，UC Berkeley，统计系。'
- en: '[3] Bénard, C., Da Veiga, S., and Scornet, E. (2022). Mean decrease accuracy
    for random forests: inconsistency, and a practical solution via the Sobol-MDA.
    Biometrika, 109(4):881–900.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Bénard, C., Da Veiga, S., 和 Scornet, E. (2022). 随机森林的均值减少准确度：不一致性及通过Sobol-MDA的实际解决方案。Biometrika,
    109(4):881–900。'
- en: '[4] Clément Bénard, Jeffrey Näf, and Julie Josse. MMD-based variable importance
    for distributional random forest, 2023.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Clément Bénard, Jeffrey Näf, 和 Julie Josse. 基于MMD的分布随机森林变量重要性，2023。'
- en: '[5] Strobl, C., Boulesteix, A.-L., Zeileis, A., and Hothorn, T. (2007). Bias
    in random forest variable importance measures: illustrations, sources and a solution.
    BMC Bioinformatics, 8:25.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Strobl, C., Boulesteix, A.-L., Zeileis, A., 和 Hothorn, T. (2007). 随机森林变量重要性测量中的偏差：说明、来源和解决方案。BMC
    Bioinformatics, 8:25。'
- en: 'Appendix : Code'
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录：代码
- en: '[PRE10]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
