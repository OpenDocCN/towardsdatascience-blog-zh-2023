- en: Probabilistic Logistic Regression with TensorFlow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 进行概率逻辑回归
- en: 原文：[https://towardsdatascience.com/probabilistic-logistic-regression-with-tensorflow-73e18f0ddc48](https://towardsdatascience.com/probabilistic-logistic-regression-with-tensorflow-73e18f0ddc48)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/probabilistic-logistic-regression-with-tensorflow-73e18f0ddc48](https://towardsdatascience.com/probabilistic-logistic-regression-with-tensorflow-73e18f0ddc48)
- en: Probabilistic Deep Learning
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概率深度学习
- en: '[](https://medium.com/@luisroque?source=post_page-----73e18f0ddc48--------------------------------)[![Luís
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----73e18f0ddc48--------------------------------)[](https://towardsdatascience.com/?source=post_page-----73e18f0ddc48--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----73e18f0ddc48--------------------------------)
    [Luís Roque](https://medium.com/@luisroque?source=post_page-----73e18f0ddc48--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@luisroque?source=post_page-----73e18f0ddc48--------------------------------)[![Luís
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----73e18f0ddc48--------------------------------)[](https://towardsdatascience.com/?source=post_page-----73e18f0ddc48--------------------------------)[![数据科学前沿](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----73e18f0ddc48--------------------------------)
    [Luís Roque](https://medium.com/@luisroque?source=post_page-----73e18f0ddc48--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----73e18f0ddc48--------------------------------)
    ·9 min read·Jan 25, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [数据科学前沿](https://towardsdatascience.com/?source=post_page-----73e18f0ddc48--------------------------------)
    ·阅读时间 9 分钟·2023年1月25日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Introduction
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: This article belongs to the series “Probabilistic Deep Learning”. This weekly
    series covers probabilistic approaches to deep learning. The main goal is to extend
    deep learning models to quantify uncertainty, i.e., know what they do not know.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本文属于“概率深度学习”系列。该系列每周介绍深度学习中的概率方法。主要目标是扩展深度学习模型以量化不确定性，即了解它们不知道的内容。
- en: In this article, we will introduce the concept of probabilistic logistic regression,
    a powerful technique that allows for the inclusion of uncertainty in the prediction
    process. We will explore how this approach can lead to more robust and accurate
    predictions, especially in cases where the data is noisy, or the model is overfitting.
    Additionally, by incorporating a prior distribution on the model parameters, we
    can regularize the model and prevent overfitting. This approach serves as a great
    first step into the exciting world of Bayesian Deep Learning.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将介绍概率逻辑回归的概念，这是一种强大的技术，允许在预测过程中纳入不确定性。我们将探讨这种方法如何在数据噪声大或模型过拟合的情况下提供更稳健和准确的预测。此外，通过在模型参数上引入先验分布，我们可以对模型进行正则化，防止过拟合。这种方法是进入贝叶斯深度学习激动人心的世界的绝佳起点。
- en: 'Articles published so far:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 已发布的文章：
- en: '[Gentle Introduction to TensorFlow Probability: Distribution Objects](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-distribution-objects-1bb6165abee1)'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[TensorFlow Probability 轻松入门：分布对象](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-distribution-objects-1bb6165abee1)'
- en: '[Gentle Introduction to TensorFlow Probability: Trainable Parameters](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-trainable-parameters-5098ea4fed15)'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[TensorFlow Probability 轻松入门：可训练参数](https://medium.com/towards-data-science/gentle-introduction-to-tensorflow-probability-trainable-parameters-5098ea4fed15)'
- en: '[Maximum Likelihood Estimation from scratch in TensorFlow Probability](/maximum-likelihood-estimation-from-scratch-in-tensorflow-probability-2fc0eefdbfc2)'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[从零开始的最大似然估计在 TensorFlow Probability 中](/maximum-likelihood-estimation-from-scratch-in-tensorflow-probability-2fc0eefdbfc2)'
- en: '[Probabilistic Linear Regression from scratch in TensorFlow](/probabilistic-linear-regression-from-scratch-in-tensorflow-2eb633fffc00)'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[从零开始的 TensorFlow 概率线性回归](/probabilistic-linear-regression-from-scratch-in-tensorflow-2eb633fffc00)'
- en: '[Probabilistic vs. Deterministic Regression with Tensorflow](https://medium.com/towards-data-science/probabilistic-vs-deterministic-regression-with-tensorflow-85ef791beeef)'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Tensorflow 中的概率回归与确定性回归](https://medium.com/towards-data-science/probabilistic-vs-deterministic-regression-with-tensorflow-85ef791beeef)'
- en: '[Frequentist vs. Bayesian Statistics with Tensorflow](https://medium.com/towards-data-science/frequentist-vs-bayesian-statistics-with-tensorflow-fbba2c6c9ae5)'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[频率学派与贝叶斯统计学在 Tensorflow 中的对比](https://medium.com/towards-data-science/frequentist-vs-bayesian-statistics-with-tensorflow-fbba2c6c9ae5)'
- en: '[Deterministic vs. Probabilistic Deep Learning](/deterministic-vs-probabilistic-deep-learning-5325769dc758)'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[确定性与概率深度学习](/deterministic-vs-probabilistic-deep-learning-5325769dc758)'
- en: '[Naive Bayes from scratch with TensorFlow](/naive-bayes-from-scratch-with-tensorflow-6e04c5a25947)'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[从头开始使用 TensorFlow 实现朴素贝叶斯](/naive-bayes-from-scratch-with-tensorflow-6e04c5a25947)'
- en: Probabilistic Logistic Regression with TensorFlow
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 进行**概率逻辑回归**
- en: '![](../Images/93f68e2e7ac8cbc8894c52dd44805819.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/93f68e2e7ac8cbc8894c52dd44805819.png)'
- en: 'Figure 1: The motto for today: lines can separate more things than we give
    them credit for ([source](https://unsplash.com/photos/lIdS6_XiAR0))'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：今天的座右铭：直线可以分隔更多的事物，比我们想象的要多 ([source](https://unsplash.com/photos/lIdS6_XiAR0))
- en: As usual, the code is available on my [GitHub](https://github.com/luisroque/probabilistic_deep_learning_with_TFP).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 与往常一样，代码可以在我的 [GitHub](https://github.com/luisroque/probabilistic_deep_learning_with_TFP)
    上找到。
- en: Preliminary Work
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初步工作
- en: In our previous article in this series we built the Naive Bayes algorithm from
    scratch and used it to classify wine samples based on selected characteristics.
    This time, we will be using a probabilistic logistic regression approach. Since
    we already followed the end-to-end approach, I will skip most of the Exploratory
    Data Analysis section and the class prior distribution definition.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们系列文章的前一篇中，我们从头开始构建了朴素贝叶斯算法，并使用它根据选择的特征对葡萄酒样本进行分类。这一次，我们将使用**概率逻辑回归**方法。由于我们已经采用了端到端的方法，我将跳过大部分探索性数据分析部分和类先验分布定义。
- en: The only thing to note is that there is a difference in the features that we
    selected for this model.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一需要注意的是我们为此模型选择的特征有所不同。
- en: '![](../Images/a2e8b7b5c2fe0281cf88364c78545fd9.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a2e8b7b5c2fe0281cf88364c78545fd9.png)'
- en: 'Figure 2: Target samples distribution by alcohol and hue.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：根据酒精和色调的目标样本分布。
- en: We will use hue and flavanoids as our independent variables. Notice how these
    features are more effective in separating the target variable than alcohol and
    hue.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用色调和类黄酮作为自变量。注意这些特征在分隔目标变量方面比酒精和色调更有效。
- en: '![](../Images/e7bfa9bd6d04e51dfc8d656ea2f977cf.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e7bfa9bd6d04e51dfc8d656ea2f977cf.png)'
- en: 'Figure 3: Target samples distribution by flavanoids and hue.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：根据类黄酮和色调的目标样本分布。
- en: Probabilistic Logistic Regression
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**概率逻辑回归**'
- en: Logistic regression is a widely-used statistical method for binary classification,
    which is used to model the relationship between a binary response variable and
    one or more predictors. Logistic regression can be used to model the probability
    of a binary outcome as a function of the predictor variables. The traditional
    logistic regression model is a deterministic model, which assumes that the relationship
    between the predictor variables and the response variable is fixed and known.
    However, in many real-world applications, the true relationship between the predictors
    and the response is uncertain, and it is more appropriate to use a probabilistic
    approach.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是一种广泛使用的二元分类统计方法，用于建模二元响应变量与一个或多个预测变量之间的关系。逻辑回归可以用来建模二元结果的概率作为预测变量的函数。传统的逻辑回归模型是一个确定性模型，假设预测变量与响应变量之间的关系是固定且已知的。然而，在许多实际应用中，预测变量与响应变量之间的真实关系是不确定的，因此使用**概率方法**更为合适。
- en: Probabilistic logistic regression models the relationship between the predictor
    variables and the binary response variable using a probabilistic framework, and
    is able to account for uncertainty in the data and the model parameters. This
    is achieved by placing probability distributions over the model parameters, rather
    than assuming fixed values for them. In this way, probabilistic logistic regression
    models can provide more accurate predictions and better uncertainty quantification
    compared to traditional logistic regression models.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**概率逻辑回归**模型通过概率框架建模预测变量与二元响应变量之间的关系，并能够考虑数据和模型参数的不确定性。这是通过对模型参数施加概率分布来实现的，而不是假设固定值。通过这种方式，与传统逻辑回归模型相比，**概率逻辑回归**模型可以提供更准确的预测和更好的不确定性量化。'
- en: One of the most popular probabilistic models for logistic regression is the
    Bayesian logistic regression model. These models are based on Bayes’ theorem,
    which states that the posterior probability of a model parameter given the data
    is proportional to the product of the likelihood of the data given the parameter
    and the prior probability of the parameter. Often, Bayesian logistic regression
    models use a conjugate prior distribution for the model parameters, which allows
    for closed-form solutions for the posterior distributions. This enables the computation
    of the probability of the response variable given the predictor variables, which
    is known as the posterior predictive distribution.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最流行的逻辑回归概率模型之一是贝叶斯逻辑回归模型。这些模型基于贝叶斯定理，该定理指出，给定数据的模型参数的后验概率与数据给定参数的似然和参数的先验概率的乘积成正比。通常，贝叶斯逻辑回归模型使用共轭先验分布来对模型参数进行建模，这允许对后验分布进行封闭形式的解法。这使得计算响应变量给定预测变量的概率成为可能，这被称为后验预测分布。
- en: Likelihood
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 似然
- en: In this section, we present a method for computing the class-conditional densities
    in the probabilistic approach to logistic regression. Our method is based on the
    maximum likelihood estimate for the means, which is given by
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了一种在逻辑回归的概率方法中计算类别条件密度的方法。我们的方法基于均值的最大似然估计，其由以下公式给出
- en: '![](../Images/3b98d5052f470207f04229ae1581cee8.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3b98d5052f470207f04229ae1581cee8.png)'
- en: where 𝑋(𝑛)𝑖 is the i-th feature of the n-th sample, 𝑌(𝑛) is the target label
    of the n-th sample, 𝑘 is the class label, and 𝛿(𝑌(𝑛)=𝑦𝑘) is an indicator function
    that equals 1 if 𝑌(𝑛)=𝑦𝑘, and 0 otherwise.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 𝑋(𝑛)𝑖 是第 n 个样本的第 i 个特征，𝑌(𝑛) 是第 n 个样本的目标标签，𝑘 是类别标签，𝛿(𝑌(𝑛)=𝑦𝑘) 是一个指示函数，如果 𝑌(𝑛)=𝑦𝑘
    则等于 1，否则为 0。
- en: To estimate the standard deviations 𝜎𝑖, instead of using the closed-form solution
    we will be learning these parameters from the data. We achieve this by implementing
    a custom training loop, which optimizes the values of the standard deviations
    by minimizing the average per-example negative log-likelihood of the data.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了估计标准差 𝜎𝑖，我们将通过从数据中学习这些参数来实现，而不是使用封闭形式的解法。我们通过实现自定义训练循环来实现这一点，该循环通过最小化数据的平均每个示例负对数似然来优化标准差的值。
- en: Our function computes the means 𝜇𝑖𝑘 of the class-conditional Gaussians according
    to the above equation. Then, it creates a multivariate Gaussian distribution object
    using MultivariateNormalDiag with the means set to 𝜇𝑖𝑘 and the scales set to the
    TensorFlow Variable.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的函数根据上述方程计算类别条件高斯分布的均值 𝜇𝑖𝑘。然后，它使用 MultivariateNormalDiag 创建一个多变量高斯分布对象，将均值设置为
    𝜇𝑖𝑘，将尺度设置为 TensorFlow 变量。
- en: The function runs a custom training loop for a specified number of epochs, in
    which the average per-example negative log-likelihood is computed. Next, the gradients
    are propagated and the scales variables updated accordingly. At each iteration,
    the values of the scales variable and the loss are saved.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数运行一个自定义训练循环，循环次数为指定的纪元，其中计算平均每个示例的负对数似然。接下来，梯度被传播，尺度变量相应地更新。在每次迭代中，尺度变量的值和损失都会被保存。
- en: 'It returns a tuple of three objects: the loss values, the scales variable at
    each iteration, and the final learned batched MultivariateNormalDiag distribution
    object.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 它返回一个包含三个对象的元组：损失值、每次迭代的尺度变量和最终学习到的批量 MultivariateNormalDiag 分布对象。
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let’s create our variables to be trained.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建要训练的变量。
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We are now ready to start the training procedure.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备开始训练过程。
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Finally, we can check how the model separates our classes of wine.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以检查模型如何区分我们的葡萄酒类别。
- en: '![](../Images/be1367ed1a5cbbc932d24d31412fb536.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be1367ed1a5cbbc932d24d31412fb536.png)'
- en: 'Figure 4: Class-conditional density contours.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：类别条件密度轮廓。
- en: Using the function that we defined in the previous article, we can generate
    predictions for out test set. In the plot above we can see that the classes are
    well separated and thus we get a good accuracy from our model.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们在前一篇文章中定义的函数，我们可以为测试集生成预测。在上面的图中，我们可以看到类别被很好地分开，因此我们从模型中获得了良好的准确率。
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In order to quantitatively assess the performance of our probabilistic logistic
    regression model, we plot the decision regions. These regions, defined by the
    boundaries that separate the two classes, provide insight into the ability of
    the model to separate the classes. Our analysis shows that the model is able to
    effectively separate the two classes in the dataset, as evidenced by the visually
    distinct regions. However, it is important to note that the decision boundary
    is constrained to be linear, as per the assumptions of the logistic regression
    model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了定量评估我们的概率逻辑回归模型的性能，我们绘制了决策区域。这些区域由分隔两个类别的边界定义，为模型区分类别的能力提供了见解。我们的分析表明，模型能够有效地分隔数据集中的两个类别，这从视觉上明显的区域可以看出。然而，需要注意的是，决策边界被限制为线性，这是逻辑回归模型的假设。
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/9181695e5c5d25a63646a2eab35ea724.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9181695e5c5d25a63646a2eab35ea724.png)'
- en: 'Figure 5: Class-conditional decision regions.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：类别条件决策区域。
- en: The missing link with Logistic Regression
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归中的缺失链接
- en: In this section, we link the above definitions of the class-conditional densities
    to logistic regression. We show that the predictive distribution 𝑃(𝑌=𝑦0|𝑋) can
    be written as
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将上述类别条件密度的定义与逻辑回归联系起来。我们展示了预测分布 𝑃(𝑌=𝑦0|𝑋) 可以写作
- en: '![](../Images/7db37d902814496db2cfec21a921cc07.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7db37d902814496db2cfec21a921cc07.png)'
- en: where 𝑃(𝑋|𝑌=𝑦0) and 𝑃(𝑋|𝑌=𝑦1) are the class-conditional densities, and 𝑃(𝑌=𝑦0)
    and 𝑃(𝑌=𝑦1) are the class priors.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 𝑃(𝑋|𝑌=𝑦0) 和 𝑃(𝑋|𝑌=𝑦1) 是类别条件密度，𝑃(𝑌=𝑦0) 和 𝑃(𝑌=𝑦1) 是类别先验。
- en: This equation can be re-arranged to give 𝑃(𝑌=𝑦0|𝑋)=𝜎(𝑎) where
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程可以重新排列为 𝑃(𝑌=𝑦0|𝑋)=𝜎(𝑎)，其中
- en: '![](../Images/269d6513225ebe05b27a1e7ba8b33184.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/269d6513225ebe05b27a1e7ba8b33184.png)'
- en: is the sigmoid function, and
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 是 sigmoid 函数，以及
- en: '![](../Images/06aaf728416430df589244b420b559e9.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06aaf728416430df589244b420b559e9.png)'
- en: is the log-odds.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 是对数赔率。
- en: With our additional modeling assumption of a shared covariance matrix Σ, it
    can be shown, using the Gaussian pdf, that 𝑎 is in fact a linear function of 𝑋,
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 通过我们额外的建模假设，即共享协方差矩阵 Σ，可以使用高斯概率密度函数显示 𝑎 实际上是 𝑋 的线性函数，
- en: '![](../Images/162844021b49c68e46b9aa7be22d906e.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/162844021b49c68e46b9aa7be22d906e.png)'
- en: where
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '![](../Images/be8553acfb09ac4ae9a6a406e41435ea.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be8553acfb09ac4ae9a6a406e41435ea.png)'
- en: This linear function, 𝑎=𝑤𝑇𝑋+𝑤0, explains the reason behind the decision boundary
    of a logistic regression being linear. It can be seen that the parameters 𝑤 and
    𝑤0 are functions of the class-conditional densities 𝑃(𝑋|𝑌=𝑦0) and 𝑃(𝑋|𝑌=𝑦1) and
    the class priors 𝑃(𝑌=𝑦0) and 𝑃(𝑌=𝑦1). These parameters are typically estimated
    with maximum likelihood, as we have done in previous sections.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这个线性函数 𝑎=𝑤𝑇𝑋+𝑤0 解释了逻辑回归的决策边界为何是线性的。可以看出，参数 𝑤 和 𝑤0 是类别条件密度 𝑃(𝑋|𝑌=𝑦0) 和 𝑃(𝑋|𝑌=𝑦1)
    以及类别先验 𝑃(𝑌=𝑦0) 和 𝑃(𝑌=𝑦1) 的函数。这些参数通常通过最大似然估计，如我们在前面章节中所做的那样。
- en: Generative Logistic Regression Model
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成逻辑回归模型
- en: In this section, we use the equations derived in previous sections to directly
    parameterize the output Bernoulli distribution of the generative logistic regression
    model. Specifically, we use the prior distribution and class-conditional distributions
    to compute the weights and bias terms 𝑤 and 𝑤0.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用前面章节中推导出的方程来直接参数化生成逻辑回归模型的输出伯努利分布。具体来说，我们使用先验分布和类别条件分布来计算权重和偏置项 𝑤 和
    𝑤0。
- en: To achieve this, we write a new function that takes the prior distribution and
    the class-conditional distributions as inputs. The function uses the parameters
    of these distributions to compute the weights and bias terms according to the
    equations derived in previous sections.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们编写了一个新函数，该函数以先验分布和类别条件分布作为输入。该函数使用这些分布的参数来计算权重和偏置项，根据前面章节中推导出的方程。
- en: The inputs to the function are the prior distribution prior over the two classes
    and the class-conditional distributions.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的输入是对两个类别的先验分布和类别条件分布。
- en: The function then uses these inputs to compute the weights and bias terms as
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，函数使用这些输入来计算权重和偏置项，如下所示
- en: '![](../Images/be8553acfb09ac4ae9a6a406e41435ea.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be8553acfb09ac4ae9a6a406e41435ea.png)'
- en: The function returns 𝑤 and 𝑤0, which can be used to directly parameterize the
    output Bernoulli distribution of the generative logistic regression model. This
    allows for a more direct and transparent understanding of the model parameters
    and their relationship to the prior and class-conditional distributions.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数返回𝑤和𝑤0，这可以用来直接参数化生成逻辑回归模型的输出伯努利分布。这允许对模型参数及其与先验和类条件分布的关系有更直接和透明的理解。
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We can now use these parameters to make a contour plot to display the predictive
    distribution of our logistic regression model.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用这些参数制作轮廓图，以显示我们逻辑回归模型的预测分布。
- en: '[PRE6]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/7148914bfdb2d7c07d2ef4e63c054c23.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7148914bfdb2d7c07d2ef4e63c054c23.png)'
- en: 'Figure 6: Density contours of the predictive distribution of our logistic regression
    model.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: 我们逻辑回归模型预测分布的密度轮廓。'
- en: Is our approach fully Bayesian?
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们的方法是否完全是贝叶斯的？
- en: The above approach can be considered a form of Bayesian inference, as it involves
    incorporating prior knowledge about the model parameters through the prior distribution
    and updating this knowledge using the observed data through the class-conditional
    distributions. This is a key aspect of Bayesian inference, which aims to incorporate
    prior knowledge and uncertainty about the model parameters into the inference
    process.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法可以被视为一种贝叶斯推断形式，因为它涉及通过先验分布引入关于模型参数的先验知识，并通过类条件分布使用观测数据更新这些知识。这是贝叶斯推断的一个关键方面，旨在将关于模型参数的先验知识和不确定性融入推断过程。
- en: In Bayesian inference, the goal is to compute the posterior distribution of
    the model parameters given the observed data. The above approach can be seen as
    a form of approximate Bayesian inference, as it involves using the maximum likelihood
    estimates of the class-conditional densities and the prior distribution to compute
    the weights and biases of the model. Also, the approach incorporates uncertainty
    through the shared covariance matrix, which serves as a regularization term.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在贝叶斯推断中，目标是计算给定观测数据下模型参数的后验分布。上述方法可以看作是一种近似贝叶斯推断，因为它涉及使用类条件密度和先验分布的最大似然估计来计算模型的权重和偏置。同时，该方法通过共享协方差矩阵来引入不确定性，这作为正则化项。
- en: It is worth noting that the above approach is not fully Bayesian, as it does
    not provide a closed form for the posterior of the model parameters. Instead,
    it uses an approximation based on the maximum likelihood estimates.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，上述方法并非完全贝叶斯，因为它没有提供模型参数的后验分布的封闭形式。相反，它使用基于最大似然估计的近似。
- en: Conclusions
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we proposed a probabilistic approach to logistic regression
    that addresses aleatoric uncertainty in the prediction process. Through the incorporation
    of a prior distribution on the model parameters, our approach regularizes the
    model and prevents overfitting. We show how to implement the approach using TensorFlow
    Probability and how to analyse its results.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们提出了一种概率方法来处理预测过程中的内在不确定性。通过在模型参数上引入先验分布，我们的方法对模型进行正则化，防止过拟合。我们展示了如何使用
    TensorFlow Probability 实现该方法以及如何分析其结果。
- en: It is worth noting that while our approach incorporates Bayesian principles,
    it is not a full Bayesian approach as we do not have a full posterior distribution
    of the model parameters. Nevertheless, accounting for aleatoric uncertainty in
    the prediction process already gives us more confidence about our predictive process.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，尽管我们的方法包含了贝叶斯原则，但它并不是完全贝叶斯的方法，因为我们没有模型参数的完整后验分布。然而，考虑到预测过程中的内在不确定性，已经使我们对预测过程更有信心。
- en: 'Keep in touch: [LinkedIn](https://www.linkedin.com/in/luisbrasroque/)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '保持联系: [LinkedIn](https://www.linkedin.com/in/luisbrasroque/)'
- en: References and Materials
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献和材料
- en: '[1] — [Wine Dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] — [葡萄酒数据集](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine)'
- en: '[2] — [Coursera: Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] — [Coursera: 深度学习专项课程](https://www.coursera.org/specializations/deep-learning)'
- en: '[3] — [Coursera: TensorFlow 2 for Deep Learning](https://www.coursera.org/specializations/tensorflow2-deeplearning)
    Specialization'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] — [Coursera: TensorFlow 2 深度学习专项课程](https://www.coursera.org/specializations/tensorflow2-deeplearning)'
- en: '[4] — [TensorFlow Probability Guides and Tutorials](https://www.tensorflow.org/probability/overview)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] — [TensorFlow 概率指南和教程](https://www.tensorflow.org/probability/overview)'
- en: '[5] — [TensorFlow Probability Posts in TensorFlow Blog](https://blog.tensorflow.org/search?label=TensorFlow+Probability&max-results=20)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] — [TensorFlow 博客中的 TensorFlow 概率帖子](https://blog.tensorflow.org/search?label=TensorFlow+Probability&max-results=20)'
