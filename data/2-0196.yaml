- en: A Gentle Introduction to Open Source Large Language Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对开源大型语言模型的温馨介绍
- en: 原文：[https://towardsdatascience.com/a-gentle-introduction-to-open-source-large-language-models-3643f5ca774](https://towardsdatascience.com/a-gentle-introduction-to-open-source-large-language-models-3643f5ca774)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-gentle-introduction-to-open-source-large-language-models-3643f5ca774](https://towardsdatascience.com/a-gentle-introduction-to-open-source-large-language-models-3643f5ca774)
- en: Open Language Models
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开放语言模型
- en: Why everyone is talking about Llamas, Alpacas, Falcons and other animals
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么每个人都在谈论美洲驼、羊驼、猎鹰和其他动物
- en: '[](https://donatoriccio.medium.com/?source=post_page-----3643f5ca774--------------------------------)[![Donato
    Riccio](../Images/0af2a026e72a023db4635522cbca50eb.png)](https://donatoriccio.medium.com/?source=post_page-----3643f5ca774--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3643f5ca774--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3643f5ca774--------------------------------)
    [Donato Riccio](https://donatoriccio.medium.com/?source=post_page-----3643f5ca774--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://donatoriccio.medium.com/?source=post_page-----3643f5ca774--------------------------------)[![Donato
    Riccio](../Images/0af2a026e72a023db4635522cbca50eb.png)](https://donatoriccio.medium.com/?source=post_page-----3643f5ca774--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3643f5ca774--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3643f5ca774--------------------------------)
    [Donato Riccio](https://donatoriccio.medium.com/?source=post_page-----3643f5ca774--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3643f5ca774--------------------------------)
    ·11 min read·Aug 11, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3643f5ca774--------------------------------)
    ·11分钟阅读·2023年8月11日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/b78c1db423fd8ee4ce173a12b054118c.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b78c1db423fd8ee4ce173a12b054118c.png)'
- en: Image by the author (generated with Midjourney)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像（通过 Midjourney 生成）
- en: Unless you’ve been living under a rock for the last year, you’ve witnessed the
    ChatGPT revolution and to how everyone seems unable to stop using it. In this
    article, we’ll explore its alternatives, jumping into the world of open source
    models. This first article of the series *Open Language Models* is helpful for
    people looking to get started and understand Open Source Large Language Models,
    and how and why to use them.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 除非你过去一年一直在过隐居生活，否则你已经见证了 ChatGPT 革命，以及大家似乎无法停止使用它的现象。在这篇文章中，我们将探索它的替代品，深入了解开源模型的世界。这是系列文章《开放语言模型》的第一篇，对希望入门并了解开源大型语言模型的人很有帮助，以及如何使用它们和为何使用它们。
- en: Table of contents
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目录
- en: — [Why do we need Open Source Models?](#c41d)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: — [我们为什么需要开源模型？](#c41d)
- en: — [The bigger the better? Training L](#031f)arge Language Models
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: — [越大越好？训练大型语言模型](#031f)
- en: — [Fine-tuning Large Language Models](#9419)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: — [微调大型语言模型](#9419)
- en: — [The Best Open Source Large Language Models](#1f3c)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: — [最佳开源大型语言模型](#1f3c)
- en: — [Running a Large Language Model on your computer](#2d37)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: — [在你的计算机上运行大型语言模型](#2d37)
- en: — [Limitations](#d1aa)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: — [限制](#d1aa)
- en: — [Conclusion](#2c45)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: — [结论](#2c45)
- en: What is a Large Language Model?
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是大型语言模型？
- en: A **Large Language Model (LLM)** is an AI capable of understanding and generating
    human language. At the heart, there’s a type of neural network called a transformer,
    that works by predicting what word comes next in a sentence. The word *large*
    describes these models’ extensive nature since they canhave billions or even trillions
    of parameters. What differentiates them is their ability to specialize in particular
    tasks, such as code generation or translation, or be applied to general instruction-following
    chatbots. One of the groundbreaking aspects of these models is that they enable
    *zero-shot* and *few-shot learning,* as they exhibit an unprecedented ability
    to learn tasks they have not been explicitly trained for. [1]
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一个**大型语言模型（LLM）**是一个能够理解和生成自然语言的人工智能。核心是一种叫做变换器的神经网络，它通过预测句子中下一个词来工作。词汇*大型*描述了这些模型的广泛性质，因为它们可以拥有数十亿甚至万亿个参数。它们的不同之处在于能够专注于特定任务，如代码生成或翻译，或应用于一般的指令跟随聊天机器人。这些模型的一个开创性方面是它们支持*零-shot*和*少-shot*学习，因为它们展示了前所未有的能力来学习未经过明确训练的任务。[1]
- en: '**Why do we need Open Source Models?**'
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**我们为什么需要开源模型？**'
- en: Suppose you use GPT API to create an innovative app that quickly gains traction.
    Everything is going smoothly until OpenAI changes their course of action. They
    might halt the service, escalate the cost, or even decrease the capability of
    their models — which is already happening. [2]
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你使用 GPT API 创建了一个创新的应用程序，并迅速获得了关注。一切进展顺利，直到 OpenAI 改变了他们的行动计划。他们可能会停止服务、提高费用，甚至降低模型的能力——这已经在发生。[2]
- en: Currently, your only solution would be to adjust to their new policies. Moreover,
    relying on a third-party API results in your data being transmitted to their server.
    While OpenAI may not utilize data from GPT APIs for model training, [3] deploying
    your own language model guarantees you total control over these operations. Even
    if this may seem like an ideal plan, **deploying your private LLM also comes with
    its own set of limitations and challenges**, which will be addressed in this article.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，你唯一的解决方案是适应他们的新政策。此外，依赖第三方 API 会导致你的数据传输到他们的服务器。虽然 OpenAI 可能不会利用 GPT API
    的数据进行模型训练，[3] 但**部署你自己的语言模型可以保证你对这些操作的完全控制**。即使这看起来是一个理想的计划，**部署你自己的 LLM 也有其自身的限制和挑战**，这些将在本文中讨论。
- en: '![](../Images/07203eb84cbde74e5569f1097fc7ce03.png)![](../Images/9308c09c7eb3d9aff77ddabd9f5aed2e.png)![](../Images/537439fafa4ae2af699eb2f04ab98635.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07203eb84cbde74e5569f1097fc7ce03.png)![](../Images/9308c09c7eb3d9aff77ddabd9f5aed2e.png)![](../Images/537439fafa4ae2af699eb2f04ab98635.png)'
- en: '**(Left)** A Llama. Photo by Sébastien Goldberg | **(Middle)** A Vicuña. Photo
    by Dušan veverkolog | **(Right)** An Alpaca. Photo by Adrian Dascal. All from
    Unsplash.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**（左）** 一只驼鹿。照片由 Sébastien Goldberg 拍摄 | **（中）** 一只美洲驼。照片由 Dušan veverkolog
    拍摄 | **（右）** 一只羊驼。照片由 Adrian Dascal 拍摄。所有照片均来自 Unsplash。'
- en: The bigger the better? Training Large Language Models
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 越大越好？训练大型语言模型
- en: 'If you’ve stumbled upon something like **LLaMA 65B**, you’re likely wondering
    what the meaning of ***65B***. Well, it simply refers to the number of parameters
    in the model. As the size of the model increases, it requires more time to train
    and uses up more memory for inference. Unlike what you’re used to hearing in machine
    learning, complex models can more easily generalize to different tasks. Some have
    an enormous number of parameters: GPT-3 has 175 billion, while GPT-4 has more
    than 1 trillion. Training them from scratch has been estimated to cost millions
    of dollars. For example, **Google’s PaLM 540B was trained on 2240 GPUs** [4].
    For comparison, EfficientNet-B7, one of the most popular deep learning models
    for image classification, has only 66M parameters.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你碰巧看到像**LLaMA 65B**这样的模型，你可能会想知道***65B***的含义。它简单地指的是模型中的参数数量。随着模型规模的增加，它需要更多的训练时间，并在推理时消耗更多的内存。与机器学习中的常见观点不同，复杂的模型可以更容易地泛化到不同的任务。有些模型的参数数量非常庞大：GPT-3
    拥有 1750 亿，而 GPT-4 拥有超过 1 万亿。估计从零开始训练这些模型需要数百万美元。例如，**谷歌的 PaLM 540B 在 2240 个 GPU
    上进行训练**[4]。相比之下，EfficientNet-B7 是最受欢迎的图像分类深度学习模型之一，仅有 6600 万个参数。
- en: '![](../Images/f79c3dbb127b44da8aa89b3bb2e17224.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f79c3dbb127b44da8aa89b3bb2e17224.png)'
- en: Clearly, not something you can train on your laptop. [5]
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这不是你可以在笔记本电脑上训练的模型。[5]
- en: 'In 2022, Google claimed:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2022 年，谷歌声称：
- en: As the scale of the model increases, the performance improves across tasks while
    also unlocking new capabilities.
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 随着模型规模的增加，性能在各个任务上得到提升，同时也解锁了新的能力。
- en: In the current state of LLMs, more parameters are usually better [4]. Companies
    focus on building bigger models, but the current trend in open source is to create
    smaller, more efficient models. While the most popular open source models usually
    have up to 70B parameters, it’s not uncommon to see smaller, fine-tuned models
    performing better than bigger models on a specific task. Also, larger models require
    more resources for training and inference, making them challenging to deploy.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在当前的 LLM 状态下，更多的参数通常意味着更好[4]。公司专注于构建更大的模型，但当前的开源趋势是创建更小、更高效的模型。虽然最受欢迎的开源模型通常有多达
    70B 的参数，但在特定任务上，小型的、经过微调的模型表现可能优于更大的模型。此外，更大的模型在训练和推理时需要更多资源，部署起来也更具挑战性。
- en: In fact, within a year, even Google changed their perspective.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，在一年内，连谷歌的观点也发生了变化。
- en: Open-source models are faster, more customizable, more private, and pound-for-pound
    more capable. They are doing things with $100 and 13B params that we struggle
    with at $10M and 540B. And they are doing so in weeks, not months.
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 开源模型更快、更可定制、更私密，而且在性价比方面更具优势。他们用 $100 和 13B 的参数做到了我们在 $10M 和 540B 的参数下都难以做到的事情。而且他们是在几周内完成的，而不是几个月。
- en: From the leaked document titled *We have no moat, and neither does OpenAI* [6]*,*
    where they acknowledge the incredible evolution of open source models, that are
    quickly catching up using smaller and cheaper models.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从泄露的文档*我们没有护城河，OpenAI也没有* [6]*，可以看出他们承认开源模型的惊人演变，这些模型通过使用更小、更便宜的模型迅速赶上了。
- en: '![](../Images/99f1fc4b148f9e351c0b0ca75c36b518.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99f1fc4b148f9e351c0b0ca75c36b518.png)'
- en: ChatGPT took it well.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT对此反应良好。
- en: Thanks to the outstanding work of the open source community in the last year,
    there are now accessible and free alternatives. Less than a year after Google’s
    PaLM, **LLaMA** was released; In the paper, the authors claim that their biggest
    model, **LLaMA 65B**, surpasses GPT-3 (175B) and PaLM (540B) on many tasks [5].
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 多亏了过去一年开源社区的杰出工作，现在有了可用且免费的替代品。在谷歌的PaLM发布不到一年后，**LLaMA**也发布了；在论文中，作者声称他们最大的模型**LLaMA
    65B**在许多任务上超越了GPT-3（175B）和PaLM（540B）[5]。
- en: Fine-tuning Large Language Models
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调大型语言模型
- en: LLMs have earned their reputation for their versatility in handling many language
    tasks within a single framework. Yet, your specific application might require
    the model to excel in a single task. To achieve this, you can fine-tune a pre-trained
    model using a dataset specific to your task — say, text summarization. The fascinating
    part is that good results are attainable even with a small dataset. You might
    need just 500 to 1,000 examples to improve performance significantly despite the
    model initially being trained on billions of pieces of text.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs因其在单一框架内处理多种语言任务的多功能性而赢得了声誉。然而，你的特定应用可能要求模型在单一任务上表现出色。为此，你可以使用针对你的任务的数据集（例如文本摘要）来微调一个预训练的模型。令人着迷的是，即使数据集较小，也能取得良好的结果。尽管模型最初是用数十亿个文本片段进行训练的，你可能只需要500到1000个示例就能显著提高性能。
- en: '![](../Images/1334709edb870c678d16360f80f24d4f.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1334709edb870c678d16360f80f24d4f.png)'
- en: A sample from the [Alpaca Dataset.](https://github.com/gururise/AlpacaDataCleaned)
    The model is fine-tuned to follow an instruction given by the user.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一个来自[Alpaca数据集](https://github.com/gururise/AlpacaDataCleaned)的示例。该模型经过微调，以跟随用户给出的指令。
- en: A popular technique is *instruction fine-tuning*. This method involves training
    the model using examples that illustrate how it should respond to a particular
    instruction. The outcome of this process is an instruct model — an enhanced version
    of the base model that excels at following instructions, rather than just completing
    text. Examples of instruct models are **Alpaca** and **Vicuna**.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一种流行的技术是*指令微调*。这种方法涉及使用示例来训练模型，说明它应该如何响应特定的指令。这一过程的结果是一个指令模型——这是基模型的增强版本，在遵循指令方面表现出色，而不仅仅是完成文本。指令模型的例子包括**Alpaca**和**Vicuna**。
- en: The Best Open Source Large Language Models
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳开源大型语言模型
- en: In February 2023, Meta’s **LLaMA** model hit the open-source market in various
    sizes, including 7B, 13B, 33B, and 65B. Initially, the model was only available
    to researchers under a non-commercial license, but in less than a week its weights
    were leaked. This event sparked a revolution in the open-source LLMs world as
    its training code was freely accessible under GPL 3 license. Consequently, several
    powerful fine-tuned variations have been released.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年2月，Meta的**LLaMA**模型以不同规模进入开源市场，包括7B、13B、33B和65B。最初，该模型仅对研究人员开放，使用的是非商业许可证，但不到一周时间，它的权重就被泄露了。这一事件引发了开源大型语言模型（LLMs）领域的革命，因为其训练代码在GPL
    3许可证下自由获取。因此，已经发布了几种强大的微调变体。
- en: The first was **Alpaca**, released by Stanford University. This model utilized
    GPT’s generated instructions for its fine-tuning on 52K examples. It was soon
    followed by **Vicuna**, which surprisingly outperformed Alpaca by measuring a
    whopping 90% ChatGPT quality on many tasks. Its distinguishing feature is that
    it was fine-tuned on ShareGPT data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个是**Alpaca**，由斯坦福大学发布。该模型利用GPT生成的指令进行了52K示例的微调。紧随其后的是**Vicuna**，令人惊讶的是，它在许多任务中超越了Alpaca，达到了90%的ChatGPT质量。其显著特点是它是在ShareGPT数据上进行微调的。
- en: Adding to these powerful models is **GPT4All** — inspired by its vision to make
    LLMs easily accessible, it features a range of consumer CPU-friendly models along
    with an interactive GUI application.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些强大模型的基础上，新增了**GPT4All**——它受到使LLMs易于访问的愿景的启发，提供了一系列对消费者CPU友好的模型，并附带了一个互动GUI应用程序。
- en: '**WizardLM** also joined these remarkable LLaMa-based models. Through a new
    and unique method named Evol-Instruct, it underwent fine-tuning on complex instruction
    data and has shown similar performance to ChatGPT at an average of 97.8%.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**WizardLM** 也加入了这些杰出的基于 LLaMa 的模型。通过一种名为 Evol-Instruct 的新颖独特的方法，它在复杂指令数据上进行了微调，并显示出与
    ChatGPT 相似的表现，平均达到了 97.8%。'
- en: Not all recent models are based on LLaMA, though. Models like **MPT** are known
    for achieving impressive context lengths with their variants capable of generating
    up to a staggering amount of 65k context length — a whole book at once!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，并不是所有最近的模型都基于 LLaMA。像 **MPT** 这样的模型以其变体能够生成多达惊人的 65k 上下文长度而闻名——一次生成整本书！
- en: '**Falcon** also joins this bandwagon in both 7B and 40B variants. Surprisingly
    it outperforms LLaMA on the OpenLLM leaderboard due to its high-quality training
    dataset called RefinedWeb.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**Falcon** 也加入了这个趋势，提供了 7B 和 40B 两种变体。令人惊讶的是，它在 OpenLLM 排行榜上超越了 LLaMA，这要归功于其高质量的训练数据集
    RefinedWeb。'
- en: However, Falcon’s reign on top of HuggingFace’s OpenLLM leaderboard was short-lived.
    In July 2023, Meta unveiled the successor to their famous model, **LLaMa 2**.
    This next-generation model doubled its predecessor’s token limit and increased
    its context length to 4K tokens. Simultaneously featuring **llama-2-chat** — an
    optimized version for dialogue applications — it made a significant impact. At
    the time of writing, **fine-tuned versions of LLaMa 2 such as StableBeluga2, Airoboros
    and Guanaco** are the most powerful open source large language models, dominating
    the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Falcon 在 HuggingFace 的 OpenLLM 排行榜上的统治地位并没有持续太久。2023 年 7 月，Meta 揭开了其著名模型的继任者
    **LLaMa 2** 的面纱。这个下一代模型将其前身的令牌限制翻了一番，并将其上下文长度增加到 4K 令牌。同时，**llama-2-chat**——一个针对对话应用优化的版本——产生了重大影响。撰写本文时，**像
    StableBeluga2、Airoboros 和 Guanaco 这样的 LLaMa 2 微调版本** 是最强大的开源大语言模型，主导了 [OpenLLM
    Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)。
- en: '![](../Images/bd9a5f03805a71f719c3309616366dbd.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd9a5f03805a71f719c3309616366dbd.png)'
- en: The top 10 models on the [OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
    are mostly LLaMa 2 based.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[OpenLLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)
    上的前 10 名模型大多基于 LLaMa 2。'
- en: If you’re curious about a model and you want to try it, the easiest way is probably
    going to an HuggingFace model page, and opening a Space using it. They are simple
    Gradio interfaces that allow you to send an input to a model and receive back
    an output. Since the models run on their servers and there are many requests,
    often you’ll have to wait in queue for a while. Nonetheless, it’s not a big deal
    to wait a couple of minutes sometimes, given how great their service is.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对某个模型感到好奇并且想尝试，最简单的方法可能是访问 HuggingFace 模型页面，然后打开一个使用该模型的 Space。它们是简单的 Gradio
    界面，允许你向模型发送输入并接收输出。由于模型运行在他们的服务器上并且请求量很大，你通常需要排队等待一段时间。不过，鉴于他们的服务质量，等待几分钟也不是什么大问题。
- en: '![](../Images/9ba02b0c371638a0c1bc44d625997a1f.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9ba02b0c371638a0c1bc44d625997a1f.png)'
- en: HuggingFace Spaces makes it very easy to try language models. Image by author.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: HuggingFace Spaces 使尝试语言模型变得非常简单。作者提供的图片。
- en: '**Hardware requirements and optimizations**'
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**硬件要求和优化**'
- en: Last year, you needed a high-end GPU to run a LLM locally. Even small ones required
    many gigabytes of memory, and you had to load them on your graphics card memory.
    The situation changed with the release of [**llama.cpp**](https://github.com/ggerganov/llama.cpp)**.**
    Initially a port of Facebook’s LLaMA model in C/C++, it now supports many other
    models. It allows to convert an LLM from PyTorch to **GGML**, their new format
    that allows fast inference on CPU. Being compiled in C++, the inference is multithreaded.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 去年，你需要一个高端 GPU 才能在本地运行 LLM。即使是小型模型也需要大量内存，并且你必须将它们加载到显卡内存中。情况随着 [**llama.cpp**](https://github.com/ggerganov/llama.cpp)**
    的发布发生了变化**。最初是 Facebook 的 LLaMA 模型在 C/C++ 中的移植，现在支持许多其他模型。它允许将 LLM 从 PyTorch 转换为
    **GGML**，他们的新格式，允许在 CPU 上进行快速推理。由于用 C++ 编译，推理是多线程的。
- en: '*Thanks to this amazing work, it’s now possible to run many LLMs on your desktop
    computer, or even on a MacBook.*'
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*得益于这一惊人的工作，现在可以在你的桌面电脑上，甚至在 MacBook 上运行许多 LLM。*'
- en: The main limitation is the memory they consume. For example, a 7B model weights
    around 14GB. A 65B model would require around 130GB of memory (RAM and disk space),
    which is more than most of us have on our computers. Fortunately, there is a way
    to compress them.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的限制是它们所消耗的内存。例如，一个7B模型的权重大约为14GB。一个65B的模型将需要大约130GB的内存（RAM和磁盘空间），这超过了我们大多数计算机的容量。幸运的是，有一种压缩它们的方法。
- en: Enter **Quantization.**
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 进入**量化**。
- en: In every machine learning model, a parameter is a number. This number is usually
    represented as a float32, a 32bits (4 bytes) representation.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个机器学习模型中，参数是一个数字。这个数字通常表示为float32，即32位（4字节）表示。
- en: Since every parameter takes two bytes of space, a model with 65B parameters
    would take approximately **4*65*10⁹ = 260GB** of memory.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个参数占用两个字节的空间，一个具有65B参数的模型将占用大约**4*65*10⁹ = 260GB**的内存。
- en: Model quantization refers to the idea of reducing the model weight by representing
    its parameters with a lower precision number, using rounding. For mathematical
    details about quantization, there is a great article on HuggingFace’s blog. [7]
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 模型量化指的是通过使用舍入将模型参数表示为较低精度的数字，从而减少模型权重的想法。关于量化的数学细节，HuggingFace博客上有一篇很棒的文章。[7]
- en: '![](../Images/cf72f9eb4f926c1a71b27b04bed85faf.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf72f9eb4f926c1a71b27b04bed85faf.png)'
- en: 8-bit integer quantization. [7]
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 8位整数量化。[7]
- en: Being a compression process, quantization involves some loss in performance.
    With the recent introduction of **LLM.int8()** and newer techniques, this loss
    has been greatly reduced, making it a must for LLMs.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种压缩过程，量化会导致一定的性能损失。随着**LLM.int8()**及新技术的引入，这种损失已被大大减少，使得它成为LLMs的必备技术。
- en: '**Llama.cpp** supports up to 4-bit integer quantization. Using **Q4_0**, one
    of the available options, it’s possible to **reduce the model size up to 4 times.**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**Llama.cpp**支持高达4位整数量化。使用**Q4_0**，可以**将模型大小减少最多4倍**。'
- en: '![](../Images/fa79944cc873862e975dacbd39970ada.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa79944cc873862e975dacbd39970ada.png)'
- en: Quantization effect on model perplexity and file size. Image by the author.
    Data from llama.cpp GitHub repo.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 量化对模型困惑度和文件大小的影响。图片由作者提供。数据来自llama.cpp GitHub仓库。
- en: '**Q4_0** reduced file size **from 25.8 to 6.8 GB**, while reducing perplexity
    by approximately 2%. It’s possible to load a 13B model on a computer with 16GB
    of RAM, while if you have 64GB, you can run even the biggest 70B models.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**Q4_0**将文件大小**从25.8GB减少到6.8GB**，同时将困惑度减少了约2%。在16GB内存的计算机上可以加载一个13B模型，而如果你有64GB内存，你甚至可以运行最大的70B模型。'
- en: '![](../Images/db20407583125c5b5d5b0f52e656f616.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/db20407583125c5b5d5b0f52e656f616.png)'
- en: LLaMa 2 models with different quantization techniques. [TheBloke](https://huggingface.co/TheBloke)’s
    HuggingFace profile has many pre-trained models.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 具有不同量化技术的LLaMa 2模型。[TheBloke](https://huggingface.co/TheBloke)的HuggingFace个人资料上有许多预训练模型。
- en: Another quantization technique worth mentioning is **GPTQ,** that can reduce
    VRAM usage up to 75% while preserving accuracy. [8] Released in March 2023, it
    made possible for the first time to run a 175B model on a single GPU. Talking
    about consumer hardware, you can run up to a 30B model with a single RTX 3090
    card.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个值得一提的量化技术是**GPTQ**，它可以将VRAM使用量减少多达75%，同时保持准确性。[8] 它于2023年3月发布，使得第一次可以在单个GPU上运行175B模型。谈到消费级硬件，你可以用单张RTX
    3090显卡运行最多30B的模型。
- en: Running a Large Language Model on your computer
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在你的计算机上运行大型语言模型
- en: The most similar experience to ChatGPT is the **GPT4All** app, a chat interface
    that allows you to chat with your favorite model. It’s not limited to GPT4All
    models but supports many of the most popular ones. The app runs on Windows, Mac
    OS, and Linux and it’s completely open source.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 与ChatGPT最相似的体验是**GPT4All**应用程序，它是一个聊天界面，允许你与最喜欢的模型聊天。它不仅限于GPT4All模型，还支持许多最流行的模型。该应用程序可以在Windows、Mac
    OS和Linux上运行，并且完全开源。
- en: '![](../Images/37d1240ec81d290f33f8c83c375518cf.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/37d1240ec81d290f33f8c83c375518cf.png)'
- en: LLaMA-2 7B inference on a Ryzen 5600 CPU. Image by the author.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在Ryzen 5600 CPU上进行LLaMA-2 7B推理。图片由作者提供。
- en: 'Alternatively, you can use a GUI tool like [**text-generation-webui**](https://github.com/oobabooga/text-generation-webui)
    or [**openplayground**](https://github.com/nat/openplayground)**.** While they
    both offer a graphical interface that can be easily used to generate text, the
    first is probably the most complete tool available: it offers many features such
    as chat, training, support for GPU and HTML/Markdown output, and more. The second
    is very similar to OpenAI’s Playground, and it’s a great tool for testing and
    comparing LLMs quickly with different parameters.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，你可以使用像 [**text-generation-webui**](https://github.com/oobabooga/text-generation-webui)
    或 [**openplayground**](https://github.com/nat/openplayground)** 这样的 GUI 工具。**虽然它们都提供了一个可以轻松生成文本的图形界面，但第一个可能是最完整的工具：它提供了许多功能，如聊天、训练、GPU
    支持和 HTML/Markdown 输出等。第二个与 OpenAI 的 Playground 非常相似，是一个很好的工具，可以快速测试和比较不同参数的 LLM。
- en: '![](../Images/3f42c748cf8dff9cd9146f7458816a99.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f42c748cf8dff9cd9146f7458816a99.png)'
- en: Text generation web UI is an advanced web interface built with Gradio.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 文本生成网页 UI 是一个使用 Gradio 构建的高级网页界面。
- en: 'If you want to use your LLMs in Python there are several options, such as [**llama-cpp-python**](https://github.com/abetlen/llama-cpp-python),
    or the **HuggingFace Transformers** library, which offers an high-level syntax
    to interact with any HF model. It supports PyTorch, Tensorflow, and Jax backend
    and it can be used with any pre-trained model you find on HF, for text, images
    or audio. Using transformers, generating text with your favorite LLM it’s as simple
    as writing two lines of code:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在 Python 中使用 LLM，有几个选项，比如 [**llama-cpp-python**](https://github.com/abetlen/llama-cpp-python)
    或 **HuggingFace Transformers** 库，它提供了一种与任何 HF 模型交互的高级语法。它支持 PyTorch、Tensorflow
    和 Jax 后端，可以与 HF 上找到的任何预训练模型一起使用，适用于文本、图像或音频。使用 transformers，与你喜欢的 LLM 生成文本就像编写两行代码一样简单：
- en: '[PRE0]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Among the thousands freely available models on the HF hub, there are different
    formats. The *Transformers* library require a model in **HF** format and *llama.cpp*
    requires a **GGML** model.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在 HF hub 上成千上万的免费模型中，有不同的格式。*Transformers* 库需要 **HF** 格式的模型，而 *llama.cpp* 需要
    **GGML** 模型。
- en: Limitations
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: While we have shown that you don’t need powerful computers for most models,
    **scalability** is the first concern if you are planning to build a system that
    allows hundreds or thousands of users to interact with your model. An advantage
    of using GPT is that OpenAI offers cheap, high rate-limit APIs that can scale
    your app easily.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已经展示了大多数模型不需要强大的计算机，**可扩展性**仍然是你计划构建允许数百或数千名用户与模型互动的系统时首要考虑的问题。使用 GPT 的一个优势是
    OpenAI 提供了便宜的、高速限制的 API，可以轻松地扩展你的应用。
- en: Another limitation is **safety and moderation**. Since you can be held responsible
    for the model’s output, you must be extra careful of what the model is generating.
    Commercial LLMs have advanced moderation filters built using Reinforcement Learning
    with Human Feedback (RLHF), for limiting harmful content.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个限制是 **安全性和管理**。由于你可能需要对模型的输出负责，你必须特别小心模型生成的内容。商业 LLM 具有使用强化学习与人类反馈 (RLHF)
    构建的先进审查过滤器，用于限制有害内容。
- en: Remember to check the **model’s license** before using it. *Open Source* doesn’t
    always mean that the model can be used commercially.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前记得检查 **模型的许可证**。*开源* 并不总是意味着模型可以用于商业用途。
- en: Conclusion
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The article has shown that open source models are viable and free alternatives
    that are getting better quickly. This year has witnessed incredible advancements
    in the development of these models, and it is now even possible to run them on
    your laptop.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章展示了开源模型是快速改进的可行和免费的替代方案。今年这些模型的发展取得了令人难以置信的进展，现在甚至可以在你的笔记本电脑上运行它们。
- en: I hope you found this article useful if you are considering an open source LLM
    for your next project. The following articles in the series will go more in-depth
    on *Open Language Models* different aspects and challenges.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在考虑为下一个项目使用开源 LLM，希望你觉得这篇文章对你有用。系列中的后续文章将深入探讨 *开源语言模型* 的不同方面和挑战。
- en: See you in the next one!
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 下次见！
- en: '*If you enjoyed this article, join* [***Text Generation***](https://textgeneration.substack.com/)
    *— our newsletter has two weekly posts with the latest insights on Generative
    AI and Large Language Models.*'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你喜欢这篇文章，加入* [***Text Generation***](https://textgeneration.substack.com/)
    *——我们的新闻通讯每周发布两篇文章，提供有关生成 AI 和大型语言模型的最新见解。*'
- en: '*Also, you can find me on* [***LinkedIn***](https://www.linkedin.com/in/driccio/)***.***'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*此外，你还可以在* [***LinkedIn***](https://www.linkedin.com/in/driccio/)***找到我。***'
- en: References
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考资料
- en: '[1] [Stanford Scientists Find That Yes, ChatGPT Is Getting Stupider](https://futurism.com/the-byte/stanford-chatgpt-getting-dumber)
    (2023), (futurism.com)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [斯坦福科学家发现，ChatGPT 确实变得更愚蠢了](https://futurism.com/the-byte/stanford-chatgpt-getting-dumber)
    (2023), (futurism.com)'
- en: '[2] T. Brown at al, [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
    (2020), arXiv.org'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] T. Brown 等人，[语言模型是少样本学习者](https://arxiv.org/abs/2005.14165) (2020), arXiv.org'
- en: '[3] M. Schade, [How your data is used to improve model performance](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance)
    (2023), OpenAI Help Center'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] M. Schade，[您的数据如何用于提高模型性能](https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance)
    (2023), OpenAI 帮助中心'
- en: '[4] Google AI Blog, [Pathways Language Model (PaLM): Scaling to 540 Billion
    Parameters for Breakthrough Performance](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html)
    (2022), ai.googleblog.com'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Google AI 博客，[Pathways 语言模型 (PaLM): 扩展到 5400 亿参数以实现突破性性能](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html)
    (2022), ai.googleblog.com'
- en: '[5] A. Fan et al., [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)
    (2023), arXiv.org'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] A. Fan 等人，[LLaMA: 开放且高效的基础语言模型](https://arxiv.org/abs/2302.13971) (2023),
    arXiv.org'
- en: '[6] D. Patel and A. Ahmad, [Google: We Have No Moat (And Neither Does OpenAI)](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither)
    (2023), semianalysis.com'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] D. Patel 和 A. Ahmad，[Google: 我们没有护城河（OpenAI 也没有）](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither)
    (2023), semianalysis.com'
- en: '[7] Y. Belkada and T. Dettmers, [A Gentle Introduction to 8-bit Matrix Multiplication
    for transformers at scale using Hugging Face Transformers, Accelerate and bitsandbytes](https://huggingface.co/blog/hf-bitsandbytes-integration)
    (2022), Hugging Face Blog'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Y. Belkada 和 T. Dettmers，[使用 Hugging Face Transformers、Accelerate 和 bitsandbytes
    对变换器进行大规模 8 位矩阵乘法的温和介绍](https://huggingface.co/blog/hf-bitsandbytes-integration)
    (2022), Hugging Face 博客'
- en: '[8] E. Frantar et al., [GPTQ: Accurate Post-Training Quantization for Generative
    Pre-trained Transformers](https://arxiv.org/abs/2210.17323) (2023), arXiv:2210.17323v2
    [cs.LG]'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] E. Frantar 等人，[GPTQ: 生成预训练变换器的准确后训练量化](https://arxiv.org/abs/2210.17323)
    (2023), arXiv:2210.17323v2 [cs.LG]'
