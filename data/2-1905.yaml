- en: 'Stable Diffusion as an API: Make a Person-Removing Microservice'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Stable Diffusionä½œä¸ºAPIï¼šåˆ›å»ºä¸€ä¸ªå»é™¤äººç‰©çš„å¾®æœåŠ¡
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/stable-diffusion-as-an-api-5e381aec1f6](https://towardsdatascience.com/stable-diffusion-as-an-api-5e381aec1f6)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/stable-diffusion-as-an-api-5e381aec1f6](https://towardsdatascience.com/stable-diffusion-as-an-api-5e381aec1f6)
- en: '![](../Images/a964e08268efee5aaf0f29c6237487db.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a964e08268efee5aaf0f29c6237487db.png)'
- en: Landscape image produced using Stable Diffusion 2 (by author).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Stable Diffusion 2ç”Ÿæˆçš„é£æ™¯å›¾åƒï¼ˆä½œè€…æä¾›ï¼‰ã€‚
- en: Remove people from photos with a Stable Diffusion microservice
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Stable Diffusionå¾®æœåŠ¡ä»ç…§ç‰‡ä¸­å»é™¤äººç‰©
- en: '[](https://medium.com/@masonmcgough?source=post_page-----5e381aec1f6--------------------------------)[![Mason
    McGough](../Images/4b465e0eef1590b1f12dea23a6f688e1.png)](https://medium.com/@masonmcgough?source=post_page-----5e381aec1f6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5e381aec1f6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5e381aec1f6--------------------------------)
    [Mason McGough](https://medium.com/@masonmcgough?source=post_page-----5e381aec1f6--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@masonmcgough?source=post_page-----5e381aec1f6--------------------------------)[![Mason
    McGough](../Images/4b465e0eef1590b1f12dea23a6f688e1.png)](https://medium.com/@masonmcgough?source=post_page-----5e381aec1f6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5e381aec1f6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5e381aec1f6--------------------------------)
    [Mason McGough](https://medium.com/@masonmcgough?source=post_page-----5e381aec1f6--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5e381aec1f6--------------------------------)
    Â·12 min readÂ·Feb 4, 2023
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5e381aec1f6--------------------------------)
    Â·é˜…è¯»æ—¶é—´12åˆ†é’ŸÂ·2023å¹´2æœˆ4æ—¥
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Overview
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¦‚è¿°
- en: Stable Diffusion is a cutting-edge open-source tool for generating images from
    text. The Stable Diffusion Web UI opens up many of these features with an API
    as well as the interactive UI. We will first introduce how to use this API, then
    set up an example using it as a privacy-preserving microservice to remove people
    from images.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Stable Diffusion æ˜¯ä¸€ä¸ªå‰æ²¿çš„å¼€æºå·¥å…·ï¼Œç”¨äºä»æ–‡æœ¬ç”Ÿæˆå›¾åƒã€‚Stable Diffusion Web UI é€šè¿‡APIä»¥åŠäº¤äº’å¼ç”¨æˆ·ç•Œé¢æä¾›äº†è®¸å¤šè¿™äº›åŠŸèƒ½ã€‚æˆ‘ä»¬å°†é¦–å…ˆä»‹ç»å¦‚ä½•ä½¿ç”¨è¿™ä¸ªAPIï¼Œç„¶åè®¾ç½®ä¸€ä¸ªç¤ºä¾‹ï¼Œå°†å…¶ä½œä¸ºä¸€ä¸ªéšç§ä¿æŠ¤å¾®æœåŠ¡æ¥ä»å›¾åƒä¸­å»é™¤äººç‰©ã€‚
- en: Introduction to Generative AI
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç”ŸæˆAIä»‹ç»
- en: So many innovations in machine learning-based data generators happened last
    year you might be able to call 2022 the â€œYear of Generative AI.â€ We had [DALL-E
    2](https://openai.com/dall-e-2/), the text-to-image generation model from OpenAI
    that produced strikingly realistic images of astronauts riding horses and dogs
    wearing people clothes. [GitHub Copilot](https://github.blog/2022-06-21-github-copilot-is-generally-available-to-all-developers/),
    the powerful code completion tool that will autocomplete statements, write documentation,
    and implement entire functions for you from a single comment, was released to
    the public as a subscription service. We had [Dream Fields](https://www.ajayjain.net/dreamfields/),
    [Dream Fusion](https://dreamfusion3d.github.io/), and [Magic3D](https://research.nvidia.com/labs/dir/magic3d/),
    a series of groundbreaking models capable of producing textured 3D models from
    text alone. Last but certainly not least we had [ChatGPT](https://openai.com/blog/chatgpt/),
    the cutting-edge AI chatbot which these days needs no introduction.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å»å¹´ï¼ŒåŸºäºæœºå™¨å­¦ä¹ çš„æ•°æ®ç”Ÿæˆå™¨å‡ºç°äº†è®¸å¤šåˆ›æ–°ï¼Œ2022å¹´æˆ–è®¸å¯ä»¥ç§°ä¸ºâ€œç”ŸæˆAIçš„å¹´åº¦â€ã€‚æˆ‘ä»¬è¿æ¥äº† [DALL-E 2](https://openai.com/dall-e-2/)ï¼Œè¿™æ˜¯OpenAIçš„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œç”Ÿæˆäº†ä»¤äººæƒŠå¹çš„ç°å®ä¸»ä¹‰å›¾åƒï¼Œå¦‚å®‡èˆªå‘˜éª‘é©¬å’Œç©¿ç€äººç±»è¡£ç‰©çš„ç‹—ã€‚
    [GitHub Copilot](https://github.blog/2022-06-21-github-copilot-is-generally-available-to-all-developers/)ï¼Œè¿™ä¸ªå¼ºå¤§çš„ä»£ç è¡¥å…¨å·¥å…·å¯ä»¥è‡ªåŠ¨å®Œæˆè¯­å¥ã€ç¼–å†™æ–‡æ¡£ï¼Œå¹¶æ ¹æ®å•ä¸ªè¯„è®ºå®ç°å®Œæ•´çš„åŠŸèƒ½ï¼Œå·²ç»ä½œä¸ºè®¢é˜…æœåŠ¡å…¬å¼€å‘å¸ƒã€‚æˆ‘ä»¬è¿˜çœ‹åˆ°äº†
    [Dream Fields](https://www.ajayjain.net/dreamfields/)ã€[Dream Fusion](https://dreamfusion3d.github.io/)
    å’Œ [Magic3D](https://research.nvidia.com/labs/dir/magic3d/)ï¼Œä¸€ç³»åˆ—èƒ½å¤Ÿä»…é€šè¿‡æ–‡æœ¬ç”Ÿæˆçº¹ç†åŒ–3Dæ¨¡å‹çš„çªç ´æ€§æ¨¡å‹ã€‚æœ€åä½†åŒæ ·é‡è¦çš„æ˜¯æˆ‘ä»¬è¿æ¥äº†
    [ChatGPT](https://openai.com/blog/chatgpt/)ï¼Œè¿™ä¸ªå‰æ²¿çš„AIèŠå¤©æœºå™¨äººå¦‚ä»Šæ— éœ€ä»‹ç»ã€‚
- en: This list barely even scratches the surface. In just the world of generative
    image models like DALL-E 2 we also have [Midjourney](https://www.midjourney.com/),
    [Google Imagen](https://imagen.research.google/), [StarryAI](https://starryai.com/),
    [WOMBO Dream](https://dream.ai/create), [NightCafe](https://nightcafe.studio/),
    [InvokeAI](https://invoke-ai.github.io/InvokeAI/), [Lexica Aperture](https://lexica.art/aperture),
    [Dream Studio](https://beta.dreamstudio.ai/), [Deforum](https://deforum.github.io/)â€¦
    I think you get the *picture*. ğŸ˜‰ ğŸ“· It seems like no exaggeration to say that generative
    AI has captured the imaginations of the whole world.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªåˆ—è¡¨å‡ ä¹åªæ˜¯è§¦åŠäº†è¡¨é¢ã€‚åœ¨åƒDALL-E 2è¿™æ ·çš„ç”Ÿæˆå›¾åƒæ¨¡å‹ä¸–ç•Œä¸­ï¼Œæˆ‘ä»¬è¿˜æœ‰[Midjourney](https://www.midjourney.com/)ã€[Google
    Imagen](https://imagen.research.google/)ã€[StarryAI](https://starryai.com/)ã€[WOMBO
    Dream](https://dream.ai/create)ã€[NightCafe](https://nightcafe.studio/)ã€[InvokeAI](https://invoke-ai.github.io/InvokeAI/)ã€[Lexica
    Aperture](https://lexica.art/aperture)ã€[Dream Studio](https://beta.dreamstudio.ai/)ã€[Deforum](https://deforum.github.io/)â€¦â€¦æˆ‘æƒ³ä½ å·²ç»æ˜ç™½äº†*ç”»é¢*ã€‚ğŸ˜‰
    ğŸ“· è¯´ç”ŸæˆAIå·²ç»æ•æ‰äº†æ•´ä¸ªä¸–ç•Œçš„æƒ³è±¡åŠ›ï¼Œä¼¼ä¹å¹¶ä¸å¤¸å¼ ã€‚
- en: Stable Diffusion
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¨³å®šæ‰©æ•£
- en: While many of the popular generative AI tools like ChatGPT, GitHub Copilot,
    and DALL-E 2 are proprietary and paywalled, the open-source community has not
    skipped a beat. Last year, LMU Munich, Runway, and Stability AI collaborated to
    publicly share [Stable Diffusion](https://github.com/CompVis/stable-diffusion),
    a powerful yet efficient text-to-image model efficient enough to run on consumer
    hardware. This means that anyone with a decent GPU and an internet connection
    can download the Stable Diffusion code and model weights, bringing low-cost image
    generation to the world.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡è®¸å¤šæµè¡Œçš„ç”ŸæˆAIå·¥å…·å¦‚ChatGPTã€GitHub Copilotå’ŒDALL-E 2æ˜¯ä¸“æœ‰çš„ä¸”éœ€ä»˜è´¹çš„ï¼Œä½†å¼€æºç¤¾åŒºå¹¶æ²¡æœ‰åœä¸‹è„šæ­¥ã€‚å»å¹´ï¼ŒLMUæ…•å°¼é»‘å¤§å­¦ã€Runwayå’ŒStability
    AIåˆä½œå…¬å¼€åˆ†äº«äº†[Stable Diffusion](https://github.com/CompVis/stable-diffusion)ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼ºå¤§ä½†é«˜æ•ˆçš„æ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ï¼Œè¶³å¤Ÿé«˜æ•ˆä»¥åœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šè¿è¡Œã€‚è¿™æ„å‘³ç€ä»»ä½•æ‹¥æœ‰ä¸€å°ä¸é”™GPUå’Œäº’è”ç½‘è¿æ¥çš„äººéƒ½å¯ä»¥ä¸‹è½½Stable
    Diffusionä»£ç å’Œæ¨¡å‹æƒé‡ï¼Œå°†ä½æˆæœ¬å›¾åƒç”Ÿæˆå¸¦ç»™ä¸–ç•Œã€‚
- en: Stable Diffusion Web UI
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¨³å®šæ‰©æ•£ç½‘ç»œç•Œé¢
- en: The [Stable Diffusion Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui),
    one of the most popular tools leveraging Stable Diffusion, exposes a wide range
    of the settings and features of Stable Diffusion in an interactive browser-based
    user interface. A lesser-known feature of this project is that you can use it
    as an HTTP API, allowing you to request images from your own applications.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[Stable Diffusion Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui)æ˜¯åˆ©ç”¨Stable
    Diffusionçš„æœ€å—æ¬¢è¿å·¥å…·ä¹‹ä¸€ï¼Œå®ƒåœ¨åŸºäºæµè§ˆå™¨çš„ç”¨æˆ·ç•Œé¢ä¸­æš´éœ²äº†Stable Diffusionçš„å¹¿æ³›è®¾ç½®å’ŒåŠŸèƒ½ã€‚è¯¥é¡¹ç›®ä¸€ä¸ªé²œä¸ºäººçŸ¥çš„åŠŸèƒ½æ˜¯ï¼Œä½ å¯ä»¥å°†å…¶ä½œä¸ºHTTP
    APIä½¿ç”¨ï¼Œå…è®¸ä½ ä»è‡ªå·±çš„åº”ç”¨ç¨‹åºä¸­è¯·æ±‚å›¾åƒã€‚'
- en: '![](../Images/b361ece7b8424e89b5c7e22e9230c880.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b361ece7b8424e89b5c7e22e9230c880.png)'
- en: The Stable Diffusion Web UI with an example generation (photo by author).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Stable Diffusion Web UIçš„ç¤ºä¾‹ç”Ÿæˆï¼ˆä½œè€…æä¾›çš„ç…§ç‰‡ï¼‰ã€‚
- en: It has a metric truckload of features, such as inpainting, outpainting, resizing,
    upscaling, variations, and many more. The [project wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features)
    provides a great overview of all the features. In addition, it provides scripting
    for extensibility.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒå…·æœ‰å¤§é‡åŠŸèƒ½ï¼Œå¦‚ä¿®è¡¥ã€æ‰©å±•ã€è°ƒæ•´å¤§å°ã€æ”¾å¤§ã€å˜ä½“ç­‰ã€‚ [é¡¹ç›®wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features)
    æä¾›äº†æ‰€æœ‰åŠŸèƒ½çš„è¯¦ç»†æ¦‚è¿°ã€‚æ­¤å¤–ï¼Œå®ƒè¿˜æä¾›äº†æ‰©å±•æ€§è„šæœ¬ã€‚
- en: Setup
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®¾ç½®
- en: 'Before beginning, ensure that you have a GPU (NVIDIA preferably but AMD is
    also supported) with at least 8GB of VRAM to play with on your system. That will
    ensure that you can load the model into memory. Next, you want to clone the repo
    to your system (for instance via HTTPS):'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿ä½ çš„ç³»ç»Ÿä¸­æœ‰ä¸€ä¸ªGPUï¼ˆæœ€å¥½æ˜¯NVIDIAï¼Œä½†AMDä¹Ÿæ”¯æŒï¼‰ï¼Œä¸”è‡³å°‘æœ‰8GBçš„æ˜¾å­˜ã€‚è¿™å°†ç¡®ä¿ä½ å¯ä»¥å°†æ¨¡å‹åŠ è½½åˆ°å†…å­˜ä¸­ã€‚æ¥ä¸‹æ¥ï¼Œä½ éœ€è¦å°†ä»£ç åº“å…‹éš†åˆ°ä½ çš„ç³»ç»Ÿä¸­ï¼ˆä¾‹å¦‚é€šè¿‡HTTPSï¼‰ï¼š
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Follow the [installation instructions](https://github.com/AUTOMATIC1111/stable-diffusion-webui#installation-and-running)
    for your system as they may be different from mine. I used an install of Ubuntu
    18.04 to set this up, but it should also work on Windows and Apple Silicon. These
    instructions will include setting up a Python environment, so make sure that whichever
    environment you set up is active when you launch the server later.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¹æ®ä½ çš„ç³»ç»Ÿéµå¾ª[å®‰è£…è¯´æ˜](https://github.com/AUTOMATIC1111/stable-diffusion-webui#installation-and-running)ï¼Œå› ä¸ºè¿™äº›å¯èƒ½ä¸ä½ çš„æœ‰æ‰€ä¸åŒã€‚æˆ‘ä½¿ç”¨çš„æ˜¯Ubuntu
    18.04è¿›è¡Œè®¾ç½®ï¼Œä½†å®ƒä¹Ÿåº”è¯¥é€‚ç”¨äºWindowså’ŒApple Siliconã€‚è¿™äº›è¯´æ˜å°†åŒ…æ‹¬è®¾ç½®Pythonç¯å¢ƒï¼Œå› æ­¤è¯·ç¡®ä¿åœ¨ç¨åå¯åŠ¨æœåŠ¡å™¨æ—¶ï¼Œæ‰€è®¾ç½®çš„ç¯å¢ƒå¤„äºæ´»åŠ¨çŠ¶æ€ã€‚
- en: 'Once that is done, we need a copy of the model weights. I am using [Stable
    Diffusion 2.0](https://huggingface.co/stabilityai/stable-diffusion-2), but [Stable
    Diffusion 2.1](https://huggingface.co/stabilityai/stable-diffusion-2-1) is now
    available as well. Whichever option you pick, be sure to download the weights
    for the [stablediffusion](https://github.com/Stability-AI/stablediffusion) repository.
    Lastly, copy those weights to the `models/Stable-diffusion` folder like so:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæˆåï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä»½æ¨¡å‹æƒé‡çš„å‰¯æœ¬ã€‚æˆ‘ä½¿ç”¨çš„æ˜¯ [Stable Diffusion 2.0](https://huggingface.co/stabilityai/stable-diffusion-2)ï¼Œä½†ç°åœ¨ä¹Ÿæœ‰
    [Stable Diffusion 2.1](https://huggingface.co/stabilityai/stable-diffusion-2-1)
    å¯ç”¨ã€‚æ— è®ºä½ é€‰æ‹©å“ªä¸ªé€‰é¡¹ï¼Œç¡®ä¿ä¸‹è½½ [stablediffusion](https://github.com/Stability-AI/stablediffusion)
    ä»“åº“çš„æƒé‡ã€‚æœ€åï¼Œå°†è¿™äº›æƒé‡å¤åˆ¶åˆ° `models/Stable-diffusion` æ–‡ä»¶å¤¹ä¸­ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now you should be ready to start generating images! To launch the server, execute
    the following from the root directory (be sure that the environment you set up
    is activated):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ åº”è¯¥å‡†å¤‡å¥½å¼€å§‹ç”Ÿæˆå›¾åƒäº†ï¼è¦å¯åŠ¨æœåŠ¡å™¨ï¼Œè¯·åœ¨æ ¹ç›®å½•ä¸‹æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼ˆç¡®ä¿ä½ è®¾ç½®çš„ç¯å¢ƒå·²æ¿€æ´»ï¼‰ï¼š
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The server will take some time to get set up, as it likely needs to install
    requirements, load the model weights into memory, and check for embeddings, among
    other things. When it is ready, you should see a message in your terminal that
    looks like this:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æœåŠ¡å™¨éœ€è¦ä¸€äº›æ—¶é—´æ¥è®¾ç½®ï¼Œå› ä¸ºå®ƒå¯èƒ½éœ€è¦å®‰è£…è¦æ±‚ã€å°†æ¨¡å‹æƒé‡åŠ è½½åˆ°å†…å­˜ä¸­ã€æ£€æŸ¥åµŒå…¥ç­‰ã€‚å‡†å¤‡å¥½åï¼Œä½ åº”è¯¥ä¼šåœ¨ç»ˆç«¯ä¸­çœ‹åˆ°ç±»ä¼¼ä¸‹é¢çš„æ¶ˆæ¯ï¼š
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The UI is browser-based, so navigate to â€œ[127.0.0.1:7860](http://127.0.0.1:7860)"
    in your favorite web browser. If it is working, it should look something like
    this:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç”¨æˆ·ç•Œé¢æ˜¯åŸºäºæµè§ˆå™¨çš„ï¼Œå› æ­¤åœ¨ä½ å–œæ¬¢çš„ç½‘é¡µæµè§ˆå™¨ä¸­å¯¼èˆªåˆ° â€œ[127.0.0.1:7860](http://127.0.0.1:7860)â€ ã€‚å¦‚æœå®ƒæ­£å¸¸å·¥ä½œï¼Œå®ƒåº”è¯¥çœ‹èµ·æ¥åƒè¿™æ ·ï¼š
- en: '![](../Images/3ea02f9b2002eee7e0ce6eedd81f3629.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3ea02f9b2002eee7e0ce6eedd81f3629.png)'
- en: The Stable Diffusion Web UI when first opened (photo by author).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ç¨³å®šæ‰©æ•£ Web ç”¨æˆ·ç•Œé¢é¦–æ¬¡æ‰“å¼€æ—¶ï¼ˆä½œè€…æ‹æ‘„ï¼‰ã€‚
- en: Usage
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨
- en: You should now be ready to generate some images! Go ahead and generate something
    by entering text into the â€œPromptâ€ field and clicking â€œGenerate.â€ If this is your
    first time using this UI, take a second to explore and learn some of its features
    and settings. Refer to [the wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki)
    if you have any questions. This knowledge will come in handy later when designing
    your API.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ åº”è¯¥å‡†å¤‡å¥½ç”Ÿæˆä¸€äº›å›¾åƒäº†ï¼è¿›å…¥â€œæç¤ºâ€å­—æ®µä¸­è¾“å…¥æ–‡æœ¬å¹¶ç‚¹å‡»â€œç”Ÿæˆâ€ä»¥ç”Ÿæˆä¸€äº›å†…å®¹ã€‚å¦‚æœè¿™æ˜¯ä½ ç¬¬ä¸€æ¬¡ä½¿ç”¨è¿™ä¸ªç”¨æˆ·ç•Œé¢ï¼ŒèŠ±ç‚¹æ—¶é—´æ¢ç´¢å¹¶äº†è§£å…¶ä¸€äº›åŠŸèƒ½å’Œè®¾ç½®ã€‚å¦‚æœæœ‰ä»»ä½•é—®é¢˜ï¼Œè¯·å‚è€ƒ
    [ç»´åŸº](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki)ã€‚è¿™äº›çŸ¥è¯†åœ¨è®¾è®¡ä½ çš„ API
    æ—¶ä¼šå¾ˆæœ‰ç”¨ã€‚
- en: I will not delve too deep into how to use the web UI since many others before
    me have done so. However, I will provide the following cheat sheet of basic settings
    for reference.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¸ä¼šè¿‡äºæ·±å…¥åœ°è®²è§£å¦‚ä½•ä½¿ç”¨ç½‘é¡µç”¨æˆ·ç•Œé¢ï¼Œå› ä¸ºä¹‹å‰æœ‰å¾ˆå¤šäººå·²ç»åšäº†ã€‚ä¸è¿‡ï¼Œæˆ‘ä¼šæä¾›ä»¥ä¸‹åŸºæœ¬è®¾ç½®çš„å¤‡å¿˜å•ä»¥ä¾›å‚è€ƒã€‚
- en: '**Sampling method**: The sampling algorithm. This can greatly affect the content
    of the generated image and overall appearance. The execution time and the results
    can differ greatly between methods. Ideally experiment with this option first.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é‡‡æ ·æ–¹æ³•**ï¼šé‡‡æ ·ç®—æ³•ã€‚è¿™ä¼šæå¤§åœ°å½±å“ç”Ÿæˆå›¾åƒçš„å†…å®¹å’Œæ•´ä½“å¤–è§‚ã€‚ä¸åŒæ–¹æ³•çš„æ‰§è¡Œæ—¶é—´å’Œç»“æœå¯èƒ½ä¼šæœ‰å¾ˆå¤§å·®å¼‚ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œé¦–å…ˆå°è¯•è¿™ä¸ªé€‰é¡¹ã€‚'
- en: '**Sampling steps**: The number of denoising steps during the image generation
    process. Some results will change drastically with the number of steps whereas
    others will quickly lead to diminishing returns. A value of 20â€“50 is ideal for
    most samplers.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é‡‡æ ·æ­¥éª¤**ï¼šå›¾åƒç”Ÿæˆè¿‡ç¨‹ä¸­çš„å»å™ªæ­¥éª¤æ•°ã€‚æŸäº›ç»“æœä¼šéšç€æ­¥éª¤æ•°é‡çš„å˜åŒ–è€Œå‰§çƒˆæ”¹å˜ï¼Œè€Œå…¶ä»–ç»“æœåˆ™ä¼šå¾ˆå¿«å‡ºç°è¾¹é™…æ•ˆç›Šé€’å‡ã€‚å¤§å¤šæ•°é‡‡æ ·å™¨ç†æƒ³çš„æ­¥éª¤æ•°ä¸º
    20â€“50ã€‚'
- en: '**Width**, **Height**: The output image dimensions. For SD 2.0, 768x768 is
    the preferred resolution. The resolution can affect the generated content.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å®½åº¦**ï¼Œ**é«˜åº¦**ï¼šè¾“å‡ºå›¾åƒçš„å°ºå¯¸ã€‚å¯¹äº SD 2.0ï¼Œ768x768 æ˜¯é¦–é€‰åˆ†è¾¨ç‡ã€‚åˆ†è¾¨ç‡ä¼šå½±å“ç”Ÿæˆçš„å†…å®¹ã€‚'
- en: '**CFG scale**: The [Classifier-Free Guidance](https://arxiv.org/abs/2207.12598)
    (CFG) scale. Increasing this increases how much the image is impacted by the prompt.
    Lower values produce more creative results.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CFGæ¯”ä¾‹**ï¼š [æ— åˆ†ç±»å™¨å¼•å¯¼](https://arxiv.org/abs/2207.12598) (CFG) æ¯”ä¾‹ã€‚å¢åŠ è¿™ä¸ªæ¯”ä¾‹ä¼šå¢åŠ å›¾åƒå—åˆ°æç¤ºçš„å½±å“ã€‚è¾ƒä½çš„å€¼ä¼šäº§ç”Ÿæ›´å…·åˆ›æ„çš„ç»“æœã€‚'
- en: '**Denoising strength**: Determines how much variation on the original image
    to allow for. A value of 0.0 results in no change. A value of 1.0 disregards the
    original image entirely. Starting with a value between 0.4â€“0.6 is generally a
    safe option.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å»å™ªå¼ºåº¦**ï¼šç¡®å®šå…è®¸åŸå§‹å›¾åƒçš„å˜åŒ–ç¨‹åº¦ã€‚å€¼ä¸º 0.0 æ—¶æ²¡æœ‰å˜åŒ–ã€‚å€¼ä¸º 1.0 æ—¶å®Œå…¨å¿½ç•¥åŸå§‹å›¾åƒã€‚ä¸€èˆ¬æ¥è¯´ï¼Œä» 0.4â€“0.6 çš„å€¼å¼€å§‹æ˜¯ä¸€ä¸ªå®‰å…¨çš„é€‰æ‹©ã€‚'
- en: '**Seed**: The random seed value. Useful when you want to compare the effect
    of a setting with as little variation as possible. If you like a particular generation
    but want to modify it a bit, copy the seed.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç§å­**ï¼šéšæœºç§å­å€¼ã€‚å½“ä½ æƒ³æ¯”è¾ƒè®¾ç½®æ•ˆæœè€Œå°½å¯èƒ½å‡å°‘å˜åŒ–æ—¶ï¼Œè¿™å¾ˆæœ‰ç”¨ã€‚å¦‚æœä½ å–œæ¬¢æŸä¸ªç‰¹å®šç”Ÿæˆä½†æƒ³ç¨ä½œä¿®æ”¹ï¼Œè¯·å¤åˆ¶ç§å­ã€‚'
- en: Using Stable Diffusion as an API
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å°† Stable Diffusion ä½œä¸º API ä½¿ç”¨
- en: 'The web UI is meant for a single user and works great as an interactive art
    tool for making your own creations. However, if we want to build applications
    using this as the engine then we will want an API. A lesser-known (and lesser-documented)
    feature of the stable-diffusion-webui project is that it also has a built-in API.
    The web UI is built with [Gradio](https://gradio.app/) but there is also a FastAPI
    app that can be launched with the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ç½‘ç»œç”¨æˆ·ç•Œé¢æ—¨åœ¨ä¾›å•ä¸ªç”¨æˆ·ä½¿ç”¨ï¼Œå¹¶ä¸”ä½œä¸ºä¸€ä¸ªäº’åŠ¨è‰ºæœ¯å·¥å…·åˆ¶ä½œè‡ªå·±çš„åˆ›ä½œéå¸¸å¥½ã€‚ç„¶è€Œï¼Œå¦‚æœæˆ‘ä»¬æƒ³è¦ä½¿ç”¨å®ƒä½œä¸ºå¼•æ“æ¥æ„å»ºåº”ç”¨ç¨‹åºï¼Œæˆ‘ä»¬å°†éœ€è¦ä¸€ä¸ª APIã€‚stable-diffusion-webui
    é¡¹ç›®çš„ä¸€ä¸ªé²œä¸ºäººçŸ¥ï¼ˆä¸”æ–‡æ¡£è¾ƒå°‘ï¼‰çš„åŠŸèƒ½æ˜¯å®ƒè¿˜å…·æœ‰å†…ç½®çš„ APIã€‚ç½‘ç»œç”¨æˆ·ç•Œé¢æ˜¯ç”¨ [Gradio](https://gradio.app/) æ„å»ºçš„ï¼Œä½†ä¹Ÿæœ‰ä¸€ä¸ªå¯ä»¥ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨çš„
    FastAPI åº”ç”¨ï¼š
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This gives us an API that exposes many of the features we had in the web UI.
    We can send POST requests with our prompt and parameters and receive responses
    that contain output images.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ä¸ª APIï¼Œæš´éœ²äº†æˆ‘ä»¬åœ¨ç½‘ç»œç”¨æˆ·ç•Œé¢ä¸­æ‹¥æœ‰çš„è®¸å¤šåŠŸèƒ½ã€‚æˆ‘ä»¬å¯ä»¥å‘é€å¸¦æœ‰æç¤ºå’Œå‚æ•°çš„ POST è¯·æ±‚ï¼Œå¹¶æ¥æ”¶åŒ…å«è¾“å‡ºå›¾åƒçš„å“åº”ã€‚
- en: Create a microservice
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åˆ›å»ºä¸€ä¸ªå¾®æœåŠ¡
- en: As an example, we will now set up a simple microservice that removes people
    from photos. This has many applications, such as preserving the privacy of individuals.
    We can use stable diffusion as a rudimentary privacy-preserving filter, which
    removes people from photos without any unsightly mosaicing or pixel blocking.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºç¤ºä¾‹ï¼Œæˆ‘ä»¬ç°åœ¨å°†è®¾ç½®ä¸€ä¸ªç®€å•çš„å¾®æœåŠ¡ï¼Œç”¨äºä»ç…§ç‰‡ä¸­ç§»é™¤äººç‰©ã€‚è¿™æœ‰å¾ˆå¤šåº”ç”¨ï¼Œä¾‹å¦‚ä¿æŠ¤ä¸ªäººéšç§ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç¨³å®šæ‰©æ•£ä½œä¸ºä¸€ç§åŸå§‹çš„éšç§ä¿æŠ¤è¿‡æ»¤å™¨ï¼Œå®ƒå¯ä»¥ä»ç…§ç‰‡ä¸­å»é™¤äººç‰©è€Œæ²¡æœ‰ä»»ä½•éš¾çœ‹çš„é©¬èµ›å…‹æˆ–åƒç´ å—ã€‚
- en: Note that this is a basic setup; it does not include encryption, load-balancing,
    multitenancy, RBAC, or any other features. This setup may not be suitable for
    production, but it can be useful for setting up applications on a home or private
    server.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œè¿™åªæ˜¯ä¸€ä¸ªåŸºæœ¬è®¾ç½®ï¼›å®ƒä¸åŒ…æ‹¬åŠ å¯†ã€è´Ÿè½½å‡è¡¡ã€å¤šç§Ÿæˆ·ã€RBAC æˆ–ä»»ä½•å…¶ä»–åŠŸèƒ½ã€‚è¿™ä¸ªè®¾ç½®å¯èƒ½ä¸é€‚åˆç”Ÿäº§ç¯å¢ƒï¼Œä½†å¯ä»¥ç”¨äºåœ¨å®¶åº­æˆ–ç§äººæœåŠ¡å™¨ä¸Šè®¾ç½®åº”ç”¨ç¨‹åºã€‚
- en: Start application in API mode
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä»¥ API æ¨¡å¼å¯åŠ¨åº”ç”¨ç¨‹åº
- en: 'The following instructions will use the server in API mode, so go ahead and
    stop the web UI for now with CTRL+C. Start it up again in API mode with the `--api`
    option:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹è¯´æ˜å°†ä½¿ç”¨ API æ¨¡å¼è¿è¡ŒæœåŠ¡å™¨ï¼Œå› æ­¤è¯·å…ˆé€šè¿‡ CTRL+C åœæ­¢ç½‘ç»œç”¨æˆ·ç•Œé¢ï¼ˆweb UIï¼‰ã€‚ç„¶åä½¿ç”¨ `--api` é€‰é¡¹ä»¥ API æ¨¡å¼é‡æ–°å¯åŠ¨å®ƒï¼š
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The server should print something like this when it is ready:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æœåŠ¡å™¨å‡†å¤‡å¥½æ—¶ï¼Œå®ƒåº”è¯¥ä¼šæ‰“å°å‡ºç±»ä¼¼ä»¥ä¸‹å†…å®¹çš„ä¿¡æ¯ï¼š
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'It can also be useful to run the server in a headless state without the UI.
    To enable just the API without the Gradio app:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è¿è¡ŒæœåŠ¡å™¨è€Œä¸å¸¦ç”¨æˆ·ç•Œé¢çš„æ— å¤´çŠ¶æ€ä¹Ÿå¯èƒ½å¾ˆæœ‰ç”¨ã€‚è¦ä»…å¯ç”¨ API è€Œä¸ä½¿ç”¨ Gradio åº”ç”¨ï¼š
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Send a request to the API
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‘ API å‘é€è¯·æ±‚
- en: The first thing we will want to do is demonstrate how to make a request to the
    API. We wish to send a POST request to the `txt2img` (i.e. â€œtext-to-imageâ€) API
    of the application to simply generate an image.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆè¦åšçš„æ˜¯æ¼”ç¤ºå¦‚ä½•å‘ API å‘å‡ºè¯·æ±‚ã€‚æˆ‘ä»¬å¸Œæœ›å‘åº”ç”¨ç¨‹åºçš„ `txt2img`ï¼ˆå³â€œæ–‡æœ¬åˆ°å›¾åƒâ€ï¼‰API å‘é€ POST è¯·æ±‚ï¼Œä»¥ç®€å•åœ°ç”Ÿæˆä¸€å¼ å›¾åƒã€‚
- en: 'We will use the `requests` package, so install that if you have not already:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨ `requests` åŒ…ï¼Œå› æ­¤å¦‚æœä½ è¿˜æ²¡æœ‰å®‰è£…ï¼Œè¯·å…ˆå®‰è£…å®ƒï¼š
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can send a request containing a prompt as a simple string. The server will
    return an image as a [base64](https://en.wikipedia.org/wiki/Base64)-encoded PNG
    file, which we will need to decode. To decode a base64 image, we simply use `base64.b64decode(b64_image)`.
    The following script should be all you need to test this out:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å‘é€ä¸€ä¸ªåŒ…å«ç®€å•å­—ç¬¦ä¸²çš„æç¤ºçš„è¯·æ±‚ã€‚æœåŠ¡å™¨å°†è¿”å›ä¸€ä¸ª [base64](https://en.wikipedia.org/wiki/Base64)
    ç¼–ç çš„ PNG æ–‡ä»¶ï¼Œæˆ‘ä»¬éœ€è¦å¯¹å…¶è¿›è¡Œè§£ç ã€‚è¦è§£ç  base64 å›¾åƒï¼Œæˆ‘ä»¬åªéœ€ä½¿ç”¨ `base64.b64decode(b64_image)`ã€‚ä»¥ä¸‹è„šæœ¬åº”è¯¥å¯ä»¥æµ‹è¯•è¿™ä¸ªåŠŸèƒ½ï¼š
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Copy the contents to a file and name it `sample-request.py`. Now execute this
    with:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: å°†å†…å®¹å¤åˆ¶åˆ°ä¸€ä¸ªæ–‡ä»¶ä¸­ï¼Œå¹¶å°†å…¶å‘½åä¸º `sample-request.py`ã€‚ç°åœ¨ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ‰§è¡Œå®ƒï¼š
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'If it worked, it should save a copy of the image to the file `dog.png`. Mine
    looked like this dapper fellow:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä¸€åˆ‡æ­£å¸¸ï¼Œå®ƒåº”è¯¥ä¼šå°†å›¾åƒä¿å­˜ä¸ºæ–‡ä»¶ `dog.png`ã€‚æˆ‘çš„å›¾åƒçœ‹èµ·æ¥åƒè¿™æ ·è¿™ä½è¡£ç€å…‰é²œçš„å®¶ä¼™ï¼š
- en: '![](../Images/f4ee5a19b1d19b4de18ac8f09681aebb.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f4ee5a19b1d19b4de18ac8f09681aebb.png)'
- en: Image created with â€˜sample-request.pyâ€™ (photo by author).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨â€˜sample-request.pyâ€™åˆ›å»ºçš„å›¾åƒï¼ˆä½œè€…æ‹æ‘„çš„ç…§ç‰‡ï¼‰ã€‚
- en: Keep in mind that your results will vary from mine. If you encounter issues,
    double-check the output from the terminal running the stable diffusion app. It
    could be that the server was not finished setting up yet. If you get an issue
    like â€œ404 Not Found,â€ double-check that the URL was typed correctly and is pointing
    to the correct address (e.g. 127.0.0.1).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·è®°ä½ï¼Œæ‚¨çš„ç»“æœå¯èƒ½ä¸æˆ‘çš„ä¸åŒã€‚å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·ä»”ç»†æ£€æŸ¥è¿è¡Œç¨³å®šæ‰©æ•£åº”ç”¨çš„ç»ˆç«¯è¾“å‡ºã€‚å¯èƒ½æ˜¯æœåŠ¡å™¨å°šæœªå®Œæˆè®¾ç½®ã€‚å¦‚æœé‡åˆ°â€œ404 Not Foundâ€çš„é—®é¢˜ï¼Œè¯·ä»”ç»†æ£€æŸ¥
    URL æ˜¯å¦æ­£ç¡®è¾“å…¥ï¼Œå¹¶æŒ‡å‘æ­£ç¡®çš„åœ°å€ï¼ˆä¾‹å¦‚ 127.0.0.1ï¼‰ã€‚
- en: Masking an image
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å›¾åƒæ©è†œã€‚
- en: 'If all is working so far, then great! But how can we use this to modify images
    we already have? For that we will want to use the `img2img` (i.e. â€œimage-to-imageâ€)
    API. This API uses stable diffusion to modify an image that you submit. We will
    use the [inpainting feature](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#inpainting):
    given an image and a mask, the inpainting technique will try to replace the masked
    portion of the image with content generated by stable diffusion. The mask acts
    as a weight that smoothly interpolates between the original image and a generation
    to blend the two together.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœåˆ°ç›®å‰ä¸ºæ­¢ä¸€åˆ‡æ­£å¸¸ï¼Œé‚£å°±å¤ªå¥½äº†ï¼ä½†æ˜¯æˆ‘ä»¬å¦‚ä½•ä½¿ç”¨å®ƒæ¥ä¿®æ”¹å·²æœ‰çš„å›¾åƒå‘¢ï¼Ÿä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ `img2img`ï¼ˆå³â€œå›¾åƒåˆ°å›¾åƒâ€ï¼‰APIã€‚è¯¥ API ä½¿ç”¨ç¨³å®šæ‰©æ•£æ¥ä¿®æ”¹æ‚¨æäº¤çš„å›¾åƒã€‚æˆ‘ä»¬å°†ä½¿ç”¨
    [ä¿®å¤åŠŸèƒ½](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#inpainting)ï¼šç»™å®šå›¾åƒå’Œæ©è†œï¼Œä¿®å¤æŠ€æœ¯å°†å°è¯•ç”¨ç¨³å®šæ‰©æ•£ç”Ÿæˆçš„å†…å®¹æ›¿æ¢å›¾åƒä¸­è¢«æ©è†œé®æŒ¡çš„éƒ¨åˆ†ã€‚æ©è†œä½œä¸ºæƒé‡ï¼Œå¹³æ»‘åœ°æ’å€¼äºåŸå§‹å›¾åƒå’Œç”Ÿæˆå†…å®¹ä¹‹é—´ï¼Œå°†ä¸¤è€…èåˆåœ¨ä¸€èµ·ã€‚
- en: Rather than make a mask by hand, we will attempt to generate one using one of
    the many pre-trained computer vision models available to us. We will use the â€œpersonâ€
    class of the model outputs to generate a mask. While an object detection model
    would work, I chose to use a segmentation model so that you can experiment with
    using either dense masks or bounding boxes.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å°è¯•ä½¿ç”¨è®¸å¤šå¯ç”¨çš„é¢„è®­ç»ƒè®¡ç®—æœºè§†è§‰æ¨¡å‹ä¹‹ä¸€ç”Ÿæˆæ©è†œï¼Œè€Œä¸æ˜¯æ‰‹åŠ¨åˆ¶ä½œæ©è†œã€‚æˆ‘ä»¬å°†ä½¿ç”¨æ¨¡å‹è¾“å‡ºä¸­çš„â€œpersonâ€ç±»åˆ«æ¥ç”Ÿæˆæ©è†œã€‚è™½ç„¶å¯¹è±¡æ£€æµ‹æ¨¡å‹ä¹Ÿå¯ä»¥ï¼Œä½†æˆ‘é€‰æ‹©ä½¿ç”¨åˆ†å‰²æ¨¡å‹ï¼Œä»¥ä¾¿æ‚¨å¯ä»¥å°è¯•ä½¿ç”¨å¯†é›†æ©è†œæˆ–è¾¹ç•Œæ¡†ã€‚
- en: We will need a sample image to test with. We could download one from the Internet,
    but in the spirit of preserving privacy (and copyright), why not make one with
    stable diffusion? The following is one I generated with the prompt â€œbeautiful
    mountain landscape, a woman walking away from the camera.â€
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éœ€è¦ä¸€ä¸ªç¤ºä¾‹å›¾åƒè¿›è¡Œæµ‹è¯•ã€‚æˆ‘ä»¬å¯ä»¥ä»äº’è”ç½‘ä¸Šä¸‹è½½ä¸€ä¸ªï¼Œä½†ä¸ºäº†ä¿æŠ¤éšç§ï¼ˆå’Œç‰ˆæƒï¼‰ï¼Œä¸ºä»€ä¹ˆä¸ä½¿ç”¨ç¨³å®šæ‰©æ•£ç”Ÿæˆä¸€ä¸ªå‘¢ï¼Ÿä»¥ä¸‹æ˜¯æˆ‘ä½¿ç”¨æç¤ºè¯â€œç¾ä¸½çš„å±±åœ°é£æ™¯ï¼Œä¸€ä¸ªå¥³äººèƒŒå¯¹é•œå¤´èµ°å¼€â€ç”Ÿæˆçš„å›¾åƒã€‚
- en: '![](../Images/a2a07a10abe92d5e95f51db818854298.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a2a07a10abe92d5e95f51db818854298.png)'
- en: Image generated by stable diffusion (photo by author).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ç¨³å®šæ‰©æ•£ç”Ÿæˆçš„å›¾åƒï¼ˆç…§ç‰‡ä½œè€…æä¾›ï¼‰ã€‚
- en: You can download this one, but I encourage you to try to generate one yourself.
    Of course, you can use real photos as well. The following is minimal code to apply
    a stock segmentation model from `torchvision` to this image as a mask.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å¯ä»¥ä¸‹è½½è¿™ä¸ªï¼Œä½†æˆ‘é¼“åŠ±æ‚¨å°è¯•è‡ªå·±ç”Ÿæˆä¸€ä¸ªã€‚å½“ç„¶ï¼Œæ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨çœŸå®ç…§ç‰‡ã€‚ä»¥ä¸‹æ˜¯å°† `torchvision` çš„åº“å­˜åˆ†å‰²æ¨¡å‹åº”ç”¨äºæ­¤å›¾åƒä½œä¸ºæ©è†œçš„æœ€å°ä»£ç ã€‚
- en: '[PRE11]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Like before, copy this to a file named `segment-person.py`. Execute the code
    with the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: åƒä¹‹å‰ä¸€æ ·ï¼Œå°†å…¶å¤åˆ¶åˆ°åä¸º `segment-person.py` çš„æ–‡ä»¶ä¸­ã€‚ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ‰§è¡Œä»£ç ï¼š
- en: '[PRE12]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The resulting prediction should look something like this:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœé¢„æµ‹åº”è¯¥ç±»ä¼¼äºè¿™ä¸ªï¼š
- en: '![](../Images/042dc0bd35ef951cfb12daf3ecb788ff.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/042dc0bd35ef951cfb12daf3ecb788ff.png)'
- en: Result of segmentation mask applied to image (photo by author).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: åº”ç”¨åˆ°å›¾åƒä¸Šçš„åˆ†å‰²æ©è†œç»“æœï¼ˆç…§ç‰‡ä½œè€…æä¾›ï¼‰ã€‚
- en: We now have the machinery to make requests to the API and to predict bounding
    boxes. Now we can start building out our microservice.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æœ‰äº†å‘ API å‘å‡ºè¯·æ±‚å’Œé¢„æµ‹è¾¹ç•Œæ¡†çš„å·¥å…·ã€‚ç°åœ¨å¯ä»¥å¼€å§‹æ„å»ºæˆ‘ä»¬çš„å¾®æœåŠ¡äº†ã€‚
- en: Person removal microservice
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: äººç‰©ç§»é™¤å¾®æœåŠ¡ã€‚
- en: 'Let us now turn to our practical example: removing people from images. The
    microservice should do the following:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬è½¬åˆ°å®é™…ç¤ºä¾‹ï¼šä»å›¾åƒä¸­ç§»é™¤äººç‰©ã€‚å¾®æœåŠ¡åº”æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
- en: Read a number of input arguments
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è¯»å–ä¸€äº›è¾“å…¥å‚æ•°ã€‚
- en: Load an image from a file
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»æ–‡ä»¶ä¸­åŠ è½½å›¾åƒã€‚
- en: Apply a segmentation model with the class â€œpersonâ€ to the image to create a
    mask
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†â€œpersonâ€ç±»åˆ«çš„åˆ†å‰²æ¨¡å‹åº”ç”¨äºå›¾åƒä»¥åˆ›å»ºæ©è†œã€‚
- en: Convert the image and mask to base64 encoding
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†å›¾åƒå’Œæ©è†œè½¬æ¢ä¸º base64 ç¼–ç ã€‚
- en: Send a request containing the base64-encoded image, the base64-encoded mask,
    the prompt, and any arguments to the `img2img` API of the local server
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å‘é€ä¸€ä¸ªè¯·æ±‚ï¼ŒåŒ…å« base64 ç¼–ç çš„å›¾åƒã€base64 ç¼–ç çš„æ©è†œã€æç¤ºè¯å’Œä»»ä½•å‚æ•°åˆ°æœ¬åœ°æœåŠ¡å™¨çš„ `img2img` APIã€‚
- en: Decode and save the output image as a file
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è§£ç å¹¶å°†è¾“å‡ºå›¾åƒä¿å­˜ä¸ºæ–‡ä»¶ã€‚
- en: 'Since we already covered all of these steps individually, the microservice
    has already been implemented for you in [this GitHub Gist](https://gist.github.com/Mason-McGough/9733aff5bc9d04faecfbb81074617315).
    Now download the script and execute it on the image â€œwoman-on-trail.pngâ€ (or whichever
    image you like) using the following command:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæˆ‘ä»¬å·²ç»å•ç‹¬æ¶µç›–äº†æ‰€æœ‰è¿™äº›æ­¥éª¤ï¼Œå¾®æœåŠ¡å·²ç»ä¸ºæ‚¨åœ¨ [è¿™ä¸ª GitHub Gist](https://gist.github.com/Mason-McGough/9733aff5bc9d04faecfbb81074617315)
    ä¸­å®ç°äº†ã€‚ç°åœ¨ä¸‹è½½è„šæœ¬å¹¶ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åœ¨â€œwoman-on-trail.pngâ€ï¼ˆæˆ–æ‚¨å–œæ¬¢çš„ä»»ä½•å›¾ç‰‡ï¼‰ä¸Šæ‰§è¡Œï¼š
- en: '[PRE13]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The `-W` and `-H` indicate the desired output width and height, respectively.
    It will save the generated image as `inpaint-person.png` and the corresponding
    mask as `mask_inpaint-person.png`. Yours will be different, but this is the output
    I received:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`-W` å’Œ `-H` åˆ†åˆ«è¡¨ç¤ºæ‰€éœ€çš„è¾“å‡ºå®½åº¦å’Œé«˜åº¦ã€‚å®ƒå°†ç”Ÿæˆçš„å›¾åƒä¿å­˜ä¸º `inpaint-person.png`ï¼Œå¯¹åº”çš„æ©æ¨¡ä¿å­˜ä¸º `mask_inpaint-person.png`ã€‚æ‚¨çš„è¾“å‡ºä¼šæœ‰æ‰€ä¸åŒï¼Œä½†è¿™æ˜¯æˆ‘æ”¶åˆ°çš„ç»“æœï¼š'
- en: '![](../Images/c2b2c2a0fdc22772c274443559b753a3.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c2b2c2a0fdc22772c274443559b753a3.png)'
- en: Result of API call using the raw segmentation mask (image by author).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨åŸå§‹åˆ†å‰²æ©æ¨¡çš„APIè°ƒç”¨ç»“æœï¼ˆå›¾åƒç”±ä½œè€…æä¾›ï¼‰ã€‚
- en: Hmm, not quite what we are looking for. Seems that much of the person still
    remains, particularly the silhouette. We may need to mask a larger area. For this,
    let us try converting the mask to a bounding box. We can do this using the `-B`
    flag.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ï¼Œè¿™ä¸æ˜¯æˆ‘ä»¬æƒ³è¦çš„ã€‚ä¼¼ä¹å¾ˆå¤šäººçš„è½®å»“ä»ç„¶å­˜åœ¨ï¼Œç‰¹åˆ«æ˜¯å‰ªå½±ã€‚æˆ‘ä»¬å¯èƒ½éœ€è¦æ©ç›–æ›´å¤§çš„åŒºåŸŸã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•å°†æ©æ¨¡è½¬æ¢ä¸ºè¾¹ç•Œæ¡†ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨`-B`æ ‡å¿—æ¥å®Œæˆè¿™ä¸€æ“ä½œã€‚
- en: '[PRE14]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output I received is this:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ”¶åˆ°çš„è¾“å‡ºæ˜¯ï¼š
- en: '![](../Images/13239ae2bb31628762428c05fb379093.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13239ae2bb31628762428c05fb379093.png)'
- en: Result of API call using a bounding box as mask (photo by author).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¾¹ç•Œæ¡†ä½œä¸ºæ©æ¨¡çš„APIè°ƒç”¨ç»“æœï¼ˆç…§ç‰‡ç”±ä½œè€…æä¾›ï¼‰ã€‚
- en: That is also not quite right! A concrete column is not something we would expect
    to find in the middle of a trail. Perhaps bringing in a prompt will help steer
    things in the right direction. We use the `-p` flag to add the prompt â€œmountain
    scenery, landscape, trailâ€ to the request. We also dilate the bounding box with
    `-D 32` to remove some of the edge effects and blur the bounding box with `-b
    16` to blend the mask with the background a bit.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¹Ÿä¸å®Œå…¨æ­£ç¡®ï¼æ··å‡åœŸæŸ±å­ä¸æ˜¯æˆ‘ä»¬æœŸæœ›åœ¨å°å¾„ä¸­é—´æ‰¾åˆ°çš„ä¸œè¥¿ã€‚ä¹Ÿè®¸å¼•å…¥ä¸€ä¸ªæç¤ºä¼šæœ‰åŠ©äºå¼•å¯¼æ–¹å‘ã€‚æˆ‘ä»¬ä½¿ç”¨`-p`æ ‡å¿—å°†æç¤ºâ€œå±±åœ°é£æ™¯ï¼Œæ™¯è§‚ï¼Œå°å¾„â€æ·»åŠ åˆ°è¯·æ±‚ä¸­ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨`-D
    32`æ‰©å±•è¾¹ç•Œæ¡†ï¼Œä»¥å»é™¤ä¸€äº›è¾¹ç¼˜æ•ˆæœï¼Œå¹¶ä½¿ç”¨`-b 16`æ¨¡ç³Šè¾¹ç•Œæ¡†ï¼Œä½¿æ©æ¨¡ä¸èƒŒæ™¯ç¨å¾®èåˆã€‚
- en: '[PRE15]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'With this I received the following output:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¿™ä¸ªæˆ‘å¾—åˆ°äº†ä»¥ä¸‹è¾“å‡ºï¼š
- en: '![](../Images/af78bf46b1810c8eef81bd6d66fd9685.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/af78bf46b1810c8eef81bd6d66fd9685.png)'
- en: Result of final API call (photo by author).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆAPIè°ƒç”¨çš„ç»“æœï¼ˆç…§ç‰‡ç”±ä½œè€…æä¾›ï¼‰ã€‚
- en: Now that is looking plausible! Keep playing around with different images, settings,
    and prompts to get it working for your use case. To see a complete list of arguments
    and hints available with this script, enter `python inpaint-person.py -h`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨çœ‹èµ·æ¥å¾ˆæœ‰å¯èƒ½ï¼ç»§ç»­å°è¯•ä¸åŒçš„å›¾ç‰‡ã€è®¾ç½®å’Œæç¤ºï¼Œä»¥ä¾¿é€‚åº”æ‚¨çš„ç”¨ä¾‹ã€‚è¦æŸ¥çœ‹è¯¥è„šæœ¬å¯ç”¨çš„å®Œæ•´å‚æ•°å’Œæç¤ºåˆ—è¡¨ï¼Œè¯·è¾“å…¥ `python inpaint-person.py
    -h`ã€‚
- en: Discussion
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®¨è®º
- en: It is very likely that your images looked very different from the ones above.
    Because it is an inherently stochastic process, even using stable diffusion with
    the same settings can produce radically different outputs. There is quite a steep
    learning curve to understand all of the features and proper prompt design and
    even then the results can be finicky. Making an image look exactly the way you
    like is extremely difficult and requires much trial and error.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨çš„å›¾ç‰‡å¾ˆå¯èƒ½ä¸ä¸Šè¿°å›¾ç‰‡å·®åˆ«å¾ˆå¤§ã€‚å› ä¸ºè¿™æ˜¯ä¸€ä¸ªæœ¬è´¨ä¸Šéšæœºçš„è¿‡ç¨‹ï¼Œå³ä½¿ä½¿ç”¨ç›¸åŒè®¾ç½®çš„ç¨³å®šæ‰©æ•£ä¹Ÿä¼šäº§ç”Ÿæˆªç„¶ä¸åŒçš„ç»“æœã€‚ç†è§£æ‰€æœ‰ç‰¹æ€§å’Œæ­£ç¡®çš„æç¤ºè®¾è®¡æœ‰ç›¸å½“é™¡å³­çš„å­¦ä¹ æ›²çº¿ï¼Œå³ä¾¿å¦‚æ­¤ï¼Œç»“æœä¹Ÿå¯èƒ½å¾ˆæŒ‘å‰”ã€‚è®©ä¸€å¼ å›¾ç‰‡å®Œå…¨ç¬¦åˆæ‚¨çš„å–œå¥½æå…¶å›°éš¾ï¼Œéœ€è¦ç»è¿‡å¤§é‡çš„è¯•éªŒå’Œé”™è¯¯ã€‚
- en: 'To aid in your quest, keep the following tips in mind:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å¸®åŠ©æ‚¨ï¼Œè¯·è®°ä½ä»¥ä¸‹æç¤ºï¼š
- en: Use the web UI to find the right parameters that work for your use case before
    moving to the API.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨è½¬åˆ°APIä¹‹å‰ï¼Œä½¿ç”¨Web UIæ‰¾åˆ°é€‚åˆæ‚¨ç”¨ä¾‹çš„æ­£ç¡®å‚æ•°ã€‚
- en: Rely on the prompt matrix and X/Y plot features when finetuning an image to
    your liking. These will help you rapidly explore the parameter search space.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨å°†å›¾åƒå¾®è°ƒåˆ°æ‚¨å–œæ¬¢çš„çŠ¶æ€æ—¶ï¼Œä¾èµ–äºæç¤ºçŸ©é˜µå’ŒX/Yå›¾ç‰¹æ€§ã€‚è¿™äº›å°†å¸®åŠ©æ‚¨å¿«é€Ÿæ¢ç´¢å‚æ•°æœç´¢ç©ºé—´ã€‚
- en: Be mindful of the seeds. If you like a specific output but want to iterate on
    it, copy the seed.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ç§å­ã€‚å¦‚æœæ‚¨å–œæ¬¢ç‰¹å®šçš„è¾“å‡ºä½†æƒ³è¦å¯¹å…¶è¿›è¡Œè¿­ä»£ï¼Œè¯·å¤åˆ¶ç§å­ã€‚
- en: Try a different generator like Midjourney! Every tool is slightly different.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°è¯•ä½¿ç”¨ä¸åŒçš„ç”Ÿæˆå™¨ï¼Œå¦‚ Midjourneyï¼æ¯ä¸ªå·¥å…·ç•¥æœ‰ä¸åŒã€‚
- en: Use Internet resources like [Lexica](https://lexica.art/) as inspiration and
    to find good prompts.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨äº’è”ç½‘èµ„æºï¼Œå¦‚ [Lexica](https://lexica.art/) æ¥è·å–çµæ„Ÿå¹¶å¯»æ‰¾å¥½çš„æç¤ºã€‚
- en: Use the â€œCreate a text file next to every image with generation parametersâ€
    option in the settings menu to keep track of the prompts and settings you use
    to make every image.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨è®¾ç½®èœå•ä¸­ä½¿ç”¨â€œåœ¨æ¯å¼ å›¾ç‰‡æ—è¾¹åˆ›å»ºä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶ï¼Œå¹¶è®°å½•ç”Ÿæˆå‚æ•°â€é€‰é¡¹ï¼Œä»¥è·Ÿè¸ªä½ ç”¨æ¥åˆ¶ä½œæ¯å¼ å›¾ç‰‡çš„æç¤ºå’Œè®¾ç½®ã€‚
- en: Most importantly, have fun!
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€é‡è¦çš„æ˜¯ï¼Œè¦ç©å¾—å¼€å¿ƒï¼
