- en: Deploying Cohere Language Models On Amazon SageMaker
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Amazon SageMaker上部署Cohere语言模型
- en: 原文：[https://towardsdatascience.com/deploying-cohere-language-models-on-amazon-sagemaker-23a3f79639b1](https://towardsdatascience.com/deploying-cohere-language-models-on-amazon-sagemaker-23a3f79639b1)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/deploying-cohere-language-models-on-amazon-sagemaker-23a3f79639b1](https://towardsdatascience.com/deploying-cohere-language-models-on-amazon-sagemaker-23a3f79639b1)
- en: Scale and Host LLMs on AWS
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在AWS上扩展和托管LLMs
- en: '[](https://ram-vegiraju.medium.com/?source=post_page-----23a3f79639b1--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page-----23a3f79639b1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----23a3f79639b1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----23a3f79639b1--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page-----23a3f79639b1--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ram-vegiraju.medium.com/?source=post_page-----23a3f79639b1--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page-----23a3f79639b1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----23a3f79639b1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----23a3f79639b1--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page-----23a3f79639b1--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----23a3f79639b1--------------------------------)
    ·7 min read·May 18, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----23a3f79639b1--------------------------------)
    ·阅读时间7分钟·2023年5月18日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/57c4cb0d9ebd119ae5f3cd5986778694.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57c4cb0d9ebd119ae5f3cd5986778694.png)'
- en: Image from [Unsplash](https://unsplash.com/photos/EgwhIBec0Ck) by [Sigmund](https://unsplash.com/@sigmund)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [Unsplash](https://unsplash.com/photos/EgwhIBec0Ck) 由 [Sigmund](https://unsplash.com/@sigmund)
    提供
- en: Large Language Models (LLMs) and Generative AI are accelerating Machine Learning
    growth across various industries. With LLMs the scope for Machine Learning has
    increased to incredible heights, but has also been accompanied with a new set
    of challenges.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）和生成型人工智能正在加速各行各业的机器学习增长。LLMs使机器学习的范围达到了惊人的高度，但也带来了新的一系列挑战。
- en: The size of LLMs lead to difficult problems in both the Training and Hosting
    portions of the ML lifecycle. Specifically for Hosting LLMs there are a myriad
    of challenges to consider. How can we fit a model into a singular GPU for inference?
    How can we apply model compression and partitioning techniques without compromising
    on accuracy? How can we improve inference latency and throughput for these LLMs?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的规模在机器学习生命周期的训练和托管部分都带来了困难的问题。尤其是在LLMs的托管方面，有许多挑战需要考虑。我们如何将模型适配到单个GPU上进行推理？我们如何在不妨碍准确性的情况下应用模型压缩和分割技术？我们如何提高这些LLMs的推理延迟和吞吐量？
- en: To be able to address many of these questions requires advanced ML Engineering
    where we have to orchestrate model hosting on a platform that can apply compression
    and parallelization techniques at a container and hardware level. There are solutions
    such as [DJL Serving](https://github.com/deepjavalibrary/djl-serving/tree/master)
    that provide [containers](https://aws.amazon.com/blogs/machine-learning/deploy-bloom-176b-and-opt-30b-on-amazon-sagemaker-with-large-model-inference-deep-learning-containers-and-deepspeed/)
    tuned for LLM hosting, but we will not explore them in this article.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 要解决这些问题，需要高级机器学习工程技术，我们必须在一个可以在容器和硬件层面应用压缩和并行化技术的平台上协调模型托管。有像 [DJL Serving](https://github.com/deepjavalibrary/djl-serving/tree/master)
    这样的解决方案提供 [适配于LLM托管的容器](https://aws.amazon.com/blogs/machine-learning/deploy-bloom-176b-and-opt-30b-on-amazon-sagemaker-with-large-model-inference-deep-learning-containers-and-deepspeed/)，但我们在本文中不会探讨这些。
- en: In this article, we’ll explore [SageMaker JumpStart Foundational Models](https://aws.amazon.com/sagemaker/jumpstart/getting-started/?sagemaker-jumpstart-cards.sort-by=item.additionalFields.priority&sagemaker-jumpstart-cards.sort-order=asc&awsf.sagemaker-jumpstart-filter-product-type=*all&awsf.sagemaker-jumpstart-filter-text=*all&awsf.sagemaker-jumpstart-filter-vision=*all&awsf.sagemaker-jumpstart-filter-tabular=*all&awsf.sagemaker-jumpstart-filter-audio-tasks=*all&awsf.sagemaker-jumpstart-filter-multimodal=*all&awsf.sagemaker-jumpstart-filter-RL=*all).
    With Foundational Models we don’t worry about containers or model parallelization
    and compression techniques, but focus primarily on directly deploying a pre-trained
    model with hardware of your choice. Specifically in this article we’ll explore
    a popular LLM provider known as [Cohere](https://cohere.com/) and how we can host
    one of their popular language models on SageMaker for Inference.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将探讨[SageMaker JumpStart 基础模型](https://aws.amazon.com/sagemaker/jumpstart/getting-started/?sagemaker-jumpstart-cards.sort-by=item.additionalFields.priority&sagemaker-jumpstart-cards.sort-order=asc&awsf.sagemaker-jumpstart-filter-product-type=*all&awsf.sagemaker-jumpstart-filter-text=*all&awsf.sagemaker-jumpstart-filter-vision=*all&awsf.sagemaker-jumpstart-filter-tabular=*all&awsf.sagemaker-jumpstart-filter-audio-tasks=*all&awsf.sagemaker-jumpstart-filter-multimodal=*all&awsf.sagemaker-jumpstart-filter-RL=*all)。使用基础模型时，我们无需担心容器或模型并行化和压缩技术，而是主要关注直接部署预训练模型和选择硬件。特别是本文中，我们将探讨一个名为[Cohere](https://cohere.com/)的热门
    LLM 提供商，以及如何在 SageMaker 上托管其流行的语言模型进行推理。
- en: '**NOTE**: For those of you new to AWS, make sure you make an account at the
    following [link](https://aws.amazon.com/console/) if you want to follow along.
    The article also assumes an intermediate understanding of SageMaker Deployment,
    I would suggest following this [article](https://aws.amazon.com/blogs/machine-learning/part-2-model-hosting-patterns-in-amazon-sagemaker-getting-started-with-deploying-real-time-models-on-sagemaker/)
    for understanding Deployment/Inference more in depth. In particular, for SageMaker
    JumpStart, I would reference this following [blog](https://awstip.com/automl-beyond-with-sagemaker-jumpstart-9962ffc4bcd1).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：对于新接触 AWS 的用户，如果你想跟随教程，确保在以下[链接](https://aws.amazon.com/console/)上创建一个账户。本文还假设你对
    SageMaker 部署有一定的了解，我建议你参考这篇[文章](https://aws.amazon.com/blogs/machine-learning/part-2-model-hosting-patterns-in-amazon-sagemaker-getting-started-with-deploying-real-time-models-on-sagemaker/)以更深入地理解部署/推理。特别是对于
    SageMaker JumpStart，我建议参考这篇[博客](https://awstip.com/automl-beyond-with-sagemaker-jumpstart-9962ffc4bcd1)。'
- en: What is SageMaker JumpStart? What Are Foundational Models?
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是 SageMaker JumpStart？什么是基础模型？
- en: SageMaker JumpStart in essence is SageMaker’s Model Zoo. There are a variety
    of different pre-trained models that are already containerized and can be deployed
    via the SageMaker Python SDK. The main value here is that customers don’t need
    to worry about tuning or configuring a container to host a specific model, that
    heavy lift is taken care of.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker JumpStart 本质上是 SageMaker 的模型库。在这里有各种不同的预训练模型，这些模型已经容器化并可以通过 SageMaker
    Python SDK 部署。主要的价值在于，客户无需担心调整或配置容器来托管特定模型，这些繁重的工作已经处理好了。
- en: Specifically for LLMs, JumpStart Foundational Models were launched with popular
    language models from a variety of providers such as Stability AI and in this case
    Cohere. You can view a full list of the Foundational Models that are available
    on the SageMaker Console.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 针对大型语言模型（LLMs），JumpStart 基础模型推出了来自各种提供商的热门语言模型，如 Stability AI 和 Cohere。在 SageMaker
    控制台上，你可以查看所有可用的基础模型的完整列表。
- en: '![](../Images/64fc70670ee9dabdfa2b690889f90864.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/64fc70670ee9dabdfa2b690889f90864.png)'
- en: SageMaker JumpStart Foundational Models (Screenshot by Author)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker JumpStart 基础模型（作者截图）
- en: These Foundational Models are also exposed via the [AWS MarketPlace](https://aws.amazon.com/marketplace)
    where you can subscribe for specific models that may not be accessible by default.
    In the case of Cohere’s Medium model that we will be working with, this should
    be accessible via JumpStart without any subscription, but in the case you do run
    into any issues you can enlist for access at the following [link](https://aws.amazon.com/marketplace/pp/prodview-6dmzzso5vu5my).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这些基础模型也可以通过[AWS MarketPlace](https://aws.amazon.com/marketplace)访问，你可以订阅那些默认情况下可能无法访问的特定模型。在我们将使用的
    Cohere Medium 模型的情况下，这应该可以通过 JumpStart 无需任何订阅即可访问，但如果遇到任何问题，你可以通过以下[链接](https://aws.amazon.com/marketplace/pp/prodview-6dmzzso5vu5my)申请访问。
- en: Cohere Medium Language Model Deployment
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Cohere Medium 语言模型部署
- en: For this example we’ll specifically explore how we can deploy Cohere’s GPT Medium
    Language Model via SageMaker JumpStart. Before we start, we install the [cohere-sagemaker](https://github.com/cohere-ai/cohere-sagemaker/tree/main)
    SDK. This SDK further simplifies the deployment process as it builds a wrapper
    around the usual SageMaker Inference constructs (SageMaker Model, SageMaker Endpoint
    Configuration, and SageMaker Endpoint).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将特别探讨如何通过 SageMaker JumpStart 部署 Cohere 的 GPT Medium 语言模型。在开始之前，我们需要安装
    [cohere-sagemaker](https://github.com/cohere-ai/cohere-sagemaker/tree/main) SDK。这个
    SDK 进一步简化了部署过程，因为它在通常的 SageMaker 推理构造（SageMaker 模型、SageMaker 端点配置和 SageMaker 端点）之上构建了一个包装器。
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: From this SDK we import the [Client](https://github.com/cohere-ai/cohere-sagemaker/blob/main/cohere_sagemaker/client.py)
    object that will help us create our endpoint and also perform inference.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个 SDK 中，我们导入了 [Client](https://github.com/cohere-ai/cohere-sagemaker/blob/main/cohere_sagemaker/client.py)
    对象，它将帮助我们创建端点并进行推理。
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If we go to the Marketplace link we see that this model is available via Model
    Package. Thus, for the next step we provide the Model Package ARN for the Cohere
    Medium model. Note that this specific model is currently only available in US-East-1
    and EU-West-1 regions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们访问 Marketplace 链接，会看到这个模型通过 Model Package 提供。因此，下一步我们提供 Cohere Medium 模型的
    Model Package ARN。请注意，这个特定模型目前仅在 US-East-1 和 EU-West-1 区域可用。
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now that we have our model package we can instantiate our Client object and
    create our endpoint. With JumpStart we have to provide our Model Package Details,
    Instance Type and Count, as well as the Endpoint Name.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了模型包，我们可以实例化我们的 Client 对象并创建端点。使用 JumpStart，我们需要提供模型包详细信息、实例类型和数量，以及端点名称。
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: For language models such as Cohere for the instance type we recommend mostly
    GPU based instances such as the g5 family, or the p3/p2 family, and the g4dn instance
    class. All these instances have enough compute and memory to be able to handle
    the size of these models. For further guidance you can also follow the MarketPlace
    recommendation for the instance to use for the specific model you choose.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像 Cohere 这样的语言模型，我们推荐主要使用基于 GPU 的实例类型，例如 g5 系列、p3/p2 系列和 g4dn 实例类。这些实例都有足够的计算和内存来处理这些模型的大小。有关进一步的指导，您还可以参考
    MarketPlace 对您选择的特定模型所建议的实例。
- en: Next we perform a sample inference with the [generate](https://docs.cohere.com/reference/generate)
    API call which will create text for the prompt we provide our endpoint with. This
    generate API call serves as a Cohere wrapper around the [invoke_endpoint](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime/client/invoke_endpoint.html)
    API call we traditionally see with SageMaker endpoints.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用 [generate](https://docs.cohere.com/reference/generate) API 调用进行样例推理，该调用将为我们提供的提示创建文本。这个
    generate API 调用作为 Cohere 对 [invoke_endpoint](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime/client/invoke_endpoint.html)
    API 调用的封装，后者通常用于 SageMaker 端点。
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/727db58cdb45532fda628b508efb1f29.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/727db58cdb45532fda628b508efb1f29.png)'
- en: Sample Inference (Screenshot by Author)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 样例推理（作者截图）
- en: Parameter Tuning
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参数调整
- en: For an extensive understanding of the different LLM parameters that you can
    tune I would reference Cohere’s official article [here](https://txt.cohere.com/llm-parameters-best-outputs-language-ai/).
    We primarily focus on tuning two different parameters which we saw in our generate
    API call.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 要深入了解可以调整的不同 LLM 参数，我会参考 Cohere 的官方文章 [这里](https://txt.cohere.com/llm-parameters-best-outputs-language-ai/)。我们主要关注于调整在
    generate API 调用中看到的两个不同参数。
- en: '**max_tokens**: Max tokens as the word indicates is the limit to number of
    tokens our LLM can generate. What an LLM defines as a token varies, it can be
    a character, word, phrase, or more. Cohere utilizes byte-pair encoding for their
    tokens. To fully understand how their models define tokens please refer to the
    following [documentation](https://docs.cohere.com/docs/tokens?ref=txt.cohere.com&__hstc=14363112.43b26d84fbf6221bc6cd1a688e60c028.1684346313808.1684346313808.1684346313808.1&__hssc=14363112.2.1684346313808&__hsfp=4188686977).
    In essence we can iterate on this parameter to find an optimal value as we don’t
    want a value that’s too small as it won’t properly answer our prompt or a value
    that’s too large to the point where the response does not make much sense. Cohere’s
    generation models support up to 2048 tokens.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**max_tokens**：正如词语所示，最大token数量是我们LLM可以生成的token数量限制。LLM定义的token可能有所不同，可以是字符、词、短语或更多。Cohere使用字节对编码来定义他们的tokens。要完全理解他们的模型如何定义token，请参考以下[文档](https://docs.cohere.com/docs/tokens?ref=txt.cohere.com&__hstc=14363112.43b26d84fbf6221bc6cd1a688e60c028.1684346313808.1684346313808.1684346313808.1&__hssc=14363112.2.1684346313808&__hsfp=4188686977)。实际上，我们可以在此参数上进行迭代，以找到一个最佳值，因为我们不希望一个太小的值，因为它不能很好地回答我们的提示，也不希望一个太大的值，以至于响应没有多大意义。Cohere的生成模型支持最多2048个tokens。'
- en: '**temperature**: The temperature parameter helps control the “creativity” of
    the model. For example when one word is generated, there’s a list of words with
    varying probabilities for the next word. When the temperature parameter is lower
    the model tends to pick the word with the highest probability. When we increase
    the temperature the responses tend to get a large amount of variety as the model
    starts selecting words with lower probabilities. This parameter ranges from 0
    to 5 for this model.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**温度**：温度参数帮助控制模型的“创造力”。例如，当生成一个词时，会有一个词的列表，其中每个词有不同的概率用于下一个词。当温度参数较低时，模型倾向于选择概率最高的词。当我们增加温度时，响应往往会有较大的多样性，因为模型开始选择概率较低的词。对于这个模型，该参数的范围是0到5。'
- en: First we can explore iterating upon the max_token size. We create an array of
    5 arbitrary token sizes and loop through them for inference while keeping temperature
    constant.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以探索在`max_token`大小上的迭代。我们创建一个包含5个任意token大小的数组，并在保持温度不变的情况下循环进行推断。
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As expected we can see the difference in the length of each of the responses.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，我们可以看到每个响应的长度差异。
- en: '![](../Images/a4b66cb79016467d7449ef0a69a035f9.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4b66cb79016467d7449ef0a69a035f9.png)'
- en: Token Size 200 (Screenshot by Author)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Token Size 200（作者截图）
- en: '![](../Images/992870d9b384c66ca6c2044719c34734.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/992870d9b384c66ca6c2044719c34734.png)'
- en: Token Size 300 (Screenshot by Author)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Token Size 300（作者截图）
- en: We can also test the temperature parameter by iterating through values between
    0 to 5.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过遍历0到5之间的值来测试温度参数。
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We can see that at a value of 1 we have a very realistic output that makes sense
    for the most part.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在值为1时，我们获得了一个非常现实的输出，大部分情况下是有意义的。
- en: '![](../Images/bed75307a71819b3a3ae1671e7741051.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bed75307a71819b3a3ae1671e7741051.png)'
- en: Temperature 1 (Screenshot by Author)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 温度1（作者截图）
- en: At a temperature of 5 we see an output that makes somewhat sense, but is deviating
    extremely from the topic due to the word selection.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在温度为5时，我们看到的输出虽然有一定的意义，但由于词汇选择，偏离主题极为严重。
- en: '![](../Images/48f7f34f950b2583436b9846616869d6.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/48f7f34f950b2583436b9846616869d6.png)'
- en: Temperature 5 (Screenshot by Author)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 温度5（作者截图）
- en: If you would like to test all the different combinations of these parameters
    for your optimal configuration you can also run the following code block.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想测试这些参数的所有不同组合以找到最佳配置，你也可以运行以下代码块。
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Additional Resources & Conclusion
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 额外资源与结论
- en: '[](https://github.com/RamVegiraju/SageMaker-Deployment/blob/master/LLM-Hosting/JumpStart/cohere-medium.ipynb?source=post_page-----23a3f79639b1--------------------------------)
    [## SageMaker-Deployment/cohere-medium.ipynb at master · RamVegiraju/SageMaker-Deployment'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/RamVegiraju/SageMaker-Deployment/blob/master/LLM-Hosting/JumpStart/cohere-medium.ipynb?source=post_page-----23a3f79639b1--------------------------------)
    [## SageMaker-Deployment/cohere-medium.ipynb at master · RamVegiraju/SageMaker-Deployment'
- en: You can't perform that action at this time. You signed in with another tab or
    window. You signed out in another tab or…
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你现在无法执行该操作。你在另一个标签或窗口中登录了。你在另一个标签或窗口中注销了…
- en: github.com](https://github.com/RamVegiraju/SageMaker-Deployment/blob/master/LLM-Hosting/JumpStart/cohere-medium.ipynb?source=post_page-----23a3f79639b1--------------------------------)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/RamVegiraju/SageMaker-Deployment/blob/master/LLM-Hosting/JumpStart/cohere-medium.ipynb?source=post_page-----23a3f79639b1--------------------------------)
- en: The code for the entire example can be found at the link above (stay tuned for
    more LLM and JumpStart examples). With SageMaker JumpStart’s Foundational Models
    it becomes easy to host LLMs via an API call without doing the grunt of work of
    containerizing and model serving. I hope this article was a useful introduction
    to LLMs with Amazon SageMaker, feel free to leave any feedback or questions as
    always.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 整个示例的代码可以在上面的链接中找到（请继续关注更多LLM和JumpStart示例）。借助SageMaker JumpStart的基础模型，可以轻松通过API调用来托管LLM，而无需处理容器化和模型服务的繁琐工作。希望这篇文章对LLM与Amazon
    SageMaker的介绍有所帮助，随时欢迎留下反馈或提问。
- en: '*If you enjoyed this article feel free to connect with me on* [*LinkedIn*](https://www.linkedin.com/in/ram-vegiraju-81272b162/)
    *and subscribe to my Medium* [*Newsletter*](https://ram-vegiraju.medium.com/subscribe)*.
    If you’re new to Medium, sign up using my* [*Membership Referral*](https://ram-vegiraju.medium.com/membership)*.*'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你喜欢这篇文章，请随时通过* [*LinkedIn*](https://www.linkedin.com/in/ram-vegiraju-81272b162/)
    *与我联系，并订阅我的Medium* [*Newsletter*](https://ram-vegiraju.medium.com/subscribe)*。如果你是Medium的新用户，可以使用我的*
    [*Membership Referral*](https://ram-vegiraju.medium.com/membership)*进行注册。*'
