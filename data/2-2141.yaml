- en: Training XGBoost On A 1TB Dataset
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在1TB数据集上训练XGBoost
- en: 原文：[https://towardsdatascience.com/training-xgboost-on-a-1tb-dataset-8790e2bc8672](https://towardsdatascience.com/training-xgboost-on-a-1tb-dataset-8790e2bc8672)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/training-xgboost-on-a-1tb-dataset-8790e2bc8672](https://towardsdatascience.com/training-xgboost-on-a-1tb-dataset-8790e2bc8672)
- en: SageMaker Distributed Training Data Parallel
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SageMaker分布式训练数据并行
- en: '[](https://ram-vegiraju.medium.com/?source=post_page-----8790e2bc8672--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page-----8790e2bc8672--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8790e2bc8672--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8790e2bc8672--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page-----8790e2bc8672--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ram-vegiraju.medium.com/?source=post_page-----8790e2bc8672--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page-----8790e2bc8672--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8790e2bc8672--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8790e2bc8672--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page-----8790e2bc8672--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8790e2bc8672--------------------------------)
    ·7 min read·Feb 8, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----8790e2bc8672--------------------------------)·7分钟阅读·2023年2月8日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/8cc115fe1d75334d1b016c71f5e63cec.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8cc115fe1d75334d1b016c71f5e63cec.png)'
- en: Image from [Unsplash](https://unsplash.com/photos/LqKhnDzSF-8) by [Joshua Sortino](https://unsplash.com/@sortino)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自[Unsplash](https://unsplash.com/photos/LqKhnDzSF-8)，由[Joshua Sortino](https://unsplash.com/@sortino)拍摄
- en: As Machine Learning continues to evolve we’re seeing [larger models](https://docs.cohere.ai/docs/introduction-to-large-language-models)
    with more and more parameters. At the same time we also see incredibly large datasets,
    at the end of the day any model is only as good as the data that it’s trained
    on. Working with large models and datasets can be computationally expensive and
    difficult to iterate or experiment on in a timely manner. For this article we’ll
    focus on the large dataset portion of the problem. Specifically we will look into
    something known as [Distributed Data Parallel](https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel.html)
    utilizing Amazon SageMaker to optimize and reduce training time across a large
    real-world dataset.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习的不断发展，我们看到越来越多参数的[大型模型](https://docs.cohere.ai/docs/introduction-to-large-language-models)。与此同时，我们也看到极其庞大的数据集，归根结底，任何模型的好坏都取决于其训练数据。处理大型模型和数据集可能计算成本高昂且难以快速迭代或实验。本文将重点关注数据集的这一部分问题。具体来说，我们将探讨一种称为[分布式数据并行](https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel.html)的技术，利用Amazon
    SageMaker优化和减少在大型真实世界数据集上的训练时间。
- en: For today’s example we’ll train the [SageMaker XGBoost algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html)
    on an artificially generated **1 TB dataset**. In this example we’ll get a deeper
    understanding of how to prepare and structure your data source for faster training
    as well as understand how to kick off **distributed training** with SageMaker’s
    built in [Data Parallelism Library](https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel.html).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在今天的示例中，我们将在一个人工生成的**1 TB数据集**上训练[SageMaker XGBoost算法](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html)。通过这个示例，我们将深入了解如何准备和构建数据源以加快训练速度，并理解如何利用SageMaker内置的[数据并行库](https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel.html)启动**分布式训练**。
- en: '**NOTE**: This article will assume basic knowledge of AWS and SageMaker specific
    sub-features such as SageMaker Training and interacting with SageMaker and AWS
    as a whole via the [SageMaker Python SDK and Boto3 AWS Python SDK](/sagemaker-python-sdk-vs-boto3-sdk-45c424e8e250).
    For a proper introduction and overview of SageMaker Training, I would reference
    this [article](/training-and-deploying-custom-tensorflow-models-with-aws-sagemaker-72027722ad76).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**：本文假设读者具备基本的AWS和SageMaker特定子功能的知识，例如SageMaker Training以及通过[SageMaker
    Python SDK和Boto3 AWS Python SDK](/sagemaker-python-sdk-vs-boto3-sdk-45c424e8e250)与SageMaker和AWS进行交互。有关SageMaker
    Training的适当介绍和概述，请参考这篇[文章](/training-and-deploying-custom-tensorflow-models-with-aws-sagemaker-72027722ad76)。'
- en: What is Distributed Data Parallel? Why do we need it?
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是分布式数据并行？我们为什么需要它？
- en: Before we can get started with the implementation it’s crucial to understand
    Distributed Data Training. With large datasets it’s really difficult to optimize
    training times as this is very computationally intensive. You have to consider
    both being able to download the dataset into memory as well as whatever training
    and hyperparameter computations the machine will have to perform. With a single
    machine this can be possible (if computationally powerful enough), but also inefficient
    from a time stand point and experimentation becomes a nightmare.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始实现之前，理解 Distributed Data Training 至关重要。对于大型数据集，优化训练时间非常困难，因为这非常计算密集。你必须考虑能够将数据集下载到内存中，以及机器需要执行的训练和超参数计算。对于单台机器来说，这可能是可行的（如果计算能力足够强大），但从时间上来看效率较低，实验过程也会变得很棘手。
- en: With Distributed Data Parallel you can work with a **cluster of instances**.
    Each of these instances can contain multiple CPUs/GPUs. Creating this Distributed
    Data Parallel setup from ground-up can be challenging and there is a lot of overhead
    with node to node communication that needs to be addressed. To simplify matters
    we can utilize the built-in SageMaker [Distributed Data Parallel Library](https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel-intro.html).
    Here the hard work of building and optimizing node to node communication is abstracted
    out and you can focus on model development.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Distributed Data Parallel 你可以处理 **实例集群**。这些实例可以包含多个 CPU/GPU。从零开始创建 Distributed
    Data Parallel 设置可能会很具挑战性，并且节点间通信的开销需要解决。为了简化，我们可以利用 SageMaker 内置的 [Distributed
    Data Parallel Library](https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel-intro.html)。在这里，构建和优化节点间通信的繁重工作被抽象化，你可以专注于模型开发。
- en: Why does data source matter with SageMaker?
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么数据来源在 SageMaker 中很重要？
- en: Where and how our data is provided is essential to optimizing training time.
    With SageMaker the de-facto storage service has always been S3 and that’s still
    an option here. There’s the vanilla training mode where you can upload your dataset
    directly into S3, this is known as **File Mode**. Here SageMaker downloads your
    dataset into the instance memory before training kicks off. For today’s example
    we will work with an optimized S3 mode known as **Fast File Mode**. With Fast
    File Mode the dataset is **streamed** into the instance in **real-time** so we
    can avoid the overhead of downloading the entire dataset. This also leads to the
    question, how should I provide/split my dataset? For this example we will split
    our dataset into multiple smaller files that add up to 1TB, this once again will
    help with download or in our case streaming time as well as our dataset scale
    is quite large.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的提供方式和位置对于优化训练时间至关重要。SageMaker 中默认的存储服务一直是 S3，这里依然是一个选项。你可以使用原始的训练模式，将数据集直接上传到
    S3，这称为 **文件模式**。在这种模式下，SageMaker 会在训练开始前将数据集下载到实例内存中。今天的例子中，我们将使用一种优化的 S3 模式，称为
    **快速文件模式**。在快速文件模式下，数据集会 **实时流式传输** 到实例中，以避免下载整个数据集的开销。这也引出了一个问题，我应该如何提供/拆分我的数据集？在这个例子中，我们将数据集拆分成多个小文件，总大小为
    1TB，这将有助于下载或在我们案例中的流式传输时间，并且数据集规模较大。
- en: Outside of S3 there’s also options to work with **Elastic File System (EFS)**
    and **FsX Lustre** on SageMaker. If your training data already resides on EFS
    it is easy to mount onto SageMaker. With FsX Lustre you can scale at a greater
    rate compared to other options, but there is operational overhead of setting up
    the VPC for this option.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在 S3 之外，你还可以在 SageMaker 上使用 **Elastic File System (EFS)** 和 **FsX Lustre**。如果你的训练数据已经存在于
    EFS 上，那么将其挂载到 SageMaker 上非常简单。使用 FsX Lustre，你可以比其他选项更快地扩展，但设置 VPC 时会有一定的操作开销。
- en: At the end of the day there’s numerous factors you should consider when deciding
    between the different training options for the Data Source with SageMaker. The
    two major points to consider are the dataset size and how you can shard the dataset,
    these factors combined with the current location of your dataset will help you
    make the right choice to optimize training time. For a more comprehensive guide
    please reference this [article](https://aws.amazon.com/blogs/machine-learning/choose-the-best-data-source-for-your-amazon-sagemaker-training-job/)
    around data sources with SageMaker Training.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，在决定SageMaker的数据源训练选项时，你应该考虑许多因素。两个主要的考虑点是数据集的大小和如何分片数据集，这些因素结合你数据集的当前位置将帮助你做出正确的选择以优化训练时间。有关更全面的指南，请参考这篇关于SageMaker训练的数据源的[文章](https://aws.amazon.com/blogs/machine-learning/choose-the-best-data-source-for-your-amazon-sagemaker-training-job/)。
- en: Dataset Creation
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集创建
- en: For this example we’ll utilize the Abalone dataset and run a SageMaker XGBoost
    algorithm on it for a regression model. You can download the dataset from the
    publically available Amazon datasets.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用Abalone数据集，并在其上运行SageMaker XGBoost算法以进行回归模型。你可以从公开的亚马逊数据集中下载数据集。
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This dataset itself is only a 100KB, so we need to make numerous copies of it
    to create a 1TB dataset. For this dataset preparation, I utilized an [EC2 instance](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/get-set-up-for-amazon-ec2.html)
    (r6a.48xlarge) for development. This is a high memory and compute instance that
    will allow for quick preparation of our dataset. Once setup we run the following
    script to make our dataset into a larger 100MB file. You can splice your actual
    data as needed, this is not a set recipe/size that needs to be followed.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集本身只有100KB，因此我们需要制作多个副本来创建1TB的数据集。为了准备这个数据集，我使用了一个[EC2实例](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/get-set-up-for-amazon-ec2.html)（r6a.48xlarge）进行开发。这是一个高内存和计算实例，可以快速准备数据集。一旦设置完成，我们运行以下脚本将数据集制作成更大的100MB文件。你可以根据需要切割你的实际数据，这不是一个固定的配方/大小。
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: With the 100MB dataset we can make 10,000 copies to create our 1TB dataset.
    We then upload these 10,000 copies to S3 which is our Data Source for FastFile
    mode.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 使用100MB的数据集，我们可以制作10,000个副本来创建1TB的数据集。然后我们将这10,000个副本上传到S3，这是我们FastFile模式的数据源。
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This script should take about 2 hours to run, but if you would like to speed
    up the operation you can use some form of multiprocessing Python code with [Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)
    to speed up the upload time.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本运行大约需要2小时，但如果你想加快操作速度，可以使用一些形式的多处理Python代码与[Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)来加快上传时间。
- en: Training Setup
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练设置
- en: Before we can get to running a [SageMaker Training Job](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html)
    we need to setup the proper clients and configuration. Here we specifically define
    our **training instance type**, you can find an extensive list of options at the
    following [page](https://aws.amazon.com/sagemaker/pricing/). In terms of choosing
    an instance type you want to consider the type of model you are dealing with and
    the domain you’re in. With NLP and Computer Vision use-cases generally GPU instances
    have proven to be a better fit, in this case we use a memory optimized instance
    with our XGBoost algorithm.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们运行[SageMaker Training Job](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html)之前，我们需要设置合适的客户端和配置。在这里我们特别定义了**训练实例类型**，你可以在以下[页面](https://aws.amazon.com/sagemaker/pricing/)找到详细的选项列表。在选择实例类型时，你需要考虑你所处理的模型类型和所在领域。对于NLP和计算机视觉应用场景，通常GPU实例被证明是更合适的，在这种情况下，我们使用了一个内存优化实例与我们的XGBoost算法。
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Next we prepare our TrainingInput, here we specify that we are utilizing FastFile
    mode, otherwise it defaults to File mode. We also specify the distribution as
    “ShardedByS3Key”, this indicates we want to distribute all our different S3 files
    across all instances. Otherwise all data files will get loaded into each and every
    single instance leading to a much longer training time.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们准备我们的TrainingInput，在这里我们指定我们使用的是FastFile模式，否则默认为File模式。我们还指定了分布方式为“ShardedByS3Key”，这表示我们希望将所有不同的S3文件分布到所有实例上。否则，所有数据文件将被加载到每个实例中，从而导致训练时间显著增加。
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We then prepare our XGBoost estimator, to get a deeper understanding of the
    algorithm and it’s available hyperparameters, please reference this [article](https://aws.plainenglish.io/end-to-end-example-of-sagemaker-xgboost-eb9eae8a5207).
    The other key here is that we specify our instance count to be 25 (please note
    you may need to request a limit increase here depending on the instance). Our
    10,000 data files will be distributed across these 25 ml.m5.24xlarge for instances.
    Once we specify a count greater than one, SageMaker infers Distributed Data Parallel
    for our model.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们准备XGBoost估算器，要深入了解该算法及其可用的超参数，请参考这篇[文章](https://aws.plainenglish.io/end-to-end-example-of-sagemaker-xgboost-eb9eae8a5207)。另一个关键点是我们将实例数量指定为25（请注意，根据实例类型，您可能需要申请增加限制）。我们的10,000个数据文件将分布在这25个ml.m5.24xlarge实例上。一旦我们指定的数量大于1，SageMaker会推断出分布式数据并行处理我们的模型。
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We can then kick off a training job by fitting the algorithm on the training
    input.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以通过将算法应用于训练输入来启动训练任务。
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: With our current setup this training job takes approximately 11 hours to complete.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们当前的设置下，此训练任务大约需要11小时完成。
- en: '![](../Images/1f7c9a6e176e2e89dfc784332796880a.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1f7c9a6e176e2e89dfc784332796880a.png)'
- en: Distributed Setup (Screenshot by Author)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式设置（作者截图）
- en: '![](../Images/9644166f2707830e8229638e25b897b7.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9644166f2707830e8229638e25b897b7.png)'
- en: SageMaker Training Job Completed (Screenshot by Author)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker训练任务已完成（作者截图）
- en: How can we further optimize?
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们如何进一步优化？
- en: We can tune this training time in a few different ways. One option is simply
    horizontally scaling by increasing the instance count. Another option is going
    the GPU instance route which may require a smaller instance count, but this is
    not always a direct science.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过几种不同的方式来调整训练时间。一种选择是通过增加实例数量来水平扩展。另一种选择是使用GPU实例，这可能需要较小的实例数量，但这并不总是直接的科学。
- en: Outside of tuning the hardware behind the training job we can revisit the data
    source format we were talking about. You can evaluate FsX Lustre which can scale
    to 100s of GB/s throughput. Another option is sharding the dataset in a different
    format to try various combinations of number of files and file size.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 除了调整训练任务背后的硬件，我们还可以重新审视我们讨论过的数据源格式。您可以评估FsX Lustre，它可以扩展到每秒100GB的吞吐量。另一种选择是将数据集分片成不同格式，以尝试不同的文件数量和文件大小组合。
- en: Pricing
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定价
- en: For SageMaker Training Jobs you can estimate cost at the following [page](https://aws.amazon.com/sagemaker/pricing/).
    In essence training jobs are billed by the hour and the instance type. With a
    ml.m5.24xlarge this comes out to $5.53 per hour. With 25 instances and a run time
    of 11 hours this training job comes out to approximately **$1500**, so **please
    keep this in mind** if you run the example. Once again you can further tune this
    by testing out different instances on smaller subsets of data to estimate an approximate
    training time before training on your entire corpus.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于SageMaker训练任务，您可以在以下[页面](https://aws.amazon.com/sagemaker/pricing/)估算费用。本质上，训练任务按小时和实例类型计费。使用ml.m5.24xlarge时，每小时费用为5.53美元。使用25个实例和11小时的运行时间，此训练任务的费用约为**1500美元**，因此**请记住**在运行示例时。再次提醒，您可以通过在较小的数据子集上测试不同的实例来进一步调整，以估算整个语料库训练前的近似训练时间。
- en: Credits/Additional Resources
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考资料/附加资源
- en: '[https://aws.amazon.com/blogs/machine-learning/choose-the-best-data-source-for-your-amazon-sagemaker-training-job/](https://aws.amazon.com/blogs/machine-learning/choose-the-best-data-source-for-your-amazon-sagemaker-training-job/)'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://aws.amazon.com/blogs/machine-learning/choose-the-best-data-source-for-your-amazon-sagemaker-training-job/](https://aws.amazon.com/blogs/machine-learning/choose-the-best-data-source-for-your-amazon-sagemaker-training-job/)'
- en: '[https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html](https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html](https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html)'
- en: Conclusion
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: '[](https://github.com/RamVegiraju/distributed-xgboost-sagemaker?source=post_page-----8790e2bc8672--------------------------------)
    [## GitHub - RamVegiraju/distributed-xgboost-sagemaker: Example of training XGBoost
    algorithm on a 1TB…'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/RamVegiraju/distributed-xgboost-sagemaker?source=post_page-----8790e2bc8672--------------------------------)
    [## GitHub - RamVegiraju/distributed-xgboost-sagemaker: 示例：在1TB数据上训练XGBoost算法…]'
- en: For this example we'll be scaling the Abalone dataset to 1TB size and training
    the SageMaker XGBoost algorithm on it…
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将Abalone数据集扩展到1TB大小，并在上面训练SageMaker XGBoost算法…
- en: github.com](https://github.com/RamVegiraju/distributed-xgboost-sagemaker?source=post_page-----8790e2bc8672--------------------------------)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/RamVegiraju/distributed-xgboost-sagemaker?source=post_page-----8790e2bc8672--------------------------------)'
- en: You can find the entire code for the example above. SageMaker Distributed Training
    offers the ability to train at scale. I also encourage you to explore [Model Parallel](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel.html),
    as the name indicates this looks into model parallelism across multiple instances.
    I hope this article was a useful introduction to SageMaker Training and it’s distributed
    capabilities, please feel free to follow up with any questions.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以找到上述示例的完整代码。SageMaker 分布式训练提供了大规模训练的能力。我也鼓励你探索一下[模型并行](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel.html)，正如名字所示，这涉及到跨多个实例的模型并行。我希望这篇文章对
    SageMaker 训练及其分布式能力提供了有用的介绍，欢迎随时提出任何问题。
- en: '*If you enjoyed this article feel free to connect with me on* [*LinkedIn*](https://www.linkedin.com/in/ram-vegiraju-81272b162/)
    *and subscribe to my Medium* [*Newsletter*](https://ram-vegiraju.medium.com/subscribe)*.
    If you’re new to Medium, sign up using my* [*Membership Referral*](https://ram-vegiraju.medium.com/membership)*.*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你喜欢这篇文章，可以通过* [*LinkedIn*](https://www.linkedin.com/in/ram-vegiraju-81272b162/)
    *与我联系，并订阅我的 Medium* [*新闻通讯*](https://ram-vegiraju.medium.com/subscribe)*。如果你是
    Medium 新手，可以使用我的* [*会员推荐链接*](https://ram-vegiraju.medium.com/membership)*注册。*'
