- en: 'Courage to Learn ML: Demystifying L1 & L2 Regularization (part 4)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 勇敢学习机器学习：揭示 L1 和 L2 正则化（第 4 部分）
- en: 原文：[https://towardsdatascience.com/courage-to-learn-ml-demystifying-l1-l2-regularization-part-4-27c13dc250f9?source=collection_archive---------6-----------------------#2023-12-11](https://towardsdatascience.com/courage-to-learn-ml-demystifying-l1-l2-regularization-part-4-27c13dc250f9?source=collection_archive---------6-----------------------#2023-12-11)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/courage-to-learn-ml-demystifying-l1-l2-regularization-part-4-27c13dc250f9?source=collection_archive---------6-----------------------#2023-12-11](https://towardsdatascience.com/courage-to-learn-ml-demystifying-l1-l2-regularization-part-4-27c13dc250f9?source=collection_archive---------6-----------------------#2023-12-11)
- en: Explore L1 & L2 Regularization as Bayesian Priors
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索将 L1 和 L2 正则化作为贝叶斯先验
- en: '[](https://amyma101.medium.com/?source=post_page-----27c13dc250f9--------------------------------)[![Amy
    Ma](../Images/2edf55456a1f92724535a1441fa2bef5.png)](https://amyma101.medium.com/?source=post_page-----27c13dc250f9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----27c13dc250f9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----27c13dc250f9--------------------------------)
    [Amy Ma](https://amyma101.medium.com/?source=post_page-----27c13dc250f9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://amyma101.medium.com/?source=post_page-----27c13dc250f9--------------------------------)[![Amy
    Ma](../Images/2edf55456a1f92724535a1441fa2bef5.png)](https://amyma101.medium.com/?source=post_page-----27c13dc250f9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----27c13dc250f9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----27c13dc250f9--------------------------------)
    [Amy Ma](https://amyma101.medium.com/?source=post_page-----27c13dc250f9--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd6d8df787b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcourage-to-learn-ml-demystifying-l1-l2-regularization-part-4-27c13dc250f9&user=Amy+Ma&userId=d6d8df787b&source=post_page-d6d8df787b----27c13dc250f9---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----27c13dc250f9--------------------------------)
    ·8 min read·Dec 11, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F27c13dc250f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcourage-to-learn-ml-demystifying-l1-l2-regularization-part-4-27c13dc250f9&user=Amy+Ma&userId=d6d8df787b&source=-----27c13dc250f9---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd6d8df787b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcourage-to-learn-ml-demystifying-l1-l2-regularization-part-4-27c13dc250f9&user=Amy+Ma&userId=d6d8df787b&source=post_page-d6d8df787b----27c13dc250f9---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----27c13dc250f9--------------------------------)
    · 8 分钟阅读·2023年12月11日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F27c13dc250f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcourage-to-learn-ml-demystifying-l1-l2-regularization-part-4-27c13dc250f9&user=Amy+Ma&userId=d6d8df787b&source=-----27c13dc250f9---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F27c13dc250f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcourage-to-learn-ml-demystifying-l1-l2-regularization-part-4-27c13dc250f9&source=-----27c13dc250f9---------------------bookmark_footer-----------)![](../Images/22bdc3089e02a827c9315e4de1e8cd0f.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F27c13dc250f9&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcourage-to-learn-ml-demystifying-l1-l2-regularization-part-4-27c13dc250f9&source=-----27c13dc250f9---------------------bookmark_footer-----------)![](../Images/22bdc3089e02a827c9315e4de1e8cd0f.png)'
- en: Photo by [Dominik Jirovský](https://unsplash.com/@dominik_jirovsky?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Dominik Jirovský](https://unsplash.com/@dominik_jirovsky?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'Welcome back to ‘[Courage to Learn ML](/towardsdatascience.com/tagged/courage-to-learn-ml):
    Unraveling L1 & L2 Regularization,’ in its fourth post. Last time, our mentor-learner
    pair explored [the properties of L1 and L2 regularization through the lens of
    Lagrange Multipliers](https://medium.com/p/ee27cd4b557a).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎回到‘[勇敢学习机器学习](/towardsdatascience.com/tagged/courage-to-learn-ml)：揭示 L1 和
    L2 正则化’，这是第四篇文章。上次，我们的导师-学习者配对通过 [拉格朗日乘子法的视角探讨了 L1 和 L2 正则化的属性](https://medium.com/p/ee27cd4b557a)。
- en: In this concluding segment on L1 and L2 regularization, the duo will delve into
    these topics from a fresh angle — [Bayesian priors](https://medium.com/p/65218b2c2b99).
    We’ll also summarize how L1 and L2 regularizations are applied across different
    algorithms.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在关于L1和L2正则化的总结部分，这对将从全新的角度探讨这些话题——[贝叶斯先验](https://medium.com/p/65218b2c2b99)。我们还将总结L1和L2正则化在不同算法中的应用。
- en: In this article, we’ll address several intriguing questions. If any of these
    topics spark your curiosity, you’ve come to the right place!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将探讨几个有趣的问题。如果这些话题引起了你的兴趣，你来对地方了！
- en: How MAP priors relate to L1 and L2 regularizations
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MAP先验如何与L1和L2正则化相关
- en: An intuitive breakdown of using Laplace and normal distributions as priors
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直观地分解使用拉普拉斯和正态分布作为先验
- en: Understanding the sparsity induced by L1 regularization with a Laplace prior
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解L1正则化引发的稀疏性及其与拉普拉斯先验的关系
- en: Algorithms that are compatible with L1 and L2 regularization
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 兼容L1和L2正则化的算法
- en: Why L2 regularization is often referred to as ‘weight decay’ in neural network
    training
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么在神经网络训练中，L2正则化通常被称为“权重衰减”
- en: The reasons behind the less frequent use of L1 norm in neural networks
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络中不常使用L1范数的原因
- en: '**So, we’ve talked about how**…'
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**所以，我们讨论了如何**…'
