- en: Image Classification For Beginners
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初学者的图像分类
- en: 原文：[https://towardsdatascience.com/image-classification-for-beginners-8546aa75f331](https://towardsdatascience.com/image-classification-for-beginners-8546aa75f331)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/image-classification-for-beginners-8546aa75f331](https://towardsdatascience.com/image-classification-for-beginners-8546aa75f331)
- en: VGG and ResNet architecture from 2014
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VGG和ResNet架构来自2014年
- en: '[](https://medium.com/@mina.ghashami?source=post_page-----8546aa75f331--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page-----8546aa75f331--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8546aa75f331--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8546aa75f331--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page-----8546aa75f331--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mina.ghashami?source=post_page-----8546aa75f331--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page-----8546aa75f331--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8546aa75f331--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8546aa75f331--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page-----8546aa75f331--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8546aa75f331--------------------------------)
    ·10 min read·Oct 17, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----8546aa75f331--------------------------------)
    ·10分钟阅读·2023年10月17日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f097fdb7f59ef0feba34382bfc8b5311.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f097fdb7f59ef0feba34382bfc8b5311.png)'
- en: Images from [unsplash](https://unsplash.com/photos/Sg3XwuEpybU) — modified by
    author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于[unsplash](https://unsplash.com/photos/Sg3XwuEpybU) — 作者修改
- en: Image classification was the first topic I taught at [*Interview Kickstart*](https://www.interviewkickstart.com/)to
    prepare professionals for landing jobs in top tech companies. I wrote this post
    when I was preparing for one of my lectures there. So if you are unfamiliar with
    this topic, this intuitive explanation might help you too.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类是我在[*Interview Kickstart*](https://www.interviewkickstart.com/)教授的第一个主题，旨在帮助专业人士在顶尖科技公司找到工作。当我准备其中的一次讲座时，我写了这篇文章。因此，如果你对这个主题不熟悉，这个直观的解释可能对你也有帮助。
- en: In this post, we look at VGG and ResNet models; Both are seminal and influential
    works in the development of convolutional neural networks (CNNs) for computer
    vision. The VGG[2] was proposed in 2014 from a research group at Oxford, and the
    ResNet[3] was proposed by Microsoft Researchers in 2015.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们探讨了VGG和ResNet模型；这两者都是卷积神经网络（CNNs）在计算机视觉领域发展中的开创性和影响力巨大的作品。VGG[2] 是2014年由牛津大学的研究小组提出的，而ResNet[3]
    是2015年由微软研究人员提出的。
- en: Let’s get started.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: What Is VGG?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是VGG？
- en: '**VGG** stands for **Visual Geometry Group** and is a research group at the
    university of Oxford. In 2014, they designed a deep convolutional neural network
    architecture for image classification task and named it after themselves; i.e.
    VGG. [2].'
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**VGG** 代表**视觉几何组**，是牛津大学的一个研究小组。2014年，他们为图像分类任务设计了一个深度卷积神经网络架构，并以他们的名字命名了它，即VGG。[2].'
- en: VGG Network Architecture
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VGG网络架构
- en: This network comes in few configurations; all have the same architecture just
    the number of layers are different. The most famous ones are VGG16 and VGG19\.
    The VGG19 is deeper and has better performance than VGG16\. For the sake of simplicity,
    we focus on VGG16.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这个网络有几种配置；所有配置的架构相同，只是层数不同。最著名的有VGG16和VGG19。VGG19比VGG16更深，性能更好。为了简化，我们关注VGG16。
- en: VGG16’s architecture is depicted in the image below. As we see it has 16 layers;
    **13 convolutional layers and 3 fully connected layers**.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: VGG16的架构如下图所示。正如我们所见，它有16层；**13个卷积层和3个全连接层**。
- en: '![](../Images/1a93019488a70fbad1b4c5ba68ad1e8a.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a93019488a70fbad1b4c5ba68ad1e8a.png)'
- en: VGG16 architecture — image by the author
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: VGG16架构 — 图片由作者提供
- en: It is a very simple architecture; it consists of 6 blocks where the first 5
    blocks contain convolutional layers followed by a max pool, and the 6-th block
    contains only fully connected layers.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简单的架构；它由6个块组成，其中前5个块包含卷积层，之后是一个最大池化层，第6个块仅包含全连接层。
- en: '**All convolutional layers use 3x3 filters with stride=1,** and all the **max
    pooling layers are 2x2 with stride=2** so they halve width and height of the input
    feature map. This is called *downsampling* as it reduces the size of the output
    feature map.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**所有卷积层使用 3x3 滤波器，步幅为 1**，所有 **最大池化层为 2x2，步幅为 2**，因此它们将输入特征图的宽度和高度减半。这称为 *下采样*，因为它减少了输出特征图的大小。'
- en: 'Note that convolutional layers start with 64 filters and doubles after every
    pooling until it reaches 512 filters. All convolutional layers use “same” padding
    to maintain the same size between input and output, and they all use RELU activation
    function. Below, we explain these concepts:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，卷积层从 64 个滤波器开始，并在每次池化后翻倍，直到达到 512 个滤波器。所有卷积层使用“相同”填充以保持输入和输出之间的相同大小，并且它们都使用
    RELU 激活函数。下面，我们解释这些概念：
- en: '**Same padding**: Same padding is a padding technique to ensure the output
    volume of a convolutional operation has the same height and width as the input
    volume. It works by padding the input with zeros evenly on all sides such that
    the spatial dimensions are unchanged after the convolution operation.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**相同填充**：相同填充是一种填充技术，以确保卷积操作的输出体积具有与输入体积相同的高度和宽度。它通过在所有边缘均匀填充零来工作，使得卷积操作后空间维度保持不变。'
- en: '**Max pooling**: As we saw above after each block, a 2x2 max pool with stride=2
    is applied. Max pooling outputs the maximum value in a window. The stride=2 halves
    spatial dimension, and retains critical information for robust feature detection.
    In addition, this reduction leads to computational efficiency.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**最大池化**：如我们上面所见，在每个块之后应用 2x2 最大池化，步幅为 2。最大池化输出窗口中的最大值。步幅为 2 将空间维度减半，并保留了对强大特征检测至关重要的信息。此外，这种减少带来了计算效率。'
- en: '**RELU activation**: As we mentioned the activation function VGG uses is RELU.
    The RELU sets negative values to zero, and passes positive values unchanged. The
    non-linearity it adds contributes to increasing model’s expressiveness and helps
    in detecting complex patterns. The VGG model uses RELU after each convolutional
    layer.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**RELU 激活函数**：如我们所提到的，VGG 使用的激活函数是 RELU。RELU 将负值设为零，保持正值不变。它所增加的非线性有助于提升模型的表现力，并有助于检测复杂的模式。VGG
    模型在每个卷积层后使用 RELU。'
- en: '![](../Images/99dc7f62c27de6010a081f9cbf70be6a.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99dc7f62c27de6010a081f9cbf70be6a.png)'
- en: image by author
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: 'Let’s go through VGG16 architecture layer by layer:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐层了解 VGG16 架构：
- en: Let’s say input is a colored image with dimensions of height and width, then
    its size is (height, width, 3) . Note RGB is 3 channels.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设输入是一个彩色图像，其尺寸为高度和宽度，则其大小为（高度，宽度，3）。注意 RGB 有 3 个通道。
- en: The first layer has 64 neurons and applies 3x3 convolutions with “same” padding
    so the output feature map from first layer is (height, width, 64).
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一层具有 64 个神经元，并应用 3x3 卷积，具有“相同”填充，因此第一层的输出特征图为（高度，宽度，64）。
- en: The second layer is the same as first layer so the output feature map from this
    layer is also (height, width, 64).
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二层与第一层相同，因此这一层的输出特征图也为（高度，宽度，64）。
- en: The third layer is 2x2 max pooling with stride=2, so it shrinks the size to
    (height/2, width/2, 64)
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三层是 2x2 最大池化，步幅为 2，因此它将大小缩小到（高度/2，宽度/2，64）
- en: The forth and fifth layers are conv3–128 with “same” padding, so they change
    the output size to (height/2, width/2, 128).
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第四层和第五层是 conv3–128，具有“相同”填充，因此它们将输出大小更改为（高度/2，宽度/2，128）。
- en: The sixth layer is 2x2 max pooling again and it changes the output size to (height/4,
    width/4, 128).
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第六层再次是 2x2 最大池化，它将输出大小更改为（高度/4，宽度/4，128）。
- en: If we continue like this, we see that when data reaches the first fully connected
    layer, it is of shape (height/32, width/32, 512). So we see that number of channels
    increased from 3 to 512, while height and width reduced by a factor of 32!!! Think
    of it as compressing the information and instead capturing patterns in channels.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们继续这样下去，我们会发现当数据到达第一个全连接层时，它的形状是（高度/32，宽度/32，512）。因此，我们看到通道的数量从 3 增加到 512，同时高度和宽度减少了
    32 倍！！！可以把它想象成压缩信息，而是捕捉通道中的模式。
- en: VGG Computational Cost
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VGG 计算成本
- en: 'VGG16 is one of the *largest* CNN models; it has 138 Million parameters. In
    the image below, we are seeing two variants of VGG: VGG16 (has 16 layers) and
    VGG19 (with 19 layers).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: VGG16 是 *最大的* CNN 模型之一；它拥有 1.38 亿个参数。在下图中，我们看到 VGG 的两个变体：VGG16（具有 16 层）和 VGG19（具有
    19 层）。
- en: '![](../Images/31aee1966d67683340cdba6303ee7a0d.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/31aee1966d67683340cdba6303ee7a0d.png)'
- en: image from [[1](https://arxiv.org/pdf/1605.07678.pdf)]
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图像来自 [[1](https://arxiv.org/pdf/1605.07678.pdf)]
- en: We see that both VGG16 and VGG19 are largest CNN models in terms of number of
    operations it takes to do in one forward pass. Note that number of operations
    is proportional to the number of parameters the model has. In next post, we will
    look into ResNet[3] model that as we see it is much smaller than VGG and outperforms
    VGG.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到VGG16和VGG19在一次前向传播中需要的操作次数是最大的CNN模型。注意，操作次数与模型的参数数量成正比。在下一篇文章中，我们将探讨ResNet[3]模型，它比VGG小得多，并且表现优于VGG。
- en: Why Was VGG Proposed?
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么提出了VGG？
- en: Prior to VGG, CNN models had fewer layers and larger convolutional filters.
    The VGG network was introduced to show that a simple CNN with only 3x3 convolutional
    layers stacked together would work as good as complex models with large filters.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在VGG之前，CNN模型的层数较少，卷积滤波器较大。VGG网络的提出是为了展示一个只有3x3卷积层堆叠在一起的简单CNN可以与具有大滤波器的复杂模型一样好。
- en: It also showed the *importance of depth* in convolutional networks. They showed
    that stacking many small 3x3 convolutional layers effectively simulates larger
    receptive fields. At the time VGG was introduced, it outperformed every other
    model on image classification task on ImageNet dataset.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 它还展示了卷积网络中*深度的重要性*。他们表明，堆叠许多小的3x3卷积层可以有效模拟较大的感受野。在VGG被提出时，它在ImageNet数据集上的图像分类任务中超越了所有其他模型。
- en: What is ResNet?
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ResNet是什么？
- en: ResNet short for Residual Network was proposed by Microsoft Researchers in 2015
    [3]. Before, we dive into its architecture, let’s first see why it was proposed.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet，即残差网络，是微软研究人员在2015年提出的[3]。在深入了解其架构之前，先来看看它为何被提出。
- en: Why ResNet was proposed?
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么提出了ResNet？
- en: '*In a nutshell, ResNet was proposed to address the vanishing gradient problem
    in very deep networks.*Let’s take a closer look:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*总而言之，ResNet的提出是为了解决在非常深的网络中的梯度消失问题。* 让我们更深入地看看：'
- en: As we saw in the case of VGG, *deep* neural networks are extremely powerful.
    But they also have more parameters and so they take longer to train and cost higher
    in computation. In addition, we need more training data to train them too.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在VGG的案例中所看到的，*深度*神经网络极其强大。但它们也有更多的参数，因此训练时间较长，计算成本较高。此外，我们还需要更多的训练数据来训练它们。
- en: Besides, computational cost and size of training data, there is an obstacle
    in training deep neural network too. As the image below shows, when we train shallow
    neural networks, the training loss starts decreasing in early epochs. But in deep
    neural networks the training loss decreases very minimal in early epochs, and
    after first few epochs it suddenly drops. This is a big obstacle in practical
    training in deep neural networks.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 除了计算成本和训练数据的大小外，训练深度神经网络也面临障碍。正如下图所示，当我们训练浅层神经网络时，训练损失在早期周期中开始减少。但在深度神经网络中，训练损失在早期周期中减少很少，经过几个周期后突然下降。这是深度神经网络实际训练中的一个大障碍。
- en: Now why does it happen?
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 那么为什么会发生这种情况呢？
- en: '![](../Images/01f5eeb746fe4ad621928547d8c35405.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01f5eeb746fe4ad621928547d8c35405.png)'
- en: training loss decreasing by epoch in shallow and deep neural networks — image
    by author
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 浅层和深度神经网络中训练损失随周期减少 —— 图片作者
- en: 'It happens because of two reasons:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这发生的原因有两个：
- en: In early layers of a deep NN**, vanishing gradient descent** problem kicks in;
    i.e. the gradient of the loss vanishes by the time it reaches early layers of
    the networks and so the parameters of these layers receive very little update.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在深度神经网络的早期层中，**梯度消失**问题出现；即，损失的梯度在到达网络的早期层时会消失，因此这些层的参数更新非常少。
- en: In late layers of a deep NN, **very little of the original signal** (i.e. original
    input) arrives. Why is that? because the signal is getting multiplied by the weight
    of all previous layers and going through activation which pushes signals to zero.
    So output of these layers in early epochs is almost random noise. So the gradient
    with respect to the loss is random noise, and the update to these layers’ parameters
    is meaningless.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在深度神经网络的晚期层中，**原始信号非常少**（即原始输入）。这是为什么呢？因为信号被所有前面层的权重乘以，并通过激活函数，这会将信号推向零。因此，这些层在早期周期的输出几乎是随机噪声。因此，相对于损失的梯度是随机噪声，对这些层参数的更新没有意义。
- en: '![](../Images/e284be927213916abb9c32070a239a82.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e284be927213916abb9c32070a239a82.png)'
- en: deep neural networks suffer from learning in first few epochs — image by author
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络在前几个训练周期中学习受阻 —— 图片作者
- en: So this is why we do not see much improvement in first few epochs of training
    deep NNs.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么在训练深度神经网络的前几个周期中没有看到太多改进的原因。
- en: To address this, we’d like to find a way for the input to reaches late layers,
    and for the gradients to reaches early layers. We can achieve both using skip
    connections.
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们希望找到一种方法，使得输入能够到达后期层，梯度能够到达早期层。我们可以使用跳跃连接来实现这两者。
- en: Skip Connection
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跳跃连接
- en: 'The idea of skip connection, is to group layers of the network into blocks,and
    for each block make the input to both go through and around the block. Like this
    :'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 跳跃连接的理念是将网络层分组到块中，对于每个块，使输入同时通过和绕过该块。像这样：
- en: '![](../Images/39b52b02aac6d342d7bb0eaf4a018887.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/39b52b02aac6d342d7bb0eaf4a018887.png)'
- en: image by author
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Within each block, the layers pass their data forward normally, and between
    blocks, we have a new type of connections.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个块内，层正常地向前传递它们的数据，而在块之间，我们有一种新类型的连接。
- en: '![](../Images/e7ef74bba2b9268cd774e9962bde39b8.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e7ef74bba2b9268cd774e9962bde39b8.png)'
- en: 'As we see above, this connection works by combining the input to the block
    with the output from the block. So basically data has two paths to flow: one through
    the block, and one around the block.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，这种连接通过将块的输入与块的输出结合起来工作。因此，数据基本上有两个路径流动：一个通过块，另一个绕过块。
- en: 'So one residual block looks like this:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 所以一个残差块看起来像这样：
- en: '![](../Images/67cd10e87566acce4d89ca5e9d932d84.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67cd10e87566acce4d89ca5e9d932d84.png)'
- en: residual block — image by author
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 残差块 — 图片由作者提供
- en: 'The “+” sign above shows the “combine” symbol which combines input tensor and
    output tensor. It has to be an operation that passes gradient undisturbed. The
    “+” operation can be either of the following:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的“+”符号表示“组合”符号，它将输入张量和输出张量结合在一起。它必须是一个不会干扰梯度传递的操作。“+”操作可以是以下任意一种：
- en: element-wise addition of the two tensors
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 两个张量的逐元素相加
- en: concatenation of the two tensors
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 两个张量的拼接
- en: It is worth emphasizing that a residual block is called “residual” because it
    enables a residual learning approach. Each residual block learns a residual function
    with reference to its input, rather than directly fitting a desired underlying
    mapping.
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 值得强调的是，残差块之所以被称为“残差”，是因为它实现了一种残差学习方法。每个残差块学习一个相对于其输入的残差函数，而不是直接拟合一个期望的基础映射。
- en: 'In feed-forward networks, we learn the direct mapping from input to output,
    i.e. *f(x): x->y*. In residual blocks however, as we see in the above, instead
    of trying to learn the direct mapping, each residual block learns a residual function,
    i.e. *x->f(x)+x*. **This residual function represents the modification that needs
    to be made to the input to get the desired output.**'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '在前馈网络中，我们学习从输入到输出的直接映射，即 *f(x): x->y*。然而，在残差块中，如上所示，每个残差块学习一个残差函数，即 *x->f(x)+x*。**这个残差函数表示需要对输入进行的修改，以获得期望的输出。**'
- en: '![](../Images/3de40c0a87cf3a5f924061bba215d996.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3de40c0a87cf3a5f924061bba215d996.png)'
- en: image taken from [[3](https://arxiv.org/abs/1512.03385)]
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源 [[3](https://arxiv.org/abs/1512.03385)]
- en: ResNet Is Easier to Train
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ResNet 更容易训练
- en: A network made of residual blocks is called a *residual networks or ResNet*.
    They have couple advantages that make them faster and easier to train.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 由残差块组成的网络称为*残差网络或 ResNet*。它们有几个优势，使得它们更快、更容易训练。
- en: 'One is that every residual block augment the data: Since they pass the input
    around the block unchanged, the job of residual block is not to figure out what
    important information is in the input, but rather its job is to figure out what
    additional information we can add to the input to reach the output. And this turns
    out to be a simpler job.'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其中之一是每个残差块都会增强数据：由于它们将输入绕过块而不变，残差块的工作不是去判断输入中包含什么重要信息，而是去确定我们可以向输入中添加哪些额外的信息以达到输出。结果发现这是一项更简单的工作。
- en: The network has shorter gradient paths. Since each block has a path that goes
    around the block, the gradient goes through that path too. And so any layer in
    the network has relatively a short path by which loss gradients can arrive.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网络具有更短的梯度路径。由于每个块都有一个绕过块的路径，梯度也会经过这条路径。因此，网络中的任何层都有相对较短的路径，使得损失梯度能够到达。
- en: '![](../Images/77278ae9db798c37a27bff4e73c7226b.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77278ae9db798c37a27bff4e73c7226b.png)'
- en: 'gradient flows in two paths: through the layers and around the blocks — image
    by author'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度在两个路径中流动：通过层和绕过块 — 图片由作者提供
- en: Concerns For ResNet
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于 ResNet 的关注点
- en: 'There are few concerns around the residual blocks that we have to pay attention
    to them when we design residual networks:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 关于残差块，有一些关注点，我们在设计残差网络时需要注意：
- en: To be able to add/concatenate the input and output of a residual blocks, we
    have to have both tensors to be of the same shape. Obviously, if we enforce the
    shape of every layer’s output to be the same as its input, this issue will not
    arise. But enforcing this constraint limits model’s capacity.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了能够添加/连接残差块的输入和输出，我们必须确保两个张量的形状相同。显然，如果我们强制每一层的输出形状与其输入相同，这个问题将不会出现。但是，强制这种约束会限制模型的容量。
- en: If we use concatenation instead of element-wise addition to combine input and
    output tensor of each block, then we will end up with a very large tensor and
    we will have an explosion of parameters. So we should not overuse concatenation
    operation and must prefer addition if our network is deep. Often, concatenation
    is used in one or two blocks maximum.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们使用连接而不是逐元素相加来组合每个块的输入和输出张量，那么我们将得到一个非常大的张量，并且参数会爆炸。因此，我们不应过度使用连接操作，如果我们的网络很深，必须优先考虑相加。通常，连接操作在一个或两个块中最多使用。
- en: ResNet Architecture
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ResNet 架构
- en: Now that we know about residual block and skip connections, ResNet[3] for image
    classification is built by stacking multiple residual blocks. We can construct
    very deep networks of over 100 layers. Original ResNet has variant architectures
    from 18 layers to 152 layers [3].
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了残差块和跳跃连接，ResNet[3] 用于图像分类，通过堆叠多个残差块来构建。我们可以构建超过 100 层的非常深的网络。原始的 ResNet
    具有从 18 层到 152 层的变体架构 [3]。
- en: Each residual block consists of a convolutional layer, batch normalization and
    and RELU activation functions. As we see in the image below, “batch normalization”
    is used after every convolutional layer; this normalizes activations by subtracting
    the mean and dividing it by standard deviation. This operation stabilizes the
    training procedure.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 每个残差块由一个卷积层、批量归一化和 RELU 激活函数组成。正如我们在下图中看到的那样，“批量归一化”在每个卷积层之后使用；它通过减去均值并除以标准差来规范化激活。这一操作稳定了训练过程。
- en: '![](../Images/27914325ac4eb96961583c958c043ae4.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/27914325ac4eb96961583c958c043ae4.png)'
- en: Residual block - image by author
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 残差块 - 作者提供的图像
- en: When ResNet was proposed, it achieved state-of-the-art results on ImageNet classification
    task[3].
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当 ResNet 被提出时，它在 ImageNet 分类任务上取得了最先进的结果 [3]。
- en: Takeaways
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 要点
- en: '**Takeaway1:** Last layers in a deep NN receive very little of the input signal.
    This is because each intermediate layer’s activation functions like sigmoid or
    tanh saturate to 0 or 1 for large positive/negative inputs. This diminishes signal
    as it passes through layers. This is referred to as “saturation”.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**要点1：** 深层神经网络中的最后几层接收到的输入信号非常少。这是因为每个中间层的激活函数如 sigmoid 或 tanh 对于大的正/负输入会饱和到
    0 或 1。这会随着信号通过层而减弱。这被称为“饱和”。'
- en: '**Takeaway2**: Early layers of a deep NN receive very little of the gradient
    in first few epochs of training a network. This is because the error gradient
    is propagated back through many layers during training, it shrinks exponentially.
    This makes it difficult for early layers to learn effectively. This problem is
    referred to as “vanishing gradient problem”.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**要点2：** 深层神经网络的早期层在训练网络的前几个时期接收到的梯度非常少。这是因为在训练过程中，误差梯度通过许多层反向传播，它会指数级地缩小。这使得早期层难以有效学习。这个问题被称为“梯度消失问题”。'
- en: '**Takeaway3:** VGG was proposed to show deep networks with simple 3x3 filters
    resemble complex networks with large convolutions. ResNet was proposed to address
    vanishing gradients in very deep networks.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**要点3：** VGG 的提出是为了展示使用简单的 3x3 滤波器的深层网络如何类似于使用大卷积的复杂网络。ResNet 的提出是为了解决非常深层网络中的梯度消失问题。'
- en: Summary
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this post, we looked at two seminal CNN architectures called VGG and ResNet.
    VGG is a deep CNN which contains only 3x3 convolutional layers. It was historically
    used for image classification task, and at the time it was proposed it outperformed
    AlexNet and other competing models on ImageNet challenge. It demonstrated the
    power of depth in CNNs and the fact that using simple 3x3 convolutions can resemble
    the effect of larger kernels. ResNet was introduced after VGG and it outperformed
    VGG. ResNet innovation was in introducing residual blocks which made training
    of deep networks easier and faster.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们研究了两个开创性的 CNN 架构，即 VGG 和 ResNet。VGG 是一个深层 CNN，仅包含 3x3 卷积层。它历史上用于图像分类任务，并且在提出时，它在
    ImageNet 挑战中优于 AlexNet 和其他竞争模型。它展示了 CNN 中深度的力量，以及使用简单的 3x3 卷积可以类似于更大卷积核的效果。ResNet
    在 VGG 之后被引入，并且优于 VGG。ResNet 的创新在于引入了残差块，这使得深层网络的训练变得更容易和更快。
- en: 'If you have any questions or suggestions, feel free to reach out to me:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有任何问题或建议，请随时联系我：
- en: 'Email: mina.ghashami@gmail.com'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 电子邮件：mina.ghashami@gmail.com
- en: 'LinkedIn: [https://www.linkedin.com/in/minaghashami/](https://www.linkedin.com/in/minaghashami/)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 领英：[https://www.linkedin.com/in/minaghashami/](https://www.linkedin.com/in/minaghashami/)
- en: References
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[An analysis for deep neural network models for practical applications](https://arxiv.org/pdf/1605.07678.pdf)'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[实际应用中的深度神经网络模型分析](https://arxiv.org/pdf/1605.07678.pdf)'
- en: '[Very deep convolutional networks for large scale image recognition](https://arxiv.org/pdf/1409.1556.pdf)'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[非常深的卷积网络用于大规模图像识别](https://arxiv.org/pdf/1409.1556.pdf)'
- en: '[Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[深度残差学习用于图像识别](https://arxiv.org/abs/1512.03385)'
