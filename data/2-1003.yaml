- en: 'GPT-4 vs. ChatGPT: An exploration of training, performance, capabilities, and
    limitations'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPT-4ä¸ChatGPTï¼šå¯¹è®­ç»ƒã€æ€§èƒ½ã€èƒ½åŠ›å’Œå±€é™æ€§çš„æ¢è®¨
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/gpt-4-vs-chatgpt-an-exploration-of-training-performance-capabilities-and-limitations-35c990c133c5](https://towardsdatascience.com/gpt-4-vs-chatgpt-an-exploration-of-training-performance-capabilities-and-limitations-35c990c133c5)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/gpt-4-vs-chatgpt-an-exploration-of-training-performance-capabilities-and-limitations-35c990c133c5](https://towardsdatascience.com/gpt-4-vs-chatgpt-an-exploration-of-training-performance-capabilities-and-limitations-35c990c133c5)
- en: GPT-4 is an improvement, but temper your expectations.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-4æ˜¯ä¸€æ¬¡æ”¹è¿›ï¼Œä½†è¦é€‚åº¦æœŸæœ›ã€‚
- en: '[](https://medium.com/@mary.newhauser?source=post_page-----35c990c133c5--------------------------------)[![Mary
    Newhauser](../Images/7f0d7287f7b735bb9391858f1fc641ee.png)](https://medium.com/@mary.newhauser?source=post_page-----35c990c133c5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----35c990c133c5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----35c990c133c5--------------------------------)
    [Mary Newhauser](https://medium.com/@mary.newhauser?source=post_page-----35c990c133c5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mary.newhauser?source=post_page-----35c990c133c5--------------------------------)[![Mary
    Newhauser](../Images/7f0d7287f7b735bb9391858f1fc641ee.png)](https://medium.com/@mary.newhauser?source=post_page-----35c990c133c5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----35c990c133c5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----35c990c133c5--------------------------------)
    [Mary Newhauser](https://medium.com/@mary.newhauser?source=post_page-----35c990c133c5--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----35c990c133c5--------------------------------)
    Â·7 min readÂ·Mar 17, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page-----35c990c133c5--------------------------------)
    Â·é˜…è¯»æ—¶é•¿7åˆ†é’ŸÂ·2023å¹´3æœˆ17æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/8ebc0c71e1eaa32026c30fe54eb8b0d9.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8ebc0c71e1eaa32026c30fe54eb8b0d9.png)'
- en: Image created by the author.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…åˆ›ä½œã€‚
- en: OpenAI stunned the world when it dropped [ChatGPT](https://openai.com/blog/chatgpt)
    in late 2022\. The new generative language model is expected to totally transform
    entire industries, including media, education, law, and tech. In short, ChatGPT
    threatens to disrupt just about everything. And even before we had time to truly
    envision a post-ChatGPT world, OpenAI dropped [GPT-4](https://openai.com/research/gpt-4).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAIåœ¨2022å¹´åº•å‘å¸ƒäº†[ChatGPT](https://openai.com/blog/chatgpt)ï¼Œéœ‡æƒŠäº†ä¸–ç•Œã€‚è¿™ä¸ªæ–°çš„ç”Ÿæˆè¯­è¨€æ¨¡å‹é¢„è®¡å°†å½»åº•æ”¹å˜æ•´ä¸ªè¡Œä¸šï¼ŒåŒ…æ‹¬åª’ä½“ã€æ•™è‚²ã€æ³•å¾‹å’ŒæŠ€æœ¯ã€‚ç®€è€Œè¨€ä¹‹ï¼ŒChatGPTå¨èƒåˆ°å‡ ä¹æ‰€æœ‰çš„é¢†åŸŸã€‚å³ä¾¿åœ¨æˆ‘ä»¬è¿˜æ²¡æ¥å¾—åŠçœŸæ­£è®¾æƒ³åChatGPTæ—¶ä»£çš„ä¸–ç•Œæ—¶ï¼ŒOpenAIåˆå‘å¸ƒäº†[GPT-4](https://openai.com/research/gpt-4)ã€‚
- en: In recent months, the speed with which groundbreaking large language models
    have been released is astonishing. If you still donâ€™t understand how ChatGPT differs
    from GPT-3, let alone GPT-4, I donâ€™t blame you.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘å‡ ä¸ªæœˆï¼Œçªç ´æ€§çš„å¤§å‹è¯­è¨€æ¨¡å‹å‘å¸ƒçš„é€Ÿåº¦ä»¤äººæƒŠå¹ã€‚å¦‚æœä½ ä»ç„¶ä¸ç†è§£ChatGPTä¸GPT-3ï¼Œç”šè‡³æ˜¯GPT-4çš„åŒºåˆ«ï¼Œæˆ‘ä¸æ€ªä½ ã€‚
- en: In this article, we will cover the key similarities and differences between
    ChatGPT and GPT-4, including their training methods, performance and capabilities,
    and limitations.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨ChatGPTå’ŒGPT-4ä¹‹é—´çš„å…³é”®ç›¸ä¼¼ç‚¹å’Œå·®å¼‚ï¼ŒåŒ…æ‹¬å®ƒä»¬çš„è®­ç»ƒæ–¹æ³•ã€æ€§èƒ½å’Œèƒ½åŠ›ï¼Œä»¥åŠå±€é™æ€§ã€‚
- en: 'ChatGPT vs. GPT-4: Similarities & differences in training methods'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ChatGPTä¸GPT-4ï¼šè®­ç»ƒæ–¹æ³•çš„ç›¸ä¼¼æ€§ä¸å·®å¼‚
- en: ChatGPT and GPT-4 both stand on the shoulders of giants, building on previous
    versions of GPT models while adding improvements to model architecture, employing
    more sophisticated training methods, and increasing the number of training parameters.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPTå’ŒGPT-4éƒ½å»ºç«‹åœ¨å‰äººçš„åŸºç¡€ä¸Šï¼Œç»§æ‰¿äº†å…ˆå‰ç‰ˆæœ¬çš„GPTæ¨¡å‹ï¼ŒåŒæ—¶åœ¨æ¨¡å‹æ¶æ„ä¸Šè¿›è¡Œæ”¹è¿›ï¼Œé‡‡ç”¨æ›´å¤æ‚çš„è®­ç»ƒæ–¹æ³•ï¼Œå¹¶å¢åŠ äº†è®­ç»ƒå‚æ•°çš„æ•°é‡ã€‚
- en: Both models are based on the transformer architecture. [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)
    and [GPT-3](https://arxiv.org/pdf/2005.14165.pdf) use multi-headed self-attention
    to decide which text inputs to pay the most attention to. The models also use
    a [decoder-only](https://jalammar.github.io/illustrated-gpt2/) architecture that
    generates output sequences one token at a time, iteratively predicting the next
    token in a sequence. Although the precise architectures for ChatGPT and GPT-4
    have not been released, we can assume they continue to be decoder-only models.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¤ç§æ¨¡å‹éƒ½åŸºäºå˜æ¢å™¨æ¶æ„ã€‚[GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)å’Œ
    [GPT-3](https://arxiv.org/pdf/2005.14165.pdf) ä½¿ç”¨å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶æ¥å†³å®šå…³æ³¨å“ªäº›æ–‡æœ¬è¾“å…¥ã€‚æ¨¡å‹è¿˜ä½¿ç”¨[ä»…è§£ç å™¨](https://jalammar.github.io/illustrated-gpt2/)æ¶æ„ï¼Œé€ä¸ªç”Ÿæˆè¾“å‡ºåºåˆ—ï¼Œè¿­ä»£é¢„æµ‹åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªæ ‡è®°ã€‚è™½ç„¶ChatGPTå’ŒGPT-4çš„å…·ä½“æ¶æ„å°šæœªå…¬å¸ƒï¼Œä½†æˆ‘ä»¬å¯ä»¥å‡è®¾å®ƒä»¬ä»ç„¶æ˜¯ä»…è§£ç å™¨æ¨¡å‹ã€‚
- en: OpenAIâ€™s [GPT-4 Technical Report](https://cdn.openai.com/papers/gpt-4.pdf) offers
    little information on GPT-4â€™s model architecture and training process, citing
    the â€œcompetitive landscape and the safety implications of large-scale models.â€
    What we do know is that ChatGPT and GPT-4 are probably trained in a similar manner,
    which is a departure from training methods used for GPT-2 and GPT-3\. We know
    much more about the training methods for ChatGPT than GPT-4, so weâ€™ll start there.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAIçš„[GPT-4æŠ€æœ¯æŠ¥å‘Š](https://cdn.openai.com/papers/gpt-4.pdf)å¯¹äºGPT-4çš„æ¨¡å‹æ¶æ„å’Œè®­ç»ƒè¿‡ç¨‹ä¿¡æ¯æœ‰é™ï¼Œå¼•ç”¨äº†â€œå¤§è§„æ¨¡æ¨¡å‹çš„ç«äº‰æ ¼å±€å’Œå®‰å…¨æ€§å½±å“â€ã€‚æˆ‘ä»¬æ‰€çŸ¥é“çš„æ˜¯ï¼ŒChatGPTå’ŒGPT-4å¯èƒ½ä»¥ç±»ä¼¼çš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œè¿™ä¸GPT-2å’ŒGPT-3çš„è®­ç»ƒæ–¹æ³•æœ‰æ‰€ä¸åŒã€‚æˆ‘ä»¬å¯¹ChatGPTçš„è®­ç»ƒæ–¹æ³•äº†è§£æ›´å¤šï¼Œå› æ­¤æˆ‘ä»¬ä»è¿™é‡Œå¼€å§‹ã€‚
- en: ChatGPT
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ChatGPT
- en: To start with, ChatGPT is trained on dialogue datasets, including demonstration
    data, in which human annotators provide demonstrations of the expected output
    of a chatbot assistant in response to specific prompts. This data is used to fine-tune
    GPT3.5 with supervised learning, producing a policy model, which is used to generate
    multiple responses when fed prompts. Human annotators then rank which of the responses
    for a given prompt produced the best results, which is used to train a reward
    model. The reward model is then used to iteratively fine-tune the policy model
    using reinforcement learning.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼ŒChatGPTåœ¨å¯¹è¯æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼ŒåŒ…æ‹¬æ¼”ç¤ºæ•°æ®ï¼Œå…¶ä¸­äººç±»æ³¨é‡Šå‘˜æä¾›å¯¹ç‰¹å®šæç¤ºçš„èŠå¤©åŠ©æ‰‹é¢„æœŸè¾“å‡ºçš„æ¼”ç¤ºã€‚è¿™äº›æ•°æ®ç”¨äºç”¨ç›‘ç£å­¦ä¹ å¾®è°ƒGPT3.5ï¼Œç”Ÿæˆä¸€ä¸ªç­–ç•¥æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åœ¨è¾“å…¥æç¤ºæ—¶ç”Ÿæˆå¤šä¸ªå“åº”ã€‚ç„¶åï¼Œäººç±»æ³¨é‡Šå‘˜å¯¹æ¯ä¸ªæç¤ºçš„å“åº”è¿›è¡Œæ’åï¼Œä»¥ç¡®å®šå“ªä¸ªå“åº”æ•ˆæœæœ€ä½³ï¼Œè¿™ç”¨äºè®­ç»ƒå¥–åŠ±æ¨¡å‹ã€‚å¥–åŠ±æ¨¡å‹éšåç”¨äºé€šè¿‡å¼ºåŒ–å­¦ä¹ è¿­ä»£å¾®è°ƒç­–ç•¥æ¨¡å‹ã€‚
- en: '![](../Images/c520850d7d2cfde00d40aafaf710a0b7.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c520850d7d2cfde00d40aafaf710a0b7.png)'
- en: Image created by the author.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…åˆ›å»ºã€‚
- en: To sum it up in one sentence, ChatGPT is trained using [Reinforcement Learning
    from Human Feedback](https://openai.com/research/learning-from-human-preferences)
    (RLHF), a way of incorporating human feedback to improve a language model during
    training. *This allows the modelâ€™s output to align to the task requested by the
    user, rather than just predict the next word in a sentence based on a corpus of
    generic training data, like GPT-3.*
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ç®€è€Œè¨€ä¹‹ï¼ŒChatGPTä½¿ç”¨[æ¥è‡ªäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ](https://openai.com/research/learning-from-human-preferences)ï¼ˆRLHFï¼‰è¿›è¡Œè®­ç»ƒï¼Œè¿™æ˜¯ä¸€ç§åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ•´åˆäººç±»åé¦ˆä»¥æ”¹è¿›è¯­è¨€æ¨¡å‹çš„æ–¹æ³•ã€‚*è¿™ä½¿å¾—æ¨¡å‹çš„è¾“å‡ºå¯ä»¥ä¸ç”¨æˆ·è¯·æ±‚çš„ä»»åŠ¡å¯¹é½ï¼Œè€Œä¸ä»…ä»…æ˜¯æ ¹æ®é€šç”¨è®­ç»ƒæ•°æ®è¯­æ–™åº“é¢„æµ‹å¥å­ä¸­çš„ä¸‹ä¸€ä¸ªè¯ï¼Œå°±åƒGPT-3ä¸€æ ·ã€‚*
- en: GPT-4
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT-4
- en: OpenAI has yet to divulge details on how it trained GPT-4\. Their Technical
    Report doesnâ€™t include â€œdetails about the architecture (including model size),
    hardware, training compute, dataset construction, training method, or similar.â€
    *What we do know is that GPT-4 is a transformer-style generative multimodal model
    trained on both publicly available data and licensed third-party data and subsequently
    fine-tuned using RLHF.* Interestingly, OpenAI did share details regarding their
    upgraded RLHF techniques to make the model responses more accurate and less likely
    to veer outside safety guardrails.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAIå°šæœªé€éœ²å¦‚ä½•è®­ç»ƒGPT-4ã€‚ä»–ä»¬çš„æŠ€æœ¯æŠ¥å‘Šä¸­ä¸åŒ…æ‹¬â€œå…³äºæ¶æ„ï¼ˆåŒ…æ‹¬æ¨¡å‹å¤§å°ï¼‰ã€ç¡¬ä»¶ã€è®­ç»ƒè®¡ç®—ã€æ•°æ®é›†æ„å»ºã€è®­ç»ƒæ–¹æ³•æˆ–ç±»ä¼¼å†…å®¹çš„è¯¦ç»†ä¿¡æ¯â€ã€‚*æˆ‘ä»¬æ‰€çŸ¥é“çš„æ˜¯ï¼ŒGPT-4æ˜¯ä¸€ä¸ªå˜æ¢å™¨é£æ ¼çš„ç”Ÿæˆå¤šæ¨¡æ€æ¨¡å‹ï¼Œè®­ç»ƒæ•°æ®åŒ…æ‹¬å…¬å¼€å¯ç”¨çš„æ•°æ®å’Œæˆæƒçš„ç¬¬ä¸‰æ–¹æ•°æ®ï¼Œå¹¶éšåä½¿ç”¨RLHFè¿›è¡Œå¾®è°ƒã€‚*
    æœ‰è¶£çš„æ˜¯ï¼ŒOpenAIåˆ†äº«äº†ä»–ä»¬å‡çº§çš„RLHFæŠ€æœ¯ç»†èŠ‚ï¼Œä»¥ä½¿æ¨¡å‹å“åº”æ›´å‡†ç¡®ï¼Œå‡å°‘åç¦»å®‰å…¨ä¿æŠ¤æªæ–½çš„å¯èƒ½æ€§ã€‚
- en: After training a policy model (as with ChatGPT), RLHF is used in adversarial
    training, a process that trains a model on malicious examples intended to deceive
    the model in order to defend the model against such examples in the future. In
    the case of GPT-4, human domain experts across several fields rate the responses
    of the policy model to adversarial prompts. These responses are then used to train
    additional reward models that iteratively fine-tune the policy model, resulting
    in a model thatâ€™s less likely to give out dangerous, evasive, or inaccurate responses.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒä¸€ä¸ªç­–ç•¥æ¨¡å‹ï¼ˆå¦‚ ChatGPTï¼‰åï¼ŒRLHF è¢«ç”¨äºå¯¹æŠ—æ€§è®­ç»ƒï¼Œè¿™æ˜¯ä¸€ä¸ªé€šè¿‡æ¶æ„ç¤ºä¾‹è®­ç»ƒæ¨¡å‹çš„è¿‡ç¨‹ï¼Œç›®çš„æ˜¯è®©æ¨¡å‹é˜²å¾¡æœªæ¥çš„æ­¤ç±»ç¤ºä¾‹ã€‚ä»¥ GPT-4
    ä¸ºä¾‹ï¼Œæ¥è‡ªå¤šä¸ªé¢†åŸŸçš„äººç±»é¢†åŸŸä¸“å®¶å¯¹ç­–ç•¥æ¨¡å‹å¯¹æŠ—æ€§æç¤ºçš„å“åº”è¿›è¡Œè¯„åˆ†ã€‚è¿™äº›å“åº”éšåè¢«ç”¨æ¥è®­ç»ƒé¢å¤–çš„å¥–åŠ±æ¨¡å‹ï¼Œè¿­ä»£åœ°å¾®è°ƒç­–ç•¥æ¨¡å‹ï¼Œæœ€ç»ˆå¾—åˆ°ä¸€ä¸ªä¸å®¹æ˜“äº§ç”Ÿå±é™©ã€å›é¿æˆ–ä¸å‡†ç¡®å›ç­”çš„æ¨¡å‹ã€‚
- en: '![](../Images/3f22d2d1c180b28800d23f55ed6a6a46.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f22d2d1c180b28800d23f55ed6a6a46.png)'
- en: Image created by the author.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±ä½œè€…åˆ›å»ºçš„å›¾åƒã€‚
- en: 'ChatGPT vs. GPT-4: Similarities & differences in performance and capabilities'
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ChatGPT ä¸ GPT-4ï¼šæ€§èƒ½å’Œèƒ½åŠ›çš„ç›¸ä¼¼æ€§ä¸å·®å¼‚
- en: Capabilities
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: èƒ½åŠ›
- en: In terms of capabilities, ChatGPT and GPT-4 are more similar than they are different.
    Like its predecessor, GPT-4 also interacts in a conversational style that aims
    to align with the user. As you can see below, the responses between the two models
    for a broad question are very similar.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨èƒ½åŠ›æ–¹é¢ï¼ŒChatGPT å’Œ GPT-4 æ¯”è¾ƒç±»ä¼¼ï¼Œè€Œä¸æ˜¯ä¸åŒã€‚åƒå…¶å‰èº«ä¸€æ ·ï¼ŒGPT-4 ä¹Ÿä»¥å¯¹è¯é£æ ¼è¿›è¡Œäº’åŠ¨ï¼Œæ—¨åœ¨ä¸ç”¨æˆ·å¯¹é½ã€‚å¦‚ä¸‹é¢æ‰€ç¤ºï¼Œå¯¹äºä¸€ä¸ªå¹¿æ³›çš„é—®é¢˜ï¼Œä¸¤è€…çš„å›åº”éå¸¸ç›¸ä¼¼ã€‚
- en: '![](../Images/678e8b3ff33d0bfa3463a090d0379362.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/678e8b3ff33d0bfa3463a090d0379362.png)'
- en: Image created by the author.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±ä½œè€…åˆ›å»ºçš„å›¾åƒã€‚
- en: OpenAI agrees that the distinction between the models can be subtle and claims
    that â€œdifference comes out when the complexity of the task reaches a sufficient
    threshold.â€ Given the six months of adversarial training the GPT-4 base model
    underwent in its post-training phase, this is probably an accurate characterization.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI åŒæ„æ¨¡å‹ä¹‹é—´çš„åŒºåˆ«å¯èƒ½å¾ˆå¾®å¦™ï¼Œå¹¶å£°ç§°â€œå½“ä»»åŠ¡çš„å¤æ‚æ€§è¾¾åˆ°è¶³å¤Ÿçš„é˜ˆå€¼æ—¶ï¼ŒåŒºåˆ«æ‰ä¼šæ˜¾ç°ã€‚â€é‰´äº GPT-4 åŸºç¡€æ¨¡å‹åœ¨å…¶åè®­ç»ƒé˜¶æ®µç»å†äº†å…­ä¸ªæœˆçš„å¯¹æŠ—æ€§è®­ç»ƒï¼Œè¿™å¤§æ¦‚æ˜¯ä¸€ä¸ªå‡†ç¡®çš„æè¿°ã€‚
- en: Unlike ChatGPT, which accepts only text, GPT-4 accepts prompts composed of both
    images and text, returning textual responses. As of the publishing of this article,
    unfortunately, the capacity for using image inputs is not yet available to the
    public.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸åªæ¥å—æ–‡æœ¬çš„ ChatGPT ä¸åŒï¼ŒGPT-4 æ¥å—ç”±å›¾åƒå’Œæ–‡æœ¬ç»„æˆçš„æç¤ºï¼Œè¿”å›æ–‡æœ¬å›åº”ã€‚æˆªè‡³æœ¬æ–‡å‘å¸ƒæ—¶ï¼Œä¸å¹¸çš„æ˜¯ï¼Œä½¿ç”¨å›¾åƒè¾“å…¥çš„åŠŸèƒ½å°šæœªå‘å…¬ä¼—å¼€æ”¾ã€‚
- en: Performance
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ€§èƒ½
- en: As referenced earlier, OpenAI reports significant improvement in safety performance
    for GPT-4, compared to GPT-3.5 (from which ChatGPT was fine-tuned). However, whether
    the reduction in responses to requests for disallowed content, reduction in toxic
    content generation, and improved responses to sensitive topics are due to the
    GPT-4 model itself or the additional adversarial testing is unclear at this time.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼ŒOpenAI æŠ¥å‘Šç§°ï¼Œç›¸æ¯”äº GPT-3.5ï¼ˆChatGPT æ˜¯åœ¨æ­¤åŸºç¡€ä¸Šå¾®è°ƒçš„ï¼‰ï¼ŒGPT-4 åœ¨å®‰å…¨æ€§è¡¨ç°ä¸Šæœ‰æ˜¾è‘—æ”¹è¿›ã€‚ç„¶è€Œï¼Œç›®å‰å°šä¸æ¸…æ¥šå¯¹ç¦æ­¢å†…å®¹è¯·æ±‚çš„å“åº”å‡å°‘ã€æ¯’æ€§å†…å®¹ç”Ÿæˆå‡å°‘å’Œå¯¹æ•æ„Ÿè¯é¢˜çš„æ”¹å–„æ˜¯å¦ç”±äº
    GPT-4 æ¨¡å‹æœ¬èº«è¿˜æ˜¯é¢å¤–çš„å¯¹æŠ—æ€§æµ‹è¯•ã€‚
- en: Additionally, GPT-4 outperforms GPT-3.5 on most academic and professional exams
    taken by humans. Notably, GPT-4 scores in the 90th percentile on the Uniform Bar
    Exam compared to GPT-3.5, which scores in the 10th percentile. GPT-4 also significantly
    outperforms its predecessor on traditional language model benchmarks as well as
    other SOTA models (although sometimes just barely).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼ŒGPT-4 åœ¨å¤§å¤šæ•°ç”±äººç±»å‚åŠ çš„å­¦æœ¯å’ŒèŒä¸šè€ƒè¯•ä¸­è¡¨ç°ä¼˜äº GPT-3.5ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒGPT-4 åœ¨ç»Ÿä¸€å¾‹å¸ˆè€ƒè¯•ä¸­å¾—åˆ†åœ¨ç¬¬ 90 ä¸ªç™¾åˆ†ä½ï¼Œè€Œ
    GPT-3.5 çš„å¾—åˆ†åœ¨ç¬¬ 10 ä¸ªç™¾åˆ†ä½ã€‚GPT-4 åœ¨ä¼ ç»Ÿè¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•å’Œå…¶ä»– SOTA æ¨¡å‹ä¸Šä¹Ÿæ˜¾è‘—ä¼˜äºå…¶å‰èº«ï¼ˆå°½ç®¡æœ‰æ—¶åªæ˜¯ç•¥å¾®ï¼‰ã€‚
- en: 'ChatGPT vs. GPT-4: Similarities & differences in limitations'
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ChatGPT ä¸ GPT-4ï¼šé™åˆ¶çš„ç›¸ä¼¼æ€§ä¸å·®å¼‚
- en: Both ChatGPT and GPT-4 have significant limitations and risks. The GPT-4 System
    Card includes insights from a detailed exploration of such risks conducted by
    OpenAI.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT å’Œ GPT-4 éƒ½æœ‰æ˜¾è‘—çš„é™åˆ¶å’Œé£é™©ã€‚GPT-4 ç³»ç»Ÿå¡åŒ…æ‹¬ OpenAI å¯¹è¿™äº›é£é™©çš„è¯¦ç»†æ¢ç´¢æ‰€å¾—çš„è§è§£ã€‚
- en: 'These are just a few of the risks associated with both models:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›åªæ˜¯ä¸è¿™ä¸¤ä¸ªæ¨¡å‹ç›¸å…³çš„ä¸€äº›é£é™©ï¼š
- en: Hallucination (the tendency to produce nonsensical or factually inaccurate content)
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¹»è§‰ï¼ˆäº§ç”Ÿæ— æ„ä¹‰æˆ–äº‹å®ä¸å‡†ç¡®å†…å®¹çš„å€¾å‘ï¼‰
- en: Producing harmful content that violates OpenAIâ€™s policies (e.g. hate speech,
    incitements to violence)
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”Ÿæˆè¿å OpenAI æ”¿ç­–çš„æœ‰å®³å†…å®¹ï¼ˆä¾‹å¦‚ä»‡æ¨è¨€è®ºã€ç…½åŠ¨æš´åŠ›ï¼‰
- en: Amplifying and perpetuating stereotypes of marginalized people
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ”¾å¤§å’Œå»¶ç»­è¾¹ç¼˜åŒ–ç¾¤ä½“çš„åˆ»æ¿å°è±¡
- en: Generating realistic disinformation intended to deceive
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç”Ÿæˆæ—¨åœ¨æ¬ºéª—çš„é€¼çœŸè™šå‡ä¿¡æ¯
- en: While ChatGPT and GPT-4 struggle with the same limitations and risks, OpenAI
    has made special efforts, including extensive adversarial testing, to mitigate
    them for GPT-4\. While this is encouraging, the GPT-4 System Card ultimately demonstrates
    how vulnerable ChatGPT was (and possibly still is). For a more detailed explanation
    of harmful unintended consequences, I recommend reading the GPT-4 System Card,
    which starts on page 38 of the [GPT-4 Technical Report](https://cdn.openai.com/papers/gpt-4.pdf).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶ ChatGPT å’Œ GPT-4 é‡åˆ°ç›¸åŒçš„å±€é™æ€§å’Œé£é™©ï¼ŒOpenAI å·²åšå‡ºç‰¹åˆ«åŠªåŠ›ï¼ŒåŒ…æ‹¬å¹¿æ³›çš„å¯¹æŠ—æ€§æµ‹è¯•ï¼Œä»¥å‡è½»è¿™äº›é—®é¢˜å¯¹ GPT-4 çš„å½±å“ã€‚è™½ç„¶è¿™ä»¤äººé¼“èˆï¼Œä½†
    GPT-4 ç³»ç»Ÿå¡ç‰‡æœ€ç»ˆå±•ç¤ºäº† ChatGPT çš„è„†å¼±æ€§ï¼ˆä»¥åŠå¯èƒ½ä»ç„¶å­˜åœ¨çš„è„†å¼±æ€§ï¼‰ã€‚æœ‰å…³æœ‰å®³æ„å¤–åæœçš„æ›´è¯¦ç»†è§£é‡Šï¼Œæˆ‘æ¨èé˜…è¯» GPT-4 ç³»ç»Ÿå¡ç‰‡ï¼Œè¯¥æ–‡æ¡£ä»
    [GPT-4 æŠ€æœ¯æŠ¥å‘Š](https://cdn.openai.com/papers/gpt-4.pdf) ç¬¬ 38 é¡µå¼€å§‹ã€‚
- en: Conclusion
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: In this article, we review the most important similarities and differences between
    ChatGPT and GPT-4, including their training methods, performance and capabilities,
    and limitations and risks.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å›é¡¾äº† ChatGPT å’Œ GPT-4 ä¹‹é—´æœ€é‡è¦çš„ç›¸ä¼¼æ€§å’Œå·®å¼‚ï¼ŒåŒ…æ‹¬å®ƒä»¬çš„è®­ç»ƒæ–¹æ³•ã€æ€§èƒ½å’Œèƒ½åŠ›ï¼Œä»¥åŠé™åˆ¶å’Œé£é™©ã€‚
- en: While we know much less about the model architecture and training methods behind
    GPT-4, it appears to be a refined version of ChatGPT that now accepts image and
    text inputs and claims to be safer, more accurate, and more creative. Unfortunately,
    we will have to take OpenAIâ€™s word for it, as GPT-4 is only available as part
    of the ChatGPT Plus subscription.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æˆ‘ä»¬å¯¹ GPT-4 èƒŒåçš„æ¨¡å‹æ¶æ„å’Œè®­ç»ƒæ–¹æ³•çŸ¥ä¹‹ç”šå°‘ï¼Œä½†å®ƒä¼¼ä¹æ˜¯ ChatGPT çš„ä¸€ä¸ªæ”¹è¿›ç‰ˆæœ¬ï¼Œç°åœ¨æ”¯æŒå›¾åƒå’Œæ–‡æœ¬è¾“å…¥ï¼Œå¹¶å£°ç§°æ›´å®‰å…¨ã€æ›´å‡†ç¡®ã€æ›´å…·åˆ›é€ æ€§ã€‚ä¸å¹¸çš„æ˜¯ï¼Œæˆ‘ä»¬åªèƒ½å¬ä»
    OpenAI çš„è¯´æ³•ï¼Œå› ä¸º GPT-4 ä»…ä½œä¸º ChatGPT Plus è®¢é˜…çš„ä¸€éƒ¨åˆ†æä¾›ã€‚
- en: 'The table below illustrates the most important similarities and differences
    between ChatGPT and GPT-4:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹è¡¨å±•ç¤ºäº† ChatGPT å’Œ GPT-4 ä¹‹é—´æœ€é‡è¦çš„ç›¸ä¼¼æ€§å’Œå·®å¼‚ï¼š
- en: '![](../Images/38414f3c07eba431c9d5bd868106ee91.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/38414f3c07eba431c9d5bd868106ee91.png)'
- en: Image created by the author.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…åˆ›å»ºã€‚
- en: The race for creating the most accurate and dynamic large language models has
    reached breakneck speed, with the release of ChatGPT and GPT-4 within mere months
    of each other. Staying informed on the advancements, risks, and limitations of
    these models is essential as we navigate this exciting but rapidly evolving landscape
    of large language models.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ›å»ºæœ€å‡†ç¡®å’ŒåŠ¨æ€çš„å¤§å‹è¯­è¨€æ¨¡å‹çš„ç«èµ›å·²è¾¾åˆ°å¿«é€Ÿå‘å±•çš„é˜¶æ®µï¼ŒChatGPT å’Œ GPT-4 çš„å‘å¸ƒä»…ç›¸éš”æ•°æœˆã€‚äº†è§£è¿™äº›æ¨¡å‹çš„è¿›å±•ã€é£é™©å’Œå±€é™æ€§æ˜¯è‡³å…³é‡è¦çš„ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨è¿™ä¸ªä»¤äººå…´å¥‹ä½†å¿«é€Ÿå˜åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹é¢†åŸŸä¸­èˆªè¡Œã€‚
- en: '*If youâ€™d like to stay up-to-date on the latest data science trends, technologies,
    and packages, consider becoming a Medium member. Youâ€™ll get unlimited access to
    articles and blogs like Towards Data Science and youâ€™ll be supporting my writing.
    (I earn a small commission for each membership).*'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¦‚æœä½ å¸Œæœ›è·Ÿä¸Šæœ€æ–°çš„æ•°æ®ç§‘å­¦è¶‹åŠ¿ã€æŠ€æœ¯å’Œå·¥å…·ï¼Œå¯ä»¥è€ƒè™‘æˆä¸º Medium ä¼šå‘˜ã€‚ä½ å°†è·å¾—å¯¹åƒ Towards Data Science è¿™æ ·çš„æ–‡ç« å’Œåšå®¢çš„æ— é™è®¿é—®æƒï¼Œå¹¶ä¸”ä½ ä¹Ÿåœ¨æ”¯æŒæˆ‘çš„å†™ä½œã€‚ï¼ˆæˆ‘æ¯å–å‡ºä¸€ä»½ä¼šå‘˜èµ„æ ¼éƒ½ä¼šè·å¾—å°‘é‡ä½£é‡‘ï¼‰ã€‚*'
- en: '[](https://medium.com/@mary.newhauser/membership?source=post_page-----35c990c133c5--------------------------------)
    [## Join Medium with my referral link - Mary Newhauser'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mary.newhauser/membership?source=post_page-----35c990c133c5--------------------------------)
    [## é€šè¿‡æˆ‘çš„æ¨èé“¾æ¥åŠ å…¥ Medium - Mary Newhauser'
- en: Get access to unlimited Medium articles for $5 per month ğŸ¤— Your membership fee
    directly supports Mary Newhauser andâ€¦
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä»¥æ¯æœˆ 5 ç¾å…ƒè·å–æ— é™ Medium æ–‡ç«  ğŸ¤— ä½ çš„ä¼šå‘˜è´¹ç”¨ç›´æ¥æ”¯æŒ Mary Newhauser å’Œâ€¦
- en: medium.com](https://medium.com/@mary.newhauser/membership?source=post_page-----35c990c133c5--------------------------------)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@mary.newhauser/membership?source=post_page-----35c990c133c5--------------------------------)
- en: Want to connect?
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æƒ³è¦è”ç³»æˆ‘ï¼Ÿ
- en: ğŸ“– Follow me on [Medium](https://medium.com/@mary.newhauser)
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğŸ“– åœ¨ [Medium](https://medium.com/@mary.newhauser) ä¸Šå…³æ³¨æˆ‘
- en: ğŸ’Œ [Subscribe](https://medium.com/@mary.newhauser/subscribe) to get an email
    whenever I publish
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğŸ’Œ [è®¢é˜…](https://medium.com/@mary.newhauser/subscribe) ä»¥åœ¨æˆ‘å‘å¸ƒæ–°å†…å®¹æ—¶æ”¶åˆ°é‚®ä»¶
- en: ğŸ–Œï¸ Check out my generative AI [blog](https://www.gptechblog.com/)
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğŸ–Œï¸ æŸ¥çœ‹æˆ‘çš„ç”Ÿæˆå¼ AI [åšå®¢](https://www.gptechblog.com/)
- en: ğŸ”— Take a look at my [portfolio](https://www.datascienceportfol.io/marynewhauser)
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğŸ”— æŸ¥çœ‹æˆ‘çš„ [ä½œå“é›†](https://www.datascienceportfol.io/marynewhauser)
- en: ğŸ‘©â€ğŸ« Iâ€™m also a data science [coach](https://www.datajump.co/)!
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğŸ‘©â€ğŸ« æˆ‘ä¹Ÿæ˜¯ä¸€åæ•°æ®ç§‘å­¦ [æ•™ç»ƒ](https://www.datajump.co/)ï¼
- en: 'Iâ€™ve also written:'
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æˆ‘è¿˜å†™è¿‡ï¼š
- en: '[](https://medium.com/nlplanet/fine-tuning-distilbert-on-senator-tweets-a6f2425ca50e?source=post_page-----35c990c133c5--------------------------------)
    [## Fine-tuning DistilBERT on senator tweets'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/nlplanet/fine-tuning-distilbert-on-senator-tweets-a6f2425ca50e?source=post_page-----35c990c133c5--------------------------------)
    [## å¾®è°ƒ DistilBERT ä»¥å¤„ç†å‚è®®å‘˜æ¨æ–‡'
- en: A guide to fine-tuning DistilBERT on the tweets of American Senators with snscrape,
    SQLite, and Transformers (PyTorch)â€¦
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä¸€ä»½å…³äºå¦‚ä½•ä½¿ç”¨ snscrapeã€SQLite å’Œ Transformers (PyTorch) å¾®è°ƒ DistilBERTï¼Œä»¥å¤„ç†ç¾å›½å‚è®®å‘˜çš„æ¨æ–‡çš„æŒ‡å—â€¦â€¦
- en: medium.com](https://medium.com/nlplanet/fine-tuning-distilbert-on-senator-tweets-a6f2425ca50e?source=post_page-----35c990c133c5--------------------------------)
    [](/making-the-jump-from-data-analyst-to-data-scientist-in-2023-74e2cf7fc139?source=post_page-----35c990c133c5--------------------------------)
    [## Making the Jump from Data Analyst to Data Scientist in 2023
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/nlplanet/fine-tuning-distilbert-on-senator-tweets-a6f2425ca50e?source=post_page-----35c990c133c5--------------------------------)
    [](/making-the-jump-from-data-analyst-to-data-scientist-in-2023-74e2cf7fc139?source=post_page-----35c990c133c5--------------------------------)
    [## ä»æ•°æ®åˆ†æå¸ˆåˆ°æ•°æ®ç§‘å­¦å®¶çš„è·³è·ƒï¼ˆ2023 å¹´ï¼‰
- en: The skills and resources you need to transition from a data analyst to data
    scientist position.
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½ éœ€è¦çš„æŠ€èƒ½å’Œèµ„æºï¼Œä»¥ä»æ•°æ®åˆ†æå¸ˆèŒä½è½¬å˜ä¸ºæ•°æ®ç§‘å­¦å®¶èŒä½ã€‚
- en: towardsdatascience.com](/making-the-jump-from-data-analyst-to-data-scientist-in-2023-74e2cf7fc139?source=post_page-----35c990c133c5--------------------------------)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/making-the-jump-from-data-analyst-to-data-scientist-in-2023-74e2cf7fc139?source=post_page-----35c990c133c5--------------------------------)
- en: References
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: (1) OpenAI, [Introducing ChatGPT](https://openai.com/blog/chatgpt) (2022).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: (1) OpenAI, [ä»‹ç» ChatGPT](https://openai.com/blog/chatgpt) (2022)ã€‚
- en: (2) OpenAI, [GPT-4](https://openai.com/research/gpt-4) (2023).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: (2) OpenAI, [GPT-4](https://openai.com/research/gpt-4) (2023)ã€‚
- en: (3) A. Radford et al., [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)
    (2019).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: (3) A. Radford ç­‰, [è¯­è¨€æ¨¡å‹æ˜¯æ— ç›‘ç£çš„å¤šä»»åŠ¡å­¦ä¹ è€…](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)
    (2019)ã€‚
- en: (4) T. Brown et al., [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)
    (2020).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: (4) T. Brown ç­‰, [è¯­è¨€æ¨¡å‹æ˜¯å°‘é‡æ ·æœ¬å­¦ä¹ è€…](https://arxiv.org/pdf/2005.14165.pdf) (2020)ã€‚
- en: (5) J. Alammar, [The Illustrated GPT-2 (Visualizing Transformer Language Models)](https://jalammar.github.io/illustrated-gpt2/)
    (2019).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: (5) J. Alammar, [æ’å›¾ç‰ˆ GPT-2ï¼ˆå¯è§†åŒ– Transformer è¯­è¨€æ¨¡å‹ï¼‰](https://jalammar.github.io/illustrated-gpt2/)
    (2019)ã€‚
- en: (6) OpenAI, [GPT-4 Technical Report](https://cdn.openai.com/papers/gpt-4.pdf)
    (2023).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: (6) OpenAI, [GPT-4 æŠ€æœ¯æŠ¥å‘Š](https://cdn.openai.com/papers/gpt-4.pdf) (2023)ã€‚
- en: (7) OpenAI, [Learning from human preferences](https://openai.com/research/learning-from-human-preferences)
    (2017).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: (7) OpenAI, [ä»äººç±»åå¥½ä¸­å­¦ä¹ ](https://openai.com/research/learning-from-human-preferences)
    (2017)ã€‚
