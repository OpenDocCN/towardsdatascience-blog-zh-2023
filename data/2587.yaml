- en: Spoken language recognition on Mozilla Common Voice — Audio Transformations.
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Mozilla Common Voice 上的口语语言识别——音频变换。
- en: 原文：[https://towardsdatascience.com/spoken-language-recognition-on-mozilla-common-voice-audio-transformations-24d5ceaa832b?source=collection_archive---------1-----------------------#2023-08-13](https://towardsdatascience.com/spoken-language-recognition-on-mozilla-common-voice-audio-transformations-24d5ceaa832b?source=collection_archive---------1-----------------------#2023-08-13)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/spoken-language-recognition-on-mozilla-common-voice-audio-transformations-24d5ceaa832b?source=collection_archive---------1-----------------------#2023-08-13](https://towardsdatascience.com/spoken-language-recognition-on-mozilla-common-voice-audio-transformations-24d5ceaa832b?source=collection_archive---------1-----------------------#2023-08-13)
- en: '[](https://medium.com/@sergeyvilov?source=post_page-----24d5ceaa832b--------------------------------)[![Sergey
    Vilov](../Images/42efe223e2aa575250e050cf3660cf20.png)](https://medium.com/@sergeyvilov?source=post_page-----24d5ceaa832b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----24d5ceaa832b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----24d5ceaa832b--------------------------------)
    [Sergey Vilov](https://medium.com/@sergeyvilov?source=post_page-----24d5ceaa832b--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@sergeyvilov?source=post_page-----24d5ceaa832b--------------------------------)[![Sergey
    Vilov](../Images/42efe223e2aa575250e050cf3660cf20.png)](https://medium.com/@sergeyvilov?source=post_page-----24d5ceaa832b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----24d5ceaa832b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----24d5ceaa832b--------------------------------)
    [Sergey Vilov](https://medium.com/@sergeyvilov?source=post_page-----24d5ceaa832b--------------------------------)'
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F33297faf768d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspoken-language-recognition-on-mozilla-common-voice-audio-transformations-24d5ceaa832b&user=Sergey+Vilov&userId=33297faf768d&source=post_page-33297faf768d----24d5ceaa832b---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----24d5ceaa832b--------------------------------)
    ·5 min read·Aug 13, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F24d5ceaa832b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspoken-language-recognition-on-mozilla-common-voice-audio-transformations-24d5ceaa832b&user=Sergey+Vilov&userId=33297faf768d&source=-----24d5ceaa832b---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F33297faf768d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspoken-language-recognition-on-mozilla-common-voice-audio-transformations-24d5ceaa832b&user=Sergey+Vilov&userId=33297faf768d&source=post_page-33297faf768d----24d5ceaa832b---------------------post_header-----------)
    发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----24d5ceaa832b--------------------------------)
    ·5 min read·2023年8月13日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F24d5ceaa832b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspoken-language-recognition-on-mozilla-common-voice-audio-transformations-24d5ceaa832b&user=Sergey+Vilov&userId=33297faf768d&source=-----24d5ceaa832b---------------------clap_footer-----------)'
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F24d5ceaa832b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspoken-language-recognition-on-mozilla-common-voice-audio-transformations-24d5ceaa832b&source=-----24d5ceaa832b---------------------bookmark_footer-----------)![](../Images/a05c695d6e175040ecd08602ecb19ba9.png)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F24d5ceaa832b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fspoken-language-recognition-on-mozilla-common-voice-audio-transformations-24d5ceaa832b&source=-----24d5ceaa832b---------------------bookmark_footer-----------)![](../Images/a05c695d6e175040ecd08602ecb19ba9.png)'
- en: Photo by [Kelly Sikkema](https://unsplash.com/@kellysikkema?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Kelly Sikkema](https://unsplash.com/@kellysikkema?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: This is the third article on spoken language recognition based on the [Mozilla
    Common Voice](https://commonvoice.mozilla.org/en) dataset. In [Part I](/spoken-language-recognition-on-mozilla-common-voice-part-i-3f5400bbbcd8),
    we discussed data selection and data preprocessing and in [Part II](/spoken-language-recognition-on-mozilla-common-voice-part-ii-models-b32780ea1ee4)
    we analysed performance of several neural network classifiers.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这是基于 [Mozilla Common Voice](https://commonvoice.mozilla.org/en) 数据集的第三篇关于语音语言识别的文章。在
    [第一部分](/spoken-language-recognition-on-mozilla-common-voice-part-i-3f5400bbbcd8)，我们讨论了数据选择和数据预处理，在
    [第二部分](/spoken-language-recognition-on-mozilla-common-voice-part-ii-models-b32780ea1ee4)
    中我们分析了几种神经网络分类器的性能。
- en: The final model achieved 92% accuracy and 97% pairwise accuracy. Since this
    model suffers from somewhat high variance, the accuracy could potentially be improved
    by adding more data. One very common way to get extra data is to synthesize it
    by performing various transformations on the available dataset.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最终模型达到了 92% 的准确率和 97% 的配对准确率。由于此模型存在较高的方差，通过添加更多数据可能会提高准确率。获取额外数据的一种非常常见的方法是通过对现有数据集执行各种变换来合成数据。
- en: 'In this article, we will consider 5 popular transformations for audio data
    augmentation: adding noise, changing speed, changing pitch, time masking, and
    cut & splice.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将考虑 5 种流行的音频数据增强变换：添加噪声、改变速度、改变音调、时间掩蔽和剪切 & 拼接。
- en: '**The tutorial notebook can be found** [**here**](https://github.com/sergeyvilov/ML-tutorials/blob/main/audio_transforms/audio_transforms.ipynb)**.**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**教程笔记本可以在** [**这里**](https://github.com/sergeyvilov/ML-tutorials/blob/main/audio_transforms/audio_transforms.ipynb)
    **找到**。'
- en: For illustration purposes, will use the sample *common_voice_en_100040* from
    the [Mozilla Common Voice](https://commonvoice.mozilla.org/) (MCV) dataset. This
    is the sentence *The burning fire had been extinguished*.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明，我们将使用来自 [Mozilla Common Voice](https://commonvoice.mozilla.org/)（MCV）数据集的样本
    *common_voice_en_100040*。这是句子 *The burning fire had been extinguished*。
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Original sample *common_voice_en_100040 from MCV.*
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 原始样本 *common_voice_en_100040 来自 MCV*。
- en: '![](../Images/5ae08da5e063d63b7a63fc9bd9a7ac5a.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5ae08da5e063d63b7a63fc9bd9a7ac5a.png)'
- en: Original signal waveform (image by the author)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 原始信号波形（作者提供的图像）
- en: Adding noise
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 添加噪声
- en: Adding noise is the simplest audio augmentation. The amount of noise is characterised
    by the signal-to-noise ratio (SNR) — the ratio between maximal signal amplitude
    and standard deviation of noise. We will generate several noise levels, defined
    with SNR, and see how they change the signal.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 添加噪声是最简单的音频增强方法。噪声量由信噪比（SNR）来表征——即最大信号幅度与噪声标准差的比率。我们将生成几个定义为 SNR 的噪声水平，并查看它们如何改变信号。
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Signals obtained by superimposing noise with SNR=5 and SNR=1000 on the original
    MCV sample common_voice_en_100040(generated by the author).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将噪声 SNR=5 和 SNR=1000 叠加到原始 MCV 样本 common_voice_en_100040 上获取的信号。
- en: '![](../Images/c4d4745c92215c63a55fa7f8bea5f45c.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c4d4745c92215c63a55fa7f8bea5f45c.png)'
- en: Signal waveform for several noise levels (image by the author)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 几种噪声水平的信号波形（作者提供的图像）
- en: So, SNR=1000 sounds almost like the unperturbed audio, while at SNR=5 one can
    only distinguish the strongest parts of the signal. In practice, the SNR level
    is hyperparameter that depends on the dataset and the chosen classifier.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，SNR=1000 听起来几乎像未受干扰的音频，而在 SNR=5 时只能区分信号的最强部分。在实践中，SNR 级别是一个超参数，取决于数据集和选择的分类器。
- en: Changing speed
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改变速度
- en: 'The simplest way to change the speed is just to pretend that the signal has
    a different sample rate. However, this will also change the pitch (how low/high
    in frequency the audio sounds). Increasing the sampling rate will make the voice
    sound higher. To illustrate this we shall “increase” the sampling rate for our
    example by 1.5:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 改变速度的最简单方法就是假装信号有不同的采样率。然而，这也会改变音调（声音的频率高低）。增加采样率会使声音听起来更高。为了说明这一点，我们将对我们的示例“增加”采样率
    1.5 倍：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Signal obtained by using a false sampling rate for the original MCV sample common_voice_en_100040(generated
    by the author).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 使用虚假采样率获取的信号用于原始 MCV 样本 common_voice_en_100040（作者生成）。
- en: Changing the speed without affecting the pitch is more challenging. One needs
    to use the [Phase Vocoder](https://en.wikipedia.org/wiki/Phase_vocoder)(PV) algorithm.
    In brief, the input signal is first split into overlapping frames. Then, the spectrum
    within each frame is computed by applying Fast Fourier Transformation (FFT). The
    playing speed is then modifyed by resynthetizing frames at a different rate. Since
    the frequency content of each frame is not affected, the pitch remains the same.
    The PV interpolates between the frames and uses the phase information to achieve
    smoothness.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 改变速度而不影响音高更具挑战性。需要使用[相位声码器](https://en.wikipedia.org/wiki/Phase_vocoder)(PV)算法。简言之，输入信号首先被分割成重叠的帧。然后，通过应用快速傅里叶变换（FFT）计算每帧内的频谱。播放速度通过以不同的速率重新合成帧来修改。由于每帧的频率内容未受影响，因此音高保持不变。PV在帧之间进行插值，并使用相位信息实现平滑。
- en: For our experiments, we will use the *stretch_wo_loop* time stretching function
    from [this](https://github.com/gaganbahga/time_stretch) PV implementation.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的实验，我们将使用来自[这个](https://github.com/gaganbahga/time_stretch)PV实现的*stretch_wo_loop*时间伸缩函数。
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Signal obtained by varying the speed of the original MCV sample common_voice_en_100040(generated
    by the author).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 通过改变原始MCV样本common_voice_en_100040的速度获得的信号（由作者生成）。
- en: '![](../Images/de5b95ce491cb4b12c56d1e58edd6779.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de5b95ce491cb4b12c56d1e58edd6779.png)'
- en: Signal waveform after speed increase (image by the author)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 速度增加后的信号波形（图片由作者提供）
- en: So, the duration of the signal decreased since we increased the speed. However,
    one can hear that the pitch has not changed. Note that when the stretching factor
    is substantial, the phase interpolation between frames might not work well. As
    a result, echo artefacts may appear in the transformed audio.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们增加了速度，所以信号的持续时间减少了。然而，可以听到音高没有变化。请注意，当伸缩因子很大时，帧间的相位插值可能效果不好。因此，变换后的音频可能会出现回声伪影。
- en: Changing pitch
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改变音高
- en: 'To alter the pitch without affecting the speed, we can use the same PV time
    stretch but pretend that the signal has a different sampling rate such that the
    total duration of the signal stays the same:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 要在不改变速度的情况下改变音高，我们可以使用相同的PV时间伸缩，但假装信号具有不同的采样率，以使信号的总持续时间保持不变：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Signal obtained by varying pitch of the original MCV sample common_voice_en_100040(generated
    by the author).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通过改变原始MCV样本common_voice_en_100040的音高获得的信号（由作者生成）。
- en: Why do we ever bother with this PV while [*librosa*](https://librosa.org/) already
    has *time_stretch* and *pitch_shift* functions? Well, these functions transform
    the signal back to the time domain. When you need to compute embeddings afterwards,
    you will lose time on redundant Fourier transforms. On the other hand, it is easy
    to modify the *stretch_wo_loop* functionsuch that it yields Fourier output without
    taking the inverse transform. One could probably also try to dig into *librosa*
    codes to achieve similar results.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们还要使用这个PV，而[*librosa*](https://librosa.org/)已经有*time_stretch*和*pitch_shift*函数？这些函数会将信号变换回时间域。当你需要后续计算嵌入时，你将浪费时间在冗余的傅里叶变换上。另一方面，很容易修改*stretch_wo_loop*函数，使其产生傅里叶输出而不进行逆变换。也可以尝试深入*librosa*代码以获得类似的结果。
- en: Time masking and cut & splice
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间掩蔽和切割&拼接
- en: These two transformation were initially proposed in the *frequency* domain ([Park
    et al. 2019](https://www.isca-speech.org/archive_v0/Interspeech_2019/pdfs/2680.pdf)).
    The idea was to save time on FFT by using precomputed spectra for audio augmentations.
    For simplicity, we will demonstrate how these transformations work in the *time*
    domain. The listed operations can be easily transferred to the frequency domain
    by replacing the time axis with frame indices.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种变换最初在*频率*域中提出（[Park等，2019](https://www.isca-speech.org/archive_v0/Interspeech_2019/pdfs/2680.pdf)）。其想法是通过使用预计算的频谱进行音频增强以节省FFT的时间。为了简单起见，我们将演示这些变换如何在*时间*域中工作。所列操作可以通过用帧索引替换时间轴轻松转移到频率域。
- en: Time masking
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间掩蔽
- en: The idea of time masking is to cover up a random region in the signal. The neural
    network has then less chances to learn signal-specific temporal variations that
    are not generalizable.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 时间掩蔽的想法是遮盖信号中的随机区域。神经网络将更少地学习到无法泛化的信号特定时间变化。
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Signal obtained by applying time mask transformation on the original MCV sample
    common_voice_en_100040(generated by the author).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对原始MCV样本common_voice_en_100040应用时间掩蔽变换获得的信号（由作者生成）。
- en: '![](../Images/93e8ac68243f0db355fe067d836a0c65.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/93e8ac68243f0db355fe067d836a0c65.png)'
- en: Signal waveform after time masking (the masked region is indicated with orange)
    (image by the author)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 时间掩蔽后的信号波形（掩蔽区域用橙色标示）（图片由作者提供）
- en: Cut & splice
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Cut & splice
- en: The idea is to replace a randomly selected region of the signal with a random
    fragment from another signal having the same label. The implementation is almost
    the same as for time masking, except that a piece of another signal is placed
    instead of the mask.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是用另一个具有相同标签的信号的随机片段替换信号的随机选定区域。实现几乎与时间掩蔽相同，只是用另一个信号的片段代替了掩蔽。
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Synthetic signal obtained by applying cut&splice transformation on the original
    MCV sample common_voice_en_100040 (generated by the author).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对原始MCV样本common_voice_en_100040（由作者生成）应用cut&splice变换得到的合成信号。
- en: '![](../Images/e3c3f47ff55952f337762f862acfcead.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e3c3f47ff55952f337762f862acfcead.png)'
- en: Signal waveform after cut&splice transformation (the inserted fragment from
    the other signal is indicated with orange) (image by the author)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: cut&splice变换后的信号波形（从其他信号中插入的片段用橙色标示）（图片由作者提供）
- en: 'The table below shows the accuracy of the [AttNN model](/spoken-language-recognition-on-mozilla-common-voice-part-ii-models-b32780ea1ee4)
    on the validation set for each of these transformations with typical values of
    parameters:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 下表显示了[AttNN模型](/spoken-language-recognition-on-mozilla-common-voice-part-ii-models-b32780ea1ee4)在验证集上对每个变换的准确率及其典型参数值：
- en: '![](../Images/70d59432a66adb1d9d311230e919c991.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/70d59432a66adb1d9d311230e919c991.png)'
- en: AttNN accuracy on Mozilla Common Voice dataset for each transformation with
    typical parameters (image by the author).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Mozilla Common Voice数据集上每个变换的AttNN准确率及其典型参数（图片由作者提供）。
- en: As can be seen, none of the transformations changed accuracy of our MCV-based
    spoken recognition setup significantly. It is, however, well possible that these
    transformations could boost performance on some other datasets. Lastly, when looking
    for optimal hyperparameters, it maskes sense to try these transformations one
    by one instead of random/grid search. After that, the effective transformations
    may be combined together.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如所见，这些变换没有显著改变我们基于MCV的语音识别系统的准确性。然而，这些变换有可能在某些其他数据集上提升性能。最后，在寻找最佳超参数时，逐个尝试这些变换而不是随机/网格搜索是有意义的。之后，可以将有效的变换结合在一起。
