- en: The Gradient Descent Algorithm and the Intuition Behind It
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降算法及其背后的直觉
- en: 原文：[https://towardsdatascience.com/the-gradient-descent-algorithm-4d54e0d446cd?source=collection_archive---------6-----------------------#2023-08-19](https://towardsdatascience.com/the-gradient-descent-algorithm-4d54e0d446cd?source=collection_archive---------6-----------------------#2023-08-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-gradient-descent-algorithm-4d54e0d446cd?source=collection_archive---------6-----------------------#2023-08-19](https://towardsdatascience.com/the-gradient-descent-algorithm-4d54e0d446cd?source=collection_archive---------6-----------------------#2023-08-19)
- en: A technical description of the Gradient Descent method, complemented with a
    graphical representation of the algorithm at work
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对梯度下降方法的技术描述，并配有算法运行的图示
- en: '[](https://medium.com/@fyi.oamd?source=post_page-----4d54e0d446cd--------------------------------)[![Antonieta
    Mastrogiuseppe](../Images/3b9e70a54fcb887f5ccee6d305085675.png)](https://medium.com/@fyi.oamd?source=post_page-----4d54e0d446cd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4d54e0d446cd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4d54e0d446cd--------------------------------)
    [Antonieta Mastrogiuseppe](https://medium.com/@fyi.oamd?source=post_page-----4d54e0d446cd--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@fyi.oamd?source=post_page-----4d54e0d446cd--------------------------------)[![Antonieta
    Mastrogiuseppe](../Images/3b9e70a54fcb887f5ccee6d305085675.png)](https://medium.com/@fyi.oamd?source=post_page-----4d54e0d446cd--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4d54e0d446cd--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4d54e0d446cd--------------------------------)
    [Antonieta Mastrogiuseppe](https://medium.com/@fyi.oamd?source=post_page-----4d54e0d446cd--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa8ee237975ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-gradient-descent-algorithm-4d54e0d446cd&user=Antonieta+Mastrogiuseppe&userId=a8ee237975ec&source=post_page-a8ee237975ec----4d54e0d446cd---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4d54e0d446cd--------------------------------)
    ·9 min read·Aug 19, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4d54e0d446cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-gradient-descent-algorithm-4d54e0d446cd&user=Antonieta+Mastrogiuseppe&userId=a8ee237975ec&source=-----4d54e0d446cd---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa8ee237975ec&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-gradient-descent-algorithm-4d54e0d446cd&user=Antonieta+Mastrogiuseppe&userId=a8ee237975ec&source=post_page-a8ee237975ec----4d54e0d446cd---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4d54e0d446cd--------------------------------)
    ·9 分钟阅读·2023年8月19日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4d54e0d446cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-gradient-descent-algorithm-4d54e0d446cd&user=Antonieta+Mastrogiuseppe&userId=a8ee237975ec&source=-----4d54e0d446cd---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4d54e0d446cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-gradient-descent-algorithm-4d54e0d446cd&source=-----4d54e0d446cd---------------------bookmark_footer-----------)![](../Images/6e6a647dfadc374fcf9425097033183a.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4d54e0d446cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-gradient-descent-algorithm-4d54e0d446cd&source=-----4d54e0d446cd---------------------bookmark_footer-----------)![](../Images/6e6a647dfadc374fcf9425097033183a.png)'
- en: “Once you’re over the hill you begin to pick up speed” by Arthur Schopenhauer.
    Photo taken by the author.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 亚瑟·叔本华的名言：“一旦你越过山丘，你会开始加速”。照片由作者拍摄。
- en: '**INTRODUCING SOME KEY DEFINITIONS**'
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**引入一些关键定义**'
- en: Within optimization methods, and in the first order algorithm type, one has
    certainly heard of the one known as Gradient Descent. It is of the first-order
    optimization type as it requires the first-order derivative, namely the gradient.
    By optimizing, gradient descent aims to minimize the difference between the “actual”
    output and the predicted output of the model as measured by the objective function,
    namely a cost function. The gradient, or slope, is defined as the direction of
    the line drawn by such function (curved or straight) at a given point of such
    line. Iteratively, gradient descent aims to differentiate the cost function at
    different points of the line so to derive the degree of change in direction at
    these points and hence take steps towards the steepest descent, namely the local
    minimum. As its name indicates, the *gradient* is used as the direction towards
    the steepest *descent* in search for the local minimum where the parameters’ values
    of the cost function being optimized are minimized hence at its lowest values.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在优化方法中，尤其是在一阶算法类型中，您肯定听说过一种被称为梯度下降的方法。它是一阶优化类型，因为它需要一阶导数，即梯度。通过优化，梯度下降旨在最小化“实际”输出和模型预测输出之间的差异，这种差异由目标函数（即成本函数）测量。梯度或斜率被定义为在该函数（弯曲或直线）在给定点处绘制的线的方向。迭代地，梯度下降旨在在不同点对成本函数进行微分，从而推导出这些点上方向变化的程度，并朝向最陡下降的方向，即局部最小值。正如其名字所示，*梯度*
    用作寻找局部最小值的*下降*方向，在那里优化的成本函数的参数值被最小化，从而达到最低值。
- en: Gradient Descent is mostly used (among others) to train machine learning models
    and deep learning models, the latter based on a neural network architectural type.
    From linear regression and logistic regression to neural networks, gradient descent
    aims to calculate the function’s best parameters values. In its simplest form,
    gradient descent aims to minimize the error term of the below linear regression
    by deriving the optimal values of the parameters for the independent variables.
    This is,
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降主要用于（包括但不限于）训练机器学习模型和深度学习模型，后者基于神经网络结构类型。从线性回归和逻辑回归到神经网络，梯度下降旨在计算函数的最佳参数值。在其最简单的形式中，梯度下降旨在通过推导独立变量的参数的最佳值来最小化下面线性回归的误差项。这是，
- en: y = β0 + β1 * X1 + … βk * Xk + Ɛ
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: y = β0 + β1 * X1 + … βk * Xk + Ɛ
- en: where,
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，
- en: '*y* is the dependent variable'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* 是因变量'
- en: '*k* number of independent variables'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*k* 是独立变量的数量'
- en: '*X* independent variable'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*X* 是独立变量'
- en: β parameter
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: β 参数
- en: '*Ɛ* error term component'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*Ɛ* 是误差项成分'
- en: 'In its more complex form, gradient descent is most frequently defined as the
    optimizer when training a deep learning model, specifically on the compiling phase.
    Deep learning is based on an interconnected network to learn and improve continuously,
    namely a neural network. Inspired by the human brain, a neural network is a highly
    complex network represented by artificial neurons, known as nodes. At the top
    level, the nodes have the important role to process and analyse the data from
    a node in the previous layer and pass it to another node in the next layer. In
    a neural network, a weight, namely the parameters of interest for optimization,
    are the link between nodes. They are the link between inputs/features and hidden
    layers hence they represent the importance of a given feature in predicting the
    final output. Finding the optimal value of a single weight depends on the value
    of many weights. And this optimization is taking placing for many weights at once,
    which in a deep neural network can be substantially large even in millions. Here
    is where gradient descent is found to perform highly efficiently on the large
    number of calculations involved, these based on the three main layers of a neural
    network: 1) Input Layer 2) Hidden Layer and 3) Output Layer.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在更复杂的形式中，梯度下降最常被定义为训练深度学习模型时的优化器，特别是在编译阶段。深度学习基于一个互连的网络进行不断的学习和改进，这个网络被称为神经网络。神经网络受到人脑的启发，是由人工神经元（称为节点）组成的高度复杂的网络。在最上层，节点在处理和分析来自前一层节点的数据，并将其传递到下一层节点方面扮演着重要角色。在神经网络中，权重，即优化的参数，是节点之间的联系。它们连接了输入/特征和隐藏层，因此它们表示了特定特征在预测最终输出时的重要性。找到单个权重的最佳值取决于许多权重的值。而这种优化是同时进行的，在深度神经网络中，即使是数百万个权重也可能会大大增加。在这里，梯度下降在涉及大量计算时表现得非常高效，这些计算基于神经网络的三个主要层：1）输入层
    2）隐藏层 和 3）输出层。
- en: There are numerous papers that properly detail and expand on methods such deep
    learning and methods that estimates the value of a function’s parameters hence
    expand on the difference between gradient descent and Ordinary Least Square (OLS),
    in the case of linear regression for example. As this is not the focus of this
    paper, the reader is prompted to investigate further and expand for a good understanding
    on such methodologies.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 有大量文献详细阐述和扩展了诸如深度学习方法及估计函数参数值的方法，进而扩展了梯度下降和普通最小二乘法（OLS）之间的差异，例如在线性回归的情况下。由于这不是本文的重点，读者可以进一步调查和扩展，以便更好地理解这些方法论。
- en: '**2\. TIME FOR SOME CALCULUS!**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 计算时间！**'
- en: 'For a good understanding of gradient descent, we need to expand on the definition
    of a differentiable function. A function, explicitly *ƒ(x),* is differentiable
    when the derivative can be defined at any point of the curved line derived by
    such function. This is, *for all* points in the domain of the function *ƒ(x).*
    Here, two concepts reinforce this definition: first-order derivative and second
    order derivative. The first-order derivative formula is defined as follow:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解梯度下降，我们需要扩展对可微分函数的定义。一个函数，明确地说是 *ƒ(x)*，当其导数在该函数的曲线上的任意点都可以定义时，该函数就是可微分的。这就是说，对于函数
    *ƒ(x)* 的定义域中的 *所有* 点。这里，有两个概念支持这个定义：一阶导数和二阶导数。一阶导数公式定义如下：
- en: '![](../Images/dbe88c79581bd5d59b2871fba3e17a34.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dbe88c79581bd5d59b2871fba3e17a34.png)'
- en: Strictly speaking, the first-order derivative of a function, denoted *ƒ’(x)*
    or *df(x)/dx*, is the slope of the function *ƒ(x)* at a given point value of x.
    If the slope is positive (negative), it indicates the function is increasing (decreasing)
    and by how much. A positive slope is an indication that as the value of x increases,
    so is the function *ƒ(x).* A negative slope, on the other hand, indicates that
    as the value of x increases, *ƒ(x)* decreases. The second-order derivative is
    the derivative of the derivative of the function *ƒ(x).* Denoted *ƒ’’(x)* or *d2f(x)/dx2*,
    the second derivative indicates the shape of the function *ƒ(x).* This is, whether
    such function is concave or convex. Mathematically speaking, (and this is important!!!)
    a second derivative will distinguish a relative maximum from a relative minimum.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 严格来说，函数的一阶导数，记作 *ƒ’(x)* 或 *df(x)/dx*，是函数 *ƒ(x)* 在给定 x 点的斜率。如果斜率为正（负），则表明函数在增加（减少），以及增加（减少）的幅度。正斜率表示随着
    x 值的增加，函数 *ƒ(x)* 也增加。相反，负斜率表示随着 x 值的增加，*ƒ(x)* 减少。二阶导数是函数 *ƒ(x)* 的导数的导数。记作 *ƒ’’(x)*
    或 *d2f(x)/dx2*，二阶导数指示了函数 *ƒ(x)* 的形状，即该函数是凹的还是凸的。从数学上讲（这一点很重要！！！），二阶导数可以区分相对最大值和相对最小值。
- en: where,
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，
- en: '*ƒ’’(x)* > 0 then *ƒ(x)* is convex at x = *a*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 *ƒ’’(x)* > 0，则 *ƒ(x)* 在 x = *a* 处是凸的。
- en: and if *ƒ’(a)* = 0 then *a* is a critical point hence a relative minimum
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 *ƒ’(a)* = 0，则 *a* 是一个临界点，因此是相对最小值。
- en: '![](../Images/92b0ff98218d34de751a3104e0fd8502.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/92b0ff98218d34de751a3104e0fd8502.png)'
- en: Graph drawn by the author
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图由作者绘制
- en: where,
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，
- en: '*ƒ’’(x)* < 0 then *ƒ(x)* is concave at x = *a*'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 *ƒ’’(x)* < 0，则 *ƒ(x)* 在 x = *a* 处是凹的。
- en: and if *ƒ’(a)* = 0 then *a* is a critical point hence a relative maximum
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 *ƒ’(a)* = 0，则 *a* 是一个临界点，因此是相对最大值。
- en: '![](../Images/15d54f3b0ef54d9df8b2ef80dba5c37b.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/15d54f3b0ef54d9df8b2ef80dba5c37b.png)'
- en: Graph drawn by the author
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图由作者绘制
- en: 'or if the second derivative is equal to zero, then either 1) the function *ƒ(x)*
    is in a turning point, known as Inflection point, where it changes from concave
    to convex or vice versa or 2) the function at that point is undefined (i.e., non-continuous).
    In the case of the former:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果二阶导数等于零，则可能有两种情况：1) 函数 *ƒ(x)* 处于转折点，即拐点，在该点函数从凹变为凸，或反之；2) 在该点函数未定义（即，不连续）。对于前者：
- en: '*ƒ’’(x)* = 0 then *ƒ(x)* is at an inflection point at x = *2*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 *ƒ’’(x)* = 0，则 *ƒ(x)* 在 x = *2* 处是拐点。
- en: '![](../Images/705d5e1f36ea883f6348237a83a1dea8.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/705d5e1f36ea883f6348237a83a1dea8.png)'
- en: Graph drawn by the author
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图由作者绘制
- en: The above has focused on a function with a single independent variable, namely
    a univariate function, y = *ƒ(x).* In the real world, one will be studying and
    modelling multivariable functions, where the variable under study is impacted
    by multiple factors, this is two or more independent variables, y = ƒ(x, z). To
    measure the impact of a change of the independent variable x in the dependent
    variable y, keeping z constant, the partial derivative of the function with respect
    to x is taken. Thus, partial derivatives calculate the rate of change in the cost
    function caused by a change in each of their inputs. Gradient descent iteratively
    calculates these changes in the cost function and at each different step updates
    the values of the parameters of such functions till reaching the minimum point
    where the value of such parameters is optimized hence the cost function is minimized.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 上述内容集中于一个具有单一自变量的函数，即一元函数 y = *ƒ(x)*。在现实世界中，人们会研究和建模多变量函数，其中研究的变量受到多个因素的影响，这就是两个或更多自变量
    y = ƒ(x, z)。为了测量自变量 x 对因变量 y 的影响，并保持 z 不变，需要计算函数关于 x 的偏导数。因此，偏导数计算了因每个输入的变化引起的成本函数的变化率。梯度下降迭代地计算这些成本函数的变化，并在每一步更新这些函数参数的值，直到达到使参数值优化、从而最小化成本函数的最小点。
- en: '**3\. THE GRADIENT DESCENT ALGORITHM AT WORK**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. 梯度下降算法的实际应用**'
- en: The larger the absolute value of the slope, the further we can step, and/or
    we can keep taking steps towards the steepest descent, namely the local minimum.
    As we approach the lowest/minimum point, the slope diminishes so one can take
    smaller steps until reaching a flat surface where the slope is equal to zero (0),
    *ƒ’(x)* = 0, this is lowest value of βi as pointed by the red arrow in the graph
    below. This is where the local minimum of the curved line is, and optimum values
    of the function’s parameters are derived.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 斜率的绝对值越大，我们可以迈出的步伐就越大，或者我们可以继续朝着最陡下降的方向迈步，即局部最小值。随着我们接近最低/最小点，斜率减小，因此可以迈出更小的步伐，直到到达斜率为零（0）的平坦表面，即*ƒ’(x)*
    = 0，这是下图中红色箭头指向的βi的最低值。这就是曲线的局部最小值，函数参数的最优值也在此处得出。
- en: '![](../Images/45f0e3a2e12d36989c395262e5df3fda.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/45f0e3a2e12d36989c395262e5df3fda.png)'
- en: Graph drawn by the author
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 作者绘制的图表
- en: Thus, if a function is *strictly* convex (concave), there will only be one critical
    point. Now, there is also the case where there are multiple local minima. In this
    case, the search is for the single lowest value the function can achieve. This
    is known as Global Minimum.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果一个函数是*严格*凸的（凹的），则只有一个临界点。现在，也有可能存在多个局部最小值的情况。在这种情况下，搜索的是函数能够达到的最低值，这被称为全局最小值。
- en: '![](../Images/83c582728280d36bd59544b797ffe5a8.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/83c582728280d36bd59544b797ffe5a8.png)'
- en: Graph drawn by the author
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 作者绘制的图表
- en: 'The following two key questions arise:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 以下两个关键问题出现：
- en: 1) In which direction to step?
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 应该朝哪个方向迈步？
- en: 2) How big the steps should be?
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 步长应该有多大？
- en: Let us recap what have we have said so far. Gradient descent is an algorithm,
    that while in the training phase of a model, iteratively adjusts hence optimizes
    the values of the function’s parameters by taking the partial derivative of the
    function with respect to each of its inputs at every step it takes towards the
    steepest descent, defined as the local minimum. If the derivative is positive,
    the function is increasing. Thus, steps should be taken opposite direction. The
    gradient indicates in which direction the step should be taken. If gradient is
    large, namely large slope absolute value, larger steps should be taken towards
    the local minimum. In fact, gradient descent takes increasingly smaller steps
    on the direction of the local minima within each iteration as shown by the blue
    arrows in the graph above.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下到目前为止的内容。梯度下降是一种算法，在模型的训练阶段，通过在每一步向最陡下降的方向（即局部最小值）前进时，对函数的每个输入取其偏导数，从而迭代地调整和优化函数参数的值。如果导数为正，函数值在增加。因此，应该朝相反方向迈步。梯度指示了应该迈步的方向。如果梯度很大，即斜率的绝对值很大，则应该朝着局部最小值迈较大的步伐。实际上，梯度下降在每次迭代中朝着局部最小值方向采取越来越小的步伐，如上图中的蓝色箭头所示。
- en: 'How big the step to take relates to the learning rate. This is how fast the
    algorithm learns/moves towards the steepest descent. At the highest gradient,
    large absolute value of the slope, the fastest the algorithm learns. As it approaches
    the local minimum, smaller the steps to take. Thus, learning rate value as any
    hyperparameter is set after trying different values so that the cost function
    decreases across iterations. If too big, the local minimum can be missed. A small
    learning rate might lead to small updates to the weights causing the model not
    to improve significantly. If too small, it will take time to reach convergence.
    Convergence is reached when the cost function does not longer decrease. Thus,
    the cost function is the indicator of the algorithm performance. In a multivariate
    function world, this is denoted:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 步长的大小与学习率有关。这决定了算法学习/移动到最陡下降方向的速度。在最高梯度下，即斜率的绝对值最大时，算法学习最快。当接近局部最小值时，步长会变小。因此，学习率作为超参数在尝试不同值后设置，以便成本函数在迭代中减少。如果学习率过大，可能会错过局部最小值。学习率过小可能导致权重更新较小，使模型没有显著改善。如果学习率过小，可能需要时间才能收敛。收敛是指成本函数不再减少。因此，成本函数是算法性能的指标。在多变量函数的世界中，这表示为：
- en: '![](../Images/7967ef48d2b7ebc592bb70eace9eb39e.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7967ef48d2b7ebc592bb70eace9eb39e.png)'
- en: where,
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，
- en: '*df/d*β partial derivative of the cost function with respect to the parameterβ'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*df/d*β 表示成本函数对参数 β 的偏导数。'
- en: '*m* number of data points'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*m* 数据点数量。'
- en: '*yi* actual values of the dependent/target variable for the i-th data point'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*yi* 是第 i 个数据点的实际依赖/目标变量值。'
- en: '*ŷi* predicted values by the model of the dependent/target variable for the
    i-th data point'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*ŷi* 是模型预测的第 i 个数据点的依赖/目标变量值。'
- en: '*xi* represents the i-th input associated with the data point.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*xi* 表示与数据点相关的第 i 个输入。'
- en: '![](../Images/650f42910dde7c968ff22971e7f731d7.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/650f42910dde7c968ff22971e7f731d7.png)'
- en: where,
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，
- en: '*▽f* represents the gradient vector of the function *f(x)* with respect to
    the parameters β'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*▽f* 表示函数 *f(x)* 对参数 β 的梯度向量。'
- en: '*df/d*βk represents the partial derivative of the function *f(x)* with respect
    to the k-th parameter β'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*df/d*βk 表示函数 *f(x)* 对第 k 个参数 β 的偏导数。'
- en: '![](../Images/a47daebf7247b61566e975b9730e8eef.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a47daebf7247b61566e975b9730e8eef.png)'
- en: were,
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，
- en: New β represents the current value of the i-th parameter β
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 新的 β 表示第 i 个参数 β 的当前值。
- en: Old β represents the updated value of the i-th parameter β
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 旧的 β 表示第 i 个参数 β 的更新值。
- en: '*n* is the *learning rate: length of the step to take!*'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*n* 是 *学习率：采取步长的长度！*'
- en: '*▽f* is the gradient vector pointing in the direction of the steepest descent
    of the function *f(x)* with respect to changes in the parameters β to minimize
    *f(x)*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*▽f* 是指向函数 *f(x)* 在参数 β 变化方向上的最陡下降方向的梯度向量，以最小化 *f(x)*。'
- en: '**4\. LIMITATIONS OF GRADIENT DESCENT**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**4\. 梯度下降的局限性**'
- en: One of the limitations of gradient descent is related to one of the criteria
    mentioned above where the function must be differentiable at every point of its
    domain. When this is not the case and the algorithm finds a point that is undefined,
    (i.e., non-continuous) then it fails.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降的一个局限性与上述提到的标准之一有关，即函数必须在其定义域的每一点都可微分。当情况不是这样，算法找到一个未定义的点（即不连续的点）时，算法会失败。
- en: Another limitation is related to the size of the steps, namely the learning
    rate (*n*), taken towards the steepest descent. If too large it is likely to miss
    the local minimum or even not reach converge at all. If too small, it will take
    much longer to converge. If the number of inputs is large, this becomes even more
    problematic.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个局限性与步长的大小有关，即学习率 (*n*)，即朝着最陡下降方向采取的步伐。如果步长过大，可能会错过局部最小值，甚至可能无法收敛。如果步长过小，则需要更长的时间才能收敛。如果输入数量很大，这种情况会更加严重。
- en: Finally, gradient descent might never find the global minimum. The algorithm
    is not able to distinguish between a local and global minimum. As it descent in
    search of the local minimum, once it converges it will then stop. The local minimum
    will corner the algorithm in the local minimum own valley preventing the step
    to be large enough to exit.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，梯度下降可能永远无法找到全局最小值。算法无法区分局部最小值和全局最小值。当算法在寻找局部最小值时，一旦收敛就会停止。局部最小值会把算法困在局部最小值所在的谷底，阻止步伐足够大以便退出。
- en: '**5\. CONCLUSIONS**'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**5\. 结论**'
- en: 'In summary, gradient descent is:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，梯度下降是：
- en: 1) An iterative, first-order optimization algorithm type
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 一种迭代的、一阶优化算法类型。
- en: 2) Within each iteration, the parameters of the differentiable function are
    updated, and the cost function is minimized.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 在每次迭代中，对可微函数的参数进行更新，并最小化成本函数。
- en: 3) Thus, convergence is reached on the local minimum.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 因此，收敛于局部最小值。
- en: Based on the limitations of the gradient descent, there are motivations to explore
    different and more advanced type of gradient descent methods or even other types
    of algorithms in the realm of optimization such as the second-order type. This,
    however, will go out of the scope of this article hence I will leave it as a topic
    for my next article 😊
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 基于梯度下降法的局限性，有动机探索不同的、更先进的梯度下降方法，甚至其他类型的优化算法，如二阶方法。然而，这超出了本文的范围，因此我会把它留作我下一篇文章的主题
    😊
- en: Thanks for reading!
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: Please add any comment that you feel enhances the knowledge in the topic!
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 请添加任何你认为能增强该主题知识的评论！
