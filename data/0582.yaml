- en: Behind the Scenes of a Deep Learning Neural Network for Image Classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习神经网络在图像分类中的幕后故事
- en: 原文：[https://towardsdatascience.com/behind-the-scenes-of-a-deep-learning-neural-network-for-image-classification-5222aee3231d?source=collection_archive---------5-----------------------#2023-02-10](https://towardsdatascience.com/behind-the-scenes-of-a-deep-learning-neural-network-for-image-classification-5222aee3231d?source=collection_archive---------5-----------------------#2023-02-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/behind-the-scenes-of-a-deep-learning-neural-network-for-image-classification-5222aee3231d?source=collection_archive---------5-----------------------#2023-02-10](https://towardsdatascience.com/behind-the-scenes-of-a-deep-learning-neural-network-for-image-classification-5222aee3231d?source=collection_archive---------5-----------------------#2023-02-10)
- en: Is it magic or linear algebra and calculus?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这是魔法还是线性代数和微积分？
- en: '[](https://brunocaraffa.medium.com/?source=post_page-----5222aee3231d--------------------------------)[![Bruno
    Caraffa](../Images/414f88c6974dba79ec9851c051eff49f.png)](https://brunocaraffa.medium.com/?source=post_page-----5222aee3231d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5222aee3231d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5222aee3231d--------------------------------)
    [Bruno Caraffa](https://brunocaraffa.medium.com/?source=post_page-----5222aee3231d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://brunocaraffa.medium.com/?source=post_page-----5222aee3231d--------------------------------)[![布鲁诺·卡拉法](../Images/414f88c6974dba79ec9851c051eff49f.png)](https://brunocaraffa.medium.com/?source=post_page-----5222aee3231d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5222aee3231d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5222aee3231d--------------------------------)
    [布鲁诺·卡拉法](https://brunocaraffa.medium.com/?source=post_page-----5222aee3231d--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F657289ec5532&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbehind-the-scenes-of-a-deep-learning-neural-network-for-image-classification-5222aee3231d&user=Bruno+Caraffa&userId=657289ec5532&source=post_page-657289ec5532----5222aee3231d---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5222aee3231d--------------------------------)
    ·16 min read·Feb 10, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5222aee3231d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbehind-the-scenes-of-a-deep-learning-neural-network-for-image-classification-5222aee3231d&user=Bruno+Caraffa&userId=657289ec5532&source=-----5222aee3231d---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F657289ec5532&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbehind-the-scenes-of-a-deep-learning-neural-network-for-image-classification-5222aee3231d&user=Bruno+Caraffa&userId=657289ec5532&source=post_page-657289ec5532----5222aee3231d---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5222aee3231d--------------------------------)
    ·16分钟阅读·2023年2月10日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5222aee3231d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbehind-the-scenes-of-a-deep-learning-neural-network-for-image-classification-5222aee3231d&user=Bruno+Caraffa&userId=657289ec5532&source=-----5222aee3231d---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5222aee3231d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbehind-the-scenes-of-a-deep-learning-neural-network-for-image-classification-5222aee3231d&source=-----5222aee3231d---------------------bookmark_footer-----------)![](../Images/e6ae84b0e2b26d22873a0927970063ac.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5222aee3231d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fbehind-the-scenes-of-a-deep-learning-neural-network-for-image-classification-5222aee3231d&source=-----5222aee3231d---------------------bookmark_footer-----------)![](../Images/e6ae84b0e2b26d22873a0927970063ac.png)'
- en: Photo by [Pietro Jeng](https://unsplash.com/@pietrozj?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [皮耶特罗·詹](https://unsplash.com/@pietrozj?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: 'Deep Learning Neural Networks are getting a lot of attention lately and for
    a good reason. It’s the technology behind speech recognition, face detection,
    voice control, autonomous cars, brain tumor detection, and that kind of technologies
    that 20 years ago were not part of our lives. As complex as those networks seem
    they are learning just as humans do: by example. The networks are trained using
    large sets of data and optimized through numerous layers and multiple iterations
    to achieve optimal results. Over the last 20 years exponential increases in computational
    power and data volume created the perfect storm for deep learning neural networks.
    And even though we stumble at flashy terms like *machine learning* and *artificial
    intelligence*, it’s just linear algebra and calculus combined with computation.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习神经网络最近受到很多关注，这有充分的理由。它是语音识别、人脸检测、语音控制、自动驾驶汽车、脑肿瘤检测等技术的基础，这些技术在20年前还不曾进入我们的生活。尽管这些网络看起来很复杂，但它们的学习方式与人类一样：通过示例。网络通过大量数据集进行训练，并通过多个层次和多次迭代进行优化，以实现最佳结果。在过去20年中，计算能力和数据量的指数级增长为深度学习神经网络创造了完美的条件。即使我们在*机器学习*和*人工智能*等华丽术语面前感到困惑，但它们不过是线性代数和微积分与计算结合的产物。
- en: Frameworks such as Keras, PyTorch, and TensorFlow facilitate the arduous building,
    training, validating, and deploying of custom deep networks. They are the obvious
    go-to choice when creating deep learning applications in real life. Nevertheless,
    sometimes it’s essential to step back to move forward and I mean by really understanding
    what is happening behind the scenes of the framework. In this article, we’ll do
    that by creating a network using only NumPy and applying it to an image classification
    problem. You might get lost somewhere during the calculations, especially in the
    backpropagation where the calculus kicks in, but don’t worry. The intuition about
    the process is more important than the calculations as the frameworks take care
    of them.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 像Keras、PyTorch和TensorFlow这样的框架简化了定制深度网络的构建、训练、验证和部署。这些框架在创建现实生活中的深度学习应用时是显而易见的首选。然而，有时候，退一步思考，真正理解框架背后的运行机制是至关重要的。在本文中，我们将通过仅使用NumPy来创建一个网络，并将其应用于图像分类问题。你可能会在计算过程中，尤其是在反向传播的微积分阶段感到迷茫，但不用担心。对过程的直观理解比计算本身更重要，因为框架会处理这些计算。
- en: 'In this article, we will build an image classification (cat or no cat) neural
    network which will be trained with 1.652 images from two sets: 852 cat images
    from the [Dogs & Cats Images Dataset](https://www.kaggle.com/datasets/chetankv/dogs-cats-images)
    and 800 random images from the [Unsplash Random Images Collection](https://www.kaggle.com/datasets/lprdosmil/unsplash-random-images-collection).
    First of all, the images need to be converted to arrays and we’ll do that by reducing
    the original dimensions to 128x128 pixels to speed up computation since if we
    keep the original shapes it will take too long to train the model. All those 128x128
    images have three layers of colors (red, green, and blue) that when mixed reach
    the original color of the image. Each of the 128x128 pixels on every image has
    a range of red, green, and blue values going from 0 to 255 and those are the values
    in our image vectors. Therefore, in our computations, we will deal with 128x128x3
    vectors of 1.652 images.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将构建一个图像分类（猫或非猫）神经网络，该网络将使用来自两个数据集的1,652张图像进行训练：来自[狗与猫图像数据集](https://www.kaggle.com/datasets/chetankv/dogs-cats-images)的852张猫图像和来自[Unsplash随机图像集合](https://www.kaggle.com/datasets/lprdosmil/unsplash-random-images-collection)的800张随机图像。首先，图像需要被转换为数组，我们将通过将原始尺寸缩小到128x128像素来实现这一点，以加快计算速度，因为如果保持原始尺寸，训练模型将花费太长时间。所有这些128x128的图像都有三个颜色层（红色、绿色和蓝色），当混合在一起时，能还原图像的原始颜色。每张128x128图像上的每个像素都有从0到255的红色、绿色和蓝色值，这些就是我们图像向量中的值。因此，在我们的计算中，我们将处理1,652张图像的128x128x3向量。
- en: To run this vector through the network it is necessary to reshape it by stacking
    the three layers of colors into a single array as the image below displays. Then
    we’ll get a (49.152,1.652) vector that will be split by using 1.323 of the image
    vectors to train the model and 331 to test it by predicting the image classification
    (cat or no cat) using the trained model. After comparing those predictions with
    the true classification label of the image it will be possible to estimate the
    model’s accuracy.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要将这个向量传递通过网络，需要将其重新调整，将三层颜色堆叠成一个单一的数组，如下图所示。然后我们会得到一个 (49.152,1.652) 的向量，用1.323个图像向量来训练模型，使用331个图像向量来测试，通过预测图像分类（猫或非猫）来验证训练模型。通过将这些预测与图像的真实分类标签进行比较，就可以估算模型的准确性。
- en: '![](../Images/e65a038ee29aef694bf405c31072b111.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e65a038ee29aef694bf405c31072b111.png)'
- en: 'Image 1 — The process of transforming images into vectors. Source: The author.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图1 — 将图像转化为向量的过程。来源：作者。
- en: Finally, with the training vector explained, it’s time to talk about the architecture
    of the network, displayed in Image 2\. As there are 49.152 values in the training
    vector the input layer of the model must have the same number of nodes (or neurons).
    Then, there are three hidden layers until the output layer, which will be the
    probability of a cat in that picture. In real-life models usually, there are much
    more than 3 hidden layers as the networks need to be deeper to perform well in
    a Big Data context, still, in this article, only three hidden layers will be used
    because they are good enough for a simple classification model. However, despite
    this architecture having only 4 layers (the output layer doesn’t count), the code
    is applicable to create deeper neural networks by using the dimensions of the
    layers as a parameter to the training function.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，既然训练向量已经解释完毕，就该谈谈网络的架构，如图2所示。由于训练向量中有49.152个值，模型的输入层必须具有相同数量的节点（或神经元）。然后，有三个隐藏层，直到输出层，该层表示图片中猫的概率。在实际模型中，通常会有远超过3层隐藏层，因为网络需要更深以便在大数据背景下表现良好，但在这篇文章中，仅使用了三层隐藏层，因为它们对于简单的分类模型已经足够。然而，尽管这个架构只有4层（输出层不算在内），代码仍然可以应用于创建更深的神经网络，只需将层的维度作为训练函数的参数即可。
- en: '![](../Images/693e936138bdde413259975ac96f9d50.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/693e936138bdde413259975ac96f9d50.png)'
- en: 'Image 2 — The architecture of the network. Source: The author.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图2 — 网络架构。来源：作者。
- en: 'Now that the image vectors and network architecture have been explained, the
    optimization algorithm is described in Image 3: T[he Gradient Descent](/gradient-descent-algorithm-a-deep-dive-cf04e8115f21).
    And again, don’t worry if you don’t get all of it right away because each of the
    steps will be detailed later on in the coding part of the article.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在图像向量和网络架构已经解释完毕，优化算法在图3中进行了描述：[梯度下降](https://example.org/gradient-descent-algorithm-a-deep-dive-cf04e8115f21)。如果你一开始没有完全理解也不用担心，因为每一步将在文章的编码部分中详细讲解。
- en: '![](../Images/e617ce11b1f56a7bfdb200965181c87b.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e617ce11b1f56a7bfdb200965181c87b.png)'
- en: 'Image 3 — The training process. Source: The author.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图3 — 训练过程。来源：作者。
- en: First, we initiate the parameters of the network. Those parameters are weights
    (***w***) and biases (***b)*** to each of the connections of the nodes displayed
    in Image 2\. During the code, it will be easier to understand how each of the
    weight and biases parameters work and how they are initialized. Later, with those
    parameters initialized it is time to run the forward propagation block and finally
    apply a sigmoid function in our last activation to obtain the probability prediction.
    In our case, it’s the probability of a cat being in that picture. Later we compare
    our prediction with the true label (cat or no cat) of the image through the cross-entropy
    cost, a widely used loss function to optimize classification models. Finally,
    with the cost calculated, we run it back through the backpropagation module to
    calculate its gradient with respect to the parameters ***w*** and ***b***. With
    the gradients of the loss function with respect to ***w*** and ***b*** in hands,
    is possible to update the parameters by summing the respective gradients as they
    point in the direction of the ***w*** and ***b*** values that minimize the loss
    function.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们初始化网络的参数。这些参数是每个节点连接的权重 (***w***) 和偏置 (***b***)，如图像 2 所示。在代码中，将更容易理解每个权重和偏置参数是如何工作的以及它们如何初始化。随后，初始化这些参数后，就可以运行前向传播模块，并在最后应用
    sigmoid 函数以获得概率预测。在我们的案例中，它是猫出现在那张图片中的概率。然后，我们通过交叉熵成本来比较我们的预测与图像的真实标签（猫或非猫），交叉熵成本是优化分类模型的广泛使用的损失函数。最后，计算成本后，我们将其传递回反向传播模块，以计算相对于参数
    ***w*** 和 ***b*** 的梯度。掌握了相对于 ***w*** 和 ***b*** 的损失函数梯度后，可以通过将相应的梯度相加来更新参数，因为它们指向最小化损失函数的
    ***w*** 和 ***b*** 值。
- en: Since the goal is to minimize the loss function this loop should run through
    a predefined number of iterations to take small steps toward the minimum value
    of the loss function. At some point, the parameters will just stop changing because
    the gradients will tend to zero as the minimum is near.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 由于目标是最小化损失函数，因此此循环应经过预定义的迭代次数，以小步向损失函数的最小值逼近。在某些时候，参数将停止变化，因为梯度将趋于零，最小值已经接近。
- en: 1\. Load the data
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 加载数据
- en: First, the libraries need to be loaded. Only Numpy, Pandas, and OS will be necessary
    other than the *keras.preprocessing.image,* to convert the images to vectors,
    and *sklearn.model_selection* to split the image vector into train and test vectors.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，需要加载库。除了 *keras.preprocessing.image*（用于将图像转换为向量）和 *sklearn.model_selection*（用于将图像向量拆分为训练和测试向量）之外，只需
    Numpy、Pandas 和 OS。
- en: 'The data must be loaded from the two folders: cats and random images. This
    can be done by getting all the filenames and building the path to each file. Then
    it’s just consolidating all the file paths in a data frame and creating a conditional
    column “*is_cat*” with values 1 if that path is in the cat folder or 0 if not.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 数据必须从两个文件夹加载：cats 和 random images。这可以通过获取所有文件名并构建每个文件的路径来完成。然后，只需将所有文件路径汇总到数据框中，并创建一个条件列
    "*is_cat*"，如果路径在猫文件夹中则值为 1，否则为 0。
- en: 'With the paths dataset in hand, it is time to build our training and testing
    vectors by splitting the images with 80% dedicated to training and 20% to testing.
    Y represents the true labels of the features while X represents the RGB values
    of the images, so X is defined as the column in the data frame with the file paths
    to the images and then they are loaded using the *load_img* function with target_size
    set to 128x128 pixels in order et o enable quicker computations. At last, the
    images are converted to arrays using the *img_to_array* function. Those are the
    shapes of the X_train and X_test vectors:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有路径数据集后，接下来是通过将图像分成80%用于训练和20%用于测试来构建我们的训练和测试向量。Y 代表特征的真实标签，而 X 代表图像的 RGB 值，因此
    X 被定义为数据框中包含图像文件路径的列，然后使用 *load_img* 函数加载图像，并将 target_size 设置为 128x128 像素以便加快计算速度。最后，图像使用
    *img_to_array* 函数转换为数组。这些是 X_train 和 X_test 向量的形状：
- en: '![](../Images/07c0cfed985d85db99caca250bad438c.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07c0cfed985d85db99caca250bad438c.png)'
- en: 'Image 4 — Shapes of X_train and X_test. Source: The author.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图像 4 — X_train 和 X_test 的形状。来源：作者。
- en: '**2\. Initialize the parameters**'
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**2\. 初始化参数**'
- en: Since the linear function is `z = w*x + b` and the network has 4 layers, the
    parameters vectors to be initialized are w1, w2, w3, w4, b1, b2, b3, and b4\.
    In the code, this is done by looping over the length of the layer dimensions list,
    which will be defined later, but is a hard-coded list with the number of neurons
    in each of the layers in the network.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 由于线性函数为`z = w*x + b`，网络有4层，需要初始化的参数向量包括w1、w2、w3、w4、b1、b2、b3和b4。在代码中，通过遍历层维度列表的长度来完成这项工作，该列表将在后面定义，但它是一个硬编码的列表，包含网络中每一层的神经元数量。
- en: 'The parameters ***w*** and ***b*** must have different initializations: ***w***
    must be initialized to a random small numbers matrix and ***b*** to a zeros matrix.
    This happens because if we initialized the weights to zero the derivative of the
    weights wrt (with respect to) loss function would all be the same, thus the values
    in subsequent iterations would always be the same and the hidden layers would
    all be symmetric causing the neurons to learn only the same few features. Therefore,
    the weights are initialized to random numbers to break this symmetry and allow
    the neurons to learn different features. It is important to note that the bias
    can be initialized to zeros because the symmetry is already broken by the weights
    and the values in the neurons will all be different.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 参数***w***和***b***必须有不同的初始化：***w***必须初始化为随机小数矩阵，而***b***初始化为零矩阵。这是因为如果我们将权重初始化为零，权重对损失函数的导数将全部相同，因此在后续迭代中的值总是相同，隐藏层将全都对称，导致神经元只能学习相同的少量特征。因此，权重被初始化为随机数以打破这种对称性，并允许神经元学习不同的特征。需要注意的是，偏差可以初始化为零，因为权重已经打破了对称性，神经元中的值将全部不同。
- en: Finally, to understand the shapes defined on the initialization of the parameter
    vectors one must know that the weights take part in matrix multiplications while
    the bias in matrix sums (remember `z1 = w1*x + b1`?). Matrix addition can be done
    with arrays of different sizes, because of [Python broadcasting](https://www.geeksforgeeks.org/python-broadcasting-with-numpy-arrays/).
    [Matrix multiplication](https://www.cuemath.com/algebra/multiplication-of-matrices/),
    on the other hand, it’s only possible when the shapes are compatible as in `(m,n)
    x (n,k) = (m,k)`*,*meaning that the number of columns on the first array needs
    to match the number of rows on the second array and the final matrix will have
    the number of rows from array 1 and the number of columns from array 2\. Image
    5 presents the shapes of all the parameter vectors used on the neural network.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，要理解参数向量初始化时定义的形状，需要知道权重参与矩阵乘法，而偏差则参与矩阵加法（记住`z1 = w1*x + b1`？）。矩阵加法可以在不同大小的数组中进行，因为[Python
    广播](https://www.geeksforgeeks.org/python-broadcasting-with-numpy-arrays/)的存在。另一方面，[矩阵乘法](https://www.cuemath.com/algebra/multiplication-of-matrices/)仅在形状兼容时才可能发生，如`(m,n)
    x (n,k) = (m,k)`，这意味着第一个数组的列数需要与第二个数组的行数匹配，最终矩阵将具有第一个数组的行数和第二个数组的列数。图5展示了神经网络中所有参数向量的形状。
- en: '![](../Images/b1354a879200959578d8d0aa98b90854.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b1354a879200959578d8d0aa98b90854.png)'
- en: 'Image 5 — Shapes of the parameters vectors. Source: The author.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图5 — 参数向量的形状。来源：作者。
- en: In the first layer, as we are multiplying the ***w1*** parameters vectors by
    the original 49.152 input values, we need the ***w1*** shape to be `(20,49.152)`
    because `(20,49.152) * (49.152,1.323) = (20,1.323)`, which is the shape of 1st
    hidden layer activations. The ***b1*** parameter sums to the result of the matrix
    multiplication (remember`z1 = w1*x + b1)`so we can add a `(20,1)` array to the
    `(20,1.323)` result of the multiplication, as broadcasting will take care of the
    mismatching shapes. This logic goes on to the next layers, so we can assume the
    formula of **w(l)** shape is `(number of nodes layer l+1, number of nodes layer
    l)`while the formula of **b(l)** shape is`(number of nodes layer l+1, 1)`***.***
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一层中，由于我们将***w1***参数向量与原始的49.152个输入值相乘，因此需要将***w1***的形状设置为`(20,49.152)`，因为`(20,49.152)
    * (49.152,1.323) = (20,1.323)`，这就是第1个隐藏层激活的形状。***b1***参数将矩阵乘法的结果相加（记住`z1 = w1*x
    + b1`），因此我们可以将一个`(20,1)`的数组添加到`(20,1.323)`的乘法结果中，因为广播会处理形状不匹配的问题。这种逻辑适用于接下来的层，因此我们可以假设**w(l)**的形状公式是`(层
    l+1 的节点数, 层 l 的节点数)`，而**b(l)**的形状公式是`(层 l+1 的节点数, 1)`***。***
- en: Finally, an important observation on weight vector initialization. We should
    divide the random initialization by the square root of the number of nodes on
    the respective layer we are initializing the w parameters vector. For instance,
    the input layer has 49.152 nodes so we divide the randomly initialized parameters
    by √49.152 which is 222, while the 1st hidden layer has 20 nodes so we divide
    the randomly initialized w2 parameters by √20 which is 4,5\. The initializations
    must be kept small because that is a [requirement of stochastic gradient descent](https://machinelearningmastery.com/why-initialize-a-neural-network-with-random-weights/).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，对于权重向量初始化有一个重要的观察。我们应该将随机初始化的权重除以所在层的节点数的平方根。例如，输入层有49.152个节点，所以我们将随机初始化的参数除以√49.152，结果是222，而第一个隐藏层有20个节点，所以我们将随机初始化的w2参数除以√20，结果是4.5。初始化的值必须保持较小，因为这是[随机梯度下降的要求](https://machinelearningmastery.com/why-initialize-a-neural-network-with-random-weights/)。
- en: 3\. Forward Propagation
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 前向传播
- en: Now that the parameter vectors are initialized we can go to the forward propagation
    which is combining the linear operation `z = w*x + b`followed by a ReLU activation
    until the last layer when the sigmoid activation substitutes the ReLU and we get
    a probability as the last activation. The output of the linear operation is usually
    denominated with the letter “z” and called the pre-activation parameter. Therefore,
    the pre-activation parameter ***z*** will be an input to the ReLU and sigmoid
    activations.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在参数向量已经初始化，我们可以进行前向传播，这是通过进行线性操作`z = w*x + b`，然后进行一个ReLU激活，一直到最后一层，最后一层使用sigmoid激活替代ReLU激活，并获得一个概率作为最后的激活。线性操作的输出通常用字母“z”表示，并称为预激活参数。因此，预激活参数***z***将成为ReLU和sigmoid激活的输入。
- en: After the input layer, the linear operation on a given layer L will be `z[L]
    = w[L] * A[L-1] + b[L]`, using the activation value of the previous layer instead
    of the data input x. The parameters of both linear operation and activations will
    be stored on a cache list to serve as inputs to calculate the gradients later
    on the Backpropagation block.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在输入层之后，对给定层L上的线性操作将为`z[L] = w[L] * A[L-1] + b[L]`，使用前一层的激活值而不是数据输入x。线性操作和激活的参数都将存储在缓存列表中，以便作为后续反向传播块中计算梯度的输入。
- en: 'So let the linear forward function be defined first:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在首先定义线性前向函数：
- en: And now the Sigmoid and ReLU functions must be defined. Image 6 presents a plot
    of both functions. Sigmoid activations are commonly used in two-class classification
    problems to predict the probability of a binary variable. This happens because
    of the S-shaped curve that puts most of the values close to 0 or 1\. Therefore,
    we will use the sigmoid activation only on the last layer of the network to predict
    the probability of a cat being in a picture.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在必须定义Sigmoid和ReLU函数。图6展示了这两个函数的图表。Sigmoid激活通常用于两类分类问题，以预测二进制变量的概率。这是因为S形曲线使大部分值接近0或1。因此，我们只会在网络的最后一层使用sigmoid激活来预测图片中是否有猫的概率。
- en: On the other hand, the ReLU function will output the input directly if it is
    positive, otherwise, it will output zero. This is a quite simple operation as
    it does not have any exponential operations and helps speed up the computations
    on the inner layers. Furthermore, using ReLU as an activation reduces the likelihood
    of the [vanishing gradient problem](/the-vanishing-gradient-problem-69bf08b15484),
    unlike the tanh and sigmoid functions.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果ReLU函数的输出为正，则直接输出输入，否则输出为零。这是一个非常简单的操作，因为它没有任何指数运算，并且有助于加快内层的计算速度。此外，使用ReLU作为激活函数减少了[梯度消失问题](/the-vanishing-gradient-problem-69bf08b15484)的可能性，与tanh和sigmoid函数不同。
- en: 'The ReLU activation makes not all the nodes being activated at the same time
    as the negative values will be turned to zero after the activation. Having some
    0 values throughout the network is important because it adds a desired property
    of neural networks: sparsity, meaning that the network has better predictive power
    and less overfitting. After all, the neurons are processing meaningful parts of
    information. Like in our example there may be a specific neuron that can identify
    the cat ears, which obviously should be set to 0 if the image is a human or a
    landscape.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 激活使得不是所有的节点同时被激活，因为负值在激活后会被变为零。网络中有一些 0 值很重要，因为它增加了神经网络所需的特性：稀疏性，意味着网络具有更好的预测能力和更少的过拟合。毕竟，神经元正在处理有意义的信息部分。例如，在我们的示例中，可能有一个特定的神经元可以识别猫的耳朵，如果图像是人类或风景，这个神经元显然应该被设置为
    0。
- en: '![](../Images/1e6083b622f69814d0d5bf0399e2c94b.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1e6083b622f69814d0d5bf0399e2c94b.png)'
- en: 'Image 6 — Sigmoid and ReLU functions. Source: The author.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图像 6 — Sigmoid 和 ReLU 函数。来源：作者。
- en: Now it is possible to implement the full activation functions.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以实现完整的激活函数了。
- en: Finally, it is time to consolidate the activations in a function according to
    the planned network architecture. First, the caches list is created, the first
    activation is set as the data input (the training vector) and since there are
    two parameters in the network (***w*** and ***b***), the number of layers can
    be defined as half the length of the parameters dictionary. Then the function
    loops over all the layers except the last one applying the linear forward function
    followed by the ReLU activation and wraps up in the last layer of the network
    with a final linear forward propagation followed by a sigmoid activation to generate
    the prediction probability which is the last activation.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，是时候根据计划的网络架构整合激活函数了。首先，创建缓存列表，将第一个激活设置为数据输入（训练向量），由于网络中有两个参数（***w*** 和 ***b***），层数可以定义为参数字典长度的一半。然后，函数循环遍历所有层，除了最后一层，应用线性前向函数，然后是
    ReLU 激活，最后在网络的最后一层用最终的线性前向传播和 sigmoid 激活生成预测概率，即最后的激活。
- en: 4\. Cross-Entropy Loss
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 交叉熵损失
- en: 'A loss function quantifies how well a model is performing on a given data by
    comparing the predicted probabilities (the result of the last activation) with
    the real labels of the images. If the network is learning with the data, the cost
    (the result of the loss function) must drop after every iteration. In classification
    problems, the [cross-entropy loss function](/cross-entropy-loss-function-f38c4ec8643e)
    is commonly used for optimization and its formula is presented in Image 6 below:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数通过将预测概率（最后激活的结果）与图像的真实标签进行比较，量化模型在给定数据上的表现。如果网络在学习数据，成本（损失函数的结果）应该在每次迭代后下降。在分类问题中，[交叉熵损失函数](/cross-entropy-loss-function-f38c4ec8643e)常用于优化，其公式见下图像
    6：
- en: '![](../Images/b50c407a1a011f4e5af0a02fd7ceae16.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b50c407a1a011f4e5af0a02fd7ceae16.png)'
- en: 'Image 7— Cost of a neural network. Source: The author.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图像 7 — 神经网络的成本。来源：作者。
- en: 'Defining the cross-entropy cost function with NumPy:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 NumPy 定义交叉熵成本函数：
- en: 5\. Backpropagation
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 反向传播
- en: In the Backpropagation module, we should move from right to left over the network
    calculating the gradients of the parameters with respect to the loss function
    to update them later. Just like in the forward propagation module, the linear
    backpropagation will be presented first followed by the sigmoid and relu and,
    finally, a function will consolidate all functions over the architecture of the
    network.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播模块中，我们应该从右到左遍历网络，计算相对于损失函数的参数梯度，然后进行更新。就像在前向传播模块一样，首先呈现线性反向传播，然后是 sigmoid
    和 relu，最后一个函数将整合所有函数以适应网络架构。
- en: For a given layer L, the linear part is `z[L] = w[L] * A[L-1] + b[L]`***.***
    Suppose you have already calculated the derivative **dZ[L]**, the derivative of
    cost wrt the linear output. Its formula will be presented soon, but first let’s
    take a look at the formulas of the derivatives of the **dW[L]**, **dA[L-1]**,
    and **db[L]** presented in Image 8 below to implement the linear backward function
    first.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的层 L，线性部分是 `z[L] = w[L] * A[L-1] + b[L]`***.*** 假设你已经计算了导数 **dZ[L]**，即线性输出的成本导数。其公式将很快呈现，但首先让我们查看图像
    8 中呈现的 **dW[L]**、**dA[L-1]** 和 **db[L]** 的导数公式，以便首先实现线性反向函数。
- en: '![](../Images/c96851125b05385c747407b0b4ae3954.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c96851125b05385c747407b0b4ae3954.png)'
- en: 'Image 8 — Derivatives of the cost wrt weight, bias, and previous activation.
    Source: The author.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8 — 成本相对于权重、偏置和之前激活的导数。来源：作者。
- en: Those formulas are the derivatives of the cross-entropy cost function with respect
    to the weight, bias, and previous activation **(a[L-1])**. This article will not
    go through the derivative calculations but they are presented in [this Towards
    Data Science article](https://medium.com/@pdquant/all-the-backpropagation-derivatives-d5275f727f60).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这些公式是交叉熵成本函数相对于权重、偏置和之前激活 **(a[L-1])** 的导数。本文不会详细介绍导数计算，但可以参考 [这篇 Towards Data
    Science 文章](https://medium.com/@pdquant/all-the-backpropagation-derivatives-d5275f727f60)。
- en: Defining the linear backward function requires using **dZ** as an input because
    in the backpropagation the linear part comes after the sigmoid or relu backward.
    In the next code section **dZ** will be calculated, but to follow the same function
    implementation logic on the forward propagation the linear backward function will
    come first.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 定义线性反向传播函数时需要使用 **dZ** 作为输入，因为在反向传播中，线性部分位于 sigmoid 或 relu 反向传播之后。在下一个代码部分将计算
    **dZ**，但为了遵循前向传播的相同函数实现逻辑，线性反向传播函数将首先出现。
- en: Before implementing the gradient computations is necessary to load the parameters
    ***weight***, ***bias,*** and ***activation*** from the previous layer, all stored
    in the cache during the linear propagation. The parameter ***m*** comes originally
    from the cross-entropy cost formula and is the vector’s size of the previous activation,
    which can be obtained with `previous_activation.shape[1]`. Then it is possible
    to implement the vectorized computations of the gradient formulas with NumPy.
    In the bias gradient, the `keepdims=True` and `axis=1` parameters are necessary,
    since the sum needs to be carried out in the rows of the vector and the original
    dimensions of the vector must be kept, meaning that **dB** will have the same
    dimensions as **dZ**.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现梯度计算之前，需要从之前的层加载参数 ***weight***、***bias*** 和 ***activation***，这些参数在线性传播过程中都存储在缓存中。参数
    ***m*** 最初来源于交叉熵成本公式，并且是之前激活向量的大小，可以通过 `previous_activation.shape[1]` 获得。然后可以使用
    NumPy 实现梯度公式的向量化计算。在偏置梯度中，需要 `keepdims=True` 和 `axis=1` 参数，因为需要在向量的行中进行求和，并且必须保持向量的原始维度，即
    **dB** 将与 **dZ** 具有相同的维度。
- en: The derivative of the cost wrt to the linear output (**dZ**) formula is presented
    in Image 9, where **g’(Z[L])** is the derivative of the activation function.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 成本相对于线性输出的导数 (**dZ**) 公式如图 9 所示，其中 **g’(Z[L])** 是激活函数的导数。
- en: '![](../Images/33ad7aa1c483cb148a9b4bde45625ed8.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/33ad7aa1c483cb148a9b4bde45625ed8.png)'
- en: 'Image 9— Derivative of the cost wrt the linear output. Source: The author.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9— 成本相对于线性输出的导数。来源：作者。
- en: Thus, the derivatives of the sigmoid and ReLU functions must be computed first.
    In the ReLU the derivative is 1 if the value is positive and undefined otherwise,
    but for computational purposes to obtain **dZ** in the ReLU backward, it is possible
    to just copy the dactivation vector (since `dactivation * 1 = dactivation`) and
    set **dZ** to 0 when *z* is negative. For a sigmoid function ***s*** its derivative
    is `s * (1-s)`***,*** and multiplying this derivative by ***dactivation,*** thevector
    **dZ** is implemented in the sigmoid backward function.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，必须首先计算 sigmoid 和 ReLU 函数的导数。在 ReLU 中，当值为正时，导数为 1，否则为未定义，但为了计算目的，在 ReLU 反向传播中获取
    **dZ**，可以直接复制 dactivation 向量（因为 `dactivation * 1 = dactivation`），并在 *z* 为负时将 **dZ**
    设置为 0。对于 sigmoid 函数 ***s***，其导数为 `s * (1-s)`***，*** 乘以这个导数后，***dactivation,***
    向量 **dZ** 就在 sigmoid 反向传播函数中实现了。
- en: Now it is possible to implement the `linear_activation_backward` function.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以实现 `linear_activation_backward` 函数。
- en: First, the linear and activation caches have to be retrieved from the `cache`list.
    Then for each activation run first the `activation_backward` function, obtaining
    **dZ**, and then use it as input, combined with the `linear cache`, to the `linear_backward`
    function. In the end, the function returns the **dW**, **dB**, and **dprevious_activation**
    gradients. Remember that this is the inverse order of the forward propagation
    as we are going from the right to the left on the network.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，需要从 `cache` 列表中检索线性和激活缓存。然后对每个激活，首先运行 `activation_backward` 函数，获取 **dZ**，然后将其作为输入，与
    `linear cache` 结合，传递给 `linear_backward` 函数。最后，该函数返回 **dW**、**dB** 和 **dprevious_activation**
    梯度。请记住，这与前向传播的顺序相反，我们在网络中从右到左进行。
- en: Now it is time to implement the backward function for the whole network. This
    function will iterate over all the hidden layers backward starting from the last
    layer L. Thus, the code needs to compute **dAL**, which is the derivative of the
    cost function wrt the last activation, to use it as an input for the `linear_activation_backward`
    function of the sigmoid activation. The formula for dAL is presented in Image
    10 below.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候为整个网络实现反向传播函数了。这个函数将从最后一层L开始，向后遍历所有隐藏层。因此，代码需要计算**dAL**，即成本函数对最后一次激活的导数，以便将其作为`linear_activation_backward`函数的输入，该函数用于sigmoid激活。dAL的公式在下面的图10中展示。
- en: '![](../Images/2f84009e0489c5d5c61180b68b7a8269.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f84009e0489c5d5c61180b68b7a8269.png)'
- en: 'Image 10 — Derivative of the cost wrt the last activation. Source: The author.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图10 — 成本函数对最后一次激活的导数。来源：作者。
- en: Now everything is set to implement the backpropagation function.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在一切准备就绪，可以实现反向传播函数了。
- en: First, the gradient dictionary is created. The number of layers of the network
    is defined by taking the length of the caches dictionary as each layer had its
    linear and activation caches stored during the forward propagation block, so the
    caches list length is the same as the number of layers. Later the function will
    iterate over those layers’ caches to retrieve the input values for the linear
    activation backward function. Also, the true labels vector (**Y_train**) is reshaped
    for the dimensions to match the last activation’s shape since that is a requirement
    to divide one by the other in the **dAL** computation, the next line of the code.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，创建梯度字典。网络的层数通过获取缓存字典的长度来定义，因为每层在前向传播块中都有其线性和激活缓存，因此缓存列表的长度与层数相同。之后，函数将遍历这些层的缓存，以检索线性激活反向传播函数的输入值。此外，真实标签向量
    (**Y_train**) 被重塑为与最后一次激活的形状匹配的维度，因为这是在**dAL**计算中将一个除以另一个的要求，接下来的代码行。
- en: The ***current_cache*** object is created and set to retrieve the last layer’s
    linear and activation caches (remember python indexing starts at 0 so the last
    layer is n_layers — 1). Then, to the last layer, on the `linear_activation_backward`
    function, the activation cache will be used on the `sigmoid_backward`function
    while the linear cache will be an input to the `linear_backward` function. Finally,
    the function gathers the returns of the functions and assigns them to the gradients
    dictionary. In the case of **dA** as the gradient formula computed is from the
    previous activation it’s necessary to assign it using n_layers-1 on the indexation.
    After this code block, the gradients are computed for the last layer of the network.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 创建并设置***current_cache***对象以检索最后一层的线性和激活缓存（记住Python索引从0开始，所以最后一层是n_layers - 1）。然后，在`linear_activation_backward`函数中，将激活缓存用于`sigmoid_backward`函数，而线性缓存将作为`linear_backward`函数的输入。最后，函数收集这些函数的返回值，并将其分配给梯度字典。在**dA**的情况下，由于梯度公式计算的是来自前一激活的，因此需要使用n_layers-1进行索引分配。在这个代码块之后，计算了网络最后一层的梯度。
- en: Following the reverse order of the network, the next step is to reverse loop
    over the linear->relu layers and calculate their gradients. However, during the
    reverse loop, the `linear_activation_backward` function must use the ‘relu’ parameter
    instead of ‘sigmoid’ since the `relu_backward` function needs to be called for
    the remaining layers. In the end, the function returns the ***dA***, ***dW,***
    and ***dB*** gradients of all layers calculated and the backpropagation is finished.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 按照网络的反向顺序，下一步是反向循环遍历linear->relu层并计算它们的梯度。然而，在反向循环过程中，`linear_activation_backward`函数必须使用‘relu’参数而不是‘sigmaid’，因为`relu_backward`函数需要被调用以处理其余层。最后，函数返回所有层计算得到的***dA***、***dW***和***dB***梯度，反向传播也就完成了。
- en: 6\. Parameters Update
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 参数更新
- en: With the gradients calculated it is time to wrap up the gradient descent by
    updating the original parameters with the gradients to move towards the minimum
    of the cost function.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算出梯度后，是时候通过使用梯度更新原始参数来结束梯度下降，向着成本函数的最小值移动。
- en: The function does that by looping over the layers and assigning the ***w***
    and ***b*** parameters their original values subtracted by the learning rate input
    times the respective gradient. Multiplying by the learning rate is a way to control
    how much to change the network parameters ***w*** and ***b*** in response to the
    estimated error each time the model weights are updated.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数通过遍历层并将***w***和***b***参数赋予其原始值减去学习率输入乘以相应的梯度来实现。乘以学习率是控制每次模型权重更新时网络参数***w***和***b***变化多少的一种方式。
- en: 7\. Pre-processing the Vectors
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 向量的预处理
- en: Finally, all the functions necessary for the gradient descent optimization are
    implemented, so the training and testing vectors can be pre-processed and get
    ready for the training.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，所有必要的梯度下降优化函数都已实现，因此训练和测试向量可以预处理并准备好进行训练。
- en: The layers_dimensions, input for the initialization function has to be hard-coded,
    and this is done by creating a list with the number of neurons in each layer.
    Later, the X_train and X_test vectors must be flattened to serve as inputs for
    the network, as has been presented in Image 1\. This can be done by using the
    NumPy function reshape. Also, is necessary to divide the X_train and X_test values
    by 255, since they are in pixels (which range from 0 to 255) and it’s a good practice
    to normalize the values to range from 0 to 1\. That way the numbers will be smaller
    and the computations faster. Finally, Y_train and Y_test are converted to arrays
    and also flattened.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: layers_dimensions，初始化函数的输入必须硬编码，通过创建一个包含每层神经元数量的列表来完成。随后，X_train和X_test向量必须被展平以作为网络的输入，如图像1所示。这可以通过使用NumPy函数reshape来完成。此外，需要将X_train和X_test的值除以255，因为它们是像素（范围从0到255），而将值归一化到0到1的范围是一个良好的实践。这样，数字会更小，计算也会更快。最后，Y_train和Y_test被转换为数组并且也展平。
- en: 'And those are the final dimensions of the training and testing vectors:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是训练和测试向量的最终维度：
- en: '![](../Images/d6a379c2e4cd10c2358156774071bc35.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6a379c2e4cd10c2358156774071bc35.png)'
- en: 'Image 11 — Dimensions of the training and testing vectors. Source: The author.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图片 11 — 训练和测试向量的维度。 来源：作者。
- en: 8\. Training
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8\. 训练
- en: With all functions in hand, it's just necessary to organize them into a loop
    to create the training iterations.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好所有函数后，只需将它们组织成一个循环来创建训练迭代。
- en: But first, create an empty list to store the cost output from the `cross_entropy_cost`
    function and initialize the parameters, as this has to be done once before the
    iterations since these parameters will be updated by the gradients.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 但首先，创建一个空列表来存储来自`cross_entropy_cost`函数的成本输出，并初始化参数，因为这必须在迭代之前完成一次，因为这些参数将通过梯度更新。
- en: 'Now create the loop over the inputted number of iterations calling the implemented
    functions in the correct order: `l_layer_model_forward`, `cross_entropy_cost`,
    `l_layer_model_backward`, and `update_parameters`. Finally, a conditional statement
    to print the cost every 50 iterations or in the last one.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在创建一个循环，遍历输入的迭代次数，按照正确的顺序调用实现的函数：`l_layer_model_forward`、`cross_entropy_cost`、`l_layer_model_backward`和`update_parameters`。最后，添加一个条件语句以每50次迭代或在最后一次迭代时打印成本。
- en: 'Calling the function for 2500 iterations:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 调用函数进行2500次迭代：
- en: The cost decreases from 0.69 in the first iteration to 0.09 in the last one.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 成本从第一次迭代的0.69降到最后一次的0.09。
- en: '![](../Images/d5b943ab866c84b97c960ef303e24fc6.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d5b943ab866c84b97c960ef303e24fc6.png)'
- en: 'Image 12 — Source: The author.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图片 12 — 来源：作者。
- en: This means the gradient descent functions developed in NumPy have optimized
    the parameters along the training, which leads to better predictions and hence
    a lower cost. With the training concluded, we can check how the trained model
    predicts the testing image labels.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着在NumPy中开发的梯度下降函数已经在训练过程中优化了参数，从而导致更好的预测和更低的成本。训练完成后，我们可以检查训练好的模型如何预测测试图像标签。
- en: 9\. Predictions
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9\. 预测
- en: By using the trained parameters, this function runs the forward propagation
    for the **X_test** vector to obtain the predictions and then compares them with
    the true labels vector **Y_test** to return the accuracy.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用训练好的参数，此函数运行**X_test**向量的前向传播以获得预测，然后将其与真实标签向量**Y_test**进行比较，以返回准确率。
- en: '![](../Images/b860b6a1811e08e42634b30f621a23ca.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b860b6a1811e08e42634b30f621a23ca.png)'
- en: 'Image 13— Source: The author.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图片 13 — 来源：作者。
- en: The model has reached an accuracy of almost 77% in detecting cats on the testing
    images. This is a pretty decent accuracy, considering that NumPy alone was used
    to build the network. Adding new images to the training dataset, increasing the
    complexity of the network, or using data augmentation techniques to convert the
    existing training images into new ones would all be possibilities to increase
    the accuracy.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在测试图像中检测猫的准确率已达到近77%。考虑到仅使用了NumPy构建网络，这个准确率相当不错。添加新图像到训练数据集中、增加网络复杂性，或使用数据增强技术将现有训练图像转换为新图像，都是提高准确率的可能方法。
- en: But again, accuracy was not the focus here as we dove deep into the mathematical
    fundamentals. This is where the value of this article lies. Learning the fundamentals
    of the networks lays the knowledge base for the fascinating world of deep learning
    network applications. Hope you keep diving!
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，再次强调，准确率不是我们关注的重点，因为我们深入探讨了数学基础。这也是本文的价值所在。学习网络的基础知识为迷人的深度学习网络应用世界奠定了知识基础。希望你继续深入探索！
