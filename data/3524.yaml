- en: 'Large Language Models: DeBERTa — Decoding-Enhanced BERT with Disentangled Attention'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大语言模型：DeBERTa — 解码增强型 BERT 与解耦注意力
- en: 原文：[https://towardsdatascience.com/large-language-models-deberta-decoding-enhanced-bert-with-disentangled-attention-90016668db4b?source=collection_archive---------2-----------------------#2023-11-28](https://towardsdatascience.com/large-language-models-deberta-decoding-enhanced-bert-with-disentangled-attention-90016668db4b?source=collection_archive---------2-----------------------#2023-11-28)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/large-language-models-deberta-decoding-enhanced-bert-with-disentangled-attention-90016668db4b?source=collection_archive---------2-----------------------#2023-11-28](https://towardsdatascience.com/large-language-models-deberta-decoding-enhanced-bert-with-disentangled-attention-90016668db4b?source=collection_archive---------2-----------------------#2023-11-28)
- en: Exploring the advanced version of the attention mechanism in Transformers
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索 Transformers 中注意力机制的高级版本
- en: '[](https://medium.com/@slavahead?source=post_page-----90016668db4b--------------------------------)[![Vyacheslav
    Efimov](../Images/db4b02e75d257063e8e9d3f1f75d9d6d.png)](https://medium.com/@slavahead?source=post_page-----90016668db4b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----90016668db4b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----90016668db4b--------------------------------)
    [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page-----90016668db4b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@slavahead?source=post_page-----90016668db4b--------------------------------)[![Vyacheslav
    Efimov](../Images/db4b02e75d257063e8e9d3f1f75d9d6d.png)](https://medium.com/@slavahead?source=post_page-----90016668db4b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----90016668db4b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----90016668db4b--------------------------------)
    [Vyacheslav Efimov](https://medium.com/@slavahead?source=post_page-----90016668db4b--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8a0ca9d85d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flarge-language-models-deberta-decoding-enhanced-bert-with-disentangled-attention-90016668db4b&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=post_page-c8a0ca9d85d8----90016668db4b---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----90016668db4b--------------------------------)
    ·9 min read·Nov 28, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F90016668db4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flarge-language-models-deberta-decoding-enhanced-bert-with-disentangled-attention-90016668db4b&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=-----90016668db4b---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fc8a0ca9d85d8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flarge-language-models-deberta-decoding-enhanced-bert-with-disentangled-attention-90016668db4b&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=post_page-c8a0ca9d85d8----90016668db4b---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----90016668db4b--------------------------------)
    ·9分钟阅读·2023年11月28日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F90016668db4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flarge-language-models-deberta-decoding-enhanced-bert-with-disentangled-attention-90016668db4b&user=Vyacheslav+Efimov&userId=c8a0ca9d85d8&source=-----90016668db4b---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F90016668db4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flarge-language-models-deberta-decoding-enhanced-bert-with-disentangled-attention-90016668db4b&source=-----90016668db4b---------------------bookmark_footer-----------)![](../Images/bb49940006ced82ce8dc2dc788b1226d.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F90016668db4b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flarge-language-models-deberta-decoding-enhanced-bert-with-disentangled-attention-90016668db4b&source=-----90016668db4b---------------------bookmark_footer-----------)![](../Images/bb49940006ced82ce8dc2dc788b1226d.png)'
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In recent years, BERT has become the number one tool in many natural language
    processing tasks. Its outstanding ability to process, understand information and
    construct word embeddings with high accuracy reach state-of-the-art performance.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，BERT 已成为许多自然语言处理任务中的首选工具。它在处理、理解信息和构建高精度词嵌入方面的卓越能力达到了最先进的水平。
- en: As a well-known fact, BERT is based on the **attention** mechanism derived from
    the Transformer architecture. Attention is the key component of most large language
    models nowadays.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，BERT 基于从 Transformer 架构中衍生出的**注意力**机制。注意力是现在大多数大语言模型的关键组成部分。
- en: '[](/bert-3d1bf880386a?source=post_page-----90016668db4b--------------------------------)
    [## Large Language Models: BERT — Bidirectional Encoder Representations from Transformer'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/bert-3d1bf880386a?source=post_page-----90016668db4b--------------------------------)
    [## 大型语言模型：BERT — 基于 Transformer 的双向编码表示'
- en: Understand how BERT constructs state-of-the-art embeddings
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解 BERT 如何构建最先进的嵌入
- en: towardsdatascience.com](/bert-3d1bf880386a?source=post_page-----90016668db4b--------------------------------)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/bert-3d1bf880386a?source=post_page-----90016668db4b--------------------------------)
- en: Nevertheless, new ideas and approaches evolve regularly in the machine learning
    world. One of the most innovative techniques in BERT-like models appeared in 2021
    and introduced an enhanced attention version called “**Disentangled attention**”.
    The implementation of this concept gave rise to **DeBERTa** — the model incorporating
    disentangled attention. Though DeBERTa introduces only a pair of new architecture
    principles, its improvements are prominent on top NLP benchmarks, compared to
    other large models.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在机器学习领域，新的思想和方法不断发展。2021 年，BERT 类模型中出现了一种创新技术，称为“**解耦注意力**”。这一概念的实现催生了 **DeBERTa**——一种融合了解耦注意力的模型。虽然
    DeBERTa 只引入了一对新的架构原则，但与其他大型模型相比，其在顶级 NLP 基准测试中的改进是显著的。
- en: In this article, we will refer to the original [DeBERTa paper](https://arxiv.org/pdf/2006.03654.pdf)
    and cover all the necessary details to understand how it works.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将参考原始的 [DeBERTa 论文](https://arxiv.org/pdf/2006.03654.pdf)，并覆盖理解其工作原理所需的所有细节。
- en: 1\. Disentangled attention
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 解耦注意力
- en: 'In the original Transformer block, each token is represented by a single vector
    which contains information about token content and position in the form of the
    element-wise embedding sum. The disadvantage of this approach is potential information
    loss: the model might not differentiate whether a word itself or its position
    gives more importance to a certain embedded vector component.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始的 Transformer 块中，每个 token 都由一个单一的向量表示，该向量以逐元素嵌入和的形式包含关于 token 内容和位置的信息。这种方法的缺点是潜在的信息丢失：模型可能无法区分一个词本身还是它的位置对某个嵌入向量分量的重要性。
- en: '![](../Images/d1b3f4d2a1192c1a465ec4c12966365c.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1b3f4d2a1192c1a465ec4c12966365c.png)'
- en: Embedding construction in BERT and DeBERTa. Instead of storing all the information
    in a single vector, DeBERTa uses two separate vectors to store word and position
    embeddings.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 和 DeBERTa 中的嵌入构造。与将所有信息存储在一个向量中的方法不同，DeBERTa 使用两个独立的向量来存储词和位置的嵌入。
- en: DeBERTa proposes a novel mechanism in which the same information is stored in
    two different vectors. Furthermore, the algorithm for attention computation is
    also modified to explicitly take into account the relations between the content
    and positions of tokens. For instance, the words *“research”* and *“paper”* are
    much more dependent when they appear near each other than in different text parts.
    This example clearly justifies why it is necessary to consider content-to-position
    relations as well.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: DeBERTa 提出了一个新颖的机制，其中相同的信息存储在两个不同的向量中。此外，注意力计算算法也被修改，以显式考虑 token 内容和位置之间的关系。例如，词汇
    *“research”* 和 *“paper”* 在彼此接近时的相关性远大于它们在不同文本部分出现时的相关性。这个例子清楚地说明了为什么也需要考虑内容到位置的关系。
- en: 'The introduction of disentangled attention requires modification in attention
    score computation. As it turns out, this process is very simple. Calculation of
    cross-attention scores between two embeddings each consisting of two vectors can
    be easily decomposed into the sum of four pairwise multiplication of their subvectors:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 解耦注意力的引入需要修改注意力分数的计算。事实证明，这一过程非常简单。计算两个嵌入之间的交叉注意力分数，每个嵌入由两个向量组成，可以简单地分解为四个子向量的成对乘积之和：
- en: '![](../Images/fac234d13e63f69ecd245e2272935594.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fac234d13e63f69ecd245e2272935594.png)'
- en: Computation of cross-attentin score between two embedding vectors.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 计算两个嵌入向量之间的交叉注意力分数。
- en: 'The same methodology can be generalized in the matrix form. From the diagram,
    we can observe four different types of matrices (vectors) each representing a
    certain combination of content and position information:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的方法可以推广到矩阵形式。从图中，我们可以观察到四种不同类型的矩阵（向量），每种矩阵代表内容和位置信息的某种组合：
- en: '*content-to-content* matrix;'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*内容到内容* 矩阵；'
- en: '*content-to-position* matrix;'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*内容到位置* 矩阵；'
- en: '*position-to-content* matrix;'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*位置到内容* 矩阵；'
- en: '*position-to-position* matrix.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*位置到位置* 矩阵。'
- en: It is possible to observe position-to-position matrix does not store any valuable
    information as it does not have any details on the words’ content. This is the
    reason why this term is discarded in disentangled attention.
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 可以观察到位置到位置的矩阵没有存储任何有价值的信息，因为它没有关于单词内容的细节。这就是为什么在解耦注意力中会忽略这个术语的原因。
- en: For the resting three terms, the final output attention matrix is calculated
    similarly as in the original Transformer.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其余三个术语，最终输出的注意力矩阵计算方式与原始 Transformer 相似。
- en: '![](../Images/9764f1b5ed0a7066a6f49ab5019453d1.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9764f1b5ed0a7066a6f49ab5019453d1.png)'
- en: Output disentangled attention calculation in DeBERTa
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: DeBERTa 中输出解耦注意力的计算
- en: Even though the calculation process looks similar, there is a pair of subtleties
    that need to be taken into consideration.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管计算过程看起来类似，但有一对细微差别需要考虑。
- en: From the diagram above, we can notice that the multiplication symbol *** used
    for multiplication between *query-content Qc and key-position Krᵀ matrices* &
    *key-content Kc and query-position Qrᵀ matrices* differs from the normal matrix
    multiplication symbol *x*. In reality, this is done not by accident as the mentioned
    pairs of matrices in DeBERTa are multiplied in slightly another way to take into
    account the relative positioning of tokens.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的图示中，我们可以注意到用于表示*查询内容 Qc 和键位置 Krᵀ 矩阵*与*键内容 Kc 和查询位置 Qrᵀ 矩阵*之间乘法的符号***与普通矩阵乘法符号*x*不同。实际上，这并非偶然，因为在
    DeBERTa 中提到的矩阵对的乘法方式略有不同，以考虑到令牌的相对位置。
- en: According to the normal matrix multiplication rules, if *C = A x B*, then the
    element *C[i][j]* is computed by element-wise multiplication of the *i*-th row
    of *A* by the *j*-th column of *B*.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据普通矩阵乘法规则，如果 *C = A x B*，则元素 *C[i][j]* 通过 *A* 的第 *i* 行与 *B* 的第 *j* 列的逐元素乘法来计算。
- en: 'In a special case of DeBERTa, if *C = A * B*, then *C[i][j]* is calculated
    as the multiplication of the *i*-th row of *A* by *δ(i, j)*-th column of *B* where
    *δ* denotes a relative distance function between indexes *i* and *j* which is
    defined by the formula below:'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 DeBERTa 的特殊情况下，如果 *C = A * B*，则 *C[i][j]* 计算为 *A* 的第 *i* 行与 *B* 的 *δ(i, j)*
    列的乘积，其中 *δ* 表示索引 *i* 和 *j* 之间的相对距离函数，其定义如下：
- en: '![](../Images/3dd94b4a3f79eb8bfb1be35a58acb77a.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3dd94b4a3f79eb8bfb1be35a58acb77a.png)'
- en: Relative distance definition between indexes i and j. k is a hyperparameter
    controlling the maximum possible relative distance. Image adopted by the author.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 索引 i 和 j 之间的相对距离定义。*k* 是一个超参数，用于控制最大可能的相对距离。图片由作者采用。
- en: '*k* can be thought of as a hyperparameter controlling the maximum possible
    relative distance between indexes *i* and *j*. In DeBERTa, *k* is set to 512\.
    To get a better sense of the formula, let us plot a heatmap visualising relative
    distances (*k = 6*) for different indexes of *i* and *j*.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*k* 可以被视为一个超参数，用于控制索引 *i* 和 *j* 之间的最大可能相对距离。在 DeBERTa 中，*k* 被设置为 512。为了更好地理解公式，我们可以绘制一个热图，展示不同索引的
    *i* 和 *j* 的相对距离（*k = 6*）。'
- en: '![](../Images/b8a55757372bc75fa189973b2517bc74.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b8a55757372bc75fa189973b2517bc74.png)'
- en: For example, if *k = 6*, *i = 15* and *j = 13*, then the relative distance *δ*
    between *i* and *j* is equal to 8\. To obtain a content-to-position score for
    indexes *i = 15* and *j = 13*, during the multiplication of query-content *Qc*
    and key-position *Kr* matrices, the 15-th row of *Qc* should be multiplied by
    the 8-th column of *Kr*ᵀ.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果 *k = 6*，*i = 15* 和 *j = 13*，则 *i* 和 *j* 之间的相对距离 *δ* 等于 8。为了获得索引 *i = 15*
    和 *j = 13* 的内容到位置评分，在查询内容 *Qc* 和键位置 *Kr* 矩阵的乘法中，*Qc* 的第 15 行应乘以 *Kr*ᵀ 的第 8 列。
- en: '![](../Images/1669db0b584b51badfa0185f0658d4db.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1669db0b584b51badfa0185f0658d4db.png)'
- en: Content-to-position score computation for tokens i and j
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 内容到位置的评分计算用于令牌 *i* 和 *j*
- en: 'However, for position-to-content scores, the algorithm works a bit differently:
    instead of the relative distance being *δ(i, j)*, this time the algorithm uses
    the value of *δ(j, i)* in matrix multiplication. As the authors of the paper explain:
    “*this is because* *for a given position i, position-to-content computes the attention
    weight of the key content at j with respect to the query position at i, thus the
    relative distance is δ(j, i)”.*'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于位置到内容的评分，算法的工作方式稍有不同：这次算法在矩阵乘法中使用的是 *δ(j, i)* 的值，而不是 *δ(i, j)*。正如论文的作者所解释的：“*这是因为*
    *对于给定的位置 i，位置到内容计算的是键内容在 j 的注意力权重相对于位置 i 的查询，因此相对距离是 δ(j, i)*”。
- en: '![](../Images/415637a6a2592a6ad3ba601bbdc4da59.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/415637a6a2592a6ad3ba601bbdc4da59.png)'
- en: Position-to-content score computation for tokens i and j
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 位置到内容的得分计算对于令牌 i 和 j
- en: δ(i, j) ≠ δ(j, i), i.e. δ is not a symmetric function meaning that the distance
    between i and j is not the same as the distance between j and i.
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: δ(i, j) ≠ δ(j, i)，即 δ 不是对称函数，这意味着 i 和 j 之间的距离与 j 和 i 之间的距离不同。
- en: Before applying the softmax transformation, attention scores are divided by
    a constant *√(3d)* for more stable training. This scaling factor is different
    to the one used in the original Transformer (*√d*). This difference in *√*3 timesis
    justified by larger magnitudes resulting from the summation of 3 matrices in the
    DeBERTa attention mechanism (instead of a single matrix in Transformer).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用 softmax 转换之前，注意力得分会被一个常数 *√(3d)* 除以，以便更稳定地训练。这个缩放因子不同于原始 Transformer 中使用的（*√d*）。这个
    *√*3 倍的差异由 DeBERTa 注意力机制中 3 个矩阵的求和导致的较大幅度（而不是 Transformer 中的单个矩阵）所证明。
- en: 2\. Enhanced mask decoder
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 增强的掩码解码器
- en: 'Disentangled attention takes into account only content and relative positioning.
    However, no information about absolute positioning is considered which might actually
    play an important role in ultimate prediction. The authors of the DeBERTa paper
    give a concrete example of such a situation: a sentence “*a new store opened beside
    the new mall*” which is fed to BERT with the masked words “*store*” and “*mall*”
    for prediction. Though the masked words have a similar meaning and local context
    (the adjective “*new*”), they represent completely different roles in the sentence.
    Without the full information about starting and ending positions of the masked
    words, it becomes much harder to correctly restore the original sentence.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 解耦注意力仅考虑内容和相对位置。然而，没有关于绝对位置的信息被考虑，这可能在最终预测中扮演重要角色。DeBERTa 论文的作者给出了一个具体的例子：句子“*a
    new store opened beside the new mall*”被送入 BERT，掩盖的单词是“*store*”和“*mall*”用于预测。虽然掩盖的单词具有相似的含义和局部上下文（形容词“*new*”），但它们在句子中代表完全不同的角色。如果没有关于掩盖单词的起始和结束位置的完整信息，正确恢复原始句子变得更加困难。
- en: '![](../Images/def2c9300938cc8902ce7f883fa659a9.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/def2c9300938cc8902ce7f883fa659a9.png)'
- en: Text example from the [DeBERTa paper](https://arxiv.org/pdf/2006.03654.pdf).
    By using only disentangled attention, the model cannot correctly restore the original
    phrase.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 [DeBERTa 论文](https://arxiv.org/pdf/2006.03654.pdf) 的文本示例。仅使用解耦注意力，模型无法正确恢复原始短语。
- en: To better understand this problem, imagine that you are aware that in reality
    the mall opened first and the store opened after it. Then you have to fill in
    the sentence *a new ___ store opened beside the new ___* . As an English speaker,
    you are aware that whatever comes after the construction *opened beside,* grammatically
    indicates that it opened first. At the same time, whatever comes before these
    words, opened later. Therefore, you confidently fill it with the words *store*
    and *mall* respectively. Why was it easy to do it for you? Because as a human,
    you naturally consider the absolute positions of the masked words.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这个问题，假设你知道实际上是商场先开了，商店在它之后开张。然后你需要填入句子 *a new ___ store opened beside
    the new ___*。作为一个英语使用者，你知道任何在 *opened beside* 后面的词语，从语法上看意味着它先开张。同时，任何在这些词之前的词则晚开张。因此，你自信地分别填入
    *store* 和 *mall*。为什么对你来说这很容易做到？因为作为人类，你自然会考虑被掩盖单词的绝对位置。
- en: Right now imagine that you did not know anything about the absolute positions
    of the masked words. Thus it would be impossible for you to use the mentioned
    hint about the grammatical word order around the construction *opened beside*.
    As a result, despite having semantic meanings of the words and their local context,
    you would still not be able to give a correct answer. This is the analogous situation
    for the model when it does not have access to absolute positioning.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设你对掩盖单词的绝对位置一无所知。因此，你将无法使用关于 *opened beside* 构造周围的语法词序的提示。结果是，尽管有单词的语义含义和局部上下文，你仍然无法给出正确的答案。这对于模型而言是类似的情况，当它无法访问绝对位置时。
- en: In a language there can be numerous similar examples, which is why it is crucial
    to incorporate **absolute positioning** into the model.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在语言中可以有许多类似的例子，这就是为什么将 **绝对位置** 纳入模型中至关重要。
- en: In BERT, absolute positioning is taken into account in input embeddings. Speaking
    of DeBERTa, it incorporates absolute positioning after all Transformer layers
    but before applying the softmax layer. It was shown in experiments that capturing
    relative positioning in all Transformer layers and only after introducing absolute
    positioning improves the model’s performance. According to the researchers, doing
    it inversely could prevent the model from learning sufficient information about
    relative positioning.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在 BERT 中，输入嵌入考虑了绝对位置。谈到 DeBERTa，它在所有 Transformer 层之后但在应用 softmax 层之前纳入了绝对位置。实验表明，在所有
    Transformer 层中捕捉相对位置，并且只有在引入绝对位置后，模型性能有所提升。研究人员表示，反向处理可能会阻止模型学习足够的相对位置信息。
- en: Architecture
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 架构
- en: 'According to the [paper](https://arxiv.org/pdf/2006.03654.pdf), the enhanced
    mask decoder (EMD) has two input blocks:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 [论文](https://arxiv.org/pdf/2006.03654.pdf)，增强掩码解码器（EMD）有两个输入块：
- en: '*H* — the hidden states from the previous Transformer layer.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*H* — 来自前一层 Transformer 的隐藏状态。'
- en: '*I* — any necessary information for decoding (e.g. hidden states *H*, absolute
    position embedding or output from the previous EMD layer).'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*I* — 解码所需的任何信息（例如，隐藏状态 *H*、绝对位置嵌入或来自前一层 EMD 的输出）。'
- en: '![](../Images/9b644de9e542c66fd583e2b171cae780.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b644de9e542c66fd583e2b171cae780.png)'
- en: Enhanced mask decoder in DeBERTa. Image adopted by the author.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: DeBERTa 中的增强掩码解码器。图片由作者提供。
- en: 'In general, there can be multiple *n* EMD blocks inside a model. If so, they
    are constructed with the following rules:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，一个模型中可以有多个 *n* EMD 块。如果是这样，它们会按照以下规则构建：
- en: the output of each EMD layer is the input *I* for the next EMD layer;
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个 EMD 层的输出是下一层 EMD 层的输入 *I*；
- en: the output of the last EMD layer is fed to the language model head.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一层 EMD 层的输出被送入语言模型头。
- en: In the case of DeBERTa, the number of EMD layers is set to *n = 2* with the
    position embedding used for *I* in the first EMD layer.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 DeBERTa，EMD 层的数量设置为 *n = 2*，位置嵌入用于第一层 EMD 层中的 *I*。
- en: Another frequently used technique in NLP is weights sharing across different
    layers with the objective of reducing the model complexity (e.g. [ALBERT](https://medium.com/towards-data-science/albert-22983090d062)).
    This idea is also implemented in EMD blocks of DeBERTa.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: NLP 中另一个常用技术是跨不同层共享权重，目的是减少模型复杂度（例如，[ALBERT](https://medium.com/towards-data-science/albert-22983090d062)）。这一思想也在
    DeBERTa 的 EMD 块中得到实现。
- en: '[](/albert-22983090d062?source=post_page-----90016668db4b--------------------------------)
    [## Large Language Models, ALBERT — A Lite BERT for Self-supervised Learning'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/albert-22983090d062?source=post_page-----90016668db4b--------------------------------)
    [## 大型语言模型，ALBERT — 用于自监督学习的轻量级 BERT'
- en: Understand essential techniques behind BERT architecture choices for producing
    a compact and efficient model
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解 BERT 架构选择背后的基本技术，以产生紧凑且高效的模型
- en: towardsdatascience.com](/albert-22983090d062?source=post_page-----90016668db4b--------------------------------)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/albert-22983090d062?source=post_page-----90016668db4b--------------------------------)'
- en: When I = H and n = 1, EMD becomes the equivalent of the BERT decoder layer.
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当 I = H 且 n = 1 时，EMD 等同于 BERT 解码器层。
- en: DeBERTa settings
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DeBERTa 设置
- en: Ablation studies
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 消融研究
- en: Experiments demonstrated that all introduced components in DeBERTa (position-to-content
    attention, content-to-position attention and enhanced mask decoder) boost performance.
    Removing any of them would result in inferior metrics.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 实验表明，DeBERTa 中引入的所有组件（位置到内容的注意力、内容到位置的注意力和增强掩码解码器）都提高了性能。去除任何一个组件都会导致指标降低。
- en: Scale-invariant-fine Tuning
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 尺度不变的精细调优
- en: Additionally, the authors proposed a new adversarial algorithm called “**Scale
    Invariant Fine-Tuning**” to improving the model’s generalization. The idea is
    to incorporate small perturbations to input sequences making the model more resilient
    to adversial examples. In DeBERTa, perturbations are applied to normalized input
    word embeddings. This technique works even better for larger fine-tuned DeBERTa
    models.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，作者提出了一种新的对抗算法，称为“**尺度不变精细调优**”，以提高模型的泛化能力。其思想是对输入序列加入小的扰动，使模型对对抗样本更具弹性。在
    DeBERTa 中，扰动应用于标准化的输入词嵌入。对于更大的精细调优 DeBERTa 模型，这种技术效果更佳。
- en: DeBERTa variants
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DeBERTa 变体
- en: DeBERTa’s paper presents three models. The comparison between them is shown
    in the diagram below.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: DeBERTa 的论文展示了三种模型。它们之间的比较见下图。
- en: '![](../Images/9a59d7f6a0a41036ca13841be378a2b4.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9a59d7f6a0a41036ca13841be378a2b4.png)'
- en: DeBERTa variants
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: DeBERTa 变体
- en: Data
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据
- en: 'For pre-training, the base and large versions of DeBERTa use a combination
    of the following datasets:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 对于预训练，DeBERTa 的基础版和大型版使用了以下数据集的组合：
- en: English Wikipedia + BookCorpus (16 GB)
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: English Wikipedia + BookCorpus (16 GB)
- en: 'OpenWebText (public Reddit content: 38 GB)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenWebText（公开 Reddit 内容：38 GB）
- en: Stories (31 GB)
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stories (31 GB)
- en: After data deduplication, the resulting dataset size is reduced to 78 GB. For
    DeBERTa 1.5B, the authors used more twice more data (160 GB) with an impressive
    vocabulary size of 128K.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 数据去重后，结果数据集的大小减少到 78 GB。对于 DeBERTa 1.5B，作者使用了两倍以上的数据（160 GB），且词汇量达到了 128K。
- en: In comparison, other large models like RoBERTa, XLNet and ELECTRA are pre-trained
    on 160 GB of data. At the same time, DeBERTa shows a comparable or better performance
    than these models on a variety of NLP tasks.
  id: totrans-87
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 相比之下，其他大型模型如 RoBERTa、XLNet 和 ELECTRA 在 160 GB 数据上进行预训练。同时，DeBERTa 在各种 NLP 任务中展现了与这些模型相当或更优的性能。
- en: Spearking of training, DeBERTa is pre-trained for one million steps with 2K
    samples in each step.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 说到训练，DeBERTa 在每一步使用 2K 样本进行了一百万步的预训练。
- en: Conclusion
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Conclusion
- en: We have walked through the main aspects of DeBERTa architecture. By possessing
    disentangled attention and enhanced masked encoding algorithms inside, DeBERTa
    has become an extremely popular choice in NLP pipelines for many data scientists
    and also a winning ingredient in many Kaggle competitions. Another amazing fact
    about DeBERTa is that it is one of the first NLP models which outperforms humans
    on the SuperGLUE benchmark. This single piece of evidence is enough to conclude
    that DeBERTa will remain for a long time in the history of LLMs.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经深入探讨了 DeBERTa 架构的主要方面。凭借解耦的注意力机制和增强的掩码编码算法，DeBERTa 已成为许多数据科学家在 NLP 流水线中的极受欢迎的选择，也是许多
    Kaggle 竞赛中的获胜要素。另一个关于 DeBERTa 的令人惊讶的事实是，它是首批在 SuperGLUE 基准测试中超越人类的 NLP 模型之一。这一单一证据足以得出结论：DeBERTa
    将在 LLM 历史上长时间存在。
- en: Resources
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Resources
- en: '[DeBERTa: Decoding-Enhanced BERT with Disentangled Attention](https://arxiv.org/pdf/2006.03654.pdf)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[DeBERTa: 解码增强的 BERT，带有解耦注意力](https://arxiv.org/pdf/2006.03654.pdf)'
- en: '*All images unless otherwise noted are by the author*'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有说明，否则所有图片均由作者提供*'
