- en: 'TiDE: the ‘embarrassingly’ simple MLP that beats Transformers'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TiDE：那个‘令人尴尬’的简单MLP，击败了Transformers
- en: 原文：[https://towardsdatascience.com/tide-the-embarrassingly-simple-mlp-that-beats-transformers-7db77d588079](https://towardsdatascience.com/tide-the-embarrassingly-simple-mlp-that-beats-transformers-7db77d588079)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/tide-the-embarrassingly-simple-mlp-that-beats-transformers-7db77d588079](https://towardsdatascience.com/tide-the-embarrassingly-simple-mlp-that-beats-transformers-7db77d588079)
- en: A deep exploration of TiDE, its implementation using Darts and a real life use
    case comparison with DeepAR and TFT (a Transformer architecture)
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入探讨TiDE，使用Darts的实现，以及与DeepAR和TFT（一个Transformer架构）的实际案例比较
- en: '[](https://medium.com/@rjguedes?source=post_page-----7db77d588079--------------------------------)[![Rafael
    Guedes](../Images/b3d000b3bce0113d2b2727e84db04870.png)](https://medium.com/@rjguedes?source=post_page-----7db77d588079--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7db77d588079--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7db77d588079--------------------------------)
    [Rafael Guedes](https://medium.com/@rjguedes?source=post_page-----7db77d588079--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@rjguedes?source=post_page-----7db77d588079--------------------------------)[![Rafael
    Guedes](../Images/b3d000b3bce0113d2b2727e84db04870.png)](https://medium.com/@rjguedes?source=post_page-----7db77d588079--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7db77d588079--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7db77d588079--------------------------------)
    [Rafael Guedes](https://medium.com/@rjguedes?source=post_page-----7db77d588079--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7db77d588079--------------------------------)
    ·9 min read·Dec 8, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7db77d588079--------------------------------)
    ·9分钟阅读·2023年12月8日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: As industries continue to evolve, the importance of an accurate forecasting
    becomes a non-negotiable asset whether you work in e-commerce, healthcare, retail
    or even in agriculture. The importance of being able to foresee what comes next
    and plan accordingly to overcome future challenges is what can make you ahead
    of competition and thrive in an economy where margins are tight and the customers
    are more demanding than ever.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 随着各行业的不断发展，准确预测的重要性成为了一项不可妥协的资产，无论你是在电子商务、医疗保健、零售业，甚至是农业领域工作。能够预见未来并相应地制定计划，以克服未来的挑战，是使你在竞争中领先并在利润紧张、客户要求比以往任何时候都高的经济环境中蓬勃发展的关键。
- en: Transformer architectures have been the hot topic in AI for the past few years,
    specially due to their success in Natural Language Processing (NLP) being one
    of the most successful use cases the chatGPT that took the attention of everyone
    regardless if you were an AI enthusiastic or not. But NLP is not the only subject
    where Transformers have been shown to outperform the state-of-the-art solutions,
    in Computer Vision as well with Stable Diffusion and its variants.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer架构在过去几年里一直是AI的热门话题，特别是由于它们在自然语言处理（NLP）中的成功，chatGPT就是最成功的用例之一，引起了所有人的关注，无论你是否是AI爱好者。然而，NLP并不是唯一一个展示Transformers超越最先进解决方案的领域，在计算机视觉中，Stable
    Diffusion及其变体也同样如此。
- en: But can Transformers outperform state-of-the-art models in time series? Although
    many efforts have been made to develop Transformers for time series forecasting,
    it seems that for long term horizons, simple linear models can outperform several
    Transformer based approaches.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 但Transformers能否超越最先进的时间序列模型？虽然已经付出了很多努力来开发用于时间序列预测的Transformers，但似乎对于长期预测，简单的线性模型能够超越几种基于Transformer的方法。
- en: In this article I explore TiDE, a simple deep learning model which is able to
    beat Transformer architectures in long term forecasting. I also provide a step-by-step
    implementation of TiDE to forecast weekly sales in a dataset from Walmart using
    Darts a forecasting library for Python. And finally, I compare the performance
    of TiDE, DeepAR and TFT in a real life use case from my company.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我探讨了TiDE，一个能够在长期预测中击败Transformer架构的简单深度学习模型。我还提供了使用Python的预测库Darts对Walmart数据集进行周销量预测的逐步实现。最后，我将TiDE、DeepAR和TFT在我公司实际案例中的表现进行了比较。
- en: '![](../Images/626ec398068535678f10ad555c839ffa.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/626ec398068535678f10ad555c839ffa.png)'
- en: 'Figure 1: TiDE a new forecasting model that is ‘embarrassingly’ simple MLP
    to beat Transformers ([source](https://unsplash.com/photos/landscape-photo-of-seashore-ZhsYjreGsSE))'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：TiDE 是一种全新的预测模型，它是“简单到尴尬”的 MLP，旨在超越 Transformers ([source](https://unsplash.com/photos/landscape-photo-of-seashore-ZhsYjreGsSE))
- en: As always, the code is available on [Github](https://github.com/rjguedes8/TiDE).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，代码可以在 [Github](https://github.com/rjguedes8/TiDE) 上找到。
- en: Time-series Dense Encoder Model (TiDE)
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间序列稠密编码器模型 (TiDE)
- en: TiDE is a novel time-series encoder-decoder model that has shown to outperform
    state-of-the-art Transformer models in long-time horizon forecast [1]. It is a
    multivariate time-series model that is able to use static covariates (e.g. brand
    of a product) and dynamic covariates (e.g. price of a product) which can be known
    or unknown for the forecast horizon.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: TiDE 是一种新颖的时间序列编码器-解码器模型，已显示出在长期预测中优于最先进的 Transformer 模型 [1]。它是一种多变量时间序列模型，能够使用静态协变量（例如产品品牌）和动态协变量（例如产品价格），这些协变量可以在预测范围内已知或未知。
- en: Unlike the complex architecture of Transformers, TiDE is based on a simple Encoder-Decoder
    architecture with only Multi-Layer Perceptron (MLP) and without any Attention
    layer.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Transformers 复杂的架构不同，TiDE 基于简单的编码器-解码器架构，仅使用多层感知机（MLP），没有任何注意力层。
- en: 'The **Encoder** is responsible for mapping the past and the covariates of a
    time-series into a dense representation of features through two key steps:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**编码器**负责通过两个关键步骤将时间序列的过去和协变量映射到特征的稠密表示：'
- en: '**Feature Projection** which reduces the dimensionality of the dynamic covariates
    for the whole look-back and the horizon. and;'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征投影**，它减少了整个回溯和预测范围内动态协变量的维度。并且；'
- en: '**Dense Encoder** which receives the output of the Feature Projection concatenated
    with static covariates and the past of the time-series and maps them into an embedding.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**稠密编码器**，它接收特征投影的输出，与静态协变量和时间序列的过去连接，并将它们映射到嵌入中。'
- en: 'The **Decoder** receives the embedding from the encoder and converts it into
    future predictions through two operations:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**解码器**接收来自编码器的嵌入，并通过两项操作将其转换为未来预测：'
- en: '**Dense Decoder** which maps the embedding into a vector per time-step in the
    horizon; and'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**稠密解码器**，它将嵌入映射到预测范围内每个时间步的向量；以及'
- en: '**Temporal Decoder** which combines the output of the Dense Decoder with the
    projected features of that time-step to produce the predictions.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间解码器**，它将稠密解码器的输出与该时间步的投影特征结合，以生成预测。'
- en: The **Residual Connection** linearly maps the look-back to a vector with the
    size of the horizon which is added to the output of the Temporal Decoder to produce
    the final predictions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**残差连接**将回溯线性映射到与预测范围大小相同的向量，并将其添加到时间解码器的输出中，以生成最终预测。'
- en: '![](../Images/19529d8caa00d1eacc12ba8367f9189b.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/19529d8caa00d1eacc12ba8367f9189b.png)'
- en: 'Figure 2: TiDE architecture ([source](https://arxiv.org/pdf/2304.08424.pdf))'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：TiDE 架构 ([source](https://arxiv.org/pdf/2304.08424.pdf))
- en: How to use TiDE in practice
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何在实践中使用 TiDE
- en: 'This section covers a step by step implementation of TiDE using a weekly sales
    dataset from Walmart available on [kaggle](https://www.kaggle.com/datasets/aslanahmedov/walmart-sales-forecast/data?select=train.csv)
    (License CC0: Public Domain) and resorting to a package called Darts.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了使用 Walmart 的每周销售数据集（可在 [kaggle](https://www.kaggle.com/datasets/aslanahmedov/walmart-sales-forecast/data?select=train.csv)（许可证
    CC0：公共领域）上获取）逐步实现 TiDE 的过程，并借助名为 Darts 的包。
- en: Darts is a Python library for forecasting and anomaly detection [2] that contains
    several models such as naive models to serve as baseline, traditional models like
    ARIMA or Holt-Winters, deep learning models like TiDE or TFT or tree based models
    like LightGBM or Random Forest. It also supports both univariate and multivariate
    models and some of them offer probabilistic forecasting solutions.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Darts 是一个用于预测和异常检测的 Python 库 [2]，包含多个模型，如用于基线的简单模型、传统模型（如 ARIMA 或 Holt-Winters）、深度学习模型（如
    TiDE 或 TFT）或基于树的模型（如 LightGBM 或随机森林）。它还支持单变量和多变量模型，其中一些模型提供概率预测解决方案。
- en: 'The training dataset has 2 years and 8 months of weekly sales and 5 columns:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据集包含 2 年 8 个月的每周销售数据和 5 列：
- en: '*Store* — store number and one of the static covariates'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Store* — 存储编号和一个静态协变量'
- en: '*Dept* — department number and the other static covariates'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Dept* — 部门编号和其他静态协变量'
- en: '*Date* — the temporal index of the time series which is weekly and it will
    be used to extract dynamic covariates like the week number and the month'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Date* — 时间序列的时间索引，按周分布，将用于提取动态协变量，如周数和月份'
- en: '*Weekly_Sales* — the target variable'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Weekly_Sales* — 目标变量'
- en: '*IsHoliday* — another dynamic covariate that identifies if there is a holiday
    in a certain week'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*IsHoliday* — 另一个动态协变量，用于识别某一周是否有假期'
- en: The test dataset has the same columns except for the target (***Weekly_Sales****)*.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 测试数据集具有相同的列，除了目标 (***Weekly_Sales****)*。
- en: 'We start by importing the libraries we need and defining some global variables
    like the date column, target column, static covariates, the frequency of our series
    and the scaler to use:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入所需的库，并定义一些全局变量，如日期列、目标列、静态协变量、序列的频率以及要使用的缩放器：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The default scaler is MinMax Scaler, but we can use any we want from scikit-learn
    as long as it has `fit()`, `transform()` and `inverse_transform()` methods. The
    same happens for the transformer which by default is Label Encoder from scikit-learn.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的缩放器是 MinMax Scaler，但我们可以使用来自 scikit-learn 的任何缩放器，只要它具备 `fit()`、`transform()`
    和 `inverse_transform()` 方法。转换器也是如此，默认情况下是来自 scikit-learn 的 Label Encoder。
- en: After that, we load our training dataset and we convert the pandas data frame
    into TimeSeries which is the expected format from Darts.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们加载训练数据集，并将 pandas 数据框转换为 TimeSeries，这是 Darts 所期望的格式。
- en: I did not performed an EDA since my goal is just to show you how to implement
    it, but I noticed some negative values which might indicate returns. Nevertheless,
    I considered them as errors and I replaced them with 0.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我没有进行探索性数据分析，因为我的目标只是向你展示如何实现它，但我注意到了一些负值，这可能表示退货。然而，我将这些负值视为错误，并将其替换为 0。
- en: I also used the argument `fill_missing_dates` to add missing weeks and I filled
    those dates also with 0.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我还使用了 `fill_missing_dates` 参数来添加缺失的周，并将这些日期也填充为 0。
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We also load our test dataset so that we define what is our forecast horizon
    and what are the holidays in the forecast horizon.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还加载了测试数据集，以便定义我们的预测范围以及预测范围中的假期。
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'After that, for each series in the training set we create dynamic covariates
    (week, month and a binary column that idenitfies the presence of a holiday in
    a specific week):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，对于训练集中的每个序列，我们创建动态协变量（周、月和一个识别特定周假期的二进制列）：
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, that we have all the data ready we just need to scale our data:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好所有数据，只需对数据进行缩放：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Finally, we are ready to make predictions!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们准备好进行预测了！
- en: In our case, we will predict for the next 38 weeks the weekly sales for the
    same series that we trained with, however you can also forecast for time series
    that are not part of your training set as long as they have the same static covariates.
    For that, you have to repeat the same process of data preparation.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们将预测接下来的 38 周的同一系列的每周销售额，不过你也可以预测不在训练集中的时间序列，只要它们具有相同的静态协变量。为此，你必须重复相同的数据准备过程。
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here we have an example of the forecast for Store 1 and Dept 1, where we can
    see that the model was able to forecast a spike in the week of Black Friday and
    Thanksgiving from 2012 due to the three dynamic covariates that we had (week,
    month and the binary column which identifies a holiday in a certain week). We
    can also see that there are several spikes across the year that comes probably
    from discounts or marketing campaigns that can be handled by dynamic covariates
    to improve our forecast and that data is also available on kaggle.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个商店 1 和部门 1 的预测示例，我们可以看到模型能够预测 2012 年黑色星期五和感恩节那周的激增，这是由于我们拥有的三个动态协变量（周、月以及识别某一周是否有假期的二进制列）。我们还可以看到，一年中有几个激增，可能来自折扣或营销活动，这可以通过动态协变量来处理，以改进我们的预测，这些数据也可以在
    kaggle 上找到。
- en: '![](../Images/a2d585f533cb93ed6e3f4d3cf249e02e.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a2d585f533cb93ed6e3f4d3cf249e02e.png)'
- en: 'Figure 3: Forecast result for Store 1 and Department 1 (image made by the author)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：商店 1 和部门 1 的预测结果（图片由作者制作）
- en: TiDE vs DeepAR and TiDE vs TFT in a real life use case
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TiDE 与 DeepAR 的比较以及 TiDE 与 TFT 在实际应用中的比较
- en: In my company, we deployed, in the end of 2022, a new forecasting model which
    aimed to predict the volume of orders of 264 time series for the next 16 weeks.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的公司，我们在 2022 年底部署了一个新的预测模型，旨在预测接下来 16 周内 264 个时间序列的订单量。
- en: The model that beat the one in production at that time was DeepAR, a deep learning
    architecture available in the Python library GluonTS [3]. Like TiDE, DeepAR allows
    the use of static and dynamic covariates.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当时击败生产中模型的模型是 DeepAR，一种可在 Python 库 GluonTS [3] 中找到的深度学习架构。像 TiDE 一样，DeepAR 允许使用静态和动态协变量。
- en: Although DeepAR was providing us good results, it suffered from a problem on
    longer horizons (+8 weeks) that we called *‘Exploding Predictions’.* From one
    week to the other, with just one more week of data, the model would make a total
    different forecast from the previous week with volumes higher than normal.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管DeepAR为我们提供了良好的结果，但它在较长时间范围（+8周）内出现了我们称之为*‘预测爆炸’*的问题。从一周到另一周，仅增加一周的数据，模型就会给出与前一周完全不同的预测，预测的量高于正常水平。
- en: Nevertheless, we came up with mechanisms to control this and change one particular
    hyper parameter (context length, that DeepAR is really sensitive to) to avoid
    this kind of situations, but lately it has been shown to not be enough and we
    had to closely monitor every week the results and change other hyper parameters
    to come up with a reasonable forecast.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，我们制定了控制机制，并更改了一个特定的超参数（上下文长度，DeepAR对此非常敏感），以避免此类情况，但最近发现这还不够，我们不得不每周密切监控结果，并调整其他超参数以获得合理的预测。
- en: So we decided to start a new round of research to find a model that is more
    stable and reliable and it was when we found TiDE. We optimised TiDE in terms
    of hyper parameters, which dynamic and static covariates to use, which series
    would go to training and which would not, etc. And we compared both optimised
    models, DeepAR and TiDE, in 26 different cut off dates for a entire year of data
    from July 2022 to July 2023.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们决定开始新一轮研究，寻找一个更稳定可靠的模型，这时我们发现了TiDE。我们对TiDE进行了超参数优化，包括使用哪些动态和静态协变量，哪些系列用于训练，哪些不用于训练等。然后我们在2022年7月到2023年7月的整整一年数据中，比较了优化后的DeepAR和TiDE模型在26个不同的截止日期上的表现。
- en: The results showed that TiDE was not just better than DeepAR in the short and
    long term (as you can see in Figure 4) but it did not suffer from the initial
    problem we wanted to solve of *‘Exploding Predictions’.*
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，TiDE不仅在短期和长期内优于DeepAR（如图4所示），而且也解决了我们希望解决的*‘预测爆炸’*初始问题。
- en: '![](../Images/21367689ffd66f6b2cde6f28259487f9.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/21367689ffd66f6b2cde6f28259487f9.png)'
- en: 'Figure 4: Comparison between DeepAR and TiDE for a real use case. The metric
    used for comparison was MSE, however it is not seen in the y axis for confidential
    purposes (image made by the author).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：DeepAR与TiDE在实际用例中的比较。用于比较的指标是MSE，但由于保密原因在y轴上未显示（图像由作者制作）。
- en: During our model research we also compared TiDE with TFT [4], a Transformer
    architecture to verify what the authors in [1] stated about TiDE beating Transformer
    architectures in long term forecasting. And, as we can see from Figure 5, TiDE
    was able to beat TFT, specially in the long term (6+ weeks).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的模型研究中，我们还将TiDE与TFT [4]（一种Transformer架构）进行了比较，以验证[1]中作者关于TiDE在长期预测中超越Transformer架构的说法。正如图5所示，TiDE能够击败TFT，特别是在长期预测（6周以上）中。
- en: '![](../Images/76fd9954ac8aec86b18cdfbfda0d967e.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/76fd9954ac8aec86b18cdfbfda0d967e.png)'
- en: 'Figure 5: Comparison between TFT and TiDE for a real use case (image made by
    the author).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：TFT与TiDE在实际用例中的比较（图像由作者制作）。
- en: Conclusion
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Transformer architectures will be the foundation for the next big revolution
    in the history of humanity. Although they are amazing for NLP and Computer Vision,
    they cannot outperform simpler models on long term forecasting as stated by the
    authors in [1].
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer架构将是人类历史上下一次重大革命的基础。尽管它们在NLP和计算机视觉领域表现出色，但正如[1]中的作者所述，它们在长期预测方面无法超越更简单的模型。
- en: In this article we compared TFT, a Transformer architecture, and DeepAR with
    TiDE, and we verified that, for our use case, TiDE was able to beat both models.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将TFT（一种Transformer架构）和DeepAR与TiDE进行了比较，验证了对于我们的用例，TiDE能够超越这两个模型。
- en: Nevertheless, if Transformer architectures were robust enough for long term
    forecasting, why would Google develop a new non Transformer time series model?
    TSMixer [5] is the latest model developed by Google for time series that beats
    transformers (including TFT) in M5 competition.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Transformer架构足够强大以进行长期预测，但为什么谷歌还会开发一种新的非Transformer时间序列模型？TSMixer [5] 是谷歌开发的最新时间序列模型，它在M5竞赛中击败了包括TFT在内的Transformers。
- en: For now, simpler models seem to be better for forecasting but let’s see what
    the future holds in this subject and if Transformers can be improved to provide
    better results on the long term!
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，更简单的模型似乎在预测方面更具优势，但让我们拭目以待未来的发展，看看Transformers是否能得到改进，以提供更好的长期结果！
- en: '**Keep in touch:** [LinkedIn](https://www.linkedin.com/in/rafaelguedes97/),
    [Medium](https://medium.com/@rjguedes97)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**保持联系：** [LinkedIn](https://www.linkedin.com/in/rafaelguedes97/), [Medium](https://medium.com/@rjguedes97)'
- en: References
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Abhimanyu Das, Weihao Kong, Andrew Leach, Shaan Mathur, Rajat Sen, Rose
    Yu. Long-term Forecasting with TiDE: Time-series Dense Encoder. arXiv:2304.08424,
    2023'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Abhimanyu Das, Weihao Kong, Andrew Leach, Shaan Mathur, Rajat Sen, Rose
    Yu. 使用TiDE进行长期预测：时间序列密集编码器。arXiv:2304.08424, 2023'
- en: '[2] Julien Herzen, Francesco Lässig, Samuele Giuliano Piazzetta, Thomas Neuer,
    Léo Tafti, Guillaume Raille, Tomas Van Pottelbergh, Marek Pasieka, Andrzej Skrodzki,
    Nicolas Huguenin, Maxime Dumonal, Jan Kościsz, Dennis Bader, Frédérick Gusset,
    Mounir Benheddi, Camila Williamson, Michal Kosinski, Matej Petrik, Gaël Grosch.
    Darts: User-Friendly Modern Machine Learning for Time Series, 2022'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Julien Herzen, Francesco Lässig, Samuele Giuliano Piazzetta, Thomas Neuer,
    Léo Tafti, Guillaume Raille, Tomas Van Pottelbergh, Marek Pasieka, Andrzej Skrodzki,
    Nicolas Huguenin, Maxime Dumonal, Jan Kościsz, Dennis Bader, Frédérick Gusset,
    Mounir Benheddi, Camila Williamson, Michal Kosinski, Matej Petrik, Gaël Grosch.
    Darts: 用户友好的现代机器学习时间序列，2022'
- en: '[3] Alexander Alexandrov, Konstantinos Benidis, Michael Bohlke-Schneider, Valentin
    Flunkert, Jan Gasthaus, Tim Januschowski, Danielle C. Maddix, Syama Rangapuram,
    David Salinas, Jasper Schulz, Lorenzo Stella, Ali Caner Türkmen, Yuyang Wang.
    GluonTS: Probabilistic Time Series Models in Python. arXiv:1906.05264, 2019'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Alexander Alexandrov, Konstantinos Benidis, Michael Bohlke-Schneider, Valentin
    Flunkert, Jan Gasthaus, Tim Januschowski, Danielle C. Maddix, Syama Rangapuram,
    David Salinas, Jasper Schulz, Lorenzo Stella, Ali Caner Türkmen, Yuyang Wang.
    GluonTS: Python中的概率时间序列模型。arXiv:1906.05264, 2019'
- en: '[4] Bryan Lim, Sercan O. Arik, Nicolas Loeff, Tomas Pfister. Temporal Fusion
    Transformers for Interpretable Multi-horizon Time Series Forecasting. arXiv:1912.09363,
    2019'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Bryan Lim, Sercan O. Arik, Nicolas Loeff, Tomas Pfister. 时间融合变压器用于可解释的多时间跨度时间序列预测。arXiv:1912.09363,
    2019'
- en: '[5] Si-An Chen, Chun-Liang Li, Nate Yoder, Sercan O. Arik, Tomas Pfister. TSMixer:
    An All-MLP Architecture for Time Series Forecasting. arXiv:2303.06053, 2023'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Si-An Chen, Chun-Liang Li, Nate Yoder, Sercan O. Arik, Tomas Pfister. TSMixer:
    用于时间序列预测的全MLP架构。arXiv:2303.06053, 2023'
