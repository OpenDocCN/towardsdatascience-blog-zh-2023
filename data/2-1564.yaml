- en: 'Learning Transformers Code First: Part 1 — The Setup'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习 Transformers 代码优先：第 1 部分 — 设置
- en: 原文：[https://towardsdatascience.com/nanogpt-learning-transformers-code-first-part-1-f2044cf5bca0](https://towardsdatascience.com/nanogpt-learning-transformers-code-first-part-1-f2044cf5bca0)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/nanogpt-learning-transformers-code-first-part-1-f2044cf5bca0](https://towardsdatascience.com/nanogpt-learning-transformers-code-first-part-1-f2044cf5bca0)
- en: A 4 Part Exploration of Transformers Using nanoGPT as a Starting Point
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 nanoGPT 作为起点的 4 部分 Transformers 探索
- en: '[](https://medium.com/@oaguy1?source=post_page-----f2044cf5bca0--------------------------------)[![Lily
    Hughes-Robinson](../Images/b610721a40e274e7fb81418395314ae3.png)](https://medium.com/@oaguy1?source=post_page-----f2044cf5bca0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f2044cf5bca0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f2044cf5bca0--------------------------------)
    [Lily Hughes-Robinson](https://medium.com/@oaguy1?source=post_page-----f2044cf5bca0--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@oaguy1?source=post_page-----f2044cf5bca0--------------------------------)[![Lily
    Hughes-Robinson](../Images/b610721a40e274e7fb81418395314ae3.png)](https://medium.com/@oaguy1?source=post_page-----f2044cf5bca0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----f2044cf5bca0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----f2044cf5bca0--------------------------------)
    [Lily Hughes-Robinson](https://medium.com/@oaguy1?source=post_page-----f2044cf5bca0--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f2044cf5bca0--------------------------------)
    ·8 min read·Jul 7, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----f2044cf5bca0--------------------------------)
    ·阅读时间 8 分钟·2023年7月7日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/dfca366786f1d736d9f6aebbeedc4b65.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dfca366786f1d736d9f6aebbeedc4b65.png)'
- en: Photo by [Josh Riemer](https://unsplash.com/@joshriemer?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Josh Riemer](https://unsplash.com/@joshriemer?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: I don’t know about you, but sometime looking at code is easier than reading
    papers. When I was working on [AdventureGPT](https://github.com/oaguy1/AdventureGPT),
    I started by reading the source code to [BabyAGI](https://github.com/yoheinakajima/babyagi),
    an implementation of the [ReAct paper](https://arxiv.org/abs/2210.03629) in around
    600 lines of python.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 不知道你是否和我一样，有时查看代码比阅读论文更简单。当我在开发 [AdventureGPT](https://github.com/oaguy1/AdventureGPT)
    时，我首先阅读了 [BabyAGI](https://github.com/yoheinakajima/babyagi) 的源代码，这是一种用大约 600
    行 Python 实现的 [ReAct 论文](https://arxiv.org/abs/2210.03629)。
- en: Recently, I became aware of a recent paper called [TinyStories](https://arxiv.org/abs/2305.07759)
    through [episode 33](https://www.cognitiverevolution.ai/e33-the-tiny-model-revolution-with-ronen-eldan-and-yuanzhi-li-of-microsoft-research/)
    of the excellent [Cognitive Revolution Podcast](https://www.cognitiverevolution.ai/).
    TinyStories attempts to show that models trained on millions (not billions) of
    parameters can be effective with high-enough quality data. In the case of the
    Microsoft researchers in the paper, they utilized synthetic data generated from
    GPT-3.5 and GPT-4 that would have cost around $10k retail to generate. The dataset
    and models are available from the author’s [HuggingFace repo](https://huggingface.co/roneneldan).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，我通过优秀的 [Cognitive Revolution Podcast](https://www.cognitiverevolution.ai/)
    的 [第33集](https://www.cognitiverevolution.ai/e33-the-tiny-model-revolution-with-ronen-eldan-and-yuanzhi-li-of-microsoft-research/)
    了解到了一篇名为 [TinyStories](https://arxiv.org/abs/2305.07759) 的最新论文。TinyStories 尝试展示经过数百万（而不是数十亿）参数训练的模型，在足够高质量的数据下也能有效。在论文中的微软研究人员的案例中，他们使用了从
    GPT-3.5 和 GPT-4 生成的合成数据，这些数据生成的零售成本大约为 $10k。数据集和模型可以从作者的 [HuggingFace repo](https://huggingface.co/roneneldan)
    获取。
- en: I was captivated to hear that a model could be trained on 30M and fewer parameters.
    For reference, I am running all my model training and inference on a Lenovo Legion
    5 laptop with a GTX 1660 Ti. Even just for inference, most models with over 3B
    parameters are too large to run on my machine. I know there are cloud compute
    resources available for a price, but I am learning all this in my spare time and
    can really only afford the modest OpenAI bill I rack up via API calls. Therefore,
    the idea that there were models I could train on my modest hardware instantly
    lit me up.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 听到一个模型可以在30M及更少参数上进行训练，我感到很着迷。作为参考，我在搭载GTX 1660 Ti的Lenovo Legion 5笔记本上进行所有模型训练和推理。即使仅仅是推理，大多数拥有超过3B参数的模型也太大，无法在我的机器上运行。我知道有付费的云计算资源，但我是在空闲时间学习这些内容，实际上只能负担得起通过API调用产生的适度OpenAI账单。因此，能够在我的普通硬件上训练模型的想法让我立刻振奋起来。
- en: I started reading the TinyStories paper and soon realized that they utilized
    the now defunct [GPT Neo](https://github.com/EleutherAI/gpt-neo) model in there
    model training. I started digging into the code to see if I could understand it
    and I realized I needed something even smaller to start from. For context, I am
    mainly a backend software engineer with just enough machine learning experience
    to not get completely lost when hearing people talk about neural nets. I am nowhere
    near a proper ML engineer and this led me to type “gpt from scratch” into my preferred
    search engine to find a gentler introduction. I found the video below and everything
    shifted.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我开始阅读TinyStories论文，很快意识到他们在模型训练中使用了已经停用的[GPT Neo](https://github.com/EleutherAI/gpt-neo)模型。我开始深入研究代码，看看是否能理解它，并意识到我需要从更小的东西开始。为了提供背景，我主要是一名后端软件工程师，具有足够的机器学习经验以便在听到别人谈论神经网络时不会完全迷失。我离一个真正的ML工程师还很远，这使我在搜索引擎中输入了“gpt
    from scratch”以寻找更温和的入门介绍。我找到下面的视频，一切都发生了变化。
- en: This was what I was looking for. In addition to the basic repo linked in the
    video, there is a polished version called [nanoGPT](https://github.com/karpathy/nanoGPT)
    which is still under active development. What is more, **the training code and
    model code are around 300 lines of python each**. To me, that was even more exciting
    than the video. I closed the video and started pouring over the source code. nanoGPT
    utilizes PyTorch, which I’ve never used before. It also features just enough math
    to make and machine learning jargon to make the neophyte in me anxious. This was
    going to be something of a bigger undertaking than I anticipated.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我一直在寻找的。除了视频中链接的基本代码库，还有一个名为[nanoGPT](https://github.com/karpathy/nanoGPT)的精简版本，目前仍在积极开发中。更重要的是，**训练代码和模型代码每个大约有300行python**。对我来说，这比视频更令人兴奋。我关闭了视频，开始仔细研究源代码。nanoGPT利用了我从未使用过的PyTorch。它还涉及了足够的数学和机器学习术语，让我这位新手感到紧张。这将是一个比我预期更大的工程。
- en: One of the best ways to understand something is to write about. Therefore, I
    plan on picking apart the code in the nanoGPT repo, reading the famous “[Attention
    is All You Need](https://arxiv.org/abs/1706.03762)” paper, and learning transformers
    in a bottoms-up, hands on way. Whatever I learn along the way I hope to write
    about in this series. If you want to follow along, clone the nanoGPT repo to your
    machine (the model can even be trained on CPU, so no hardware excuses) and follow
    along.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 理解某件事的最佳方法之一是写下来。因此，我计划深入研究nanoGPT代码库中的代码，阅读著名的“[Attention is All You Need](https://arxiv.org/abs/1706.03762)”论文，并以自下而上的实践方式学习变换器。无论我在这个过程中学到什么，我都希望在这个系列中写下来。如果你想跟随学习，可以将nanoGPT代码库克隆到你的机器上（模型甚至可以在CPU上训练，所以没有硬件借口），并进行跟随。
- en: The first thing I did after cloning the repo was follow the README’s instructions
    for training the simplest model, the character-level generation model using the
    [tiny_shakespeare dataset](https://huggingface.co/datasets/tiny_shakespeare).
    There is a script to prepare the dataset for training, a script to do the actual
    training, and a sampling script to output generated text. With a few terminal
    commands and an hour+ of training, I had simple model to output Shakespearean-sounding
    text.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 克隆代码库后，我做的第一件事是按照README的指示训练最简单的模型，即使用[tiny_shakespeare数据集](https://huggingface.co/datasets/tiny_shakespeare)的字符级生成模型。这里有一个脚本用于准备训练数据，一个脚本用于实际训练，还有一个采样脚本用于输出生成的文本。通过几个终端命令和一个多小时的训练，我得到一个能够输出类似莎士比亚风格文本的简单模型。
- en: Following instructions is all well and good, but I don’t really understand something
    until I modify it to work for my own use case. My goal here was to train a similar
    character-level model using the TinyStories dataset. This required creating my
    own data preparation script to get the dataset ready for training. Let’s dig into
    that deeper.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循说明是好的，但我直到将其修改为适合自己的用例后才真正理解某些东西。我的目标是使用 TinyStories 数据集训练一个类似的字符级模型。这需要创建我自己的数据准备脚本，以使数据集准备好进行训练。让我们*深入探讨*一下这个问题。
- en: 'The nanoGPT has two types of data preparation scripts: one for GPT-2 style
    models and one for character-level models. I grabbed some of the code from the
    GPT-2 models for downloading from HuggingFace repositories and took everything
    else from the tiny_shakespeare character-level script. One important point here,
    tiny_shakespeare is just over 1MB and contains only 40k lines of Shakespeare.
    TinyStories is over 3GB compressed and contains 39.7M stories. The methods for
    tokenizing and slicing tiny_shakespeare were not directly transferable, at least
    not with the 32GB of RAM my laptop has. I crashed my machine several times trying
    pythonic, easy-to-read methods of preparing TinyStories. The final script uses
    a few tricks I will detail below.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: nanoGPT 有两种类型的数据准备脚本：一种用于 GPT-2 风格模型，一种用于字符级模型。我从 HuggingFace 仓库下载的 GPT-2 模型中提取了一些代码，其余的都来自
    tiny_shakespeare 字符级脚本。这里一个重要的点是，tiny_shakespeare 仅有 1MB 多一点，仅包含 40k 行莎士比亚的作品。而
    TinyStories 压缩后超过 3GB，包含 39.7M 个故事。对 tiny_shakespeare 进行分词和切片的方法不能直接转移，至少在我的笔记本电脑上（拥有
    32GB RAM）是这样。我在尝试 pythonic、易读的数据准备方法时多次崩溃了我的机器。最终脚本使用了一些技巧，我将在下面详细说明。
- en: First off, my preferred solution for processing lists of data is [list comprehension](https://realpython.com/list-comprehension-python/),
    a syntax for generating new lists from existing lists with modifications. The
    issue with list comprehension in this case is that that 3GB of compressed text
    becomes closer to 10GB in RAM. Now, list comprehension requires multiple copies
    of the list in RAM. Not an issue for small data, but unworkable for TinyStories.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我处理数据列表的首选方案是 [列表推导式](https://realpython.com/list-comprehension-python/)，这是一种从现有列表生成新列表并进行修改的语法。在这种情况下，列表推导式的问题是，3GB
    的压缩文本在 RAM 中变得接近 10GB。现在，列表推导式需要在 RAM 中多次复制列表。对于小数据不是问题，但对 TinyStories 来说不可行。
- en: 'The outputs of the data preparation scripts is a compressed NumPy array of
    character level encoding for the train and validation data plus a metadata pickle
    which includes the full list of unique characters and the encoding/decoding maps
    to convert these characters to numbers. Using this as reference, we don’t need
    anything other than the final encoded array of numbers once the unique characters
    are found and mapped to numbers. The best way to do this memory efficiently is
    to iterate through a the data with a simple for-loop while building these outputs
    piece-mils. To do this, you initialize an initial variable before the loop which
    then gets updated each interaction. This prevents multiple versions of the dataset
    from being held in RAM and only outputs what we need. The final vocab generation
    code is below:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备脚本的输出是一个压缩的 NumPy 数组，包含训练和验证数据的字符级编码，以及一个包含唯一字符完整列表和编码/解码映射的元数据 pickle，以将这些字符转换为数字。以此为参考，一旦找到并映射到数字，我们不需要其他任何东西，除了最终的编码数字数组。最有效的内存使用方法是通过简单的
    for 循环迭代数据，同时分段构建这些输出。为此，在循环前初始化一个变量，然后在每次交互中更新。这样可以防止在 RAM 中保存数据集的多个版本，只输出我们需要的内容。最终的词汇生成代码如下：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: That said, an array of 30.7M stories (over 4B characters) encoded as numbers
    still takes up a non-trivial amount of RAM because Python is storing the ints
    dynamically. Enter NumPy, which has a much more efficient array storage where
    you can specify the exact size of the ints. In addition to the efficient storage,
    NumPy also has a memory efficient array concatenation which can be used to build
    the final encoded array iteratively rather than all at once.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，将 30.7M 个故事（超过 40 亿字符）编码为数字的数组仍然占用大量 RAM，因为 Python 是动态存储整数的。这里引入 NumPy，它具有更高效的数组存储，可以指定整数的确切大小。除了高效的存储，NumPy
    还有内存高效的数组连接，可以用于逐步构建最终的编码数组，而不是一次性完成。
- en: My finishing touch on the script was to add a progress bar using [tqdm](https://pypi.org/project/tqdm/)
    for each step and I was finally ready to run the script. So, I ran it overnight
    and came back in the morning. When I came back, the script was still running,
    with over 100 estimated hours of compute time remaining.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我对脚本的最后修饰是使用 [tqdm](https://pypi.org/project/tqdm/) 为每一步添加一个进度条，最后我准备好运行脚本。所以，我让它过夜运行，早上回来时，脚本仍在运行，估计剩余计算时间超过
    100 小时。
- en: 'This is when it really hit me: 30.7M stories is small for a language model,
    but is very much not a toy dataset to be processed on a single thread. It was
    time to bring in the big guns: parallelization. Parallelism brings in a lot of
    complexity and overhead, but the performance gains was worth the trade off. Luckily,
    there are a number of ways to parallelize Python code. Many of these solutions
    require major rewrites to a serially executed script or complicated abstractions.
    With a little digging, I found something that allowed me to keep most of my script
    the same but still run multiple processes to take advantage of all of my threads.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这时我真正意识到：30.7M 的故事对于语言模型来说虽然小巧，但绝对不是一个可以在单线程上处理的玩具数据集。是时候引入大招：并行化了。并行化带来了许多复杂性和开销，但性能提升是值得的。幸运的是，有许多方法可以并行化
    Python 代码。许多解决方案需要对串行执行的脚本进行重大重写或复杂的抽象。经过一番挖掘，我找到了一种方法，让我可以保持大部分脚本不变，但仍然运行多个进程以利用所有线程。
- en: '[Ray](https://www.ray.io) is a library for easily parallelizing methods in
    Python and can easily be run locally or as a cluster. It handles running tasks
    in a queue and spinning up worker processes to eat away at that queue. There is
    an excellent guide to ray below if this has whet your appetite.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ray](https://www.ray.io) 是一个可以轻松地将 Python 方法并行化的库，可以在本地或集群中运行。它处理任务队列中的任务并启动工作进程来处理队列。如果这引起了你的兴趣，下面有一个很好的
    ray 指南。'
- en: '[](/modern-parallel-and-distributed-python-a-quick-tutorial-on-ray-99f8d70369b8?source=post_page-----f2044cf5bca0--------------------------------)
    [## Modern Parallel and Distributed Python: A Quick Tutorial on Ray'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/modern-parallel-and-distributed-python-a-quick-tutorial-on-ray-99f8d70369b8?source=post_page-----f2044cf5bca0--------------------------------)
    [## 现代并行和分布式 Python：Ray 的快速教程'
- en: Ray is an open source project for parallel and distributed Python.
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Ray 是一个用于并行和分布式 Python 的开源项目。
- en: towardsdatascience.com](/modern-parallel-and-distributed-python-a-quick-tutorial-on-ray-99f8d70369b8?source=post_page-----f2044cf5bca0--------------------------------)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/modern-parallel-and-distributed-python-a-quick-tutorial-on-ray-99f8d70369b8?source=post_page-----f2044cf5bca0--------------------------------)
- en: 'When it came to choosing what to parallelize, the encode function seemed like
    a good candidate. It has clear inputs and outputs, no side effects on those inputs,
    and was easily one of the largest portions of the compute time. Adapting the existing
    code to work with ray couldn’t have been easier: the function becomes accessible
    to ray via a decorator, the functional call changes slightly to add a remote attribute,
    and there is a function to kick off executing all the data. Below is an example
    of how it looked in my code base initially:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择并行化内容时，encode 函数似乎是一个不错的候选者。它有明确的输入和输出，对这些输入没有副作用，而且是计算时间中最大的一部分之一。将现有代码适配
    ray 变得非常简单：通过装饰器使函数对 ray 可用，功能调用稍微更改以添加远程属性，并有一个函数来启动所有数据的执行。以下是最初在我的代码库中呈现的样子：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Armed with all my CPU’s power, I forged ahead only to immediately crash my laptop.
    With the locally distributed call stack used by ray, the entire dataset was in
    memory several times over. Simply enqueuing the entire dataset caused an out-of-memory
    error. Annoyed, I used this as an excuse to buy more RAM (64GB here we come!),
    but continued to tweak the code while the RAM shipped.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有了所有 CPU 的力量，我继续前进，却立即导致了我的笔记本电脑崩溃。由于 ray 使用的本地分布式调用栈，整个数据集在内存中存在了几次。仅仅将整个数据集放入队列就导致了内存不足错误。我感到恼火，借此机会买了更多的
    RAM（64GB，来了！），但在 RAM 发货期间继续调整代码。
- en: The next logical place was to batch the requests being handled by ray into something
    that could fit inside a reasonable amount of memory. Adding batching logic was
    reasonably straightforward and is present in the final codebase I will link to
    at the end of the article. What actually became interesting was experimenting
    with the batch size. Initially, I chose a random batch size (5000) and it started
    out well, but it became obvious to me that a fair amount of time was being spent
    on the single-threaded code during each batch.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个合乎逻辑的步骤是将ray处理的请求批量化成适合合理内存的大小。添加批量逻辑相对简单，并且在最终的代码库中有体现，链接将在文章末尾提供。实际上，令人感兴趣的是实验批量大小。一开始，我选择了一个随机的批量大小（5000），效果不错，但我很快意识到在每个批次中，单线程代码占用了相当多的时间。
- en: Essentially, watching my preferred system monitor, I saw a single core pinned
    for minutes before finally all my laptop’s cores lit up for a few seconds before
    going back to a only a single core being utilized. This lead my to play with the
    batch size a bit, hoping to feed the starving CPU cores faster and keep them engaged
    longer. Lowering the batch size didn’t help because there was so much synchronous
    code in each batch used to slice and prepare a batch from the full dataset. That
    code couldn’t be parallelized, so it meant that each batch had a large startup
    cost time wise generating the chunk. This led me to try the opposite, increasing
    the chunk size to keep the cores more engaged for longer. This worked, as chunk
    generation took the same amount of time regardless of chunk size, but each chunk
    processed more data. Combining this with moving my encoding post-processing into
    ray functions, I was able to chew through 30% of the training dataset in just
    a few hours, all on a single laptop.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，在观察我首选的系统监视器时，我看到一个核心被占用数分钟，最后我所有笔记本的核心都亮起了几秒钟，然后又回到只有一个核心被使用。这使我尝试调整批量大小，希望能更快地喂养那些饥饿的CPU核心，并让它们保持更长时间的活动。减少批量大小没有帮助，因为每批中的同步代码用于从完整数据集中切分和准备一个批次。这些代码无法并行化，因此每个批次生成数据块的启动成本时间较大。这使我尝试了相反的方法，即增加数据块大小，以使核心保持更长时间的活跃。这有效，因为数据块生成的时间不论数据块大小如何都相同，但每个数据块处理了更多的数据。结合将编码后处理移到ray函数中，我能够在短短几个小时内处理30%的训练数据集，全部在一台笔记本电脑上完成。
- en: Finally, after a few more hours, I had a fully prepared, custom dataset to feed
    to the character-level model. I was pleased that I didn’t have to resort to utilizing
    expensive cloud compute to process the training set, which was my next move if
    the RAM increase didn’t work. What’s more, I learned intimately what it meant
    to create/process a dataset for a character-level model.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，在再过几个小时后，我拥有了一个完全准备好的自定义数据集，以供字符级模型使用。我很高兴我不需要求助于昂贵的云计算来处理训练集，如果RAM增加不起作用，这将是我的下一个步骤。更重要的是，我深入了解了为字符级模型创建/处理数据集的含义。
- en: In the next article in this series, I will be examining the actual model code,
    explaining as best I can and linking to copious external resources to provide
    additional information where my knowledge falls short. Once the article is written,
    I will go back and provide a link here. In the meantime, I’ve linked the final
    version of my dataset preparation script below so you can follow along and see
    what it takes to process a somewhat large dataset on a limited compute platform.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本系列的下一篇文章中，我将检查实际的模型代码，尽我所能地进行解释，并链接大量外部资源以提供额外的信息，以弥补我的知识不足。一旦文章写好，我将回来在这里提供一个链接。与此同时，我已经在下面链接了我数据集准备脚本的最终版本，您可以跟随并了解在有限计算平台上处理较大数据集所需的内容。
- en: '[](https://github.com/oaguy1/nanoGPT/blob/master/data/tinystories_char/prepare.py?source=post_page-----f2044cf5bca0--------------------------------)
    [## nanoGPT/data/tinystories_char/prepare.py at master · oaguy1/nanoGPT'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/oaguy1/nanoGPT/blob/master/data/tinystories_char/prepare.py?source=post_page-----f2044cf5bca0--------------------------------)
    [## nanoGPT/data/tinystories_char/prepare.py at master · oaguy1/nanoGPT'
- en: The simplest, fastest repository for training/finetuning medium-sized GPTs.
    - nanoGPT/data/tinystories_char/prepare.py…
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最简单、最快速的中型GPT训练/微调的代码库。 - nanoGPT/data/tinystories_char/prepare.py…
- en: github.com](https://github.com/oaguy1/nanoGPT/blob/master/data/tinystories_char/prepare.py?source=post_page-----f2044cf5bca0--------------------------------)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/oaguy1/nanoGPT/blob/master/data/tinystories_char/prepare.py?source=post_page-----f2044cf5bca0--------------------------------)
- en: Also, part two of the series is available! Click below to read!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，系列的第二部分已经上线！点击下面阅读！
- en: '[](/learning-transformers-code-first-part-2-gpt-up-close-and-personal-1635b52ae0d7?source=post_page-----f2044cf5bca0--------------------------------)
    [## Learning Transformers Code First Part 2 — GPT Up Close and Personal'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/learning-transformers-code-first-part-2-gpt-up-close-and-personal-1635b52ae0d7?source=post_page-----f2044cf5bca0--------------------------------)
    [## 学习变换器代码第一部分 — GPT 近距离了解'
- en: Digging into Generative Pre-Trained Transformers via nanoGPT
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深入研究通过nanoGPT生成预训练变换器
- en: towardsdatascience.com](/learning-transformers-code-first-part-2-gpt-up-close-and-personal-1635b52ae0d7?source=post_page-----f2044cf5bca0--------------------------------)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/learning-transformers-code-first-part-2-gpt-up-close-and-personal-1635b52ae0d7?source=post_page-----f2044cf5bca0--------------------------------)'
