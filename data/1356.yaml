- en: Fine-tune a Large Language Model with Python
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Python 微调大型语言模型
- en: 原文：[https://towardsdatascience.com/fine-tune-a-large-language-model-with-python-b1c09dbc58b2?source=collection_archive---------6-----------------------#2023-04-18](https://towardsdatascience.com/fine-tune-a-large-language-model-with-python-b1c09dbc58b2?source=collection_archive---------6-----------------------#2023-04-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/fine-tune-a-large-language-model-with-python-b1c09dbc58b2?source=collection_archive---------6-----------------------#2023-04-18](https://towardsdatascience.com/fine-tune-a-large-language-model-with-python-b1c09dbc58b2?source=collection_archive---------6-----------------------#2023-04-18)
- en: '![](../Images/1d8727d8fba25a5fd2ed429d267bf0c1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1d8727d8fba25a5fd2ed429d267bf0c1.png)'
- en: Photo by [Manouchehr Hejazi](https://unsplash.com/@patrol?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Manouchehr Hejazi](https://unsplash.com/@patrol?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Learn how to fine-tune a BERT from scratch on a custom dataset
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习如何从头开始在自定义数据集上微调 BERT
- en: '[](https://medium.com/@marcellopoliti?source=post_page-----b1c09dbc58b2--------------------------------)[![Marcello
    Politi](../Images/484e44571bd2e75acfe5fef3146ab3c2.png)](https://medium.com/@marcellopoliti?source=post_page-----b1c09dbc58b2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b1c09dbc58b2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b1c09dbc58b2--------------------------------)
    [Marcello Politi](https://medium.com/@marcellopoliti?source=post_page-----b1c09dbc58b2--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@marcellopoliti?source=post_page-----b1c09dbc58b2--------------------------------)[![Marcello
    Politi](../Images/484e44571bd2e75acfe5fef3146ab3c2.png)](https://medium.com/@marcellopoliti?source=post_page-----b1c09dbc58b2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b1c09dbc58b2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b1c09dbc58b2--------------------------------)
    [Marcello Politi](https://medium.com/@marcellopoliti?source=post_page-----b1c09dbc58b2--------------------------------)'
- en: ·
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7390355d40fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-a-large-language-model-with-python-b1c09dbc58b2&user=Marcello+Politi&userId=7390355d40fe&source=post_page-7390355d40fe----b1c09dbc58b2---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b1c09dbc58b2--------------------------------)
    ·4 min read·Apr 18, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb1c09dbc58b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-a-large-language-model-with-python-b1c09dbc58b2&user=Marcello+Politi&userId=7390355d40fe&source=-----b1c09dbc58b2---------------------clap_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7390355d40fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-a-large-language-model-with-python-b1c09dbc58b2&user=Marcello+Politi&userId=7390355d40fe&source=post_page-7390355d40fe----b1c09dbc58b2---------------------post_header-----------)
    发布在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b1c09dbc58b2--------------------------------)
    ·4 分钟阅读·2023年4月18日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb1c09dbc58b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-a-large-language-model-with-python-b1c09dbc58b2&user=Marcello+Politi&userId=7390355d40fe&source=-----b1c09dbc58b2---------------------clap_footer-----------)'
- en: --
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb1c09dbc58b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-a-large-language-model-with-python-b1c09dbc58b2&source=-----b1c09dbc58b2---------------------bookmark_footer-----------)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb1c09dbc58b2&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tune-a-large-language-model-with-python-b1c09dbc58b2&source=-----b1c09dbc58b2---------------------bookmark_footer-----------)'
- en: In this article, we will deal with the **fine-tuning of BERT for sentiment classification**
    using PyTorch. BERT is a large language model that offers a good balance between
    popularity and model size, which can be fine-tuned **using a simple GPU**. We
    can download a **pre-trained BERT from Hugging Face (HF)**, so there is no need
    to train it from scratch. In particular, we will use the distilled (smaller) version
    of BERT, called **Distil-BERT.**
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将探讨使用 PyTorch 对 **BERT 进行情感分类的微调**。BERT 是一种大型语言模型，在流行度和模型大小之间提供了良好的平衡，可以
    **使用简单的 GPU** 进行微调。我们可以从 Hugging Face (HF) 下载 **预训练的 BERT**，因此不需要从头开始训练。特别地，我们将使用
    BERT 的精简版，即 **Distil-BERT**。
- en: '[Distil-BERT](https://huggingface.co/docs/transformers/model_doc/distilbert#:~:text=DistilBERT%20is%20a%20small%2C%20fast,the%20GLUE%20language%20understanding%20benchmark.)
    is widely used in production since it has **40% fewer parameters** than BERT uncased.
    It runs **60% faster and retains 95% performance** in the [GLUE](https://arxiv.org/abs/1804.07461)
    language comprehension benchmark.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[Distil-BERT](https://huggingface.co/docs/transformers/model_doc/distilbert#:~:text=DistilBERT%20is%20a%20small%2C%20fast,the%20GLUE%20language%20understanding%20benchmark.)
    在生产中被广泛使用，因为它比 BERT uncased **减少了 40% 的参数**。它运行 **快 60% 并在 [GLUE](https://arxiv.org/abs/1804.07461)
    语言理解基准中保留了 95% 的性能**。'
- en: We start by installing all the necessary libraries. The first line is to capture
    the output of the installation and keep your notebook clean.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先安装所有必要的库。第一行代码用于捕获安装的输出，以保持你的笔记本整洁。
- en: I will use Deepnote to run the code in this article but you also use Google
    Colab if you prefer.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我将使用 Deepnote 来运行本文中的代码，但如果你愿意，也可以使用 Google Colab。
