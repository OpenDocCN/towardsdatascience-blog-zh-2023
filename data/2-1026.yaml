- en: Hands-On Deep Q-Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实战深度 Q 学习
- en: 原文：[https://towardsdatascience.com/hands-on-deep-q-learning-9073040ce841](https://towardsdatascience.com/hands-on-deep-q-learning-9073040ce841)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/hands-on-deep-q-learning-9073040ce841](https://towardsdatascience.com/hands-on-deep-q-learning-9073040ce841)
- en: '[Reinforcement Learning](https://medium.com/tag/reinforcement-learning)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[强化学习](https://medium.com/tag/reinforcement-learning)'
- en: Level up your agent to win more difficult games!
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提升你的代理，以赢得更困难的游戏！
- en: '[](https://dr-robert-kuebler.medium.com/?source=post_page-----9073040ce841--------------------------------)[![Dr.
    Robert Kübler](../Images/3b8d8b88f76c0c43d9c305e3885e7ab9.png)](https://dr-robert-kuebler.medium.com/?source=post_page-----9073040ce841--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9073040ce841--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9073040ce841--------------------------------)
    [Dr. Robert Kübler](https://dr-robert-kuebler.medium.com/?source=post_page-----9073040ce841--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://dr-robert-kuebler.medium.com/?source=post_page-----9073040ce841--------------------------------)[![Dr.
    Robert Kübler](../Images/3b8d8b88f76c0c43d9c305e3885e7ab9.png)](https://dr-robert-kuebler.medium.com/?source=post_page-----9073040ce841--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9073040ce841--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9073040ce841--------------------------------)
    [Dr. Robert Kübler](https://dr-robert-kuebler.medium.com/?source=post_page-----9073040ce841--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9073040ce841--------------------------------)
    ·14 min read·Nov 25, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9073040ce841--------------------------------)
    ·14 分钟阅读·2023 年 11 月 25 日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/8bdefc9ca92e8dc70018967dbcd36267.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8bdefc9ca92e8dc70018967dbcd36267.png)'
- en: Photo by [Sean Stratton](https://unsplash.com/@seanstratton?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Sean Stratton](https://unsplash.com/@seanstratton?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Reinforcement Learning is one of the most fascinating fields of machine learning.
    Unlike supervised learning, reinforcement learning models can learn complex processes
    independently, even without beautifully tabulated data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是机器学习中最迷人的领域之一。与监督学习不同，强化学习模型可以独立学习复杂的过程，即使没有精美整理的数据。
- en: For me, it is most fun to see AI agents win video games, but you can also use
    reinforcement learning to solve business problems. Just phrase it as a game, and
    off you go! You only have to define…
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对我来说，看到 AI 代理赢得视频游戏是最有趣的，但你也可以使用强化学习来解决商业问题。只需将其表达为游戏，就可以开始了！你只需要定义……
- en: the environment your agent lives in,
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的代理所处的环境，
- en: what decisions your agent can take, and
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的代理可以做出什么决策，以及
- en: what success and failure look like.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成功和失败的表现。
- en: '![](../Images/86ef974389efe4be3138d88a39347db3.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/86ef974389efe4be3138d88a39347db3.png)'
- en: Example of an AI agent mastering a game. Pick up a customer and bring them to
    the hotel. Image by the author.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: AI 代理掌握游戏的示例。接客并将其送到酒店。图片来源：作者。
- en: Before you continue, please read my introductory article about reinforcement
    learning. It gives you some more context and shows you how to conduct a simple,
    yet effective form of reinforcement learning yourself. It also serves as a basis
    for this article.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，请阅读我关于强化学习的介绍文章。它会给你更多背景信息，并展示如何自己进行简单但有效的强化学习。它也为本文提供了基础。
- en: '[](/a-practitioners-guide-to-reinforcement-learning-1f1e249f8fa5?source=post_page-----9073040ce841--------------------------------)
    [## A Practitioner’s Guide to Reinforcement Learning'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 实践者的强化学习指南](https://dr-robert-kuebler.medium.com/?source=post_page-----9073040ce841--------------------------------)'
- en: Take your first steps in writing game-winning AI agents
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 开始编写获胜的游戏 AI 代理的第一步
- en: towardsdatascience.com](/a-practitioners-guide-to-reinforcement-learning-1f1e249f8fa5?source=post_page-----9073040ce841--------------------------------)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[实践者的强化学习指南](/a-practitioners-guide-to-reinforcement-learning-1f1e249f8fa5?source=post_page-----9073040ce841--------------------------------)'
- en: In this article, you will learn about deep Q-learning, why we need it, and how
    to implement it yourself to master a game that looks much more difficult than
    the ones in my other article.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，你将了解深度 Q 学习，为什么我们需要它，以及如何自己实现它以掌握看起来比我另一篇文章中的游戏更复杂的游戏。
- en: '**You can find the code in** [**my Github**](https://github.com/Garve/towards_data_science/blob/main/Hands-On%20Deep%20Q-Learning/Hands-On%20Deep%20Q-Learning.ipynb)**.**'
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**你可以在** [**我的Github**](https://github.com/Garve/towards_data_science/blob/main/Hands-On%20Deep%20Q-Learning/Hands-On%20Deep%20Q-Learning.ipynb)**找到代码**。'
- en: Large Observation Spaces
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型观察空间
- en: In the article linked above, we conducted Q-learning to make an agent play some
    simple games with small **discrete observation spaces**. In the Frozen Lake game,
    as an example, you have 16 fields (=states or observations, I use these terms
    interchangeably from now on.) you can stand on in the 4x4 map. In the [gymnasium
    version of the card game Blackjack](https://gymnasium.farama.org/environments/toy_text/blackjack/),
    there are 32 · 11 · 2 = 704 states.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述链接的文章中，我们进行了Q学习，使得一个智能体能够在具有小的**离散观察空间**的一些简单游戏中进行游戏。例如，在Frozen Lake游戏中，你可以在4x4地图上的16个领域（=状态或观察）中站立。在[Blackjack卡牌游戏的gymnasium版本](https://gymnasium.farama.org/environments/toy_text/blackjack/)中，有32
    · 11 · 2 = 704个状态。
- en: Q-Learning inefficiencies
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Q学习的低效性
- en: In simple games such as the ones I mentioned, Q-learning works very well since
    the Q-tables stay quite small. However, bigger tables mean more work for the algorithm,
    hence Q-learning becomes more inefficient.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在我提到的简单游戏中，Q学习效果非常好，因为Q表保持得相当小。然而，更大的表意味着算法需要更多工作，因此Q学习变得更加低效。
- en: This typically happens when you have **too many observations** rather than too
    many actions because the action space is usually tiny compared to the observation
    space. In the Blackjack environment, you only have 2 actions but 704 states, for
    example.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常发生在你有**过多的观察**而不是过多的动作时，因为动作空间通常比观察空间小得多。例如，在Blackjack环境中，你只有2个动作但有704个状态。
- en: The cart pole game
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卡车杆游戏
- en: 'Or even worse: imagine that your observation space is **continuous**, such
    as in the [cart pole game](https://gymnasium.farama.org/environments/classic_control/cart_pole/)
    that gymnasium offers. In this game, the agent has to learn how to balance a pole
    by steering a cart left or right.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 或者更糟的是：想象一下你的观察空间是**连续的**，例如在gymnasium提供的[卡车杆游戏](https://gymnasium.farama.org/environments/classic_control/cart_pole/)中。在这个游戏中，智能体必须通过左右转动卡车来学习如何平衡杆子。
- en: '![](../Images/fbcfb671e985c4768fe4f3d0d861e5db.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fbcfb671e985c4768fe4f3d0d861e5db.png)'
- en: Balance that pole. Image by the author.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 平衡那个杆子。图片由作者提供。
- en: While the action space of this game only consists of **two actions** — going
    left or right — the observation space consists of
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个游戏的动作空间仅由**两个动作**组成——向左或向右——观察空间由
- en: the position of the car,
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 车的位置，
- en: its velocity,
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其速度，
- en: the pole angle and
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杆的角度和
- en: the pole angular velocity,
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杆的角速度，
- en: each of them being **real numbers**. This means that your observation space
    is **infinite**, and creating a Q-table for it… is challenging.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 每一个都是**实数**。这意味着你的观察空间是**无限**的，创建一个Q表对于它来说……是具有挑战性的。
- en: Discretized Q-learning
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 离散化Q学习
- en: As a workaround, you could **discretize** the space, i.e., slicing it into a
    **finite number** of buckets and mapping each continuous state to one of these
    buckets. Then you do normal Q-learning.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 作为解决方法，你可以**离散化**空间，即将其划分为**有限数量**的桶，并将每个连续状态映射到这些桶之一。然后你可以进行正常的Q学习。
- en: '![](../Images/8d71791766cfccb16dfea9deccaaec24.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8d71791766cfccb16dfea9deccaaec24.png)'
- en: Four buckets for the cart position, not a political spectrum. Image by the author.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 四个桶用于卡车位置，并非政治光谱。图片由作者提供。
- en: This poses the problem that you have to decide how many buckets you want to
    use. If you use too many, the Q-table will be too large again. If there are too
    few, very different states will be treated equally, which can lead to poor agent
    performance. This might happen in the image above, where all cart positions between
    0 and 3 are treated the same, for example. Being in position 0.1 is the same as
    being int he position 2.9, as far as the model is concerned.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这就提出了一个问题，你必须决定要使用多少个桶。如果你使用的桶太多，Q表将会过大。如果桶太少，非常不同的状态将被同等对待，这可能导致智能体性能较差。这可能发生在上面的图像中，例如，其中0到3之间的所有卡车位置都被视为相同。在模型看来，位置0.1与位置2.9是一样的。
- en: We might still try out this approach in another article, but be aware that it
    is basically the Q-learning that we know with an extra discretization step for
    the observations. However, in this article, we want to apply **deep Q-learning**
    that can **deal with continuous observation spaces** out of the box!
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能在另一篇文章中尝试这种方法，但请注意，它基本上是我们所知道的 Q 学习，只是对观察进行了额外的离散化步骤。然而，在这篇文章中，我们想应用 **深度
    Q 学习**，它能够 **直接处理连续观察空间**！
- en: Deep Q-Learning
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度 Q 学习
- en: 'Before, we reserved a cell for each Q-value *Q*(*s*, *a*) for each state *s*
    and action *a* in the Q-table. If these tables get too big, let us try a different
    strategy: **modeling the Q-values via functions**!'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们为 Q 表中的每个状态 *s* 和动作 *a* 保留了一个单元。如果这些表格变得过大，我们可以尝试另一种策略：**通过函数建模 Q 值**！
- en: For a given state *s* and action *a*, we would like an output that is (close
    to) the Q-value. Since we can’t make up the function on our own — at least I can’t
    — let us use a neural network with learnable weights as an approximation.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的状态 *s* 和动作 *a*，我们希望输出 (接近) Q 值。由于我们不能自己构建这个函数——至少我不能——让我们使用一个具有可学习权重的神经网络作为近似。
- en: '![](../Images/ece725a20c5708d290889e20dab7fed1.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ece725a20c5708d290889e20dab7fed1.png)'
- en: Q-learning and deep Q-learning compared. Image by the author.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Q 学习与深度 Q 学习的比较。作者提供的图片。
- en: This is nothing that I have made up, it was described as early as 1993 under
    the name **QCON** (connectionist Q-learning)in the dissertation “[Reinforcement
    Learning for Robots Using Neural Networks](https://isl.anthropomatik.kit.edu/pdf/Lin1993.pdf)”
    by Long-Ji Lin. *Happy 30th anniversary!*
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是我编造的，它早在 1993 年就以 **QCON**（连接主义 Q 学习）的名称在 Long-Ji Lin 的论文 “[使用神经网络的机器人强化学习](https://isl.anthropomatik.kit.edu/pdf/Lin1993.pdf)”
    中描述过。*祝 30 周年快乐！*
- en: The author also introduces the concept of **experience replay** in its current
    form that is used in the famous DeepMind paper [Playing Atari with Deep Reinforcement
    Learning](https://arxiv.org/abs/1312.5602).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 作者还在其目前的形式中引入了 **经验回放** 的概念，这种形式在著名的 DeepMind 论文 [使用深度强化学习玩 Atari 游戏](https://arxiv.org/abs/1312.5602)
    中被使用。
- en: '**Note:** You might wonder why the network doesn’t use the state *s* and action
    *a* as input since this would be the straightforward way to replace the table
    with a model. Input the state and action, receive a Q-value. The DeepMind paper
    authors say the following:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：** 你可能会想知道为什么网络不使用状态 *s* 和动作 *a* 作为输入，因为这将是用模型替代表的直接方法。输入状态和动作，接收 Q 值。DeepMind
    论文的作者说如下：'
- en: There are several possible ways of parameterizing Q using a neural network.
    Since Q maps history-action pairs to scalar estimates of their Q-value, the history
    and the action have been used as inputs to the neural network by some previous
    approaches […]. The main drawback of this type of architecture is that a separate
    forward pass is required to compute the Q-value of each action, resulting in a
    cost that scales linearly with the number of actions. […] The main advantage of
    [our] type of architecture is the ability to compute Q-values for all possible
    actions in a given state with only a single forward pass through the network.
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 有几种可能的方法可以使用神经网络对 Q 进行参数化。由于 Q 将历史-动作对映射到它们的 Q 值的标量估计，一些先前的方法使用历史和动作作为神经网络的输入。[……]
    这种架构的主要缺点是需要单独的前向传播来计算每个动作的 Q 值，这导致成本随动作数量线性增长。[……] 我们类型架构的主要优势是能够通过对网络进行单次前向传播来计算给定状态下所有可能动作的
    Q 值。
- en: Intuition
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 直观理解
- en: In my other article, I told you that the updates in the Q-table are done via
    the formula
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的另一篇文章中，我告诉你 Q 表中的更新是通过以下公式完成的
- en: '![](../Images/14044cb86ca0795fd4f22516a755c5d4.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14044cb86ca0795fd4f22516a755c5d4.png)'
- en: Image by the author.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片。
- en: In words, it means that we shift our old value *Q*(*s*, *a*) slightly into the
    direction of the target value *R*(*s*, *a*) + *γ*·max*ₐ* *Q*(*s*’, *a*), where
    *s* is a state, and *s’* the next state after taking action *a*, *R* is the reward
    function, and *γ* < 1 is a discount factor. *α* is the learning rate that tells
    us how far we shift our old Q-value *Q*(*s*, *a*) into the direction *R*(*s*,
    *a*) + *γ*·max*ₐ* *Q*(*s*’, *a*).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 用简单的话说，就是我们将旧的价值 *Q*(*s*, *a*) 稍微向目标值 *R*(*s*, *a*) + *γ*·max*ₐ* *Q*(*s*’, *a*)
    的方向移动，其中 *s* 是状态，*s’* 是在采取行动 *a* 后的下一个状态，*R* 是奖励函数，*γ* < 1 是折扣因子。*α* 是学习率，告诉我们将旧的
    Q 值 *Q*(*s*, *a*) 移动到方向 *R*(*s*, *a*) + *γ*·max*ₐ* *Q*(*s*’, *a*) 的程度。
- en: '**Small example.** Let us assume that *Q*(*s*, *a*) = 3, *R*(*s*, *a*) + *γ*·max*ₐ*
    *Q*(*s*’, *a*) = 4, and *α =* 0.1*.* Then our old value of 3 would get updated
    to 3 + 0.1·(4–3) = 3.1, a little bit further away from 3 into the direction of
    4.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**小示例。** 假设 *Q*(*s*, *a*) = 3， *R*(*s*, *a*) + *γ*·max*ₐ* *Q*(*s*’, *a*) =
    4，并且 *α =* 0.1*。* 那么我们原来的值 3 会更新为 3 + 0.1·(4–3) = 3.1，稍微偏离 3 向 4 的方向。'
- en: This means that this update rule changes the entries step by step, such that
    at some point we have *Q*(*s*, *a*) = *R*(*s*, *a*) + *γ*·max*ₐ* *Q*(*s*’, *a*),
    which is also called the **Bellman equation**.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着这个更新规则逐步改变条目，使得某个时刻我们有 *Q*(*s*, *a*) = *R*(*s*, *a*) + *γ*·max*ₐ* *Q*(*s*’,
    *a*)，这也称为**贝尔曼方程**。
- en: 'The brilliant observation is that we can translate this goal into a simple
    machine learning task for our neural network. So, let us assume that we do a step
    in our environment. We start in state *s,* do some action *a,* and end up in a
    new state *s’.* We also got a reward *r* = *R*(*s*, *a*) from the environment.
    Then we proceed with a training step as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊叹的观察是，我们可以将这个目标转换为一个简单的机器学习任务给我们的神经网络。所以，假设我们在环境中执行一步。我们从状态 *s* 开始，执行某个动作
    *a*，然后进入新状态 *s’*。我们还从环境中获得了奖励 *r* = *R*(*s*, *a*)。接下来，我们按照以下步骤进行训练：
- en: Compute *N*(*s*) and *N*(*s*’), which are both vectors of real numbers, one
    for each action.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 *N*(*s*) 和 *N*(*s*’)，它们都是实数向量，每个动作一个向量。
- en: Check out the *a*-th action of *N*(*s*), let’s call it *N*(*s*)[*a*].
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看 *N*(*s*) 的 *a* 号动作，称其为 *N*(*s*)[*a*]。
- en: Make sure that *N*(*s*)[*a*] gets closer to *R*(*s*, *a*) + *γ*·max*ₐ* *N*(*s*’)
    by performing a gradient update step minimizing the mean squared error, which
    means (*R*(*s*, *a*) + *γ*·max*ₐ* *N*(*s*’) - *Q*(*s*, *a*))².
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保 *N*(*s*)[*a*] 更接近 *R*(*s*, *a*) + *γ*·max*ₐ* *N*(*s*’) 通过执行一个梯度更新步骤来最小化均方误差，这意味着
    (*R*(*s*, *a*) + *γ*·max*ₐ* *N*(*s*’) - *Q*(*s*, *a*))²。
- en: We can then take another step in the environment, get new data (*s*, *a*, *s’*,
    *r*) (**read:** the agent was in state *s*, did *a*, ended up in *s’,* and got
    reward *r*), and do steps 1–3 all over again. We do this until we have an agent
    that is good enough, or we run out of patience or money to train.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以在环境中再进行一步，获取新数据 (*s*, *a*, *s’*, *r*) (**阅读：** 代理在状态 *s* 中，执行 *a*，进入 *s’*，并获得奖励
    *r*)，然后再次执行步骤 1–3。我们这样做直到有一个足够好的代理，或者我们耗尽耐心或资金进行训练。
- en: Experience replay
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 经验回放
- en: 'The intuition from above has a flaw: *it does not work very well like that*.
    When I first started, I implemented this logic and never got any good model.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 上述直觉有一个缺陷：*它并不像那样有效*。当我刚开始时，我实现了这个逻辑，但从未得到过好的模型。
- en: 'The reason was that I updated the model with only a **single training example**
    each time. This is like performing pure stochastic gradient descent. It might
    work, but during optimization, the model parameters jump around a lot and it might
    be hard for them to converge. Another problem is that **subsequent actions are
    highly correlated** since the state usually does not change much with a single
    action. Nowadays, we know an easy way to fix both problems: **use mini-batches**!'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 原因是我每次只用**单个训练样本**更新模型。这就像进行纯随机梯度下降。它可能有效，但在优化过程中，模型参数会剧烈波动，可能很难收敛。另一个问题是**后续动作高度相关**，因为状态通常不会因为单个动作而变化太多。现在，我们知道一种简单的方法来解决这两个问题：**使用小批量**！
- en: The fancy term *experience replay* is not much more than that.
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 花哨的术语 *经验回放* 仅仅是这样。
- en: You don’t do a single step in the environment, but several ones, and you store
    the **memories** (*s*, *a*, *s’*, *r*) away into some data structure, the **replay
    memory**. It can just be an array, but usually, people use [deques](https://en.wikipedia.org/wiki/Double-ended_queue)
    (double-ended queues) to realize a **limited** replay memory.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 你不是在环境中执行单个步骤，而是多个步骤，并将**记忆** (*s*, *a*, *s’*, *r*) 存储到某种数据结构中，即**回放记忆**。它可以是一个数组，但通常，人们使用
    [双端队列](https://en.wikipedia.org/wiki/Double-ended_queue) 来实现**有限的**回放记忆。
- en: '![](../Images/9f5ea3399d5de8e8fd65dbf6b49c0948.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9f5ea3399d5de8e8fd65dbf6b49c0948.png)'
- en: Image by the author.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: Newer memories kick older ones out, once the size limit of the replay memory
    is reached. This makes sense since very old memories shouldn’t have much impact
    on what happens now anymore. This also enables the model to adapt to new rules
    in the game.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 新的记忆会将旧的记忆淘汰，一旦回放记忆的大小限制达到。这是有道理的，因为非常旧的记忆不应该对现在发生的事情产生太大影响。这也使得模型能够适应游戏中的新规则。
- en: '![](../Images/a3ed1f5030d692784cf9b2127c71644b.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a3ed1f5030d692784cf9b2127c71644b.png)'
- en: 7 kicks out 1\. Image by the author.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 7 淘汰 1。图片由作者提供。
- en: Pseudocode
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伪代码
- en: Alright, with this knowledge, let us take a look at the pseudocode from the
    Playing Atari with Deep Reinforcement Learning paper. The only bits that differ
    are that they potentially preprocess the state *s* with some function φ (which
    we don’t do here) and that they remove the max term when the agent reaches a terminal
    state, i.e., the game ends.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，了解了这些知识后，让我们看看《Playing Atari with Deep Reinforcement Learning》论文中的伪代码。唯一的区别是，他们可能会使用某些函数
    φ 预处理状态 *s*（我们这里没有这样做），并且在代理达到终止状态时，即游戏结束时，他们会移除最大项。
- en: '![](../Images/ebad2f580c430a3c52d916896a03cf7d.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ebad2f580c430a3c52d916896a03cf7d.png)'
- en: From their paper.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 来自他们的论文。
- en: We can now implement this idea in Python!
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以在 Python 中实现这个想法了！
- en: Implementation
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现
- en: First, let us import some libraries and define some parameters.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们导入一些库并定义一些参数。
- en: '[PRE0]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Nothing wild going on here, but please read the comments if something is unclear.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这里没有什么特别的，但如果有不清楚的地方请阅读注释。
- en: Replay memory
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回放记忆
- en: Now, we define the replay, which is only a thin wrapper around a Python deque.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们定义了回放，这只是一个 Python deque 的薄包装器。
- en: '[PRE1]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, we defined the memory replay in a way that is easy to use. Let’s play
    around with it before we use it in action. We can do
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了一个易于使用的记忆回放。让我们先玩一下，然后再实际使用它。我们可以进行
- en: '[PRE2]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If we add another memory, we will lose the first one:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们添加另一个记忆，我们将丢失第一个记忆：
- en: '[PRE3]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The model
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型
- en: Let us define a simple model! We will use TensorFlow here, but feel free to
    use PyTorch, [JAX](https://github.com/google/jax), or whatever.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个简单的模型！我们将在这里使用 TensorFlow，但也可以使用 PyTorch，[JAX](https://github.com/google/jax)
    或其他工具。
- en: '[PRE4]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Training
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: With these preparations, let us conduct the training now.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好了，就让我们开始训练吧。
- en: '[PRE5]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: And that’s it! It kind of looks like the code for Q-learning, but the simple
    update of a cell in the table got replaced with one step of gradient descent in
    the form of a single `fit` .
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！它看起来有点像 Q-learning 的代码，但表格中单元格的简单更新被替换为一个 `fit` 步骤的梯度下降。
- en: Results
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: 'The code might run for some hours. You can also log some data during training,
    such as the reward per episode. Another common practice is to **log the model
    every few episodes,** so you don’t lose too work much when the training crashes.
    However, saving models has another benefit: **very often, the best model is not
    the latest one**. You can see this when you take a look at my reward plot:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 代码可能会运行几个小时。你也可以在训练期间记录一些数据，例如每个回合的奖励。另一个常见做法是**每隔几个回合记录模型，** 这样当训练崩溃时，你不会丢失太多工作。不过，保存模型还有另一个好处：**通常情况下，最好的模型不是最新的那个**。你可以从我的奖励图中看到这一点：
- en: '![](../Images/80e4eb25687823fc3ae665262219405a.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/80e4eb25687823fc3ae665262219405a.png)'
- en: Image by the author.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片。
- en: You can see that at first, the agent was quite bad. Around episode 100, the
    agent got rewards of around 200 already before it dropped again. Around episode
    230, the model became exceptional, just to dumb down again until episode 1000,
    where I ended the training. It is weird, but a common pattern that I also see
    when other people do reinforcement learning.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到最开始，代理表现很差。大约第 100 集时，代理已经获得了大约 200 的奖励，然后又下降了。大约第 230 集时，模型变得异常优秀，但直到第
    1000 集时才又变得较差，那时我结束了训练。这很奇怪，但这是一个我在其他人做强化学习时也常见的模式。
- en: Let’s play!
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 来玩吧！
- en: What could be better than seeing the model we have trained so painstakingly
    in action? Right, *nothing*, so let’s get started! First, let us see how a random
    agent competes.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 有什么比看到我们辛苦训练的模型实际运作更好的呢？对，*没有*，所以让我们开始吧！首先，让我们看看随机代理如何竞争。
- en: '![](../Images/55259aa459a1311b4c538892d2a7bf11.png)![](../Images/63493a6d3a03e735fd789209597a15b4.png)![](../Images/e72ccb63fbe8908731feb854a5042f52.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/55259aa459a1311b4c538892d2a7bf11.png)![](../Images/63493a6d3a03e735fd789209597a15b4.png)![](../Images/e72ccb63fbe8908731feb854a5042f52.png)'
- en: Three different runs of a random agent. White flashes indicate a game over.
    Image by the author.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 随机代理的三次不同运行。白色闪光表示游戏结束。图片由作者提供。
- en: Wow, the random agent **really** sucks. The run in the middle looked okay in
    the beginning, but then the randomness kicked it and prevented the agent from
    going left. You can recreate sad runs like this via
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 哇，随机代理**真的**很糟糕。中间的运行开始时看起来还不错，但随机性使其无法向左移动。你可以通过以下方式重现这样的悲惨运行：
- en: '[PRE6]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: But you are here for something else, right?
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 但你来这里是为了别的，对吧？
- en: A new challenger appears
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 新的挑战者出现
- en: Let us load the model of episode 230 to see how it performs in a live test!
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载第 230 集的模型，看看它在实时测试中的表现如何！
- en: '[PRE7]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/ebcf2a47b21c58dc85e62200ea844867.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ebcf2a47b21c58dc85e62200ea844867.png)'
- en: Smooth moves by our agent of episode 230\. After 500 timesteps, I cut the animation,
    but the agent did not fail. Image by the author.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第 230 集的智能体动作流畅。经过 500 个时间步后，我切断了动画，但智能体并没有失败。图片由作者提供。
- en: 'Well, look at that! The agent can effortlessly balance the pole with minimal
    movement. However, you can see that it is slightly drifting right, which *might*
    be a problem in a long run. Let us increase the artificial restriction of 500
    time steps to several thousands and look at a sped-up version:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 看这儿！智能体可以轻松地用最小的动作保持杆子的平衡。然而，你可以看到它略微向右漂移，这在长时间内*可能*会成为一个问题。让我们将 500 个时间步的人工限制增加到几千个，并查看加速版本：
- en: '![](../Images/e95b77d67a30262f38b36642ec0158a9.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e95b77d67a30262f38b36642ec0158a9.png)'
- en: Agent 230 is at it again in 60fps, gamer style. Image by the author.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体 230 再次以 60fps 的游戏风格行动。图片由作者提供。
- en: It seems like the agent is doing fine, even after 500 steps. Awesome!
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来这个智能体表现不错，即使经过了 500 步。太棒了！
- en: Conclusion
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we have explained why vanilla Q-learning fails when dealing
    with large or even potentially infinite observation spaces. Discretized Q-learning
    can help in this case, but you have to think about how much you discretize to
    find a balance between the agent’s performance and training time.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们解释了为什么普通的 Q-learning 在处理大规模甚至潜在无限的观察空间时会失败。离散化 Q-learning 在这种情况下可以有所帮助，但你必须考虑你离散化的程度，以找到智能体性能和训练时间之间的平衡。
- en: The discretized Q-learning approach is perfectly fine, but we went for another
    technique that was successfully applied to learn more complex games — **deep Q-learning**.
    Instead of updating entries of a potentially large Q-table, we train a model with
    a **fixed amount of parameters**.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 离散化的 Q-learning 方法完全没问题，但我们选择了另一种成功应用于更复杂游戏的技术——**深度 Q-learning**。我们不是更新一个可能很大的
    Q 表条目，而是训练一个具有**固定参数量**的模型。
- en: '*Let us be honest though*. Deep Q-learning is *also* a trade-off between training
    time and agent performance. A small network such as linear regression (no hidden
    layers) will probably perform horribly. If you have millions of parameters for
    the cart pole game, it is overkill, and training takes ages.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*不过，我们还是要诚实*。深度 Q-learning *也是*在训练时间和智能体性能之间的权衡。像线性回归（没有隐藏层）这样的小型网络可能表现得很糟。如果你有数百万个参数用于摆杆游戏，那就是过度配置了，而且训练需要很长时间。'
- en: You have to find a balance as well, it doesn’t come for free.
  id: totrans-122
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你也必须找到一个平衡，这不是免费的。
- en: However, judging from the papers, it seems that deep Q-learning is still more
    promising than discretized Q-learning for more complex games such as [Atari games](https://gymnasium.farama.org/environments/atari/).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从论文来看，似乎深度 Q-learning 对于像 [Atari 游戏](https://gymnasium.farama.org/environments/atari/)
    这样更复杂的游戏仍然更有前景。
- en: Learning from sensory input
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从感官输入中学习
- en: So far, we have used the **internal state** of the environment. In the cart
    pole game, we were given the position of the car, the speed, angles, and so on.
    When we, as humans, play games, usually **we do not have that luxury**. I mean,
    we couldn’t process this information fast enough anyway to be good at the game.
    We have to rely on visual feedback — the screen — to make quick decisions.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们使用了环境的**内部状态**。在摆杆游戏中，我们获得了汽车的位置、速度、角度等信息。当我们作为人类玩游戏时，通常**我们没有这种奢侈**。我的意思是，我们无法足够快地处理这些信息以在游戏中表现出色。我们必须依赖视觉反馈——屏幕——来做出快速决策。
- en: 'What if the agent could only use the screen output as well? As the authors
    from the Playing Atari with Deep Reinforcement Learning paper show, it works quite
    well! As they write:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如果智能体也只能使用屏幕输出呢？正如《用深度强化学习玩 Atari》的作者所展示的那样，它效果相当好！正如他们所写：
- en: We apply our method to seven Atari 2600 games from the Arcade Learning Environment,
    with no adjustment of the architecture or learning algorithm. We find that it
    outperforms all previous approaches on six of the games and surpasses a human
    expert on three of them.
  id: totrans-127
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们将我们的方法应用于来自 Arcade Learning Environment 的七款 Atari 2600 游戏，未对架构或学习算法进行任何调整。我们发现它在六款游戏中优于所有以前的方法，并在三款游戏中超越了人类专家。
- en: The authors use a deep convolutional neural network that takes the screen, processes
    the image it sees, and outputs the Q-values. To be precise, they **don’t use a
    single frame**, but the **last 4 frames** of the game. This allows the model to
    learn the movement speed and directions of the objects on the screen.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用了一个深度卷积神经网络，它接收屏幕，处理它所看到的图像，并输出 Q 值。准确地说，他们**不使用单帧图像**，而是使用**游戏的最后 4 帧**。这使模型能够学习屏幕上物体的移动速度和方向。
- en: But I will leave it like this for now, we can dive into this topic with a fresh
    mind in another article!
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 但我现在先这样，我们可以在另一篇文章中以清新的思维深入探讨这个话题！
- en: I hope that you learned something new, interesting, and valuable today. Thanks
    for reading!
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你今天学到了一些新、趣味且有价值的东西。感谢阅读！
- en: '*If you have any questions, write me on* [*LinkedIn*](https://www.linkedin.com/in/dr-robert-k%C3%BCbler-983859150/)*!*'
  id: totrans-131
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*如果你有任何问题，可以在* [*LinkedIn*](https://www.linkedin.com/in/dr-robert-k%C3%BCbler-983859150/)*上联系我！*'
- en: And if you want to dive deeper into the world of algorithms, give my new publication
    All About Algorithms a try! I’m still searching for writers!
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想更深入了解算法的世界，可以尝试我的新出版物《算法全景》！我仍在寻找作者！
- en: '[](https://allaboutalgorithms.com/?source=post_page-----9073040ce841--------------------------------)
    [## All About Algorithms'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://allaboutalgorithms.com/?source=post_page-----9073040ce841--------------------------------)
    [## 《算法全景》'
- en: From intuitive explanations to in-depth analysis, algorithms come to life with
    examples, code, and awesome…
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从直观解释到深入分析，算法通过示例、代码和令人惊叹的内容变得生动。
- en: allaboutalgorithms.com](https://allaboutalgorithms.com/?source=post_page-----9073040ce841--------------------------------)
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[allaboutalgorithms.com](https://allaboutalgorithms.com/?source=post_page-----9073040ce841--------------------------------)'
