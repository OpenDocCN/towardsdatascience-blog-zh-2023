- en: 'ToolFormer: Guiding AI Models To Use External Tools'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ToolFormer：指导AI模型使用外部工具
- en: 原文：[https://towardsdatascience.com/toolformer-guiding-ai-models-to-use-external-tools-37e4227996f1](https://towardsdatascience.com/toolformer-guiding-ai-models-to-use-external-tools-37e4227996f1)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/toolformer-guiding-ai-models-to-use-external-tools-37e4227996f1](https://towardsdatascience.com/toolformer-guiding-ai-models-to-use-external-tools-37e4227996f1)
- en: Meta’s LLM teaches itself to call External APIs
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Meta的LLM自学调用外部API
- en: '[](https://medium.com/@nikoskafritsas?source=post_page-----37e4227996f1--------------------------------)[![Nikos
    Kafritsas](../Images/de965cfcd8fbd8e1baf849017d365cbb.png)](https://medium.com/@nikoskafritsas?source=post_page-----37e4227996f1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----37e4227996f1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----37e4227996f1--------------------------------)
    [Nikos Kafritsas](https://medium.com/@nikoskafritsas?source=post_page-----37e4227996f1--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@nikoskafritsas?source=post_page-----37e4227996f1--------------------------------)[![Nikos
    Kafritsas](../Images/de965cfcd8fbd8e1baf849017d365cbb.png)](https://medium.com/@nikoskafritsas?source=post_page-----37e4227996f1--------------------------------)[](https://towardsdatascience.com/?source=post_page-----37e4227996f1--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----37e4227996f1--------------------------------)
    [Nikos Kafritsas](https://medium.com/@nikoskafritsas?source=post_page-----37e4227996f1--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----37e4227996f1--------------------------------)
    ·14 min read·Oct 23, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----37e4227996f1--------------------------------)
    ·阅读时长14分钟·2023年10月23日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/180485d9bb3167a1e863f926c118935d.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/180485d9bb3167a1e863f926c118935d.png)'
- en: Image created by author using Midjourney
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用Midjourney创建的图像
- en: '**Now that the dust has settled, the weaknesses of LLMs are known.**'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**现在尘埃落定，LLMs的弱点已被揭示。**'
- en: Even the powerful GPT-4 struggles with math operations.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 即便是强大的GPT-4在数学运算方面也有困难。
- en: Also, the training cut-off time is an inherent weakness of every LLM. They struggle
    to answer queries on new things.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，训练截止时间是每个LLM的固有弱点。它们在回答新事物时会遇到困难。
- en: A loose fix is to use external Plugins (e.g. ChatGPT plugins). Still, the user
    has to manually specify some actions, and these plugins are sometimes unreliable.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 一个临时解决方案是使用外部插件（例如ChatGPT插件）。然而，用户仍需手动指定某些操作，这些插件有时也不够可靠。
- en: What if there was a model that knew its weaknesses — and was trained to **natively**
    call the optimal external tool when uncertain?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有一个模型知道自己的弱点，并且经过训练可以在不确定时**本地**调用最佳外部工具，那会怎么样？
- en: 'That’s what Meta did, by creating **ToolFormer[1].** In this article, we discuss
    :'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是Meta所做的，通过创建**ToolFormer[1]**。在这篇文章中，我们讨论：
- en: What is ToolFormer and why is it a breakthrough?
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是ToolFormer，为什么它是一个突破？
- en: How the model works.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型如何运作。
- en: How ToolFormer’s methodology can be applied to any LLM.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ToolFormer的方法论如何应用于任何LLM。
- en: Why AI research heads towards ToolFormer’s vision.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么人工智能研究朝着ToolFormer的愿景发展。
- en: Let’s dive in.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解一下。
- en: Weaknesses of Large Language Models
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型的弱点
- en: 'Before starting to describe ToolFormer, let’s explore what issues the modern
    LLMs face:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始描述ToolFormer之前，让我们探讨一下现代LLMs面临的问题：
- en: '**Progression of Time:** Every LLM has a training cutoff date. Hence, they
    can’t access up-to-date information and recent events.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间进展：** 每个LLM都有一个训练截止日期。因此，它们无法访问最新信息和最近事件。'
- en: '**Incorrect Facts:** LLMs are infamous for making up facts, places, events,
    products, and even research papers.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不准确的事实：** LLMs以编造事实、地点、事件、产品，甚至研究论文而闻名。'
- en: '**Arithmetic operations:** LLMs struggle with mathematical calculations.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**算术运算：** LLMs在数学计算方面存在困难。'
- en: '**Rare languages:** LLMs cannot handle low-resource languages, usually due
    to a lack of training data.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**稀有语言：** LLMs无法处理资源匮乏的语言，通常是由于缺乏训练数据。'
- en: Obviously, these issues are irrelevant to language mechanics. An ideal solution
    would be to combine text generation with external tools.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这些问题与语言机制无关。理想的解决方案是将文本生成与外部工具结合起来。
- en: Here comes ToolFormer.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这就引出了ToolFormer。
- en: What is ToolFormer?
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是ToolFormer？
- en: ToolFormer is an LLM, trained to decide which APIs to call, when to call them,
    and what arguments to pass to call them.
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ToolFormer是一个LLM，经过训练以决定何时调用哪些API，以及传递哪些参数来调用它们。
- en: 'ToolFormer is amazing because of:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ToolFormer令人惊叹的原因是：
- en: '**Best of both worlds:** ToolFormer is an LLM, like GPT-3\. But when uncertain,
    it learns to call external APIs — thus avoiding common mistakes.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**两全其美：** ToolFormer是一种LLM，类似于GPT-3。但在不确定的情况下，它会学习调用外部API，从而避免常见错误。'
- en: '**Portability:** The methodology of training ToolFormer can be applied to any
    LLM.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可移植性：** 训练ToolFormer的方法可以应用于任何LLM。'
- en: '**Superior Performance:** ToolFormer is smaller, but outperforms much larger
    models like OPT and GPT-3.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卓越性能：** ToolFormer虽然体积较小，但其性能超过了更大的模型，如OPT和GPT-3。'
- en: '**Open Source:** While Meta has not released the original version yet, the
    community has created a [few great open-source implementations](https://github.com/xrsrke/toolformer).'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开源：** 尽管Meta尚未发布原始版本，但社区已经创建了[一些很棒的开源实现](https://github.com/xrsrke/toolformer)。'
- en: ToolFormer provides the following tools. These are shown in **Figure 1:**
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ToolFormer提供以下工具。这些工具在**图1**中展示：
- en: '![](../Images/774f968cd54d92335dba8f79c75c1d56.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/774f968cd54d92335dba8f79c75c1d56.png)'
- en: '**Figure 1:** ToolFormer autonomously calls external APIs to obtain accurate
    information and complete the output text (highlighted). From top to bottom, the
    APIS are: a question-answering system, a calculator, a machine translation system,
    and a Wikipedia search engine. ([Source](https://arxiv.org/pdf/2302.04761.pdf))'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**图1：** ToolFormer自主调用外部API以获取准确的信息并完成输出文本（突出显示）。从上到下，API包括：问答系统、计算器、机器翻译系统和维基百科搜索引擎。
    ([来源](https://arxiv.org/pdf/2302.04761.pdf))'
- en: 'According to **Figure 1**, ToolFormer provides:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 根据**图1**，ToolFormer提供：
- en: '**QA:** A question-answering system'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**QA：** 一个问答系统'
- en: '**Calculator**'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算器**'
- en: '**MT:** a machine translation system'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MT：** 一个机器翻译系统'
- en: '**WikiSearch:** a Wikipedia search engine API'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**WikiSearch：** 一个维基百科搜索引擎API'
- en: '**Calendar:** a calendar API that returns the current date (not shown in **Figure
    1**)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**日历：** 一个返回当前日期的日历API（**图1**中未显示）'
- en: '**How ToolFormer generates text** In each case, the model decides which API
    to call and what arguments to use. The tool names like `**QA**`and `**Calculator**`
    are special tokens.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**ToolFormer如何生成文本** 在每种情况下，模型决定调用哪个API以及使用什么参数。像`**QA**`和`**Calculator**`这样的工具名称是特殊的标记。'
- en: '![](../Images/d6abda84152483aabd12c17572da1263.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6abda84152483aabd12c17572da1263.png)'
- en: If the model generates `**Calculator**(400/1400)` , then the model is ready
    to call the **Calculator API** with `(400/1400)` as argument.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型生成`**Calculator**(400/1400)`，那么模型准备调用**Calculator API**，参数为`(400/1400)`。
- en: The `**->**`token signifies that the model next expects the response for the
    API call.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`**->**`标记表示模型接下来期望API调用的响应。'
- en: When that happens, decoding(inference) is interrupted, and the model places
    the answer from the corresponding API. Decoding then continues, if necessary,
    until the answer is complete (*…passed the test.*).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当发生这种情况时，解码（推断）被中断，模型将相应API的答案放入其中。然后，解码继续进行（如有必要），直到答案完整（*…通过测试。*）。
- en: How ToolFormer is Built
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ToolFormer的构建方法
- en: It’s time to delve into technical stuff.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候深入技术细节了。
- en: The key innovation of ToolFormer isn’t the base pretrained model — it’s the
    dataset used for training and particularly the **unique way** the authors augmented
    it.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ToolFormer的关键创新不是基础的预训练模型，而是用于训练的数据集，特别是作者增强数据集的**独特方式**。
- en: 'Fundamentally, ToolFormer is a **GPT-J** pretrained model:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 从根本上说，ToolFormer是一个**GPT-J**预训练模型：
- en: ToolFormer, a small pretrained **GPT-J** 6.7B model, beats the much larger GPT-3
    and OPT on numerous tasks.
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ToolFormer，一个小型的预训练**GPT-J** 6.7B模型，在众多任务上优于更大的GPT-3和OPT。
- en: The authors used a subset of the CCNet training dataset (abbreviated as **C**).
    Then, they augmented that dataset with API calls — and called it **C***
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用了CCNet训练数据集的一个子集（简称**C**）。然后，他们通过API调用增强了这个数据集，并称之为**C***。
- en: '![](../Images/4df20583ef9593c2e37d622a69e4bb2c.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4df20583ef9593c2e37d622a69e4bb2c.png)'
- en: '**Figure 2:** Augmentation of a training example for the QA tool (Image by
    author)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2：** 针对QA工具的训练示例增强（作者图片）'
- en: The process of augmenting C to C* is the real novelty of ToolFormer; This dataset
    can be used to teach any model how to effectively use API calls.
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 将C增强为C*的过程是ToolFormer的真正创新；这个数据集可以用来教任何模型如何有效地使用API调用。
- en: However, augmenting the training dataset is not an easy feat. We discuss this
    next.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，增强训练数据集并不是一件容易的事。我们将在接下来讨论这一点。
- en: Augmenting the Training Dataset
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 增强训练数据集
- en: 'This is the most crucial part. The authors have 3 goals here:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最关键的部分。作者在这里有三个目标：
- en: '**No human intervention.** We don’t expect that a human will manually perform
    the process shown in **Figure 2.**'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**无人工干预。** 我们不期望有人手动执行**图 2**中显示的过程。'
- en: '**Top-quality data:** The augmentation should be meaningful and helpful for
    the model. For example, the augmentation: **Pittsburgh is also known as the [QA:
    (*What type of material characterizes Pittsburgh?* -*> Steel*)] the Steel City**
    is wrong and not meaningful.'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**高质量数据：** 增强应对模型有意义且有帮助。例如，增强：**匹兹堡也被称为 [QA: (*匹兹堡的特点是什么？* -*> 钢铁*)] 钢铁城**
    是错误的且不具意义。'
- en: '**No loss of generality:** Withthe new dataset, ToolFormer will still function
    as an LLM (able to optimally predict the next word).'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**无损广泛性：** 使用新数据集，ToolFormer 仍将作为 LLM 运行（能够最佳预测下一个词）。'
- en: Now, it’s time to zoom in on **Figure 2** and reveal the intermediate steps
    between Dataset **C** and Dataset **C***.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候放大**图 2**并揭示数据集**C**和数据集**C***之间的中间步骤了。
- en: A more detailed view is shown in **Figure 3:**
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 更详细的视图见**图 3：**
- en: '![](../Images/8fac962f59a9d4e0c8511b9351428cf8.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8fac962f59a9d4e0c8511b9351428cf8.png)'
- en: '**Figure 3:** All 3 steps of augmenting a training example `**x**` for the
    QA tool. ([Source](https://arxiv.org/pdf/2302.04761.pdf))'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 3：** 对 QA 工具的训练示例`**x**`进行增强的全部 3 个步骤。 ([来源](https://arxiv.org/pdf/2302.04761.pdf))'
- en: 'There are 3 steps — let’s decompose them. Given a sentence `**x**`**,** wehave:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 有 3 个步骤——让我们逐一分解它们。给定一个句子`**x**`，我们有：
- en: '**Sample API Calls:** Sample a position `**i**` in sentence `**x**` that is
    likely to be used for an API call. Then, generate sample candidate API calls **[**`**c1**`**,**`**c2**`**..**`**ck**`**]**'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**采样 API 调用：** 采样句子`**x**`中可能用于 API 调用的位置`**i**`。然后，生成样本候选 API 调用 **[**`**c1**`**,**`**c2**`**..**`**ck**`**]**'
- en: '**Execute API Calls:** Execute those API calls, and take the responses **[**`**r1**`**,**`**r2**`**..**`**rk**`**]**'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**执行 API 调用：** 执行这些 API 调用，并获取响应 **[**`**r1**`**,**`**r2**`**..**`**rk**`**]**'
- en: '**Filter API Calls:** Not all pairs (`ci`-> `ri`) are useful or correct. We
    filter the API calls that don’t reduce the loss function L over the next tokens.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过滤 API 调用：** 不是所有的对 (`ci`-> `ri`) 都是有用或正确的。我们过滤掉那些不会降低下一个令牌的损失函数 L 的 API
    调用。'
- en: Don’t worry if you don’t fully understand the steps. In the next section, we
    will delve deeper into each step.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有完全理解这些步骤，不用担心。在下一部分，我们将深入探讨每一步。
- en: 'Step 1: Sample API Calls'
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 1：采样 API 调用
- en: In this step, we generate possible API calls — from the dataset **C**.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步，我们从数据集**C**生成可能的 API 调用。
- en: The prerequisites are i) no manual intervention and ii) the API calls should
    be as meaningful as possible.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 先决条件是 i) 无人工干预和 ii) API 调用应尽可能有意义。
- en: The best way to automate this task is to ask the GPT-J model to make the annotations
    itself!
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化此任务的最佳方法是让 GPT-J 模型自行进行标注！
- en: 'Specifically, we will write a prompt `P(x)` with instructions and a few examples
    — and encourage the model to annotate a sentence `**x**`withAPI calls. For the
    QA tool, the authors use the following prompt:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将编写一个包含指令和几个示例的提示`P(x)`——并鼓励模型为句子`**x**`标注 API 调用。对于 QA 工具，作者使用了以下提示：
- en: '![](../Images/06e38268a69aa487779eb41d57f1b8b3.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06e38268a69aa487779eb41d57f1b8b3.png)'
- en: '**Figure 4:** Example of prompt P(x) that generates API calls for the QA tool.
    ([Source](https://arxiv.org/pdf/2302.04761.pdf))'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 4：** 生成 QA 工具 API 调用的提示 P(x) 示例。 ([来源](https://arxiv.org/pdf/2302.04761.pdf))'
- en: '**Note:** The process of including a few examples in the prompt to help the
    model better understand the given task is called ***in-context learning.***'
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** 在提示中包含几个示例以帮助模型更好地理解给定任务的过程称为***上下文学习。***'
- en: But remember, language models tend to hallucinate or produce errors — that’s
    why we need the filtering process in **Step 3.**
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 但请记住，语言模型容易产生幻觉或错误——这就是为什么我们需要**步骤 3**中的过滤过程。
- en: 'Next, let’s examine how **Step 1** works in practice:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们检验一下**步骤 1**在实践中的运作方式：
- en: We will use the prompt `P(x)` from **Figure 4** to annotate a sentence `**x**`withsome
    candidate API Calls.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将使用**图 4**中的提示`P(x)`来标注句子`**x**`并生成一些候选 API 调用。
- en: Let’s try the sentence `**x**`= **Pittsburgh is also known as the Steel City.**
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让我们尝试句子`**x**`= **匹兹堡也被称为钢铁城。**
- en: '![](../Images/755138fc16feccfd26552786bdd678cd.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/755138fc16feccfd26552786bdd678cd.png)'
- en: '**Figure 5:** Example of candidate API calls using as input the sentence `**x**`=**Pittsburgh
    is also known as the Steel City.** (Image by author)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 5：** 使用句子`**x**`=**匹兹堡也被称为钢铁城。** 作为输入的候选 API 调用示例。（作者提供的图像）'
- en: We got 3 annotated sentences as output. Obviously, only the 2nd candidate API
    call is meaningful here.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了 3 个注释句子作为输出。显然，只有第二个候选 API 调用在这里是有意义的。
- en: The purpose of this step is to generate multiple annotated sentences without
    human effort. We will address how to filter out the incorrect ones later.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步的目的是在没有人工干预的情况下生成多个注释句子。我们稍后会解决如何筛选出不正确的句子。
- en: '**Note:** The authors also impose a minimal filtering process here, to save
    costs. For more info, check the **Appendix** at the end of the article.'
  id: totrans-88
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** 作者在这里也施加了最小的过滤过程，以节省成本。有关更多信息，请查看文章末尾的**附录**。'
- en: 'Step 2: Execute API Calls'
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 2：执行 API 调用
- en: 'This is straightforward — given the candidate calls from **Step 1**, we ask
    APIs for responses:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这很简单——根据**步骤 1**中的候选调用，我们请求 API 响应：
- en: '![](../Images/52f9439b605ad59c9ff52722f7364d92.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52f9439b605ad59c9ff52722f7364d92.png)'
- en: '**Figure 6:** In Step 2, we get responses for each candidate call, regarding
    the QA tool (Image by author)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 6：** 在步骤 2 中，我们获取每个候选调用的响应，关于 QA 工具（图片由作者提供）'
- en: In reality, there are no actual APIs for all cases — except for the **Calculator**
    and the **Calendar** cases, which are simple scripts.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，并不是所有情况下都有实际的 API——除了**计算器**和**日历**情况，这些是简单的脚本。
- en: For the other cases, the authors use specialized LMs. For example, for the QA
    tool, they use Atlas, (Izacard et al., 2022), a retrieval-augmented LM finetuned
    on Natural Questions.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他情况，作者使用了专门的语言模型。例如，对于 QA 工具，他们使用 Atlas（Izacard 等，2022），这是一个在自然问题上微调的检索增强型语言模型。
- en: Feel free to check the paper for further details on each tool.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 可以查看论文以获取每个工具的更多详细信息。
- en: 'Step 3: Filter API Calls'
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 3：过滤 API 调用
- en: While **Step 1** generated numerous API Calls, **Step 3** keeps only the meaningful
    ones.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然**步骤 1**生成了大量 API 调用，但**步骤 3**只保留了有意义的调用。
- en: 'Meaningful API calls: **Those** **which** **improve the model’s capability
    to call external APIS.**'
  id: totrans-98
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 有意义的 API 调用：**那些** **提高模型调用外部 API 的能力**。
- en: This improvement is measured with a loss function and a formula — called **helpfulness
    ratio.**
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这一改进通过一个损失函数和一个公式来衡量——称为**有用性比率**。
- en: 'The whole process is displayed in **Figure 7**:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 整个过程展示在**图 7**中：
- en: 'The goal is to check if the following example is meaningful enough to be included
    in the augmented **C*** dataset:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是检查以下示例是否足够有意义以包含在增强**C***数据集中：
- en: '![](../Images/1343abd93ade940fa614900934b827cb.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1343abd93ade940fa614900934b827cb.png)'
- en: '**Figure 7:** The helpfulness ratio determines whether an augmented candidate
    sentence is useful for the model (Image by author)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 7：** 有用性比率决定了一个增强候选句子是否对模型有用（图片由作者提供）'
- en: 'Let’s break down what happens here:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析一下发生了什么：
- en: We calculate the *negative log-likelihood* of each case as the prefix to the
    phrase **“the Steel City”:**
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算每个案例的*负对数似然*，作为短语**“钢铁城”**的前缀：
- en: '**No API:** We calculate `p(the Steel City | **Pittsburgh is also known as**)`
    . Here, we don’t help the model much — that’s why the loss is high.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无 API：** 我们计算`p(钢铁城 | **匹兹堡也被称为** )`。在这里，我们对模型的帮助不大——这就是损失高的原因。'
- en: '**Input Only:** We calculate`p(the Steel City | **Pittsburgh is also known
    as [QA(“What other name is Pittsburgh known by?“) ->?]** )`'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**仅输入：** 我们计算`p(钢铁城 | **匹兹堡也被称为[QA(“匹兹堡还有什么名字？“)->?]** )`'
- en: '**Full API:** We calculate`p(the Steel City | **Pittsburgh is also known as
    [QA(“What other name is Pittsburgh known by?“) ->Steel City]** )`'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完整的 API：** 我们计算`p(钢铁城 | **匹兹堡也被称为[QA(“匹兹堡还有什么名字？“)->钢铁城]** )`'
- en: The full API case is the most helpful. Besides, the prefix contains the correct
    answer ‘**Steel City**’. That’s why we obtain the lowest loss here!
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的 API 案例最有帮助。此外，前缀包含了正确的答案‘**钢铁城**’。这就是我们在这里获得最低损失的原因！
- en: However, we have to quantify how helpful an annotation is. Here comes the ***helpfulness
    ratio***. The higher the ratio, the more helpful the annotation is for training
    ToolFormer.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们必须量化一个注释的有用性。这里引入了***有用性比率***。比率越高，注释对训练 ToolFormer 的帮助越大。
- en: In **Figure 7**, we achieve a high helpfulness ratio, so our annotated example
    `[**Pittsburgh is also known as [QA … city]**` goes into the augmented dataset
    **C*.**
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在**图 7**中，我们达到了高的有用性比率，因此我们的注释示例`[**匹兹堡也被称为[QA … 城市]**]`进入了增强数据集**C*。
- en: The role of the helpfulness ratio
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有用性比率的作用
- en: 'Now, consider this:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑一下：
- en: A more powerful model like GPT-4 probably knows that Pittsburgh is also known
    as the “Steel City”. In that case, the loss of the **No API** case would be low
    and almost similar to the **Full API** case. That leads to a helpfulness ratio
    close to 0.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 像 GPT-4 这样更强大的模型可能知道 Pittsburgh 也被称为“钢铁之城”。在这种情况下，**No API** 的损失会较低，并且与 **Full
    API** 的情况几乎相似。这会导致有用性比例接近 0。
- en: But GPT-J, being a smaller model, doesn’t know the answer.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 但由于 GPT-J 是较小的模型，它不知道答案。
- en: Hence, the GPT-J model benefits from being finetuned on the annotated example
    `**Pittsburgh is..city**`**,** while **GPT-4** doesn't. Probably, GPT-4 would
    require more complex examples.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，GPT-J 模型通过对注释示例 `**Pittsburgh is..city**` 进行微调受益，而 **GPT-4** 则不受益。可能，GPT-4
    需要更复杂的示例。
- en: Thus, the process of training ToolFormer can be applied to any LM — thanks to
    the 3-step pipeline and the helpfulness ratio formula.
  id: totrans-117
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 因此，ToolFormer 的训练过程可以应用于任何语言模型——得益于 3 步流程和有用性比例公式。
- en: A rejection example
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个拒绝示例
- en: Remember, for our case, we sampled 3 API calls (**Figure 5**). Only the 2nd
    one is meaningful — the other 2 should be rejected.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，对于我们的情况，我们采样了 3 次 API 调用（**图 5**）。只有第 2 次是有意义的——其他 2 次应被拒绝。
- en: 'Let’s see a rejection example. We will use as an example the following API
    call (the 1st in **Figure 5**):'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个拒绝示例。我们将使用以下 API 调用作为示例（**图 5** 中的第 1 个）：
- en: '`Pittsburgh is also known as **[QA(In which state is Pittsburgh?) -> Pensylvania]**
    the Steel City**.**`'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pittsburgh 也被称为 **[QA（Pittsburgh 属于哪个州？）-> Pensylvania]** 钢铁之城**。**`'
- en: '![](../Images/1897151bb19ad6d1206ad48acd2f6bfc.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1897151bb19ad6d1206ad48acd2f6bfc.png)'
- en: '**Figure 8:** Here, the annotated example is not helpful for our model (Image
    by author)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 8：** 在这里，注释示例对我们的模型没有帮助（作者提供的图像）'
- en: Here, we make an API call for a completely irrelevant question — “w*hich state
    does Pittsburgh belong*”. This does not help our model answer “*how else is Pittsburgh
    known*”.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们为一个完全无关的问题发出 API 调用——“*Pittsburgh 属于哪个州*”。这对我们的模型回答“*Pittsburgh 还有什么别名*”没有帮助。
- en: Hence, we get a high loss, which means a negative helpfulness ratio. Thus, that
    annotated API call is not inserted in the **C*** dataset.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们得到较高的损失，这意味着负有用性比例。因此，该注释的 API 调用未被插入 **C*** 数据集中。
- en: The threshold τf for the helpfulness ratio
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有用性比例的阈值 τf
- en: So far so good, but how high should the helpfulness ratio of an example be to
    be considered meaningful— and eligible to enter C*?
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止一切顺利，但一个示例的有用性比例应达到多高才能被认为有意义——并符合进入 C* 的条件？
- en: 'The authors found this threshold `τf` experimentally — by considering the number
    of training examples per category on **C*** for different values of the helpfulness
    ratio. The results are shown in **Figure 9**:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 作者通过实验找到这个阈值 `τf`——考虑到不同有用性比例下 **C*** 中每个类别的训练示例数量。结果如 **图 9** 所示：
- en: '![](../Images/833a96c425054100381372709c5813af.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/833a96c425054100381372709c5813af.png)'
- en: '**Figure 9:** Number of examples with API calls in **C∗** for different values
    of our filtering threshold τf.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 9：** 不同有用性比例阈值下 **C∗** 中具有 API 调用的示例数量。'
- en: Obviously, by increasing the threshold `τf`, fewer examples are going into **C*.**
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，通过增加阈值 `τf`，进入 **C* 的示例数量会减少。
- en: 'However, **Machine Translation** and **Calculator** examples were fewer than
    in the other categories. Hence, to avoid a serious imbalance, the authors set:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，**机器翻译** 和 **计算器** 示例比其他类别少。因此，为了避免严重的不平衡，作者设定：
- en: For **QA**, **Wiki Search,** and **Calendar**, `τf = 1`
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 **QA**、**Wiki 搜索** 和 **日历**，`τf = 1`
- en: For **Machine Translation** and **Calculator**, `τf = 0.5`
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 **机器翻译** 和 **计算器**，`τf = 0.5`
- en: 'Final Step: Finetuning ToolFormer'
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终步骤：微调 ToolFormer
- en: The augmented dataset **C*** is now ready.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 增强数据集 **C*** 现在已经准备好了。
- en: We finetune GPT-J on **C***— and voila, we get **ToolFormer!**
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对 **C*** 微调 GPT-J——然后，瞧，我们得到了 **ToolFormer！**
- en: Finetuning is trivial. The authors use ***perplexity*** for the finetuning objective.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 微调是简单的。作者使用 ***困惑度*** 作为微调目标。
- en: '**Perplexity** is a standard metric to evaluate how uncertain a language model
    is. For instance, a perplexity of 32 means the model is as sure for predicting
    the next word as throwing a 32-side die. Hence, lower is better.'
  id: totrans-139
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**困惑度** 是评估语言模型不确定性的标准指标。例如，困惑度为 32 意味着模型在预测下一个词时的确定性相当于抛一个 32 面的骰子。因此，值越低越好。'
- en: Evaluation
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估
- en: Next, the authors evaluate ToolFormer.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，作者评估 ToolFormer。
- en: 'In total, the authors use the following models:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，作者使用了以下模型：
- en: '**GPT-J:** The original GPT-J pretrained model.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPT-J：** 原始的 GPT-J 预训练模型。'
- en: '**GPT-J on C:** Here, GPT-J fine-tuned on the **C** dataset (the one without
    API calls).'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPT-J on C：** 这里，GPT-J 在 **C** 数据集（没有 API 调用的数据集）上进行了微调。'
- en: '**ToolFormer:** GPT-J model fine-tuned on **C*** dataset.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ToolFormer：** GPT-J 模型在 **C** 数据集上进行了微调。'
- en: '**ToolFormer (disabled):** ToolFormer, but using API calls is disabled. (This
    is done by setting the probability of generating the `**[**` token during inference
    to zero)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ToolFormer（禁用）：** ToolFormer，但使用 API 调用已被禁用。（这是通过将推理过程中生成 `**[**` 令牌的概率设置为零来实现的）'
- en: 'The goal is to evaluate the model for each separate task: QA, Calculator, Calendar,
    and so on. Let’s start:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是评估模型在每个单独任务中的表现：QA、计算器、日历等。我们开始吧：
- en: QA Evaluation (LAMA Benchmark)
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: QA 评估（LAMA 基准）
- en: 'The authors evaluate ToolFormer on 3 subsets of the [LAMA Benchmark](https://github.com/facebookresearch/LAMA):'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在 [LAMA 基准](https://github.com/facebookresearch/LAMA) 的 3 个子集上评估了 ToolFormer：
- en: '*SQuAD*, *Google-RE,* and *T-REx*.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*SQuAD*、*Google-RE* 和 *T-REx*。'
- en: For each of these subsets, the task is to complete a short statement with a
    missing fact — e.g. `The theory of relativity is developped by ___` and the model
    should fill in the correct fact.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些子集，每项任务都是用缺失的事实来完成一个简短的陈述——例如`相对论的理论是由 ___ 发展起来的`，模型应该填写正确的事实。
- en: The results of this benchmark are shown in **Figure 10.**
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 该基准测试的结果显示在 **图 10** 中。
- en: '**Note:** The perfomance scores below represent metrics that are evaluated
    differently for each dataset. To avoid getting into details, consider that **higher
    is better**. This is true for the other benchmarks throughout this paper.'
  id: totrans-153
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** 以下的性能评分代表了每个数据集评估的不同指标。为了避免细节问题，考虑到 **分数越高越好**。这在本文中的其他基准测试中也适用。'
- en: '![](../Images/118bc395fb10e87ed755288b753f5b96.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/118bc395fb10e87ed755288b753f5b96.png)'
- en: '**Figure 10:** Performance of ToolFormer on the LAMA benchmark. ([Source](https://arxiv.org/pdf/2302.04761.pdf))'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 10：** ToolFormer 在 LAMA 基准上的表现。 ([来源](https://arxiv.org/pdf/2302.04761.pdf))'
- en: 'The results are particularly interesting: ToolFormer **outperforms** the much
    larger OPT and GPT-3 on all benchmarks.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 结果特别有趣：ToolFormer 在所有基准测试中都**超越**了更大的 OPT 和 GPT-3。
- en: The power of ToolFormer comes from its ability to call external APIs in challenging
    situations.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ToolFormer 的力量来自于其在挑战性情况下调用外部 API 的能力。
- en: Specifically, the model decided to call the QA API in **98.1%** of all cases.
    For only very few examples, it uses a different tool **(0.7%)** or no tool at
    all **(1.2%)**.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，模型决定在 **98.1%** 的情况下调用 QA API。只有极少数的例子中，它使用了其他工具 **(0.7%)** 或根本不使用任何工具
    **(1.2%)**。
- en: Calculator Evaluation (Math Benchmark)
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算器评估（数学基准）
- en: '![](../Images/e7165f857045bdb8928835b11d9bc8f4.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e7165f857045bdb8928835b11d9bc8f4.png)'
- en: '**Figure 11:** Performance of ToolFormer on the math benchmarks. ([Source](https://arxiv.org/pdf/2302.04761.pdf))'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 11：** ToolFormer 在数学基准测试中的表现。 ([来源](https://arxiv.org/pdf/2302.04761.pdf))'
- en: ToolFormer again outperforms OPT and GPT-3 by a large margin.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ToolFormer 再次大幅超越 OPT 和 GPT-3。
- en: The model decides to call the Calculator API in 97.9% of all cases.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型在 97.9% 的情况下决定调用计算器 API。
- en: Wiki Search Evaluation (Search Benchmark)
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 维基搜索评估（搜索基准）
- en: Here, ToolFormer is **not the best model:**
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，ToolFormer **不是最好的模型：**
- en: '![](../Images/7d4774b195d4404259186f446533bfa2.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7d4774b195d4404259186f446533bfa2.png)'
- en: '**Figure 12:** Performance of ToolFormer on Search benchmarks. ([Source](https://arxiv.org/pdf/2302.04761.pdf))'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 12：** ToolFormer 在搜索基准测试中的表现。 ([来源](https://arxiv.org/pdf/2302.04761.pdf))'
- en: 'ToolFormer outperforms OPT but loses to GPT-3\. The authors provide the following
    reasons:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ToolFormer 超越了 OPT，但输给了 GPT-3。作者提供了以下原因：
- en: ToolFormer Wiki Tool searches on **Wikipedia** only, instead of the whole Web
    (GPT-3 was trained on a huge portion of online content).
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ToolFormer 仅在 **Wikipedia** 上进行 Wiki 工具搜索，而不是整个 Web（GPT-3 在大量在线内容上进行了训练）。
- en: ToolFormer would have outperformed GPT-3 if it was able to call the QA Tool
    as well. The authors disabled the QA Tool on purpose — because the datasets used
    to train the QA system have some potential overlap with the data in these benchmarks.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 ToolFormer 能够调用 QA 工具，它将超越 GPT-3。作者故意禁用了 QA 工具——因为用于训练 QA 系统的数据集与这些基准中的数据可能存在重叠。
- en: An additional layer on top of the **Wiki Tool** isnecessary — to reformulate
    the return results and provide clearer answers.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 **Wiki Tool** 之上需要一个额外的层——以重新表述返回的结果并提供更清晰的答案。
- en: Translation Evaluation (MLQA Benchmark)
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 翻译评估（MLQA 基准）
- en: 'Here, the results are very interesting:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，结果非常有趣：
- en: '![](../Images/3f5cf87ad826a54e8fd97641da4b10d1.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f5cf87ad826a54e8fd97641da4b10d1.png)'
- en: '**Figure 13:** Performance of ToolFormer on Translation benchmarks. ([Source](https://arxiv.org/pdf/2302.04761.pdf))'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 13:** ToolFormer 在翻译基准测试中的表现。 ([来源](https://arxiv.org/pdf/2302.04761.pdf))'
- en: ToolFormer easily outperformed OPT and GPT-3 in all languages (except Zh, with
    GPT-3).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ToolFormer 在所有语言中（除了与 GPT-3 的中文）都轻松超越了 OPT 和 GPT-3。
- en: However, ToolFormer was surpassed by the original GPT-J. The authors explained
    this was because GPT-J was also pretrained on multilingual data — while **C**
    had very few multilingual examples.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，ToolFormer 被原始的 GPT-J 超越。作者解释这是因为 GPT-J 也在多语言数据上进行了预训练——而 **C** 只有很少的多语言示例。
- en: Calendar Evaluation (Temporal Datasets)
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**日历评估（时间数据集）**'
- en: Finally, we evaluate ToolFormer’s ability to extract dates and recent information.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们评估了 ToolFormer 提取日期和最新信息的能力。
- en: 'The authors used 2 datasets:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用了 2 个数据集：
- en: '**TempLAMA**: Contains masked data that change with time (e.g., “Kylian Mbappé
    plays for ___”'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TempLAMA**: 包含随时间变化的遮蔽数据（例如，“基利安·姆巴佩为 ___ 球队效力”）'
- en: '**DATESET:** Contains random queries about dates (e.g. “What day of the week
    was it 30 days ago?”'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DATESET:** 包含有关日期的随机查询（例如：“30 天前是星期几？”）'
- en: '**Figure 14** displays the results:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 14** 显示了结果：'
- en: '![](../Images/76744b4e274f4bddabbbd5d2f8e2a285.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/76744b4e274f4bddabbbd5d2f8e2a285.png)'
- en: '**Figure 14:** Performance of ToolFormer on Temporal benchmarks. ([Source](https://arxiv.org/pdf/2302.04761.pdf))'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 14:** ToolFormer 在时间基准测试中的表现。 ([来源](https://arxiv.org/pdf/2302.04761.pdf))'
- en: Again, ToolFormer outperforms the much larger models. The difference is huge
    in **DATESET** — this is expected since finding dates is an inherent weakness
    of LLMs.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，ToolFormer 超越了更大的模型。在 **DATESET** 中差异巨大——这是预期中的，因为找出日期是大型语言模型的固有弱点。
- en: Scaling Laws
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**扩展法则**'
- en: An integral part of training LLMs is whether the training model obeys the scaling
    laws.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 训练大型语言模型的一个重要部分是训练模型是否遵循扩展法则。
- en: Scaling laws are empirical rules that describe the relationship between a LM’s
    parameter size, tokens(dataset size), training time and performance.
  id: totrans-189
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 扩展法则是描述语言模型参数大小、令牌（数据集大小）、训练时间和性能之间关系的经验法则。
- en: Scaling laws were first introduced in [2], but were later re-examined in [3],
    where Deepmind researchers discovered that many LMs were significantly undertrained.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展法则首次在 [2] 中提出，但后来在 [3] 中重新审视，Deepmind 的研究人员发现许多语言模型显著欠训练。
- en: Here, the authors explored ToolFormer’s ability to scale, compared to the other
    models of the benchmark. The results are shown in **Figure 15:**
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，作者探索了 ToolFormer 的扩展能力，与基准测试中的其他模型进行比较。结果见 **图 15:**
- en: '![](../Images/059a380ad0f792baf659516cbd0affc4.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/059a380ad0f792baf659516cbd0affc4.png)'
- en: '**Figure 15:** Performance of ToolFormer vs GPT3 on LAMA, math, and QA benchmarks
    in terms of model size. While API calls are not helpful to the smallest models,
    larger models learn how to make good use of them ([Source](https://arxiv.org/pdf/2302.04761.pdf))'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 15:** ToolFormer 与 GPT3 在 LAMA、数学和 QA 基准测试中的表现，按模型大小划分。虽然 API 调用对最小模型没有帮助，但较大的模型学会了如何有效利用它们
    ([来源](https://arxiv.org/pdf/2302.04761.pdf))'
- en: Evidently, ToolFormer displays excellent signs of scalability — following scaling
    laws.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，ToolFormer 展现出了优良的扩展性——遵循扩展法则。
- en: Smaller models (less than 775M parameters) achieve similar performance — they
    don’t gain an advantage by calling external APIs. After 775M parameter size, ToolFormer
    starts to scale dramatically.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 较小的模型（少于 775M 参数）实现了类似的性能——它们通过调用外部 API 并没有获得优势。参数大小超过 775M 后，ToolFormer 开始显著扩展。
- en: Decoding Strategy
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**解码策略**'
- en: An interesting part about ToolFormer is how the authors implemented the decoding
    strategy.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ToolFormer 的一个有趣之处在于作者如何实施解码策略。
- en: In truth, `**[**`is a special token — and signifies the start of an API call.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，`**[**` 是一个特殊的令牌——表示 API 调用的开始。
- en: During word generation, an LM model calculates the probabilities of every token
    in the vocabulary, and generates **the one with the highest probability** (I explain
    it roughly). This is known as greedy decoding.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在词生成过程中，语言模型计算词汇表中每个令牌的概率，并生成 **概率最高的那个**（我大致解释一下）。这称为贪婪解码。
- en: The authors found experimentally that ToolFormer performs better if `**[**`is
    generated when it is in the top `**k**=10` most probable tokens, instead of getting
    generated when it’s the most likely token (`k=1`).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 作者通过实验发现，如果在前 `**k**=10` 个最可能的令牌中生成 `**[**`，而不是在最可能的令牌（`k=1`）中生成，ToolFormer
    的表现更好。
- en: The parameter `**k=10**` is found experimentally. The results are shown in **Figure
    16:**
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 参数 `**k=10**` 是通过实验发现的。结果见 **图 16:**
- en: '![](../Images/08ac48f2e95b50b4c858e2c91229567c.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08ac48f2e95b50b4c858e2c91229567c.png)'
- en: '**Figure 16:** ToolFormer results on the T-REx subset of LAMA and WebQS for
    different values of k used during decoding. ([Source](https://arxiv.org/pdf/2302.04761.pdf))'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '**图16：** ToolFormer在T-REx的LAMA和WebQS子集上的结果，展示了不同k值在解码过程中使用的情况。（[来源](https://arxiv.org/pdf/2302.04761.pdf)）'
- en: Clearly, the model performs best, on average when `**k**=10` . Figure **16**
    displays only 2 datasets. However, the pattern holds for the other benchmarks
    as well.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，当`**k**=10`时，模型的表现最好，平均而言。**图16**仅显示了2个数据集。然而，这种模式在其他基准测试中也适用。
- en: The Tools LLMs ecosystem
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工具LLMs生态系统
- en: ToolFormer uses external APIs to solve maths and reasoning problems. But these
    problems can also be addressed by other approaches.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ToolFormer使用外部API来解决数学和推理问题。但这些问题也可以通过其他方法解决。
- en: The most popular approach is called **‘chain-of-thoughts’ [4]:**
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 最流行的方法称为**“思维链”**[4]：
- en: In ‘chain of thoughts‘, the LLM learns to break a prompt into intermediate steps
    — solving each step individually before giving the final answer.
  id: totrans-208
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在“思维链”中，LLM学会将提示分解为中间步骤——逐步解决每个步骤后再给出最终答案。
- en: 'Here, we don’t call external APIs. Instead, we teach the model to decompose
    a prompt into smaller parts — which helps the model with arithmetic tasks. An
    example is shown in **Figure 17**:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们不调用外部API。而是教模型将提示分解为更小的部分——这有助于模型进行算术任务。一个示例显示在**图17**中：
- en: '![](../Images/0a5fcd3a9b57b307071ccdd89af506b2.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a5fcd3a9b57b307071ccdd89af506b2.png)'
- en: '**Figure 17:** Using chain-of-thought prompting (right) the model can figure
    out the correct answer. Chain-of-thought reasoning processes are highlighted in
    green [[Wei et al.](https://arxiv.org/pdf/2201.11903.pdf)]'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '**图17：** 使用思维链提示（右）模型可以找出正确答案。思维链推理过程用绿色突出显示[[Wei et al.](https://arxiv.org/pdf/2201.11903.pdf)]'
- en: The **‘chain-of-thoughts’** paradigm has been improved in the latest research.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '**“思维链”**范式在最新的研究中得到了改进。'
- en: '**Program-aided Language models** (PAL) **[5]** achieve even better results
    by breaking down the prompts into both textual intermediate steps and Python code
    (**Figure 18)**:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '**程序辅助语言模型**（PAL）**[5]**通过将提示分解为文本中间步骤和Python代码（**图18**）获得了更好的结果：'
- en: '![](../Images/9c4ecec1f8d7bf9dd83b8a6bacc7e907.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c4ecec1f8d7bf9dd83b8a6bacc7e907.png)'
- en: '**Figure 18:** **Chain-of-thought** (left) gives a wrong answer, while **PAL**
    (right) is correct. PAL combines Chain-of-thought reasoning (highlighted in blue)
    with programming annotations (highlighted in pink) [[Luyu Gao et al.]](https://arxiv.org/pdf/2211.10435.pdf)'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '**图18：** **思维链**（左）给出了错误的答案，而**PAL**（右）是正确的。PAL结合了思维链推理（用蓝色标记）和编程注释（用粉色标记）[[Luyu
    Gao et al.]](https://arxiv.org/pdf/2211.10435.pdf)'
- en: 'Lastly, we can use LangChain, an application framework for LLMs. Langchain
    uses agents to integrate with various search APIs, capable of searching the web.
    **Figure 19** shows the SerpAPI tool:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用LangChain，一个用于LLMs的应用框架。Langchain使用代理与各种搜索API集成，能够在网络上进行搜索。**图19**展示了SerpAPI工具：
- en: '![](../Images/e6bf672cea31edf8bbe011451479ff8e.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e6bf672cea31edf8bbe011451479ff8e.png)'
- en: '**Figure 19:** We can instruct a language model to use an API for searching
    the web, and integrating the search results into the prompt so we get the correct
    answer. ([Source](https://python.langchain.com/docs/integrations/tools/serpapi))'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '**图19：** 我们可以指示语言模型使用API进行网络搜索，并将搜索结果整合到提示中，从而获得正确的答案。（[来源](https://python.langchain.com/docs/integrations/tools/serpapi)）'
- en: '**What is the difference between ToolFormer and Langchain agents?**'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '**ToolFormer和Langchain代理之间有什么区别？**'
- en: Langchain agents have to first use the appropriate API (which a human specifies)
    and then combine the results with the prompt to get a correct answer.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Langchain代理首先必须使用适当的API（由人类指定），然后将结果与提示结合以获得正确答案。
- en: In contrast, ToolFormer was **explicitly trained** to call and integrate API
    tools (no manual intervention).
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相比之下，ToolFormer被**明确训练**用于调用和整合API工具（无需人工干预）。
- en: Closing Remarks
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结语
- en: This article explored ToolFormer, a model capable of calling external Tools.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 本文探讨了ToolFormer，这是一种能够调用外部工具的模型。
- en: Essentially, ToolFormer is a process that can teach any LLM to call external
    APIs.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，ToolFormer是一个可以教任何LLM调用外部API的过程。
- en: With the adoption of LLMS, the necessity to call external resources will become
    apparent. Even ChatGPT now allows the user to enrich his prompt with search results
    from the web.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 随着LLMS的采用，调用外部资源的必要性将变得显而易见。即使是ChatGPT现在也允许用户用来自网络的搜索结果丰富他的提示。
- en: Thank you for reading!
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: I write an in-depth analysis of an impactful AI paper once a month.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我每月撰写一次对有影响力的 AI 论文的深入分析。
- en: '**Stay connected!**'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '**保持联系！**'
- en: Follow me on [Linkedin](https://www.linkedin.com/in/nikos-kafritsas-b3699180/)!
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 [Linkedin](https://www.linkedin.com/in/nikos-kafritsas-b3699180/) 上关注我！
- en: Subscribe to my newsletter, [**AI Horizon Forecast**](https://aihorizonforecast.substack.com)!
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 订阅我的新闻通讯，[**AI Horizon Forecast**](https://aihorizonforecast.substack.com)！
- en: '[](https://aihorizonforecast.substack.com/?source=post_page-----37e4227996f1--------------------------------)
    [## AI Horizon Forecast | Nikos Kafritsas | Substack'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://aihorizonforecast.substack.com/?source=post_page-----37e4227996f1--------------------------------)
    [## AI Horizon Forecast | Nikos Kafritsas | Substack'
- en: Explaining complex AI models as clear as daylight. Focusing on time series and
    latest AI research. Click to read AI…
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 以清晰明了的方式解释复杂的 AI 模型。专注于时间序列和最新的 AI 研究。点击阅读 AI…
- en: aihorizonforecast.substack.com](https://aihorizonforecast.substack.com/?source=post_page-----37e4227996f1--------------------------------)
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '[aihorizonforecast.substack.com](https://aihorizonforecast.substack.com/?source=post_page-----37e4227996f1--------------------------------)'
- en: Appendix
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录
- en: The authors also impose a minimal filtering process on the 1st step of the data
    augmentation process, to save costs. For example, the sentences that are not annotated
    at all with special tokens `***[QA..***`etc are rejected from the next steps.
  id: totrans-235
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 作者还在数据增强过程的第一步上施加了最小化筛选过程，以节省成本。例如，没有用特殊标记 `***[QA..***` 等注释的句子会被拒绝进入下一步。
- en: ''
  id: totrans-236
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Also, the authors calculate the most probable positions in the sentence that
    are likely to initiate an API call. The symbols `***[***`***,*** `***]***`arealsospecial
    tokens and signify the start and the end of an API call.
  id: totrans-237
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 此外，作者计算了句子中最可能引发 API 调用的位置。符号`***[***`***,*** `***]***`也是特殊标记，表示 API 调用的开始和结束。
- en: ''
  id: totrans-238
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: So, the authors calculate the position `***i***` where the token `***[***`has
    the highest probability of appearing. Hence, only the sentences where the start
    of the API call(the token `***[***`) is generated on the most probable position
    `***i***`, are passed to the next step.
  id: totrans-239
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 因此，作者计算了位置 `***i***`，其中标记 `***[***` 出现的概率最高。因此，只有在 API 调用的开始（标记 `***[***`）生成在最可能位置
    `***i***` 的句子会被传递到下一步。
- en: References
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Timo Schick et al. [*Toolformer: Language Models Can Teach Themselves to
    Use Tools*](https://arxiv.org/pdf/2302.04761.pdf)'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Timo Schick 等人 [*Toolformer: Language Models Can Teach Themselves to Use
    Tools*](https://arxiv.org/pdf/2302.04761.pdf)'
- en: '[2] Jared Kaplan et al. [*Scaling Laws for Neural Language Models*](https://arxiv.org/pdf/2001.08361.pdf)'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Jared Kaplan 等人 [*Scaling Laws for Neural Language Models*](https://arxiv.org/pdf/2001.08361.pdf)'
- en: '[3] Jordan Hoffmann et al. [*Training Compute-Optimal Large Language Models*](https://arxiv.org/pdf/2203.15556.pdf)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Jordan Hoffmann 等人 [*Training Compute-Optimal Large Language Models*](https://arxiv.org/pdf/2203.15556.pdf)'
- en: '[4] Jason Wei et al. [*Chain-of-Thought Prompting Elicits Reasoning in Large
    Language Models*](https://arxiv.org/pdf/2201.11903.pdf) *(January 2023)*'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Jason Wei 等人 [*Chain-of-Thought Prompting Elicits Reasoning in Large Language
    Models*](https://arxiv.org/pdf/2201.11903.pdf) *(2023年1月)*'
- en: '[5] Luyu Gao et al. [*PAL: Program-aided Language Models*](https://arxiv.org/pdf/2211.10435.pdf)
    *(January 2023)*'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Luyu Gao 等人 [*PAL: Program-aided Language Models*](https://arxiv.org/pdf/2211.10435.pdf)
    *(2023年1月)*'
