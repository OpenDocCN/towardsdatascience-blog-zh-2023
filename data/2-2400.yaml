- en: Writing a book on NLP is a bit like solving a complex data science project
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写关于自然语言处理的书籍有点像解决一个复杂的数据科学项目
- en: 原文：[https://towardsdatascience.com/writing-a-book-on-nlp-is-a-bit-like-solving-a-complex-data-science-project-c0848f975ca](https://towardsdatascience.com/writing-a-book-on-nlp-is-a-bit-like-solving-a-complex-data-science-project-c0848f975ca)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/writing-a-book-on-nlp-is-a-bit-like-solving-a-complex-data-science-project-c0848f975ca](https://towardsdatascience.com/writing-a-book-on-nlp-is-a-bit-like-solving-a-complex-data-science-project-c0848f975ca)
- en: An interview with Lewis Tunstall, co-author of the book- [**Natural Language
    Processing with Transformers**](https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/)
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对刘易斯·坦斯特尔的访谈，他是《[**自然语言处理与变压器**](https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/)》一书的合著者
- en: '[](https://pandeyparul.medium.com/?source=post_page-----c0848f975ca--------------------------------)[![Parul
    Pandey](../Images/760b72a4feacfad6fc4224835c2e1f19.png)](https://pandeyparul.medium.com/?source=post_page-----c0848f975ca--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c0848f975ca--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c0848f975ca--------------------------------)
    [Parul Pandey](https://pandeyparul.medium.com/?source=post_page-----c0848f975ca--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pandeyparul.medium.com/?source=post_page-----c0848f975ca--------------------------------)[![帕鲁尔·潘迪](../Images/760b72a4feacfad6fc4224835c2e1f19.png)](https://pandeyparul.medium.com/?source=post_page-----c0848f975ca--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c0848f975ca--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c0848f975ca--------------------------------)
    [Parul Pandey](https://pandeyparul.medium.com/?source=post_page-----c0848f975ca--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c0848f975ca--------------------------------)
    ·7 min read·Feb 6, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----c0848f975ca--------------------------------)
    ·7分钟阅读·2023年2月6日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '*A series of interviews highlighting the incredible work of writers in the
    space of data science and their path of writing.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*一系列访谈，突显数据科学领域作家的非凡工作及其写作历程。*'
- en: '![](../Images/5fddda751df11beee48e146f3e6a365f.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5fddda751df11beee48e146f3e6a365f.png)'
- en: Photo courtesy of Lewis Tunstall
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由刘易斯·坦斯特尔提供
- en: “In fiction, the language and the senses it evokes are important, whereas in
    technical writing, the content, and the information it conveys, are important.”
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “在小说中，语言和引发的感官体验很重要，而在技术写作中，内容和传达的信息才是重要的。”
- en: ― **Krista Van Laan,** [**The Insider’s Guide to Technical Writing**](https://www.goodreads.com/work/quotes/20300600)
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ― **克里斯塔·范·兰**，[**《技术写作内幕指南》**](https://www.goodreads.com/work/quotes/20300600)
- en: '*Last edited on Feb 6, 2023*'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*最后编辑于2023年2月6日*'
- en: Being a writer myself, I have a keen interest in uncovering the narratives behind
    the books we read, especially in the machine learning realm. These writers possess
    an uncanny ability to translate the complexities of AI into words that are both
    informative and interesting is truly remarkable. It is my goal, through a series
    of interviews, to bring their stories to the forefront and shed light on the story
    of some of the well-known authors in the field of Artificial Intelligence.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一名作家，我对揭示我们阅读的书籍背后的故事充满了浓厚的兴趣，特别是在机器学习领域。这些作家具有将人工智能的复杂性转化为既信息丰富又引人入胜的文字的非凡能力，实在是令人惊叹。我的目标是通过一系列访谈，把他们的故事展现出来，揭示一些在人工智能领域的知名作者的故事。
- en: 'Meet the Author: Lewis Tunstall'
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 认识作者：刘易斯·坦斯特尔
- en: '**Lewis Tunstall** is an accomplished machine learning engineer currently working
    at Hugging Face. He has extensive experience in building machine learning applications
    for startups and enterprises, with a focus on the areas of NLP, topological data
    analysis, and time series. With a PhD in theoretical physics, Lewis has had the
    opportunity to hold research positions in various countries, including Australia,
    the USA, and Switzerland. His current work focuses on developing innovative tools
    for the NLP community and empowering individuals with the knowledge and skills
    to use them effectively.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**刘易斯·坦斯特尔** 是一位杰出的机器学习工程师，目前在Hugging Face工作。他在为初创企业和大型企业构建机器学习应用方面拥有丰富的经验，专注于自然语言处理、拓扑数据分析和时间序列等领域。拥有理论物理学博士学位的刘易斯曾有机会在澳大利亚、美国和瑞士等多个国家担任研究职位。他目前的工作集中在为自然语言处理社区开发创新工具，并赋予个人有效使用这些工具的知识和技能。'
- en: Lewis is the co-author of the book -” Natural Language Processing with Transformers”
    along with [**Leandro von Werra**](https://twitter.com/lvwerra?s=20&t=XOZTW3iOxNZoAopPR5LRsw)and[**Thomas
    Wolf**](https://twitter.com/Thom_Wolf)**.** The book is a comprehensive guide
    to the latest advancements in the field of NLP and is a great resource for anyone
    looking to gain a deeper understanding of NLP and how it can be applied to real-world
    problems.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 刘易斯是书籍《自然语言处理与 Transformers》的合著者之一，与[**Leandro von Werra**](https://twitter.com/lvwerra?s=20&t=XOZTW3iOxNZoAopPR5LRsw)和[**Thomas
    Wolf**](https://twitter.com/Thom_Wolf)**合作**。这本书是该领域最新进展的全面指南，是任何希望深入理解自然语言处理以及如何将其应用于现实世界问题的人的绝佳资源。
- en: '[](https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/?source=post_page-----c0848f975ca--------------------------------)
    [## Natural Language Processing with Transformers, Revised Edition'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/?source=post_page-----c0848f975ca--------------------------------)
    [## 自然语言处理与 Transformers, 修订版'
- en: Since their introduction in 2017, transformers have quickly become the dominant
    architecture for achieving…
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自2017年引入以来，transformers 已迅速成为实现...
- en: learning.oreilly.com](https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/?source=post_page-----c0848f975ca--------------------------------)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[learning.oreilly.com](https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/?source=post_page-----c0848f975ca--------------------------------)'
- en: '**Q: How did the idea of this book originate?**'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**问: 这本书的构思是如何产生的？**'
- en: '*Lewis:* Although we began the book in 2020, its origin story really began
    in 2019 when Leandro and I first started working with Transformer models. At the
    time, Jay Alammar’s amazing [blog posts](https://jalammar.github.io/) and [The
    Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
    by Sasha Rush were among the few written resources available to understand how
    these models work. These articles were (and are!) great for developing understanding,
    but we felt there was a gap in guiding people on how to apply Transformers to
    industrial use cases. So in 2020, we had the somewhat foolhardy idea to combine
    the knowledge we’d learned from our jobs as a book. My wife suggested that we
    contact Thomas to see if he’d be interested in being a co-author, and to our great
    surprise, he agreed!'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*刘易斯:* 尽管我们在2020年开始了这本书，但它的起源故事实际上可以追溯到2019年，当时Leandro和我首次开始使用 Transformer
    模型。当时，Jay Alammar 的精彩[博客文章](https://jalammar.github.io/)和 Sasha Rush 的[《注释 Transformer》](http://nlp.seas.harvard.edu/2018/04/03/attention.html)是理解这些模型如何工作的为数不多的书面资源。这些文章（并且仍然是！）非常有助于理解，但我们觉得在指导人们如何将
    Transformer 应用于工业用例方面存在一个空白。因此，在2020年，我们有了一个有些冒失的想法，将我们从工作中学到的知识结合成一本书。我的妻子建议我们联系
    Thomas 看他是否有兴趣成为合著者，令我们惊讶的是，他同意了！'
- en: '**Q: Could you summarize the main points covered in the book for the readers?**'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**问: 能否为读者总结一下书中涵盖的主要内容？**'
- en: '*Lewis:* As you might expect from the title, this book is about applying Transformer
    models to NLP tasks. Most chapters are structured around a single use case you’re
    likely to encounter in the industry. The book covers core applications such as
    text classification, named entity recognition, and question answering. We take
    a lot of inspiration from the fantastic [fast.ai](http://fast.ai/) course (which
    is how I got started with deep learning!), so the book is written in a hands-on
    style, emphasising solving real-world problems with code. In the early chapters,
    we introduce the concepts of self-attention and transfer learning, which underpins
    the success of Transformers.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*刘易斯:* 正如标题所示，这本书是关于将 Transformer 模型应用于自然语言处理任务的。大多数章节围绕着你在行业中可能遇到的单一用例进行结构化。书中涵盖了文本分类、命名实体识别和问答等核心应用。我们从精彩的[fast.ai](http://fast.ai/)课程中汲取了很多灵感（这也是我开始深度学习的途径！），因此本书采用了动手实践的风格，强调通过代码解决现实世界的问题。在早期章节中，我们介绍了自注意力机制和迁移学习的概念，这些概念是
    Transformer 成功的基础。'
- en: The main advice I’d suggest to new writers is to find co-authors or colleagues
    who can deeply critique your ideas and writing.
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我对新作者的主要建议是找到可以深度批评你的想法和写作的合著者或同事。
- en: The latter part of the book dives into more advanced topics, such as optimising
    Transformers for production environments and handling scenarios where you have
    little labelled data (i.e. every data scientist’s nightmare 😃. One of my favourite
    chapters is about training a GPT-2 scale model from scratch, including how to
    create a large-scale corpus and train on distributed infrastructure! The book
    concludes with an eye towards the future by highlighting some of the exciting
    recent developments involving Transformers and other modalities like images and
    audio.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 书的后半部分深入探讨了更高级的主题，如为生产环境优化 Transformers 和处理标注数据稀少的场景（即每个数据科学家的噩梦 😃）。我最喜欢的章节之一是关于从头开始训练一个
    GPT-2 规模模型，包括如何创建一个大规模语料库并在分布式基础设施上进行训练！书的结尾着眼于未来，通过突出一些涉及 Transformers 和其他模式（如图像和音频）的激动人心的最新进展来作结。
- en: '**Q: You co-authored the book with Leandro von Werra and Thomas Wolf. How is
    it to author a book with multiple writers?**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**问：你与 Leandro von Werra 和 Thomas Wolf 共同编写了这本书。与多位作者一起编写书籍是什么感觉？**'
- en: Writing a book on NLP is a bit like solving a complex data science project.
    Among various challenges, you need to design the use case and story for each chapter,
    find appropriate data and models, and make sure that you can keep code complexity
    to a minimum because reading long blocks of code is no fun at all. And just like
    any data science project, some chapters involved running dozens of experiments.
    For all these challenges, I found writing the book with Leandro and Thomas to
    be an extremely valuable experience! In particular, having co-authors with whom
    you can brainstorm ideas or sanity-check your code was especially helpful. Of
    course, one challenge that arises with multiple authors is trying to keep the
    same writing style throughout the book, and we were lucky to have great editors
    at O’Reilly to help us achieve this.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 写一本关于 NLP 的书有点像解决一个复杂的数据科学项目。在各种挑战中，你需要为每个章节设计用例和故事，找到合适的数据和模型，并确保代码复杂性保持在最低限度，因为阅读长块代码一点也不有趣。而且就像任何数据科学项目一样，一些章节涉及到进行几十次实验。面对这些挑战，我发现与
    Leandro 和 Thomas 一起写书是一个极其宝贵的经历！特别是，与能够头脑风暴或检查代码的合著者一起工作尤其有帮助。当然，多位作者面临的一个挑战是保持全书的写作风格一致，我们很幸运能有
    O’Reilly 的优秀编辑帮助我们实现这一点。
- en: '**Q: Who do you think is the target audience for the book?**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**问：你认为这本书的目标读者是谁？**'
- en: We wrote this book for data scientists and machine learning/software engineers
    who may have heard about the recent breakthroughs involving Transformers but still
    need an in-depth guide to help them adapt these models to their own use cases.
    In other words, people like myself about 1–2 years ago! We envision our book will
    be most helpful to industry practitioners or those hoping to break into NLP from
    a nearby field (e.g. a software engineer who wants to build machine learning-powered
    applications).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们写这本书是为了数据科学家和机器学习/软件工程师，他们可能听说过最近涉及 Transformers 的突破，但仍需要一个深入的指南来帮助他们将这些模型适应到自己的用例中。换句话说，就是像我自己大约1-2年前一样的人！我们设想这本书对行业从业者或希望从相关领域（例如，想要构建机器学习驱动应用的工程师）转向自然语言处理（NLP）的人会最有帮助。
- en: '**Q: What, according to you, is the best way to make the most out of this book-
    read first and code later or code along?**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**问：你认为怎样才能最大化利用这本书——先读后写代码，还是边读边写代码？**'
- en: We actually wrote the whole book using Jupyter notebooks and a tool called [fastdoc](https://github.com/fastai/fastdoc),
    so every line of code can be executed in the accompanying notebooks that we provide
    on [GitHub](https://github.com/nlp-with-transformers/notebooks). I suggest having
    the book chapter and accompanying code side-by-side, so you can quickly experiment
    with the inputs and outputs while you’re reading. We are also planning some live
    events around the book’s release, so stay tuned on the [book’s website](https://nlp-with-transformers.github.io/website/)
    to find out when these events will happen.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上使用 Jupyter notebooks 和一个名为 [fastdoc](https://github.com/fastai/fastdoc)
    的工具写了整本书，因此每一行代码都可以在我们提供的 [GitHub](https://github.com/nlp-with-transformers/notebooks)
    附带的 notebooks 中执行。我建议将书的章节和附带的代码放在一起，这样你可以在阅读时快速实验输入和输出。我们还计划在书发布时举办一些直播活动，请关注
    [书籍官网](https://nlp-with-transformers.github.io/website/) 以了解这些活动的时间。
- en: '**Q: What advice would you give a new writer, someone just starting?**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**问：你会给刚开始写作的新作者什么建议？**'
- en: Hehe, my first piece of advice would be to ask whether you’re really sure you
    want to write a book. My second piece of advice would be don’t write a book just
    after you’ve had a baby … in the middle of a pandemic 😃
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 呵呵，我的第一条建议是问自己是否真的确定想写一本书。我的第二条建议是不要在刚刚有了宝宝之后……在疫情期间写书 😃
- en: Writing a book on NLP is a bit like solving a complex data science project.
    Among various challenges, you need to design the use case and story for each chapter,
    find appropriate data and models, and make sure that you can keep code complexity
    to a minimum because reading long blocks of code is no fun at all.
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 写一本关于NLP的书有点像解决一个复杂的数据科学项目。在各种挑战中，你需要为每一章设计用例和故事，找到合适的数据和模型，并确保代码复杂性保持在最低，因为阅读长篇代码一点也不有趣。
- en: Jokes aside, the main advice I’d suggest to new writers is to find co-authors
    or colleagues who can deeply critique your ideas and writing. We were fortunate
    to have experienced writers like Aurélien Geron and Hamel Husain provide us with
    detailed comments on our drafts, and their feedback significantly improved the
    book. This type of feedback is invaluable because, as a writer, you sometimes
    forget which aspects were challenging to master the first time you learned a subject.
    In a curious twist of fate, Aurélien’s written the foreword to our book!
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 说正经的，我给新作者的主要建议是找到可以深入批评你想法和写作的合著者或同事。我们很幸运地有像Aurélien Geron和Hamel Husain这样的经验丰富的作者提供了详细的评论，他们的反馈显著提高了书籍质量。这种反馈非常宝贵，因为作为一个作者，你有时会忘记最初学习一个主题时哪些方面很难掌握。巧合的是，Aurélien为我们的书写了前言！
- en: '**Q: Who is your favourite book and author (in technical or non-technical space)?**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**问：你最喜欢的书和作者是谁（技术或非技术领域）？**'
- en: Ooh, this is a tricky question! I’m an avid reader of science fiction, and [**The
    Three-Body Problem**](https://en.wikipedia.org/wiki/The_Three-Body_Problem_(novel))
    **by Liu Cixin** is one of my favourite series because it provides a really novel
    solution to the **Fermi paradox** — i.e. why is there no observable evidence of
    alien civilisations, given the large number of stars with Earth-like planets?
    I also enjoy reading books and biographies about the history of scientific fields
    like physics and computer science. One recent favourite of mine is [**The Man
    from the Future** by Ananyo Bhattacharya](https://www.amazon.in/Man-Future-Visionary-Life-Neumann/dp/0241398851),
    which is about the life of the extraordinary John von Neumann. Although I had
    encountered some of von Neumann’s work as a physics student (he wrote a masterpiece
    on quantum mechanics), I was not fully aware of how vast his contributions were
    to mathematics and science at large. The book does a wonderful job of describing
    his work and the fascinating history around it — highly recommended!
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 哦，这是个棘手的问题！我是科幻小说的忠实读者，[**《三体》**](https://en.wikipedia.org/wiki/The_Three-Body_Problem_(novel))
    **由刘慈欣** 编写，是我最喜欢的系列之一，因为它提供了对**费米悖论**的一个全新解决方案——即为什么没有可观察到的外星文明证据，尽管有大量类地行星的恒星？我也喜欢阅读关于物理学和计算机科学等科学领域历史的书籍和传记。我最近的一个最爱是
    [**《未来之人》由Ananyo Bhattacharya编写**](https://www.amazon.in/Man-Future-Visionary-Life-Neumann/dp/0241398851)，这本书讲述了杰出的约翰·冯·诺依曼的生活。虽然作为物理学学生我遇到过冯·诺依曼的一些工作（他写了关于量子力学的杰作），但我没有完全意识到他对数学和科学领域的广泛贡献。这本书非常出色地描述了他的工作及其迷人的历史——强烈推荐！
- en: 👉 **Are you looking forward to connecting with Lewis? Follow him on** [**Twitter**](https://twitter.com/_lewtun)**.**
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 👉 **你想与Lewis建立联系吗？请关注他的** [**Twitter**](https://twitter.com/_lewtun)**.**
- en: 👉 **Read other interviews in this series:**
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 👉 **阅读本系列的其他访谈：**
- en: '[](/dont-just-take-notes-turn-them-into-articles-and-share-them-with-others-72aa43b83e29?source=post_page-----c0848f975ca--------------------------------)
    [## Don’t just take notes — turn them into articles and share them with others'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/dont-just-take-notes-turn-them-into-articles-and-share-them-with-others-72aa43b83e29?source=post_page-----c0848f975ca--------------------------------)
    [## 不要只是做笔记 — 把它们变成文章并与他人分享'
- en: An interview with Alexey Grigorev, author of the book- Machine Learning Bookcamp.
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一次与Alexey Grigorev的访谈，他是《机器学习书营》的作者。
- en: towardsdatascience.com](/dont-just-take-notes-turn-them-into-articles-and-share-them-with-others-72aa43b83e29?source=post_page-----c0848f975ca--------------------------------)
    [](/you-do-not-become-better-by-employing-fancy-techniques-but-by-working-on-the-fundamentals-17d5c471c69c?source=post_page-----c0848f975ca--------------------------------)
    [## You do not become better by employing fancy techniques but by working on the
    fundamentals
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[不要只是做笔记，把它们转化为文章并与他人分享](https://towardsdatascience.com/dont-just-take-notes-turn-them-into-articles-and-share-them-with-others-72aa43b83e29?source=post_page-----c0848f975ca--------------------------------)
    [](/you-do-not-become-better-by-employing-fancy-techniques-but-by-working-on-the-fundamentals-17d5c471c69c?source=post_page-----c0848f975ca--------------------------------)
    [## 你并不会通过使用花哨的技巧变得更好，而是通过专注于基础'
- en: 'An interview with Radek Osmulski, author of the book- Meta Learning: How To
    Learn Deep Learning And Thrive In The…'
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '与Radek Osmulski的访谈，他是《Meta Learning: How To Learn Deep Learning And Thrive
    In The…》一书的作者'
- en: towardsdatascience.com](/you-do-not-become-better-by-employing-fancy-techniques-but-by-working-on-the-fundamentals-17d5c471c69c?source=post_page-----c0848f975ca--------------------------------)
    [](/publishing-is-powerful-as-it-serves-as-a-catalyst-for-scope-and-writing-decisions-713306e8a0d?source=post_page-----c0848f975ca--------------------------------)
    [## Publishing Is Powerful as It Serves as a Catalyst for Scope and Writing Decisions
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[你并不会通过使用花哨的技巧变得更好，而是通过专注于基础](https://towardsdatascience.com/you-do-not-become-better-by-employing-fancy-techniques-but-by-working-on-the-fundamentals-17d5c471c69c?source=post_page-----c0848f975ca--------------------------------)
    [](/publishing-is-powerful-as-it-serves-as-a-catalyst-for-scope-and-writing-decisions-713306e8a0d?source=post_page-----c0848f975ca--------------------------------)
    [## 发布是强大的，因为它作为范围和写作决策的催化剂'
- en: An interview with Christoph Molnar, author of the book- Interpretable Machine
    Learning
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 与Christoph Molnar的访谈，他是《Interpretable Machine Learning》一书的作者
- en: towardsdatascience.com](/publishing-is-powerful-as-it-serves-as-a-catalyst-for-scope-and-writing-decisions-713306e8a0d?source=post_page-----c0848f975ca--------------------------------)
    ![](../Images/90c29813c4ef86e9865b1cc09aca3796.png)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[发布是强大的，因为它作为范围和写作决策的催化剂](https://towardsdatascience.com/publishing-is-powerful-as-it-serves-as-a-catalyst-for-scope-and-writing-decisions-713306e8a0d?source=post_page-----c0848f975ca--------------------------------)
    ![](../Images/90c29813c4ef86e9865b1cc09aca3796.png)'
