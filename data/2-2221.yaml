- en: 'Unlocking MLOps using Airflow: A Comprehensive Guide to ML System Orchestration'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Airflow è§£é” MLOpsï¼šML ç³»ç»Ÿç¼–æ’çš„å…¨é¢æŒ‡å—
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/unlocking-mlops-using-airflow-a-comprehensive-guide-to-ml-system-orchestration-880aa9be8cff](https://towardsdatascience.com/unlocking-mlops-using-airflow-a-comprehensive-guide-to-ml-system-orchestration-880aa9be8cff)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/unlocking-mlops-using-airflow-a-comprehensive-guide-to-ml-system-orchestration-880aa9be8cff](https://towardsdatascience.com/unlocking-mlops-using-airflow-a-comprehensive-guide-to-ml-system-orchestration-880aa9be8cff)
- en: '[THE FULL STACK 7-STEPS MLOPS FRAMEWORK](https://towardsdatascience.com/tagged/full-stack-mlops)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[å®Œæ•´çš„ 7 æ­¥ MLOps æ¡†æ¶](https://towardsdatascience.com/tagged/full-stack-mlops)'
- en: 'Lesson 4: Private PyPi Server. Orchestrate Everything with Airflow.'
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬ 4 èŠ‚ï¼šç§äºº PyPi æœåŠ¡å™¨ã€‚ç”¨ Airflow ç¼–æ’ä¸€åˆ‡ã€‚
- en: '[](https://pauliusztin.medium.com/?source=post_page-----880aa9be8cff--------------------------------)[![Paul
    Iusztin](../Images/d07551a78fa87940220b49d9358f3166.png)](https://pauliusztin.medium.com/?source=post_page-----880aa9be8cff--------------------------------)[](https://towardsdatascience.com/?source=post_page-----880aa9be8cff--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----880aa9be8cff--------------------------------)
    [Paul Iusztin](https://pauliusztin.medium.com/?source=post_page-----880aa9be8cff--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pauliusztin.medium.com/?source=post_page-----880aa9be8cff--------------------------------)[![Paul
    Iusztin](../Images/d07551a78fa87940220b49d9358f3166.png)](https://pauliusztin.medium.com/?source=post_page-----880aa9be8cff--------------------------------)[](https://towardsdatascience.com/?source=post_page-----880aa9be8cff--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----880aa9be8cff--------------------------------)
    [Paul Iusztin](https://pauliusztin.medium.com/?source=post_page-----880aa9be8cff--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----880aa9be8cff--------------------------------)
    Â·17 min readÂ·May 23, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----880aa9be8cff--------------------------------)
    Â·17 åˆ†é’Ÿé˜…è¯»Â·2023å¹´5æœˆ23æ—¥
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/3c440d7e58f12e1041fcde014eea4fda.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3c440d7e58f12e1041fcde014eea4fda.png)'
- en: Photo by [Hassan Pasha](https://unsplash.com/@hpzworkz?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”± [Hassan Pasha](https://unsplash.com/@hpzworkz?utm_source=medium&utm_medium=referral)
    æä¾›ï¼Œæ¥æºäº [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: This tutorial represents **lesson 4 out of a 7-lesson course** that will walk
    you step-by-step through how to **design, implement, and deploy an ML system**
    using **MLOps good practices**. During the course, you will build a production-ready
    model to forecast energy consumption levels for the next 24 hours across multiple
    consumer types from Denmark.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ•™ç¨‹ä»£è¡¨**7 èŠ‚è¯¾ç¨‹ä¸­çš„ç¬¬ 4 èŠ‚è¯¾**ï¼Œå°†ä¸€æ­¥ä¸€æ­¥æŒ‡å¯¼ä½ å¦‚ä½•**è®¾è®¡ã€å®ç°å’Œéƒ¨ç½²ä¸€ä¸ª ML ç³»ç»Ÿ**ï¼Œå¹¶è¿ç”¨**MLOps å¥½å®è·µ**ã€‚åœ¨è¯¾ç¨‹ä¸­ï¼Œä½ å°†æ„å»ºä¸€ä¸ªå‡†å¤‡æŠ•å…¥ç”Ÿäº§çš„æ¨¡å‹ï¼Œä»¥é¢„æµ‹ä¸¹éº¦å¤šä¸ªæ¶ˆè´¹è€…ç±»å‹åœ¨æ¥ä¸‹æ¥çš„
    24 å°æ—¶å†…çš„èƒ½æºæ¶ˆè€—æ°´å¹³ã€‚
- en: '*By the end of this course, you will understand all the fundamentals of designing,
    coding and deploying an ML system using a batch-serving architecture.*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*åœ¨æœ¬è¯¾ç¨‹ç»“æŸæ—¶ï¼Œä½ å°†ç†è§£å¦‚ä½•ä½¿ç”¨æ‰¹é‡æœåŠ¡æ¶æ„è®¾è®¡ã€ç¼–ç å’Œéƒ¨ç½² ML ç³»ç»Ÿçš„æ‰€æœ‰åŸºç¡€çŸ¥è¯†ã€‚*'
- en: This course *targets mid/advanced machine learning engineers* who want to level
    up their skills by building their own end-to-end projects.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬è¯¾ç¨‹*é’ˆå¯¹ä¸­çº§/é«˜çº§æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆ*ï¼Œå¸Œæœ›é€šè¿‡æ„å»ºè‡ªå·±çš„ç«¯åˆ°ç«¯é¡¹ç›®æ¥æå‡æŠ€èƒ½ã€‚
- en: '*Nowadays, certificates are everywhere. Building advanced end-to-end projects
    that you can later show off is the best way to get recognition as a professional
    engineer.*'
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*å¦‚ä»Šï¼Œè¯ä¹¦åˆ°å¤„éƒ½æ˜¯ã€‚æ„å»ºå¯ä»¥å±•ç¤ºçš„é«˜çº§ç«¯åˆ°ç«¯é¡¹ç›®æ˜¯è·å¾—ä¸“ä¸šå·¥ç¨‹å¸ˆè®¤å¯çš„æœ€ä½³æ–¹å¼ã€‚*'
- en: 'Table of Contents:'
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç›®å½•ï¼š
- en: Course Introduction
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯¾ç¨‹ä»‹ç»
- en: Course Lessons
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯¾ç¨‹å†…å®¹
- en: Data Source
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ•°æ®æº
- en: 'Lesson 4: Private PyPi Server. Orchestrate Everything with Airflow.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¬¬ 4 èŠ‚ï¼šç§äºº PyPi æœåŠ¡å™¨ã€‚ç”¨ Airflow ç¼–æ’ä¸€åˆ‡ã€‚
- en: 'Lesson 4: Code'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¬¬ 4 èŠ‚ï¼šä»£ç 
- en: Conclusion
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: References
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‚è€ƒèµ„æ–™
- en: Course Introduction
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¯¾ç¨‹ä»‹ç»
- en: '***At the end of this 7 lessons course, you will know how to:***'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '***åœ¨è¿™ä¸ª 7 èŠ‚è¯¾ç¨‹ç»“æŸæ—¶ï¼Œä½ å°†çŸ¥é“å¦‚ä½•ï¼š***'
- en: design a batch-serving architecture
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®¾è®¡æ‰¹é‡æœåŠ¡æ¶æ„
- en: use Hopsworks as a feature store
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Hopsworks ä½œä¸ºç‰¹å¾å­˜å‚¨
- en: design a feature engineering pipeline that reads data from an API
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®¾è®¡ä¸€ä¸ªä» API è¯»å–æ•°æ®çš„ç‰¹å¾å·¥ç¨‹ç®¡é“
- en: build a training pipeline with hyper-parameter tunning
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ„å»ºä¸€ä¸ªåŒ…å«è¶…å‚æ•°è°ƒæ•´çš„è®­ç»ƒç®¡é“
- en: use W&B as an ML Platform to track your experiments, models, and metadata
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ W&B ä½œä¸º ML å¹³å°è·Ÿè¸ªä½ çš„å®éªŒã€æ¨¡å‹å’Œå…ƒæ•°æ®
- en: implement a batch prediction pipeline
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®ç°ä¸€ä¸ªæ‰¹é‡é¢„æµ‹ç®¡é“
- en: use Poetry to build your own Python packages
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Poetry æ„å»ºä½ è‡ªå·±çš„ Python åŒ…
- en: deploy your own private PyPi server
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: éƒ¨ç½²ä½ è‡ªå·±çš„ç§äºº PyPi æœåŠ¡å™¨
- en: orchestrate everything with Airflow
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Airflow åè°ƒä¸€åˆ‡
- en: use the predictions to code a web app using FastAPI and Streamlit
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨é¢„æµ‹æ¥ç¼–å†™ä¸€ä¸ªä½¿ç”¨ FastAPI å’Œ Streamlit çš„ web åº”ç”¨ç¨‹åº
- en: use Docker to containerize your code
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Docker å¯¹ä»£ç è¿›è¡Œå®¹å™¨åŒ–
- en: use Great Expectations to ensure data validation and integrity
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Great Expectations ç¡®ä¿æ•°æ®çš„éªŒè¯å’Œå®Œæ•´æ€§
- en: monitor the performance of the predictions over time
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç›‘æ§é¢„æµ‹æ€§èƒ½çš„å˜åŒ–æƒ…å†µ
- en: deploy everything to GCP
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰å†…å®¹éƒ¨ç½²åˆ° GCP
- en: build a CI/CD pipeline using GitHub Actions
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ GitHub Actions æ„å»º CI/CD ç®¡é“
- en: If that sounds like a lot, don't worry. After you cover this course, you will
    understand everything I said before. Most importantly, you will know WHY I used
    all these tools and how they work together as a system.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœè¿™å¬èµ·æ¥å¾ˆå¤šï¼Œä¸è¦æ‹…å¿ƒã€‚åœ¨ä½ å®Œæˆè¿™é—¨è¯¾ç¨‹åï¼Œä½ å°†ç†è§£æˆ‘ä¹‹å‰è¯´çš„æ‰€æœ‰å†…å®¹ã€‚æœ€é‡è¦çš„æ˜¯ï¼Œä½ å°†çŸ¥é“æˆ‘ä¸ºä»€ä¹ˆä½¿ç”¨è¿™äº›å·¥å…·ï¼Œä»¥åŠå®ƒä»¬å¦‚ä½•ä½œä¸ºä¸€ä¸ªç³»ç»ŸååŒå·¥ä½œã€‚
- en: '**If you want to get the most out of this course,** [**I suggest you access
    the GitHub repository**](https://github.com/iusztinpaul/energy-forecasting) **containing
    all the lessons'' code. This course is designed to read and replicate the code
    along the articles quickly.**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¦‚æœä½ æƒ³ä»è¿™é—¨è¯¾ç¨‹ä¸­è·å¾—æœ€å¤§çš„æ”¶ç›Šï¼Œ** [**æˆ‘å»ºè®®ä½ è®¿é—®åŒ…å«æ‰€æœ‰è¯¾ç¨‹ä»£ç çš„ GitHub ä»“åº“**](https://github.com/iusztinpaul/energy-forecasting)
    **ã€‚è¿™é—¨è¯¾ç¨‹æ—¨åœ¨è®©ä½ å¿«é€Ÿé˜…è¯»å’Œå¤åˆ¶æ–‡ç« ä¸­çš„ä»£ç ã€‚**'
- en: By the end of the course, you will know how to implement the diagram below.
    Don't worry if something doesn't make sense to you. I will explain everything
    in detail.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°è¯¾ç¨‹ç»“æŸæ—¶ï¼Œä½ å°†çŸ¥é“å¦‚ä½•å®ç°ä¸‹å›¾æ‰€ç¤ºçš„å†…å®¹ã€‚å¦‚æœæœ‰ä»€ä¹ˆä¸æ˜ç™½çš„åœ°æ–¹ï¼Œè¯·ä¸è¦æ‹…å¿ƒã€‚æˆ‘ä¼šè¯¦ç»†è§£é‡Šä¸€åˆ‡ã€‚
- en: '![](../Images/4b5c3b0b8e2162ea8fd268ca745199ec.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b5c3b0b8e2162ea8fd268ca745199ec.png)'
- en: Diagram of the architecture you will build during the course [Image by the Author].
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¾ç¨‹ä¸­ä½ å°†æ„å»ºçš„æ¶æ„å›¾ [ä½œè€…æä¾›çš„å›¾ç‰‡]ã€‚
- en: By the **end of Lesson 4**, you will know how to host your PiPy repository and
    orchestrate the three pipelines using Airflow. You will learn how to schedule
    the pipelines to create hourly forecasts.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°**ç¬¬ 4 èŠ‚è¯¾ç»“æŸæ—¶**ï¼Œä½ å°†çŸ¥é“å¦‚ä½•æ‰˜ç®¡ä½ çš„ PyPi ä»“åº“ï¼Œå¹¶ä½¿ç”¨ Airflow åè°ƒä¸‰ä¸ªç®¡é“ã€‚ä½ å°†å­¦ä¹ å¦‚ä½•è°ƒåº¦ç®¡é“ä»¥åˆ›å»ºæ¯å°æ—¶çš„é¢„æµ‹ã€‚
- en: 'Course Lessons:'
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¯¾ç¨‹å†…å®¹ï¼š
- en: '[Batch Serving. Feature Stores. Feature Engineering Pipelines.](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[æ‰¹é‡æœåŠ¡ã€‚ç‰¹å¾å­˜å‚¨ã€‚ç‰¹å¾å·¥ç¨‹ç®¡é“ã€‚](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)'
- en: '[Training Pipelines. ML Platforms. Hyperparameter Tuning.](https://medium.com/towards-data-science/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee)'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[è®­ç»ƒç®¡é“ã€‚ML å¹³å°ã€‚è¶…å‚æ•°è°ƒæ•´ã€‚](https://medium.com/towards-data-science/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee)'
- en: '[Batch Prediction Pipeline. Package Python Modules with Poetry.](https://medium.com/towards-data-science/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489)'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[æ‰¹é‡é¢„æµ‹ç®¡é“ã€‚ä½¿ç”¨ Poetry æ‰“åŒ… Python æ¨¡å—ã€‚](https://medium.com/towards-data-science/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489)'
- en: '**Private PyPi Server. Orchestrate Everything with Airflow.**'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**ç§æœ‰ PyPi æœåŠ¡å™¨ã€‚ä½¿ç”¨ Airflow åè°ƒä¸€åˆ‡ã€‚**'
- en: '[Data Validation for Quality and Integrity using GE. Model Performance Continuous
    Monitoring.](/ensuring-trustworthy-ml-systems-with-data-validation-and-real-time-monitoring-89ab079f4360)'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ä½¿ç”¨ GE è¿›è¡Œæ•°æ®éªŒè¯ä»¥ç¡®ä¿è´¨é‡å’Œå®Œæ•´æ€§ã€‚æ¨¡å‹æ€§èƒ½æŒç»­ç›‘æ§ã€‚](/ensuring-trustworthy-ml-systems-with-data-validation-and-real-time-monitoring-89ab079f4360)'
- en: '[Consume and Visualize your Modelâ€™s Predictions using FastAPI and Streamlit.
    Dockerize Everything.](https://medium.com/towards-data-science/fastapi-and-streamlit-the-python-duo-you-must-know-about-72825def1243)'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ä½¿ç”¨ FastAPI å’Œ Streamlit æ¶ˆè´¹å’Œå¯è§†åŒ–ä½ çš„æ¨¡å‹é¢„æµ‹ã€‚å¯¹ä¸€åˆ‡è¿›è¡Œ Docker åŒ–ã€‚](https://medium.com/towards-data-science/fastapi-and-streamlit-the-python-duo-you-must-know-about-72825def1243)'
- en: '[Deploy All the ML Components to GCP. Build a CI/CD Pipeline Using Github Actions.](https://medium.com/towards-data-science/seamless-ci-cd-pipelines-with-github-actions-on-gcp-your-tools-for-effective-mlops-96f676f72012)'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[å°†æ‰€æœ‰ ML ç»„ä»¶éƒ¨ç½²åˆ° GCPã€‚ä½¿ç”¨ Github Actions æ„å»º CI/CD ç®¡é“ã€‚](https://medium.com/towards-data-science/seamless-ci-cd-pipelines-with-github-actions-on-gcp-your-tools-for-effective-mlops-96f676f72012)'
- en: '[[Bonus] Behind the Scenes of an â€˜Imperfectâ€™ ML Project â€” Lessons and Insights](https://medium.com/towards-data-science/imperfections-unveiled-the-intriguing-reality-behind-our-mlops-course-creation-6ff7d52ecb7e)'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[[é¢å¤–å†…å®¹] â€˜ä¸å®Œç¾â€™ ML é¡¹ç›®çš„å¹•å â€” ç»éªŒæ•™è®­å’Œè§è§£](https://medium.com/towards-data-science/imperfections-unveiled-the-intriguing-reality-behind-our-mlops-course-creation-6ff7d52ecb7e)'
- en: 'If you want to grasp this lesson fully, we recommend you check out [Lesson
    1](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f),
    [Lesson 2](https://medium.com/towards-data-science/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee),
    and [Lesson 3](https://medium.com/towards-data-science/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489),
    which explain in detail the implementation of the pipelines that you will orchestrate
    in this article:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³å…¨é¢æŒæ¡è¿™èŠ‚è¯¾ï¼Œæˆ‘ä»¬å»ºè®®ä½ æŸ¥çœ‹[è¯¾ç¨‹ 1](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)ï¼Œ[è¯¾ç¨‹
    2](https://medium.com/towards-data-science/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee)
    å’Œ[è¯¾ç¨‹ 3](https://medium.com/towards-data-science/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489)ï¼Œè¿™äº›è¯¾ç¨‹è¯¦ç»†è§£é‡Šäº†ä½ å°†åœ¨æœ¬æ–‡ä¸­åè°ƒçš„ç®¡é“å®ç°ï¼š
- en: '[Feature Engineering Pipeline](/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ç‰¹å¾å·¥ç¨‹ç®¡é“](/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)'
- en: '[Training Pipeline](/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee)'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[è®­ç»ƒç®¡é“](/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee)'
- en: '[Batch Prediction Pipeline](/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489)'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[æ‰¹é‡é¢„æµ‹ç®¡é“](/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489)'
- en: Data Source
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ•°æ®æº
- en: We used a free & open API that provides hourly energy consumption values for
    all the energy consumer types within Denmark [1].
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªå…è´¹ä¸”å¼€æ”¾çš„ APIï¼Œè¯¥ API æä¾›äº†ä¸¹éº¦æ‰€æœ‰èƒ½æºæ¶ˆè´¹ç±»å‹çš„æ¯å°æ—¶èƒ½æºæ¶ˆè€—å€¼[1]ã€‚
- en: They provide an intuitive interface where you can easily query and visualize
    the data. [You can access the data here](https://www.energidataservice.dk/tso-electricity/ConsumptionDE35Hour)
    [1].
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä»¬æä¾›äº†ä¸€ä¸ªç›´è§‚çš„ç•Œé¢ï¼Œä½ å¯ä»¥è½»æ¾æŸ¥è¯¢å’Œå¯è§†åŒ–æ•°æ®ã€‚[ä½ å¯ä»¥åœ¨è¿™é‡Œè®¿é—®æ•°æ®](https://www.energidataservice.dk/tso-electricity/ConsumptionDE35Hour)
    [1]ã€‚
- en: 'The data has 4 main attributes:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®å…·æœ‰ 4 ä¸ªä¸»è¦å±æ€§ï¼š
- en: '**Hour UTC:** the UTC datetime when the data point was observed.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å°æ—¶ UTCï¼š** æ•°æ®ç‚¹è§‚å¯Ÿæ—¶çš„ UTC æ—¥æœŸæ—¶é—´ã€‚'
- en: '**Price Area:** Denmark is divided into two price areas: DK1 and DK2 â€” divided
    by the Great Belt. DK1 is west of the Great Belt, and DK2 is east of the Great
    Belt.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä»·æ ¼åŒºåŸŸï¼š** ä¸¹éº¦è¢«åˆ’åˆ†ä¸ºä¸¤ä¸ªä»·æ ¼åŒºåŸŸï¼šDK1 å’Œ DK2â€”â€”ç”±å¤§è´å°”ç‰¹æµ·å³¡åˆ’åˆ†ã€‚DK1 ä½äºå¤§è´å°”ç‰¹æµ·å³¡è¥¿ä¾§ï¼ŒDK2 ä½äºå¤§è´å°”ç‰¹æµ·å³¡ä¸œä¾§ã€‚'
- en: '**Consumer Type:** The consumer type is the Industry Code DE35, owned and maintained
    by Danish Energy.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ¶ˆè´¹ç±»å‹ï¼š** æ¶ˆè´¹ç±»å‹æ˜¯ç”±ä¸¹éº¦èƒ½æºå…¬å¸æ‹¥æœ‰å’Œç»´æŠ¤çš„è¡Œä¸šä»£ç  DE35ã€‚'
- en: '**Total Consumption:** Total electricity consumption in kWh'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ€»æ¶ˆè€—ï¼š** æ€»ç”µåŠ›æ¶ˆè€—ï¼ˆå•ä½ï¼šåƒç“¦æ—¶ï¼‰'
- en: '**Note:** The observations have a lag of 15 days! But for our demo use case,
    that is not a problem, as we can simulate the same steps as it would in real-time.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼š** è§‚å¯Ÿæ•°æ®æœ‰ 15 å¤©çš„æ»åï¼ä½†å¯¹äºæˆ‘ä»¬çš„æ¼”ç¤ºç”¨ä¾‹æ¥è¯´ï¼Œè¿™ä¸æ˜¯é—®é¢˜ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥æ¨¡æ‹Ÿä¸å®æ—¶ç›¸åŒçš„æ­¥éª¤ã€‚'
- en: '![](../Images/e0bc098121320b6b981889d8d712952d.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e0bc098121320b6b981889d8d712952d.png)'
- en: A screenshot from our web app showing how we forecasted the energy consumption
    for area = 1 and consumer_type = 212 [Image by the Author].
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ç½‘ç»œåº”ç”¨ç¨‹åºçš„å±å¹•æˆªå›¾ï¼Œå±•ç¤ºäº†æˆ‘ä»¬å¦‚ä½•é¢„æµ‹åŒºåŸŸ = 1 å’Œæ¶ˆè´¹ç±»å‹ = 212 çš„èƒ½æºæ¶ˆè€— [ä½œè€…æä¾›çš„å›¾ç‰‡]ã€‚
- en: 'The data points have an hourly resolution. For example: "2023â€“04â€“15 21:00Z",
    "2023â€“04â€“15 20:00Z", "2023â€“04â€“15 19:00Z", etc.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®ç‚¹å…·æœ‰æ¯å°æ—¶çš„åˆ†è¾¨ç‡ã€‚ä¾‹å¦‚ï¼šâ€œ2023â€“04â€“15 21:00Zâ€ï¼Œâ€œ2023â€“04â€“15 20:00Zâ€ï¼Œâ€œ2023â€“04â€“15 19:00Zâ€ç­‰ã€‚
- en: We will model the data as multiple time series. Each unique **price area** and
    **consumer type tuple represents its** unique time series.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æ•°æ®å»ºæ¨¡ä¸ºå¤šä¸ªæ—¶é—´åºåˆ—ã€‚æ¯ä¸ªå”¯ä¸€çš„**ä»·æ ¼åŒºåŸŸ**å’Œ**æ¶ˆè´¹ç±»å‹**ç»„åˆè¡¨ç¤ºä¸€ä¸ªå”¯ä¸€çš„æ—¶é—´åºåˆ—ã€‚
- en: Thus, we will build a model that independently forecasts the energy consumption
    for the next 24 hours for every time series.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬å°†å»ºç«‹ä¸€ä¸ªç‹¬ç«‹é¢„æµ‹æœªæ¥ 24 å°æ—¶æ¯ä¸ªæ—¶é—´åºåˆ—çš„èƒ½æºæ¶ˆè€—çš„æ¨¡å‹ã€‚
- en: '*Check out the video below to better understand what the data looks like* ğŸ‘‡'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '*æŸ¥çœ‹ä¸‹é¢çš„è§†é¢‘ï¼Œä»¥æ›´å¥½åœ°ç†è§£æ•°æ®çš„æ ·å­* ğŸ‘‡'
- en: Course & data source overview [Video by the Author].
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¾ç¨‹ä¸æ•°æ®æºæ¦‚è¿° [ä½œè€…æä¾›çš„è§†é¢‘]ã€‚
- en: 'Lesson 4: **Private PyPi Server. Orchestrate Everything with Airflow.**'
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¯¾ç¨‹ 4ï¼š**ç§äºº PyPi æœåŠ¡å™¨ã€‚ç”¨ Airflow åè°ƒä¸€åˆ‡ã€‚**
- en: The goal of Lesson 4
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬ 4 èŠ‚è¯¾çš„ç›®æ ‡
- en: This lesson will teach you how to use Airflow to orchestrate the three pipelines
    you have implemented so far.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™èŠ‚è¯¾å°†æ•™ä½ å¦‚ä½•ä½¿ç”¨ Airflow æ¥åè°ƒä½ è¿„ä»Šä¸ºæ­¢å®ç°çš„ä¸‰ä¸ªç®¡é“ã€‚
- en: Also, to run the code inside Airflow, you will learn to host your PiPy repository
    and deploy the pipelines as 3 different Python modules. Later you will install
    your modules inside Airflow directly from your PiPy repository.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œè¦è¿è¡ŒAirflowä¸­çš„ä»£ç ï¼Œä½ å°†å­¦ä¹ å¦‚ä½•æ‰˜ç®¡ä½ çš„PiPyä»“åº“ï¼Œå¹¶å°†ç®¡é“éƒ¨ç½²ä¸º3ä¸ªä¸åŒçš„Pythonæ¨¡å—ã€‚ä¹‹åï¼Œä½ å°†ç›´æ¥ä»ä½ çš„PiPyä»“åº“ä¸­å®‰è£…è¿™äº›æ¨¡å—åˆ°Airflowä¸­ã€‚
- en: '![](../Images/0867f47c56b09ac9cb2dddc9a885283f.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0867f47c56b09ac9cb2dddc9a885283f.png)'
- en: Diagram of the final architecture with the Lesson 4 components highlighted in
    blue [Image by the Author].
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬4è¯¾ç»„ä»¶ç”¨è“è‰²é«˜äº®çš„æœ€ç»ˆæ¶æ„å›¾ [ä½œè€…æä¾›çš„å›¾ç‰‡]ã€‚
- en: By orchestrating everything using Airflow, you will automate your entire process.
    Instead of running manually 10 different scripts, you will hit once a "Run" button
    to run the whole code.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ä½¿ç”¨Airflowç¼–æ’æ‰€æœ‰å†…å®¹ï¼Œä½ å°†è‡ªåŠ¨åŒ–æ•´ä¸ªè¿‡ç¨‹ã€‚ä½ ä¸å†éœ€è¦æ‰‹åŠ¨è¿è¡Œ10ä¸ªä¸åŒçš„è„šæœ¬ï¼Œè€Œåªéœ€ç‚¹å‡»ä¸€æ¬¡â€œè¿è¡Œâ€æŒ‰é’®å³å¯è¿è¡Œæ•´ä¸ªä»£ç ã€‚
- en: Also, connecting all the steps together in a programmatic way is less prone
    to bugs.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: åŒæ—¶ï¼Œä»¥ç¼–ç¨‹æ–¹å¼å°†æ‰€æœ‰æ­¥éª¤è¿æ¥èµ·æ¥çš„ç¨‹åºæ›´ä¸å®¹æ˜“å‡ºç°é”™è¯¯ã€‚
- en: '**Why?**'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä¸ºä»€ä¹ˆï¼Ÿ**'
- en: Because every script needs its configurations. For example, the batch prediction
    pipeline needs the feature view version (data version) and the model version as
    input.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºæ¯ä¸ªè„šæœ¬éƒ½éœ€è¦è‡ªå·±çš„é…ç½®ã€‚ä¾‹å¦‚ï¼Œæ‰¹é‡é¢„æµ‹ç®¡é“éœ€è¦ç‰¹å¾è§†å›¾ç‰ˆæœ¬ï¼ˆæ•°æ®ç‰ˆæœ¬ï¼‰å’Œæ¨¡å‹ç‰ˆæœ¬ä½œä¸ºè¾“å…¥ã€‚
- en: This information is generated as metadata from previous scripts. When you run
    everything manually, you can easily copy the wrong version. But when you wrap
    up everything inside a single DAG, you have to build it once, and afterward, it
    will work all the time.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ä¿¡æ¯æ˜¯ä»ä¹‹å‰çš„è„šæœ¬ç”Ÿæˆçš„å…ƒæ•°æ®ã€‚å½“ä½ æ‰‹åŠ¨è¿è¡Œä¸€åˆ‡æ—¶ï¼Œå®¹æ˜“å¤åˆ¶é”™è¯¯çš„ç‰ˆæœ¬ã€‚ä½†å½“ä½ å°†æ‰€æœ‰å†…å®¹å°è£…åœ¨ä¸€ä¸ªDAGä¸­æ—¶ï¼Œä½ åªéœ€æ„å»ºä¸€æ¬¡ï¼Œä¹‹åå®ƒå°†å§‹ç»ˆæ­£å¸¸å·¥ä½œã€‚
- en: 'Also, by using Airflow, you can:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œé€šè¿‡ä½¿ç”¨Airflowï¼Œä½ å¯ä»¥ï¼š
- en: schedule the pipeline to run periodically (you will run it hourly);
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®šæœŸè°ƒåº¦ç®¡é“è¿è¡Œï¼ˆä½ å°†æ¯å°æ—¶è¿è¡Œä¸€æ¬¡ï¼‰ï¼›
- en: configure your entire process using Airflow Variables;
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Airflowå˜é‡é…ç½®æ•´ä¸ªè¿‡ç¨‹ï¼›
- en: monitor the logs of every task.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç›‘æ§æ¯ä¸ªä»»åŠ¡çš„æ—¥å¿—ã€‚
- en: Here is an overview of what you will build in Airflow ğŸ‘‡
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ä½ å°†åœ¨Airflowä¸­æ„å»ºçš„æ¦‚è§ˆ ğŸ‘‡
- en: Theoretical Concepts & Tools
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç†è®ºæ¦‚å¿µä¸å·¥å…·
- en: '**Airflow:** Airflow is oneof the most popular orchestration tools out there.
    The project was developed at Airbnb but is now open source under the Apache License.
    That means that you can modify and host it yourself for free. Airflow lets you
    build, schedule and monitor DAGs.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**Airflowï¼š** Airflowæ˜¯æœ€å—æ¬¢è¿çš„ç¼–æ’å·¥å…·ä¹‹ä¸€ã€‚è¿™ä¸ªé¡¹ç›®æœ€åˆåœ¨Airbnbå¼€å‘ï¼Œä½†ç°åœ¨åœ¨Apacheè®¸å¯è¯ä¸‹å¼€æºã€‚è¿™æ„å‘³ç€ä½ å¯ä»¥å…è´¹ä¿®æ”¹å’Œæ‰˜ç®¡å®ƒã€‚Airflowå…è®¸ä½ æ„å»ºã€è°ƒåº¦å’Œç›‘æ§DAGã€‚'
- en: '**DAG (Directed Acyclic Graph):** A DAG is a graph with no loops, meaning the
    logic flow can go only one way.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**DAGï¼ˆæœ‰å‘æ— ç¯å›¾ï¼‰ï¼š** DAGæ˜¯ä¸€ç§æ²¡æœ‰å¾ªç¯çš„å›¾ï¼Œè¿™æ„å‘³ç€é€»è¾‘æµåªèƒ½æœä¸€ä¸ªæ–¹å‘è¿›è¡Œã€‚'
- en: '**PyPi Registry:** A PiPy registry is a server where you can host various Python
    modules. When you run "**pip install <your_package>**", pip knows how to look
    at the official PyPi repository for your package and install it. Hosting your
    own PyPi registry will behave precisely the same, but you must configure pip to
    know how to access it. Only people with access to your PyPi server can install
    packages from it.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**PyPiæ³¨å†Œè¡¨ï¼š** PiPyæ³¨å†Œè¡¨æ˜¯ä¸€ä¸ªå¯ä»¥æ‰˜ç®¡å„ç§Pythonæ¨¡å—çš„æœåŠ¡å™¨ã€‚å½“ä½ è¿è¡Œâ€œ**pip install <your_package>**â€æ—¶ï¼ŒpipçŸ¥é“å¦‚ä½•æŸ¥çœ‹å®˜æ–¹PyPiä»“åº“ä¸­çš„ä½ çš„åŒ…å¹¶å®‰è£…å®ƒã€‚æ‰˜ç®¡è‡ªå·±çš„PyPiæ³¨å†Œè¡¨çš„è¡Œä¸ºå®Œå…¨ç›¸åŒï¼Œä½†ä½ å¿…é¡»é…ç½®pipä»¥çŸ¥é“å¦‚ä½•è®¿é—®å®ƒã€‚åªæœ‰è®¿é—®ä½ PyPiæœåŠ¡å™¨çš„äººæ‰èƒ½ä»ä¸­å®‰è£…åŒ…ã€‚'
- en: 'Lesson 4: Code'
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬4è¯¾ï¼šä»£ç 
- en: '[You can access the GitHub repository here.](https://github.com/iusztinpaul/energy-forecasting)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[ä½ å¯ä»¥åœ¨è¿™é‡Œè®¿é—®GitHubä»“åº“ã€‚](https://github.com/iusztinpaul/energy-forecasting)'
- en: '**Note:** All the installation instructions are in the READMEs of the repository.
    Here you will jump straight to the code.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼š** æ‰€æœ‰çš„å®‰è£…è¯´æ˜éƒ½åœ¨ä»“åº“çš„READMEæ–‡ä»¶ä¸­ã€‚è¿™é‡Œä½ å°†ç›´æ¥è·³è½¬åˆ°ä»£ç éƒ¨åˆ†ã€‚'
- en: '*All the code within Lesson 4 is located under the* [***airflow***](https://github.com/iusztinpaul/energy-forecasting/tree/main/airflow)
    *folder.*'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*ç¬¬4è¯¾ä¸­çš„æ‰€æœ‰ä»£ç éƒ½ä½äº* [***airflow***](https://github.com/iusztinpaul/energy-forecasting/tree/main/airflow)
    *æ–‡ä»¶å¤¹ä¸‹ã€‚*'
- en: 'The files under the [**airflow**](https://github.com/iusztinpaul/energy-forecasting/tree/main/airflow)
    folderare structured as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[**airflow**](https://github.com/iusztinpaul/energy-forecasting/tree/main/airflow)
    æ–‡ä»¶å¤¹ä¸‹çš„æ–‡ä»¶ç»“æ„å¦‚ä¸‹ï¼š'
- en: '![](../Images/626fcaf985c055b03a8e886cca969cbc.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/626fcaf985c055b03a8e886cca969cbc.png)'
- en: A screenshot that shows the structure of the airflow folder [Image by the Author].
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¾ç¤ºairflowæ–‡ä»¶å¤¹ç»“æ„çš„æˆªå›¾ [ä½œè€…æä¾›çš„å›¾ç‰‡]ã€‚
- en: All the code is located under the [**dags**](https://github.com/iusztinpaul/energy-forecasting/tree/main/airflow/dags)
    directory**.** Every DAG will have its own Python file.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰ä»£ç éƒ½ä½äº[**dags**](https://github.com/iusztinpaul/energy-forecasting/tree/main/airflow/dags)
    ç›®å½•ä¸‹**ã€‚** æ¯ä¸ªDAGéƒ½æœ‰å…¶è‡ªå·±çš„Pythonæ–‡ä»¶ã€‚
- en: The Docker files will help you quickly host Airflow and the PiPy repository.
    I will explain them in detail later.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Dockeræ–‡ä»¶å°†å¸®åŠ©ä½ å¿«é€Ÿæ‰˜ç®¡Airflowå’ŒPiPyä»“åº“ã€‚æˆ‘ä¼šåœ¨åé¢è¯¦ç»†è§£é‡Šã€‚
- en: Directly storing credentials in your git repository is a huge security risk.
    That is why you will inject sensitive information using a **.env** file.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ç›´æ¥å°†å‡­è¯å­˜å‚¨åœ¨ä½ çš„gitä»“åº“ä¸­æ˜¯ä¸€ä¸ªå·¨å¤§çš„å®‰å…¨é£é™©ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆä½ å°†ä½¿ç”¨**.env**æ–‡ä»¶æ¥æ³¨å…¥æ•æ„Ÿä¿¡æ¯ã€‚
- en: The **.env.default** is an example of all the variables you must configure.
    It is also helpful to store default values for attributes that are not sensitive
    (e.g., project name).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**.env.default**æ˜¯ä½ å¿…é¡»é…ç½®çš„æ‰€æœ‰å˜é‡çš„ç¤ºä¾‹ã€‚å®ƒä¹Ÿæœ‰åŠ©äºå­˜å‚¨ä¸æ•æ„Ÿçš„å±æ€§çš„é»˜è®¤å€¼ï¼ˆä¾‹å¦‚ï¼Œé¡¹ç›®åç§°ï¼‰ã€‚'
- en: '![](../Images/5e1d209fa47b2b18eea853fa1a9848b8.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e1d209fa47b2b18eea853fa1a9848b8.png)'
- en: A screenshot of the .env.default file [Image by the Author].
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: .env.defaultæ–‡ä»¶çš„æˆªå›¾ [ä½œè€…æä¾›çš„å›¾ç‰‡]ã€‚
- en: Prepare Credentials
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‡†å¤‡å‡­è¯
- en: As Lesson 4 talks about orchestrating the code from all the other lessons, if
    you want to reproduce the code, you need to check how to set up the 3 pipelines
    from [Lesson 1](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f),
    [Lesson 2](https://medium.com/towards-data-science/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee),
    and [Lesson 3](https://medium.com/towards-data-science/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºç¬¬4èŠ‚è¯¾è®¨è®ºäº†å¦‚ä½•åè°ƒå…¶ä»–æ‰€æœ‰è¯¾ç¨‹ä¸­çš„ä»£ç ï¼Œå¦‚æœä½ æƒ³é‡ç°ä»£ç ï¼Œä½ éœ€è¦æ£€æŸ¥å¦‚ä½•è®¾ç½®[ç¬¬1èŠ‚](https://medium.com/towards-data-science/a-framework-for-building-a-production-ready-feature-engineering-pipeline-f0b29609b20f)ã€[ç¬¬2èŠ‚](https://medium.com/towards-data-science/a-guide-to-building-effective-training-pipelines-for-maximum-results-6fdaef594cee)å’Œ[ç¬¬3èŠ‚](https://medium.com/towards-data-science/unlock-the-secret-to-efficient-batch-prediction-pipelines-using-python-a-feature-store-and-gcs-17a1462ca489)ä¸­çš„3ä¸ªç®¡é“ã€‚
- en: These three lessons will show you how to set up all the necessary tools and
    services. Also, it will show you how to create and complete the required .env
    file that contains all the credentials.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸‰èŠ‚è¯¾å°†å±•ç¤ºå¦‚ä½•è®¾ç½®æ‰€æœ‰å¿…è¦çš„å·¥å…·å’ŒæœåŠ¡ã€‚è¿˜ä¼šå±•ç¤ºå¦‚ä½•åˆ›å»ºå¹¶å®ŒæˆåŒ…å«æ‰€æœ‰å‡­è¯çš„æ‰€éœ€.envæ–‡ä»¶ã€‚
- en: '![](../Images/5e1d209fa47b2b18eea853fa1a9848b8.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e1d209fa47b2b18eea853fa1a9848b8.png)'
- en: A screenshot of the .env.default file [Image by the Author].
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: .env.defaultæ–‡ä»¶çš„æˆªå›¾ [ä½œè€…æä¾›çš„å›¾ç‰‡]ã€‚
- en: '*The only thing to be careful of is* ğŸ‘‡'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*å”¯ä¸€éœ€è¦æ³¨æ„çš„æ˜¯* ğŸ‘‡'
- en: This time you have to place the **.env** that contains your credentials under
    the [**airflow/dags**](https://github.com/iusztinpaul/energy-forecasting/tree/main/airflow/dags)folder**.**
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ¬¡ä½ éœ€è¦å°†åŒ…å«å‡­è¯çš„**.env**æ–‡ä»¶æ”¾ç½®åœ¨[**airflow/dags**](https://github.com/iusztinpaul/energy-forecasting/tree/main/airflow/dags)æ–‡ä»¶å¤¹ä¸‹**ã€‚**
- en: We have set up a default value of **/opt/airflow/dags** for the **ML_PIPELINE_ROOT_DIR**
    environment variable inside the docker-compose.yaml file. Thus, when running the
    pipelines inside Airflow, it will know to load the **.env** file from **/opt/airflow/dags**
    by default.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨docker-compose.yamlæ–‡ä»¶ä¸­ä¸º**ML_PIPELINE_ROOT_DIR**ç¯å¢ƒå˜é‡è®¾ç½®äº†é»˜è®¤å€¼**/opt/airflow/dags**ã€‚å› æ­¤ï¼Œåœ¨Airflowå†…éƒ¨è¿è¡Œç®¡é“æ—¶ï¼Œå®ƒå°†é»˜è®¤ä»**/opt/airflow/dags**åŠ è½½**.env**æ–‡ä»¶ã€‚
- en: Also, note that there is another **.env** file under the /[**airflow**](https://github.com/iusztinpaul/energy-forecasting/tree/main/airflow)folder.
    This one doesn't contain your custom credentials, but Airflow needs some custom
    configurations. This is what it looks like ğŸ‘‡
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: åŒæ—¶ï¼Œè¯·æ³¨æ„åœ¨/ [**airflow**](https://github.com/iusztinpaul/energy-forecasting/tree/main/airflow)æ–‡ä»¶å¤¹ä¸‹è¿˜æœ‰å¦ä¸€ä¸ª**.env**æ–‡ä»¶ã€‚è¿™ä¸ªæ–‡ä»¶ä¸åŒ…å«ä½ çš„è‡ªå®šä¹‰å‡­è¯ï¼Œä½†Airflowéœ€è¦ä¸€äº›è‡ªå®šä¹‰é…ç½®ã€‚å®ƒçš„æ ·å­å¦‚ä¸‹
    ğŸ‘‡
- en: '![](../Images/262b1e1f5c8889636694e184cec858b4.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/262b1e1f5c8889636694e184cec858b4.png)'
- en: A screenshot of the .env file from the /airflow folder [Image by the Author].
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: /airflowæ–‡ä»¶å¤¹ä¸­çš„.envæ–‡ä»¶æˆªå›¾ [ä½œè€…æä¾›çš„å›¾ç‰‡]ã€‚
- en: I explained how to complete this **.env** file in the [README.md](https://github.com/iusztinpaul/energy-forecasting/tree/main#usage)
    of the repository. But as a side note, **AIRFLOW_UID** represents your computer's
    USER ID, and you know what **ML_PIPELINE_ROOT_DIR** is.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨ä»“åº“çš„[README.md](https://github.com/iusztinpaul/energy-forecasting/tree/main#usage)ä¸­è§£é‡Šäº†å¦‚ä½•å®Œæˆè¿™ä¸ª**.env**æ–‡ä»¶ã€‚ä½†ä½œä¸ºé™„æ³¨ï¼Œ**AIRFLOW_UID**ä»£è¡¨ä½ è®¡ç®—æœºçš„ç”¨æˆ·IDï¼Œè€Œä½ çŸ¥é“**ML_PIPELINE_ROOT_DIR**æ˜¯ä»€ä¹ˆã€‚
- en: I just wanted to show you that you can override the default value for **ML_PIPELINE_ROOT_DIR**
    here. Note that this path will be used inside the Docker container, hence the
    path that starts with **/opt/**.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åªæ˜¯æƒ³å‘ä½ å±•ç¤ºä½ å¯ä»¥åœ¨è¿™é‡Œè¦†ç›–**ML_PIPELINE_ROOT_DIR**çš„é»˜è®¤å€¼ã€‚è¯·æ³¨æ„ï¼Œè¿™ä¸ªè·¯å¾„å°†åœ¨Dockerå®¹å™¨å†…ä½¿ç”¨ï¼Œå› æ­¤è·¯å¾„ä»¥**/opt/**å¼€å¤´ã€‚
- en: '[PRE0]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Setup Private PyPi Server
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è®¾ç½®ç§äººPyPiæœåŠ¡å™¨
- en: You can easily host a PiPy server using [this repository](https://github.com/pypiserver/pypiserver).
    But let me explain how we did it in our setup.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥ä½¿ç”¨ [è¿™ä¸ªä»“åº“](https://github.com/pypiserver/pypiserver) å®¹æ˜“åœ°æ‰˜ç®¡ä¸€ä¸ª PiPy æœåŠ¡å™¨ã€‚ä¸è¿‡è®©æˆ‘è§£é‡Šä¸€ä¸‹æˆ‘ä»¬åœ¨è®¾ç½®ä¸­æ˜¯å¦‚ä½•åšçš„ã€‚
- en: The first step is to create a set of credentials that you will need to connect
    to the PyPi server.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ­¥æ˜¯åˆ›å»ºä¸€ç»„å‡­è¯ï¼Œè¿™äº›å‡­è¯æ˜¯ä½ è¿æ¥åˆ° PyPi æœåŠ¡å™¨æ‰€éœ€çš„ã€‚
- en: '[PRE1]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The PyPi repository will know to load the credentials from the **~/.htpasswd/htpasswd.txt**
    file.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: PyPi ä»“åº“å°†çŸ¥é“ä» **~/.htpasswd/htpasswd.txt** æ–‡ä»¶ä¸­åŠ è½½å‡­è¯ã€‚
- en: 'Now, you will add the new private PyPi repository to Poetry. To configure Poetry,
    you need to specify the URL of the server, the name of the server and the username
    & password to use to authenticate (which are the ones you configured one step
    before):'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œä½ å°†æŠŠæ–°çš„ç§æœ‰ PyPi ä»“åº“æ·»åŠ åˆ° Poetry ä¸­ã€‚è¦é…ç½® Poetryï¼Œä½ éœ€è¦æŒ‡å®šæœåŠ¡å™¨çš„ URLã€æœåŠ¡å™¨åç§°ä»¥åŠç”¨äºè®¤è¯çš„ç”¨æˆ·åå’Œå¯†ç ï¼ˆè¿™äº›æ˜¯ä½ ä¹‹å‰é…ç½®çš„ï¼‰ï¼š
- en: '[PRE2]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In our example:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ï¼š
- en: '**name of the server:** my-pypy'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æœåŠ¡å™¨åç§°ï¼š** my-pypy'
- en: '**URL:** [http://localhost](http://localhost)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**URLï¼š** [http://localhost](http://localhost)'
- en: '**username:** energy-forecasting'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç”¨æˆ·åï¼š** energy-forecasting'
- en: '**password:** <password>'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¯†ç ï¼š** <password>'
- en: 'Check if your credentials are set correctly in your Poetry **auth.toml** file:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: æ£€æŸ¥ä½ çš„ Poetry **auth.toml** æ–‡ä»¶ä¸­çš„å‡­è¯è®¾ç½®æ˜¯å¦æ­£ç¡®ï¼š
- en: '[PRE3]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: So, you finished preparing the username and password that will be loaded by
    your PyPi repository to authenticate. Also, you configured Poetry to be aware
    of your PyPi server.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å·²ç»å‡†å¤‡å¥½äº†ç”¨æˆ·åå’Œå¯†ç ï¼Œè¿™äº›å°†è¢«ä½ çš„ PyPi ä»“åº“ç”¨äºè®¤è¯ã€‚åŒæ—¶ï¼Œä½ ä¹Ÿé…ç½®äº† Poetry ä»¥è¯†åˆ«ä½ çš„ PyPi æœåŠ¡å™¨ã€‚
- en: Now, let's see how to run the PyPi server.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•è¿è¡Œ PyPi æœåŠ¡å™¨ã€‚
- en: The [pyserver code](https://github.com/pypiserver/pypiserver) you will be using
    is already dockerized.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å°†ä½¿ç”¨çš„ [pyserver ä»£ç ](https://github.com/pypiserver/pypiserver) å·²ç»è¿›è¡Œäº† Docker åŒ–ã€‚
- en: To simplify things, we added the PyPi server as an additional service to the
    docker-compose.yaml file that runs the Airflow application.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬å°† PyPi æœåŠ¡å™¨ä½œä¸ºé¢å¤–çš„æœåŠ¡æ·»åŠ åˆ°è¿è¡Œ Airflow åº”ç”¨ç¨‹åºçš„ docker-compose.yaml æ–‡ä»¶ä¸­ã€‚
- en: To better understand the docker-compose.yaml file check [Airflow's official
    documentation](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html)
    [2] and [our README.md](https://github.com/iusztinpaul/energy-forecasting/tree/main#the-pipeline).
    But be careful to use the docker-compose.yaml file from our repository as we modified
    the original one, as you will see below.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´å¥½åœ°ç†è§£ docker-compose.yaml æ–‡ä»¶ï¼Œè¯·æŸ¥çœ‹ [Airflow å®˜æ–¹æ–‡æ¡£](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html)
    [2] å’Œ [æˆ‘ä»¬çš„ README.md](https://github.com/iusztinpaul/energy-forecasting/tree/main#the-pipeline)ã€‚ä½†è¯·æ³¨æ„ä½¿ç”¨æˆ‘ä»¬ä»“åº“ä¸­çš„
    docker-compose.yaml æ–‡ä»¶ï¼Œå› ä¸ºæˆ‘ä»¬ä¿®æ”¹äº†åŸå§‹æ–‡ä»¶ï¼Œæ­£å¦‚ä¸‹æ–‡æ‰€ç¤ºã€‚
- en: 'Scroll at the bottom of the [**airflow/docker-compose.yaml**](https://github.com/iusztinpaul/energy-forecasting/blob/main/airflow/docker-compose.yaml)
    file, and you will see:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: æ»šåŠ¨åˆ° [**airflow/docker-compose.yaml**](https://github.com/iusztinpaul/energy-forecasting/blob/main/airflow/docker-compose.yaml)
    æ–‡ä»¶çš„åº•éƒ¨ï¼Œä½ ä¼šçœ‹åˆ°ï¼š
- en: '[PRE4]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This code uses the PyPi server''slatest image, exposes the server under the
    **80 port**, loads the **~/.htpasswd** folder that contains your credentials as
    a volume and runs the server with the following command:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ®µä»£ç ä½¿ç”¨äº† PyPi æœåŠ¡å™¨çš„æœ€æ–°é•œåƒï¼Œå°†æœåŠ¡å™¨æš´éœ²åœ¨ **80 ç«¯å£**ï¼Œå°†åŒ…å«ä½ å‡­è¯çš„ **~/.htpasswd** æ–‡ä»¶å¤¹ä½œä¸ºå·åŠ è½½ï¼Œå¹¶ç”¨ä»¥ä¸‹å‘½ä»¤è¿è¡ŒæœåŠ¡å™¨ï¼š
- en: '[PRE5]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '"-P .htpasswd/htpasswd.txt" explicitly tells the server what credentials to
    use.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"-P .htpasswd/htpasswd.txt" æ˜ç¡®å‘Šè¯‰æœåŠ¡å™¨ä½¿ç”¨å“ªäº›å‡­è¯ã€‚'
- en: '"â€” overwrite" states that if a new module with the same version is deployed,
    it will overwrite the last one.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"â€” overwrite" è¡¨ç¤ºå¦‚æœéƒ¨ç½²äº†ç›¸åŒç‰ˆæœ¬çš„æ–°æ¨¡å—ï¼Œå®ƒå°†è¦†ç›–ä¸Šä¸€ä¸ªç‰ˆæœ¬ã€‚'
- en: Thats it! When you run the Airflow application, you automatically start the
    PyPi server.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: å°±è¿™æ ·ï¼å½“ä½ è¿è¡Œ Airflow åº”ç”¨ç¨‹åºæ—¶ï¼Œä½ ä¼šè‡ªåŠ¨å¯åŠ¨ PyPi æœåŠ¡å™¨ã€‚
- en: '**Note:** In a production environment, you will likely host the PyPi server
    on a different server than Airflow. The steps are identical except for adding
    everything in a single docker-compose.yaml file. In this tutorial, we wanted to
    make everything easy to run.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼š** åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œä½ å¯èƒ½ä¼šå°† PyPi æœåŠ¡å™¨æ‰˜ç®¡åœ¨ä¸ Airflow ä¸åŒçš„æœåŠ¡å™¨ä¸Šã€‚æ­¥éª¤æ˜¯ç›¸åŒçš„ï¼Œåªæ˜¯å°†æ‰€æœ‰å†…å®¹æ·»åŠ åˆ°å•ä¸ª docker-compose.yaml
    æ–‡ä»¶ä¸­ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›ä¸€åˆ‡è¿è¡Œèµ·æ¥æ›´åŠ ç®€å•ã€‚'
- en: Customize Airflow Docker File
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è‡ªå®šä¹‰ Airflow Docker æ–‡ä»¶
- en: Because you have to run all the code in Python 3.9, you have to inherit the
    default **apache/airflow:2.5.2** Airflow Docker image and add some extra dependencies.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºä½ å¿…é¡»åœ¨ Python 3.9 ä¸­è¿è¡Œæ‰€æœ‰ä»£ç ï¼Œå› æ­¤ä½ éœ€è¦ç»§æ‰¿é»˜è®¤çš„ **apache/airflow:2.5.2** Airflow Docker
    é•œåƒï¼Œå¹¶æ·»åŠ ä¸€äº›é¢å¤–çš„ä¾èµ–é¡¹ã€‚
- en: 'This is what is going on in the Docker file below:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ Docker æ–‡ä»¶ä¸­çš„å†…å®¹ï¼š
- en: inherit **apache/airflow:2.5.2**
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç»§æ‰¿ **apache/airflow:2.5.2**
- en: switch to the root user to install system dependencies
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ‡æ¢åˆ° root ç”¨æˆ·ä»¥å®‰è£…ç³»ç»Ÿä¾èµ–é¡¹
- en: install Python 3.9 dependencies needed to install packages from the private
    PyPi server
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®‰è£…ä»ç§æœ‰PyPiæœåŠ¡å™¨å®‰è£…åŒ…æ‰€éœ€çš„Python 3.9ä¾èµ–é¡¹
- en: switch back to the default user
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ‡æ¢å›é»˜è®¤ç”¨æˆ·
- en: 'Because we switched:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºæˆ‘ä»¬åˆ‡æ¢äº†ï¼š
- en: '[PRE6]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ï¼š
- en: '[PRE7]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Docker will know to use your custom image instead of **apache/airflow:2.5.2
    when running docker-compose**.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Dockerä¼šçŸ¥é“åœ¨è¿è¡Œdocker-composeæ—¶ä½¿ç”¨ä½ çš„è‡ªå®šä¹‰é•œåƒï¼Œè€Œä¸æ˜¯**apache/airflow:2.5.2**ã€‚
- en: Run Airflow
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¿è¡ŒAirflow
- en: 'Now that you understand how to prepare the credentials and how the Docker files
    work, go to the [./airflow](https://github.com/iusztinpaul/energy-forecasting/tree/main/airflow)
    directory and run:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ äº†è§£äº†å¦‚ä½•å‡†å¤‡å‡­æ®ä»¥åŠDockeræ–‡ä»¶å¦‚ä½•å·¥ä½œï¼Œå‰å¾€[./airflow](https://github.com/iusztinpaul/energy-forecasting/tree/main/airflow)ç›®å½•å¹¶è¿è¡Œï¼š
- en: '[PRE8]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[Check out the ***Usage*** *section* of the GitHub repository for more info.](https://github.com/iusztinpaul/energy-forecasting/tree/main#-usage-)'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[æŸ¥çœ‹***ä½¿ç”¨*** *éƒ¨åˆ†* è·å–æ›´å¤šä¿¡æ¯ã€‚](https://github.com/iusztinpaul/energy-forecasting/tree/main#-usage-)'
- en: 'After you finish your Airflow setup, you can access Airflow at **127.0.0.1:8080**
    with the default credentials:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: å®ŒæˆAirflowè®¾ç½®åï¼Œä½ å¯ä»¥ä½¿ç”¨é»˜è®¤å‡­æ®åœ¨**127.0.0.1:8080**è®¿é—®Airflowï¼š
- en: 'username: airflow'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'ç”¨æˆ·å: airflow'
- en: 'password: airflow'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'å¯†ç : airflow'
- en: '![](../Images/76a9df4ab40a4c9b99fb00b950b8b489.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/76a9df4ab40a4c9b99fb00b950b8b489.png)'
- en: Screenshot of the Airflow login page [Image by the Author].
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Airflowç™»å½•é¡µé¢çš„æˆªå›¾ [ä½œè€…æä¾›çš„å›¾ç‰‡]ã€‚
- en: Deploy Modules to Private PyPi Server
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å°†æ¨¡å—éƒ¨ç½²åˆ°ç§æœ‰PyPiæœåŠ¡å™¨
- en: 'Remember that you added to Poetry your new PyPi server using the following
    commands:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: è®°ä½ä½ ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å°†æ–°çš„PyPiæœåŠ¡å™¨æ·»åŠ åˆ°Poetryä¸­ï¼š
- en: '[PRE9]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now, using **my-pypi** as an identifier, you can quickly push new packages to
    your PyPi repository.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œä½¿ç”¨**my-pypi**ä½œä¸ºæ ‡è¯†ç¬¦ï¼Œä½ å¯ä»¥å¿«é€Ÿå°†æ–°åŒ…æ¨é€åˆ°ä½ çš„PyPiä»“åº“ã€‚
- en: 'Using the [deploy/ml-pipeline.sh](https://github.com/iusztinpaul/energy-forecasting/blob/main/deploy/ml-pipeline.sh)
    shell script, you can build & deploy all the 3 pipelines using solely Poetry:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨[deploy/ml-pipeline.sh](https://github.com/iusztinpaul/energy-forecasting/blob/main/deploy/ml-pipeline.sh)è„šæœ¬ï¼Œä½ å¯ä»¥ä»…ä½¿ç”¨Poetryæ„å»ºå’Œéƒ¨ç½²æ‰€æœ‰3ä¸ªç®¡é“ï¼š
- en: '[PRE10]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As you can see, we iteratively go to the folders of the 3 pipelines and run:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼Œæˆ‘ä»¬è¿­ä»£åœ°è¿›å…¥3ä¸ªç®¡é“çš„æ–‡ä»¶å¤¹å¹¶è¿è¡Œï¼š
- en: '[PRE11]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Poetry uses these two commands to look for the **pyproject.toml** and **poetry.lock**
    files inside the folders and knows how to build the package.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Poetryä½¿ç”¨è¿™ä¸¤ä¸ªå‘½ä»¤åœ¨æ–‡ä»¶å¤¹ä¸­æŸ¥æ‰¾**pyproject.toml**å’Œ**poetry.lock**æ–‡ä»¶ï¼Œå¹¶çŸ¥é“å¦‚ä½•æ„å»ºåŒ…ã€‚
- en: Afterward, based on the generated **wheel** file, running **"poetry publish
    -r my-pypi",** you push itto your **my-pipy** repository.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæ ¹æ®ç”Ÿæˆçš„**wheel**æ–‡ä»¶ï¼Œè¿è¡Œ**"poetry publish -r my-pypi"**ï¼Œä½ å°†å…¶æ¨é€åˆ°ä½ çš„**my-pipy**ä»“åº“ã€‚
- en: Remember that you labeled your PyPi server as **my-pipy**.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: è®°ä½ä½ å°†ä½ çš„PyPiæœåŠ¡å™¨æ ‡è®°ä¸º**my-pipy**ã€‚
- en: You are done. You have your own PyPi repository.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæˆäº†ã€‚ä½ æ‹¥æœ‰äº†è‡ªå·±çš„PyPiä»“åº“ã€‚
- en: In future sections, I will show you how to install packages from your private
    PyPi repository.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­ï¼Œæˆ‘å°†å‘ä½ å±•ç¤ºå¦‚ä½•ä»ä½ çš„ç§æœ‰PyPiä»“åº“ä¸­å®‰è£…åŒ…ã€‚
- en: '**Note:** You used Poetry just to build & deploy the modules. Airflow will
    use pip to install them from your PiPy repository.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼š** ä½ åªä½¿ç”¨Poetryæ¥æ„å»ºå’Œéƒ¨ç½²æ¨¡å—ã€‚Airflowå°†ä½¿ç”¨pipä»ä½ çš„PyPiä»“åº“ä¸­å®‰è£…è¿™äº›æ¨¡å—ã€‚'
- en: Define the DAG Object
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®šä¹‰DAGå¯¹è±¡
- en: Your dag is defined under the [**airflow/dags/ml_pipeline_dag.py**](https://github.com/iusztinpaul/energy-forecasting/blob/main/airflow/dags/ml_pipeline_dag.py)
    file.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ çš„dagå®šä¹‰åœ¨[**airflow/dags/ml_pipeline_dag.py**](https://github.com/iusztinpaul/energy-forecasting/blob/main/airflow/dags/ml_pipeline_dag.py)æ–‡ä»¶ä¸­ã€‚
- en: Using the ***API of Airflow 2.0***, you can define a DAG using the **dag()**
    Python decorator.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨***Airflow 2.0çš„API***ï¼Œä½ å¯ä»¥ä½¿ç”¨**dag()** Python è£…é¥°å™¨å®šä¹‰ä¸€ä¸ªDAGã€‚
- en: Your dag will be defined inside the **ml_pipeline()** function, which is called
    at the end of the file. Also, Airflow knows to load all the DAGs defined under
    the [airflow/dags](https://github.com/iusztinpaul/energy-forecasting/tree/main/airflow/dags)
    directory.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ çš„dagå°†åœ¨**ml_pipeline()**å‡½æ•°ä¸­å®šä¹‰ï¼Œè¯¥å‡½æ•°åœ¨æ–‡ä»¶æœ«å°¾è°ƒç”¨ã€‚æ­¤å¤–ï¼ŒAirflowçŸ¥é“åŠ è½½[airflow/dags](https://github.com/iusztinpaul/energy-forecasting/tree/main/airflow/dags)ç›®å½•ä¸‹å®šä¹‰çš„æ‰€æœ‰DAGã€‚
- en: 'The DAG has the following properties:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: DAGå…·æœ‰ä»¥ä¸‹å±æ€§ï¼š
- en: '**dag_id**: the ID of the DAG'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**dag_id**: DAGçš„ID'
- en: '**schedule:** itdefines how often the DAG runs'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**schedule:** å®ƒå®šä¹‰äº†DAGè¿è¡Œçš„é¢‘ç‡'
- en: '**start_date:** when should the DAG start running based on the given schedule'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**start_date:** æ ¹æ®ç»™å®šçš„æ—¶é—´è¡¨ï¼ŒDAGåº”è¯¥ä½•æ—¶å¼€å§‹è¿è¡Œ'
- en: '**catchup:** automatically backfill between [start_date, present]'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**catchup:** è‡ªåŠ¨å¡«è¡¥[start_date, ç°åœ¨]ä¹‹é—´çš„æ—¶é—´'
- en: '**tags:** tags ğŸ˜„'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**tags:** æ ‡ç­¾ ğŸ˜„'
- en: '**max_active_runs:** how many instances of this DAG can run in parallel'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**max_active_runs:** æ­¤DAGå¯ä»¥å¹¶è¡Œè¿è¡Œçš„å®ä¾‹æ•°'
- en: Define the Tasks
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®šä¹‰ä»»åŠ¡
- en: The code below might look long, but you can easily read it once you understand
    the main ideas.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢çš„ä»£ç å¯èƒ½çœ‹èµ·æ¥å¾ˆé•¿ï¼Œä½†ä¸€æ—¦ä½ ç†è§£äº†ä¸»è¦æ€è·¯ï¼Œå®ƒå¾ˆå®¹æ˜“é˜…è¯»ã€‚
- en: Inside a DAG, you have defined multiple tasks. A task is a single logic unit/step
    that performs a specific operation.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨DAGä¸­ï¼Œä½ å®šä¹‰äº†å¤šä¸ªä»»åŠ¡ã€‚ä¸€ä¸ªä»»åŠ¡æ˜¯ä¸€ä¸ªå•ç‹¬çš„é€»è¾‘å•å…ƒ/æ­¥éª¤ï¼Œæ‰§è¡Œç‰¹å®šçš„æ“ä½œã€‚
- en: 'The tasks are defined similarly to the DAG: a function + a decorator. Every
    task has its function and decorator.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ä»»åŠ¡çš„å®šä¹‰ç±»ä¼¼äºDAGï¼šä¸€ä¸ªå‡½æ•°+ä¸€ä¸ªè£…é¥°å™¨ã€‚æ¯ä¸ªä»»åŠ¡éƒ½æœ‰å…¶å‡½æ•°å’Œè£…é¥°å™¨ã€‚
- en: '***Note:*** *This a simple reminder that we used the API for Airflow 2.0, not
    1.0.*'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '***æ³¨æ„ï¼š*** *è¿™æ˜¯ä¸€ä¸ªç®€å•çš„æé†’ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯Airflow 2.0çš„APIï¼Œè€Œä¸æ˜¯1.0çš„ã€‚*'
- en: In our case, a task will represent a main pipeline script. For example, the
    feature engineering pipeline will run inside a single task.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œä¸€ä¸ªä»»åŠ¡å°†ä»£è¡¨ä¸€ä¸ªä¸»ç®¡é“è„šæœ¬ã€‚ä¾‹å¦‚ï¼Œç‰¹å¾å·¥ç¨‹ç®¡é“å°†åœ¨ä¸€ä¸ªå•ç‹¬çš„ä»»åŠ¡ä¸­è¿è¡Œã€‚
- en: You will use a DAG to glue all your scripts under a single "program", where
    every script has a 1:1 representation with a task.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å°†ä½¿ç”¨DAGå°†æ‰€æœ‰è„šæœ¬ç²˜åˆåœ¨ä¸€ä¸ªâ€œç¨‹åºâ€ä¸‹ï¼Œæ¯ä¸ªè„šæœ¬ä¸ä¸€ä¸ªä»»åŠ¡æœ‰1:1çš„å¯¹åº”å…³ç³»ã€‚
- en: '![](../Images/832932dc0905d7afa066c5026aa25207.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/832932dc0905d7afa066c5026aa25207.png)'
- en: Visual representation of the "ml_pipeline" (see the Youtube video for a better
    view) [Image by the Author].
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '"ml_pipeline"çš„è§†è§‰è¡¨ç¤ºï¼ˆè¯·å‚è§YouTubeè§†é¢‘ä»¥è·å–æ›´å¥½çš„è§†å›¾ï¼‰ [ä½œè€…æä¾›çš„å›¾åƒ]ã€‚'
- en: As you can see inside every task, you just import and call the function from
    its own moduleâ€¦ and maybe add some additional logs.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚ä½ åœ¨æ¯ä¸ªä»»åŠ¡ä¸­çœ‹åˆ°çš„ï¼Œä½ åªéœ€ä»å…¶æ¨¡å—ä¸­å¯¼å…¥å¹¶è°ƒç”¨å‡½æ•°â€¦â€¦å¹¶å¯èƒ½æ·»åŠ ä¸€äº›é¢å¤–çš„æ—¥å¿—ã€‚
- en: The key step in defining a task is in the arguments of the **task.virtualenv()**
    Python decorator.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: å®šä¹‰ä»»åŠ¡çš„å…³é”®æ­¥éª¤åœ¨äº**task.virtualenv()** Pythonè£…é¥°å™¨çš„å‚æ•°ä¸­ã€‚
- en: For every task, this specific decorator will create a different Python virtual
    environment inside which it will install all the given requirements.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ¯ä¸ªä»»åŠ¡ï¼Œè¿™ä¸ªç‰¹å®šçš„è£…é¥°å™¨å°†åˆ›å»ºä¸€ä¸ªä¸åŒçš„Pythonè™šæ‹Ÿç¯å¢ƒï¼Œåœ¨å…¶ä¸­å®‰è£…æ‰€æœ‰ç»™å®šçš„éœ€æ±‚ã€‚
- en: '**Note:** **172.17.0.1** is the IP address of your private PyPi repository.
    Remember that you host your PyPi repository using docker-compose under the same
    network as Airflow. **172.17.0.1** is the bridge IP address accessible by every
    Docker container inside the **default Docker** network. Thus, the Airflow container
    can access the PyPi server container using the bridge IP address.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼š** **172.17.0.1** æ˜¯ä½ ç§æœ‰PyPiä»“åº“çš„IPåœ°å€ã€‚è®°ä½ï¼Œä½ é€šè¿‡docker-composeåœ¨ä¸Airflowç›¸åŒçš„ç½‘ç»œä¸‹æ‰˜ç®¡ä½ çš„PyPiä»“åº“ã€‚**172.17.0.1**
    æ˜¯æ¯ä¸ªDockerå®¹å™¨åœ¨**default Docker**ç½‘ç»œå†…å¯ä»¥è®¿é—®çš„æ¡¥æ¥IPåœ°å€ã€‚å› æ­¤ï¼ŒAirflowå®¹å™¨å¯ä»¥é€šè¿‡æ¡¥æ¥IPåœ°å€è®¿é—®PyPiæœåŠ¡å™¨å®¹å™¨ã€‚'
- en: 'As you can see in the **requirements** argument, we defined the following:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚åœ¨**requirements**å‚æ•°ä¸­æ‰€ç¤ºï¼Œæˆ‘ä»¬å®šä¹‰äº†ä»¥ä¸‹å†…å®¹ï¼š
- en: '"**â€” trusted-host 172.17.0.1**": As the PyPi server is not secured with HTTPS,
    you must explicitly say that you trust this source.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"**â€” trusted-host 172.17.0.1**": ç”±äºPyPiæœåŠ¡å™¨æ²¡æœ‰ç”¨HTTPSä¿æŠ¤ï¼Œä½ å¿…é¡»æ˜ç¡®è¡¨ç¤ºä½ ä¿¡ä»»è¿™ä¸ªæ¥æºã€‚'
- en: '"**â€” extra-index-url http://172.17.0.1**": Tell Pip to also look at this PyPi
    repository when searching for a Python package. Note that Pip will still look
    in the official PyPi repository in addition to yours.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"**â€” extra-index-url http://172.17.0.1**": å‘Šè¯‰Pipåœ¨æœç´¢PythonåŒ…æ—¶ä¹ŸæŸ¥çœ‹è¿™ä¸ªPyPiä»“åº“ã€‚è¯·æ³¨æ„ï¼ŒPipåœ¨æœç´¢æ—¶ä»ä¼šæŸ¥çœ‹å®˜æ–¹PyPiä»“åº“ã€‚'
- en: '"**<your_python_packages>**": After the two lines described above, you can
    add any Python package. But note that you installed **feature_pipeline**, **training_pipeline**,
    and **batch_prediction_pipeline** asPython packages you built and deployed using
    Poetry.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '"**<your_python_packages>**": åœ¨ä¸Šè¿°ä¸¤è¡Œä¹‹åï¼Œä½ å¯ä»¥æ·»åŠ ä»»ä½•PythonåŒ…ã€‚ä½†è¯·æ³¨æ„ï¼Œä½ å·²ç»å®‰è£…äº†**feature_pipeline**ã€**training_pipeline**å’Œ**batch_prediction_pipeline**ä½œä¸ºä½ ç”¨Poetryæ„å»ºå’Œéƒ¨ç½²çš„PythonåŒ…ã€‚'
- en: 'The other arguments aren''t that interesting, but let me explain them:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä»–å‚æ•°å¹¶ä¸æ˜¯é‚£ä¹ˆæœ‰è¶£ï¼Œä½†è®©æˆ‘æ¥è§£é‡Šä¸€ä¸‹ï¼š
- en: '**task_id=" <task_id>":** The unique ID of a task.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**task_id=" <task_id>":** ä»»åŠ¡çš„å”¯ä¸€IDã€‚'
- en: '**python_version=" 3.9"**: When I was writing this course, Hopsworks worked
    only with Python 3.9, so we had to enforce this version of Python.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**python_version=" 3.9"**: å½“æˆ‘å†™è¿™é—¨è¯¾ç¨‹æ—¶ï¼ŒHopsworksåªæ”¯æŒPython 3.9ï¼Œæ‰€ä»¥æˆ‘ä»¬å¿…é¡»å¼ºåˆ¶ä½¿ç”¨è¿™ä¸ªç‰ˆæœ¬çš„Pythonã€‚'
- en: '**multiple_outputs=True**: The task returns a Python dictionary.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**multiple_outputs=True**: ä»»åŠ¡è¿”å›ä¸€ä¸ªPythonå­—å…¸ã€‚'
- en: '**system_site_packages=True:** Install default system packages.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**system_site_packages=True:** å®‰è£…é»˜è®¤çš„ç³»ç»ŸåŒ…ã€‚'
- en: '***Important***'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '***é‡è¦***'
- en: 'Notethat almost every task returns a dictionary of metadata that contains information
    such as:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œå‡ ä¹æ¯ä¸ªä»»åŠ¡éƒ½ä¼šè¿”å›ä¸€ä¸ªåŒ…å«ä¿¡æ¯çš„å…ƒæ•°æ®å­—å…¸ï¼Œä¾‹å¦‚ï¼š
- en: the date range when the data was extracted,
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ•°æ®æå–çš„æ—¥æœŸèŒƒå›´ï¼Œ
- en: the version of the feature group, feature view, etc.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç‰¹å¾ç»„ã€ç‰¹å¾è§†å›¾ç­‰çš„ç‰ˆæœ¬ã€‚
- en: the version of the sweep,
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: sweepçš„ç‰ˆæœ¬ã€‚
- en: the version of the model, etc.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¨¡å‹çš„ç‰ˆæœ¬ç­‰ã€‚
- en: This information is essential to be passed between tasks. For example, the **create_feature_view**
    task needs to know what version of the **feature_group** to use to create the
    next feature view. Also, when running **batch_predict**, you have to know the
    version of the feature view and model to use to generate the predictions.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ä¿¡æ¯åœ¨ä»»åŠ¡ä¹‹é—´ä¼ é€’æ˜¯è‡³å…³é‡è¦çš„ã€‚ä¾‹å¦‚ï¼Œ**create_feature_view** ä»»åŠ¡éœ€è¦çŸ¥é“ä½¿ç”¨å“ªä¸ªç‰ˆæœ¬çš„ **feature_group**
    æ¥åˆ›å»ºä¸‹ä¸€ä¸ªç‰¹å¾è§†å›¾ã€‚æ­¤å¤–ï¼Œåœ¨è¿è¡Œ **batch_predict** æ—¶ï¼Œä½ éœ€è¦çŸ¥é“è¦ä½¿ç”¨å“ªä¸ªç‰ˆæœ¬çš„ç‰¹å¾è§†å›¾å’Œæ¨¡å‹æ¥ç”Ÿæˆé¢„æµ‹ã€‚
- en: One interesting task is **task.branch(task_id=" if_run_hyperparameter_tuning_branching")**,
    which defines an if-else logic between whether to run the hyperparameter tuning
    logic or not.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªæœ‰è¶£çš„ä»»åŠ¡æ˜¯ **task.branch(task_id=" if_run_hyperparameter_tuning_branching")**ï¼Œå®ƒå®šä¹‰äº†ä¸€ä¸ªæ˜¯å¦è¿è¡Œè¶…å‚æ•°è°ƒæ•´é€»è¾‘çš„
    if-else é€»è¾‘ã€‚
- en: This special type of task returns a list of **task_ids** that will be executed
    next. For example, if it returns **["branch_run_hyperparameter_tuning"],** it
    will run only the task with the **task_id =** **branch_run_hyperparameter_tuning**.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§ç‰¹æ®Šç±»å‹çš„ä»»åŠ¡è¿”å›ä¸€ä¸ª **task_ids** åˆ—è¡¨ï¼Œè¿™äº›ä»»åŠ¡å°†ä¼šè¢«æ‰§è¡Œã€‚ä¾‹å¦‚ï¼Œå¦‚æœå®ƒè¿”å› **["branch_run_hyperparameter_tuning"]**ï¼Œåˆ™ä»…è¿è¡Œ
    **task_id =** **branch_run_hyperparameter_tuning** çš„ä»»åŠ¡ã€‚
- en: As you can see below, two empty operators (tasks) are defined with the task_ids
    used inside the **task.branch()** logic. It is a common pattern suggested by Airflow
    to use a set of empty operators (no operation) when choosing between multiple
    branches.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä¸‹æ‰€ç¤ºï¼Œå®šä¹‰äº†ä¸¤ä¸ªç©ºçš„æ“ä½œç¬¦ï¼ˆä»»åŠ¡ï¼‰ï¼Œå¹¶åœ¨ **task.branch()** é€»è¾‘ä¸­ä½¿ç”¨äº† task_idsã€‚è¿™æ˜¯ Airflow å»ºè®®çš„ä¸€ç§å¸¸è§æ¨¡å¼ï¼Œå½“åœ¨å¤šä¸ªåˆ†æ”¯ä¹‹é—´è¿›è¡Œé€‰æ‹©æ—¶ï¼Œä½¿ç”¨ä¸€ç»„ç©ºæ“ä½œç¬¦ï¼ˆæ— æ“ä½œï¼‰ã€‚
- en: Connect the Tasks into a DAG
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å°†ä»»åŠ¡è¿æ¥æˆä¸€ä¸ªDAG
- en: Now that you defined all the tasks, the final step is to connect them into a
    DAG. You have to perform this step so Airflow knows in what order to run every
    task.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ å·²ç»å®šä¹‰äº†æ‰€æœ‰çš„ä»»åŠ¡ï¼Œæœ€åä¸€æ­¥æ˜¯å°†å®ƒä»¬è¿æ¥æˆä¸€ä¸ª DAGã€‚ä½ å¿…é¡»æ‰§è¡Œè¿™ä¸€æ­¥ï¼Œä»¥ä¾¿ Airflow çŸ¥é“æ¯ä¸ªä»»åŠ¡çš„è¿è¡Œé¡ºåºã€‚
- en: Basically, here you will define the logic graph.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºæœ¬ä¸Šï¼Œè¿™é‡Œä½ å°†å®šä¹‰é€»è¾‘å›¾ã€‚
- en: '**#1.** The first step is to ***determine the set of variables*** that you
    will use to configure the DAG, such as **days_delay, days_export, feature_group_version,
    etc.** You can access these variables from the â€œAdmin -> Variablesâ€ panel of Airflow.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '**#1.** ç¬¬ä¸€æ­¥æ˜¯ ***ç¡®å®šä½ å°†ç”¨äºé…ç½®DAGçš„å˜é‡é›†åˆ***ï¼Œå¦‚ **days_delay, days_export, feature_group_version,
    ç­‰**ã€‚ä½ å¯ä»¥ä» Airflow çš„â€œAdmin -> Variablesâ€é¢æ¿ä¸­è®¿é—®è¿™äº›å˜é‡ã€‚'
- en: Note that you have to add them using the blue plus button explicitly.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œä½ å¿…é¡»æ˜¾å¼åœ°ä½¿ç”¨è“è‰²åŠ å·æŒ‰é’®æ·»åŠ å®ƒä»¬ã€‚
- en: '![](../Images/c016988f56a6918bcf17af862524385d.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c016988f56a6918bcf17af862524385d.png)'
- en: Screenshot of the Variables Airflow panel [Image by the Author].
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: å˜é‡ Airflow é¢æ¿çš„æˆªå›¾ [ä½œè€…æä¾›çš„å›¾ç‰‡]ã€‚
- en: '**#2.** The second step is to ***call the tasks with the right parameters***.
    As you can see, because of the Airflow 2.0 API, this step is just like calling
    a bunch of Python functions in a specific order.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '**#2.** ç¬¬äºŒæ­¥æ˜¯ ***è°ƒç”¨å…·æœ‰æ­£ç¡®å‚æ•°çš„ä»»åŠ¡***ã€‚å¦‚ä½ æ‰€è§ï¼Œç”±äº Airflow 2.0 APIï¼Œè¿™ä¸€æ­¥å°±åƒæŒ‰ç‰¹å®šé¡ºåºè°ƒç”¨ä¸€ç³»åˆ— Python
    å‡½æ•°ä¸€æ ·ã€‚'
- en: '**Note:** A dependency in the graph is automatically created if the output
    of a function is added as an input to another function.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ³¨æ„ï¼š** å¦‚æœä¸€ä¸ªå‡½æ•°çš„è¾“å‡ºä½œä¸ºè¾“å…¥æ·»åŠ åˆ°å¦ä¸€ä¸ªå‡½æ•°ä¸­ï¼Œåˆ™å›¾ä¸­çš„ä¾èµ–å…³ç³»ä¼šè‡ªåŠ¨åˆ›å»ºã€‚'
- en: It is essential to highlight how we passed the metadata of every pipeline element
    to the next ones. In doing so, we enforce the following scripts to use the correct
    data and model versions.
  id: totrans-234
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¼ºè°ƒå¦‚ä½•å°†æ¯ä¸ªç®¡é“å…ƒç´ çš„å…ƒæ•°æ®ä¼ é€’åˆ°ä¸‹ä¸€ä¸ªå…ƒç´ æ˜¯è‡³å…³é‡è¦çš„ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å¼ºåˆ¶æ‰§è¡Œä»¥ä¸‹è„šæœ¬ï¼Œä»¥ä½¿ç”¨æ­£ç¡®çš„æ•°æ®å’Œæ¨¡å‹ç‰ˆæœ¬ã€‚
- en: 'I also want to emphasize the following piece of code:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è¿˜æƒ³å¼ºè°ƒä»¥ä¸‹è¿™æ®µä»£ç ï¼š
- en: '[PRE12]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**â€œ{{ dag_run.logical_date }}"** is a template variable injected by Airflow
    that reflects the logical date when the DAG is run. Not the current date. By doing
    so, using the Airflow backfill features, you can easily use this as a datetime
    reference to backfill in a given time window. Now you can easily manipulate your
    extraction window''s starting and ending points.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '**â€œ{{ dag_run.logical_date }}"** æ˜¯ Airflow æ³¨å…¥çš„æ¨¡æ¿å˜é‡ï¼Œåæ˜ äº† DAG è¿è¡Œæ—¶çš„é€»è¾‘æ—¥æœŸï¼Œè€Œä¸æ˜¯å½“å‰æ—¥æœŸã€‚é€šè¿‡è¿™æ ·åšï¼Œåˆ©ç”¨
    Airflow å›å¡«åŠŸèƒ½ï¼Œä½ å¯ä»¥è½»æ¾åœ°å°†å…¶ä½œä¸ºæ—¥æœŸæ—¶é—´å‚è€ƒæ¥å›å¡«ç»™å®šæ—¶é—´çª—å£ä¸­çš„æ•°æ®ã€‚ç°åœ¨ä½ å¯ä»¥è½»æ¾åœ°æ“ä½œæå–çª—å£çš„èµ·å§‹å’Œç»“æŸç‚¹ã€‚'
- en: For example, if you want to run the DAG to backfill the energy consumption predictions
    between 10 and 11 May 2023, you will run the Airflow backfill logic with the "10
    May 2023 00:00 am date".
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¦‚æœä½ æƒ³è¿è¡Œ DAG ä»¥å›å¡« 2023å¹´5æœˆ10æ—¥è‡³11æ—¥çš„èƒ½æºæ¶ˆè€—é¢„æµ‹ï¼Œä½ å°†ä½¿ç”¨â€œ2023å¹´5æœˆ10æ—¥ 00:00 amâ€æ—¥æœŸè¿è¡Œ Airflow
    å›å¡«é€»è¾‘ã€‚
- en: '**#3\.** The last step is to enforce a specific ***DAG structure*** using the
    "***>>"*** operator.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '**#3\.** æœ€åä¸€æ­¥æ˜¯ä½¿ç”¨ "***>>"*** æ“ä½œç¬¦æ¥å¼ºåˆ¶æ‰§è¡Œç‰¹å®šçš„ ***DAG ç»“æ„***ã€‚'
- en: '"**A >> B >> C**" means run A, then B, then C.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '"**A >> B >> C**" æ„å‘³ç€å…ˆè¿è¡Œ Aï¼Œç„¶å Bï¼Œå†è¿è¡Œ Cã€‚'
- en: 'The only trickier piece of code is this one:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: å”¯ä¸€ç¨å¾®å¤æ‚ä¸€ç‚¹çš„ä»£ç æ˜¯è¿™ä¸ªï¼š
- en: '[PRE13]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ', where based on the **branch** operator, the DAG will either run the **branch_run_hyperparameter_tuning_operator**
    or **branch_skip_hyperparameter_tuning_operator** branches of the DAG.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ï¼Œå…¶ä¸­æ ¹æ®**branch**è¿ç®—ç¬¦ï¼ŒDAG å°†è¿è¡Œ**branch_run_hyperparameter_tuning_operator**æˆ–**branch_skip_hyperparameter_tuning_operator**åˆ†æ”¯ã€‚
- en: Read more about branching in Airflow [here](https://docs.astronomer.io/learn/airflow-branch-operator)
    [3].
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: é˜…è¯»æœ‰å…³ Airflow ä¸­åˆ†æ”¯çš„æ›´å¤šä¿¡æ¯[è¿™é‡Œ](https://docs.astronomer.io/learn/airflow-branch-operator)
    [3]ã€‚
- en: In English, it will run hyper optimization tunning or skip it, as shown in the
    image below â€” I know the image is quite small. Check the video for a better view.
    ğŸ‘‡
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥è‹±è¯­è¿è¡Œæ—¶ï¼Œå®ƒå°†è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–æˆ–è·³è¿‡ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºâ€”â€”æˆ‘çŸ¥é“å›¾ç‰‡æ¯”è¾ƒå°ã€‚æŸ¥çœ‹è§†é¢‘ä»¥è·å–æ›´æ¸…æ™°çš„è§†å›¾ã€‚ ğŸ‘‡
- en: '![](../Images/832932dc0905d7afa066c5026aa25207.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/832932dc0905d7afa066c5026aa25207.png)'
- en: Screenshot of the ml_pipeline DAG [Image by the Author].
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ml_pipeline DAG æˆªå›¾ [ä½œè€…æä¾›çš„å›¾ç‰‡]ã€‚
- en: That is it! You orchestrated all the 3 pipelines using Airflow. Congrats!
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: å°±è¿™æ ·ï¼ä½ ä½¿ç”¨ Airflow åè°ƒäº†æ‰€æœ‰3ä¸ªç®¡é“ã€‚æ­å–œï¼
- en: Run the ML Pipeline DAG
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¿è¡Œ ML Pipeline DAG
- en: This step is easy.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€æ­¥å¾ˆç®€å•ã€‚
- en: Just go to your **ml_pipeline** DAG and click the play button.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: åªéœ€è¿›å…¥ä½ çš„**ml_pipeline** DAG å¹¶ç‚¹å‡»æ’­æ”¾æŒ‰é’®ã€‚
- en: '![](../Images/8797258b1819ae7d4eab3b52f7bb272e.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8797258b1819ae7d4eab3b52f7bb272e.png)'
- en: Screenshot of the ml_pipeline DAG view [Image by the Author].
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ml_pipeline DAG è§†å›¾æˆªå›¾ [ä½œè€…æä¾›çš„å›¾ç‰‡]ã€‚
- en: Backfill using Airflow
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Airflow è¿›è¡Œå›å¡«
- en: 'Find your **airflow-webserver** docker container ID:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥æ‰¾ä½ çš„**airflow-webserver** docker å®¹å™¨ IDï¼š
- en: '[PRE14]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Start a shell inside the **airflow-webserver** container and run **airflow
    dags backfill** as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨**airflow-webserver**å®¹å™¨å†…å¯åŠ¨ shell å¹¶è¿è¡Œ**airflow dags backfill**ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE15]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'If you want to clear the tasks and rerun them, run these commands:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³æ¸…é™¤ä»»åŠ¡å¹¶é‡æ–°è¿è¡Œå®ƒä»¬ï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
- en: '[PRE16]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Conclusion
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: Congratulations! You finished the **fourth lesson** from the **Full Stack 7-Steps
    MLOps Framework** course.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: æ­å–œï¼ä½ å®Œæˆäº†**ç¬¬å››è¯¾**æ¥è‡ª**å…¨æ ˆ7æ­¥MLOpsæ¡†æ¶**è¯¾ç¨‹ã€‚
- en: 'If you have reached this far, you know how to:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å·²ç»çœ‹åˆ°è¿™é‡Œï¼Œä½ çŸ¥é“å¦‚ä½•ï¼š
- en: Host your own PyPi server
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ‰˜ç®¡è‡ªå·±çš„ PyPi æœåŠ¡å™¨
- en: Build & deploy your Python modules using Poetry
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Poetry æ„å»ºå’Œéƒ¨ç½² Python æ¨¡å—
- en: Orchestrate multiple pipelines using Airflow
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Airflow åè°ƒå¤šä¸ªç®¡é“
- en: Now that you understand the power of using an orchestrator such as Airflow,
    you can build robust production-ready pipelines that you can quickly schedule,
    configure, and monitor.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ å·²ç»ç†è§£äº†ä½¿ç”¨åƒ Airflow è¿™æ ·çš„åè°ƒå·¥å…·çš„å¼ºå¤§åŠŸèƒ½ï¼Œä½ å¯ä»¥æ„å»ºå¼ºå¤§çš„ç”Ÿäº§å°±ç»ªç®¡é“ï¼Œå¹¶å¿«é€Ÿè°ƒåº¦ã€é…ç½®å’Œç›‘æ§ã€‚
- en: Check out [Lesson 5](/ensuring-trustworthy-ml-systems-with-data-validation-and-real-time-monitoring-89ab079f4360)
    to learn how to use Great Expectations to validate the integrity and quality of
    your data. Also, you will understand how to implement a monitoring component on
    top of your ML system.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹[ç¬¬5è¯¾](/ensuring-trustworthy-ml-systems-with-data-validation-and-real-time-monitoring-89ab079f4360)äº†è§£å¦‚ä½•ä½¿ç”¨
    Great Expectations éªŒè¯æ•°æ®çš„å®Œæ•´æ€§å’Œè´¨é‡ã€‚æ­¤å¤–ï¼Œä½ å°†äº†è§£å¦‚ä½•åœ¨æœºå™¨å­¦ä¹ ç³»ç»Ÿä¸Šå®ç°ç›‘æ§ç»„ä»¶ã€‚
- en: '**Also,** [**you can access the GitHub repository here**](https://github.com/iusztinpaul/energy-forecasting)**.**'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¤å¤–ï¼Œ** [**ä½ å¯ä»¥åœ¨è¿™é‡Œè®¿é—® GitHub ä»“åº“**](https://github.com/iusztinpaul/energy-forecasting)**ã€‚**'
- en: ğŸ’¡ My goal is to help machine learning engineers level up in designing and productionizing
    ML systems. Follow me on [LinkedIn](https://www.linkedin.com/in/pauliusztin/)
    or subscribe to my [weekly newsletter](https://pauliusztin.substack.com/) for
    more insights!
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’¡ æˆ‘çš„ç›®æ ‡æ˜¯å¸®åŠ©æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆæå‡è®¾è®¡å’Œç”Ÿäº§æœºå™¨å­¦ä¹ ç³»ç»Ÿçš„èƒ½åŠ›ã€‚å…³æ³¨æˆ‘åœ¨[LinkedIn](https://www.linkedin.com/in/pauliusztin/)æˆ–è®¢é˜…æˆ‘çš„[æ¯å‘¨é€šè®¯](https://pauliusztin.substack.com/)ä»¥è·å–æ›´å¤šè§è§£ï¼
- en: ğŸ”¥ If you enjoy reading articles like this and wish to support my writing, consider
    [becoming a Medium member](https://pauliusztin.medium.com/membership). By using
    [my referral link](https://pauliusztin.medium.com/membership), you can support
    me without any extra cost while enjoying limitless access to Mediumâ€™s rich collection
    of stories.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”¥ å¦‚æœä½ å–œæ¬¢é˜…è¯»è¿™æ ·çš„æ–‡ç« å¹¶å¸Œæœ›æ”¯æŒæˆ‘çš„å†™ä½œï¼Œè€ƒè™‘[æˆä¸º Medium ä¼šå‘˜](https://pauliusztin.medium.com/membership)ã€‚é€šè¿‡ä½¿ç”¨[æˆ‘çš„æ¨èé“¾æ¥](https://pauliusztin.medium.com/membership)ï¼Œä½ å¯ä»¥åœ¨ä¸å¢åŠ ä»»ä½•é¢å¤–è´¹ç”¨çš„æƒ…å†µä¸‹æ”¯æŒæˆ‘ï¼ŒåŒæ—¶äº«å—
    Medium ä¸°å¯Œæ•…äº‹çš„æ— é™è®¿é—®æƒé™ã€‚
- en: '[](https://pauliusztin.medium.com/membership?source=post_page-----880aa9be8cff--------------------------------)
    [## Join Medium with my referral link - Paul Iusztin'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pauliusztin.medium.com/membership?source=post_page-----880aa9be8cff--------------------------------)
    [## ä½¿ç”¨æˆ‘çš„æ¨èé“¾æ¥åŠ å…¥ Medium - Paul Iusztin'
- en: ğŸ¤– Join to get exclusive content about designing and building production-ready
    ML systems ğŸš€ Unlock full access toâ€¦
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ğŸ¤– åŠ å…¥ä»¥è·å–æœ‰å…³è®¾è®¡å’Œæ„å»ºç”Ÿäº§å°±ç»ªæœºå™¨å­¦ä¹ ç³»ç»Ÿçš„ç‹¬å®¶å†…å®¹ ğŸš€ è§£é”å…¨éƒ¨è®¿é—®æƒé™â€¦
- en: pauliusztin.medium.com](https://pauliusztin.medium.com/membership?source=post_page-----880aa9be8cff--------------------------------)
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '[pauliusztin.medium.com](https://pauliusztin.medium.com/membership?source=post_page-----880aa9be8cff--------------------------------)'
- en: References
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒèµ„æ–™
- en: '[1] [Energy Consumption per DE35 Industry Code from Denmark API](https://www.energidataservice.dk/tso-electricity/ConsumptionDE35Hour),
    [Denmark Energy Data Service](https://www.energidataservice.dk/about/)'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [ä¸¹éº¦ API æ¯å°æ—¶ DE35 è¡Œä¸šä»£ç çš„èƒ½è€—](https://www.energidataservice.dk/tso-electricity/ConsumptionDE35Hour)ï¼Œ[ä¸¹éº¦èƒ½æºæ•°æ®æœåŠ¡](https://www.energidataservice.dk/about/)'
- en: '[2] [Running Airflow in Docker](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html),
    Airflow Documentation'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [åœ¨ Docker ä¸­è¿è¡Œ Airflow](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html)ï¼ŒAirflow
    æ–‡æ¡£'
- en: '[3] [Branching in Airflow](https://docs.astronomer.io/learn/airflow-branch-operator),
    Airflow Documentation on Astronomer'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [Airflow ä¸­çš„åˆ†æ”¯](https://docs.astronomer.io/learn/airflow-branch-operator)ï¼ŒAstronomer
    ä¸Šçš„ Airflow æ–‡æ¡£'
