- en: How Do We Know if a Text Is AI-generated?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们如何知道一篇文本是AI生成的？
- en: 原文：[https://towardsdatascience.com/how-do-we-know-if-a-text-is-ai-generated-82e710ea7b51](https://towardsdatascience.com/how-do-we-know-if-a-text-is-ai-generated-82e710ea7b51)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-do-we-know-if-a-text-is-ai-generated-82e710ea7b51](https://towardsdatascience.com/how-do-we-know-if-a-text-is-ai-generated-82e710ea7b51)
- en: Different Statistical Approaches to Detecting AI-generated Text.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检测AI生成文本的不同统计方法。
- en: '[](https://saraametwalli.medium.com/?source=post_page-----82e710ea7b51--------------------------------)[![Sara
    A. Metwalli](../Images/d6861f7bc1879bf68d4b7116c335c7e5.png)](https://saraametwalli.medium.com/?source=post_page-----82e710ea7b51--------------------------------)[](https://towardsdatascience.com/?source=post_page-----82e710ea7b51--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----82e710ea7b51--------------------------------)
    [Sara A. Metwalli](https://saraametwalli.medium.com/?source=post_page-----82e710ea7b51--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://saraametwalli.medium.com/?source=post_page-----82e710ea7b51--------------------------------)[![Sara
    A. Metwalli](../Images/d6861f7bc1879bf68d4b7116c335c7e5.png)](https://saraametwalli.medium.com/?source=post_page-----82e710ea7b51--------------------------------)[](https://towardsdatascience.com/?source=post_page-----82e710ea7b51--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----82e710ea7b51--------------------------------)
    [Sara A. Metwalli](https://saraametwalli.medium.com/?source=post_page-----82e710ea7b51--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----82e710ea7b51--------------------------------)
    ·6 min read·May 26, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----82e710ea7b51--------------------------------)
    ·阅读时间6分钟·2023年5月26日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/615b8a12bbfc4f3f47dc8e3e67c98809.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/615b8a12bbfc4f3f47dc8e3e67c98809.png)'
- en: Photo by [Andreas Fickl](https://unsplash.com/@afafa?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Andreas Fickl](https://unsplash.com/@afafa?utm_source=medium&utm_medium=referral)
    在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: In the fascinating and rapidly advancing realm of artificial intelligence, one
    of the most exciting advances has been the development of AI text generation.
    AI models, like [GPT-3](https://openai.com/blog/gpt-3-apps), [Bloom](https://bloomai.co/),
    [BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html),
    [AlexaTM](https://www.amazon.science/publications/alexatm-20b-few-shot-learning-using-a-large-scale-multilingual-seq2seq-model),
    and other large language models, can produce remarkably human-like text. This
    is both exciting and concerning at the same time. Such technological advances
    allow us to be creative in ways we didn’t before. Still, they also open the door
    to deception. And the better these models get, the more challenging it will be
    to distinguish between a human-written text and an AI-generated text.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工智能这一迷人且迅速发展的领域中，最令人兴奋的进展之一就是AI文本生成的出现。像[GPT-3](https://openai.com/blog/gpt-3-apps)、[Bloom](https://bloomai.co/)、[BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)、[AlexaTM](https://www.amazon.science/publications/alexatm-20b-few-shot-learning-using-a-large-scale-multilingual-seq2seq-model)以及其他大型语言模型，能够生成极其类似人类的文本。这既令人兴奋又让人担忧。这些技术进步使我们能够以之前无法实现的方式进行创造，但同时也打开了欺骗的大门。随着这些模型的不断改进，区分人类编写的文本和AI生成的文本将变得越来越具有挑战性。
- en: Since the release of [ChatGPT](https://openai.com/blog/chatgpt), people all
    over the globe have been testing the limits of such AI models and using them to
    both gain knowledge, but also, in the case of some students, to solve homework
    and exams, which challenges the ethical implications of such technology. Especially
    as these models have become sophisticated enough to mimic human writing styles
    and maintain context over multiple passages, they still need to be fixed, even
    if their errors are minor.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 自[ChatGPT](https://openai.com/blog/chatgpt)发布以来，全球范围内的人们一直在测试这些AI模型的极限，利用它们获取知识，同时也有一些学生用它们来完成作业和考试，这对这种技术的伦理影响提出了挑战。尤其是当这些模型变得足够复杂以模仿人类写作风格并在多个段落中保持上下文时，即便是微小的错误也需要修正。
- en: That raises an important question, a question I get asked quite often by my
    friends and family members (I got asked that question many many times since ChatGPT
    was released…),
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这引发了一个重要的问题，这是我经常被朋友和家人问到的问题（自从ChatGPT发布以来，我被问到过很多次……）
- en: '*How can we know if a text is human-written or AI-generated?*'
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*我们怎么知道一篇文本是人类编写的还是AI生成的？*'
- en: '[](/how-to-evaluate-the-performance-of-your-ml-ai-models-ba1debc6f2fa?source=post_page-----82e710ea7b51--------------------------------)
    [## How to Evaluate the Performance of Your ML/ AI Models'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/how-to-evaluate-the-performance-of-your-ml-ai-models-ba1debc6f2fa?source=post_page-----82e710ea7b51--------------------------------)
    [## 如何评估你的 ML/AI 模型的性能'
- en: An accurate evaluation is the only way to performance improvement
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准确的评估是性能改进的唯一途径
- en: towardsdatascience.com](/how-to-evaluate-the-performance-of-your-ml-ai-models-ba1debc6f2fa?source=post_page-----82e710ea7b51--------------------------------)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/how-to-evaluate-the-performance-of-your-ml-ai-models-ba1debc6f2fa?source=post_page-----82e710ea7b51--------------------------------)
- en: This question is not new to the research world; detecting AI-generated text,
    we call this “deep fake text detection.” Today, there are different tools that
    you can use to detect if a text is human-written or AI-generated, such as GPT-2
    by OpenAI. But how do such tools work?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题在研究界并不新鲜；检测 AI 生成的文本，我们称之为“深度伪造文本检测”。今天，你可以使用不同的工具来检测文本是由人类撰写还是 AI 生成的，例如
    OpenAI 的 GPT-2。那么这些工具是如何工作的呢？
- en: Different approaches are currently used to detect AI-generated text; new techniques
    are being researched and implemented to detect such text as the models used to
    generate these texts get more advanced.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当前有不同的方法用于检测 AI 生成的文本；随着用于生成这些文本的模型变得更加先进，新的技术也在被研究和实施，以检测这种文本。
- en: This article will explore 5 different statistical approaches that can be used
    to detect AI-generated text.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将探讨 5 种不同的统计方法，用于检测 AI 生成的文本。
- en: Let’s get right to it…
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直接进入主题……
- en: '**1\. N-gram Analysis:**'
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**1\. N-gram 分析：**'
- en: 'An N-gram is a sequence of N words or tokens from a given text sample. The
    “N” in N-gram is how many words are in the N-gram. For example:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: N-gram 是从给定文本样本中提取的 N 个单词或标记的序列。N-gram 中的“N”代表序列中的单词数。例如：
- en: New York (2-gram).
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 纽约（2-gram）。
- en: The Three Musketeers (3-gram).
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 三剑客（3-gram）。
- en: The group met regularly (4-gram).
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个小组定期会面（4-gram）。
- en: Analyzing the frequency of different N-grams in a text makes it possible to
    determine patterns. For example, among the three N-gram examples we just went
    through, the first is the most common, and the third is the least common. By tracking
    the different N-grams, we can decide that they are more or less common in AI-generated
    text than in human-written text. For instance, an AI might use specific phrases
    or word combinations more frequently than a human writer. We can find the relation
    between the frequency of N-grams used by AI vs. humans by training our model on
    data generated by humans and AI.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 分析文本中不同 N-gram 的频率可以确定模式。例如，在我们刚刚经历的三个 N-gram 示例中，第一个是最常见的，第三个是最少见的。通过跟踪不同的
    N-gram，我们可以决定它们在 AI 生成的文本中比在人工撰写的文本中更常见或更少见。例如，AI 可能会比人类作者更频繁地使用特定的短语或词汇组合。通过训练我们的模型来处理人类和
    AI 生成的数据，我们可以找到 AI 和人类使用的 N-gram 频率之间的关系。
- en: '**2\. Perplexity:**'
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**2\. 困惑度：**'
- en: If you look up the word perplexed in the English dictionary, it will be defined
    as surprised or shocked, but, in the context of AI and NLP, in particular, perplexity
    measures how confidently a language model predicts a text. Estimating the perplexity
    of a model is done by quantifying how long a model needs to respond to a new text,
    or in other words, how “surprised” the model is by the new text. For example,
    an AI-generated text might lower the perplexity of a model; the better the model
    predicts the text. Perplexity is fast to calculate, which gives it an advantage
    over other approaches.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在英文词典中查找“perplexed”这个词，它会被定义为惊讶或震惊。然而，在 AI 和自然语言处理（NLP）的背景下，特别是，perplexity（困惑度）用来衡量语言模型预测文本的信心程度。估计模型的困惑度是通过量化模型对新文本的响应时间，换句话说，就是模型对新文本的“惊讶”程度。例如，AI
    生成的文本可能会降低模型的困惑度；模型对文本的预测越准确，困惑度就越低。困惑度计算速度快，这使得它在其他方法中具有优势。
- en: '**3\. Burstiness:**'
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**3\. 突发性：**'
- en: In NLP, [Slava Katz](https://dl.acm.org/doi/10.1017/S1351324996001246) defines
    burstiness as the phenomenon where certain words appear in “bursts” within a document
    or a set of documents. The idea is that when a word is used once in a document,
    it’s likely to be used again in the same document. AI-generated texts exhibit
    different patterns of burstiness than that written by a human, as they don’t have
    the required cognitive processes to choose other synonyms.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）中，[Slava Katz](https://dl.acm.org/doi/10.1017/S1351324996001246)
    将 burstiness（突发性）定义为在文档或一组文档中某些词汇出现“突发”的现象。这个概念是，当一个词在文档中出现一次时，它很可能会在同一文档中再次出现。AI
    生成的文本表现出不同于人类写作的突发性模式，因为它们没有选择其他同义词所需的认知过程。
- en: '[](/top-5-data-labeling-tools-to-use-in-2023-52bbc905ebe3?source=post_page-----82e710ea7b51--------------------------------)
    [## Top 6 Data Labeling Tools To Use In 2023'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/top-5-data-labeling-tools-to-use-in-2023-52bbc905ebe3?source=post_page-----82e710ea7b51--------------------------------)
    [## 2023年使用的前6大数据标注工具'
- en: Speed up your data labeling and have better results with these tools.
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 利用这些工具加快数据标注速度并获得更好的结果。
- en: towardsdatascience.com](/top-5-data-labeling-tools-to-use-in-2023-52bbc905ebe3?source=post_page-----82e710ea7b51--------------------------------)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/top-5-data-labeling-tools-to-use-in-2023-52bbc905ebe3?source=post_page-----82e710ea7b51--------------------------------)
- en: '**4\. Stylometry:**'
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**4\. 文体分析：**'
- en: Stylometry is the study of linguistic style, and it can be used to identify
    authors or, in this case, the source of a text (human vs. AI). Everyone uses language.
    Differently some prefer short sentences, and some prefer long, connected ones.
    People use semi-colons and em0dashes (And other unique punctuations) differently
    from one person to another. Moreover, some people use the passive voice more than
    the active one or use more complex vocabulary. An AI-generated text might exhibit
    different stylistic features, even writing about the same topic more than once.
    And since an AI doesn’t have a style, these different styles can be used to detect
    if an AI writes a text.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 文体分析是对语言风格的研究，可以用来识别作者，或者在这种情况下，识别文本的来源（人类与AI）。每个人使用语言的方式不同，有些人喜欢短句，而有些人则喜欢长且连贯的句子。人们使用分号和破折号（以及其他独特的标点符号）的方式也各不相同。此外，有些人更频繁使用被动语态而不是主动语态，或使用更复杂的词汇。AI生成的文本可能会展现不同的风格特征，即使是关于同一主题的内容也可能有所不同。而且由于AI没有自身的风格，这些不同的风格可以用来检测是否由AI编写文本。
- en: '**5\. Consistency and Coherence Analysis:**'
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**5\. 一致性和连贯性分析：**'
- en: Following up on Stylometry, since AI models don’t have their own style, the
    text they generate sometimes needs more consistency and long-term coherence. For
    example, AI might contradict itself or change topics and style abruptly in the
    middle of the text, leading to a more difficult-to-follow flow of ideas.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 延续文体分析，由于AI模型没有自己的风格，它们生成的文本有时需要更多的一致性和长期的连贯性。例如，AI可能会自我矛盾或在文本中途突然改变话题和风格，从而导致思想流动更难以跟随。
- en: We can use Python to implement a very simplistic version to understand how each
    approach works.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用Python实现一个非常简化的版本，以理解每种方法的工作原理。
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If you want to explore how to implement these approaches in-depth, check out
    these tutorials:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想深入探讨如何实施这些方法，请查看这些教程：
- en: '[N-grams](https://www.analyticsvidhya.com/blog/2021/09/what-are-n-grams-and-how-to-implement-them-in-python/).'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[N-gram](https://www.analyticsvidhya.com/blog/2021/09/what-are-n-grams-and-how-to-implement-them-in-python/)。'
- en: '[Perplexity](https://keras.io/api/keras_nlp/metrics/perplexity/).'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[困惑度](https://keras.io/api/keras_nlp/metrics/perplexity/)。'
- en: '[Stylometry](https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python).'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[文体分析](https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python)。'
- en: '[Consistency](https://medium.com/@david_73439/nlp-corpus-consistency-and-accuracy-ebb33b46cb0c).'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[一致性](https://medium.com/@david_73439/nlp-corpus-consistency-and-accuracy-ebb33b46cb0c)。'
- en: Final Thoughts
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: As we advance into AI, we will need more advanced and complex tools to detect
    AI-generated text to avoid misinformation and deception. Although this is a very
    active area of research today, researchers have developed tools to detect AI-written
    text, an example of such work is done by [Edward Tian](https://www.linkedin.com/in/ed-tian/?originalSubdomain=ca)
    from Princeton University. Tian developed an experimental tool called [GPTZero](https://gptzero.me/)
    that uses “perplexity” and “burstiness” to estimate the likelihood that an AI-generated
    a piece of content. Another example is [Noah Smith](https://nasmith.github.io/),
    a professor and NLP researcher at the University of Washington whose research
    focuses on the unique quality of human-written text intentionality. AI often generates
    text that needs to be more intentional and consistent, which may change as these
    language models improve. None of the approaches explored in this article is bulletproof;
    a combination of different techniques and an extensive training set is often used
    to build real-life AI-generated text classifiers.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们向人工智能领域的深入，我们将需要更先进和复杂的工具来检测AI生成的文本，以避免误信息和欺骗。虽然这是一个非常活跃的研究领域，但研究人员已经开发了检测AI写作文本的工具，举例来说，[爱德华·田](https://www.linkedin.com/in/ed-tian/?originalSubdomain=ca)来自普林斯顿大学完成了这样的工作。田开发了一种名为[GPTZero](https://gptzero.me/)的实验工具，该工具使用“困惑度”和“突发性”来估计AI生成内容的可能性。另一个例子是[诺亚·史密斯](https://nasmith.github.io/)，他是华盛顿大学的教授和自然语言处理研究员，他的研究集中在人工写作文本的独特质量和意图上。AI通常生成的文本缺乏意图和一致性，这可能会随着语言模型的改进而改变。本文中探讨的任何方法都不是万无一失的；通常需要结合不同的技术和广泛的训练集来构建现实生活中的AI生成文本分类器。
- en: Technology is always an exciting addition to our lives, capable of improving
    our quality of life. However, it can also present us with new challenges and obstacles.
    However, these obstacles are never a reason to abandon using new technologies
    but rather an opportunity to develop protocols and rules that allow us to take
    advantage of such great technology ethically.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 科技始终是我们生活中令人兴奋的补充，能够提升我们的生活质量。然而，它也可能给我们带来新的挑战和障碍。然而，这些障碍从未成为放弃使用新技术的理由，而是一个发展协议和规则的机会，让我们能够以伦理的方式利用这些伟大的技术。
