- en: 'ğŸ¦œğŸ”— LangChain: Develop applications powered by Language Models'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ¦œğŸ”— LangChainï¼šå¼€å‘ç”±è¯­è¨€æ¨¡å‹é©±åŠ¨çš„åº”ç”¨ç¨‹åº
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/develop-applications-powered-by-language-models-with-langchain-d2f7a1d1ad1a](https://towardsdatascience.com/develop-applications-powered-by-language-models-with-langchain-d2f7a1d1ad1a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/develop-applications-powered-by-language-models-with-langchain-d2f7a1d1ad1a](https://towardsdatascience.com/develop-applications-powered-by-language-models-with-langchain-d2f7a1d1ad1a)
- en: '![](../Images/eadb36548cf05e78b4161169a551f8aa.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eadb36548cf05e78b4161169a551f8aa.png)'
- en: Photo by [Choong Deng Xiang](https://unsplash.com/@dengxiangs?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”± [Choong Deng Xiang](https://unsplash.com/@dengxiangs?utm_source=medium&utm_medium=referral)
    æä¾›ï¼Œæ¥æºäº [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Get started using LangChain with Python to leverage LLMs
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¼€å§‹ä½¿ç”¨ LangChain å’Œ Python åˆ©ç”¨ LLM
- en: '[](https://medium.com/@marcellopoliti?source=post_page-----d2f7a1d1ad1a--------------------------------)[![Marcello
    Politi](../Images/484e44571bd2e75acfe5fef3146ab3c2.png)](https://medium.com/@marcellopoliti?source=post_page-----d2f7a1d1ad1a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d2f7a1d1ad1a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d2f7a1d1ad1a--------------------------------)
    [Marcello Politi](https://medium.com/@marcellopoliti?source=post_page-----d2f7a1d1ad1a--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@marcellopoliti?source=post_page-----d2f7a1d1ad1a--------------------------------)[![Marcello
    Politi](../Images/484e44571bd2e75acfe5fef3146ab3c2.png)](https://medium.com/@marcellopoliti?source=post_page-----d2f7a1d1ad1a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----d2f7a1d1ad1a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----d2f7a1d1ad1a--------------------------------)
    [Marcello Politi](https://medium.com/@marcellopoliti?source=post_page-----d2f7a1d1ad1a--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d2f7a1d1ad1a--------------------------------)
    Â·12 min readÂ·Apr 26, 2023
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒåœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----d2f7a1d1ad1a--------------------------------)
    Â·é˜…è¯»æ—¶é—´ 12 åˆ†é’ŸÂ·2023å¹´4æœˆ26æ—¥
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»‹ç»
- en: '**LangChain is a framework that enables quick and easy development of applications
    that make use of Large Language Models**, for example, GPT-3.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**LangChain æ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œä½¿å¾—åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ GPT-3ï¼‰å¿«é€Ÿè€Œè½»æ¾åœ°å¼€å‘åº”ç”¨ç¨‹åºæˆä¸ºå¯èƒ½**ã€‚'
- en: The framework, however, introduces additional possibilities, for example, the
    one of easily using external data sources, such as Wikipedia, to amplify the capabilities
    provided by the model. I am sure that you have all probably tried to use Chat-GPT
    and find that it fails to answer about events that occurred beyond a certain date.
    In this case, a search on Wikipedia could help GPT to answer more questions.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œè¿™ä¸ªæ¡†æ¶å¼•å…¥äº†é¢å¤–çš„å¯èƒ½æ€§ï¼Œä¾‹å¦‚ï¼Œè½»æ¾ä½¿ç”¨å¤–éƒ¨æ•°æ®æºï¼Œå¦‚ç»´åŸºç™¾ç§‘ï¼Œä»¥å¢å¼ºæ¨¡å‹æä¾›çš„èƒ½åŠ›ã€‚æˆ‘ç›¸ä¿¡ä½ ä»¬å¯èƒ½éƒ½å°è¯•è¿‡ä½¿ç”¨ Chat-GPTï¼Œå¹¶å‘ç°å®ƒæ— æ³•å›ç­”æŸä¸ªæ—¥æœŸä¹‹åå‘ç”Ÿçš„äº‹ä»¶ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç»´åŸºç™¾ç§‘çš„æœç´¢å¯ä»¥å¸®åŠ©
    GPT å›ç­”æ›´å¤šé—®é¢˜ã€‚
- en: LangChain Structure
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LangChain ç»“æ„
- en: The framework is organized into six modules each module allows you to manage
    a different aspect of the interaction with the LLM. Letâ€™s see what the modules
    are.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¡†æ¶åˆ†ä¸ºå…­ä¸ªæ¨¡å—ï¼Œæ¯ä¸ªæ¨¡å—å…è®¸ä½ ç®¡ç†ä¸ LLM äº¤äº’çš„ä¸åŒæ–¹é¢ã€‚è®©æˆ‘ä»¬çœ‹çœ‹è¿™äº›æ¨¡å—æ˜¯ä»€ä¹ˆã€‚
- en: '[**Models**](https://python.langchain.com/en/latest/modules/models.html): Allows
    you to instantiate and use different models.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**æ¨¡å‹**](https://python.langchain.com/en/latest/modules/models.html)ï¼šå…è®¸ä½ å®ä¾‹åŒ–å’Œä½¿ç”¨ä¸åŒçš„æ¨¡å‹ã€‚'
- en: '[**Prompts**](https://python.langchain.com/en/latest/modules/prompts.html):
    The prompt is how we interact with the model to try to obtain an output from it.
    By now knowing how to write an effective prompt is of critical importance. This
    framework module allows us to better manage prompts. For example, by creating
    templates that we can reuse.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**æç¤º**](https://python.langchain.com/en/latest/modules/prompts.html)ï¼šæç¤ºæ˜¯æˆ‘ä»¬ä¸æ¨¡å‹äº’åŠ¨ä»¥å°è¯•è·å¾—è¾“å‡ºçš„æ–¹å¼ã€‚ç°åœ¨çŸ¥é“å¦‚ä½•ç¼–å†™æœ‰æ•ˆçš„æç¤ºè‡³å…³é‡è¦ã€‚è¿™ä¸ªæ¡†æ¶æ¨¡å—å…è®¸æˆ‘ä»¬æ›´å¥½åœ°ç®¡ç†æç¤ºã€‚ä¾‹å¦‚ï¼Œé€šè¿‡åˆ›å»ºå¯ä»¥é‡ç”¨çš„æ¨¡æ¿ã€‚'
- en: '[**Indexes**](https://python.langchain.com/en/latest/modules/indexes.html):
    The best models are often those that are combined with some of your textual data,
    in order to add context or explain something to the model. This module helps us
    do just that.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**ç´¢å¼•**](https://python.langchain.com/en/latest/modules/indexes.html)ï¼šæœ€å¥½çš„æ¨¡å‹é€šå¸¸æ˜¯é‚£äº›ä¸ä¸€äº›æ–‡æœ¬æ•°æ®ç›¸ç»“åˆçš„æ¨¡å‹ï¼Œä»¥ä¾¿ä¸ºæ¨¡å‹æ·»åŠ ä¸Šä¸‹æ–‡æˆ–è§£é‡ŠæŸäº›å†…å®¹ã€‚è¿™ä¸ªæ¨¡å—å¸®åŠ©æˆ‘ä»¬åšåˆ°è¿™ä¸€ç‚¹ã€‚'
- en: '[**Chains**](https://python.langchain.com/en/latest/modules/chains.html): Many
    times to solve tasks a single API call to an LLM is not enough. This module allows
    other tools to be integrated. For example, one call can be a composed chain with
    the purpose of getting information from Wikipedia and then giving this information
    as input to the model. This module allows multiple tools to be concatenated in
    order to solve complex tasks.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**é“¾**](https://python.langchain.com/en/latest/modules/chains.html)ï¼šå¾ˆå¤šæ—¶å€™ï¼Œå•æ¬¡è°ƒç”¨
    LLM API æ˜¯ä¸å¤Ÿçš„ã€‚è¯¥æ¨¡å—å…è®¸æ•´åˆå…¶ä»–å·¥å…·ã€‚ä¾‹å¦‚ï¼Œä¸€æ¬¡è°ƒç”¨å¯ä»¥æ˜¯ä¸€ä¸ªå¤åˆé“¾ï¼Œå…¶ç›®çš„æ˜¯ä»ç»´åŸºç™¾ç§‘è·å–ä¿¡æ¯ï¼Œç„¶åå°†è¿™äº›ä¿¡æ¯ä½œä¸ºè¾“å…¥æä¾›ç»™æ¨¡å‹ã€‚è¿™ä¸ªæ¨¡å—å…è®¸å°†å¤šä¸ªå·¥å…·ä¸²è”èµ·æ¥ï¼Œä»¥è§£å†³å¤æ‚çš„ä»»åŠ¡ã€‚'
- en: '[**Memory**](https://python.langchain.com/en/latest/modules/memory.html): This
    module allows us to create a persisting state between calls of a model. Being
    able to use a model that remembers what has been said in the past will surely
    improve our application.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**è®°å¿†**](https://python.langchain.com/en/latest/modules/memory.html)ï¼šè¯¥æ¨¡å—å…è®¸æˆ‘ä»¬åœ¨æ¨¡å‹è°ƒç”¨ä¹‹é—´åˆ›å»ºæŒä¹…çŠ¶æ€ã€‚èƒ½å¤Ÿä½¿ç”¨ä¸€ä¸ªè®°ä½è¿‡å»æ‰€è¯´å†…å®¹çš„æ¨¡å‹ï¼Œæ— ç–‘ä¼šæé«˜æˆ‘ä»¬çš„åº”ç”¨ç¨‹åºçš„æ•ˆæœã€‚'
- en: '[**Agents**](https://python.langchain.com/en/latest/modules/agents.html): An
    agent is an LLM that makes a decision, takes an action, makes an observation about
    what it has done, and continues in this manner until it can complete its task.
    This module provides a set of agents that can be used.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**ä»£ç†**](https://python.langchain.com/en/latest/modules/agents.html)ï¼šä»£ç†æ˜¯ä¸€ä¸ª
    LLMï¼Œå®ƒåšå‡ºå†³ç­–ã€é‡‡å–è¡ŒåŠ¨ã€å¯¹è‡ªå·±æ‰€åšçš„äº‹æƒ…åšå‡ºè§‚å¯Ÿï¼Œå¹¶ä»¥è¿™ç§æ–¹å¼ç»§ç»­ï¼Œç›´åˆ°å®Œæˆä»»åŠ¡ã€‚è¯¥æ¨¡å—æä¾›äº†ä¸€ç»„å¯ä»¥ä½¿ç”¨çš„ä»£ç†ã€‚'
- en: Now letâ€™s go into a little more detail and see how to implement code by taking
    advantage of the different modules.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬æ›´è¯¦ç»†åœ°äº†è§£ä¸€ä¸‹å¦‚ä½•é€šè¿‡åˆ©ç”¨ä¸åŒçš„æ¨¡å—æ¥å®ç°ä»£ç ã€‚
- en: Models
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¨¡å‹
- en: '*Models* allows the use of three different types of language-models, which
    are:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ¨¡å‹* å…è®¸ä½¿ç”¨ä¸‰ç§ä¸åŒç±»å‹çš„è¯­è¨€æ¨¡å‹ï¼Œå®ƒä»¬æ˜¯ï¼š'
- en: '**Large Language Models (LLMs):** these foundational machine learning models
    that are able to understand natural language. These accept strings in input and
    generate strings in output.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼š** è¿™äº›æ˜¯èƒ½å¤Ÿç†è§£è‡ªç„¶è¯­è¨€çš„åŸºç¡€æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹æ¥å—å­—ç¬¦ä¸²ä½œä¸ºè¾“å…¥ï¼Œå¹¶ç”Ÿæˆå­—ç¬¦ä¸²ä½œä¸ºè¾“å‡ºã€‚'
- en: '**Chat Models:** models powered by LLM but are specialized to chat with the
    user. You can read more [here](https://medium.com/aiguys/reinforcement-learning-from-human-feedback-instructgpt-and-chatgpt-693d00cb9c58).'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**èŠå¤©æ¨¡å‹ï¼š** è¿™äº›æ¨¡å‹ç”± LLM æä¾›æ”¯æŒï¼Œä½†ä¸“é—¨ç”¨äºä¸ç”¨æˆ·èŠå¤©ã€‚ä½ å¯ä»¥åœ¨[è¿™é‡Œ](https://medium.com/aiguys/reinforcement-learning-from-human-feedback-instructgpt-and-chatgpt-693d00cb9c58)é˜…è¯»æ›´å¤šä¿¡æ¯ã€‚'
- en: '**Text Embedding Models:** these models are used to project textual data into
    a geometric space. These models take text as input and return a list of numbers,
    the embedding of the text.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ–‡æœ¬åµŒå…¥æ¨¡å‹ï¼š** è¿™äº›æ¨¡å‹ç”¨äºå°†æ–‡æœ¬æ•°æ®æŠ•å½±åˆ°å‡ ä½•ç©ºé—´ä¸­ã€‚è¿™äº›æ¨¡å‹å°†æ–‡æœ¬ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›ä¸€ä¸ªæ•°å­—åˆ—è¡¨ï¼Œå³æ–‡æœ¬çš„åµŒå…¥ã€‚'
- en: '![](../Images/81937b2be62eb65b9c3931398deda8af.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81937b2be62eb65b9c3931398deda8af.png)'
- en: Open AI API Key
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Open AI API å¯†é’¥
- en: Letâ€™s start using this module. First, we install and import the libraries. To
    use this library you will need an API KEY from the open AI site.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¼€å§‹ä½¿ç”¨è¿™ä¸ªæ¨¡å—ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å®‰è£…å¹¶å¯¼å…¥åº“ã€‚è¦ä½¿ç”¨è¿™ä¸ªåº“ï¼Œä½ éœ€è¦ä¸€ä¸ªæ¥è‡ª Open AI ç½‘ç«™çš„ API å¯†é’¥ã€‚
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now we are ready to instantiate a LLM model.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å‡†å¤‡å®ä¾‹åŒ–ä¸€ä¸ª LLM æ¨¡å‹ã€‚
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '***My Output****:* *A young woman went to a rave she had never heard of. She
    was the only person there with a light in the dark, and the only one who could
    see the good in people. She drank and danced and did anything to make it so. After
    hour after hour of dancing and drinkin'', she got to know one of the people in
    the party. He was a bit of a looking man, with a moustache and a goatee. She said,
    "Hi, I''m the only person there with a light in the dark." He said, "Hi, I''m
    the only person there with a light in the dark.*'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '***æˆ‘çš„è¾“å‡º****:* *ä¸€ä¸ªå¹´è½»çš„å¥³äººå»äº†ä¸€ä¸ªå¥¹ä»æœªå¬è¯´è¿‡çš„ç‹‚æ¬¢æ´¾å¯¹ã€‚å¥¹æ˜¯é‚£é‡Œå”¯ä¸€ä¸€ä¸ªåœ¨é»‘æš—ä¸­å¸¦æœ‰å…‰äº®çš„äººï¼Œä¹Ÿæ˜¯å”¯ä¸€ä¸€ä¸ªèƒ½å¤Ÿçœ‹åˆ°äººä»¬ç¾å¥½çš„ä¸€é¢çš„äººã€‚å¥¹å–é…’ã€è·³èˆï¼Œåšäº†æ‰€æœ‰å¯èƒ½çš„äº‹æƒ…æ¥è®©è¿™ä¸€åˆ‡å‘ç”Ÿã€‚åœ¨å‡ ä¸ªå°æ—¶çš„èˆè¹ˆå’Œé¥®é…’ä¹‹åï¼Œå¥¹è®¤è¯†äº†æ´¾å¯¹ä¸Šçš„ä¸€ä¸ªäººã€‚ä»–æ˜¯ä¸€ä¸ªæœ‰äº›å¸å¼•äººçš„ç”·äººï¼Œç•™ç€èƒ¡é¡»å’Œå±±ç¾Šèƒ¡ã€‚å¥¹è¯´ï¼šâ€œå—¨ï¼Œæˆ‘æ˜¯é‚£é‡Œå”¯ä¸€ä¸€ä¸ªåœ¨é»‘æš—ä¸­å¸¦æœ‰å…‰äº®çš„äººã€‚â€ä»–è¯´ï¼šâ€œå—¨ï¼Œæˆ‘æ˜¯é‚£é‡Œå”¯ä¸€ä¸€ä¸ªåœ¨é»‘æš—ä¸­å¸¦æœ‰å…‰äº®çš„äººã€‚â€*'
- en: With the generate() method you can also feed a list of input and receive multiple
    answers, letâ€™s see how.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ generate() æ–¹æ³•ï¼Œä½ è¿˜å¯ä»¥è¾“å…¥ä¸€ä¸ªåˆ—è¡¨å¹¶æ¥æ”¶å¤šä¸ªç­”æ¡ˆï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹æ€ä¹ˆåšã€‚
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/2a2fbc6563852fd6091ef5140276f8b6.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2a2fbc6563852fd6091ef5140276f8b6.png)'
- en: llm.generate output
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: llm.generate è¾“å‡º
- en: You can also extract some additional information about the results of the large
    language model.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ è¿˜å¯ä»¥æå–æœ‰å…³å¤§å‹è¯­è¨€æ¨¡å‹ç»“æœçš„ä¸€äº›é¢å¤–ä¿¡æ¯ã€‚
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '***My Output:*** *{â€˜token_usageâ€™: {â€˜completion_tokensâ€™: 527, â€˜total_tokensâ€™:
    544, â€˜prompt_tokensâ€™: 17}, â€˜model_nameâ€™: â€˜text-ada-001â€™}*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '***æˆ‘çš„è¾“å‡ºï¼š*** *{â€˜token_usageâ€™: {â€˜completion_tokensâ€™: 527, â€˜total_tokensâ€™: 544,
    â€˜prompt_tokensâ€™: 17}, â€˜model_nameâ€™: â€˜text-ada-001â€™}*'
- en: LLMs are not able to understand input texts that are too long. In particular,
    text that contains too many tokens (think about the syllables of words if you
    donâ€™t know what tokens are). Before passing the string into the model you can
    estimate the number of tokens with a simple method.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs æ— æ³•ç†è§£è¿‡é•¿çš„è¾“å…¥æ–‡æœ¬ã€‚ç‰¹åˆ«æ˜¯ï¼ŒåŒ…å«å¤ªå¤šæ ‡è®°çš„æ–‡æœ¬ï¼ˆå¦‚æœä½ ä¸çŸ¥é“æ ‡è®°æ˜¯ä»€ä¹ˆï¼Œå¯ä»¥è€ƒè™‘å•è¯çš„éŸ³èŠ‚ï¼‰ã€‚åœ¨å°†å­—ç¬¦ä¸²ä¼ é€’ç»™æ¨¡å‹ä¹‹å‰ï¼Œä½ å¯ä»¥ä½¿ç”¨ç®€å•çš„æ–¹æ³•ä¼°è®¡æ ‡è®°çš„æ•°é‡ã€‚
- en: In order to do this you need to install the [tiktoken](https://github.com/openai/tiktoken)
    library.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œä½ éœ€è¦å®‰è£…[tiktoken](https://github.com/openai/tiktoken)åº“ã€‚
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Prompts
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æç¤º
- en: Prompting is the new way of programming NLP models. Creating a good prompt though
    is not trivial. Asking the same thing in a different way can lead to a different
    result that is more or less accurate. The prompt can also change depending on
    the use case you are facing. Letâ€™s see how this module can help us create a good
    prompt.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æç¤ºæ˜¯ç¼–ç¨‹NLPæ¨¡å‹çš„æ–°æ–¹å¼ã€‚ç„¶è€Œï¼Œåˆ›å»ºä¸€ä¸ªå¥½çš„æç¤ºå¹¶éæ˜“äº‹ã€‚ä»¥ä¸åŒçš„æ–¹å¼æé—®å¯èƒ½ä¼šå¯¼è‡´ä¸åŒçš„ç»“æœï¼Œè¿™äº›ç»“æœå¯èƒ½æ›´å‡†ç¡®ä¹Ÿå¯èƒ½ä¸å‡†ç¡®ã€‚æç¤ºä¹Ÿå¯ä»¥æ ¹æ®ä½ æ‰€é¢å¯¹çš„ä½¿ç”¨æ¡ˆä¾‹è€Œæœ‰æ‰€ä¸åŒã€‚è®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªæ¨¡å—å¦‚ä½•å¸®åŠ©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå¥½çš„æç¤ºã€‚
- en: Prompt Template
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æç¤ºæ¨¡æ¿
- en: As the name implies, the prompt template allows us to create templates that
    we can reuse to ask things of our model. The template will contain variables that
    will be the only thing that the user will change from time to time to adapt the
    prompt to its particular use case.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚åç§°æ‰€ç¤ºï¼Œæç¤ºæ¨¡æ¿å…è®¸æˆ‘ä»¬åˆ›å»ºå¯ä»¥é‡å¤ä½¿ç”¨çš„æ¨¡æ¿ï¼Œä»¥ä¾¿å‘æˆ‘ä»¬çš„æ¨¡å‹æå‡ºé—®é¢˜ã€‚æ¨¡æ¿å°†åŒ…å«å˜é‡ï¼Œè¿™äº›å˜é‡æ˜¯ç”¨æˆ·ä¼šä¸æ—¶æ›´æ”¹çš„å”¯ä¸€å†…å®¹ï¼Œä»¥ä½¿æç¤ºé€‚åº”å…¶ç‰¹å®šçš„ä½¿ç”¨æ¡ˆä¾‹ã€‚
- en: Let us now see how they can be used.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å®ƒä»¬å¦‚ä½•è¢«ä½¿ç”¨ã€‚
- en: '[PRE9]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now we can replace the variable â€˜productâ€™ within the prompt with whatever string
    we want. This way we can customize the prompt any way we like.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥ç”¨æˆ‘ä»¬æƒ³è¦çš„ä»»ä½•å­—ç¬¦ä¸²æ›¿æ¢æç¤ºä¸­çš„å˜é‡â€˜productâ€™ã€‚è¿™æ ·æˆ‘ä»¬å°±å¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€è¦è‡ªå®šä¹‰æç¤ºã€‚
- en: '[PRE10]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '***My Outupt:*** *I want you to act as businessman. You have few passions in
    your life which are:- Money- Data- Basketball. I want to write a Medium Blog post
    about how to make a bucnh of money. What is a good for a title of such post?*'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '***æˆ‘çš„è¾“å‡ºï¼š*** *æˆ‘å¸Œæœ›ä½ å……å½“å•†äººã€‚ä½ ç”Ÿæ´»ä¸­æœ‰å‡ ä¸ªçƒ­æƒ…æ‰€åœ¨ï¼š- é’±- æ•°æ®- ç¯®çƒã€‚æˆ‘æƒ³å†™ä¸€ç¯‡å…³äºå¦‚ä½•èµšä¸€å¤§ç¬”é’±çš„Mediumåšå®¢æ–‡ç« ã€‚è¿™æ ·çš„æ–‡ç« æ ‡é¢˜åº”è¯¥æ˜¯ä»€ä¹ˆï¼Ÿ*'
- en: There are templates that have already been written and you can simply import
    them. To find out what they are you can look at the documentation. Letâ€™s try to
    import one now.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å·²ç»æœ‰ä¸€äº›æ¨¡æ¿è¢«ç¼–å†™å¥½ï¼Œä½ å¯ä»¥ç›´æ¥å¯¼å…¥å®ƒä»¬ã€‚è¦äº†è§£è¿™äº›æ¨¡æ¿æ˜¯ä»€ä¹ˆï¼Œä½ å¯ä»¥æŸ¥çœ‹æ–‡æ¡£ã€‚æˆ‘ä»¬ç°åœ¨æ¥å°è¯•å¯¼å…¥ä¸€ä¸ªã€‚
- en: '[PRE11]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '***My Outupt:*** *PromptTemplate(input_variables=[â€˜historyâ€™, â€˜inputâ€™], output_parser=None,
    partial_variables={}, template=â€™The following is a friendly conversation between
    a human and an AI. The AI is talkative and provides lots of specific details from
    its context. If the AI does not know the answer to a question, it truthfully says
    it does not know.\n\nCurrent conversation:\n{history}\nHuman: {input}\nAI:â€™, template_format=â€™f-stringâ€™,
    validate_template=True)*'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '***æˆ‘çš„è¾“å‡ºï¼š*** *PromptTemplate(input_variables=[â€˜historyâ€™, â€˜inputâ€™], output_parser=None,
    partial_variables={}, template=â€™ä»¥ä¸‹æ˜¯äººç±»å’ŒAIä¹‹é—´çš„å‹å¥½å¯¹è¯ã€‚AIå¾ˆå¥è°ˆï¼Œå¹¶ä»å…¶ä¸Šä¸‹æ–‡ä¸­æä¾›äº†è®¸å¤šå…·ä½“ç»†èŠ‚ã€‚å¦‚æœAIä¸çŸ¥é“é—®é¢˜çš„ç­”æ¡ˆï¼Œå®ƒä¼šå¦‚å®åœ°è¯´ä¸çŸ¥é“ã€‚\n\nå½“å‰å¯¹è¯ï¼š\n{history}\näººç±»ï¼š{input}\nAIï¼šâ€™,
    template_format=â€™f-stringâ€™, validate_template=True)*'
- en: In this template, we have multiple variables that we can fill in. One of them
    is *history*. We need the history to tell the template about something that happened
    previously so that it has more context. If we want to request the output of the
    model from this template, it is very simple.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªæ¨¡æ¿ä¸­ï¼Œæˆ‘ä»¬æœ‰å¤šä¸ªå¯ä»¥å¡«å†™çš„å˜é‡ã€‚å…¶ä¸­ä¹‹ä¸€æ˜¯*history*ã€‚æˆ‘ä»¬éœ€è¦å†å²è®°å½•æ¥å‘Šè¯‰æ¨¡æ¿ä¸€äº›å…ˆå‰å‘ç”Ÿçš„äº‹æƒ…ï¼Œä»¥ä¾¿å®ƒæœ‰æ›´å¤šçš„ä¸Šä¸‹æ–‡ã€‚å¦‚æœæˆ‘ä»¬æƒ³ä»è¿™ä¸ªæ¨¡æ¿ä¸­è¯·æ±‚æ¨¡å‹çš„è¾“å‡ºï¼Œè¿™éå¸¸ç®€å•ã€‚
- en: '[PRE12]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Few shot examples
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å°‘é‡ç¤ºä¾‹
- en: Sometimes it happens that we want to question the Machine Learning model about
    things that are particularly tricky. One way to get a more accurate answer, in
    this case, is to show the model similar examples of the correct answer and then
    ask it our question. LangChain provides a method for saving templates that are
    designed specifically to save these examples. Letâ€™s see how to do this.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰æ—¶å€™æˆ‘ä»¬æƒ³å‘æœºå™¨å­¦ä¹ æ¨¡å‹è¯¢é—®ç‰¹åˆ«æ£˜æ‰‹çš„äº‹æƒ…ã€‚æ­¤æ—¶ï¼Œè·å¾—æ›´å‡†ç¡®å›ç­”çš„ä¸€ç§æ–¹æ³•æ˜¯å‘æ¨¡å‹å±•ç¤ºæ­£ç¡®ç­”æ¡ˆçš„ç±»ä¼¼ç¤ºä¾‹ï¼Œç„¶åæå‡ºæˆ‘ä»¬çš„é—®é¢˜ã€‚LangChain æä¾›äº†ä¸€ç§ä¿å­˜ä¸“é—¨ç”¨äºä¿å­˜è¿™äº›ç¤ºä¾‹çš„æ¨¡æ¿çš„æ–¹æ³•ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹ã€‚
- en: 'First of all, letâ€™s create examples. In case I want to create the superlatives
    of the words: tall -> tallest.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€äº›ç¤ºä¾‹ã€‚å¦‚æœæˆ‘æƒ³åˆ›å»ºå•è¯çš„æœ€é«˜çº§ï¼štall -> tallestã€‚
- en: '[PRE13]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now we create the template as done before by including two variables, one for
    the base word and one for the superlative.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬åˆ›å»ºæ¨¡æ¿ï¼Œå¦‚ä¹‹å‰æ‰€åšçš„é‚£æ ·ï¼ŒåŒ…å«ä¸¤ä¸ªå˜é‡ï¼Œä¸€ä¸ªç”¨äºåŸºæœ¬è¯ï¼Œä¸€ä¸ªç”¨äºæœ€é«˜çº§è¯ã€‚
- en: '[PRE14]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Now we combine everything with the FewShotPromptTemplate class that takes as
    input the examples, the template, the prefix which is usually some instruction
    we want to give to the template, and a suffix which is the form of the template
    output.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å°†ä¸€åˆ‡ç»“åˆèµ·æ¥ï¼Œä½¿ç”¨FewShotPromptTemplateç±»ï¼Œå®ƒæ¥å—ç¤ºä¾‹ã€æ¨¡æ¿ã€å‰ç¼€ï¼ˆé€šå¸¸æ˜¯æˆ‘ä»¬å¸Œæœ›ç»™æ¨¡æ¿çš„æŒ‡ä»¤ï¼‰å’Œä¸€ä¸ªåç¼€ï¼ˆå³æ¨¡æ¿è¾“å‡ºçš„å½¢å¼ï¼‰ä½œä¸ºè¾“å…¥ã€‚
- en: '[PRE15]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '***My Output:*** *Give the superlative of every input Word: cool Superlative:
    coolest Word: tall Superlative: tallest Word: big Superlative:*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '***æˆ‘çš„è¾“å‡ºï¼š*** *ç»™å‡ºæ¯ä¸ªè¾“å…¥å•è¯çš„æœ€é«˜çº§ï¼šcool æœ€é«˜çº§ï¼šcoolest å•è¯ï¼štall æœ€é«˜çº§ï¼štallest å•è¯ï¼šbig æœ€é«˜çº§ï¼š*'
- en: Now you can feed the model and receive the actual output.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ å¯ä»¥è¾“å…¥æ¨¡å‹å¹¶è·å¾—å®é™…è¾“å‡ºã€‚
- en: '[PRE17]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Indexes
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç´¢å¼•
- en: This module is the one that allows us to interact with external documents that
    we want to feed to the model. This module is based on the concept of Retriever.
    In fact, the most common thing we want to do with this module is to go and fetch
    the document that most answers our query. Thus an information retrieval system.
    Letâ€™s see what the Retriever interface looks like to understand it better. ( For
    those who donâ€™t already know, an interface is a class that cannot be instantiated,
    if you want to learn more you can read my articles on [design patterns](https://medium.com/towards-data-science/design-patterns-with-python-for-machine-learning-engineers-observer-23cde7ecb2ed)).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¨¡å—å…è®¸æˆ‘ä»¬ä¸å¤–éƒ¨æ–‡æ¡£äº¤äº’ï¼Œæˆ‘ä»¬æƒ³è¦å°†å…¶æä¾›ç»™æ¨¡å‹ã€‚è¿™ä¸ªæ¨¡å—åŸºäºRetrieverçš„æ¦‚å¿µã€‚å®é™…ä¸Šï¼Œæˆ‘ä»¬æœ€å¸¸åšçš„å°±æ˜¯å»è·å–æœ€èƒ½å›ç­”æˆ‘ä»¬æŸ¥è¯¢çš„æ–‡æ¡£ã€‚å› æ­¤ï¼Œè¿™æ˜¯ä¸€ä¸ªä¿¡æ¯æ£€ç´¢ç³»ç»Ÿã€‚è®©æˆ‘ä»¬çœ‹çœ‹Retrieveræ¥å£çš„æ ·å­ï¼Œä»¥ä¾¿æ›´å¥½åœ°ç†è§£å®ƒã€‚ï¼ˆå¯¹äºé‚£äº›è¿˜ä¸çŸ¥é“çš„äººï¼Œæ¥å£æ˜¯ä¸€ä¸ªä¸èƒ½å®ä¾‹åŒ–çš„ç±»ï¼Œå¦‚æœä½ æƒ³äº†è§£æ›´å¤šï¼Œä½ å¯ä»¥é˜…è¯»æˆ‘å…³äº[è®¾è®¡æ¨¡å¼](https://medium.com/towards-data-science/design-patterns-with-python-for-machine-learning-engineers-observer-23cde7ecb2ed)çš„æ–‡ç« ã€‚ï¼‰
- en: '[PRE18]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The *get_relevant_documents* method is very simple, you just need to know how
    to read English to understand what it does. The string can be changed to your
    liking, so if you want to modify or implement a custom Retriever it is nothing
    too complicated.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*get_relevant_documents*æ–¹æ³•éå¸¸ç®€å•ï¼Œä½ åªéœ€äº†è§£å¦‚ä½•é˜…è¯»è‹±è¯­å³å¯ç†è§£å®ƒçš„ä½œç”¨ã€‚å­—ç¬¦ä¸²å¯ä»¥æ ¹æ®ä½ çš„å–œå¥½è¿›è¡Œæ›´æ”¹ï¼Œæ‰€ä»¥å¦‚æœä½ æƒ³ä¿®æ”¹æˆ–å®ç°ä¸€ä¸ªè‡ªå®šä¹‰çš„Retrieverï¼Œä¹Ÿæ²¡æœ‰ä»€ä¹ˆå¤æ‚çš„ã€‚'
- en: Letâ€™s look at a practical example now. We want to create a question-answering
    application about a specific document. That is, the model needs to be able to
    answer questions of mine about a specific document.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªå®é™…çš„ä¾‹å­ã€‚æˆ‘ä»¬æƒ³è¦åˆ›å»ºä¸€ä¸ªå…³äºç‰¹å®šæ–‡æ¡£çš„é—®ç­”åº”ç”¨ç¨‹åºã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæ¨¡å‹éœ€è¦èƒ½å¤Ÿå›ç­”æˆ‘å…³äºç‰¹å®šæ–‡æ¡£çš„é—®é¢˜ã€‚
- en: We first need to install *Chroma* which allows us to work with *Vectorstores*,
    weâ€™ll see later what it is for.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆéœ€è¦å®‰è£…*Chroma*ï¼Œå®ƒå…è®¸æˆ‘ä»¬ä¸*Vectorstores*ä¸€èµ·å·¥ä½œï¼Œæˆ‘ä»¬ç¨åä¼šçœ‹åˆ°å®ƒçš„ç”¨é€”ã€‚
- en: '[PRE19]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Letâ€™s import some classes we will need.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¯¼å…¥ä¸€äº›æˆ‘ä»¬éœ€è¦çš„ç±»ã€‚
- en: '[PRE20]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now letâ€™s download a txt document from the web.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬ä»ç½‘ç»œä¸Šä¸‹è½½ä¸€ä¸ªtxtæ–‡æ¡£ã€‚
- en: '[PRE21]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Letâ€™s load the document with the TextLoader.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä½¿ç”¨TextLoaderåŠ è½½æ–‡æ¡£ã€‚
- en: '[PRE22]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The *Retriever* always relies on what is called *Vectorstore* retriever. So
    we can instantiate one *Vectorstore retriever* and pass to it our text loader.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '*Retriever* æ€»æ˜¯ä¾èµ–äºæ‰€è°“çš„*Vectorstore*æ£€ç´¢å™¨ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å®ä¾‹åŒ–ä¸€ä¸ª*Vectorstore retriever*å¹¶å°†æˆ‘ä»¬çš„æ–‡æœ¬åŠ è½½å™¨ä¼ é€’ç»™å®ƒã€‚'
- en: '[PRE23]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Now that we finally have an index, we can ask questions about the data.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬ç»ˆäºæœ‰äº†ç´¢å¼•ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹æ•°æ®æé—®äº†ã€‚
- en: '[PRE24]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '***My Output:*** *Ohio Senator Sherrod Brown said, â€œItâ€™s time to bury the label
    â€˜Rust Belt.â€™â€*'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '***æˆ‘çš„è¾“å‡ºï¼š*** *ä¿„äº¥ä¿„å·å‚è®®å‘˜è°¢ç½—å¾·Â·å¸ƒæœ—è¯´ï¼Œâ€œæ˜¯æ—¶å€™åŸ‹è‘¬â€˜é“é”ˆå¸¦â€™è¿™ä¸ªæ ‡ç­¾äº†ã€‚â€*'
- en: Chains
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é“¾
- en: Chains allow us to create more complicated applications. **Often simply using
    an LLM is not enough**, we want to do more. For example, we want to first create
    a template and then give the compiled template as input to the LLM, this can be
    done simply with a chain. Itâ€™s easy for me to think of chains as the Pipeline
    that you use in scikit-learn, for example.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: é“¾å…è®¸æˆ‘ä»¬åˆ›å»ºæ›´å¤æ‚çš„åº”ç”¨ç¨‹åºã€‚**ä»…ä»…ä½¿ç”¨LLMé€šå¸¸æ˜¯ä¸å¤Ÿçš„**ï¼Œæˆ‘ä»¬æƒ³è¦åšå¾—æ›´å¤šã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ›å»ºä¸€ä¸ªæ¨¡æ¿ï¼Œç„¶åå°†ç¼–è¯‘å¥½çš„æ¨¡æ¿ä½œä¸ºè¾“å…¥æä¾›ç»™LLMï¼Œè¿™å¯ä»¥é€šè¿‡é“¾ç®€å•å®ç°ã€‚å¯¹æˆ‘æ¥è¯´ï¼Œå¯ä»¥å°†é“¾æƒ³è±¡æˆä½ åœ¨scikit-learnä¸­ä½¿ç”¨çš„Pipelineã€‚
- en: The LLMChain is a chain that does just that, takes an input, formats it within
    a template, and then passes it as input to a template.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: LLMChainå°±æ˜¯ä¸€ä¸ªè¿™æ ·çš„é“¾ï¼Œå®ƒæ¥å—è¾“å…¥ï¼Œå°†å…¶æ ¼å¼åŒ–åˆ°æ¨¡æ¿ä¸­ï¼Œç„¶åå°†å…¶ä½œä¸ºè¾“å…¥ä¼ é€’ç»™æ¨¡æ¿ã€‚
- en: First, letâ€™s create a simple template.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç®€å•çš„æ¨¡æ¿ã€‚
- en: '[PRE25]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Now we create a chain by specifying the template and model to be used.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬é€šè¿‡æŒ‡å®šè¦ä½¿ç”¨çš„æ¨¡æ¿å’Œæ¨¡å‹æ¥åˆ›å»ºä¸€ä¸ªé“¾ã€‚
- en: '[PRE26]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: You can also create a custom chain but Iâ€™ll write a more detailed article about
    it.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ä¹Ÿå¯ä»¥åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰é“¾ï¼Œä½†æˆ‘ä¼šå†™ä¸€ç¯‡æ›´è¯¦ç»†çš„æ–‡ç« æ¥è®²è§£ã€‚
- en: Memory
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å†…å­˜
- en: Every time we interact with a model, it will give us an answer that wonâ€™t depend
    on the context because it doesnâ€™t remember past events. Itâ€™s kind of like every
    time we are talking to a new model. **In applications we usually want the model
    to have some kind of memory so that it learns to reply to us by improving as it
    goes along depending on what we said before** especially if we are developing
    chatbots. Thatâ€™s what this module is for.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯æ¬¡æˆ‘ä»¬ä¸æ¨¡å‹äº’åŠ¨æ—¶ï¼Œå®ƒéƒ½ä¼šç»™æˆ‘ä»¬ä¸€ä¸ªç­”æ¡ˆï¼Œè¿™ä¸ªç­”æ¡ˆä¸ä¼šä¾èµ–äºä¸Šä¸‹æ–‡ï¼Œå› ä¸ºå®ƒä¸ä¼šè®°ä½è¿‡å»çš„äº‹ä»¶ã€‚è¿™å°±åƒæ¯æ¬¡æˆ‘ä»¬éƒ½æ˜¯åœ¨ä¸ä¸€ä¸ªæ–°çš„æ¨¡å‹äº¤è°ˆä¸€æ ·ã€‚**åœ¨åº”ç”¨ç¨‹åºä¸­ï¼Œæˆ‘ä»¬é€šå¸¸å¸Œæœ›æ¨¡å‹å…·å¤‡æŸç§è®°å¿†ï¼Œä»¥ä¾¿å®ƒé€šè¿‡æ ¹æ®ä¹‹å‰æˆ‘ä»¬è¯´çš„è¯ä¸æ–­æ”¹è¿›æ¥å­¦ä¹ å¦‚ä½•å›å¤æˆ‘ä»¬**ï¼Œç‰¹åˆ«æ˜¯å½“æˆ‘ä»¬åœ¨å¼€å‘èŠå¤©æœºå™¨äººæ—¶ã€‚è¿™å°±æ˜¯è¿™ä¸ªæ¨¡å—çš„ç”¨é€”ã€‚
- en: Memory can be implemented in a variety of ways. For example, we can take the
    previous N messages, and feed them to the model as a single string or as a string
    sequence for instance. Now letâ€™s look at the simplest type of memory we can implement
    called a buffer.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: å†…å­˜å¯ä»¥é€šè¿‡å¤šç§æ–¹å¼å®ç°ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥å°†ä¹‹å‰çš„Næ¡æ¶ˆæ¯ä½œä¸ºä¸€ä¸ªå­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åºåˆ—æä¾›ç»™æ¨¡å‹ã€‚ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬å¯ä»¥å®ç°çš„æœ€ç®€å•ç±»å‹çš„å†…å­˜ï¼Œç§°ä¸ºç¼“å†²åŒºã€‚
- en: We can use a ChatMessageHistory class that allows us to easily save all the
    messages sent to the model.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€ä¸ª ChatMessageHistory ç±»ï¼Œå®ƒå…è®¸æˆ‘ä»¬è½»æ¾ä¿å­˜æ‰€æœ‰å‘é€ç»™æ¨¡å‹çš„æ¶ˆæ¯ã€‚
- en: '[PRE27]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Now we can retrieve the messages easily.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥è½»æ¾æ£€ç´¢æ¶ˆæ¯ã€‚
- en: '[PRE29]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Now that we understand the concept, we can use a wrapper of the class we just
    used called ConversationBufferMemory that allows us to actually use the message
    history.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬ç†è§£äº†è¿™ä¸ªæ¦‚å¿µï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æˆ‘ä»¬åˆšåˆšä½¿ç”¨çš„ç±»çš„åŒ…è£…å™¨ï¼Œç§°ä¸º ConversationBufferMemoryï¼Œå®ƒå…è®¸æˆ‘ä»¬å®é™…ä½¿ç”¨æ¶ˆæ¯å†å²è®°å½•ã€‚
- en: '[PRE30]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Letâ€™s see finally how to use this feature in a chain for a chat.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹å¦‚ä½•åœ¨å¯¹è¯é“¾ä¸­ä½¿ç”¨è¿™ä¸ªåŠŸèƒ½ã€‚
- en: So letâ€™s start by having a conversation with the model.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è®©æˆ‘ä»¬å¼€å§‹ä¸æ¨¡å‹è¿›è¡Œå¯¹è¯ã€‚
- en: '[PRE33]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '![](../Images/7836436ee44205cce342194f148231e1.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7836436ee44205cce342194f148231e1.png)'
- en: '[PRE35]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![](../Images/a108320e04ad98ef250e62891677400d.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a108320e04ad98ef250e62891677400d.png)'
- en: '[PRE36]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '![](../Images/52ae722345c2a49f47f024a439091eb0.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/52ae722345c2a49f47f024a439091eb0.png)'
- en: You can retrieve the old messages by accessing the memory.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥é€šè¿‡è®¿é—®å†…å­˜æ¥æ£€ç´¢æ—§æ¶ˆæ¯ã€‚
- en: '[PRE37]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Now you can save your messages to load them again and feed the model when you
    want to start a conversation from a certain point.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ å¯ä»¥ä¿å­˜ä½ çš„æ¶ˆæ¯ï¼Œä»¥ä¾¿åœ¨ä½ æƒ³ä»æŸä¸ªç‰¹å®šç‚¹å¼€å§‹å¯¹è¯æ—¶é‡æ–°åŠ è½½å®ƒä»¬å¹¶æä¾›ç»™æ¨¡å‹ã€‚
- en: Agents
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»£ç†
- en: The kind of chains we have seen, follow predetermined steps like a pipeline.
    But we often donâ€™t know what steps the model has to take to answer a given question,
    because it also depends on what the user answers it from time to time.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çœ‹åˆ°çš„é‚£ç§é“¾æ¡ï¼Œéµå¾ªåƒæµæ°´çº¿ä¸€æ ·çš„é¢„å®šæ­¥éª¤ã€‚ä½†æ˜¯æˆ‘ä»¬é€šå¸¸ä¸çŸ¥é“æ¨¡å‹å›ç­”ç‰¹å®šé—®é¢˜æ—¶éœ€è¦é‡‡å–å“ªäº›æ­¥éª¤ï¼Œå› ä¸ºè¿™ä¹Ÿå–å†³äºç”¨æˆ·ä¸æ—¶ç»™å‡ºçš„å›ç­”ã€‚
- en: We can have a model use various tools, to improve its answers. A common example
    is to first go and read some information on Wikipedia and then answer a particular
    question.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥è®©æ¨¡å‹ä½¿ç”¨å„ç§å·¥å…·æ¥æ”¹è¿›å…¶å›ç­”ã€‚ä¸€ä¸ªå¸¸è§çš„ä¾‹å­æ˜¯é¦–å…ˆå»ç»´åŸºç™¾ç§‘ä¸Šé˜…è¯»ä¸€äº›ä¿¡æ¯ï¼Œç„¶åå›ç­”ä¸€ä¸ªç‰¹å®šçš„é—®é¢˜ã€‚
- en: '[PRE38]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We are going to use two tools in particular SERPAPI which allows the model to
    do browser searches and take information, llm-math to improve its math skills.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨ä¸¤ä¸ªç‰¹å®šçš„å·¥å…·ï¼ŒSERPAPIï¼Œå®ƒå…è®¸æ¨¡å‹è¿›è¡Œæµè§ˆå™¨æœç´¢å¹¶è·å–ä¿¡æ¯ï¼Œä»¥åŠllm-mathï¼Œä»¥æé«˜å…¶æ•°å­¦æŠ€èƒ½ã€‚
- en: To install SERPAPI you must register on the site and copy the API Token.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å®‰è£…SERPAPIï¼Œä½ å¿…é¡»åœ¨ç½‘ç«™ä¸Šæ³¨å†Œå¹¶å¤åˆ¶API Tokenã€‚
- en: 'This is the website: [https://serpapi.com/](https://serpapi.com/)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ç½‘ç«™ï¼š[https://serpapi.com/](https://serpapi.com/)
- en: Once done we install the library and set the token as an environment variable.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæˆåï¼Œæˆ‘ä»¬å®‰è£…åº“å¹¶å°†ä»¤ç‰Œè®¾ç½®ä¸ºç¯å¢ƒå˜é‡ã€‚
- en: '[PRE39]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Now we are ready to instantiate our agent with the tools it will need to use.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½ä½¿ç”¨å®ƒæ‰€éœ€çš„å·¥å…·æ¥å®ä¾‹åŒ–æˆ‘ä»¬çš„ä»£ç†äº†ã€‚
- en: 'We, therefore, need 3 things:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦3æ ·ä¸œè¥¿ï¼š
- en: 'LLM: the large language model we want to use'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMï¼šæˆ‘ä»¬æƒ³è¦ä½¿ç”¨çš„å¤§å‹è¯­è¨€æ¨¡å‹
- en: 'Tools: we want to use to improve the basic LLM'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å·¥å…·ï¼šæˆ‘ä»¬å¸Œæœ›ç”¨æ¥æ”¹è¿›åŸºæœ¬LLMçš„å·¥å…·
- en: 'Agent: handles the interaction between the LLM and Tools'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»£ç†ï¼šå¤„ç†LLMä¸å·¥å…·ä¹‹é—´çš„äº’åŠ¨
- en: '[PRE40]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Now we can ask our model a question, relying on the fact that it is going to
    use the various tools available to it to answer us.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥å‘æ¨¡å‹æé—®ï¼Œä¾é å®ƒå°†ä½¿ç”¨å„ç§å¯ç”¨çš„å·¥å…·æ¥å›ç­”æˆ‘ä»¬ã€‚
- en: '[PRE43]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Final Thoughts
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: In this article, we introduced LangChain with its various modules. Each module
    is useful for improving the capabilities of large language models and is essential
    for developing applications based on these models. Be careful because the model
    I used in this article is not among the best so the answers may not be optimal,
    plus the answers you get may be very different from mine. The purpose though was
    just to get familiar with this library. I am curious to see all the applications
    that will be developed in the future based on the new Large Language Models. Follow
    me to read my upcoming in-depth articles on these topics! [ğŸ˜‰](https://emojipedia.org/it/apple/ios-15.4/faccina-che-fa-l-occhiolino/)
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†LangChainåŠå…¶å„ç§æ¨¡å—ã€‚æ¯ä¸ªæ¨¡å—éƒ½å¯¹æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›æœ‰ç”¨ï¼Œå¹¶ä¸”å¯¹äºåŸºäºè¿™äº›æ¨¡å‹å¼€å‘åº”ç”¨ç¨‹åºè‡³å…³é‡è¦ã€‚è¯·æ³¨æ„ï¼Œå› ä¸ºæˆ‘åœ¨æœ¬æ–‡ä¸­ä½¿ç”¨çš„æ¨¡å‹ä¸æ˜¯æœ€å¥½çš„ï¼Œæ‰€ä»¥å›ç­”å¯èƒ½ä¸æ˜¯æœ€ä¼˜çš„ï¼Œè€Œä¸”ä½ å¾—åˆ°çš„å›ç­”å¯èƒ½ä¸æˆ‘çš„å·®å¼‚å¾ˆå¤§ã€‚ä¸è¿‡ï¼Œç›®çš„æ˜¯ä¸ºäº†ç†Ÿæ‚‰è¿™ä¸ªåº“ã€‚æˆ‘å¾ˆæœŸå¾…çœ‹åˆ°æœªæ¥åŸºäºæ–°å¤§å‹è¯­è¨€æ¨¡å‹å¼€å‘çš„æ‰€æœ‰åº”ç”¨ç¨‹åºã€‚å…³æ³¨æˆ‘ï¼Œé˜…è¯»æˆ‘å³å°†å‘å¸ƒçš„å…³äºè¿™äº›è¯é¢˜çš„æ·±å…¥æ–‡ç« ï¼
    [ğŸ˜‰](https://emojipedia.org/it/apple/ios-15.4/faccina-che-fa-l-occhiolino/)
- en: The End
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“æŸ
- en: '*Marcello Politi*'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*Marcello Politi*'
- en: '[Linkedin](https://www.linkedin.com/in/marcello-politi/), [Twitter](https://twitter.com/_March08_),
    [Website](https://marcello-politi.super.site/)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[Linkedin](https://www.linkedin.com/in/marcello-politi/)ï¼Œ[Twitter](https://twitter.com/_March08_)ï¼Œ
    [Website](https://marcello-politi.super.site/)'
