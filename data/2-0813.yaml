- en: 'Ensemble Learning with Scikit-Learn: A Friendly Introduction'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Scikit-Learn 进行集成学习：友好的介绍
- en: 原文：[https://towardsdatascience.com/ensemble-learning-with-scikit-learn-a-friendly-introduction-5dd64650de6c](https://towardsdatascience.com/ensemble-learning-with-scikit-learn-a-friendly-introduction-5dd64650de6c)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/ensemble-learning-with-scikit-learn-a-friendly-introduction-5dd64650de6c](https://towardsdatascience.com/ensemble-learning-with-scikit-learn-a-friendly-introduction-5dd64650de6c)
- en: Ensemble learning algorithms like XGBoost or Random Forests are among the top-performing
    models in Kaggle competitions. How do they work?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 像 XGBoost 或随机森林这样的集成学习算法是 Kaggle 比赛中的顶尖模型之一。它们是如何工作的？
- en: '[](https://medium.com/@riccardo.andreoni?source=post_page-----5dd64650de6c--------------------------------)[![Riccardo
    Andreoni](../Images/5e22581e419639b373019a809d6e65c1.png)](https://medium.com/@riccardo.andreoni?source=post_page-----5dd64650de6c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5dd64650de6c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5dd64650de6c--------------------------------)
    [Riccardo Andreoni](https://medium.com/@riccardo.andreoni?source=post_page-----5dd64650de6c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@riccardo.andreoni?source=post_page-----5dd64650de6c--------------------------------)[![Riccardo
    Andreoni](../Images/5e22581e419639b373019a809d6e65c1.png)](https://medium.com/@riccardo.andreoni?source=post_page-----5dd64650de6c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5dd64650de6c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5dd64650de6c--------------------------------)
    [Riccardo Andreoni](https://medium.com/@riccardo.andreoni?source=post_page-----5dd64650de6c--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5dd64650de6c--------------------------------)
    ·7 min read·Sep 9, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5dd64650de6c--------------------------------)
    ·阅读时长7分钟·2023年9月9日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/d2fbdc1ade153ba4bfe5318e61d89304.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d2fbdc1ade153ba4bfe5318e61d89304.png)'
- en: 'Source: [unsplash.com](https://unsplash.com/photos/0u_vbeOkMpk)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[unsplash.com](https://unsplash.com/photos/0u_vbeOkMpk)
- en: Fundamental learning algorithms as logistic regression or linear regression
    are often too simple to achieve adequate results for a machine learning problem.
    While a possible solution is to use neural networks, they require a vast amount
    of training data, which is rarely available. Ensemble learning techniques can
    boost the performance of simple models even with a limited amount of data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 基础学习算法如逻辑回归或线性回归通常过于简单，无法为机器学习问题提供足够的结果。虽然使用神经网络可能是一种解决方案，但它们需要大量的训练数据，这种数据很少有。集成学习技术可以在数据量有限的情况下提升简单模型的性能。
- en: Imagine asking a person to guess how many jellybeans there are inside a big
    jar. One person's answer will unlikely be a precise estimate of the correct number.
    Instead, if we ask a thousand people the same question, the average answer will
    likely be close to the actual number. This phenomenon is called the [*wisdom of
    the crowd*](https://en.wikipedia.org/wiki/Wisdom_of_the_crowd)[1]. When dealing
    with complex estimation tasks, the crowd can be considerably more precise than
    an individual.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，让一个人猜测一个大罐子里有多少颗糖豆。一个人的答案不太可能是准确的估计数字。相反，如果我们问一千个人同样的问题，平均答案可能会接近实际数字。这种现象被称为[*集体智慧*](https://en.wikipedia.org/wiki/Wisdom_of_the_crowd)[1]。在处理复杂的估计任务时，集体往往比个体更为精准。
- en: Ensemble learning algorithms take advantage of this simple principle by aggregating
    the predictions of a group of models, like regressors or classifiers. For an aggregation
    of classifiers, the ensemble model could simply pick the most common class between
    the predictions of the low-level classifiers. Instead, the ensemble can use the
    mean or the median of all the predictions for a regression task.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习算法利用这一简单原理，通过聚合一组模型（如回归模型或分类器）的预测来提升性能。对于分类器的聚合，集成模型可以简单地选择低级分类器预测中最常见的类别。相反，对于回归任务，集成模型可以使用所有预测的均值或中位数。
- en: '![](../Images/2bbd943e229c446136d0adc9f5d9e60a.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2bbd943e229c446136d0adc9f5d9e60a.png)'
- en: Image by the author.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于作者。
- en: By aggregating a large number of weak learners, i.e. classifiers or regressors
    which are only slightly better than random guessing, we can achieve unthinkable
    results. Consider a binary classification task. By aggregating 1000 independent
    classifiers with individual accuracy of 51% we can create an ensemble achieving
    an accuracy of 75% [2].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 通过集成大量的弱学习者，即那些仅比随机猜测略好一点的分类器或回归器，我们可以取得难以想象的结果。考虑一个二分类任务。通过集成 1000 个独立的分类器，每个分类器的准确率为
    51%，我们可以创建一个准确率为 75% 的集成模型 [2]。
- en: This is the reason why ensemble algorithms are often the winning solutions in
    many machine-learning competitions!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么集成算法在许多机器学习竞赛中常常获胜的原因！
- en: There exist several techniques to build an ensemble learning algorithm. The
    principal ones are bagging, boosting, and stacking. In the following sections,
    I briefly describe each of these principles and present the machine learning algorithms
    to implement them.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现有几种技术可以构建集成学习算法。主要有 bagging、boosting 和 stacking。以下部分我将简要描述每一种原理，并介绍实现这些原理的机器学习算法。
- en: Bagging
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Bagging
- en: The first technique to form an ensemble algorithm is called bagging, which is
    the abbreviation of *bootstrap aggregating*. The core idea is to provide each
    weak learner with a slightly different training set. This is done by randomly
    sampling the original training set.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 形成集成算法的第一种技术称为 bagging，它是*bootstrap aggregating*的缩写。核心思想是为每个弱学习者提供一个略有不同的训练集。这是通过随机抽样原始训练集来完成的。
- en: If the sampling is done with replacement, the technique is called *bagging*,
    otherwise, if the sampling is done without replacement, the technique is properly
    called *pasting*.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果采样是有放回的，这种技术称为*bagging*，否则如果采样是没有放回的，这种技术则被称为*pasting*。
- en: The key idea of bagging (and pasting) is to create weak learners as diverse
    as possible, and this is done by training them with randomly generated training
    sets. As the sampling is performed with repetition, bagging introduces slightly
    more diversity than pasted and for this reason, is typically preferred.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: bagging（套袋）和 pasting（粘贴）的关键思想是尽可能创建多样化的弱学习者，这通过使用随机生成的训练集来完成。由于采样是有放回的，bagging
    引入了比 pasting 略多的多样性，因此通常更受青睐。
- en: 'Let’s see how bagging works through an example. Suppose the original training
    set is made of 10 examples, and we want to build an ensemble consisting of 3 different
    weak learners. Moreover, we want to train each learner on a subset of the original
    training set of dimension 5\. The following picture illustrates how the training
    set could be divided:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来看看 bagging 的工作原理。假设原始训练集包含 10 个示例，我们想要构建一个由 3 个不同的弱学习者组成的集成。此外，我们希望在原始训练集的一个维度为
    5 的子集上训练每个学习者。以下图片展示了训练集的可能划分方式：
- en: '![](../Images/286887e1e529db33babfbc12a002f112.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/286887e1e529db33babfbc12a002f112.png)'
- en: Image by the author.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: 'Pasting instead doesn’t allow the repetition of the same training example in
    a model’s training subset:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 而 pasting 不允许在模型的训练子集中重复相同的训练示例：
- en: '![](../Images/7d4b3bec9175022557b1fbe86300e509.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7d4b3bec9175022557b1fbe86300e509.png)'
- en: Image by the author.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: Random Forests
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林
- en: The most common bagging model is *Random Forests*. A Random Forest is an ensemble
    of decision trees, each one trained on a slightly different training set.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的 bagging 模型是*随机森林*。随机森林是决策树的集成，每棵树都在一个略有不同的训练集上训练。
- en: For each node of each decision tree in a Random Forest, the algorithm randomly
    selects the features between which to search for the best possible split. In other
    words, the algorithm doesn’t look for the best possible split among all the features,
    but instead it searches for the best possible split among a subset of the available
    features. This is the explanation of the name “random”.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于随机森林中每棵决策树的每个节点，算法会随机选择特征来搜索最佳的分裂点。换句话说，算法不会在所有特征中寻找最佳的分裂点，而是在一部分可用特征中寻找最佳的分裂点。这就是“随机”一词的解释。
- en: The following snippet shows how to create and fit a Random Forest Classifier
    on some training data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了如何在一些训练数据上创建和拟合一个随机森林分类器。
- en: Some attributes are extremely important and I suggest tuning them through a
    grid search approach. **n_estimators** defines the number of weak learners, **max_features**
    is the number of features to be considered at each split, **max_depth** is the
    maximum depth of the trees. These hyperparameters play a fundamental role in regularization.
    Increasing **n_estimators** and **min_sample_leaf** or reducing **max_features**
    and **max_depth** helps to create trees as diverse as possible, and consequently
    to avoid overfitting.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一些属性非常重要，我建议通过网格搜索方法对其进行调优。**n_estimators**定义了弱学习器的数量，**max_features**是每次分裂时考虑的特征数量，**max_depth**是树的最大深度。这些超参数在正则化中起着基础性作用。增加**n_estimators**和**min_sample_leaf**或减少**max_features**和**max_depth**有助于创建尽可能多样化的树，从而避免过拟合。
- en: For all the other attributes check the comprehensive [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有其他属性，请查看全面的[文档](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)。
- en: Extra Trees
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 极端随机树（Extra Trees）
- en: In order to increase even more the randomness of the trees, we can use the Extremely
    Randomized Trees algorithm, Extra Trees for short. It works similarly to Random
    Forests, but at each node of each decision tree, the algorithm searches for the
    best possible split within random thresholds for each feature. Random Forest,
    instead, searches for the best possible split among the whole value range of the
    features.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步增加树的随机性，我们可以使用极端随机树算法，简称为极端随机树（Extra Trees）。它的工作方式类似于随机森林，但在每个决策树的每个节点，算法在每个特征的随机阈值范围内寻找最佳可能的分裂。而随机森林则在特征的整个值范围内寻找最佳分裂。
- en: Extra Trees has one additional random component with respect to Random Forest.
    For this reason, it trades more bias for a lower variance [2].
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于随机森林，极端随机树有一个额外的随机组件。因此，它用更高的偏差换取了较低的方差[2]。
- en: The second advantage of Extra Trees is that its training is faster than Random
    Forest as it doesn’t have to look for the best possible split in the entire value
    domain of the selected features. Instead, it considers only a portion of it.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 极端随机树的第二个优势是训练比随机森林更快，因为它不需要在选定特征的整个值域中寻找最佳分裂。相反，它仅考虑其中的一部分。
- en: 'The implementation of an Extra Trees regressor or classifier with Scikit-Learn
    is as simple as Random Forest’s implementation:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Scikit-Learn实现极端随机树回归器或分类器与实现随机森林一样简单：
- en: The parameters are the same as for the Random Forest model.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 参数与随机森林模型相同。
- en: It is generally very difficult to predict whether Random Forest or Extra Trees
    work better for a given task. For this reason, I suggest training both of them
    and comparing them later.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通常很难预测随机森林（Random Forest）或极端随机树（Extra Trees）对于特定任务的表现哪个更好。因此，我建议同时训练这两种模型，并在之后进行比较。
- en: Feature Importance
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征重要性
- en: 'A nice aspect of both Random Forest and Extra Trees is that they provide a
    measure of the ability of each feature to reduce the impurity of the dataset:
    this is called *feature importance*. In other words, a feature with an high importance
    is able to provide better insights for the classification or regression problem
    than a feature with a low importance.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林和极端随机树的一个优点是它们提供了每个特征减少数据集纯度的能力的度量：这称为*特征重要性*。换句话说，一个重要性高的特征比一个重要性低的特征能提供更好的分类或回归问题的见解。
- en: After training a model, we can access this measure with the attribute `**feature_importances_**`.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型后，我们可以通过属性`**feature_importances_**`访问这个度量。
- en: '![](../Images/5c09688db32363bd4f4c6dfc301a86f6.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5c09688db32363bd4f4c6dfc301a86f6.png)'
- en: Image by the author.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: Once the model is trained, Scikit-Learn automates the computation of the features'
    importance. The output score is scaled into the [0,1] range, meaning that scores
    closer to 1 are assigned to the most important features.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，Scikit-Learn 会自动计算特征的重要性。输出分数被缩放到[0,1]范围内，意味着接近1的分数分配给最重要的特征。
- en: In the example above, the features ‘petal length’ and ‘petal width’ receive
    a substantially higher score than the others. While ‘sepal length’ has moderate
    importance for our model, the feature ‘sepal width’ appears to be useless for
    this classification task.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的例子中，‘花瓣长度’和‘花瓣宽度’的分数显著高于其他特征。虽然‘萼片长度’对我们的模型有中等重要性，但‘萼片宽度’在这个分类任务中似乎毫无用处。
- en: Boosting
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升法
- en: Moving on to another noteworthy ensemble technique, we dive into the boosting
    techniques. This method stands as a potent counterpart to bagging in the world
    of machine learning.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 继续探讨另一种值得注意的集成技术，我们深入了解 boosting 技术。这种方法在机器学习领域是 bagging 的有力对手。
- en: Boosting, short for hypothesis boosting, follows an interesting philosophy.
    Instead of creating diverse learners through random sampling, boosting focuses
    on enhancing the performance of individual weak learners by training predictors
    sequentialy. Each predictor aims at correcting its predecessor. It’s like a coach
    putting extra practice into a player’s weak spots to make them an all-around star.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Boosting，即假设提升，遵循一种有趣的理念。它不是通过随机抽样创建多样化的学习者，而是通过顺序训练预测器来提升单个弱学习者的表现。每个预测器旨在纠正其前一个预测器的错误。这就像教练在球员的弱点上加练，以使他们成为全能明星。
- en: Boosting, with its emphasis on self-improvement and gradual refinement, often
    outperforms bagging techniques in scenarios where precision and attention to detail
    are predominant.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Boosting 由于其强调自我改进和渐进细化，通常在对精度和细节关注的场景中表现优于 bagging 技术。
- en: 'We’ll now see the most famous and awarded boosting ensemble technique: XGBoost.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将看到最著名且获奖的 boosting 集成技术：XGBoost。
- en: XGBoost
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XGBoost
- en: XGBoost, short for Extreme Gradient Boosting, is an ensemble learning method
    that employs gradient descent optimization and it is designed to improve the predictive
    performance of weak learners systematically.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost，全称极端梯度提升，是一种集成学习方法，采用梯度下降优化，旨在系统性地提高弱学习者的预测性能。
- en: In XGBoost, each weak learner is assigned a weight based on their error rate.
    Learners that perform poorly on specific instances are given higher weights to
    prioritize the corection of those errors. This iterative process continues, with
    the model adapting its focus to address the most challenging data points.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在 XGBoost 中，每个弱学习者根据其错误率被分配一个权重。表现不佳的学习者会被赋予更高的权重，以优先纠正这些错误。这个迭代过程持续进行，模型调整其重点以解决最具挑战的数据点。
- en: XGBoost is a favored choice among data scientists and machine learning practitioners
    for its ability to extract valuable insights and deliver superior predictive accuracy.
    It is also largely employed in ML competitions due to its speed and scalability.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 是数据科学家和机器学习从业者的热门选择，因为它能够提取有价值的见解并提供卓越的预测准确性。它也因其速度和可扩展性而被广泛应用于 ML 竞赛中。
- en: 'Here we can see how to apply XGBoost to real data:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们可以看到如何将 XGBoost 应用到实际数据中：
- en: Without hyperparameters tuning, the model is able to achieve a 98% accuracy.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有调整超参数的情况下，模型能够实现 98% 的准确率。
- en: Conclusion
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: As we conclude this introductory journey on ensemble algorithms in machine learning,
    it’s important to recognize that our journey has only laid the foundation for
    deeper and more specialized investigations. While we’ve touched upon some fundamental
    concepts, the field extends far beyond these introductory insights.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们结束这次关于机器学习中集成算法的初步旅程时，重要的是要认识到我们的旅程仅仅奠定了更深层次和更专业研究的基础。虽然我们触及了一些基本概念，但这一领域远远超出了这些入门见解。
- en: I recommend digging into the resources and references attached to this article.
    These sources offer detailed insights into advanced ensemble methodologies, algorithm
    optimizations, and practical implementation tips.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我推荐深入研究附带的资源和参考文献。这些来源提供了有关高级集成方法、算法优化和实际实施技巧的详细见解。
- en: If you liked this story, consider following me to be notified of my upcoming
    projects and articles!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这个故事，请考虑关注我，以便接收我即将发布的项目和文章的通知！
- en: 'Here are some of my past projects:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我过去的一些项目：
- en: '[](/euro-trip-optimization-genetic-algorithms-and-google-maps-python-api-solve-the-traveling-salesman-4ad8e1548207?source=post_page-----5dd64650de6c--------------------------------)
    [## Euro Trip Optimization: Genetic Algorithms and Google Maps API Solve the Traveling
    Salesman Problem'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/euro-trip-optimization-genetic-algorithms-and-google-maps-python-api-solve-the-traveling-salesman-4ad8e1548207?source=post_page-----5dd64650de6c--------------------------------)
    [## 欧洲旅行优化：遗传算法和 Google Maps API 解决旅行推销员问题'
- en: Navigate the charm of Europe’s 50 most visited cities using genetic algorithms
    and Google Maps API, unlocking efficient…
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用遗传算法和 Google Maps API 浏览欧洲 50 个最受欢迎城市的魅力，开启高效…
- en: towardsdatascience.com](/euro-trip-optimization-genetic-algorithms-and-google-maps-python-api-solve-the-traveling-salesman-4ad8e1548207?source=post_page-----5dd64650de6c--------------------------------)
    [](/building-a-convolutional-neural-network-from-scratch-using-numpy-a22808a00a40?source=post_page-----5dd64650de6c--------------------------------)
    [## Building a Convolutional Neural Network from Scratch using Numpy
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/euro-trip-optimization-genetic-algorithms-and-google-maps-python-api-solve-the-traveling-salesman-4ad8e1548207?source=post_page-----5dd64650de6c--------------------------------)
    [](/building-a-convolutional-neural-network-from-scratch-using-numpy-a22808a00a40?source=post_page-----5dd64650de6c--------------------------------)
    [## 从头开始使用 Numpy 构建卷积神经网络
- en: As Computer Vision applications are becoming omnipresent in our lives, understanding
    the functioning principles of…
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随着计算机视觉应用在我们生活中的普及，理解其工作原理...
- en: towardsdatascience.com](/building-a-convolutional-neural-network-from-scratch-using-numpy-a22808a00a40?source=post_page-----5dd64650de6c--------------------------------)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/building-a-convolutional-neural-network-from-scratch-using-numpy-a22808a00a40?source=post_page-----5dd64650de6c--------------------------------)
- en: References
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] [The Wisdom of the Crowd — University of Cambridge, Faculty of Mathematics](https://nrich.maths.org/9601)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [群体智慧 — 剑桥大学数学系](https://nrich.maths.org/9601)'
- en: '[2] [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd
    Edition — Aurélien Géron](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [使用 Scikit-Learn、Keras 和 TensorFlow 的动手机器学习（第2版）— Aurélien Géron](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)'
- en: '[3] [Python Data Science Handbook Essential Tools for Working with Data](https://jakevdp.github.io/PythonDataScienceHandbook/)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [Python 数据科学手册 数据处理的必要工具](https://jakevdp.github.io/PythonDataScienceHandbook/)'
- en: '[4] [XGBoost documentation](https://xgboost.readthedocs.io/en/stable/index.html)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [XGBoost 文档](https://xgboost.readthedocs.io/en/stable/index.html)'
- en: '[5] [Ensemble Methods Foundations and Algorithms](http://didattica.cs.unicam.it/old/lib/exe/fetch.php?media=didattica%3Amagistrale%3Aml%3Aay_1920%3Aensemble_methods_zhou.pdf)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] [集成方法基础与算法](http://didattica.cs.unicam.it/old/lib/exe/fetch.php?media=didattica%3Amagistrale%3Aml%3Aay_1920%3Aensemble_methods_zhou.pdf)'
