- en: Semantic Textual Similarity with BERT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用BERT进行语义文本相似度分析
- en: 原文：[https://towardsdatascience.com/semantic-textual-similarity-with-bert-fc800656e7a3](https://towardsdatascience.com/semantic-textual-similarity-with-bert-fc800656e7a3)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/semantic-textual-similarity-with-bert-fc800656e7a3](https://towardsdatascience.com/semantic-textual-similarity-with-bert-fc800656e7a3)
- en: How to use BERT to calculate the semantic similarity between two texts
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用BERT计算两段文本之间的语义相似度
- en: '[](https://medium.com/@marcellusruben?source=post_page-----fc800656e7a3--------------------------------)[![Ruben
    Winastwan](../Images/15ad0dd03bf5892510abdf166a1e91e1.png)](https://medium.com/@marcellusruben?source=post_page-----fc800656e7a3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fc800656e7a3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fc800656e7a3--------------------------------)
    [Ruben Winastwan](https://medium.com/@marcellusruben?source=post_page-----fc800656e7a3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@marcellusruben?source=post_page-----fc800656e7a3--------------------------------)[![Ruben
    Winastwan](../Images/15ad0dd03bf5892510abdf166a1e91e1.png)](https://medium.com/@marcellusruben?source=post_page-----fc800656e7a3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fc800656e7a3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fc800656e7a3--------------------------------)
    [Ruben Winastwan](https://medium.com/@marcellusruben?source=post_page-----fc800656e7a3--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fc800656e7a3--------------------------------)
    ·11 min read·Feb 15, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fc800656e7a3--------------------------------)
    ·阅读时间11分钟·2023年2月15日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/618fe6986fde72b9eaae6d126ce78716.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/618fe6986fde72b9eaae6d126ce78716.png)'
- en: 'Photo by Leeloo Thefirst: [https://www.pexels.com/photo/brown-wooden-ruler-and-colored-pencils-on-papers-8970296/](https://www.pexels.com/photo/brown-wooden-ruler-and-colored-pencils-on-papers-8970296/)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '图片来源于Leeloo Thefirst: [https://www.pexels.com/photo/brown-wooden-ruler-and-colored-pencils-on-papers-8970296/](https://www.pexels.com/photo/brown-wooden-ruler-and-colored-pencils-on-papers-8970296/)'
- en: Ever since its inception in 2017 by Google Brain team, Transformers have rapidly
    become the state-of-the-art model for various use cases within the fields of Computer
    Vision and NLP. Its superior performance led to the development of several state-of-the-art
    models such as BERT and its variants like distilBERT and RoBERTa.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 自从2017年由Google Brain团队首次推出以来，Transformers迅速成为计算机视觉和自然语言处理领域中各种应用的**最先进模型**。其卓越的性能促成了多个**最先进模型**的开发，如BERT及其变体distilBERT和RoBERTa。
- en: BERT outperformed old recurrent models in various NLP tasks such as text classification,
    Named Entity Recognition (NER), question answering, and even the task that we’re
    going to focus on in this article, which is semantic textual similarity (STS).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: BERT在各种自然语言处理任务中优于旧的递归模型，如文本分类、命名实体识别（NER）、问答系统，甚至是我们在本文中将重点讨论的任务——语义文本相似度（STS）。
- en: Thus, in this article, we’re going to see how we can train a BERT model for
    STS task with the help of Sentence Transformers library. Next, we’re going to
    use the trained model to predict unknown data. But as a starter, we need to know
    first what STS task actually is and the dataset that we will use for this task.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本文中，我们将探讨如何利用Sentence Transformers库训练BERT模型以执行STS任务。接下来，我们将使用训练好的模型来预测未知数据。但作为开始，我们需要首先了解STS任务的实际内容以及我们将用于该任务的数据集。
- en: Semantic Textual Similarity and the Dataset
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语义文本相似度及数据集
- en: Semantic textual similarity (STS) refers to a task in which we compare the similarity
    between one text to another.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 语义文本相似度（STS）指的是一个任务，我们比较一段文本与另一段文本的相似性。
- en: '![](../Images/e96254c14992c643deb413b7d5ac368e.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e96254c14992c643deb413b7d5ac368e.png)'
- en: Image by author
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: The output that we get from a model for STS task is usually a floating number
    indicating the similarity between two texts being compared.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从模型得到的STS任务输出通常是一个浮点数，表示比较的两个文本之间的相似度。
- en: There are several ways to quantify the similarity between a pair of texts. Let’s
    take a look at the dataset that we’re going to use in this article as an example,
    which is [the STSB dataset](https://huggingface.co/datasets/stsb_multi_mt) (licensed
    under CC-Share Alike 4.0).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以量化一对文本之间的相似度。让我们以本文将使用的数据集为例，即 [STSB数据集](https://huggingface.co/datasets/stsb_multi_mt)（使用CC-Share
    Alike 4.0许可）。
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The similarity between a pair of texts is labeled between a number from 1 to
    5; 1 if a pair of texts is completely dissimilar, and 5 if a pair of texts is
    exactly similar in terms of their semantic meaning.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一对文本之间的相似性被标记为从 1 到 5 的数字；如果一对文本完全不相似，则标记为 1；如果一对文本在语义意义上完全相似，则标记为 5。
- en: However, there is a catch. When we want to train a BERT model with the help
    of Sentence Transformers library, we need to normalize the similarity score such
    that it has a range between 0 to 1\. This can be achieved simply by dividing each
    similarity score by 5.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有一个问题。当我们想要使用 Sentence Transformers 库训练 BERT 模型时，我们需要将相似性分数标准化到 0 到 1 之间。这可以通过简单地将每个相似性分数除以
    5 来实现。
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now that we know the dataset that we will be working with, let’s now proceed
    to the model that we’re going to use in this article.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了要使用的数据集，接下来让我们继续讨论本文中要使用的模型。
- en: How Transformers-Based Model Measure Similarity Between a Pair of Texts
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何基于 Transformers 的模型衡量一对文本之间的相似性
- en: Transformers-based models such as BERT, distilBERT, or RoBERTa expect a sequence
    of tokens as input. Thus, the very first step that should be done is to convert
    our input text into a sequence of tokens. This process is called tokenization.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 Transformers 的模型，如 BERT、distilBERT 或 RoBERTa，期望输入为标记序列。因此，第一步应该是将输入文本转换为标记序列。这个过程称为标记化。
- en: 'The tokenization process for BERT models consists of two steps. First, our
    input text will be split into several small chunks called tokens; one token can
    be a word or a sub-word. Second, two special tokens are added to our sequence
    of tokens: one at the beginning and one at the end. These two special tokens are:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 模型的标记化过程包括两个步骤。首先，将输入文本分割成几个小块，称为标记；一个标记可以是一个词或一个子词。其次，在标记序列的开头和结尾添加两个特殊标记。这两个特殊标记是：
- en: '**[CLS]:** this is the first token in each sequence of token'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**[CLS]:** 这是每个标记序列中的第一个标记'
- en: '**[SEP]:** this token is important to give BERT a hint about which token belongs
    to which sequence. If there is only one sequence of tokens, then this token will
    be the last token in the sequence'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**[SEP]:** 这个标记对于给 BERT 提供哪个标记属于哪个序列的提示是很重要的。如果只有一个标记序列，那么这个标记将是序列中的最后一个标记。'
- en: Depending on the maximum sequence length of the tokenizer that you define in
    advance, a bunch of **[PAD]** tokens will also be appended after the **[SEP]**
    token.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你事先定义的标记器的最大序列长度，一些 **[PAD]** 标记也会在 **[SEP]** 标记之后被追加。
- en: The tokenized input then will be passed into the model and as the output, we
    will get the embedding vector of each token. Each embedding vector has 768 dimensions.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 标记化的输入将被传递到模型中，作为输出，我们将获得每个标记的嵌入向量。每个嵌入向量具有 768 个维度。
- en: If we use BERT for classification purposes, then normally we take the embedding
    vector of the **[CLS]** token and pass it to a softmax or sigmoid layer in the
    end that will act as a classifier.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用 BERT 进行分类目的，那么通常我们会取 **[CLS]** 标记的嵌入向量，并将其传递给最终的 softmax 或 sigmoid 层，这一层将作为分类器。
- en: '![](../Images/b1c5eb953bf4030bda9271f53d09d598.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b1c5eb953bf4030bda9271f53d09d598.png)'
- en: Image by author
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'If we use BERT for STS task, the workflow would be something like this:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用 BERT 进行 STS 任务，工作流程将类似于以下步骤：
- en: '![](../Images/a11d3c5682ffb80333007d6c1e73a01d.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a11d3c5682ffb80333007d6c1e73a01d.png)'
- en: Image by author
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'With the workflow shown above, BERT achieved state-of-the-art performance on
    the STS benchmark. However, there is one major drawback to that workflow: the
    scalability factor.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上述工作流程，BERT 在 STS 基准测试中达到了最先进的性能。然而，这种工作流程有一个主要的缺点：可扩展性因素。
- en: Imagine we have a brand new text. Next, we want to query the most similar entry
    to this new text in our database which consists of 100K different texts. If we
    use BERT architecture as above, then we need to compare our new text with each
    entry in our database 100K times. This means 100K times of tokenization process
    and forward pass.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一段全新的文本。接下来，我们希望在包含 10 万个不同文本的数据库中查询与这段新文本最相似的条目。如果我们使用上面提到的 BERT 架构，那么我们需要将新文本与数据库中的每一个条目进行
    10 万次比较。这意味着需要进行 10 万次的标记化过程和前向传递。
- en: The main problem of this scalability factor is the fact that BERT outputs the
    embedding vector of each token and not the embedding vector of each text/sentence.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个可扩展性因素的主要问题是 BERT 输出的是每个标记的嵌入向量，而不是每个文本/句子的嵌入向量。
- en: '![](../Images/e6b1ae51b5adcff10c165bd27cc9de1e.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e6b1ae51b5adcff10c165bd27cc9de1e.png)'
- en: Image by author
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: If BERT somehow can give us a meaningful sentence-level embedding, then we can
    save the embedding of each entry in our database. Once we have a new text, then
    we only need to compare the sentence embedding of our new text with each entry’s
    sentence embedding in our database with the help of cosine similarity, which is
    a way faster method.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 BERT 能够给我们提供有意义的句子级嵌入，那么我们可以将每个条目的嵌入保存到我们的数据库中。一旦我们有了新的文本，我们只需通过余弦相似度将新文本的句子嵌入与数据库中每个条目的句子嵌入进行比较，这是一种更快的方法。
- en: 'This is what Sentence BERT (SBERT) tries to tackle. You can view SBERT as a
    fine-tuned version of BERT by applying siamese-type model architecture, as you
    can see below:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Sentence BERT (SBERT) 试图解决的问题。你可以将 SBERT 视为通过应用孪生网络模型架构来微调的 BERT，如下所示：
- en: '![](../Images/741d4914c4d7b0775fb7fd74fa0c2362.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/741d4914c4d7b0775fb7fd74fa0c2362.png)'
- en: Image by author
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: 'The problem with the architecture above is that it still generates token-level
    embedding. Thus, SBERT implements an additional pooling layer on top of BERT.
    There are three different pooling strategies implemented by SBERT:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 上述架构的问题在于它仍然生成 token 级嵌入。因此，SBERT 在 BERT 的基础上实现了一个额外的池化层。SBERT 实现了三种不同的池化策略：
- en: Using the embedding of **[CLS]** token
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 **[CLS]** token 的嵌入
- en: Using the mean of all token-level embedding vectors (this is the default implementation)
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用所有 token 级嵌入向量的均值（这是默认实现）
- en: Using the max-over-time token-level embedding vectors
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用时间上的最大 token 级嵌入向量
- en: '![](../Images/5f6347879747195ded968a8d41f0d833.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5f6347879747195ded968a8d41f0d833.png)'
- en: Image by author
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像
- en: The illustration above is the final architecture of SBERT model. What we get
    after the pooling layer is the embedding vector of a text that has 768 dimensions.
    This embedding then can be compared to each other with pairwise distance or cosine
    similarity, which is exactly what STS task is all about.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 上述插图是 SBERT 模型的最终架构。我们在池化层之后得到的嵌入是一个具有 768 维度的文本向量。然后，可以通过成对距离或余弦相似度对这些嵌入进行比较，这正是
    STS 任务的核心。
- en: 'To implement SBERT, we can use `sentence-transformers` library. If you haven’t
    installed it yet, you can do so via pip:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现 SBERT，我们可以使用 `sentence-transformers` 库。如果你还没有安装，可以通过 pip 安装：
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now we are going to implement SBERT model based on BERT, but you can also implement
    SBERT with BERT variants like distilBERT or RoBERTa, or even load a model that
    has been pretrained on particular dataset. You can find all of the [available
    models here](https://huggingface.co/sentence-transformers).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将实现基于 BERT 的 SBERT 模型，但你也可以使用如 distilBERT 或 RoBERTa 等 BERT 变体来实现 SBERT，或者加载一个在特定数据集上预训练的模型。你可以在[这里找到所有可用的模型](https://huggingface.co/sentence-transformers)。
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: From the code snippet above, we first load a BERT model as our word embedding
    model, and then we apply a pooling layer on top of the BERT model to obtain the
    sentence-level embedding in the end.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的代码片段中，我们首先加载 BERT 模型作为我们的词嵌入模型，然后在 BERT 模型上应用一个池化层，最终获得句子级嵌入。
- en: 'Let’s say that we have a pair of sentences and we want to fetch the sentence-level
    embedding of each sentence. We can do so by doing the following:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一对句子，我们想要获取每个句子的句子级嵌入。我们可以通过以下步骤做到这一点：
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Semantic Textual Similarity Implementation
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语义文本相似度实现
- en: In this section, we’re going to train an SBERT model on the dataset that we’ve
    discussed in the previous section for STS task.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将对在前一节中讨论的 STS 任务数据集进行 SBERT 模型训练。
- en: Model Architecture Definition
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型架构定义
- en: Let’s define the model architecture first.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 首先定义模型架构。
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The model architecture above is similar to what we’ve seen in the previous section.
    We use a BERT base model as our word embedding model. The output of this model
    is still a token-level embedding. Thus, we need to add a pooling layer on top
    of it.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 上述模型架构类似于我们在前一节中看到的内容。我们使用 BERT 基础模型作为我们的词嵌入模型。该模型的输出仍然是 token 级嵌入。因此，我们需要在其上添加一个池化层。
- en: The final output that we get from our SBERT model above is 768 dimensions of
    sentence-level embedding vector. Since the input of our model is a pair of texts,
    then the output will also be a pair of 768 dimensions of sentence-level embedding
    vector.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们的 SBERT 模型中得到的最终输出是 768 维的句子级嵌入向量。由于模型的输入是一对文本，因此输出也将是一对 768 维的句子级嵌入向量。
- en: Data Loader
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据加载器
- en: Data loader is necessary to create batch on our dataset. This process is important
    because we can’t just feed our model with the whole dataset at once during the
    training process.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 数据加载器是创建数据集批次所必需的。这个过程很重要，因为我们不能在训练过程中一次性将整个数据集输入到模型中。
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We’ve seen in the above sections what our dataset looks like and how to prepare
    it so that it can be used by our model for STS task. The code above does exactly
    the same:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在上面的部分中了解了我们的数据集是什么样的以及如何准备它，以便模型可以用于STS任务。上面的代码正是做了这些事情：
- en: The similarity score between each pair of texts is normalized, and this will
    be our ground truth label for model training
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每对文本之间的相似度得分被归一化，这将作为模型训练的真实标签。
- en: Each pair of texts is tokenized with the exact same tokenizer and the exact
    same step that we saw in the previous section. The tokenized pair of texts will
    be the input of our model during training.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每对文本都使用完全相同的分词器和步骤进行分词，这与我们在前一节中看到的一样。分词后的文本对将作为我们训练模型的输入。
- en: The collate_fn above is an important function to group each pair of texts together
    after the tokenization process for batching purposes.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的collate_fn函数在分词处理后将每对文本分组在一起，以便于批处理，这是一个重要的功能。
- en: Loss Function
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数
- en: In an STS task, our goal is to train a model such that it can distinguish between
    similar and dissimilar pairs of texts in terms of their semantic meaning. This
    means that we want the model to push the distance of dissimilar pairs of texts
    far apart, whilst keeping the distance of similar ones close to each other.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在STS任务中，我们的目标是训练一个模型，使其能够根据语义意义区分相似和不相似的文本对。这意味着我们希望模型将不相似的文本对的距离推得更远，同时将相似的文本对的距离保持得更近。
- en: 'There are a few common loss functions that we can use to achieve this objective:
    cosine similarity loss, triplet loss, and contrastive loss.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用几个常见的损失函数来实现这个目标：余弦相似度损失、三元组损失和对比损失。
- en: Normally we can use contrastive loss for this case. However, contrastive loss
    expects our label to be binary, i.e the label is 1 if the pair is semantically
    similar, and 0 otherwise. Meanwhile, what we have as the label on this dataset
    is a floating number that ranges between 0 to 1, thus cosine similarity loss would
    be a better loss function to implement.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们可以使用对比损失来处理这种情况。然而，对比损失期望我们的标签是二进制的，即如果文本对在语义上相似，标签为1，否则为0。与此同时，我们在这个数据集中获得的标签是一个范围在0到1之间的浮动数值，因此余弦相似度损失将是一个更好的损失函数。
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This loss function takes the sentence-level embedding of each text, and then
    it computes the cosine similarity between the two embeddings. As a result, the
    loss function will push dissimilar pairs far apart from each other in the vector
    space, whilst keeping the similar pairs close to each other.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这个损失函数接收每个文本的句子级嵌入，然后计算两个嵌入之间的余弦相似度。结果是，损失函数将在向量空间中将不相似的文本对推得更远，同时将相似的文本对保持得更近。
- en: Model Training
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型训练
- en: 'Now that we have set up the model’s architecture, the data loader, and the
    loss function, it’s time for us to train the model. The code is just a standard
    Pytorch training script, as you can see below:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置好了模型的架构、数据加载器和损失函数，是时候训练模型了。代码只是一个标准的Pytorch训练脚本，如下所示：
- en: '[PRE8]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the implementation above, we train our model for 8 epochs , the learning
    rate is set to 10e-6, and the batch size is set to 8\. These are hyperparameters
    that you can play around to suit your own need.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的实现中，我们将模型训练8个周期，学习率设置为10e-6，批量大小设置为8。这些是你可以调整以适应自己需求的超参数。
- en: 'If you run the `model_train` function above, you’ll get a training progress
    that looks something like this:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行上面的`model_train`函数，你将看到类似这样的训练进度：
- en: '![](../Images/9757fd8d3c000192156a5c193edc83ec.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9757fd8d3c000192156a5c193edc83ec.png)'
- en: Image by author
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Model Prediction
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型预测
- en: After we trained our model, now we can use it to predict unseen data, i.e an
    unseen pair of texts. However, before we feed the model with an unseen pair of
    texts, let’s create a function that enables us to obtain the similarity prediction
    from our model.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练完模型后，我们现在可以使用它来预测未见的数据，即未见的文本对。然而，在将模型输入未见的文本对之前，让我们创建一个函数，以便从模型中获取相似度预测。
- en: '[PRE9]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The code implementation above includes all of the preprocessing steps of the
    data as well as the steps to fetch the model’s prediction.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码实现包括了数据的所有预处理步骤以及获取模型预测的步骤。
- en: 'Let’s say that we have a similar pair of texts as can be seen below:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一对类似的文本，如下所示：
- en: '[PRE10]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now we can just call `predict_sts` function and we get the cosine similarity
    between two texts inferred by our model. In this case, we get a similarity of
    0.860\. This means that our pair of texts are very similar to each other.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们只需调用`predict_sts`函数，就可以得到由我们的模型推断出的两个文本之间的余弦相似度。在这个例子中，我们得到的相似度为0.860。这意味着我们的文本对彼此非常相似。
- en: For comparison, let’s now feed the model with a pair of dissimilar texts.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 进行对比，现在让我们用一对不同的文本来测试模型。
- en: '[PRE11]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As you can see above, when we have a pair of dissimilar texts, the similarity
    is just 0.055, which means that embedding between two texts in the vector space
    is far apart from each other. And this is exactly what our model has been trained
    for.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，当我们处理一对不同的文本时，相似度仅为0.055，这意味着在向量空间中两个文本的嵌入彼此相距较远。这正是我们的模型经过训练的目的。
- en: Conclusion
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we have implemented a BERT model for a semantic textual similarity
    task. Specifically, we used Sentence-Transformers library to fine-tune a BERT
    model into Siamese architecture such that we are able to get the sentence-level
    embedding for each text. The sentence-level embedding for each text then can be
    compared to each other via cosine similarity.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们实现了一个用于语义文本相似度任务的BERT模型。具体来说，我们使用了Sentence-Transformers库来微调BERT模型为Siamese架构，从而能够获取每个文本的句子级嵌入。然后，可以通过余弦相似度比较每个文本的句子级嵌入。
- en: You can find all of the code implemented in this article in [**this notebook**](https://github.com/marcellusruben/medium-resources/blob/main/STS_BERT/STS_BERT.ipynb).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[**这个笔记本**](https://github.com/marcellusruben/medium-resources/blob/main/STS_BERT/STS_BERT.ipynb)中找到本文实现的所有代码。
