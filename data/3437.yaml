- en: 'Parallelising Python on Spark: Options for Concurrency with Pandas'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Pandas 中并行化 Python：使用 Spark 的并发选项
- en: 原文：[https://towardsdatascience.com/parallelising-python-on-spark-options-for-concurrency-with-pandas-7ca553b9f265?source=collection_archive---------8-----------------------#2023-11-18](https://towardsdatascience.com/parallelising-python-on-spark-options-for-concurrency-with-pandas-7ca553b9f265?source=collection_archive---------8-----------------------#2023-11-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/parallelising-python-on-spark-options-for-concurrency-with-pandas-7ca553b9f265?source=collection_archive---------8-----------------------#2023-11-18](https://towardsdatascience.com/parallelising-python-on-spark-options-for-concurrency-with-pandas-7ca553b9f265?source=collection_archive---------8-----------------------#2023-11-18)
- en: Leverage the benefits of Spark when working with Pandas
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 当使用 Pandas 时，利用 Spark 的优势
- en: '[](https://medium.com/@mc12338?source=post_page-----7ca553b9f265--------------------------------)[![Matt
    Collins](../Images/b28ac8100d6fb287e3fa6926eec7939a.png)](https://medium.com/@mc12338?source=post_page-----7ca553b9f265--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7ca553b9f265--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7ca553b9f265--------------------------------)
    [Matt Collins](https://medium.com/@mc12338?source=post_page-----7ca553b9f265--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mc12338?source=post_page-----7ca553b9f265--------------------------------)![Matt
    Collins](../Images/b28ac8100d6fb287e3fa6926eec7939a.png)[](https://towardsdatascience.com/?source=post_page-----7ca553b9f265--------------------------------)![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)[Matt Collins](https://medium.com/@mc12338?source=post_page-----7ca553b9f265--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd1970f1605f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fparallelising-python-on-spark-options-for-concurrency-with-pandas-7ca553b9f265&user=Matt+Collins&userId=d1970f1605f1&source=post_page-d1970f1605f1----7ca553b9f265---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7ca553b9f265--------------------------------)
    ·8 min read·Nov 18, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7ca553b9f265&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fparallelising-python-on-spark-options-for-concurrency-with-pandas-7ca553b9f265&user=Matt+Collins&userId=d1970f1605f1&source=-----7ca553b9f265---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fd1970f1605f1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fparallelising-python-on-spark-options-for-concurrency-with-pandas-7ca553b9f265&user=Matt+Collins&userId=d1970f1605f1&source=post_page-d1970f1605f1----7ca553b9f265---------------------post_header-----------)
    发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----7ca553b9f265--------------------------------)
    ·8 分钟阅读·Nov 18, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F7ca553b9f265&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fparallelising-python-on-spark-options-for-concurrency-with-pandas-7ca553b9f265&user=Matt+Collins&userId=d1970f1605f1&source=-----7ca553b9f265---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7ca553b9f265&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fparallelising-python-on-spark-options-for-concurrency-with-pandas-7ca553b9f265&source=-----7ca553b9f265---------------------bookmark_footer-----------)![](../Images/e140d141606ab77d4bea08c31fd28d36.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7ca553b9f265&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fparallelising-python-on-spark-options-for-concurrency-with-pandas-7ca553b9f265&source=-----7ca553b9f265---------------------bookmark_footer-----------)![](../Images/e140d141606ab77d4bea08c31fd28d36.png)'
- en: Photo by [Florian Steciuk](https://unsplash.com/@flo_stk?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Florian Steciuk](https://unsplash.com/@flo_stk?utm_source=medium&utm_medium=referral)在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)上拍摄。
- en: In my previous role, I spent some time working on an internal project to predict
    future disk storage space usage for our Managed Services customers across thousands
    of disks. Each disk is subject to its own usage patterns and this means we need
    a separate machine learning model for each disk which takes historical data to
    predict future usage on a disk-by-disk basis. While performing this prediction
    and choosing the correct algorithm for the job is a challenge in itself, performing
    this at scale has its own problems.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在我之前的工作中，我花了一些时间在一个内部项目上，为我们的托管服务客户预测未来的磁盘存储空间使用情况。每个磁盘都有自己的使用模式，这意味着我们需要为每个磁盘建立单独的机器学习模型，这些模型利用历史数据逐盘预测未来的使用情况。尽管执行预测和选择正确的算法本身就是一个挑战，但在大规模执行时也有其自身的问题。
- en: In order to take advantage of more sophisticated infrastructure, we can look
    to move away from sequential predictions and speed up the operation of the forecasting
    by parallelising the workload. This blog post aims to compare Pandas UDFs and
    the ‘concurrent.futures’ module, two approaches of concurrent processing, and
    determine use cases for each.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用更先进的基础设施，我们可以考虑摆脱顺序预测，通过并行化工作负载来加快预测操作。本文旨在比较 Pandas UDF 和 ‘concurrent.futures’
    模块，这两种并发处理的方法，并确定每种方法的适用场景。
- en: The Challenge
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 挑战
- en: Pandas is a gateway package in Python for working with datasets in the analytics
    space. Through working with DataFrames, we’re able to profile data and evaluate
    data quality, perform exploratory data analysis, build descriptive visualisations
    of the data and predict future trends.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas 是 Python 中用于处理数据集的入口包。在处理 DataFrames 时，我们能够对数据进行分析和评估数据质量，执行探索性数据分析，构建数据的描述性可视化，并预测未来趋势。
- en: While this is certainly a great tool, the single-threaded nature of Python means
    it can scale poorly when working with larger data sets, or when you need to perform
    the same analysis across multiple subsets of data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这是一个很好的工具，但 Python 的单线程特性意味着在处理更大的数据集时，或者在需要对多个数据子集执行相同分析时，它的扩展性较差。
- en: In the world of big data, we expect a bit more sophistication in our approach,
    as we have the additional focus on scalability to keep great performance. Spark,
    amongst other languages, allows us to take advantage of distributed processing
    to help us process larger and more complicated data structures.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在大数据领域，我们期望采取更复杂的方法，因为我们还需关注扩展性以保持良好的性能。Spark 以及其他语言使我们能够利用分布式处理来帮助处理更大更复杂的数据结构。
- en: 'Before digging into this specific example, we can generalise some use cases
    which summarise the need for concurrency in data processing:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入这个具体例子之前，我们可以总结一些数据处理并发需求的用例：
- en: Apply uniform transformations to multiple data files
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对多个数据文件应用统一转换
- en: Forecast future values for several subsets of data
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为多个数据子集预测未来值
- en: Tune hyperparameters for machine learning model and select most efficient configuration
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整机器学习模型的超参数并选择最有效的配置
- en: When escalating our requirement to perform workloads like those suggested above
    and in our case, the most straightforward approach in Python and Pandas is to
    process this data sequentially. For our example, we would run the above flow for
    one disk at a time.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们需要执行如上所述的工作负载时，在 Python 和 Pandas 中最简单的方法是顺序处理这些数据。对于我们的例子，我们将对每个磁盘依次运行上述流程。
- en: The Data
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据
- en: In our example, we have data for thousands of disks that show the free space
    recorded over time and we want to predict future free space values for each of
    the disks.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们有成千上万的磁盘数据，显示了随时间记录的空闲空间，我们希望预测每个磁盘的未来空闲空间值。
- en: To paint the picture a bit more clearly, I’ve provided a csv file containing
    1,000 disks each with one month of historical data for free space measured in
    GB. This is of sufficient size for us to see the impact of the different approaches
    to predicting at scale.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清晰地描述情况，我提供了一个包含 1,000 个磁盘的 csv 文件，每个磁盘有一个月的以 GB 为单位的空闲空间历史数据。这一数据量足以让我们看到不同方法在大规模预测中的影响。
- en: '![](../Images/e992deee09e07e1de9e148a170ee1aa3.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e992deee09e07e1de9e148a170ee1aa3.png)'
- en: 'Image by Author: Example DataFrame'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像：示例 DataFrame
- en: For a time-series problem like this, we’re looking to use historical data to
    predict future trends and we want to understand which Machine Learning (ML) algorithm
    is going to be most appropriate for each disk. Tools like AutoML are great for
    this when looking to determine the appropriate model for one dataset, but we’re
    dealing with 1,000 datasets here — so this is excessive for our comparison.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像这样的时间序列问题，我们希望使用历史数据来预测未来的趋势，并且我们希望了解对于每个磁盘哪种机器学习（ML）算法将最合适。像 AutoML 这样的工具在寻找一个数据集的适当模型时非常有效，但对于我们这里的
    1,000 个数据集来说则有些过于复杂了。
- en: 'In this case, we’ll limit the number of algorithms we want to compare to two
    and see which is the most appropriate model to use, for each disk, using the Root
    Mean Squared Error (RMSE) as a validation metric. Further information on RMSE
    can be found [here](https://www.statology.org/how-to-interpret-rmse/) . These
    algorithms are:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们将限制要比较的算法数量为两个，并查看每个磁盘使用均方根误差（RMSE）作为验证指标时最适合使用的模型。关于 RMSE 的更多信息可以在
    [这里](https://www.statology.org/how-to-interpret-rmse/) 找到。这些算法包括：
- en: Linear regression
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归
- en: Fbprophet (fitting the data to a more complex line)
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fbprophet（将数据拟合到更复杂的线条）
- en: Facebook’s time-series forecasting model.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Facebook 的时间序列预测模型。
- en: Built for more complex predictions with hyperparameters for seasonality.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了更复杂的季节性超参数预测而建立。
- en: 'We’ve got all the components ready now if we wanted to predict a single disk’s
    future free space. The set of actions follows the below flow:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好所有组件，如果我们想预测单个磁盘的未来剩余空间。操作步骤如下：
- en: '![](../Images/a783b1227a738d2c25c1db93079c43b3.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a783b1227a738d2c25c1db93079c43b3.png)'
- en: 'Image by Author: Data Lifecycle'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像：数据生命周期
- en: We now want to scale this out, performing this flow for multiple disks, 1,000
    in our example.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们希望将此扩展，对多个磁盘执行此流程，例如我们的示例中的 1,000 个。
- en: As part of our review, we’ll compare the performance of calculating RMSE values
    for the different algorithms at different scales. As such, I’ve created a subset
    of the first 100 disks to mimic this.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 作为我们评估的一部分，我们将比较在不同规模下计算不同算法的 RMSE 值的性能。因此，我创建了一个包含前 100 个磁盘子集的子集来模拟这一点。
- en: This should give some interesting insights into performance on different-sized
    datasets, performing operations of varying complexity.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该对不同大小的数据集上的性能提供一些有趣的见解，执行各种复杂度的操作。
- en: Introducing concurrency
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入并发性
- en: Python is famously single-threaded and subsequently does not make use of all
    the compute resources available at a point in time.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Python 以单线程著称，因此不会在某一时间点利用所有可用的计算资源。
- en: 'As a result, I saw three options:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我看到了三个选项：
- en: Implement a for loop to calculate the predictions sequentially, taking the single-threaded
    approach.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现一个 for 循环，顺序计算预测，采用单线程方法。
- en: Use Python’s [futures](https://docs.python.org/3/library/concurrent.futures.html)
    module to run multiple processes at once.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Python 的 [futures](https://docs.python.org/3/library/concurrent.futures.html)
    模块同时运行多个进程。
- en: Use Pandas UDFs (user-defined functions) to leverage distributed computing in
    PySpark while maintaining our Pandas syntax and compatible packages.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Pandas UDFs（用户定义的函数）在 PySpark 中利用分布式计算，同时保持我们的 Pandas 语法和兼容的包。
- en: I wanted to do a fairly in-depth comparison under different environment conditions,
    so have used a single-node Databricks cluster and another Databricks cluster with
    4 worker nodes to leverage Spark for our Pandas UDF approach.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望在不同的环境条件下进行相当深入的比较，因此使用了一个单节点的 Databricks 集群和另一个具有 4 个工作节点的 Databricks 集群，以利用
    Spark 进行我们的 Pandas UDF 方法。
- en: 'We’ll follow the following approach to evaluate the suitability of the Linear
    Regression and fbprophet models for each disk:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用以下方法来评估线性回归和 fbprophet 模型在每个磁盘上的适用性：
- en: Split the data into train and test sets
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据分割为训练集和测试集
- en: Use the training set as input and predict over the test set dates
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用训练集作为输入，在测试集日期上进行预测
- en: Compare the predicted values with the actual values in the test set to get an
    Root Mean Squared Error (RMSE) score
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将预测值与测试集中的实际值进行比较，以获取均方根误差（RMSE）分数
- en: 'We’re going to return two things in our outputs: a modified DataFrame with
    the predictions, giving us the additional benefit of plotting and comparing the
    predicted vs actual values, and a DataFrame containing the RMSE scores for each
    disk and algorithm.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在输出中返回两件事情：一个带有预测的修改后的 DataFrame，为我们提供绘制和比较预测与实际值的额外好处，以及一个包含每个磁盘和算法的 RMSE
    分数的 DataFrame。
- en: 'The functions to do so look like the below:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此操作的函数如下所示：
- en: 'We’re going to compare the three approaches outlined above. We’ve got a few
    different scenarios, so we can fill out a table of what we’re collecting results
    against:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将比较上述概述的三种方法。我们有几个不同的场景，因此可以填写一个表格，列出我们收集结果的标准：
- en: 'With the following combinations:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下组合：
- en: Method
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法
- en: Sequential
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顺序
- en: futures
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: futures
- en: Pandas UDFs
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pandas UDFs
- en: Algorithm
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 算法
- en: Linear regression
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归
- en: Fbprophet
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fbprophet
- en: Combined (both algorithms for each disk) — most efficient way to gather a comparison.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组合（每个磁盘的两种算法）——最有效的比较方式。
- en: Cluster mode
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集群模式
- en: Single Node Cluster
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单节点集群
- en: Standard Cluster with 4 workers
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准集群，4 个工作节点
- en: Number of disks
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 磁盘数量
- en: '100'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '100'
- en: '1000'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1000'
- en: The results are presented in this format in the appendix of this blog, should
    you wish to take a further look.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 结果以这种格式呈现在本博客的附录中，如果您想进一步查看。
- en: The Methods
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法
- en: 'Method 1: Sequential'
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法 1：顺序
- en: 'Method 2: concurrent.futures'
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法 2：concurrent.futures
- en: 'There are two options in using this module: parallelising memory-intensive
    operations (using ThreadPoolExecutor) or CPU-intensive operations (ProcessPoolExecutor).
    One descriptive explanation of this is found in the following [blog](https://medium.com/@bfortuner/python-multithreading-vs-multiprocessing-73072ce5600b).
    As we’re going to be working on a CPU intensive problem, ProcessPoolExecutor is
    fitting for what we’re trying to achieve.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此模块有两个选项：并行内存密集型操作（使用 ThreadPoolExecutor）或 CPU 密集型操作（ProcessPoolExecutor）。关于这一点的描述性解释可以在以下[博客](https://medium.com/@bfortuner/python-multithreading-vs-multiprocessing-73072ce5600b)中找到。由于我们将处理
    CPU 密集型问题，ProcessPoolExecutor 适合我们要实现的目标。
- en: 'Method 3: Pandas UDFs'
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法 3：Pandas UDFs
- en: Now we’re going to switch gear and use Spark and leverage distributed computing
    to help with our efficiency. Since we’re using Databricks, most of our Spark configuration
    is done for us but there are some tweaks to our general handling of the data.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将切换到 Spark 并利用分布式计算来提高效率。由于我们使用的是 Databricks，大部分 Spark 配置已经为我们完成，但我们对数据的通用处理仍有一些调整。
- en: 'First, import the data to a PySpark DataFrame:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，将数据导入到 PySpark DataFrame：
- en: We’re going to make use of the Pandas grouped map UDF (PandasUDFType.GROUPED_MAP),
    since we want to pass in a DataFrame and return a DataFrame. Since [Apache Spark
    3.0](https://databricks.com/blog/2020/05/20/new-pandas-udfs-and-python-type-hints-in-the-upcoming-release-of-apache-spark-3-0.html)
    we don’t need to explicitly declare this decorator anymore!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Pandas 分组映射 UDF（PandasUDFType.GROUPED_MAP），因为我们想传入一个 DataFrame 并返回一个 DataFrame。自
    [Apache Spark 3.0](https://databricks.com/blog/2020/05/20/new-pandas-udfs-and-python-type-hints-in-the-upcoming-release-of-apache-spark-3-0.html)
    起，我们不再需要显式声明此装饰器！
- en: We need to split out our fbprophet, regression and RMSE functions for Pandas
    UDFs due to DataFrame structuring in PySpark, but don’t require a massive code
    overhaul to achieve this.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 PySpark 中的 DataFrame 结构，我们需要将 fbprophet、回归和 RMSE 函数拆分为 Pandas UDFs，但不需要对代码进行大规模重构即可实现。
- en: We can then use applyInPandas to produce our results.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`applyInPandas`来生成我们的结果。
- en: 'Note: the examples above are only demonstrating the process for using Linear
    Regression for readability. Please see the full [notebook](https://github.com/MattPCollins/ConcurrentModelTraining/blob/main/concurrent_model_training.ipynb)
    for the complete demonstration of this.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：上述示例仅演示了使用线性回归的过程以提高可读性。有关完整演示，请参阅完整的[笔记本](https://github.com/MattPCollins/ConcurrentModelTraining/blob/main/concurrent_model_training.ipynb)。
- en: Interpreting the results
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果解读
- en: '![](../Images/3098a8f18ce7b30e907ab60668ebf311.png)![](../Images/46707ea00c9a1d05785f17a06739cae1.png)![](../Images/2fd9e8b2bbc38d819d3cdec9eb645ec0.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3098a8f18ce7b30e907ab60668ebf311.png)![](../Images/46707ea00c9a1d05785f17a06739cae1.png)![](../Images/2fd9e8b2bbc38d819d3cdec9eb645ec0.png)'
- en: 'Images by Author: Execution durations for forecasting algorithms, for each
    suggested method'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片：每种建议方法的预测算法执行时间
- en: We’ve created plots for the different methods and different environment set-ups,
    then grouped the data by algorithm and number of disks for easy comparison.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为不同的方法和不同的环境设置创建了图表，然后按算法和磁盘数量对数据进行分组，以便于比较。
- en: Please note that the tabular results are found in the appendix of this post.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，表格结果见本帖的附录。
- en: 'I’ve summarised the highlights of these findings below:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经总结了这些发现的亮点，如下所示：
- en: As expected, predicting 1,000 disks compared to 100 disks is (generally) a more
    time-consuming process.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正如预期的那样，预测 1,000 个磁盘与 100 个磁盘相比是（通常）一个更耗时的过程。
- en: The sequential approach is generally the slowest, being unable to take advantage
    of underlying resources in an efficient manner.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顺序方法通常是最慢的，无法有效利用底层资源。
- en: Pandas UDFs are quite inefficient on the smaller, simpler tasks. The overhead
    of transforming the data is more expensive — parallelising helps to compensate
    for this.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于较小、更简单的任务来说，Pandas UDFs效率很低。数据转换的开销更高 — 并行化有助于弥补这一点。
- en: Both sequential and concurrent.futures approaches are oblivious to the clustering
    available in Databricks — missing out on additional compute.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顺序和`concurrent.futures`方法都没有意识到Databricks提供的聚类 —— 无法利用额外的计算资源。
- en: Closing thoughts
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结语
- en: Context certainly plays a big part in which approach is most successful, but
    given Databricks and Spark are often used for Big Data problems, we can see the
    benefit of using Pandas UDFs with those larger, more complex datasets that we’ve
    seen here today.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，背景在确定最成功的方法时起着重要作用，但考虑到Databricks和Spark通常用于大数据问题，我们可以看到在处理这里今天看到的那些更大更复杂的数据集时使用Pandas
    UDFs的好处。
- en: Using a Spark environment for smaller datasets can be done just as efficiently
    on a smaller (and less expensive!) compute configuration at great efficiency as
    demonstrated by the use of the concurrent.futures module, so do bear this in mind
    when architecting your solution.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理较小数据集时，使用Spark环境可以通过`concurrent.futures`模块在较小（且成本更低！）计算配置上以极高的效率完成，因此在设计解决方案时请记住这一点。
- en: If you’re familiar with Python a­nd Pandas then neither approach should be a
    strenuous learning curve to move away from the sequential for loop approach seen
    in beginner tutorials.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您熟悉Python和Pandas，那么从在初学者教程中看到的顺序for循环方法转变，对于任何一种方法都不应该是一种费力的学习曲线。
- en: We’ve not investigated it in this post as I have found discrepancies and incompatibilities
    with the current version, but the recent pyspark.pandas module will certainly
    be more common in the future, and one approach to look out for. This API (along
    with Koalas, developed by the guys at Databricks, but now retired) leverages the
    familiarity of Pandas with the underlying benefits of Spark.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本文中没有进行深入调查，因为我发现当前版本存在差异和不兼容性，但最近的`pyspark.pandas`模块在未来肯定会更常见，并且是一个值得关注的方法。该API（以及由Databricks开发的Koalas，现已退役）利用了Pandas的熟悉性以及Spark的底层优势。
- en: For demonstrating the effect we are trying to achieve, we’ve only gone as far
    as to look at the RMSE values produced for each disk, rather than actually predict
    a future time-series set of values. The framework we’ve set up here can be applied
    in the same way for this, with logic to determine if the evaluation metric (along
    with other logic, such as physical limitations of a disk) is appropriate in each
    case and to predict the future values, where possible, using the determined algorithm.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示我们试图实现的效果，我们只查看了每个磁盘生成的RMSE值，而没有实际预测未来的时间序列值。我们在这里设置的框架可以以同样的方式应用于此，具有确定算法使用情况的逻辑，以及在可能情况下使用确定的算法预测未来值，其中包括评估指标（以及其他逻辑，例如磁盘的物理限制）是否适合每种情况。
- en: As always, the notebook can be found in my [GitHub](https://github.com/MattPCollins/ConcurrentModelTraining)
    .
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 总是可以在我的[GitHub](https://github.com/MattPCollins/ConcurrentModelTraining)找到这个笔记本。
- en: Appendix
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录
- en: '*Originally published at* [*https://blog.coeo.com*](https://blog.coeo.com/parallelising-python-on-spark-different-approaches),
    adapted for this repost*.*'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*最初发布于* [*https://blog.coeo.com*](https://blog.coeo.com/parallelising-python-on-spark-different-approaches)，为了本次转载做了调整*。*'
