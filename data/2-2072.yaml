- en: The Ultimate Guide to nnU-Net
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: nnU-Net 终极指南
- en: 原文：[https://towardsdatascience.com/the-ultimate-guide-to-nnu-net-for-state-of-the-art-image-segmentation-6dda7f44b935](https://towardsdatascience.com/the-ultimate-guide-to-nnu-net-for-state-of-the-art-image-segmentation-6dda7f44b935)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-ultimate-guide-to-nnu-net-for-state-of-the-art-image-segmentation-6dda7f44b935](https://towardsdatascience.com/the-ultimate-guide-to-nnu-net-for-state-of-the-art-image-segmentation-6dda7f44b935)
- en: Everything you need to know to understand the State of the Art nnU-Net, and
    how to apply it to your own dataset.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解最先进的 nnU-Net 及其如何应用于你自己的数据集所需了解的一切。
- en: '[](https://medium.com/@francoisporcher?source=post_page-----6dda7f44b935--------------------------------)[![François
    Porcher](../Images/9ddb233f8cadbd69026bd79e2bd62dea.png)](https://medium.com/@francoisporcher?source=post_page-----6dda7f44b935--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6dda7f44b935--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6dda7f44b935--------------------------------)
    [François Porcher](https://medium.com/@francoisporcher?source=post_page-----6dda7f44b935--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@francoisporcher?source=post_page-----6dda7f44b935--------------------------------)[![François
    Porcher](../Images/9ddb233f8cadbd69026bd79e2bd62dea.png)](https://medium.com/@francoisporcher?source=post_page-----6dda7f44b935--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6dda7f44b935--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6dda7f44b935--------------------------------)
    [François Porcher](https://medium.com/@francoisporcher?source=post_page-----6dda7f44b935--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6dda7f44b935--------------------------------)
    ·13 min read·Aug 2, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6dda7f44b935--------------------------------)
    ·阅读时长 13 分钟·2023 年 8 月 2 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/e93dc02adab7ff7d7eafaebe00a09554.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e93dc02adab7ff7d7eafaebe00a09554.png)'
- en: Neuroimaging, by Milak Fakurian on Unsplash, [link](https://unsplash.com/photos/58Z17lnVS4U)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Neuroimaging，作者：Milak Fakurian，图片来源于 Unsplash，[链接](https://unsplash.com/photos/58Z17lnVS4U)
- en: During my Research internship in Deep Learning and Neurosciences at Cambridge
    University, I used the nnU-Net a lot, which is an extremely strong baseline in
    Semantic Image Segmentation.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在剑桥大学的深度学习与神经科学研究实习期间，我使用了大量的 nnU-Net，它在语义图像分割中是一个极其强大的基准。
- en: However, I struggled a little to fully understand the model and how to train
    it, and did not find so much help on internet. Now that I am comfortable with
    it, I created this tutorial to help you, either in your quest to understand better
    what is behind this model, or how to use it in your own dataset.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，我在完全理解模型及其训练方法时遇到了一些困难，并且在互联网上没有找到太多帮助。现在我对它已经很熟悉了，因此我创建了这个教程，以帮助你，无论是更好地理解这个模型的内涵，还是如何在你自己的数据集中使用它。
- en: 'Throughout this guide, you will:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本指南中，你将：
- en: Develop a concise overview of the key contributions of nnU-Net.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开发 nnU-Net 关键贡献的简要概述。
- en: Learn how to apply nnU-Net to your own dataset.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 学会如何将 nnU-Net 应用到你自己的数据集中。
- en: All code available on this [Google Collab notebook](https://colab.research.google.com/drive/1h6scef1i258x0abxT_FcSI_QBN9Eh_9e?usp=sharing)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 所有代码均可在这个 [Google Collab notebook](https://colab.research.google.com/drive/1h6scef1i258x0abxT_FcSI_QBN9Eh_9e?usp=sharing)
    中找到
- en: This work took me a significant amount of time and effort. If you find this
    content valuable, please consider following me to increase its visibility and
    help support the creation of more such tutorials!
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这项工作花费了我大量的时间和精力。如果你觉得这个内容有价值，请考虑关注我，以增加它的可见度并支持更多类似教程的创作！
- en: A Brief History of nnU-Net
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: nnU-Net 的简要历史
- en: Recognized as a **state-of-the-art model in Image Segmentation**, the nnU-Net
    is an indomitable force when it comes to both 2D and 3D image processing. Its
    performance is so robust that it serves as a strong baseline against which new
    computer vision architectures are benchmarked. In essence, if you are venturing
    into the world of developing novel computer vision models, consider the nnU-Net
    as your **‘target to surpass’.**
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 作为**图像分割领域的最先进模型**，nnU-Net 在 2D 和 3D 图像处理方面都具有强大的实力。它的表现非常稳健，成为了新的计算机视觉架构的强基准。如果你打算进入开发新型计算机视觉模型的领域，可以把
    nnU-Net 作为你的**“超越目标”**。
- en: 'This powerful tool is based on the U-Net model (You can find one of my tutorials
    here: [Cook your first U-Net](https://medium.com/@foporcher/cooking-your-first-u-net-for-image-segmentation-e812e37e9cd0)),
    which made its debut in 2015\. The appellation “nnU-Net” stands for “No New U-Net”,
    a nod to the fact that its **design doesn’t introduce revolutionary architectural
    alterations.** Instead, it takes the existing U-Net structure and squeezes out
    its full potential using a set of ingenious optimization strategies.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '这个强大的工具基于U-Net模型（你可以在这里找到我的一个教程: [Cook your first U-Net](https://medium.com/@foporcher/cooking-your-first-u-net-for-image-segmentation-e812e37e9cd0)），它首次亮相于2015年。名称“nnU-Net”代表“无新U-Net”，意味着它的**设计并未引入革命性的架构变化。**
    相反，它利用一系列巧妙的优化策略充分挖掘现有U-Net结构的潜力。'
- en: Contrary to many modern neural networks, the nnU-Net doesn’t rely on residual
    connections, dense connections, or attention mechanisms. Its strength lies in
    its meticulous optimization strategy, which includes techniques like resampling,
    normalization, judicious choice of loss function, optimiser settings, data augmentation,
    patch-based inference, and ensembling across models. This holistic approach allows
    the nnU-Net to push the boundaries of what’s achievable with the original U-Net
    architecture.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 与许多现代神经网络不同，nnU-Net并不依赖残差连接、密集连接或注意力机制。它的强项在于其细致的优化策略，包括重采样、归一化、谨慎选择损失函数、优化器设置、数据增强、基于补丁的推断和模型集成。这种整体方法使nnU-Net能够推动原始U-Net架构的极限。
- en: Exploring Diverse Architectures within nnU-Net
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 nnU-Net 中的多样化架构
- en: 'While it might seem like a singular entity, the nnU-Net is in fact an umbrella
    term for three distinct types of U-Nets:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它可能看起来像一个单一的实体，nnU-Net 实际上是对三种不同类型的 U-Net 的总称：
- en: '![](../Images/5d874d9c4bc9ca3cb7356906001aafea.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5d874d9c4bc9ca3cb7356906001aafea.png)'
- en: 2D, 3D, and cascade, Image from [nnU-Net article](https://arxiv.org/abs/1809.10486)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 2D、3D和级联，图片来源于 [nnU-Net 文章](https://arxiv.org/abs/1809.10486)
- en: '**2D U-Net:** Arguably the most well-known variant, this operates directly
    on 2D images.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**2D U-Net:** 可以说是最知名的变体，它直接处理2D图像。'
- en: '**3D U-Net:** This is an extension of the 2D U-Net and is capable of handling
    3D images directly through the application of 3D convolutions.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**3D U-Net:** 这是2D U-Net的扩展，能够通过应用3D卷积直接处理3D图像。'
- en: '**U-Net Cascade:** This model generates low-resolution segmentations and subsequently
    refines them.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**U-Net Cascade:** 该模型生成低分辨率分割结果，并随后进行精细化处理。'
- en: Each of these architectures brings its unique strengths to the table and, inevitably,
    has certain limitations.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 每种架构都有其独特的优点，并且不可避免地有一定的局限性。
- en: For instance, employing a 2D U-Net for 3D image segmentation might seem counterintuitive,
    but in practice, it can still be highly effective. This is achieved by slicing
    the 3D volume into 2D planes.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，使用2D U-Net进行3D图像分割可能看起来违反直觉，但实际上，它仍然可以非常有效。这是通过将3D体积切片为2D平面来实现的。
- en: While a 3D U-Net may seem more sophisticated, given its higher parameter count,
    it isn’t always the most efficient solution. Particularly, 3D U-Nets often struggle
    with anisotropy, which occurs when spatial resolutions differ along different
    axes (for example, 1mm along the x-axis and 1.2 mm along the z-axis).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管3D U-Net由于其更高的参数数量看起来更复杂，但它并不总是最有效的解决方案。特别是，3D U-Net通常在处理各轴空间分辨率不一致时表现不佳（例如，x轴为1mm，而z轴为1.2mm）。
- en: The U-Net Cascade variant becomes particularly handy when dealing with large
    image sizes. It employs a preliminary model to condense the image, followed by
    a standard 3D U-Net that outputs low-resolution segmentations. The generated predictions
    are then upscaled, resulting in a refined, comprehensive output.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理大型图像时，U-Net Cascade变体尤其有用。它采用初步模型来压缩图像，然后使用标准的3D U-Net生成低分辨率分割结果。生成的预测结果随后被放大，从而得到精细化的全面输出。
- en: '![](../Images/a664858b9bbc6fc6c909880bdc09e685.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a664858b9bbc6fc6c909880bdc09e685.png)'
- en: Image from [nnU-Net article](https://arxiv.org/abs/1809.10486)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 [nnU-Net 文章](https://arxiv.org/abs/1809.10486)
- en: Typically, the methodology involves training all three model variants within
    the nnU-Net framework. The subsequent step may be to either choose the best performer
    among the three or employ ensembling techniques. One such technique might involve
    integrating the predictions of both the 2D and 3D U-Nets.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这一方法包括在 nnU-Net 框架内训练所有三个模型变体。接下来的步骤可能是从这三者中选择表现最佳的一个，或者使用集成技术。其中一种技术可能涉及整合
    2D 和 3D U-Net 的预测结果。
- en: However, it’s worth noting that this procedure can be quite time-consuming (and
    also money because you need GPU credits). If your constraints only allow for the
    training of a single model, fret not. You can choose to only train one model,
    since the ensembling model only brings very marginal gains.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，值得注意的是，这一过程可能非常耗时（并且也需要资金，因为你需要 GPU 计算资源）。如果你的限制只允许训练一个模型，不必担心。你可以选择只训练一个模型，因为集成模型仅带来微小的收益。
- en: 'This table illustrates the best-performing model variant in relation to specific
    datasets:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 此表格展示了与特定数据集相关的最佳性能模型变体：
- en: '![](../Images/47f06af7a3d5b2e7a3c57dde766c77aa.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/47f06af7a3d5b2e7a3c57dde766c77aa.png)'
- en: Image from [nnU-Net article](https://arxiv.org/abs/1809.10486)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自 [nnU-Net 文章](https://arxiv.org/abs/1809.10486)
- en: Dynamic adaptation of network topologies
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络拓扑的动态适应
- en: Given the significant discrepancies in image size (consider the median shape
    of 482 × 512 × 512 for liver images versus 36 × 50 × 35 for hippocampus images),
    the nnU-Net intelligently adapts the input patch size and the number of pooling
    operations per axis. This essentially implies an automatic adjustment of the number
    of convolutional layers per dataset, facilitating the effective aggregation of
    spatial information. In addition to adapting to the varied image geometries, this
    model takes into account technical constraints, such as available memory.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于图像大小差异显著（考虑肝脏图像的中位形状为482 × 512 × 512，而海马体图像为36 × 50 × 35），nnU-Net智能地调整了输入补丁的大小以及每个轴的池化操作数量。这本质上意味着对每个数据集卷积层数量的自动调整，从而促进了空间信息的有效汇聚。除了适应不同的图像几何形状外，该模型还考虑了技术约束，例如可用内存。
- en: It’s crucial to note that the model doesn’t perform segmentation directly on
    the entire image but instead on carefully extracted patches with overlapping regions.
    The predictions on these patches are subsequently averaged, leading to the final
    segmentation output.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，该模型并不是直接在整个图像上进行分割，而是在精心提取的重叠区域的补丁上进行。对这些补丁的预测结果随后会进行平均，从而得到最终的分割输出。
- en: But having a large patch means more memory usage, and the batch size also consumes
    memory. The tradeoff taken is to always prioritize the patch size (the model’s
    capacity) rather than the batch size (only useful for optimization).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，较大的补丁意味着更多的内存使用，同时批量大小也会消耗内存。所做的权衡是始终优先考虑补丁大小（模型的容量）而非批量大小（仅对优化有用）。
- en: 'Here is the Heuristic algorithm used to compute the optimal patch size and
    batch size:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这是计算最佳补丁大小和批量大小的启发式算法：
- en: '![](../Images/7048bc42072c03bd4ca7efa8edaac8c8.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7048bc42072c03bd4ca7efa8edaac8c8.png)'
- en: Heuristic Rule for Batch and Patch Size, Image from [nnU-Net article](https://arxiv.org/abs/1809.10486)
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 批量和补丁大小的启发式规则，图片来自 [nnU-Net 文章](https://arxiv.org/abs/1809.10486)
- en: 'And this is what it looks like for different Datasets and input dimensions:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是在不同数据集和输入维度下的表现：
- en: '![](../Images/a092fe248ec185074a2cd72b71c96018.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a092fe248ec185074a2cd72b71c96018.png)'
- en: Architecture in function of the input image resolution, Image from [nnU-Net
    article](https://arxiv.org/abs/1809.10486)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 根据输入图像分辨率的架构，图片来自 [nnU-Net 文章](https://arxiv.org/abs/1809.10486)
- en: 'Great! Now Let’s quickly go over all the techniques used in nnU-Net:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 很好！现在让我们快速回顾一下 nnU-Net 中使用的所有技术：
- en: Training
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: All models are trained from scratch and evaluated using five-fold cross-validation
    on the training set, meaning that the original training dataset is randomly divided
    into five equal parts, or ‘folds’. In this cross-validation process, four of these
    folds are used for the training of the model, and the remaining one fold is used
    for the evaluation or testing. This process is then repeated five times, with
    each of the five folds being used exactly once as the evaluation set.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 所有模型都是从头开始训练的，并使用五折交叉验证进行评估，这意味着原始训练数据集被随机分为五个相等的部分或“折”。在这个交叉验证过程中，四个折用于模型的训练，剩下的一个折用于评估或测试。然后这个过程重复五次，每个折都被用作评估集一次。
- en: For the loss, we use a combination of Dice and Cross Entropy Loss. This is a
    very frequent loss in Image Segmentation. More details on the Dice Loss in [V-Net,
    the U-Net big’s brother](https://medium.com/ai-mind-labs/v-net-u-nets-big-brother-in-image-segmentation-906e393968f7)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对于损失，我们使用Dice损失和交叉熵损失的组合。这是图像分割中非常常见的损失。有关Dice损失的更多细节，请参阅[V-Net，U-Net的大兄弟](https://medium.com/ai-mind-labs/v-net-u-nets-big-brother-in-image-segmentation-906e393968f7)
- en: Data Augmentation techniques
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据增强技术
- en: The nnU-Net have a very strong Data Augmentation pipeline. The authors use random
    rotations, random scaling, random elastic deformation, gamma correction and mirroring.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: nnU-Net具有非常强大的数据增强管道。作者使用了随机旋转、随机缩放、随机弹性变形、伽马校正和镜像。
- en: 'NB: You can add your own transformations by modifying the source code'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: NB：你可以通过修改源代码添加自己的变换
- en: '![](../Images/56fa89a9ffb590f5d2d10c2f0d621694.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/56fa89a9ffb590f5d2d10c2f0d621694.png)'
- en: Elastic deformation, from this [article](https://www.researchgate.net/figure/Grid-distortion-and-elastic-transform-applied-to-a-medical-image_fig4_327742409)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 弹性变形，来自这篇[文章](https://www.researchgate.net/figure/Grid-distortion-and-elastic-transform-applied-to-a-medical-image_fig4_327742409)
- en: '![](../Images/d5d84805c71d600cd88c93a8d63a7aee.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d5d84805c71d600cd88c93a8d63a7aee.png)'
- en: Image from [OpenCV library](https://www.google.com/url?sa=i&url=https%3A%2F%2Fpyimagesearch.com%2F2015%2F10%2F05%2Fopencv-gamma-correction%2F&psig=AOvVaw2LDHscTrcGOq-tR4Ki2AbS&ust=1691168921010000&cd=vfe&opi=89978449&ved=0CBAQjRxqFwoTCKi4i6j9wIADFQAAAAAdAAAAABAE)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自[OpenCV库](https://www.google.com/url?sa=i&url=https%3A%2F%2Fpyimagesearch.com%2F2015%2F10%2F05%2Fopencv-gamma-correction%2F&psig=AOvVaw2LDHscTrcGOq-tR4Ki2AbS&ust=1691168921010000&cd=vfe&opi=89978449&ved=0CBAQjRxqFwoTCKi4i6j9wIADFQAAAAAdAAAAABAE)
- en: Patch based Inference
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于补丁的推断
- en: So as we said, the model does not predict directly on the full resolution image,
    it does that on extracted patches and then aggregates the prediction.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所说，模型不会直接在全分辨率图像上进行预测，而是在提取的补丁上进行预测，然后汇总预测结果。
- en: 'This is what it looks like:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是它的样子：
- en: '![](../Images/cb25522983df31b26778b3f89f8ad728.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb25522983df31b26778b3f89f8ad728.png)'
- en: Patch Based inference, Image by Author
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 基于补丁的推断，作者提供的图片
- en: 'NB: The patches in the center of the picture are given more weight than the
    ones on the side, because they contain more information and the model performs
    better on them'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: NB：图像中心的补丁比边缘的补丁赋予更高的权重，因为它们包含更多信息，模型在这些区域表现更好
- en: Pairwise Model Ensembling
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 成对模型集成
- en: '![](../Images/6da3d92b7f49ba1c22aaf9b0e0a7ad5c.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6da3d92b7f49ba1c22aaf9b0e0a7ad5c.png)'
- en: Model Ensembling, Image by author
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 模型集成，作者提供的图片
- en: So if you remember well, we can train up to 3 different models, 2D, 3D, and
    cascade. But when we make inference we can only use one model at a time right?
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你记得的话，我们可以训练多达3个不同的模型，2D、3D和级联。但在推断时，我们只能一次使用一个模型，对吗？
- en: Well turns out that no, different models have different strengths and weaknesses.
    So we can actually combine the predictions of several models so that if one model
    is very confident, we prioritize its prediction.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 实际情况是，不同模型具有不同的优缺点。因此，我们可以结合多个模型的预测，以便当一个模型非常自信时，我们优先考虑它的预测。
- en: nnU-Net tests every combination of 2 models among the 3 available models and
    picks up the best one.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: nnU-Net测试所有3个可用模型中的2个模型组合，并选择最佳的一个。
- en: 'In Practice, there are 2 ways to do that:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，有2种方法可以做到这一点：
- en: '**Hard voting:** For each pixel, we look at all the probabilities outputted
    by the 2 models, and we take the class with the highest probability.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**硬投票：** 对于每个像素，我们查看2个模型输出的所有概率，然后选择概率最高的类别。'
- en: '**Soft Voting:** For each pixel, we average the probability of the models,
    and then we take the class with the maximum probability.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**软投票：** 对于每个像素，我们计算模型概率的平均值，然后选择概率最大的类别。'
- en: Practical implementation
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实际实施
- en: Before we begin, you can download the dataset [here](https://drive.google.com/drive/folders/1rQeujQZs5rPH1AeI-KoUICOAzs4u-Eie?usp=sharing)
    and follow the [Google Collab notebook](https://colab.research.google.com/drive/1h6scef1i258x0abxT_FcSI_QBN9Eh_9e?usp=sharing).
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在我们开始之前，你可以从[这里](https://drive.google.com/drive/folders/1rQeujQZs5rPH1AeI-KoUICOAzs4u-Eie?usp=sharing)下载数据集，并跟随[Google
    Colab笔记本](https://colab.research.google.com/drive/1h6scef1i258x0abxT_FcSI_QBN9Eh_9e?usp=sharing)。
- en: If you did not understand anything about the first part, no worries, this is
    the practical part, you just need to follow me, and you are still going to get
    the best results.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有理解第一部分的内容，不用担心，这是实践部分，你只需跟随我，你仍然会获得最佳结果。
- en: You need a GPU to train the model otherwise it does not work. You can either
    do it locally, or on Google Collab, **don’t forget to change the runtime > GPU**
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要一个 GPU 来训练模型，否则无法运行。你可以在本地或者在 Google Collab 上进行训练，**不要忘记将运行时更改为 GPU**
- en: So, first of all, you need to have a dataset ready with input images and their
    corresponding segmentation. You can follow my tutorial by downloading this ready
    dataset for 3D Brain segmentation, and then you can replace it with your own dataset.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，首先，你需要准备一个包含输入图像及其对应分割的数据集。你可以按照我的教程下载这个用于 3D 大脑分割的准备好数据集，然后你可以用自己的数据集替换它。
- en: Downloading data
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载数据
- en: First of all you should download your data and place them in the data folder,
    by naming the two folders “input” and “ground_truth” which contains the segmentation.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你应该下载数据并将其放置在数据文件夹中，将两个文件夹命名为“input”和“ground_truth”，其中包含分割数据。
- en: 'For the rest of the tutorial I will use the MindBoggle dataset for image segmentation.
    You can download it on this [Google Drive](https://drive.google.com/drive/folders/1rQeujQZs5rPH1AeI-KoUICOAzs4u-Eie?usp=sharing):'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的教程中，我将使用 MindBoggle 数据集进行图像分割。你可以在这个[Google Drive](https://drive.google.com/drive/folders/1rQeujQZs5rPH1AeI-KoUICOAzs4u-Eie?usp=sharing)上下载：
- en: 'We are given 3D MRI scans of the Brain and we want to segment the White and
    Gray matter:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的是 3D 大脑 MRI 扫描，我们想要分割白质和灰质：
- en: '![](../Images/105b1c3c613a1f97eca045e11b3bac52.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/105b1c3c613a1f97eca045e11b3bac52.png)'
- en: Image by Author
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'It should look like this:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 它应该是这样的：
- en: '![](../Images/08fad5b409f9fd75a6221ba5f8318385.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08fad5b409f9fd75a6221ba5f8318385.png)'
- en: Tree, Image by Author
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 树，作者提供的图片
- en: Setting up the main directory
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置主目录
- en: If you run this on Google Colab, set collab = True, otherwise collab = False
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在 Google Colab 上运行此代码，将 collab 设置为 True，否则 collab 设置为 False
- en: '[PRE0]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now we are going to define a function that creates folders for us:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将定义一个函数来为我们创建文件夹：
- en: '[PRE1]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: And we use this function to create our “my_nnunet” folder where everything is
    going to be saved
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 并且我们使用这个函数来创建我们的“my_nnunet”文件夹，在那里一切都将被保存
- en: '[PRE2]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Library installation
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装库
- en: 'Now we are going to install all the requirements. First let’s install the nnunet
    library. If you are in a notebook run this in a cell:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将安装所有的依赖项。首先，让我们安装 nnunet 库。如果你在笔记本中，请在单元格中运行以下命令：
- en: '[PRE3]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Otherwise you can install nnunet directly from the terminal with
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 否则你可以直接通过终端安装 nnunet，命令如下：
- en: '[PRE4]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now we are going to clone the nnUnet git repository and NVIDIA apex. This contains
    the training scripts as well as a GPU accelerator.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将克隆 nnUnet git 仓库和 NVIDIA apex。这些包含了训练脚本以及 GPU 加速器。
- en: '[PRE5]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Creation of the folders
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建文件夹
- en: nnUnet requires a very specific structure for the folders.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: nnUnet 对文件夹结构有非常具体的要求。
- en: '[PRE6]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Originally the nnU-Net was designed for a decathlon challenge with different
    tasks. If you have different tasks just run this cell for all your tasks.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，nnU-Net 是为具有不同任务的十项全能挑战设计的。如果你有不同的任务，请为所有任务运行此单元格。
- en: '[PRE7]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You should have a structure like that now:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你应该有如下的结构：
- en: '![](../Images/90cb0cd8c344fe8b30fade3ab7cdc037.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/90cb0cd8c344fe8b30fade3ab7cdc037.png)'
- en: Image by Author
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: Setting the enironment variables
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置环境变量
- en: The script needs to know where you put your raw_data, where it can find the
    preprocessed data, and where it had to save the results.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本需要知道你将 raw_data 放在哪里，在哪里可以找到预处理的数据，以及需要将结果保存到哪里。
- en: '[PRE8]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Move the files in the right repositories:'
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将文件移动到正确的仓库中：
- en: 'We define a function that will move our images to the right repositories in
    the nnunet folder:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个函数，将我们的图像移动到 nnunet 文件夹中的正确仓库：
- en: '[PRE9]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now let’s run this function for the input and ground truth images:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们运行这个函数来处理输入和真实值图像：
- en: '[PRE10]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now we have to rename the files to be accepted by the nnUnet format, for example
    subject.nii.gz will become subject_0000.nii.gz
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要将文件重命名为 nnUnet 格式接受的名称，例如 subject.nii.gz 将变为 subject_0000.nii.gz
- en: '[PRE11]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Setting up the JSON file
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置 JSON 文件
- en: We are almost done!
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们快完成了！
- en: 'You mostly need to modify 2 things:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 你主要需要修改 2 件事：
- en: The Modality (if its CT or MRI this changes the normalization)
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模态（如果是 CT 或 MRI，这会改变归一化）
- en: 'The labels: Enter your own classes'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标签：输入你自己的类别
- en: '[PRE12]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Preprocess the data for nnU-Net format
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据预处理为 nnU-Net 格式
- en: This creates the dataset for the nnU-Net format
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建 nnU-Net 格式的数据集
- en: '[PRE13]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Train the models
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练模型
- en: We are now ready to train the models!
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好训练模型了！
- en: 'To train the 3D U-Net:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 训练 3D U-Net：
- en: '[PRE14]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To train the 2D U-Net:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 训练 2D U-Net：
- en: '[PRE15]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'To train the cascade model:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 训练级联模型：
- en: '[PRE16]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Note: If you pause the traning and want to resume it, add a “-c” in the end
    for “continue”.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果你暂停训练并想继续，末尾添加“-c”表示“继续”。
- en: 'For example:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: '[PRE17]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Inference
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推断
- en: 'Now we can run the inference:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以进行推断：
- en: '[PRE18]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Visualization of the predictions
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测的可视化
- en: First let’s check the training loss. This looks very healthy, and we have a
    Dice Score > 0.9 (green curve).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们检查训练损失。这看起来非常健康，我们有一个Dice分数 > 0.9（绿色曲线）。
- en: This is truly excellent for so little work and a 3D Neuroimaging segmentation
    task.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这么少的工作量和一个3D神经影像分割任务，这确实非常出色。
- en: '![](../Images/c211daa238ad5b5ee4c518148ee9fb4b.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c211daa238ad5b5ee4c518148ee9fb4b.png)'
- en: Training loss, test loss, validation Dice, Image by Author
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 训练损失、测试损失、验证Dice，图像由作者提供
- en: 'Let’s look at one sample:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个样本：
- en: '![](../Images/c6a69656aceafc62f4945052409777de.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c6a69656aceafc62f4945052409777de.png)'
- en: Prediction on the MindBoggle dataset, Image by Author
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 对MindBoggle数据集的预测，图像由作者提供
- en: The results are indeed impressive! It’s clear that the model has effectively
    learned how to segment brain images with high accuracy. While there may be minor
    imperfections, it’s important to remember that the field of image segmentation
    is advancing rapidly, and we’re making significant strides towards perfection.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 结果确实令人印象深刻！显然模型已经有效地学会了如何高精度地对脑部图像进行分割。虽然可能存在一些小的瑕疵，但要记住，图像分割领域正在快速发展，我们正朝着完美迈出重要步伐。
- en: In the future, there’s scope to further optimize the performance of nnU-Net,
    but that will be for an other article
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 未来有机会进一步优化nnU-Net的性能，但这将是另一篇文章的内容。
- en: 'Thanks for reading! Before you go:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！在你离开之前：
- en: Check my [compilation of AI tutorials](https://github.com/FrancoisPorcher/awesome-ai-tutorials)
    on Github
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看我在GitHub上的[AI教程合集](https://github.com/FrancoisPorcher/awesome-ai-tutorials)
- en: '[](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----6dda7f44b935--------------------------------)
    [## GitHub - FrancoisPorcher/awesome-ai-tutorials: The best collection of AI tutorials
    to make you a…'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----6dda7f44b935--------------------------------)
    [## GitHub - FrancoisPorcher/awesome-ai-tutorials: 最佳AI教程合集，让你成为…'
- en: The best collection of AI tutorials to make you a boss of Data Science! - GitHub
    …
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最佳AI教程合集，让你成为数据科学的高手！ - GitHub …
- en: github.com](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----6dda7f44b935--------------------------------)
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/FrancoisPorcher/awesome-ai-tutorials?source=post_page-----6dda7f44b935--------------------------------)
- en: Y*ou should get my articles in your inbox.* [***Subscribe here.***](https://medium.com/@francoisporcher/subscribe)
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '*你应该在收件箱中收到我的文章。* [***订阅这里。***](https://medium.com/@francoisporcher/subscribe)'
- en: '*If you want to have access to premium articles on Medium, you only need a
    membership for $5 a month. If you sign up* [***with my link***](https://medium.com/@francoisporcher/membership)*,
    you support me with a part of your fee without additional costs.*'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你想访问Medium上的高级文章，只需每月$5的会员资格。如果你通过* [***我的链接***](https://medium.com/@francoisporcher/membership)*注册，你将以没有额外费用的方式支持我。*'
- en: If you found this article insightful and beneficial, please consider following
    me and leaving a clap for more in-depth content! Your support helps me continue
    producing content that aids our collective understanding.
  id: totrans-159
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你觉得这篇文章有见地且有益，请考虑关注我并点赞以获取更多深入内容！你的支持帮助我继续制作有助于我们共同理解的内容。
- en: References
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Ronneberger, O., Fischer, P., & Brox, T. (2015). U-net: Convolutional networks
    for biomedical image segmentation. In International Conference on Medical image
    computing and computer-assisted intervention (pp. 234–241). Springer, Cham.'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ronneberger, O., Fischer, P., & Brox, T. (2015). U-net：用于生物医学图像分割的卷积网络。在医学图像计算与计算机辅助手术国际会议（第234–241页）。Springer,
    Cham。
- en: 'Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H.
    (2021). nnU-Net: a self-configuring method for deep learning-based biomedical
    image segmentation. Nature Methods, 18(2), 203–211.'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021).
    nnU-Net：一种自配置的深度学习生物医学图像分割方法。Nature Methods, 18(2), 203–211。
- en: 'Ioffe, S., & Szegedy, C. (2015). Batch normalization: Accelerating deep network
    training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167.'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ioffe, S., & Szegedy, C. (2015). 批量归一化：通过减少内部协变量偏移加速深度网络训练。arXiv 预印本 arXiv:1502.03167。
- en: 'Ulyanov, D., Vedaldi, A., & Lempitsky, V. (2016). Instance normalization: The
    missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022.'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ulyanov, D., Vedaldi, A., & Lempitsky, V. (2016). 实例归一化：快速风格化的缺失成分。arXiv 预印本
    arXiv:1607.08022。
- en: '[MindBoggle dataset](https://mindboggle.info)'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[MindBoggle 数据集](https://mindboggle.info)'
