- en: Training Sentence Transformers with Softmax Loss
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练句子变换器与 Softmax 损失
- en: 原文：[https://towardsdatascience.com/training-sentence-transformers-with-softmax-loss-c114e74e1583?source=collection_archive---------17-----------------------#2023-01-12](https://towardsdatascience.com/training-sentence-transformers-with-softmax-loss-c114e74e1583?source=collection_archive---------17-----------------------#2023-01-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/training-sentence-transformers-with-softmax-loss-c114e74e1583?source=collection_archive---------17-----------------------#2023-01-12](https://towardsdatascience.com/training-sentence-transformers-with-softmax-loss-c114e74e1583?source=collection_archive---------17-----------------------#2023-01-12)
- en: '[NLP For Semantic Search](https://jamescalam.medium.com/list/nlp-for-semantic-search-d3a4b96a52fe)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[NLP 语义搜索](https://jamescalam.medium.com/list/nlp-for-semantic-search-d3a4b96a52fe)'
- en: How the original sentence transformer (SBERT) was built
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何构建原始句子变换器（SBERT）
- en: '[](https://jamescalam.medium.com/?source=post_page-----c114e74e1583--------------------------------)[![James
    Briggs](../Images/cb34b7011748e4d8607b7ff4a8510a93.png)](https://jamescalam.medium.com/?source=post_page-----c114e74e1583--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c114e74e1583--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c114e74e1583--------------------------------)
    [James Briggs](https://jamescalam.medium.com/?source=post_page-----c114e74e1583--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[![James Briggs](../Images/cb34b7011748e4d8607b7ff4a8510a93.png)](https://jamescalam.medium.com/?source=post_page-----c114e74e1583--------------------------------)
    [![Towards Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c114e74e1583--------------------------------)
    [James Briggs](https://jamescalam.medium.com/?source=post_page-----c114e74e1583--------------------------------)'
- en: ·
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb9d77a4ca1d1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-sentence-transformers-with-softmax-loss-c114e74e1583&user=James+Briggs&userId=b9d77a4ca1d1&source=post_page-b9d77a4ca1d1----c114e74e1583---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c114e74e1583--------------------------------)
    ·9 min read·Jan 12, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fc114e74e1583&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-sentence-transformers-with-softmax-loss-c114e74e1583&user=James+Briggs&userId=b9d77a4ca1d1&source=-----c114e74e1583---------------------clap_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb9d77a4ca1d1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-sentence-transformers-with-softmax-loss-c114e74e1583&user=James+Briggs&userId=b9d77a4ca1d1&source=post_page-b9d77a4ca1d1----c114e74e1583---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c114e74e1583--------------------------------)
    ·9 min read·2023年1月12日'
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fc114e74e1583&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftraining-sentence-transformers-with-softmax-loss-c114e74e1583&source=-----c114e74e1583---------------------bookmark_footer-----------)![](../Images/e9e629a36ae10fd37e92fbd2fa3327d2.png)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/e9e629a36ae10fd37e92fbd2fa3327d2.png) '
- en: Photo by [Possessed Photography](https://unsplash.com/@possessedphotography?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral). Originally
    posted in the [NLP for Semantic Search ebook](https://www.pinecone.io/learn/sentence-embeddings/)
    at Pinecone (where the author is employed).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Possessed Photography](https://unsplash.com/@possessedphotography?utm_source=medium&utm_medium=referral)
    提供，发布在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
    上。原文发布于 [NLP for Semantic Search ebook](https://www.pinecone.io/learn/sentence-embeddings/)
    在 Pinecone（作者的工作单位）。
- en: Search is entering a golden age. Thanks to *“sentence embeddings”* and specially
    trained models called*“sentence transformers”* we can now search for information
    using concepts rather than keyword matching. Unlocking a human-like information
    discovery process.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索正进入黄金时代。得益于*“句子嵌入”*和特别训练的模型*“句子转换器”*，我们现在可以通过概念而非关键词匹配来搜索信息，从而解锁类人信息发现过程。
- en: This article will explore the training process of the first sentence transformer,
    *sentence-BERT* — more commonly known as *SBERT*. We will explore the **N**atural
    **L**anguage **I**nference (NLI) training approach of *softmax loss* to fine-tune
    models for producing sentence embeddings.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将探讨第一个句子转换器的训练过程，即 *sentence-BERT*——更常被称为 *SBERT*。我们将深入探讨**N**atural **L**anguage
    **I**nference (NLI) 训练方法中的 *softmax 损失*，以微调模型以生成句子嵌入。
- en: Be aware that softmax loss is no longer the preferred approach to training sentence
    transformers and has been superseded by other methods such as MSE margin and [multiple
    negatives ranking loss](https://www.pinecone.io/learn/fine-tune-sentence-transformers-mnr/).
    But we’re covering this training method as an important milestone in the development
    of ever-improving sentence embeddings.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，softmax 损失不再是训练句子转换器的首选方法，而是被 MSE margin 和 [多负样本排序损失](https://www.pinecone.io/learn/fine-tune-sentence-transformers-mnr/)
    等其他方法取代。但我们仍将这一训练方法作为句子嵌入不断改进过程中的一个重要里程碑。
- en: This article also covers *two approaches* to fine-tuning. The first shows how
    NLI training with softmax loss works. The second uses the excellent training utilities
    provided by the `sentence-transformers` library — it’s more abstracted, making
    building good sentence transformer models *much easier*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本文还涵盖了*两种方法*来进行微调。第一种方法展示了 softmax 损失的 NLI 训练如何工作。第二种方法使用了 `sentence-transformers`
    库提供的优秀训练工具——它更具抽象性，使得构建良好的句子转换器模型*容易得多*。
