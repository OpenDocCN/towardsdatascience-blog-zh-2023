- en: Building a Conformal Chatbot in Julia
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åœ¨Juliaä¸­æ„å»ºä¸€ä¸ªç¬¦åˆé¢„æµ‹çš„èŠå¤©æœºå™¨äºº
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/building-a-conformal-chatbot-in-julia-1ed23363a280](https://towardsdatascience.com/building-a-conformal-chatbot-in-julia-1ed23363a280)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/building-a-conformal-chatbot-in-julia-1ed23363a280](https://towardsdatascience.com/building-a-conformal-chatbot-in-julia-1ed23363a280)
- en: Conformal Prediction, LLMs and HuggingFace â€” Part 1
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¦åˆé¢„æµ‹ã€LLMså’ŒHuggingFace â€” ç¬¬1éƒ¨åˆ†
- en: '[](https://medium.com/@patrick.altmeyer?source=post_page-----1ed23363a280--------------------------------)[![Patrick
    Altmeyer](../Images/b4c0bd875390f6dc8b81480f0712fea5.png)](https://medium.com/@patrick.altmeyer?source=post_page-----1ed23363a280--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1ed23363a280--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1ed23363a280--------------------------------)
    [Patrick Altmeyer](https://medium.com/@patrick.altmeyer?source=post_page-----1ed23363a280--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@patrick.altmeyer?source=post_page-----1ed23363a280--------------------------------)[![Patrick
    Altmeyer](../Images/b4c0bd875390f6dc8b81480f0712fea5.png)](https://medium.com/@patrick.altmeyer?source=post_page-----1ed23363a280--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1ed23363a280--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1ed23363a280--------------------------------)
    [Patrick Altmeyer](https://medium.com/@patrick.altmeyer?source=post_page-----1ed23363a280--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1ed23363a280--------------------------------)
    Â·7 min readÂ·Jul 5, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page-----1ed23363a280--------------------------------)
    Â·é˜…è¯»æ—¶é—´7åˆ†é’ŸÂ·2023å¹´7æœˆ5æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Large Language Models (LLM) are all the buzz right now. They are used for a
    variety of tasks, including text classification, question answering, and text
    generation. In this tutorial, we will show how to conformalize a transformer language
    model for text classification using `[ConformalPrediction.jl](https://juliatrustworthyai.github.io/ConformalPrediction.jl/dev/)`.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç›®å‰éå¸¸å—å…³æ³¨ã€‚å®ƒä»¬è¢«ç”¨äºå„ç§ä»»åŠ¡ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ†ç±»ã€é—®ç­”å’Œæ–‡æœ¬ç”Ÿæˆã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨`[ConformalPrediction.jl](https://juliatrustworthyai.github.io/ConformalPrediction.jl/dev/)`å°†å˜æ¢å™¨è¯­è¨€æ¨¡å‹ç¬¦åˆåŒ–ï¼Œä»¥è¿›è¡Œæ–‡æœ¬åˆ†ç±»ã€‚
- en: ğŸ‘€ At a Glance
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ‘€ ä¸€è§ˆ
- en: In particular, we are interested in the task of intent classification as illustrated
    in the sketch below. Firstly, we feed a customer query into an LLM to generate
    embeddings. Next, we train a classifier to match these embeddings to possible
    intents. Of course, for this supervised learning problem we need training data
    consisting of inputs â€” queries â€” and outputs â€” labels indicating the true intent.
    Finally, we apply Conformal Predition to quantify the predictive uncertainty of
    our classifier.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç‰¹åˆ«å…³æ³¨æ„å›¾åˆ†ç±»ä»»åŠ¡ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†å®¢æˆ·æŸ¥è¯¢è¾“å…¥åˆ°LLMä¸­ä»¥ç”ŸæˆåµŒå…¥ã€‚æ¥ç€ï¼Œæˆ‘ä»¬è®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨ï¼Œå°†è¿™äº›åµŒå…¥ä¸å¯èƒ½çš„æ„å›¾åŒ¹é…ã€‚å½“ç„¶ï¼Œå¯¹äºè¿™ä¸ªç›‘ç£å­¦ä¹ é—®é¢˜ï¼Œæˆ‘ä»¬éœ€è¦ç”±è¾“å…¥â€”â€”æŸ¥è¯¢â€”â€”å’Œè¾“å‡ºâ€”â€”æŒ‡ç¤ºçœŸå®æ„å›¾çš„æ ‡ç­¾â€”â€”ç»„æˆçš„è®­ç»ƒæ•°æ®ã€‚æœ€åï¼Œæˆ‘ä»¬åº”ç”¨ç¬¦åˆé¢„æµ‹æ¥é‡åŒ–åˆ†ç±»å™¨çš„é¢„æµ‹ä¸ç¡®å®šæ€§ã€‚
- en: Conformal Prediction (CP) is a rapidly emerging methodology for Predictive Uncertainty
    Quantification. If youâ€™re unfamiliar with CP, you may want to first check out
    my 3-part introductory series on the topic starting with this [post](https://medium.com/towards-data-science/conformal-prediction-in-julia-351b81309e30).
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ç¬¦åˆé¢„æµ‹ï¼ˆCPï¼‰æ˜¯ä¸€ç§å¿«é€Ÿå‘å±•çš„é¢„æµ‹ä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•ã€‚å¦‚æœä½ ä¸ç†Ÿæ‚‰CPï¼Œå»ºè®®ä½ é¦–å…ˆæŸ¥çœ‹æˆ‘å…³äºè¿™ä¸€ä¸»é¢˜çš„ä¸‰éƒ¨åˆ†ä»‹ç»ç³»åˆ—ï¼Œä»è¿™ç¯‡[æ–‡ç« ](https://medium.com/towards-data-science/conformal-prediction-in-julia-351b81309e30)å¼€å§‹ã€‚
- en: '![](../Images/2bdcbc51d0e1128fa62d46fefe8a4188.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2bdcbc51d0e1128fa62d46fefe8a4188.png)'
- en: High-level overview of a conformalized intent classifier. Image by author.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¦åˆåŒ–æ„å›¾åˆ†ç±»å™¨çš„é«˜çº§æ¦‚è¿°ã€‚å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: ğŸ¤— HuggingFace
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ¤— HuggingFace
- en: We will use the [Banking77](https://arxiv.org/abs/2003.04807) dataset (Casanueva
    et al., 2020), which consists of 13,083 queries from 77 intents related to banking.
    On the model side, we will use the [DistilRoBERTa](https://huggingface.co/mrm8488/distilroberta-finetuned-banking77)
    model, which is a distilled version of [RoBERTa](https://arxiv.org/abs/1907.11692)
    (Liu et al., 2019) fine-tuned on the Banking77 dataset.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨[Banking77](https://arxiv.org/abs/2003.04807)æ•°æ®é›†ï¼ˆCasanuevaç­‰ï¼Œ2020ï¼‰ï¼Œè¯¥æ•°æ®é›†åŒ…å«77ä¸ªä¸é“¶è¡Œç›¸å…³çš„æ„å›¾ä¸­çš„13,083ä¸ªæŸ¥è¯¢ã€‚åœ¨æ¨¡å‹æ–¹é¢ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨[DistilRoBERTa](https://huggingface.co/mrm8488/distilroberta-finetuned-banking77)æ¨¡å‹ï¼Œå®ƒæ˜¯[RoBERTa](https://arxiv.org/abs/1907.11692)ï¼ˆLiuç­‰ï¼Œ2019ï¼‰çš„è’¸é¦ç‰ˆï¼Œå¹¶åœ¨Banking77æ•°æ®é›†ä¸Šè¿›è¡Œäº†å¾®è°ƒã€‚
- en: The model can be loaded from HF straight into our running Julia session using
    the `[Transformers.jl](https://github.com/chengchingwen/Transformers.jl/tree/master)`
    package.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥ä½¿ç”¨ `[Transformers.jl](https://github.com/chengchingwen/Transformers.jl/tree/master)`
    åŒ…å°†æ¨¡å‹ä»HFç›´æ¥åŠ è½½åˆ°æˆ‘ä»¬æ­£åœ¨è¿è¡Œçš„Juliaä¼šè¯ä¸­ã€‚
- en: This package makes working with HF models remarkably easy in Julia. Kudos to
    the devs! *ğŸ™*
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è¿™ä¸ªåŒ…ä½¿å¾—åœ¨Juliaä¸­ä½¿ç”¨HFæ¨¡å‹å˜å¾—éå¸¸ç®€å•ã€‚å‘å¼€å‘è€…ä»¬è‡´æ•¬ï¼*ğŸ™*
- en: Below we load the tokenizer `tkr` and the model `mod`. The tokenizer is used
    to convert the text into a sequence of integers, which is then fed into the model.
    The model outputs a hidden state, which is then fed into a classifier to get the
    logits for each class. Finally, the logits are then passed through a softmax function
    to get the corresponding predicted probabilities. Below we run a few queries through
    the model to see how it performs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢æˆ‘ä»¬åŠ è½½åˆ†è¯å™¨`tkr`å’Œæ¨¡å‹`mod`ã€‚åˆ†è¯å™¨ç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºæ•´æ•°åºåˆ—ï¼Œç„¶åå°†å…¶è¾“å…¥æ¨¡å‹ã€‚æ¨¡å‹è¾“å‡ºä¸€ä¸ªéšè—çŠ¶æ€ï¼Œç„¶åå°†å…¶è¾“å…¥åˆ†ç±»å™¨ï¼Œä»¥è·å¾—æ¯ä¸ªç±»åˆ«çš„logitsã€‚æœ€åï¼Œè¿™äº›logitsé€šè¿‡softmaxå‡½æ•°ä»¥è·å¾—ç›¸åº”çš„é¢„æµ‹æ¦‚ç‡ã€‚ä¸‹é¢æˆ‘ä»¬è¿è¡Œå‡ ä¸ªæŸ¥è¯¢æ¥æŸ¥çœ‹æ¨¡å‹çš„è¡¨ç°ã€‚
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ğŸ” `MLJ` Interface
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ” `MLJ`æ¥å£
- en: Since our package is interfaced to `[MLJ.jl](https://alan-turing-institute.github.io/MLJ.jl/dev/)`,
    we need to define a wrapper model that conforms to the `MLJ` interface. In order
    to add the model for general use, we would probably go through `[MLJFlux.jl](https://github.com/FluxML/MLJFlux.jl)`,
    but for this tutorial, we will make our life easy and simply overload the `MLJBase.fit`
    and `MLJBase.predict` methods.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæˆ‘ä»¬çš„åŒ…ä¸ `[MLJ.jl](https://alan-turing-institute.github.io/MLJ.jl/dev/)` æ¥å£å¯¹æ¥ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€ä¸ªç¬¦åˆ`MLJ`æ¥å£çš„åŒ…è£…æ¨¡å‹ã€‚ä¸ºäº†å°†æ¨¡å‹æ·»åŠ åˆ°é€šç”¨ä½¿ç”¨ä¸­ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šé€šè¿‡
    `[MLJFlux.jl](https://github.com/FluxML/MLJFlux.jl)` æ¥å®ç°ï¼Œä½†åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ç®€åŒ–æ“ä½œï¼Œç›´æ¥é‡è½½`MLJBase.fit`å’Œ`MLJBase.predict`æ–¹æ³•ã€‚
- en: Since the model from HF is already pre-trained and we are not interested in
    further fine-tuning, we will simply return the model object in the `MLJBase.fit`
    method. The `MLJBase.predict` method will then take the model object and the query
    and return the predicted probabilities. We also need to define the `MLJBase.target_scitype`
    and `MLJBase.predict_mode` methods. The former tells `MLJ` what the output type
    of the model is, and the latter can be used to retrieve the label with the highest
    predicted probability.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºHFçš„æ¨¡å‹å·²ç»æ˜¯é¢„è®­ç»ƒçš„ï¼Œæˆ‘ä»¬ä¸æ‰“ç®—è¿›ä¸€æ­¥å¾®è°ƒï¼Œå› æ­¤æˆ‘ä»¬å°†åœ¨`MLJBase.fit`æ–¹æ³•ä¸­ç®€å•åœ°è¿”å›æ¨¡å‹å¯¹è±¡ã€‚`MLJBase.predict`æ–¹æ³•å°†æ¥æ”¶æ¨¡å‹å¯¹è±¡å’ŒæŸ¥è¯¢ï¼Œå¹¶è¿”å›é¢„æµ‹æ¦‚ç‡ã€‚æˆ‘ä»¬è¿˜éœ€è¦å®šä¹‰`MLJBase.target_scitype`å’Œ`MLJBase.predict_mode`æ–¹æ³•ã€‚å‰è€…å‘Šè¯‰`MLJ`æ¨¡å‹çš„è¾“å‡ºç±»å‹æ˜¯ä»€ä¹ˆï¼Œåè€…å¯ä»¥ç”¨æ¥æ£€ç´¢å…·æœ‰æœ€é«˜é¢„æµ‹æ¦‚ç‡çš„æ ‡ç­¾ã€‚
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To test that everything is working as expected, we fit the model and generated
    predictions for a subset of the test data:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æµ‹è¯•ä¸€åˆ‡æ˜¯å¦æŒ‰é¢„æœŸå·¥ä½œï¼Œæˆ‘ä»¬æ‹Ÿåˆäº†æ¨¡å‹å¹¶ä¸ºæµ‹è¯•æ•°æ®çš„å­é›†ç”Ÿæˆäº†é¢„æµ‹ï¼š
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note that even though the LLM weâ€™re using here isnâ€™t really that large at all,
    even a simple forward pass does take considerable time.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œå³ä½¿æˆ‘ä»¬ä½¿ç”¨çš„LLMå¹¶ä¸å¤§ï¼Œä½†å³ä½¿æ˜¯ç®€å•çš„å‰å‘ä¼ é€’ä¹Ÿéœ€è¦ç›¸å½“çš„æ—¶é—´ã€‚
- en: ğŸ¤– Conformal Chatbot
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ¤– åˆæˆèŠå¤©æœºå™¨äºº
- en: To turn the wrapped, pre-trained model into a conformal intent classifier, we
    can now rely on standard API calls. We first wrap our atomic model where we also
    specify the desired coverage rate and method. Since even simple forward passes
    are computationally expensive for our (small) LLM, we rely on Simple Inductive
    Conformal Classification.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å°†åŒ…è£…å¥½çš„é¢„è®­ç»ƒæ¨¡å‹è½¬å˜ä¸ºåˆæˆæ„å›¾åˆ†ç±»å™¨ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥ä¾é æ ‡å‡†APIè°ƒç”¨ã€‚æˆ‘ä»¬é¦–å…ˆåŒ…è£…æˆ‘ä»¬çš„åŸå­æ¨¡å‹ï¼Œå¹¶æŒ‡å®šæ‰€éœ€çš„è¦†ç›–ç‡å’Œæ–¹æ³•ã€‚ç”±äºå³ä½¿æ˜¯ç®€å•çš„å‰å‘ä¼ é€’å¯¹æˆ‘ä»¬ï¼ˆå°ï¼‰LLMæ¥è¯´ä¹Ÿéå¸¸è®¡ç®—å¯†é›†ï¼Œæˆ‘ä»¬ä¾èµ–äºç®€å•å½’çº³åˆæˆåˆ†ç±»ã€‚
- en: '[PRE5]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, we use our conformal LLM to build a simple yet powerful chatbot that
    runs directly in the Julia REPL. Without dwelling on the details too much, the
    `conformal_chatbot` works as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬ä½¿ç”¨åˆæˆLLMæ„å»ºä¸€ä¸ªç®€å•è€Œå¼ºå¤§çš„èŠå¤©æœºå™¨äººï¼Œç›´æ¥åœ¨Julia REPLä¸­è¿è¡Œã€‚åœ¨ä¸è¯¦ç»†æ¢è®¨ç»†èŠ‚çš„æƒ…å†µä¸‹ï¼Œ`conformal_chatbot`
    çš„å·¥ä½œåŸç†å¦‚ä¸‹ï¼š
- en: Prompt user to explain their intent.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æç¤ºç”¨æˆ·è§£é‡Šä»–ä»¬çš„æ„å›¾ã€‚
- en: Feed user input through conformal LLM and present the output to the user.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€šè¿‡åˆæˆLLMå¤„ç†ç”¨æˆ·è¾“å…¥ï¼Œå¹¶å°†è¾“å‡ºå‘ˆç°ç»™ç”¨æˆ·ã€‚
- en: If the conformal prediction set includes more than one label, prompt the user
    to either refine their input or choose one of the options included in the set.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚æœåˆæˆé¢„æµ‹é›†åŒ…å«å¤šä¸ªæ ‡ç­¾ï¼Œè¯·æç¤ºç”¨æˆ·è¦ä¹ˆç»†åŒ–è¾“å…¥ï¼Œè¦ä¹ˆé€‰æ‹©é¢„æµ‹é›†ä¸­çš„é€‰é¡¹ä¹‹ä¸€ã€‚
- en: 'The following code implements these ideas:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹ä»£ç å®ç°äº†è¿™äº›æƒ³æ³•ï¼š
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Below we show the output for two example queries. The first one is very ambiguous
    (and misspelled as I just realised): â€œtransfer mondey?â€. As expected, the size
    of the prediction set is therefore large.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢æˆ‘ä»¬å±•ç¤ºäº†ä¸¤ä¸ªç¤ºä¾‹æŸ¥è¯¢çš„è¾“å‡ºã€‚ç¬¬ä¸€ä¸ªæŸ¥è¯¢éå¸¸æ¨¡ç³Šï¼ˆè€Œä¸”åˆšåˆšå‘ç°æ‹¼å†™é”™è¯¯ï¼‰ï¼šâ€œtransfer mondeyï¼Ÿâ€ã€‚å› æ­¤ï¼Œé¢„æµ‹é›†çš„å¤§å°å¾ˆå¤§ã€‚
- en: '[PRE7]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following is a more refined version of the prompt: â€œI tried to transfer
    money to my friend, but it failedâ€. It yields a smaller prediction set, since
    less ambiguous prompts result in lower predictive uncertainty.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯æ›´ç²¾ç‚¼çš„æç¤ºç‰ˆæœ¬ï¼šâ€œæˆ‘è¯•å›¾ç»™æœ‹å‹è½¬è´¦ï¼Œä½†å¤±è´¥äº†ã€‚â€ ç”±äºä¸é‚£ä¹ˆæ¨¡ç³Šçš„æç¤ºä¼šå¯¼è‡´è¾ƒä½çš„é¢„æµ‹ä¸ç¡®å®šæ€§ï¼Œå› æ­¤å®ƒäº§ç”Ÿäº†è¾ƒå°çš„é¢„æµ‹é›†ã€‚
- en: '[PRE9]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The video below shows the REPL-based chatbot in action. You can recreate this
    yourself and run the bot right from you terminal. To do so, just check out the
    original [post](https://www.paltmeyer.com/blog/posts/conformal-llm/) on my blog
    to find the full source code.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢çš„è§†é¢‘å±•ç¤ºäº†REPLåŸºç¡€èŠå¤©æœºå™¨äººåœ¨å®é™…åº”ç”¨ä¸­çš„è¡¨ç°ã€‚ä½ å¯ä»¥è‡ªå·±é‡ç°è¿™ä¸ªè¿‡ç¨‹ï¼Œå¹¶ç›´æ¥ä»ä½ çš„ç»ˆç«¯è¿è¡Œæœºå™¨äººã€‚ä¸ºæ­¤ï¼Œè¯·æŸ¥çœ‹æˆ‘åšå®¢ä¸Šçš„åŸå§‹[å¸–å­](https://www.paltmeyer.com/blog/posts/conformal-llm/)ä»¥è·å–å®Œæ•´çš„æºä»£ç ã€‚
- en: '![](../Images/397e9866869f4464276f0c5e5bb44e9c.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/397e9866869f4464276f0c5e5bb44e9c.png)'
- en: Demo of the REPL-based Conformal Chatbot. Created by author.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: REPLåŸºç¡€çš„ç¬¦åˆæ€§èŠå¤©æœºå™¨äººçš„æ¼”ç¤ºã€‚ç”±ä½œè€…åˆ›å»ºã€‚
- en: ğŸŒ¯ Wrapping Up
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸŒ¯ æ€»ç»“
- en: This work was done in collaboration with colleagues at ING as part of the ING
    Analytics 2023 Experiment Week. Our team demonstrated that Conformal Prediction
    provides a powerful and principled alternative to top-*K* intent classification.
    We won the first prize by popular vote.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é¡¹å·¥ä½œæ˜¯ä¸INGçš„åŒäº‹åˆä½œå®Œæˆçš„ï¼Œä½œä¸ºING Analytics 2023å®éªŒå‘¨çš„ä¸€éƒ¨åˆ†ã€‚æˆ‘ä»¬çš„å›¢é˜Ÿå±•ç¤ºäº†ç¬¦åˆæ€§é¢„æµ‹æä¾›äº†å¯¹é¡¶çº§-*K*æ„å›¾åˆ†ç±»çš„å¼ºå¤§è€Œæœ‰åŸåˆ™çš„æ›¿ä»£æ–¹æ¡ˆã€‚æˆ‘ä»¬é€šè¿‡å¤§ä¼—æŠ•ç¥¨èµ¢å¾—äº†ç¬¬ä¸€åã€‚
- en: Of course, there are a lot of things that can be improved here. As far as Large
    LMs are concerned, we have used a small one. In terms of Conformal Prediction,
    we have only looked at simple inductive conformal classification. This is a good
    starting point, but there are more advanced methods available, which are implemented
    in the package and were investigated during the competition. Another thing we
    did not take into consideration here is that we have many outcome classes and
    may in practice be interested in achieving class-conditional coverage. Stay tuned
    for more on this in future posts.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œè¿™é‡Œè¿˜æœ‰å¾ˆå¤šå¯ä»¥æ”¹è¿›çš„åœ°æ–¹ã€‚å°±å¤§å‹è¯­è¨€æ¨¡å‹è€Œè¨€ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªè¾ƒå°çš„æ¨¡å‹ã€‚åœ¨ç¬¦åˆæ€§é¢„æµ‹æ–¹é¢ï¼Œæˆ‘ä»¬åªå…³æ³¨äº†ç®€å•çš„å½’çº³ç¬¦åˆæ€§åˆ†ç±»ã€‚è¿™æ˜¯ä¸€ä¸ªå¥½çš„èµ·ç‚¹ï¼Œä½†è¿˜æœ‰æ›´é«˜çº§çš„æ–¹æ³•å¯ç”¨ï¼Œè¿™äº›æ–¹æ³•å·²ç»åœ¨è½¯ä»¶åŒ…ä¸­å®ç°ï¼Œå¹¶åœ¨ç«èµ›ä¸­è¿›è¡Œäº†ç ”ç©¶ã€‚å¦ä¸€ä¸ªæˆ‘ä»¬æ²¡æœ‰è€ƒè™‘çš„æ–¹é¢æ˜¯æˆ‘ä»¬æœ‰è®¸å¤šç»“æœç±»åˆ«ï¼Œå®é™…ä¸Šå¯èƒ½å¸Œæœ›å®ç°ç±»åˆ«æ¡ä»¶è¦†ç›–ã€‚è¯·å…³æ³¨æœªæ¥çš„å¸–å­äº†è§£æ›´å¤šå†…å®¹ã€‚
- en: If youâ€™re interested in finding out more about Conformal Prediction in Julia,
    go ahead and check out the [repo](https://github.com/JuliaTrustworthyAI/ConformalPrediction.jl)
    and [docs](https://juliatrustworthyai.github.io/ConformalPrediction.jl/dev/).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å¯¹åœ¨Juliaä¸­äº†è§£æ›´å¤šå…³äºç¬¦åˆæ€§é¢„æµ‹çš„å†…å®¹æ„Ÿå…´è¶£ï¼Œè¯·æŸ¥çœ‹[ä»£ç åº“](https://github.com/JuliaTrustworthyAI/ConformalPrediction.jl)å’Œ[æ–‡æ¡£](https://juliatrustworthyai.github.io/ConformalPrediction.jl/dev/)ã€‚
- en: ğŸ‰ JuliaCon 2023 is around the corner and this year I will be giving a [talk](https://pretalx.com/juliacon2023/talk/JQWNNP/)
    about *ConformalPrediction.jl.* Check out the details of my talk [here](https://pretalx.com/juliacon2023/talk/JQWNNP/)
    and have a look at the full jam-packed conference [schedule](https://pretalx.com/juliacon2023/schedule/).
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ğŸ‰ JuliaCon 2023 å³å°†åˆ°æ¥ï¼Œä»Šå¹´æˆ‘å°†è¿›è¡Œä¸€åœºå…³äº*ConformalPrediction.jl*çš„[è®²åº§](https://pretalx.com/juliacon2023/talk/JQWNNP/)ã€‚è¯·æŸ¥çœ‹æˆ‘çš„è®²åº§[è¯¦ç»†ä¿¡æ¯](https://pretalx.com/juliacon2023/talk/JQWNNP/)ï¼Œå¹¶æµè§ˆå†…å®¹ä¸°å¯Œçš„ä¼šè®®[æ—¥ç¨‹](https://pretalx.com/juliacon2023/schedule/)ã€‚
- en: ğŸ“ References
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ğŸ“ å‚è€ƒæ–‡çŒ®
- en: 'Casanueva, IÃ±igo, Tadas TemÄinas, Daniela Gerz, Matthew Henderson, and Ivan
    VuliÄ‡. 2020\. â€œEfficient Intent Detection with Dual Sentence Encoders.â€ In *Proceedings
    of the 2nd Workshop on Natural Language Processing for Conversational AI* , 38â€“45\.
    Online: Association for Computational Linguistics. [https://doi.org/10.18653/v1/2020.nlp4convai-1.5](https://doi.org/10.18653/v1/2020.nlp4convai-1.5).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Casanueva, IÃ±igo, Tadas TemÄinas, Daniela Gerz, Matthew Henderson, å’Œ Ivan VuliÄ‡.
    2020\. â€œä½¿ç”¨åŒå¥å­ç¼–ç å™¨çš„é«˜æ•ˆæ„å›¾æ£€æµ‹ã€‚â€ *ç¬¬äºŒå±Šå¯¹è¯AIè‡ªç„¶è¯­è¨€å¤„ç†ç ”è®¨ä¼šè®ºæ–‡é›†* , 38â€“45\. åœ¨çº¿ï¼šè®¡ç®—è¯­è¨€å­¦åä¼š. [https://doi.org/10.18653/v1/2020.nlp4convai-1.5](https://doi.org/10.18653/v1/2020.nlp4convai-1.5)ã€‚
- en: 'Liu, Yinhan, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
    Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019\. â€œRoBERTa: A Robustly
    Optimized BERT Pretraining Approach.â€ arXiv. [https://doi.org/10.48550/arXiv.1907.11692](https://doi.org/10.48550/arXiv.1907.11692).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Liu, Yinhan, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
    Levy, Mike Lewis, Luke Zettlemoyer, å’Œ Veselin Stoyanov. 2019\. â€œRoBERTaï¼šä¸€ç§ç¨³å¥ä¼˜åŒ–çš„BERTé¢„è®­ç»ƒæ–¹æ³•ã€‚â€
    arXiv. [https://doi.org/10.48550/arXiv.1907.11692](https://doi.org/10.48550/arXiv.1907.11692)ã€‚
- en: ğŸ’¾ Data and Model
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ğŸ’¾ æ•°æ®å’Œæ¨¡å‹
- en: The [Banking77](https://arxiv.org/abs/2003.04807) dataset was retrieved from
    HuggingFace. It is published under the Creative Commons Attribution 4.0 International
    license (CC BY 4.0) and curated by [PolyAI](https://github.com/PolyAI-LDN) and
    was originally published by Casanueva et al. (2020). With thanks also to [Manuel
    Romero](https://twitter.com/mrm8488) who contributed the fine-tuned [DistilRoBERTa](https://huggingface.co/mrm8488/distilroberta-finetuned-banking77)
    to HuggingFace.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[Banking77](https://arxiv.org/abs/2003.04807) æ•°æ®é›†æ˜¯ä» HuggingFace è·å–çš„ã€‚å®ƒåœ¨çŸ¥è¯†å…±äº«ç½²å
    4.0 å›½é™…è®¸å¯åè®®ï¼ˆCC BY 4.0ï¼‰ä¸‹å‘å¸ƒï¼Œç”± [PolyAI](https://github.com/PolyAI-LDN) ç­–åˆ’ï¼Œå¹¶ç”± Casanueva
    ç­‰äººï¼ˆ2020å¹´ï¼‰æœ€åˆå‘å¸ƒã€‚è¿˜è¦æ„Ÿè°¢ [Manuel Romero](https://twitter.com/mrm8488) ä¸º HuggingFace
    è´¡çŒ®äº†ç»è¿‡å¾®è°ƒçš„ [DistilRoBERTa](https://huggingface.co/mrm8488/distilroberta-finetuned-banking77)ã€‚'
- en: '*Originally published at* [*https://www.paltmeyer.com*](https://www.paltmeyer.com/blog/posts/conformal-llm/)
    *on July 5, 2023.*'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*æœ€åˆå‘å¸ƒäº* [*https://www.paltmeyer.com*](https://www.paltmeyer.com/blog/posts/conformal-llm/)
    *äº2023å¹´7æœˆ5æ—¥ã€‚*'
