- en: Linguistic Fingerprinting with Python
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Python进行语言指纹分析
- en: 原文：[https://towardsdatascience.com/linguistic-fingerprinting-with-python-5b128ae7a9fc](https://towardsdatascience.com/linguistic-fingerprinting-with-python-5b128ae7a9fc)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/linguistic-fingerprinting-with-python-5b128ae7a9fc](https://towardsdatascience.com/linguistic-fingerprinting-with-python-5b128ae7a9fc)
- en: Attributing authorship with punctuation heatmaps
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用标点热图进行作者归属分析
- en: '[](https://medium.com/@lee_vaughan?source=post_page-----5b128ae7a9fc--------------------------------)[![Lee
    Vaughan](../Images/9f6b90bb76102f438ab0b9a4a62ffa3f.png)](https://medium.com/@lee_vaughan?source=post_page-----5b128ae7a9fc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5b128ae7a9fc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5b128ae7a9fc--------------------------------)
    [Lee Vaughan](https://medium.com/@lee_vaughan?source=post_page-----5b128ae7a9fc--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@lee_vaughan?source=post_page-----5b128ae7a9fc--------------------------------)[![Lee
    Vaughan](../Images/9f6b90bb76102f438ab0b9a4a62ffa3f.png)](https://medium.com/@lee_vaughan?source=post_page-----5b128ae7a9fc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5b128ae7a9fc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5b128ae7a9fc--------------------------------)
    [Lee Vaughan](https://medium.com/@lee_vaughan?source=post_page-----5b128ae7a9fc--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5b128ae7a9fc--------------------------------)
    ·9 min read·Aug 1, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5b128ae7a9fc--------------------------------)
    ·9分钟阅读·2023年8月1日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/84b60ea1de5e65b1d5632ec1303169e2.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84b60ea1de5e65b1d5632ec1303169e2.png)'
- en: Single forensic fingerprint in yellow tones with blue semicolons (image by DALL-E2
    and author)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 单一法医学指纹，黄色调与蓝色分号（图片来源：DALL-E2 和作者）
- en: '*Stylometry* is the quantitative study of literary style through computational
    text analysis. It’s based on the idea that we all have a unique, consistent, and
    recognizable style in our writing. This includes our vocabulary, our use of punctuation,
    the average length of our words and sentences, and so on.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*文体计量学*是通过计算文本分析对文学风格进行定量研究的学科。它基于这样的观点：我们每个人在写作中都有一种独特、一致且易于识别的风格。这包括我们的词汇、标点符号的使用、单词和句子的平均长度等等。'
- en: A typical application of stylometry is *authorship attribution*. This is the
    process of identifying the author of a document, such as when investigating plagiarism
    or resolving disputes on the origin of a historical document.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 文体计量学的一个典型应用是*作者归属分析*。这是一种识别文件作者的过程，比如在调查抄袭或解决历史文件来源争议时。
- en: In this *Quick Success Data Science* project, we’ll use Python, seaborn, and
    the Natural Language Toolkit (NLTK) to see if Sir Arthur Conan Doyle left behind
    a linguistic fingerprint in his novel, *The Lost World*. More specifically, we’ll
    use *semicolons* to determine whether Sir Arthur or his contemporary, H.G. Wells,
    is the likely author of the book.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个*快速成功数据科学*项目中，我们将使用Python、seaborn和自然语言工具包（NLTK）来检验亚瑟·柯南·道尔是否在他的小说《失落的世界》中留下了语言指纹。更具体地说，我们将利用*分号*来确定亚瑟·柯南·道尔还是他的同时代人H.G.
    威尔斯更可能是该书的作者。
- en: The Hound, The War, and The Lost World
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《猎犬》、《战争》与《失落的世界》
- en: Sir Arthur Conan Doyle (1859–1930) is best known for the Sherlock Holmes stories.
    H. G. Wells (1866–1946) is famous for several groundbreaking science fiction novels,
    such as *The Invisible Man*.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 亚瑟·柯南·道尔（1859–1930）最著名的是福尔摩斯系列故事。H.G. 威尔斯（1866–1946）因几部开创性的科幻小说而闻名，如*隐形人*。
- en: 'In 1912, Strand Magazine published *The Lost World*, a serialized version of
    a science fiction novel. Although its author is known, let’s pretend it’s in dispute
    and it’s our job to solve the mystery. Experts have narrowed the field down to
    two authors: Doyle and Wells. Wells is slightly favored because *The Lost World*
    is a work of science fiction and includes troglodytes similar to the *Morlocks*
    in his 1895 book, *The Time Machine*.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 1912年，《Strand Magazine》出版了*失落的世界*，这是一个科幻小说的连载版本。虽然它的作者是已知的，但让我们假设它存在争议，我们的任务是解开这个谜团。专家们将范围缩小到两位作者：道尔和威尔斯。威尔斯略微被倾向，因为*失落的世界*是一部科幻小说，并且包含了类似于他1895年书中*莫洛克人*的穴居人。
- en: To solve this problem, we’ll need representative works for each author. For
    Doyle, we’ll use *The Hound of the Baskervilles*, published in 1901\. For Wells,
    we’ll use *The War of the Worlds*, published in 1898.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们需要每位作者的代表作品。对于道尔，我们将使用*巴斯克维尔的猎犬*，它出版于1901年。对于威尔斯，我们将使用*世界大战*，它出版于1898年。
- en: Fortunately for us, all three novels are in the public domain and available
    through [*Project Gutenberg*](https://www.gutenberg.org/). For convenience, I’ve
    downloaded them to this [Gist](https://gist.github.com/rlvaugh/b5e6033024620e261b9829261b27b6e1)
    and stripped out the licensing information.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们来说幸运的是，所有三部小说都在公共领域，并可以通过[*古腾堡计划*](https://www.gutenberg.org/)获得。为了方便，我已将它们下载到这个[Gist](https://gist.github.com/rlvaugh/b5e6033024620e261b9829261b27b6e1)并删除了版权信息。
- en: The Process
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 过程
- en: Authorship attribution requires the application of *Natural Language Processing
    (NLP).* NLP is a branch of linguistics and artificial intelligence concerned with
    giving computers the ability to derive meaning from written and spoken words.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 作者归属分析需要应用*自然语言处理（NLP）*。NLP是语言学和人工智能的一个分支，旨在赋予计算机从书面和口头语言中提取意义的能力。
- en: 'The most common NLP tests for authorship analyze the following features of
    a text:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的NLP作者分析测试会分析文本的以下特征：
- en: '**Word/Sentence length:** A frequency distribution plot of the length of words
    or sentences in a document.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词语/句子长度：** 文档中词语或句子长度的频率分布图。'
- en: '**Stop words:** A frequency distribution plot of stop words (short, noncontextual
    function words like *the*, *but*, and *if).*'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**停用词：** 停用词的频率分布图（短的、无语境的功能词，如*the*、*but* 和 *if*）。'
- en: '**Parts of speech:** A frequency distribution plot of words based on their
    syntactic functions (such as nouns and verbs).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词性：** 根据词汇的句法功能（如名词和动词）绘制的频率分布图。'
- en: '**Most common words:** A comparison of the most commonly used words in a text.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最常见的词汇：** 比较文本中最常用的词汇。'
- en: '**Jaccard similarity:** A statistic used for gauging the similarity and diversity
    of a sample set.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Jaccard相似性：** 用于衡量样本集的相似性和多样性的统计量。'
- en: '**Punctuation:** A comparison of the use of commas, colons, semicolons, and
    so on.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标点符号：** 比较逗号、冒号、分号等的使用情况。'
- en: For this toy project, we’ll focus on the *punctuation* test. Specifically, we’ll
    look at the use of *semicolons*. As “optional” grammatical elements, their use
    is potentially distinctive. They’re also agnostic to the type of book, unlike
    question marks, which may be more abundant in detective novels than in classic
    science fiction.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个玩具项目，我们将专注于*标点*测试。具体来说，我们将关注*分号*的使用。作为“可选”的语法元素，它们的使用具有潜在的独特性。它们也不受书籍类型的影响，不像问号在侦探小说中可能比在经典科幻小说中更为丰富。
- en: 'The high-level process will be to:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 高级过程将是：
- en: Load the books as text strings,
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将书籍加载为文本字符串，
- en: Use NLTK to tokenize (break out) each word and punctuation mark,
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用NLTK对每个单词和标点进行标记（拆分），
- en: Extract the punctuation marks,
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取标点符号，
- en: Assign semicolons a value of 1 and all other marks a value of 0,
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将分号的值设为1，所有其他标点的值设为0，
- en: Create 2D NumPy arrays of the numerical values,
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建数值的2D NumPy数组，
- en: Use seaborn to plot the results as a [heat map](https://en.wikipedia.org/wiki/Heat_map),
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用seaborn将结果绘制为[热图](https://en.wikipedia.org/wiki/Heat_map)，
- en: We’ll use each heat map as a digital “fingerprint.” Comparing the fingerprints
    for the *known* books with the *unknown* book (*The Lost World*) will hopefully
    suggest one author over the other.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用每个热图作为数字“指纹”。比较已知书籍的指纹与*未知*书籍（*失落的世界*）的指纹，希望能够暗示出作者之间的不同。
- en: '![](../Images/65382f1d0ad75a3cf108a4aee27703f2.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/65382f1d0ad75a3cf108a4aee27703f2.png)'
- en: Heatmaps for question mark use (blue) in a detective novel (left) vs. a sci-fi
    novel (right) (image by the author)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 问号使用的热图（蓝色），在侦探小说（左）与科幻小说（右）中（作者提供的图像）
- en: The Natural Language Toolkit
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言工具包
- en: Multiple third-party libraries can help you perform NLP with Python. These include
    NLTK, spaCy, Gensim, Pattern, and TextBlob.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 多个第三方库可以帮助你用Python进行NLP。这些库包括NLTK、spaCy、Gensim、Pattern和TextBlob。
- en: We’re going to use [NLTK](https://www.nltk.org/), which is one of the oldest,
    most powerful, and most popular. It’s open source and works on Windows, macOS,
    and Linux. It also comes with highly detailed [documentation](https://www.nltk.org/book/).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用[NLTK](https://www.nltk.org/)，它是最古老、最强大且最受欢迎的工具之一。它是开源的，适用于Windows、macOS和Linux。它还配备了详细的[文档](https://www.nltk.org/book/)。
- en: Installing Libraries
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装库
- en: 'To install NLTK with Anaconda use:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Anaconda安装NLTK：
- en: '`conda install -c anaconda nltk`'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`conda install -c anaconda nltk`'
- en: 'To install with pip use:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 使用pip安装：
- en: '`pip install nltk` (or see [https://www.nltk.org/install.html](https://www.nltk.org/install.html))'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`pip install nltk`（或参见 [https://www.nltk.org/install.html](https://www.nltk.org/install.html)）'
- en: 'We’ll also need seaborn for plotting. Built on matplotlib, this open-source
    visualization library provides an easier-to-use interface for drawing attractive
    and informative statistical graphs such as bar charts, scatterplots, heat maps,
    and so on. Here are the installation commands:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要seaborn来进行绘图。基于matplotlib，这个开源可视化库提供了一个更易于使用的界面来绘制吸引人且信息丰富的统计图表，如条形图、散点图、热图等。以下是安装命令：
- en: '`conda install seaborn`'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`conda install seaborn`'
- en: or
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 或
- en: '`pip install seaborn`'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`pip install seaborn`'
- en: The Code
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码
- en: The following code was written in JupyterLab and is described *by cell*.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码在JupyterLab中编写，并且*由单元格*描述。
- en: Importing Libraries
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导入库
- en: Among the imports, we’ll need the list of punctuation marks from the `string`
    module. We'll turn this list into a `set` datatype for faster searches.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入模块中，我们需要`string`模块中的标点符号列表。我们将把这个列表转化为`set`数据类型，以便更快地进行搜索。
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Defining a Function to Load Text Files
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义一个加载文本文件的函数
- en: Since we’ll be working with multiple files, we’ll start by defining some reusable
    functions. The first one uses Python’s `urllib` library to open a file stored
    at a URL. The first line opens the file, the second reads it as bytes data, and
    the third converts the bytes to string (`str`) format.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将处理多个文件，我们将首先定义一些可重用的函数。第一个函数使用Python的`urllib`库打开存储在URL处的文件。第一行打开文件，第二行将其读取为字节数据，第三行将字节转换为字符串（`str`）格式。
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Defining a Function to Tokenize Text
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义一个分词函数
- en: The next function accepts a dictionary where the keys are the authors’ names,
    and the values are their novels in string format. It then uses NLTK’s `word_tokenize()`
    method to reorganize the strings into *tokens*.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个函数接受一个字典，其中键是作者的名字，值是他们以字符串格式呈现的小说。然后，它使用NLTK的`word_tokenize()`方法将字符串重新组织为*令牌*。
- en: Python sees strings as a collection of *characters*, like letters, spaces, and
    punctuation. The process of tokenization groups these characters into other elements
    like words (or sentences), and punctuation. Each element is referred to as a *token,*
    and these tokens permit the use of sophisticated analyses with NLP.
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Python将字符串视为*字符*的集合，如字母、空格和标点符号。分词过程将这些字符分组为其他元素，如单词（或句子）和标点符号。每个元素称为*令牌*，这些令牌允许使用复杂的NLP分析。
- en: The function ends by returning a new dictionary with the authors’ names as keys
    and the punctuation tokens as values.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数通过返回一个新的字典来结束，其中作者的名字作为键，标点符号令牌作为值。
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: I should note here that it’s possible to use basic Python to search the text
    file for semicolons and complete this project. Tokenization, however, brings two
    things to the table.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我应该在这里指出，可以使用基本的Python来搜索文本文件中的分号并完成这个项目。然而，分词带来了两个好处。
- en: Firstly, NLTK’s default tokenizer (`word_tokenize()`) does not count apostrophes
    used in contractions or possessives, but rather, treats them as *two words* with
    the apostrophe attached to the second word (such as *I* + *‘ve*). This is in accordance
    with [grammatical usage](https://www.niu.edu/writingtutorial/punctuation/apostrophe.shtml).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，NLTK的默认分词器（`word_tokenize()`）不计算用在缩写或所有格中的撇号，而是将其视为*两个单词*，撇号附在第二个单词上（如*I*
    + *‘ve*）。这符合[语法用法](https://www.niu.edu/writingtutorial/punctuation/apostrophe.shtml)。
- en: The tokenizer also treats special marks, such as a double hyphen ( — — ) or
    ellipses (…) as a *single* mark, as the author intended, rather than as separate
    marks. With basic Python, each duplicate mark is counted.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器还将特殊标记（如双破折号（— —）或省略号（…））视为*单一*标记，如作者所意图，而不是作为单独的标记。使用基本Python时，每个重复的标记都会被计数。
- en: The impact of this can be significant. For example, the total punctuation count
    for *The Lost World* using NLTK is 10,035\. With basic Python, it’s 14,352!
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能会产生显著影响。例如，使用NLTK计算*失落的世界*的总标点符号数量为10,035，而使用基本Python则为14,352！
- en: Secondly, all of the other NLP attribution tests require the use of tokenization,
    so using NLTK here gives you the ability to expand the analysis later.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，所有其他NLP归因测试都需要使用分词，因此在这里使用NLTK让你有能力在以后扩展分析。
- en: Defining a Function to Convert Punctuation Marks to Numbers
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义一个将标点符号转换为数字的函数
- en: Seaborn’s `heatmap()` method requires *numerical* data as input. The following
    function accepts a dictionary of punctuation tokens by author, assigns semicolons
    a value of `1` and all else a value of `0`, and returns a `list` datatype.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Seaborn的`heatmap()`方法需要*数值*数据作为输入。以下函数接受按作者分类的标点符号令牌字典，将分号赋值为`1`，其他所有赋值为`0`，并返回`list`数据类型。
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Defining a Function to Find the Next Lowest Square Value
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义一个函数来查找下一个最低的平方值
- en: The list of heat values is in *1D*, but we want to plot it in *2D*. To accomplish
    this, we’ll convert the list into a square NumPy array.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 热值列表是*一维*的，但我们希望将其绘制为*二维*。为此，我们将列表转换为正方形NumPy数组。
- en: This will have to be a *true* square, and it’s unlikely that the number of samples
    in each list will have an integer as its square root. So, we have to take the
    square root of the length of each list, convert it to an integer, and square it.
    The resulting value will be the *maximum length* of the list that can be displayed
    as a true square.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这必须是一个*真正的*正方形，并且每个列表中的样本数量不太可能有一个整数作为其平方根。因此，我们需要计算每个列表长度的平方根，将其转换为整数，并进行平方。得到的值将是可以作为真正的正方形显示的*最大长度*。
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Loading and Preparing the Data for Plotting
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载和准备数据以进行绘图
- en: With the functions defined, we’re ready to apply them. First, we load the text
    data for each book into a dictionary named `strings_by_author`. For *The Lost
    World,* we'll enter “unknown” for the author’s name, as this represents a document
    of unknown origin.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 定义好函数后，我们准备应用它们。首先，我们将每本书的文本数据加载到一个名为`strings_by_author`的字典中。对于*失落的世界*，我们将作者的名字设置为“未知”，因为这代表一个来源不明的文档。
- en: We then turn this into a punctuation dictionary named `punct_by_author`. From
    this dictionary, we find the next lowest squares for each list and choose the
    *minimum* value going forward. This ensures that *all* the datasets can be converted
    into a square array and also normalizes the number of samples per dataset (*The
    Lost World*, for example, has 10,035 punctuation tokens compared to only 6,704
    for *The Hound of the Baskervilles*).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将其转换为一个名为`punct_by_author`的标点字典。从这个字典中，我们找到每个列表的下一个最低平方值，并选择*最小*值继续前进。这确保了*所有*数据集都可以转换为正方形数组，并且还规范化了每个数据集的样本数量（例如，*失落的世界*有10,035个标点符号标记，而*巴斯克维尔的猎犬*只有6,704个）。
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/1c513622c91d423e3d0f8a2b76e9813d.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c513622c91d423e3d0f8a2b76e9813d.png)'
- en: Using an array with 6,561 tokens going forward means that each punctuation list
    will be truncated to a length of 6,561 before plotting.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 未来使用一个包含6,561个标记的数组意味着每个标点符号列表将在绘图前被截断至长度6,561。
- en: Plotting the Heat Maps
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 绘制热图
- en: We’ll use a `for` loop to plot a heat map for each author's punctuation. The
    first step is to convert the punctuation tokens to numbers. Remember, semicolons
    will be represented by `1` and everything else by `0`.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个`for`循环为每位作者的标点符号绘制热图。第一步是将标点符号标记转换为数字。记住，分号将用`1`表示，其他所有标点用`0`表示。
- en: Next, we use our `perfect_square` value and NumPy’s `array()` and `reshape()`methods
    to both truncate the `heat` list and turn it into a square NumPy array. At this
    point, we only need to set up a matplotlib figure and call seaborn's `heatmap()`
    method. The loop will plot a separate figure for each author.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`perfect_square`值和NumPy的`array()`及`reshape()`方法来截断`heat`列表并将其转换为正方形NumPy数组。此时，我们只需设置一个matplotlib图形并调用seaborn的`heatmap()`方法。循环将为每个作者绘制一个单独的图形。
- en: Semicolons will be colored *blue*. You can reverse this by changing the order
    of the colors in the `cmap` argument. You can also enter new colors if you don’t
    like yellow and blue.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 分号将被标记为*蓝色*。你可以通过改变`cmap`参数中的颜色顺序来反转这一点。如果你不喜欢黄色和蓝色，也可以输入新的颜色。
- en: '[PRE6]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/a40c96d35c1f9fa54ce0645137d1485f.png)![](../Images/e01f8c9596cafbbf962bac2f7e975ba2.png)![](../Images/b1148106f8f5f9545e5c1e5c87e6acbf.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a40c96d35c1f9fa54ce0645137d1485f.png)![](../Images/e01f8c9596cafbbf962bac2f7e975ba2.png)![](../Images/b1148106f8f5f9545e5c1e5c87e6acbf.png)'
- en: Outcome
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: It should be clear from the previous plots that — based strictly on semicolon
    usage — Doyle is the most likely author of *The Lost World*.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的图表中应该可以清楚地看出——严格依据分号的使用——Doyle 是*失落的世界*最可能的作者。
- en: 'Having a visual display of these results is important. For example, the total
    semicolon counts for each book look like this:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果的可视化展示非常重要。例如，每本书的总分号数如下：
- en: '`Wells: 243`'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`Wells: 243`'
- en: '`Doyle: 45`'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`Doyle: 45`'
- en: '`Unknown: 103`'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`Unknown: 103`'
- en: 'And as a fraction of the total punctuation, they look like this:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 作为总标点符号的一部分，它们的比例如下：
- en: '`Wells: 0.032`'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`Wells: 0.032`'
- en: '`Doyle: 0.007`'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`Doyle: 0.007`'
- en: '`Unknown: 0.010`'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`Unknown: 0.010`'
- en: From this textual data, is it immediately clear to you that the Unknown author
    is most likely Doyle? Imagine presenting this to a jury. Would the jurors be swayed
    more by the plots or the text? You can’t beat visualizations for communicating
    information!
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些文本数据来看，你是否立即能判断出“未知”作者很可能是**道尔**？试想一下如果把这些呈现给陪审团，陪审员会更被情节还是文本打动？没有什么能比可视化更有效地传达信息了！
- en: Of course, for a *robust* determination of authorship, you would want to run
    *all* the NLP attribution tests listed previously, and you would want to use *all*
    of Doyle’s and Well’s novels.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，要对作者身份进行*稳健*的判断，你需要运行之前列出的*所有*NLP归属测试，并使用*所有*的道尔和威尔斯的小说。
- en: Thanks
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 谢谢
- en: 'Thanks for reading and please follow me for more *Quick Success Data Science*
    projects in the future. And if you want to see a more complete application of
    NLP tests on this dataset, see Chapter 2 of my book, [*Real Word Python: A Hacker’s
    Guide to Solving Problems with Code*](https://a.co/d/gzXvvoB).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读，未来请关注我的更多*快速成功数据科学*项目。如果你想看到对该数据集进行更全面的NLP测试，请参阅我书中的第二章，[*真实世界的Python：黑客解决代码问题的指南*](https://a.co/d/gzXvvoB)。
