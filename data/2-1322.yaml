- en: Improve Your Boosting Algorithms with Early Stopping
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过早期停止改善你的提升算法
- en: 原文：[https://towardsdatascience.com/improve-your-boosting-algorithms-with-early-stopping-99616bd15d83](https://towardsdatascience.com/improve-your-boosting-algorithms-with-early-stopping-99616bd15d83)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/improve-your-boosting-algorithms-with-early-stopping-99616bd15d83](https://towardsdatascience.com/improve-your-boosting-algorithms-with-early-stopping-99616bd15d83)
- en: Overview and Implementation with Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述及Python实现
- en: '[](https://medium.com/@aashishnair?source=post_page-----99616bd15d83--------------------------------)[![Aashish
    Nair](../Images/23f4b3839e464419332b690a4098d824.png)](https://medium.com/@aashishnair?source=post_page-----99616bd15d83--------------------------------)[](https://towardsdatascience.com/?source=post_page-----99616bd15d83--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----99616bd15d83--------------------------------)
    [Aashish Nair](https://medium.com/@aashishnair?source=post_page-----99616bd15d83--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@aashishnair?source=post_page-----99616bd15d83--------------------------------)[![Aashish
    Nair](../Images/23f4b3839e464419332b690a4098d824.png)](https://medium.com/@aashishnair?source=post_page-----99616bd15d83--------------------------------)[](https://towardsdatascience.com/?source=post_page-----99616bd15d83--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----99616bd15d83--------------------------------)
    [Aashish Nair](https://medium.com/@aashishnair?source=post_page-----99616bd15d83--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----99616bd15d83--------------------------------)
    ·6 min read·May 15, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----99616bd15d83--------------------------------)
    ·6分钟阅读·2023年5月15日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/931b4e752e9becf6f9f8aadda8a47114.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/931b4e752e9becf6f9f8aadda8a47114.png)'
- en: Photo by [Glenn Carstens-Peters](https://unsplash.com/@glenncarstenspeters?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Glenn Carstens-Peters](https://unsplash.com/@glenncarstenspeters?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Boosting algorithms are largely popular in the data science space, and rightly
    so. Models that incorporate boosting yield some of the best performances, which
    is why they are commonplace in both academia and industry.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 提升算法在数据科学领域非常流行，这一点毋庸置疑。结合提升的模型表现出色，这也是它们在学术界和工业界都很常见的原因。
- en: That being said, these types of algorithms will register suboptimal results
    if they are not configured properly.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，如果这些类型的算法没有正确配置，会得到次优结果。
- en: One such feature that is often underutilized is **early stopping**.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常常被低估的特性是**早期停止**。
- en: Here, we give a high-level overview of early stopping and why it should be incorporated
    into your boosting algorithms.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将对早期停止进行高层次的概述，并解释为何它应当被纳入你的提升算法中。
- en: Recap on Boosting
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提升回顾
- en: Before getting into early stopping, let’s briefly discuss boosting.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入早期停止之前，让我们简要讨论一下提升算法。
- en: In short, algorithms that leverage boosting train a series of sequential models,
    with each model aiming to address the error made by its predecessor.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，利用提升的算法会训练一系列顺序模型，每个模型旨在解决前一个模型所犯的错误。
- en: 'Boosting algorithms adhere to the following steps:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 提升算法遵循以下步骤：
- en: Train a weak model with initial weights
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个带有初始权重的弱模型
- en: Evaluate the “error” of this first model
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估第一个模型的“错误”
- en: Train a new model with modified weights that address the issues with the previous
    model
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个新的模型，修改权重以解决前一个模型的问题
- en: Evaluate the “error” of this new model
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估这个新模型的“错误”
- en: Repeat steps 3 or 4 until a specific criterion is met (e.g., number of iterations,
    model performance, etc.)
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤3或4，直到满足特定标准（例如，迭代次数、模型性能等）
- en: In theory, boosting serves as a perfect solution to determining the optimal
    weights for a particular model.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，提升算法是确定特定模型最优权重的完美解决方案。
- en: After all, if the model keeps learning from its previous mistakes, performing
    more iterations should yield better results. So, why not just perform as many
    iterations as possible? With a near-infinite number of models, we could achieve
    peak performance!
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 毕竟，如果模型不断从之前的错误中学习，执行更多的迭代应该会带来更好的结果。那么，为什么不进行尽可能多的迭代呢？拥有接近无限数量的模型，我们可以实现最佳性能！
- en: Unfortunately, that is usually not the case.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，通常情况并非如此。
- en: More Iterations ≠ Better
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多迭代≠更好
- en: After a select number of iterations, the model will modify its weights and likely
    become better at generalizing with the training data. However, if an algorithm
    goes past the ideal number of iterations, it will start capturing the noise and
    become unable to perform as well on unseen data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择的迭代次数后，模型将调整其权重，并可能在训练数据上变得更擅长泛化。然而，如果算法超过理想的迭代次数，它将开始捕捉噪声，并在未见过的数据上表现不佳。
- en: In other words, boosting algorithms that use too many iterations become prone
    to overfitting.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，使用过多迭代的提升算法容易过拟合。
- en: Finding the Right Number of Iterations
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 找到合适的迭代次数
- en: Now we know that a boosting algorithm needs enough iterations to find the optimal
    weights for its model, but not too many that would make it succumb to overfitting.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道提升算法需要足够的迭代次数来找到模型的最佳权重，但又不能过多以避免过拟合。
- en: 'The key then is to find the “sweet spot”: a number of iterations that is not
    too high or too low.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 关键是找到“最佳点”：一个既不高也不低的迭代次数。
- en: However, it can be challenging to determine the ideal number of iterations as
    this figure varies from case to case. There are a number of factors that influence
    this figure, ranging from the underlying data and the model being trained.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，确定理想的迭代次数可能具有挑战性，因为这个数字因情况而异。影响这个数字的因素有很多，包括底层数据和正在训练的模型。
- en: One way to address this is to use early stopping.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一种解决方法是使用提前停止。
- en: Early stopping
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提前停止
- en: Early stopping entails ending the training of the boosting model prematurely
    if the individual model’s performance against the validation set does not improve
    after a given number of iterations.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 提前停止意味着如果个别模型在验证集上的表现经过一定次数的迭代后没有改善，就会提前结束提升模型的训练。
- en: Essentially, instead of training weak models a fixed number of times, we can
    configure it to continue training only if it is showing better results.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，与其训练弱模型固定次数，我们可以配置它，仅在显示更好结果时继续训练。
- en: The benefits of this early stopping are obvious at a glance. With this technique,
    we can ensure that the model will stop training before it succumbs to overfitting,
    thereby improving performance. It also reduces the run time for the training process
    since it leads to fewer iterations.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这种提前停止的好处一目了然。通过这种技术，我们可以确保模型在过拟合之前停止训练，从而提高性能。它还减少了训练过程的运行时间，因为它减少了迭代次数。
- en: Case Study
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 案例研究
- en: The best way to demonstrate the early stopping is with a case study. Let’s work
    with the built-in titanic dataset from the Scikit Learn library.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最好用案例研究来演示提前停止。让我们使用Scikit Learn库中的内置泰坦尼克数据集。
- en: The objective is to train a light gradient boosting machine (LGBM) *with and
    without early stopping* and compare their results in terms of their f1-scores
    and run time.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是训练一个有*提前停止*和*无提前停止*的轻量级梯度提升机（LGBM），并比较它们在f1分数和运行时间方面的结果。
- en: '**Without Early Stopping**'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**没有提前停止**'
- en: Let’s create an LGBM classifier that uses 1000 iterations (specified in the
    `n_estimators` hyperparameter) and evaluate it against the testing set.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个LGBM分类器，使用1000次迭代（在`n_estimators`超参数中指定），并对测试集进行评估。
- en: '![](../Images/d2a881d19c821e019689a2b29159dbf7.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d2a881d19c821e019689a2b29159dbf7.png)'
- en: F-1 Score (Created by Author)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: F-1 Score（作者创建）
- en: 'Next, lets use the `%%timeit` command to determine the run time for these operations:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们使用`%%timeit`命令来确定这些操作的运行时间：
- en: '![](../Images/fb3069130edaeedc2ff51db60e6a7545.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fb3069130edaeedc2ff51db60e6a7545.png)'
- en: Code Output (Created By Author)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 代码输出（作者创建）
- en: By using a boosting algorithm with 1000 iterations, the model yields an f-1
    score of about 0.85 in about 473 ms.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用具有1000次迭代的提升算法，模型在约473毫秒内产生了约0.85的f-1分数。
- en: Not bad, but do we even need 1000 iterations in the first place?
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 还不错，但我们真的需要1000次迭代吗？
- en: For a clearer picture, let’s see how the f-1 score of the model changes as we
    increase the number of iterations.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清楚地了解，让我们查看模型的f-1分数如何随着迭代次数的增加而变化。
- en: '![](../Images/579a0fe9be6c17c78ed32086c94bc4fc.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/579a0fe9be6c17c78ed32086c94bc4fc.png)'
- en: Code Output (Created By Author)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 代码输出（作者创建）
- en: Shockingly enough, the model’s performance against the test set steadily decreases
    after around the first 50 iterations!
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 令人震惊的是，模型在测试集上的表现在前50次迭代后稳步下降！
- en: It’s clear that the boosting algorithm does not need that many iterations to
    reach peak performance for this dataset.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，对于这个数据集，提升算法不需要那么多的迭代次数就能达到最佳性能。
- en: '**2\. With Early Stopping**'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 使用提前停止**'
- en: This time let’s see how the model will perform after incorporating early stopping.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这次让我们看看在加入提前停止后，模型的表现如何。
- en: Using early stopping in an LGBM classifier requires the explicit assignment
    of two hyperparameters. The first one is called `eval_set`, which contains the
    validation set. The validation set is what the model will use to evaluate its
    performance at each iteration.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LGBM 分类器中使用提前停止需要明确设置两个超参数。第一个叫做 `eval_set`，它包含验证集。验证集是模型在每次迭代时用来评估其性能的数据集。
- en: The second hyperparameter is called `early_stopping_rounds` , which contains
    the number of iterations that the model can run without yielding a greater performance
    against the validation set. If the performance does not improve within these iterations,
    the model will stop training prematurely.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个超参数叫做 `early_stopping_rounds`，它包含了模型可以在没有对验证集表现进行更大提升的情况下运行的迭代次数。如果在这些迭代内性能没有提升，模型将提前停止训练。
- en: For this case, the value assigned to `early_stopping_rounds` is 20\. This means
    that if the model’s f-1 score against the validation set does not improve over
    its predecessors within 20 iterations, the training process will stop, even though
    it has been configured to run for 1000 iterations.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个案例，`early_stopping_rounds` 的值被设置为 20。这意味着如果模型在 20 次迭代内其对验证集的 f-1 分数没有超过前几次的结果，即使它被配置为运行
    1000 次迭代，训练过程也会停止。
- en: '![](../Images/ce247c549354e33dbca97f77940d317d.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce247c549354e33dbca97f77940d317d.png)'
- en: Code Output (Created by Author)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 代码输出（由作者创建）
- en: With early stopping, the model yields an f-1 score of about 0.92, which is a
    considerable improvement compared to the model that doesn’t use early stopping!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 使用提前停止的模型获得了约 0.92 的 f-1 分数，这相比于不使用提前停止的模型有了显著的提高！
- en: Furthermore, the model should now train in less time since it uses fewer iterations.
    We can confirm this with the `%%timeit` operation.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于使用了较少的迭代次数，模型现在应该可以在更短的时间内完成训练。我们可以通过 `%%timeit` 操作来确认这一点。
- en: '![](../Images/4ce1f52c15ee1a103be4a397d45589d3.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ce1f52c15ee1a103be4a397d45589d3.png)'
- en: As expected, the model that *does* early stopping is trained in a fraction of
    the time it takes to train the model that *does not* use early stopping.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，*使用* 提前停止的模型在训练时间上仅为*不使用* 提前停止的模型的一小部分。
- en: Conclusion
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: '![](../Images/727edf2cd6c22ef7f2a648bc2c6f4ffa.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/727edf2cd6c22ef7f2a648bc2c6f4ffa.png)'
- en: Photo by [Prateek Katyal](https://unsplash.com/@prateekkatyal?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[Prateek Katyal](https://unsplash.com/@prateekkatyal?utm_source=medium&utm_medium=referral)
    摄影，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)'
- en: Overall, algorithms that leverage boosting tend to be more robust, given how
    they “learn” from multiple weak models. However, maximizing the number of iterations
    used in these algorithms is not a viable solution.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，利用提升方法的算法通常更加稳健，因为它们从多个弱模型中“学习”。然而，最大化这些算法使用的迭代次数并不是一个可行的解决方案。
- en: Different use cases will call for a different number of iterations, which is
    why the ideal number of iterations used in an algorithm should be based on the
    individual models’ performances.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的使用场景会需要不同的迭代次数，这就是为什么算法中使用的理想迭代次数应基于单个模型的表现。
- en: For that reason, early stopping is a technique that has immense practical value.
    It uses the validation set as an indicator of whether the algorithm should have
    more iterations or stop prematurely. As explained and demonstrated in the case
    study, early stopping enables models to achieve better performance with less training
    time.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，提前停止是一种具有巨大实际价值的技术。它使用验证集作为算法是否需要更多迭代或提前停止的指标。如案例研究中所解释和演示的，提前停止使得模型在较少的训练时间内获得更好的性能。
- en: I wish you the best of luck in your data science endeavors!
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 祝你在数据科学的工作中好运！
