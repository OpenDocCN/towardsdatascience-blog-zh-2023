- en: Extractive vs Generative Q&A — Which is better for your business?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取式与生成式问答——哪种更适合您的业务？
- en: 原文：[https://towardsdatascience.com/extractive-vs-generative-q-a-which-is-better-for-your-business-5a8a1faab59a](https://towardsdatascience.com/extractive-vs-generative-q-a-which-is-better-for-your-business-5a8a1faab59a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/extractive-vs-generative-q-a-which-is-better-for-your-business-5a8a1faab59a](https://towardsdatascience.com/extractive-vs-generative-q-a-which-is-better-for-your-business-5a8a1faab59a)
- en: The arrival of ChatGPT hints at a new era of search engines, this tutorial dives
    into the 2 basic types of AI based question answering
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ChatGPT的出现暗示着一个搜索引擎的新纪元，本教程深入探讨了两种基本的AI问答类型
- en: '[](https://skanda-vivek.medium.com/?source=post_page-----5a8a1faab59a--------------------------------)[![Skanda
    Vivek](../Images/9d25bee2fb75176ca7f7ea6eff7d7ab5.png)](https://skanda-vivek.medium.com/?source=post_page-----5a8a1faab59a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5a8a1faab59a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5a8a1faab59a--------------------------------)
    [Skanda Vivek](https://skanda-vivek.medium.com/?source=post_page-----5a8a1faab59a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://skanda-vivek.medium.com/?source=post_page-----5a8a1faab59a--------------------------------)[![Skanda
    Vivek](../Images/9d25bee2fb75176ca7f7ea6eff7d7ab5.png)](https://skanda-vivek.medium.com/?source=post_page-----5a8a1faab59a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5a8a1faab59a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5a8a1faab59a--------------------------------)
    [Skanda Vivek](https://skanda-vivek.medium.com/?source=post_page-----5a8a1faab59a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5a8a1faab59a--------------------------------)
    ·6 min read·Feb 6, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5a8a1faab59a--------------------------------)
    ·阅读时间6分钟·2023年2月6日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/17ef8827814e6d88ceadb0b0aa8cbbdb.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/17ef8827814e6d88ceadb0b0aa8cbbdb.png)'
- en: Extractive vs Abstractive Question Answering | Skanda Vivek
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 提取式与生成式问答 | Skanda Vivek
- en: Transformer models [introduced in 2017](https://arxiv.org/abs/1706.03762) have
    led to a breakthrough in solving hard language related tasks. Variations of the
    original transformer architecture in models like BERT, GPT, etc. trained on large
    amounts of text data have produced state of the art results on language related
    tasks.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 变换模型 [于2017年引入](https://arxiv.org/abs/1706.03762) 已经在解决难度大的语言相关任务上取得了突破。像BERT、GPT等模型在大量文本数据上训练的原始变换架构变体，在语言相关任务上产生了最先进的结果。
- en: One of the greatest benefits of AI is in the ability to perform tasks that previously
    needed domain expertise and careful perusal — faster and at a fraction of the
    cost. I believe this will revolutionize industries in the coming decade.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: AI的一个重大好处在于其执行任务的能力，这些任务之前需要领域专业知识和仔细审阅——现在可以更快地完成，并且成本仅为以前的一小部分。我相信这将在未来十年内彻底改变各个行业。
- en: 'A typical task is extracting information from text. Question Answering is a
    powerful information extraction tool, whereby models can be trained to extract
    specific bits of information through complex queries. Think about the potential
    time and money saved by AI models in answering hard questions from legal documents,
    instead of asking an experienced lawyer or hiring an intern to pour over the document
    for hours. Let’s take a dive into the 2 basic types of AI based QA: Extractive
    vs Abstractive.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一项典型的任务是从文本中提取信息。问答系统是一个强大的信息提取工具，通过复杂的查询，模型可以被训练来提取特定的信息。想象一下，如果用AI模型回答法律文件中的难题，可以节省多少时间和金钱，而不是请经验丰富的律师或聘请实习生花费数小时细读文件。让我们深入了解两种基本的基于AI的问答类型：提取式与生成式。
- en: Extractive QA
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取式问答
- en: The BERT transformer model was released in 2019 by the Google Language team.
    BERT was trained on unlabeled text data by masking words and training the model
    to predict masked words based on context. This masked word prediction is a common
    test, administered to gauge language proficiency.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: BERT变换模型由Google语言团队于2019年发布。BERT通过遮蔽单词并训练模型基于上下文预测遮蔽的单词，利用未标记的文本数据进行训练。这种遮蔽单词预测是一个常见的测试，用于评估语言能力。
- en: After training the model, BERT was later fine-tuned on multiple tasks. In particular,
    BERT was fine-tuned on hundreds of thousands of question answer pairs from the
    SQUAD dataset, consisting of questions posed on Wikipedia articles, where the
    answer to every question is a segment of text, or *span*, from the corresponding
    passage.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型后，BERT 后来在多个任务上进行了微调。特别是，BERT 在来自 SQUAD 数据集的数十万个问答对上进行了微调，该数据集包含在 Wikipedia
    文章上提出的问题，每个问题的答案是对应段落中的一段文本或 *跨度*。
- en: '![](../Images/b8216f58e2743b33f3e2a89ddd41ddb8.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b8216f58e2743b33f3e2a89ddd41ddb8.png)'
- en: BERT Transformer Architecture from [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: BERT Transformer 架构来自 [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)
- en: The architecture of BERT and BERT-like models compose one-half of the original
    transformer architecture proposed in the 2017 paper, known as the encoder. In
    this model, ***E***denotes the token embeddings wherein the original sentence
    of length *M* is converted to a length *M’* (BERT used the WordPiece embeddings).
    The final hidden vector *T* can be used to predict which part of the text represents
    the start of the answer and the end of the answer using a softmax.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 和类似 BERT 的模型架构构成了2017年论文中提出的原始 Transformer 架构的一半，被称为编码器。在这个模型中，***E***表示令牌嵌入，其中原始句子长度为
    *M* 被转换为长度 *M’*（BERT 使用了 WordPiece 嵌入）。最终隐藏向量 *T* 可以用来预测文本中表示答案开始和结束的部分，通过 softmax
    实现。
- en: 'RoBERTa is a variation of BERT that modified key hyperparameters during training
    and improved overall performance. Let’s look at the output of a [fine-tuned RoBERTa
    model on huggingface released by deepset.](https://huggingface.co/deepset/roberta-base-squad2)
    As you can see below, in extractive QA the answering you are limited to text contained
    within the original context:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: RoBERTa 是 BERT 的一个变体，通过在训练过程中修改关键超参数来提高整体性能。让我们看看 [huggingface 发布的经过微调的 RoBERTa
    模型的输出。](https://huggingface.co/deepset/roberta-base-squad2) 如下所示，在抽取式问答中，你只能回答原始上下文中的文本：
- en: '![](../Images/5521944641c87a3397c4e828283621e5.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5521944641c87a3397c4e828283621e5.png)'
- en: '[RoBERTa fine-tuned QA model output](https://huggingface.co/deepset/roberta-base-squad2)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[RoBERTa 微调问答模型输出](https://huggingface.co/deepset/roberta-base-squad2)'
- en: However, the answer is not always the best. As you can see below, for a movie
    review the answer I would have chosen would have been “*What would life on Earth
    look like in a future where humans are still very much alive but no longer in
    charge*”
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，答案并不总是最佳的。如下面所示，对于电影评论，我选择的答案会是“*在一个未来的世界中，人类仍然非常活跃但不再掌控一切的地球上生活会是什么样子*”
- en: '![](../Images/1a89d4e9255183918352814ca29bea6d.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a89d4e9255183918352814ca29bea6d.png)'
- en: '[RoBERTa fine-tuned QA model output](https://huggingface.co/deepset/roberta-base-squad2)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[RoBERTa 微调问答模型输出](https://huggingface.co/deepset/roberta-base-squad2)'
- en: The solution to getting more relevant results is fine-tuning. In the article
    below, I have discussed how to fine-tune extractive question answering models
    on the HuggingFace hub using custom data. Fine-tuning based on just a few thousand
    examples can vastly improve performance, sometimes by **more than 50%**.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 获得更相关结果的解决方案是微调。在下面的文章中，我讨论了如何使用自定义数据在 HuggingFace hub 上微调抽取式问答模型。仅通过几千个示例进行微调可以大幅提高性能，有时提高幅度达到
    **超过 50%**。
- en: '[](/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=post_page-----5a8a1faab59a--------------------------------)
    [## Fine-Tune Transformer Models For Question Answering On Custom Data'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=post_page-----5a8a1faab59a--------------------------------)
    [## 对自定义数据进行微调的 Transformer 模型以进行问答'
- en: A tutorial on fine-tuning the Hugging Face RoBERTa QA Model on custom data and
    obtaining significant performance boosts
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于如何在自定义数据上微调 Hugging Face RoBERTa QA 模型并获得显著性能提升的教程
- en: towardsdatascience.com](/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=post_page-----5a8a1faab59a--------------------------------)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/fine-tune-transformer-models-for-question-answering-on-custom-data-513eaac37a80?source=post_page-----5a8a1faab59a--------------------------------)
- en: However, extractive QA does not do so well in cases where the answer is not
    explicitly in the context like below.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，抽取式问答在答案没有明确存在于上下文中的情况下效果不佳，如下所示。
- en: '![](../Images/c1df25fccb77b442930dabf652d6ccda.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c1df25fccb77b442930dabf652d6ccda.png)'
- en: Model yielding useless results when the answer is not explicitly present
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当答案没有明确存在时，模型产生无用结果
- en: This issue can be circumvented by appending “ANSWERNOTFOUND” and fine-tuning
    on these cases so that the model does not yield an answer when it is unsure.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过附加“ANSWERNOTFOUND”并在这些案例上进行微调来规避这个问题，以确保模型在不确定时不提供答案。
- en: Abstractive QA
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 抽象问答
- en: While ChatGPT has taken the whole world by storm recently, [the original GPT
    model](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
    was released before BERT. GPT models use the decoder layer of the original 2017
    Transformer. GPT models are trained to predict the next word in a sequence in
    an unsupervised manner. Next, they are fine-tuned in a supervised fashion. For
    QA, GPT models are presented during fine-tuning with multiple answer choices across
    numerous examples, and they are trained to pick the right choice. One important
    difference at inference is that GPT models output one token at a time and thus
    are generative, rather than extractive.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 ChatGPT 最近在全球范围内引起了轰动，[原始的 GPT 模型](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
    在 BERT 之前发布。GPT 模型使用了 2017 年 Transformer 的解码器层。GPT 模型被训练来以无监督的方式预测序列中的下一个词。然后，它们在监督的方式下进行微调。对于问答，GPT
    模型在微调时会接触到多个答案选项的众多示例，并被训练选择正确的选项。一个重要的推断区别是，GPT 模型一次输出一个 token，因此是生成式的，而不是提取式的。
- en: 'Currently, OpenAI has 4 major language models that they offer API access to:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，OpenAI 提供了 4 个主要的语言模型 API 访问：
- en: Ada ($0.0004 / 1K tokens — Fastest)
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ada ($0.0004 / 1K tokens — 最快)
- en: Babbage ($0.0005 / 1K tokens)
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Babbage ($0.0005 / 1K tokens)
- en: Curie ($0.0020 / 1K tokens)
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Curie ($0.0020 / 1K tokens)
- en: Davinci ($0.0200 / 1K tokens — Most powerful)
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Davinci ($0.0200 / 1K tokens — 最强大)
- en: 'For reference, 1K tokens is basically 750 words that you send in to the API
    to process. So let’s see how this model does for similar questions:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 作为参考，1K tokens 基本上是你发送给 API 处理的 750 个单词。那我们来看看这个模型对类似问题的表现如何：
- en: '![](../Images/191f90cc5451722d0fdbd5f006f9c87d.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/191f90cc5451722d0fdbd5f006f9c87d.png)'
- en: Davinci OpenAI Model based on GPT3 for QA
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 GPT3 的 Davinci OpenAI 模型用于问答
- en: '![](../Images/2dad6116cd83bcff8cb601ee5abc85ef.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2dad6116cd83bcff8cb601ee5abc85ef.png)'
- en: Davinci OpenAI Model based on GPT3 for QA
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 GPT3 的 Davinci OpenAI 模型用于问答
- en: As you can see, the Davinci model does pretty well in summarizing movie plots
    as well as saying “I don’t know” when the answer is not clearly in the context.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，Davinci 模型在总结电影情节方面表现很好，同时在答案不明确的情况下会说“我不知道”。
- en: Which Model is Better — Abstractive or Extractive??
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 哪种模型更好 — 抽象还是提取式？
- en: 'You might be tempted to say that OpenAI’s abstractive QA is clearly superior
    to extractive QA models. However, that is where the business case matters. I’ll
    break it down below:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会倾向于说 OpenAI 的抽象问答明显优于提取式问答模型。然而，这就是商业案例发挥作用的地方。我将在下面详细说明：
- en: Cost
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 成本
- en: The Davinci model is clearly more expensive, at a large enough scale. It amounts
    to 0.02$ per 1K tokens which might as well be 0.02$ for 1–10 queries. Whereas
    hosting a model from Hugging Face on AWS might amount to a fraction of the cost,
    at 0.5 cents to 1$ per hour running thousands or more queries every hour.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Davinci 模型在大规模时明显更贵。每 1K tokens 需 $0.02，这可能也适用于 1–10 个查询。而在 AWS 上托管 Hugging
    Face 模型可能成本更低，运行数千次或更多查询每小时只需 0.5 美分到 1 美元。
- en: Output
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输出
- en: If you are interested in building a chat bot type interface and expect free
    response answers, abstractive QA is the way to go. Your users might not be satisfied
    by dry extractive answers that paraphrase the text. However, if you are doing
    post processing on the answers obtained — say storing numbers in a database, abstractive
    QA might be a hindrance as you need to use additional logic to strip out extra
    words.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣构建一个聊天机器人类型的界面并期望自由响应的答案，那么抽象问答是最佳选择。用户可能不会满意仅仅是将文本改写的干巴巴的提取式答案。然而，如果你在对获得的答案进行后处理——比如将数字存入数据库，那么抽象问答可能会成为障碍，因为你需要使用额外的逻辑来剥离多余的词汇。
- en: Customizability
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义
- en: OpenAI API usage requires reliance on OpenAI servers. While they do make it
    possible to fine-tune their models on custom data, it is not possible to host
    these models on separate infrastructures like AWS. But you can take open-source
    models on Hugging Face and create APIs on AWS, and not have to rely any more on
    Hugging Face for model serving. This is powerful in that it allows companies to
    keep all the infrastructure in-house and rely only on cloud providers like AWS.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 使用OpenAI API需要依赖OpenAI服务器。虽然他们确实允许在自定义数据上微调模型，但无法在AWS等独立基础设施上托管这些模型。不过，你可以使用Hugging
    Face上的开源模型，在AWS上创建API，从而不再依赖Hugging Face进行模型服务。这一做法的强大之处在于，它允许公司将所有基础设施保留在内部，只依赖像AWS这样的云服务提供商。
- en: One thing I would like to point out is that Hugging Face does also support abstractive
    QA models. In fact, they released a [text2text generation model Flan T5](https://huggingface.co/docs/transformers/model_doc/flan-t5)
    on the model hub recently. But I have noticed that this model does not perform
    as well on QA tasks as the Davinci GPT-3 model. Very soon, I expect Hugging Face
    to also host open-source fine-tuned models like the Davinci GPT-3 model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我想指出的一点是，Hugging Face也支持抽象型QA模型。事实上，他们最近在模型中心发布了一个 [text2text生成模型Flan T5](https://huggingface.co/docs/transformers/model_doc/flan-t5)。但我发现这个模型在QA任务上的表现不如Davinci
    GPT-3模型。很快，我期望Hugging Face也会托管像Davinci GPT-3这样的开源微调模型。
- en: I hope this article was a useful walkthrough in using AI for question answering.
    In conjunction with existing methods for information retrieval and searching through
    large amounts of data, AI based information extraction can help extract needles
    from haystacks, and greatly improve efficiency in extracting essential details
    from large amounts of data, previously possible only through human comprehension.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这篇文章对使用AI进行问题回答提供了有用的指导。结合现有的信息检索方法和通过大量数据进行搜索，基于AI的信息提取可以帮助从大海捞针中提取有用信息，大大提高从大量数据中提取关键细节的效率，这在过去只能通过人工理解来实现。
- en: '***UPDATE:*** [***https://www.answerchatai.com/***](https://www.answerchatai.com/)
    ***— our QA engine using generative AI to answer questions and extract key knowledge
    from custom text is now live! Answer domain specific questions 3 easy steps!***'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '***更新：*** [***https://www.answerchatai.com/***](https://www.answerchatai.com/)
    ***— 我们的QA引擎使用生成式AI回答问题并从自定义文本中提取关键信息现已上线！只需3个简单步骤即可回答领域特定的问题！***'
- en: '***Upload a URL or paste a text and hit the search button***'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***上传一个URL或粘贴文本并点击搜索按钮***'
- en: '***Ask a question specific to the context and hit query***'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***提出一个与上下文相关的问题并点击查询***'
- en: '***Get your answer!***'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***获取你的答案！***'
- en: '***Feel free to use and let me know your feedback!***'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '***随意使用，并告诉我你的反馈！***'
- en: '*If you are not yet a Medium member and want to support writers like me, feel
    free to sign-up through my referral link:* [*https://skanda-vivek.medium.com/membership*](https://skanda-vivek.medium.com/membership)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果你还不是Medium会员并想支持像我这样的写作者，可以通过我的推荐链接注册：* [*https://skanda-vivek.medium.com/membership*](https://skanda-vivek.medium.com/membership)'
- en: '*For weekly data-based perspectives* [*subscribe here*](https://skandavivek.substack.com/)*!*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*想要获取基于数据的每周视角* [*请在这里订阅*](https://skandavivek.substack.com/)*！*'
