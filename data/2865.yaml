- en: Deploying PyTorch Models with Nvidia Triton Inference Server
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Nvidia Triton 推理服务器部署 PyTorch 模型
- en: 原文：[https://towardsdatascience.com/deploying-pytorch-models-with-nvidia-triton-inference-server-bb139066a387?source=collection_archive---------4-----------------------#2023-09-14](https://towardsdatascience.com/deploying-pytorch-models-with-nvidia-triton-inference-server-bb139066a387?source=collection_archive---------4-----------------------#2023-09-14)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/deploying-pytorch-models-with-nvidia-triton-inference-server-bb139066a387?source=collection_archive---------4-----------------------#2023-09-14](https://towardsdatascience.com/deploying-pytorch-models-with-nvidia-triton-inference-server-bb139066a387?source=collection_archive---------4-----------------------#2023-09-14)
- en: A flexible high-performant model serving solution
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一种灵活的高性能模型服务解决方案
- en: '[](https://ram-vegiraju.medium.com/?source=post_page-----bb139066a387--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page-----bb139066a387--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bb139066a387--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bb139066a387--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page-----bb139066a387--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ram-vegiraju.medium.com/?source=post_page-----bb139066a387--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page-----bb139066a387--------------------------------)[](https://towardsdatascience.com/?source=post_page-----bb139066a387--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----bb139066a387--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page-----bb139066a387--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6e49569edd2b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeploying-pytorch-models-with-nvidia-triton-inference-server-bb139066a387&user=Ram+Vegiraju&userId=6e49569edd2b&source=post_page-6e49569edd2b----bb139066a387---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bb139066a387--------------------------------)
    ·7 min read·Sep 14, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbb139066a387&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeploying-pytorch-models-with-nvidia-triton-inference-server-bb139066a387&user=Ram+Vegiraju&userId=6e49569edd2b&source=-----bb139066a387---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6e49569edd2b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeploying-pytorch-models-with-nvidia-triton-inference-server-bb139066a387&user=Ram+Vegiraju&userId=6e49569edd2b&source=post_page-6e49569edd2b----bb139066a387---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----bb139066a387--------------------------------)
    ·7分钟阅读·2023年9月14日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbb139066a387&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeploying-pytorch-models-with-nvidia-triton-inference-server-bb139066a387&user=Ram+Vegiraju&userId=6e49569edd2b&source=-----bb139066a387---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbb139066a387&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeploying-pytorch-models-with-nvidia-triton-inference-server-bb139066a387&source=-----bb139066a387---------------------bookmark_footer-----------)![](../Images/eb2afba9c81fb0c85c927e7f318bc8e1.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbb139066a387&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeploying-pytorch-models-with-nvidia-triton-inference-server-bb139066a387&source=-----bb139066a387---------------------bookmark_footer-----------)![](../Images/eb2afba9c81fb0c85c927e7f318bc8e1.png)'
- en: Image from [Unsplash](https://unsplash.com/photos/8fVK5TFKaSE)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自于 [Unsplash](https://unsplash.com/photos/8fVK5TFKaSE)
- en: Machine Learning’s (ML) value is truly recognized in real-world applications
    when we arrive at [Model Hosting and Inference](https://cloud.google.com/bigquery/docs/inference-overview#:~:text=Machine%20learning%20inference%20is%20the,machine%20learning%20model%20into%20production.%22).
    It’s hard to productionize ML workloads if you don’t have a highly performant
    model-serving solution that will help your model scale up and down.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（ML）的价值在实际应用中真正体现，当我们进入到[模型托管和推理](https://cloud.google.com/bigquery/docs/inference-overview#:~:text=Machine%20learning%20inference%20is%20the,machine%20learning%20model%20into%20production.%22)。如果没有高性能的模型服务解决方案，帮助你的模型进行扩展和收缩，就很难将ML工作负载投入生产。
- en: '**What is a model server/what is model serving?** Think of a model server as
    a bit of an equivalent to a web server in the ML world. It’s not enough to just
    throw large amounts of hardware behind the model, you need a communication layer
    that will help process your client’s requests while efficiently allocating the
    hardware needed to address the traffic your application is seeing. Model Servers
    are a tunable feature for users: we can squeeze performance from a latency perspective
    by controlling aspects such as gRPC vs REST, etc. Popular examples of model servers
    include the following:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**什么是模型服务器/什么是模型服务？** 可以把模型服务器看作是机器学习领域中类似于 web 服务器的东西。仅仅依靠大量的硬件是不够的，你还需要一个通信层来处理客户端的请求，同时有效地分配硬件，以应对应用程序所遇到的流量。模型服务器是一个可调节的功能：我们可以通过控制诸如
    gRPC 与 REST 等方面，从延迟的角度挤出性能。常见的模型服务器示例包括以下几种：'
- en: '[TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving)'
- en: '[TorchServe](https://pytorch.org/serve/)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TorchServe](https://pytorch.org/serve/)'
- en: '[Multi-Model Server (MMS)](https://github.com/awslabs/multi-model-server)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Multi-Model Server (MMS)](https://github.com/awslabs/multi-model-server)'
- en: '[Deep Java Library (DJL)](https://github.com/deepjavalibrary/djl-serving)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Deep Java Library (DJL)](https://github.com/deepjavalibrary/djl-serving)'
- en: The one we explore today is [Nvidia Triton Inference Server](https://github.com/triton-inference-server/server),
    a highly flexible and performant model serving solution. Each model server requires
    for model artifacts and inference scripts to…
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们今天探索的是[Nvidia Triton Inference Server](https://github.com/triton-inference-server/server)，这是一种高度灵活且高性能的模型服务解决方案。每个模型服务器都需要模型工件和推理脚本来…
