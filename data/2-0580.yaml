- en: 'Courage to Learn ML: Demystifying L1 & L2 Regularization (part 2)'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习机器学习的勇气：揭开L1与L2正则化的神秘面纱（第二部分）
- en: 原文：[https://towardsdatascience.com/courage-to-learn-ml-unraveling-l1-l2-regularization-part-2-1bb171e43b35](https://towardsdatascience.com/courage-to-learn-ml-unraveling-l1-l2-regularization-part-2-1bb171e43b35)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/courage-to-learn-ml-unraveling-l1-l2-regularization-part-2-1bb171e43b35](https://towardsdatascience.com/courage-to-learn-ml-unraveling-l1-l2-regularization-part-2-1bb171e43b35)
- en: Unlocking the Intuition Behind L1 Sparsity with Lagrange multipliers
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 揭示拉格朗日乘子背后的L1稀疏性直觉
- en: '[](https://amyma101.medium.com/?source=post_page-----1bb171e43b35--------------------------------)[![Amy
    Ma](../Images/2edf55456a1f92724535a1441fa2bef5.png)](https://amyma101.medium.com/?source=post_page-----1bb171e43b35--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1bb171e43b35--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1bb171e43b35--------------------------------)
    [Amy Ma](https://amyma101.medium.com/?source=post_page-----1bb171e43b35--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://amyma101.medium.com/?source=post_page-----1bb171e43b35--------------------------------)[![Amy
    Ma](../Images/2edf55456a1f92724535a1441fa2bef5.png)](https://amyma101.medium.com/?source=post_page-----1bb171e43b35--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1bb171e43b35--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1bb171e43b35--------------------------------)
    [Amy Ma](https://amyma101.medium.com/?source=post_page-----1bb171e43b35--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1bb171e43b35--------------------------------)
    ·6 min read·Nov 25, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1bb171e43b35--------------------------------)
    ·阅读时间6分钟·2023年11月25日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: 'Welcome back to ‘[Courage to Learn ML](/towardsdatascience.com/tagged/courage-to-learn-ml):
    Demystifying L1 & L2 Regularization,’ Part Two. In [our previous discussion](https://medium.com/@yujing-ma45/understanding-l1-l2-regularization-part-1-9c7affe6f920),
    we explored the benefits of smaller coefficients and the means to attain them
    through weight penalization techniques. Now, in this follow-up, our mentor and
    learner will delve even deeper into the realm of L1 and L2 regularization.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎回到‘[学习机器学习的勇气](/towardsdatascience.com/tagged/courage-to-learn-ml)：揭开L1与L2正则化的神秘面纱’，第二部分。在
    [我们之前的讨论](https://medium.com/@yujing-ma45/understanding-l1-l2-regularization-part-1-9c7affe6f920)
    中，我们探讨了较小系数的好处及通过权重惩罚技术获得这些系数的方法。现在，在这篇续集中，我们的导师和学习者将进一步深入L1和L2正则化的领域。
- en: 'If you’ve been pondering questions like these, you’re in the right place:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经思考过这些问题，你来对地方了：
- en: What’s the reason behind the names L1 and L2 regularization?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L1和L2正则化名称背后的原因是什么？
- en: How do we interpret the classic L1 and L2 regularization graph?
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何解读经典的L1和L2正则化图？
- en: What are Lagrange multipliers, and how can we understand them intuitively?
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拉格朗日乘子是什么，我们如何直观地理解它们？
- en: Applying Lagrange multipliers to comprehend L1 sparsity.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用拉格朗日乘子来理解L1稀疏性。
- en: Your engagement — likes, comments, and follows — does more than just boost morale;
    it powers our journey of discovery! So, let’s dive in.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你的参与——点赞、评论和关注——不仅仅是提升士气；它推动了我们的发现之旅！那么，让我们开始吧。
- en: '![](../Images/eb38cadf6bb97d614c0c0bc8ddd9755a.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eb38cadf6bb97d614c0c0bc8ddd9755a.png)'
- en: Photo by [Aarón Blanco Tejedor](https://unsplash.com/@the_meaning_of_love?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Aarón Blanco Tejedor](https://unsplash.com/@the_meaning_of_love?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Why they call L1, l2 regularization?
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么它们被称为L1和L2正则化？
- en: The name, L1 and L2 regularization, comes from the concept of Lp norms directly.
    Lp norms represent different ways to calculate distances from a point to the origin
    in a space. For instance, the L1 norm, also known as Manhattan distance, calculates
    the distance using the absolute values of coordinates, like ∣*x*∣+∣*y*∣. On the
    other hand, the L2 norm, or Euclidean distance, calculates it as the square root
    of the sum of the squared values, which is sqrt(x² + y²)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: L1和L2正则化的名称直接源于Lp范数的概念。Lp范数代表了计算空间中某点到原点距离的不同方式。例如，L1范数，也称为曼哈顿距离，计算距离时使用坐标的绝对值，如∣*x*∣+∣*y*∣。另一方面，L2范数，或称为欧几里得距离，则计算为平方值和的平方根，即sqrt(x²
    + y²)。
- en: 'In the context of regularization in machine learning, these norms are used
    to create penalty terms that are added to the loss function. You can think of
    Lp regularization as measuring the total distance of the model’s weights from
    the origin in a high-dimensional space. The choice of norm affects the nature
    of this penalty: the L1 norm tends to make some coefficients zero, effectively
    selecting more important features, while the L2 norm shrinks the coefficients
    towards zero, ensuring no single feature disproportionately influences the model.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中的正则化背景下，这些范数用于创建添加到损失函数中的惩罚项。你可以将Lp正则化视为测量模型权重在高维空间中与原点的总距离。范数的选择会影响惩罚的性质：L1范数倾向于使一些系数为零，从而有效地选择更重要的特征，而L2范数则将系数缩小到接近零，确保没有单一特征对模型的影响过大。
- en: Therefore, L1 and L2 regularization are named after these mathematical norms
    — L1 norm and L2 norm — due to the way they apply their respective distance calculations
    as penalties to the model’s weights. This helps in controlling overfitting.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，L1和L2正则化的名称来源于这些数学范数——L1范数和L2范数——因为它们将各自的距离计算作为惩罚施加于模型的权重。这有助于控制过拟合。
- en: '**I often come across the graph below when studying L1 and L2 regularization,
    but I find it quite challenging to interpret. Could you help clarify what it represents?**'
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**我在学习L1和L2正则化时经常遇到下面的图表，但我发现很难解读。您能帮忙澄清一下它表示什么吗？**'
- en: '![](../Images/9c4081ca5e2c0ac4ec1bb37796236b09.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c4081ca5e2c0ac4ec1bb37796236b09.png)'
- en: 'The traditional yet perplexing L1 and L2 regularization graph found in textbooks..
    source: [https://commons.wikimedia.org/wiki/File:Regularization.jpg](https://commons.wikimedia.org/wiki/File:Regularization.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '传统但令人困惑的L1和L2正则化图表在教科书中常见…… 来源: [https://commons.wikimedia.org/wiki/File:Regularization.jpg](https://commons.wikimedia.org/wiki/File:Regularization.jpg)'
- en: Alright, let’s unpack this graph step-by-step. To start, it’s essential to understand
    what its different elements signify. Imagine our loss function is defined by just
    two weights, w1 and w2 (in the graph we use beta instead of w, but they represent
    the same concept). The axes of the graph represent these weights we aim to optimize.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我们一步一步地解析这张图。首先，理解其不同元素的意义是至关重要的。假设我们的损失函数由两个权重w1和w2定义（在图中我们用beta代替w，但它们表示相同的概念）。图表的轴表示这些我们希望优化的权重。
- en: Without any weight penalties, our goal is to find w1 and w2 values that minimize
    our loss function. You can visualize this function’s landscape as a valley or
    basin, illustrated in the graph by the elliptical contours.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有任何权重惩罚的情况下，我们的目标是找到使损失函数最小化的w1和w2值。你可以将这个函数的景观想象成一个山谷或盆地，在图中通过椭圆轮廓来表示。
- en: Now, let’s delve into the penalties. The L1 norm, shown as a diamond shape,
    essentially measures the Manhattan distance of w1 and w2 from the origin. The
    L2 norm forms a circle, representing the sum of squared weights.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入了解惩罚。L1范数，呈菱形，基本上测量了w1和w2到原点的曼哈顿距离。L2范数形成一个圆，表示权重的平方和。
- en: The center of the elliptical contour indicates the global minimum of the objective
    function, where we find our ideal weights. The centers of the L1 and L2 shapes
    (diamond and circle) at the origin, where all weights are zero, highlight the
    minimal weight penalty scenario. As we increasing the penalty term’s intensity,
    the model’s weights would gravitate closer to zero. This graph is a visual guide
    to understanding these dynamics and the impact of penalties on the weights.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 椭圆轮廓的中心表示目标函数的全局最小值，在那里我们找到理想的权重。L1和L2形状（菱形和圆形）在原点的中心，其中所有权重为零，突出显示了最小权重惩罚的情况。随着惩罚项强度的增加，模型的权重会趋近于零。这个图表是理解这些动态以及惩罚对权重影响的视觉指南。
- en: Understood. So…. The graph shows a dimension created by the weights, and the
    two distinct shapes illustrate the objective function and the penalty, respectively.
    How should we interpret the intersection point labeled w*? What does this point
    signify?
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 明白了。所以…… 图表展示了由权重创建的一个维度，而两个不同的形状分别表示目标函数和惩罚。我们应该如何解读标记为w*的交点？这个点意味着什么？
- en: To understand the above graph as a whole, it’s essential to grasp the concept
    of Lagrange multipliers, a key tool in optimization. Lagrange multipliers aid
    in finding the optimal points (maximum or minimum) of a function within certain
    constraints.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 要全面理解上述图表，必须掌握拉格朗日乘子这一优化关键工具。拉格朗日乘子有助于在特定约束条件下找到函数的最优点（最大值或最小值）。
- en: Imagine you’re hiking up a mountain with the goal of reaching the peak. There
    are various paths, but due to safety, you’re required to stay on a designated
    safe path. Here, reaching the peak represents the optimization problem, and the
    safe path symbolizes the constraints.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下你正在登山，目标是到达山顶。虽然有多条路径，但为了安全，你必须沿着指定的安全路径行走。在这里，到达山顶代表了优化问题，而安全路径则象征着约束。
- en: Mathematically, suppose you have a function f(x, y) to optimize. This optimization
    must adhere to a constraint, represented by another function g(x, y) = 0.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，假设你有一个函数f(x, y)需要优化。这一优化必须遵守一个约束，由另一个函数g(x, y) = 0表示。
- en: '![](../Images/68a20f2eaf9a3abd420a45a322c23f22.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/68a20f2eaf9a3abd420a45a322c23f22.png)'
- en: 'Lagrange Multipliers 2D. Source: [https://upload.wikimedia.org/wikipedia/commons/b/bf/LagrangeMultipliers2D.svg](https://upload.wikimedia.org/wikipedia/commons/b/bf/LagrangeMultipliers2D.svg)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 拉格朗日乘子2D。来源：[https://upload.wikimedia.org/wikipedia/commons/b/bf/LagrangeMultipliers2D.svg](https://upload.wikimedia.org/wikipedia/commons/b/bf/LagrangeMultipliers2D.svg)
- en: In the ‘Lagrange Multipliers 2D’ graph from Wikipedia, the blue contours represent
    f(x, y) (the mountain’s landscape), and the red curves indicate the constraints.
    The point where these two intersect, although not the peak point on the f(x, y)
    contour, represents the optimal solution under the given constraint. Lagrange
    multipliers solve this by merging the objective function with its constraints.
    In the other word, Lagrange multipliers will help you find the point easier.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在维基百科的‘拉格朗日乘子2D’图中，蓝色轮廓表示f(x, y)（山的地形），而红色曲线表示约束。这两者的交点，虽然不是f(x, y)轮廓上的最高点，但代表了在给定约束下的最优解。拉格朗日乘子通过将目标函数与其约束合并来解决这个问题。换句话说，拉格朗日乘子会帮助你更容易找到这个点。
- en: So, if we circle back to that L1 and L2 graph, are you suggesting that the diamond
    and circle shapes represent constraints? And does that mean the spot where they
    intersect, that tangent point, is essentially the sweet spot for hitting the max
    of f(x, y) within those constraints?
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 所以，如果我们回到L1和L2图，你是否暗示菱形和圆形代表约束？这是否意味着它们交点的切点本质上是满足这些约束下f(x, y)的最大值的最佳点？
- en: Correct! The L1 and L2 regularization techniques can indeed be visualized as
    imposing constraints in the form of a diamond and a circle, respectively. So the
    graph helps us understand how these regularization methods impact the optimization
    of a function, typically the loss function in machine learning models.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 正确！L1和L2正则化技术的确可以被可视化为分别施加菱形和圆形的约束。因此，这个图帮助我们理解这些正则化方法如何影响函数的优化，通常是机器学习模型中的损失函数。
- en: '![](../Images/bf23589d50e185f421663dc8687a7a73.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bf23589d50e185f421663dc8687a7a73.png)'
- en: 'A better illustration on L2 (left), L1 (right) regularizations. Source: [https://www.researchgate.net/figure/Parameter-norm-penalties-L2-norm-regularization-left-and-L1-norm-regularization_fig2_355020694](https://www.researchgate.net/figure/Parameter-norm-penalties-L2-norm-regularization-left-and-L1-norm-regularization_fig2_355020694)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: L2（左）和L1（右）正则化的更好示例。来源：[https://www.researchgate.net/figure/Parameter-norm-penalties-L2-norm-regularization-left-and-L1-norm-regularization_fig2_355020694](https://www.researchgate.net/figure/Parameter-norm-penalties-L2-norm-regularization-left-and-L1-norm-regularization_fig2_355020694)
- en: '**L1 Regularization (Diamond Shape)**: The L1 norm creates a diamond-shaped
    constraint. This shape is characterized by **its sharp corners along the axes**.
    When the optimization process (like gradient descent) seeks the point that minimizes
    the loss function while staying within this diamond, it’s more likely to hit these
    corners. At these corners, one of the weights (parameters of the model) becomes
    zero while others remain non-zero**. This property of the L1 norm leads to sparsity
    in the model parameters, meaning some weights are exactly zero. This sparsity
    is useful for feature selection, as it effectively removes some features from
    the model.**'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**L1 正则化（菱形形状）**：L1范数创建了一个菱形约束。这种形状的特点是**沿轴的尖角**。当优化过程（如梯度下降）在这个菱形内寻找最小化损失函数的点时，它更可能碰到这些角落。在这些角落，模型的一个权重（参数）变为零，而其他权重保持非零**。L1范数的这种特性导致了模型参数的稀疏性，这意味着某些权重为零。这种稀疏性对特征选择非常有用，因为它有效地从模型中移除了一些特征。**'
- en: '**L2 Regularization (Circle Shape)**: On the other hand, the L2 norm creates
    a circular-shaped constraint. **The smooth, round nature of the circle means that
    the optimization process is less likely to find solutions at the axes where weights
    are zero.** Instead, the L2 norm tends to shrink the weights uniformly without
    necessarily driving any to zero. This controls the model complexity by preventing
    weights from becoming too large, thereby helping to avoid overfitting. However,
    unlike the L1 norm, it doesn’t lead to sparsity in the model parameters.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**L2 正则化（圆形约束）**：另一方面，L2 范数创建了一个圆形的约束。**圆的平滑、圆润的特性意味着优化过程不太可能在权重为零的坐标轴上找到解决方案。**
    相反，L2 范数倾向于均匀地缩小权重，而不一定将任何权重驱动为零。这通过防止权重过大来控制模型复杂性，从而帮助避免过拟合。然而，与 L1 范数不同，它不会导致模型参数的稀疏性。'
- en: Keep in mind, Lagrange multipliers aren’t the only method to grasp L1 and L2
    regularizations. Let’s take a break here, and I’ll address more of your questions
    in [our next installment](/courage-to-learn-ml-demystifying-l1-l2-regularization-part-3-ee27cd4b557a).
    See you soon!
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，拉格朗日乘数并不是掌握 L1 和 L2 正则化的唯一方法。我们在这里休息一下，我将在[下一期](/courage-to-learn-ml-demystifying-l1-l2-regularization-part-3-ee27cd4b557a)中回答更多的问题。很快见！
- en: 'Other posts in this series:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 本系列的其他文章：
- en: '[Courage to Learn ML: Demystifying L1 & L2 Regularization (part 1)](https://medium.com/@yujing-ma45/understanding-l1-l2-regularization-part-1-9c7affe6f920)'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[勇敢学习机器学习：揭示 L1 和 L2 正则化（第 1 部分）](https://medium.com/@yujing-ma45/understanding-l1-l2-regularization-part-1-9c7affe6f920)'
- en: '[Courage to Learn ML:](/courage-to-learn-ml-demystifying-l1-l2-regularization-part-3-ee27cd4b557a)
    [Demystifying](https://medium.com/@yujing-ma45/understanding-l1-l2-regularization-part-1-9c7affe6f920)
    [L1 & L2 Regularization (part 3)](/courage-to-learn-ml-demystifying-l1-l2-regularization-part-3-ee27cd4b557a)'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[勇敢学习机器学习：](/courage-to-learn-ml-demystifying-l1-l2-regularization-part-3-ee27cd4b557a)
    [揭示](https://medium.com/@yujing-ma45/understanding-l1-l2-regularization-part-1-9c7affe6f920)
    [L1 和 L2 正则化（第 3 部分）](/courage-to-learn-ml-demystifying-l1-l2-regularization-part-3-ee27cd4b557a)'
- en: '***If you liked the article, you can find me on*** [***LinkedIn***](https://www.linkedin.com/in/amyma101/)***.***'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '***如果你喜欢这篇文章，你可以在*** [***LinkedIn***](https://www.linkedin.com/in/amyma101/)***上找到我。***'
