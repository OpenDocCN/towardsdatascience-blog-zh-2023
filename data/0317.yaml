- en: 'Applied Reinforcement Learning V: Normalized Advantage Function (NAF) for Continuous
    Control'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用强化学习 V：用于连续控制的归一化优势函数（NAF）
- en: 原文：[https://towardsdatascience.com/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095?source=collection_archive---------11-----------------------#2023-01-19](https://towardsdatascience.com/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095?source=collection_archive---------11-----------------------#2023-01-19)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095?source=collection_archive---------11-----------------------#2023-01-19](https://towardsdatascience.com/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095?source=collection_archive---------11-----------------------#2023-01-19)
- en: Introduction and explanation of the NAF algorithm, widely used in continuous
    control tasks
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍和解释 NAF 算法，该算法广泛应用于连续控制任务
- en: '[](https://medium.com/@JavierMtz5?source=post_page-----62ad143d3095--------------------------------)[![Javier
    Martínez Ojeda](../Images/5b5df4220fa64c13232c29de9b4177af.png)](https://medium.com/@JavierMtz5?source=post_page-----62ad143d3095--------------------------------)[](https://towardsdatascience.com/?source=post_page-----62ad143d3095--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----62ad143d3095--------------------------------)
    [Javier Martínez Ojeda](https://medium.com/@JavierMtz5?source=post_page-----62ad143d3095--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@JavierMtz5?source=post_page-----62ad143d3095--------------------------------)[![哈维尔·马丁内斯·奥赫达](../Images/5b5df4220fa64c13232c29de9b4177af.png)](https://medium.com/@JavierMtz5?source=post_page-----62ad143d3095--------------------------------)[](https://towardsdatascience.com/?source=post_page-----62ad143d3095--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----62ad143d3095--------------------------------)
    [哈维尔·马丁内斯·奥赫达](https://medium.com/@JavierMtz5?source=post_page-----62ad143d3095--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F74d7213a71a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapplied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095&user=Javier+Mart%C3%ADnez+Ojeda&userId=74d7213a71a8&source=post_page-74d7213a71a8----62ad143d3095---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----62ad143d3095--------------------------------)
    ·9 min read·Jan 19, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F62ad143d3095&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapplied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095&user=Javier+Mart%C3%ADnez+Ojeda&userId=74d7213a71a8&source=-----62ad143d3095---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F74d7213a71a8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapplied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095&user=Javier+Mart%C3%ADnez+Ojeda&userId=74d7213a71a8&source=post_page-74d7213a71a8----62ad143d3095---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----62ad143d3095--------------------------------)
    ·9 min read·2023年1月19日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F62ad143d3095&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapplied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095&user=Javier+Mart%C3%ADnez+Ojeda&userId=74d7213a71a8&source=-----62ad143d3095---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F62ad143d3095&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapplied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095&source=-----62ad143d3095---------------------bookmark_footer-----------)![](../Images/980806b6e676bcf2f2e28c3dca2000f0.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F62ad143d3095&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapplied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control-62ad143d3095&source=-----62ad143d3095---------------------bookmark_footer-----------)![](../Images/980806b6e676bcf2f2e28c3dca2000f0.png)'
- en: Photo by [Sufyan](https://unsplash.com/@blenderdesigner_1688?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Sufyan](https://unsplash.com/@blenderdesigner_1688?utm_source=medium&utm_medium=referral)
    提供，刊登在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: If you want to read this article without a Premium Medium account, you can do
    it from this friend link :)
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你想在没有 Premium Medium 账户的情况下阅读这篇文章，可以通过这个朋友链接访问 :)
- en: '[https://www.learnml.wiki/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control/](https://www.learnml.wiki/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control/)'
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[https://www.learnml.wiki/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control/](https://www.learnml.wiki/applied-reinforcement-learning-v-normalized-advantage-function-naf-for-continuous-control/)'
- en: Previous articles in this series have introduced and explained two Reinforcement
    Learning algorithms that have been widely used since their inception:[**Q-Learning**](https://medium.com/towards-data-science/applied-reinforcement-learning-i-q-learning-d6086c1f437)
    and [**DQN**](https://medium.com/towards-data-science/applied-reinforcement-learning-iii-deep-q-networks-dqn-8f0e38196ba9).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本系列的前几篇文章介绍并解释了两种自其诞生以来被广泛使用的强化学习算法：[**Q-Learning**](https://medium.com/towards-data-science/applied-reinforcement-learning-i-q-learning-d6086c1f437)
    和 [**DQN**](https://medium.com/towards-data-science/applied-reinforcement-learning-iii-deep-q-networks-dqn-8f0e38196ba9)。
- en: '**Q-Learning** stores the Q-Values in an action-state matrix, such that to
    obtain the action *a* with the largest Q-Value in state *s*, the largest element
    of the Q-Value matrix for row *s* must be found, which makes its application to
    continuous state or action spaces impossible since the Q-Value matrix would be
    infinite.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**Q-Learning** 将 Q-值存储在一个动作-状态矩阵中，因此要获得状态 *s* 下具有最大 Q-值的动作 *a*，必须找到 Q-值矩阵中第
    *s* 行的最大元素，这使得它在连续状态或动作空间中的应用变得不可能，因为 Q-值矩阵将是无限的。'
- en: On the other hand, **DQN** partially solves this problem by making use of a
    neural network to obtain the Q-Values associated to a state *s*, such that the
    output of the neural network are the Q-Values for each possible action of the
    agent (the equivalent to a row in the action-state matrix of Q-Learning). This
    algorithm allows training in environments with a continuous state…
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，**DQN** 部分解决了这个问题，通过使用神经网络来获得与状态 *s* 相关的 Q-值，使得神经网络的输出是代理的每个可能动作的 Q-值（相当于
    Q-Learning 中的动作-状态矩阵的一行）。这个算法允许在具有连续状态的环境中进行训练……
