- en: How to Use Regex Patterns in Pandas to Work With Complex Strings
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何在Pandas中使用正则表达式模式处理复杂字符串
- en: 原文：[https://towardsdatascience.com/regex-patterns-in-pandas-api-afe70178f9e9](https://towardsdatascience.com/regex-patterns-in-pandas-api-afe70178f9e9)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/regex-patterns-in-pandas-api-afe70178f9e9](https://towardsdatascience.com/regex-patterns-in-pandas-api-afe70178f9e9)
- en: Regex simplifies pattern-matching tasks on large amounts of text — Pandas makes
    it elegant
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则表达式简化了大规模文本的模式匹配任务——Pandas使它变得优雅
- en: '[](https://thuwarakesh.medium.com/?source=post_page-----afe70178f9e9--------------------------------)[![Thuwarakesh
    Murallie](../Images/44f1a14a899426592bbd8c7f73ce169d.png)](https://thuwarakesh.medium.com/?source=post_page-----afe70178f9e9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----afe70178f9e9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----afe70178f9e9--------------------------------)
    [Thuwarakesh Murallie](https://thuwarakesh.medium.com/?source=post_page-----afe70178f9e9--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://thuwarakesh.medium.com/?source=post_page-----afe70178f9e9--------------------------------)[![Thuwarakesh
    Murallie](../Images/44f1a14a899426592bbd8c7f73ce169d.png)](https://thuwarakesh.medium.com/?source=post_page-----afe70178f9e9--------------------------------)[](https://towardsdatascience.com/?source=post_page-----afe70178f9e9--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----afe70178f9e9--------------------------------)
    [Thuwarakesh Murallie](https://thuwarakesh.medium.com/?source=post_page-----afe70178f9e9--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----afe70178f9e9--------------------------------)
    ·7 min read·Feb 27, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----afe70178f9e9--------------------------------)
    ·阅读时间7分钟·2023年2月27日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/624b2b35b8a6ef2a7e48b83e5d3c1041.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/624b2b35b8a6ef2a7e48b83e5d3c1041.png)'
- en: Photo by [Chris Moore](https://unsplash.com/@chrismoore_?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Chris Moore](https://unsplash.com/@chrismoore_?utm_source=medium&utm_medium=referral)拍摄，发布在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Regex is the most potent technique to clean and extract data. If you’ve ever
    worked with a large text dataset, you’d know how time-consuming and energy-draining
    it would be.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式是清理和提取数据的最强大技术。如果你曾经处理过大型文本数据集，你会知道这是一项多么耗时和耗力的工作。
- en: I often use regex to clean phone numbers and emails and standardize addresses.
    But there are complex use cases as well.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我经常使用正则表达式来清理电话号码和电子邮件，并标准化地址。但也有复杂的用例。
- en: We noticed inconsistent office columns in our recent data pipeline from a specific
    data source. We only needed the office code from this column. It’s two or three
    letters followed by a colon and a two-digit number. Earlier, we used a simple
    replace operation to map the column to our desired values. But as new data proved
    inconsistent with our assumption, we had to change the strategy. Since we can
    ensure the pattern is consistent, we used regex to clean them. This way, we never
    have to worry about changing column values.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在最近的数据管道中注意到了来自特定数据源的办公栏位不一致。我们只需要这个栏位中的办公代码。它由两个或三个字母后跟一个冒号和一个两位数字组成。早期，我们使用简单的替换操作将该栏位映射到我们期望的值。但由于新数据与我们的假设不一致，我们不得不改变策略。由于我们可以确保模式的一致性，我们使用正则表达式来清理它们。这样，我们就不必担心更改栏位值的问题。
- en: But if your dataset is significantly large, and you need the extracted values
    stored in new columns next to each row, you’d be tempted to use the map or apply
    methods in Pandas. But Pandas natively provides excellent APIs for string operations.
    Almost all of them support regex.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果你的数据集非常大，并且你需要将提取的值存储在每行旁边的新列中，你可能会倾向于使用Pandas中的map或apply方法。但Pandas原生提供了优秀的字符串操作API。几乎所有的API都支持正则表达式。
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Before we discuss the benefits of these native APIs over the map/apply method,
    here’s what I mean.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论这些原生API相较于map/apply方法的优势之前，这里是我所说的意思。
- en: Comparing Pandas string extract with the usual regex
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较Pandas字符串提取与常规正则表达式
- en: The following code generates a synthetic dataset using the Faker. It generates
    100K fake addresses and stores them in a Pandas Series. You can adjust the size
    by changing n to a more considerable value your computer can support.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码使用Faker生成一个合成数据集。它生成10万条虚假地址，并将它们存储在Pandas Series中。你可以通过将n更改为计算机支持的更大值来调整大小。
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Our goal is to extract the state and zip codes to separate columns. This task
    is easy enough to do in spreadsheet software. But let’s keep it for our discussion.
    And let’s assume we’re using regex.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是将状态和邮政编码提取到不同的列。这项任务在电子表格软件中足够简单。但让我们将其保留用于讨论。假设我们使用正则表达式。
- en: Here is my usual way of mapping or applying a regex to the series.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我通常的正则表达式应用或映射方法。
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The code above is easy to understand. We match all the two letters followed
    by a whitespace and five digits. Then we do a string split and expand it to separate
    columns. Finally, we name the columns ‘state’ and ‘zip_code.’
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码很容易理解。我们匹配所有两个字母后跟空格和五位数字。然后我们进行字符串分割，并将其展开到不同的列。最后，我们将列命名为“state”和“zip_code”。
- en: But here’s the Pandas’s way of doing it.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 但这是 Pandas 的处理方式。
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This is unarguably more elegant than the previous code. We use named groups
    in our regex pattern, which becomes the column name.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这无疑比之前的代码更优雅。我们在正则表达式模式中使用了命名组，这些组成为了列名。
- en: On a separate note, you can make part of a regex pattern a group by wrapping
    them in parentheses. You can name each group by adding `?P<group_name>` before
    you start describing your pattern.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，你可以通过将正则表达式模式的一部分用括号括起来，使其成为一个组。你可以通过在描述模式之前添加`?P<group_name>`来命名每个组。
- en: Okay, the native method is excellent in readability. But what about performance?
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，原生方法在可读性方面表现出色。但性能怎么样呢？
- en: I’ve used the `timit` utility in Jupyter notebook to record the execution times.
    I don’t see the native way having the upper hand on performance. Mapping is faster.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我在 Jupyter notebook 中使用 `timit` 工具记录了执行时间。我不认为原生方法在性能上占有优势。映射更快。
- en: But our desired output is not yet finished with a single map function. We need
    to do additional steps to get in there. The whole set of operations costs slightly
    more time than the extract method.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们期望的输出并没有通过单一的 map 函数完成。我们需要做额外的步骤才能实现。整个操作集所花费的时间略多于提取方法。
- en: '![](../Images/50cc7d2a64757523350385abfa884291.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/50cc7d2a64757523350385abfa884291.png)'
- en: Comparing the map/apply method performance against the extract method — Screenshot
    by the author.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 将映射/应用方法性能与提取方法进行比较 — 作者截图。
- en: Besides readability, both methods aren’t too different. But the difference becomes
    significant if you’re working with a vast dataset.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 除了可读性，两个方法之间没有太大区别。但如果你处理的是大规模数据集，差异就会变得显著。
- en: '[](/black-with-git-hub-actions-4ffc5c61b5fe?source=post_page-----afe70178f9e9--------------------------------)
    [## Maintain Clean Python Code With Black and GitHub Actions.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 使用 Black 和 GitHub Actions 保持 Python 代码整洁](https://towardsdatascience.com/black-with-git-hub-actions-4ffc5c61b5fe?source=post_page-----afe70178f9e9--------------------------------)'
- en: Nobody wants a messy codebase; few have the patience to clean it.
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 没有人想要一个混乱的代码库；很少有人有耐心去清理它。
- en: towardsdatascience.com](/black-with-git-hub-actions-4ffc5c61b5fe?source=post_page-----afe70178f9e9--------------------------------)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 使用 Black 和 GitHub Actions 保持 Python 代码整洁](https://towardsdatascience.com/black-with-git-hub-actions-4ffc5c61b5fe?source=post_page-----afe70178f9e9--------------------------------)'
- en: Also, if your code runs in a resource-constrained environment, you must decide
    carefully. That’s often my case, as I build data pipelines mostly. I need to ensure
    that I’m using the optimal code for faster and cheaper processing of live data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果你的代码运行在资源受限的环境中，你必须仔细决策。这通常是我的情况，因为我主要构建数据管道。我需要确保我使用的是最优代码，以便更快、更便宜地处理实时数据。
- en: We know there is more than one way to do wrangling in Pandas. If you anticipate
    re-running the scripts in the future, you may have to spend some time experimenting
    with different alternatives.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道在 Pandas 中有多种处理方式。如果你预见到将来会重新运行这些脚本，你可能需要花时间尝试不同的替代方案。
- en: '[](/a-little-pandas-hack-to-handle-large-datasets-with-limited-memory-6745140f473b?source=post_page-----afe70178f9e9--------------------------------)
    [## A Little Pandas Hack to Handle Large Datasets with Limited Memory'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 一个小巧的 Pandas 技巧处理有限内存的大数据集](https://towardsdatascience.com/a-little-pandas-hack-to-handle-large-datasets-with-limited-memory-6745140f473b?source=post_page-----afe70178f9e9--------------------------------)'
- en: The Pandas defaults aren’t optimal. A tiny configuration can compress your dataframe
    to fit in your memory.
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Pandas 的默认设置并不理想。稍微调整一下配置，可以压缩你的 dataframe 以适应内存。
- en: towardsdatascience.com](/a-little-pandas-hack-to-handle-large-datasets-with-limited-memory-6745140f473b?source=post_page-----afe70178f9e9--------------------------------)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 一个小巧的 Pandas 技巧处理有限内存的大数据集](https://towardsdatascience.com/a-little-pandas-hack-to-handle-large-datasets-with-limited-memory-6745140f473b?source=post_page-----afe70178f9e9--------------------------------)'
- en: Useful Pandas string methods with regex
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有用的 Pandas 字符串方法与正则表达式
- en: Now that we know how easy to use regex operations directly without mapping or
    applying a function, here are some methods I frequently use.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何直接使用正则表达式操作，而无需映射或应用函数，下面是一些我经常使用的方法。
- en: We’ve already seen one example of using the `extract` API in the previous section.
    It is handy with regex patterns; perhaps that’s the one I use most.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中我们已经看到过使用 `extract` API 的一个示例。它在处理正则表达式模式时非常方便，也许这是我使用最频繁的一个。
- en: Here are three other techniques I often use.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有另外三种我经常使用的技巧。
- en: '**1\. Split text to separate columns with a complex pattern.**'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**1\. 将文本分割为带有复杂模式的单独列。**'
- en: Let’s suppose a column contains state and zip codes. We need them separated
    into individual columns. Since this is sourced from a free-form input form, the
    separator is not always a whitespace or a comma.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个列包含州和邮政编码。我们需要将它们分离成单独的列。由于这些数据来自自由格式的输入表单，因此分隔符不总是空格或逗号。
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**2\. Filter records that contain a text pattern somewhere in the middle.**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 筛选包含文本模式的记录。**'
- en: I had a dataset where there was an office serial number. The number had a pattern.
    The first two letters denote the country’s shortcode. A location code follows
    the country code. It’s a three-digit number. Then a hyphen and a department id,
    which is another three-digit number.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我曾经有一个数据集，其中有一个办公室序列号。这个号码有一个模式。前两个字母表示国家的简码。紧接着是位置代码，包含三位数字。然后是一个连字符和一个部门编号，也是一组三位数字。
- en: Suppose we need to filter all the records related to the finance department
    of countries, UK, India, and Australia. We can do something like this.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们需要筛选与国家的财务部门相关的记录，包括英国、印度和澳大利亚。我们可以这样做。
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This could be a tough task if it's not for regex. And it is not a readable one,
    either.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有正则表达式，这可能是个棘手的任务。而且它也不是一个容易阅读的任务。
- en: '**3\. Replacing patterns with a new string**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. 用新字符串替换模式**'
- en: Replace is a frequent string operation. Even in Excel, we do it a lot. But some
    replacement operations are more complex than a straightforward find and replace.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 替换是一个常见的字符串操作。即使在 Excel 中，我们也经常这样做。但有些替换操作比简单的查找和替换更复杂。
- en: We need to find patterns and replace them with new strings.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要找到模式并将其替换为新的字符串。
- en: Take a phone number column, for example. You need to remove the country codes
    from the column. Some records have a country code, and some don't. Even the one
    with a country code has different formats.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 以电话号码列为例。你需要从列中删除国家代码。有些记录有国家代码，有些没有。即使有国家代码的记录也有不同的格式。
- en: Here’s a simple regex example of doing this.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个简单的正则表达式示例。
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Conclusion
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: You’d be fine with your spreadsheet software for basic string manipulations.
    But for more advanced use cases, a programming language could save hours of your
    time.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基本的字符串操作，你的电子表格软件已经足够了。但对于更复杂的用例，编程语言可以节省你大量的时间。
- en: Some operations are complex even to handle even with the basic APIs of a programming
    language. Especially the one where patterns are involved. This is where we get
    a regex for rescue.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一些操作即使使用编程语言的基本 API 也很复杂，特别是涉及模式的操作。这时候我们需要正则表达式的帮助。
- en: If you’re a Pandas user, you can directly use regex in its native APIs. This
    has the advantage of a clean codebase with fewer lines. That’s the focus of this
    post.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是 Pandas 用户，可以直接在其原生 API 中使用正则表达式。这具有代码简洁、行数更少的优势。这也是本文的重点。
- en: I’ve discussed some of my favorite regex tricks in Pandas. Although there aren’t
    any significant performance improvements, I still prefer these methods because
    they are straightforward.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经讨论了一些我喜欢的 Pandas 中的正则表达式技巧。虽然没有显著的性能提升，但我仍然偏爱这些方法，因为它们简单明了。
- en: Thanks for reading, friend! Say Hi to me on [**LinkedIn**](https://www.linkedin.com/in/thuwarakesh/),
    [**Twitter**](https://twitter.com/Thuwarakesh), and [**Medium**](https://thuwarakesh.medium.com/).
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 感谢你的阅读，朋友！在 [**LinkedIn**](https://www.linkedin.com/in/thuwarakesh/)、[**Twitter**](https://twitter.com/Thuwarakesh)
    和 [**Medium**](https://thuwarakesh.medium.com/) 上向我打个招呼吧！
- en: ''
  id: totrans-63
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Not a Medium member yet? Please use this link to [**become a member**](https://thuwarakesh.medium.com/membership)
    because, at no extra cost for you, I earn a small commission for referring you.
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 还不是 Medium 会员？请使用这个链接来 [**成为会员**](https://thuwarakesh.medium.com/membership)，因为在不增加你额外费用的情况下，我可以通过推荐你赚取少量佣金。
