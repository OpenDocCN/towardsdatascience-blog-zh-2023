- en: 'Language Models and Friends: Gorilla, HuggingGPT, TaskMatrix, and More'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语言模型及其相关：Gorilla、HuggingGPT、TaskMatrix 以及更多
- en: 原文：[https://towardsdatascience.com/language-models-and-friends-gorilla-hugginggpt-taskmatrix-and-more-b88c1200afd3?source=collection_archive---------9-----------------------#2023-09-04](https://towardsdatascience.com/language-models-and-friends-gorilla-hugginggpt-taskmatrix-and-more-b88c1200afd3?source=collection_archive---------9-----------------------#2023-09-04)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/language-models-and-friends-gorilla-hugginggpt-taskmatrix-and-more-b88c1200afd3?source=collection_archive---------9-----------------------#2023-09-04](https://towardsdatascience.com/language-models-and-friends-gorilla-hugginggpt-taskmatrix-and-more-b88c1200afd3?source=collection_archive---------9-----------------------#2023-09-04)
- en: What happens when we give LLMs access to thousands of deep learning models?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 当我们让大型语言模型（LLMs）访问成千上万的深度学习模型时会发生什么？
- en: '[](https://wolfecameron.medium.com/?source=post_page-----b88c1200afd3--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----b88c1200afd3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b88c1200afd3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b88c1200afd3--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----b88c1200afd3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wolfecameron.medium.com/?source=post_page-----b88c1200afd3--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----b88c1200afd3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b88c1200afd3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b88c1200afd3--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----b88c1200afd3--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F28aa6026c553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-and-friends-gorilla-hugginggpt-taskmatrix-and-more-b88c1200afd3&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=post_page-28aa6026c553----b88c1200afd3---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b88c1200afd3--------------------------------)
    ·18 min read·Sep 4, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb88c1200afd3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-and-friends-gorilla-hugginggpt-taskmatrix-and-more-b88c1200afd3&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=-----b88c1200afd3---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F28aa6026c553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-and-friends-gorilla-hugginggpt-taskmatrix-and-more-b88c1200afd3&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=post_page-28aa6026c553----b88c1200afd3---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b88c1200afd3--------------------------------)
    ·18分钟阅读·2023年9月4日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb88c1200afd3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-and-friends-gorilla-hugginggpt-taskmatrix-and-more-b88c1200afd3&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=-----b88c1200afd3---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb88c1200afd3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-and-friends-gorilla-hugginggpt-taskmatrix-and-more-b88c1200afd3&source=-----b88c1200afd3---------------------bookmark_footer-----------)![](../Images/6d46238c8cde54e36f1961a4a4509706.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb88c1200afd3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flanguage-models-and-friends-gorilla-hugginggpt-taskmatrix-and-more-b88c1200afd3&source=-----b88c1200afd3---------------------bookmark_footer-----------)![](../Images/6d46238c8cde54e36f1961a4a4509706.png)'
- en: (Photo by [Mike Arney](https://unsplash.com/@mikearney?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/rJ5vHo8gr2U?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: （照片由 [Mike Arney](https://unsplash.com/@mikearney?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，来源于 [Unsplash](https://unsplash.com/photos/rJ5vHo8gr2U?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)）
- en: Recently, we have witnessed a rise of foundation models to popularity within
    deep learning research. Pre-trained large language models (LLMs) have led to a
    new paradigm, in which a single model can be used — with surprising success —
    to solve many different problems. Despite the popularity of generic LLMs, however,
    fine-tuning models in a task-specific manner tends to outperform approaches that
    leverage foundation models. Put simply, *specialized models are still very hard
    to beat*! With this being said, we might start to wonder whether the powers of
    foundation models and specialized deep learning models can be combined. Within
    this overview, we will study recent research that integrates LLMs with other,
    specialized deep learning models by learning to call their associated APIs. The
    resulting framework uses the language model as a centralized controller that forms
    a plan for solving a complex, AI-related tasks and delegates specialized portions
    of the solution process to more appropriate models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，我们见证了基础模型在深度学习研究中的流行。预训练的大型语言模型（LLMs）引领了一种新范式，在这种范式中，单一模型可以用来——以令人惊讶的成功——解决许多不同的问题。尽管通用
    LLMs 很受欢迎，但以任务特定方式微调的模型往往优于利用基础模型的方法。简单来说，*专业化模型仍然很难被超越*！话虽如此，我们可能会开始思考基础模型和专业化深度学习模型的力量是否可以结合。在这次概述中，我们将研究近期将
    LLMs 与其他专业化深度学习模型结合的研究，通过学习调用它们相关的 API。所得到的框架将语言模型作为一个集中控制器，用于制定解决复杂 AI 相关任务的计划，并将解决过程中的专业部分委派给更合适的模型。
- en: “By providing only the model descriptions, HuggingGPT can continuously and conveniently
    integrate diverse expert models from AI communities, without altering any structure
    or prompt…
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “通过仅提供模型描述，HuggingGPT 可以持续且便捷地集成来自 AI 社区的各种专家模型，而无需更改任何结构或提示…
