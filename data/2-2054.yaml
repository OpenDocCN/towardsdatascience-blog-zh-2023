- en: 'The Power of Retrieval Augmented Generation: A Comparison between Base and
    RAG LLMs with Llama2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**æ£€ç´¢å¢å¼ºç”Ÿæˆçš„åŠ›é‡ï¼šBase LLM ä¸ RAG LLMs çš„æ¯”è¾ƒï¼ŒåŸºäº Llama2**'
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/the-power-of-retrieval-augmented-generation-a-comparison-between-base-and-rag-llms-with-llama2-368865762c0d](https://towardsdatascience.com/the-power-of-retrieval-augmented-generation-a-comparison-between-base-and-rag-llms-with-llama2-368865762c0d)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/the-power-of-retrieval-augmented-generation-a-comparison-between-base-and-rag-llms-with-llama2-368865762c0d](https://towardsdatascience.com/the-power-of-retrieval-augmented-generation-a-comparison-between-base-and-rag-llms-with-llama2-368865762c0d)
- en: A deep dive into tailoring pre-trained LLMs for custom use cases using a RAG
    approach, featuring LangChain and Hugging Face integration
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ·±å…¥æ¢è®¨ä½¿ç”¨ RAG æ–¹æ³•å®šåˆ¶é¢„è®­ç»ƒ LLM ä»¥é€‚åº”ç‰¹å®šä½¿ç”¨æ¡ˆä¾‹ï¼Œæ¶‰åŠ LangChain å’Œ Hugging Face é›†æˆ
- en: '[](https://medium.com/@luisroque?source=post_page-----368865762c0d--------------------------------)[![LuÃ­s
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----368865762c0d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----368865762c0d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----368865762c0d--------------------------------)
    [LuÃ­s Roque](https://medium.com/@luisroque?source=post_page-----368865762c0d--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@luisroque?source=post_page-----368865762c0d--------------------------------)[![LuÃ­s
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----368865762c0d--------------------------------)[](https://towardsdatascience.com/?source=post_page-----368865762c0d--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----368865762c0d--------------------------------)
    [LuÃ­s Roque](https://medium.com/@luisroque?source=post_page-----368865762c0d--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----368865762c0d--------------------------------)
    Â·12 min readÂ·Nov 29, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----368865762c0d--------------------------------)
    Â·é˜…è¯»æ—¶é—´ 12 åˆ†é’ŸÂ·2023å¹´11æœˆ29æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '*This post was co-authored with Rafael Guedes.*'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*æœ¬æ–‡ç”± Rafael Guedes å…±åŒæ’°å†™ã€‚*'
- en: Introduction
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»‹ç»
- en: Since the release of ChatGPT in November of 2022, Large Language Models (LLMs)
    have been the hot topic in the AI community for their capabilities in understanding
    and generating human-like text, pushing the boundaries of what was previously
    possible in natural language processing (NLP).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ª 2022 å¹´ 11 æœˆ ChatGPT å‘å¸ƒä»¥æ¥ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å› å…¶ç†è§£å’Œç”Ÿæˆç±»ä¼¼äººç±»æ–‡æœ¬çš„èƒ½åŠ›ï¼Œæˆä¸º AI ç¤¾åŒºçš„çƒ­é—¨è¯é¢˜ï¼Œæ¨åŠ¨äº†è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸçš„è¾¹ç•Œã€‚
- en: LLMs have been shown to be versatile by tackling different use cases in various
    industries since they are not limited to a specific task. They can be adapted
    to several domains, making them attractive for organizations and the research
    community. Several applications have been explored using LLMs such as content
    generation, chatbots, code generation, creative writing, virtual assistants, and
    many more.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs å·²è¢«è¯æ˜å…·æœ‰å¤šæ ·æ€§ï¼Œé€šè¿‡å¤„ç†ä¸åŒè¡Œä¸šçš„ä¸åŒä½¿ç”¨æ¡ˆä¾‹ï¼Œå› ä¸ºå®ƒä»¬ä¸å±€é™äºç‰¹å®šä»»åŠ¡ã€‚å®ƒä»¬å¯ä»¥é€‚åº”å¤šä¸ªé¢†åŸŸï¼Œè¿™ä½¿å¾—å®ƒä»¬å¯¹ç»„ç»‡å’Œç ”ç©¶ç¤¾åŒºå…·æœ‰å¸å¼•åŠ›ã€‚å·²ç»æ¢ç´¢äº†è®¸å¤šä½¿ç”¨
    LLMs çš„åº”ç”¨ç¨‹åºï¼Œä¾‹å¦‚å†…å®¹ç”Ÿæˆã€èŠå¤©æœºå™¨äººã€ä»£ç ç”Ÿæˆã€åˆ›æ„å†™ä½œã€è™šæ‹ŸåŠ©æ‰‹ç­‰ã€‚
- en: Another characteristic that makes LLMs so attractive is the fact that there
    are open-source options. Companies like Meta made their pre-trained LLM (Llama2
    ğŸ¦™) available in repositories like Hugging Face ğŸ¤—. Are these pre-trained LLMs good
    enough for each companyâ€™s specific use case? Certainly not.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ LLMs éå¸¸å¸å¼•äººçš„å¦ä¸€ä¸ªç‰¹å¾æ˜¯æœ‰å¼€æºé€‰é¡¹ã€‚åƒ Meta è¿™æ ·çš„å…¬å¸å°†å…¶é¢„è®­ç»ƒçš„ LLMï¼ˆLlama2 ğŸ¦™ï¼‰åœ¨ Hugging Face ğŸ¤— ç­‰å­˜å‚¨åº“ä¸­æä¾›ã€‚è¿™äº›é¢„è®­ç»ƒçš„
    LLM å¯¹äºæ¯ä¸ªå…¬å¸çš„ç‰¹å®šä½¿ç”¨æ¡ˆä¾‹è¶³å¤Ÿå¥½å—ï¼Ÿæ˜¾ç„¶ä¸å¤Ÿã€‚
- en: 'Organizations could train an LLM from scratch with their own data. But the
    vast majority of them (almost all of them) wouldnâ€™t have either the data or the
    computing capacity required for the task. It requires datasets with trillions
    of tokens, thousands of GPUs, and several months. Another option is to use a pre-trained
    LLM and tailor it for a specific use case. There are two main approaches to follow:
    fine-tuning and **RAGs (Retrieval Augmented Generation)**.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ç»„ç»‡å¯ä»¥ç”¨è‡ªå·±çš„æ•°æ®ä»å¤´å¼€å§‹è®­ç»ƒ LLMã€‚ä½†ç»å¤§å¤šæ•°ç»„ç»‡ï¼ˆå‡ ä¹æ‰€æœ‰ç»„ç»‡ï¼‰æ—¢æ²¡æœ‰æ‰€éœ€çš„æ•°æ®ï¼Œä¹Ÿæ²¡æœ‰å®Œæˆä»»åŠ¡æ‰€éœ€çš„è®¡ç®—èƒ½åŠ›ã€‚è¿™éœ€è¦æ‹¥æœ‰æ•°ä¸‡äº¿ä¸ªæ ‡è®°çš„æ•°æ®é›†ï¼Œæˆåƒä¸Šä¸‡çš„
    GPUï¼Œä»¥åŠå‡ ä¸ªæœˆçš„æ—¶é—´ã€‚å¦ä¸€ä¸ªé€‰æ‹©æ˜¯ä½¿ç”¨é¢„è®­ç»ƒçš„ LLMï¼Œå¹¶å°†å…¶è°ƒæ•´ä¸ºç‰¹å®šçš„ä½¿ç”¨æ¡ˆä¾‹ã€‚æœ‰ä¸¤ç§ä¸»è¦çš„æ–¹æ³•ï¼šå¾®è°ƒå’Œ**RAGsï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰**ã€‚
- en: In this article, we will compare the performance of an isolated pre-trained
    Llama2 with a pre-trained LLama2 integrated in a RAG system to answer questions
    about the latest news regarding OpenAI. We will start by explaining how RAGs work
    and the architecture of their sub-modules (the retriever and the generator). We
    finish with a step-by-step implementation of how we can build a RAG system for
    any use case using LangChain **ğŸ¦œï¸** and Hugging Face.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æ¯”è¾ƒå•ç‹¬é¢„è®­ç»ƒçš„Llama2ä¸é›†æˆåœ¨RAGç³»ç»Ÿä¸­çš„é¢„è®­ç»ƒLLama2åœ¨å›ç­”å…³äºOpenAIæœ€æ–°æ–°é—»çš„é—®é¢˜ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬å°†ä»è§£é‡ŠRAGçš„å·¥ä½œåŸç†åŠå…¶å­æ¨¡å—ï¼ˆæ£€ç´¢å™¨å’Œç”Ÿæˆå™¨ï¼‰çš„æ¶æ„å¼€å§‹ã€‚æœ€åï¼Œæˆ‘ä»¬å°†é€æ­¥å®ç°å¦‚ä½•ä½¿ç”¨LangChain
    **ğŸ¦œï¸** å’ŒHugging Faceæ„å»ºä¸€ä¸ªé€‚ç”¨äºä»»ä½•ç”¨ä¾‹çš„RAGç³»ç»Ÿã€‚
- en: '![](../Images/684645ae6f864ac647968b569236e55b.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/684645ae6f864ac647968b569236e55b.png)'
- en: 'Figure 1: Llamas are getting more powerful with the RAG approach (image by
    author)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾1ï¼šé€šè¿‡RAGæ–¹æ³•ï¼ŒLlamaså˜å¾—è¶Šæ¥è¶Šå¼ºå¤§ï¼ˆå›¾åƒæ¥æºï¼šä½œè€…ï¼‰
- en: As always, the code is available on our [Github](https://github.com/zaai-ai/large-language-models).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€å¦‚æ—¢å¾€ï¼Œä»£ç å¯ä»¥åœ¨æˆ‘ä»¬çš„[Github](https://github.com/zaai-ai/large-language-models)ä¸Šæ‰¾åˆ°ã€‚
- en: What is Retrieval Augmented Generation (RAG)?
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰ï¼Ÿ
- en: Retrieval Augmented Generation (RAG) is a technique that combines a retriever
    (a non-parametric memory like vector databases or feature store) and a generator
    (a parametric memory like a pre-trained *seq2seq* transformer). They are used
    to improve the prediction quality of an LLM [1]. It uses the retriever during
    inference time to build a richer prompt by adding context/knowledge based on the
    most relevant documents for the user query.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æ˜¯ä¸€ç§ç»“åˆäº†æ£€ç´¢å™¨ï¼ˆå¦‚å‘é‡æ•°æ®åº“æˆ–ç‰¹å¾å­˜å‚¨çš„éå‚æ•°è®°å¿†ï¼‰å’Œç”Ÿæˆå™¨ï¼ˆå¦‚é¢„è®­ç»ƒçš„*seq2seq*å˜æ¢å™¨çš„å‚æ•°è®°å¿†ï¼‰æŠ€æœ¯çš„æ–¹æ³•ã€‚å®ƒä»¬ç”¨äºæé«˜LLM
    [1] çš„é¢„æµ‹è´¨é‡ã€‚å®ƒåœ¨æ¨ç†æ—¶ä½¿ç”¨æ£€ç´¢å™¨ï¼Œé€šè¿‡æ·»åŠ åŸºäºæœ€ç›¸å…³æ–‡æ¡£çš„ä¸Šä¸‹æ–‡/çŸ¥è¯†æ¥æ„å»ºæ›´ä¸°å¯Œçš„æç¤ºï¼Œä»¥å“åº”ç”¨æˆ·æŸ¥è¯¢ã€‚
- en: 'The advantages of this kind of architecture over the traditional LLM are:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ¶æ„ç›¸å¯¹äºä¼ ç»ŸLLMçš„ä¼˜åŠ¿æ˜¯ï¼š
- en: We can easily update its knowledge by replacing or adding more documents/information
    to the non-parametric memory. Thus, it does not require retraining the model.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€šè¿‡æ›¿æ¢æˆ–æ·»åŠ æ›´å¤šæ–‡æ¡£/ä¿¡æ¯åˆ°éå‚æ•°è®°å¿†ä¸­ï¼Œè½»æ¾æ›´æ–°å…¶çŸ¥è¯†ã€‚å› æ­¤ï¼Œå®ƒä¸éœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚
- en: It provides explainability over predictions because it allows the user to check
    which documents were retrieved to provide context, something we cannot get from
    traditional LLMs.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®ƒæä¾›äº†å¯¹é¢„æµ‹çš„å¯è§£é‡Šæ€§ï¼Œå› ä¸ºå®ƒå…è®¸ç”¨æˆ·æ£€æŸ¥å“ªäº›æ–‡æ¡£è¢«æ£€ç´¢ä»¥æä¾›ä¸Šä¸‹æ–‡ï¼Œè€Œè¿™æ˜¯æˆ‘ä»¬ä»ä¼ ç»ŸLLMä¸­æ— æ³•è·å¾—çš„ã€‚
- en: It reduces the well-known problem of *â€˜hallucinationsâ€™* by providing more accurate
    and up-to-date information through the documents provided by the retriever.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®ƒé€šè¿‡æä¾›æ›´å‡†ç¡®å’Œæœ€æ–°çš„ä¿¡æ¯ï¼Œå‡å°‘äº†*â€œå¹»è§‰â€*çš„è‘—åé—®é¢˜ï¼Œè¿™äº›ä¿¡æ¯é€šè¿‡æ£€ç´¢å™¨æä¾›çš„æ–‡æ¡£è·å–ã€‚
- en: '![](../Images/547b6b71f7ad85370f0aaab3c1299483.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/547b6b71f7ad85370f0aaab3c1299483.png)'
- en: 'Figure 2: Schematic view of how Retrieval Augmented Generation (RAG) can be
    set up (image by author)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2ï¼šæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰è®¾ç½®çš„ç¤ºæ„å›¾ï¼ˆå›¾åƒæ¥æºï¼šä½œè€…ï¼‰
- en: Retriever â€” what is it and how it works?
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ£€ç´¢å™¨â€”â€”å®ƒæ˜¯ä»€ä¹ˆä»¥åŠå¦‚ä½•å·¥ä½œï¼Ÿ
- en: Retrievers were developed to solve the problem of Question-Answering (QA), where
    we expect a system to answer questions like *â€œWhat is Retrieval Augmented Generation?â€.*
    It does it by accessing a database of documents that contain information about
    the topic.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ£€ç´¢å™¨çš„å¼€å‘æ—¨åœ¨è§£å†³é—®ç­”ï¼ˆQAï¼‰é—®é¢˜ï¼Œæˆ‘ä»¬æœŸæœ›ç³»ç»Ÿèƒ½å¤Ÿå›ç­”ç±»ä¼¼*â€œä»€ä¹ˆæ˜¯æ£€ç´¢å¢å¼ºç”Ÿæˆï¼Ÿâ€*çš„é—®é¢˜ã€‚å®ƒé€šè¿‡è®¿é—®åŒ…å«æœ‰å…³ä¸»é¢˜ä¿¡æ¯çš„æ–‡æ¡£æ•°æ®åº“æ¥å®ç°ã€‚
- en: The database is populated by splitting our documents into *passages* of equal
    lengths, where each passage is represented as a sequence of tokens. Given a question,
    the system needs to span the database to find *â€˜the passageâ€™* that can better
    answer the question.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®åº“é€šè¿‡å°†æˆ‘ä»¬çš„æ–‡æ¡£æ‹†åˆ†æˆç­‰é•¿çš„*æ®µè½*æ¥å¡«å……ï¼Œæ¯ä¸ªæ®µè½è¢«è¡¨ç¤ºä¸ºä¸€ç³»åˆ—æ ‡è®°ã€‚ç»™å®šä¸€ä¸ªé—®é¢˜ï¼Œç³»ç»Ÿéœ€è¦éå†æ•°æ®åº“ï¼Œä»¥æ‰¾åˆ°å¯ä»¥æ›´å¥½å›ç­”é—®é¢˜çš„*â€œæ®µè½â€*ã€‚
- en: For these systems to work in several domains, their databases need to be populated
    with millions or billions of documents. Therefore, to be able to span the database
    searching for the right *passage,* the **retriever** needs to be very efficient
    in selecting a set of candidate *passages.*
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä½¿è¿™äº›ç³»ç»Ÿåœ¨å¤šä¸ªé¢†åŸŸä¸­æœ‰æ•ˆå·¥ä½œï¼Œå®ƒä»¬çš„æ•°æ®åº“éœ€è¦å¡«å……æ•°ç™¾ä¸‡æˆ–æ•°åäº¿çš„æ–‡æ¡£ã€‚å› æ­¤ï¼Œä¸ºäº†èƒ½å¤Ÿéå†æ•°æ®åº“å¯»æ‰¾åˆé€‚çš„*æ®µè½*ï¼Œ**æ£€ç´¢å™¨**éœ€è¦åœ¨é€‰æ‹©ä¸€ç»„å€™é€‰*æ®µè½*æ—¶éå¸¸é«˜æ•ˆã€‚
- en: '**Dense Passage Retriever (DPR)** [2] is the retriever used by the authors
    in [1]. Its goal is to index millions of passages into a low-dimensional and continuous
    space to efficiently retrieve the top *k* passages that are the most relevant
    for a specific question.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¯†é›†æ®µè½æ£€ç´¢å™¨ (DPR)** [2] æ˜¯ä½œè€…åœ¨ [1] ä¸­ä½¿ç”¨çš„æ£€ç´¢å™¨ã€‚å®ƒçš„ç›®æ ‡æ˜¯å°†æ•°ç™¾ä¸‡ä¸ªæ®µè½ç´¢å¼•åˆ°ä¸€ä¸ªä½ç»´çš„è¿ç»­ç©ºé—´ï¼Œä»¥é«˜æ•ˆåœ°æ£€ç´¢ä¸ç‰¹å®šé—®é¢˜æœ€ç›¸å…³çš„å‰
    *k* ä¸ªæ®µè½ã€‚'
- en: DPR uses two **Dense Encoders:**
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: DPR ä½¿ç”¨ä¸¤ä¸ª **å¯†é›†ç¼–ç å™¨**ï¼š
- en: The **Passage Encoder** converts each passage into a d-dimensional vector and
    indexes them using **FAISS** [3]. FAISS is an open-source library for similarity
    search of dense vectors which can be applied to billions of vectors.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ®µè½ç¼–ç å™¨**å°†æ¯ä¸ªæ®µè½è½¬æ¢ä¸ºä¸€ä¸ª d ç»´å‘é‡ï¼Œå¹¶ä½¿ç”¨ **FAISS** [3] å¯¹å®ƒä»¬è¿›è¡Œç´¢å¼•ã€‚FAISS æ˜¯ä¸€ä¸ªç”¨äºå¯†é›†å‘é‡ç›¸ä¼¼æ€§æœç´¢çš„å¼€æºåº“ï¼Œå¯ä»¥åº”ç”¨äºæ•°åäº¿ä¸ªå‘é‡ã€‚'
- en: The **Question Encoder** converts the input question to a d-dimensional vector
    and then uses **FAISS** to retrieve the **k** passages that have the closest vector
    to the question vector. The similarity between vectors can be computed by using
    the dot product between them.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**é—®é¢˜ç¼–ç å™¨**å°†è¾“å…¥é—®é¢˜è½¬æ¢ä¸ºä¸€ä¸ª d ç»´å‘é‡ï¼Œç„¶åä½¿ç”¨ **FAISS** æ£€ç´¢ä¸é—®é¢˜å‘é‡æœ€æ¥è¿‘çš„ **k** ä¸ªæ®µè½ã€‚å‘é‡ä¹‹é—´çš„ç›¸ä¼¼æ€§å¯ä»¥é€šè¿‡å®ƒä»¬ä¹‹é—´çš„ç‚¹ç§¯æ¥è®¡ç®—ã€‚'
- en: The architecture for the encoder used by DPR is a BERT [4] network which converts
    the input into a high dimensional vector. However, we can use any architecture
    as long as it fits our use case.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DPR ä½¿ç”¨çš„ç¼–ç å™¨æ¶æ„æ˜¯ä¸€ä¸ª BERT [4] ç½‘ç»œï¼Œå®ƒå°†è¾“å…¥è½¬æ¢ä¸ºé«˜ç»´å‘é‡ã€‚ç„¶è€Œï¼Œåªè¦ç¬¦åˆæˆ‘ä»¬çš„ç”¨ä¾‹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»»ä½•æ¶æ„ã€‚
- en: '![](../Images/a52940dd39b60aefb3c1047fbbc1dd37.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a52940dd39b60aefb3c1047fbbc1dd37.png)'
- en: 'Figure 3: Overview of the RAG process with a pre-trained retriever that combines
    a query encoder, document index, and a pre-trained generator (seq2seq model) to
    predict the output in a free-text form ([source](https://arxiv.org/pdf/2005.11401.pdf)).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 3ï¼šRAG è¿‡ç¨‹çš„æ¦‚è¿°ï¼Œä½¿ç”¨ä¸€ä¸ªé¢„è®­ç»ƒçš„æ£€ç´¢å™¨ï¼Œè¯¥æ£€ç´¢å™¨ç»“åˆäº†æŸ¥è¯¢ç¼–ç å™¨ã€æ–‡æ¡£ç´¢å¼•å’Œä¸€ä¸ªé¢„è®­ç»ƒç”Ÿæˆå™¨ï¼ˆseq2seq æ¨¡å‹ï¼‰ä»¥é¢„æµ‹è‡ªç”±æ–‡æœ¬å½¢å¼çš„è¾“å‡º
    ([source](https://arxiv.org/pdf/2005.11401.pdf))ã€‚
- en: Generator â€” what is it and how does it work?
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç”Ÿæˆå™¨ â€” å®ƒæ˜¯ä»€ä¹ˆï¼Œå¦‚ä½•å·¥ä½œï¼Ÿ
- en: The generator is an LLM responsible for generating text given a certain input,
    well known as a prompt.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆå™¨æ˜¯ä¸€ä¸ª LLMï¼Œè´Ÿè´£æ ¹æ®ç‰¹å®šè¾“å…¥ç”Ÿæˆæ–‡æœ¬ï¼Œé€šå¸¸è¢«ç§°ä¸ºæç¤ºã€‚
- en: 'LLMs are transformer models which are composed mainly of 2 types of layers
    [5]:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: LLM æ˜¯ä¸»è¦ç”±ä¸¤ç§å±‚ç»„æˆçš„å˜æ¢å™¨æ¨¡å‹ [5]ï¼š
- en: A **fully connected feed-forward network (FFN)** maps one embedding vector to
    a new embedding vector through linear and non-linear transformations.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**å…¨è¿æ¥å‰é¦ˆç½‘ç»œ (FFN)** é€šè¿‡çº¿æ€§å’Œéçº¿æ€§å˜æ¢å°†ä¸€ä¸ªåµŒå…¥å‘é‡æ˜ å°„åˆ°ä¸€ä¸ªæ–°çš„åµŒå…¥å‘é‡ã€‚'
- en: The **attention layer** aims to select which parts of the input embedding are
    more useful for the task at hand, producing a new embedding vector.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ³¨æ„åŠ›å±‚**æ—¨åœ¨é€‰æ‹©å“ªäº›è¾“å…¥åµŒå…¥éƒ¨åˆ†å¯¹å½“å‰ä»»åŠ¡æ›´æœ‰ç”¨ï¼Œäº§ç”Ÿä¸€ä¸ªæ–°çš„åµŒå…¥å‘é‡ã€‚'
- en: 'BART [6] was the LLM chosen by the authors in [1] for the Generator, which
    is a sequence-to-sequence model with the following architecture [7]:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: BART [6] æ˜¯ä½œè€…åœ¨ [1] ä¸­ä¸ºç”Ÿæˆå™¨é€‰æ‹©çš„ LLMï¼Œå®ƒæ˜¯ä¸€ä¸ªåºåˆ—åˆ°åºåˆ—æ¨¡å‹ï¼Œå…·æœ‰ä»¥ä¸‹æ¶æ„ [7]ï¼š
- en: The **encoder** receives the input embedding and produces a 512-dimensional
    vector as output for the decoder through its six layers with two sub-layers (multi-head
    self-attention mechanism and FFN).
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç¼–ç å™¨**æ¥æ”¶è¾“å…¥åµŒå…¥ï¼Œå¹¶é€šè¿‡å…¶å…­å±‚ï¼ˆåŒ…æ‹¬ä¸¤ä¸ªå­å±‚ï¼šå¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œ FFNï¼‰ç”Ÿæˆä¸€ä¸ª 512 ç»´çš„å‘é‡ä½œä¸ºè§£ç å™¨çš„è¾“å‡ºã€‚'
- en: The **decoder** follows the same logic as the encoder with six layers and two
    sub-layers for the previously generated outputs. It has an additional 3rd sub-layer,
    which performs multi-head attention over the output of the encoder.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è§£ç å™¨**éµå¾ªä¸ç¼–ç å™¨ç›¸åŒçš„é€»è¾‘ï¼Œå…·æœ‰å…­å±‚å’Œä¸¤ä¸ªå­å±‚ï¼Œç”¨äºä¹‹å‰ç”Ÿæˆçš„è¾“å‡ºã€‚å®ƒè¿˜æœ‰ä¸€ä¸ªé¢å¤–çš„ç¬¬ä¸‰ä¸ªå­å±‚ï¼Œæ‰§è¡Œå¯¹ç¼–ç å™¨è¾“å‡ºçš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ã€‚'
- en: The decoder output is then passed to a linear layer followed by a softmax layer
    that will predict the likelihood of the next word.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è§£ç å™¨è¾“å‡ºæ¥ç€ä¼ é€’åˆ°ä¸€ä¸ªçº¿æ€§å±‚ï¼Œç„¶åæ˜¯ä¸€ä¸ª softmax å±‚ï¼Œè¯¥å±‚å°†é¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„å¯èƒ½æ€§ã€‚
- en: As mentioned in the previous section, BART is not required to be used as the
    generator. With the advancements in this field, mainly since November of 2022
    with the release of chatGPT, we can use any architecture that fits our needs.
    For example, one can use open-source approaches like [Llama](https://medium.com/towards-data-science/leveraging-llama-2-features-in-real-world-applications-building-scalable-chatbots-with-fastapi-406f1cbeb935)2
    or [Falcon](https://medium.com/towards-data-science/harnessing-the-falcon-40b-model-the-most-powerful-open-source-llm-f70010bc8a10).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰ä¸€èŠ‚æ‰€è¿°ï¼ŒBARTä¸éœ€è¦ä½œä¸ºç”Ÿæˆå™¨ä½¿ç”¨ã€‚éšç€è¯¥é¢†åŸŸçš„å‘å±•ï¼Œç‰¹åˆ«æ˜¯è‡ª2022å¹´11æœˆchatGPTå‘å¸ƒä»¥æ¥ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»»ä½•ç¬¦åˆæˆ‘ä»¬éœ€æ±‚çš„æ¶æ„ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨å¼€æºæ–¹æ³•å¦‚[Llama](https://medium.com/towards-data-science/leveraging-llama-2-features-in-real-world-applications-building-scalable-chatbots-with-fastapi-406f1cbeb935)2æˆ–[Falcon](https://medium.com/towards-data-science/harnessing-the-falcon-40b-model-the-most-powerful-open-source-llm-f70010bc8a10)ã€‚
- en: '![](../Images/f3730669abcb2a77ffae2369dfc0fcba.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f3730669abcb2a77ffae2369dfc0fcba.png)'
- en: 'Figure 4: The general architecture of a transformer. It only differs from BARTâ€™s
    architecture on the activation functions, which are GeLUs rather than ReLUs ([source](https://arxiv.org/pdf/1706.03762.pdf)).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾4ï¼šå˜å‹å™¨çš„ä¸€èˆ¬æ¶æ„ã€‚å®ƒåªåœ¨æ¿€æ´»å‡½æ•°ä¸Šä¸BARTçš„æ¶æ„ä¸åŒï¼Œæ¿€æ´»å‡½æ•°æ˜¯GeLUsè€Œä¸æ˜¯ReLUs ([source](https://arxiv.org/pdf/1706.03762.pdf))ã€‚
- en: How to implement a RAG using LangChain ğŸ¦œï¸ and HuggingFace ğŸ¤—?
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦‚ä½•ä½¿ç”¨LangChain ğŸ¦œï¸å’ŒHuggingFace ğŸ¤—å®ç°RAGï¼Ÿ
- en: This section describes how you can create your RAG using LangChain. LangChain
    is a framework to easily develop applications powered by LLMs, while HuggingFace
    is a platform that provides open-source LLMs and datasets for research and commercial
    usage.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚æè¿°äº†å¦‚ä½•ä½¿ç”¨LangChainåˆ›å»ºæ‚¨çš„RAGã€‚LangChainæ˜¯ä¸€ä¸ªæ¡†æ¶ï¼Œç”¨äºè½»æ¾å¼€å‘ç”±LLMsé©±åŠ¨çš„åº”ç”¨ç¨‹åºï¼Œè€ŒHuggingFaceæ˜¯ä¸€ä¸ªæä¾›å¼€æºLLMså’Œæ•°æ®é›†ç”¨äºç ”ç©¶å’Œå•†ä¸šç”¨é€”çš„å¹³å°ã€‚
- en: In our case, and as stated in the introduction, we created an RAG where the
    generator is a Llama2 model to compare the quality of its output with a base Llama2\.
    We will make Llama2 answer the question *â€œWhat happened to the CEO of OpenAI?â€.*
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæ­£å¦‚ä»‹ç»ä¸­æ‰€è¿°ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªRAGï¼Œå…¶ä¸­ç”Ÿæˆå™¨æ˜¯ä¸€ä¸ªLlama2æ¨¡å‹ï¼Œä»¥ä¾¿å°†å…¶è¾“å‡ºçš„è´¨é‡ä¸åŸºç¡€Llama2è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬å°†ä½¿Llama2å›ç­”é—®é¢˜*â€œOpenAIçš„CEOå‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿâ€*ã€‚
- en: The process starts with loading a dataset of news from HuggingFace ([cnn_dailymail](https://huggingface.co/datasets/cnn_dailymail)
    â€” apache 2.0 license) and supplementing it with more recent news about OpenAI,
    based on Luisâ€™s latest posts on X/Twitter regarding the subject, including the
    resignation of its CEO. We then preprocess it by creating a list of ***documents***
    (the expected format from LangChain) to fill our vector database.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥è¿‡ç¨‹ä»åŠ è½½HuggingFaceçš„æ•°æ®é›†æ–°é—»ï¼ˆ[cnn_dailymail](https://huggingface.co/datasets/cnn_dailymail)
    â€” apache 2.0è®¸å¯è¯ï¼‰å¼€å§‹ï¼Œå¹¶é€šè¿‡Luisæœ€è¿‘åœ¨X/Twitterä¸Šå…³äºè¯¥ä¸»é¢˜çš„å¸–å­ï¼ŒåŒ…æ‹¬CEOè¾èŒï¼Œè¡¥å……äº†æœ‰å…³OpenAIçš„æœ€æ–°æ–°é—»ã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡åˆ›å»ºä¸€ä¸ª***æ–‡æ¡£***åˆ—è¡¨ï¼ˆLangChainæœŸæœ›çš„æ ¼å¼ï¼‰æ¥é¢„å¤„ç†å®ƒï¼Œä»¥å¡«å……æˆ‘ä»¬çš„å‘é‡æ•°æ®åº“ã€‚
- en: '[PRE0]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next, we are ready to create the 2 modules for our RAG.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å‡†å¤‡ä¸ºæˆ‘ä»¬çš„RAGåˆ›å»ºè¿™ä¸¤ä¸ªæ¨¡å—ã€‚
- en: Retriever
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ£€ç´¢å™¨
- en: 'In the retriever, we have two sub-modules: the encoder and the retriever.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ£€ç´¢å™¨ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸¤ä¸ªå­æ¨¡å—ï¼šç¼–ç å™¨å’Œæ£€ç´¢å™¨ã€‚
- en: The **encoder** converts the passages into a d-dimensional embedding vector.
    For that, we import `HuggingFaceEmbeddings` from `langchain.embeddings` and we
    select the model we want to use to create the embeddings.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**ç¼–ç å™¨**å°†æ®µè½è½¬æ¢ä¸ºdç»´çš„åµŒå…¥å‘é‡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä»`langchain.embeddings`å¯¼å…¥`HuggingFaceEmbeddings`ï¼Œå¹¶é€‰æ‹©æˆ‘ä»¬æƒ³è¦ä½¿ç”¨çš„æ¨¡å‹æ¥åˆ›å»ºåµŒå…¥ã€‚'
- en: In our case, we have chosen `sentence-transformers/all-MiniLM-l6-v2` because
    it creates 384-dimensional vectors with good quality to calculate the similarity
    between them. It is memory efficient and fast. You can check more details about
    this and other models [here](https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models/).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬é€‰æ‹©äº†`sentence-transformers/all-MiniLM-l6-v2`ï¼Œå› ä¸ºå®ƒåˆ›å»ºäº†384ç»´çš„å‘é‡ï¼Œå…·æœ‰è‰¯å¥½çš„è´¨é‡ç”¨äºè®¡ç®—å®ƒä»¬ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚å®ƒåœ¨å†…å­˜ä½¿ç”¨ä¸Šé«˜æ•ˆä¸”å¿«é€Ÿã€‚æ‚¨å¯ä»¥åœ¨[è¿™é‡Œ](https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models/)æŸ¥çœ‹æœ‰å…³æ­¤æ¨¡å‹åŠå…¶ä»–æ¨¡å‹çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The **retriever** splits the documents into passages of a certain length using
    `CharacterTextSplitter` from `langchain.text_splitter` .
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ£€ç´¢å™¨**ä½¿ç”¨`langchain.text_splitter`ä¸­çš„`CharacterTextSplitter`å°†æ–‡æ¡£æ‹†åˆ†ä¸ºä¸€å®šé•¿åº¦çš„æ®µè½ã€‚'
- en: In our case, we chose a length of 1000\. We started with 100, as stated in the
    paper [1], but through some preliminary experiments, we found out that 1000 gives
    better results in our use case.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬é€‰æ‹©äº†1000çš„é•¿åº¦ã€‚æˆ‘ä»¬å¼€å§‹æ—¶é€‰æ‹©äº†100ï¼Œå¦‚æ–‡çŒ®[1]ä¸­æ‰€è¿°ï¼Œä½†é€šè¿‡ä¸€äº›åˆæ­¥å®éªŒï¼Œæˆ‘ä»¬å‘ç°1000åœ¨æˆ‘ä»¬çš„ä½¿ç”¨æ¡ˆä¾‹ä¸­èƒ½å–å¾—æ›´å¥½çš„ç»“æœã€‚
- en: We then use the **encoder** to convert the passages into embeddings. Finally,
    we can store them in a vector store such as `FAISS` from `langchain.vectorstores`
    . From those, we can later retrieve the top *k* documents more similar to the
    question.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬ä½¿ç”¨**ç¼–ç å™¨**å°†æ®µè½è½¬æ¢ä¸ºåµŒå…¥ã€‚æœ€åï¼Œæˆ‘ä»¬å¯ä»¥å°†å®ƒä»¬å­˜å‚¨åœ¨å¦‚`FAISS`çš„å‘é‡å­˜å‚¨ä¸­ã€‚ä»è¿™äº›å­˜å‚¨ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ç¨åæ£€ç´¢å‡ºä¸é—®é¢˜æœ€ç›¸ä¼¼çš„å‰*k*ä¸ªæ–‡æ¡£ã€‚
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Generator
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç”Ÿæˆå™¨
- en: As mentioned previously, the LLM for text generation is Llama2\. It uses *quantization,*
    a technique to reduce the precision of how the weights are represented to minimize
    the memory required to use the model. Notice that since no [free lunches exist](https://en.wikipedia.org/wiki/No_free_lunch_theorem),
    we are trading off memory size with accuracy.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼Œç”Ÿæˆæ–‡æœ¬çš„LLMæ˜¯Llama2ã€‚å®ƒä½¿ç”¨*é‡åŒ–*æŠ€æœ¯ï¼Œè¿™æ˜¯ä¸€ç§é™ä½æƒé‡è¡¨ç¤ºç²¾åº¦çš„æŠ€æœ¯ï¼Œä»¥æœ€å°åŒ–ä½¿ç”¨æ¨¡å‹æ‰€éœ€çš„å†…å­˜ã€‚è¯·æ³¨æ„ï¼Œç”±äºä¸å­˜åœ¨[å…è´¹çš„åˆé¤](https://en.wikipedia.org/wiki/No_free_lunch_theorem)ï¼Œæˆ‘ä»¬åœ¨å†…å­˜å¤§å°å’Œå‡†ç¡®æ€§ä¹‹é—´è¿›è¡Œäº†æƒè¡¡ã€‚
- en: This process brings advantages like the possibility of running LLMs with less
    resources but, also, disadvantages such as a reduction in performance due to *quantization.*
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè¿‡ç¨‹å¸¦æ¥äº†å¦‚è¿è¡ŒLLMæ—¶èµ„æºéœ€æ±‚å‡å°‘ç­‰ä¼˜ç‚¹ï¼Œä½†ä¹Ÿæœ‰å¦‚*é‡åŒ–*å¯¼è‡´æ€§èƒ½é™ä½ç­‰ç¼ºç‚¹ã€‚
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Once we have our LLM, it is time to set the **Prompt Template.** Prompt Engineering
    is relevant when interacting with LLMs since it can significantly impact its output.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æˆ‘ä»¬æ‹¥æœ‰äº†LLMï¼Œå°±è¯¥è®¾ç½®**æç¤ºæ¨¡æ¿**äº†ã€‚æç¤ºå·¥ç¨‹åœ¨ä¸LLMäº¤äº’æ—¶æ˜¯ç›¸å…³çš„ï¼Œå› ä¸ºå®ƒå¯ä»¥æ˜¾è‘—å½±å“è¾“å‡ºã€‚
- en: When we find a prompt that produces the desired output for the use case, we
    can create a template. LangChain offers a simple solution to create a **Prompt
    Template**. We start by defining the structure of the prompt and adding the dynamic
    variables in a dictionary format based on the userâ€™s query. In our case, the `{context}`
    is given by the retriever and the userâ€™s `{question}`.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬æ‰¾åˆ°ä¸€ä¸ªèƒ½ä¸ºä½¿ç”¨æ¡ˆä¾‹ç”Ÿæˆæ‰€éœ€è¾“å‡ºçš„æç¤ºæ—¶ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªæ¨¡æ¿ã€‚LangChainæä¾›äº†ä¸€ä¸ªç®€å•çš„è§£å†³æ–¹æ¡ˆæ¥åˆ›å»º**æç¤ºæ¨¡æ¿**ã€‚æˆ‘ä»¬é¦–å…ˆå®šä¹‰æç¤ºçš„ç»“æ„ï¼Œå¹¶æ ¹æ®ç”¨æˆ·çš„æŸ¥è¯¢ä»¥å­—å…¸æ ¼å¼æ·»åŠ åŠ¨æ€å˜é‡ã€‚åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œ`{context}`ç”±æ£€ç´¢å™¨æä¾›ï¼Œç”¨æˆ·çš„`{question}`åˆ™ç”±ç”¨æˆ·æä¾›ã€‚
- en: '[PRE4]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now that we have defined our LLM and Prompt Template, we create an `LLMChain`
    from `langchain.chains` . It allows us to combine multiple components to create
    a coherent application.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»å®šä¹‰äº†æˆ‘ä»¬çš„LLMå’Œæç¤ºæ¨¡æ¿ï¼Œæˆ‘ä»¬ä»`langchain.chains`ä¸­åˆ›å»ºä¸€ä¸ª`LLMChain`ã€‚å®ƒå…è®¸æˆ‘ä»¬ç»“åˆå¤šä¸ªç»„ä»¶ä»¥åˆ›å»ºä¸€ä¸ªè¿è´¯çš„åº”ç”¨ç¨‹åºã€‚
- en: In our case, we can create a chain that takes the userâ€™s question and the context,
    formats it with a Prompt Template, and then passes the formatted response to the
    LLM to get a response.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªé“¾æ¡ï¼Œè·å–ç”¨æˆ·çš„é—®é¢˜å’Œä¸Šä¸‹æ–‡ï¼Œç”¨æç¤ºæ¨¡æ¿æ ¼å¼åŒ–å®ƒï¼Œç„¶åå°†æ ¼å¼åŒ–çš„å“åº”ä¼ é€’ç»™LLMä»¥è·å¾—ç­”æ¡ˆã€‚
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We are now ready to use our model and compare it with a base Llama2 model.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å‡†å¤‡ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹ï¼Œå¹¶å°†å…¶ä¸åŸºç¡€Llama2æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚
- en: 'Results: Base Llama2 vs RAG Llama2'
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“æœï¼šåŸºç¡€Llama2 vs RAG Llama2
- en: â€œWhat happened to the CEO of OpenAI?â€
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: â€œOpenAIçš„CEOå‘ç”Ÿäº†ä»€ä¹ˆäº‹ï¼Ÿâ€
- en: Considering the question, *â€œWhat happened to the CEO of OpenAI?â€* Our goal is
    to show that without fine-tuning an LLM and using an RAG system, we can easily
    improve the prompt with relevant information which will make the LLM be more accurate
    and more updated in its answer. On the contrary, we expect the Base Llama2 to
    provide an accurate answer but is outdated since its training data did not contain
    information from last week.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘åˆ°é—®é¢˜ï¼Œ*â€œOpenAIçš„CEOå‘ç”Ÿäº†ä»€ä¹ˆäº‹ï¼Ÿâ€* æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å±•ç¤ºåœ¨ä¸å¯¹LLMè¿›è¡Œå¾®è°ƒå¹¶ä½¿ç”¨RAGç³»ç»Ÿçš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ç›¸å…³ä¿¡æ¯è½»æ¾æ”¹è¿›æç¤ºï¼Œä½¿LLMåœ¨å›ç­”æ—¶æ›´åŠ å‡†ç¡®å’Œæ›´æ–°ã€‚ç›¸åï¼Œæˆ‘ä»¬æœŸæœ›åŸºç¡€çš„Llama2æä¾›å‡†ç¡®çš„ç­”æ¡ˆï¼Œä½†ç”±äºå…¶è®­ç»ƒæ•°æ®æœªåŒ…å«ä¸Šå‘¨çš„ä¿¡æ¯ï¼Œå› æ­¤å¯èƒ½è¿‡æ—¶ã€‚
- en: '**Base Llama2:**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**åŸºç¡€ Llama2:**'
- en: OpenAI is a non-profit artificial intelligence research organization based in
    San Francisco, California, that focuses on developing friendly AI and ensuring
    its safe use. The CEO of OpenAI is Sam Altman, who has been at the helm since
    2010 when he co-founded the company with Elon Musk, Greg Brockman, Ilya Sutskever,
    and Vince Conitzer. In terms of recent news, there havenâ€™t been any major developments
    regarding the CEO or OpenAI specifically, but it is known that OpenAI has recently
    expanded its team by hiring an additional 15 employees to work on various projects
    related to AI, including the development of new models and tools for machine learning.
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: OpenAIæ˜¯ä¸€ä¸ªéè¥åˆ©çš„äººå·¥æ™ºèƒ½ç ”ç©¶ç»„ç»‡ï¼Œæ€»éƒ¨ä½äºåŠ åˆ©ç¦å°¼äºšå·æ—§é‡‘å±±ï¼Œä¸“æ³¨äºå¼€å‘å‹å¥½çš„AIå¹¶ç¡®ä¿å…¶å®‰å…¨ä½¿ç”¨ã€‚OpenAIçš„CEOæ˜¯Sam Altmanï¼Œè‡ª2010å¹´ä¸Elon
    Muskã€Greg Brockmanã€Ilya Sutskeverå’ŒVince Conitzerå…±åŒåˆ›ç«‹å…¬å¸ä»¥æ¥ï¼Œä»–ä¸€ç›´æ‹…ä»»æ­¤èŒåŠ¡ã€‚å°±æœ€è¿‘çš„æ–°é—»è€Œè¨€ï¼Œå…³äºCEOæˆ–OpenAIæœ¬èº«æ²¡æœ‰é‡å¤§è¿›å±•ï¼Œä½†å·²çŸ¥OpenAIæœ€è¿‘é€šè¿‡æ‹›è˜é¢å¤–çš„15åå‘˜å·¥æ‰©å¤§äº†å›¢é˜Ÿï¼Œä»¥ä»äº‹ä¸AIç›¸å…³çš„å„ç§é¡¹ç›®ï¼ŒåŒ…æ‹¬æ–°æ¨¡å‹å’Œæœºå™¨å­¦ä¹ å·¥å…·çš„å¼€å‘ã€‚
- en: '**RAG Llama2:**'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**RAG Llama2:**'
- en: Sam Altman stepped down from his role as CEO of OpenAI on November 20th, 2023\.
    He was replaced by Mira Murati who assumed the position of interim CEO. However,
    Sam Altman returned to the company a few days later as CEO with a new initial
    board consisting of Bret Taylor (Chair), Larry Summers and Adam Dâ€™Angelo.
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Sam Altman äº2023å¹´11æœˆ20æ—¥è¾å»äº† OpenAI é¦–å¸­æ‰§è¡Œå®˜èŒåŠ¡ã€‚ä»–è¢« Mira Murati å–ä»£ï¼Œåè€…æ‹…ä»»äº†ä¸´æ—¶é¦–å¸­æ‰§è¡Œå®˜ã€‚ç„¶è€Œï¼ŒSam
    Altman åœ¨å‡ å¤©åä»¥é¦–å¸­æ‰§è¡Œå®˜èº«ä»½å›åˆ°å…¬å¸ï¼Œæ–°çš„åˆå§‹è‘£äº‹ä¼šæˆå‘˜åŒ…æ‹¬ Bret Taylorï¼ˆä¸»å¸­ï¼‰ã€Larry Summers å’Œ Adam Dâ€™Angeloã€‚
- en: As we can see from the examples above, the RAG Llama managed to provide an answer
    with updated information without any additional fine-tuning process which would
    be expensive and time-consuming.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬ä»ä¸Šè¿°ç¤ºä¾‹ä¸­çœ‹åˆ°çš„ï¼ŒRAG Llama èƒ½å¤Ÿæä¾›å¸¦æœ‰æ›´æ–°ä¿¡æ¯çš„ç­”æ¡ˆï¼Œè€Œæ— éœ€ä»»ä½•é¢å¤–çš„å¾®è°ƒè¿‡ç¨‹ï¼Œè¿™ç§è¿‡ç¨‹æ—¢æ˜‚è´µåˆè€—æ—¶ã€‚
- en: Conclusion
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: RAGs opened the possibility of deploying LLMs faster and in a more affordable
    way for organizations than fine-tuning LLMs for every single use case.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: RAGs ä¸ºç»„ç»‡æä¾›äº†æ¯”ä¸ºæ¯ä¸€ä¸ªä½¿ç”¨æ¡ˆä¾‹å¾®è°ƒ LLMs æ›´å¿«ä¸”æ›´å®æƒ çš„éƒ¨ç½² LLMs çš„å¯èƒ½æ€§ã€‚
- en: As we saw in our use case, adding just twelve documents about the scandal regarding
    OpenAI and its CEO from last week to the set of 10k news that we fetched from
    HuggingFace was enough. Our retriever was able to create enough context for our
    generator to produce a more accurate and updated answer about the topic.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨ä½¿ç”¨æ¡ˆä¾‹ä¸­çœ‹åˆ°çš„ï¼Œå°†å…³äº OpenAI åŠå…¶é¦–å¸­æ‰§è¡Œå®˜ä¸Šå‘¨ä¸‘é—»çš„åäºŒç¯‡æ–‡æ¡£æ·»åŠ åˆ°æˆ‘ä»¬ä» HuggingFace è·å–çš„ 10k æ–°é—»é›†åˆä¸­å°±è¶³å¤Ÿäº†ã€‚æˆ‘ä»¬çš„æ£€ç´¢å™¨èƒ½å¤Ÿä¸ºæˆ‘ä»¬çš„ç”Ÿæˆå™¨åˆ›å»ºè¶³å¤Ÿçš„ä¸Šä¸‹æ–‡ï¼Œä»è€Œç”Ÿæˆå…³äºè¯¥ä¸»é¢˜çš„æ›´å‡†ç¡®ä¸”æ›´æ–°çš„ç­”æ¡ˆã€‚
- en: When it comes to accessing external information RAGs are a very good option
    because they augment LLMs capabilities by retrieving relevant information from
    knowledge sources before generating a response. Nevertheless, when it comes to
    adjusting the LLM behavior to tailor its responses for a specific writing style
    with uncommon words or expressions, a combination of both might be more suitable.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®¿é—®å¤–éƒ¨ä¿¡æ¯æ–¹é¢ï¼ŒRAGs æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é€‰æ‹©ï¼Œå› ä¸ºå®ƒä»¬é€šè¿‡åœ¨ç”Ÿæˆå“åº”ä¹‹å‰ä»çŸ¥è¯†æ¥æºä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯æ¥å¢å¼º LLMs çš„èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå½“æ¶‰åŠåˆ°è°ƒæ•´ LLM
    è¡Œä¸ºä»¥é€‚åº”ç‰¹å®šçš„å†™ä½œé£æ ¼æ—¶ï¼Œä½¿ç”¨ä¸å¸¸è§çš„å•è¯æˆ–è¡¨è¾¾å¼ï¼Œç»“åˆä½¿ç”¨å¯èƒ½æ›´ä¸ºåˆé€‚ã€‚
- en: About me
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å…³äºæˆ‘
- en: Serial entrepreneur and leader in the AI space. I develop AI products for businesses
    and invest in AI-focused startups.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: è¿ç»­åˆ›ä¸šè€…å’Œ AI é¢†åŸŸçš„é¢†å¯¼è€…ã€‚æˆ‘å¼€å‘ AI äº§å“ä»¥æœåŠ¡äºä¼ä¸šï¼Œå¹¶æŠ•èµ„äºä»¥ AI ä¸ºé‡ç‚¹çš„åˆåˆ›å…¬å¸ã€‚
- en: '[Founder @ ZAAI](http://zaai.ai) | [LinkedIn](https://www.linkedin.com/in/luisbrasroque/)
    | [X/Twitter](https://x.com/luisbrasroque)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[åˆ›å§‹äºº @ ZAAI](http://zaai.ai) | [LinkedIn](https://www.linkedin.com/in/luisbrasroque/)
    | [X/Twitter](https://x.com/luisbrasroque)'
- en: References
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[1] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
    Karpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel,
    Sebastian Riedel, Douwe Kiela. Retrieval-Augmented Generation for Knowledge-Intensive
    NLP Tasks. arXiv:2005.11401, 2021'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
    Karpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel,
    Sebastian Riedel, Douwe Kiela. é€‚ç”¨äºçŸ¥è¯†å¯†é›†å‹ NLP ä»»åŠ¡çš„æ£€ç´¢å¢å¼ºç”Ÿæˆã€‚arXiv:2005.11401, 2021'
- en: '[2] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi
    Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering.arXiv:2004.04906,
    2020'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi
    Chen, å’Œ Wen-tau Yih. ç”¨äºå¼€æ”¾åŸŸé—®ç­”çš„å¯†é›†é€šé“æ£€ç´¢ã€‚arXiv:2004.04906, 2020'
- en: '[3] Jeff Johnson, Matthijs Douze, and HervÃ© JÃ©gou. Billion-scale similarity
    search with GPUs. arXiv:1702.08734, 2017'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Jeff Johnson, Matthijs Douze, å’Œ HervÃ© JÃ©gou. åŸºäº GPU çš„äº¿è§„æ¨¡ç›¸ä¼¼æ€§æœç´¢ã€‚arXiv:1702.08734,
    2017'
- en: '[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019\.
    BERT: Pre-training of deep bidirectional transformers for language understanding.
    arXiv:1810.04805, 2019'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, å’Œ Kristina Toutanova. 2019\.
    BERT: æ·±åº¦åŒå‘å˜æ¢å™¨çš„é¢„è®­ç»ƒç”¨äºè¯­è¨€ç†è§£ã€‚arXiv:1810.04805, 2019'
- en: '[5] Michael R. Douglas. Large Language Models. arXiv:2307.05782, 2023.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Michael R. Douglas. å¤§å‹è¯­è¨€æ¨¡å‹ã€‚arXiv:2307.05782, 2023.'
- en: '[6] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman
    Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer. BART: Denoising Sequence-to-Sequence
    Pre-training for Natural Language Generation, Translation, and Comprehension.
    arXiv:1910.13461, 2019'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman
    Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer. BART: ç”¨äºè‡ªç„¶è¯­è¨€ç”Ÿæˆã€ç¿»è¯‘å’Œç†è§£çš„å»å™ªåºåˆ—åˆ°åºåˆ—é¢„è®­ç»ƒã€‚arXiv:1910.13461,
    2019'
- en: '[7] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. Attention Is All You Need. arXiv:1706.03762,
    2017.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. æ³¨æ„åŠ›æœºåˆ¶æ˜¯å”¯ä¸€éœ€è¦çš„ã€‚arXiv:1706.03762,
    2017.'
