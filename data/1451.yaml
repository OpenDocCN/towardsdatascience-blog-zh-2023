- en: Effectively Annotate Text Data for Transformers via Active Learning + Re-labeling
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过主动学习 + 重新标注有效标注Transformer的文本数据
- en: 原文：[https://towardsdatascience.com/effectively-annotate-text-data-for-transformers-via-active-learning-re-labeling-25fe036d79f?source=collection_archive---------2-----------------------#2023-04-27](https://towardsdatascience.com/effectively-annotate-text-data-for-transformers-via-active-learning-re-labeling-25fe036d79f?source=collection_archive---------2-----------------------#2023-04-27)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/effectively-annotate-text-data-for-transformers-via-active-learning-re-labeling-25fe036d79f?source=collection_archive---------2-----------------------#2023-04-27](https://towardsdatascience.com/effectively-annotate-text-data-for-transformers-via-active-learning-re-labeling-25fe036d79f?source=collection_archive---------2-----------------------#2023-04-27)
- en: Boost Transformer model performance with Active Learning assisted data labeling
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用主动学习辅助数据标注来提升Transformer模型性能
- en: '[](https://medium.com/@chrismauck10?source=post_page-----25fe036d79f--------------------------------)[![Chris
    Mauck](../Images/d0aeb4d0458544afdfdd59915a962b18.png)](https://medium.com/@chrismauck10?source=post_page-----25fe036d79f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----25fe036d79f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----25fe036d79f--------------------------------)
    [Chris Mauck](https://medium.com/@chrismauck10?source=post_page-----25fe036d79f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@chrismauck10?source=post_page-----25fe036d79f--------------------------------)[![Chris
    Mauck](../Images/d0aeb4d0458544afdfdd59915a962b18.png)](https://medium.com/@chrismauck10?source=post_page-----25fe036d79f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----25fe036d79f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----25fe036d79f--------------------------------)
    [Chris Mauck](https://medium.com/@chrismauck10?source=post_page-----25fe036d79f--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F96a38f7ac238&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feffectively-annotate-text-data-for-transformers-via-active-learning-re-labeling-25fe036d79f&user=Chris+Mauck&userId=96a38f7ac238&source=post_page-96a38f7ac238----25fe036d79f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----25fe036d79f--------------------------------)
    ·9 min read·Apr 27, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F25fe036d79f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feffectively-annotate-text-data-for-transformers-via-active-learning-re-labeling-25fe036d79f&user=Chris+Mauck&userId=96a38f7ac238&source=-----25fe036d79f---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F96a38f7ac238&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feffectively-annotate-text-data-for-transformers-via-active-learning-re-labeling-25fe036d79f&user=Chris+Mauck&userId=96a38f7ac238&source=post_page-96a38f7ac238----25fe036d79f---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----25fe036d79f--------------------------------)
    ·9分钟阅读·2023年4月27日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F25fe036d79f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feffectively-annotate-text-data-for-transformers-via-active-learning-re-labeling-25fe036d79f&user=Chris+Mauck&userId=96a38f7ac238&source=-----25fe036d79f---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F25fe036d79f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feffectively-annotate-text-data-for-transformers-via-active-learning-re-labeling-25fe036d79f&source=-----25fe036d79f---------------------bookmark_footer-----------)![](../Images/368c78a18b228e319f02837c08250e17.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F25fe036d79f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Feffectively-annotate-text-data-for-transformers-via-active-learning-re-labeling-25fe036d79f&source=-----25fe036d79f---------------------bookmark_footer-----------)![](../Images/368c78a18b228e319f02837c08250e17.png)'
- en: ActiveLab chooses which data you should (re)label to train a more effective
    model. Given the same labeling budget, ActiveLab outperforms other selection methods.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ActiveLab 选择你应该（重新）标注的数据，以训练更有效的模型。给定相同的标注预算，ActiveLab 优于其他选择方法。
- en: In this article, I highlight the use of active learning to improve a fine-tuned
    Transformer model for text classification, while keeping the total number of collected
    labels from human annotators low. When resource constraints prevent you from acquiring
    labels for the entirety of your data, active learning aims to save both time and
    money by selecting which examples data annotators should spend their effort labeling.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我重点介绍了使用主动学习来改进一个针对文本分类的微调Transformer模型，同时保持从人工标注者那里收集的标签总数较低。当资源限制使你无法为全部数据获取标签时，主动学习旨在通过选择数据标注者应花费精力标注的示例来节省时间和金钱。
- en: ActiveLab greatly reduces labeling costs and time spent to achieve a given model
    performance compared to standard data annotation. In the experiments demonstrated
    in this article, ActiveLab hits 90% model accuracy at only 35% of the label spend
    as standard training.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 与标准数据标注相比，ActiveLab显著降低了标签成本和实现特定模型性能所需的时间。在本文展示的实验中，ActiveLab在仅使用标准训练的35%标签开支的情况下达到了90%的模型准确率。
- en: What is Active Learning?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是主动学习？
- en: Active Learning helps prioritize what data to label in order to maximize the
    performance of a supervised machine learning model trained on the labeled data.
    This process usually happens iteratively — at each round, active learning tells
    us which examples we should collect additional annotations for to improve our
    current model the most **under a limited labeling budget**.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 主动学习有助于优先标注哪些数据，以最大化在标注数据上训练的监督机器学习模型的性能。这个过程通常是迭代进行的——在每一轮，主动学习告诉我们应该收集哪些示例的额外注释，以在**有限的标注预算下**最大限度地提高我们当前模型的表现。
- en: Active Learning is most useful to efficiently annotate data in settings where
    you have a large pool of unlabeled data and a limited labeling budget. Here you
    want to decide which examples to label in order to train an accurate model. An
    important assumption of the methods presented here (and most Machine Learning)
    is that the individual examples are independent and identically distributed.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 主动学习最适用于在你有大量未标注数据和有限标注预算的情况下高效地标注数据。在这种情况下，你需要决定标注哪些示例以训练一个准确的模型。这里提出的方法（以及大多数机器学习）一个重要假设是，个别示例是独立且同分布的。
- en: What is ActiveLab?
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是ActiveLab？
- en: '[ActiveLab](https://arxiv.org/abs/2301.11856) is an active learning algorithm
    that is particularly useful when the annotators are noisy because it helps decide
    when we should collect one more annotation for a previously annotated example
    (whose label seems suspect) vs. for a not-yet-annotated example. After collecting
    these new annotations for a batch of data to increase our training dataset, we
    re-train our model and evaluate its test accuracy.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[ActiveLab](https://arxiv.org/abs/2301.11856)是一种主动学习算法，特别适用于标注者有噪声的情况，因为它有助于决定我们应该为一个之前已标注但其标签似乎有问题的示例（标签看起来可疑）再收集一个标注，还是为一个尚未标注的示例收集标注。在收集这些新标注以增加我们的训练数据集后，我们重新训练模型并评估其测试准确率。'
- en: '[CROWDLAB](https://cleanlab.ai/blog/multiannotator/) is a method to estimate
    our confidence in a consensus label from multi-annotator data, which produces
    accurate estimates via a weighted ensemble of *any* trained model’s probabilistic
    prediction *p_M*​ and the individual labels assigned by each annotator *j*. ActiveLab
    forms a similar weighted ensemble estimate, treating each annotator’s selection
    as a probabilistic decision *p_j* output by another predictor:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[CROWDLAB](https://cleanlab.ai/blog/multiannotator/)是一种估计我们在多标注数据中的共识标签信心的方法，它通过加权集成*任何*训练模型的概率预测*p_M*以及每个标注者分配的单个标签*j*来生成准确的估计。ActiveLab形成了类似的加权集成估计，将每个标注者的选择视为由另一个预测器输出的概率决策*p_j*：'
- en: '![](../Images/e514002fa30e38d71af8523c27eab17f.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e514002fa30e38d71af8523c27eab17f.png)'
- en: ActiveLab decides to re-label data when the probability of the current consensus
    label for a previously-annotated datapoint falls below our (purely model-based)
    confidence in the predicted label for an unannotated datapoint.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当之前标注的数据点的当前共识标签的概率低于我们对未标注数据点预测标签的（纯模型基础上的）信心时，ActiveLab决定重新标注数据。
- en: ActiveLab is best for data labeling applications where data annotators are imperfect
    and you are able to train a reasonable classifier model (which is able to produce
    better than random predictions). The method works with any data modality and classifier
    model.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ActiveLab最适合数据标注应用，其中数据标注者并不完美，并且你能够训练出一个合理的分类器模型（能够产生比随机预测更好的结果）。该方法适用于任何数据模态和分类器模型。
- en: Motivation
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动机
- en: I recently joined [Cleanlab](https://cleanlab.ai/) as a Data Scientist and am
    excited to share how ActiveLab (part of our [open-source library](https://github.com/cleanlab/cleanlab?utm_medium=tds),
    freely available under AGPL-v3 license) can be used in various workflows to improve
    datasets.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我最近加入了 [Cleanlab](https://cleanlab.ai/) 担任数据科学家，并很高兴分享如何使用 ActiveLab（我们 [开源库](https://github.com/cleanlab/cleanlab?utm_medium=tds)
    的一部分，AGPL-v3 许可证下免费提供）在各种工作流程中改进数据集。
- en: 'Here I consider a binary text classification task: predicting whether a specific
    phrase is polite or impolite. Compared to random selection of which examples to
    collect an additional annotation for, active learning with ActiveLab consistently
    produces much better Transformer models at each round (around **50%** of the error-rate),
    no matter the total labeling budget!'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我考虑的是一个二分类文本任务：预测特定短语是礼貌还是不礼貌。与随机选择要收集额外标注的示例相比，使用 ActiveLab 的主动学习在每一轮中始终能产生更好的
    Transformer 模型（大约 **50%** 的错误率），无论总标注预算是多少！
- en: 'The rest of this article walks through the open-source code you can use to
    achieve these results. You can run all of the code to reproduce my active learning
    experiments here: [Colab Notebook](https://colab.research.google.com/github/cmauck10/active-learning/blob/master/active-learning.ipynb)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分将介绍你可以使用的开源代码，以实现这些结果。你可以运行所有代码以重现我的主动学习实验，点击这里查看：[Colab Notebook](https://colab.research.google.com/github/cmauck10/active-learning/blob/master/active-learning.ipynb)
- en: Classifying the Politeness of Text
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本礼貌性的分类
- en: 'The dataset I consider here is a variant of the [Stanford Politeness Corpus](https://convokit.cornell.edu/documentation/wiki_politeness.html).
    It is structured as a binary text classification task, to classify whether each
    phrase is polite or impolite. Human annotators are given a selected text phrase
    and they provide an (imperfect) annotation regarding its politeness: 0 for impolite
    and 1 for polite.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我考虑的数据集是 [斯坦福礼貌语料库](https://convokit.cornell.edu/documentation/wiki_politeness.html)
    的一个变体。它被结构化为一个二分类文本任务，用于分类每个短语是否礼貌。人工注释员会收到选定的文本短语，并提供关于其礼貌性的（不完美的）注释：0 代表不礼貌，1
    代表礼貌。
- en: Training a Transformer classifier on the annotated data, we measure model accuracy
    over a set of held-out test examples, where I feel confident about their ground
    truth labels because they are derived from a consensus amongst 5 annotators who
    labeled each of these examples.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在标注数据上训练一个 Transformer 分类器，我们在一组保留的测试示例上测量模型准确性，我对这些示例的真实标签很有信心，因为它们是基于 5 位注释员对这些示例的一致性标记得出的。
- en: 'As for the training data, we have:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 关于训练数据，我们有：
- en: '`X_labeled_full`: our initial training set with just a small set of 100 text
    examples labeled with 2 annotations per example.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`X_labeled_full`：我们初始的训练集，仅有 100 个文本示例，每个示例有 2 个标注。'
- en: '`X_unlabeled`: a large set of 1900 unlabeled text examples we can consider
    having annotators label.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`X_unlabeled`：一个包含 1900 个未标记文本示例的大集合，我们可以考虑让注释员进行标记。'
- en: 'Here are a few examples from `X_labeled_full`:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些来自 `X_labeled_full` 的示例：
- en: '**Hi, nice article. What is meant by “speculative townhouses?” Ones that were
    built for prospective renters rather than for committed buyers**'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**嗨，文章写得很好。“投机性的联排别墅”是什么意思？是为潜在租户而建造的，而不是为已确认的买家而建造的？**'
- en: 'Annotation (by annotator #61): polite'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '注释（由注释员 #61 提供）：礼貌'
- en: 'Annotation (by annotator #99): polite'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '注释（由注释员 #99 提供）：礼貌'
- en: 2\. **Congrats, or should I say good luck?**
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. **恭喜，还是说好运？**
- en: 'Annotation (by annotator #16): polite'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '注释（由注释员 #16 提供）：礼貌'
- en: 'Annotation (by annotator #22): impolite'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '注释（由注释员 #22 提供）：不礼貌'
- en: 3\. **4.5 million hits on Google turning up the non-Columbia campuses. What
    are you talking about?**
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. **谷歌上有 450 万条结果显示非哥伦比亚大学的校园。你在说什么？**
- en: 'Annotation (by annotator #22): impolite'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '注释（由注释员 #22 提供）：不礼貌'
- en: 'Annotation (by annotator #61): impolite'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '注释（由注释员 #61 提供）：不礼貌'
- en: Methodology
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法论
- en: 'For each **active learning** round we:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每一轮 **主动学习** 我们：
- en: Compute ActiveLab consensus labels for each training example derived from all
    annotations collected thus far.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 ActiveLab 共识标签，用于从迄今为止收集到的所有标注中获得每个训练示例的标签。
- en: Train our Transformer classification model on the current training set using
    these consensus labels.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这些共识标签在当前训练集上训练我们的 Transformer 分类模型。
- en: Evaluate test accuracy on the test set (which has high-quality ground truth
    labels).
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上评估测试准确性（测试集具有高质量的真实标签）。
- en: Run cross-validation to get out-of-sample predicted class probabilities from
    our model for the entire training set and unlabeled set.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行交叉验证以从我们的模型中获得整个训练集和未标记集的样本外预测类别概率。
- en: Get ActiveLab active learning scores for each example in the training set and
    unlabeled set. These scores estimate how informative it would be to collect another
    annotation for each example.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取训练集和未标注集中的每个示例的ActiveLab主动学习得分。这些得分估计了为每个示例收集另一个注释的有用性。
- en: Select a subset (*n = batch_size*) of examples with the lowest active learning
    scores.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择具有最低主动学习得分的示例子集(*n = batch_size*)。
- en: Collect one additional annotation for each of the *n* selected examples.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个* n *个选定示例收集一个额外的注释。
- en: Add the new annotations (and new previously non-annotated examples if selected)
    to our training set for the next iteration.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将新的注释（以及如果被选中的新的之前未标注的示例）添加到我们下一个迭代的训练集中。
- en: I subsequently compare models trained on data labeled via active learning vs.
    data labeled via **random selection**. For each random selection round, I use
    majority vote consensus instead of ActiveLab consensus (in Step 1) and then just
    randomly select the **n** examples to collect an additional label for instead
    of using ActiveLab scores (in Step 6).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我随后比较了通过主动学习标注的数据与通过**随机选择**标注的数据训练的模型。对于每轮随机选择，我使用多数投票共识，而不是主动学习共识（在步骤1中），然后随机选择**n**个示例以收集额外的标签，而不是使用ActiveLab得分（在步骤6中）。
- en: Model Training and Evaluation
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型训练与评估
- en: Here is the code we use for model training and evaluation, using the [Hugging
    Face library](https://huggingface.co/docs/transformers/model_doc/distilbert) which
    offers many state-of-the-art Transformer networks.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们用于模型训练和评估的代码，使用了提供许多最先进Transformer网络的[Hugging Face库](https://huggingface.co/docs/transformers/model_doc/distilbert)。
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: I first tokenize my test and train sets, and then initialize a pre-trained DistilBert
    Transformer model. Fine-tuning DistilBert with 300 training steps produced a good
    balance between accuracy and training time for my data. This classifier outputs
    predicted class probabilities which I convert to class predictions before evaluating
    their accuracy.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我首先对测试集和训练集进行分词，然后初始化一个预训练的DistilBert Transformer模型。对DistilBert进行300次训练步骤的微调在我的数据中取得了准确性和训练时间的良好平衡。该分类器输出预测的类别概率，我将其转换为类别预测，然后评估其准确性。
- en: Use Active Learning Scores to Decide What to Label Next
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用主动学习得分来决定下一步标注什么
- en: Here is the code we use to score each example via an active learning estimate
    of how informative collecting one more label for this example will be.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们用来评分每个示例的代码，基于主动学习对获取该示例一个更多标签的有用性的估计。
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: During each round of Active Learning, we fit our Transformer model via 3-fold
    cross-validation on the current training set. This allows us to get out-of-sample
    predicted class probabilities for each example in the training set and we can
    also use the trained Transformer to get out-of-sample predicted class probabilities
    for each example in the unlabeled pool. All of this is internally implemented
    in the `get_pred_probs` helper method. The use of out-of-sample predictions helps
    us avoid bias due to potential overfitting.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在每轮主动学习过程中，我们通过对当前训练集进行3折交叉验证来训练我们的Transformer模型。这使我们能够获得训练集中每个示例的样本外预测类别概率，我们还可以使用训练好的Transformer模型获取未标注池中每个示例的样本外预测类别概率。所有这些操作都在`get_pred_probs`辅助方法中实现。使用样本外预测有助于我们避免由于潜在的过拟合而产生的偏差。
- en: Once I have these probabilistic predictions, I pass them into the `get_active_learning_scores`
    method from the open-source [cleanlab](https://github.com/cleanlab/cleanlab) package,
    which implements the [ActiveLab algorithm](https://arxiv.org/abs/2301.11856).
    This method provides us with scores for all of our labeled and unlabeled data.
    Lower scores indicate data points for which collecting one additional label should
    be most informative for our current model (scores are directly comparable between
    labeled and unlabeled data).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我获得这些概率预测，我将它们传递到开源的[cleanlab](https://github.com/cleanlab/cleanlab)包中的`get_active_learning_scores`方法，该方法实现了[ActiveLab算法](https://arxiv.org/abs/2301.11856)。该方法为我们提供了所有标注和未标注数据的得分。较低的得分表示收集一个额外标签对当前模型的有用性最高的数据点（标注数据和未标注数据之间的得分可以直接比较）。
- en: I form a batch of examples with the lowest scores as the examples to collect
    an annotation for (via the `get_idx_to_label` method). Here I always collect the
    exact same number of annotations in each round (under both the active learning
    and random selection approaches). For this application, I limit the maximum number
    of annotations per example to 5 (don’t want to spend effort labeling the same
    example over and over again).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我将一批得分最低的示例作为收集注释的例子（通过`get_idx_to_label`方法）。在每一轮中，我始终收集相同数量的注释（无论是在主动学习还是随机选择方法下）。在此应用中，我将每个示例的最大注释数量限制为5个（不想反复花费精力对相同示例进行标注）。
- en: Adding new Annotations
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 添加新注释
- en: Here is the code used to add new annotations for the chosen examples to the
    current training dataset.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于将新的注释添加到当前训练数据集的代码。
- en: '[PRE2]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `combined_example_ids` are the ids of the text examples we want to collect
    an annotation for. For each of these, we use the `get_annotation` helper method
    to collect a new annotation from an annotator. Here, we prioritize selecting annotations
    from annotators who have already annotated another example. If none of the annotators
    for the given example exist in the training set, we randomly select one. In this
    case, we add a new column to our training set which represents the new annotator.
    Finally, we add the newly collected annotation to the training set. If the corresponding
    example was previously non-annotated, we also add it to the training set and remove
    it from the unlabeled collection.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`combined_example_ids`是我们希望收集注释的文本示例的ID。对于每个示例，我们使用`get_annotation`辅助方法从标注者那里收集新的注释。在这里，我们优先选择已经标注过其他示例的标注者。如果给定示例的标注者在训练集中不存在，我们将随机选择一个。在这种情况下，我们在训练集中添加一个新列，表示新的标注者。最后，我们将新收集的注释添加到训练集中。如果相应的示例之前未被标注，我们还将其添加到训练集中，并从未标记集合中移除。'
- en: We’ve now completed one round of collecting new annotations and retrain the
    Transformer model on the updated training set. We repeat this process in multiple
    rounds to keep growing the training dataset and improving our model.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经完成了一轮新的注释收集，并在更新后的训练集上重新训练了Transformer模型。我们在多个回合中重复这个过程，以不断扩展训练数据集并提升模型性能。
- en: Results
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: I ran 25 rounds of active learning (labeling batches of data and retraining
    the Transformer model), collecting 25 annotations in each round. I repeated all
    of this, the next time using random selection to choose which examples to annotate
    in each round — as a baseline comparison. Before additional data are annotated,
    both approaches start with the same initial training set of 100 examples (hence
    achieving roughly the same Transformer accuracy in the first round). Because of
    inherent stochasticity in training Transformers, I ran this entire process five
    times (for each data labeling strategy) and report the standard deviation (shaded
    region) and mean (solid line) of test accuracies across the five replicate runs.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我进行了25轮主动学习（标注数据批次并重新训练Transformer模型），每轮收集25个注释。我重复了这一过程，下一次使用随机选择来确定每轮标注的示例——作为基线比较。在额外数据被注释之前，这两种方法都以相同的初始训练集100个示例开始（因此在第一轮中实现了大致相同的Transformer准确率）。由于训练Transformers固有的随机性，我对每种数据标注策略进行了五次完整的过程，并报告了五次重复运行中的测试准确率的标准差（阴影区域）和均值（实线）。
- en: '**Takeaway: ActiveLab greatly reduces time and labeling costs to achieve a
    given model performance compared to standard data annotation. For example, ActiveLab
    hits 90% model accuracy at only 35% of the label spend as standard training.**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结：与标准数据注释相比，ActiveLab大大减少了时间和标注成本以实现相同的模型性能。例如，ActiveLab以仅35%的标注费用达到90%的模型准确率。**'
- en: '![](../Images/211af98037fbd0c02e3592929148ee88.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/211af98037fbd0c02e3592929148ee88.png)'
- en: ActiveLab outperforms random selection substantially when averaged over 5 runs.
    The standard deviation is shaded and the solid line is the mean.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于随机选择，ActiveLab在5次运行中的表现明显优越。标准差以阴影表示，实线为均值。
- en: We see that choosing what data to annotate next has drastic effects on model
    performance. Active learning using ActiveLab consistently outperforms random selection
    by a significant margin at each round. For example, in round 4 with 275 total
    annotations in the training set, we obtain **91% accuracy via active learning**
    vs. only 76% accuracy without a clever selection strategy of what to annotate.
    Overall, the resulting Transformer models fit on the dataset constructed via active
    learning have around **50%** of the error-rate, no matter the total labeling budget.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，选择接下来标注的数据对模型性能有着深远的影响。使用ActiveLab进行的主动学习在每一轮中都明显优于随机选择。例如，在第4轮中，训练集总共有275个标注，通过主动学习获得**91%的准确率**，而没有聪明选择标注数据策略时，仅获得76%的准确率。总体而言，经过主动学习构建的数据集上训练得到的Transformer模型，无论总标注预算是多少，其错误率大约只有**50%**。
- en: While active learning has its advantages, it may not always be the most beneficial
    approach. For instance, when the data labeling process is inexpensive, or when
    there is a selection bias or distribution shift between the unlabeled dataset
    and the data that the model will encounter during deployment. Additionally, active
    learning feedback loops rely on the classifier model’s ability to generate predictions
    that are more informative than random. When this is not the case, active learning
    may not provide any significant signal about the data’s informativeness.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管主动学习有其优点，但它可能并不总是最有利的方法。例如，当数据标注过程成本低廉，或在未标记的数据集与模型在部署过程中遇到的数据之间存在选择偏差或分布变化时。此外，主动学习反馈循环依赖于分类器模型生成比随机更具信息性的预测。当情况并非如此时，主动学习可能无法提供关于数据信息量的显著信号。
- en: '**When labeling data for text classification, you should consider active learning
    with the re-labeling option to better account for imperfect annotators.**'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**在进行文本分类数据标注时，您应考虑使用具有重新标注选项的主动学习，以更好地应对标注者的不完美。**'
- en: '*All images unless otherwise noted are by the author.*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*除非另有说明，否则所有图片均由作者提供。*'
