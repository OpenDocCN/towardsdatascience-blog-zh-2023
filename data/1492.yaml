- en: 'The Art of Prompt Design: Use Clear Syntax'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示设计艺术：使用清晰的语法
- en: 原文：[https://towardsdatascience.com/the-art-of-prompt-design-use-clear-syntax-4fc846c1ebd5?source=collection_archive---------0-----------------------#2023-05-02](https://towardsdatascience.com/the-art-of-prompt-design-use-clear-syntax-4fc846c1ebd5?source=collection_archive---------0-----------------------#2023-05-02)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-art-of-prompt-design-use-clear-syntax-4fc846c1ebd5?source=collection_archive---------0-----------------------#2023-05-02](https://towardsdatascience.com/the-art-of-prompt-design-use-clear-syntax-4fc846c1ebd5?source=collection_archive---------0-----------------------#2023-05-02)
- en: Explore how clear syntax can enable you to communicate intent to language models,
    and also help ensure that outputs are easy to parse
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索如何通过清晰的语法使你能够向语言模型传达意图，并帮助确保输出结果易于解析
- en: '[](https://medium.com/@scottmlundberg?source=post_page-----4fc846c1ebd5--------------------------------)[![Scott
    Lundberg](../Images/99f1c984f0aaabfe4e348a92fa50a1ee.png)](https://medium.com/@scottmlundberg?source=post_page-----4fc846c1ebd5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4fc846c1ebd5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4fc846c1ebd5--------------------------------)
    [Scott Lundberg](https://medium.com/@scottmlundberg?source=post_page-----4fc846c1ebd5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@scottmlundberg?source=post_page-----4fc846c1ebd5--------------------------------)[![Scott
    Lundberg](../Images/99f1c984f0aaabfe4e348a92fa50a1ee.png)](https://medium.com/@scottmlundberg?source=post_page-----4fc846c1ebd5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----4fc846c1ebd5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----4fc846c1ebd5--------------------------------)
    [Scott Lundberg](https://medium.com/@scottmlundberg?source=post_page-----4fc846c1ebd5--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3a739af9ef3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-art-of-prompt-design-use-clear-syntax-4fc846c1ebd5&user=Scott+Lundberg&userId=3a739af9ef3a&source=post_page-3a739af9ef3a----4fc846c1ebd5---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----4fc846c1ebd5--------------------------------)
    ·9 min read·May 2, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4fc846c1ebd5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-art-of-prompt-design-use-clear-syntax-4fc846c1ebd5&user=Scott+Lundberg&userId=3a739af9ef3a&source=-----4fc846c1ebd5---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F3a739af9ef3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-art-of-prompt-design-use-clear-syntax-4fc846c1ebd5&user=Scott+Lundberg&userId=3a739af9ef3a&source=post_page-3a739af9ef3a----4fc846c1ebd5---------------------post_header-----------)
    发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----4fc846c1ebd5--------------------------------)
    ·9分钟阅读·2023年5月2日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F4fc846c1ebd5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-art-of-prompt-design-use-clear-syntax-4fc846c1ebd5&user=Scott+Lundberg&userId=3a739af9ef3a&source=-----4fc846c1ebd5---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4fc846c1ebd5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-art-of-prompt-design-use-clear-syntax-4fc846c1ebd5&source=-----4fc846c1ebd5---------------------bookmark_footer-----------)![](../Images/acca20e7fd6c31390e54b99cbc5ca055.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F4fc846c1ebd5&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-art-of-prompt-design-use-clear-syntax-4fc846c1ebd5&source=-----4fc846c1ebd5---------------------bookmark_footer-----------)![](../Images/acca20e7fd6c31390e54b99cbc5ca055.png)'
- en: All images were generated by Scott and Marco.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 所有图片均由Scott和Marco生成。
- en: This is the first installment of a series on how to use `[guidance](https://github.com/guidance-ai/guidance)`
    to control large language models (LLMs), written jointly with [Marco Tulio Ribeiro](https://medium.com/@marcotcr).
    We’ll start from the basics and work our way up to more advanced topics.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这是关于如何使用`[guidance](https://github.com/guidance-ai/guidance)`来控制大型语言模型（LLMs）系列文章的第一篇，由[Marco
    Tulio Ribeiro](https://medium.com/@marcotcr)和我共同撰写。我们将从基础知识开始，逐步深入到更高级的主题。
- en: In this post, we’ll show that having **clear syntax** enables you to communicate
    your intent to the LLM, and also ensure that outputs are easy to parse (like JSON
    that is guaranteed to be valid). For the sake of clarity and reproducibility we’ll
    start with an open source Mistral 7B model without fine tuning. Then, we will
    show how the same ideas apply to fine-tuned models like ChatGPT / GPT-4\. All
    the code below is [available in a notebook](https://github.com/guidance-ai/guidance/blob/main/notebooks/art_of_prompt_design/use_clear_syntax.ipynb)
    for you to reproduce if you like.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将展示**明确的语法**如何使你能够向LLM传达你的意图，并确保输出易于解析（就像保证有效的JSON）。为了清晰和可重复性，我们将从一个未微调的开源Mistral
    7B模型开始。然后，我们将展示这些相同的想法如何应用于像ChatGPT / GPT-4这样的微调模型。下面的所有代码都[可在笔记本中找到](https://github.com/guidance-ai/guidance/blob/main/notebooks/art_of_prompt_design/use_clear_syntax.ipynb)，如果你愿意，可以复现。
- en: '**Clear syntax helps with parsing the output**'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**明确的语法有助于解析输出**'
- en: 'The first, and most obvious benefit of using clear syntax is that it makes
    it easier to parse the output of the LLM. Even if the LLM is able to generate
    a correct output, it may be difficult to programatically extract the desired information
    from the output. For example, consider the following Guidance prompt (where `gen()`
    is a `guidance` command to generate text from the LLM):'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 使用明确语法的第一个也是最明显的好处是它使得解析LLM的输出更容易。即使LLM能够生成正确的输出，可能也难以程序化地从输出中提取所需的信息。例如，考虑以下Guidance提示（其中`gen()`是一个`guidance`命令，用于从LLM生成文本）：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/0548bcc5abcc476471a6c9524df32715.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0548bcc5abcc476471a6c9524df32715.png)'
- en: Output as it appears in a notebook.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 输出在笔记本中的显示方式。
- en: 'While the answer is readable, the output *format* is arbitrary (i.e. we don’t
    know it in advance), and thus hard to parse programatically. For example here
    is another run of a similar prompt where the output format is very different:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然答案是可读的，但输出的*格式*是任意的（即我们事先不知道），因此难以程序化解析。例如，这里是类似提示的另一轮，其中输出格式非常不同：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/688808521a984ae3605665ffeebaf7d3.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/688808521a984ae3605665ffeebaf7d3.png)'
- en: 'Enforcing clear syntax in your prompts can help reduce the problem of arbitrary
    output formats. There are a couple ways you can do this:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在提示中强制使用明确的语法可以帮助减少任意输出格式的问题。你可以通过几种方式来实现这一点：
- en: 1\. Giving structure hints to the LLM inside a standard prompt (perhaps even
    using few-shot examples).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 在标准提示中给LLM提供结构提示（甚至可以使用少量示例）。
- en: 2\. Using `guidance` (or some other package) that enforces a specific output
    format.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 使用`guidance`（或其他包）来强制特定的输出格式。
- en: These are not mutually exclusive. Let’s see an example of each approach.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这些并不是互相排斥的。让我们看一下每种方法的示例。
- en: '**Traditional prompt with structure hints**'
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**传统的提示与结构提示**'
- en: Here is an example of a traditional prompt that uses structure hints to encourage
    the use of a specific output format. The prompt is designed to generate a list
    of 5 items that is easy to parse. Note that in comparison to the previous prompt,
    we have written this prompt in such a way that it has committed the LLM to a specific
    clear syntax (numbers followed by a quoted string). This makes it much easier
    to parse the output after generation.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个传统提示的示例，它使用结构提示来鼓励使用特定的输出格式。提示旨在生成一个易于解析的5项列表。请注意，与之前的提示相比，我们编写了这个提示，使得LLM承诺使用特定的明确语法（数字后跟引号字符串）。这使得生成后的输出更容易解析。
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/fe034d617f6cacdc6e02272c0d8a869c.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fe034d617f6cacdc6e02272c0d8a869c.png)'
- en: 'Note that the LLM follows the syntax correctly, but does not stop after generating
    5 items. We can fix this by creating a clear stopping criteria, e.g. asking for
    6 items and stopping when we see the start of the sixth item (so we end up with
    five):'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，LLM正确地遵循了语法，但在生成5项之后没有停止。我们可以通过创建明确的停止标准来解决这个问题，例如，要求生成6项，当看到第六项的开始时停止（这样我们就能得到五项）：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/a5a63e9d54cf5c5824316e5e74fc7fec.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a5a63e9d54cf5c5824316e5e74fc7fec.png)'
- en: '**Enforcing syntax with a guidance program**'
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**使用指导程序强制语法**'
- en: Rather than using *hints*, a Guidance program *enforces* a specific output format,
    inserting the tokens that are part of the structure rather than getting the LLM
    to generate them.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用*提示*不同，Guidance程序*强制*特定的输出格式，插入属于结构的一部分的标记，而不是让LLM生成它们。
- en: 'For example, this is what we would do if we wanted to enforce a numbered list
    as a format:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这就是我们如何在需要强制编号列表作为格式时操作：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/24ec248c9af9283ac431a453774896cc.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/24ec248c9af9283ac431a453774896cc.png)'
- en: In the above prompt the `lm2 = lm + …` command saves the new model state that
    results from adding a string to the starting `lm` state into the variable `lm2`.
    The `for` loop then iteratively updates `lm2` by adding a mixture of strings and
    generated sequences. Note that the structure (the numbers, and quotes) are *not*
    generated by the LLM.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的提示中，`lm2 = lm + …` 命令将添加一个字符串到起始 `lm` 状态后产生的新模型状态保存到变量 `lm2` 中。然后 `for`
    循环通过添加混合字符串和生成序列来迭代更新 `lm2`。请注意，结构（包括数字和引号）*不是*由 LLM 生成的。
- en: 'Output parsing is done automatically by the `guidance` program, so we don’t
    need to worry about it. In this case, the `commands` variable will be the list
    of generated command names:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 输出解析由 `guidance` 程序自动完成，所以我们不需要担心这个问题。在这种情况下，`commands` 变量将是生成的命令名称列表：
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/3a5fc333af8e650f1669402eaf801e78.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3a5fc333af8e650f1669402eaf801e78.png)'
- en: '**Forcing valid JSON syntax:** Using `guidance` we can create any syntax we
    want with absolute confidence that what we generate will exactly follow the format
    we specify. This is particularly useful for things like JSON:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**强制有效的 JSON 语法：** 使用 `guidance` 我们可以以绝对的信心创建任何我们想要的语法，确保生成的内容完全遵循我们指定的格式。这对于
    JSON 等内容特别有用：'
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/c685b4d1950771ec7bca47d112ff533e.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c685b4d1950771ec7bca47d112ff533e.png)'
- en: '**Guidance acceleration:** Another benefit of `guidance` programs is speed
    — incremental generation is actually faster than a single generation of the entire
    list, because the LLM does not have to generate the syntax tokens for the list
    itself, only the actual command names (this makes more of a difference when the
    output structure is richer).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**Guidance 加速：** `guidance` 程序的另一个好处是速度——增量生成实际上比一次性生成整个列表要快，因为 LLM 不需要为列表本身生成语法标记，只需生成实际的命令名称（当输出结构更丰富时，这种差异更为明显）。'
- en: If you are using a model endpoint that does not support such [acceleration](https://github.com/guidance-ai/guidance/blob/main/notebooks/guidance_acceleration.ipynb)
    (e.g. OpenAI models), then many incremental API calls will slow you down, so `guidance`
    uses a single running stream (see details below when we demo chat models).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的模型端点不支持这种[加速](https://github.com/guidance-ai/guidance/blob/main/notebooks/guidance_acceleration.ipynb)（例如
    OpenAI 模型），那么许多增量的 API 调用会让你变慢，因此 `guidance` 使用一个单独的运行流（请参见下面我们演示聊天模型的详细信息）。
- en: '**Clear syntax gives the user more power**'
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**清晰的语法赋予用户更多控制权**'
- en: 'Getting stuck in a low-diversity rut is a common failure mode of LLMs, which
    can happen even if we use a relatively high temperature:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 陷入低多样性的困境是大型语言模型（LLMs）常见的失败模式，即使我们使用相对较高的温度也可能发生：
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/7d040cda0ddafe6e5afb630705db5aad.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7d040cda0ddafe6e5afb630705db5aad.png)'
- en: 'When generating a list of items previous items in the list influence future
    items. This can lead to unhelpful biases or trends in what gets generated. One
    possible fix to this problem is asking for parallel completions (so that prior
    generated commands do not influence the next command generation):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成项目列表时，列表中的前面项目会影响未来的项目。这可能会导致生成内容中的无用偏差或趋势。解决这个问题的一种可能方法是请求并行完成（以便之前生成的命令不会影响下一条命令的生成）：
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/09878da99e9a21ee5f36152eabbaa0dd.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09878da99e9a21ee5f36152eabbaa0dd.png)'
- en: We still get some repetition, but much less than before. Also, since clear structure
    gives us outputs that are easy to parse and manipulate, we can easily take the
    output, remove duplicates, and use them in the next step of our program.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然会有一些重复，但比之前少得多。此外，由于清晰的结构使我们可以轻松解析和操作输出，我们可以轻松地提取输出，去除重复项，并在程序的下一步中使用它们。
- en: 'Here is an example program that takes the listed commands, picks one, and does
    further operations on it:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个示例程序，它接受列出的命令，挑选其中一个，并对其进行进一步操作：
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/d4cc838a677258ddc73f16a75de1e3d5.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d4cc838a677258ddc73f16a75de1e3d5.png)'
- en: 'We introduced one import control method in the above program: the `regex` parameter
    for generation. The command `gen(''coolness'', regex=''[0–9]+'')` uses a regular
    expression to enforce a certain syntax on the output (i.e. forcing the output
    to match an arbitrary regular experession). In this case we force the coolness
    score to be a whole number (note that generation stops once the model has completed
    generation of the pattern and starts to generate something else).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上述程序中引入了一种导入控制方法：生成的 `regex` 参数。命令 `gen('coolness', regex='[0–9]+')` 使用正则表达式对输出强制执行某种语法（即强制输出匹配任意正则表达式）。在这种情况下，我们强制
    coolness 分数为一个整数（注意一旦模型完成模式生成并开始生成其他内容，生成过程将停止）。
- en: '**Combining clear syntax with model-specific structure like chat**'
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**将清晰的语法与特定模型的结构（如聊天）结合**'
- en: All the examples above used a base model without any later fine-tuning. But
    if the model you are using has fine tuning, it is important to combine clear syntax
    with the structure that has been tuned into the model.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 上述所有示例都使用了基础模型，而没有经过后续的微调。但如果你使用的模型有微调，重要的是将清晰的语法与已经调整到模型中的结构结合起来。
- en: For example, chat models have been fine tuned to expect several “role” tags
    in the prompt. We can leverage these tags to further enhance the structure of
    our programs/prompts.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，聊天模型已经过微调，以期望在提示中包含多个“角色”标签。我们可以利用这些标签进一步增强程序/提示的结构。
- en: The following example adapts the above prompt for use with a chat based model.
    `guidance` has special role tags (like `user()`), which allow you to mark out
    various roles and get them automatically translated into the right special tokens
    or API calls for the LLM you are using. This helps make prompts easier to read
    and makes them more general across different chat models.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例将上述提示调整为适用于基于聊天的模型。`guidance` 有特殊的角色标签（如 `user()`），这些标签允许你标记不同的角色，并自动将其转换为你使用的
    LLM 所需的正确特殊令牌或 API 调用。这有助于使提示更易读，并使其在不同的聊天模型中更加通用。
- en: '[PRE10]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/3e1ab452e5003faf896e165d9858da32.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e1ab452e5003faf896e165d9858da32.png)'
- en: Output as it appears in a notebook.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 以笔记本中显示的形式输出。
- en: '**Using API-restricted models**'
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**使用 API 限制模型**'
- en: When we have control over generation, we can guide the output at any step of
    the process. But some model endpoints (e.g. OpenAI’s ChatGPT) currently have a
    much more limited API, e.g. we can’t control what happens inside each `role` block.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们控制生成时，我们可以在过程的任何步骤中引导输出。但一些模型端点（例如 OpenAI 的 ChatGPT）目前有更有限的 API，例如，我们不能控制每个
    `role` 块内部发生的事情。
- en: 'While this limits the user’s power, we can still use a subset of syntax hints,
    and enforce the structure outside of the role blocks:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这限制了用户的能力，但我们仍然可以使用语法提示的子集，并在角色块之外强制执行结构：
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/a93792841ec7832e19c368d6e6d9a79a.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a93792841ec7832e19c368d6e6d9a79a.png)'
- en: '**Summary**'
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**总结**'
- en: Whenever you are building a prompt to control a model it is important to consider
    not only the content of the prompt, but also the `syntax`.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 每当你构建一个控制模型的提示时，重要的是要考虑提示的内容以及 `syntax`。
- en: Clear syntax makes it easier to parse the output, helps the LLM produce output
    that matches your intent, and lets you write complex multi-step programs.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 清晰的语法使解析输出变得更容易，有助于 LLM 生成符合你意图的输出，并且让你能够编写复杂的多步骤程序。
- en: While even a trivial example (listing common OS commands) benefits from clear
    syntax, most tasks are much more complex, and benefit even more. We hope this
    post gives you some ideas on how to use clear syntax to improve your prompts.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是一个微不足道的示例（列出常见的操作系统命令）也会从清晰的语法中受益，更复杂的任务受益更多。我们希望这篇文章能给你一些如何利用清晰的语法来改进提示的想法。
- en: Also, make sure to check out `[guidance](https://github.com/guidance-ai/guidance)`.
    You certainly don’t need it to write prompts with clear syntax, but we think it
    makes it *much easier* to do so.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，请务必查看一下 `[guidance](https://github.com/guidance-ai/guidance)`。虽然你不一定需要它来编写具有清晰语法的提示，但我们认为它可以让你做到这一点变得*容易得多*。
