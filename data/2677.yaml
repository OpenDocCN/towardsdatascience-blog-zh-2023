- en: An Accessible Derivation of Linear Regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性回归的易懂推导
- en: 原文：[https://towardsdatascience.com/an-accessible-derivation-of-linear-regression-69fa6aefbd93?source=collection_archive---------8-----------------------#2023-08-23](https://towardsdatascience.com/an-accessible-derivation-of-linear-regression-69fa6aefbd93?source=collection_archive---------8-----------------------#2023-08-23)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/an-accessible-derivation-of-linear-regression-69fa6aefbd93?source=collection_archive---------8-----------------------#2023-08-23](https://towardsdatascience.com/an-accessible-derivation-of-linear-regression-69fa6aefbd93?source=collection_archive---------8-----------------------#2023-08-23)
- en: The math behind the model, from additive assumptions to pseudoinverse matrices
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型背后的数学，从加法假设到伪逆矩阵
- en: '[](https://william-caicedo.medium.com/?source=post_page-----69fa6aefbd93--------------------------------)[![William
    Caicedo-Torres, PhD](../Images/06123561570e6a998e8d1c47808fc6e2.png)](https://william-caicedo.medium.com/?source=post_page-----69fa6aefbd93--------------------------------)[](https://towardsdatascience.com/?source=post_page-----69fa6aefbd93--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----69fa6aefbd93--------------------------------)
    [William Caicedo-Torres, PhD](https://william-caicedo.medium.com/?source=post_page-----69fa6aefbd93--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://william-caicedo.medium.com/?source=post_page-----69fa6aefbd93--------------------------------)[![威廉·凯塞多-托雷斯博士](../Images/06123561570e6a998e8d1c47808fc6e2.png)](https://william-caicedo.medium.com/?source=post_page-----69fa6aefbd93--------------------------------)[](https://towardsdatascience.com/?source=post_page-----69fa6aefbd93--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----69fa6aefbd93--------------------------------)
    [威廉·凯塞多-托雷斯博士](https://william-caicedo.medium.com/?source=post_page-----69fa6aefbd93--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7a925ed40bb4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-accessible-derivation-of-linear-regression-69fa6aefbd93&user=William+Caicedo-Torres%2C+PhD&userId=7a925ed40bb4&source=post_page-7a925ed40bb4----69fa6aefbd93---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----69fa6aefbd93--------------------------------)
    ·9 min read·Aug 23, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F69fa6aefbd93&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-accessible-derivation-of-linear-regression-69fa6aefbd93&user=William+Caicedo-Torres%2C+PhD&userId=7a925ed40bb4&source=-----69fa6aefbd93---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7a925ed40bb4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-accessible-derivation-of-linear-regression-69fa6aefbd93&user=William+Caicedo-Torres%2C+PhD&userId=7a925ed40bb4&source=post_page-7a925ed40bb4----69fa6aefbd93---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----69fa6aefbd93--------------------------------)
    ·9分钟阅读·2023年8月23日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F69fa6aefbd93&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-accessible-derivation-of-linear-regression-69fa6aefbd93&user=William+Caicedo-Torres%2C+PhD&userId=7a925ed40bb4&source=-----69fa6aefbd93---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F69fa6aefbd93&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-accessible-derivation-of-linear-regression-69fa6aefbd93&source=-----69fa6aefbd93---------------------bookmark_footer-----------)![](../Images/e5a0163271c749e55718fb68f4f0f088.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F69fa6aefbd93&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fan-accessible-derivation-of-linear-regression-69fa6aefbd93&source=-----69fa6aefbd93---------------------bookmark_footer-----------)![](../Images/e5a0163271c749e55718fb68f4f0f088.png)'
- en: Photo by [Saad Ahmad](https://unsplash.com/@saadahmad_umn?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Saad Ahmad](https://unsplash.com/@saadahmad_umn?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '***Technical disclaimer****: It is possible to derive a model without normality
    assumptions. We’ll go down this route because it’s straightforward enough to understand
    and by assuming normality of the model’s output, we can reason about the uncertainty
    of our predictions.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '***技术免责声明***：可以在没有正态性假设的情况下推导出模型。我们选择这个路径是因为它足够简单易懂，并且通过假设模型输出的正态性，我们可以推理预测的不确定性。'
- en: This post is intended for people who are already aware of what linear regression
    is (and maybe have used it once or twice) and want a more principled understanding
    of the math behind it.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文面向那些已经了解线性回归是什么（并可能使用过一两次）并希望深入了解其背后数学原理的人。
- en: 'Some background in basic probability (probability distributions, joint probability,
    mutually exclusive events), linear algebra, and stats is probably required to
    make the most of what follows. Without further ado, here we go:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 需要一些基本概率（概率分布、联合概率、互斥事件）、线性代数和统计学的背景知识，以充分理解接下来的内容。废话不多说，我们开始吧：
- en: 'The machine learning world is full of amazing connections: the exponential
    family, regularization and prior beliefs, KNN and SVMs, Maximum Likelihood and
    Information Theory — it’s all connected! (I love [Dark](https://www.themoviedb.org/tv/70523-dark)).
    This time we’ll discuss how to derive another one of the members of the exponential
    family: the Linear Regression model, and in the process we’ll see that the Mean
    Squared Error loss is theoretically well motivated. As with any regression model,
    we’ll be able to use it to predict numerical, continuous targets. It’s a simple
    yet powerful model that happens to be one of the workhorses of statistical inference
    and experimental design. However we will be concerned only with its usage as a
    predictive tool. No pesky inference (and God forbid, causal) stuff here.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习世界充满了惊人的联系：指数族、正则化和先验信念、KNN 和 SVM、最大似然和信息理论——这一切都连接在一起！（我喜欢[黑暗](https://www.themoviedb.org/tv/70523-dark)）。这次我们将讨论如何推导出指数族的另一个成员：线性回归模型，在这个过程中，我们将看到均方误差损失在理论上是有良好动机的。与任何回归模型一样，我们将能够用它来预测数值型连续目标。它是一个简单却强大的模型，恰好是统计推断和实验设计中的一个重要工具。然而，我们将仅关注其作为预测工具的使用。这里没有烦人的推断（更别说因果）问题。
- en: 'Alright, let us begin. We want to predict something based on something else.
    We’ll call the ***predicted*** thing y and the ***something******else*** x. As
    a concrete example, I offer the following toy situation: You are a credit analyst
    working in a bank and you’re interested in automatically finding out the right
    credit limit for a bank customer. You also happen to have a dataset pertaining
    to past clients and what credit limit (the ***predicted*** thing) was approved
    for them, together with some of their features such as demographic info, past
    credit performance, income, etc. (the ***something else***).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我们开始。我们想根据其他东西来预测某物。我们将把***预测的***东西叫做y，将***其他东西***叫做x。作为一个具体的例子，我提供以下玩具情境：你是一名在银行工作的信用分析师，你希望自动找出银行客户的正确信用额度。你也恰好拥有一个关于过去客户的数据库，其中包含了为他们批准的信用额度（即***预测的***东西），以及一些他们的特征，如人口信息、过去的信用表现、收入等（即***其他东西***）。
- en: 'So we have a great idea and write down a model that explains the credit limit
    in terms of those features available to you, with the model’s main assumption
    being that each feature contributes something to the observed output in an additive
    manner. Since the credit stuff was just a motivating (and contrived) example,
    let’s go back to our pure math world of spherical cows, with our model turning
    into something like this:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有一个很好的想法，写下一个模型，该模型根据你拥有的特征来解释信用额度，模型的主要假设是每个特征以加法方式对观察到的输出有所贡献。由于信用额度只是一个激励性（和人为设计的）例子，我们回到纯数学的球形牛世界中，我们的模型变成了如下形式：
- en: '![](../Images/48fafb4f29622acec9fdb9e608c789db.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/48fafb4f29622acec9fdb9e608c789db.png)'
- en: 'We still have the predicted stuff (y) and the something else we use to predict
    it (x). We concede that some sort of noise is unavoidable (be it by virtue of
    imperfect measuring or our own blindness) and the best we can do is to assume
    that the model behind the data we observe is stochastic. The consequence of this
    is that we might see slightly different outputs for the same input, so instead
    of neat point estimates we are “stuck” with a probability distribution over the
    outputs (y) conditioned on the inputs (x):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然有预测的东西（y）和用于预测的其他东西（x）。我们承认某种噪声是不可避免的（无论是由于测量不完美还是我们的盲点），我们能做的最好是假设我们观察到的数据背后的模型是随机的。其结果是我们可能会看到相同输入的略微不同的输出，因此我们不再是精确的点估计，而是“困在”以输入（x）为条件的输出（y）的概率分布中：
- en: '![](../Images/524edb479fcf4dda6233d7a024390d13.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/524edb479fcf4dda6233d7a024390d13.png)'
- en: Every data point in y is replaced by a little bell curve, whose mean lies in
    the observed values of y, and has some variance which we don’t care about at the
    moment. Then our little model will take the place of the distribution mean.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: y 中的每个数据点都被一个小的钟形曲线替代，其均值位于观察到的 y 值中，并且具有一些我们目前不关心的方差。然后，我们的小模型将取代分布均值的位置。
- en: Assuming all those bell curves are actually normal distributions and their means
    (data points in y) are independent from each other, the (joint) probability of
    observing the dataset is
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 假设所有这些钟形曲线实际上是正态分布，并且它们的均值（y 中的数据点）彼此独立，则观察到数据集的（联合）概率为
- en: '![](../Images/ab47a367efd762543f8daa2199a634c8.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ab47a367efd762543f8daa2199a634c8.png)'
- en: 'Logarithms and some algebra to the rescue:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对数和一些代数来拯救我们：
- en: '![](../Images/b3e255bf5869558a0fb27f75dcec3f5d.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3e255bf5869558a0fb27f75dcec3f5d.png)'
- en: 'Logarithms are cool, aren’t they? Logs transform multiplication into sum, division
    into subtraction, and powers into multiplication. Quite handy from both algebraic
    and numerical standpoints. Getting rid of constant stuff, which is irrelevant
    in this case, we arrive to the following maximum likelihood problem:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对数是很酷的，不是吗？对数将乘法转化为加法，除法转化为减法，幂运算转化为乘法。从代数和数值的角度来看都非常方便。去掉与此无关的常数项，我们得到以下最大似然问题：
- en: '![](../Images/9e917197b615317260addf6c89c1dd5d.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e917197b615317260addf6c89c1dd5d.png)'
- en: Well, that’s the same as
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，这等于
- en: '![](../Images/c32bb978408caf4f0cae950f0144b883.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c32bb978408caf4f0cae950f0144b883.png)'
- en: The expression we are about to minimize is something very close to the famous
    **Mean Square Error** loss. In fact, for optimization purposes they’re equivalent.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们即将最小化的表达式非常接近著名的 **均方误差** 损失。实际上，在优化目的上它们是等价的。
- en: So what now? This minimization problem can be solved exactly using derivatives.
    We’ll take advantage of the fact that the loss is quadratic, which means convex,
    which means one global minima; allowing us to take its derivative, set it to zero
    and solve for theta. Doing this we’ll find the value of the parameters theta that
    makes the derivative of the loss zero. And why? because it is precisely at the
    point where the derivative is zero, that the loss is at its minimum.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 那么接下来呢？这个最小化问题可以通过求导精确解决。我们将利用损失是二次的这一事实，这意味着它是凸的，即有一个全局最小值；这允许我们对其进行求导，设置为零并求解
    theta。这样我们将找到使损失的导数为零的参数 theta 的值。为什么？因为正是导数为零的点，损失达到其最小值。
- en: 'To make everything somewhat simpler, let’s express the loss in vector notation:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让一切变得更简单一些，让我们用向量表示法来表达损失：
- en: '![](../Images/7e9f4d49f8aaa407c62dc74d2ccc4667.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e9f4d49f8aaa407c62dc74d2ccc4667.png)'
- en: Here, X is an *NxM* matrix representing our whole dataset of N examples and
    M features and y is a vector containing the expected responses per training example.
    Taking the derivative and setting it to zero we get
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，X 是一个 *NxM* 矩阵，表示我们的整个数据集，其中包含 N 个示例和 M 个特征，而 y 是一个包含每个训练示例期望响应的向量。对其进行求导并将其设置为零，我们得到
- en: '![](../Images/287c5728c06bbd5fbe61a029794a0c3f.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/287c5728c06bbd5fbe61a029794a0c3f.png)'
- en: There you have it, the solution to the optimization problem we have cast our
    original machine learning problem into. If you go ahead and plug those parameter
    values into your model, you’ll have a trained ML model ready to be evaluated using
    some holdout dataset (or maybe through cross-validation).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们将原始机器学习问题转化为的优化问题的解决方案。如果你继续将这些参数值代入你的模型，你将拥有一个经过训练的机器学习模型，准备使用一些保留数据集（或通过交叉验证）进行评估。
- en: If you think that final expression looks an awful lot like the solution of a
    linear system,
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你觉得最终的表达式看起来非常像线性系统的解，
- en: '![](../Images/e60393dc618d09b9412c625b72840dde.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e60393dc618d09b9412c625b72840dde.png)'
- en: it’s because it does. The extra stuff comes from the fact that for our problem
    to be equivalent to a vanilla linear system, we’d need an equal number of features
    and training examples so we can invert X. Since that’s seldom the case we can
    only hope for a “best fit” solution — in some sense of best — resorting to the
    Moore-Penrose Pseudoinverse of X, which is a generalization of the good ol’ inverse
    matrix. The associated [*wikipedia*](https://en.wikipedia.org/wiki/Moore–Penrose_inverse#Definition)
    entry makes for a fun reading.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 正是因为它确实如此。额外的内容来自于这样一个事实：为了使我们的问题等同于一个普通的线性系统，我们需要相等数量的特征和训练样本，以便我们可以反转 X。由于这种情况很少出现，我们只能希望得到一个“最佳拟合”解决方案——在某种意义上的最佳——借助
    X 的 Moore-Penrose 伪逆，这是一种广义的逆矩阵。相关的[*wikipedia*](https://en.wikipedia.org/wiki/Moore–Penrose_inverse#Definition)
    条目非常有趣。
- en: A Small Incursion into Regularization
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对正则化的小探讨
- en: 'From now on I’ll assume you may have heard the term “regularization” somewhere
    and maybe you know what it means. In any case I’ll tell you how I understand it:
    regularization is anything you do to a model that makes its out-of-sample performance
    better at the expense of a worse in-sample performance. Why is this even a thing?
    because in the end, out of sample (i.e. real life) performance is what matters
    — no one is gonna care how low your training error is, or how amazing your AUC
    is on the training set unless your model performs comparatively well in production.
    And we have found that if we are not careful we will run into something called
    overfitting, where your model memorizes the training set all the way down to the
    noise, becoming potentially useless for out of sample predictions. We also know
    that there is a trade-off between model complexity and out of sample performance
    (Vapnik’s statistical learning theory) and regularization can be seen as an attempt
    to keep model complexity under control in the hope of achieving better generalization
    abilities (here generalization is another name for out of sample performance).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始，我假设你可能听说过“正则化”这个术语，也许你知道它的意思。无论如何，我会告诉你我对它的理解：正则化是你对模型所做的任何事情，以使其在样本外的表现更好，虽然这可能会牺牲样本内的表现。为什么这是一个问题？因为最终，样本外（即实际生活中的）表现才是重要的——没有人会关心你的训练误差有多低，或者你的
    AUC 在训练集上有多么惊人，除非你的模型在实际应用中表现得相对较好。而且我们发现，如果我们不小心，会遇到所谓的过拟合问题，在这种情况下，你的模型会记住训练集直到噪声，可能对样本外预测无用。我们还知道模型复杂性与样本外表现之间存在权衡（Vapnik
    的统计学习理论），正则化可以看作是试图控制模型复杂性，希望实现更好的泛化能力（这里的泛化是样本外表现的另一种说法）。
- en: 'Regularization can take many forms, but for the purposes of this article we’ll
    only discuss one approach that is often used for linear regression models. Remember
    I said that with regularization we are trying to keep complexity under control,
    right? Well, unwanted complexity can creep in mainly via two ways: 1\. too many
    input features and 2\. large values for their corresponding theta values. It turns
    out that something called L2 regularization can help with both. Remember our loss?'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化可以采取多种形式，但本文仅讨论一种常用于线性回归模型的方法。记住我说过，通过正则化我们是为了控制复杂性，对吗？好吧，不需要的复杂性主要通过两种方式潜入：1.
    输入特征过多和 2. 对应的 theta 值过大。事实证明，所谓的 L2 正则化可以帮助解决这两个问题。记住我们的损失吗？
- en: '![](../Images/89c6726caace2f5db0163487734d7b21.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89c6726caace2f5db0163487734d7b21.png)'
- en: 'we were only concerned to minimize the squared differences between predicted
    and observed values of y, without regard for anything else. Such myopic focus
    could be a problem because you could in principle game the system and keep adding
    in features till the error is minuscule, even if those extra features have nothing
    to do with the signal we are trying to learn. If we had perfect knowledge about
    the things that are relevant for our problem, we wouldn’t include useless features!
    (to be fair we wouldn’t be using machine learning on the first place but i digress).
    To compensate for our ignorance and mitigate the risk of introducing said unwanted
    complexity, we’ll add an extra term to the loss:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前只关心最小化预测值和观察值 y 之间的平方差，而不考虑其他任何因素。这种狭隘的关注可能会成为问题，因为原则上你可以利用系统，不断添加特征，直到误差微乎其微，即使这些额外的特征与我们尝试学习的信号无关。如果我们对与问题相关的事物有完美的了解，我们就不会包含无用的特征！（公平地说，我们本来不会使用机器学习，但我扯远了）。为了弥补我们的无知并减少引入不必要复杂性的风险，我们将向损失中添加一个额外的项：
- en: '![](../Images/0060ad42cb1aafd84f30553538c2a73d.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0060ad42cb1aafd84f30553538c2a73d.png)'
- en: 'What does that do, you might ask. I’ll tell you — we’ve just introduced an
    incentive to keep the magnitude of the parameters as close to zero as possible.
    In other words we want to keep complexity at a minimum. Now instead of having
    just one objective (the minimization of prediction errors), we’ve got one more
    (minimization of complexity), and the solution for the overall problem will have
    to balance both. We do have a say in how that balance looks like, through that
    shiny new lambda coefficient: the larger it is, the more importance we give to
    the low complexity target.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这有什么作用，你可能会问。我告诉你——我们刚刚引入了一个激励，以使参数的幅度尽可能接近零。换句话说，我们希望保持复杂性在最小化。现在，我们不再只有一个目标（最小化预测误差），我们多了一个（最小化复杂性），解决整体问题的方案将需要平衡这两个目标。通过那个闪亮的新lambda系数，我们对这种平衡有一定的发言权：它越大，我们就越重视低复杂性的目标。
- en: That’s a lot of talk. Let’s see what the extra term does to the optimal theta.
    The new loss in vectorized form is
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 说了这么多。让我们看看额外的项对最优theta的影响。新的向量化形式的损失是
- en: '![](../Images/9b7bcdcf0721cc6a4002890b98aa5a45.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b7bcdcf0721cc6a4002890b98aa5a45.png)'
- en: 'taking its derivative, setting it to zero and solving for theta:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 计算其导数，将其设置为零并求解theta：
- en: '![](../Images/63ad7a2d5ae456ee7c203fe6acc9a825.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63ad7a2d5ae456ee7c203fe6acc9a825.png)'
- en: 'It turns out that the new solution is not too different from the previous one.
    We’ve got an extra diagonal matrix which is going to take care of any rank issues
    that would prevent us from inverting the XTX matrix, by adding some “jitter” into
    the mix. That illuminates another way in which unnecessary complexity might be
    introduced: collinear features, that is, features that are linear transformations
    (scaled and shifted versions) of other features. As far as linear regression cares,
    two collinear features are the same feature, count as only one column, and the
    extra feature doesn’t add any new information to help train our model; rendering
    the XTX matrix rank-deficient, i.e. impossible to invert. With regularization,
    in every row of XTX, a different column is affected by lambda, the jitter — stopping
    any possible collinearity and making it full-rank. We see that adding random crap
    as predictors could actually ruin the whole learning process. Thankfully regularization
    is here to save the day (please don’t misconstrue my praise of regularization
    as an invitation to be careless when selecting your predictors — it’s not).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，新解决方案与之前的方案并没有太大区别。我们多了一个额外的对角矩阵，它将处理任何可能导致我们无法反转XTX矩阵的秩问题，通过在混合中加入一些“抖动”。这揭示了另一种可能引入不必要复杂性的方式：共线特征，即线性变换（缩放和移动版本）的特征。就线性回归而言，两个共线特征实际上是同一个特征，只算作一列，额外的特征不会为训练模型提供任何新的信息；这使得XTX矩阵的秩不足，即无法反转。通过正则化，在XTX的每一行中，lambda（抖动）会影响不同的列——从而阻止任何可能的共线性，并使其变为满秩。我们看到，添加随机变量作为预测器实际上可能破坏整个学习过程。幸运的是，正则化可以拯救这一切（请不要误解我对正则化的赞美为在选择预测器时可以粗心大意——绝不是这样）。
- en: So there you have it, a regularized linear regression model ready to use. Where
    do we go from here? well, you could expand on the idea that regularization encodes
    a prior belief about the parameter space, revealing its connection with Bayesian
    inference. Or you could double down on its connection with Support Vector Machines
    and eventually with non-parametric models like KNN. Also, you could ponder about
    the fact that all those fancy gradients from linear and logistic regression look
    suspiciously similar. Rest assured I’ll be touching on those subjects in a future
    post!
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，就这样，一个正则化的线性回归模型准备好了。接下来我们怎么做？嗯，你可以扩展正则化编码了关于参数空间的先验信念的想法，揭示其与贝叶斯推断的联系。或者你可以深入研究其与支持向量机的联系，并最终与像KNN这样的非参数模型的联系。同时，你还可以思考线性和逻辑回归中的那些华丽梯度看起来非常相似的事实。请放心，我将在未来的文章中触及这些主题！
