- en: Say Once! Repeating Words Is Not Helping AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 说一次！重复词汇并没有帮助人工智能
- en: 原文：[https://towardsdatascience.com/say-once-repeating-words-is-not-helping-ai-58f38035f66e?source=collection_archive---------9-----------------------#2023-06-20](https://towardsdatascience.com/say-once-repeating-words-is-not-helping-ai-58f38035f66e?source=collection_archive---------9-----------------------#2023-06-20)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/say-once-repeating-words-is-not-helping-ai-58f38035f66e?source=collection_archive---------9-----------------------#2023-06-20](https://towardsdatascience.com/say-once-repeating-words-is-not-helping-ai-58f38035f66e?source=collection_archive---------9-----------------------#2023-06-20)
- en: '| ARTIFICIAL INTELLIGENCE | NLP | LLMs'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '| 人工智能 | 自然语言处理 | 大型语言模型'
- en: How and why is repeating tokens harming LLMs? Why is this a problem?
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么重复令牌对LLMs（大型语言模型）有害？这是什么问题？
- en: '[](https://salvatore-raieli.medium.com/?source=post_page-----58f38035f66e--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----58f38035f66e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----58f38035f66e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----58f38035f66e--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----58f38035f66e--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://salvatore-raieli.medium.com/?source=post_page-----58f38035f66e--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----58f38035f66e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----58f38035f66e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----58f38035f66e--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----58f38035f66e--------------------------------)'
- en: ·
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff1a08d9452cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsay-once-repeating-words-is-not-helping-ai-58f38035f66e&user=Salvatore+Raieli&userId=f1a08d9452cd&source=post_page-f1a08d9452cd----58f38035f66e---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----58f38035f66e--------------------------------)
    ·14 min read·Jun 20, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F58f38035f66e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsay-once-repeating-words-is-not-helping-ai-58f38035f66e&user=Salvatore+Raieli&userId=f1a08d9452cd&source=-----58f38035f66e---------------------clap_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff1a08d9452cd&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsay-once-repeating-words-is-not-helping-ai-58f38035f66e&user=Salvatore+Raieli&userId=f1a08d9452cd&source=post_page-f1a08d9452cd----58f38035f66e---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----58f38035f66e--------------------------------)
    ·14分钟阅读·2023年6月20日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F58f38035f66e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsay-once-repeating-words-is-not-helping-ai-58f38035f66e&user=Salvatore+Raieli&userId=f1a08d9452cd&source=-----58f38035f66e---------------------clap_footer-----------)'
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F58f38035f66e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsay-once-repeating-words-is-not-helping-ai-58f38035f66e&source=-----58f38035f66e---------------------bookmark_footer-----------)![](../Images/40a23dee61acd2fe912d9497b6a07d5f.png)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F58f38035f66e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsay-once-repeating-words-is-not-helping-ai-58f38035f66e&source=-----58f38035f66e---------------------bookmark_footer-----------)![](../Images/40a23dee61acd2fe912d9497b6a07d5f.png)'
- en: image by [Kristina Flour](https://unsplash.com/it/@tinaflour) on Unsplash
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Kristina Flour](https://unsplash.com/it/@tinaflour) 在 Unsplash
- en: '[Large Language Models (LLMs)](https://en.wikipedia.org/wiki/Large_language_model)
    have shown their capabilities and have taken the world by storm. Every big company
    now has a model with a fancy name. But, under the hood, they are all [transformers](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)).
    **Everyone dreams of the trillion parameters, but is there no limit?**'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[大型语言模型（LLMs）](https://en.wikipedia.org/wiki/Large_language_model)已经展示了它们的能力，并在全球范围内引起了轰动。现在每个大公司都有一个花哨的模型名字。但在幕后，它们都是[变压器模型](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))。**每个人都梦想拥有万亿参数，但难道没有限制吗？**'
- en: 'In this article, we discuss that:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们讨论了：
- en: Is it guaranteed that a bigger model has better performance than a small model?
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更大的模型是否保证性能优于小模型？
- en: Do we have the data for huge models?
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们是否拥有巨型模型的数据？
- en: What happens if instead of collecting new data you use the data again?
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你不收集新的数据，而是重复使用已有的数据，会发生什么？
- en: 'Scaling over the sky: what is hurting the wing?'
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在天空中的扩展：是什么导致了机翼的损伤？
- en: '![](../Images/4c30898f3c8683fc888bf2424f1a20f8.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c30898f3c8683fc888bf2424f1a20f8.png)'
- en: Image by [Sean Pollock](https://unsplash.com/it/@seanpollock) on Unsplash
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Sean Pollock](https://unsplash.com/it/@seanpollock) 提供，来自 Unsplash
- en: '[OpenAI has defined the scaling law](https://arxiv.org/abs/2001.08361), stating
    that model performance follows a power law according to how many parameters are
    used and the number of data points. This along with the search for emergent properties
    has created the parameter race: **the bigger the model, the better.**'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[OpenAI 已经定义了扩展法则](https://arxiv.org/abs/2001.08361)，该法则指出，模型性能遵循一个幂律，具体取决于使用的参数数量和数据点的数量。这与对新兴属性的探索一起，形成了参数竞赛：**模型越大，效果越好。**'
