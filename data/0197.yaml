- en: 'Creating Custom Loss Functions in TensorFlow: Understanding the Theory and
    Practicalities'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在TensorFlow中创建自定义损失函数：理解理论与实际应用
- en: 原文：[https://towardsdatascience.com/creating-custom-loss-functions-in-tensorflow-understanding-the-theory-and-practicalities-383a19e387d6?source=collection_archive---------7-----------------------#2023-01-12](https://towardsdatascience.com/creating-custom-loss-functions-in-tensorflow-understanding-the-theory-and-practicalities-383a19e387d6?source=collection_archive---------7-----------------------#2023-01-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/creating-custom-loss-functions-in-tensorflow-understanding-the-theory-and-practicalities-383a19e387d6?source=collection_archive---------7-----------------------#2023-01-12](https://towardsdatascience.com/creating-custom-loss-functions-in-tensorflow-understanding-the-theory-and-practicalities-383a19e387d6?source=collection_archive---------7-----------------------#2023-01-12)
- en: Maximizing Model Performance with Custom Loss Functions in TensorFlow
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用自定义损失函数最大化TensorFlow中的模型性能
- en: '[](https://marcosanguineti.medium.com/?source=post_page-----383a19e387d6--------------------------------)[![Marco
    Sanguineti](../Images/9c426e512b31b77734801912d81f51c1.png)](https://marcosanguineti.medium.com/?source=post_page-----383a19e387d6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----383a19e387d6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----383a19e387d6--------------------------------)
    [Marco Sanguineti](https://marcosanguineti.medium.com/?source=post_page-----383a19e387d6--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://marcosanguineti.medium.com/?source=post_page-----383a19e387d6--------------------------------)[![Marco
    Sanguineti](../Images/9c426e512b31b77734801912d81f51c1.png)](https://marcosanguineti.medium.com/?source=post_page-----383a19e387d6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----383a19e387d6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----383a19e387d6--------------------------------)
    [Marco Sanguineti](https://marcosanguineti.medium.com/?source=post_page-----383a19e387d6--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F33141be0f14d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-custom-loss-functions-in-tensorflow-understanding-the-theory-and-practicalities-383a19e387d6&user=Marco+Sanguineti&userId=33141be0f14d&source=post_page-33141be0f14d----383a19e387d6---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----383a19e387d6--------------------------------)
    ·7 min read·Jan 12, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F383a19e387d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-custom-loss-functions-in-tensorflow-understanding-the-theory-and-practicalities-383a19e387d6&user=Marco+Sanguineti&userId=33141be0f14d&source=-----383a19e387d6---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F33141be0f14d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-custom-loss-functions-in-tensorflow-understanding-the-theory-and-practicalities-383a19e387d6&user=Marco+Sanguineti&userId=33141be0f14d&source=post_page-33141be0f14d----383a19e387d6---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----383a19e387d6--------------------------------)
    ·7分钟阅读·2023年1月12日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F383a19e387d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-custom-loss-functions-in-tensorflow-understanding-the-theory-and-practicalities-383a19e387d6&user=Marco+Sanguineti&userId=33141be0f14d&source=-----383a19e387d6---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F383a19e387d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-custom-loss-functions-in-tensorflow-understanding-the-theory-and-practicalities-383a19e387d6&source=-----383a19e387d6---------------------bookmark_footer-----------)![](../Images/ca25b8e66b5b042aedf727f806276652.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F383a19e387d6&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fcreating-custom-loss-functions-in-tensorflow-understanding-the-theory-and-practicalities-383a19e387d6&source=-----383a19e387d6---------------------bookmark_footer-----------)![](../Images/ca25b8e66b5b042aedf727f806276652.png)'
- en: Photo by [Fotis Fotopoulos](https://unsplash.com/@ffstop?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由[Fotis Fotopoulos](https://unsplash.com/@ffstop?utm_source=medium&utm_medium=referral)在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)拍摄的照片
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: In machine learning, the loss function is a crucial component of the training
    process. It measures the difference between the model’s predictions and the true
    output and is used to update the model’s parameters to minimize this difference.
    While many commonly used loss functions are built into TensorFlow, there may be
    situations where you need to define a custom loss function to better suit the
    specific requirements of your model.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，损失函数是训练过程中的一个关键组成部分。它衡量模型预测与真实输出之间的差异，并用于更新模型的参数，以最小化这一差异。虽然许多常用的损失函数都内置于TensorFlow中，但在某些情况下，你可能需要定义一个自定义损失函数，以更好地适应模型的具体要求。
- en: '[## Module: tf.keras.losses | TensorFlow v2.11.0'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 模块：tf.keras.losses | TensorFlow v2.11.0'
- en: Built-in loss functions.
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内置损失函数。
- en: www.tensorflow.org](https://www.tensorflow.org/api_docs/python/tf/keras/losses?source=post_page-----383a19e387d6--------------------------------)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: www.tensorflow.org](https://www.tensorflow.org/api_docs/python/tf/keras/losses?source=post_page-----383a19e387d6--------------------------------)
- en: This can be useful for handling imbalanced datasets, incorporating domain knowledge,
    and other specific use cases. In this article, we will explore the theory behind
    custom loss functions, the benefits of using them, and the practicalities of creating
    them in TensorFlow.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于处理不平衡的数据集、融入领域知识以及其他特定用例可能非常有用。本文将探讨自定义损失函数的理论、使用自定义损失函数的好处以及在TensorFlow中创建它们的实际操作。
- en: Understanding the Role of Loss Functions
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解损失函数的作用
- en: In machine learning, the goal of training a model is to minimize the difference
    between its predictions and the true output. This difference is measured by a
    loss function, also known as a cost function. A loss function is a scalar function
    that compares the predicted output of a model with its true output. The most commonly
    used loss functions are mean squared error, mean absolute error, and cross-entropy.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，训练模型的目标是最小化模型预测与真实输出之间的差异。这种差异通过损失函数来衡量，损失函数也称为成本函数。损失函数是一个标量函数，用于比较模型的预测输出与真实输出。最常用的损失函数有均方误差、平均绝对误差和交叉熵。
- en: '[## Loss function - Wikipedia'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 损失函数 - 维基百科'
- en: In mathematical optimization and decision theory, a loss function or cost function
    (sometimes also called an error…
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在数学优化和决策理论中，损失函数或成本函数（有时也称为误差函数）是用于衡量预测与真实结果之间差异的工具。
- en: en.wikipedia.org](https://en.wikipedia.org/wiki/Loss_function?source=post_page-----383a19e387d6--------------------------------)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: en.wikipedia.org](https://en.wikipedia.org/wiki/Loss_function?source=post_page-----383a19e387d6--------------------------------)
- en: How do Loss functions drive the optimization?
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数如何驱动优化？
- en: The optimization algorithm uses the value of the loss function to adjust the
    parameters of the model so that the difference between the predicted and true
    output is minimized. During the training phase, the model is presented with a
    set of inputs and corresponding true outputs, and the parameters of the model
    are adjusted to minimize the loss. The process is iterative, and it stops when
    the loss function reaches a minimum value or when a maximum number of iterations
    is reached.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 优化算法使用损失函数的值来调整模型的参数，以使预测输出与真实输出之间的差异最小化。在训练阶段，模型会接收一组输入及相应的真实输出，模型的参数被调整以最小化损失。这个过程是迭代的，直到损失函数达到最小值或达到最大迭代次数时停止。
- en: '[](https://en.wikipedia.org/wiki/Gradient_descent?source=post_page-----383a19e387d6--------------------------------)
    [## Gradient descent - Wikipedia'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://en.wikipedia.org/wiki/Gradient_descent?source=post_page-----383a19e387d6--------------------------------)
    [## 梯度下降 - 维基百科'
- en: In mathematics, gradient descent (also often called steepest descent) is a first-order
    iterative optimization algorithm…
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在数学中，梯度下降（也常称为最陡下降）是一种一阶迭代优化算法…
- en: en.wikipedia.org](https://en.wikipedia.org/wiki/Gradient_descent?source=post_page-----383a19e387d6--------------------------------)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: en.wikipedia.org](https://en.wikipedia.org/wiki/Gradient_descent?source=post_page-----383a19e387d6--------------------------------)
- en: The Need for Custom Loss Functions
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义损失函数的必要性
- en: While the built-in loss functions provided by TensorFlow are sufficient for
    many cases, there may be situations where a custom loss function is needed. One
    of the most common reasons is handling imbalanced datasets. In such cases, the
    data may contain a large number of examples of one class and only a few examples
    of another class. This can lead to a model that is highly accurate but does not
    perform well for the minority class. In such cases, custom loss functions that
    optimize for recall or precision can be used to balance the trade-off between
    accuracy and performance on the minority class.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 TensorFlow 提供的内置损失函数足以应对许多情况，但可能会出现需要自定义损失函数的情况。最常见的原因之一是处理不平衡数据集。在这种情况下，数据可能包含大量某一类别的示例和少量另一类别的示例。这可能导致一个高度准确但对少数类别表现不佳的模型。在这种情况下，可以使用优化召回率或精确度的自定义损失函数来平衡准确性和对少数类别的性能之间的权衡。
- en: Gradient Descent (C1W2L04) — [Source](https://www.youtube.com/@Deeplearningai)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Gradient Descent (C1W2L04) — [来源](https://www.youtube.com/@Deeplearningai)
- en: Another reason to use a custom loss function is to incorporate domain knowledge
    into the model. For example, in some cases, the predictions of a model need to
    satisfy specific constraints, such as being non-negative or having a specific
    range. Custom loss functions can be defined to enforce these constraints. Additionally,
    there are also some specific research areas such as object detection, and semantic
    segmentation has their own specific losses like cross-entropy with mask, dice
    loss, focal loss, etc.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 使用自定义损失函数的另一个原因是将领域知识融入模型。例如，在某些情况下，模型的预测需要满足特定的约束，如非负或具有特定范围。可以定义自定义损失函数来强制执行这些约束。此外，还有一些特定的研究领域，如目标检测和语义分割，有其特定的损失函数，如带掩码的交叉熵、Dice
    损失、焦点损失等。
- en: Creating Custom Loss Functions in TensorFlow
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中创建自定义损失函数
- en: TensorFlow provides several tools for creating custom loss functions, including
    the `tf.keras.losses` module. To create a custom loss function in TensorFlow,
    you can subclass the `tf.keras.losses.Loss` class and define a `call` method.
    The `call` the method should take in the predicted and true outputs and return
    the calculated loss. It's also possible to pass additional arguments to the custom
    loss function's constructor to use them in the loss calculation.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 提供了多个工具用于创建自定义损失函数，包括`tf.keras.losses`模块。要在 TensorFlow 中创建自定义损失函数，你可以继承`tf.keras.losses.Loss`类并定义`call`方法。`call`方法应接收预测输出和真实输出，并返回计算的损失。也可以将额外的参数传递给自定义损失函数的构造函数，以便在损失计算中使用。
- en: It is also possible to use functions from the TensorFlow library to create custom
    loss functions, such as using mathematical operations or using the `tf.nn` module.
    One important point to consider when defining custom loss functions is how the
    differentiable, as the optimizer is going to use the gradients of the loss w.r.t
    the model's parameters to update the model.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以使用 TensorFlow 库中的函数来创建自定义损失函数，例如使用数学运算或使用`tf.nn`模块。定义自定义损失函数时，一个重要的点是如何使其可微分，因为优化器将使用损失对模型参数的梯度来更新模型。
- en: One common pitfall to avoid when creating custom loss functions is to forget
    to properly handle the case where the inputs are batched, this means that the
    inputs are matrices and not single values, so the mathematical operations should
    work on matrix dimensions as well.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 创建自定义损失函数时一个常见的陷阱是忘记正确处理输入被批处理的情况，这意味着输入是矩阵而不是单一值，因此数学操作也应适用于矩阵维度。
- en: It’s also important to note that the custom loss function’s class should implement
    the `__init__`, `__call__` and `get_config` methods, which is the standard way
    to create a subclass in Keras.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 还需注意，自定义损失函数的类应实现`__init__`、`__call__`和`get_config`方法，这是在 Keras 中创建子类的标准方式。
- en: Let’s start with an example
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我们从一个示例开始
- en: Here’s an example of creating a custom loss function in TensorFlow for handling
    imbalanced datasets. The example is a binary classification problem where the
    goal is to classify data points as either class A or class B, where class A is
    the minority class.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个在 TensorFlow 中创建自定义损失函数以处理不平衡数据集的示例。该示例是一个二分类问题，目标是将数据点分类为类 A 或类 B，其中类
    A 是少数类。
- en: The custom loss function we will create will be a weighted cross-entropy loss,
    which assigns a higher weight to the minority class to balance the trade-off between
    the accuracy and performance of the minority class.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建的自定义损失函数将是加权交叉熵损失，它为少数类分配更高的权重，以平衡少数类的准确性和性能之间的权衡。
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, we are subclassing the `tf.keras.losses.Loss` class and defining a `call`
    the method which takes in y_true and y_pred, the true and predicted labels respectively.
    The method calculates the weighted cross-entropy loss by using the weight variable
    passed during initialization and returning the mean of loss.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将 `tf.keras.losses.Loss` 类进行子类化，并定义一个 `call` 方法，该方法接受 y_true 和 y_pred，分别表示真实标签和预测标签。该方法通过使用初始化时传递的权重变量来计算加权交叉熵损失，并返回损失的均值。
- en: In order to use this custom loss function, you can pass an instance of it to
    the `compile` method of your model when defining the model.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用这个自定义损失函数，你可以在定义模型时将其实例传递给模型的 `compile` 方法。
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here we passed the WeightedCrossEntropy object with weight=0.8 while compiling
    the model which will be used as the loss function during training.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们在编译模型时传递了一个权重为 0.8 的 WeightedCrossEntropy 对象，它将作为训练期间的损失函数使用。
- en: It’s important to note that the weight parameter should be set based on the
    relative frequencies of the minority and majority classes in the training dataset.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，权重参数应根据训练数据集中少数类和多数类的相对频率来设置。
- en: Please keep in mind that this is an example and you might need to adjust it
    to fit your specific use case, it should also be coupled with techniques such
    as over-sampling, under-sampling, or synthetic data generation in order to handle
    imbalanced data properly.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这只是一个示例，你可能需要根据你的具体用例调整它，它还应该与过采样、欠采样或合成数据生成等技术结合使用，以便正确处理不平衡数据。
- en: '![](../Images/c979dae66ba8c762241c2df6dfc9548f.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c979dae66ba8c762241c2df6dfc9548f.png)'
- en: Some samples of the MNIST database of handwritten digits— [Source](https://www.tensorflow.org/datasets/catalog/mnist)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST 手写数字数据库的一些样本— [来源](https://www.tensorflow.org/datasets/catalog/mnist)
- en: Here’s an example of how to use the custom loss function we created earlier
    in the context of a digit classification problem using the MNIST dataset. First,
    we will load the MNIST dataset using TensorFlow’s built-in `tf.keras.datasets`
    module and split it into training and testing sets. Next, we will define a simple
    model for classifying the digits in the MNIST dataset. Now, we will determine
    the weight for the custom loss function based on the relative frequencies of the
    minority and majority classes in the training dataset. In this case, the minority
    class is ‘0’ and the majority class is ‘1’ to ‘9’.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个如何在使用 MNIST 数据集的数字分类问题中使用我们之前创建的自定义损失函数的示例。首先，我们将使用 TensorFlow 内置的 `tf.keras.datasets`
    模块加载 MNIST 数据集，并将其分为训练集和测试集。接下来，我们将定义一个简单的模型来分类 MNIST 数据集中的数字。现在，我们将根据训练数据集中少数类和多数类的相对频率确定自定义损失函数的权重。在这种情况下，少数类是
    ‘0’，多数类是 ‘1’ 到 ‘9’。
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We then define the custom loss function and pass the weight parameter to it.
    Finally, we compile our model, specifying the custom loss function, optimizer
    and metrics and we train our model with the training data. It’s important to keep
    in mind that this is just one example of how to use a custom loss function in
    TensorFlow, and there are many other ways you could use it depending on the problem
    you’re trying to solve. Additionally, it’s important to evaluate the performance
    of the model on the test set, as well as interpret the results and compare them
    with other techniques to handle imbalanced datasets.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义自定义损失函数并将权重参数传递给它。最后，我们编译我们的模型，指定自定义损失函数、优化器和评估指标，然后使用训练数据训练我们的模型。需要记住的是，这只是如何在
    TensorFlow 中使用自定义损失函数的一个示例，根据你要解决的问题，可能还有许多其他方法可以使用它。此外，重要的是评估模型在测试集上的表现，并解释结果，并与处理不平衡数据集的其他技术进行比较。
- en: Conclusion
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: Custom loss functions can be a powerful tool for improving the performance of
    machine learning models, particularly when dealing with imbalanced datasets or
    incorporating domain knowledge. While creating a custom loss function can seem
    daunting, TensorFlow provides several tools and libraries to make the process
    easier. By understanding the theory and practicalities of custom loss functions,
    you’ll be well-equipped to tackle any challenges that come your way.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义损失函数可以成为提升机器学习模型性能的强大工具，特别是在处理不平衡数据集或融入领域知识时。虽然创建自定义损失函数可能看起来令人畏惧，但 TensorFlow
    提供了多种工具和库来简化这一过程。通过理解自定义损失函数的理论和实践，你将能够充分应对遇到的各种挑战。
- en: Join Medium Membership
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入 Medium 会员
- en: If you enjoyed this article and want to keep learning more about this topic,
    I invite you to join Medium membership at this [link](https://marcosanguineti.medium.com/membership).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这篇文章并希望继续了解更多相关内容，我邀请你通过这个[链接](https://marcosanguineti.medium.com/membership)加入
    Medium 会员。
- en: '[](https://marcosanguineti.medium.com/membership?source=post_page-----383a19e387d6--------------------------------)
    [## Join Medium with my referral link - Marco Sanguineti'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 使用我的推荐链接加入 Medium - Marco Sanguineti](https://marcosanguineti.medium.com/membership?source=post_page-----383a19e387d6--------------------------------)'
- en: Read every story from Marco Sanguineti (and thousands of other writers on Medium).
    Investment in culture is the best…
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阅读 Marco Sanguineti（以及 Medium 上其他成千上万位作者）的每一个故事。对文化的投资是最好的……
- en: marcosanguineti.medium.com](https://marcosanguineti.medium.com/membership?source=post_page-----383a19e387d6--------------------------------)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[marcosanguineti.medium.com](https://marcosanguineti.medium.com/membership?source=post_page-----383a19e387d6--------------------------------)'
- en: By becoming a member, you’ll have access to a wider variety of high-quality
    content, and exclusive access to member-only stories, and you’ll be supporting
    independent writers and creators like myself. Plus, as a member, you’ll be able
    to highlight your favourite passages, save stories for later, and get personalized
    reading recommendations. Sign up today and let’s continue exploring this topic
    and others together.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 成为会员后，你将能访问更多高质量内容，独享会员专属故事，还能支持像我这样的独立作家和创作者。此外，作为会员，你还可以高亮你喜欢的段落，保存故事以供日后阅读，并获得个性化的阅读推荐。立即注册，让我们继续一起探索这一主题和其他主题。
- en: Thank you for your support! Until next,
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢您的支持！下次见，
- en: Marco
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Marco
