- en: Improving Performance and Explainability of Zero-Shot CLIP
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改善零-shot CLIP 的性能和可解释性
- en: 原文：[https://towardsdatascience.com/improving-performance-and-explainability-of-zero-shot-clip-33e579d3f4bb](https://towardsdatascience.com/improving-performance-and-explainability-of-zero-shot-clip-33e579d3f4bb)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/improving-performance-and-explainability-of-zero-shot-clip-33e579d3f4bb](https://towardsdatascience.com/improving-performance-and-explainability-of-zero-shot-clip-33e579d3f4bb)
- en: Part 2 — Visual classification via description from LLMs
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二部分 — 通过 LLM 描述进行视觉分类
- en: '[](https://medium.com/@alexml0123?source=post_page-----33e579d3f4bb--------------------------------)[![Alexey
    Kravets](../Images/3b31f9b3c73c6c7ca709f845e6f70023.png)](https://medium.com/@alexml0123?source=post_page-----33e579d3f4bb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----33e579d3f4bb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----33e579d3f4bb--------------------------------)
    [Alexey Kravets](https://medium.com/@alexml0123?source=post_page-----33e579d3f4bb--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@alexml0123?source=post_page-----33e579d3f4bb--------------------------------)[![Alexey
    Kravets](../Images/3b31f9b3c73c6c7ca709f845e6f70023.png)](https://medium.com/@alexml0123?source=post_page-----33e579d3f4bb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----33e579d3f4bb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----33e579d3f4bb--------------------------------)
    [Alexey Kravets](https://medium.com/@alexml0123?source=post_page-----33e579d3f4bb--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----33e579d3f4bb--------------------------------)
    ·6 min read·Nov 25, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----33e579d3f4bb--------------------------------)
    ·阅读时间6分钟·2023年11月25日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: This is the second part of a series on enhancing Zero-Shot CLIP performance.
    In the first part, I provided a detailed explanation of how the CLIP model operates
    and described a straightforward method to improve its performance. This involved
    extending standard prompts like *“A picture of {class}”* with customized prompts
    generated by a large language model (LLM). If you haven’t already, you can find
    part 1 [here](https://medium.com/towards-data-science/simple-way-of-improving-zero-shot-clip-performance-4eae474cb447).
    In this article we will present a relatively similar method to improve zero-shot
    CLIP performance which is additionally highly explainable.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这是关于提升零-shot CLIP 性能系列文章的第二部分。在第一部分中，我详细解释了 CLIP 模型的工作原理，并描述了一种简单的方法来提高其性能。这包括通过大型语言模型（LLM）生成的定制提示来扩展标准提示，如
    *“{class} 的图片”*。如果你还没有阅读第一部分，可以在 [这里](https://medium.com/towards-data-science/simple-way-of-improving-zero-shot-clip-performance-4eae474cb447)
    找到。在这篇文章中，我们将介绍一种相对类似的方法来提高零-shot CLIP 性能，同时这种方法具有很高的可解释性。
- en: Introduction
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: The CLIP model is an impressive zero-shot predictor, enabling predictions on
    tasks it hasn’t explicitly been trained for. Despite its inherent capabilities,
    there exist several strategies to notably improve its performance. In the first
    article we have seen one of these strategies, however, while achieving enhanced
    performance is valuable, there are instances where we might be willing to make
    trade-offs to prioritize better explainability. In this second article of our
    series we will explore a method that not only enhances the performance of the
    zero-shot CLIP model but also ensures that its predictions are easily understandable
    and interpretable.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP 模型是一个令人印象深刻的零-shot 预测器，能够对其没有明确训练过的任务进行预测。尽管它具有固有的能力，但仍然存在几种策略可以显著提高其性能。在第一篇文章中，我们已经见过其中一种策略，然而，虽然提高性能是有价值的，但有时我们可能愿意做出权衡，以优先考虑更好的可解释性。在本系列的第二篇文章中，我们将探讨一种不仅提升零-shot
    CLIP 模型性能，还确保其预测结果易于理解和解释的方法。
- en: Explainability in Deep Neural Networks
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度神经网络的可解释性
- en: Various explainability techniques are available for deep learning models today.
    In a [previous article](https://medium.com/towards-data-science/integrated-gradients-from-scratch-b46311e4ab4),
    I delved into Integrated Gradients, a method that tells how each feature of an
    input influences the output of a machine learning model, especially deep neural
    networks. Another popular approach for model interpretation relies on Shap values,
    where we assign the contribution of each feature to the model’s output based on
    concepts from cooperative game theory. While these methods are versatile and can
    be applied to any deep learning model, they can be somewhat challenging to implement
    and interpret. CLIP, which has been trained to map image and text features into
    the same embedding space, provides an alternative explainability method based
    on text. This approach is more user-friendly and offers easy interpretability,
    providing a different perspective on model explanation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有多种解释性技术可用于深度学习模型。在[上一篇文章](https://medium.com/towards-data-science/integrated-gradients-from-scratch-b46311e4ab4)中，我深入探讨了集成梯度，这是一种说明输入的每个特征如何影响机器学习模型输出的方法，尤其是深度神经网络。另一种流行的模型解释方法是基于Shap值，我们根据合作博弈理论中的概念来分配每个特征对模型输出的贡献。虽然这些方法是通用的，可以应用于任何深度学习模型，但它们的实现和解释可能有些挑战。CLIP，它已经训练将图像和文本特征映射到相同的嵌入空间，提供了一种基于文本的替代解释方法。这种方法更具用户友好性，并提供了易于解释的方式，为模型解释提供了不同的视角。
- en: Quick refresh of the problem
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题的快速回顾
- en: 'As a quick refresh from the first part of this series, the problem we are tackling
    here is to predict the class of the image displayed below:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 作为这一系列的第一部分的快速回顾，我们在这里解决的问题是预测下面显示图像的类别：
- en: '![](../Images/5bd2eb765643b658df5c6cd4e04b9d8d.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5bd2eb765643b658df5c6cd4e04b9d8d.png)'
- en: 'Tree Frog image from [FreeImages](https://images.freeimages.com/images/large-previews/342/green-tree-frog2-1616738.jpg)
    (license: [https://www.freeimages.com/license](https://www.freeimages.com/license))'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 来自[FreeImages](https://images.freeimages.com/images/large-previews/342/green-tree-frog2-1616738.jpg)的树蛙图像（许可证：[https://www.freeimages.com/license](https://www.freeimages.com/license)）
- en: A standard method of using a simple prompt *“Picture of a {class}”* gives the
    wrong answer predicting *“tailed frog”* with 0.68 probability score*:*
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 使用简单提示 *“{class}的图片”* 的标准方法给出了错误的答案，预测 *“有尾巴的青蛙”* 的概率分数为0.68*：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let’s now see how we can improve it.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何改进它。
- en: Visual classification via Description from LLMs
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过LLMs的描述进行视觉分类
- en: 'To improve the predictive accuracy of zero-shot CLIP, we are going to implement
    a similar idea to what we discussed in the first article. However, this time,
    rather than providing generic prompts for a class like *“tree frog”* such as *“The
    identifying characteristics of a tree frog vary depending on the species, but
    some common features include large adhesive toes, protruding eyes, and bright
    colors”* we will separate it into specific **descriptive features**. For example,
    considering the *“tree frog”* and *“tailed frog”* classes descriptive features
    are:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高零-shot CLIP的预测准确性，我们将实现类似于我们在第一篇文章中讨论的思想。然而，这一次，我们不会提供像 *“树蛙”* 这样的通用提示，例如
    *“树蛙的识别特征因物种而异，但一些常见特征包括大粘附趾、突出的眼睛和明亮的颜色”*，而是将其分为具体的 **描述性特征**。例如，考虑到 *“树蛙”* 和
    *“有尾巴的青蛙”* 类别的描述性特征是：
- en: '*Tree frog*:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*树蛙*：'
- en: “Protruding eyes”
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “突出的眼睛”
- en: “Large mouth”
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “大嘴巴”
- en: “Without a tail”
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “没有尾巴”
- en: “Bright green color”
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “明亮的绿色”
- en: '*Tailed frog*:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*有尾巴的青蛙*：'
- en: “Tiny eyes”
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “小眼睛”
- en: “Small mouth”
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “小嘴巴”
- en: “Dark color”
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “深色”
- en: “Has long tail”
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “有长尾巴”
- en: '*These features can be again generated using a LLM with a prompt like:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*这些特征可以通过类似于以下的LLM生成：'
- en: 'Q: What are useful features for distinguishing a {class} in a photo?'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Q：在照片中区分{class}的有用特征是什么？
- en: 'A: There are several useful visual features to tell there is a {class} in a
    photo:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: A：有几个有用的视觉特征可以用来识别照片中的{class}：
- en: -*
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: -*
- en: The *“-”* is important as it will force the model to generate a list of features.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*“-”* 是重要的，因为它会迫使模型生成一个特征列表。'
- en: 'Next, similarly to what we have done in the first article, to classify the
    image of a frog we take the average vector embedding of these textual features
    descriptions that represent each class in the multi-modal space and evaluate which
    average vector is the closest to the test image we want to classify. In code,
    we have:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，类似于我们在第一篇文章中所做的，为了对青蛙的图像进行分类，我们会取这些文本特征描述的平均向量嵌入，这些描述代表了多模态空间中的每个类别，并评估哪个平均向量最接近我们要分类的测试图像。在代码中，我们有：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'First of all, we observe that our prediction is now accurate and the model
    correctly identifies the class as *“tree frog”*.Although we achieved the right
    classification result also with the method in part 1 of this series, there is
    a notable distinction in this method — it offers high explainability. Rather than
    simply taking the average of the features’ descriptions we can examine the non-standardized
    scores *S(feature)* for each feature description. This allows us to understand
    why the model predicted a particular class:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们观察到我们的预测现在是准确的，模型正确地识别出类别为*“树蛙”*。虽然我们在本系列的第1部分中也达到了正确的分类结果，但该方法的显著区别在于它提供了高可解释性。我们可以检查每个特征描述的非标准化分数*S(feature)*，而不是简单地取特征描述的平均值。这使我们能够理解模型为何预测了某个特定类别：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: S(“protruding eyes”) = 25.5400 > S(“tiny eyes”) = 24.0911;
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: S(“突出眼睛”) = 25.5400 > S(“小眼睛”) = 24.0911;
- en: S(“large mouth”) = 22.6840 > S(“small mouth”) = 22.3996;
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: S(“大嘴”) = 22.6840 > S(“小嘴”) = 22.3996;
- en: S(“without a tail”) ~ S(“has no tail”) are similar probably because the tail
    is
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: S(“没有尾巴”) ~ S(“无尾”) 可能相似，因为尾巴是
- en: not visible in the picture;
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图片中不可见;
- en: S(“bright green colour”) = 25.9017 > S(“dark colour”)= 21.0066;
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: S(“亮绿色”) = 25.9017 > S(“深色”)= 21.0066;
- en: The scores for features belonging to the *“tree frog”* class are higher than
    those for the features describing the *“tailed frog”* class. Analysing these feature
    scores helps us understand **why** the model predicted a certain class. In this
    example, very high scores were given to features like “protruding eyes,” “bright
    green colour,” and “large mouth,” providing a clear explanation for the predicted
    class. This level of explainability was not available in the method described
    in the first part because the generated prompts were quite generic and contained
    sentences that included different concepts. Changing prompts to simple feature
    descriptions gives us the best of both worlds — high accuracy and great explainability.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 属于*“树蛙”*类别的特征分数高于*“有尾蛙”*类别的特征分数。分析这些特征分数有助于我们理解**为何**模型预测了某个类别。在这个例子中，像“突出眼睛”、“亮绿色”和“大嘴”等特征得到了非常高的分数，为预测的类别提供了清晰的解释。在第一部分描述的方法中没有这种解释水平，因为生成的提示非常通用，并且包含了不同概念的句子。将提示改为简单的特征描述可以让我们兼得两全其美——高准确性和良好的可解释性。
- en: Conclusions
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In the second part of our series we have seen how to improve the standard prompt
    *“Picture of a {class}”* boosting performance. This solution is not only scalable,
    as LLMs can generate descriptive features for any number of classes and datasets,
    but it is also highly explainable. In the upcoming articles, we will explore few-shot
    learning methods that leverage few-shot image examples for each class to achieve
    higher accuracy than zero-shot methods.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在系列的第二部分，我们展示了如何通过改进标准提示*“{类别}的图片”*来提升性能。这个解决方案不仅具有可扩展性，因为LLM可以为任何数量的类别和数据集生成描述性特征，而且具有高度的可解释性。在即将到来的文章中，我们将探讨利用每个类别的少量图像示例的少样本学习方法，以实现比零样本方法更高的准确性。
- en: References
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] [CLIP (huggingface.co)](https://huggingface.co/docs/transformers/model_doc/clip)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [CLIP (huggingface.co)](https://huggingface.co/docs/transformers/model_doc/clip)'
- en: '[2] [https://openreview.net/pdf?id=jlAjNL8z5cs](https://openreview.net/pdf?id=jlAjNL8z5cs)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [https://openreview.net/pdf?id=jlAjNL8z5cs](https://openreview.net/pdf?id=jlAjNL8z5cs)'
