- en: Fine-Tune Your LLM Without Maxing Out Your GPU
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**优化您的LLM而不会让GPU负荷过重**'
- en: 原文：[https://towardsdatascience.com/fine-tune-your-llm-without-maxing-out-your-gpu-db2278603d78](https://towardsdatascience.com/fine-tune-your-llm-without-maxing-out-your-gpu-db2278603d78)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/fine-tune-your-llm-without-maxing-out-your-gpu-db2278603d78](https://towardsdatascience.com/fine-tune-your-llm-without-maxing-out-your-gpu-db2278603d78)
- en: How you can fine-tune your LLMs with limited hardware and a tight budget
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何在有限的硬件和紧张的预算下优化您的LLM
- en: '[](https://johnadeojo.medium.com/?source=post_page-----db2278603d78--------------------------------)[![John
    Adeojo](../Images/f6460fae462b055d36dce16fefcd142c.png)](https://johnadeojo.medium.com/?source=post_page-----db2278603d78--------------------------------)[](https://towardsdatascience.com/?source=post_page-----db2278603d78--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----db2278603d78--------------------------------)
    [John Adeojo](https://johnadeojo.medium.com/?source=post_page-----db2278603d78--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://johnadeojo.medium.com/?source=post_page-----db2278603d78--------------------------------)[![John
    Adeojo](../Images/f6460fae462b055d36dce16fefcd142c.png)](https://johnadeojo.medium.com/?source=post_page-----db2278603d78--------------------------------)[](https://towardsdatascience.com/?source=post_page-----db2278603d78--------------------------------)[![数据科学前沿](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----db2278603d78--------------------------------)
    [John Adeojo](https://johnadeojo.medium.com/?source=post_page-----db2278603d78--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----db2278603d78--------------------------------)
    ·8 min read·Aug 1, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[数据科学前沿](https://towardsdatascience.com/?source=post_page-----db2278603d78--------------------------------)
    ·阅读时间8分钟·2023年8月1日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ebaa7adc1f03dc619e66b6d330a31748.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ebaa7adc1f03dc619e66b6d330a31748.png)'
- en: 'Image by Author: Generated with Midjourney'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片：由Midjourney生成
- en: Demand for Bespoke LLMs
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定制LLM的需求
- en: With the success of ChatGPT, we have witnessed a surge in demand for bespoke
    large language models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着ChatGPT的成功，我们见证了对定制大型语言模型需求的激增。
- en: However, there has been a barrier to adoption. As these models are so large,
    it has been challenging for businesses, researchers, or hobbyists with a modest
    budget to customise them for their own datasets.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，采用的障碍依然存在。由于这些模型体积庞大，对于预算有限的企业、研究人员或爱好者来说，定制这些模型以适应自己的数据集是一项挑战。
- en: Now with innovations in parameter efficient fine-tuning (PEFT) methods, it is
    entirely possible to fine-tune large language models at a relatively low cost.
    In this article, I demonstrate how to achieve this in a [Google Colab](https://colab.research.google.com/).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，通过参数高效优化（PEFT）方法的创新，我们完全有可能以相对较低的成本优化大型语言模型。在本文中，我展示了如何在[Google Colab](https://colab.research.google.com/)中实现这一目标。
- en: I anticipate that this article will prove valuable for practitioners, hobbyists,
    learners, and even hands-on start-up founders.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我预计这篇文章对从业者、爱好者、学习者甚至实践型初创企业创始人都将大有裨益。
- en: So, if you need to mock up a cheap prototype, test an idea, or create a cool
    data science project to stand out from the crowd — keep reading.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如果你需要制作一个便宜的原型、测试一个想法，或者创建一个引人注目的数据科学项目——请继续阅读。
- en: Why Do We Fine-tune?
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么我们要优化？
- en: Businesses often have private datasets that drive some of their processes.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 企业通常拥有驱动其某些流程的私有数据集。
- en: To give you an example, I worked for a bank where we logged customer complaints
    in an Excel spreadsheet. An analyst was responsible for categorising these complaints
    (manually) for reporting purposes. Dealing with thousands of complaints each month,
    this process was time-consuming and prone to human error.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，我曾在一家银行工作，我们将客户投诉记录在Excel电子表格中。一名分析师负责（手动）对这些投诉进行分类以便于报告。每月处理成千上万的投诉，这一过程耗时且容易出错。
- en: Had we had the resources, we could have fine-tuned a large language model to
    carry out this categorisation for us, saving time through automation and potentially
    reducing the rate of incorrect categorisations.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们拥有足够的资源，我们可以优化一个大型语言模型来进行这一分类，从而通过自动化节省时间，并可能减少错误分类的发生率。
- en: Inspired by this example, the remainder of this article demonstrates how we
    can fine-tune an LLM for categorising consumer complaints about financial products
    and services.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 受到这个例子的启发，本文的其余部分展示了如何优化LLM以对金融产品和服务的消费者投诉进行分类。
- en: The Dataset
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集
- en: The dataset comprises real consumer complaints data for financial services and
    products. It is open, publicly available data published by the [Consumer Financial
    Protection Bureau](https://www.consumerfinance.gov/complaint/data-use/).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含了金融服务和产品的真实消费者投诉数据。这是由[消费者金融保护局](https://www.consumerfinance.gov/complaint/data-use/)发布的公开、公开可用的数据。
- en: There are over 120k anonymised complaints, categorised into approximately 214
    “subissues”.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中有超过12万条匿名投诉，分类为大约214个“子问题”。
- en: I have a version of the [dataset](https://huggingface.co/datasets/JAdeojo/consumer_complaints_cfpb)
    on my hugging face page that you can explore for yourself.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我在我的Hugging Face页面上有一个版本的[数据集](https://huggingface.co/datasets/JAdeojo/consumer_complaints_cfpb)，你可以自己探索。
- en: The Hardware
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 硬件
- en: The hardware I used for training was a V100 GPU with 16 GB of RAM, accessed
    via Google Colab. This is a relatively inexpensive and accessible infrastructure,
    available for rent via Google Colab Pro at approximately 9.99 USD per 100 compute
    units.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我用于训练的硬件是通过Google Colab访问的16GB RAM的V100 GPU。这是一种相对便宜且可访问的基础设施，通过Google Colab
    Pro以约9.99美元每100计算单元租用。
- en: The Large Language Model
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型
- en: The LLM used is RoBERTa¹ (XLM), which has approximately 563 million parameters.
    An overview of the model and its specification can be found [here](https://huggingface.co/xlm-roberta-large).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的LLM是RoBERTa¹（XLM），其参数约为5.63亿。模型及其规格的概述可以在[这里](https://huggingface.co/xlm-roberta-large)找到。
- en: Though not the largest model currently available, RoBERTa still presents a demanding
    workload for those with access only to small-scale infrastructure. This makes
    it an ideal choice to demonstrate that training a relatively large model on small-scale
    infrastructure is feasible.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然目前不是最大的模型，但RoBERTa对于仅能访问小规模基础设施的用户来说仍然是一个挑战。这使其成为展示在小规模基础设施上训练相对较大模型可行性的理想选择。
- en: '*Note — RoBERTa is a pre-trained model pulled from the* [*Hugging Face Hub*](https://huggingface.co/)*.*'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意 — RoBERTa是从* [*Hugging Face Hub*](https://huggingface.co/)*中提取的预训练模型*。'
- en: Fine-tuning at Low Cost with LoRA
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LoRA低成本微调
- en: As stated in the introduction, PEFT methods have made it possible to fine-tune
    LLMs at a low cost. One such method is LoRA, which stands for Low-Rank Adaptations
    of large language models.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如引言中所述，PEFT方法使得低成本微调LLMs成为可能。其中一种方法是LoRA，即大型语言模型的低秩适配。
- en: At a high level, LoRA accomplishes two things. First, it freezes the existing
    weights of the LLM (rendering them non-trainable); second, it injects trainable
    “lower-dimensional” layers into specified layers of the architecture.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，LoRA实现了两件事。首先，它冻结了LLM的现有权重（使其不可训练）；其次，它将可训练的“低维”层注入到架构的指定层中。
- en: This technique yields a model with far fewer trainable parameters while still
    preserving performance. LoRA has been shown to reduce GPU memory consumption by
    a factor of three compared to standard fine-tuning.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术产生了一个可训练参数远少于标准微调的模型，同时保持性能。与标准微调相比，LoRA已被证明能将GPU内存消耗降低三倍。
- en: For further details on LoRA, please read the full [paper](https://arxiv.org/pdf/2106.09685.pdf).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 关于LoRA的更多详细信息，请阅读完整的[论文](https://arxiv.org/pdf/2106.09685.pdf)。
- en: Technical Details
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术细节
- en: In the past, the key challenge for training large language models on limited
    hardware was adapting the training parameters to prevent the process from crashing
    due to exceeding your GPU’s memory capacity.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 过去，在有限硬件上训练大型语言模型的主要挑战是调整训练参数，以防止因超出GPU内存容量而导致过程崩溃。
- en: With LoRA, one can push the boundaries of their hardware with just a few adjustments.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LoRA，可以通过一些调整突破硬件的限制。
- en: Applying LoRA
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用LoRA
- en: Assuming you have your dataset prepared, the first thing you need to do is set
    your LoRA configurations.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你已经准备好数据集，首先需要做的是设置LoRA配置。
- en: 'Script by Author: Applying LoRA to RoBERTa'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 作者脚本：将LoRA应用于RoBERTa
- en: '**task_type** — The task for which you’re fine-tuning the model. For complaints
    classification, we are focusing on sequence classification.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**task_type** — 你微调模型的任务。对于投诉分类，我们专注于序列分类。'
- en: '**r** — A hyperparameter named the LoRA attention dimension which affects the
    scaling.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**r** — 一个名为LoRA注意力维度的超参数，影响缩放。'
- en: '**lora_alpha** — Another hyperparameter that impacts the scaling.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**lora_alpha** — 另一个影响缩放的超参数。'
- en: '**target_modules** — Here we specify that the transformation should be applied
    to the attention modules in our transformer, hence we set this to “query” and
    “value”.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**target_modules** — 在这里我们指定变换应应用于我们变换器中的注意力模块，因此我们将其设置为“query”和“value”。'
- en: '**lora_dropout** — The dropout probability for the LoRA layers.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**lora_dropout** — LoRA 层的丢弃概率。'
- en: '**bias** — The bias type for LoRA.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**bias** — LoRA 的偏置类型。'
- en: '**modules_to_save** — Excluding the LoRA layers, we declare which layers of
    our model we wish to make trainable and save at the final checkpoint. For our
    purposes, we require the classification head to be trainable.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**modules_to_save** — 除了 LoRA 层，我们声明希望使模型的哪些层可训练并在最终检查点保存。对于我们的目的，我们需要将分类头设置为可训练。'
- en: The next part of the script sets up the model itself. Here we are simply loading
    the pre-trained RoBERTa model from Hugging Face. The additional model parameters
    are simply passing dictionaries to the classification IDs such that the returned
    labels are in text form rather than numerically encoded.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本的下一部分设置了模型本身。这里我们只是从 Hugging Face 加载预训练的 RoBERTa 模型。额外的模型参数仅仅是将字典传递给分类 ID，使得返回的标签为文本形式而非数字编码。
- en: Finally, we want to transform the pre-trained model based on our LoRA configuration.
    This is done with the get_peft_model function.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们希望基于 LoRA 配置转换预训练模型。这是通过 get_peft_model 函数完成的。
- en: 'After applying LoRA, we are left with a network with the following structure:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 应用 LoRA 后，我们得到一个具有以下结构的网络：
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*Note: the original RoBERTa parameters are still in the network; they are simply
    frozen and therefore not trainable. Instead, we have the LoRA layers, which are
    fewer in number and trainable.*'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：原始的 RoBERTa 参数仍然存在于网络中；它们只是被冻结，因此不可训练。相反，我们有 LoRA 层，它们数量较少且可训练。*'
- en: Initiate Training
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启动训练
- en: Once we have our LoRA model, it’s simply a matter of setting up our trainer
    and initiating the training process.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了 LoRA 模型，接下来只需设置训练器并启动训练过程。
- en: 'Script by Author: Setup for training the LoRA model'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本作者：LoRA 模型训练设置
- en: Most aspects in the script are standard for training deep learning models; however,
    we have incorporated a few additional elements to improve GPU memory efficiency.
    Let’s briefly outline them.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本中的大多数方面在训练深度学习模型时都是标准的；然而，我们加入了一些额外的元素来提高 GPU 内存效率。让我们简要概述一下它们。
- en: '**Per_device_train_batch_size** — Setting this parameter to lower values maintains
    low RAM usage on your GPU at the expense of training speed. The lower the number,
    the more training steps are required to complete one epoch. Remember, one epoch
    is a complete run through the entire training dataset.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Per_device_train_batch_size** — 将此参数设置为较低的值可以保持 GPU 的低 RAM 使用，但会降低训练速度。数值越低，完成一个
    epoch 所需的训练步骤就越多。请记住，一个 epoch 是对整个训练数据集的完整运行。'
- en: '**Per_device_eval_batch** — Setting this parameter dictates how many data samples
    are processed on each GPU core. Increasing this number increases the speed of
    evaluation and therefore the speed of training, but at the cost of GPU memory
    usage.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Per_device_eval_batch** — 设置此参数决定了每个 GPU 核心处理多少数据样本。增加此数值会提高评估速度，从而提高训练速度，但会增加
    GPU 内存使用。'
- en: '**Gradient_accumulation_steps** — Usually, when updating deep learning models,
    there is a forward pass through the network and a loss is calculated. Following
    this, through a process known as backpropagation, the gradients are calculated
    and used to update the model parameters. With gradient accumulation, model parameters
    are not updated immediately after one forward pass. Instead, the gradients are
    stored and accumulated over the number of batches specified. Only after all the
    batches have passed through the network will the accumulated gradients be applied
    to update the parameters in the network. The effect here is to increase the effective
    batch size, meaning the model can effectively train on a larger batch size enabling
    you to reap the benefits of training on larger batches without the cost in GPU
    RAM usage.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Gradient_accumulation_steps** — 通常，在更新深度学习模型时，网络会进行前向传播并计算损失。随后，通过称为反向传播的过程，计算梯度并用以更新模型参数。使用梯度累积时，模型参数不会在一次前向传播后立即更新。相反，梯度会在指定的批次数中存储和累积。只有在所有批次通过网络后，累积的梯度才会应用于更新网络中的参数。其效果是增加有效批量大小，即模型可以有效地在更大的批量大小上进行训练，从而在
    GPU 内存使用不增加的情况下，享受训练更大批量的好处。'
- en: '**Fp16 (mixed-precision training)** — Setting mixed precision training to true
    can help improve training speed at the cost of GPU usage. Essentially, some calculations
    in the network are done in 16-bit (half precision) instead of 32-bit (full precision)
    to speed up the calculations. However, the method requires both a 16-bit and 32-bit
    version of the model to be stored on your GPU device meaning 1.5 RAM usage.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Fp16（混合精度训练）** — 将混合精度训练设置为 true 可以帮助提高训练速度，但代价是 GPU 使用量增加。本质上，网络中的一些计算以
    16 位（半精度）而非 32 位（全精度）进行，以加快计算速度。然而，这种方法需要在 GPU 设备上存储 16 位和 32 位版本的模型，即 1.5 倍的
    RAM 使用量。'
- en: Model Diagnostics
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型诊断
- en: Let’s see how everything performed. Since this is about training effectively
    with limited hardware, let’s start by looking at the GPU RAM diagnostics.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一切的表现。由于这涉及到在有限硬件下有效训练，让我们从 GPU RAM 诊断开始。
- en: '![](../Images/0591e1f3771d790299cc834ca2b32a1b.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0591e1f3771d790299cc834ca2b32a1b.png)'
- en: 'Image by Author: GPU RAM usage chart'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图片作者提供：GPU RAM 使用图表
- en: We were able to train our model for 9 hours without exceeding the RAM on our
    GPU. This is a good result.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能够在不超出 GPU RAM 的情况下训练模型 9 小时。这是一个不错的结果。
- en: What about the model performance? Let’s take a look at the validation and training
    loss.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 那么模型的表现如何？让我们看看验证和训练损失。
- en: '![](../Images/af3c0fad0f5073e5e86d2f810b428c75.png)![](../Images/6a7b88bc7aeaba5d09b6860e10cab2e5.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/af3c0fad0f5073e5e86d2f810b428c75.png)![](../Images/6a7b88bc7aeaba5d09b6860e10cab2e5.png)'
- en: 'Image by Author: validation and training loss'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图片作者提供：验证和训练损失
- en: Upon examining the loss charts, it seems that the models have yet to converge,
    despite the training session lasting approximately nine hours.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查损失图表，似乎模型尚未收敛，尽管训练过程持续了大约九小时。
- en: This might not be surprising considering the substantial model size and limited
    infrastructure at our disposal, alongside the fact that many of the training parameter
    decisions were made to prioritise memory preservation over training speed.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到模型的庞大尺寸和我们有限的基础设施，以及许多训练参数决策优先考虑内存保存而非训练速度，这可能并不令人惊讶。
- en: The model’s precision, recall, and F1 show that it’s prbably not ready for full
    production usage.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的精准度、召回率和 F1 指标显示，它可能还没有准备好进行全面的生产使用。
- en: '![](../Images/ab9a7ded433bcfbab3a8a2d271d298ac.png)![](../Images/204a27f935e99914efa5181d2128ae06.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ab9a7ded433bcfbab3a8a2d271d298ac.png)![](../Images/204a27f935e99914efa5181d2128ae06.png)'
- en: 'image by Author: Precision and recall tracking for the model'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图片作者提供：模型的精准度和召回率跟踪
- en: '![](../Images/46024b59c06227f09daedc22ee1496bb.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/46024b59c06227f09daedc22ee1496bb.png)'
- en: 'Image by Author: F1 tracking for the model'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图片作者提供：模型的 F1 跟踪
- en: Cost
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 成本
- en: 'Google levies charges for renting GPUs via Colab based on compute units. Consequently,
    to calculate the cost of training the model for nine hours, we need to ascertain
    the number of compute units utilised. I was unable to locate explicit details
    from Google on their computation of these units, but it is [suggested](https://help.apify.com/en/articles/3490384-what-is-a-compute-unit)
    that a compute unit is determined as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Google 根据计算单元对通过 Colab 租用 GPU 收取费用。因此，要计算训练模型九小时的成本，我们需要确定使用的计算单元数量。我无法从 Google
    获得有关这些单元计算的明确细节，但有[建议](https://help.apify.com/en/articles/3490384-what-is-a-compute-unit)计算单元的定义如下：
- en: 1 GB memory x 1 hour = 1 compute unit
  id: totrans-77
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 1 GB 内存 x 1 小时 = 1 个计算单元
- en: The model was trained for approximately nine hours and utilised around 15 GB
    of RAM. Therefore, the training run consumed roughly 135 compute units. With a
    price of 9.99 USD per 100 compute units, the cost of training the model amounts
    to approximately 13.49 USD.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练了大约九小时，并使用了约 15 GB 的 RAM。因此，训练过程消耗了大约 135 个计算单元。以每 100 个计算单元 9.99 美元的价格，训练模型的成本大约为
    13.49 美元。
- en: To achieve a usable state for the model, convergence may necessitate a slightly
    higher expenditure.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使模型达到可用状态，收敛可能需要稍微高一点的开支。
- en: Conclusion
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: We have demonstrated that LoRA makes it possible for individuals with limited
    resources to engage in the world of LLMs. For interested readers, I recommend
    exploring other [parameter efficient fine-tuning methods](https://huggingface.co/blog/peft),
    as well as strategies for improving data efficiency in your training processes.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经证明 LoRA 使得资源有限的个人能够参与 LLM 的世界。对于感兴趣的读者，我推荐探索其他[参数高效微调方法](https://huggingface.co/blog/peft)，以及改进训练过程中数据效率的策略。
- en: The end-to-end Colab with the full model run is available [here](https://colab.research.google.com/drive/1jmUPbg6G2uLkpRg7DQzPzerDLU_d9N2-?usp=sharing).
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有完整模型运行的端到端Colab可以在[这里](https://colab.research.google.com/drive/1jmUPbg6G2uLkpRg7DQzPzerDLU_d9N2-?usp=sharing)获取。
- en: I have hosted an app demonstrating the model [Hugging Face](https://huggingface.co/spaces/JAdeojo/consumer-finance-complaints-app-demo)
    for you to try for yourself.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我已托管一个演示该模型的应用程序，[Hugging Face](https://huggingface.co/spaces/JAdeojo/consumer-finance-complaints-app-demo)供你自行尝试。
- en: The full model diagnostics for the training run is available for you to review
    [here](https://api.wandb.ai/links/data-centric-solutions/mai8vzsz).
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练运行的完整模型诊断信息可以在[这里](https://api.wandb.ai/links/data-centric-solutions/mai8vzsz)查看。
- en: The Link to the model checkpoint is [here](https://huggingface.co/JAdeojo/xlm-roberta-large-lora-consumer-complaints-cfpb_49k).
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型检查点的链接在[这里](https://huggingface.co/JAdeojo/xlm-roberta-large-lora-consumer-complaints-cfpb_49k)。
- en: Watch a live tutorial on [YouTube](https://youtu.be/HMbctCYJLbw).
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在[YouTube](https://youtu.be/HMbctCYJLbw)上观看直播教程。
- en: Thanks for reading.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读。
- en: If you’re keen to enhance your skills in artificial intelligence, join the waiting
    list for [my course](https://www.data-centric-solutions.com/course), where I will
    guide you through the process of developing large language model powered applications.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你渴望提升在人工智能方面的技能，加入[我的课程](https://www.data-centric-solutions.com/course)的等待名单吧，在课程中我将引导你开发大型语言模型驱动的应用程序。
- en: If you’re seeking AI-transformation for your business, book a discovery call
    today.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在寻求为你的业务进行AI转型，今天就预约一次发现电话吧。
- en: '[](https://www.brainqub3.com/?source=post_page-----db2278603d78--------------------------------)
    [## Brainqub3 | AI software development'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://www.brainqub3.com/?source=post_page-----db2278603d78--------------------------------)
    [## Brainqub3 | AI软件开发'
- en: At Brainqub3 we develop bespoke AI software. We create qub3s, advanced artificial
    brains, using the latest AI to…
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在Brainqub3，我们开发定制的AI软件。我们使用最新的AI技术创建qub3s，先进的人工智能大脑，以...
- en: www.brainqub3.com](https://www.brainqub3.com/?source=post_page-----db2278603d78--------------------------------)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[www.brainqub3.com](https://www.brainqub3.com/?source=post_page-----db2278603d78--------------------------------)'
- en: For more insights on artificial intelligence, data science, and large language
    models you can subscribe to the [YouTube](https://www.youtube.com/channel/UCkXe-exqi25V4GnZendgEaA)
    channel.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 欲获取更多关于人工智能、数据科学和大型语言模型的见解，你可以订阅[YouTube](https://www.youtube.com/channel/UCkXe-exqi25V4GnZendgEaA)频道。
- en: Citations
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引用
- en: '[1] Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzmán,
    F., Grave, E., Ott, M., Zettlemoyer, L., & Stoyanov, V. (2019). Unsupervised cross-lingual
    representation learning at scale. *CoRR*. Retrieved from [http://arxiv.org/abs/1911.02116](http://arxiv.org/abs/1911.02116)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzmán,
    F., Grave, E., Ott, M., Zettlemoyer, L., & Stoyanov, V. (2019). 大规模无监督跨语言表示学习。*CoRR*。取自[http://arxiv.org/abs/1911.02116](http://arxiv.org/abs/1911.02116)'
