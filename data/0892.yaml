- en: 'Apache Spark MLlib vs Scikit-learn: Building Machine Learning Pipelines'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark MLlib 与 Scikit-learn：构建机器学习流水线
- en: 原文：[https://towardsdatascience.com/apache-spark-mllib-vs-scikit-learn-building-machine-learning-pipelines-be49ecc69a82?source=collection_archive---------2-----------------------#2023-03-09](https://towardsdatascience.com/apache-spark-mllib-vs-scikit-learn-building-machine-learning-pipelines-be49ecc69a82?source=collection_archive---------2-----------------------#2023-03-09)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/apache-spark-mllib-vs-scikit-learn-building-machine-learning-pipelines-be49ecc69a82?source=collection_archive---------2-----------------------#2023-03-09](https://towardsdatascience.com/apache-spark-mllib-vs-scikit-learn-building-machine-learning-pipelines-be49ecc69a82?source=collection_archive---------2-----------------------#2023-03-09)
- en: 'Code implementations for ML pipelines: from raw data to predictions'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ML流水线的代码实现：从原始数据到预测
- en: '[](https://brunocaraffa.medium.com/?source=post_page-----be49ecc69a82--------------------------------)[![Bruno
    Caraffa](../Images/414f88c6974dba79ec9851c051eff49f.png)](https://brunocaraffa.medium.com/?source=post_page-----be49ecc69a82--------------------------------)[](https://towardsdatascience.com/?source=post_page-----be49ecc69a82--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----be49ecc69a82--------------------------------)
    [Bruno Caraffa](https://brunocaraffa.medium.com/?source=post_page-----be49ecc69a82--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://brunocaraffa.medium.com/?source=post_page-----be49ecc69a82--------------------------------)[![Bruno
    Caraffa](../Images/414f88c6974dba79ec9851c051eff49f.png)](https://brunocaraffa.medium.com/?source=post_page-----be49ecc69a82--------------------------------)[](https://towardsdatascience.com/?source=post_page-----be49ecc69a82--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----be49ecc69a82--------------------------------)
    [Bruno Caraffa](https://brunocaraffa.medium.com/?source=post_page-----be49ecc69a82--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F657289ec5532&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapache-spark-mllib-vs-scikit-learn-building-machine-learning-pipelines-be49ecc69a82&user=Bruno+Caraffa&userId=657289ec5532&source=post_page-657289ec5532----be49ecc69a82---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----be49ecc69a82--------------------------------)
    ·8 min read·Mar 9, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fbe49ecc69a82&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapache-spark-mllib-vs-scikit-learn-building-machine-learning-pipelines-be49ecc69a82&user=Bruno+Caraffa&userId=657289ec5532&source=-----be49ecc69a82---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F657289ec5532&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapache-spark-mllib-vs-scikit-learn-building-machine-learning-pipelines-be49ecc69a82&user=Bruno+Caraffa&userId=657289ec5532&source=post_page-657289ec5532----be49ecc69a82---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----be49ecc69a82--------------------------------)
    · 8分钟阅读 · 2023年3月9日'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbe49ecc69a82&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapache-spark-mllib-vs-scikit-learn-building-machine-learning-pipelines-be49ecc69a82&source=-----be49ecc69a82---------------------bookmark_footer-----------)![](../Images/f0e56fa05aee5a0b3ede725550076f2f.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fbe49ecc69a82&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fapache-spark-mllib-vs-scikit-learn-building-machine-learning-pipelines-be49ecc69a82&source=-----be49ecc69a82---------------------bookmark_footer-----------)![](../Images/f0e56fa05aee5a0b3ede725550076f2f.png)'
- en: Photo by [Rodion Kutsaiev](https://unsplash.com/@frostroomhead?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Rodion Kutsaiev](https://unsplash.com/@frostroomhead?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Real-life machine learning involves a series of tasks to prepare the data before
    the magic predictions take place. Filling the missing values, one hot encoding
    for the categorical features, standardization and scaling for the numeric ones,
    feature extraction, and model fitting are just some of the stages that take part
    during a machine learning project before making any predictions. When working
    with NLP applications it gets even deeper with stages like stemming, lemmatization,
    stop word removal, tokenization, vectorization, and part of speech tagging (POS
    tagging).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 真实的机器学习涉及一系列准备数据的任务，然后才是神奇的预测。填充缺失值、对分类特征进行独热编码、对数值特征进行标准化和缩放、特征提取和模型拟合只是机器学习项目中进行预测之前的一些阶段。在处理NLP应用时，过程更加深入，包括词干提取、词形还原、停用词移除、分词、向量化和词性标注（POS标注）等阶段。
- en: It is perfectly possible to execute these steps using libraries like Pandas
    and NumPy or NLTK and SpaCy for NLP. The problem is when using the model in production
    it may lose code consistency among the team members and also the chances of human
    error in the code are higher when not operating a specially designed framework
    through the steps. This is why ML teams tend to work with high-level APIs called
    **Pipelines,** specially designed to link all the stages in a single object.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 完全可以使用像Pandas和NumPy或NLTK和SpaCy这样的库来执行这些步骤。问题在于，当模型投入生产时，团队成员之间的代码一致性可能会丧失，并且在没有操作特别设计的框架时，代码中的人为错误几率更高。这就是为什么ML团队倾向于使用称为**Pipelines**的高级API，这些API特别设计用于将所有阶段链接为一个对象。
- en: 'This article will present a code implementation for ML Pipelines using two
    of the main libraries available: Apache Spark’s MLLib and Scikit-learn. Scikit-learn
    has a fantastic performance running only on the CPU due to smart NumPy vectorized
    executions, which are friendly to CPU cache lines, and highly efficient use of
    the CPU available cores. Although within a big data context, Apache Spark’s MLLib
    tends to overperform scikit-learn due to its fit for distributed computation,
    as it is designed to run on Spark.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将介绍如何使用两大主要库：Apache Spark的MLLib和Scikit-learn，实现ML Pipelines。由于智能的NumPy矢量化执行，Scikit-learn在仅使用CPU时表现出色，这些执行方式对CPU缓存行友好，并且高效利用了CPU的可用核心。尽管在大数据背景下，Apache
    Spark的MLLib由于其适合分布式计算，通常会优于scikit-learn，因为它设计用于在Spark上运行。
- en: 'Datasets containing attributes of [Airbnb listings in 10 European cities](https://zenodo.org/record/4446043#.ZAfeMh_MK3D)¹
    will be used to create the same Pipeline in scikit-learn and MLLib. The Pipeline
    will manipulate the numerical and categorical features in the pre-processing stage
    before applying a Random Forest Regressor to generate price predictions for the
    listings. Those are the features and their respective data types:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 包含[10个欧洲城市的Airbnb房源数据集](https://zenodo.org/record/4446043#.ZAfeMh_MK3D)¹将被用于在scikit-learn和MLLib中创建相同的Pipeline。该Pipeline将在应用随机森林回归器生成房源价格预测之前，在预处理阶段操作数值和分类特征。这些特征及其各自的数据类型如下：
- en: '![](../Images/1f889bb45ceb3eee9de7136454ddfc6b.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1f889bb45ceb3eee9de7136454ddfc6b.png)'
- en: 'Image 1 —Features and data types. Source: The author.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图1 — 特征和数据类型。来源：作者。
- en: First, let’s load the datasets. As the prices tend to vary a lot during weekends
    and weekdays, there is a csv file with a dataset containing data of the listings
    during weekdays and weekends to each of the cities. Thus, to facilitate our job
    it is possible to consolidate all the datasets into a single dataframe and create
    the “***city***” and “***weekday_or_weekend***” features, which definitely will
    be essential features to the model.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们加载数据集。由于价格在周末和工作日之间变化很大，因此有一个包含各城市工作日和周末房源数据的csv文件。因此，为了简化我们的工作，可以将所有数据集整合到一个数据框中，并创建“***city***”和“***weekday_or_weekend***”特征，这些特征对于模型来说肯定是必不可少的。
- en: It is necessary to group the numerical features in a list by selecting the float
    columns and using the pop method we exclude the first feature, realSum, which
    is the price of the listings and our target feature. The multi and biz features
    have 1/0 encodings and scikit-learn allows us to input them to the model as bool
    types, so we’ll convert them. Finally, we create the categorical features list
    by selecting the columns of bool and object data types.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 必须通过选择浮点数列并使用 pop 方法将第一个特征 realSum（即房源价格和我们的目标特征）排除，来将数值特征分组到一个列表中。multi 和 biz
    特征采用 1/0 编码，scikit-learn 允许我们将它们作为布尔类型输入到模型中，因此我们将对它们进行转换。最后，我们通过选择布尔型和对象型数据列来创建分类特征列表。
- en: Then we can import all the necessary classes of the Scikit-learn lib. There
    is a class called ColumnTransformer that applies the transformers in the dataframe
    columns. The numeric transformer will be a standard scaler and the categorical
    feature will be a traditional one hot encoder. In the calling of Column Transformer,
    the previously created lists will be assigned to each transformer and it’s possible
    to join them in the pipeline with the Random Forest Regressor. And that’s it.
    When working with pipelines the feature manipulation process becomes more fluid
    and organized as this simple, but operationally dense, code below shows.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以导入 Scikit-learn 库中所有必要的类。有一个叫做 ColumnTransformer 的类，它会在数据框的列中应用变换器。数值变换器将是标准缩放器，而分类特征将使用传统的一热编码。在调用
    Column Transformer 时，之前创建的列表将分配给每个变换器，可以将它们与随机森林回归器一起加入到管道中。就这样。当处理管道时，特征操作过程变得更加流畅和有序，如下面这段简单但操作密集的代码所示。
- en: Finally, we can fit the pipeline into the training dataset and predict the prices
    for the test dataset listings just like is done with any other model. The mean
    absolute error of the pipeline was 67 euros, which is pretty good considering
    we did not apply any outlier treatment technique, created only two new features
    and just set the maximum depth and number of estimators parameters on the Random
    Forest Regressor, which leaves room for hyperparameter tuning on the regressor.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以将管道拟合到训练数据集，并预测测试数据集房源的价格，就像处理其他模型一样。管道的平均绝对误差为 67 欧元，考虑到我们没有应用任何异常值处理技术，只创建了两个新特征，并仅设置了随机森林回归器的最大深度和估算器数量参数，这还留有超参数调整的空间。
- en: Now that we’ve successfully created and applied the pipeline with scikit-learn
    let’s do the same with Apache Spark’s library MLLib. For that, I used [Databricks](https://www.databricks.com/),
    a cloud data platform created by the same founders of Spark for deploying advanced
    machine learning projects. Obviously, it runs on Apache Spark, which makes it
    the right choice when dealing with a big data context because of Spark’s properties
    of large-scale distributed computing. Databricks has a [community edition](https://www.databricks.com/product/faq/community-edition)
    hosted in AWS that is free and allows users to access one micro-cluster and build
    codes in Spark using Python or Scala. I highly recommend it to improve Spark and
    Databricks skills without incurring any costs even though there is only 15 GB
    of available memory on the free micro-cluster, which limits real-life big data
    applications.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经成功创建并应用了 scikit-learn 的管道，让我们用 Apache Spark 的 MLLib 库做同样的事情。为此，我使用了 [Databricks](https://www.databricks.com/)，这是一个由
    Spark 的创始人创建的云数据平台，用于部署先进的机器学习项目。显然，它运行在 Apache Spark 上，这使其成为处理大数据环境的正确选择，因为 Spark
    具有大规模分布式计算的属性。Databricks 提供了一个 [社区版](https://www.databricks.com/product/faq/community-edition)，托管在
    AWS 上，免费提供给用户访问一个微型集群，并使用 Python 或 Scala 在 Spark 上编写代码。我强烈推荐它来提高 Spark 和 Databricks
    技能，尽管免费微型集群只有 15 GB 的可用内存，这限制了现实生活中的大数据应用。
- en: The first thing to do on Databricks is creating a cluster, which is a set of
    computation resources and configurations on which we’ll run our Pipeline. This
    can be done by clicking create -> cluster on the top left menu. The cluster is
    already started with Spark and all the essential machine-learning libraries installed
    and it is even possible to import any other necessary lib. The only limitations
    are the limited memory, 2 CPU cores, and the auto-termination of the cluster after
    two hours of idle activity.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Databricks 上做的第一件事是创建一个集群，这是一组计算资源和配置，我们将在其上运行我们的管道。可以通过点击左上角菜单中的 create ->
    cluster 来完成。集群已经启动了 Spark，并安装了所有必需的机器学习库，甚至可以导入其他必要的库。唯一的限制是有限的内存、2 个 CPU 核心以及集群在两小时空闲后自动终止。
- en: '![](../Images/2c5756a5ad148f1708ee70ff930df3d6.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2c5756a5ad148f1708ee70ff930df3d6.png)'
- en: 'Image 2— Starting the Databricks cluster. Source: The author.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2— 启动 Databricks 集群。来源：作者。
- en: Databricks also has a user interface (UI) for table upload, which can be accessed
    by following create -> table on the top left menu. On the data loading UI it is
    easy to load the csv files into Delta Lake tables and also manipulate some of
    the properties of the table like inferring the schema, selecting the data types,
    and promoting the first row to the header. So let’s load our csv file with the
    dataframe created during the scikit-learn pipeline.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks 还具有一个表上传的用户界面（UI），可以通过在左上菜单中选择 create -> table 来访问。在数据加载 UI 中，可以轻松将
    csv 文件加载到 Delta Lake 表中，并操作一些表的属性，如推断模式、选择数据类型以及将第一行提升为表头。因此，让我们加载我们在 scikit-learn
    管道中创建的数据框的 csv 文件。
- en: '![](../Images/c22eadf3c79c46f86d0ca94edb0a9792.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c22eadf3c79c46f86d0ca94edb0a9792.png)'
- en: 'Image 3 — Loading the csv file using the Databricks data loading UI. Source:
    The author.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3 — 使用 Databricks 数据加载 UI 加载 csv 文件。来源：作者。
- en: Now that the cluster is created and the data is in order we can start the notebook
    by creating it on the same top-left menu used for the cluster and table setup.
    The read.csv function of Spark also allows us to define the first row as the header
    and infer the data schema, and we simply use the directory of the table to load
    the dataframe and remove the index column.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在集群已创建且数据已整理好，我们可以通过在用于集群和表设置的相同左上菜单中创建笔记本来开始使用。Spark 的 `read.csv` 函数还允许我们将第一行定义为表头并推断数据模式，我们只需使用表的目录来加载数据框并删除索引列。
- en: Time to meet the MLLib. To reproduce the pipeline built in the first section
    we must load the OneHotEncoder, StandardScaler, VectorAssembler, and StringIndexer
    functions from the feature class, the RandomForestRegressor from the regression
    class, and the Pipeline from the class with the same name. Unlike in scikit-learn,
    MLLib does not allow bool columns to be used in the operations so first, we must
    convert them to integers. Another difference is that for the categorical columns,
    we must add a stage of string indexing before the one hot encoding since in MLLib’s
    [OHE function](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.ml.feature.OneHotEncoder.html)
    uses only categorical indices as input, and not strings or boolean values.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是了解 MLLib 的时候了。为了重现第一部分中构建的管道，我们必须从 feature 类中加载 OneHotEncoder、StandardScaler、VectorAssembler
    和 StringIndexer 函数，从回归类中加载 RandomForestRegressor，从同名类中加载 Pipeline。与 scikit-learn
    不同，MLLib 不允许在操作中使用布尔列，因此我们必须先将其转换为整数。另一个不同之处是，对于类别列，我们必须在进行 one hot 编码之前添加字符串索引阶段，因为
    MLLib 的 [OHE 函数](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.ml.feature.OneHotEncoder.html)
    仅使用类别索引作为输入，而不是字符串或布尔值。
- en: A good practice when building a Pipeline on MLLib is to write the name of the
    input and output columns to keep track of the features since, unlike in scikit-learn’s
    Column Transformer, the transformations do not happen in the input column itself,
    instead, they add a new column to the existing dataframe.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MLLib 上构建管道时，一个好的实践是编写输入和输出列的名称，以跟踪特征，因为与 scikit-learn 的 Column Transformer
    不同，变换不会发生在输入列本身，而是将新列添加到现有数据框中。
- en: The final difference between the two pipelines is the order of the transformations.
    In scikit-learn, the categorical and numerical transformation occur in parallel
    during the pre-processing stage of the pipeline, while in MLLib we first apply
    the one hot encoder, then consolidate all the features in a single vector using
    the VectorAssembler and finally apply the StandardScaler function before the random
    forest regression. Thus, instead of a pre-processing stage and a regression stage,
    all the steps of the pre-processing are added to the pipeline in order before
    the regression.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 两个管道之间的最终差异是变换的顺序。在 scikit-learn 中，类别和数值变换在管道的预处理阶段并行进行，而在 MLLib 中，我们首先应用 one
    hot 编码器，然后使用 VectorAssembler 将所有特征合并到一个向量中，最后在随机森林回归之前应用 StandardScaler 函数。因此，与预处理阶段和回归阶段不同，所有的预处理步骤都按顺序添加到管道中，然后进行回归。
- en: With the pipeline finished, three lines of code are enough to split the data
    into training and test datasets, fit the model into the training dataset and make
    the price predictions for the test dataset. In MLLib, the RegressionEvaluator
    function is used to evaluate the models and contains all the evaluation metrics
    inherent to regressions (there are also functions for classification model evaluation).
    Just like in the first pipeline, the mean error average will be used as the evaluation
    metric.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 管道完成后，只需三行代码即可将数据拆分为训练集和测试集，将模型拟合到训练集上，并对测试集进行价格预测。在 MLlib 中，RegressionEvaluator
    函数用于评估模型，并包含所有回归固有的评估指标（也有用于分类模型评估的函数）。与第一个管道一样，平均误差将被用作评估指标。
- en: And to nobody’s surprise, the mean average error is almost identical to the
    first pipeline.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 不出意料，平均误差与第一个管道几乎完全一致。
- en: Wrapping it up, we’ve shown how ML pipelines simplify the coding process during
    a model implementation, making it cleaner and faster by improving the code readability
    with a logic workflow of the transformations during the model training. Pipelines
    are vital for ML projects involving large sets of data and with multiple data
    science team members involved.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们展示了 ML 管道如何简化模型实现过程中的编码，使其更清晰、更快捷，通过在模型训练过程中改进代码的可读性，提供了逻辑流的变换。管道对于涉及大量数据和多位数据科学团队成员的
    ML 项目至关重要。
- en: Along the coding implementation, we’ve seen that Scikit-learn has some tiny
    advantages in the pipeline creation flow over Apache Spark’s MLlib. The ColumnTransformer
    function is able to convert both categorical and numerical features in a single
    line of code, which reduces the Pipeline stages when compared to MLlib. Also,
    some might say that you won’t be able to duplicate a complex scikit-learn pipeline
    on MLlib, and that might be true considering how deeper are scikit-learn functions
    when compared with MLlib.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在编码实现过程中，我们看到 Scikit-learn 在管道创建流程中相对于 Apache Spark 的 MLlib 有一些小优势。ColumnTransformer
    函数能够在一行代码中转换分类和数值特征，这减少了与 MLlib 相比的 Pipeline 阶段。此外，有人可能会说，你无法在 MLlib 上复制一个复杂的
    scikit-learn 管道，这可能是因为 scikit-learn 的函数相比 MLlib 更加深奥。
- en: Nevertheless, MLlib’s ability to scale projects by processing big sets of data
    using Spark’s distributed processing makes it extremely valuable as well. Further,
    the pipeline to choose is highly dependable on the project’s context. For instance,
    when working with Databricks one might access data through the Delta Lake within
    the platform, and in that case, might be a better option to use MLlib to run the
    pipeline. It is even possible to join the two worlds and use MLlib pipeline only
    as a data engineering tool to prepare data, and considering a large dataset that
    would be a wise choice, to then build modeling and run the training and evaluation
    with scikit-learn.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，MLlib 通过使用 Spark 的分布式处理来处理大数据集，从而扩展项目的能力也非常宝贵。此外，选择哪个管道高度依赖于项目的上下文。例如，当在
    Databricks 中工作时，可能通过平台内的 Delta Lake 访问数据，在这种情况下，使用 MLlib 运行管道可能是更好的选择。甚至可以将两个世界结合起来，仅将
    MLlib 管道作为数据工程工具来准备数据，并考虑到大型数据集，这是明智的选择，然后使用 scikit-learn 进行建模、训练和评估。
- en: Hope you got to know (or review) the importance of pipelines in a machine learning
    project and the differences between scikit-learn and MLlib pipelines. It might
    take some time to structure them, but in the long run the organization pays off
    in multiple ways.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你了解（或复习）机器学习项目中管道的重要性，以及 scikit-learn 和 MLlib 管道之间的差异。虽然构建它们可能需要一些时间，但从长远来看，良好的组织会带来多方面的回报。
- en: '[1] Gyódi, Kristóf, & Nawaro, Łukasz. (2021). Determinants of Airbnb prices
    in European cities: A spatial econometrics approach (Supplementary Material) [Data
    set]. Zenodo. [https://doi.org/10.5281/zenodo.4446043](https://doi.org/10.5281/zenodo.4446043).
    CC BY 4.0 License.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Gyódi, Kristóf, & Nawaro, Łukasz. (2021). 《欧洲城市 Airbnb 价格的决定因素：一种空间计量经济学方法》（补充材料）[数据集]。Zenodo.
    [https://doi.org/10.5281/zenodo.4446043](https://doi.org/10.5281/zenodo.4446043)。CC
    BY 4.0 许可证。'
