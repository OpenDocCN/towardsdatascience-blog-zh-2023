- en: Transformers — Intuitively and Exhaustively Explained
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变换器 — 直观且详尽的解释
- en: 原文：[https://towardsdatascience.com/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb](https://towardsdatascience.com/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb](https://towardsdatascience.com/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)
- en: 'Exploring the modern wave of machine learning: taking apart the transformer
    step by step'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索现代机器学习的浪潮：逐步拆解变换器
- en: '[](https://medium.com/@danielwarfield1?source=post_page-----58a5c5df8dbb--------------------------------)[![Daniel
    Warfield](../Images/c1c8b4dd514f6813e08e401401324bca.png)](https://medium.com/@danielwarfield1?source=post_page-----58a5c5df8dbb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----58a5c5df8dbb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----58a5c5df8dbb--------------------------------)
    [Daniel Warfield](https://medium.com/@danielwarfield1?source=post_page-----58a5c5df8dbb--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@danielwarfield1?source=post_page-----58a5c5df8dbb--------------------------------)[![丹尼尔·沃菲尔德](../Images/c1c8b4dd514f6813e08e401401324bca.png)](https://medium.com/@danielwarfield1?source=post_page-----58a5c5df8dbb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----58a5c5df8dbb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----58a5c5df8dbb--------------------------------)
    [丹尼尔·沃菲尔德](https://medium.com/@danielwarfield1?source=post_page-----58a5c5df8dbb--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----58a5c5df8dbb--------------------------------)
    ·15 min read·Sep 20, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----58a5c5df8dbb--------------------------------)
    ·15分钟阅读·2023年9月20日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/edb20757aa401a62d13a932e35ee4b95.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/edb20757aa401a62d13a932e35ee4b95.png)'
- en: Image by author using MidJourney. All images by the author unless otherwise
    specified.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图像由作者使用 MidJourney 制作。所有图像由作者提供，除非另有说明。
- en: In this post you will learn about the transformer architecture, which is at
    the core of the architecture of nearly all cutting-edge large language models.
    We’ll start with a brief chronology of some relevant natural language processing
    concepts, then we’ll go through the transformer step by step and uncover how it
    works.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，你将学习变换器架构，它是几乎所有前沿大型语言模型的核心架构。我们将从一些相关自然语言处理概念的简要时间线开始，然后逐步讲解变换器，揭示它的工作原理。
- en: '**Who is this useful for?** Anyone interested in natural language processing
    (NLP).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**这对谁有用？** 对自然语言处理（NLP）感兴趣的任何人。'
- en: '**How advanced is this post?** This is not a complex post, but there are a
    lot of concepts, so it might be daunting to less experienced data scientists.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**这篇文章的难度如何？** 这篇文章不是很复杂，但有很多概念，因此对经验较少的数据科学家可能会有些令人生畏。'
- en: '**Pre-requisites:** A good working understanding of a standard neural network.
    Some cursory experience with embeddings, encoders, and decoders would probably
    also be helpful.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**前提条件：** 对标准神经网络有良好的工作理解。对嵌入、编码器和解码器有一些初步了解也会很有帮助。'
- en: A Brief Chronology of NLP Up To The Transformer
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理到变换器的简要时间线
- en: The following sections contain useful concepts and technologies to know before
    getting into transformers. Feel free to skip ahead if you feel confident.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 以下章节包含在了解变换器之前需要知道的有用概念和技术。如果你感觉自信，可以跳过前面的内容。
- en: Word Vector Embeddings
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词向量嵌入
- en: A conceptual understanding of word vector embeddings is pretty much fundamental
    to understanding natural language processing. In essence, a word vector embedding
    takes individual words and translates them into a vector which somehow represents
    its meaning.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对词向量嵌入的概念理解对于理解自然语言处理至关重要。实质上，词向量嵌入将单个单词转换成一个向量，这个向量以某种方式表示其含义。
- en: '![](../Images/dc19100e60b99abf50827b475f328713.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dc19100e60b99abf50827b475f328713.png)'
- en: 'The job of a word to vector embedder: turn words into numbers which somehow
    capture their general meaning.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 词向量嵌入器的工作：将单词转换为数字，这些数字以某种方式捕捉其一般意义。
- en: The details can vary from implementation to implementation, but the end result
    can be thought of as a “space of words”, where the space obeys certain convenient
    relationships. **Words are hard to do math on, but vectors which contain information
    about a word, and how they relate to other words, are significantly easier to
    do math on.** This task of converting words to vectors is often referred to as
    an “embedding”.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 细节可能因实现而异，但最终结果可以被认为是一个“单词空间”，其中空间遵循某些便利的关系。**单词很难进行数学运算，但包含有关单词及其与其他单词关系信息的向量，进行数学运算则显著容易得多。**
    将单词转换为向量的任务通常被称为“嵌入”。
- en: Word2Vect, a landmark paper in the natural language processing space, sought
    to create an embedding which obeyed certain useful characteristics. Essentially,
    they wanted to be able to do algebra with words, and created an embedding to facilitate
    that. With Word2Vect, you could embed the word “king”, subtract the embedding
    for “man”, add the embedding for “woman”, and you would get a vector who’s nearest
    neighbor was the embedding for “queen”.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vect，作为自然语言处理领域的一篇开创性论文，旨在创建一个符合某些有用特性的嵌入。实际上，他们希望能够对单词进行代数运算，并创建了一个嵌入以促进这一点。通过Word2Vect，你可以嵌入“king”这个词，减去“man”的嵌入，再加上“woman”的嵌入，你会得到一个与“queen”的嵌入最接近的向量。
- en: '![](../Images/f7c80912fb726a665edb7051aa12138d.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f7c80912fb726a665edb7051aa12138d.png)'
- en: A conceptual demonstration of doing algebra on word embeddings. If you think
    of each of the points as a vector from the origin, if you subtracted the vector
    for “man” from the vector for “king”, and added the vector for “woman”, the resultant
    vector would be near the word queen. In actuality these embedding spaces are of
    much higher dimensions, and the measurement for “closeness” can be a bit less
    intuitive (like cosine similarity), but the intuition remains the same.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对单词嵌入进行代数运算的概念性演示。如果你将每个点看作是从原点的向量，如果你从“king”的向量中减去“man”的向量，并加上“woman”的向量，那么结果向量将接近单词“queen”。实际上，这些嵌入空间的维度要高得多，而“接近度”的度量可能不那么直观（例如余弦相似度），但直觉保持不变。
- en: As the state of the art has progressed, word embeddings have maintained an important
    tool, with GloVe, Word2Vec, and FastText all being popular choices. Sub-word embeddings
    are generally much more powerful than full word embeddings, but are out of scope
    of this post.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 随着技术的发展，单词嵌入仍然是一个重要工具，其中GloVe、Word2Vec和FastText都是流行的选择。子词嵌入通常比完整的单词嵌入更强大，但超出了本文的范围。
- en: Recurrent Networks (RNNs)
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 递归网络（RNNs）
- en: Now that we can convert words into numbers which hold some meaning, we can start
    analyzing sequences of words. One of the early strategies was using a recurrent
    neural network, where you would train a neural network that would feed into itself
    over sequential inputs.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将单词转换为具有某些意义的数字，我们可以开始分析单词序列。早期的一种策略是使用递归神经网络，你将训练一个在顺序输入上自我反馈的神经网络。
- en: '![](../Images/a3a6d104fc11d9c3d2ebc8f65e8f1fd4.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a3a6d104fc11d9c3d2ebc8f65e8f1fd4.png)'
- en: The general idea of an RNN, which is a normal fully connected neural network
    which feeds into itself.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: RNN的一般思想是一个正常的全连接神经网络，它自我反馈。
- en: '![](../Images/5d8049d2ef520a6125a2fec62ffbf303.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5d8049d2ef520a6125a2fec62ffbf303.png)'
- en: What an RNN might look like if it had 3 hidden neurons and was used across 2
    inputs. The arrows in red are the recursive connections which connect the information
    from subsequent recurrent layers. The blue arrows are internal connections, like
    a dense layer. The neural network is copied for illustrative purposes, but keep
    in mind the network is actually feeding back into itself, meaning the parameters
    for the second (and subsequent) modules would be the same as the first.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个RNN有3个隐藏神经元，并且用于2个输入，它可能看起来像这样。红色的箭头是递归连接，它们连接来自后续递归层的信息。蓝色的箭头是内部连接，类似于一个密集层。神经网络被复制以便说明，但请记住，网络实际上是自我反馈的，这意味着第二（以及后续）模块的参数将与第一个模块相同。
- en: Unlike a traditional neural network, because recurrent networks feed into themselves
    they can be used for sequences of arbitrary length. they will have the same number
    of parameters for a sequences of length 10 or a sequence of length 100 because
    they reuse the same parameters for each recursive connection.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统神经网络不同，由于递归网络自我反馈，它们可以用于任意长度的序列。对于长度为10的序列或长度为100的序列，它们将具有相同数量的参数，因为它们重用每个递归连接的相同参数。
- en: This network style was employed across numerous modeling problems which could
    generally be categorized as sequence to sequence modeling, sequence to vector
    modeling, vector to sequence modeling, and sequence to vector to sequence modeling.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这种网络风格在许多建模问题中得到应用，这些问题通常可以归类为序列到序列建模、序列到向量建模、向量到序列建模和序列到向量到序列建模。
- en: '![](../Images/936d21d358a8f635f28ff1d1bdf1ce66.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/936d21d358a8f635f28ff1d1bdf1ce66.png)'
- en: Conceptual diagrams of a few applications of different modeling strategies which
    might use RNNs. Sequence to Sequence might be predicting the next word for text
    complete. Sequence to vector might be scoring how satisfied a customer was with
    a review. Vector to sequence might be compressing an image into a vector and asking
    the model to describe that image as a sequence of text. Sequence to vector to
    sequence might be text translation, where you need to understand a sentence, compress
    it into some representation, then construct a translation of that compressed representation
    in a different language.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 几种不同建模策略的概念图，这些策略可能会使用RNNs。序列到序列可能是预测文本完成的下一个词。序列到向量可能是评分客户对评论的满意度。向量到序列可能是将图像压缩成向量，并要求模型将该图像描述为文本序列。序列到向量到序列可能是文本翻译，其中你需要理解一个句子，将其压缩成某种表示，然后用不同的语言构建该压缩表示的翻译。
- en: While the promise of infinite length sequence modeling is enticing, it’s not
    practical. Because each layer shares the same weights it’s easy for recurrent
    models to forget the content of inputs. As a result, RNNs could only practically
    be used for very short sequences of words.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管无限长度序列建模的前景令人心动，但实际上并不切实际。由于每一层共享相同的权重，递归模型很容易忘记输入内容。因此，RNNs实际上只能用于非常短的词序列。
- en: There were some attempts to solve this problem by using “gated” and “leaky”
    RNNs. The most famous of these was the LSTM, which is described in the next section.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 曾经有一些尝试通过使用“门控”和“泄漏”RNNs来解决这个问题。其中最著名的是LSTM，下一部分将描述它。
- en: Long/Short Term Memory (LSTMs)
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 长短期记忆（LSTMs）
- en: The LSTM was created as an attempt to improve the ability of recurrent networks
    to recall important information. LSTM’s have a short term and long term memory,
    where certain information can be checked into or removed from the long term memory
    at any given element in the sequence.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM的创建旨在提高递归网络记忆重要信息的能力。LSTM具有短期和长期记忆，可以在序列中的任何给定元素中将某些信息检查进或从长期记忆中移除。
- en: '![](../Images/a3991fe7436ae9116f390d042247bc17.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a3991fe7436ae9116f390d042247bc17.png)'
- en: an LSTM in a nutshell
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM的核心要点
- en: Conceptually, an LSTM has three key subcomponents, the “forget gate” which is
    used to forget previous long term memories, the “input gate” which is used to
    commit things to long term memory, and the “output gate” which is used to formulate
    the short term memory for the next iteration.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，LSTM有三个关键子组件：用于忘记之前长期记忆的“遗忘门”，用于将信息写入长期记忆的“输入门”，以及用于制定下一次迭代的短期记忆的“输出门”。
- en: '![](../Images/3cef86252cf77623f233230baefe8610.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3cef86252cf77623f233230baefe8610.png)'
- en: The parameters in an LSTM. This particular LSTM expects an input vector of dimension
    3, and holds internal state vectors of dimension 2\. the dimension of vectors
    is a configurable hyperparameter. Also, notice the “S” and “T” at the end of each
    of the gates. These stand for sigmoid or tanh activation functions, which are
    used to squash values into certain ranges, like 0 to 1 or -1 to 1\. This “squashing”
    is what allows the network to “forget” and “commit to memory” certain information.
    image by the author, heavily inspired by [Source](/animated-rnn-lstm-and-gru-ef124d06cf45)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM中的参数。这个特定的LSTM期望输入向量的维度为3，并持有维度为2的内部状态向量。向量的维度是一个可配置的超参数。另外，请注意每个门末尾的“S”和“T”。这些代表sigmoid或tanh激活函数，用于将值压缩到某些范围，如0到1或-1到1。这个“压缩”使网络能够“忘记”和“记忆”某些信息。图像由作者提供，深受[来源](/animated-rnn-lstm-and-gru-ef124d06cf45)的启发。
- en: LSTMs, and similar architectures like GRUs, proved to be a significant improvement
    on the classic RNN discussed in the previous section. The ability to hold memory
    as a separate concept which is checked in and checked out of proved to be incredibly
    powerful. However, while LSTMs could model longer sequences, they were too forgetful
    for many language modeling tasks. Also, because they relied on previous inputs
    (like RNNs), their training was difficult to parallelize and, as a result, slow.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 和类似的架构，如 GRU，证明比前面讨论的经典 RNN 有了显著改善。能够将记忆作为一个单独的概念进行检查和释放，证明是非常强大的。然而，尽管
    LSTM 能够建模更长的序列，但它们在许多语言建模任务中太过遗忘。此外，由于它们依赖于先前的输入（像 RNN 一样），它们的训练难以并行化，结果也很慢。
- en: Attention Through Alignment
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过对齐提取注意力
- en: The Landmark Paper, [Neural Machine Translation by Jointly Learning to Align
    and Translate](https://arxiv.org/abs/1409.0473) popularized the general concept
    of attention and was the conceptual precursor to the multi-headed self attention
    mechanisms used in transformers.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 标志性论文，[通过联合学习对齐和翻译的神经机器翻译](https://arxiv.org/abs/1409.0473)普及了注意力的一般概念，并且是变换器中多头自注意力机制的概念先驱。
- en: I have a whole [article on this specific topic](https://medium.com/roundtableml/attention-from-alignment-practically-explained-548ef6588aa4),
    along with example code in PyTorch. In a nutshell, the attention mechanism in
    this paper looks at all potential inputs and decides which one to present to an
    RNN at any given output. **In other words, it decides which inputs are currently
    relevant, and which inputs are not currently relevant**.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我有一篇关于这个具体主题的[文章](https://medium.com/roundtableml/attention-from-alignment-practically-explained-548ef6588aa4)，其中包含了
    PyTorch 的示例代码。简而言之，本文中的注意力机制会查看所有潜在的输入，并决定在任何给定的输出时将哪个输入呈现给 RNN。**换句话说，它决定了哪些输入当前是相关的，哪些输入当前是不相关的**。
- en: '[](https://blog.roundtableml.com/attention-from-alignment-practically-explained-548ef6588aa4?source=post_page-----58a5c5df8dbb--------------------------------)
    [## Attention from Alignment, Practically Explained'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://blog.roundtableml.com/attention-from-alignment-practically-explained-548ef6588aa4?source=post_page-----58a5c5df8dbb--------------------------------)
    [## 从对齐中提取注意力，实际解释'
- en: Learn from what matters, Ignore what doesn’t.
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关注重要的，忽略不重要的。
- en: blog.roundtableml.com](https://blog.roundtableml.com/attention-from-alignment-practically-explained-548ef6588aa4?source=post_page-----58a5c5df8dbb--------------------------------)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[blog.roundtableml.com](https://blog.roundtableml.com/attention-from-alignment-practically-explained-548ef6588aa4?source=post_page-----58a5c5df8dbb--------------------------------)'
- en: This approach proved to have a massive impact, particularly in translation tasks.
    It allowed recurrent networks to figure out which information is currently relevant,
    thus allowing previously unprecedented performance in translation tasks specifically.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法被证明对翻译任务有着巨大的影响。它使递归网络能够确定哪些信息当前是相关的，从而在翻译任务中实现了前所未有的性能。
- en: '![](../Images/e96ba2aa559332e97758204b267938f2.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e96ba2aa559332e97758204b267938f2.png)'
- en: A figure from the linked article. The squares represent the word vector embeddings,
    and the circles represent intermediary vector representations. The red and blue
    circles are hidden states from a recurrent network, and the white circles are
    hidden states created by the attention through alignment mechanism. The punchline
    is that the attention mechanism can choose the right inputs to present to the
    output at any given step.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 链接文章中的一张图。方块代表词向量嵌入，圆圈代表中介向量表示。红色和蓝色圆圈是来自递归网络的隐藏状态，白色圆圈是由通过对齐机制的注意力创建的隐藏状态。关键在于，注意力机制可以选择在任何给定步骤向输出呈现正确的输入。
- en: The Transformer
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变换器
- en: In the previous sections we covered some forest through the trees knowledge.
    Now we’ll look at the transformer, which used a combination of previously successful
    and novel ideas to revolutionize natural language processing.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们涉及了一些宏观的知识。现在我们将深入探讨变换器，它结合了先前成功的和新颖的想法，彻底改变了自然语言处理。
- en: '![](../Images/67d76e5da4c7da11abe8fa358593a263.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67d76e5da4c7da11abe8fa358593a263.png)'
- en: The transformer diagram. [source](https://arxiv.org/pdf/1706.03762.pdf)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器图示。[source](https://arxiv.org/pdf/1706.03762.pdf)
- en: We’ll go through the transformer element by element and discuss how each module
    works. There’s a lot to go over, but it’s not math-heavy and the concepts are
    pretty approachable.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐个元素讨论变换器，并讨论每个模块如何工作。内容很多，但不涉及复杂的数学，概念也相当易懂。
- en: High Level Architecture
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高级架构
- en: At it’s most fundamental, the transformer is an encoder/decoder style model,
    kind of like the sequence to vector to sequence model we discussed previously.
    The encoder takes some input and compresses it to a representation which encodes
    the meaning of the entire input. The decoder then takes that embedding and recurrently
    constructs the output.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 从根本上讲，变换器是一个编码器/解码器风格的模型，有点像我们之前讨论的序列到向量到序列模型。编码器接受某些输入并将其压缩为一个表示，编码整个输入的意义。然后解码器接受这个嵌入并递归地构造输出。
- en: '![](../Images/8ab4230b9e11ee438636995bbba2317e.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8ab4230b9e11ee438636995bbba2317e.png)'
- en: A transformer working in a sequence to vector to sequence task, in a nutshell.
    The input (I am a manager) is compressed to some abstract representation that
    encodes the meaning of the entire input. The decoder works recurrently, like our
    RNNs previously discussed, to construct the output.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一个变换器在序列到向量到序列任务中的工作，总的来说。输入（我是一名经理）被压缩为某种抽象表示，编码整个输入的意义。解码器像我们之前讨论的RNN一样递归地构造输出。
- en: Input Embedding and Positional Encoding
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输入嵌入和位置编码
- en: '![](../Images/eac5999b4ebbaaab2705a3f3fc62f80b.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eac5999b4ebbaaab2705a3f3fc62f80b.png)'
- en: Input embedding within the original diagram. [source](https://arxiv.org/pdf/1706.03762.pdf)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 原图中的输入嵌入。[source](https://arxiv.org/pdf/1706.03762.pdf)
- en: The input embedding for a transformer is similar to previously discussed strategies;
    a word space embedder similar to word2vect converts all input words into a vector.
    This embedding is trained alongside the model itself, as essentially a lookup
    table which is improved through model training. So, there would be a randomly
    initialized vector corresponding to each word in the vocabulary, and this vector
    would change as the model learned about each word.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对于变换器，输入嵌入类似于之前讨论的策略；一个类似于word2vect的词空间嵌入器将所有输入词转换为向量。这个嵌入与模型一起训练，基本上是一个通过模型训练得到改进的查找表。因此，每个词汇表中的词会有一个随机初始化的向量，而这个向量会随着模型对每个词的学习而变化。
- en: Unlike recurrent strategies, transformers encode the entire input in one shot.
    As a result the encoder might lose information about the location of words in
    an input. To resolve this, transformers also use positional encoders, which is
    a vector encoding information about where a particular word was in the sequence.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 与递归策略不同，变换器一次性编码整个输入。因此，编码器可能会丢失关于输入中词位置的信息。为了解决这个问题，变换器还使用位置编码器，它是一个编码关于特定词在序列中位置的信息的向量。
- en: '[PRE0]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/2a3b13933101dc0757176687c913fdc6.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2a3b13933101dc0757176687c913fdc6.png)'
- en: Example of positional encoding. The Y axis represents subsequent words, and
    the x axis represents values within a particular words positional encoding. Each
    row in this image represents an individual word.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 位置编码的示例。Y轴表示随后的词，X轴表示特定词位置编码中的值。图中的每一行代表一个单独的词。
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/7a19f1ecdb46cf79387936d3962b1b5d.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7a19f1ecdb46cf79387936d3962b1b5d.png)'
- en: Values of the position vector relative to different indexes in a sequence. K
    represents the index in a sequence, and the graph represents values in a vector.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 相对于序列中不同索引的位置向量值。K表示序列中的索引，图表示向量中的值。
- en: 'This system uses the sin and cosin function in unison to encode position, which
    you can gain some intuition about in this article:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这个系统将正弦和余弦函数联合使用来编码位置，你可以在这篇文章中获得一些直观感受：
- en: '[](https://blog.roundtableml.com/use-frequency-more-frequently-14715714de38?source=post_page-----58a5c5df8dbb--------------------------------)
    [## Use Frequency More Frequently'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[## 更频繁地使用频率](https://blog.roundtableml.com/use-frequency-more-frequently-14715714de38?source=post_page-----58a5c5df8dbb--------------------------------)'
- en: A handbook from simple to advanced frequency analysis. Exploring a vital tool
    which is widely underutilized in data…
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从简单到高级的频率分析手册。探索在数据中广泛未被充分利用的关键工具…
- en: blog.roundtableml.com](https://blog.roundtableml.com/use-frequency-more-frequently-14715714de38?source=post_page-----58a5c5df8dbb--------------------------------)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[blog.roundtableml.com](https://blog.roundtableml.com/use-frequency-more-frequently-14715714de38?source=post_page-----58a5c5df8dbb--------------------------------)'
- en: I won’t harp on it, but a fascinating note; this system of encoding position
    is remarkably similarity to positional encoders used in motors, where two sin
    waves offset by 90 degrees allow a motor driver to understand position, direction,
    and speed of a motor.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会多讲，但值得注意的是；这个位置编码系统与电机中使用的位置编码器非常相似，其中两个相位差90度的正弦波使电机驱动器能够理解电机的位置、方向和速度。
- en: The vector used to encode the position of a word is added to the embedding of
    that word, creating a vector which contains both information about where that
    word is in a sentence, and the word itself. You might think “if your adding these
    wiggly waves to the embedding vector, wouldn’t that mask some of the meaning of
    the original embedding, and maybe confuse the model”? To that, I would say that
    neural networks (which the transformer employs for it’s learnable parameters)
    are incredibly good at understanding and manipulating smooth and continuous functions,
    so this is practically of little consequence for a sufficiently large model.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 用于编码词位置的向量会加到该词的嵌入中，创建一个包含该词在句子中位置以及词本身信息的向量。你可能会想“如果你将这些波动的波形加到嵌入向量中，这不会掩盖原始嵌入的一些含义，并可能混淆模型吗”？对此，我会说神经网络（变换器用来作为其可学习参数的网络）对理解和操作平滑且连续的函数非常擅长，因此对于一个足够大的模型来说，这几乎没有什么影响。
- en: 'Multi-Headed Self Attention: High Level'
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多头自注意力：高级概述
- en: This is probably the most important sub-component of the transformer mechanism.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是变换器机制中最重要的子组件。
- en: '![](../Images/9db82d332f334cc053e58aafcdfad9c4.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9db82d332f334cc053e58aafcdfad9c4.png)'
- en: The multi-headed self attention mechanism within the original diagram. [source](https://arxiv.org/pdf/1706.03762.pdf)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 原始图中的多头自注意力机制。 [source](https://arxiv.org/pdf/1706.03762.pdf)
- en: In this authors humble opinion, calling this an “attention” mechanism is a bit
    of a misnomer. Really, it’s a “co-relation” and “contextualization” mechanism.
    It allows words to interact with other words to transform the input (which is
    a list of embedded vectors for each word) into a matrix which represents the meaning
    of the entire input.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在作者谦虚的观点中，将其称为“注意力”机制有点用词不当。实际上，这是一种“关联”和“上下文化”机制。它允许词与其他词进行交互，将输入（即每个词的嵌入向量列表）转换为表示整个输入含义的矩阵。
- en: '![](../Images/9d9141780d17ff587a29b0cac4b7ac59.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d9141780d17ff587a29b0cac4b7ac59.png)'
- en: Multi Headed self attention, in a nutshell. The mechanism mathematically combines
    the vectors for different words, creating a matrix which encodes a deeper meaning
    of the entire input.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，多头自注意力机制。该机制从数学上将不同词的向量结合，创建一个矩阵，编码了整个输入的更深层次的含义。
- en: 'This mechanism can be thought of four individual steps:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这个机制可以分为四个独立的步骤：
- en: Creation of the Query, Key, and Value
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建查询、键和值
- en: Division into multiple heads
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分成多个头部
- en: The Attention Head
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意力头
- en: Composing the final output
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 组合最终输出
- en: Multi Head Self Att. Step 1) Creation of the Query, Key, and Value
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多头自注意力步骤1）创建查询、键和值
- en: First of all, don’t be too worried about the names “Query”, “Key”, and “Value”.
    These are vaguely inspired by databases, but really only in the most obtuse sense.
    The query, key, and value are essentially different representations of the embedded
    input which will be co-related to each-other throughout the attention mechanism.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，不必过于担心“查询”、“键”和“值”这些名称。这些名称受到数据库的模糊启发，但实际上只是最模糊的意义。查询、键和值本质上是嵌入输入的不同表示，在整个注意力机制中会彼此关联。
- en: '![](../Images/5437782be2ff6d394a82062848453bce.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5437782be2ff6d394a82062848453bce.png)'
- en: Turning the embedded input into the query key and value. The input has a dimension
    which is num_words by embedding_size, The query, key, and value all have the same
    dimensions as the input. In essence, a dense network projects the input into a
    tensor with three times the number of features while maintaining sequence length.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 将嵌入输入转换为查询、键和值。输入的维度为 num_words 乘 embedding_size，查询、键和值的维度与输入相同。实质上，密集网络将输入投射到一个具有三倍特征数量的张量中，同时保持序列长度。
- en: The dense network shown above includes the only learnable parameters in the
    multi headed self attention mechanism. Multi headed self attention can be thought
    of as a function, and the model learns the inputs (Query, Key, and Value) which
    maximizes the performance of that function for the final modeling task.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 上面显示的密集网络包括多头自注意力机制中唯一的可学习参数。多头自注意力可以被视为一个函数，模型学习输入（查询、键和值），以最大化该函数在最终建模任务中的性能。
- en: Multi Head Self Att. Step 2) Division into multiple heads
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多头自注意力步骤2）分成多个头部
- en: Before we do the actual contextualization which makes self attention so powerful,
    we’re going to divide the query, key, and value into chunks. The core idea is
    that instead of co-relating our words one way, we can co-relate our words numerous
    different ways. In doing so we can encode more subtle and complex meaning.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行实际的上下文化处理之前，我们将查询、键和值分成若干部分。核心思想是，我们可以用多种不同的方式来关联我们的词汇，而不是单一的方式。这样做可以编码出更微妙和复杂的含义。
- en: '![](../Images/7e6aa573cebef5051a49a9638e442374.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e6aa573cebef5051a49a9638e442374.png)'
- en: In this example we have 3 attention heads. As a result the query, key, and value
    are divided into 3 sections and passed to each head. Note that we’re dividing
    along the feature axis, not the word axis. Different aspects of each word’s embedding
    are passed to a different attention head, but each word is still present for each
    attention head.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们有 3 个注意力头。因此，查询、键和值被分成 3 部分，并传递给每个头。注意，我们是沿特征轴进行划分，而不是沿词汇轴。每个词的不同方面被传递到不同的注意力头，但每个词仍然存在于每个注意力头中。
- en: Multi Head Self Att. Step 3) The Attention Head
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多头自注意力机制。步骤 3) 注意力头
- en: Now that we have the sub-components of the query, key, and value which is passed
    to an attention head, we can discuss how the attention heads combines values in
    order to contextualize results. In Attention is all you need, this is done with
    matrix multiplication.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了传递给注意力头的查询、键和值的子组件，我们可以讨论注意力头如何结合这些值来上下文化结果。在《Attention is All You Need》中，这通过矩阵乘法完成。
- en: '![](../Images/82ef6e9249baeccc399ca73acd5d45af.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/82ef6e9249baeccc399ca73acd5d45af.png)'
- en: Matrix Multiplication. [Source](https://en.wikipedia.org/wiki/Matrix_multiplication#/media/File:Matrix_multiplication_diagram_2.svg)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵乘法。[来源](https://en.wikipedia.org/wiki/Matrix_multiplication#/media/File:Matrix_multiplication_diagram_2.svg)
- en: In matrix multiplication rows in one matrix get combined with columns in another
    matrix via a dot product to create a resultant matrix. In the attention mechanism
    the Query and Key are matrix multiplied together to create what I refer to as
    the “Attention Matrix”.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在矩阵乘法中，一个矩阵的行与另一个矩阵的列通过点积结合，从而创建出结果矩阵。在注意力机制中，查询和键矩阵相乘，产生我所称之为“注意力矩阵”的东西。
- en: '![](../Images/0b758201a67271bc4aaabe5167f51d6b.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0b758201a67271bc4aaabe5167f51d6b.png)'
- en: Calculating the attention matrix with the query and key. Note that the key is
    transposed to allow for matrix multiplication to result in the correct attention
    matrix shape.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 使用查询和键计算注意力矩阵。注意，键被转置，以便矩阵乘法产生正确的注意力矩阵形状。
- en: This is a fairly simple operation, and as a result it’s easy to underestimate
    its impact. The usage of a matrix multiplication at this point forces the representations
    of each word to be combined with the representations of each other word. Because
    the Query and Key are defined by a dense network, the attention mechanism learns
    how to translate the query and key to optimize the content of this matrix.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相当简单的操作，因此很容易低估其影响。在这一点上使用矩阵乘法迫使每个词的表示与其他词的表示结合。由于查询和键是由密集网络定义的，注意力机制学习如何转换查询和键，以优化这个矩阵的内容。
- en: 'Now that we have the attention matrix, it can be multiplied by the value matrix.
    This serves three key purposes:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了注意力矩阵，它可以与值矩阵相乘。这有三个主要目的：
- en: Add a bit more contextualization by relating another representation of the input.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过关联输入的另一种表示来增加一些上下文。
- en: Make a system such that the query and key function to transform the value which
    allows for either self attention or cross attention depending on where the query,
    key, and value come from.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建立一个系统，使查询和键能够转换值，从而根据查询、键和值的来源实现自注意力或交叉注意力。
- en: Perhaps most importantly, it make the output of the attention mechanism the
    same size as the input, which makes certain implementation details easier to handle.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 或许最重要的是，这使得注意力机制的输出与输入大小相同，从而使某些实现细节更容易处理。
- en: '**Important Correction**'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**重要更正**'
- en: The attention matrix is softmaxed row wise before being multiplying by the value
    matrix. This single mathematical detail completely changes the conceptual implications
    of the attention matrix and its relationship with the value matrix.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力矩阵在乘以值矩阵之前，按行进行 softmax 处理。这一数学细节完全改变了注意力矩阵的概念意义及其与值矩阵的关系。
- en: because each row in the attention matrix is softmaxed, each row becomes a probability,
    This is very similar to the attention through alignment concept I cover in a different
    article.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 因为注意力矩阵的每一行都经过 softmax 处理，每一行都变成一个概率。这与我在另一篇文章中讨论的注意力对齐概念非常相似。
- en: '![](../Images/2a53d42bdbb6ea8bc3af514762660db7.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2a53d42bdbb6ea8bc3af514762660db7.png)'
- en: From my [attention through alignment article](/attention-from-alignment-practically-explained-548ef6588aa4).
    Each row is a probability distribution which sums to 1, forcing the most important
    things to be co-related with other important things.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 来自我的[通过对齐的注意力文章](/attention-from-alignment-practically-explained-548ef6588aa4)。每一行是一个概率分布，总和为
    1，强制最重要的内容与其他重要内容相关联。
- en: This detail is frequently lost in greater explanations on transformers, but
    it is arguably the most important operation in the transformer architecture as
    it turns vague correlation into something with sparse and meaningful choices.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这个细节在对变压器的更大解释中经常被忽视，但可以说这是变压器架构中最重要的操作，因为它将模糊的关联转化为稀疏且有意义的选择。
- en: '![](../Images/b3fda4dcbbf3a775146f918a8e50f21f.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3fda4dcbbf3a775146f918a8e50f21f.png)'
- en: The attention matrix (which is the matrix multiplication of the query and key)
    multiplied by the value matrix to yield the final result of the attention mechanism.
    Because of the shape of the attention matrix, the result is the same shape as
    the value matrix. Keep in mind, this is the result from a single attention head.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力矩阵（即查询和键的矩阵乘法）与值矩阵相乘以产生注意力机制的最终结果。由于注意力矩阵的形状，结果与值矩阵的形状相同。请记住，这是来自单个注意力头的结果。
- en: Multi Head Self Att. Step 4) Composing the final output
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多头自注意力。步骤 4）合成最终输出
- en: In the last section we used the query, key, and value to construct a new result
    matrix which has the same shape as the value matrix, but with significantly more
    context awareness.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们使用了查询（query）、键（key）和值（value）来构建一个新的结果矩阵，该矩阵与值矩阵具有相同的形状，但具有显著更高的上下文感知能力。
- en: Recall that the attention head only computes the attention for a subcomponent
    of the input space (divided along the feature axis).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，注意力头仅计算输入空间子组件（沿特征轴划分）的注意力。
- en: '![](../Images/7e6aa573cebef5051a49a9638e442374.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e6aa573cebef5051a49a9638e442374.png)'
- en: Recall that the inputs were split into multiple attention heads. In this example,
    3 heads.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，输入被分成了多个注意力头。在这个例子中，有 3 个头。
- en: Each of these heads now outputs a different result, which can then be concatenated
    together.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 每个注意力头现在输出不同的结果，这些结果可以拼接在一起。
- en: '![](../Images/72e8e1c0a4cd4c927303e588cc1c5ecc.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/72e8e1c0a4cd4c927303e588cc1c5ecc.png)'
- en: The results of each attention head gets concatenated together
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 每个注意力头的结果被拼接在一起
- en: The shape of matrix is the same exact shape as the input matrix. However, unlike
    the input where each row related cleanly with a singular word, this matrix is
    much more abstract.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵的形状与输入矩阵的形状完全相同。然而，与每行与单一单词清晰相关的输入不同，这个矩阵要抽象得多。
- en: '![](../Images/9d9141780d17ff587a29b0cac4b7ac59.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d9141780d17ff587a29b0cac4b7ac59.png)'
- en: Recall that the attention mechanism, in a nutshell, transforms the embedded
    input into an abstract context rich representation.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，注意力机制简而言之是将嵌入的输入转换为一个抽象的、丰富上下文的表示。
- en: Add and Norm
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Add 和 Norm
- en: '![](../Images/a2c6c90e29890ac11c551427f41512a0.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a2c6c90e29890ac11c551427f41512a0.png)'
- en: Add and Norm within the original diagram. [source](https://arxiv.org/pdf/1706.03762.pdf)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始图中添加和归一化。[source](https://arxiv.org/pdf/1706.03762.pdf)
- en: The Add and Norm operations are applied twice in the encoder, and both times
    its effect is the same. There’s really two key concepts at play here; skip connections
    and layer normalization.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Add 和 Norm 操作在编码器中应用了两次，每次效果相同。这里有两个关键概念：跳跃连接（skip connections）和层归一化（layer normalization）。
- en: Skip connections are used all over the shop in machine learning. my favorite
    example is in image segmentation using a U-net architecture, if you’re familiar
    with that. Basically, when you do complex operations, it’s easy for the model
    to “get away from itself”. This has all sorts of fancy mathematical definitions
    like gradient explosions and rank collapse, but conceptually it’s pretty simple;
    a model can overthink a problem, and as a result it can be useful to re-introduce
    older data to re-introduce some of that simpler structure.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 跳跃连接在机器学习中随处可见。我最喜欢的例子是在图像分割中使用 U-net 架构，如果你熟悉的话。基本上，当你进行复杂操作时，模型容易“脱离自我”。这有各种复杂的数学定义，比如梯度爆炸和秩崩溃，但从概念上讲很简单；模型可能过度思考问题，因此重新引入旧数据可以重新引入一些简单的结构。
- en: '![](../Images/78a32038112aa03fa990fbe210a37c82.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/78a32038112aa03fa990fbe210a37c82.png)'
- en: 'What a skip connected add might look like. In this example the matrix on the
    left represents the original encoded input. The matrix in the middle represents
    the hyper contextual result from the attention matrix. The Right represents the
    result of a skip connection: a context aware matrix which still contains some
    order from the original input.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 跳跃连接添加可能的样子。在这个例子中，左侧的矩阵代表原始编码输入。中间的矩阵代表注意力矩阵的超上下文化结果。右侧代表跳跃连接的结果：一个上下文感知的矩阵，仍然保留了原始输入的一些顺序。
- en: Layer normalization is similar to skip connections in that it, conceptually,
    reigns in wonkiness. A lot of operations have been done to this data, which has
    resulted in who knows how large and small of values. If you do data science on
    this matrix, you might have to deal with both incredibly small and massively large
    values. This is known to be problematic.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 层归一化类似于跳跃连接，从概念上讲，它可以控制数据的异常情况。对这些数据进行了许多操作，导致了值的大小不一。如果你对这个矩阵进行数据科学分析，可能会遇到非常小和极大的值。这被认为是一个问题。
- en: Layer normalization computes the mean and standard deviation (how widely distributed
    the values are) and uses those values to squash the data back into a reasonable
    distribution.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 层归一化计算均值和标准差（值的分布范围），并利用这些值将数据压缩回合理的分布中。
- en: Feed Forward
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前馈
- en: '![](../Images/b28950a18ae9c716142281a3c741c1d0.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b28950a18ae9c716142281a3c741c1d0.png)'
- en: Feed Forward within the original diagram. [source](https://arxiv.org/pdf/1706.03762.pdf)
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 原始图示中的前馈。[source](https://arxiv.org/pdf/1706.03762.pdf)
- en: This part’s simple. We can take the output from the add norm after the attention
    mechanism and pass it through a simple dense network. I like to see this as a
    projection, where the model can learn how to project the attention output into
    a format which will be useful for the decoder.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分很简单。我们可以将注意力机制后的添加归一化的输出通过一个简单的全连接网络。我喜欢把这看作一种投影，模型可以学习如何将注意力输出投影到对解码器有用的格式中。
- en: The output of the feed forward network is then passed through another add norm
    layer, and that results in the final output. This final output will be used by
    the decoder to generate output.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将前馈网络的输出通过另一个添加归一化层，这样就得到了最终输出。解码器将使用这个最终输出来生成结果。
- en: General Function of the Decoder
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解码器的一般功能
- en: We’ve completely covered the encoder, and have a highly contextualized representation
    of the input. Now we’ll discuss how the decoder uses that representation to generate
    some output.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完全覆盖了编码器，并且得到了一个高度上下文化的输入表示。现在我们将讨论解码器如何利用这些表示生成输出。
- en: '![](../Images/58b4ebd22d397ff600a3fac8532c0db0.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/58b4ebd22d397ff600a3fac8532c0db0.png)'
- en: high level representation of how the output of the encoder relates to the decoder.
    the decoder references the encoded input for every recursive loop of the output.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 高层次的表示解码器如何与编码器的输出相关。解码器在每次递归输出时都参考编码的输入。
- en: The decoder is very similar to the encoder with a few minor variations. Before
    we talk about the variations, let's talk about similarities
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器与编码器非常相似，只是有一些小的变化。在谈论这些变化之前，让我们先讨论相似之处。
- en: '![](../Images/67d76e5da4c7da11abe8fa358593a263.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67d76e5da4c7da11abe8fa358593a263.png)'
- en: The Transformer Architecture [source](https://arxiv.org/pdf/1706.03762.pdf)
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 架构 [source](https://arxiv.org/pdf/1706.03762.pdf)
- en: As can be seen in the image above, the decoder uses the same word to vector
    embedding approach, and employs the same positional encoder. The decoder uses
    “Masked” multi headed self attention, which we’ll discuss in the next section,
    and uses another multi-headed attention block.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图所示，解码器使用相同的词向量嵌入方法，并采用相同的位置编码器。解码器使用“掩码”多头自注意力机制，我们将在下一节讨论，并使用另一个多头注意力块。
- en: The second multi-headed self attention uses the encoded input for the key and
    value, and uses the decoder input to generate the query. As a result, the attention
    matrix gets calculated from the embedding for the encoder and the decoder, which
    then gets applied to the value from the encoder. This allows the decoder to decide
    what it should finally output based on both the encoder input and the decoder
    input.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种多头自注意力机制使用编码后的输入作为键和值，并使用解码器输入生成查询。因此，注意力矩阵从编码器和解码器的嵌入中计算，然后应用于来自编码器的值。这使得解码器能够根据编码器输入和解码器输入来决定最终应该输出什么。
- en: The rest is the same boiler plate you might find on any other model. The results
    pass through another feed forward, an add norm, a linear layer, and a softmax.
    This softmax would then output probabilities for a bag of words, for instance,
    allowing the model to decide on a word to output.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 其余部分与你在其他模型中可能发现的模板相同。结果经过另一层前馈、一个加法归一化、一个线性层和一个softmax。这一softmax会输出例如一组单词的概率，从而允许模型决定输出哪个单词。
- en: Masked Multi Headed Self Attention
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 掩码多头自注意力
- en: So the only thing really new about the decoder is the “masked” attention. This
    has a to do with how these models are trained.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 所以解码器真正新的部分就是“掩码”注意力。这与这些模型的训练方式有关。
- en: One of the core flaws of recurrent neural networks is that you need to train
    them sequentially. An RNN intimately relies on the analysis of the previous step
    to inform the next step.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络的一个核心缺陷是你需要顺序训练它们。RNN密切依赖于对前一步的分析来指导下一步。
- en: '![](../Images/5d8049d2ef520a6125a2fec62ffbf303.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5d8049d2ef520a6125a2fec62ffbf303.png)'
- en: RNNs intimate dependencies between steps
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: RNN在步骤之间建立亲密依赖关系
- en: This makes training RNNs incredibly slow as each sequence in the training set
    needs to be sequentially fed through the model one by one. With some careful modifications
    to the attention mechanism transformers can get around this problem, allowing
    the model to be trained for an entire sequence in parallel.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得训练RNN变得非常缓慢，因为训练集中每个序列需要一个接一个地顺序传递通过模型。通过对注意力机制的一些精心修改，变换器可以解决这个问题，使模型能够并行训练整个序列。
- en: 'The details can get a bit fiddly, but the essence is this: When training a
    model, you have access to the desired output sequence. As a result, you can feed
    the entire output sequence to the decoder (including outputs you havent predicted
    yet) and use a mask to hide them from the model. This allows you to train on all
    positions of the sequence simultaneously.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 细节可能会有些繁琐，但核心是：在训练模型时，你可以访问期望的输出序列。因此，你可以将整个输出序列（包括你还未预测的输出）馈送给解码器，并使用掩码将它们隐藏在模型之外。这使得你可以同时对序列的所有位置进行训练。
- en: '![](../Images/f23f8faa166561e07e311fe4585d8504.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f23f8faa166561e07e311fe4585d8504.png)'
- en: 'A diagram of the mask working in mask multiheaded self attention, for an english
    to french translation task. The input of this task is the phrase “I am a manager”
    and the desired output is the phrase “Je suis directeur”. Note, for simplicities
    sake, I’ve generally neglected the concept of utiltiy tokens. They’re pretty easy
    to understand, though: start the sequence, end the sequence, etc.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码多头自注意力机制的工作示意图，用于英语到法语的翻译任务。该任务的输入是短语“我是一名经理”，期望的输出是短语“Je suis directeur”。请注意，为了简化起见，我通常忽略了实用令牌的概念。它们很容易理解，比如：开始序列、结束序列等。
- en: Conclusion
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: And that’s it! We broke down some of the technical innovations that lead to
    the discovery of the transformer and how the transformer works, then we went over
    the transformer’s high level architecture as an encoder-decoder model and discussed
    important sub-components like multi-headed self attention, input embeddings, positional
    encoding, skip connections, and normalization.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们分解了一些导致变换器发现的技术创新以及变换器的工作原理，然后我们回顾了变换器作为编码器-解码器模型的高级架构，并讨论了重要的子组件，如多头自注意力、输入嵌入、位置编码、跳跃连接和归一化。
- en: Follow For More!
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关注更多！
- en: I describe papers and concepts in the ML space, with an emphasis on practical
    and intuitive explanations.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我描述了机器学习领域的论文和概念，重点在于实用和直观的解释。
- en: '[](https://medium.com/@danielwarfield1/subscribe?source=post_page-----58a5c5df8dbb--------------------------------)
    [## Get an email whenever Daniel Warfield publishes'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@danielwarfield1/subscribe?source=post_page-----58a5c5df8dbb--------------------------------)
    [## 每当丹尼尔·沃菲尔德发布文章时获取邮件'
- en: High quality data science articles straight to your inbox. Get an email whenever
    Daniel Warfield publishes. By signing up, you…
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高质量的数据科学文章直接送到您的收件箱。每当丹尼尔·沃菲尔德发布文章时获取邮件。通过注册，您…
- en: medium.com](https://medium.com/@danielwarfield1/subscribe?source=post_page-----58a5c5df8dbb--------------------------------)
    [![](../Images/1f6f4c8a07d69cf53e055e0130a85b03.png)](https://www.buymeacoffee.com/danielwarfield)
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@danielwarfield1/subscribe?source=post_page-----58a5c5df8dbb--------------------------------)
    [![](../Images/1f6f4c8a07d69cf53e055e0130a85b03.png)](https://www.buymeacoffee.com/danielwarfield)
- en: Never expected, always appreciated. By donating you allow me to allocate more
    time and resources towards more frequent and higher quality articles. [Learn More](https://www.buymeacoffee.com/danielwarfield)
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 从未预期，总是感激。通过捐赠，您使我能够将更多时间和资源分配到更频繁和更高质量的文章上。[了解更多](https://www.buymeacoffee.com/danielwarfield)
- en: '**Attribution:** All of the images in this document were created by Daniel
    Warfield, unless a source is otherwise provided. You can use any images in this
    post for your own non-commercial purposes, so long as you reference this article,
    [https://danielwarfield.dev](https://danielwarfield.dev/), or both.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**归属:** 本文档中的所有图像均由**丹尼尔·沃菲尔德**创建，除非另有来源说明。您可以将本文中的任何图像用于您的非商业用途，只要您引用了这篇文章，[https://danielwarfield.dev](https://danielwarfield.dev/)，或两者都引用。'
