- en: A Visual Learner’s Guide to Explain, Implement and Interpret Principal Component
    Analysis (PCA)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视觉学习者指南：解释、实现和解读主成分分析（PCA）
- en: 原文：[https://towardsdatascience.com/a-visual-learners-guide-to-explain-implement-and-interpret-principal-component-analysis-cc9b345b75be](https://towardsdatascience.com/a-visual-learners-guide-to-explain-implement-and-interpret-principal-component-analysis-cc9b345b75be)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-visual-learners-guide-to-explain-implement-and-interpret-principal-component-analysis-cc9b345b75be](https://towardsdatascience.com/a-visual-learners-guide-to-explain-implement-and-interpret-principal-component-analysis-cc9b345b75be)
- en: Linear Algebra for Machine Learning — Covariance Matrix, Eigenvector and Principal
    Component
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习中的线性代数——协方差矩阵、特征向量和主成分
- en: '[](https://destingong.medium.com/?source=post_page-----cc9b345b75be--------------------------------)[![Destin
    Gong](../Images/c93d4341a928c36bc47031f02e23ebbf.png)](https://destingong.medium.com/?source=post_page-----cc9b345b75be--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cc9b345b75be--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cc9b345b75be--------------------------------)
    [Destin Gong](https://destingong.medium.com/?source=post_page-----cc9b345b75be--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://destingong.medium.com/?source=post_page-----cc9b345b75be--------------------------------)[![Destin
    Gong](../Images/c93d4341a928c36bc47031f02e23ebbf.png)](https://destingong.medium.com/?source=post_page-----cc9b345b75be--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cc9b345b75be--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cc9b345b75be--------------------------------)
    [Destin Gong](https://destingong.medium.com/?source=post_page-----cc9b345b75be--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cc9b345b75be--------------------------------)
    ·11 min read·Jan 25, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----cc9b345b75be--------------------------------)
    ·阅读时间11分钟·2023年1月25日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/2d801a017da88b5bf08832433651460a.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d801a017da88b5bf08832433651460a.png)'
- en: Principal Component Analysis for ML (image from my [website](https://www.visual-design.net/))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析在机器学习中的应用（图片来自我的[网站](https://www.visual-design.net/)）
- en: In my previous article, we have talked about applying linear algebra for data
    representation in machine learning algorithms, but the application of linear algebra
    in ML is much broader than that.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在我之前的文章中，我们讨论了线性代数在数据表示中的应用，但线性代数在机器学习中的应用远不止于此。
- en: '[](/how-is-linear-algebra-applied-for-machine-learning-d193bdeed268?source=post_page-----cc9b345b75be--------------------------------)
    [## How is Linear Algebra Applied for Machine Learning?'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/how-is-linear-algebra-applied-for-machine-learning-d193bdeed268?source=post_page-----cc9b345b75be--------------------------------)
    [## 线性代数如何应用于机器学习？'
- en: Starting from using matrix and vector for data representation
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从使用矩阵和向量进行数据表示开始
- en: towardsdatascience.com](/how-is-linear-algebra-applied-for-machine-learning-d193bdeed268?source=post_page-----cc9b345b75be--------------------------------)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/how-is-linear-algebra-applied-for-machine-learning-d193bdeed268?source=post_page-----cc9b345b75be--------------------------------)
- en: This article will introduce more linear algebra concepts with the main focus
    on how these concepts are applied for dimensionality reduction, specially **Principal
    Component Analysis (PCA)**. In the second half of this post, we will also implement
    and interpret PCA using a few lines of code with the help of Python scikit-learn.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将介绍更多的线性代数概念，主要关注这些概念如何应用于降维，特别是**主成分分析（PCA）**。在文章的后半部分，我们还将使用Python scikit-learn库中的几行代码来实现和解释PCA。
- en: When to Use PCA?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 何时使用PCA？
- en: High-dimensional data is a common issue experienced in machine learning practices,
    as we typically feed a large amount of features for model training. This results
    in the caveat of models having less interpretability and higher complexity — also
    known as the curse of dimensionality. PCA can be beneficial when the dataset is
    high-dimensional (i.e. contains many features) and it is widely applied for dimensionality
    reduction.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 高维数据是机器学习实践中常见的问题，因为我们通常会提供大量特征用于模型训练。这会导致模型可解释性降低和复杂度增加，这也被称为维度诅咒。当数据集是高维的（即包含许多特征）时，PCA（主成分分析）会有所帮助，并且它被广泛应用于降维。
- en: Additionally, PCA is also used for discovering the hidden relationships among
    features and reveal underlying patterns that can be very insightful. PCA attempts
    to find linear components that capture as much variance in the data as possible,
    and the first principal component (PC1) is typically composed of features that
    contributes the most to model predictions.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，PCA 还用于发现特征之间的隐藏关系，并揭示可以非常有洞察力的潜在模式。PCA 试图找到线性成分，以捕捉数据中尽可能多的方差，第一个主成分 (PC1)
    通常由对模型预测贡献最大的特征组成。
- en: How Does PCA Work?
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PCA 是如何工作的？
- en: 'The objective of PCA is to find the principal components that represents the
    data variance in a lower dimension and we are going to unfold the process into
    following steps:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 的目标是找到表示数据方差的主成分，并将过程展开为以下步骤：
- en: represent the data variance using **covariance matrix**
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 **协方差矩阵** 表示数据方差
- en: '**eigenvector and eigenvalue** capture data variance in a lower dimensionality'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**特征向量和特征值** 捕捉低维度中的数据方差'
- en: '**principal components** are the eigenvectors of the covariance matrix'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**主成分** 是协方差矩阵的特征向量'
- en: To understand how PCA works, we need to answer the questions of what are covariance
    matrix and eigenvector/eigenvalue. It is also helpful to fundamentally shift our
    perspectives of viewing matrix multiplication as a math operation to a visual
    transformation.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解 PCA 是如何工作的，我们需要回答协方差矩阵和特征向量/特征值是什么的问题。将矩阵乘法从数学操作的视角转变为视觉变换的视角也很有帮助。
- en: '**Matrix Transformation**'
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**矩阵变换**'
- en: We have previously introduced how matrix dot product is computed from a math
    operation perspectives. We can also interpret the dot product as a visual transformation
    which assists in understanding more complex linear algebra concepts. As illustrated
    below, let us use a 2x2 matrix as an example. We split the matrix vertically into
    two vectors where the left one represents the basis vector of x-axis, and the
    right one represents the basis vector of the y-axis. Therefore, a matrix represents
    a 2D space constructed by the x-axis and y-axis.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前介绍了从数学操作的角度计算矩阵点积的方法。我们还可以将点积解释为视觉变换，这有助于理解更复杂的线性代数概念。如下面所示，让我们以 2x2 矩阵为例。我们将矩阵垂直拆分为两个向量，其中左侧的向量表示
    x 轴的基向量，而右侧的向量表示 y 轴的基向量。因此，矩阵表示由 x 轴和 y 轴构成的二维空间。
- en: '![](../Images/d020c29190f15717d042715dbb87dfc8.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d020c29190f15717d042715dbb87dfc8.png)'
- en: matrix transformation — identity matrix (image by author)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵变换 — 单位矩阵（作者提供的图像）
- en: It is not hard to understand that an identity matrix has `[1,0]` as the basis
    vector on the x-axis and `[0,1]` as the basis vector on the y-axis, so that the
    dot product between any vectors and the identity matrix will return the vector
    itself.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 不难理解，单位矩阵的 `[1,0]` 是 x 轴上的基向量，而 `[0,1]` 是 y 轴上的基向量，因此任何向量与单位矩阵的点积将返回向量本身。
- en: Matrix transformation boils down to changing the scale and shifting the direction
    of the axis. For example, changing the basis vector of x-axis from `[1,0]` to
    `[2,0]` means that the mapping space has been scaled two times in the x coordinate
    direction.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵变换归结为改变轴的尺度和方向。例如，将 x 轴的基向量从 `[1,0]` 改为 `[2,0]` 意味着映射空间在 x 坐标方向上缩放了两倍。
- en: '![](../Images/2855794ba4c819ecc4ddc95880359597.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2855794ba4c819ecc4ddc95880359597.png)'
- en: matrix transformation — x-axis scaled matrix (image by author)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵变换 — x 轴缩放矩阵（作者提供的图像）
- en: We can additionally combine both the x-axis and y-axis for more complicated
    scaling, rotating or shearing transformation. A typically example is the mirror
    matrix where we swap the x and y axis. For a given vector `[1,2]`, we will get
    `[2,1]` after the mirror transformation.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以结合 x 轴和 y 轴进行更复杂的缩放、旋转或剪切变换。一个典型的例子是镜像矩阵，其中我们交换 x 和 y 轴。对于给定的向量 `[1,2]`，经过镜像变换后我们会得到
    `[2,1]`。
- en: '![](../Images/3fd81afd5d25c72099fc673f2f97b105.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3fd81afd5d25c72099fc673f2f97b105.png)'
- en: matrix transformation — mirror matrix (image by author)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵变换 — 镜像矩阵（作者提供的图像）
- en: If you would like to practice these transformations in python and skip the manual
    calculations, we can use following code to perform these dot products and visualize
    the result of the transformation using `plt.quiver()` function.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在 Python 中练习这些变换并跳过手动计算，我们可以使用以下代码来执行这些点积，并使用 `plt.quiver()` 函数可视化变换结果。
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/30d0be7669a84ef125818aac15ecc8a2.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/30d0be7669a84ef125818aac15ecc8a2.png)'
- en: matrix transformation result in python (image by author)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Python中的矩阵变换结果（图像由作者提供）
- en: '**Covariance Matrix**'
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**协方差矩阵**'
- en: '*In Short: covariance matrix represents the pairwise correlations among a group
    of variables in a matrix form.*'
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*简而言之：协方差矩阵以矩阵形式表示一组变量之间的成对相关性。*'
- en: Covariance matrix is another critical concept in PCA process that represents
    the data variance in the dataset. To understand the details of covariance matrix,
    we firstly need to know that **covariance measures the magnitude of how one random
    variable varies with another random variable**. For two random variable x and
    y, their covariance is formulated as below and higher covariance value indicates
    stronger correlation between two variables.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 协方差矩阵是PCA过程中另一个关键概念，它表示数据集中数据的方差。要了解协方差矩阵的细节，我们首先需要知道**协方差衡量了一个随机变量与另一个随机变量变化的大小**。对于两个随机变量
    x 和 y，它们的协方差公式如下，高协方差值表示两个变量之间的相关性更强。
- en: '![](../Images/79ec551b71501c21bcad9b968aa20b6f.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/79ec551b71501c21bcad9b968aa20b6f.png)'
- en: covariance formula (image by author)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 协方差公式（图像由作者提供）
- en: When given a set of variables (e.g. *x1, x2, … xn*) in a dataset, covariance
    matrix is used for representing the covariance value between each variable pairs
    in a matrix format.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当给定数据集中的一组变量（例如 *x1, x2, … xn*）时，协方差矩阵用于以矩阵格式表示每对变量之间的协方差值。
- en: '![](../Images/7c7044d8e0f568131c459d0713907736.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7c7044d8e0f568131c459d0713907736.png)'
- en: covariance matrix (image by author)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 协方差矩阵（图像由作者提供）
- en: Multiplying any vector with the covariance matrix will transform it towards
    the direction that captures the trend of variance in the original dataset.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 任何向量与协方差矩阵相乘将把它变换到捕捉原始数据集中方差趋势的方向。
- en: Let us use a simple example to simulate the effect of this transformation. Firstly,
    we randomly generate the variable *x0, x1* and then compute the covariance matrix.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个简单的例子来模拟这种变换的效果。首先，我们随机生成变量 *x0, x1*，然后计算协方差矩阵。
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/16b1f8ec3f3ffbb27495035cebe7242a.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/16b1f8ec3f3ffbb27495035cebe7242a.png)'
- en: We then transform some random vectors by taking the dot product between each
    of them and the covariance matrix.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们通过计算每个随机向量与协方差矩阵的点积来对这些向量进行变换。
- en: '[PRE2]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/97dd095e4118abeee51c53aa0056539d.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/97dd095e4118abeee51c53aa0056539d.png)'
- en: vectors transformed by covariance matrix (image by author)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 被协方差矩阵变换的向量（图像由作者提供）
- en: Original vectors prior to the transformation are in black, and after transformation
    are in brown. As you can see, the original vectors that are pointing at different
    directions have become more conformed to the general trend displayed in the original
    dataset (i.e. the blue dots). Because of this property, covariance matrix is important
    to PCA in terms of describing the relationship between features.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 变换前的原始向量为黑色，变换后的向量为棕色。正如你所看到的，指向不同方向的原始向量在变换后变得更加符合原始数据集中显示的一般趋势（即蓝色点）。由于这一特性，协方差矩阵在PCA中用于描述特征之间的关系非常重要。
- en: '**Eigenvalue and Eigenvector**'
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**特征值和特征向量**'
- en: '*In Short: Eigenvector (*v*) of a matrix (*A*) remains at the same direction
    after the matrix transformation, hence* Av = λv *where* v *represents the corresponding
    eigenvalue. Representing data using eigenvector and eigenvalue reduces the dimensionality
    while maintaining the data variance as much as possible.*'
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*简而言之：矩阵 (*A*) 的特征向量 (*v*) 在矩阵变换后保持相同的方向，因此* Av = λv *其中* v *代表相应的特征值。使用特征向量和特征值表示数据可以减少维度，同时尽可能保持数据的方差。*'
- en: To bring more intuitions to this concept, we can use a simple demonstration.
    For example, we have the matrix `[[0,1],[1,0]]`, and one of the eigenvector for
    matrix is `[1,1]` and the corresponding eigenvalue is 1.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更直观地理解这个概念，我们可以使用一个简单的示例。例如，我们有矩阵 `[[0,1],[1,0]]`，该矩阵的一个特征向量是 `[1,1]`，相应的特征值是1。
- en: '![](../Images/0a33765ba4fc1f3c4eae327d1c8719dd.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a33765ba4fc1f3c4eae327d1c8719dd.png)'
- en: eigenvector and eigenvalue (image by author)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 特征向量和特征值（图像由作者提供）
- en: From matrix transformation, we know that matrix`[[0,1],[1,0]]` acts as a mirror
    matrix that swaps the x, y coordinate of the vector. Therefore, the direction
    of vector `[1,1]` will not change after the mirror transformation, thus it meets
    the criteria of being the eigenvector of the matrix. The eigenvalue 1 indicates
    that the vector remains at the same scale and direction as prior to the transformation.
    Consequently, we are able to represent the effect of a matrix transform *A* (i.e.
    2 dimensional) using a scalar *λ* (i.e. 1 dimension) and eigenvalue tells us how
    much variance are preserved by the eigenvector.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 从矩阵变换中，我们知道矩阵`[[0,1],[1,0]]`充当镜像矩阵，交换向量的x和y坐标。因此，向量`[1,1]`在镜像变换后方向不会改变，因此它符合成为矩阵特征向量的标准。特征值1表示向量在变换前后保持相同的尺度和方向。因此，我们能够使用标量*λ*（即1维）来表示矩阵变换*
    A*（即2维）的效果，特征值告诉我们特征向量保留了多少方差。
- en: Let’s continue with the example above and use this code snippet to overlay the
    eigenvector with the greatest eigenvalue (in red color). As you can see, it is
    aligned with the direction with the greatest data variance.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续使用上述示例，并使用此代码片段叠加具有最大特征值的特征向量（红色）。如你所见，它与数据方差最大的方向对齐。
- en: '[PRE3]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/4b4baf1aacfecffd7ff26add293341d7.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b4baf1aacfecffd7ff26add293341d7.png)'
- en: visualize eigenvector (image by author)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化特征向量（图片来源于作者）
- en: '**Principal Components**'
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**主成分**'
- en: Now that we have discussed that covariance matrix can represent the data variance
    when multiple variables are present and eigenvector can capture the data variance
    in a lower dimensionality. By computing the eigenvector/eigenvalue of the covariance
    matrix, we get the principal components. There are more than one eigenvector for
    a matrix and they are typically arranged in a descending order of the their eigenvalue,
    denoted by *PC1, PC2* …*PCn.* The first principal component (*PC1*) is the eigenvector
    with the highest eigenvalue which is the red vector shown in the image, which
    explains the maximum variance in the data. Therefore, when using principal components
    to reduce data dimensionality, we select the ones with higher eigenvalues as it
    preserves more information in the original dataset.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了协方差矩阵可以表示数据方差的情况以及特征向量可以在较低维度中捕捉数据方差。通过计算协方差矩阵的特征向量/特征值，我们得到主成分。一个矩阵有多个特征向量，它们通常按特征值的降序排列，用*PC1,
    PC2* …*PCn*表示。第一个主成分（*PC1*）是特征值最大的特征向量，即图片中显示的红色向量，它解释了数据中的最大方差。因此，在使用主成分降低数据维度时，我们选择特征值较大的那些，因为它们保留了原始数据集中的更多信息。
- en: '**Thanks for reaching so far. If you would like to read more of my articles
    on Medium, I would really appreciate your support by signing up** [**Medium membership**](https://destingong.medium.com/membership)☕**.**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**感谢你阅读到这里。如果你想阅读更多我在Medium上的文章，我将非常感激你通过注册** [**Medium会员**](https://destingong.medium.com/membership)☕**。**'
- en: PCA Implementation in Machine Learning
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PCA在机器学习中的实现
- en: We have walked through the theory behind PCA and now let’s step into the practical
    part. Luckily, scikit-learn has provided us an easy implementation of PCA. We
    will use the public dataset “college major” from [fivethirtyeight](https://github.com/fivethirtyeight/data)
    GitHub repository [1].
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探讨了PCA背后的理论，现在让我们进入实际部分。幸运的是，scikit-learn为我们提供了PCA的简单实现。我们将使用来自[fivethirtyeight](https://github.com/fivethirtyeight/data)
    GitHub 仓库的公开数据集“college major” [1]。
- en: '**1\. Standardize data into the same scale**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**1. 数据标准化到相同的尺度**'
- en: PCA is sensitive to data with different scales, as covariance matrix requires
    the data at the same scale to measure the correlation between features with a
    consistent standard. To achieve that, data standardization is applied before PCA,
    which means that each feature has a mean of zero and a standard deviation of one.
    We use the following code snippet to perform data standardization. If you wish
    to know more data transformation techniques such as normalization, min-max scaling,
    please visit my article on “[3 Common Techniques for Data Transformation](https://medium.com/towards-data-science/data-transformation-and-feature-engineering-e3c7dfbb4899)”.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: PCA对不同尺度的数据非常敏感，因为协方差矩阵需要相同尺度的数据来测量特征之间的相关性。为了实现这一点，在PCA之前应用数据标准化，即每个特征的均值为零，标准差为一。我们使用以下代码片段进行数据标准化。如果你希望了解更多数据转换技术，如归一化、最小-最大缩放，请访问我关于“[数据转换的3种常见技术](https://medium.com/towards-data-science/data-transformation-and-feature-engineering-e3c7dfbb4899)”的文章。
- en: '[](/data-transformation-and-feature-engineering-e3c7dfbb4899?source=post_page-----cc9b345b75be--------------------------------)
    [## 3 Common Techniques for Data Transformation'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/data-transformation-and-feature-engineering-e3c7dfbb4899?source=post_page-----cc9b345b75be--------------------------------)
    [## 3种常见的数据变换技术'
- en: How to Choose the Appropriate One for Your Data
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何为您的数据选择合适的方法
- en: towardsdatascience.com](/data-transformation-and-feature-engineering-e3c7dfbb4899?source=post_page-----cc9b345b75be--------------------------------)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/data-transformation-and-feature-engineering-e3c7dfbb4899?source=post_page-----cc9b345b75be--------------------------------)'
- en: '[PRE4]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**2\. Apply PCA on the scaled data**'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**2\. 在缩放数据上应用PCA**'
- en: We then import PCA from `sklearn.decomposition` and specify the number of components
    to generate. The number of components is determined by how much data variance
    to explain by the principal components. Here we will generate 3 components to
    balance the trade off between the explained variance and dimensionality.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们从`sklearn.decomposition`导入PCA，并指定生成的组件数量。组件数量由主成分解释的数据方差量决定。在这里，我们将生成3个组件，以平衡解释方差和维度之间的权衡。
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**3\. Visualize explained variance using scree plot**'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**3\. 使用碎石图可视化解释方差**'
- en: Some information of the original dataset will be lost after shrinking it to
    a lower dimensionality, hence it is important to keep as much information as possible
    while limiting the number of principal components. To help us with the interpretation,
    we can visualize the explained variance using a scree plot. **Explained variance**
    of a principal component indicates the magnitude of data variance in the direction
    of the eigenvector and it correlates to the eigenvalue. Higher explained variance
    means that it preserves more information and the one with highest explained variance
    is the first principal component. We can use the `explained_variance_ratio_` attribute
    to get the explained variance. The code snippet below visualizes the explained
    variance and also the cumulative variance (i.e. sum of variance if we add previous
    principal components together).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 将原始数据集缩减到较低的维度后会丢失一些信息，因此在限制主成分数量的同时尽可能保留信息非常重要。为了帮助我们进行解释，我们可以使用碎石图可视化解释方差。**解释方差**表示主成分在特征向量方向上的数据方差大小，并与特征值相关。较高的解释方差意味着它保留了更多的信息，具有最高解释方差的主成分是第一个主成分。我们可以使用`explained_variance_ratio_`属性获取解释方差。下面的代码片段可视化了解释方差以及累积方差（即如果我们将前面的主成分加在一起的方差总和）。
- en: '[PRE6]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/9356087e69cec799f794d1629ac6219e.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9356087e69cec799f794d1629ac6219e.png)'
- en: scree plot (image by author)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 碎石图（图像由作者提供）
- en: The scree plot tells us about the explained variances when three principal components
    were generated. The first principal component (PC1) explains 60% of the variance
    and 84% of the variance are explained with the first 3 components combined.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 碎石图告诉我们生成三个主成分时的解释方差。第一个主成分（PC1）解释了60%的方差，前三个组件组合解释了84%的方差。
- en: '**4\. Interpret the principal components composition**'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**4\. 解释主成分的组成**'
- en: Principal components additionally provide us some evidence of the importance
    of original features. By evaluating the magnitude and direction of the coefficients
    for each original feature, we know whether the feature is strongly correlated
    with the component. As show below, we generate the coefficients of the features
    with respects to the components.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分还为我们提供了原始特征重要性的一些证据。通过评估每个原始特征系数的大小和方向，我们可以知道该特征是否与主成分高度相关。如下面所示，我们生成了与主成分相关的特征系数。
- en: '[PRE7]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/2f0d591963a2f18ee0e4a6feec98ce7b.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f0d591963a2f18ee0e4a6feec98ce7b.png)'
- en: component coefficients (image by author)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 组件系数（图像由作者提供）
- en: Additionally, we can use `heatmap` from seaborn library to highlight the features
    with high absolute coefficient values.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以使用seaborn库中的`heatmap`来突出显示具有高绝对系数值的特征。
- en: '[PRE8]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/a0d79b9929f257186f719a2078230bee.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a0d79b9929f257186f719a2078230bee.png)'
- en: component coefficients heatmap (image by author)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 组件系数热图（图像由作者提供）
- en: If we interpret PC1 (i.e. row 0), we can see there are multiple features have
    relatively higher association with PC1, such as “Total” (number of enrolled students),
    “Employed”, “Full_time”, “Unemployed” etc, indicating that these features contribute
    more to the data variance. Additionally, you may notice that some features are
    directly correlated with each other, and PCA brings the extra benefit of removing
    multicollinearity among these features.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们解释PC1（即第0行），我们可以看到多个特征与PC1的关联相对较高，例如“Total”（入学学生人数）、“Employed”、“Full_time”、“Unemployed”等，这表明这些特征对数据方差的贡献更大。此外，你可能会注意到一些特征之间存在直接的相关性，而PCA带来了消除这些特征之间多重共线性的额外好处。
- en: '**5\. Use principal components in ML algorithm**'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**5\. 在机器学习算法中使用主成分**'
- en: Finally, we have reduced the dimensionality to a handful of principal components
    which are ready to be utilized as the new features in machine learning algorithms.
    To do so, we are going to use the transformed dataset from the output of PCA process
    — `pca_df`. We can examine the shape of this dataset using `pca_df.shape` and
    we get 173 rows and 3 columns. We then add the label (e.g. “Rank”) back to this
    dataset with 3 principal components from the PCA process and this will become
    the new dataframe to build the ML model.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将维度减少到少量的主成分，这些主成分已准备好作为机器学习算法的新特征。为此，我们将使用PCA过程输出的转化数据集——`pca_df`。我们可以使用`pca_df.shape`检查此数据集的形状，得到173行和3列。然后，我们将标签（例如“Rank”）添加回包含3个主成分的数据集，这将成为构建机器学习模型的新数据框。
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/3f78b1d29f0261e74b8591c68d0a191d.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f78b1d29f0261e74b8591c68d0a191d.png)'
- en: new_df (image by author)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: new_df（图像作者提供）
- en: The remaining process will follow the standard procedure of a machine learning
    lifecycle, that is — split the dataset into train-test, building model and then
    model evaluation. Here we won’t dive into the details of building ML models, but
    if you are interested, please have a look at my article on classification algorithms
    as the starting point.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的过程将遵循机器学习生命周期的标准程序，即——将数据集分为训练集和测试集、建立模型，然后进行模型评估。在这里，我们不会深入探讨构建机器学习模型的细节，但如果你感兴趣，可以查看我的关于分类算法的文章作为起点。
- en: '[](/top-machine-learning-algorithms-for-classification-2197870ff501?source=post_page-----cc9b345b75be--------------------------------)
    [## Top 6 Machine Learning Algorithms for Classification'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/top-machine-learning-algorithms-for-classification-2197870ff501?source=post_page-----cc9b345b75be--------------------------------)
    [## 顶级6种机器学习分类算法'
- en: How to Build a Machine Learning Model Pipeline in Python
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何在Python中构建机器学习模型管道
- en: towardsdatascience.com](/top-machine-learning-algorithms-for-classification-2197870ff501?source=post_page-----cc9b345b75be--------------------------------)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/top-machine-learning-algorithms-for-classification-2197870ff501?source=post_page-----cc9b345b75be--------------------------------)
- en: Take-Home Message
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主要信息
- en: 'In the previous article, we have introduced using linear algebra for data representation
    in machine learning. Now we introduced another common use case of linear algebra
    in ML for dimensionality reduction — Principal Component Analysis (PCA). We firstly
    discussed the theory behind PCA:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一篇文章中，我们介绍了线性代数在机器学习中的数据表示应用。现在我们介绍了线性代数在机器学习中的另一个常见用例——主成分分析（PCA）。我们首先讨论了PCA背后的理论：
- en: represent the data variance using **covariance matrix**
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用**协方差矩阵**表示数据方差
- en: use **eigenvector and eigenvalue** to capture data variance in a lower dimensionality
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用**特征向量和特征值**来捕捉低维度数据中的方差
- en: the **principal component** is the eigenvector and eigenvalue of the covariance
    matrix
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**主成分**是协方差矩阵的特征向量和特征值'
- en: 'Furthermore, we utilize scikit-learn to implement PCA through the following
    procedures:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们利用scikit-learn通过以下程序实现PCA：
- en: standardize data into the same scale
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据标准化为相同的尺度
- en: apply PCA on the scaled data
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在标准化数据上应用PCA
- en: visualize explained variance using scree plot
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用碎石图可视化解释方差
- en: interpret the principal components composition
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释主成分的组成
- en: use principal components in ML algorithms
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在机器学习算法中使用主成分
- en: More Articles Like This
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多相关文章
- en: Linear Algebra for Machine Learning
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的线性代数
- en: '[](/how-is-linear-algebra-applied-for-machine-learning-d193bdeed268?source=post_page-----cc9b345b75be--------------------------------)
    [## How is Linear Algebra Applied for Machine Learning?'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/how-is-linear-algebra-applied-for-machine-learning-d193bdeed268?source=post_page-----cc9b345b75be--------------------------------)
    [## 线性代数如何应用于机器学习？'
- en: Starting from using matrix and vector for data representation
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从使用矩阵和向量进行数据表示开始
- en: towardsdatascience.com](/how-is-linear-algebra-applied-for-machine-learning-d193bdeed268?source=post_page-----cc9b345b75be--------------------------------)
    ![Destin Gong](../Images/dcd4375055f8aa7602b1433a60ad5ca3.png)
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/how-is-linear-algebra-applied-for-machine-learning-d193bdeed268?source=post_page-----cc9b345b75be--------------------------------)
    ![Destin Gong](../Images/dcd4375055f8aa7602b1433a60ad5ca3.png)
- en: '[Destin Gong](https://destingong.medium.com/?source=post_page-----cc9b345b75be--------------------------------)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[Destin Gong](https://destingong.medium.com/?source=post_page-----cc9b345b75be--------------------------------)'
- en: Practical Guides to Machine Learning
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习实用指南
- en: '[View list](https://destingong.medium.com/list/practical-guides-to-machine-learning-a877c2a39884?source=post_page-----cc9b345b75be--------------------------------)10
    stories![Principal Component Analysis for ML](../Images/1edea120a42bd7dc8ab4a4fcdd5b822d.png)![Time
    Series Analysis](../Images/fda8795039b423777fc8e9d8c0dc0d07.png)![deep learning
    cheatsheet for beginner](../Images/b2a4e3806c454a795ddfae0b02828b30.png)![Destin
    Gong](../Images/dcd4375055f8aa7602b1433a60ad5ca3.png)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://destingong.medium.com/list/practical-guides-to-machine-learning-a877c2a39884?source=post_page-----cc9b345b75be--------------------------------)10
    个故事![主成分分析（PCA）](../Images/1edea120a42bd7dc8ab4a4fcdd5b822d.png)![时间序列分析](../Images/fda8795039b423777fc8e9d8c0dc0d07.png)![深度学习初学者备忘单](../Images/b2a4e3806c454a795ddfae0b02828b30.png)![Destin
    Gong](../Images/dcd4375055f8aa7602b1433a60ad5ca3.png)'
- en: '[Destin Gong](https://destingong.medium.com/?source=post_page-----cc9b345b75be--------------------------------)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[Destin Gong](https://destingong.medium.com/?source=post_page-----cc9b345b75be--------------------------------)'
- en: EDA and Feature Engineering Techniques
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: EDA 和特征工程技巧
- en: '[View list](https://destingong.medium.com/list/eda-and-feature-engineering-techniques-e0696974ed54?source=post_page-----cc9b345b75be--------------------------------)9
    stories![](../Images/7fc2bdc73b7b052566cf26034941c232.png)![](../Images/a7c4110e9a854cf9e9eba83dfa46e7d3.png)![](../Images/3ac6d4f7832c8daa758f71b1e479406c.png)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[查看列表](https://destingong.medium.com/list/eda-and-feature-engineering-techniques-e0696974ed54?source=post_page-----cc9b345b75be--------------------------------)9
    个故事![](../Images/7fc2bdc73b7b052566cf26034941c232.png)![](../Images/a7c4110e9a854cf9e9eba83dfa46e7d3.png)![](../Images/3ac6d4f7832c8daa758f71b1e479406c.png)'
- en: Reference
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考
- en: '[1] College Major (FiveThirtyEight.) Retrieved from [https://github.com/fivethirtyeight/data/tree/master/college-majors](https://github.com/fivethirtyeight/data/tree/master/college-majors)
    [[CC-BY-4.0 license](https://github.com/fivethirtyeight/data/blob/master/LICENSE)]'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 大学专业（FiveThirtyEight。） 来源于 [https://github.com/fivethirtyeight/data/tree/master/college-majors](https://github.com/fivethirtyeight/data/tree/master/college-majors)
    [[CC-BY-4.0 许可证](https://github.com/fivethirtyeight/data/blob/master/LICENSE)]'
