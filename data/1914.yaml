- en: PyTorch Model Performance Analysis and Optimization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch 模型性能分析与优化
- en: 原文：[https://towardsdatascience.com/pytorch-model-performance-analysis-and-optimization-10c3c5822869?source=collection_archive---------1-----------------------#2023-06-12](https://towardsdatascience.com/pytorch-model-performance-analysis-and-optimization-10c3c5822869?source=collection_archive---------1-----------------------#2023-06-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/pytorch-model-performance-analysis-and-optimization-10c3c5822869?source=collection_archive---------1-----------------------#2023-06-12](https://towardsdatascience.com/pytorch-model-performance-analysis-and-optimization-10c3c5822869?source=collection_archive---------1-----------------------#2023-06-12)
- en: How to Use PyTorch Profiler and TensorBoard to Accelerate Training and Reduce
    Cost
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用 PyTorch Profiler 和 TensorBoard 加速训练并降低成本
- en: '[](https://chaimrand.medium.com/?source=post_page-----10c3c5822869--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page-----10c3c5822869--------------------------------)[](https://towardsdatascience.com/?source=post_page-----10c3c5822869--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----10c3c5822869--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page-----10c3c5822869--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://chaimrand.medium.com/?source=post_page-----10c3c5822869--------------------------------)[![Chaim
    Rand](../Images/c52659c389f167ad5d6dc139940e7955.png)](https://chaimrand.medium.com/?source=post_page-----10c3c5822869--------------------------------)[](https://towardsdatascience.com/?source=post_page-----10c3c5822869--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----10c3c5822869--------------------------------)
    [Chaim Rand](https://chaimrand.medium.com/?source=post_page-----10c3c5822869--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9440b37e27fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-model-performance-analysis-and-optimization-10c3c5822869&user=Chaim+Rand&userId=9440b37e27fe&source=post_page-9440b37e27fe----10c3c5822869---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----10c3c5822869--------------------------------)
    ·14 min read·Jun 12, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F10c3c5822869&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-model-performance-analysis-and-optimization-10c3c5822869&user=Chaim+Rand&userId=9440b37e27fe&source=-----10c3c5822869---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9440b37e27fe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-model-performance-analysis-and-optimization-10c3c5822869&user=Chaim+Rand&userId=9440b37e27fe&source=post_page-9440b37e27fe----10c3c5822869---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----10c3c5822869--------------------------------)
    ·14 分钟阅读·2023年6月12日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F10c3c5822869&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-model-performance-analysis-and-optimization-10c3c5822869&user=Chaim+Rand&userId=9440b37e27fe&source=-----10c3c5822869---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F10c3c5822869&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-model-performance-analysis-and-optimization-10c3c5822869&source=-----10c3c5822869---------------------bookmark_footer-----------)![](../Images/7f84d26fc7820a898904c7592bfaed50.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F10c3c5822869&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fpytorch-model-performance-analysis-and-optimization-10c3c5822869&source=-----10c3c5822869---------------------bookmark_footer-----------)![](../Images/7f84d26fc7820a898904c7592bfaed50.png)'
- en: Photo by [Torsten Dederichs](https://unsplash.com/es/@tdederichs?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Torsten Dederichs](https://unsplash.com/es/@tdederichs?utm_source=medium&utm_medium=referral)
    提供，来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Training deep learning models, especially large ones, can be a costly expenditure.
    One of the main methods we have at our disposal for managing these costs is performance
    optimization. Performance optimization is an iterative process in which we consistently
    search for opportunities to increase the performance of our application and then
    take advantage of those opportunities. In previous posts (e.g., [here](/overcoming-data-preprocessing-bottlenecks-with-tensorflow-data-service-nvidia-dali-and-other-d6321917f851))
    we have stressed the importance of having appropriate tools for conducting this
    analysis. The tools of choice will likely depend on a number of factors including
    the type of training accelerator (e.g., GPU, HPU, or other) and the training framework.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 训练深度学习模型，尤其是大型模型，可能是一项昂贵的开支。我们可以用来管理这些成本的主要方法之一是性能优化。性能优化是一个迭代的过程，在这个过程中我们不断寻找提高应用程序性能的机会，并利用这些机会。在之前的文章中（例如，[这里](/overcoming-data-preprocessing-bottlenecks-with-tensorflow-data-service-nvidia-dali-and-other-d6321917f851)），我们强调了进行这种分析时拥有适当工具的重要性。所选择的工具可能会依赖于多种因素，包括训练加速器的类型（例如，GPU、HPU或其他）以及训练框架。
- en: '![](../Images/e7992ab67fcc7dccf2f93ae2d9e2c713.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e7992ab67fcc7dccf2f93ae2d9e2c713.png)'
- en: Performance Optimization Flow (By Author)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 性能优化流程（作者提供）
- en: The focus in this post will be on training in PyTorch on GPU. More specifically,
    we will focus on the PyTorch’s built-in performance analyzer, [PyTorch Profiler](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html),
    and on one of the ways to view its results, the [PyTorch Profiler TensorBoard
    plugin](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将重点讨论在GPU上使用PyTorch进行训练。更具体地说，我们将重点关注PyTorch内置的性能分析器，[PyTorch Profiler](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html)，以及查看其结果的方式之一，[PyTorch
    Profiler TensorBoard插件](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html)。
- en: This post is not meant to be a replacement for the official PyTorch documentation
    on either [PyTorch Profiler](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html)
    or the use of the TensorBoard plugin [for analyzing the profiler results](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html).
    Our intention is rather to demonstrate how these tools might be used during the
    course of one’s daily development. In fact, if you haven’t already, we recommend
    that you take a look over the official documentation before reading this post.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本文并不打算取代官方的PyTorch文档，无论是关于[PyTorch Profiler](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html)还是使用TensorBoard插件[分析性能分析结果](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html)。我们的意图是展示这些工具如何在日常开发过程中使用。实际上，如果你还没有阅读官方文档，我们建议你在阅读本文之前先查看一下官方文档。
- en: 'For a while, I have been intrigued by one portion in particular of the [TensorBoard-plugin
    tutorial](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html).
    The tutorial introduces a classification model (based on the Resnet architecture)
    that is trained on the popular Cifar10 dataset. It proceeds to demonstrate how
    PyTorch Profiler and the TensorBoard plugin can be used to [identify and fix a
    bottleneck in the data loader](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html#improve-performance-with-the-help-of-profiler).
    Performance bottlenecks in the input data pipeline are not uncommon and we have
    discussed them at length in some of our previous posts (e.g., [here](/overcoming-data-preprocessing-bottlenecks-with-tensorflow-data-service-nvidia-dali-and-other-d6321917f851)).
    What is surprising about the tutorial is the final (post-optimization) results
    that are presented (as of the time of this writing) which we have pasted in below:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一段时间以来，我特别对[TensorBoard插件教程](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html)中的一个部分感到好奇。该教程介绍了一个基于Resnet架构的分类模型，并在流行的Cifar10数据集上进行训练。它接着展示了如何使用PyTorch
    Profiler和TensorBoard插件来[识别和修复数据加载器中的瓶颈](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html#improve-performance-with-the-help-of-profiler)。输入数据管道中的性能瓶颈并不少见，我们在之前的一些文章中对此进行了详细讨论（例如，[这里](/overcoming-data-preprocessing-bottlenecks-with-tensorflow-data-service-nvidia-dali-and-other-d6321917f851)）。令我惊讶的是教程中呈现的最终（优化后）结果（截至本文撰写时），我们已将其粘贴如下：
- en: '![](../Images/3037fbe369a1e810193072797343f468.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3037fbe369a1e810193072797343f468.png)'
- en: Performance Following Optimization (From [PyTorch Website](https://pytorch.org/tutorials/_static/img/profiler_overview2.png))
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 优化后的性能（来自 [PyTorch 网站](https://pytorch.org/tutorials/_static/img/profiler_overview2.png)）
- en: 'If you look closely, you will see that the post-optimization GPU utilization
    is 40.46%. Now there is no way to sugarcoat this: These results are absolutely
    abysmal and should keep you up at night. As we have expanded on in the past (e.g.,
    [here](/cloud-ml-performance-checklist-caa51e798002)), the GPU is the most expensive
    resource in our training machine and our goal should be to maximize its utilization.
    A 40.46% utilization result usually represents a significant opportunity for training
    acceleration and cost savings. Surely, we can do better! In this blog post we
    will *try* to do better. We will start by attempting to reproduce the results
    presented in the official tutorial and see whether we can use the same tools to
    further improve the training performance.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细观察，你会发现优化后的 GPU 使用率为 40.46%。现在无法掩饰这一点：这些结果绝对糟糕，应该让你夜不能寐。正如我们以前扩展过的（例如，[这里](/cloud-ml-performance-checklist-caa51e798002)），GPU
    是我们训练机器中最昂贵的资源，我们的目标应是最大化其使用率。40.46% 的利用率结果通常代表了显著的训练加速和成本节省机会。我们肯定能做得更好！在这篇博客文章中，我们将*尝试*做得更好。我们将从尝试重现官方教程中展示的结果开始，并看看是否可以使用相同的工具进一步提高训练性能。
- en: Toy Example
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 玩具示例
- en: 'The code block below contains the training loop defined by the [TensorBoard-plugin
    tutorial](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html),
    with two minor modifications:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块包含了由 [TensorBoard-plugin tutorial](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html)
    定义的训练循环，做了两个小的修改：
- en: We use a fake dataset with the same properties and behaviors as the [CIFAR10](https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html)
    dataset that was used in the tutorial. The motivation for this change can be found
    [here](http://groups.csail.mit.edu/vision/TinyImages/).
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用了一个与教程中使用的 [CIFAR10](https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html)
    数据集具有相同属性和行为的虚拟数据集。进行此更改的动机可以在 [这里](http://groups.csail.mit.edu/vision/TinyImages/)
    找到。
- en: We initialize the [torch.profiler.schedule](https://pytorch.org/docs/stable/profiler.html#torch.profiler.schedule)
    with the *warmup* flag set to *3* and the *repeat* flag set to *1*. We found that
    this slight increase in the number of warmup steps improves the stability of the
    profiling results.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们用 *warmup* 标志设置为 *3* 和 *repeat* 标志设置为 *1* 初始化了 [torch.profiler.schedule](https://pytorch.org/docs/stable/profiler.html#torch.profiler.schedule)。我们发现，这稍微增加的预热步骤数量改善了分析结果的稳定性。
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The GPU that was used in the tutorial was a Tesla V100-DGXS-32GB. In this post
    we attempt to reproduce — and improve on — the performance results from the tutorial
    using an [Amazon EC2 p3.2xlarge](https://aws.amazon.com/ec2/instance-types/p3/)
    instance that contains a Tesla V100-SXM2–16GB GPU. Although they share the same
    architecture, there are some differences between the two GPUs which you can learn
    about [here](https://www.nvidia.com/en-us/data-center/v100/). We ran the training
    script using an [AWS PyTorch 2.0 Docker image](https://github.com/aws/deep-learning-containers).
    The performance results of the training script as displayed in the overview page
    of the TensorBoard viewer is captured in the image below:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在教程中使用的 GPU 是 Tesla V100-DGXS-32GB。在这篇文章中，我们尝试重现并改进教程中的性能结果，使用的是包含 Tesla V100-SXM2–16GB
    GPU 的 [Amazon EC2 p3.2xlarge](https://aws.amazon.com/ec2/instance-types/p3/) 实例。尽管它们共享相同的架构，但这两种
    GPU 之间存在一些差异，你可以在 [这里](https://www.nvidia.com/en-us/data-center/v100/) 了解更多信息。我们使用
    [AWS PyTorch 2.0 Docker 镜像](https://github.com/aws/deep-learning-containers) 运行训练脚本。TensorBoard
    观众概览页面中显示的训练脚本性能结果如下面的图像所示：
- en: '![](../Images/1be993ae78153718412f84ece3a05abf.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1be993ae78153718412f84ece3a05abf.png)'
- en: Baseline Performance Results as Shown in the TensorBoard Profiler Overview Tab
    (Captured by Author)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard Profiler 概览标签中的基线性能结果（作者捕获）
- en: We first note that, contrary to the tutorial, the Overview page (of [torch-tb-profiler](https://pypi.org/project/torch-tb-profiler/)
    version 0.4.1) in our experiment combined the three profiling steps into one .
    Thus, the average overall step time is 80 milliseconds and not 240 milliseconds
    as reported. This can be seen clearly in the Trace tab (which, in our experience,
    almost always provides a more accurate report) where each step takes ~80 milliseconds.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先注意到，与教程相反，我们实验中的概览页面（[torch-tb-profiler](https://pypi.org/project/torch-tb-profiler/)
    版本0.4.1）将三个分析步骤合并为一个。因此，平均总体步骤时间是80毫秒，而不是报告中的240毫秒。这在Trace标签中可以清楚地看到（根据我们的经验，Trace标签几乎总是提供更准确的报告），每个步骤大约需要80毫秒。
- en: '![](../Images/fa7e6b427fb8929b878a816affdb8996.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fa7e6b427fb8929b878a816affdb8996.png)'
- en: Baseline Performance Results as Shown in the TensorBoard Profiler Trace View
    Tab (Captured by Author)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard Profiler跟踪视图标签中的基准性能结果（作者捕获）
- en: Note that our starting point of 31.65% GPU utilization and a step time of 80
    milliseconds is different than the starting point presented in the tutorial of
    23.54% and 132 milliseconds, respectively. This is likely a result of differences
    in the training environment including the GPU type and the PyTorch version. We
    also note that while the tutorial baseline results clearly diagnose the performance
    issue as a bottleneck in the DataLoader, our results do not. We have often found
    that data loading bottlenecks will disguise themselves as a high percentage of
    “CPU Exec” or “Other” in the Overview tab.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们的起始点31.65% GPU利用率和80毫秒步骤时间与教程中的起始点23.54%和132毫秒有所不同。这可能是由于训练环境的差异，包括GPU类型和PyTorch版本。我们还注意到，虽然教程基准结果明确诊断出DataLoader是瓶颈，我们的结果则没有。我们发现数据加载瓶颈往往会伪装成“CPU
    Exec”或“Other”在概览标签中显示为高百分比。
- en: 'Optimization #1: Multi-process Data Loading'
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '优化 #1：多进程数据加载'
- en: 'Let’s start by applying [multi process data loading](https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-loading)
    as [described in the tutorial](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html#improve-performance-with-the-help-of-profiler).
    Being that the [Amazon EC2 p3.2xlarge](https://aws.amazon.com/ec2/instance-types/p3/)
    instance has 8 vCPUs, we set the number of [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)
    workers to 8 for maximum performance:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始应用[多进程数据加载](https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-loading)如[教程中所述](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html#improve-performance-with-the-help-of-profiler)。鉴于[Amazon
    EC2 p3.2xlarge](https://aws.amazon.com/ec2/instance-types/p3/)实例具有8个vCPUs，我们将[DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)工作进程数设置为8，以获得最佳性能：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The results of this optimization are displayed below:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 该优化的结果如下所示：
- en: '![](../Images/f7c0643684275394383b2a2154d54f5b.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f7c0643684275394383b2a2154d54f5b.png)'
- en: Results of Multi-proc Data Loading in the TensorBoard Profiler Overview Tab
    (Captured by Author)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard Profiler概览标签中的多进程数据加载结果（作者捕获）
- en: The change to a single line of code increased the GPU utilization by more than
    200% (31.65% from to 72.81%), and more than halved our training step time, (from
    80 milliseconds down to 37).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 单行代码的更改使GPU利用率提高了200%以上（从31.65%提升至72.81%），并且将我们的训练步骤时间减少了一半以上（从80毫秒降至37毫秒）。
- en: This is where the optimization process in the tutorial comes to end. Although
    our GPU utilization (72.81%) is quite a bit higher than the results in the tutorial
    (40.46%), I have no doubt that, like us, you find these results to still be quite
    unsatisfactory.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是教程中的优化过程结束的地方。尽管我们的GPU利用率（72.81%）明显高于教程中的结果（40.46%），但我毫不怀疑，你会发现这些结果仍然相当不令人满意。
- en: '**Personal commentary that you should feel free to skip**: Imagine how much
    global money could be saved if PyTorch applied multi-process data loading **by
    default** when training on GPU! True, there may be some unwanted side-effects
    to using multiprocessing. Nevertheless, there must be some form of auto-detection
    algorithm that could be run that would rule out the presence of potentially problematic
    scenarios and apply this optimization accordingly.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**个人评论，你可以选择跳过**：想象一下如果PyTorch在GPU训练时**默认**应用多进程数据加载将节省多少全球资金！的确，使用多进程可能会有一些不良副作用。然而，必须有某种形式的自动检测算法可以运行，以排除潜在问题的存在并相应地应用此优化。'
- en: 'Optimization #2: Memory Pinning'
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '优化 #2：内存锁定'
- en: If we analyze the Trace view of our last experiment, we can see that a significant
    amount of time (10 out of 37 milliseconds) is still spent on loading the training
    data into the GPU.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们分析最后一次实验的Trace视图，我们可以看到仍然有相当多的时间（37毫秒中的10毫秒）花费在将训练数据加载到GPU上。
- en: '![](../Images/6d1d7033866a30dc56d38e38515eba0d.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d1d7033866a30dc56d38e38515eba0d.png)'
- en: Results of Multi-proc Data Loading in the Trace View Tab (Captured by Author)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Trace视图标签中的多进程数据加载结果（由作者捕获）
- en: To address this, we will apply another PyTorch-recommended optimization for
    streamlining the data input flow, [memory pinning](https://pytorch.org/docs/stable/notes/cuda.html#use-pinned-memory-buffers).
    Using pinned memory can increase the speed of host to GPU data copy and, more
    importantly, allows us to make them asynchronous. This means that we can prepare
    the next training batch **in the GPU** in parallel to running the training step
    on the current batch. It is important to note that although [asynchronous execution](https://pytorch.org/docs/stable/notes/cuda.html#asynchronous-execution)
    will generally increase performance, it can also **reduce the accuracy of time
    measurements**. For the purposes of our blog post we will continue to use the
    measurements reported by PyTorch Profiler. See [here](https://pytorch.org/docs/stable/notes/cuda.html#asynchronous-execution)
    for instructions on how to attain precise measurements. For additional details
    on memory pinning and its side effects, please see the [PyTorch documentation](https://pytorch.org/docs/stable/notes/cuda.html#use-pinned-memory-buffers).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们将应用另一种PyTorch推荐的优化来简化数据输入流程，[内存固定](https://pytorch.org/docs/stable/notes/cuda.html#use-pinned-memory-buffers)。使用固定内存可以提高主机到GPU的数据复制速度，更重要的是，可以使它们异步进行。这意味着我们可以在对当前批次进行训练步骤的同时**在GPU中**准备下一个训练批次。需要注意的是，尽管[异步执行](https://pytorch.org/docs/stable/notes/cuda.html#asynchronous-execution)通常会提高性能，但也可能**降低时间测量的准确性**。为了我们博客文章的目的，我们将继续使用PyTorch
    Profiler报告的测量结果。有关如何获得准确测量的说明，请参见[这里](https://pytorch.org/docs/stable/notes/cuda.html#asynchronous-execution)。有关内存固定及其副作用的更多详细信息，请参阅[PyTorch文档](https://pytorch.org/docs/stable/notes/cuda.html#use-pinned-memory-buffers)。
- en: This memory-pinning optimization requires changes to two lines of code. First,
    we set the *pin_memory* flag of the [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)
    to True.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这种内存固定优化需要对两行代码进行更改。首先，我们将[DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)的*pin_memory*标志设置为True。
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then we modify the host-to-device memory transfer (in the *train* function)
    to be non-blocking:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将主机到设备的内存传输（在*train*函数中）修改为非阻塞模式：
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The results of the memory pinning optimization are displayed below:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 内存固定优化的结果如下所示：
- en: '![](../Images/99308bf791b6cab1174661fa1a488451.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99308bf791b6cab1174661fa1a488451.png)'
- en: Results of Memory Pinning in the TensorBoard Profiler Overview Tab (Captured
    by Author)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard Profiler概览标签中的内存固定结果（由作者捕获）
- en: Our GPU utilization now stands at a respectable 92.37% and our step time has
    further decreased. But we can still do better. Note that despite this optimization,
    the performance report continues to indicate that we are spending a lot of time
    copying the data into the GPU. We will come back to this in step 4 below.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的GPU利用率现在达到了92.37%，并且步骤时间进一步减少。但我们仍然可以做得更好。请注意，尽管进行了优化，性能报告仍继续表明我们花费了大量时间将数据复制到GPU上。我们将在下面的第4步中回到这个问题。
- en: 'Optimization #3: Increase Batch Size'
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '优化 #3：增加批量大小'
- en: 'For our next optimization, we draw our attention to the [Memory View](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html#analyze-performance-with-other-advanced-features)
    of the last experiment:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的下一个优化，我们将注意力转向最后一次实验的[内存视图](https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html#analyze-performance-with-other-advanced-features)：
- en: '![](../Images/2b16dc265436222bd68a440d24fd5ac9.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b16dc265436222bd68a440d24fd5ac9.png)'
- en: Memory View in TensorBoard Profiler (Captured by Author)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard Profiler中的内存视图（由作者捕获）
- en: The chart shows that out of 16 GB of GPU memory, we are peaking at less than
    1 GB of utilization. This is an extreme example of resource under-utilization
    that often (though not always) indicates an opportunity to boost performance.
    One way to control the memory utilization is to increase the batch size. In the
    image below we display the performance results when we increase the batch size
    to 512 (and the memory utilization to 11.3 GB).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图表显示，在 16 GB 的 GPU 内存中，我们的利用率峰值低于 1 GB。这是资源利用不足的极端例子，通常（但并非总是）表明存在提升性能的机会。控制内存利用率的一种方法是增加批量大小。在下面的图像中，我们展示了将批量大小增加到
    512（内存利用率增加到 11.3 GB）时的性能结果。
- en: '![](../Images/0c4604a9367bc204d0d41ea6113f89f3.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c4604a9367bc204d0d41ea6113f89f3.png)'
- en: Results of Increasing Batch Size in the TensorBoard Profiler Overview Tab (Captured
    by Author)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorBoard Profiler 概述选项卡中增加批量大小的结果（由作者捕捉）
- en: Although the GPU utilization measure did not change much, our training speed
    has increased considerably, from 1200 samples per second (46 milliseconds for
    batch size 32) to 1584 samples per second (324 milliseconds for batch size 512).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 GPU 利用率的度量变化不大，但我们的训练速度已经显著提高，从每秒 1200 个样本（批量大小 32 时 46 毫秒）增加到每秒 1584 个样本（批量大小
    512 时 324 毫秒）。
- en: '**Caution**: Contrary to our previous optimizations, increasing the batch size
    could have an impact on the behavior of your training application. Different models
    exhibit different levels of sensitivity to a change in batch size. Some may require
    nothing more than some tuning to the optimizer settings. For others, adjusting
    to a large batch size may be more difficult or even impossible. See [this previous
    post](/a-guide-to-highly-distributed-dnn-training-9e4814fb8bd3) for some of the
    challenges involved in training on large batches.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**警告**：与我们之前的优化相反，增加批量大小可能会对您的训练应用程序的行为产生影响。不同的模型对批量大小的变化表现出不同程度的敏感性。有些模型可能只需要对优化器设置进行一些调整。对于其他模型，调整到较大的批量大小可能会更困难甚至不可能。有关训练大批量数据的一些挑战，请参见[这篇之前的文章](/a-guide-to-highly-distributed-dnn-training-9e4814fb8bd3)。'
- en: 'Optimization #4: Reduce Host to Device Copy'
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '优化 #4：减少主机到设备的拷贝'
- en: 'You probably noticed the big red eyesore representing the host-to-device data
    copy in the pie chart from our previous results. The most direct way of trying
    to address this kind of bottleneck is to see if we can reduce the amount of data
    in each batch. Notice that in the case of our image input, we convert the data
    type from an 8-bit unsigned integer to a 32-bit float and apply normalization
    before performing the data copy. In the code block below, we propose a change
    to the input data flow in which we delay the data type conversion and normalization
    until the data is on the GPU:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能注意到了之前结果中饼图上代表主机到设备数据拷贝的大红色眼睛。解决这种瓶颈的最直接方法是查看是否可以减少每批次中的数据量。注意，在我们的图像输入的情况下，我们将数据类型从
    8 位无符号整数转换为 32 位浮点数，并在执行数据拷贝之前应用归一化。在下面的代码块中，我们建议对输入数据流进行更改，将数据类型转换和归一化的操作延迟到数据在
    GPU 上之后：
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As a result of this change the amount of data being copied from the CPU to
    the GPU is reduced by 4x and the red eyesore virtually disappears:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 由于此更改，从 CPU 到 GPU 复制的数据量减少了 4 倍，红色的眼睛几乎消失：
- en: '![](../Images/f892c522888084fa498da4e0e0103bbe.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f892c522888084fa498da4e0e0103bbe.png)'
- en: Results of Reducing CPU to GPU Copy in the TensorBoard Profiler Overview Tab
    (Captured by Author)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorBoard Profiler 概述选项卡中减少 CPU 到 GPU 拷贝的结果（由作者捕捉）
- en: We now stand at a new high of 97.51%(!!) GPU utilization and a training speed
    of 1670 samples per second! Let’s see what else we can do.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在达到了 97.51%(!!) 的 GPU 利用率和每秒 1670 个样本的训练速度！让我们看看还能做些什么。
- en: 'Optimization #5: Set Gradients to None'
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '优化 #5：将梯度设置为 None'
- en: 'At this stage we appear to be fully utilizing the GPU, but that doesn’t mean
    that we can’t utilize it more effectively. One popular optimization that is said
    to reduce memory operations in the GPU is to set the model parameters gradients
    to *None* rather than *zero* in each training step. Please see the [PyTorch documentation](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#use-parameter-grad-none-instead-of-model-zero-grad-or-optimizer-zero-grad)
    for more details on this optimization. All that is required to implement this
    optimization is to set the *set_to_none* of the *optimizer.zero_grad* call to
    *True*:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一阶段，我们似乎已经完全利用了 GPU，但这并不意味着我们不能更有效地利用它。一种流行的优化方法是将模型参数梯度设置为 *None* 而不是 *zero*，以减少
    GPU 中的内存操作。有关此优化的更多细节，请参阅 [PyTorch 文档](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html#use-parameter-grad-none-instead-of-model-zero-grad-or-optimizer-zero-grad)。实施此优化所需的只是将
    *optimizer.zero_grad* 调用的 *set_to_none* 设置为 *True*：
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In our case this optimization did not boost our performance in any meaningful
    way.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，这种优化并未显著提升我们的性能。
- en: 'Optimization #6: Automatic Mixed Precision'
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '优化 #6: 自动混合精度'
- en: 'The GPU Kernel View displays the amount of time that the GPU kernels were active
    and can be a helpful resource for improving GPU utilization:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 内核视图显示了 GPU 内核活动的时间量，可以作为提高 GPU 利用率的有用资源：
- en: '![](../Images/0a4d656c21df4c0b7f6dc79187fd8989.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a4d656c21df4c0b7f6dc79187fd8989.png)'
- en: Kernel View in TensorBoard Profiler (Captured by Author)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard Profiler 中的内核视图（作者捕获）
- en: One of the most glaring details in this report is the lack of use of the GPU
    [Tensor Cores](https://www.nvidia.com/en-us/data-center/tensor-cores/). Available
    on relatively newer GPU architectures, [Tensor Cores](https://www.nvidia.com/en-us/data-center/tensor-cores/)
    are dedicated processing units for matrix multiplication that can boost AI application
    performance significantly. Their lack of use may represent a major opportunity
    for optimization.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 本报告中最明显的细节之一是 GPU [Tensor Cores](https://www.nvidia.com/en-us/data-center/tensor-cores/)
    的使用不足。Tensor Cores 是相对较新的 GPU 架构上提供的矩阵乘法专用处理单元，可以显著提升 AI 应用的性能。它们的未被使用可能代表了一个重要的优化机会。
- en: Being that Tensor Cores are specifically designed for mixed-precision computing,
    one straight-forward way to increase their utilization is to modify our model
    to use [Automatic Mixed Precision (AMP)](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html).
    In AMP mode portions of the model are automatically cast to lower-precision 16-bit
    floats and run on the GPU TensorCores.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Tensor Cores 是专门为混合精度计算设计的，一种直接增加其利用率的方法是修改我们的模型以使用 [自动混合精度（AMP）](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html)。在
    AMP 模式下，模型的部分会自动转换为较低精度的 16 位浮点数，并在 GPU TensorCores 上运行。
- en: Importantly, note that a full implementation of AMP may require [gradient scaling](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html#adding-gradscaler)
    which we do not include in our demonstration. Be sure to see the [documentation](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html)
    on mixed precision training before adapting it.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，完整实现 AMP 可能需要 [梯度缩放](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html#adding-gradscaler)，我们在演示中没有包括。请在适应之前务必查看有关混合精度训练的
    [文档](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html)。
- en: The modification to the training step required to enable AMP is demonstrated
    in the code block below.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 启用 AMP 所需对训练步骤的修改在下面的代码块中演示。
- en: '[PRE6]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The impact to the Tensor Core utilization is displayed in the image below. Although
    it continues to indicate opportunity for further improvement, with just one line
    of code the utilization jumped from 0% to 26.3%.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Tensor Core 利用率的影响显示在下图中。尽管它继续显示出进一步改进的机会，但仅通过一行代码，利用率从 0% 跃升至 26.3%。
- en: '![](../Images/9c1640ca3893f2528ea939153468c655.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c1640ca3893f2528ea939153468c655.png)'
- en: Tensor Core Utilization with AMP optimization from Kernel View in TensorBoard
    Profiler (Captured by Author)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Tensor Core 利用率与 AMP 优化在 TensorBoard Profiler 中的内核视图（作者捕获）
- en: 'In addition to increasing Tensor Core utilization, using AMP lowers the GPU
    memory utilization freeing up more space to increase the batch size. The image
    below captures the training performance results following the AMP optimization
    and the batch size set to **1024**:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 除了增加 Tensor Core 利用率外，使用 AMP 还降低了 GPU 内存利用率，从而释放出更多空间以增加批量大小。下图捕获了在 AMP 优化后和批量大小设置为
    **1024** 时的训练性能结果：
- en: '![](../Images/fbeaa6ad297094aa3f249dc2a64ade35.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fbeaa6ad297094aa3f249dc2a64ade35.png)'
- en: Results of AMP Optimization in the TensorBoard Profiler Overview Tab (Captured
    by Author)
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard Profiler 概览标签中的AMP优化结果（作者捕获）
- en: Although the GPU utilization has slightly decreased, our primary throughput
    metric has further increased by nearly 50%, from 1670 samples per second to 2477\.
    We are on a roll!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管GPU利用率略有下降，但我们的主要吞吐量指标仍然增加了近50%，从每秒1670个样本提高到2477个样本。我们正处于一个好状态！
- en: '**Caution**: Lowering the precision of portions of your model could have a
    meaningful effect on its convergence. As in the case of increasing the batch size
    (see above) the impact of using mixed precision will vary per model. In some cases,
    AMP will work with little to no effort. Other times you might need to work a bit
    harder to tune the autoscaler. Still other times you might need to set the precision
    types of different portions of the model explicitly (i.e., *manual* mixed precision).'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**警告**：降低模型部分的精度可能会对其收敛性产生重要影响。与增加批量大小的情况一样（见上文），使用混合精度的影响会因模型而异。在某些情况下，AMP将几乎无需努力即可工作。其他情况下，可能需要更多的工作来调整自动缩放器。还有些情况下，可能需要明确设置模型不同部分的精度类型（即，*手动*混合精度）。'
- en: For more details on using mixed precision as a method for memory optimization
    please see our [previous blog post](/how-to-increase-training-performance-through-memory-optimization-1000d30351c8)
    on the topic.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解有关使用混合精度作为内存优化方法的更多细节，请参阅我们的[上一篇博客文章](/how-to-increase-training-performance-through-memory-optimization-1000d30351c8)。
- en: 'Optimization #7: Train in Graph Mode'
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '优化 #7：以图模式训练'
- en: The final optimization we will apply is model compilation. Contrary to the default
    PyTorch eager-execution mode in which each PyTorch operation is run “eagerly”,
    the [compile](https://pytorch.org/docs/stable/generated/torch.compile.html) API
    converts your model into an intermediate computation graph which it then compiles
    into low-level compute kernels in a manner that is optimal for the underlying
    training accelerator. For more on model compilation in PyTorch 2, check out our
    [previous post](https://medium.com/towards-data-science/tips-and-tricks-for-upgrading-to-pytorch-2-3127db1d1f3d)
    on the topic.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将应用的最终优化是模型编译。与PyTorch默认的“急切执行”模式相反，[compile](https://pytorch.org/docs/stable/generated/torch.compile.html)
    API将模型转换为中间计算图，然后将其编译为低级计算内核，以优化底层训练加速器的方式进行处理。有关PyTorch 2中模型编译的更多信息，请查看我们的[上一篇文章](https://medium.com/towards-data-science/tips-and-tricks-for-upgrading-to-pytorch-2-3127db1d1f3d)。
- en: 'The following code block demonstrates the change required to apply model compilation:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块演示了应用模型编译所需的更改：
- en: '[PRE7]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The results of the model compilation optimization are displayed below:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 模型编译优化的结果如下所示：
- en: '![](../Images/bb76afa59cdbd5da27b174ba4da08e85.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bb76afa59cdbd5da27b174ba4da08e85.png)'
- en: Results of Graph Compilation in the TensorBoard Profiler Overview Tab (Captured
    by Author)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard Profiler 概览标签中的图编译结果（作者捕获）
- en: Model compilation further increases our throughput to 3268 samples per second
    compared to 2477 in the previous experiment, an additional 32% (!!) boost in performance.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前实验中的2477个样本相比，模型编译进一步将我们的吞吐量提高到每秒3268个样本，性能提升了额外的32%（!!）。
- en: The manner in which graph compilation changes the training step is very evident
    in the different views of the TensorBoard plugin. The Kernel View, for example,
    indicates the use of new (fused) GPU kernels, and the Trace View (shown below)
    displays a wholly different pattern than what we saw previously.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图编译改变训练步骤的方式在TensorBoard插件的不同视图中非常明显。例如，Kernel View显示了新（融合）GPU内核的使用，而Trace View（如下所示）显示出与之前完全不同的模式。
- en: '![](../Images/1c8f0a07215f762ff0d0d411563111c8.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c8f0a07215f762ff0d0d411563111c8.png)'
- en: Results of Graph Compilation in the TensorBoard Profiler Trace View Tab (Captured
    by Author)
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard Profiler 追踪视图标签中的图编译结果（作者捕获）
- en: Interim Results
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 临时结果
- en: In the table below we summarize the results of the successive optimizations
    we have applied.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在下表中，我们总结了我们应用的连续优化的结果。
- en: '![](../Images/d3db3ec31e38435735bf806d5c9f6f2d.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d3db3ec31e38435735bf806d5c9f6f2d.png)'
- en: Performance Results Summary (By Author)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 性能结果总结（作者）
- en: By applying our iterative approach of **analysis and optimization** using PyTorch
    Profiler and the TensorBoard plugin, **we were able to increase performance by
    817%**!!
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用PyTorch Profiler和TensorBoard插件进行**分析和优化**的迭代方法，**我们将性能提高了817%**！！
- en: '**Is our work complete? Absolutely not!** Each optimization that we implement
    uncovers new potential opportunities for performance improvement. These opportunities
    are presented in the form of resources being freed up (e.g., the way in which
    moving to mixed precision enabled our increasing the batch size) or in the form
    of newly uncovered performance bottlenecks (e.g., the way in which our final optimization
    uncovered a bottleneck in host-to-device data transfer). Furthermore, there are
    many other well-known forms of optimization that we did not attempt in this post
    (e.g., see [here](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)
    and [here](https://pytorch.org/docs/stable/notes/cuda.html)). And lastly, new
    library optimizations (e.g., the model compilation feature that we demonstrated
    in step 7), are released all the time, further enabling our performance improvement
    objectives. As we emphasized in the introduction, to fully leverage such opportunities,
    performance optimization must be an iterative and consistent part of your development
    workflow.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们的工作完成了吗？绝对没有！** 我们实施的每项优化都会揭示新的性能改进机会。这些机会以资源被释放的形式呈现（例如，转向混合精度使我们能够增加批量大小），或者以新发现的性能瓶颈的形式呈现（例如，我们最终的优化揭示了主机到设备数据传输中的瓶颈）。此外，还有许多我们在这篇文章中没有尝试的知名优化形式（例如，参见
    [这里](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html) 和 [这里](https://pytorch.org/docs/stable/notes/cuda.html)）。最后，新库优化（例如，我们在第7步演示的模型编译功能）不断发布，进一步推动我们的性能提升目标。正如我们在介绍中强调的，要充分利用这些机会，性能优化必须成为你开发工作流程中的一个迭代且持续的部分。'
- en: Summary
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this post we have demonstrated the significant potential of performance optimization
    on a toy classification model. Although there are other performance analyzers
    that you can use, each with their pros and cons, we chose PyTorch Profiler and
    the TensorBoard plugin due to their ease of integration.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们展示了对一个玩具分类模型进行性能优化的重大潜力。虽然你可以使用其他性能分析工具，每种工具都有其优缺点，但我们选择了 PyTorch Profiler
    和 TensorBoard 插件，因为它们的集成方便。
- en: We should emphasize that the path to successful optimization will vary greatly
    based on the details of the training project, including the model architecture
    and training environment. In practice, reaching your goals may be more difficult
    than in the example we presented here. Some of the techniques we described may
    have little impact on your performance or might even make it worse. We also note
    that the precise optimizations that we chose, and the order in which we chose
    to apply them, was somewhat arbitrary. You are highly encouraged to develop your
    own tools and techniques for reaching your optimization goals based on the specific
    details of your project.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该强调的是，成功优化的路径会因训练项目的细节而大相径庭，包括模型架构和训练环境。在实践中，实现你的目标可能比我们在这里展示的例子要困难得多。我们描述的一些技术可能对你的性能影响不大，甚至可能使其更糟。我们还指出，我们选择的具体优化以及选择应用的顺序在某种程度上是随意的。强烈建议你根据项目的具体细节开发自己的工具和技术，以实现优化目标。
- en: Performance optimization of machine learning workloads is sometimes viewed as
    secondary, non-critical, and odious. I hope that we have succeeded in convincing
    you that the potential for savings in development time and cost warrant a meaningful
    investment in performance analysis and optimization. And, hey, you might even
    find it to be fun :).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习工作负载的性能优化有时被视为次要的、非关键的和令人厌烦的。我希望我们已经成功地说服你，开发时间和成本的节省潜力值得对性能分析和优化进行有意义的投资。而且，你可能会发现这其实也很有趣
    :）。
- en: What Next?
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下一步？
- en: This was just the tip of the iceberg. There is a lot more to performance optimization
    than we have covered here. In a [sequel to this post](https://medium.com/towards-data-science/pytorch-model-performance-analysis-and-optimization-part-2-3bc241be91),
    we will dive into a performance issue that is quite common in PyTorch models in
    which portions of the computation is run on the CPU rather than the GPU, often
    in a manner that is unbeknownst to the developer. We also encourage you to check
    out our [other posts on medium](https://chaimrand.medium.com/), many of which
    cover different elements of performance optimization of machine learning workloads.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这仅仅是冰山一角。性能优化远比我们在这里所涵盖的内容要复杂得多。在[这篇文章的续集](https://medium.com/towards-data-science/pytorch-model-performance-analysis-and-optimization-part-2-3bc241be91)中，我们将深入探讨一个在
    PyTorch 模型中相当常见的性能问题，即计算的部分在 CPU 上运行而不是 GPU 上，这种情况通常对开发者来说是未知的。我们还鼓励你查看我们在[Medium上的其他文章](https://chaimrand.medium.com/)，其中许多文章涉及机器学习工作负载性能优化的不同方面。
