- en: PyTorch Image Classification Tutorial for Beginners
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch 初学者图像分类教程
- en: 原文：[https://towardsdatascience.com/pytorch-image-classification-tutorial-for-beginners-94ea13f56f2](https://towardsdatascience.com/pytorch-image-classification-tutorial-for-beginners-94ea13f56f2)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/pytorch-image-classification-tutorial-for-beginners-94ea13f56f2](https://towardsdatascience.com/pytorch-image-classification-tutorial-for-beginners-94ea13f56f2)
- en: Fine-tuning pre-trained Deep Learning models in Python
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Python 中微调预训练的深度学习模型
- en: '[](https://medium.com/@iamleonie?source=post_page-----94ea13f56f2--------------------------------)[![Leonie
    Monigatti](../Images/4044b1685ada53a30160b03dc78f9626.png)](https://medium.com/@iamleonie?source=post_page-----94ea13f56f2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----94ea13f56f2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----94ea13f56f2--------------------------------)
    [Leonie Monigatti](https://medium.com/@iamleonie?source=post_page-----94ea13f56f2--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@iamleonie?source=post_page-----94ea13f56f2--------------------------------)[![Leonie
    Monigatti](../Images/4044b1685ada53a30160b03dc78f9626.png)](https://medium.com/@iamleonie?source=post_page-----94ea13f56f2--------------------------------)[](https://towardsdatascience.com/?source=post_page-----94ea13f56f2--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----94ea13f56f2--------------------------------)
    [Leonie Monigatti](https://medium.com/@iamleonie?source=post_page-----94ea13f56f2--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----94ea13f56f2--------------------------------)
    ·22 min read·May 9, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----94ea13f56f2--------------------------------)
    ·22 min 阅读·2023年5月9日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/f968859c4ae9f104408eddb0177d11ab.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f968859c4ae9f104408eddb0177d11ab.png)'
- en: “Not sure if this is supposed to be a lion or a cheetah…”
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: “不确定这应该是狮子还是猎豹……”
- en: This practical tutorial shows you how to classify images using a pre-trained
    Deep Learning model with the PyTorch framework.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实用教程将展示如何使用 PyTorch 框架中的预训练深度学习模型进行图像分类。
- en: The difference between this beginner-friendly image classification tutorial
    to others is that we are not building and training the Deep neural network from
    scratch. In practice, only a few people train neural networks from scratch. Instead,
    most Deep Learning practitioners use a pre-trained model and fine-tune it to a
    new task.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这个面向初学者的图像分类教程与其他教程的不同之处在于，我们并不会从头开始构建和训练深度神经网络。在实际操作中，只有少数人会从头训练神经网络。相反，大多数深度学习从业者会使用预训练模型，并将其微调以适应新任务。
- en: In practice, only a few people train neural networks from scratch.
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在实际操作中，只有少数人会从头训练神经网络。
- en: The specific problem setting is to build a binary image classification model
    to classify images of cheetahs and lions based on a small dataset. For this purpose,
    we will fine-tune a pre-trained image classification model using PyTorch.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 特定的问题设置是构建一个二分类图像分类模型，以根据小型数据集对猎豹和狮子的图像进行分类。为此，我们将使用 PyTorch 微调一个预训练的图像分类模型。
- en: '![](../Images/70915a988ae6fb23d0bca9760fa94edc.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/70915a988ae6fb23d0bca9760fa94edc.png)'
- en: Sample images from the dataset [1].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的示例图像 [1]。
- en: 'This tutorial follows a basic Machine Learning workflow:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程遵循基本的机器学习工作流程：
- en: '[Prepare and explore data](#758f)'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[准备和探索数据](#758f)'
- en: '[Build a baseline](#9377)'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[建立基准](#9377)'
- en: '[Run experiments](#a980)'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[运行实验](#a980)'
- en: '[Make predictions](#52fc)'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[做出预测](#52fc)'
- en: You can follow along in [my related Kaggle Notebook](https://www.kaggle.com/iamleonie/pytorch-image-classification-tutorial-for-beginner).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 [我的相关 Kaggle 笔记本](https://www.kaggle.com/iamleonie/pytorch-image-classification-tutorial-for-beginner)中跟随教程。
- en: Prerequisites and Setup
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前提条件和设置
- en: Ideally, you should have some familiarity with Python.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，你应该对 Python 有一定的了解。
- en: 'As this is a practical tutorial, we will only cover how to build an image classification
    model at a high level. We will not cover a lot of theory, such as how convolutional
    layers or backpropagation work. I will mark sections where you can dig deeper
    once you feel comfortable with this topic with this sign: ⚒️'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个实用教程，我们只会在高层次上覆盖如何构建图像分类模型。我们不会涵盖很多理论，比如卷积层或反向传播的工作原理。一旦你对这个话题感到舒适，我会用这个标志
    ⚒️ 标记你可以深入了解的部分。
- en: If you want to supplement this guide with some theoretical background information,
    I recommend the free Kaggle Learn courses on [Deep Learning](https://www.kaggle.com/learn/intro-to-deep-learning)
    and [Computer Vision](https://www.kaggle.com/learn/computer-vision).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想为本指南补充一些理论背景信息，我推荐免费的Kaggle Learn课程，内容包括[深度学习](https://www.kaggle.com/learn/intro-to-deep-learning)和[计算机视觉](https://www.kaggle.com/learn/computer-vision)。
- en: 'Let’s begin by importing PyTorch and other relevant libraries:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始导入PyTorch和其他相关库：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The essential libraries are PyTorch (version 1.13.0) for deep learning, OpenCV
    (version 4.5.4) for image processing, and Albumentations (version 1.3.0) for data
    augmentation.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的库包括用于深度学习的PyTorch（版本1.13.0）、用于图像处理的OpenCV（版本4.5.4）以及用于数据增强的Albumentations（版本1.3.0）。
- en: 'Step 1: Prepare and Explore Data'
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一步：准备和探索数据
- en: The first step is to become familiar with the data. For this tutorial, we will
    keep the exploratory data analysis step short.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是熟悉数据。对于本教程，我们将简短地进行探索性数据分析。
- en: First, we will load the data. The example dataset [1] has two folders with images
    — one folder for each class.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将加载数据。示例数据集[1]包含两个文件夹，每个文件夹中都有图像——每个类别一个文件夹。
- en: '![](../Images/f9d20055393d1e5aa66c4f43970b2317.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f9d20055393d1e5aa66c4f43970b2317.png)'
- en: Example dataset [1] for binary image classification.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 二分类图像分类的示例数据集[1]。
- en: The following code goes through all subfolders and creates a Pandas dataframe
    containing the file name and its label.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码遍历所有子文件夹，并创建一个包含文件名及其标签的Pandas数据框。
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*Insert your data here!* — To follow along in this article, your dataset should
    look something like this:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*在此处插入你的数据！* — 为了跟上本文的内容，你的数据集应该类似于此：'
- en: '![](../Images/034de54983eaa8ef3e481109edfeb194.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/034de54983eaa8ef3e481109edfeb194.png)'
- en: Example dataset [1] for binary image classification. Insert your data here.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 二分类图像分类的示例数据集[1]。在此处插入你的数据。
- en: 'We have about 170 photographs: roughly 85 lions and 85 cheetahs (see remark
    in [1]). This is a very small but balanced dataset. It’s perfect for fine-tuning!'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们大约有170张照片：大致85张狮子照片和85张猎豹照片（见[1]中的备注）。这是一个非常小但平衡的数据集，非常适合微调！
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/3cc4c584f515ba83009a692a57ec0241.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3cc4c584f515ba83009a692a57ec0241.png)'
- en: Class distribution of sample dataset for image classification plotted with seaborn
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 使用seaborn绘制的图像分类样本数据集的类别分布
- en: 'To get a feeling for the dataset, it is always a good idea to plot a few samples:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对数据集有一个感觉，绘制一些样本总是个好主意：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/70915a988ae6fb23d0bca9760fa94edc.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/70915a988ae6fb23d0bca9760fa94edc.png)'
- en: Sample images from the dataset [1].
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集[1]中的样本图像。
- en: By exploring a dataset like this, you can gain some insights. E.g. as you can
    see here, the images are not limited to the animals but also to statues.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通过探索这样的数据集，你可以获得一些见解。例如，正如你在这里看到的，图像不仅限于动物，还包括雕像。
- en: Before we go any further, let’s split the dataset into training and testing
    data. The training data will be used to build our model, and the test data will
    be a hold-out dataset to evaluate the final model’s performance on unseen data.
    In this example, we will set 10% of the data aside for testing.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进一步操作之前，让我们将数据集拆分为训练数据和测试数据。训练数据将用于构建我们的模型，而测试数据将作为保留数据集来评估最终模型在未见数据上的性能。在本示例中，我们将把10%的数据留作测试用。
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/47d794f1876c7aa60f46a35c06c9603b.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/47d794f1876c7aa60f46a35c06c9603b.png)'
- en: Splitting the data into training and testing datasets (Inspired by [scikit-learn](https://scikit-learn.org/stable/modules/cross_validation.html))
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据拆分为训练集和测试集（灵感来源于[scikit-learn](https://scikit-learn.org/stable/modules/cross_validation.html)）
- en: 'Step 2: Build a Baseline'
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二步：建立基线
- en: 'Next, we will build a baseline. A baseline consists of three key components:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将构建一个基线。基线包括三个关键组件：
- en: A [data pipeline for loading images](#d7e6)
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个[加载图像的数据管道](#d7e6)
- en: A [model](#e194) with [loss function and optimizer](#9994)
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个[模型](#e194)，包含[损失函数和优化器](#9994)
- en: A [training pipeline](#8867), including a [cross-validation strategy](#bfd5)
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个[训练管道](#8867)，包括一个[交叉验证策略](#bfd5)
- en: In this section, we will go through each component and finally [wrap it up](#8d70)
    nicely.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将逐个组件进行讲解，并最终[整理](#8d70)好。
- en: 'Because training a Deep Learning model includes a lot of experimentation, we
    want to be able to switch out specific parts of the code quickly. Thus, we will
    try to make the following code as modular as possible and work with a configuration
    for tuning:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 由于训练深度学习模型涉及大量实验，我们希望能够快速切换代码的特定部分。因此，我们将尽可能使以下代码模块化，并使用配置进行调优：
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We will add the configurable parameters as we go along.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将随着进展添加可配置参数。
- en: Build a data pipeline for loading images
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建用于加载图像的数据管道
- en: 'First, you must build a pipeline to load, preprocess and feed your images to
    the neural network in batches (instead of all at once). PyTorch provides two core
    classes you can use for this purpose:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你必须构建一个管道，以批次方式加载、预处理和喂入图像到神经网络中（而不是一次性加载）。PyTorch 提供了两个核心类供你使用：
- en: '`Dataset` class: Loads and preprocesses the dataset. You will need to customize
    this class for your purpose.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Dataset` 类：加载和预处理数据集。你需要根据你的需求自定义这个类。'
- en: '`Dataloader` class: Loads batches of data samples to the neural network.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Dataloader` 类：将数据样本批次加载到神经网络中。'
- en: 'First, you need to customize the `Dataset` class. Its key components are:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要自定义 `Dataset` 类。其关键组件是：
- en: 'Constructor: to load the dataset as, e.g., Pandas Dataframe'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构造函数：用于加载数据集，例如 Pandas Dataframe
- en: '`__len__()`: to get the length of the dataset. This usually will require minimal
    adjustments related to how you pass in the dataset.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`__len__()`：获取数据集的长度。这通常只需要对数据集传递方式进行最小的调整。'
- en: '`__getitem__()`: to get a sample from the dataset by index. This is usually
    the part where you modify most of the code depending on any preprocessing you
    want to do.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`__getitem__()`：通过索引从数据集中获取样本。这通常是你根据需要执行预处理时修改代码的地方。'
- en: Below you can find a template to customize the `Dataset` class.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 下面你可以找到一个用于自定义 `Dataset` 类的模板。
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: When loading your dataset, you can also perform any required preprocessing,
    such as transforms or image standardization. This happens in `__getitem__()`.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载数据集时，你也可以执行任何需要的预处理，如变换或图像标准化。这发生在 `__getitem__()` 中。
- en: 'In this example, we first load the image from the root directory (`cfg.root_dir`)
    with OpenCV and convert it to the RGB color space. Then we will apply basic transforms:
    Resize the image (`cfg.image_size`) and convert the image from a NumPy array to
    a tensor. Finally, we will normalize the values of the image to be in the [0,
    1] range by dividing the values by 255.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们首先使用 OpenCV 从根目录 (`cfg.root_dir`) 加载图像，并将其转换为 RGB 颜色空间。然后我们将应用基本转换：调整图像大小
    (`cfg.image_size`)，并将图像从 NumPy 数组转换为张量。最后，我们将图像的值标准化到 [0, 1] 范围，通过除以 255 实现。
- en: '[PRE7]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Next, we need a `Dataloader` to feed the samples of the `Dataset` to the neural
    network in batches because we (probably) don’t have enough RAM to feed all the
    images to the model at once.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要一个 `Dataloader` 来将 `Dataset` 的样本批次喂入神经网络，因为我们（可能）没有足够的 RAM 一次性喂入所有图像。
- en: You need to provide the `Dataloader` the instance of the `Dataset` you want
    to navigate, the size of the batches (`cfg.batch_size`), and the information on
    whether to shuffle the data.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要提供 `Dataloader` 你要遍历的 `Dataset` 实例、批次大小 (`cfg.batch_size`)，以及是否打乱数据的信息。
- en: '[PRE8]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The batch size should be fixed throughout the training and not be tuned [2].
    Because the training speed is related to the batch size, we want to use the biggest
    batch size possible. Start with a batch size of 32 and then increase it in powers
    of two (64, 128, etc.) until you get a memory error, and then use the last batch
    size.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 批次大小应在训练过程中保持固定，不应调整 [2]。因为训练速度与批次大小相关，我们希望使用尽可能大的批次大小。首先使用 32 的批次大小，然后按二的幂次增加（64,
    128 等），直到出现内存错误，然后使用最后的批次大小。
- en: 'When you iterate over the `Dataloader`, it will give you batches of samples
    from the customized `Dataset`. Let’s retrieve the first batch for a sanity check:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当你遍历 `Dataloader` 时，它会给你来自自定义 `Dataset` 的样本批次。让我们取出第一个批次进行验证：
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `Dataloader` returns the image batch and a label batch. The `image_batch`
    is a tensor of the shape `(32, 3, 256, 256)`. This is a batch of 32 (`batch_size`)
    images with the shape `(3, 256, 256)` (`color_channels, image_height, image_width`).
    The `label_batch` is a tensor of the shape `(32)`. These are the corresponding
    labels to the 32 images.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`Dataloader` 返回图像批次和标签批次。`image_batch` 是形状为 `(32, 3, 256, 256)` 的张量。这是一个包含
    32 张图像的批次，每张图像的形状为 `(3, 256, 256)`（`color_channels, image_height, image_width`）。`label_batch`
    是形状为 `(32)` 的张量。这些是与 32 张图像对应的标签。'
- en: '![](../Images/ddbe050da4919b3d8f12a78c7e466dd0.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ddbe050da4919b3d8f12a78c7e466dd0.png)'
- en: Example output of the Dataloader with customized Dataset
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义数据集的 Dataloader 示例输出
- en: This section explained how to build a data pipeline. In a later section (see
    [Setup a training pipeline](#8867)), we will use the `Dataset` and `Dataloader`
    to create separate pipelines for training, validation, and testing.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 本节解释了如何构建数据管道。在后面的章节中（见 [设置训练管道](#8867)），我们将使用 `Dataset` 和 `Dataloader` 创建用于训练、验证和测试的独立管道。
- en: Before we train the model, we need to split the training data again into a training
    and a validation dataset. Training the model on a dataset and then evaluating
    the model on the same data is a methodological mistake because the model just
    needs to memorize the labels of the seen samples. Thus, instead of generalizing,
    the model will overfit to the training data.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型之前，我们需要将训练数据再次拆分为训练集和验证集。在一个数据集上训练模型然后在相同数据上评估模型是一种方法上的错误，因为模型只需记住已见样本的标签。因此，模型会过拟合训练数据，而不是进行泛化。
- en: To avoid overfitting, let’s randomly partition the training data into training
    and validation sets with the `train_test_split()` function for now. This section
    will later be replaced with a [cross-validation strategy](#bfd5).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免过拟合，我们暂时使用 `train_test_split()` 函数将训练数据随机分割为训练集和验证集。本节稍后将被 [交叉验证策略](#bfd5)
    替代。
- en: '[PRE11]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/5e5fed82448ece523ca25ca82c849770.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e5fed82448ece523ca25ca82c849770.png)'
- en: Splitting the training data again into training and validation (Inspired by
    [scikit-learn](https://scikit-learn.org/stable/modules/cross_validation.html))
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 将训练数据再次拆分为训练集和验证集（灵感来源于 [scikit-learn](https://scikit-learn.org/stable/modules/cross_validation.html)）
- en: 'With this split, we can now create `Datasets` and `Dataloaders` for the training
    and validation data:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个分割，我们现在可以为训练和验证数据创建 `Datasets` 和 `Dataloaders`：
- en: '[PRE12]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Prepare the model
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备模型
- en: This is the part where we would learn about building a neural network in PyTorch.
    When I started learning about Deep Learning, I thought building neural networks
    was an important part of training Deep Learning models. But the reality is that
    this is what researchers do for us. We, the practitioners, get to lean back and
    use the final models for our purposes.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分将学习如何在 PyTorch 中构建神经网络。当我开始学习深度学习时，我认为构建神经网络是训练深度学习模型的重要部分。但实际上，这是研究人员为我们完成的工作。我们这些从业者只需使用最终模型即可。
- en: The researchers try different model architectures, such as convolutional neural
    networks (CNNs), and usually train image classification models on large baseline
    datasets, such as ImageNet [3]. We call these models **backbones**.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员尝试不同的模型架构，例如卷积神经网络（CNN），通常会在大型基准数据集（如 ImageNet [3]）上训练图像分类模型。我们称这些模型为 **骨干网**。
- en: '![](../Images/71e4d33bf3881fa1181e6f58a6a1e940.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/71e4d33bf3881fa1181e6f58a6a1e940.png)'
- en: 'Expectation vs. reality: In practice, only a few people train neural networks
    from scratch for image classification'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 期望与现实：实际上，只有少数人从头开始训练用于图像分类的神经网络
- en: Fine-tuning a pre-trained neural network works so well because the first few
    layers often learn general features (such as edge detection).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 微调预训练神经网络之所以有效，是因为前几层通常会学习到通用特征（如边缘检测）。
- en: '*⚒️ Of course, you should understand how neural networks work in general, including
    backpropagation, and how different layers, such as convolutional layers, work.
    However, to follow along in this practical tutorial, you don’t need to understand
    these details right now. Once you have finished this tutorial, you can fill in
    some theoretical gaps with the free Kaggle Learn courses on* [*Deep Learning*](https://www.kaggle.com/learn/intro-to-deep-learning)
    *and* [*Computer Vision*](https://www.kaggle.com/learn/computer-vision)*.*'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*⚒️ 当然，你应该了解神经网络的一般工作原理，包括反向传播，以及不同层（如卷积层）的工作方式。然而，为了跟上这个实际教程，你现在不需要理解这些细节。完成本教程后，你可以通过免费的
    Kaggle Learn 课程填补一些理论空白，课程包括* [*深度学习*](https://www.kaggle.com/learn/intro-to-deep-learning)
    *和* [*计算机视觉*](https://www.kaggle.com/learn/computer-vision)*。*'
- en: '**Fantastic backbones and where to find them** — Now, which of these pre-trained
    models should you choose, and where do you get these from?'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**绝妙的骨干网及其获取途径** — 现在，你应该选择哪些预训练模型，以及从哪里获得这些模型？'
- en: In this tutorial, we will use `[timm](https://timm.fast.ai/)` — a Deep Learning
    library containing a collection of state-of-the-art computer vision models created
    by [Ross Wightman](https://twitter.com/wightmanr) — to get pre-trained models.
    (You can use `torchvision.models` for pre-trained models, but I personally find
    it easier to switch out backbones during experimentation with `timm`.)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将使用 `[timm](https://timm.fast.ai/)` — 一个包含由 [Ross Wightman](https://twitter.com/wightmanr)
    创建的先进计算机视觉模型集合的深度学习库 — 来获取预训练模型。（你可以使用 `torchvision.models` 来获取预训练模型，但我个人觉得在实验中使用
    `timm` 更容易更换骨干网络。）
- en: '[PRE13]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'There is a lot to unpack in this little piece of code. Let’s go step-by-step:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码包含很多内容。让我们一步步来解读：
- en: '`backbone = ''resnet18''`— In this example, we use a ResNet [5] with 18 layers.
    ResNet stands for Residual Network, and it is a type of CNN using so-called residual
    blocks.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`backbone = ''resnet18''`— 在这个例子中，我们使用一个 18 层的 ResNet [5]。ResNet 代表残差网络，它是一种使用所谓的残差块的
    CNN。'
- en: ⚒️*We will skip over the details of ResNet and residual blocks. If you are interested
    in the technical details,* [*you can dig deeper into this post, for example.*](/introduction-to-resnets-c0a830a288a4)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ⚒️*我们将跳过 ResNet 和残差块的详细内容。如果你对技术细节感兴趣，* [*你可以深入了解这篇文章，例如。*](/introduction-to-resnets-c0a830a288a4)
- en: 'There are many different models in the ResNet family, such as ResNet18, ResNet34,
    etc., where the number stands for how many layers the network has. As a (very
    rough) rule of thumb: The higher the number of layers, the better the performance.
    You can print `timm.list_models(''*resnet*'')` to see what other models are available.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet 系列中有许多不同的模型，如 ResNet18、ResNet34 等，其中的数字表示网络的层数。一个（非常粗略的）经验法则是：层数越多，性能越好。你可以打印
    `timm.list_models('*resnet*')` 来查看其他可用的模型。
- en: ⚒️ *Learn about different backbones for computer vision/image classification
    like ResNet, DenseNet, and EfficientNet.*
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ⚒️ *了解不同的计算机视觉/图像分类骨干网络，如 ResNet、DenseNet 和 EfficientNet。*
- en: '`pretrained = True` — This means we want the weights of the model trained on
    ImageNet [3]. If this is set to `False`, you will only get the model''s architecture
    without the weights [6].'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`pretrained = True` — 这意味着我们希望使用在 ImageNet [3] 上训练的模型权重。如果设置为 `False`，你将只得到模型的结构而没有权重
    [6]。'
- en: '`num_classes = cfg.n_classes` — Because the model was pre-trained on ImageNet
    [3], you will get a classifier with the 1000 classes that are in ImageNet. Thus,
    you need to remove the ImageNet classifier and define how many classes you have
    in your problem [6]. If you set `num_classes = 0`, you will get the model without
    a classifier [6].'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`num_classes = cfg.n_classes` — 由于模型是在 ImageNet [3] 上预训练的，你将得到一个包含 ImageNet
    中 1000 个类别的分类器。因此，你需要移除 ImageNet 分类器并定义你问题中的类别数量 [6]。如果你设置 `num_classes = 0`，你将得到没有分类器的模型
    [6]。'
- en: To check output size, you can pass in a sample batch `X` with 3 channels of
    random values in the image size [6].
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查输出大小，你可以传入一个具有随机值的 3 通道样本批次 `X`，尺寸为 [6]。
- en: '[PRE14]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: It will output `torch.Size([1, cfg.n_classes])` [6].
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 它将输出 `torch.Size([1, cfg.n_classes])` [6]。
- en: '![](../Images/1c2041f5ab5aaf83088a4affb21f9796.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c2041f5ab5aaf83088a4affb21f9796.png)'
- en: Model inputs and outputs
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的输入和输出
- en: Prepare loss function and optimizer
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备损失函数和优化器
- en: 'Next, to train a model, there are two key ingredients:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，训练一个模型有两个关键要素：
- en: a loss function (criterion),
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个损失函数（准则），
- en: an optimization algorithm (optimizer), and
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个优化算法（优化器），和
- en: optionally a learning rate scheduler.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可选的学习率调度器。
- en: '**Loss function** — Common loss functions are the following:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**损失函数** — 常见的损失函数有：'
- en: '**Binary cross-entropy (BCE) loss** for binary classification.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**二分类交叉熵（BCE）损失**用于二分类。'
- en: '**Categorical cross-entropy** loss for multi-class classification.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类交叉熵**损失用于多分类。'
- en: '**Mean squared loss** for regression.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均方损失**用于回归。'
- en: Although we have a binary classification problem, you can also use categorical
    cross-entropy loss. If you like, you can switch out the loss function with BCE.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们有一个二分类问题，但你也可以使用分类交叉熵损失。如果你愿意，可以将损失函数更换为 BCE。
- en: '[PRE15]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**Optimizer —** The optimization algorithm minimizes the loss function (in
    our case, the cross-entropy loss). There are many different optimizers available.
    Let’s use a popular one: [Adam](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam).'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**优化器 —** 优化算法通过最小化损失函数（在我们的例子中是交叉熵损失）来优化模型。有很多不同的优化器可以选择。我们使用一个流行的优化器：[Adam](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam)。'
- en: '[PRE16]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[**Learning rate scheduler**](/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863)—
    A learning rate scheduler adapts the value of the learning rate during the training
    process. Although you don’t have to use a learning rate scheduler, using one can
    help the algorithm converge faster. This is because if the learning rate stays
    constant, it can prevent you from finding the optimum if it is too large, and
    it can take too long to converge if it is too small.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[**学习率调度器**](/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863)—
    学习率调度器会在训练过程中调整学习率的值。虽然你不必使用学习率调度器，但使用它可以帮助算法更快收敛。这是因为如果学习率保持不变，如果学习率过大，它可能会阻碍你找到最佳解，如果学习率过小，则可能需要很长时间才能收敛。'
- en: There are many different learning rate schedulers available, but Kaggle Grandmasters
    recommend using **cosine decay as a learning rate scheduler for fine-tuning**
    [2].
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同的学习率调度器可用，但 Kaggle 大师建议将**余弦衰减作为微调的学习率调度器** [2]。
- en: '[PRE17]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '`T_max` defines the half period and should be equal to the maximum number of
    iterations (`np.ceil(len(train_dataloader.dataset) /cfg.batch_size)*cfg.epochs`).'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`T_max` 定义了半周期，应该等于最大迭代次数（`np.ceil(len(train_dataloader.dataset) /cfg.batch_size)*cfg.epochs`）。'
- en: 'The resulting learning rates will look as follows over the course of a training
    run:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程中，学习率的变化情况如下所示：
- en: '![](../Images/1e89dff534f52a0dae5b40aa8617819a.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1e89dff534f52a0dae5b40aa8617819a.png)'
- en: Cosine decay learning rate scheduler
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦衰减学习率调度器
- en: '**Metric** — While we’re at it, let’s also define a metric to evaluate the
    model’s overall performance. Again, there are many different metrics. For this
    example, we will use accuracy as the metric:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**指标** — 既然我们在谈论它，我们还需要定义一个指标来评估模型的整体性能。同样，有许多不同的指标。对于这个示例，我们将使用准确率作为指标：'
- en: '[PRE18]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Don’t confuse the metric with the loss function. The loss function is used to
    optimize the learning function during training, while the metric measures the
    model’s performance after the training.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 不要将指标与损失函数混淆。损失函数用于在训练过程中优化学习函数，而指标则在训练后衡量模型的性能。
- en: ⚒️ *Learn about different metrics and which ones are suited for which problems.*
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ⚒️ *了解不同的指标以及哪些指标适用于哪些问题。*
- en: Setup a training pipeline
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置训练管道
- en: This is probably, the most complex but also most interesting part of this tutorial.
    Are you ready?
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是本教程中最复杂但也是最有趣的部分。你准备好了吗？
- en: A model is typically trained in iterations. One iteration is called an epoch.
    Training from scratch usually requires many epochs, while fine-tuning requires
    only a few (roughly 5 to 10) epochs.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 模型通常以迭代的方式进行训练。一轮迭代称为一个 epoch。从头开始训练通常需要许多 epochs，而微调只需要几个（大约 5 到 10）epochs。
- en: 'In each epoch, the model is trained on the full training data and then validated
    on the full validation data. We will now define two functions: One function to
    train (`train_an_epoch()`) and one function to validate the model on an epoch
    (`validate_an_epoch()`).'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个 epoch 中，模型会在完整的训练数据上进行训练，然后在完整的验证数据上进行验证。我们现在将定义两个函数：一个用于训练（`train_an_epoch()`），另一个用于在一个
    epoch 上验证模型（`validate_an_epoch()`）。
- en: 'Below you can see the training function:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 下面你可以看到训练函数：
- en: '[PRE19]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Let’s go through it step-by-step:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步步来看：
- en: Set the model to the training mode. The model can also be in evaluation mode.
    This mode affects the behavior of the layers `[Dropout](https://pytorch.org/docs/stable/_modules/torch/nn/modules/dropout.html)`
    and `[BatchNorm](https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html)`
    in a model.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型设置为训练模式。模型也可以处于评估模式。这种模式会影响模型中 `[Dropout](https://pytorch.org/docs/stable/_modules/torch/nn/modules/dropout.html)`
    和 `[BatchNorm](https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html)`
    层的行为。
- en: Iterate over the training data in small batches. The samples and labels need
    to be moved to GPU if you use one for faster training (`cfg.device`).
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以小批量迭代训练数据。如果你使用 GPU 进行更快的训练，则需要将样本和标签移动到 GPU (`cfg.device`)。
- en: Clear the last error gradient of the optimizer.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 清除优化器的最后一个误差梯度。
- en: Do a forward pass of the input through the model.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过模型进行一次前向传播。
- en: Calculate the loss for the model output.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算模型输出的损失。
- en: Backpropagate the error through the model.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过模型反向传播误差。
- en: Update the model to reduce the loss.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新模型以减少损失。
- en: Step the learning rate scheduler.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步进学习率调度器。
- en: Calculate the loss and metric for statistics. Because the predictions will be
    Tensors on the GPU, just like the inputs, we need to [detach the Tensor](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.detach)
    from the automatic differentiation graph and call the NumPy function to convert
    them to NumPy arrays
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算损失和指标以获取统计数据。由于预测将是GPU上的张量，就像输入一样，我们需要[分离张量](https://pytorch.org/docs/stable/autograd.html#torch.Tensor.detach)以便将它们从自动微分图中分离，并调用NumPy函数将其转换为NumPy数组。
- en: 'Next, we define the validation function as shown below:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义如下所示的验证函数：
- en: '[PRE20]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let’s go through it step-by-step again:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再一步步看一遍：
- en: Set the model to the evaluation mode.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型设置为评估模式。
- en: Iterate over the validation data in small batches. The samples and labels need
    to be moved to GPU if you use one for faster training.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对验证数据进行小批量迭代。如果使用GPU进行更快的训练，样本和标签需要移动到GPU上。
- en: Do a forward pass of the input through the model.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过模型进行前向传播。
- en: Calculate the loss and metric for statistics.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算损失和指标以获取统计数据。
- en: 'At first glance, training and validating an epoch looks similar. Let’s look
    at a code comparison to make the differences clearer:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 初看，训练和验证一个时期看起来很相似。让我们看看代码比较，以使差异更清晰：
- en: '![](../Images/3cc0433231a4710415b44ccf035e3136.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3cc0433231a4710415b44ccf035e3136.png)'
- en: Screenshot of side-by-side code comparison in BeyondCompare of training and
    validation code in PyTorch
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中训练和验证代码的对比截图
- en: 'You can see the following differences:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到以下差异：
- en: The model has to be in training or evaluation mode.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型必须处于训练或评估模式。
- en: For training the model, we need an optimizer and an optional scheduler. For
    validation, we only need the model.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练模型时，我们需要一个优化器和一个可选的调度器。对于验证，我们只需要模型。
- en: The gradient calculation is only active for training. For validation, we don’t
    need it.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度计算仅在训练时激活。对于验证，我们不需要它。
- en: Cross-validation strategy
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉验证策略
- en: Now, we are not yet done with the training pipeline. Earlier, we divided the
    training data into training and validation data. But partitioning the available
    data into two fixed sets limits the number of training samples.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们还没有完成训练管道。之前，我们将训练数据分为训练数据和验证数据。但是，将可用数据分为两个固定的集合限制了训练样本的数量。
- en: 'Instead, we will use a cross-validation strategy by splitting the training
    data into *k* folds. The model is then trained in *k* separate iterations, in
    which the model is trained on *k*-1 folds and validated on one fold for each iteration
    while the folds switch at every iteration as shown below:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们将使用交叉验证策略，将训练数据分为*k*个折叠。然后，模型将在*k*次独立的迭代中训练，其中每次迭代模型在*k*-1个折叠上训练，并在一个折叠上进行验证，每次迭代折叠都会切换，如下所示：
- en: '![](../Images/3862554f761e899f0916eceefae3063e.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3862554f761e899f0916eceefae3063e.png)'
- en: Splitting the training data again into training and validation (Inspired by
    [scikit-learn](https://scikit-learn.org/stable/modules/cross_validation.html))
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 将训练数据再次分为训练和验证（灵感来源于[scikit-learn](https://scikit-learn.org/stable/modules/cross_validation.html)）
- en: In this example, we are using `[StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html)`
    to create the splits. You could use `KFold` instead but `StratifiedKFold` has
    the advantage that it preserves the class distribution.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用了`[StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html)`来创建分割。你也可以使用`KFold`，但`StratifiedKFold`的优点是它保持了类分布。
- en: '[PRE21]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Adding data augmentation
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加数据增强
- en: When the difference between the training and validation metric is significant,
    this indicates that the model is overfitting to the training data. Overfitting
    occurs when a model is trained on only a few examples and learns irrelevant details
    or noise from the training data. This negatively affects the model’s performance
    when it’s presented with new examples. As a result, the model struggles to generalize
    on new images.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练和验证指标之间的差异显著时，这表明模型正在过拟合训练数据。过拟合发生在模型仅在少量示例上进行训练，并从训练数据中学习无关的细节或噪声。这会对模型在呈现新示例时的表现产生负面影响。结果，模型在新图像上的泛化能力受限。
- en: To overcome overfitting during the training process, you can use data augmentation.
    Data augmentation generates additional training data by randomly transforming
    existing images. This technique exposes the model to more aspects of the data,
    helping it to generalize better.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在训练过程中克服过拟合，你可以使用数据增强。数据增强通过随机变换现有图像生成额外的训练数据。这种技术让模型接触到数据的更多方面，从而帮助它更好地泛化。
- en: 'We can use some prepared data augmentations from the `albumentations` package,
    such as:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`albumentations`包中的一些准备好的数据增强方法，例如：
- en: Rotating images (`A.Rotate()`)
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像旋转（`A.Rotate()`）
- en: Horizontal flipping (`A.HorizontalFlip()`)
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 水平翻转（`A.HorizontalFlip()`）
- en: Cutout [4] (`A.CoarseDropout()`)
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 切割 [4]（`A.CoarseDropout()`）
- en: Earlier, we defined a basic transform to resize and convert the image to a tensor.
    We will continue to use it for the validation and testing datasets because they
    don’t need any augmentations. For the training dataset, we create a new transform
    `transform_soft` , which has the three above augmentations in addition to the
    resizing and conversion to tensor.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们定义了一个基本的变换来调整图像大小并将其转换为张量。我们将继续在验证和测试数据集中使用它，因为它们不需要任何增强。对于训练数据集，我们创建了一个新的变换`transform_soft`，它在调整大小和转换为张量之外，还包含了上述三种增强。
- en: '[PRE22]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: You can control the percentage of images the augmentations are applied to with
    the parameter `p`.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过参数`p`来控制增强应用到图像的百分比。
- en: 'If we visualize a few samples from the augmented dataset, we can see that the
    three augmentations are applied successfully:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们可视化从增强数据集中提取的一些样本，我们可以看到三种增强成功应用：
- en: Rotation in images 0, 1, 2, 4
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像0、1、2、4中的旋转
- en: Horizontal flip is difficult to detect if you don’t know the original image,
    but we can see that image 2 must be horizontally flipped
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 水平翻转很难检测，如果你不知道原始图像，但我们可以看到图像2必须是水平翻转的
- en: Cutout (coarse dropout) in images 1 and 4
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像1和4中的切割（粗略丢弃）
- en: '![](../Images/0ab00b43ca8d9fdcd569f85b8599abfe.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ab00b43ca8d9fdcd569f85b8599abfe.png)'
- en: Augmented training dataset
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 增强后的训练数据集
- en: ⚒️ *Next, you can review and add other image augmentation techniques, e.g.,
    Mixup and Cutmix, to your pipeline.*
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ⚒️ *接下来，你可以审查并添加其他图像增强技术，例如Mixup和Cutmix，到你的管道中。*
- en: '[](/cutout-mixup-and-cutmix-implementing-modern-image-augmentations-in-pytorch-a9d7db3074ad?source=post_page-----94ea13f56f2--------------------------------)
    [## Cutout, Mixup, and Cutmix: Implementing Modern Image Augmentations in PyTorch'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/cutout-mixup-and-cutmix-implementing-modern-image-augmentations-in-pytorch-a9d7db3074ad?source=post_page-----94ea13f56f2--------------------------------)
    [## Cutout, Mixup, and Cutmix: 在PyTorch中实现现代图像增强'
- en: Data augmentation techniques for Computer Vision implemented in Python
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在Python中实现的计算机视觉数据增强技术
- en: towardsdatascience.com](/cutout-mixup-and-cutmix-implementing-modern-image-augmentations-in-pytorch-a9d7db3074ad?source=post_page-----94ea13f56f2--------------------------------)
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/cutout-mixup-and-cutmix-implementing-modern-image-augmentations-in-pytorch-a9d7db3074ad?source=post_page-----94ea13f56f2--------------------------------)
- en: Putting it all together
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将所有内容整合在一起
- en: 'Now that we have discussed each component of the baseline from the [data pipeline](#d7e6)
    to the [model](#e194) with [loss function and optimizer](#9994), to the [training
    pipeline](#8867), including a [cross-validation strategy](#bfd5), we can put it
    all together as shown in the image below:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了从[data pipeline](#d7e6)到[model](#e194)的基线的每个组件，包括[loss function and
    optimizer](#9994)，到[training pipeline](#8867)，以及[cross-validation strategy](#bfd5)，我们可以将它们综合起来，如下图所示：
- en: '![](../Images/787eb3ca571d744451bf7876241a0a59.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/787eb3ca571d744451bf7876241a0a59.png)'
- en: Flowchart of the baseline code
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 基线代码的流程图
- en: We will iterate over each fold of our [cross-validation strategy](#bfd5). Within
    each fold, we set up a [data pipeline](#d7e6) for the training and validation
    data and a [model](#e194) with [loss function and optimizer](#9994). Then for
    each epoch, we will [train and validate the model](#8867).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将遍历[交叉验证策略](#bfd5)的每一个折叠。在每个折叠中，我们为训练和验证数据设置一个[data pipeline](#d7e6)和一个[model](#e194)，并配备[loss
    function and optimizer](#9994)。然后，对于每个时期，我们将[训练和验证模型](#8867)。
- en: Before we touch anything, let’s set ourselves up for success and fix the random
    seeds to ensure **reproducible** results.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在动手之前，让我们为成功做好准备，固定随机种子以确保**可重复**的结果。
- en: '[PRE23]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Next, we will write a `fit()` function that fits the model for all epochs. The
    function iterates over the number of epochs, while the training and validation
    functions contain inner loops that iterate over the batches in the training and
    validation datasets, as discussed in the section about the [training pipeline](#8867).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将编写一个`fit()`函数，该函数为所有周期拟合模型。该函数会迭代周期数，而训练和验证函数包含内循环，这些内循环会迭代训练和验证数据集中的批次，如[训练管道](#8867)部分所述。
- en: '[PRE24]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![](../Images/9119aad99942cb843e6f9b296c2cb37a.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9119aad99942cb843e6f9b296c2cb37a.png)'
- en: Log of the fit function
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 拟合函数的日志
- en: 'For visualization purposes, we will also create plots of the loss and accuracy
    on the training and validation sets:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化目的，我们还将创建训练和验证集上的损失和准确性图：
- en: '[PRE25]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![](../Images/811ac6b17726482a35bd9b79bdc5bf83.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/811ac6b17726482a35bd9b79bdc5bf83.png)'
- en: Plotted history of metric and loss over epochs
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制的指标和损失随周期变化的历史
- en: 'When we combine everything, it will look as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将所有内容结合起来时，它将如下所示：
- en: '[PRE26]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Step 3: Run Experiments'
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 3：运行实验
- en: Data Science is an experimental science. Thus, the aim of this step is to find
    the best configuration of hyperparameters, data augmentations, model backbones,
    and cross-validation strategy that achieve the best performance (or whatever your
    objective may be — e.g., best trade-off between performance and inference time).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学是一门实验科学。因此，这一步的目的是找到实现最佳性能的超参数配置、数据增强、模型主干和交叉验证策略（或任何你的目标，例如，性能和推理时间之间的最佳权衡）。
- en: Setup experiment tracking
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置实验跟踪
- en: Before jumping into this step, take a minute to think about how you will track
    your experiments. [Experiment tracking](https://medium.com/@iamleonie/intro-to-mlops-experiment-tracking-for-machine-learning-858e432bd133)
    can be as simple as writing everything down with pen and paper. Alternatively,
    you can track everything in a spreadsheet or even use an experiment tracking system
    to automate the whole process.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入这一步之前，花点时间考虑一下你将如何跟踪实验。[实验跟踪](https://medium.com/@iamleonie/intro-to-mlops-experiment-tracking-for-machine-learning-858e432bd133)可以简单到用笔和纸记录一切。或者，你可以在电子表格中跟踪所有内容，甚至使用实验跟踪系统来自动化整个过程。
- en: '[](https://medium.com/@iamleonie/intro-to-mlops-experiment-tracking-for-machine-learning-858e432bd133?source=post_page-----94ea13f56f2--------------------------------)
    [## Intro to MLOps: Experiment Tracking for Machine Learning'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@iamleonie/intro-to-mlops-experiment-tracking-for-machine-learning-858e432bd133?source=post_page-----94ea13f56f2--------------------------------)
    [## 介绍 MLOps：机器学习的实验跟踪'
- en: Why it matters and three different ways you can log and organize your ML experiments
    with pen and paper, spreadsheets…
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 这为什么重要，以及你可以用笔和纸、电子表格等三种不同方式记录和组织你的 ML 实验。
- en: medium.com](https://medium.com/@iamleonie/intro-to-mlops-experiment-tracking-for-machine-learning-858e432bd133?source=post_page-----94ea13f56f2--------------------------------)
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@iamleonie/intro-to-mlops-experiment-tracking-for-machine-learning-858e432bd133?source=post_page-----94ea13f56f2--------------------------------)
- en: 'If you are an absolute beginner, I recommend starting simple and tracking your
    experiments manually in a spreadsheet at first. Open an empty spreadsheet and
    create columns for all inputs, such as:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是绝对初学者，我建议一开始简单地手动在电子表格中跟踪你的实验。打开一个空电子表格，并为所有输入创建列，例如：
- en: backbone,
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主干，
- en: learning rate,
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率，
- en: epochs,
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 周期，
- en: augmentations, and
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增强方式，以及
- en: image size
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像大小
- en: and outputs, such as loss and metrics for training and validation, you want
    to track.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 以及你想要跟踪的输出，如训练和验证的损失和指标。
- en: 'The resulting spreadsheet could look something like this:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 结果电子表格可能会如下所示：
- en: '![](../Images/99a24e70879082b1d3cc3351300f1a67.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/99a24e70879082b1d3cc3351300f1a67.png)'
- en: Example spreadsheet to track experiments for beginners
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 初学者跟踪实验的示例电子表格
- en: ⚒️ *Once you feel comfortable with the Deep Learning techniques, you can level
    up by* [*implementing an experiment tracking system*](https://medium.com/@iamleonie/intro-to-mlops-experiment-tracking-for-machine-learning-858e432bd133)
    *into your pipeline to automate experiment tracking, such as* [*Weights & Biases*](https://wandb.ai/site)*,*
    [*Neptune*](https://neptune.ai/)*, or* [*MLFlow*](https://mlflow.org/docs/latest/tracking.html)*.*
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ⚒️ *一旦你对深度学习技术感到舒适，你可以通过* [*实现实验跟踪系统*](https://medium.com/@iamleonie/intro-to-mlops-experiment-tracking-for-machine-learning-858e432bd133)
    *来提升你的水平，以自动化实验跟踪，例如* [*Weights & Biases*](https://wandb.ai/site)*，* [*Neptune*](https://neptune.ai/)*，或*
    [*MLFlow*](https://mlflow.org/docs/latest/tracking.html)*。*
- en: Experimentation and hyperparameter tuning
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实验和超参数调整
- en: 'Now that you have an experiment tracking system let’s run some experiments.
    You can start by tweaking the following hyperparameters:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有了实验跟踪系统，让我们开始进行一些实验。你可以从调整以下超参数开始：
- en: 'Number of training steps: range of 2 to 10'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练步骤数：范围为 2 到 10
- en: 'Learning rate: range of 0.0001 to 0.001'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率：范围为 0.0001 到 0.001
- en: 'Image size: range of 128 to 1028'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像大小：范围为 128 到 1028
- en: 'Backbone: Try different backbones. First, try deeper models from the ResNet
    family (print `timm.list_models(''*resnet*'')` to see what other models are available),
    then try a different backbone family like `timm.list_models(''*densenet*'')` or
    `timm.list_models(''*efficientnet*'')`'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主干网络：尝试不同的主干网络。首先，尝试 ResNet 家族的更深模型（打印 `timm.list_models('*resnet*')` 查看其他可用模型），然后尝试不同的主干网络家族，如
    `timm.list_models('*densenet*')` 或 `timm.list_models('*efficientnet*')`
- en: ⚒️ *Once you feel comfortable with the Deep Learning techniques, you can level
    up by automating this step with* [*Optuna*](https://optuna.org/) *or* [*Weights
    & Biases*](https://wandb.ai/site)*.*
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ⚒️ *当你对深度学习技术感到熟练时，可以通过使用* [*Optuna*](https://optuna.org/) *或* [*Weights & Biases*](https://wandb.ai/site)*
    来自动化这一步，进一步提升自己。*
- en: Now it’s your turn! — Tweak a few notches and see how the model's performance
    changes. Once you’re happy with the results, move on to the next step.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 现在轮到你了！— 调整几个参数，看看模型的性能如何变化。一旦你对结果满意，就可以进入下一步。
- en: '![](../Images/8096166e016abadf0db4531e0d946ea0.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8096166e016abadf0db4531e0d946ea0.png)'
- en: Example log of experiments
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 实验日志示例
- en: 'Step 4: Make Predictions (Inference)'
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四步：进行预测（推断）
- en: Drum roll, please! Now that we have found the configuration that will give us
    the best model, we want to put it to good use.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 请敲鼓！现在我们已经找到能够给我们最优模型的配置，我们希望将其充分利用。
- en: First, let’s fine-tune the model with the optimal configuration on the full
    dataset to take advantage of every data sample. We don’t split the data into training
    and validation data in this step. Instead, we only have one big training dataset.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们用最佳配置在完整数据集上微调模型，以利用每个数据样本。在这一步中，我们不会将数据拆分为训练数据和验证数据。相反，我们只有一个大的训练数据集。
- en: '[PRE27]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: But the rest of the training pipeline stays the same.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 但其余的训练流程保持不变。
- en: '[PRE28]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '**Inference** — And finally, we will use the model to predict the hold-out
    test set.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '**推断** — 最后，我们将使用模型来预测保留的测试集。'
- en: '[PRE29]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Below you can see the results of our model:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 下面你可以看到我们模型的结果：
- en: '![](../Images/c7c480d2b6e80753e441aa9fce32adc9.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7c480d2b6e80753e441aa9fce32adc9.png)'
- en: Predictions
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 预测
- en: '![](../Images/f9166b757b2019b4c1d383c0d111beaa.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f9166b757b2019b4c1d383c0d111beaa.png)'
- en: Summary and Next Steps
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结与下一步
- en: This tutorial showed you how to fine-tune a pre-trained image classification
    model for your specific task, evaluate it, and perform inference on unseen data
    using the PyTorch framework in Python.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程向你展示了如何微调预训练的图像分类模型以适应你的特定任务，评估它，并使用 Python 中的 PyTorch 框架对未见数据进行推断。
- en: Once you feel comfortable, you can level up by reviewing the sections marked
    with ⚒️ to level up to an intermediate level.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 当你感到熟练后，可以通过查看标有 ⚒️ 的部分来提升到中级水平。
- en: '[](/intermediate-deep-learning-with-transfer-learning-f1aba5a814f?source=post_page-----94ea13f56f2--------------------------------)
    [## Intermediate Deep Learning with Transfer Learning'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/intermediate-deep-learning-with-transfer-learning-f1aba5a814f?source=post_page-----94ea13f56f2--------------------------------)
    [## 中级深度学习与迁移学习'
- en: A practical guide for fine-tuning Deep Learning models for computer vision and
    natural language processing
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度学习模型在计算机视觉和自然语言处理中的微调实用指南
- en: towardsdatascience.com](/intermediate-deep-learning-with-transfer-learning-f1aba5a814f?source=post_page-----94ea13f56f2--------------------------------)
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/intermediate-deep-learning-with-transfer-learning-f1aba5a814f?source=post_page-----94ea13f56f2--------------------------------)
- en: Enjoyed This Story?
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 喜欢这个故事吗？
- en: '[*Subscribe for free*](https://medium.com/subscribe/@iamleonie) *to get notified
    when I publish a new story.*'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '[*免费订阅*](https://medium.com/subscribe/@iamleonie) *以便在我发布新故事时获得通知。*'
- en: '[](https://medium.com/@iamleonie/subscribe?source=post_page-----94ea13f56f2--------------------------------)
    [## Get an email whenever Leonie Monigatti publishes.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@iamleonie/subscribe?source=post_page-----94ea13f56f2--------------------------------)
    [## 每当 Leonie Monigatti 发布新内容时获取电子邮件通知。'
- en: Get an email whenever Leonie Monigatti publishes. By signing up, you will create
    a Medium account if you don’t already…
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每当 Leonie Monigatti 发布新内容时获取电子邮件通知。通过注册，如果你还没有的话，你将创建一个 Medium 账户……
- en: medium.com](https://medium.com/@iamleonie/subscribe?source=post_page-----94ea13f56f2--------------------------------)
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: medium.com](https://medium.com/@iamleonie/subscribe?source=post_page-----94ea13f56f2--------------------------------)
- en: '*Find me on* [*LinkedIn*](https://www.linkedin.com/in/804250ab/),[*Twitter*](https://twitter.com/helloiamleonie)*,
    and* [*Kaggle*](https://www.kaggle.com/iamleonie)*!*'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '*在* [*LinkedIn*](https://www.linkedin.com/in/804250ab/)，[*Twitter*](https://twitter.com/helloiamleonie)*，以及*
    [*Kaggle*](https://www.kaggle.com/iamleonie)*上找到我！*'
- en: References
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Dataset
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集
- en: '[1] [MikołajFish99](https://www.kaggle.com/mikoajfish99) (2023). [Lions or
    Cheetahs — Image Classification](https://www.kaggle.com/datasets/mikoajfish99/lions-or-cheetahs-image-classification/code)
    in Kaggle Datasets.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [MikołajFish99](https://www.kaggle.com/mikoajfish99) (2023). [狮子还是猎豹——图像分类](https://www.kaggle.com/datasets/mikoajfish99/lions-or-cheetahs-image-classification/code)
    在Kaggle数据集中。'
- en: '**License:** According to the original image source [(Open Images Dataset V6](https://storage.googleapis.com/openimages/web/factsfigures.html))
    the annotations are licensed by Google LLC under [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)
    license, and the images are listed as having a [CC BY 2.0](https://creativecommons.org/licenses/by/2.0/)
    license.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '**许可：** 根据原始图像来源 [(开放图像数据集V6](https://storage.googleapis.com/openimages/web/factsfigures.html))，注释由Google
    LLC根据[CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)许可授权，图像的许可列为[CC
    BY 2.0](https://creativecommons.org/licenses/by/2.0/)。'
- en: '*Note the original dataset contains 200 images, with 100 images of each class.
    But the dataset needed some cleaning, including removing images of other animals;
    thus, the final dataset is slightly smaller. To keep this tutorial short, we will
    skip the data cleaning process here.*'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意，原始数据集包含200张图像，每个类别各100张图像。但数据集需要一些清理，包括移除其他动物的图像；因此，最终数据集略小。为了保持教程简短，我们将跳过数据清理过程。*'
- en: Images
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像
- en: If not otherwise stated, all images are created by the author.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有其他说明，所有图像均由作者创作。
- en: Literature
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文献
- en: '[2] S. Bhutani with H20.ai (2023). [Best Practises for Training ML Models |
    @ChaiTimeDataScience #160](https://www.youtube.com/watch?v=_mzrfMA8Qx4) presented
    on YouTube in January 2023.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] S. Bhutani 与 H20.ai (2023). [训练ML模型的最佳实践 | @ChaiTimeDataScience #160](https://www.youtube.com/watch?v=_mzrfMA8Qx4)
    在2023年1月于YouTube上发布。'
- en: '[3] Deng, J., Dong, W., Socher, R., Li, L. J., Li, K., & Fei-Fei, L. (2009,
    June). Imagenet: A large-scale hierarchical image database. In *2009 IEEE conference
    on computer vision and pattern recognition* (pp. 248–255). Ieee.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Deng, J., Dong, W., Socher, R., Li, L. J., Li, K., & Fei-Fei, L. (2009年6月).
    Imagenet：一个大规模的分层图像数据库。见 *2009年IEEE计算机视觉与模式识别会议*（第248–255页）。Ieee。'
- en: '[4] DeVries, T., & Taylor, G. W. (2017). Improved regularization of convolutional
    neural networks with cutout. *arXiv preprint arXiv:1708.04552*.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] DeVries, T., & Taylor, G. W. (2017). 使用cutout改进卷积神经网络的正则化。 *arXiv预印本arXiv:1708.04552*。'
- en: '[5] K. He, X. Zhang, S. Ren, & J. Sun (2016). Deep residual learning for image
    recognition. In *Proceedings of the IEEE conference on computer vision and pattern
    recognition* (pp. 770–778).'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] K. He, X. Zhang, S. Ren, & J. Sun (2016). 深度残差学习用于图像识别。见 *IEEE计算机视觉与模式识别会议论文集*（第770–778页）。'
- en: '[6] timmdocs (2022). [Pytorch Image Models (timm)](https://timm.fast.ai/) (accessed
    April 10th, 2023).'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] timmdocs (2022). [Pytorch图像模型（timm）](https://timm.fast.ai/)（访问日期：2023年4月10日）。'
