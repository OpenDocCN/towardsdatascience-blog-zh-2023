- en: XGBoost Now Supports MAE as an Objective
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: XGBoost 现在支持 MAE 作为目标函数
- en: 原文：[https://towardsdatascience.com/xgboost-now-support-mae-as-objective-fdbbe18e173](https://towardsdatascience.com/xgboost-now-support-mae-as-objective-fdbbe18e173)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/xgboost-now-support-mae-as-objective-fdbbe18e173](https://towardsdatascience.com/xgboost-now-support-mae-as-objective-fdbbe18e173)
- en: How is that possible, as MAE is non-smooth?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 这怎么可能，因为 MAE 是非光滑的？
- en: '[](https://medium.com/@guillaume.saupin?source=post_page-----fdbbe18e173--------------------------------)[![Saupin
    Guillaume](../Images/d9112d3cdfe6f335b6ff2c875fba6bb5.png)](https://medium.com/@guillaume.saupin?source=post_page-----fdbbe18e173--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fdbbe18e173--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fdbbe18e173--------------------------------)
    [Saupin Guillaume](https://medium.com/@guillaume.saupin?source=post_page-----fdbbe18e173--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@guillaume.saupin?source=post_page-----fdbbe18e173--------------------------------)[![Saupin
    Guillaume](../Images/d9112d3cdfe6f335b6ff2c875fba6bb5.png)](https://medium.com/@guillaume.saupin?source=post_page-----fdbbe18e173--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fdbbe18e173--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fdbbe18e173--------------------------------)
    [Saupin Guillaume](https://medium.com/@guillaume.saupin?source=post_page-----fdbbe18e173--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fdbbe18e173--------------------------------)
    ·5 min read·Jan 18, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----fdbbe18e173--------------------------------)
    ·阅读时间 5 分钟·2023年1月18日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/929fde4fb46becfb224284a1e3507976.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/929fde4fb46becfb224284a1e3507976.png)'
- en: Photo by [Ajay Karpur](https://unsplash.com/@ajaykarpur?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Ajay Karpur](https://unsplash.com/@ajaykarpur?utm_source=medium&utm_medium=referral)拍摄，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: When working on a model based on Gradient Boosting, a key parameter to choose
    from is the objective. Indeed, the whole building process of the decision tree
    derives from the objective and its first and second derivatives.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于梯度提升的模型中，一个关键的参数是目标函数。实际上，决策树的整个构建过程都来源于目标函数及其一阶和二阶导数。
- en: 'XGBoost has recently introduced support for a new kind of objective: non-smooth
    objectives with no second derivative. Amongst them, the famous **MAE** (mean absolute
    error) is now natively activable inside XGBoost.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 最近引入了一种新类型的目标函数支持：没有二阶导数的非光滑目标函数。其中，著名的**MAE**（均方绝对误差）现在可以在 XGBoost
    中原生启用。
- en: In this post, we will detail how XGBoost has been modified to handle this kind
    of objective.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将详细介绍 XGBoost 如何被修改以处理这种类型的目标函数。
- en: Gradient Boosting needs smooth objective
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度提升需要光滑的目标函数
- en: 'XGBoost, LightGBM, and CatBoost all share a common limitation: they need smooth
    (mathematically speaking) objectives to compute the optimal weights for the leaves
    of the decision trees.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost、LightGBM 和 CatBoost 都有一个共同的限制：它们需要光滑（从数学上讲）的目标函数来计算决策树叶子的最优权重。
- en: This is not true anymore for XGBoost, which has recently introduced, support
    for the MAE using line search, starting with release 1.7.0
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 XGBoost，这种情况不再成立。XGBoost 最近引入了使用线性搜索支持 MAE，从 1.7.0 版本开始。
- en: 'If you’re willing to master Gradient Boosting in detail, have a look at my
    book:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你愿意详细掌握梯度提升，可以查看我的书：
- en: '[](https://amzn.to/3CTMDuH?source=post_page-----fdbbe18e173--------------------------------)
    [## Practical Gradient Boosting: A deep dive into Gradient Boosting in Python'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://amzn.to/3CTMDuH?source=post_page-----fdbbe18e173--------------------------------)
    [## 实用梯度提升：深入探讨 Python 中的梯度提升'
- en: This book on Gradient Boosting methods is intended for students, academics,
    engineers, and data scientists who wish …](https://amzn.to/3CTMDuH?source=post_page-----fdbbe18e173--------------------------------)
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 这本关于梯度提升方法的书面向希望深入了解这一领域的学生、学者、工程师和数据科学家……](https://amzn.to/3CTMDuH?source=post_page-----fdbbe18e173--------------------------------)
- en: Gradient and Hessian are the core of Gradient Boosting
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度和赫西矩阵是梯度提升的核心
- en: The core of gradient boosting-based methods is the idea of applying descent
    gradient to functional space instead of parameter space.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 基于梯度提升的方法的核心思想是将梯度下降应用于函数空间而不是参数空间。
- en: As a reminder, the core of the method is to linearize an objective function
    around the previous prediction `t-1`, and to add a small increment that minimizes
    this objective.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，该方法的核心是在线性化目标函数的基础上进行，并添加一个小的增量，以最小化该目标。
- en: This small increment is expressed in the functional space, and it is a new binary
    node represented by the function `f_t.`
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这个小的增量在函数空间中表示，并且它是一个新的二叉节点，由函数 `f_t` 表示。
- en: 'This objective combines a loss function `l` with a regularization function
    Ω:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这个目标将损失函数 `l` 与正则化函数 Ω 结合：
- en: '![](../Images/1af64dc9bf2cc7b01f99b2a2f7174565.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1af64dc9bf2cc7b01f99b2a2f7174565.png)'
- en: Objective function. Formula by the author.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 目标函数。公式由作者提供。
- en: 'Once linearized, we get:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦线性化，我们得到：
- en: '![](../Images/ea320d4e3ee7154dc9c00586f5e81767.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea320d4e3ee7154dc9c00586f5e81767.png)'
- en: Objective function linearized near ŷ[t-1]. Formula by the author.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ŷ[t-1] 附近线性化的目标函数。公式由作者提供。
- en: 'Where:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '![](../Images/05390d425a0ceccae933ab8d708d7e89.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05390d425a0ceccae933ab8d708d7e89.png)'
- en: First and second derivative. Formula by the author.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一阶和二阶导数。公式由作者提供。
- en: 'Minimizing this linearized objective function boils down to reducing the constant
    part, i.e:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化这个线性化目标函数归结为减少常数部分，即：
- en: '![](../Images/9052bac80de3182e95ad7cafc52438ae.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9052bac80de3182e95ad7cafc52438ae.png)'
- en: Variable part of the objective to minimize. Formula by the author.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 目标的变量部分需要最小化。公式由作者提供。
- en: 'As the new stage of the model `f_t`is a binary decision node that will generate
    two values (its leaves) : `w_left` and `w_right`it is possible to reorganize the
    sum above as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型 `f_t` 的新阶段是一个二叉决策节点，将生成两个值（其叶子）：`w_left` 和 `w_right`，因此可以将上述和重新组织如下：
- en: '![](../Images/5c34ea8f16f9d3c9e40812e896216665.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5c34ea8f16f9d3c9e40812e896216665.png)'
- en: Reorganize linearized objective. Formula by the author.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 重新组织线性化目标。公式由作者提供。
- en: At this stage, minimizing the linearized objective simply implies finding the
    optimal weight `w_left` and `w_right` . As they are both implied in a simple second-order
    polynomial, the solution is well the known `-b/2a` expression where `b` is `G`
    and `a` is `1/2H` , hence for the left node, we get
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一阶段，最小化线性化目标仅涉及找到最佳权重 `w_left` 和 `w_right`。由于它们都涉及一个简单的二阶多项式，解是著名的 `-b/2a`
    表达式，其中 `b` 是 `G`，`a` 是 `1/2H`，因此对于左节点，我们得到
- en: '![](../Images/9edd28657903b085c6d58e2057d7bb9f.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9edd28657903b085c6d58e2057d7bb9f.png)'
- en: Formula for then optimal left weight. Formula by the author.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧最佳权重的公式。公式由作者提供。
- en: The exact same formula stands for the right weight.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 完全相同的公式适用于右侧权重。
- en: Note the regularization parameter λ, which is an L2 regularisation term, proportional
    to the square of the weight.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 注意正则化参数 λ，它是一个 L2 正则化项，与权重的平方成正比。
- en: MAE has no second derivative
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MAE 没有二阶导数
- en: The issue with the **Mean Absolute Error** is that is it’s second derivative
    is null, hence `H` is zero.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**平均绝对误差** 的问题在于它的二阶导数为零，因此 `H` 为零。'
- en: Regularization
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则化
- en: 'One possible option to circumvent this limitation is to regularize this function.
    This means substituting this formula with another one that has the property of
    being at least twice derivable. See my article below that shows how to do that
    with the `logcosh` :'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一种可能的解决方案是对该函数进行正则化。这意味着用一个至少二阶可导的函数替代这个公式。请参见下面的文章，展示了如何使用 `logcosh` 实现：
- en: '[](/confidence-intervals-for-xgboost-cac2955a8fde?source=post_page-----fdbbe18e173--------------------------------)
    [## Confidence intervals for XGBoost'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/confidence-intervals-for-xgboost-cac2955a8fde?source=post_page-----fdbbe18e173--------------------------------)
    [## XGBoost 的置信区间'
- en: Build a regularized Quantile Regression Objective to get HP tuning free confidence
    intervals
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建一个正则化的分位回归目标，以获得不需要超参数调优的置信区间
- en: towardsdatascience.com](/confidence-intervals-for-xgboost-cac2955a8fde?source=post_page-----fdbbe18e173--------------------------------)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/confidence-intervals-for-xgboost-cac2955a8fde?source=post_page-----fdbbe18e173--------------------------------)
- en: Line search
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线搜索
- en: Another option, the one recently introduced by XGBoost since its release 1.7.0,
    is the use of an iterative method for finding the best weight for each node.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择，即 XGBoost 自 1.7.0 版本发布以来最近引入的方法，是使用一种迭代方法来找到每个节点的最佳权重。
- en: 'To do so, the current XGBoost implementation uses a trick:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，目前的 XGBoost 实现使用了一种技巧：
- en: First, it computes the leaf values as usual, simply forcing the second derivative
    to 1.0
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，像往常一样计算叶子值，只需将二阶导数强制为 1.0
- en: Then, once the whole tree is built, XGBoost updates the leaf values using an
    α-quantile
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，一旦整个树构建完成，XGBoost 使用 α-分位数更新叶子值
- en: If you’re curious to see how this is implemented (and are not afraid of modern
    C++) the detail can be found [here](https://github.com/dmlc/xgboost/pull/7812).
    `UpdateTreeLeaf`, and more specifically `UpdateTreeLeafHost` the method of interest.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解如何实现（并且不怕现代 C++），详细信息可以在[这里](https://github.com/dmlc/xgboost/pull/7812)找到。关注`UpdateTreeLeaf`，更具体地说是`UpdateTreeLeafHost`方法。
- en: How to use it
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用它
- en: 'It’s plain and simple: just pick a release of XGBoost that is greater than
    1.7.0 and use `objective: mae` as parameter.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '很简单：只需选择一个版本大于 1.7.0 的 XGBoost，并使用 `objective: mae` 作为参数。'
- en: '[![](../Images/646ded91846ff6aa7fa8814985c9837d.png)](https://www.buymeacoffee.com/guillaumes0)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[![](../Images/646ded91846ff6aa7fa8814985c9837d.png)](https://www.buymeacoffee.com/guillaumes0)'
- en: '[https://www.buymeacoffee.com/guillaumes0](https://www.buymeacoffee.com/guillaumes0)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.buymeacoffee.com/guillaumes0](https://www.buymeacoffee.com/guillaumes0)'
- en: Conclusion
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: XGBoost has introduced a new way to cope with non-smooth objectives, like the
    MAE, that does not require the regularization of a function.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 引入了一种新的方法来处理非平滑目标，如 MAE，这不需要对函数进行正则化。
- en: The MAE is a very convenient metric to use, as it is easy to understand. Moreover,
    it does not over penalize large errors as would the MSE. This is handy when trying
    to predict large as well as small values using the same model.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: MAE 是一个非常方便的指标，因为它容易理解。此外，它不像 MSE 那样过度惩罚大错误。这在使用同一模型预测大值和小值时非常有用。
- en: Being able to use a non-smooth objective is very appealing as it not only avoids
    the need for approximation but also opens the door to other non-smooth objectives
    like the MAPE.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 能够使用非平滑目标函数非常吸引人，因为它不仅避免了对函数进行近似的需求，还为其他非平滑目标函数（如 MAPE）打开了大门。
- en: Clearly, a new feature to try and follow.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这是一个值得尝试并关注的新特性。
- en: 'More on Gradient Boosting, XGBoost, LightGBM, and CaBoost in my book:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于梯度提升、XGBoost、LightGBM 和 CaBoost 的内容，请参见我的书：
- en: '[](https://amzn.to/3CTMDuH?source=post_page-----fdbbe18e173--------------------------------)
    [## Practical Gradient Boosting: A deep dive into Gradient Boosting in Python'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://amzn.to/3CTMDuH?source=post_page-----fdbbe18e173--------------------------------)
    [## 实用梯度提升：深入探讨 Python 中的梯度提升'
- en: This book on Gradient Boosting methods is intended for students, academics,
    engineers, and data scientists who wish to…](https://amzn.to/3CTMDuH?source=post_page-----fdbbe18e173--------------------------------)
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本书关于梯度提升方法，旨在为希望深入了解梯度提升的学生、学者、工程师和数据科学家提供帮助…](https://amzn.to/3CTMDuH?source=post_page-----fdbbe18e173--------------------------------)
