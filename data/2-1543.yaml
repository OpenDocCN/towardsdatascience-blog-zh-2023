- en: 'Multi-Task Learning in Recommender Systems: A Primer'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推荐系统中的多任务学习：基础知识
- en: 原文：[https://towardsdatascience.com/multi-task-learning-in-recommender-systems-a-primer-508e661a2029](https://towardsdatascience.com/multi-task-learning-in-recommender-systems-a-primer-508e661a2029)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/multi-task-learning-in-recommender-systems-a-primer-508e661a2029](https://towardsdatascience.com/multi-task-learning-in-recommender-systems-a-primer-508e661a2029)
- en: The science and engineering behind algorithms that try to do it all
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 试图做到这一切的算法背后的科学和工程
- en: '[](https://medium.com/@samuel.flender?source=post_page-----508e661a2029--------------------------------)[![Samuel
    Flender](../Images/390d82a673de8a8bb11cef66978269b5.png)](https://medium.com/@samuel.flender?source=post_page-----508e661a2029--------------------------------)[](https://towardsdatascience.com/?source=post_page-----508e661a2029--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----508e661a2029--------------------------------)
    [Samuel Flender](https://medium.com/@samuel.flender?source=post_page-----508e661a2029--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@samuel.flender?source=post_page-----508e661a2029--------------------------------)[![Samuel
    Flender](../Images/390d82a673de8a8bb11cef66978269b5.png)](https://medium.com/@samuel.flender?source=post_page-----508e661a2029--------------------------------)[](https://towardsdatascience.com/?source=post_page-----508e661a2029--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----508e661a2029--------------------------------)
    [Samuel Flender](https://medium.com/@samuel.flender?source=post_page-----508e661a2029--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----508e661a2029--------------------------------)
    ·8 min read·Jul 25, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----508e661a2029--------------------------------)
    ·阅读时长8分钟·2023年7月25日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/14dcc1089b980b3a39af30f387ebe796.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14dcc1089b980b3a39af30f387ebe796.png)'
- en: Photo by [Mike Kononov](https://unsplash.com/@mikofilm?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[Mike Kononov](https://unsplash.com/@mikofilm?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral) 的照片'
- en: While multi-task learning has been has been well established in computer vision
    and natural language processing, its use in modern [recommender systems](/deep-learning-in-recommender-systems-a-primer-96e4b07b54ca)
    is still relatively new and therefore not very well understood.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管多任务学习在计算机视觉和自然语言处理领域已经得到很好的应用，但在现代 [推荐系统](/deep-learning-in-recommender-systems-a-primer-96e4b07b54ca)
    中的使用仍然相对较新，因此理解还不够充分。
- en: In this post, we’ll take a deep dive into some of the most important design
    considerations and recent research breakthroughs in multi-task recommenders. We’ll
    cover
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将深入探讨多任务推荐系统中的一些重要设计考虑因素和最新研究突破。我们将涵盖
- en: why we need multi-task recommender systems in the first place,
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么我们首先需要多任务推荐系统，
- en: 'positive and negative transfer: the key challenge in multi-task learners,'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正向迁移和负向迁移：多任务学习者的关键挑战，
- en: hard parameter sharing and expert modeling, and
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬参数共享和专家建模，以及
- en: 'auxiliary learning: the idea of adding new tasks for the sole purpose of improving
    the main task.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 辅助学习：为了提高主任务的性能而添加新任务的想法。
- en: Let’s get started.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: Why multi-task recommender systems?
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么要使用多任务推荐系统？
- en: The key advantage of multi-task recommender systems is their ability to solve
    for multiple business objectives at the same time. For example, in a video recommender
    system we may want to optimize for clicks, but also for watch times, likes, shares,
    comments, or other forms of user engagement. In such a situation, a single multi-task
    model is not only computationally cheaper than multiple single-task models, it
    can also have better predictive accuracy per task.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务推荐系统的关键优势在于其能够同时解决多个业务目标。例如，在视频推荐系统中，我们可能希望优化点击量，但同时也要优化观看时长、点赞、分享、评论或其他形式的用户互动。在这种情况下，单一的多任务模型不仅比多个单任务模型在计算上更便宜，而且每个任务的预测准确性也可能更高。
- en: 'Even in cases where we only want to predict one event, such as “purchase” in
    an e-commerce recommender system, we can still add additional tasks with the sole
    purpose of improving performance on the main task. We call these additional tasks
    “auxiliary tasks”, and this form of learning “auxiliary learning”. In the e-commerce
    example, it may make sense to also learn “add-to-cart” as well as “add-to-list”
    along with “purchase”, given that all of these events are closely related to each
    other: they indicate shopping intent.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在我们只想预测一个事件的情况下，比如电子商务推荐系统中的“购买”行为，我们仍然可以添加额外的任务，其唯一目的是提高主要任务的性能。我们称这些额外的任务为“辅助任务”，这种学习形式为“辅助学习”。在电子商务的例子中，除了“购买”之外，还可以学习“加入购物车”和“添加到列表”，因为这些事件彼此紧密相关：它们表示购物意图。
- en: Which tasks learn well together?
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 哪些任务可以很好地一起学习？
- en: 'At a high level, predicting a second task can either help with the first task
    or do the opposite: make the prediction of the first task worse. We call the former
    case “positive transfer” and the latter “negative transfer”. The challenge in
    multi-task learning is then to only learn tasks together that have positive transfer,
    and avoid negative transfer, which can be detrimental to model performance.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，预测第二个任务要么有助于第一个任务，要么产生相反的效果：使第一个任务的预测变得更差。我们称前者为“正向迁移”，后者为“负向迁移”。多任务学习的挑战在于仅学习那些具有正向迁移的任务，并避免负向迁移，因为后者可能会对模型性能产生不利影响。
- en: 'A key question in multi-task learning is then which tasks learn well together.
    In many cases, we can make reasonable guesses with domain knowledge. We’ve already
    seen an example above: “purchase” and “add-to-cart” both indicate shopping intent,
    and therefore should work well together in a multi-task learner (and in fact,
    they do).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务学习中的一个关键问题是哪些任务可以很好地一起学习。在许多情况下，我们可以凭借领域知识做出合理的猜测。我们在上面已经看到一个例子：“购买”和“加入购物车”都表示购物意图，因此应该在多任务学习者中表现良好（实际上，它们确实如此）。
- en: 'However, if the number of tasks becomes large, we may need to determine algorithmically
    which tasks should be learned together, and which should be learned separately.
    Notably, this is an NP-hard problem because the number of possible task groupings
    scales exponentially with the number of tasks. This is not easy, but doable: in
    a 2020 [paper](https://www.google.com/search?client=safari&rls=en&q=Which+Tasks+Should+Be+Learned+Together+in+Multi-task+Learning%3F&ie=UTF-8&oe=UTF-8)
    form Stanford University, the authors solve this task grouping problem on a computer
    vision dataset using the “branch and bound” algorithm, resulting in a solution
    that outperforms both multiple single-task learners and a single multi-task learner.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果任务数量变得很大，我们可能需要算法上确定哪些任务应该一起学习，哪些应该分开学习。值得注意的是，这是一个NP难题，因为可能的任务分组数量随着任务数量的增加而呈指数级增长。这并不容易，但可以做到：在2020年的一篇
    [论文](https://www.google.com/search?client=safari&rls=en&q=Which+Tasks+Should+Be+Learned+Together+in+Multi-task+Learning%3F&ie=UTF-8&oe=UTF-8)
    中，斯坦福大学的作者使用“分支限界”算法解决了计算机视觉数据集上的任务分组问题，得出的解决方案优于多个单任务学习者和单一的多任务学习者。
- en: '![](../Images/44a1d96a0142deb474556d8877272549.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/44a1d96a0142deb474556d8877272549.png)'
- en: 3 different multi-task modeling paradigms. Figure from [Ma et al 2018](https://dl.acm.org/doi/pdf/10.1145/3219819.3220007).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 3种不同的多任务建模范式。图源自 [Ma et al 2018](https://dl.acm.org/doi/pdf/10.1145/3219819.3220007)。
- en: 'Hard parameter sharing: multi-task learning under the hood'
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 硬参数共享：多任务学习的幕后
- en: The simplest way to build a multi-task neural network is a technique known as
    “hard parameter sharing”, or “shared bottom”, where we combine a shared bottom
    module with task-specific top modules. In this way, the bottom module can learn
    patterns that are task-generic, while the top modules can learn patterns that
    are task-specific. (“Modules” here are simply multi-layer perceptrons (MLPs) with
    certain activation.)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 构建多任务神经网络的最简单方法是一个被称为“硬参数共享”或“共享底部”的技术，在这种方法中，我们将一个共享底部模块与任务特定的顶部模块结合在一起。通过这种方式，底部模块可以学习任务通用的模式，而顶部模块则可以学习任务特定的模式。（“模块”在这里指的是具有特定激活函数的多层感知机（MLP））。
- en: In the simplest case, the task-specific module can be single output neuron that
    makes the prediction. However, in practice we can often achieve better performance
    by adding a dedicated module for each task which can learn task-specific internal
    representations of the data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在最简单的情况下，任务特定模块可以是一个单一的输出神经元，用于进行预测。然而，在实践中，我们通常可以通过为每个任务添加一个专用模块来实现更好的性能，该模块可以学习数据的任务特定内部表示。
- en: The output of the multi-task learner is going to be a list of predictions which
    we can combine into a final loss as
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务学习者的输出将是一个预测列表，我们可以将其合并为最终损失，如
- en: '![](../Images/be7a7ca40e729ba6830d930fcdcf17d9.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be7a7ca40e729ba6830d930fcdcf17d9.png)'
- en: where p are the predictions, y are the labels, and w are the (optional) task-specific
    weights, controlling the relative importance of each task.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 其中p是预测值，y是标签，w是（可选的）任务特定权重，控制每个任务的相对重要性。
- en: Expert modeling
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 专家建模
- en: 'Hard parameter sharing is perhaps the most common and also simplest way to
    solve multi-task learning problems, however it has a major disadvantage: we have
    to decide ahead of time which parts of the network should be shared and which
    shouldn’t. This requires us to know which tasks learn well together and which
    don’t, but in practice, we simply may not have this information ahead of time.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 硬参数共享也许是解决多任务学习问题最常见且最简单的方法，但它有一个主要缺点：我们必须提前决定网络的哪些部分应该共享，哪些不应该。这要求我们知道哪些任务能很好地一起学习，哪些不能，但实际上，我们可能无法提前获得这些信息。
- en: Enter “expert modeling”, also known as “mixtures of experts” (MoE), which traces
    back to a [paper](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf) from 1991
    by some of the big names in AI, Robert Jacobs, Michael Jordan, Steven Nolan, and
    Geoffrey Hinton. The key idea behind MoE is to combine N experts using a gating
    network that selects the best expert, given the input data. Here,
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 进入“专家建模”，也称为“专家混合”（MoE），追溯到1991年一些AI大咖，如Robert Jacobs、Michael Jordan、Steven
    Nolan和Geoffrey Hinton的[论文](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf)。MoE的关键思想是通过门控网络结合N个专家，根据输入数据选择最佳专家。在这里，
- en: an “expert” is an MLP that processes the data, resulting in either an embedding
    or a prediction, and
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “专家”是一个处理数据的MLP，结果是一个嵌入或预测，并且
- en: 'the “gating network” is simply a softmax function:'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “门控网络”只是一个softmax函数：
- en: '![](../Images/fd228b1883eb57129a0e9dbceb2f367a.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd228b1883eb57129a0e9dbceb2f367a.png)'
- en: where x is the input data, and W is a learnable matrix. In other words, W (and
    thus the gate) learns to select the right expert given the input data.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其中x是输入数据，W是一个可学习的矩阵。换句话说，W（因此门控）学习根据输入数据选择正确的专家。
- en: MoE however uses just a single gate, which may not work well if we have multiple
    tasks, each of which would require their own set of experts. One of the breakthroughs
    in multi-task recommender systems was therefore to replace the single gate in
    MoE with multiple gates, one per task, resulting in “MMoE”, or “Multi-gate Mixtures
    of Experts”, introduced in a 2018 [paper](https://dl.acm.org/doi/pdf/10.1145/3219819.3220007)
    from Google.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，MoE只使用一个门控，这在有多个任务的情况下可能效果不好，每个任务都需要自己的专家集。因此，多任务推荐系统的一个突破是用多个门控替代MoE中的单一门控，每个任务一个，结果就是“MMoE”，即“多门控专家混合”，在2018年由Google提出的[论文](https://dl.acm.org/doi/pdf/10.1145/3219819.3220007)中介绍。
- en: The authors show that MMoE outperforms both hard parameter sharing and MoE on
    synthetic data, census data, as well as a large-scale production recommendation
    dataset. MMoE works particularly well (compared to MoE) when the tasks become
    less correlated, highlighting the advantage of having multiple gates.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 作者展示了MMoE在合成数据、普查数据以及大规模生产推荐数据集上均优于硬参数共享和MoE。MMoE特别适用于（相比于MoE）任务相关性较低的情况，突显了拥有多个门控的优势。
- en: Auxiliary learning
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 辅助学习
- en: '![](../Images/be210010812e4882cab810fdf7ad6c4a.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be210010812e4882cab810fdf7ad6c4a.png)'
- en: The key idea behind MetaBalance is to scale auxiliary gradients to match those
    of the main tasks, as seen in the transition of plots from left to right. Figure
    from [He et al 2022](https://arxiv.org/abs/2203.06801).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: MetaBalance的关键思想是将辅助梯度缩放到与主要任务的梯度相匹配，如从左到右的图表过渡所示。图源自[He et al 2022](https://arxiv.org/abs/2203.06801)。
- en: In many recommendation problems, the predictive performance on the main task
    can be improved via joint learning of auxiliary tasks. For example,
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多推荐问题中，通过联合学习辅助任务可以改善主要任务的预测性能。例如，
- en: when trying to predict conversion rates, predictive performance improves when
    jointly learning the auxiliary task of predicting click-through rates ([Ma et
    al 2018](https://dl.acm.org/doi/10.1145/3209978.3210104)),
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在预测转化率时，当联合学习预测点击率的辅助任务时，预测性能会提高（[Ma et al 2018](https://dl.acm.org/doi/10.1145/3209978.3210104)），
- en: when trying to predict user ratings, predictive performance improves when jointly
    learning the auxiliary task of predicting item metadata such as genre and tags
    ([Bansal et al 2016](https://arxiv.org/abs/1609.02116)),
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当尝试预测用户评分时，预测性能在联合学习辅助任务预测项目元数据（如类别和标签）时会得到改善（[班萨尔等 2016](https://arxiv.org/abs/1609.02116)）。
- en: when trying to predict reading duration in a newsfeed, predictive performance
    improves when jointly learning the auxiliary task of predicting click-through
    rates ([Zhao et al 2021](https://arxiv.org/pdf/2102.07142.pdf)), just to name
    a few examples. In all of these cases, the purpose of the auxiliary task is not
    to use the prediction at inference time, but instead solely to boost predictive
    performance on the main task we’re trying to learn.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当尝试预测新闻推送中的阅读时长时，预测性能会在联合学习辅助任务预测点击率时得到改善（[赵等 2021](https://arxiv.org/pdf/2102.07142.pdf)），仅举几个例子。在所有这些情况下，辅助任务的目的是不是在推理时使用预测，而仅仅是提升我们试图学习的主要任务的预测性能。
- en: Auxiliary learning works because we’re adding gradient signal which helps the
    model find the best potential minimum in the parameter space, and this extra signal
    can be particularly useful when the gradient signal from the main task is sparse.
    For example, conversion rates are much lower than click-through rates, and therefore
    it is expected that the gradient signal from the latter signal is richer, and
    can supplement the former.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 辅助学习之所以有效，是因为我们添加了梯度信号，这有助于模型在参数空间中找到最佳潜在最小值，而当主任务的梯度信号稀疏时，这种额外的信号尤为有用。例如，转换率远低于点击率，因此预计后者的梯度信号更丰富，可以补充前者。
- en: However, it has been shown that the gradients in auxiliary learning can be highly
    imbalanced such that the auxiliary gradients either dominate the learning or don’t
    matter at all. This is a problem, hypothesize the authors of “MetaBalance”, an
    advanced auxiliary learning algorithm introduced 2022 [paper](https://arxiv.org/abs/2203.06801)
    from, you guessed it, Meta. The key idea in MetaBalance is to scale the auxiliary
    gradients to be of the same magnitude as those of the main task(s).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，已显示辅助学习中的梯度可能高度不平衡，以至于辅助梯度要么主导学习，要么根本无关紧要。这是一个问题，假设“MetaBalance”的作者，2022
    年提出的高级辅助学习算法[论文](https://arxiv.org/abs/2203.06801)来自 Meta。MetaBalance 的关键思想是将辅助梯度缩放到与主要任务梯度相同的数量级。
- en: 'Formally:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 形式化地说：
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'where `g_aux` is the gradient from the auxiliary task, `g_main` is the gradient
    from the main task, and r is a hyper-parameter which the authors determine empirically.
    r~0.7 appears to work best in the paper: in other words, auxiliary tasks help
    the most when the magnitude of their gradients are close but not quite the same
    as the gradients of the main tasks.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 `g_aux` 是来自辅助任务的梯度，`g_main` 是来自主要任务的梯度，r 是作者通过经验确定的超参数。论文中 r~0.7 似乎效果最佳：换句话说，当辅助任务的梯度与主要任务的梯度接近但不完全相同时，辅助任务帮助最大。
- en: And in fact, MetaBalance shows promising results. The authors consider two e-commerce
    shopping dataset where the target is “purchase”, and the auxiliary tasks are “click”,
    “add-to-card”, and “add-to-list”. The improvements of MetaBalance are substantial
    with respect to single-task and vanilla (shared-bottom) multi-task modeling. In
    one problem, they were able to improve NDGC@10 from 0.82 (vanilla multitask) to
    0.99 (MetaBalance), a 17% improvement!
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，MetaBalance 展示了令人鼓舞的结果。作者考虑了两个电子商务购物数据集，其中目标是“购买”，辅助任务包括“点击”、“添加到购物车”和“添加到列表”。相较于单任务和普通（共享底层）多任务建模，MetaBalance
    的改进是显著的。在一个问题中，他们将 NDGC@10 从 0.82（普通多任务）提高到 0.99（MetaBalance），提升了 17%！
- en: Summary
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: 'Let’s recap:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下：
- en: Multi-task learning matters because modern recommender systems often need to
    optimize for multiple business objectives at the same time.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多任务学习很重要，因为现代推荐系统通常需要同时优化多个业务目标。
- en: Not all tasks can be learned well together. Tasks can help each other, creating
    positive transfer, or do the opposite — fight with each other — creating negative
    transfer. Figuring out which tasks to learn together is an NP-hard problem!
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不是所有任务都能很好地一起学习。任务可以互相帮助，产生正迁移，也可以相反——彼此对抗——产生负迁移。确定哪些任务可以一起学习是一个 NP-hard 问题！
- en: Hard parameter sharing (aka “shared bottom”) is the simplest and most common
    way to solve multi-task learning. It’s the first thing you should try in order
    to establish a solid baseline.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬参数共享（即“共享底层”）是解决多任务学习最简单和最常见的方法。这是建立稳固基线时应该首先尝试的方法。
- en: Expert modeling, and in particular MMoE, is the most advanced technique today
    to solve multi-task learning problems while at the same time mitigating negative
    transfer.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 专家建模，特别是MMoE，是当前解决多任务学习问题的最先进技术，同时能够减轻负迁移。
- en: In auxiliary learning, we add additional tasks with the sole purpose of improving
    performance on the main task(s). Auxiliary learning has been shown to bring substantial
    modeling improvements in multiple use-cases, and can be further improved by scaling
    the auxiliary gradients.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在辅助学习中，我们增加了额外的任务，其唯一目的是提高主要任务的性能。辅助学习在多个应用场景中已被证明能带来显著的建模改进，并且通过扩展辅助梯度可以进一步提升效果。
- en: 'And this is just the tip of the iceberg. There’s a wealth of open questions
    left in this domain: How to craft good auxiliary tasks? What’s a good ratio of
    auxiliary to main tasks? What’s the best number of experts? How does it scale
    with the number of tasks? How large should expert modules be?'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 而这仅仅是冰山一角。在这个领域还有很多未解之谜：如何设计良好的辅助任务？辅助任务与主要任务的最佳比例是多少？最佳的专家数量是多少？它如何随着任务数量的增加而扩展？专家模块应该有多大？
- en: Watch this space. New breakthroughs are certainly on the horizon.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 请继续关注这个领域。新的突破无疑就在前方。
- en: '*Still curious about modern recommender systems? Continue here:*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*对现代推荐系统仍感到好奇？请继续阅读：*'
- en: '[Deep Learning in Recommender Systems: A Primer](https://medium.com/towards-data-science/deep-learning-in-recommender-systems-a-primer-96e4b07b54ca)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[推荐系统中的深度学习：入门指南](https://medium.com/towards-data-science/deep-learning-in-recommender-systems-a-primer-96e4b07b54ca)'
- en: '[Hashing in Modern Recommender Systems: A Primer](https://medium.com/towards-data-science/hashing-in-modern-recommender-systems-a-primer-9c6b2cf4497a)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[现代推荐系统中的哈希：入门指南](https://medium.com/towards-data-science/hashing-in-modern-recommender-systems-a-primer-9c6b2cf4497a)'
- en: '[Learning to rank: A primer](https://medium.com/towards-data-science/learning-to-rank-a-primer-40d2ff9960af)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[学习排名：入门指南](https://medium.com/towards-data-science/learning-to-rank-a-primer-40d2ff9960af)'
- en: '[Breaking Down YouTube’s Recommendation Algorithm](https://medium.com/towards-data-science/breaking-down-youtubes-recommendation-algorithm-94aa3aa066c6)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[解析 YouTube 的推荐算法](https://medium.com/towards-data-science/breaking-down-youtubes-recommendation-algorithm-94aa3aa066c6)'
- en: '[Biases in Recommender Systems: Top Challenges and Recent Breakthroughs](https://medium.com/towards-data-science/biases-in-recommender-systems-top-challenges-and-recent-breakthroughs-edcda59d30bf)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[推荐系统中的偏差：主要挑战与近期突破](https://medium.com/towards-data-science/biases-in-recommender-systems-top-challenges-and-recent-breakthroughs-edcda59d30bf)'
- en: '[Machine Learning Does Not Only Predict the Future, It Actively Creates It](/machine-learning-does-not-only-predict-the-future-it-actively-creates-it-1615895c80a9)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[机器学习不仅预测未来，它还主动创造未来](/machine-learning-does-not-only-predict-the-future-it-actively-creates-it-1615895c80a9)'
