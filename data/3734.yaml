- en: Tree of Thoughts Prompting
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 思维树提示
- en: 原文：[https://towardsdatascience.com/tree-of-thoughts-prompting-65a3e51f9ac4?source=collection_archive---------7-----------------------#2023-12-22](https://towardsdatascience.com/tree-of-thoughts-prompting-65a3e51f9ac4?source=collection_archive---------7-----------------------#2023-12-22)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/tree-of-thoughts-prompting-65a3e51f9ac4?source=collection_archive---------7-----------------------#2023-12-22](https://towardsdatascience.com/tree-of-thoughts-prompting-65a3e51f9ac4?source=collection_archive---------7-----------------------#2023-12-22)
- en: Solving multi-step problems with LLMs via deliberate planning and exploration…
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过深思熟虑的规划和探索来解决多步骤问题…
- en: '[](https://wolfecameron.medium.com/?source=post_page-----65a3e51f9ac4--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----65a3e51f9ac4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----65a3e51f9ac4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----65a3e51f9ac4--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----65a3e51f9ac4--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wolfecameron.medium.com/?source=post_page-----65a3e51f9ac4--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----65a3e51f9ac4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----65a3e51f9ac4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----65a3e51f9ac4--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----65a3e51f9ac4--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F28aa6026c553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftree-of-thoughts-prompting-65a3e51f9ac4&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=post_page-28aa6026c553----65a3e51f9ac4---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----65a3e51f9ac4--------------------------------)
    ·20 min read·Dec 22, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F65a3e51f9ac4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftree-of-thoughts-prompting-65a3e51f9ac4&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=-----65a3e51f9ac4---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F28aa6026c553&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftree-of-thoughts-prompting-65a3e51f9ac4&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=post_page-28aa6026c553----65a3e51f9ac4---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----65a3e51f9ac4--------------------------------)
    · 20分钟阅读 · 2023年12月22日 [](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F65a3e51f9ac4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftree-of-thoughts-prompting-65a3e51f9ac4&user=Cameron+R.+Wolfe%2C+Ph.D.&userId=28aa6026c553&source=-----65a3e51f9ac4---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F65a3e51f9ac4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftree-of-thoughts-prompting-65a3e51f9ac4&source=-----65a3e51f9ac4---------------------bookmark_footer-----------)![](../Images/3a1d8bce28681223f010bacd921267e1.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F65a3e51f9ac4&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftree-of-thoughts-prompting-65a3e51f9ac4&source=-----65a3e51f9ac4---------------------bookmark_footer-----------)![](../Images/3a1d8bce28681223f010bacd921267e1.png)'
- en: (Photo by [Johann Siemens](https://unsplash.com/@emben?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    on [Unsplash](https://unsplash.com/photos/green-tree-on-grassland-during-daytime-EPy0gBJzzZU?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash))
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: （图片由 [约翰·西门斯](https://unsplash.com/@emben?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)
    提供，来源于 [Unsplash](https://unsplash.com/photos/green-tree-on-grassland-during-daytime-EPy0gBJzzZU?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash)）
- en: As large language models (LLMs) first started to gain in popularity, they were
    criticized for their shortcomings in solving complex, reasoning-based problems.
    Although scaling up these models (i.e., more parameters and more data) provided
    a near-uniform performance improvement across tasks, we saw virtually no boost
    in performance on reasoning-based tasks with modern LLMs. This changed with the
    proposal of advanced prompting techniques, such as chain of thought prompting
    [2] and self-consistency [3]. Such methods showed us that LLMs are more than capable
    of reasoning and solving complex, multi-step problems. They just have to be properly
    prompted to fully leverage these abilities.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当大型语言模型（LLMs）首次开始受到关注时，它们因在解决复杂的推理问题上的不足而受到批评。虽然扩大这些模型（即，增加更多参数和数据）在任务上几乎提供了统一的性能提升，但我们几乎没有看到现代LLMs在推理任务上的性能提升。这一情况随着先进提示技术的提出而发生了变化，例如思维链提示
    [2] 和自一致性 [3]。这些方法向我们展示了LLMs完全有能力进行推理和解决复杂的多步骤问题。它们只需适当的提示即可充分发挥这些能力。
- en: “It is perhaps surprising that underlying all this progress is still the original
    autoregressive mechanism for generating text, which makes token-level decisions
    one by one and in a left-to-right fashion.” *— from [1]*
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “或许令人惊讶的是，这些进展背后仍然是最初的自回归机制，它逐一进行标记级决策，并且是从左到右的。” *— 引自 [1]*
- en: Even if proper prompting can enable LLMs to solve complex problems, these techniques
    are lacking. Namely, we typically *i)* provide a prompt to the LLM and *ii)* expect
    the model to use next token prediction to generate a full solution. Certain approaches
    may generate solutions in a step-by-step fashion (e.g., least-to-most…
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 即使适当的提示能够使LLMs解决复杂问题，这些技术仍然存在不足。即，我们通常 *i)* 向LLM提供提示，并 *ii)* 期望模型利用下一个标记预测来生成完整的解决方案。某些方法可能会以逐步方式生成解决方案（例如，从最少到最多...
