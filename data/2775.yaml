- en: Is PyTorch’s Nesterov Momentum Implementation Wrong?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch 的 Nesterov 动量实现是否有误？
- en: 原文：[https://towardsdatascience.com/is-pytorchs-nesterov-momentum-implementation-wrong-5dc5f5032008?source=collection_archive---------10-----------------------#2023-09-02](https://towardsdatascience.com/is-pytorchs-nesterov-momentum-implementation-wrong-5dc5f5032008?source=collection_archive---------10-----------------------#2023-09-02)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/is-pytorchs-nesterov-momentum-implementation-wrong-5dc5f5032008?source=collection_archive---------10-----------------------#2023-09-02](https://towardsdatascience.com/is-pytorchs-nesterov-momentum-implementation-wrong-5dc5f5032008?source=collection_archive---------10-----------------------#2023-09-02)
- en: '[](https://medium.com/@jasonvega14?source=post_page-----5dc5f5032008--------------------------------)[![Jason
    Vega](../Images/712b2e34c1af7a8572193296bdd8b206.png)](https://medium.com/@jasonvega14?source=post_page-----5dc5f5032008--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5dc5f5032008--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5dc5f5032008--------------------------------)
    [Jason Vega](https://medium.com/@jasonvega14?source=post_page-----5dc5f5032008--------------------------------)'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@jasonvega14?source=post_page-----5dc5f5032008--------------------------------)[![Jason
    Vega](../Images/712b2e34c1af7a8572193296bdd8b206.png)](https://medium.com/@jasonvega14?source=post_page-----5dc5f5032008--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5dc5f5032008--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5dc5f5032008--------------------------------)
    [Jason Vega](https://medium.com/@jasonvega14?source=post_page-----5dc5f5032008--------------------------------)'
- en: ·
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa9932c231079&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-pytorchs-nesterov-momentum-implementation-wrong-5dc5f5032008&user=Jason+Vega&userId=a9932c231079&source=post_page-a9932c231079----5dc5f5032008---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5dc5f5032008--------------------------------)
    ·6 min read·Sep 2, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5dc5f5032008&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-pytorchs-nesterov-momentum-implementation-wrong-5dc5f5032008&user=Jason+Vega&userId=a9932c231079&source=-----5dc5f5032008---------------------clap_footer-----------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fa9932c231079&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-pytorchs-nesterov-momentum-implementation-wrong-5dc5f5032008&user=Jason+Vega&userId=a9932c231079&source=post_page-a9932c231079----5dc5f5032008---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5dc5f5032008--------------------------------)
    ·6 分钟阅读·2023年9月2日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5dc5f5032008&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-pytorchs-nesterov-momentum-implementation-wrong-5dc5f5032008&user=Jason+Vega&userId=a9932c231079&source=-----5dc5f5032008---------------------clap_footer-----------)'
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5dc5f5032008&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-pytorchs-nesterov-momentum-implementation-wrong-5dc5f5032008&source=-----5dc5f5032008---------------------bookmark_footer-----------)![](../Images/0e58bb7374f65a47beb941d924f19ba1.png)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5dc5f5032008&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fis-pytorchs-nesterov-momentum-implementation-wrong-5dc5f5032008&source=-----5dc5f5032008---------------------bookmark_footer-----------)![](../Images/0e58bb7374f65a47beb941d924f19ba1.png)'
- en: Momentum helps SGD traverse complex loss landscapes more efficiently. Photo
    by [Maxim Berg](https://unsplash.com/@maxberg?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 动量帮助 SGD 更有效地遍历复杂的损失景观。图片由 [Maxim Berg](https://unsplash.com/@maxberg?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)。
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: If you look closely at PyTorch’s [documentation](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)
    of SGD, you will find that their implementation of Nesterov momentum has a few
    differences from the formulation found in the [original paper](http://proceedings.mlr.press/v28/sutskever13.pdf).
    Most notably, PyTorch’s implementation evaluates the gradient at the current parameters,
    whereas the whole point of Nesterov momentum is to evaluate the gradient at shifted
    parameters. Unfortunately, it appears that discussion about these discrepancies
    on the internet is scarce. In this post, we will examine and explain the differences
    between PyTorch’s implementation and the original formulation of Nesterov momentum.
    Ultimately, we will see how PyTorch’s implementation is not wrong, but rather
    an approximation, and speculate about the benefit of their implementation.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果仔细查看 PyTorch 的 [文档](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)，你会发现他们对
    Nesterov 动量的实现与 [原始论文](http://proceedings.mlr.press/v28/sutskever13.pdf) 中的公式有一些不同。最显著的是，PyTorch
    的实现是在当前参数上计算梯度，而 Nesterov 动量的核心是评估偏移参数的梯度。不幸的是，关于这些差异的讨论在互联网上很少。在这篇文章中，我们将检查并解释
    PyTorch 实现与 Nesterov 动量原始公式之间的差异。最终，我们将看到 PyTorch 的实现并非错误，而是一种近似，并推测其实现的好处。
- en: The Formulations
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 公式
- en: 'The [original paper](http://proceedings.mlr.press/v28/sutskever13.pdf) describes
    Nesterov momentum using the following update rules:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[原始论文](http://proceedings.mlr.press/v28/sutskever13.pdf) 使用以下更新规则描述了 Nesterov
    动量：'
- en: 'where *v_{t+1}* and *θ_{t+1}* are the velocity vector and model parameters
    respectively at time *t*, *μ* is the momentum factor, and *ε* is the learning
    rate. The note in PyTorch’s SGD [documentation](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)
    states they use the following update rules:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *`v_{t+1}`* 和 *`θ_{t+1}`* 分别是时间 *t* 的速度向量和模型参数，*μ* 是动量因子，*ε* 是学习率。PyTorch
    的 SGD [文档](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) 中的说明称他们使用以下更新规则：
- en: 'where *g_{t+1}* represents the gradient used to compute *v_{t+1}*. We can expand
    the update rule for *θ_{t+1}* to get:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *`g_{t+1}`* 代表用于计算 *`v_{t+1}`* 的梯度。我们可以展开 *`θ_{t+1}`* 的更新规则得到：
- en: 'From this we can infer that:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 从中我们可以推断：
- en: 'and the update rules become:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 和更新规则变成：
- en: These are the update rules that PyTorch uses in theory. I mentioned earlier
    that PyTorch actually evaluates the gradient at the current parameters instead
    of the shifted parameters. This can be seen by looking at the algorithm description
    in the PyTorch SGD documentation. We will investigate this further later on.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是 PyTorch 理论上使用的更新规则。我之前提到过，PyTorch 实际上是在当前参数上评估梯度，而不是偏移参数。通过查看 PyTorch SGD
    文档中的算法描述可以看到这一点。我们将在后面进一步调查这一点。
- en: 'Note that for both the original (1, 2) and PyTorch (3, 4) formulations, if
    *v_0 = 0*, then the first update to *θ* becomes:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对于原始公式 (1, 2) 和 PyTorch 公式 (3, 4) 来说，如果 *`v_0 = 0`*，则 *`θ`* 的第一次更新变为：
- en: Although the PyTorch SGD documentation note states that the algorithm initializes
    the momentum buffer to the gradient at the first step, we will later show that
    this implies *v_0 = 0*.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 PyTorch SGD 文档中说明算法在第一步将动量缓冲区初始化为梯度，但我们稍后会显示这意味着 *`v_0 = 0`*。
- en: Preliminary Differences
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初步差异
- en: 'There are two immediate differences when going from the original (1, 2) to
    the PyTorch (3, 4) formulation:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 从原始公式 (1, 2) 到 PyTorch 公式 (3, 4) 存在两个直接的区别：
- en: The learning rate is moved outside of *v_{t+1}*.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 学习率被移出了 *`v_{t+1}`*。
- en: In the update rule for *v_{t+1}*, the term involving the gradient is added instead
    of subtracted, and in the update rule for *θ_{t+1}*, the term involving the velocity
    vector is subtracted instead of added. The difference in sign inside the gradient
    term is simply a consequence of this as shown in the previous section.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在*`v_{t+1}`*的更新规则中，涉及梯度的项被添加而不是减去，而在*`θ_{t+1}`*的更新规则中，涉及速度向量的项被减去而不是添加。这种梯度项符号的差异仅仅是前一节中展示的结果。
- en: 'To understand these differences, let’s first expand the update rules. As hinted
    at [here](https://github.com/pytorch/pytorch/issues/1099#issuecomment-289190614),
    the effect of the first difference is more apparent if we consider learning rate
    schedules. So, we consider a generalization of the update rules where *ε* is no
    longer fixed but can now vary over time, and denote *ε_t* as the learning rate
    at time step *t*. For brevity, let:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解这些差异，让我们首先展开更新规则。如 [这里](https://github.com/pytorch/pytorch/issues/1099#issuecomment-289190614)
    所提示的，如果我们考虑学习率调度，第一种差异的影响会更为明显。因此，我们考虑一个更新规则的广义化，其中 *ε* 不再是固定的，而是可以随时间变化，并将 *ε_t*
    记作时间步 *t* 时的学习率。为简洁起见，设：
- en: 'Assuming *v_0 = 0,* the original formulation becomes:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 *v_0 = 0*，原始公式变为：
- en: 'and the PyTorch formulation becomes:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 那么 PyTorch 公式变为：
- en: In the original formulation (6), if the learning rate were to change at time
    *t*, then only the magnitude of the term at *i = t* in the summation would be
    affected, and the magnitudes of all the other terms would remain the same. As
    a result, the immediate influence of the learning rate change is quite limited,
    and we would have to wait for the learning rate change to “trickle” down over
    subsequent time steps to have a stronger influence on the overall step size. In
    contrast, in the PyTorch formulation (7), if the learning rate were to change
    at time *t*, then the magnitude of the entire step would be affected immediately.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始公式（6）中，如果学习率在时间 *t* 发生变化，那么仅会影响求和中 *i = t* 的项的大小，而所有其他项的大小保持不变。因此，学习率变化的直接影响是相当有限的，我们必须等待学习率变化在随后的时间步中“逐渐”发挥更强的作用。相比之下，在
    PyTorch 公式（7）中，如果学习率在时间 *t* 发生变化，那么整个步骤的大小会立即受到影响。
- en: For *v_0 = 0*, it is clear from the expanded rules that the second difference
    ultimately has no effect; in either formulation, the step works out to a discounted
    sum of gradients that is subtracted from the current parameters.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *v_0 = 0*，从展开的规则可以看出，第二种差异最终没有影响；在任一公式中，这一步的结果都是从当前参数中减去折扣后的梯度和。
- en: Primary Differences
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主要差异
- en: 'Ignoring weight decay and dampening, by analyzing the SGD algorithm in PyTorch’s
    [documentation](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html),
    we can see that the implemented update rules are:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 忽略权重衰减和阻尼，通过分析 PyTorch 的 [文档](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)，我们可以看到实现的更新规则是：
- en: where *θ’_{t+1}* are the model parameters at time *t* and
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *θ’_{t+1}* 是时间 *t* 时的模型参数
- en: We will refer to equations 3 and 4 as the PyTorch “note” formulation, and equations
    8 and 9 as the PyTorch “implemented” formulation. We make a distinction between
    *θ* and *θ’* for a reason that will become apparent soon. The most glaring difference
    from the note formulation is that the gradient is evaluated at the current parameters
    rather than the shifted parameters. From this alone it may appear that the update
    rules the algorithm implements is not a proper implementation of Nesterov momentum.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将方程 3 和 4 称为 PyTorch 的“笔记”公式，将方程 8 和 9 称为 PyTorch 的“实现”公式。我们在*θ*和*θ’*之间做了区分，原因会很快显现出来。与笔记公式最明显的区别是梯度在当前参数处进行评估，而不是在偏移后的参数处。从这一点来看，算法实现的更新规则似乎并不是
    Nesterov 动量的正确实现。
- en: 'We will now examine how the PyTorch algorithm ultimately approximates Nesterov
    momentum. Derivations for an older version of PyTorch can be found [here](https://github.com/fidlej/optim/raw/master/dok/nesterov_simple.pdf)
    from Ivo Danihelka, referenced in [this GitHub issue](https://github.com/torch/optim/issues/27).
    Derivations for the current version of PyTorch can be found [here](https://github.com/pytorch/pytorch/pull/5920#issuecomment-375181908),
    which is a relatively straightforward adjustment from the previous derivations.
    We provide a LaTeX rendering of these (re-derived) derivations here for the reader’s
    convenience. The implemented formulation is derived by a simple change of variables.
    Specifically, we let:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将考察 PyTorch 算法如何最终近似 Nesterov 动量。旧版本 PyTorch 的推导可以在 Ivo Danihelka 提供的 [这里](https://github.com/fidlej/optim/raw/master/dok/nesterov_simple.pdf)
    找到，并在 [这个 GitHub 问题](https://github.com/torch/optim/issues/27) 中引用。当前版本 PyTorch
    的推导可以在 [这里](https://github.com/pytorch/pytorch/pull/5920#issuecomment-375181908)
    找到，这相对是对之前推导的简单调整。为了方便读者，我们在此提供这些（重新推导的）推导的 LaTeX 渲染。实现的公式是通过简单的变量变换得到的。具体而言，我们设：
- en: 'It immediately becomes clear that the note update rule for *v_{t+1}* (3) becomes
    equivalent to the implemented update rule for *v_{t+1}* (8) after the change of
    variables. We now want to derive an update rule for *θ’_{t+1}* in terms of *θ’_t*:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，*v_{t+1}* 的注释更新规则 (3) 在变量变换后与 *v_{t+1}* 的实现更新规则 (8) 等效。我们现在想要推导一个关于 *θ’_{t+1}*
    的更新规则，以 *θ’_t* 为变量：
- en: 'This is exactly the update rule we saw implemented in PyTorch (9). At a high
    level, the PyTorch implementation assumes the current parameters *θ’_t* are already
    the shifted version of the “actual” parameters *θ_t*. Hence, at each time step,
    the “actual” parameters *θ_t* are related to the current parameters *θ’_t* by:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们在 PyTorch (9) 中看到的更新规则。总体上，PyTorch 实现假设当前参数 *θ’_t* 已经是“实际”参数 *θ_t* 的偏移版本。因此，在每一个时间步，
    “实际”参数 *θ_t* 与当前参数 *θ’_t* 之间的关系为：
- en: However, it appears from the source code that the PyTorch SGD implementation
    does not make any correction at the end of the algorithm to retrieve the final
    “actual” parameters, so the final output is technically an approximation of the
    “actual” parameters.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从源代码来看，PyTorch SGD 实现似乎在算法结束时没有对“实际”参数进行任何修正，因此最终输出在技术上是“实际”参数的近似值。
- en: 'Finally, we now show that *v_0* must be 0:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们现在展示 *v_0* 必须为 0：
- en: 'Moreover, we can confirm that the first update to the “actual” parameters is
    the same first update made in the original formulation when *v_0 = 0*:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以确认对“实际”参数的第一次更新与原始表述中 *v_0 = 0* 时的第一次更新相同：
- en: We can see that this is equivalent to equation 5.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到这等同于方程 5。
- en: The Benefit of the Implemented Formulation
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现表述的好处
- en: 'Of course, the big remaining question is: Why does PyTorch bother at all to
    reformulate Nesterov momentum from equations 3 and 4 to equations 8 and 9? One
    possible explanation is that the reformulation might provide some savings in the
    number of arithmetic operations required. To evaluate this possible explanation,
    let’s count the number of arithmetic operations. For the note formulation (3,
    4), we have:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，剩下的大问题是：为什么 PyTorch 要从方程 3 和 4 重新表述 Nesterov 动量到方程 8 和 9？一个可能的解释是，重新表述可能在所需的算术操作数量上有所节省。为了评估这个可能的解释，我们来计算一下算术操作的数量。对于注释的表述
    (3, 4)，我们有：
- en: 'Here, there are a total of seven operations. For the implemented formulation
    (8, 9), we have:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，总共有七个操作。对于实现的表述 (8, 9)，我们有：
- en: Here, there are a total of six operations. The second gradient in the PyTorch
    implementation just uses the saved result from the first gradient computation,
    so only one gradient computation is performed at each time step. So, one apparent
    benefit is that the PyTorch implementation cuts down on one additional multiplication
    operation at each step.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，总共有六个操作。PyTorch 实现中的第二个梯度只是使用了第一个梯度计算的保存结果，因此每个时间步只执行一个梯度计算。因此，一个明显的好处是
    PyTorch 实现减少了每一步的一个额外乘法操作。
- en: Conclusion
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'In summary:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 总结
- en: The update rules stated in PyTorch’s SGD documentation note (3, 4) have a different
    location for the learning rate compared to the original Nesterov momentum update
    rules (1, 2). This allows learning rate schedules to have an immediate effect
    on the overall step size, whereas the original formulation would have the effect
    of learning rate changes to “trickle” down over subsequent time steps.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PyTorch 的 SGD 文档注释 (3, 4) 中的更新规则与原始 Nesterov 动量更新规则 (1, 2) 中的学习率位置不同。这允许学习率调度对整体步长有直接影响，而原始表述则会使学习率变化在随后的时间步中“逐步”体现。
- en: The update rules implemented in the PyTorch SGD algorithm (8, 9) are an approximation
    to the update rules stated in the documentation note (3, 4) after a simple change
    of variables. Although the “actual” parameters are easily recoverable from the
    current parameters at each time step, the PyTorch implementation does not make
    any such correction at the end of the algorithm, and so the final parameters technically
    remain an approximation of the “actual” final parameters.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PyTorch SGD 算法 (8, 9) 中实现的更新规则是在简单的变量变换后对文档注释 (3, 4) 中更新规则的近似。尽管“实际”参数可以在每个时间步从当前参数中轻松恢复，但
    PyTorch 实现并没有在算法结束时进行任何修正，因此最终参数在技术上仍然是“实际”最终参数的近似值。
- en: An apparent benefit of the PyTorch implementation is that it avoids an additional
    multiplication operation at each time step.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PyTorch 实现的一个明显好处是它避免了每个时间步的额外乘法操作。
- en: References
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: “SGD.” SGD — PyTorch 2.0 Documentation, pytorch.org/docs/stable/generated/torch.optim.SGD.html.
    Accessed 2 Sept. 2023.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “SGD。” SGD — PyTorch 2.0 文档，pytorch.org/docs/stable/generated/torch.optim.SGD.html。访问日期：2023年9月2日。
- en: Sutskever, Ilya, et al. “[On the importance of initialization and momentum in
    deep learning.](http://proceedings.mlr.press/v28/sutskever13.pdf)” International
    Conference on Machine Learning. PMLR, 2013.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Sutskever, Ilya等人。“[深度学习中初始化和动量的重要性](http://proceedings.mlr.press/v28/sutskever13.pdf)”。国际机器学习会议。PMLR，2013年。
- en: Danihelka, Ivo. “[Nesterov’s Momentum Made Simple.](https://github.com/fidlej/optim/raw/master/dok/nesterov_simple.pdf)”
    25 Aug. 2012.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Danihelka, Ivo. “[Nesterov的动量简明介绍](https://github.com/fidlej/optim/raw/master/dok/nesterov_simple.pdf)”。2012年8月25日。
- en: 'Chintala, Soumith. “nesterov momentum is wrong in sgd · Issue #27 · torch/optim.”
    GitHub, 13 Oct. 2014, [github.com/torch/optim/issues/27](https://github.com/torch/optim/issues/27).'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Chintala, Soumith. “SGD中的Nesterov动量是错误的 · Issue #27 · torch/optim。” GitHub，2014年10月13日，[github.com/torch/optim/issues/27](https://github.com/torch/optim/issues/27)。'
- en: 'Gross, Sam. “Add a note in the docs about the momentum formulation used in
    optim · Issue #1099 · pytorch/pytorch.” GitHub, 25 Mar. 2017, [github.com/pytorch/pytorch/issues/1099#issuecomment-289190614](https://github.com/pytorch/pytorch/issues/1099#issuecomment-289190614).'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Gross, Sam. “在文档中添加关于优化中使用的动量公式的说明 · Issue #1099 · pytorch/pytorch。” GitHub，2017年3月25日，[github.com/pytorch/pytorch/issues/1099#issuecomment-289190614](https://github.com/pytorch/pytorch/issues/1099#issuecomment-289190614)。'
- en: 'Zhao, Yilong. “fix Nesterov Momentum Bug · Issue #5920 · pytorch/pytorch.”
    GitHub, 21 Mar. 2018, [https://github.com/pytorch/pytorch/pull/5920#issuecomment-375181908](https://github.com/pytorch/pytorch/pull/5920#issuecomment-375181908).'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Zhao, Yilong. “修复Nesterov动量错误 · Issue #5920 · pytorch/pytorch。” GitHub，2018年3月21日，[https://github.com/pytorch/pytorch/pull/5920#issuecomment-375181908](https://github.com/pytorch/pytorch/pull/5920#issuecomment-375181908)。'
