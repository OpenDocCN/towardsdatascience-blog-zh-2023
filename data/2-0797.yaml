- en: 'Elliot Activation Function: What Is It and Is It Effective?'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Elliot 激活函数：它是什么，它有效吗？
- en: 原文：[https://towardsdatascience.com/elliot-activation-function-what-is-it-and-is-it-effective-59b63ec1fd8a](https://towardsdatascience.com/elliot-activation-function-what-is-it-and-is-it-effective-59b63ec1fd8a)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/elliot-activation-function-what-is-it-and-is-it-effective-59b63ec1fd8a](https://towardsdatascience.com/elliot-activation-function-what-is-it-and-is-it-effective-59b63ec1fd8a)
- en: What is the Elliot Activation Function and is it a good alternative to the other
    activation functions used in neural networks?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是 Elliot 激活函数，它是否是神经网络中其他激活函数的良好替代方案？
- en: '[](https://ben-mccloskey20.medium.com/?source=post_page-----59b63ec1fd8a--------------------------------)[![Benjamin
    McCloskey](../Images/7118f5933f2affe2a7a4d3375452fa4c.png)](https://ben-mccloskey20.medium.com/?source=post_page-----59b63ec1fd8a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----59b63ec1fd8a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----59b63ec1fd8a--------------------------------)
    [Benjamin McCloskey](https://ben-mccloskey20.medium.com/?source=post_page-----59b63ec1fd8a--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ben-mccloskey20.medium.com/?source=post_page-----59b63ec1fd8a--------------------------------)[![本杰明·麦克洛斯基](../Images/7118f5933f2affe2a7a4d3375452fa4c.png)](https://ben-mccloskey20.medium.com/?source=post_page-----59b63ec1fd8a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----59b63ec1fd8a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----59b63ec1fd8a--------------------------------)
    [本杰明·麦克洛斯基](https://ben-mccloskey20.medium.com/?source=post_page-----59b63ec1fd8a--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----59b63ec1fd8a--------------------------------)
    ·7 min read·Feb 4, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----59b63ec1fd8a--------------------------------)
    ·阅读时间 7 分钟·2023年2月4日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/39a7ac7751ff21380e8ecc69c64edfb1.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/39a7ac7751ff21380e8ecc69c64edfb1.png)'
- en: Elliot Activation Function (Image from Author)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Elliot 激活函数（图片来源：作者）
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Are you in the middle of creating a new machine-learning model and unsure of
    what activation function you should be using?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否在创建新的机器学习模型时，不确定应该使用什么激活函数？
- en: '**But wait, *what is an activation function?***'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**但等一下，*什么是激活函数？***'
- en: Activation functions allow machine learning models to understand and solve *nonlinear*
    problems. Using an activation function in neural networks specifically helps with
    the passing of the most important information from each neuron to the next. Today,
    the ReLU Activation Function is generally used in the architecture of Neural Networks,
    however, that does not necessarily mean it is always the best choice. (Check out
    my post below on the ReLU and LReLU Activations).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数使机器学习模型能够理解和解决*非线性*问题。在神经网络中使用激活函数特别有助于将每个神经元传递给下一个神经元的最重要信息。今天，ReLU 激活函数通常用于神经网络的架构中，但这并不一定意味着它总是最佳选择。（请查看我下面关于
    ReLU 和 LReLU 激活函数的文章）。
- en: '[](/leaky-relu-vs-relu-activation-functions-which-is-better-1a1533d0a89f?source=post_page-----59b63ec1fd8a--------------------------------)
    [## Leaky ReLU vs. ReLU Activation Functions: Which is Better?'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/leaky-relu-vs-relu-activation-functions-which-is-better-1a1533d0a89f?source=post_page-----59b63ec1fd8a--------------------------------)
    [## Leaky ReLU 与 ReLU 激活函数：哪一个更好？'
- en: An experiment to investigate if there is a noticeable difference in a model’s
    performance when using a ReLU Activation…
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一项实验调查在使用 ReLU 激活函数时模型性能是否存在明显差异……
- en: towardsdatascience.com](/leaky-relu-vs-relu-activation-functions-which-is-better-1a1533d0a89f?source=post_page-----59b63ec1fd8a--------------------------------)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/leaky-relu-vs-relu-activation-functions-which-is-better-1a1533d0a89f?source=post_page-----59b63ec1fd8a--------------------------------)
- en: I recently came across the **Elliot Activation Function** which was praised
    as being a possible alternative to various activation functions, including the
    Sigmoid and Hyperbolic Tagenet. Today we will run an experiment to test the Elliot
    Activation Function’s performance.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我最近发现了**Elliot 激活函数**，它被赞誉为可能替代各种激活函数的选择，包括 Sigmoid 和双曲正切函数。今天我们将进行一个实验来测试 Elliot
    激活函数的性能。
- en: '**Experiment 1:** *Test the Elliot Activation Function’s performance against
    the Sigmoid Activation Function and Hyperbolic Tangent Activation Function.*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**实验 1：** *测试 Elliot 激活函数与 Sigmoid 激活函数和双曲正切激活函数的性能。*'
- en: '**Experiment 2:** *Test the Elliot Activation Function’s performance against
    the ReLU Activation Function.*'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**实验2:** *测试Elliot激活函数与ReLU激活函数的性能。*'
- en: 'The goal is to answer the question: I***s the Elliot Activation Function effective
    or not?***'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是回答这个问题：*Elliot激活函数是否有效？*
- en: Elliot Activation Function
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Elliot激活函数
- en: '![](../Images/77e52168c1320307627e828d342e28a2.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/77e52168c1320307627e828d342e28a2.png)'
- en: The Elliot Activation Function will result in an approximation that is relatively
    close to the Sigmoid and Hyperbolic Tangent Activation Functions. Some have found
    that Elliot performs calculations **2x faster** than the Sigmoid Activation Function
    [3]. Just like the Sigmoid Activation function, the Elliot Activation Function
    is constrained between 0 and 1.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Elliot激活函数将产生一个相对接近Sigmoid和双曲正切激活函数的近似结果。有些人发现Elliot比Sigmoid激活函数**快2倍** [3]。就像Sigmoid激活函数一样，Elliot激活函数被限制在0到1之间。
- en: '![](../Images/8d6d284c49d6dfde0ab3cd3d7f19a2f7.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8d6d284c49d6dfde0ab3cd3d7f19a2f7.png)'
- en: Elliot Activation Function (Image from Author)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Elliot激活函数（图像来自作者）
- en: Experiment
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验
- en: '**PROBLEM:** Keras currently does not have the Elliot Activation Function in
    its repository.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题:** Keras目前在其库中没有Elliot激活函数。'
- en: '**SOLUTION:** We can use the Keras backend and create it ourselves!'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决方案:** 我们可以使用Keras后端自行创建它！'
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: For this experiment, let's see how the Elliot Activation Function compares to
    its similar counterparts as well as the ReLU Activation, a fundamental activation
    function used in neural networks today.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个实验，让我们看看Elliot激活函数与类似激活函数以及ReLU激活函数的比较，ReLU是今天神经网络中使用的基本激活函数。
- en: Dataset and Setup
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集和设置
- en: The first step of all Python projects is to import your packages.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 所有Python项目的第一步是导入你的包。
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The dataset used today is the iris dataset, which can be found [here](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data).
    This dataset is publicly available and is allowed for public use (there is an
    option to load it into Python through *sklearn*).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 今天使用的数据集是鸢尾花数据集，可以在[这里](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data)找到。这个数据集是公开的，允许公开使用（可以通过*sklearn*加载到Python中）。
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Next, let’s create the four models. These will be fairly simple models. Each
    will have one layer of 8 neurons and an activation function. The final layer will
    have 3 neurons and use a Softmax Activation Function.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建四个模型。这些将是相当简单的模型。每个模型将有一层8个神经元和一个激活函数。最终层将有3个神经元并使用Softmax激活函数。
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Next, simply train the models and analyze the results.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，简单地训练模型并分析结果。
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Results
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: The results were… surprising, actually. As expected, the Elliot Activation Function
    produced models with similar performances to those adopting the Sigmoid And Hyperbolic
    Tangent Activation Functions.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 结果实际上是……令人惊讶的。正如预期的那样，Elliot激活函数生成的模型在性能上与采用Sigmoid和双曲正切激活函数的模型相似。
- en: '**1 Epoch**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**1周期**'
- en: '*Sigmoid @ 1*: Accuracy: 0.3109 | Loss: 2.0030'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Sigmoid @ 1*: 准确率: 0.3109 | 损失: 2.0030'
- en: '*Elliot @ 1*: Accuracy: 0.3361 | Loss: 1.0866'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Elliot @ 1*: 准确率: 0.3361 | 损失: 1.0866'
- en: At 1 Epoch, the Elliot Activation Function model outperformed the Sigmoid Activation
    Function model with a 2.61% higher accuracy and an *almost 100% decrease in the
    amount of loss.*
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在1个周期下，Elliot激活函数模型的准确率比Sigmoid激活函数模型高2.61%，并且*损失量减少了近100%。*
- en: '**10 Epochs**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**10周期**'
- en: '*Sigmoid @ 10*: Accuracy: 0.3529 | Loss: 1.0932'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Sigmoid @ 10*: 准确率: 0.3529 | 损失: 1.0932'
- en: '*Elliot @ 10*: Accuracy: 0.6891 | Loss: 0.9434'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Elliot @ 10*: 准确率: 0.6891 | 损失: 0.9434'
- en: At 10 epochs, the model with the Elliot Activation Function had an almost 30%
    higher accuracy with a lower loss compared to the model using the Sigmoid Activation
    Function.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在10个周期下，使用Elliot激活函数的模型比使用Sigmoid激活函数的模型准确率提高了近30%，同时损失较低。
- en: '**100 Epochs**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**100周期**'
- en: '*Sigmoid @ 100*: Accuracy: 0.9496 | Loss: 0.4596'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Sigmoid @ 100*: 准确率: 0.9496 | 损失: 0.4596'
- en: '*Elliot @ 100*: Accuracy: 0.9580 | Loss: 0.5485'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Elliot @ 100*: 准确率: 0.9580 | 损失: 0.5485'
- en: The Sigmoid Model outperformed the Elliot Model, however, it should be noted
    that their performances were almost exactly the same.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Sigmoid模型的表现优于Elliot模型，但需要注意的是它们的表现几乎完全相同。
- en: '**1000 Epochs**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**1000周期**'
- en: '*Sigmoid @ 1000*: Accuracy: 0.9832 | Loss: 0.0584'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Sigmoid @ 1000*: 准确率: 0.9832 | 损失: 0.0584'
- en: '*Elliot @ 1000*: Accuracy: 0.9832 | loss: 0.0433'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Elliot @ 1000*: 准确率: 0.9832 | 损失: 0.0433'
- en: At 1000 Epochs, the performances of the two different models were almost exactly
    the same.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在1000个周期下，两种不同模型的表现几乎完全相同。
- en: '**Overall, the model using the Elliot Activation Function performed slightly
    better than the model using the Sigmoid Activation Function.**'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**总体而言，使用Elliot激活函数的模型表现略优于使用Sigmoid激活函数的模型。**'
- en: Elliot versus Hyperbolic Tangent
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Elliot与双曲正切
- en: '**1 Epoch**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**1个周期**'
- en: '*tanh @ 1* : Accuracy: 0.3361 | loss: 1.1578'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*tanh @ 1*: 准确率: 0.3361 | 损失: 1.1578'
- en: '*Elliot @ 1*: Accuracy: 0.3361 | Loss: 1.0866'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Elliot @ 1*: 准确率: 0.3361 | 损失: 1.0866'
- en: At 1 Epoch, the Elliot Activation Function Model performs the same as the Hyperbolic
    Tangent Activation Function Model. I expected these functions to produce similar
    performing models since each similarly constrains the values passed on to the
    next layer of a neural network.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在1个周期时，Elliot激活函数模型与双曲正切激活函数模型的表现相同。我原本预期这些函数会产生相似的模型，因为它们都以类似的方式限制了传递到神经网络下一层的值。
- en: '**10 Epochs**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**10个周期**'
- en: '*tanh @ 10*: Accuracy: 0.3277 | Loss: 0.9981'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*tanh @ 10*: 准确率: 0.3277 | 损失: 0.9981'
- en: '*Elliot @ 10*: Accuracy: 0.6891 | Loss: 0.9434'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Elliot @ 10*: 准确率: 0.6891 | 损失: 0.9434'
- en: The model with the Elliot Activation Function greatly outperforms the model
    with the Hyperbolic Tangent Activation Function, just as the Elliot Model did
    at 10 Epochs when compared to the Sigmoid Model.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Elliot激活函数的模型明显优于使用双曲正切激活函数的模型，就像Elliot模型在与Sigmoid模型比较时在10个周期的表现一样。
- en: '**100 Epochs**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**100个周期**'
- en: '*tanh @ 100:* Accuracy: 0.9916 | Loss: 0.2325'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*tanh @ 100:* 准确率: 0.9916 | 损失: 0.2325'
- en: '*Elliot @ 100*: Accuracy: 0.9580 | Loss: 0.5485'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Elliot @ 100*: 准确率: 0.9580 | 损失: 0.5485'
- en: At 100 epochs, the Hyperbolic Tangent model performs much better than the Elliot
    model. At higher epochs, the Elliot Activation Function seems to underperform
    compared to the *tanh* activation function but let’s see how their performances
    differ at 1000 epochs.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在100个周期时，双曲正切模型的表现明显优于Elliot模型。在更高的周期下，Elliot激活函数似乎表现不如*tanh*激活函数，但让我们看看在1000个周期时它们的表现有何不同。
- en: '**1000 Epochs**'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**1000个周期**'
- en: '*tanh @ 1000:* Accuracy: 0.9748 | Loss: 0.0495'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*tanh @ 1000:* 准确率: 0.9748 | 损失: 0.0495'
- en: '*Elliot @ 1000*: Accuracy: 0.9832 | Loss: 0.0433'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Elliot @ 1000*: 准确率: 0.9832 | 损失: 0.0433'
- en: Well, at 1000 epochs, the Elliot Activation Function model slightly outperforms
    the Hyperbolic Tangent Activation Function model.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，在1000个周期时，Elliot激活函数模型略微超越了双曲正切激活函数模型。
- en: Overall, I would say that the Hyperbolic Tangent and Elliot Activation Function
    Models work almost the same in the layers of a neural network. **There could be
    a difference in the time to train a model**, however, these models were very simple
    and time may become a bigger factor with the more data one has as well as the
    size of the network they are creating.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我认为双曲正切和Elliot激活函数模型在神经网络的层次中几乎表现相同。**训练模型所需的时间可能会有所不同**，然而这些模型非常简单，随着数据量的增加以及网络规模的扩大，时间可能会成为一个更重要的因素。
- en: Elliot versus ReLU
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Elliot与ReLU
- en: '**1 Epoch**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**1个周期**'
- en: '*ReLU @ 1:* Accuracy: 0.6639 | Loss: 1.0221'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ReLU @ 1:* 准确率: 0.6639 | 损失: 1.0221'
- en: '*Elliot @ 1*: Accuracy: 0.3361 | Loss: 1.0866'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Elliot @ 1*: 准确率: 0.3361 | 损失: 1.0866'
- en: At 1 epoch, the model with the ReLU Activation Function does *much* better which
    outlines one observation that the Elliot Activation Function is causing the model
    to train slower.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在1个周期时，ReLU激活函数模型的表现*更好*，这表明Elliot激活函数使模型训练速度较慢。
- en: '**10 Epochs**'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**10个周期**'
- en: '*ReLU @ 10*: Accuracy: 0.6471 | Loss: 0.9413'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ReLU @ 10*: 准确率: 0.6471 | 损失: 0.9413'
- en: '*Elliot @ 10*: Accuracy: 0.6891 | Loss: 0.9434'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Elliot @ 10*: 准确率: 0.6891 | 损失: 0.9434'
- en: Wow! The model containing the Elliot activation actually performed *better*
    than the model with the ReLU Activation Function with a 4.2% higher accuracy and
    0.21% lower loss.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！包含Elliot激活函数的模型实际上表现*更好*，准确率高出4.2%，损失低了0.21%。
- en: '**100 Epochs**'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**100个周期**'
- en: '*ReLU @ 100* : Accuracy: 0.9160 | Loss: 0.4749'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ReLU @ 100*: 准确率: 0.9160 | 损失: 0.4749'
- en: '*Elliot @ 100*: Accuracy: 0.9580 | Loss: 0.5485'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Elliot @ 100*: 准确率: 0.9580 | 损失: 0.5485'
- en: Even though the model which adopted the Elliot Activation Function had a higher
    loss, it was able to achieve higher accuracy by 4.2%. *Again*, this shows the
    strength of the Elliot Activation Function when placed within neural networks.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管采用Elliot激活函数的模型损失较高，但它能够实现4.2%的更高准确率。*这再次*展示了Elliot激活函数在神经网络中的强大优势。
- en: '**1000 Epochs**'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**1000个周期**'
- en: '*ReLU @ 1000*: Accuracy: 0.9916 | Loss: 0.0494'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ReLU @ 1000*: 准确率: 0.9916 | 损失: 0.0494'
- en: '*Elliot @ 1000*: Accuracy: 0.9832 | Loss: 0.0433'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Elliot @ 1000*: 准确率: 0.9832 | 损失: 0.0433'
- en: While the model with the Elliot Activation Function did not do better in terms
    of accuracy, the loss was lower and I was still happy with the results. As shown
    at 1000 epochs, the Elliot Activation Function was almost just as good as the
    ReLU Activation Function and with the right problem and hyperparameter tuning,
    the Elliot Activation Function could the more optimal choice.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管采用 Elliot 激活函数的模型在准确性方面表现不佳，但损失较低，我对结果仍然感到满意。如 1000 次迭代所示，Elliot 激活函数几乎与 ReLU
    激活函数一样好，并且在正确的问题和超参数调整下，Elliot 激活函数可能是更优的选择。
- en: Conclusion
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'Today, we looked at a lesser-known activation function: **The Elliot Activation**.
    To test its performance, it was compared against two activation functions that
    are similar in their shape: the Sigmoid and Hyperbolic Tangent Activation Functions.
    That trial resulted in the Elliot Function performing the same if not better than
    using either of those two functions within the body of a neural network. Next,
    we compared the performance of an Elliot Activation Function to the standard ReLU
    Activation Function used in Neural Networks today. Of the 4 trials, the model
    adopting the Elliot Activation Function performed better 50% of the time. For
    the other trials it underperformed, its performance was still almost exactly the
    same as the model which was deployed with the ReLU Activation Function. I recommend
    trying the Elliot Activation Function in your next Neural Network because there
    is a chance it may perform better!'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，我们探讨了一种较不为人知的激活函数：**Elliot 激活函数**。为了测试它的性能，我们将其与形状相似的两个激活函数进行比较：Sigmoid 和双曲正切激活函数。结果显示，Elliot
    函数的表现与这两个函数相当，甚至更好。接下来，我们将 Elliot 激活函数的性能与现代神经网络中使用的标准 ReLU 激活函数进行了比较。在四次试验中，采用
    Elliot 激活函数的模型有 50% 的时间表现更好。在其他试验中，尽管表现不如前者，但其性能与使用 ReLU 激活函数的模型几乎一致。我建议在下一个神经网络中尝试
    Elliot 激活函数，因为它可能表现更佳！
- en: '**If you enjoyed today’s reading, PLEASE give me a follow and let me know if
    there is another topic you would like me to explore! If you do not have a Medium
    account, sign up through my link** [**here**](https://ben-mccloskey20.medium.com/membership)**!
    I will receive a small commission when you use my link. Additionally, add me on**
    [**LinkedIn**](https://www.linkedin.com/in/benjamin-mccloskey-169975a8/), **or
    feel free to reach out! Thanks for reading!**'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果你喜欢今天的阅读，请关注我，并告诉我是否有其他话题你希望我深入探讨！如果你没有 Medium 账户，可以通过我的链接** [**这里**](https://ben-mccloskey20.medium.com/membership)**注册！使用我的链接我将获得少量佣金。此外，也可以在**
    [**LinkedIn**](https://www.linkedin.com/in/benjamin-mccloskey-169975a8/) **添加我，或者随时联系我！感谢阅读！**'
- en: Dubey, Shiv Ram, Satish Kumar Singh, and Bidyut Baran Chaudhuri. “A comprehensive
    survey and performance analysis of activation functions in deep learning.” *arXiv
    preprint arXiv:2109.14545* (2021).
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Dubey, Shiv Ram, Satish Kumar Singh, 和 Bidyut Baran Chaudhuri. “深度学习中激活函数的全面调查与性能分析。”
    *arXiv 预印本 arXiv:2109.14545* (2021)。
- en: 'Sharma, Sagar, Simone Sharma, and Anidhya Athaiya. “Activation functions in
    neural networks.” *towards data science* 6.12 (2017): 310–316.'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Sharma, Sagar, Simone Sharma, 和 Anidhya Athaiya. “神经网络中的激活函数。” *towards data
    science* 6.12 (2017): 310–316。'
- en: '[https://www.gallamine.com/2013/01/a-sigmoid-function-without-exponential_31.html](https://www.gallamine.com/2013/01/a-sigmoid-function-without-exponential_31.html)'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[https://www.gallamine.com/2013/01/a-sigmoid-function-without-exponential_31.html](https://www.gallamine.com/2013/01/a-sigmoid-function-without-exponential_31.html)'
