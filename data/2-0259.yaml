- en: Advanced Data Preparation Using Custom Transformers in Scikit-Learn
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Scikit-Learn 中使用自定义 Transformers 进行高级数据准备
- en: 原文：[https://towardsdatascience.com/advanced-data-preparation-using-custom-transformers-in-scikit-learn-5e58d2713ac5](https://towardsdatascience.com/advanced-data-preparation-using-custom-transformers-in-scikit-learn-5e58d2713ac5)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/advanced-data-preparation-using-custom-transformers-in-scikit-learn-5e58d2713ac5](https://towardsdatascience.com/advanced-data-preparation-using-custom-transformers-in-scikit-learn-5e58d2713ac5)
- en: Go beyond “beginner mode” and take full advantage of scikit-learn’s more powerful
    capabilities
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超越“初学者模式”，充分利用 scikit-learn 更强大的功能
- en: '[](https://medium.com/@mattchapmanmsc?source=post_page-----5e58d2713ac5--------------------------------)[![Matt
    Chapman](../Images/7511deb8d9ed408ece21031f6614c532.png)](https://medium.com/@mattchapmanmsc?source=post_page-----5e58d2713ac5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5e58d2713ac5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5e58d2713ac5--------------------------------)
    [Matt Chapman](https://medium.com/@mattchapmanmsc?source=post_page-----5e58d2713ac5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mattchapmanmsc?source=post_page-----5e58d2713ac5--------------------------------)[![Matt
    Chapman](../Images/7511deb8d9ed408ece21031f6614c532.png)](https://medium.com/@mattchapmanmsc?source=post_page-----5e58d2713ac5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5e58d2713ac5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5e58d2713ac5--------------------------------)
    [Matt Chapman](https://medium.com/@mattchapmanmsc?source=post_page-----5e58d2713ac5--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5e58d2713ac5--------------------------------)
    ·10 min read·Jun 15, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5e58d2713ac5--------------------------------)
    ·阅读时间 10 分钟·2023年6月15日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/2d8d8b06eb619180898fe4e29f587be9.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2d8d8b06eb619180898fe4e29f587be9.png)'
- en: Image by [Daniel K Cheung](https://unsplash.com/@danielkcheung) on [Unsplash](https://unsplash.com/photos/cPF2nlWcMY4)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Daniel K Cheung](https://unsplash.com/@danielkcheung) 提供，来源于 [Unsplash](https://unsplash.com/photos/cPF2nlWcMY4)
- en: Scikit-Learn provides many useful tools for data preparation, but sometimes
    the pre-built options aren’t enough.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn 提供了许多有用的数据准备工具，但有时内置的选项并不够。
- en: In this article, I’ll show you how to create advanced data preparation workflows
    using custom Transformers. If you’ve been using scikit-learn for a while and want
    to level-up your skills, learning about Transformers is an excellent way to advance
    beyond “beginner mode” and learn about some of the more advanced capabilities
    required in modern Data Science teams.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我将向你展示如何使用自定义 Transformers 创建高级数据准备工作流。如果你已经使用 scikit-learn 一段时间并且希望提升技能，学习
    Transformers 是超越“初学者模式”并了解现代数据科学团队所需的一些更高级功能的绝佳方式。
- en: If the topic sounds a bit advanced, don’t worry — this article is packed full
    of examples which will help you feel confident with both the code and the concepts.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个话题听起来有点高级，不用担心——本文充满了示例，这些示例将帮助你对代码和概念充满信心。
- en: 'I’ll start with a brief overview of scikit-learn’s `Transformer` class and
    then walk through two ways to build customised Transformers:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我将从 scikit-learn 的`Transformer`类的简要概述开始，然后介绍两种构建自定义 Transformers 的方法：
- en: Using a `FunctionTransformer`
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `FunctionTransformer`
- en: Writing a custom `Transformer` from scratch
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从零开始编写自定义`Transformer`
- en: 'Transformers: The best way to preprocess data in scikit-learn'
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformers：在 scikit-learn 中预处理数据的最佳方法
- en: The `Transformer` is one of the central building blocks of scikit-learn. It’s
    so foundational, in fact, that chances are you’ve already been using one without
    even realising.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '`Transformer` 是 scikit-learn 的核心构建块之一。事实上，它如此基础，以至于你很可能已经在使用它却没有意识到。'
- en: In scikit-learn, a `Transformer` is any object with the `fit()` and `transform()`
    methods. In plain English, that means a Transformer is a class (i.e. a reusable
    chunk of code) that takes your raw dataset as an input and returns a transformed
    version of that dataset.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中，`Transformer` 是任何具有 `fit()` 和 `transform()` 方法的对象。简单来说，这意味着
    Transformer 是一个类（即可重用的代码块），它将你的原始数据集作为输入并返回该数据集的转换版本。
- en: '![](../Images/7dc632c664aee56d6b737766b6bc2db0.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7dc632c664aee56d6b737766b6bc2db0.png)'
- en: Image by author
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图片作者
- en: Importantly, scikit-learn `Transformers` are NOT the same as the “transformers”
    used in Large Language Models (LLMs) like BERT and GPT-4, or the models available
    through the HuggingFace `transformers` library. In the context of LLMs, a “transformer”
    (lower-case ‘t’) is a deep learning model; a scikit-learn `Transformer` (upper-case
    ‘T’) is a completely different (and much simpler) entity. You can think of it
    simply as a tool for preprocessing data in a typical ML workflow.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，scikit-learn 的 `Transformers` 与大语言模型（LLMs）如 BERT 和 GPT-4 中使用的“transformers”并不相同，或通过
    HuggingFace `transformers` 库提供的模型也不相同。在 LLMs 的上下文中，“transformer”（小写‘t’）是深度学习模型；而
    scikit-learn 的 `Transformer`（大写‘T’）则是完全不同（且更简单）的实体。你可以简单地把它看作是典型 ML 工作流中的数据预处理工具。
- en: 'When you import scikit-learn, you get automatic access to a bunch of pre-built
    Transformers designed for common ML data preprocessing tasks like imputing missing
    values, rescaling features and one-hot encoding. Some of the most popular Transformers
    include:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当你导入 scikit-learn 时，你可以自动访问一系列为常见 ML 数据预处理任务设计的预构建 Transformers，如填补缺失值、特征缩放和独热编码。一些最受欢迎的
    Transformers 包括：
- en: '`sklearn.impute.SimpleImputer` - a Transformer that will replace missing values
    in your dataset'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`sklearn.impute.SimpleImputer` - 一个 Transformer，用于替换数据集中缺失的值'
- en: '`sklearn.preprocessing.MinMaxScaler` - a Transformer that can rescale the numerical
    features in your dataset'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`sklearn.preprocessing.MinMaxScaler` - 一个 Transformer，可以重新缩放数据集中数值特征'
- en: '`sklearn.preprocessing.OneHotEncoder` - a Transformer for one-hot encoding
    categorical features'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`sklearn.preprocessing.OneHotEncoder` - 一个 Transformer，用于对分类特征进行独热编码'
- en: 'Using a scikit-learn `sklearn.pipeline.Pipeline`, you can even chain together
    multiple Transformers to build multi-step data preparation workflows, in preparation
    for subsequent ML modelling:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 的 `sklearn.pipeline.Pipeline`，你甚至可以将多个 Transformers 链接在一起，构建多步骤的数据准备工作流，为后续的
    ML 建模做好准备：
- en: '![](../Images/03fbf1b205ea500b325e3e7fbfed50d1.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03fbf1b205ea500b325e3e7fbfed50d1.png)'
- en: Image by author
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'If you’re not familiar with Pipelines or ColumnTransformers, they’re a great
    way to simplify your ML code, and you read more about them in my previous article:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对 Pipelines 或 ColumnTransformers 不熟悉，它们是简化 ML 代码的好方法，你可以在我之前的文章中了解更多：
- en: '[](/simplify-your-data-preparation-with-these-4-lesser-known-scikit-learn-classes-70270c94569f?source=post_page-----5e58d2713ac5--------------------------------)
    [## Simplify Your Data Preparation With These 4 Lesser-Known Scikit-Learn Classes'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/simplify-your-data-preparation-with-these-4-lesser-known-scikit-learn-classes-70270c94569f?source=post_page-----5e58d2713ac5--------------------------------)
    [## 使用这 4 个鲜为人知的 Scikit-Learn 类简化你的数据准备'
- en: 'Forget train_test_split: Pipeline, ColumnTransformer, FeatureUnion and FunctionTransformer
    are indispensable even if…'
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 忘记 train_test_split：Pipeline、ColumnTransformer、FeatureUnion 和 FunctionTransformer
    是不可或缺的，即使……
- en: towardsdatascience.com](/simplify-your-data-preparation-with-these-4-lesser-known-scikit-learn-classes-70270c94569f?source=post_page-----5e58d2713ac5--------------------------------)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/simplify-your-data-preparation-with-these-4-lesser-known-scikit-learn-classes-70270c94569f?source=post_page-----5e58d2713ac5--------------------------------)
- en: What’s wrong with the pre-built scikit-learn Transformers?
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预构建的 scikit-learn Transformers 有什么问题？
- en: Nothing at all!
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 完全没有！
- en: If you’re working with simple datasets and performing standard data preparation
    steps, chances are that scikit-learn’s pre-built transformers will be perfectly
    adequate. There’s no need to reinvent the wheel by writing custom ones from scratch.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在处理简单的数据集并执行标准的数据准备步骤，那么 scikit-learn 的预构建 Transformers 可能会完全足够。无需从头编写自定义
    Transformers 来重新发明轮子。
- en: But — and let’s be honest — when are datasets ever really simple in real life?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 但—说实话—现实生活中的数据集什么时候真的简单过？
- en: '(Spoiler: never.)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: （剧透：绝对不会。）
- en: If you’re working with real-world data or need to implement some juicy preprocessing
    method, chances are that scikit-learn’s built-in Transformers won’t always be
    adequate. Sooner or later, you’re going to need to implement custom data transformations.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在处理真实世界的数据或需要实现一些有趣的数据预处理方法，那么 scikit-learn 的内置 Transformers 可能并不总是足够的。迟早，你会需要实现自定义数据转换。
- en: Luckily, scikit-learn provides a few ways to extend its basic Transformer functionalities
    and build more customised Transformers. To showcase how these work, I’ll be using
    the canonical Titanic Survival Prediction dataset. Even on this supposedly “simple”
    dataset, you’ll find that there’s plenty of opportunity for getting creative with
    your data preparation. And, as I’ll show, custom Transformers are the ideal tool
    for the task.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，scikit-learn 提供了几种扩展其基本 Transformer 功能并构建更自定义 Transformers 的方法。为了展示这些方法的工作原理，我将使用经典的泰坦尼克号生存预测数据集。即使在这个所谓的“简单”数据集中，你会发现数据准备中还有很多创新的机会。正如我将展示的那样，自定义
    Transformers 是实现这一任务的理想工具。
- en: Data
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据
- en: 'First, let’s load the dataset and split it into training and testing subsets:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们加载数据集并将其拆分为训练集和测试集：
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/5216168cd8901e8c31de6dc524c6e539.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5216168cd8901e8c31de6dc524c6e539.png)'
- en: Image by author. [Titanic dataset](https://www.openml.org/search?type=data&sort=runs&id=40945&status=active)
    available a CC0 public domain license.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像。 [泰坦尼克号数据集](https://www.openml.org/search?type=data&sort=runs&id=40945&status=active)
    使用 CC0 公开领域许可。
- en: Because this article focuses on how to build **customised** Transformers, I
    won’t go into detail on the standard preprocessing steps which can be easily applied
    to this dataset using scikit-learn’s in-built Transformers (e.g. one-hot encoding
    categorical variables like `sex` using a `OneHotEncoder`, or replacing missing
    values using a `SimpleImputer`).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这篇文章专注于如何构建**自定义**的 Transformers，我不会详细介绍标准的预处理步骤，这些步骤可以通过 scikit-learn 的内置
    Transformers（例如，使用 `OneHotEncoder` 对分类变量如 `sex` 进行独热编码，或使用 `SimpleImputer` 替换缺失值）轻松应用于该数据集。
- en: Instead, I’ll focus on how to incorporate more complex preprocessing steps which
    cannot be implemented using “off-the-shelf” Transformers.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我将重点介绍如何整合更复杂的预处理步骤，这些步骤无法使用“现成的” Transformers 实现。
- en: One such step involves extracting each passenger’s title (e.g. Mr, Mrs, Master)
    from the `name` field. Why might we want to do this? Well, if we know that each
    passenger’s title contains an indication of their class/age/sex, and we assume
    that these factors influenced passengers’ ability to get on lifeboats, it’s reasonable
    to hypothesise that titles might be informative about survival chances. For example,
    a passenger with a “Master” title (indicating that they are a child) might be
    more likely to survive than a passenger with a “Mr” title (indicating that they
    are an adult).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个步骤是从 `name` 字段中提取每个乘客的头衔（例如 Mr, Mrs, Master）。为什么我们可能想这样做呢？如果我们知道每个乘客的头衔包含了他们的等级/年龄/性别的指示，并且我们假设这些因素影响了乘客登上救生艇的能力，那么合理地假设头衔可能对生存几率有所启示。例如，一个拥有“Master”头衔（表明他们是儿童）的乘客可能比一个拥有“Mr”头衔（表明他们是成人）的乘客更有可能生存。
- en: The problem, of course, is that there’s no in-built scikit-learn class which
    can do something as specific as extracting the title from the `name` field. To
    extract the titles, we need to build a custom Transformer.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，问题在于，没有内置的 scikit-learn 类可以执行像从 `name` 字段提取头衔这样具体的操作。为了提取头衔，我们需要构建一个自定义 Transformer。
- en: 1\. FunctionTransformer
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. FunctionTransformer
- en: The quickest way to build a custom Transformer is through using the `FunctionTransformer`
    class, which allows you to create Transformers directly from normal Python functions.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 构建自定义 Transformer 的最快方法是使用 `FunctionTransformer` 类，它允许你直接从普通的 Python 函数创建 Transformers。
- en: To use a `FunctionTransformer`, you start by defining a function which takes
    an input dataset `X`, performs the desired transformation, and returns a transformed
    version of `X`. Then, wrap your function in a `FunctionTransformer`, and scikit-learn
    will create a customised Transformer which implements your function.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `FunctionTransformer` 时，你首先需要定义一个函数，该函数接受一个输入数据集 `X`，执行所需的转换，并返回 `X` 的转换版本。然后，将你的函数封装在
    `FunctionTransformer` 中，scikit-learn 将创建一个实现你函数的自定义 Transformer。
- en: 'For example, here’s a function that can extract a passenger’s Title from the
    `name` field of our Titanic dataset:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以下是一个可以从我们的泰坦尼克号数据集的 `name` 字段中提取乘客头衔的函数：
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As you can see, wrapping the function in a `FunctionTransformer` turned it into
    a scikit-learn Transformer, giving it the `.fit()` and `.transform()` methods.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，将函数封装在 `FunctionTransformer` 中将其转换为 scikit-learn Transformer，使其具备 `.fit()`
    和 `.transform()` 方法。
- en: 'We can then incorporate this Transformer into our data preparation pipeline
    alongside any additional preprocessing steps/transformers we want to include:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以将这个 Transformer 与我们想要包含的任何额外预处理步骤/transformers 一起纳入数据准备管道中：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/a966e4f57584d07d7af04bfd212711ae.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a966e4f57584d07d7af04bfd212711ae.png)'
- en: Image by author
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'If you want to define a more complex function/Transformer that takes additional
    arguments, you can pass these to the function by incorporating them into the `kw_args`
    argument of `FunctionTransformer`. For example, let’s define another function
    which identifies whether each passenger is from an upper-class/professional background,
    based on their title:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想定义一个更复杂的函数/转换器，该函数需要额外的参数，你可以通过将这些参数纳入`FunctionTransformer`的`kw_args`参数来传递这些参数。例如，我们可以定义另一个函数，根据乘客的称谓来识别他们是否来自上层阶级/专业背景：
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/7dd3fd7c128c72d53e24e9a5b3068bde.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7dd3fd7c128c72d53e24e9a5b3068bde.png)'
- en: Image by author
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: As you can see, using `FunctionTransformer` is a really simple way to incorporate
    these complex preprocessing steps into a Pipeline without fundamentally changing
    the structure of our code.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，使用`FunctionTransformer`是一种非常简单的方法，可以将这些复杂的预处理步骤纳入Pipeline中，而不会从根本上改变我们代码的结构。
- en: '1.1 The limitations of FunctionTransformer: Stateful transformations'
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1.1 `FunctionTransformer`的局限性：有状态转换
- en: '`FunctionTransformer` is a powerful and elegant solution, but it’s only suitable
    when you want to apply ***stateless*** transformations (i.e. rule-based transformations
    which are not dependent on prior values computed from the training data). If you
    want to define a custom Transformer that can transform testing datasets based
    on the values observed in the training dataset, you can’t use a `FunctionTransformer`,
    and you’ll need to take a different approach.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`FunctionTransformer`是一个强大而优雅的解决方案，但它仅适用于你想应用***无状态***转换（即基于规则的转换，不依赖于从训练数据中计算的先前值）的情况。如果你想定义一个自定义转换器，可以根据训练数据集中观察到的值来转换测试数据集，那么你不能使用`FunctionTransformer`，你需要采取不同的方法。'
- en: 'If that sounds a bit confusing, take a minute to reconsider the function we
    just wrote to extract passenger’s titles:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这听起来有点混乱，花一分钟重新考虑一下我们刚刚编写的提取乘客称谓的函数：
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The function is ***stateless*** because it has no memory of the past; it does
    not use any pre-computed values during the operation. Each time we call this function,
    it will be applied from scratch as if it were being done for the very first time.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数是***无状态***的，因为它没有对过去的记忆；在操作过程中不使用任何预计算的值。每次调用这个函数时，它都会从头开始应用，就像是第一次执行一样。
- en: 'A **stateful** function, by contrast, retains information from previous operations
    and uses this when implementing the current operation. To illustrate this distinction,
    here are two functions that replace missing values in our dataset with a mean
    value:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，**有状态**函数保留先前操作的信息，并在实施当前操作时使用这些信息。为了说明这一点，这里有两个函数，它们用均值替换数据集中缺失的值：
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The first function is a *stateless* function because no prior information is
    used in the transformation; the mean is only calculated using the dataset `X`
    which is passed to the function.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个函数是一个*无状态*函数，因为在转换过程中没有使用先前的信息；均值仅仅是利用传递给函数的数据集`X`计算的。
- en: The second is a *stateful* function which uses `column1_mean_train` (i.e. the
    mean value of `column1` from the training set `X_train`) to replace missing values
    in `X`.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个是一个*有状态*函数，它使用`column1_mean_train`（即`column1`在训练集`X_train`中的均值）来替换`X`中的缺失值。
- en: The distinction between stateless and stateful transformation might seem a bit
    abstruse, but it’s an incredibly important concept in ML tasks where we have separate
    training and testing datasets. Whenever we want to replace missing values, scale
    features or perform one-hot encoding on our testing datasets, we want these transformations
    to be based on the values observed in the training dataset. In other words, we
    want our Transformer to be ***fit*** to the training set. Using the example of
    imputing missing values with the mean, we would want the “mean” value to be the
    mean value of the training set.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 无状态和有状态转换之间的区别可能看起来有些晦涩，但在机器学习任务中这是一个非常重要的概念，尤其是在我们有独立的训练和测试数据集时。每当我们想要替换缺失值、缩放特征或对测试数据集进行独热编码时，我们希望这些转换基于训练数据集中观察到的值。换句话说，我们希望我们的转换器能***适配***训练集。以用均值填补缺失值为例，我们希望“均值”是训练集的均值。
- en: The problem with using `FunctionTransformer` is that it can’t be used to implement
    stateful transformations. Even though a `Transformer` created with `FunctionTransformer`
    *technically* has the `.fit()` method, [calling it won’t do anything](https://stackoverflow.com/a/62225211),
    and so we can’t really “fit” this Transformer to the training data. Why? Because
    the transformations in a `FunctionTransformer`-created Transformer are always
    dependent on the function’s input value `X`. Our `Transformer` will always re-calculate
    the values using the dataset it is passed; it has no way of imputing/transforming
    with a pre-calculated value.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `FunctionTransformer` 的问题在于它不能用于实现有状态的转换。即使一个用 `FunctionTransformer` 创建的
    `Transformer` *在技术上* 拥有 `.fit()` 方法，[调用它不会有任何效果](https://stackoverflow.com/a/62225211)，因此我们不能真正将这个
    Transformer “拟合”到训练数据上。为什么？因为在 `FunctionTransformer` 创建的 Transformer 中，转换始终依赖于函数的输入值
    `X`。我们的 `Transformer` 将始终使用传递给它的数据集重新计算值；它无法用预计算的值进行插补/转换。
- en: 1.2 An example to illustrate the limitations of FunctionTransformer
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1.2 一个示例来说明 FunctionTransformer 的局限性
- en: To illustrate this, here’s an example where I try to “fit” a FunctionTransformer-based
    Transformer to a training set and then transform the testing set using this supposedly
    “fitted” transformer. As you can see, the missing values in the testing set are
    not replaced with the mean value from the training set; they are recalculated
    based on the testing set. In other words, the Transformer was unable to apply
    a stateful transformation.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，这里有一个示例，其中我尝试将一个基于 FunctionTransformer 的 Transformer “拟合”到训练集上，然后使用这个假定已“拟合”的
    Transformer 对测试集进行转换。如你所见，测试集中的缺失值没有被用训练集中的均值替代；它们是根据测试集重新计算的。换句话说，Transformer
    无法应用有状态的转换。
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/648add161d7746faa7471bc53ccb2e89.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/648add161d7746faa7471bc53ccb2e89.png)'
- en: Image by author, showing a missing value in the third row of the testing set
    in the column ‘Age’
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供，显示了测试集中“年龄”列的第三行中的缺失值
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/7973d4ccbc7bcff0978c0416e85a741a.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7973d4ccbc7bcff0978c0416e85a741a.png)'
- en: Image by author. The missing value in the third row was replaced with the mean
    of the testing set, not the mean of the training set, illustrating the inability
    of FuntionTransformer to produce Transformers capable of stateful transformations.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。第三行中的缺失值被替换为测试集的均值，而不是训练集的均值，说明了 FunctionTransformer 无法生成能够进行有状态转换的
    Transformer。
- en: 'If this all sounds a bit confusing, don’t sweat it. The key takeaway message
    is: if you want to define a custom Transformer that can preprocess testing datasets
    based on the values observed in the training dataset, you can’t use a `FunctionTransformer`,
    and you’ll need to take a different approach.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这一切听起来有点混乱，不用担心。关键的信息是：如果你想定义一个能够基于训练数据集观察到的值对测试数据集进行预处理的自定义 Transformer，你不能使用
    `FunctionTransformer`，你需要采取不同的方法。
- en: 2\. Create a custom Transformer from scratch
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 从头开始创建自定义 Transformer
- en: 'One alternative approach is to define a new Transformer class which inherits
    from a class found in the `sklearn.base` module: `TransformerMixin`. This new
    class will then function as a Transformer, and is suitable for applying both stateless
    *and* stateful transformations.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 一种替代方法是定义一个新的 Transformer 类，该类继承自 `sklearn.base` 模块中的类：`TransformerMixin`。这个新类将作为一个
    Transformer 使用，适用于无状态的 *以及* 有状态的转换。
- en: 'Here’s how we’d take our `extract_title` code snippet and turn it into a Transformer
    using this approach:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们如何将 `extract_title` 代码片段转化为一个 Transformer 的方法：
- en: '[PRE9]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/a966e4f57584d07d7af04bfd212711ae.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a966e4f57584d07d7af04bfd212711ae.png)'
- en: Image by author
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: As you can see, we achieve the exact same transformation as we did when constructing
    our Transformer using FunctionTransformer.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们达到了与构建使用 FunctionTransformer 的 Transformer 时完全相同的转换效果。
- en: 2.1 Passing arguments to a custom Transformer
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 向自定义 Transformer 传递参数
- en: 'If you need to pass data to your custom Transformer, simply define an `__init__()`
    method before defining the `fit()` and `transform()` methods:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要将数据传递给自定义 Transformer，只需在定义 `fit()` 和 `transform()` 方法之前定义一个 `__init__()`
    方法：
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/7dd3fd7c128c72d53e24e9a5b3068bde.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7dd3fd7c128c72d53e24e9a5b3068bde.png)'
- en: Image by author
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'And there you have it: two methods for building customised Transformers in
    scikit-learn. The ability to use these methods is an incredibly valuable one,
    and something which I regularly use in my day-to-day job as a Data Scientist.
    I hope you’ve found it useful.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样：在 scikit-learn 中构建自定义 Transformers 的两种方法。能够使用这些方法是非常有价值的，这也是我作为数据科学家在日常工作中经常用到的。我希望你觉得这很有用。
- en: If you liked this article and you’d like to get further tips and insights on
    working in Data Science, consider following me here on Medium or [LinkedIn](https://www.linkedin.com/in/matt-chapman-ba8488118/).
    If you’d like to get unlimited access to all of my stories (and the rest of Medium.com),
    you can sign up via my [referral link](https://medium.com/@mattchapmanmsc/membership)
    for $5 per month. It adds no extra cost to you vs. signing up via the general
    signup page, and helps to support my writing as I get a small commission.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢这篇文章并希望获得更多数据科学方面的技巧和见解，可以考虑在 Medium 上关注我或[LinkedIn](https://www.linkedin.com/in/matt-chapman-ba8488118/)。如果你希望无限访问我所有的故事（以及
    Medium.com 的其他内容），你可以通过我的[推荐链接](https://medium.com/@mattchapmanmsc/membership)以每月
    $5 的价格注册。与通过普通注册页面相比，这不会额外增加你的费用，并且支持我的写作，因为我将获得一小部分佣金。
- en: Thanks for reading!
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
