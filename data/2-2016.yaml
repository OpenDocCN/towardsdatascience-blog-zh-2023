- en: Build a Segmentation Model with One Line of Code
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用一行代码构建一个分割模型
- en: 原文：[https://towardsdatascience.com/the-essential-library-to-build-segmentation-models-6e17e81338e](https://towardsdatascience.com/the-essential-library-to-build-segmentation-models-6e17e81338e)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-essential-library-to-build-segmentation-models-6e17e81338e](https://towardsdatascience.com/the-essential-library-to-build-segmentation-models-6e17e81338e)
- en: Build and train a neural network model for image segmentation in the fastest
    way
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 以最快的方式构建和训练图像分割神经网络模型
- en: '[](https://mattiagatti.medium.com/?source=post_page-----6e17e81338e--------------------------------)[![Mattia
    Gatti](../Images/9d5aeb356ff01ed9e4ead66c18994595.png)](https://mattiagatti.medium.com/?source=post_page-----6e17e81338e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6e17e81338e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6e17e81338e--------------------------------)
    [Mattia Gatti](https://mattiagatti.medium.com/?source=post_page-----6e17e81338e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mattiagatti.medium.com/?source=post_page-----6e17e81338e--------------------------------)[![Mattia
    Gatti](../Images/9d5aeb356ff01ed9e4ead66c18994595.png)](https://mattiagatti.medium.com/?source=post_page-----6e17e81338e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----6e17e81338e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----6e17e81338e--------------------------------)
    [Mattia Gatti](https://mattiagatti.medium.com/?source=post_page-----6e17e81338e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----6e17e81338e--------------------------------)
    ·6 min read·Mar 6, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----6e17e81338e--------------------------------)
    ·阅读时间6分钟·2023年3月6日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ad811a836b54548872ee8eafd0abbba0.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad811a836b54548872ee8eafd0abbba0.png)'
- en: '[MartinThoma](https://commons.wikimedia.org/wiki/File:Image-segmentation-example-segmented.png),
    CC0, via Wikimedia Commons (edited)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[MartinThoma](https://commons.wikimedia.org/wiki/File:Image-segmentation-example-segmented.png)，CC0，通过Wikimedia
    Commons（已编辑）'
- en: Neural network models have proven to be highly effective in solving segmentation
    problems, achieving state-of-the-art accuracy. They have led to significant improvements
    in various applications, including medical image analysis, autonomous driving,
    robotics, satellite imagery, video surveillance, and much more. However, building
    these models usually takes a long time, but after reading this guide you will
    be able to build one with just a few lines of code.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络模型在解决分割问题上已被证明非常有效，达到了最先进的准确率。它们在医学图像分析、自动驾驶、机器人技术、卫星图像、视频监控等各种应用中取得了显著的改进。然而，构建这些模型通常需要较长时间，但阅读完本指南后，你将能够用仅仅几行代码来构建一个模型。
- en: Table of content
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目录
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 介绍
- en: Building blocks
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建模块
- en: Build a model
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建模型
- en: Train the model
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型
- en: Introduction
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'Segmentation is the task of dividing an image into multiple segments or regions
    based on certain characteristics or properties. A segmentation model takes an
    image as input and returns a segmentation mask:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 分割是将图像根据某些特征或属性划分为多个片段或区域的任务。一个分割模型以图像为输入，并返回一个分割掩码：
- en: '![](../Images/18664e6573f75bd5b860c3694ed659e2.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/18664e6573f75bd5b860c3694ed659e2.png)'
- en: (Left) An input image | (Right) Its segmentation mask. Both images by [PyTorch](https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: （左）输入图像 | （右）其分割掩码。两张图像均来自[PyTorch](https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/)。
- en: 'Segmentation neural network models consist of two parts:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 分割神经网络模型由两个部分组成：
- en: 'An **encoder**: takes an input image and extracts features. Examples of encoders
    are ResNet, EfficentNet, and ViT.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**编码器**：接受输入图像并提取特征。编码器的例子包括ResNet、EfficientNet和ViT。
- en: 'A **decoder**: takes the extracted features and generates a segmentation mask.
    The decoder varies on the architecture. Examples of architectures are U-Net, FPN,
    and DeepLab.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**解码器**：提取特征并生成分割掩码。解码器的架构各不相同。例如的架构包括U-Net、FPN和DeepLab。
- en: Thus, when building a segmentation model for a specific application, you need
    to choose an architecture and an encoder. However, it is difficult to choose the
    best combination without testing several. This usually takes a long time because
    changing the model requires writing a lot of boilerplate code. The [Segmentation
    Models](https://github.com/qubvel/segmentation_models.pytorch) library solves
    this problem. It allows you to create a model in a single line by specifying the
    architecture and the encoder. Then you only need to modify that line to change
    either of them.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在为特定应用构建分割模型时，你需要选择一个架构和一个编码器。然而，在不测试多个组合的情况下，很难选择最佳组合。这通常需要很长时间，因为更改模型需要编写大量的样板代码。[Segmentation
    Models](https://github.com/qubvel/segmentation_models.pytorch) 库解决了这个问题。它允许你通过指定架构和编码器在一行代码中创建一个模型。然后，你只需修改该行代码即可更改其中的一个。
- en: 'To install the latest version of Segmentation Models from PyPI use:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 要从 PyPI 安装最新版本的 Segmentation Models，请使用：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Building Blocks
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建模块
- en: 'The library provides a class for most segmentation architectures and each of
    them can be used with any of the available encoders. In the next section, you
    will see that to build a model you need to instantiate the class of the chosen
    architecture and pass the string of the chosen encoder as a parameter. The figure
    below shows the class name of each architecture provided by the library:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 该库为大多数分割架构提供了一个类，每个类都可以与任何可用的编码器一起使用。在下一部分中，你将看到要构建模型，你需要实例化所选架构的类，并将所选编码器的字符串作为参数传递。下图显示了库提供的每个架构的类名称：
- en: '![](../Images/b49a27e0de58532dca66f45a639efa55.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b49a27e0de58532dca66f45a639efa55.png)'
- en: Class names of all the architectures provided by the library.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 库提供的所有架构的类别名称。
- en: 'The figure below shows the names of the most common encoders provided by the
    library:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了库提供的最常见编码器的名称：
- en: '![](../Images/d1756893e86b8006ecc1b313fda9e6bc.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d1756893e86b8006ecc1b313fda9e6bc.png)'
- en: Names of the most common encoders provided by the library.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 库提供的最常见编码器的名称。
- en: There are over 400 encoders, thus it’s not possible to show them all, but you
    can find a comprehensive list [here](https://github.com/qubvel/segmentation_models.pytorch#encoders).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有超过400种编码器，因此不可能一一展示，但你可以在[这里](https://github.com/qubvel/segmentation_models.pytorch#encoders)找到完整的列表。
- en: Build a Model
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建模型
- en: 'Once the architecture and the encoder have been chosen from the figures above,
    building the model is very simple:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦从上面的图中选择了架构和编码器，构建模型就非常简单：
- en: 'Parameters:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 参数：
- en: '`encoder_name` is the name of the chosen encoder (e.g. resnet50, efficentnet-b7,
    mit_b5).'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_name` 是所选编码器的名称（例如：resnet50、efficientnet-b7、mit_b5）。'
- en: '`encoder_weights` is the dataset of the pre-trained. If `encoder_weights` is
    equal to `"imagenet"` the encoder weights are initialized by using the ImageNet
    pre-trained. All the encoders have at least one pre-trained and a comprehensive
    list is available [here](https://github.com/qubvel/segmentation_models.pytorch#encoders).'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encoder_weights` 是预训练数据集。如果 `encoder_weights` 等于 `"imagenet"`，则编码器权重将使用 ImageNet
    预训练权重初始化。所有编码器至少有一种预训练模型，完整列表可以在[这里](https://github.com/qubvel/segmentation_models.pytorch#encoders)找到。'
- en: '`in_channels` is the channel count of the input image (3 if RGB).'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`in_channels` 是输入图像的通道数（如果是 RGB，则为 3）。'
- en: 'Even if `in_channels` is not 3 an ImageNet pre-trained can be used: the first
    layer will be initialized by reusing the weights from the pre-trained first convolutional
    layer (the procedure is described [here](https://github.com/qubvel/segmentation_models.pytorch#input-channels)).'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 即使 `in_channels` 不是 3，也可以使用 ImageNet 预训练模型：第一层将通过重用预训练的第一个卷积层的权重来初始化（该过程描述[这里](https://github.com/qubvel/segmentation_models.pytorch#input-channels)）。
- en: '`out_classes` is the number of classes in the dataset.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`out_classes` 是数据集中的类别数量。'
- en: '`activation` is the activation function for the output layer. The possible
    choices are `None` (default), `sigmoid` and `softmax` .'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation` 是输出层的激活函数。可选的值有 `None`（默认）、`sigmoid` 和 `softmax`。'
- en: '**Note:** when using a loss function that expects logits as input, the activation
    function must be None.For example, when using the `CrossEntropyLoss` function,
    `activation` must be `None` .'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**注意：** 当使用期望 logits 作为输入的损失函数时，激活函数必须为 None。例如，当使用 `CrossEntropyLoss` 函数时，`activation`
    必须为 `None`。'
- en: Train the model
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: This section shows all the code required to perform training. However, this
    library doesn’t change the usual pipeline for training and validating a model.
    To simplify the process, the library provides the implementation of many loss
    functions such as *Jaccard Loss, Dice Loss, Dice Cross-Entropy Loss, Focal Loss,*
    and metrics such as *Accuracy, Precision, Recall, F1Score, and IOUScore*. For
    a complete list of them and their parameters, check their documentation in the
    [Losses](https://smp.readthedocs.io/en/latest/losses.html) and [Metrics](https://smp.readthedocs.io/en/latest/metrics.html)
    sections.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 本节展示了执行训练所需的所有代码。然而，这个库不会改变训练和验证模型的常规流程。为了简化过程，该库提供了许多损失函数的实现，例如*Jaccard Loss,
    Dice Loss, Dice Cross-Entropy Loss, Focal Loss*，以及*Accuracy, Precision, Recall,
    F1Score, 和 IOUScore*等指标。有关它们及其参数的完整列表，请查阅[Losses](https://smp.readthedocs.io/en/latest/losses.html)和[Metrics](https://smp.readthedocs.io/en/latest/metrics.html)部分的文档。
- en: 'The proposed training example is a binary segmentation using the [Oxford-IIIT
    Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/) (it will be downloaded
    by code). These are two samples from the dataset:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 提议的训练示例是使用[Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/)进行二分类分割（将通过代码下载）。以下是数据集中的两个样本：
- en: '![](../Images/b3778e77e2af3b35453e7c23c7171992.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b3778e77e2af3b35453e7c23c7171992.png)'
- en: A sample of a cat from the Oxford-IIIT Pet Dataset.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Oxford-IIIT Pet Dataset中的一只猫的样本。
- en: '![](../Images/9bc5248992644cf2eca7c0358331308e.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9bc5248992644cf2eca7c0358331308e.png)'
- en: A sample of a dog from the Oxford-IIIT Pet Dataset.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Oxford-IIIT Pet Dataset中的一只狗的样本。
- en: 'Finally, these are all steps to perform this type of segmentation task:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这些是执行此类分割任务的所有步骤：
- en: Build the model.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建模型。
- en: Set the activation function of the last layer depending on the loss function
    you are going to use.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你将使用的损失函数设置最后一层的激活函数。
- en: 2\. Define the parameters.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 定义参数。
- en: Remember that when using a pre-trained, the input should be normalized by using
    the mean and standard deviation of the data used to train the pre-trained.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在使用预训练模型时，输入应通过使用训练预训练模型时的数据的均值和标准差来进行归一化。
- en: 3\. Define the train function.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 定义训练函数。
- en: Nothing changes here from the train function you would have written to train
    a model without using the library.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这里没有改变，你在不使用库的情况下训练模型时所编写的训练函数。
- en: 4\. Define the validation function.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 定义验证函数。
- en: True positives, false positives, false negatives and true negatives from batches
    are all summed together to calculate metrics only at the end of batches. Note
    that logits must be converted to classes before metrics can be calculated. Call
    the train function to start training.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 真阳性、假阳性、假阴性和真阴性从批次中一起求和，仅在批次结束时计算指标。请注意，logits必须转换为类别后才能计算指标。调用训练函数开始训练。
- en: 5\. Use the model.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 5. 使用模型。
- en: 'These are some segmentations:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是一些分割示例：
- en: '![](../Images/4eb0f727a6a5f922ae61e421c080d9f0.png)![](../Images/54b4f642093a462f5e98794c8d56f3f7.png)![](../Images/6d7a007d8d96edcf33c0b1fc4abcf432.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4eb0f727a6a5f922ae61e421c080d9f0.png)![](../Images/54b4f642093a462f5e98794c8d56f3f7.png)![](../Images/6d7a007d8d96edcf33c0b1fc4abcf432.png)'
- en: Concluding remarks
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: This library has everything you need to experiment with segmentation. It’s very
    easy to build a model and apply changes, and most loss functions and metrics are
    provided. In addition, using this library doesn’t change the pipeline we’re used
    to. See the [official documentation](https://smp.readthedocs.io/en/latest/) for
    more information. I have also included some of the most common encoders and architectures
    in the references.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这个库包含了你实验分割所需的一切。构建模型和应用更改非常简单，并且大多数损失函数和指标都已提供。此外，使用这个库不会改变我们习惯的流程。有关更多信息，请参见[官方文档](https://smp.readthedocs.io/en/latest/)。我还在参考文献中包括了一些最常见的编码器和架构。
- en: '[The Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/)
    is available to download for commercial/research purposes under a [Creative Commons
    Attribution-ShareAlike 4.0 International License.](https://creativecommons.org/licenses/by-sa/4.0/)
    The copyright remains with the original owners of the images.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/)在[创作共用署名-相同方式共享
    4.0 国际许可协议](https://creativecommons.org/licenses/by-sa/4.0/)下可供下载用于商业/研究目的。版权归图像的原始所有者所有。'
- en: All images, unless otherwise noted, are by the Author. Thanks for reading, I
    hope you have found this useful.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 所有图像，除非另有说明，均由作者提供。感谢阅读，希望你觉得这些信息有用。
- en: '[1] O. Ronneberger, P. Fischer and T. Brox, [U-Net: Convolutional Networks
    for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597) (2015)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] O. Ronneberger, P. Fischer 和 T. Brox, [U-Net: 用于生物医学图像分割的卷积网络](https://arxiv.org/abs/1505.04597)
    (2015)'
- en: '[2] Z. Zhou, Md. M. R. Siddiquee, N. Tajbakhsh and J. Liang, [UNet++: A Nested
    U-Net Architecture for Medical Image Segmentation](https://arxiv.org/abs/1807.10165)
    (2018)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Z. Zhou, Md. M. R. Siddiquee, N. Tajbakhsh 和 J. Liang, [UNet++: 一种嵌套的 U-Net
    架构用于医学图像分割](https://arxiv.org/abs/1807.10165) (2018)'
- en: '[3] L. Chen, G. Papandreou, F. Schroff, H. Adam, [Rethinking Atrous Convolution
    for Semantic Image Segmentation](https://arxiv.org/abs/1706.05587) (2017)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] L. Chen, G. Papandreou, F. Schroff, H. Adam, [重新思考用于语义图像分割的膨胀卷积](https://arxiv.org/abs/1706.05587)
    (2017)'
- en: '[4] L. Chen, Y. Zhu, G. Papandreou, F. Schroff, H. Adam, [Encoder-Decoder with
    Atrous Separable Convolution for Semantic Image Segmentation](https://arxiv.org/abs/1802.02611)
    (2018)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] L. Chen, Y. Zhu, G. Papandreou, F. Schroff, H. Adam, [用于语义图像分割的编码器-解码器与膨胀可分离卷积](https://arxiv.org/abs/1802.02611)
    (2018)'
- en: '[5] R. Li, S. Zheng, C. Duan, C. Zhang, J. Su, P.M. Atkinson, [Multi-Attention-Network
    for Semantic Segmentation of Fine Resolution Remote Sensing Images](https://arxiv.org/abs/2009.02130)
    (2020)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] R. Li, S. Zheng, C. Duan, C. Zhang, J. Su, P.M. Atkinson, [用于细分辨率遥感图像语义分割的多注意力网络](https://arxiv.org/abs/2009.02130)
    (2020)'
- en: '[6] A. Chaurasia, E. Culurciello, [LinkNet: Exploiting Encoder Representations
    for Efficient Semantic Segmentation](https://arxiv.org/abs/1707.03718) (2017)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] A. Chaurasia, E. Culurciello, [LinkNet: 利用编码器表示进行高效的语义分割](https://arxiv.org/abs/1707.03718)
    (2017)'
- en: '[7] T. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, S. Belongie, [Feature
    Pyramid Networks for Object Detection](https://arxiv.org/abs/1612.03144) (2017)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] T. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, S. Belongie, [用于目标检测的特征金字塔网络](https://arxiv.org/abs/1612.03144)
    (2017)'
- en: '[8] H. Zhao, J. Shi, X. Qi, X. Wang, J. Jia, [Pyramid Scene Parsing Network](https://arxiv.org/abs/1612.01105)
    (2016)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] H. Zhao, J. Shi, X. Qi, X. Wang, J. Jia, [金字塔场景解析网络](https://arxiv.org/abs/1612.01105)
    (2016)'
- en: '[9] H. Li, P. Xiong, J. An, L. Wang, [Pyramid Attention Network for Semantic
    Segmentation](https://arxiv.org/abs/1805.10180) (2018)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] H. Li, P. Xiong, J. An, L. Wang, [用于语义分割的金字塔注意力网络](https://arxiv.org/abs/1805.10180)
    (2018)'
- en: '[10] K. Simonyan, A. Zisserman, [Very Deep Convolutional Networks for Large-Scale
    Image Recognition](https://arxiv.org/abs/1409.1556) (2014)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[10] K. Simonyan, A. Zisserman, [用于大规模图像识别的非常深的卷积网络](https://arxiv.org/abs/1409.1556)
    (2014)'
- en: '[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, [Deep Residual Learning
    for Image Recognition](https://arxiv.org/abs/1512.03385) (2015)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, [用于图像识别的深度残差学习](https://arxiv.org/abs/1512.03385)
    (2015)'
- en: '[12] S. Xie, R. Girshick, P. Dollár, Z. Tu, K. He, [Aggregated Residual Transformations
    for Deep Neural Networks](https://arxiv.org/abs/1611.05431) (2016)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[12] S. Xie, R. Girshick, P. Dollár, Z. Tu, K. He, [深度神经网络的聚合残差变换](https://arxiv.org/abs/1611.05431)
    (2016)'
- en: '[13] J. Hu, L. Shen, S. Albanie, G. Sun, E. Wu, [Squeeze-and-Excitation Networks](https://arxiv.org/abs/1709.01507)
    (2017)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[13] J. Hu, L. Shen, S. Albanie, G. Sun, E. Wu, [压缩-激励网络](https://arxiv.org/abs/1709.01507)
    (2017)'
- en: '[14] G. Huang, Z. Liu, L. van der Maaten, K. Q. Weinberger, [Densely Connected
    Convolutional Networks](https://arxiv.org/abs/1608.06993) (2016)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[14] G. Huang, Z. Liu, L. van der Maaten, K. Q. Weinberger, [密集连接卷积网络](https://arxiv.org/abs/1608.06993)
    (2016)'
- en: '[15] M. Tan, Q. V. Le, [EfficientNet: Rethinking Model Scaling for Convolutional
    Neural Networks](https://arxiv.org/abs/1905.11946) (2019)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[15] M. Tan, Q. V. Le, [EfficientNet: 重新思考卷积神经网络的模型缩放](https://arxiv.org/abs/1905.11946)
    (2019)'
- en: '[16] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, P. Luo, [SegFormer:
    Simple and Efficient Design for Semantic Segmentation with Transformers](https://arxiv.org/abs/2105.15203)
    (2021)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[16] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, P. Luo, [SegFormer:
    基于 Transformers 的简单高效语义分割设计](https://arxiv.org/abs/2105.15203) (2021)'
