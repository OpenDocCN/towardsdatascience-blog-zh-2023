- en: Probabilistic View of Principal Component Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主成分分析的概率视角
- en: 原文：[https://towardsdatascience.com/probabilistic-view-of-principal-component-analysis-9c1bbb3f167](https://towardsdatascience.com/probabilistic-view-of-principal-component-analysis-9c1bbb3f167)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/probabilistic-view-of-principal-component-analysis-9c1bbb3f167](https://towardsdatascience.com/probabilistic-view-of-principal-component-analysis-9c1bbb3f167)
- en: Latent Variables, Expectation-Maximization & Variational Inference
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 潜在变量、期望最大化与变分推断
- en: '[](https://saptashwa.medium.com/?source=post_page-----9c1bbb3f167--------------------------------)[![Saptashwa
    Bhattacharyya](../Images/b01238113a1f6b91cb6fb0fbfa50303a.png)](https://saptashwa.medium.com/?source=post_page-----9c1bbb3f167--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9c1bbb3f167--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9c1bbb3f167--------------------------------)
    [Saptashwa Bhattacharyya](https://saptashwa.medium.com/?source=post_page-----9c1bbb3f167--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://saptashwa.medium.com/?source=post_page-----9c1bbb3f167--------------------------------)[![Saptashwa
    Bhattacharyya](../Images/b01238113a1f6b91cb6fb0fbfa50303a.png)](https://saptashwa.medium.com/?source=post_page-----9c1bbb3f167--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9c1bbb3f167--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9c1bbb3f167--------------------------------)
    [Saptashwa Bhattacharyya](https://saptashwa.medium.com/?source=post_page-----9c1bbb3f167--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9c1bbb3f167--------------------------------)
    ·9 min read·Jul 12, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9c1bbb3f167--------------------------------)
    ·9分钟阅读·2023年7月12日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ea43c4d373d4f057fb224899a1bae3d3.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea43c4d373d4f057fb224899a1bae3d3.png)'
- en: 'Looking for Hidden Variables (Pic Credit: [Author](https://flickr.com/photos/suvob/52911155451/))'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找隐藏变量（图片来源：[作者](https://flickr.com/photos/suvob/52911155451/)）
- en: One of the primarily used dimension reduction techniques in data science and
    machine learning is Principal Component Analysis (PCA). Previously, We have already
    discussed a few examples of applying PCA in a [pipeline](/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976)
    with [Support Vector Machine](/visualizing-support-vector-machine-decision-boundary-69e7591dacea)
    and here we will see a probabilistic perspective of PCA to provide a more robust
    and comprehensive understanding of the underlying data structure. *One of the
    biggest advantages of Probabilistic PCA (PPCA) is that it can handle missing values
    in a dataset, which is not possible with classical PCA.* Since we will discuss
    Latent Variable Model and Expectation-Maximization algorithm, you can also check
    [this detailed post](/latent-variables-expectation-maximization-algorithm-fb15c4e0f32c).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学和机器学习中，主要使用的降维技术之一是主成分分析（PCA）。之前，我们已经讨论过将PCA应用于 [管道](/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976)与
    [支持向量机](/visualizing-support-vector-machine-decision-boundary-69e7591dacea) 的一些例子，在这里我们将从概率的角度来看PCA，以提供对数据结构的更全面和稳健的理解。*概率PCA（PPCA）的一个最大优点是它能够处理数据集中缺失的值，这是经典PCA无法做到的。*
    由于我们将讨论潜在变量模型和期望最大化算法，你还可以查看 [这篇详细的文章](/latent-variables-expectation-maximization-algorithm-fb15c4e0f32c)。
- en: What you can expect to learn from this post?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从这篇文章中学到什么？
- en: Short Intro to PCA.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PCA简短介绍。
- en: Mathematical building blocks for PPCA.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PPCA的数学构建块。
- en: Expectation Maximization (EM) algorithm or Variational Inference? What to use
    for parameter estimation?
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 期望最大化（EM）算法或变分推断？用于参数估计时应该使用哪一个？
- en: Implementing PPCA with TensorFlow Probability for a toy dataset.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用TensorFlow Probability在玩具数据集上实现PPCA。
- en: Let’s dive into this!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨一下吧！
- en: '1\. Singular Value Decomposition (SVD) and PCA:'
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 奇异值分解（SVD）和PCA：
- en: 'One of the major important concepts in Linear Algebra is SVD and it’s a factorization
    technique for real or complex matrices where for example a matrix (say *A*) can
    be factorized as:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 线性代数中一个重要的概念是**SVD**，它是一种对实数或复数矩阵进行分解的技术，例如，一个矩阵（假设为*A*）可以被分解为：
- en: '![](../Images/e2bb0c0f2b17a3d863970065a4f1196a.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e2bb0c0f2b17a3d863970065a4f1196a.png)'
- en: 'Eq. 1: SVD of a matrix A.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 方程1：矩阵A的SVD。
- en: where *U*,*Vᵀ* are orthogonal matrices (transpose equals the inverse) and Σ
    would be a diagonal matrix. *A* need not be a square matrix, say it’s a *N*×*D*
    matrix so we can already think of this as our data matrix with *N* instances and
    *D* features. *U*,*V* are square matrices (*N*×*N*) and (*D*×*D*) respectively,
    and Σ will then be an *N*×*D* matrix where the *D*×*D* subset will be diagonal
    and the remaining entries will be zero.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *U*,*Vᵀ* 是正交矩阵（转置等于逆），而 Σ 将是一个对角矩阵。*A* 不需要是方阵，例如它是一个 *N*×*D* 矩阵，因此我们可以将其视为我们的数据矩阵，其中
    *N* 个实例和 *D* 个特征。*U*,*V* 分别是方阵 (*N*×*N*) 和 (*D*×*D*)，Σ 将是一个 *N*×*D* 矩阵，其中 *D*×*D*
    的子集是对角的，其余条目为零。
- en: 'We also know Eigenvalue decomposition. Given a square matrix (*B*) which is
    diagonalizable can be factorized as:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也知道特征值分解。给定一个可以对角化的方阵 (*B*) 可以分解为：
- en: '![](../Images/a2164eb84cd5f179e2b242fe518c7692.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a2164eb84cd5f179e2b242fe518c7692.png)'
- en: 'Eq. 2: Eigenvalue decomposition of a matrix'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2：矩阵的特征值分解
- en: where *Q* is the square *N*×*N* matrix whose *i*th column is the eigenvector
    *q_i* of *B*, and Λ is the diagonal matrix whose diagonal elements are the corresponding
    eigenvalues.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *Q* 是一个方形的 *N*×*N* 矩阵，其第 *i* 列是 *B* 的特征向量 *q_i*，而 Λ 是对角矩阵，其对角元素是对应的特征值。
- en: Let’s try to modify equation (1) by multiplying it by *Aᵀ.*
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试通过乘以*Aᵀ*来修改方程 (1)。
- en: '![](../Images/5d48d952214ffe5f93db37bce97446e4.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5d48d952214ffe5f93db37bce97446e4.png)'
- en: 'Eq. 3: Multiplying Eq. 1 by the transpose of A.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 3：乘以 A 的转置。
- en: Here, *AᵀA* would be a Square matrix even though *A* initially didn’t need to
    be (could be *m*×*n*). Σ Σ*ᵀ* is a diagonal matrix and *V* is an orthogonal matrix.
    Now, this is basically the eigendecomposition of a matrix *AᵀA*. The eigenvalues
    here are squares of the singular values for *A* in eq. (1).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*AᵀA* 将是一个方阵，即使 *A* 起初不需要是（可以是 *m*×*n*）。Σ Σ*ᵀ* 是一个对角矩阵，而 *V* 是一个正交矩阵。现在，这基本上是矩阵
    *AᵀA* 的特征分解。这里的特征值是方程 (1) 中 *A* 的奇异值的平方。
- en: For a positive semi-definite matrix SVD and eigendecomposition are equivalent.
    PCA boils down to the eigendecomposition of the covariance matrix. Finding the
    maximum eigenvalue(s) and corresponding eigenvector(s) are basically then can
    be thought of as finding the direction of maximum variance.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于正半定矩阵，SVD 和特征分解是等效的。PCA 最终归结为协方差矩阵的特征分解。找到最大特征值和相应的特征向量基本上可以视为找到最大方差的方向。
- en: Given *D* dimensional data (features), a full eigendecomposition would be expensive
    ∼O(*D³*), but now if we choose some latent space dimension *M*(<*D*) then the
    calculation is cheaper ∼O(*MD²*).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 给定 *D* 维数据（特征），完全的特征分解将是昂贵的 ∼O(*D³*)，但现在如果我们选择一些潜在空间维度 *M*（<*D*），则计算会便宜 ∼O(*MD²*)。
- en: '2\. Building Blocks for PPCA:'
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. PPCA 的构建块：
- en: '2.1\. Assumptions:'
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1\. 假设：
- en: 'PPCA is a Latent Variable Model (LVM) and we’ve discussed LVM in [detail before](/latent-variables-expectation-maximization-algorithm-fb15c4e0f32c)
    including the Expectation-Maximization (EM) algorithm. LVMs offer a low-dimensional
    representation of the data. Let’s say our data (*x*) is (*N*×*D*) dimensional,
    with *D* features; then LVM for PCA seeks an *M* dimensional latent vector of
    unobserved variables *z* which can be used to generate the observed variables
    *(x)* and they are related via a linear relationship:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: PPCA 是一个潜在变量模型 (LVM)，我们之前在 [详细讨论过](/latent-variables-expectation-maximization-algorithm-fb15c4e0f32c)
    包括期望最大化 (EM) 算法。LVM 提供了数据的低维表示。假设我们的数据 (*x*) 是 *N*×*D* 维的，具有 *D* 个特征；那么 PCA 的
    LVM 寻求一个 *M* 维的潜在变量向量 *z*，它可以用来生成观察变量 *(x)*，并且它们通过线性关系相互关联：
- en: '![](../Images/de939fbaf97bec238aab132f1e06506e.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de939fbaf97bec238aab132f1e06506e.png)'
- en: 'Eq. 2.1: Generative process for PPCA; x conditioned on latent variables z.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2.1：PPCA 的生成过程；*x* 条件于潜在变量 *z*。
- en: The noise *ϵ* in the equation above is a *D* dimensional vector with a zero
    mean Gaussian and covariance with *σ²I*;
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程中的噪声 *ϵ* 是一个 *D* 维向量，具有零均值高斯分布和 *σ²I* 的协方差；
- en: '![](../Images/f9d5035c2dee33ecd708c6f909edf147.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f9d5035c2dee33ecd708c6f909edf147.png)'
- en: 'Eq. 2\. 2: The noise is modeled as a normal distribution with zero mean & co-variance
    *σ².*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 2\. 2：噪声被建模为具有零均值和协方差 *σ²* 的正态分布。
- en: 'Since we know that our latent space is *M* dimensional, this leaves our *W*
    vector to be *D*×*M* dimensional. The latent variable *z* is assumed to have a
    zero-mean, unit-covariance Gaussian distribution:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们知道潜在空间是 *M* 维的，这使得我们的 *W* 向量是 *D*×*M* 维的。假设潜在变量 *z* 具有零均值、单位协方差的高斯分布：
- en: '![](../Images/162c7ed1a0229edf2bef0391526e5661.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/162c7ed1a0229edf2bef0391526e5661.png)'
- en: 'Eq. 2\. 3: The prior for the latent variable is a zero mean and unit covariance
    normal distribution.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Eq. 2.3：潜在变量的先验是均值为零、协方差为单位的正态分布。
- en: 'The above two equations lead to a conditional distribution of *x* conditioned
    on *z* as below:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 上述两个方程导致条件分布 *x* 在 *z* 下的如下：
- en: '![](../Images/08b4b0e706ff28bd2bddc9ddca2382fe.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08b4b0e706ff28bd2bddc9ddca2382fe.png)'
- en: 'Eq. 2.4: Following the two previous equations, we obtain the conditional distribution
    of *x.*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Eq. 2.4：根据前两个方程，我们得到条件分布 *x*。
- en: which is another normal distribution with mean *Wz* (we can set *μ* = 0) and
    covariance *σ²*.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这是另一个均值为 *Wz*（我们可以设定 *μ* = 0）和协方差为 *σ²* 的正态分布。
- en: 'The above equation should remind us of a fundamental property of a normal distribution:
    i.e., if *x* follows multivariate normal distribution *x*∼N(*μ*, Σ), then any
    linear transformation of *x* is also multivariate normal distribution *y* = *Ax*
    + *b* ∼ N(*Aμ*+*b*, *A*Σ*Aᵀ*).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方程应让我们想起正态分布的一个基本属性：即，如果 *x* 服从多元正态分布 *x*∼N(*μ*, Σ)，则 *x* 的任何线性变换也是多元正态分布
    *y* = *Ax* + *b* ∼ N(*Aμ*+*b*, *A*Σ*Aᵀ*)。
- en: 'Given the joint distribution, the marginal distribution would also be Gaussian:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 给定联合分布，边际分布也将是高斯分布：
- en: '![](../Images/7decb22c5fabe1d1ff27972d8d405374.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7decb22c5fabe1d1ff27972d8d405374.png)'
- en: 'Eq. 2.5: The data distribution also follows a normal distribution.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Eq. 2.5：数据分布也服从正态分布。
- en: As we would like to determine the parameters *W*, *μ*, *σ* we can approach this
    problem via MLE or EM algorithm. Here we will focus on the EM approach and then
    on Variational Inference. Both approaches are well described in Bishop’s book.
    Bishop argues that as the data dimensionality increases, we may gain computational
    advantages over MLE via iterative EM steps. This has to do with mainly the computational
    cost of the covariance matrix, where the evaluation of the *D* dimensional data
    covariance matrix takes O(*ND²*), *N* being the number of data points.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们希望确定参数 *W*、*μ*、*σ*，我们可以通过MLE或EM算法来解决这个问题。这里我们将关注EM方法，然后是变分推断。两种方法在Bishop的书中都有详细描述。Bishop认为，随着数据维度的增加，通过迭代的EM步骤，我们可能在计算上比MLE获得优势。这主要涉及协方差矩阵的计算成本，其中
    *D* 维数据协方差矩阵的评估需要O(*ND²*)，*N*是数据点的数量。
- en: '2.2\. EM Steps for PPCA:'
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2. PPCA的EM步骤：
- en: 'Before we discussed the [EM algorithm](https://medium.com/towards-data-science/latent-variables-expectation-maximization-algorithm-fb15c4e0f32c)
    in reference to Gaussian Mixture Models here I will describe it in reference to
    PPCA. The steps for the EM algorithm are as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论了[EM算法](https://medium.com/towards-data-science/latent-variables-expectation-maximization-algorithm-fb15c4e0f32c)
    参考于高斯混合模型之后，这里我将其参考于PPCA进行描述。EM算法的步骤如下：
- en: In the expectation step, we calculate the expectation of the complete data log-likelihood
    w.r.t its posterior distribution of the latent variables (*p(z|x)*) evaluated
    using the ‘old’ parameters.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在期望步骤中，我们计算完整数据对数似然相对于潜在变量的后验分布 (*p(z|x)*) 的期望，使用的是“旧”参数。
- en: Maximizing this data log-likelihood yields ‘new’ parameters which will be plugged
    into step 1.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大化该数据对数似然函数将得到“新”的参数，这些参数将被插入到第1步中。
- en: 'As the data points are independent the complete data likelihood would be:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据点是独立的，完全数据似然将是：
- en: '![](../Images/5adc4fe25f647b0ff7325894aa3079a4.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5adc4fe25f647b0ff7325894aa3079a4.png)'
- en: 'Eq. 2.6: The complete data likelihood including observed and latent variables.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Eq. 2.6：包括观察变量和潜在变量在内的完全数据似然。
- en: The main target for the E-step is to calculate the expectation of the expression
    above. Here we have to use *p*(*x*|*z*), *p*(*z*) from equations 3 and 4 respectively
    in section 3\. The derivation is given in Bishop’s book but the important part
    is that the derivation requires the calculation of E[*z_n*], E[*z_n zᵀ_n*], and,
    this can be derived using the posterior distribution *p*(*z*|*x*).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: E步的主要目标是计算上述表达式的期望。在这里，我们需要使用方程3和4中 *p*(*x*|*z*) 和 *p*(*z*)。推导在Bishop的书中给出，但重要的是推导需要计算E[*z_n*]、E[*z_n
    zᵀ_n*]，这些可以通过后验分布 *p*(*z*|*x*) 推导得出。
- en: Once the E-step is done, the M-step then involves maximizing this expected log-likelihood
    w.r.t the parameters *W*,*σ²*.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦E步完成，M步则涉及最大化相对于参数 *W*、*σ²* 的期望对数似然。
- en: 'Variational Inference, EM & ELBO:'
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变分推断、EM 和 ELBO：
- en: The EM steps described above depend on a certain crucial assumption that the
    posterior distribution *p(z|x)* is tractable (necessary for the E step in eq.
    2.6.). *What if that’s not the case?* What if there’s not any analytic expression
    for the posterior? This is the base of Variational Inference.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 上述EM步骤依赖于一个关键假设，即后验分布*p(z|x)*是可处理的（这是方程2.6中的E步所必需的）。*如果不是这样呢？* 如果后验没有任何解析表达式？这就是变分推断的基础。
- en: 'We now take help from variational methods. The main idea is that we try to
    find a distribution *q(z)* that can be as close as the posterior distribution
    *p(z|x)*. This approximation distribution can have its own variational parameters:
    *q(z|θ)*, and we try to find the setting of the parameters that make *q* close
    to the posterior of interest. *q(z)* should be relatively easy and more tractable
    for inference. To measure the closeness of the two distributions *q*(*z*) and
    *p*(*z*|*x*), a common metric is the Kullback-Leibler (KL) divergence. The KL
    divergence for Variational Inference naturally introduces Evidence Lower Bound
    (ELBO) as:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在借助变分方法。主要思想是我们尝试找到一个分布*q(z)*，使其尽可能接近后验分布*p(z|x)*。这个近似分布可以有自己的变分参数：*q(z|θ)*，我们尝试找到使*q*接近感兴趣后验的参数设置。*q(z)*应该相对简单，更易于推断。为了衡量两个分布*q*(z)和*p*(z|x)*的接近程度，常用的度量是Kullback-Leibler
    (KL) 散度。变分推断中的KL散度自然引入了证据下界（ELBO）：
- en: '![](../Images/a9e0a98f77e8220fe44231172eee5bde.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9e0a98f77e8220fe44231172eee5bde.png)'
- en: 'Eq. 2.7: Closeness of q(z) and posterior p(z|x) and ELBO adds together to give
    us the data likelihood.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 方程2.7：*q(z)*和后验*p(z|x)*的接近程度，ELBO加在一起给我们数据的可能性。
- en: 'where ELBO *(q)* is defined as:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 其中ELBO *(q)* 被定义为：
- en: '![](../Images/46c8b760f180164dd234714d696f6ecd.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/46c8b760f180164dd234714d696f6ecd.png)'
- en: 'Eq. 2.8: Definition of ELBO'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 方程2.8：ELBO的定义
- en: For the derivation, you can check my notebook in the reference or any other
    available lecture notes.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于推导，你可以查看参考中的笔记本或其他可用的讲义。
- en: Since KL divergence is non-negative, log *p*(*x*) ≥ ELBO(*q*). So what we do
    in Variational Inference (VI) is that we maximize the ELBO.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 由于KL散度是非负的，log *p*(x)* ≥ ELBO(*q*)。所以我们在变分推断（VI）中所做的就是最大化ELBO。
- en: We can also easily see the connection between VI and the traditional EM algorithm;
    when *q*(*x*)==*p*(*z*|*x*) as the KL divergence term vanishes.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以很容易地看到VI和传统EM算法之间的联系；当*q*(x)==*p*(z|x)*时，KL散度项消失。
- en: Since we have now completed the overview of the PPCA, we will now implement
    this using TensorFlow Probability and we will use the definition of ELBO and try
    to maximize it which in turn is equivalent to maximizing the data likelihood log
    *p(x)*.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们现在已经完成了PPCA的概述，我们将使用TensorFlow Probability来实现这一点，并使用ELBO的定义尝试最大化它，这反过来等同于最大化数据的可能性log
    *p(x)*。
- en: 'Implement PPCA Using TensorFlow Probability:'
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TensorFlow Probability实现PPCA：
- en: For implementing a simple example of PPCA via Variational Inference, I’ll follow
    the [original example](https://www.tensorflow.org/probability/examples/Probabilistic_PCA)
    from the TensorFlow Probability applications.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了通过变分推断实现一个简单的PPCA示例，我将遵循TensorFlow Probability应用中的[原始示例](https://www.tensorflow.org/probability/examples/Probabilistic_PCA)。
- en: 'For implementing this we will assume that we know the standard deviation of
    the noise (*σ*, I chose 3.0) as defined in equation (2.2) and we place a prior
    over *w* and try to estimate via Variational Inference. Let’s define the joint
    distribution:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这一点时，我们将假设我们知道噪声的标准差（*σ*，我选择了3.0），如方程（2.2）所定义，并且我们对*w*进行先验设定，并尝试通过变分推断来估计。让我们定义联合分布：
- en: 'Here we’ve used `JointDistributionCoroutineAutoBatched` , which is an “auto-batched”
    version of `JointDistributionCoroutine`. It automatically applies batch semantics
    to the joint distribution based on the shape of the input arguments. It allows
    for more flexible handling of batch dimensions. We can pass batched or unbatched
    arguments to the joint distribution, and it will handle the batch semantics automatically.
    After we sample from this joint distribution, using `tf_model.sample()` in line
    51, we plot the observed data (*x*) distribution (2D):'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们使用了`JointDistributionCoroutineAutoBatched`，这是`JointDistributionCoroutine`的“自动批处理”版本。它根据输入参数的形状自动应用批处理语义，从而允许更灵活地处理批处理维度。我们可以将批处理或非批处理参数传递给联合分布，它会自动处理批处理语义。我们从这个联合分布中采样后，使用`tf_model.sample()`在第51行中，我们绘制观察数据（*x*）的分布（2D）：
- en: '![](../Images/764209f6ec9e21a31a5bcc15117316f0.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/764209f6ec9e21a31a5bcc15117316f0.png)'
- en: 'Fig. 1: Observed distribution of the data, given the joint distribution as
    defined in the code above.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：数据的观测分布，给定上述代码定义的联合分布。
- en: On a side note, as these data points are randomly sampled when you run the codes,
    you may not get exactly similar points.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，由于这些数据点是在你运行代码时随机采样的，你可能不会得到完全相似的点。
- en: We try to think that the posterior *p(W, Z|X)* can be approximated by a simpler
    distribution *q(W, Z)* parameterized by *θ*. In VI our aim is to minimize the
    KL divergence between *q(W, Z)* and *p(W, Z|X)*, which on the other hand from
    equation (2.8) would be to maximize the Evidence Lower Bound (ELBO).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试认为后验*p(W, Z|X)*可以用一个由*θ*参数化的更简单的分布*q(W, Z)*来近似。在VI中，我们的目标是最小化*q(W, Z)*和*p(W,
    Z|X)*之间的KL散度，这从方程（2.8）来看，则是最大化证据下界（ELBO）。
- en: 'The ELBO in this case:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下的ELBO：
- en: '![](../Images/b9779dcd89a1328fc44b5b29ac1119b7.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b9779dcd89a1328fc44b5b29ac1119b7.png)'
- en: ELBO that we are trying to minimize via VI.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过VI尝试最小化的ELBO。
- en: To minimize the ELBO we will first define the surrogate distribution similar
    to the way we defined the joint distribution, and use the `tfp.vi` method to fit
    a surrogate posterior to a target (unnormalized) log density.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最小化ELBO，我们将首先定义类似于定义联合分布的方式的替代分布，并使用`tfp.vi`方法将替代后验拟合到目标（未归一化）对数密度。
- en: 'After we obtain the surrogate distribution, we can then use it to sample new
    data points resulting in the plot below:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在获得替代分布后，我们可以使用它来采样新的数据点，结果如下图所示：
- en: '![](../Images/3e0343be459e7e4720978489e49e3c77.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e0343be459e7e4720978489e49e3c77.png)'
- en: 'Fig. 2: The sampled distribution resembles the original distribution quite
    well.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：采样的分布与原始分布非常相似。
- en: 'Conclusions:'
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论：
- en: We have gone through the mathematics behind the PPCA and used TensorFlow to
    test our understanding with a simple toy data-set. The possibility of generating
    new samples using the surrogate posterior gives us the power to impute missing
    values in data, generate new samples etc., which is impossible with standard PCA.
    Many real-world datasets exhibit intricate relationships, noise corruption, and
    incomplete observations, rendering classical PCA less effective. By accounting
    for uncertainty, handling missing data, and offering a probabilistic modeling
    framework, PPCA opens up new avenues for data analysis, pattern recognition, and
    machine learning. Next time if you’re planning to use PCA for your dataset, but
    you are aware that observations could be noisy and there are missing data, why
    not try PPCA?
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探讨了PPCA背后的数学原理，并使用TensorFlow通过一个简单的玩具数据集测试了我们的理解。通过使用替代后验生成新样本的可能性，使我们能够在数据中填补缺失值、生成新样本等，这在标准PCA中是不可能的。许多实际数据集展示了复杂的关系、噪声污染和不完整的观测，这使得经典PCA的效果不佳。通过考虑不确定性、处理缺失数据，并提供概率建模框架，PPCA为数据分析、模式识别和机器学习开辟了新的途径。如果下次你打算使用PCA来处理你的数据集，但你知道观测可能会有噪声且存在缺失数据，为什么不尝试PPCA呢？
- en: Probabilistic PCA is also closely related to Factor Analysis (FA) which is a
    linear-Gaussian latent variable model. The main difference between FA and PPCA
    is that in Eq. 2.2 when we describe the noise distribution, the covariance is
    assumed to be isotropic in PPCA, whereas in FA it’s a diagonal matrix. I’ll leave
    some references below for more explorations that you can do inspired by this post.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 概率PCA还与因子分析（FA）密切相关，后者是一种线性高斯潜变量模型。FA与PPCA之间的主要区别在于，在方程2.2中描述噪声分布时，PPCA假设协方差是各向同性的，而FA中它是对角矩阵。我会在下面留下更多参考文献，以便你可以根据这篇文章进行进一步探索。
- en: 'References:'
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献：
- en: '[1] ‘Probabilistic Principal Component Analysis’; M. Tipping, C. Bishop; J.R.
    Statist. Soc. B (1999).'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[1]《概率主成分分析》；M. Tipping, C. Bishop；J.R. Statist. Soc. B (1999)。'
- en: '[2] ‘Pattern Recognition and Machine Learning’; C. Bishop; Chapter 12: Continuous
    Latent Variables.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[2]《模式识别与机器学习》；C. Bishop；第12章：连续潜变量。'
- en: '[3] ‘Continuous Latent Variable Models’; University of Toronto; [Lecture Notes](https://www.cs.toronto.edu/~hinton/csc2515/notes/lec7middle.pdf).'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[3]《连续潜变量模型》；多伦多大学；[讲义](https://www.cs.toronto.edu/~hinton/csc2515/notes/lec7middle.pdf)。'
- en: '[4] [Probabilistic PCA: TensorFlow Probability Example](https://www.tensorflow.org/probability/examples/Probabilistic_PCA);'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] [概率PCA：TensorFlow概率示例](https://www.tensorflow.org/probability/examples/Probabilistic_PCA)；'
- en: '[5] Link to my notebook. [GitHub](https://github.com/suvoooo/Learn-TensorFlow/blob/master/TF-Proba/PCA_ProbabilisticApproach.ipynb)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] 我的笔记本链接。[GitHub](https://github.com/suvoooo/Learn-TensorFlow/blob/master/TF-Proba/PCA_ProbabilisticApproach.ipynb)'
