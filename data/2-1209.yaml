- en: How to Leverage Pre-Trained Transformer Models for Custom Text Categorisation?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何利用预训练的 Transformer 模型进行自定义文本分类？
- en: 原文：[https://towardsdatascience.com/how-to-leverage-pre-trained-transformer-models-for-custom-text-categorisation-3757c517bd65](https://towardsdatascience.com/how-to-leverage-pre-trained-transformer-models-for-custom-text-categorisation-3757c517bd65)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-leverage-pre-trained-transformer-models-for-custom-text-categorisation-3757c517bd65](https://towardsdatascience.com/how-to-leverage-pre-trained-transformer-models-for-custom-text-categorisation-3757c517bd65)
- en: So, you have some custom text dataset that you wish to categorise, but wondering
    how? Well, let me show you how, using pre-trained state of the art language models.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 所以，你有一些自定义的文本数据集，想要进行分类，但不知道如何做？好吧，让我展示一下，如何利用预训练的最先进语言模型来实现。
- en: '[](https://saedhussain.medium.com/?source=post_page-----3757c517bd65--------------------------------)[![Saed
    Hussain](../Images/cb8d313c1c1a15fd1632979dc3c080a7.png)](https://saedhussain.medium.com/?source=post_page-----3757c517bd65--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3757c517bd65--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3757c517bd65--------------------------------)
    [Saed Hussain](https://saedhussain.medium.com/?source=post_page-----3757c517bd65--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://saedhussain.medium.com/?source=post_page-----3757c517bd65--------------------------------)[![Saed
    Hussain](../Images/cb8d313c1c1a15fd1632979dc3c080a7.png)](https://saedhussain.medium.com/?source=post_page-----3757c517bd65--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3757c517bd65--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3757c517bd65--------------------------------)
    [Saed Hussain](https://saedhussain.medium.com/?source=post_page-----3757c517bd65--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3757c517bd65--------------------------------)
    ·11 min read·Mar 31, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----3757c517bd65--------------------------------)
    ·阅读时间 11 分钟·2023 年 3 月 31 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/948131b577b7ca3d0260d3ee6b5e9654.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/948131b577b7ca3d0260d3ee6b5e9654.png)'
- en: Photo by [Meagan Carsience](https://unsplash.com/@mcarsience_photography?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于[Meagan Carsience](https://unsplash.com/@mcarsience_photography?utm_source=medium&utm_medium=referral)在[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Ok let’s get straight to the point! You have some custom data and now you want
    to categorise it into your custom classes. In this article, I will show you how
    you can use 2 approaches to achieve this objective. Both of them utilise pre-trained
    state of the art transformed based models.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，让我们直奔主题！你有一些自定义数据，现在想将其分类到自定义类别中。在本文中，我将展示你如何使用两种方法来实现这一目标。这两种方法都利用了最先进的基于
    Transformer 的预训练模型。
- en: Please note that the goal of this article is to share with you the approaches
    and how to use them. This is in not a complete data science tutorial with best
    practices. Unfortunately that is outside the scope of this article.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，本文的目标是与您分享方法及其使用方式。这不是一个包含最佳实践的完整数据科学教程。不幸的是，这超出了本文的范围。
- en: All the code from this article can be found in this [GitHub repo](https://github.com/saedhussain/medium/tree/main/text_category/notebooks).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中的所有代码可以在这个[GitHub 仓库](https://github.com/saedhussain/medium/tree/main/text_category/notebooks)中找到。
- en: '1: Zero Shot Classification'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1: 零样本分类'
- en: Overview
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: Zero-shot classification is a technique that allows you to classify text into
    categories without training a specific model for that task. Instead, it uses pre-trained
    models that have been trained on a large amount of data to perform this classification.
    The models are typically trained on a variety of tasks, including language modelling,
    text completion, and text entailment, among others.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本分类是一种技术，它允许你在没有为特定任务训练专门模型的情况下对文本进行分类。相反，它使用已经在大量数据上训练过的预训练模型来执行分类。模型通常会在包括语言建模、文本补全和文本蕴涵等各种任务上进行训练。
- en: '![](../Images/af18cccb1f07546070ca552ef5d34bde.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/af18cccb1f07546070ca552ef5d34bde.png)'
- en: 'Zero-shot text classification using pre-trained LLMs (source: author)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练的 LLM 进行零样本文本分类（来源：作者）
- en: To perform zero-shot classification, you simply need to provide the pre-trained
    model with some text and a list of possible categories.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行零样本分类，你只需向预训练模型提供一些文本和一个可能的类别列表。
- en: The model will then use its understanding of language and its pre-existing knowledge
    to classify the text into one of the provided categories. This approach is particularly
    useful when you have limited data available for a specific classification task,
    as it allows you to leverage the pre-existing knowledge of the model.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 模型将利用其对语言的理解和已有知识将文本分类到提供的类别之一。这种方法特别有用，当你为特定分类任务拥有的数据有限时，因为它允许你利用模型的已有知识。
- en: Since it does the classification without any training on that particular task,
    it’s know as zero-shot.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它在特定任务上没有任何训练，因此被称为零样本分类。
- en: Implementation
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现
- en: All we need to implement this is to install the [hugging face transformers library](https://github.com/huggingface/transformers)
    using `pip install transformers` . We will use the pre-trained Facebook BART (Bidirectional
    and Auto-Regressive Transformers) model for this task.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的就是使用`pip install transformers`安装[hugging face transformers库](https://github.com/huggingface/transformers)。我们将使用预训练的Facebook
    BART（双向和自回归变换器）模型来完成这项任务。
- en: '***Side Note****: on first use, it will take some time to download the model.*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '***附注***：首次使用时，下载模型可能需要一些时间。'
- en: 'The output is a dictionary with 3 keys:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是一个包含3个键的字典：
- en: '**sequence**: input text that was classified by the pipeline'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**序列**：由管道分类的输入文本'
- en: '**labels**: list of candidate (category) labels provided to the pipeline, ordered
    based on their probability scores.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签**：提供给管道的候选（类别）标签的列表，根据它们的概率分数进行排序。'
- en: '**scores**: probabilities scores assigned to each candidate label based on
    the model’s prediction of how likely the input text belongs to that label.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分数**：基于模型对输入文本属于该标签的可能性的预测，分配给每个候选标签的概率分数。'
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As you can see, without any training, the model has correctly classified the
    given text into the “groceries” category. Because the model was trained on a large
    corpus of text in a given language, it can understand that language and draw inference.
    it understood the text and identified a suitable category from the list of candidate
    labels.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在没有任何训练的情况下，模型已正确地将给定文本分类到“杂货”类别。因为模型是在特定语言的大型语料库上训练的，它能够理解该语言并进行推断。它理解了文本并从候选标签列表中识别了合适的类别。
- en: Simply put, it’s brilliant!! 😊
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，真是太棒了！！😊
- en: The larger the model is, the better it is at zero-shot classification tasks.
    For more info, have a look at the [zero-shot classification page](https://huggingface.co/tasks/zero-shot-classification)
    on hugging face.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 模型越大，零样本分类任务的表现越好。有关更多信息，请查看hugging face上的[零样本分类页面](https://huggingface.co/tasks/zero-shot-classification)。
- en: ⚡️ Checkout this [notebook](https://github.com/saedhussain/medium/blob/48db9652f83cbb83b547a0a55ff5bfb8355e0d26/text_category/notebooks/zero_shot_classification.ipynb)
    for more examples.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ⚡️ 查看这个[笔记本](https://github.com/saedhussain/medium/blob/48db9652f83cbb83b547a0a55ff5bfb8355e0d26/text_category/notebooks/zero_shot_classification.ipynb)以获取更多示例。
- en: When to use it?
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么时候使用它？
- en: Given its extraordinary performance without any training, I highly recommend
    trying this first if you know the descriptions of the categories. It leverages
    the state of the art pre-trained models and gives excellent results without any
    training.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于其在没有任何训练的情况下表现卓越，如果你了解类别的描述，我强烈建议首先尝试这个方法。它利用了最先进的预训练模型，并在没有任何训练的情况下提供了卓越的结果。
- en: 'Here is a non exhaustive list of when this can be your go to approach:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些非详尽的情况，当这些情况适合你的方法时：
- en: When you have limited labeled training data
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当你拥有有限的标注训练数据时
- en: When you need to quickly prototype a solution
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当你需要快速原型一个解决方案时
- en: When you need to classify new or rare categories
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当你需要对新的或稀有的类别进行分类时
- en: When you need to classify instances into multiple categories
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当你需要将实例分类到多个类别中时
- en: When you want to leverage pre-trained models to classify instances without additional
    training
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当你想利用预训练模型对实例进行分类而无需额外训练时
- en: When you want to classify instances into categories that are defined by natural
    language descriptions rather than predefined labels.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当你想将实例分类到由自然语言描述定义的类别中，而不是预定义标签时。
- en: What are the limitations?
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有哪些限制？
- en: 'Here are some potential drawbacks and limitations to consider when using the
    zero-shot classification approach:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是使用零样本分类方法时需要考虑的一些潜在缺点和限制：
- en: 'Limited training data augmentation: In zero-shot classification, there is limited
    scope for augmenting the training data to improve model performance, unlike in
    traditional supervised learning approaches.'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有限的训练数据扩增：在零-shot 分类中，扩增训练数据以提升模型性能的范围有限，这与传统监督学习方法不同。
- en: 'Limited control over model behaviour: Zero-shot classification relies on a
    pre-trained model, which means that you have limited control over its behaviour
    and the patterns it has learned. This can lead to unexpected results, especially
    if the model was not trained on data similar to your task.'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对模型行为的控制有限：零-shot 分类依赖于预训练模型，这意味着你对其行为和学习到的模式的控制有限。这可能导致意外结果，特别是当模型没有在类似于你任务的数据上训练时。
- en: 'Limited customisation: Because zero-shot classification relies on a pre-trained
    model, there is limited scope for customisation or fine-tuning to the specific
    nuances of your task. This can limit the accuracy and performance of the model,
    especially if the task involves complex or domain-specific language.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有限的自定义：由于零-shot 分类依赖于预训练模型，因此自定义或微调以适应特定任务的余地有限。这可能限制模型的准确性和性能，尤其是当任务涉及复杂或领域特定的语言时。
- en: '🚀 Pro Tip: Always Clean Your Text!!'
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 🚀 专业提示：始终清理你的文本！！
- en: Although transformer-based LLMs are significantly better at handling noisy text
    compared to other models, it is still highly recommended to clean the text before
    feeding it into the model.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于变换器的LLM在处理嘈杂文本方面显著优于其他模型，但在将文本输入模型之前，仍然强烈建议清理文本。
- en: Not only is it a good data science practice, but it also makes a huge difference.
    The embeddings generated with noisy text data might have lower similarity scores
    to the correct category when compared against a clean text input. This undoubtably
    will reduce the category classification scores.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这不仅是良好的数据科学实践，还会产生巨大差异。与干净文本输入相比，生成的嵌入在嘈杂文本数据中的相似性评分可能较低。这无疑会降低类别分类评分。
- en: For example, below is a comparison of categorising this text — “*Tesco Semi
    Skimmed Milk 1.13L/2 Pints …… £1.30*”, before and after removing any special characters
    and numbers. Why not try it out on the [Hugging Face zero-shot classification](https://huggingface.co/tasks/zero-shot-classification)
    page?
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，下面是对这段文本——“*Tesco Semi Skimmed Milk 1.13L/2 Pints …… £1.30*”——在去除特殊字符和数字前后的分类对比。为什么不在[Hugging
    Face zero-shot classification](https://huggingface.co/tasks/zero-shot-classification)页面上试试呢？
- en: '![](../Images/233b993b53778d7ee285df8bb794c1b0.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/233b993b53778d7ee285df8bb794c1b0.png)'
- en: Text categorisation probability comparison, using zero-shot method before and
    after cleaning text. (source:author)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用零-shot 方法在清理文本前后的文本分类概率比较。（来源：作者）
- en: ☕️ Let’s take a break before we go any further…
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ☕️ 让我们在继续之前休息一下……
- en: '![](../Images/d8f8e20370669c8ee9b38864a0a45b0c.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d8f8e20370669c8ee9b38864a0a45b0c.png)'
- en: Photo by [Victoria Tronina](https://unsplash.com/@victoriaorvicky?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Victoria Tronina](https://unsplash.com/@victoriaorvicky?utm_source=medium&utm_medium=referral)拍摄，来自[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: ☕️ 5 minutes later….alright, let’s do this 🏃
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ☕️ 5分钟后……好吧，让我们开始吧 🏃
- en: '2: Transfer Learning (pre-trained model + classification model)'
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2: 迁移学习（预训练模型 + 分类模型）'
- en: Overview
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: This is the next level up from the previous approach. As briefly mentioned earlier,
    the zero-shot classification does not allow for customisation to a particular
    task. This is where this approach comes in as a **long term solution**.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种比前一种方法更高级的方式。如前所述，零-shot 分类不允许针对特定任务进行自定义。这就是这种方法作为**长期解决方案**的所在。
- en: In this approach, you use a pre-trained transformer model to create text embeddings,
    and then train a classification model to categorise these embeddings into their
    respective categories.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，你使用预训练的变换器模型创建文本嵌入，然后训练一个分类模型将这些嵌入分类到各自的类别中。
- en: '![](../Images/10998d7b476a0bcafa476ba46f9777c7.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10998d7b476a0bcafa476ba46f9777c7.png)'
- en: 'Solution overview: Converting item description text to text embeddings using
    pre-trained models and using a classification model to classify the text into
    categories. (source:author)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案概述：使用预训练模型将项目描述文本转换为文本嵌入，并使用分类模型将文本分类到不同类别中。（来源：作者）
- en: To demo this example, we will be using this [e-commerce products dataset](https://www.kaggle.com/datasets/saurabhshahane/ecommerce-text-classification)
    from Kaggle.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示这个例子，我们将使用来自Kaggle的[e-commerce产品数据集](https://www.kaggle.com/datasets/saurabhshahane/ecommerce-text-classification)。
- en: This dataset contains 4 product categories and text description of the product.
    The objective is to build a model that can classify product description text into
    these 4 categories.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含4个产品类别和产品的文本描述。目标是构建一个模型，可以将产品描述文本分类到这4个类别中。
- en: '![](../Images/3683e4c4cac6df55adb8d729f28f0595.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3683e4c4cac6df55adb8d729f28f0595.png)'
- en: 'E-commerce product description and category data from [Kaggle](https://www.kaggle.com/datasets/saurabhshahane/ecommerce-text-classification).
    (source: author)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[Kaggle](https://www.kaggle.com/datasets/saurabhshahane/ecommerce-text-classification)上的电子商务产品描述和类别数据。（来源：作者）'
- en: '![](../Images/8cd8a80a48ad90ec2d0b5f3d46fdd287.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8cd8a80a48ad90ec2d0b5f3d46fdd287.png)'
- en: 'There are 4 categories in this [dataset](https://www.kaggle.com/datasets/saurabhshahane/ecommerce-text-classification).
    (source: author)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这个[数据集](https://www.kaggle.com/datasets/saurabhshahane/ecommerce-text-classification)中有4个类别。（来源：作者）
- en: Implementation
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现
- en: 'The implementation of this approach can be broken down into 4 steps:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法的实现可以分为4个步骤：
- en: Clean the text before feeding it to the model.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在将文本输入模型之前，请先清理文本。
- en: Generate text embeddings using a pre-trained large language model (LLM).
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用预训练的大型语言模型（LLM）生成文本嵌入。
- en: Train a classification model on the custom classes using the embeddings.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用嵌入训练一个自定义类别的分类模型。
- en: Run predictions using the trained model and preprocessing pipeline.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练好的模型和预处理管道进行预测。
- en: By the end of this, you will have a classification model that leverages the
    high-quality embeddings learned by the LLM training on an extraordinary amount
    of data.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，你将拥有一个分类模型，该模型利用了LLM在大量数据上训练学到的高质量嵌入。
- en: These embeddings are the key factor in the success of the LLMs, as they capture
    rich representations of language that can effectively capture the nuances of meaning
    and context in a text.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这些嵌入是LLM成功的关键因素，因为它们捕捉了语言的丰富表示，能够有效捕捉文本中的意义和上下文的细微差别。
- en: ⚡️ Note that the full code for this can be found at the following [notebook](https://github.com/saedhussain/medium/blob/53453650d6b5281686796cdbf8e4d9592b376c85/text_category/notebooks/ecommerce_text_categorisation.ipynb).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ⚡️ 请注意，完整的代码可以在以下[笔记本](https://github.com/saedhussain/medium/blob/53453650d6b5281686796cdbf8e4d9592b376c85/text_category/notebooks/ecommerce_text_categorisation.ipynb)中找到。
- en: 'Step 1: Clean the Text'
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤1：清理文本
- en: As mentioned in the previous approach, it’s always a good practice to clean
    the text before using it for a model. Below the code used for cleaning the text
    in this article.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述方法所提，使用模型之前清理文本始终是一个好习惯。以下是本文中用于清理文本的`代码`。
- en: '***Side Note****: Cleaning is highly task-specific. In this example, I have
    only implemented some basic cleaning. As a best practice, it is always a good
    idea to first understand the data and then implement some task-specific cleaning.
    For example, you may need to decide whether to keep or remove numbers, depending
    on the requirements of your specific task. Additionally, reducing the text by
    removing unnecessary words/symbols can decrease processing time in downstream
    processes. Happy cleaning!* 😄'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '***附注****：清理是高度任务特定的。在这个例子中，我只实现了一些基本的清理。作为最佳实践，最好先了解数据，然后实施一些任务特定的清理。例如，你可能需要根据具体任务的要求决定是否保留或删除数字。此外，通过删除不必要的词语/符号来减少文本量，可以减少后续处理的时间。祝清理愉快!*
    😄'
- en: '[PRE1]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Step 2: Generate Text Embeddings'
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤2：生成文本嵌入
- en: We are going to use the [SentenceTransformer](https://huggingface.co/sentence-transformers)
    library from Hugging Face to create text embeddings. This library contains transformer-based
    models pre-trained to generate fixed-length vector representations of textual
    data, such as paragraphs or sentences.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用来自Hugging Face的[SentenceTransformer](https://huggingface.co/sentence-transformers)库来创建文本嵌入。该库包含基于变换器的预训练模型，用于生成固定长度的文本数据向量表示，如段落或句子。
- en: Specifically, we’ll use the *“paraphrase-mpnet-base-v2”* pre-trained model from
    the library for this example, which produces fixed-length vectors of length 768.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将使用库中的*“paraphrase-mpnet-base-v2”*预训练模型，该模型生成长度为768的固定长度向量。
- en: '***Side Note****: in order to save time, I have reduced the sample size of
    both the training and test dataset. Generating embeddings can take a while, especially
    on a local machine. This can effect the performance of the model.*'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '***附注***：为了节省时间，我减少了训练集和测试集的样本量。生成嵌入表示可能需要一些时间，尤其是在本地机器上。这可能会影响模型的性能。'
- en: '[PRE2]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Step 3: Train the Classification Model'
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤3：训练分类模型
- en: Now that we have the embeddings, we are ready to train a classification model
    to take in these embeddings and classify it into one of the 4 product categories.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了嵌入表示，我们准备训练一个分类模型，将这些嵌入表示分类到4个产品类别中的其中一个。
- en: In this demo, I am using the XGBoost model, but feel free to use what ever you
    fancy!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个演示中，我使用了XGBoost模型，但你可以随意使用你喜欢的模型！
- en: '***Side Note****: in order to save time during training, limited hyper-parameters
    were used in the grid search. Also, to keep things simple, we are using the accuracy
    as a measure of performance. Make sure to use the appropriate metrics to measure
    the performance for your categorisation task.*'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '***附注***：为了在训练过程中节省时间，网格搜索中使用了有限的超参数。此外，为了简化，我们使用准确率作为性能衡量标准。确保使用适当的指标来衡量你的分类任务的性能。'
- en: '[PRE3]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Step 4: Run Predictions'
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤4：运行预测
- en: Finally, package the text processing steps, load the trained classifier model
    and you are good to run predictions on your test dataset.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，打包文本处理步骤，加载训练好的分类器模型，你就可以在测试数据集上运行预测了。
- en: '[PRE4]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: And there you have it folks!! 😃
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是各位的内容！！ 😃
- en: Without much task specific text cleaning, we used the embeddings from the pre-trained
    transformer model and build a classifier to categorise it into 4 categories, with
    91% accuracy.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有进行太多特定任务的文本清理的情况下，我们使用了预训练变换器模型生成的嵌入表示，并构建了一个分类器，将其分类到4个类别中，准确率为91%。
- en: '![](../Images/bfb928bd622c8cf831b47c25500291b7.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bfb928bd622c8cf831b47c25500291b7.png)'
- en: Predictions (preds) on the test dataset. (source:author)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 测试数据集上的预测（preds）。 (来源:作者)
- en: '![](../Images/417c5d0a2b319d3987138a9bcda81370.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/417c5d0a2b319d3987138a9bcda81370.png)'
- en: Cases where the predictions were wrong on the test dataset. (source:author)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试数据集上预测错误的情况。 (来源:作者)
- en: The cases where the model failed to make correct predictions can be improved
    by using more data during training and doing task specific text cleaning. For
    example, leave dimension numbers in, as it helps differentiate household products
    from the rest.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 模型未能正确预测的情况可以通过使用更多的数据进行训练和执行特定任务的文本清理来改进。例如，保留维度编号，因为它有助于区分家庭产品与其他产品。
- en: ⚡️ Another reminder that the full code is available in this [notebook](https://github.com/saedhussain/medium/blob/53453650d6b5281686796cdbf8e4d9592b376c85/text_category/notebooks/ecommerce_text_categorisation.ipynb).
    🤗
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ⚡️ 再次提醒，完整代码可在这个[笔记本](https://github.com/saedhussain/medium/blob/53453650d6b5281686796cdbf8e4d9592b376c85/text_category/notebooks/ecommerce_text_categorisation.ipynb)中找到。
    🤗
- en: Final Thoughts
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最终想法
- en: In this article, we have explored two different approaches for harnessing the
    power of pre-trained LLMs to customise text categorisation. We can achieve impressive
    results with minimal effort thanks to the high-quality embeddings they generate.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们探讨了利用预训练的大型语言模型（LLMs）定制文本分类的两种不同方法。由于它们生成的高质量嵌入，我们可以以最小的努力取得令人印象深刻的结果。
- en: As these LLMs have been trained on large corpora of data, they possess a deep
    understanding of the language and can effectively capture the nuances of meaning
    and context in texts written in that language.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些LLMs已经在大量数据上进行过训练，它们对语言有深入的理解，并且能够有效捕捉文本中意义和语境的细微差别。
- en: Now that you know these two approaches, give it a go for your text categorisation
    task! 😃
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你知道了这两种方法，尝试一下你的文本分类任务吧！ 😃
- en: 👉 **Don’t forget to follow for more articles like this.** 🤗
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 👉 **别忘了关注以获取更多类似的文章。** 🤗
- en: 🚀 Hope you found this article helpful. Consider joining Medium using [my link](https://medium.com/@saedhussain/membership)
    for more great content from me and other amazing authors on this platform!
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 🚀 希望你觉得这篇文章对你有帮助。考虑使用[我的链接](https://medium.com/@saedhussain/membership)加入Medium，获取我和其他平台上优秀作者的更多精彩内容！
- en: '*Other articles by me:*'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '*我的其他文章：*'
- en: '[](/how-to-extract-and-convert-tables-from-pdf-files-to-pandas-dataframe-cb2e4c596fa8?source=post_page-----3757c517bd65--------------------------------)
    [## How to Extract and Convert Tables From PDF Files to Pandas Dataframe?'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/how-to-extract-and-convert-tables-from-pdf-files-to-pandas-dataframe-cb2e4c596fa8?source=post_page-----3757c517bd65--------------------------------)
    [## 如何从PDF文件中提取并转换表格到Pandas数据框？'
- en: So you have some pdf files with tables in them and want to read them into a
    pandas data frame. Let me show you how.
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 所以你有一些包含表格的 PDF 文件，并且想将它们读取到一个 pandas 数据框中。让我来展示给你看。
- en: towardsdatascience.com](/how-to-extract-and-convert-tables-from-pdf-files-to-pandas-dataframe-cb2e4c596fa8?source=post_page-----3757c517bd65--------------------------------)
    [](/how-to-schedule-a-serverless-google-cloud-function-to-run-periodically-249acf3a652e?source=post_page-----3757c517bd65--------------------------------)
    [## How to Schedule a Serverless Google Cloud Function to Run Periodically
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[如何从 PDF 文件中提取并转换表格到 pandas 数据框](https://towardsdatascience.com/how-to-extract-and-convert-tables-from-pdf-files-to-pandas-dataframe-cb2e4c596fa8?source=post_page-----3757c517bd65--------------------------------)
    [](/how-to-schedule-a-serverless-google-cloud-function-to-run-periodically-249acf3a652e?source=post_page-----3757c517bd65--------------------------------)
    [## 如何调度无服务器 Google Cloud 函数以定期运行'
- en: Do you have some code that needs to be run regularly?
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你是否有一些代码需要定期运行？
- en: towardsdatascience.com](/how-to-schedule-a-serverless-google-cloud-function-to-run-periodically-249acf3a652e?source=post_page-----3757c517bd65--------------------------------)
    [](/how-to-develop-and-test-your-google-cloud-function-locally-96a970da456f?source=post_page-----3757c517bd65--------------------------------)
    [## How to Develop and Test Your Google Cloud Function Locally
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[如何定期调度无服务器 Google Cloud 函数](https://towardsdatascience.com/how-to-schedule-a-serverless-google-cloud-function-to-run-periodically-249acf3a652e?source=post_page-----3757c517bd65--------------------------------)
    [](/how-to-develop-and-test-your-google-cloud-function-locally-96a970da456f?source=post_page-----3757c517bd65--------------------------------)
    [## 如何在本地开发和测试你的 Google Cloud 函数'
- en: So, you have written your serverless cloud function, but don’t want to waste
    time deploying it and hoping it works. Let…
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 因此，你已经编写了无服务器云函数，但不想浪费时间部署它并希望它能正常工作。让…
- en: towardsdatascience.com](/how-to-develop-and-test-your-google-cloud-function-locally-96a970da456f?source=post_page-----3757c517bd65--------------------------------)
    [](/machine-learning-model-as-a-serverless-endpoint-using-google-cloud-function-a5ad1080a59e?source=post_page-----3757c517bd65--------------------------------)
    [## Machine Learning Model as a Serverless Endpoint using Google Cloud Functions
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[如何在本地开发和测试你的 Google Cloud 函数](https://towardsdatascience.com/how-to-develop-and-test-your-google-cloud-function-locally-96a970da456f?source=post_page-----3757c517bd65--------------------------------)
    [](/machine-learning-model-as-a-serverless-endpoint-using-google-cloud-function-a5ad1080a59e?source=post_page-----3757c517bd65--------------------------------)
    [## 使用 Google Cloud Functions 将机器学习模型作为无服务器端点'
- en: So you have built a model and want to productionize it as a serverless solution
    on google cloud platform (GCP). Let me…
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 因此，你已经构建了一个模型，并且想将它作为无服务器解决方案在 Google Cloud Platform (GCP) 上投入生产。让我来…
- en: towardsdatascience.com](/machine-learning-model-as-a-serverless-endpoint-using-google-cloud-function-a5ad1080a59e?source=post_page-----3757c517bd65--------------------------------)
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[使用 Google Cloud Functions 将机器学习模型作为无服务器端点](https://towardsdatascience.com/machine-learning-model-as-a-serverless-endpoint-using-google-cloud-function-a5ad1080a59e?source=post_page-----3757c517bd65--------------------------------)'
