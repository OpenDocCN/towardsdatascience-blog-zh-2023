- en: The Magic of LLMs — Prompt Engineering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM的魔力——提示工程
- en: 原文：[https://towardsdatascience.com/the-magic-of-llms-prompt-engineering-9c3e46130131](https://towardsdatascience.com/the-magic-of-llms-prompt-engineering-9c3e46130131)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-magic-of-llms-prompt-engineering-9c3e46130131](https://towardsdatascience.com/the-magic-of-llms-prompt-engineering-9c3e46130131)
- en: Garbage in, garbage out has never been more true.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 垃圾进垃圾出，从未如此真实。
- en: '[](https://franklyai.medium.com/?source=post_page-----9c3e46130131--------------------------------)[![Frank
    Neugebauer](../Images/0da70d082d0f9c7ad8ccf574ed215df2.png)](https://franklyai.medium.com/?source=post_page-----9c3e46130131--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9c3e46130131--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9c3e46130131--------------------------------)
    [Frank Neugebauer](https://franklyai.medium.com/?source=post_page-----9c3e46130131--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://franklyai.medium.com/?source=post_page-----9c3e46130131--------------------------------)[![Frank
    Neugebauer](../Images/0da70d082d0f9c7ad8ccf574ed215df2.png)](https://franklyai.medium.com/?source=post_page-----9c3e46130131--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9c3e46130131--------------------------------)[![数据科学进展](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9c3e46130131--------------------------------)
    [Frank Neugebauer](https://franklyai.medium.com/?source=post_page-----9c3e46130131--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9c3e46130131--------------------------------)
    ·6 min read·Apr 8, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [数据科学进展](https://towardsdatascience.com/?source=post_page-----9c3e46130131--------------------------------)
    ·6分钟阅读·2023年4月8日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/8c28b5ab704518e11aef83823349fb09.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c28b5ab704518e11aef83823349fb09.png)'
- en: Photo by [Gary Chan](https://unsplash.com/es/@gary_at_unsplash?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Gary Chan](https://unsplash.com/es/@gary_at_unsplash?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Rise of the Machines
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器的崛起
- en: Large language models (LLMs) are able to conversationally provide an unprecedented
    level of ML-generated information when *asked the right question*. For example,
    LLMs can generate articles (no, an LLM didn’t generate this text), poetry, song
    lyrics, have a conversation with someone about a topic, interpret some types of
    documents / forms, and more. However, a seldom talked about problem with LLMs
    is that it’s not always easy to know the **right question** and without that critical
    part, LLMs can be overly accurate (i.e., too much information), inaccurate, or
    far too variable for many commercial applications.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）能够在*提出正确的问题*时提供前所未有的机器学习生成的信息。例如，LLMs可以生成文章（不，LLM并没有生成这段文字）、诗歌、歌词、与人讨论某个话题、解释某些类型的文档/表单等等。然而，LLMs一个很少被讨论的问题是，知道**正确的问题**并不总是容易的，而没有这个关键部分，LLMs可能会过于准确（即，信息过多）、不准确，或对许多商业应用过于多变。
- en: The art of asking the right question, or **prompt engineering**, is how we overcome
    this limitation (for at least the near term) and is *the magic behind the magic*
    of LLMs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 提问的艺术，即**提示工程**，是我们克服这一限制（至少在短期内）的方式，也是LLM的*神奇背后的魔法*。
- en: Shifting the Problem
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转变问题
- en: 'Without providing an overview of LLMs (written by an LLM), I want to jump right
    into an important characteristic of LLMs — they are non-deterministic; you won’t
    necessarily get the same answer to the same question every time. For example,
    if I ask Google Bard a simple question such as “What is 1 + 1”, multiple times
    in a row, here’s the unedited result:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有提供关于LLM（由LLM撰写）的概述的情况下，我想直接进入LLM的一个重要特性——它们是非确定性的；你不一定每次都能得到相同的答案。例如，如果我连续多次向Google
    Bard询问一个简单的问题“1 + 1是多少”，以下是未经编辑的结果：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Only the 2nd and 3rd prompts yielded the same answer but also notice that the
    answer isn’t simply “2,” it’s a variation of the prompt and the answer, and some
    fairly unpredictable formats of the answers. (NOTE: the output above is not what
    Bard looks like, it’s just the text.)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 只有第2和第3个提示产生了相同的答案，但还要注意答案不仅仅是“2”，而是提示和答案的变化以及一些相当不可预测的答案格式。（注意：上面的输出并不是Bard的样子，仅仅是文本。）
- en: Going from unstructured to unstructured doesn’t really solve the problem, it
    shifts the problem.
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 从无结构到无结构并不能真正解决问题，它只是转移了问题。
- en: In the 1+1 case, the shift is figuring out how to parse/transform the answer;
    working with a variable answer is a lot like throwing darts in a spinning room
    — the answer variation would be too great and we are almost back to the original
    problem.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在 1+1 的例子中，转变在于如何解析/转换答案；处理可变答案就像在旋转的房间里投掷飞镖——答案的变化太大，我们几乎回到了原来的问题。
- en: 'Instead, we focus on prompt engineering, or the art form of asking questions
    to get specific responses. Here’s what Bard thinks prompt engineering is:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们专注于提示工程，或询问问题以获得特定回应的艺术形式。这里是 Bard 对提示工程的定义：
- en: Prompt engineering is the process of designing and optimizing prompts to LLMs
    for a wide variety of applications and research topics. Prompts are short pieces
    of text that are used to guide the LM’s output. They can be used to specify the
    task that the LM is supposed to accomplish, provide additional information that
    the LM needs to complete the task, or simply make the task easier for the LM to
    understand.
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 提示工程是为大型语言模型设计和优化提示的过程，用于各种应用和研究主题。提示是用于引导语言模型输出的短文本片段。它们可以用来指定语言模型需要完成的任务，提供语言模型完成任务所需的额外信息，或仅仅使任务更易于语言模型理解。
- en: Thanks for that Bard, it’s a pretty good non-nuanced description, although “short”
    is relative in this context.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 谢谢你，Bard，这个描述相当不错，尽管在这个上下文中“短”是相对的。
- en: Engineering the Question — You Control That
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题工程 — 你控制这一点
- en: In the 1+1 example, the one thing I controlled is the question, or the prompt.
    For that reason, the focus is on engineering the question (or prompt) and doing
    so by adding specificity and context.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在 1+1 的例子中，我控制的唯一是问题或提示。因此，重点是设计问题（或提示），通过添加具体性和上下文来进行。
- en: Watch what happens when I add some specifics to the prompt for 1+1.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 看看当我为 1+1 的提示添加一些具体内容时会发生什么。
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Wait! That’s **SO MUCH WORSE**. I don’t want a narrative answer, I just want
    a number! And that’s why prompt engineering is an art form. Let me try again…
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 等等！那**糟糕得多**。我不想要叙述性答案，我只要一个数字！这就是为什么提示工程是一门艺术。让我再试一次…
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Look at that! How about another example…
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 看看这个！再来一个例子…
- en: In this example, I’m going to do two things — first, show how data can be included
    in a prompt and second, how a prompt can be engineered to get something specific.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我将做两件事——首先，展示如何将数据包含在提示中，其次，展示如何设计提示以获得具体的内容。
- en: I’m starting by using OCR to extract the following table as text.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我首先使用 OCR 将以下表格提取为文本。
- en: '![](../Images/c8f6523f875aeadd5cb01d5eb256e459.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c8f6523f875aeadd5cb01d5eb256e459.png)'
- en: Image by Author
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: My objective is to ask that data some questions about itself. This is a pretty
    interesting (and awesome, to be technical) use case, applicable in many (but not
    all) situations.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我的目标是向数据询问一些关于它自身的问题。这是一个相当有趣（技术上说，甚至是很棒的）用例，适用于许多（但不是全部）情况。
- en: I want to know the size of the file for 2022 Q2 (ignore the highlighted row
    in the table, I want the one below it). The answer is **48.79 MB**.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我想知道 2022 Q2 的文件大小（忽略表格中的高亮行，我想要下面的那一行）。答案是**48.79 MB**。
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: So the cool part here is that it got the answer correct (48.79 MB). The bad
    part is, as we saw with 1+1, the answer is unstructured. I need to modify the
    prompt with specificity.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这里有趣的部分是它得出了正确的答案（48.79 MB）。不好的部分是，如同我们看到的 1+1，答案是无结构的。我需要通过具体性来修改提示。
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: That’s better (even if not perfect), and really makes this useful as a zero-shot
    way to read certain types of data, maybe even form data. In a real-world situation,
    I’d continue to iterate with the prompt until I got a fully consistent deterministic-looking
    answer.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这更好（即使不是完美的），确实使其作为一种零样本方式来读取某些类型的数据（甚至可能是表单数据）非常有用。在现实世界中，我会继续迭代提示，直到得到一个完全一致且确定的答案。
- en: Beware the Ides of Hype
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 当心炒作的时机
- en: 'Reading a simple table does not mean LLMs can read all tables or forms, they
    really can’t. A good example is found in the insurance industry where very complex
    (and even illogical) forms are the way information is exchanged. LLMs can understand
    some form, but not the complex ones because:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读一个简单的表格并不意味着大型语言模型（LLMs）能够读取所有表格或表单，它们真的不能。一个好的例子是保险行业，其中非常复杂（甚至不合逻辑）的表单是信息交换的方式。LLMs
    可以理解某些表单，但不能理解复杂的表单，因为：
- en: LLMs don’t understand insurance (they can, but they don’t). The idea that an
    endorsement can contain an endorsement or a form, and the second use of endorsement
    is a noun and the use of “form” has nothing to do with an actual form, is really
    a nuance beyond general LLMs.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LLMs 不理解保险（它们可以，但它们不理解）。一个背书可以包含一个背书或一个表单，而背书的第二种用法是名词，而“表单”的使用与实际表单无关，这真的是超出了普通
    LLM 的细微差别。
- en: Complex forms contain context that is very difficult (maybe impossible?) for
    language models to understand because the form is used as a layout structure,
    not a content structure. (Tables sometimes have this same issue for the same reason
    and sometimes tables are the mechanism used for layout structure.)
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 复杂的表单包含了上下文，这对语言模型来说非常难以理解（也许是不可能的？），因为表单被用作布局结构，而不是内容结构。（表格有时也会出现类似的问题，原因相同，有时表格就是用于布局结构的机制。）
- en: To avoid getting pulled into the LLM hype spin, it’s important to get good at
    using the models to understand what they can and cannot do and how you can influence
    their behavior. Understand also that the models themselves continue to evolve
    so don’t write something off just because it doesn’t work right now.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免陷入 LLM 的炒作旋涡，重要的是要善于使用这些模型，了解它们能做什么，不能做什么，以及你如何能影响它们的行为。同时也要理解，模型本身在不断发展，因此不要因为它现在不起作用就否定它。
- en: Be Real(istic)
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 真实（现实）
- en: 'The hype cycle can cause unrealistic expectations. We’ve seen this repeated
    in history — e.g., services-oriented architecture (SOA) never resulted in dynamic
    applications, but we did get a pretty massive proliferation of APIs. LLMs could
    very well be like this because we seem to expect LLMs to solve all the world’s
    problems (my prediction: it won’t). If you’re realistic and curious about what
    these models can do, expectation and reality can align and I believe that alignment
    is where history is made.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 宣传周期可能会导致不切实际的期望。我们在历史上见过这样的重复——例如，面向服务的架构（SOA）从未导致动态应用程序，但我们确实看到了一种相当大规模的API扩展。LLMs
    也可能是这样，因为我们似乎期望 LLMs 解决所有世界上的问题（我的预测是：它不会）。如果你对这些模型能做什么保持现实和好奇，期望和现实可以对齐，而我相信这种对齐就是历史被创造的地方。
- en: Go Forth, Change the World
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 走出去，改变世界
- en: Large language models (LLMs) are powerful and usable for a variety of tasks,
    but they also have limitations, some of which can be overcome with prompt engineering.
    That engineering is an art form requiring significant experimentation, which really
    can’t be read in a book — go forth and do this yourself. Get good at prompt engineering
    and you’ll be able to maximize the value of LLMs and maybe even keep yourself
    out of the hype spin cycle.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）强大且适用于多种任务，但它们也有局限性，其中一些可以通过提示工程克服。那种工程是一种艺术形式，需要大量实验，这真的是无法通过书本获得的——去做吧。擅长提示工程，你将能够最大化
    LLM 的价值，甚至可能让自己远离炒作旋涡。
- en: The opinions expressed in this article are my own and do not reflect those of
    my employer.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中表达的观点是我个人的，并不代表我的雇主。
