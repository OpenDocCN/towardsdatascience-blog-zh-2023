- en: Now, why should we care about Recommendation Systems…? ft. A soft introduction
    to Thompson Sampling
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 那么，为什么我们应该关心推荐系统呢？特邀：对汤普森采样的简要介绍
- en: 原文：[https://towardsdatascience.com/now-why-should-we-care-about-recommendation-systems-ft-a-soft-introduction-to-thompson-sampling-b9483b43f262](https://towardsdatascience.com/now-why-should-we-care-about-recommendation-systems-ft-a-soft-introduction-to-thompson-sampling-b9483b43f262)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/now-why-should-we-care-about-recommendation-systems-ft-a-soft-introduction-to-thompson-sampling-b9483b43f262](https://towardsdatascience.com/now-why-should-we-care-about-recommendation-systems-ft-a-soft-introduction-to-thompson-sampling-b9483b43f262)
- en: An ongoing Recommendation System series
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正在进行的推荐系统系列
- en: '[](https://irenechang1510.medium.com/?source=post_page-----b9483b43f262--------------------------------)[![Irene
    Chang](../Images/02280890ed87239c75cbcbfa7c5d686c.png)](https://irenechang1510.medium.com/?source=post_page-----b9483b43f262--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b9483b43f262--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b9483b43f262--------------------------------)
    [Irene Chang](https://irenechang1510.medium.com/?source=post_page-----b9483b43f262--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://irenechang1510.medium.com/?source=post_page-----b9483b43f262--------------------------------)[![Irene
    Chang](../Images/02280890ed87239c75cbcbfa7c5d686c.png)](https://irenechang1510.medium.com/?source=post_page-----b9483b43f262--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b9483b43f262--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b9483b43f262--------------------------------)
    [Irene Chang](https://irenechang1510.medium.com/?source=post_page-----b9483b43f262--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b9483b43f262--------------------------------)
    ·12 min read·Nov 7, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b9483b43f262--------------------------------)
    ·阅读时间12分钟·2023年11月7日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a8dea9c6cebe66313b191c252f95b74c.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8dea9c6cebe66313b191c252f95b74c.png)'
- en: Photo by [Myke Simon](https://unsplash.com/@myke_simon?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Myke Simon](https://unsplash.com/@myke_simon?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: I caught myself today, once again for the 100...01-th day in a row, holding
    my late unopened dinner box as I'm browsing through Netflix for a show to watch
    while munching on my food. My feed is filled with a few too many Asian romance
    and American coming of age suggestions, probably based on a series or two from
    these categories that I watched, like, a month or two ago. “There's nothing to
    watch here…"–sighed me as I finished reading all the synopses, feeling confident
    that I could predict how the plot would unveil. I whipped out my alternative go-to
    entertainment option, Tiktok, while subconsciously thinking to myself that I will
    probably need to *Not interested* some videos and *Like*, *Save* others to…recommend
    the algorithm sending me some new stream of content today.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 今天我发现自己再次陷入了相同的情境，连续第100...01天，一边浏览Netflix寻找观看的节目，一边拿着晚餐盒子吃饭。我的推荐内容中充斥着过多的亚洲浪漫和美国成长题材的建议，可能是基于我一个月或两个月前观看过的这些类别的某几部剧。
    “这里没什么好看的…”–我一边阅读完所有简介一边叹了口气，自信地觉得自己能预测剧情的发展。我掏出了另一个备用的娱乐选项Tiktok，同时潜意识里想着我可能需要*不感兴趣*一些视频，而*喜欢*、*保存*其他视频，以便…推荐算法今天给我推送一些新的内容流。
- en: Recommendation Systems (RecSys) can be considered such an established algorithm,
    which has been deeply implanted into our every day lives, to an extent that, on
    the scale of 1 to Chat-GPT, it feels almost like an 80s trend both to the academic
    and non-academic world. Nonetheless, it is by no means a near perfect algorithm.
    The ethical, social, technical, and legal challenges that come with operating
    a recommendation application have never been at the forefront of research (as
    is the case of most other technology products…). Select group unfairness and privacy
    violation are examples of the popular concerns revolving around RecSys that are
    still not fully addressed by the companies who implemented it. Besides, there
    exists many more subtle issues that are rarely given enough deliberations, one
    of which is the loss of autonomy in an individual's decision making process. A
    “powerful” RecSys can undoubtedly nudge users in a particular direction [2], making
    them purchase, watch, think, believe in something that they wouldn't have done
    had they not been subject to such manipulation.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统（RecSys）可以被认为是一个已经非常成熟的算法，它已经深深植入我们的日常生活中，以至于在1到Chat-GPT的尺度上，它在学术界和非学术界都感觉像是80年代的趋势。然而，它绝不是一个近乎完美的算法。操作推荐应用程序所面临的伦理、社会、技术和法律挑战从未成为研究的前沿（就像大多数其他技术产品一样……）。例如，选择性群体的不公平和隐私侵犯是围绕RecSys的热门担忧，但这些问题仍未得到实施公司充分解决。此外，还存在许多更微妙的问题，通常没有得到足够的深思，其中之一是个体决策过程中的自主权丧失。一种“强大的”RecSys无疑可以将用户推向某个方向[2]，使他们购买、观看、思考、相信他们如果不受到这种操控本不会做的事情。
- en: Hence, I want to write up a series along my grad school journey as I start learning
    and dive deeper into RecSys, their strengths and shortcomings…all from scratch!
    And I figure it can start with thinking about movies and…Thompson Sampling!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我想在我的研究生学习旅程中写一系列文章，随着我开始学习并深入探讨RecSys的优缺点……一切从零开始！我觉得可以从思考电影和……汤普森采样开始！
- en: '**Thompson Sampling**'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**汤普森采样**'
- en: Thompson Sampling (TS) is one of the foundational algorithms not only in recommendation
    system literature, but also in reinforcement learning. It is arguably a better
    A/B testing in online learning settings, as clearly explained by Samuele Mazzanti
    in this amazing [article](/when-you-should-prefer-thompson-sampling-over-a-b-tests-5e789b480458).
    In simple terms, in movie recommendation context, TS tries to identify the best
    movie to recommend me that will maximize the chance that I will click to watch.
    It can do so effectively using relatively less data as it allows the parameters
    to be updated every time it observes me click or not click into a movie. Roughly
    speaking, this dynamic characteristic enables TS to take into account, on top
    of the my watch history and bookmarked series, real time factors such as the browsing,
    or the search results within the app I'm making at the moment to give me the most
    suitable suggestion. However, in this beginner friendly tutorial, let's just look
    into a simplified analysis below.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**汤普森采样** (TS) 是推荐系统文献和强化学习中的基础算法之一。正如Samuele Mazzanti在这篇精彩的[文章](/when-you-should-prefer-thompson-sampling-over-a-b-tests-5e789b480458)中清楚解释的那样，它可以被认为是在在线学习环境中更好的A/B测试。简单来说，在电影推荐的背景下，TS试图识别出最适合推荐给我的电影，以最大化我点击观看的机会。它可以通过相对较少的数据有效地做到这一点，因为它允许在每次观察到我是否点击电影时更新参数。粗略地说，这种动态特性使得TS能够在考虑我的观看历史和收藏的系列之外，还能实时考虑像浏览或在我当前正在使用的应用程序中的搜索结果等因素，以给我最合适的建议。然而，在这个适合初学者的教程中，我们仅仅看一下下面的简化分析。'
- en: Let's break it down even further!
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我们进一步分析吧！
- en: Consider these 3 movies, which, all amazing as they are, I, controversially
    enough, do have my own personal ranking for. Out of these 3 movies, say, there's
    one that I will 100% rewatch if it comes up on my feed, one that I’m highly unlikely
    going to rewatch (5%), and one that there's a 70% chance I will click watch whenever
    I see it. TS obviously does not have this information about me beforehand and
    its goal is to learn my behavior so as to, as common intuition goes, recommend
    me the movie that it knows I will for sure click watch.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这3部电影，尽管它们都很棒，但我却有自己个人的排名。假设这3部电影中，有一部是如果出现在我的推荐中我将100%重新观看，有一部是我极不可能重新观看的（5%），还有一部是我每次看到时有70%的机会会点击观看。显然，TS在事先并不了解这些关于我的信息，它的目标是学习我的行为，以便，如常识所说，推荐我它知道我一定会点击观看的电影。
- en: '![](../Images/2119d09ba44d1c295b556f676126e67e.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2119d09ba44d1c295b556f676126e67e.png)'
- en: Image by author
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'In the TS algorithm, the main workflow goes as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在TS算法中，主要的工作流程如下：
- en: '**Action**: TS suggests me a specific movie, among hundreds of others'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**行动**：TS建议我观看特定的电影，在数百部电影中选择'
- en: '**Outcome**: I decide that the movie sounds interesting enough to me and click
    to watch it, or I find it boring and click out of the page after reading the synopsis'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**结果**：我决定电影对我来说足够有趣并点击观看，或者我觉得无聊，在阅读了简介后点击退出页面。'
- en: '**Reward**: Can be thought of as the number of “points" TS scores if I click
    to watch a certain movie or TS misses if I don''t click. In basic movie or ad
    recommendation settings, we can treat reward to be an equivalent of outcome, so
    1 click on the movie = 1 point!'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**奖励**：可以被看作是TS在我点击观看某部电影时获得的“积分”数量，或者在我不点击时TS失去的积分。在基本的电影或广告推荐设置中，我们可以将奖励视为结果的等价物，因此1次点击电影=1积分！'
- en: '**Update** knowledge: TS registers my choice and update its belief as to which
    movie is my favorite.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**更新**知识：TS记录我的选择并更新其对我最喜欢电影的信念。'
- en: '**Repeat** step 1 (can be within my current browsing session, or at dinner
    time the next day), but now with some additional knowledge about my preferences.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**重复**第1步（可以在我当前的浏览会话中，或者第二天晚餐时间），但现在有了关于我偏好的额外知识。'
- en: Exploration/Exploitation
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索/利用
- en: This is the most used term in this literature, and is also what sets TS and
    other related algorithms apart. Step 5 above is where this logic kicks in. In
    TS world, everything has some degree of uncertainty to it. Me drinking latte three
    times and matcha five times in a week does not necessarily mean I love matcha
    more than latte, what if it's just that one week (and I'm actually drinking more
    latte than matcha on average per week)? For this reason, everything in TS is represented
    by some type of distribution, rather than just single numbers.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这是该文献中使用最多的术语，也是区分TS和其他相关算法的关键。上述第5步是这个逻辑开始发挥作用的地方。在TS的世界中，一切都存在某种程度的不确定性。我每周喝三次拿铁和五次抹茶并不一定意味着我比拿铁更喜欢抹茶，如果只是那一周（而我每周平均实际上喝的拿铁比抹茶多）呢？因此，TS中的一切都由某种类型的分布表示，而不仅仅是单个数字。
- en: '![](../Images/df7f1bafb93b4aac7b9a48a9e24ec6e0.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/df7f1bafb93b4aac7b9a48a9e24ec6e0.png)'
- en: '**Figure 1** In a particular week, I drink 5 cups of matcha and 3 cups of latte
    (left), but on average I drink more latte than matcha in a week (right) — Image
    by Author'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**图1** 在某一周，我喝了5杯抹茶和3杯拿铁（左），但平均每周我喝的拿铁比抹茶多（右）——作者提供的图片'
- en: Starting out, TS obviously has a lot of uncertainty around my preference for
    the movies, so its priority is to ***explore*** this by giving me many different
    movie suggestions in order to observe my response to the suggestions. After some
    few clicks and skips, TS can sort of figure out the movies that I tend to click
    and the movies that yield no benefits, and hence it has gained more confidence
    in what movie to give out to me next time. This is when TS starts to ***exploit***
    the highly rewarding options, where it gives me the movie I click watch often,
    but still leaves some room for more exploration. The confidence builds up as more
    observations come in, which, in simple cases, will reach the point where the exploration
    work is now very minimal since TS already has a lot of confidence to exploit the
    recommendation that gives a lot of rewards.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，TS显然对我对电影的偏好有很多不确定性，因此它的优先任务是***探索***这一点，通过给我提供许多不同的电影建议来观察我的反应。在经过几次点击和跳过后，TS可以大致了解我倾向于点击的电影和没有效益的电影，从而对下一次给我推荐的电影有了更多的信心。这时，TS开始***利用***高回报的选项，它会给我推荐我经常点击的电影，但仍然留有一些探索的空间。随着更多观察的积累，信心不断建立，简单情况下，探索的工作将变得非常少，因为TS已经对能够带来大量奖励的推荐有了很大的信心。
- en: Exploration vs Exploitation are thus often referred to as the **tradeoff** or
    the dilemma because too much exploration (i.e little elimination of low value
    choices despite already gaining enough evidence to know that such choices are
    not optimal) and you incur a lot of loss, too much exploitation (i.e eliminate
    too many options too quickly) and you re likely to falsely eliminate the true
    optimal action.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 探索与利用通常被称为**权衡**或困境，因为过多的探索（即使在获得足够证据后仍然没有排除低价值选项）会导致大量损失，而过多的利用（即过快地排除太多选项）可能会错误地排除真正的最佳行动。
- en: 'The distributions: Beta-Bernoulli'
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布：Beta-Bernoulli
- en: As in the matcha-latte graph above, TS works with different kinds of distributions
    to understand our preference for different options. In the most basic cases of
    movies (and ads as well), we often use the Beta-Bernoulli combo.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如上面的抹茶拿铁图所示，TS 使用不同类型的分布来理解我们对不同选项的偏好。在最基本的电影（和广告）情况下，我们通常使用 Beta-Bernoulli
    组合。
- en: '[Bernoulli distribution](/understanding-bernoulli-and-binomial-distributions-a1eef4e0da8f#:~:text=The%20Bernoulli%20distribution%20is%20the,probability%20(1%2Dp).)
    is a discrete distribution in which there are only two possible outcomes: 1 and
    0\. Bernoulli distribution consists of only one parameters, which indicates the
    probability of some variable, say Y, being 1\. So, if we say Y~ Bern(p), and for
    instance, p = 0.7, that means Y has 0.7 chance of having the value of 1, and 1–p
    = 1–0.7 = 0.3 chance of being 0\. Thus, Bernoulli distribution is suitable to
    model the reward (also outcome in our case) because our reward only has binary
    outcome: *Clicked* or *Not Clicked*.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '[伯努利分布](/understanding-bernoulli-and-binomial-distributions-a1eef4e0da8f#:~:text=The%20Bernoulli%20distribution%20is%20the,probability%20(1%2Dp).)
    是一种离散分布，其中只有两种可能的结果：1 和 0。伯努利分布只有一个参数，表示某个变量，比如 Y，取值为 1 的概率。因此，如果我们说 Y~ Bern(p)，比如
    p = 0.7，这意味着 Y 有 0.7 的机会取值为 1，而 1–p = 1–0.7 = 0.3 的机会取值为 0。因此，伯努利分布适合用于建模奖励（在我们的例子中也是结果），因为我们的奖励只有两种结果：*点击*
    或 *未点击*。'
- en: Beta distribution is on the other hand used to model TS belief regarding my
    movie interests. Beta distribution takes in two parameters, alpha and beta, that
    are often thought of as the number of successes and failures, respectively, and
    both have to be ≥ 1\. Thus, it’s suitable to use Beta distribution to model the
    number of times that I click watch and the number of times I skip a movie. Let’s
    take a look at an example. Here, these are 3 different beta distributions representing
    3 movies, over 10 observations, so the total number of clicks and skips for all
    3 movies are the same (10), but the click and skip rates are different. For movie
    1, I click watch 2 times (alpha = 2) and skip 8 times (beta = 8); for movie 2,
    I click watch 5 times and skip 5 times; for movie 3, I click watch 8 times and
    skip 2.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Beta 分布用于建模 TS 对我电影兴趣的信念。Beta 分布有两个参数，alpha 和 beta，通常被认为是成功和失败的次数，两者都必须
    ≥ 1。因此，使用 Beta 分布来建模我点击观看和跳过电影的次数是合适的。我们来看一个例子。这里有 3 个不同的 Beta 分布，代表 3 部电影，在 10
    次观察中，所以所有 3 部电影的点击和跳过次数总和相同（10），但点击和跳过率不同。对于电影 1，我点击观看 2 次（alpha = 2）和跳过 8 次（beta
    = 8）；对于电影 2，我点击观看 5 次和跳过 5 次；对于电影 3，我点击观看 8 次和跳过 2 次。
- en: '![](../Images/0123fa0018661d2a4f4501c5bf8ccaa3.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0123fa0018661d2a4f4501c5bf8ccaa3.png)'
- en: '**Figure 2.** Image by author'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2.** 图片由作者提供'
- en: According to the graph, we can see that the probability of me watching movie
    2 again peaks around 50%, whereas this probability for movie 1 is much lower,
    for example. We can think of the curves here as the probability of probability
    (of me watching a movie again), and so Beta distribution is ideal for representing
    TS’s belief about my movie preferences.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 根据图表，我们可以看到，我再次观看电影 2 的概率大约在 50% 处达到峰值，而电影 1 的这个概率要低得多。例如。我们可以将这些曲线视为观看电影的概率的概率，因此
    Beta 分布非常适合表示 TS 对我电影偏好的信念。
- en: The algorithm
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 算法
- en: In this section, I will help you gain a clear understanding of the algorithm
    implementation wise and methodology wise. Firstly, here is a snipper of the Thompson
    Sampling algorithm, in pseudocode and in Python. The pseudocode is taken from
    an amazing book on TS, called [*A tutorial on Thompson Sampling*](https://arxiv.org/abs/1707.02038)
    [Russo, 2017].
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将帮助你清楚地理解算法的实现和方法论。首先，这是 Thompson Sampling 算法的一个片段，分别是伪代码和 Python 实现。伪代码摘自一本关于
    TS 的精彩书籍，[*A tutorial on Thompson Sampling*](https://arxiv.org/abs/1707.02038)
    [Russo, 2017]。
- en: '![](../Images/bc239424d8d47da61c157fcedac73dbf.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bc239424d8d47da61c157fcedac73dbf.png)'
- en: '**Figure 3** Thompson Sampling, Python implementation (left) and pseudocode
    (right) — Image by Author'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 3** Thompson Sampling，Python 实现（左）和伪代码（右）— 图片由作者提供'
- en: Let's break it down!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来详细分析一下！
- en: Sample model
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 样本模型
- en: This first step of the algorithm involves “guessing” how much I like each of
    the movies. As illustrated in the previous section, my preference for each movie
    can be represented using Beta curves as those in Fig. 2, which TS does not have
    prior knowledge about and is trying to figure out how these Beta curves look like.
    In *t = 1* (first round), TS can start out by assuming that I have an equal preference
    for all 3 movies, i.e equal starting number of clicks and skips (and my 3 Beta
    curves will look the same).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的第一步涉及“猜测”我对每部电影的喜好。如前一节所述，我对每部电影的偏好可以使用Beta曲线表示，如图2所示，而TS对此没有先验知识，并且试图弄清楚这些Beta曲线的样子。在*t
    = 1*（第一轮）时，TS可以假设我对所有3部电影的喜好相同，即点击和跳过的初始次数相等（我的3条Beta曲线将看起来相同）。
- en: '![](../Images/5794df9be46cac194f14024774455cab.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5794df9be46cac194f14024774455cab.png)'
- en: '**Figure 4** TS''s first guess about my preference to be the same for all 3
    movies'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**图4** TS对我对3部电影的偏好的首次猜测相同'
- en: The three distributions here are what *p* is in the pseudocode in Fig. 3\. From
    each of these distributions, TS will sample a value, represented by theta, in
    order to help with the action selection in the next step.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的三种分布就是图3中伪代码中的*p*。从每个分布中，TS将采样一个值，用theta表示，以帮助下一步的动作选择。
- en: '![](../Images/a9a97d385abe5254d8229a4680b1d00d.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a9a97d385abe5254d8229a4680b1d00d.png)'
- en: '**Figure 5** Sample pairs of alpha-beta values to represent the distributions
    of our initial guess for each of the 3 movies (also referred to as actions/arms)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**图5** 示例的alpha-beta值对，表示我们对每部3部电影的初始猜测的分布（也称为动作/手臂）'
- en: Select and apply action
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择并应用动作
- en: In this step, TS chooses the action to execute (i.e choose a movie to recommend)
    based on whichever theta values sampled is the largest. Take Figure 2 as the example
    again. Let’s say we only have 2 movies instead—only movie 1 and movie 3\. The
    idea of using the largest theta to select an action is that if the true distributions
    have little overlap, and I almost surely like one movie more than the other in
    our case, then the sampled theta for movie 1 is very unlikely to be bigger than
    that of movie 3\. In a similar way, if we just consider movie 2 and 3, we can
    see that now we have more overlaps between the distributions. However, if we continue
    to sample more theta values over enough rounds, then we can observe that the proportion
    of movie 3's thetas > movie 2's thetas is larger than the other way round, and
    TS will have enough information to conclude that movie 3 is the better “action"
    to take. In general, this is also the reason why the more distinct the unknown
    true distributions are, the fewer rounds of experiments it takes TS to work out
    which action, or arm, is the most optimal.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在此步骤中，TS根据采样的theta值中最大的值选择要执行的动作（即选择推荐的电影）。以图2为例。假设我们只有2部电影——电影1和电影3。使用最大的theta选择动作的想法是，如果真实分布几乎没有重叠，而我在我们的例子中几乎肯定喜欢一部电影多于另一部，那么电影1的采样theta很可能不会大于电影3的theta。以类似的方式，如果我们只考虑电影2和3，我们可以看到现在这些分布之间有更多重叠。然而，如果我们继续在足够多的轮次中采样更多的theta值，那么我们可以观察到电影3的thetas
    > 电影2的thetas的比例大于反之，TS将有足够的信息得出电影3是更好的“动作”的结论。一般来说，这也是为什么未知真实分布越明显，TS找出哪个动作或手臂是最优的实验轮次就越少的原因。
- en: After applying the chosen action, TS will receive a response from me, that is
    whether I click watch the movie or not. This outcome, as mentioned above, is also
    considered our reward for the corresponding action. TS will record this observed
    outcome and employ it to update the belief about my movie preference in the next
    step.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用选择的动作后，TS将收到我的反馈，即我是否点击观看电影。正如上面提到的，这一结果也被视为我们对相应动作的奖励。TS将记录这一观察结果，并在下一步中用它来更新对我电影偏好的信念。
- en: Update distribution
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更新分布
- en: In the description for the Beta distribution above, we established that Beta
    distribution is characterized by the number of successes and failures. The more
    times I click watch a given movie, the more the mode of its beta distribution
    is pulled towards 1 and conversely, towards 0 the more I skip over the recommendation.
    Therefore, the update to the belief about a given movie, after it has been put
    out for suggestion and a response has been recorded, is done by simply adding
    one to either the alpha or beta parameter of the movie's beta distribution, depending
    on whether the movie is clicked or skipped.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的 Beta 分布描述中，我们确定 Beta 分布的特征是成功次数和失败次数。我点击观看某部电影的次数越多，该电影的 Beta 分布的模式就越趋近于
    1，而相反地，我跳过推荐的次数越多，模式就越趋近于 0。因此，在电影被推荐并记录响应后，对电影的信念更新是通过将电影的 Beta 分布的 alpha 或 beta
    参数加 1 来完成的，具体取决于电影是被点击还是被跳过。
- en: This straightforward, interpretable parameter update method is why the Beta
    Bernoulli is a very common TS model.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这种简单且易于解释的参数更新方法就是为什么 Beta Bernoulli 是一种非常常见的 TS 模型。
- en: Result and discussion
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果与讨论
- en: Rolling back to the scenario at the start of the article. We are on the journey
    of guessing which of the 3 movies is the most to optimal to recommend to me, provided
    that there is one that I will click watch 100%, one that I have 70% chance of
    clicking, and the other with only 5% chance (again, this knowledge is unknown
    to TS). The first row shows two different starting points for the simulation,
    which will allow us to observe whether we can reach the same final outcomes with
    different initial prior beliefs.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 回到文章开始时的情境。我们正在猜测 3 部电影中哪一部最适合推荐给我，假设有一部我会 100% 点击观看，一部我有 70% 的点击概率，另一部只有 5%
    的点击概率（再次强调，这些信息 TS 并不知道）。第一行展示了两种不同的模拟起始点，这将使我们观察是否可以通过不同的初始先验信念达到相同的最终结果。
- en: '![](../Images/afc3c98d911b6126d07b914ab3ff5e5e.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/afc3c98d911b6126d07b914ab3ff5e5e.png)'
- en: '**Figure 6** TS simulations with various numbers of rounds T = 5, 10, 20\.
    The beta distributions represent TS’s beliefs about my movie preferences at the
    end of the experiment. ***Left column***: outcome when the starting distribution
    is Beta(1, 1). Right column: outcome when the starting distribution is Beta(2,
    3)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 6** TS 模拟的不同轮数 T = 5, 10, 20。 Beta 分布代表了实验结束时 TS 对我电影偏好的信念。 ***左列***：初始分布为
    Beta(1, 1) 时的结果。右列：初始分布为 Beta(2, 3) 时的结果'
- en: From Figure 6, we can see that the final answer to which movie is my ultimate
    favorite is movie 1 — PARASITE (Sorry Marvel fans)!!
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 从图 6 中，我们可以看到我最终最喜欢的电影是电影 1 — 《寄生虫》（对不起，漫威粉丝）！！
- en: As we can see, the exploration journey of the two cases differ, in which the
    Beta(1, 1) starting guesses result in a faster arrival at the movie that's believed
    to be my most favorite. It only needs T=10 rounds to see that TS is clearly exploiting
    movie 1, implying that TS has suggested movie 1 and achieved clicks from me, for
    which its beta distribution is pulled rightwards, hence the theta sampled from
    the updated distributions is beating its competitors, resulting in the exploitation.
    This exploitation already shows up with T=5 rounds, but according to the corresponding
    graph, there is still a lot of overlap between the beta of movie 1 and 3, their
    modes are not entirely distinct, meaning TS is still not completely certain that
    movie 1 is the optimal action.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，两种情况的探索过程不同，其中 Beta(1, 1) 的初始猜测导致更快地找到被认为是我最喜欢的电影。只需要 T=10 轮就可以看出 TS 明显在开发电影
    1，这意味着 TS 已经推荐了电影 1 并得到了我的点击，因此其 Beta 分布向右拉动，因此从更新的分布中采样的 theta 超过了它的竞争对手，导致了开发。这种开发在
    T=5 轮时已经出现，但根据相应的图表，电影 1 和电影 3 的 Beta 之间仍然存在较多重叠，它们的模式并不完全不同，这意味着 TS 仍然不完全确定电影
    1 是最优的行动。
- en: On the other hand, the initial beliefs of Beta(2, 3) results in TS needing more
    rounds to arrive at movie 1 (T=20). Even at T=10, there is still a lot of uncertainty
    between movie 1 and 3, and it is observed that, due to the randomness in sampling
    theta, it can result in movie 3 being exploited as the optimal option. This experiment
    shows that the initial prior knowledge about each action can play a role in how
    fast the optimal arm is detected, the topic of which we can dive more into in
    the future articles.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Beta(2, 3) 的初始信念使得 TS 需要更多的轮次才能到达电影 1（T=20）。即使在 T=10 时，电影 1 和 电影 3 之间仍存在很大的不确定性，并且观察到由于
    theta 采样的随机性，电影 3 可能被错误地当作最佳选项。这项实验表明，每个行动的初始先验知识在检测最佳臂的速度上起着作用，关于这个主题我们可以在未来的文章中进一步深入探讨。
- en: It's good to note that if the actual distributions of the movies are almost
    identical (say if movie 1 and 3 has 100% and 98% click rate, respectively), TS
    is very likely to fail in identifying the optimal action because the proportion
    of the sampled thetas from one distribution exceeding those of the other will
    be split. Thus, if due to chance that movie 3 happens to have more “larger thetas",
    then TS will exploit this option more, leading to it being misidentified as the
    optimal action.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，如果电影的实际分布几乎相同（比如电影 1 和电影 3 的点击率分别为 100% 和 98%），TS 很可能无法识别最佳行动，因为来自一个分布的样本
    thetas 超过另一个分布的样本 thetas 的比例会被拆分。因此，如果由于偶然性，电影 3 的“较大 thetas”更多，TS 将更多地利用这个选项，导致其被错误地识别为最佳行动。
- en: Another note from the experiment is that TS is only good at telling us what
    the best action is, but cannot give us information about the remaining options.
    This is because over the course of the exploration process, TS will quickly eliminate
    the options that are deemed not optimal, so TS stops receiving further information
    on these actions and hence it doesn't give the correct ranking between the actions
    beside the best one.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 实验的另一个发现是，TS 仅能告诉我们最佳行动是什么，但不能提供关于其他选项的信息。这是因为在探索过程中，TS 会迅速淘汰那些被认为不是最优的选项，因此
    TS 停止接收这些行动的进一步信息，从而不能提供最佳选项以外的行动的正确排序。
- en: Conclusion
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we have got to explore more about the Thompson Sampling algorithm
    and work through a simulation of movie recommendation. Thompson sampling largely
    involves distributions and prior knowledge in providing a prediction, which is
    a concept that is central to Bayesian models, which I plan to cover more with
    you guys in the up and coming articles. If you reach the end of this article,
    thank you for your time and I hope that this tutorial has given you a balanced
    technical and intuitive understanding of this algorithm! If you have any further
    question, feel free to find me through my [LinkedIn](https://www.linkedin.com/in/irene-chang-4356a013a/),
    happy to connect and respond to them!
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们探讨了汤普森采样算法，并通过电影推荐模拟进行了演练。汤普森采样在提供预测时涉及大量的分布和先验知识，这是贝叶斯模型的核心概念，我计划在即将到来的文章中与大家进一步讨论。如果你读到了这里，谢谢你的时间，希望这篇教程能给你提供对这个算法的技术和直观理解！如果你有任何进一步的问题，随时通过我的
    [LinkedIn](https://www.linkedin.com/in/irene-chang-4356a013a/) 联系我，很高兴与您联系并回答问题！
- en: 'References:'
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献：
- en: '[1] [What are the potential risks and challenges of recommender systems in
    different domains and contexts?](https://www.linkedin.com/advice/3/what-potential-risks-challenges-recommender#:~:text=One%20of%20the%20ethical%20issues,groups%2C%20opinions%2C%20or%20values)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [推荐系统在不同领域和背景中的潜在风险和挑战是什么？](https://www.linkedin.com/advice/3/what-potential-risks-challenges-recommender#:~:text=One%20of%20the%20ethical%20issues,groups%2C%20opinions%2C%20or%20values)'
- en: '[2] [Recommender systems and their ethical challenges](https://link.springer.com/article/10.1007/s00146-020-00950-y)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [推荐系统及其伦理挑战](https://link.springer.com/article/10.1007/s00146-020-00950-y)'
- en: '[3] [When You Should Prefer “Thompson Sampling” Over A/B Tests](/when-you-should-prefer-thompson-sampling-over-a-b-tests-5e789b480458)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [何时应该选择“汤普森采样”而不是 A/B 测试](/when-you-should-prefer-thompson-sampling-over-a-b-tests-5e789b480458)'
