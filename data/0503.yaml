- en: How Prejudice Creeps into AI Systems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 偏见如何渗透到 AI 系统中
- en: 原文：[https://towardsdatascience.com/how-prejudice-creeps-into-ai-systems-3673646ae8e3?source=collection_archive---------8-----------------------#2023-02-03](https://towardsdatascience.com/how-prejudice-creeps-into-ai-systems-3673646ae8e3?source=collection_archive---------8-----------------------#2023-02-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-prejudice-creeps-into-ai-systems-3673646ae8e3?source=collection_archive---------8-----------------------#2023-02-03](https://towardsdatascience.com/how-prejudice-creeps-into-ai-systems-3673646ae8e3?source=collection_archive---------8-----------------------#2023-02-03)
- en: Where do AI biases actually originate from?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AI 偏见实际上源于哪里？
- en: '[](https://medium.com/@boris-ruf?source=post_page-----3673646ae8e3--------------------------------)[![Boris
    Ruf](../Images/96dc4fc2f32add89fef6911195590cd8.png)](https://medium.com/@boris-ruf?source=post_page-----3673646ae8e3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3673646ae8e3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3673646ae8e3--------------------------------)
    [Boris Ruf](https://medium.com/@boris-ruf?source=post_page-----3673646ae8e3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@boris-ruf?source=post_page-----3673646ae8e3--------------------------------)[![Boris
    Ruf](../Images/96dc4fc2f32add89fef6911195590cd8.png)](https://medium.com/@boris-ruf?source=post_page-----3673646ae8e3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3673646ae8e3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3673646ae8e3--------------------------------)
    [Boris Ruf](https://medium.com/@boris-ruf?source=post_page-----3673646ae8e3--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fed341456850c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-prejudice-creeps-into-ai-systems-3673646ae8e3&user=Boris+Ruf&userId=ed341456850c&source=post_page-ed341456850c----3673646ae8e3---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3673646ae8e3--------------------------------)
    ·4 min read·Feb 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3673646ae8e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-prejudice-creeps-into-ai-systems-3673646ae8e3&user=Boris+Ruf&userId=ed341456850c&source=-----3673646ae8e3---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fed341456850c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-prejudice-creeps-into-ai-systems-3673646ae8e3&user=Boris+Ruf&userId=ed341456850c&source=post_page-ed341456850c----3673646ae8e3---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3673646ae8e3--------------------------------)
    ·4 分钟阅读·2023年2月3日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3673646ae8e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-prejudice-creeps-into-ai-systems-3673646ae8e3&user=Boris+Ruf&userId=ed341456850c&source=-----3673646ae8e3---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3673646ae8e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-prejudice-creeps-into-ai-systems-3673646ae8e3&source=-----3673646ae8e3---------------------bookmark_footer-----------)![](../Images/68abad72609d8df6d60b05d3f299ac8f.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3673646ae8e3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-prejudice-creeps-into-ai-systems-3673646ae8e3&source=-----3673646ae8e3---------------------bookmark_footer-----------)![](../Images/68abad72609d8df6d60b05d3f299ac8f.png)'
- en: Photo by [Lucas Benjamin](https://unsplash.com/@aznbokchoy?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Lucas Benjamin](https://unsplash.com/@aznbokchoy?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: '*One challenge for systems powered with artificial intelligence (AI) is the
    biases that may be embedded in the algorithms. In* [*my previous article*](/biased-ai-a-look-under-the-hood-5d0a41968f16)*,
    I explained the inner processes which take place when AI goes rogue. In the following,
    I will deepen the question where these biases actually come from — and how those
    sources are different from well-studied bias problems in conventional technologies.*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*一个挑战是，人工智能（AI）系统可能会在算法中嵌入偏见。在* [*我之前的文章*](/biased-ai-a-look-under-the-hood-5d0a41968f16)*中，我解释了当
    AI 行为异常时发生的内部过程。接下来，我将深入探讨这些偏见实际上来自哪里——以及这些来源如何与传统技术中研究得较多的偏见问题不同。*'
- en: Machine learning (ML) algorithms identify patterns in data. Their major strength
    is the desired capability to find and discriminate classes in training data, and
    to use those insights to make predictions for new, unseen data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（ML）算法识别数据中的模式。它们的主要优势在于能够在训练数据中找到和区分类别，并利用这些洞察对新数据进行预测。
- en: In the era of “big data”, a large quantity of data is available, with all sorts
    of variables. The general assumption is that the more data is used, the more precise
    the algorithm and its predictions become. When using a large amount of data, it
    clearly contains many correlations. However, not all correlations imply causation.
    No matter how large the dataset is, it still only remains a snapshot of reality.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在“大数据”时代，有大量的数据可用，并且变量种类繁多。一般的假设是，数据越多，算法及其预测就会越精确。在使用大量数据时，数据中显然包含了许多相关性。然而，并非所有相关性都意味着因果关系。不论数据集有多大，它仍然只是现实的一个快照。
- en: 'Let’s take an example:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举个例子：
- en: In a training data set on claims of a car insurance, red cars may have caused
    more accidents than cars of another colour. The ML algorithm detects this correlation.
    However, there is no scientific proof of causation between the colour of a car
    and the risk of accidents.
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在一个汽车保险的理赔训练数据集中，红色汽车可能比其他颜色的汽车导致了更多的事故。机器学习算法检测到了这种相关性。然而，并没有科学证据表明汽车颜色与事故风险之间存在因果关系。
- en: Solely for the sake of the algorithm’s performance it is crucial to notice and
    eliminate this kind of unwanted correlations. Otherwise, the algorithm is biased
    and results on new data in production may be poor. In the case of the example,
    a competitor with a better algorithm, which does not falsely attribute a higher
    risk to drivers of red cars, can offer a lower price to those customers and entice
    them away.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 单纯为了算法的性能，注意并消除这种不想要的相关性至关重要。否则，算法会有偏见，对新数据的结果可能会很差。在这个例子中，拥有更好算法的竞争对手，如果没有错误地将更高风险归因于红色汽车的驾驶员，就可以为这些客户提供更低的价格，并将他们吸引走。
- en: Beside the performance aspect, there is a second, even more severe problem which
    appears when the predictions impact people, and when the algorithm is biased to
    favour privileged groups over unprivileged groups, resulting in discrimination.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 除了性能方面，还有一个更严重的问题，这种问题出现时预测影响到人们，并且算法偏向于优待特权群体而非非特权群体，从而导致歧视。
- en: It is important to note that these unwanted discriminations may happen without
    explicitly providing sensitive personal data. In fact, other attributes can implicitly
    reveal this information serving as proxy. For example, a car model can hint at
    the owner’s gender, or the zip code may correlate with a resident’s ethnicity
    or religion.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，这些不希望出现的歧视可能在没有明确提供敏感个人数据的情况下发生。实际上，其他属性可以隐含地揭示这些信息作为代理。例如，一款汽车型号可以暗示车主的性别，或者邮政编码可能与居民的种族或宗教相关。
- en: Where does it come from?
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它的来源是什么？
- en: The problem of bias is obviously not new. Even before the emergence of AI, [plenty
    of different forms of bias](https://en.wikipedia.org/wiki/List_of_cognitive_biases)
    were known to possibly cause unwanted and unexpected results in technical systems.
    Automation bias for example is what happens when people trust suggestions of automated
    systems over human reasoning. [Several severe airplane accidents happened](https://www.newyorker.com/science/maria-konnikova/hazards-automation)
    in the past because the pilots had trusted the autopilot over their own judgment.
    Another type of bias may occur when an algorithm is deployed in an environment
    for which it was not trained in the first place. For example, if it is applied
    in a different geographical region or on a different group of people.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 偏见问题显然不是新的。即使在人工智能出现之前，[已经知道了多种不同形式的偏见](https://en.wikipedia.org/wiki/List_of_cognitive_biases)，这些偏见可能导致技术系统中出现不必要和意外的结果。例如，自动化偏见是指人们信任自动系统的建议而非人类推理。[过去发生过几起严重的飞机事故](https://www.newyorker.com/science/maria-konnikova/hazards-automation)，因为飞行员信任了自动驾驶仪而不是自己的判断。另一种偏见可能发生在算法被应用于最初未训练的环境中。例如，如果它被应用于不同的地理区域或不同的人群中。
- en: While explicitly programmed rules in algorithms or the way they are used in
    practice may produce biased results, this is a long-known problem which already
    applies to conventional deterministic algorithms. In this article, I focus on
    the **new sources of bias**, which gain in importance with the rise of machine
    learning technologies. More specifically, I discuss human bias in data and selection
    bias.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管算法中显式编程的规则或其在实践中的使用方式可能产生偏差结果，但这是一个长期存在的问题，也适用于传统的确定性算法。在本文中，我专注于**新的偏差来源**，这些偏差随着机器学习技术的兴起而变得越来越重要。更具体地说，我讨论了数据中的人为偏差和选择偏差。
- en: Human bias
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人为偏差
- en: The first source of bias which comes naturally to mind is human bias. Different
    types of this kind of well-studied bias are outlined below.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个自然想到的偏差来源是人为偏差。下面概述了这种类型的偏差的不同类型。
- en: '![](../Images/f9710dec7d1831383ee2ab85ed2521f9.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f9710dec7d1831383ee2ab85ed2521f9.png)'
- en: Different sources of human bias
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 人为偏差的不同来源
- en: Training data can consist of labels of objective observations, as for instance
    coming from a measuring device. However, training data may also involve human
    assessment. Data labels which include human judgment may have been labelled with
    prejudice. Since the labels serve as *ground truth*, the algorithm’s performance
    directly depends on them, and any bias contained gets reproduced at scale in the
    model. So basically, a well-studied form of bias finds its way into a new technology
    here.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据可以包括来自测量设备的客观观察标签。然而，训练数据也可能涉及人为评估。包括人为判断的数据标签可能存在偏见。由于这些标签作为*真实标准*，算法的表现直接依赖于它们，任何包含的偏差都在模型中大规模复制。因此，基本上，一种经过充分研究的偏差形式在这里进入了新技术中。
- en: Selection bias
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择偏差
- en: Another, less obvious source of bias is the process of how the data are collected.
    If the data do not reflect the real distribution, a ML algorithm which is using
    it for training will learn and enforce the bias. The table below provides a list
    of different types of biases which may cause selection bias in data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个不那么明显的偏差来源是数据收集的过程。如果数据未能反映真实的分布，那么使用这些数据进行训练的机器学习算法将会学习并加剧这种偏差。下表列出了可能导致数据选择偏差的不同类型的偏差。
- en: '![](../Images/34af303bfe8f31aa1c978efd7dd28816.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/34af303bfe8f31aa1c978efd7dd28816.png)'
- en: Different sources of selection bias
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 不同来源的选择偏差
- en: So what
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 那又如何
- en: ML algorithms are strongly dependent on the data they use to create the predictive
    model. These training data may be biased, in particular in form of human biases
    or selection biases. Because the algorithms are prone to any such effects, and
    due to the potential of getting deployed at scale, even minimal systematic errors
    in the algorithms can lead to reinforced discrimination.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法在创建预测模型时非常依赖于其使用的数据。这些训练数据可能存在偏差，特别是形式上的人为偏差或选择偏差。由于算法容易受到这些影响，并且由于有可能在大规模应用中出现，即使是算法中的微小系统性错误也可能导致加剧的歧视。
- en: '*Many thanks to Antoine Pietri for his valuable support in writing this post.
    In* [*my next article*](https://medium.com/towards-data-science/to-guarantee-impartial-ai-decisions-lady-justice-needs-to-blink-2992167b2591)*,
    I explain why deleting the sensitive attributes is not a simple solution to AI
    fairness.*'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*特别感谢Antoine Pietri在撰写这篇文章时提供的宝贵支持。在* [*我的下一篇文章*](https://medium.com/towards-data-science/to-guarantee-impartial-ai-decisions-lady-justice-needs-to-blink-2992167b2591)*中，我将解释为什么删除敏感属性并不是确保AI公平性的简单解决方案。*'
