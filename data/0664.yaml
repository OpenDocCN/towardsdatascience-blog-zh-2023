- en: Transformer Aided Supply Chain Network Design
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变压器辅助的供应链网络设计
- en: 原文：[https://towardsdatascience.com/transformer-aided-supply-chain-network-design-fe45bb846f0f?source=collection_archive---------9-----------------------#2023-02-17](https://towardsdatascience.com/transformer-aided-supply-chain-network-design-fe45bb846f0f?source=collection_archive---------9-----------------------#2023-02-17)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/transformer-aided-supply-chain-network-design-fe45bb846f0f?source=collection_archive---------9-----------------------#2023-02-17](https://towardsdatascience.com/transformer-aided-supply-chain-network-design-fe45bb846f0f?source=collection_archive---------9-----------------------#2023-02-17)
- en: Using transformer to help solve a classic problem in supply chain — facility
    location problem
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用变压器来帮助解决供应链中的经典问题——设施选址问题
- en: '[](https://medium.com/@guanx92?source=post_page-----fe45bb846f0f--------------------------------)[![Guangrui
    Xie](../Images/def9aa637424a88d75a6a3bb103350bc.png)](https://medium.com/@guanx92?source=post_page-----fe45bb846f0f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fe45bb846f0f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fe45bb846f0f--------------------------------)
    [Guangrui Xie](https://medium.com/@guanx92?source=post_page-----fe45bb846f0f--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@guanx92?source=post_page-----fe45bb846f0f--------------------------------)[![Guangrui
    Xie](../Images/def9aa637424a88d75a6a3bb103350bc.png)](https://medium.com/@guanx92?source=post_page-----fe45bb846f0f--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fe45bb846f0f--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fe45bb846f0f--------------------------------)
    [Guangrui Xie](https://medium.com/@guanx92?source=post_page-----fe45bb846f0f--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F495b92f0c66d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-aided-supply-chain-network-design-fe45bb846f0f&user=Guangrui+Xie&userId=495b92f0c66d&source=post_page-495b92f0c66d----fe45bb846f0f---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fe45bb846f0f--------------------------------)
    ·15 min read·Feb 17, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffe45bb846f0f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-aided-supply-chain-network-design-fe45bb846f0f&user=Guangrui+Xie&userId=495b92f0c66d&source=-----fe45bb846f0f---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F495b92f0c66d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-aided-supply-chain-network-design-fe45bb846f0f&user=Guangrui+Xie&userId=495b92f0c66d&source=post_page-495b92f0c66d----fe45bb846f0f---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fe45bb846f0f--------------------------------)
    ·15 min read·2023年2月17日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffe45bb846f0f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-aided-supply-chain-network-design-fe45bb846f0f&user=Guangrui+Xie&userId=495b92f0c66d&source=-----fe45bb846f0f---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffe45bb846f0f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-aided-supply-chain-network-design-fe45bb846f0f&source=-----fe45bb846f0f---------------------bookmark_footer-----------)![](../Images/bd64ec628e3e129b102daba43f9a1307.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffe45bb846f0f&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftransformer-aided-supply-chain-network-design-fe45bb846f0f&source=-----fe45bb846f0f---------------------bookmark_footer-----------)![](../Images/bd64ec628e3e129b102daba43f9a1307.png)'
- en: Photo by [Mika Baumeister](https://unsplash.com/es/@mbaumi?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由[Mika Baumeister](https://unsplash.com/es/@mbaumi?utm_source=medium&utm_medium=referral)拍摄，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: ChatGPT has been a really hot topic recently due to its general intelligence
    to accomplish a broad range of tasks. The core model underneath ChatGPT is transformer,
    which was first proposed for machine translation tasks in the well-known paper
    [*Attention is all you need*](https://arxiv.org/pdf/1706.03762.pdf)by a research
    team at Google. Transformer processes an entire sentence at once using an attention
    mechanism and positional encoding rather than word by word using recursions (like
    recurrent neural networks). This reduces the information loss during recursions,
    hence giving transformer a better capability to learn long-range context dependencies,
    as compared to traditional recurrent neural networks (e.g., LSTM, GRU).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 由于ChatGPT的广泛智能来完成各种任务，最近成为了一个热门话题。ChatGPT的核心模型是变压器，最初在谷歌研究团队的著名论文[*Attention
    is all you need*](https://arxiv.org/pdf/1706.03762.pdf)中首次提出。变压器使用注意力机制和位置编码一次处理整个句子，而不是像递归神经网络（如LSTM、GRU）那样逐字处理。这样可以减少递归过程中的信息丢失，从而使变压器比传统的递归神经网络更能有效地学习长距离上下文依赖关系。
- en: The application of transformer is not limited in language models. Researchers
    have applied it to many other tasks such as time series forecasting, computer
    vision, etc., since it was first proposed. A question I have been thinking about
    is, can transformer be used to help solve practical operations research (OR) problems
    in supply chain area? In this article, I will show an example of using transformer
    to assist in solving a classic OR problem in supply chain area — facility location
    problem.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器的应用不仅限于语言模型。自首次提出以来，研究人员已将其应用于许多其他任务，如时间序列预测、计算机视觉等。我一直在思考的问题是，变压器能否用于帮助解决供应链领域的实际运筹学（OR）问题？在本文中，我将展示一个使用变压器来辅助解决供应链领域经典OR问题——设施选址问题的例子。
- en: Facility location problem
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设施选址问题
- en: Let’s first review the basic setting of this problem. I also briefly discussed
    about this problem in [my first article](https://medium.com/towards-data-science/some-thoughts-on-synergies-between-operations-research-and-machine-learning-921d78ed4bd5)
    on Towards Data Science.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先回顾一下这个问题的基本设置。我在[我的第一篇文章](https://medium.com/towards-data-science/some-thoughts-on-synergies-between-operations-research-and-machine-learning-921d78ed4bd5)中也简要讨论了这个问题。
- en: 'Let us assume that a company wants to consider building distribution centers
    (DCs) among *I* candidate sites to ship their finished goods to *J* customers.
    Each site *i* has its associate capacity for storing the finished goods with a
    maximum of *m_i* units of products*.* Buildinga DC oneach site *i* requires a
    fixed construction fee of *f_i.* Shipping each unit of product from site *i* to
    customer *j* toincurs a shipping cost of *c_ij.* Each customer *j* has a demand
    of *d_j* and the demand of all customers must be satisfied. Let binary variable
    *y_i* denote whether we build a DC at site *i* and *x_ij* denote the volume of
    products to be shipped from site *i* to customer *j.* The optimization problem
    with an objective to minimize the total cost can be formulated as below:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一家公司想要在*I*个候选地点中建设配送中心（DCs），以将成品运送给*J*个客户。每个地点*i*都有其关联的存储容量，最多可以存放*m_i*单位产品。
    在地点*i*建立一个DC需要固定的建设费用*f_i*。从地点*i*向客户*j*运送每单位产品需要运费*c_ij*。每个客户*j*的需求为*d_j*，且所有客户的需求必须得到满足。设二进制变量*y_i*表示是否在地点*i*建立一个DC，而*x_ij*表示从地点*i*向客户*j*运送的产品量。目标是最小化总成本的优化问题可以表述如下：
- en: '![](../Images/40d846413209efd414da65ab02859c3c.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/40d846413209efd414da65ab02859c3c.png)'
- en: Formulation of facility location problem (Image by author)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 设施选址问题的公式化（图由作者提供）
- en: This is a mixed integer linear programming (MILP) problem, which can be solved
    using any commercial or non-commercial solvers (e.g., CPLEX, GUROBI, SCIP). However,
    when the model gets large (*I* and *J* are large), it could take a long time to
    solve the model. The main complexity of this problem comes from the integer variables
    *y_i’*s, as these variables are the ones that need to be branched on in the branch
    and bound algorithm. If we can find a faster way to determine the values of *y_i’*s,
    that would save a lot of solving time. This is where transformer comes into play.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个混合整数线性规划（MILP）问题，可以使用任何商业或非商业求解器（例如 CPLEX、GUROBI、SCIP）来解决。然而，当模型变得很大（*I*
    和 *J* 很大）时，求解模型可能会花费很长时间。这个问题的主要复杂性来自整数变量 *y_i’*s，因为这些变量是需要在分支定界算法中进行分支的。如果我们能找到一种更快的方法来确定
    *y_i’*s 的值，那将节省大量的求解时间。这就是变压器发挥作用的地方。
- en: Transformer aided solution approach
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变压器辅助解决方案方法
- en: The basic idea of this solution approach is to train a transformer to predict
    the values of *y_i’*s by learning from the solutions of a large number of instances
    of the facility location problem given by a solver. The problem can be formulated
    as a classification problem as *y_i’*s can only take values of 0 or 1, corresponding
    to two classes. Class 0 means site *i* is not selected for building a DC, and
    class 1 means site *i* is selected. The transformer, once trained, can predict
    which class each *y_i* belongs toall at once for an unseen instance, thus saving
    a lot of time spent on branching on integer variables. The diagram below shows
    the architecture of the transformer adopted here.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这种解决方案方法的基本思想是通过从大量实例的设施选址问题的解决方案中学习，训练一个变压器来预测 *y_i’*s 的值。这个问题可以被公式化为一个分类问题，因为
    *y_i’*s 只能取 0 或 1 的值，对应两个类别。类别 0 表示站点 *i* 没有被选中用于建立 DC，类别 1 表示站点 *i* 被选中。一旦训练完成，变压器可以一次性预测每个
    *y_i* 属于哪个类别，从而节省大量用于整数变量分支的时间。下图展示了这里采用的变压器架构。
- en: '![](../Images/3eb9052c67956f25dcd012391d2866d2.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3eb9052c67956f25dcd012391d2866d2.png)'
- en: The architecture of the transformer model adopted in this article (Image by
    author)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本文采用的变压器模型的架构（作者提供的图像）
- en: As illustrated by the diagram above, the vector representations of each candidate
    DC site and customer (containing relevant features of each candidate DC site and
    customer) are passed into a linear layer. The linear layer produces an embedding
    vector for each candidate DC site and customer. These embeddings are then passed
    through *N* standard encoder blocks as depicted in the paper [*Attention is all
    you need*](https://arxiv.org/pdf/1706.03762.pdf)*.* Within the encoder blocks,
    the multi-head attention mechanism allows each candidate DC site to attend to
    all the customers’ and other candidate DC sites’ information. Then the outputs
    of the encoder blocks are passed through a linear layer + softmax layer as the
    decoding process.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图所示，每个候选 DC 站点和客户的向量表示（包含每个候选 DC 站点和客户的相关特征）被传递到一个线性层。线性层为每个候选 DC 站点和客户生成一个嵌入向量。这些嵌入向量随后通过论文中描述的
    *N* 个标准编码块 [*Attention is all you need*](https://arxiv.org/pdf/1706.03762.pdf)
    进行处理。在编码块中，多头注意机制允许每个候选 DC 站点关注所有客户和其他候选 DC 站点的信息。然后，编码块的输出通过一个线性层 + softmax 层作为解码过程。
- en: 'A few accommodations I made to the transformer architecture for this particular
    problem are:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我为这个特定问题对变压器架构所做的一些调整是：
- en: Positional encoding is removed due to the fact that the exact order of the input
    sequence *V_dc1, …, V_cusJ* does not matter to the output from the perspective
    of the optimization problem. In other words, randomly shuffling the the input
    sequence to the transformer shouldn’t affect the solution to the facility location
    problem.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于从优化问题的角度来看，输入序列 *V_dc1, …, V_cusJ* 的确切顺序并不重要，因此去除了位置编码。换句话说，随机打乱输入序列对变压器的输出不应影响设施选址问题的解决。
- en: The output layer produces a two-dimensional vector for each element in the input
    sequence *V_dc1, …, V_cusJ*. The two-dimensional vector contains the probabilities
    of this element belonging to class 0 and class 1\. Note that this two-dimensional
    vector is actually invalid for elements *V_cus1, …, V_cusJ* as we cannot build
    a DC on a customer’s site. So when training the transformer, we mask the outputs
    corresponding to *V_cus1, …, V_cusJ* to calculate the loss function, as we don’t
    care about what these outputs are.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出层为输入序列中的每个元素 *V_dc1, …, V_cusJ* 生成一个二维向量。这个二维向量包含该元素属于类别 0 和类别 1 的概率。注意，这个二维向量对于元素
    *V_cus1, …, V_cusJ* 实际上是无效的，因为我们不能在客户的站点上建立 DC。因此，在训练变换器时，我们对 *V_cus1, …, V_cusJ*
    对应的输出进行掩蔽，以计算损失函数，因为我们不关心这些输出是什么。
- en: In this facility location problem, we assume that the unit shipping cost *c_ij*
    isproportional to the distance between site *i* and customer *j.* We define *V_dci*
    as *[dci_x, dci_y, m_i, f_i, 0],* where *dci_x* and *dci_y* are the coordinates
    of candidate DC site *i,* and *0* indicates that this is a vector representation
    of the features of a candidate DC site; we define *V_cusj* as *[cusj_x, cusj_y,
    d_j, 0, 1],* where *cusj_x* and *cusj_y* are the coordinates of customer *j, 0*
    is the counterpart of *f_i* in *V_dci* meaning there’s no fixed fee incurred at
    a customer*,* and *1* indicates that this is a vector representation of a customer*.*
    Hence each element in the input sequence is a 5-dimensional vector. Through the
    first linear layer, we project each vector into a high dimensional embedding vector
    and feed them into the encoder blocks.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个设施选址问题中，我们假设单位运输成本 *c_ij* 与站点 *i* 和客户 *j* 之间的距离成正比。我们定义 *V_dci* 为 *[dci_x,
    dci_y, m_i, f_i, 0]*，其中 *dci_x* 和 *dci_y* 是候选 DC 站点 *i* 的坐标，*0* 表示这是一个候选 DC 站点特征的向量表示；我们定义
    *V_cusj* 为 *[cusj_x, cusj_y, d_j, 0, 1]*，其中 *cusj_x* 和 *cusj_y* 是客户 *j* 的坐标，*0*
    是 *V_dci* 中 *f_i* 的对应项，表示在客户处没有固定费用，并且 *1* 表示这是客户的向量表示。因此，输入序列中的每个元素是一个 5 维向量。通过第一层线性层，我们将每个向量投影到一个高维嵌入向量，并将其输入到编码器块中。
- en: To train the transformer, we first need to build a dataset. To set up the labels
    in the dataset, we solve a large number of instances of the facility location
    problem using an optimization solver (i.e., SCIP) and then record the values of
    *y_i’*s for each instance. The coordinates, *d_j’*s, *m_i’*s, *f_i’*s of each
    instances are used to construct the input sequence for the transformer. The entire
    dataset is split into a training set and a test set.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练变换器，我们首先需要构建一个数据集。为了设置数据集中的标签，我们使用优化求解器（即 SCIP）求解大量设施选址问题的实例，然后记录每个实例的 *y_i’*s
    的值。每个实例的坐标、*d_j’*s、*m_i’*s 和 *f_i’*s 用于构建变换器的输入序列。整个数据集被分成训练集和测试集。
- en: Once the transformer is trained, it can be used to predict which class each
    candidate DC site *i* belongs to, in other words, whether *y_i* is0or 1\. Then
    we fix each *y_i* to its predicted value, and solve the MILP with the rest of
    the variables *x_ij’*s*.* Note that once the values of *y_i’*s are fixed, the
    facility location problem becomes an LP, which is much easier to solve using the
    optimization solver. The resulting solution may be sub-optimal, but saves a lot
    of solving time for large instances.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦变换器训练完成，它可以用来预测每个候选DC站点 *i* 属于哪个类别，换句话说，就是 *y_i* 是 0 还是 1。然后我们将每个 *y_i* 固定为其预测值，并用剩余的变量
    *x_ij’*s* 来求解 MILP。注意，一旦 *y_i’*s 的值被固定，设施选址问题变成一个 LP，这使得使用优化求解器解决起来更为简单。得到的解可能是次优的，但对于大规模实例可以节省大量的求解时间。
- en: 'One caveat of the transformer aided solution approach is that the predicted
    *y_i* values given by the transformer may sometimes result in infeasibility for
    the MILP, due to the fact that the predictions of the transformer are not perfectly
    accurate. The infeasibility is caused by the sum of *m_i * y_i* being smaller
    thanthe sum of *d_j*, meaning there will not be enough supply to satisfy the total
    demand across all the customers if we follow the predicted values of *y_i’*s.
    In such cases, we retain the *y_i* values where *y_i* is 1, and search through
    the rest of the candidate DC sites to make up the difference between the total
    supply and demand. Essentially, we need to solve another optimization problem
    as below:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器辅助解决方案方法的一个警告是，变换器给出的预测* y_i *值有时可能导致MILP不可行，因为变换器的预测并不完全准确。不可行性是由于* m_i
    * y_i *的总和小于* d_j *的总和，这意味着如果我们按照预测的* y_i *值操作，将没有足够的供应来满足所有客户的总需求。在这种情况下，我们保留*
    y_i *为1的值，并在剩余的候选DC站点中搜索，以弥补总供应和需求之间的差距。实际上，我们需要解决如下的另一个优化问题：
- en: '![](../Images/9f98fb0bfb3931f261f93658268536a6.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9f98fb0bfb3931f261f93658268536a6.png)'
- en: Problem formulation for resolving infeasibility (Image by author)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 解决不可行性的问题表述（图片由作者提供）
- en: Here, *I_0* is the set of candidate DC sites of which the predicted *y_i* value
    is 0 and *I_1* is the set of candidate DC sites of which the predicted *y_i* value
    is 1\. This is essentially a knapsack problem which can be solved by an optimization
    solver. However, with large instances, this problem could still consume a lot
    of solving time. Hence, I decided to adopt a heuristic to solve this problem —
    the greedy algorithm for solving fractional knapsack problems. Specifically, we
    calculate the ratio *f_i/m_i* for each candidate DC site in *I_0,* and sort the
    sites in an ascending order based on the ratios. Then we select the sites from
    the first to the last one by one until the constraint is met. After this procedure,
    the *y_i* values of the selected sites in *I_0,* together with all the sites in
    I_1, are set to 1\. The *y_i* values of the rest sites are set to 0\. This configuration
    is then fed into the optimization solver to solve for the rest *x_ij* variables.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，* I_0 *是预测* y_i *值为0的候选DC站点集合，* I_1 *是预测* y_i *值为1的候选DC站点集合。这本质上是一个背包问题，可以通过优化求解器解决。然而，对于大型实例，该问题仍可能消耗大量求解时间。因此，我决定采用启发式方法解决这个问题——解决分数背包问题的贪婪算法。具体来说，我们计算每个候选DC站点在*
    I_0 *中的比例* f_i / m_i *，并根据比例对站点进行升序排序。然后我们从第一个站点到最后一个站点逐个选择，直到满足约束条件。完成此过程后，将*
    I_0 *中所选站点的* y_i *值与* I_1 *中的所有站点设置为1，其余站点的* y_i *值设置为0。然后将这种配置输入优化求解器以求解其余的*
    x_ij *变量。
- en: Numerical experiments
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数值实验
- en: I randomly created 800 instances of the facility location problem with 100 candidate
    DC sites and 200 customers. I used 560 instances of them for training and 240
    instances for testing. The non-commercial solver SCIP was adopted for solving
    the instances. The code for generating the instances is given below.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我随机创建了800个设施选址问题实例，包含100个候选DC站点和200个客户。我使用了560个实例进行训练，240个实例进行测试。非商业求解器SCIP被用于解决这些实例。生成实例的代码如下。
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Then we read in the solutions and create the dataset used for training the transformer.
    Note that here I converted the cartesian coordinate system into polar coordinate
    system when building the dataset, as the latter one leads to higher accuracy for
    the transformer.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们读取解决方案并创建用于训练变换器的数据集。请注意，在构建数据集时，我将笛卡尔坐标系转换为极坐标系，因为后者能提高变换器的准确性。
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Then we define the architecture of the transformer. Here the embedding vectors
    are 256-dimensional, the number of heads in the multi-head attention mechanism
    is 8, and the number of encoder blocks is 3\. I didn’t experiment with many hyper-parameter
    settings as this setting already achieved satisfactory accuracy. More hyper-parameter
    tuning can be done to further increase the accuracy of the transformer.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义变换器的架构。这里的嵌入向量是256维的，多头注意力机制中的头数是8，编码器块的数量是3。我没有实验太多的超参数设置，因为这个设置已经达到了令人满意的准确性。可以进行更多的超参数调整，以进一步提高变换器的准确性。
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Then we train and test the transformer with the dataset created previously.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用先前创建的数据集对变换器进行训练和测试。
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: After training, the transformer achieved approximately 92% accuracy on the test
    set containing 240 instances. Considering the test set is not imbalanced (46.57%
    and 53.43% of the labels in the test set are 0 and 1 respectively) and our transformer
    is not always predicting the same class, 92% accuracy should be a satisfactory
    score.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练后，变压器在包含240个实例的测试集上达到了约92%的准确率。考虑到测试集没有不平衡（测试集中的标签0和1分别占46.57%和53.43%），并且我们的变压器并不是总是预测相同的类别，92%的准确率应该是一个令人满意的分数。
- en: Finally, we test the transformer aided solution approach for the facility location
    problem vs solving the MILP directly using the SCIP solver only. The main purpose
    is to prove that the transformer helps reduce a lot of solving time without a
    significant deterioration in the solution quality.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们测试了变压器辅助解决方案方法与仅使用SCIP求解器直接求解MILP的比较。主要目的是证明变压器有助于减少大量的求解时间，而不会显著恶化解决方案质量。
- en: I created another 100 random unseen instances with *m_i, f_i, d_j* parameters
    following the same distribution as those instances used for training the transformer,
    and applied each solution approach to them. I tested for 3 cases with different
    numbers of candidate DC sites and customers, namely (n_dc=100,n_cus=200), (n_dc=200,n_cus=400),
    (n_dc=400,n_cus=800). The code is given below.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我创建了另外100个随机未见的实例，参数为*m_i, f_i, d_j*，这些实例遵循与用于训练变压器的实例相同的分布，并对每种解决方案方法进行应用。我测试了3种情况，不同的候选DC站点和客户数量，分别是（n_dc=100,
    n_cus=200）、（n_dc=200, n_cus=400）和（n_dc=400, n_cus=800）。代码如下。
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The average solving time (unit: seconds, on a laptop with Intel(R) Core(TM)
    i7–10750H CPU, 6 cores) and objective value of each solution approach for each
    case are reported in the table below.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 每种解决方案方法在每种情况下的平均求解时间（单位：秒，在配备Intel(R) Core(TM) i7–10750H CPU，6核心的笔记本电脑上）和目标值报告在下表中。
- en: '![](../Images/7e7206c42444311c29c69e97b2353154.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7e7206c42444311c29c69e97b2353154.png)'
- en: Test results obtained from the instances with parameters following the same
    distribution as the training set for the transformer (Image by author)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 从参数遵循与变压器训练集相同分布的实例中获得的测试结果（作者提供的图片）。
- en: We can see that for all cases, the transformer aided solution approach consumes
    around 2% of the solving time of the SCIP only solution approach, while the objective
    value deteriorates by only around 1%.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在所有情况下，变压器辅助解决方案方法消耗的时间约为仅使用SCIP解决方案方法的2%，而目标值恶化仅约1%。
- en: So far we have only tested the transformer aided solution approach on instances
    with parameters following the same distribution as the training set. What if we
    apply it to instances with parameters following a different distribution? To test
    this, I generated 100 test instances for (n_dc=100,n_cus=200) with *m_i, f_i,
    d_j* parameters following a normal distribution with double mean and double standard
    deviation, using the code below.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们仅在参数遵循与训练集相同分布的实例上测试了变压器辅助的解决方案方法。如果我们将其应用于参数遵循不同分布的实例会怎么样？为了测试这一点，我生成了100个测试实例，参数为*m_i,
    f_i, d_j*，这些参数遵循双倍均值和双倍标准差的正态分布，使用以下代码。
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The results obtained from these instances are reported in the table below.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些实例中获得的结果报告在下表中。
- en: '![](../Images/d6842de075bc800a3a9cc8bf3edba2c8.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6842de075bc800a3a9cc8bf3edba2c8.png)'
- en: Test results obtained from the instances with parameters following a distribution
    with double mean and standard deviation as compared to the training set for the
    transformer (Image by author)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 从参数遵循双倍均值和标准差的分布的实例中获得的测试结果，与变压器的训练集相比（作者提供的图片）。
- en: We see that the transformer aided solution approach consumes around 2% of the
    solving time of the SCIP only solution approach, while the objective value deteriorates
    by around 3%, a little bit more than the previous test, but still acceptable.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，变压器辅助的解决方案方法消耗的时间约为仅使用SCIP解决方案方法的2%，而目标值恶化约3%，比之前的测试稍高，但仍在接受范围内。
- en: We generate one more set of 100 test instances with *m_i, f_i, d_j* parameters
    following a normal distribution with half mean and standard deviation, using the
    code below.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生成了另一组100个测试实例，参数为*m_i, f_i, d_j*，这些参数遵循半均值和标准差的正态分布，使用以下代码。
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The results are reported in the table below.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 结果报告在下表中。
- en: '![](../Images/bd3081cc3722f32bd78bc6d249f68531.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bd3081cc3722f32bd78bc6d249f68531.png)'
- en: Test results obtained from the instances with parameters following a distribution
    with half mean and standard deviation as compared to the training set for the
    transformer (Image by author)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 从参数分布的实例中获得的测试结果，其分布的均值和标准差为训练集的一半（图像来源：作者）。
- en: We see that the transformer aided solution approach still greatly saves the
    solving time, however, there is also a significant deterioration in solution quality.
    So the transformer is indeed a bit overfitted to the parameter distributions of
    the instances in the training set.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，变压器辅助的解决方法仍然大大节省了解决时间，然而，解质量也有显著下降。因此，变压器确实有些过拟合于训练集中实例的参数分布。
- en: Conclusions
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, I trained a transformer model to help solve a classic MILP
    in supply chain area — facility location problem. The transformer is able to predict
    the correct values the integer variables should take by learning through the solutions
    of hundreds of instances given by SCIP. We then fix the integer variables in the
    SCIP solver to the predicted values given by the transformer model and solve for
    the rest variables. Numerical experiments show that the transformer aided solution
    approach reduces the solving time significantly with only slight deterioration
    in solution quality, when applied to instances with parameters following the same
    or similar distributions.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我训练了一个变压器模型，以帮助解决供应链领域的经典MILP——设施选址问题。通过学习SCIP提供的数百个实例的解决方案，该变压器能够预测整数变量应取的正确值。然后，我们将SCIP求解器中的整数变量固定为变压器模型提供的预测值，并解决其余变量。数值实验表明，当应用于参数遵循相同或类似分布的实例时，变压器辅助的解决方法显著减少了解决时间，同时仅有轻微的解质量下降。
- en: Applying machine learning (ML) to help solve MILPs faster is an emerging research
    topic in both ML and OR communities, as mentioned in [my first article.](https://medium.com/towards-data-science/some-thoughts-on-synergies-between-operations-research-and-machine-learning-921d78ed4bd5)
    Existing research ideas include supervised learning (learning from a solver) and
    reinforcement learning (learning to solve MILPs itself by exploring the solution
    space and observing rewards), etc. The core idea of this article belongs to the
    former class (supervised learning), and takes advantage of the attention mechanism
    in the transformer model to learn making decisions for the integer variables.
    Since the transformer model learns from the solver, the solution quality of the
    transformer aided solution approach can never surpass the solver, but we gain
    a huge saving in the solving time.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 将机器学习（ML）应用于加速MILP求解是ML和OR社区中的新兴研究主题，如[我的第一篇文章](https://medium.com/towards-data-science/some-thoughts-on-synergies-between-operations-research-and-machine-learning-921d78ed4bd5)中提到的。现有的研究思路包括监督学习（从求解器中学习）和强化学习（通过探索解空间和观察奖励来学习解决MILP本身）等。本文的核心思想属于前者（监督学习），并利用变压器模型中的注意力机制来学习为整数变量做决策。由于变压器模型从求解器中学习，变压器辅助的解决方法的解质量永远无法超越求解器，但我们获得了在求解时间上的巨大节省。
- en: Using a pre-trained ML model to generate partial solutions for MILPs could be
    a promising idea to expedite the solving process for large instances in commercial
    applications. Here, I selected the facility location problem as an example of
    MILPs to do experiments, but the same idea applies to other problems, such as
    assignment problems. This approach is best suited for the case where we want to
    repeatedly solve large MILP instances with parameters following the same or similar
    distributions, as the pre-trained ML model tends to be overfitted to the parameter
    distributions in the training instances. One way to improve the robustness of
    this approach may be to include more instances with parameters following more
    diverse distributions to reduce overfitting, and this can be something to explore
    in my future articles.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练的机器学习模型生成MILP的部分解法可能是加快大实例商业应用求解过程的一个有前景的想法。在这里，我选择了设施选址问题作为MILP的示例进行实验，但相同的想法适用于其他问题，如分配问题。此方法最适合用于我们希望重复解决具有相同或类似分布的参数的大型MILP实例的情况，因为预训练的机器学习模型往往会过拟合于训练实例中的参数分布。提高这种方法鲁棒性的一种方式可能是包括更多具有更广泛分布的实例，以减少过拟合，这可以成为我未来文章中的探索方向。
- en: Thanks for reading!
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
