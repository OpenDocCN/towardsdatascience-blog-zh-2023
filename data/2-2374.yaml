- en: Why More Is More (in Artificial Intelligence)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么更多即是更多（在人工智能中）
- en: 原文：[https://towardsdatascience.com/why-more-is-more-in-deep-learning-b28d7cedc9f5](https://towardsdatascience.com/why-more-is-more-in-deep-learning-b28d7cedc9f5)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/why-more-is-more-in-deep-learning-b28d7cedc9f5](https://towardsdatascience.com/why-more-is-more-in-deep-learning-b28d7cedc9f5)
- en: How Large Neural Networks Generalize
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大型神经网络如何泛化
- en: '[](https://manuel-brenner.medium.com/?source=post_page-----b28d7cedc9f5--------------------------------)[![Manuel
    Brenner](../Images/f62843c79a9b378494cb83caf3ddc792.png)](https://manuel-brenner.medium.com/?source=post_page-----b28d7cedc9f5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b28d7cedc9f5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b28d7cedc9f5--------------------------------)
    [Manuel Brenner](https://manuel-brenner.medium.com/?source=post_page-----b28d7cedc9f5--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://manuel-brenner.medium.com/?source=post_page-----b28d7cedc9f5--------------------------------)[![Manuel
    Brenner](../Images/f62843c79a9b378494cb83caf3ddc792.png)](https://manuel-brenner.medium.com/?source=post_page-----b28d7cedc9f5--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b28d7cedc9f5--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b28d7cedc9f5--------------------------------)
    [Manuel Brenner](https://manuel-brenner.medium.com/?source=post_page-----b28d7cedc9f5--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b28d7cedc9f5--------------------------------)
    ·10 min read·Aug 15, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b28d7cedc9f5--------------------------------)
    ·阅读时间10分钟·2023年8月15日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: Less is more.
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 少即是多。
- en: -***Ludwig Mies van der Rohe***
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: -***路德维希·密斯·范·德·罗赫***
- en: Less is more only when more is too much.
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 只有当更多已经太多时，少即是多。
- en: '- ***Frank Loyd Wright***'
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- ***弗兰克·劳德·赖特***'
- en: 'Deep neural networks (DNNs) have profoundly transformed the landscape of machine
    learning, often becoming synonymous with the broader fields of artificial intelligence
    and machine learning. Yet, their rise would have been unimaginable without their
    partner-in-crime: stochastic gradient descent (SGD).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络（DNNs）深刻地改变了机器学习的格局，常常与人工智能和机器学习的广泛领域同义。然而，它们的崛起若没有它们的“搭档”——随机梯度下降（SGD），将是难以想象的。
- en: 'SGD, along with its derivative optimizers, forms the core of many self-learning
    algorithms. At its heart, the concept is straightforward: calculate the task’s
    loss using training data, determine the gradients of this loss in relation to
    its parameters, and then adjust the parameters in a direction that minimizes the
    loss.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: SGD及其衍生优化器构成了许多自学习算法的核心。其核心概念很简单：使用训练数据计算任务的损失，确定相对于其参数的损失梯度，然后调整参数以最小化损失。
- en: 'It sounds simple, but in applications, it has proven to be immensely powerful:
    SGD can find solutions for all kinds of complex problems and training data, given
    it is used in conjunction with a sufficiently expressive architecture. It’s particularly
    good at finding parameter sets that make the network perform perfectly on the
    training data, something called the **interpolation regime**. But under which
    conditions are neural networks thought to **generalize well**, meaning that they
    perform well on unseen test data?'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 听起来很简单，但在应用中，它已经被证明极其强大：SGD可以为各种复杂问题和训练数据找到解决方案，只要它与足够表达力的架构结合使用。它特别擅长找到使网络在训练数据上表现完美的参数集合，这被称为**插值机制**。但在什么条件下，神经网络被认为是**良好泛化**的，即在未见过的测试数据上表现良好？
- en: '![](../Images/fcd15cc481d3711756d2673df3509c41.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fcd15cc481d3711756d2673df3509c41.png)'
- en: The quest to generalize lies at the heart of machine learning. Envisioned by
    DALL-E.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 总体化的追求是机器学习的核心。由DALL-E构想。
- en: 'In some ways, it’s almost too powerful: SGD''s abilities aren’t only limited
    to training data that can be expected to lead to good generalization. It has been
    shown e.g. [in this influential paper](https://arxiv.org/abs/1611.03530), that
    SGD can make a network perfectly memorize a set of images that were randomly labeled
    ([there is a deep relationship between memory and generalization that I have written
    about previously](/memory-and-generalization-in-artificial-intelligence-35e006ca0a9a)).
    Although this might appear challenging — given the mismatch between labels and
    image content — it’s surprisingly straightforward for neural networks trained
    with SGD. In fact, it’s not much more challenging than fitting genuine data.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些方面，这几乎过于强大：SGD的能力不仅限于可以期望带来良好泛化的训练数据。例如，[在这篇有影响力的论文中](https://arxiv.org/abs/1611.03530)已经表明，SGD可以使网络完美记忆一组随机标记的图像（[关于记忆与泛化之间的深层关系，我之前有过相关的讨论](/memory-and-generalization-in-artificial-intelligence-35e006ca0a9a)）。虽然这看起来具有挑战性——考虑到标签与图像内容之间的不匹配——但对于使用SGD训练的神经网络来说，这出奇地简单。实际上，这并不比拟合真实数据更具挑战性。
- en: This ability indicates that NNs, trained with SGD, run the risk of overfitting,
    and measures for regularizing overfitting, such as norms, early stopping, and
    **reducing model size** become crucial to avoid it.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这种能力表明，使用SGD训练的神经网络存在过拟合的风险，避免过拟合的正则化措施，如范数、早期停止和**减少模型大小**变得至关重要。
- en: 'From the point of classical statistics, less is more, and so more is less,
    as summarized concisely in this DALL-E painting:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 从经典统计学的角度来看，少即是多，因此多即是少，如这幅DALL-E画作简洁总结的那样：
- en: '![](../Images/c7ed758643d413a422f2bbdb18d99eec.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c7ed758643d413a422f2bbdb18d99eec.png)'
- en: More is Less, by DALL-E.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 更多即是更少，由DALL-E创作。
- en: '**“Everything should be made as simple as possible, but not simpler.”** *—
    Albert Einstein*'
  id: totrans-20
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**“一切事物都应尽可能简单，但不能更简单。”** *— 阿尔伯特·爱因斯坦*'
- en: 'But here is the rub: one of the most surprising properties of state-of-the-art
    deep learning is that most architectures work best in a **highly overparameterized
    regime, meaning when they have many more parameters than data points**. This is
    surprising because standard learning theories, supported by basic intuitions,
    suggest that with so many parameters, DNNs should just over-adjust to the specific
    training data, and be poor at dealing with new, unseen data.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 但问题在于：最令人惊讶的现代深度学习特性之一是，大多数架构在**高度过参数化的阶段，即参数远多于数据点时**表现最佳。这令人惊讶，因为标准学习理论，基于基本直觉，认为如此多的参数使得深度神经网络（DNNs）会过度调整到特定的训练数据上，从而在处理新的、未见过的数据时表现较差。
- en: 'This is captured nicely in the famous **“double-descent”** curve, popularized
    by the 2019 paper by Belkin et al. “Reconciling modern machine learning practice
    and the bias-variance trade-off”:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这在著名的**“双重下降”**曲线中得到了很好的体现，该曲线由Belkin等人在2019年的论文《调和现代机器学习实践与偏差-方差权衡》中推广。
- en: '![](../Images/4ab908493f79a3909e9e6f2a48d4741d.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ab908493f79a3909e9e6f2a48d4741d.png)'
- en: Classical U-shaped learning curve and double descent observed for high capacity
    function classes. [Taken from Belkin et al., 2019.](https://arxiv.org/pdf/1812.11118.pdf)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到的经典U型学习曲线和双重下降现象。 [摘自Belkin等，2019年。](https://arxiv.org/pdf/1812.11118.pdf)
- en: While in the “classical” regime, increasing capacity leads to a reduction of
    both train and test loss, beyond the sweet spot, test risk increases again, while
    training risk goes to zero (“overfitting regime”).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在“经典”领域，增加容量会导致训练和测试损失的减少，但在最佳点之后，测试风险会再次增加，而训练风险降至零（“过拟合阶段”）。
- en: 'But modern machine learning algorithms have pushed beyond this threshold: moving
    to the modern interpolating regime with highly over-parameterized models, test
    risk decreases again, even falling below levels that were feasible in the classical
    regime, achieving unheard-of generalization.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 但现代机器学习算法已经突破了这一门槛：移动到现代插值阶段，使用高度过参数化的模型，测试风险再次降低，甚至降到经典阶段无法达到的水平，实现了前所未有的泛化能力。
- en: The miracle of the appropriateness of the language of mathematics for the formulation
    of the laws of physics is a wonderful gift which we neither understand nor deserve.
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 数学语言在物理法则表述中的适用性奇迹，是我们既无法理解也不配拥有的美妙礼物。
- en: '***Eugene Wigner***'
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***尤金·维根***'
- en: The fact that neural networks generalize, even with their inherent propensity
    to overfit, indicates the presence of what is generally known as an **inductive
    bias**. An inductive bias refers to the (usually unspoken) set of assumptions
    that a learning algorithm makes to predict outputs for unseen data points, based
    on the data it was trained on. Much like with our often unconscious (and often
    rather problematic) cognitive biases, the inductive bias of the architecture determines
    **its preference for certain solutions**, which guides it when there’s uncertainty
    or multiple solutions are possible.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络能够泛化，即便其固有的过拟合倾向，表明存在所谓的**归纳偏差**。归纳偏差指的是学习算法在预测未见数据点的输出时，基于其训练数据所作的（通常未言明的）一系列假设。就像我们常常无意识的（且常常相当麻烦的）认知偏见一样，架构的归纳偏差决定了**它对某些解决方案的偏好**，在不确定性或存在多种可能解决方案时引导它。
- en: 'As it turns out, we are very lucky: much research points towards the fact that
    DNNs trained with SGD have an inductive bias towards functions that **generalize
    well** (on structured data, which counts for most of the data we are interested
    in applying DNNs to). This is among the set of surprises that make deep learning
    work so well, discussed by Terrence Sejnowski as [“The unreasonable effectiveness
    of deep learning in artificial intelligence”](https://www.pnas.org/doi/10.1073/pnas.1907373117),
    echoing Eugene Wigner’s famous paper from six decades prior on the [unreasonable
    effectiveness of mathematics in the natural sciences](https://en.wikipedia.org/wiki/The_Unreasonable_Effectiveness_of_Mathematics_in_the_Natural_Sciences)
    that we “neither understand nor deserve”.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，我们非常幸运：许多研究指出，使用SGD训练的DNNs对函数具有**良好的泛化能力**（在结构化数据上，这也是我们在应用DNNs时最关心的数据）。这是一系列使深度学习如此有效的惊喜之一，特伦斯·塞伊诺斯基在[“深度学习在人工智能中的非理性有效性”](https://www.pnas.org/doi/10.1073/pnas.1907373117)中讨论了这一点，回响了尤金·维根的六十年前关于[数学在自然科学中的非理性有效性](https://en.wikipedia.org/wiki/The_Unreasonable_Effectiveness_of_Mathematics_in_the_Natural_Sciences)的著名论文，我们“既无法理解也不配拥有”。
- en: “It is a profound and necessary truth that the deep things in science are not
    found because they are useful; they are found because it was possible to find
    them.”
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “深奥的科学真理之所以被发现，并不是因为它们有用；而是因为它们可以被发现。”
- en: '***J. Robert Oppenheimer***'
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***J. 罗伯特·奥本海默***'
- en: This unreasonable effectiveness has enabled many groundbreaking applications
    of deep learning that might have seemed impossible.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这种非理性的有效性使许多看似不可能的深度学习应用得以实现。
- en: 'It might also be something we don’t deserve, or rather, **should not (yet)
    deserve**: given AI’s rapid evolution and the increasing chorus of cautionary
    voices urging a pause in AI research, the success of deep learning has also opened
    Pandora’s box that will likely define the path of mankind in the coming century.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这也可能是我们不配拥有的，或者更确切地说，**（尚未）不该拥有的**：鉴于人工智能的迅速发展和越来越多的警示声音呼吁暂停人工智能研究，深度学习的成功也打开了潘多拉的盒子，这可能会定义未来一个世纪的人类道路。
- en: '![](../Images/dde2c148ab8d78d258af32f164873c9d.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dde2c148ab8d78d258af32f164873c9d.png)'
- en: Opening of Pandora’s box, as envisioned by DALL-E.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: DALL-E 设想的潘多拉盒子的开启。
- en: But, in contrast to the metaphysical question Wigner raised about the underlying
    relationship between mathematics and physics, between mind and reality, the success
    of deep learning is at least something we might be able to grasp.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 但，与维根提出的关于数学与物理、思想与现实之间的基础关系的形而上学问题相比，深度学习的成功至少是我们可以理解的。
- en: While much theoretical work has gone into understanding why neural networks
    generalize so well, theory in deep learning is lagging behind, and there is still
    no clear-cut, universally agreed-upon explanation for why DNNs generalize so well.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管对神经网络为何能如此良好地进行泛化进行了大量理论研究，但深度学习的理论仍滞后，目前尚无明确且普遍接受的解释来说明为何深度神经网络（DNNs）能够如此出色地进行泛化。
- en: Many theoretical proposals, such as the insight that local minima are global
    minima, often hold for only limited architectures, [such as linear neural networks](http://proceedings.mlr.press/v80/laurent18a/laurent18a.pdf)
    or simple non-linear networks. Often, results derived from learning theory can
    be at odds with practical experiences that actually make deep neural networks
    work, such as the existence of many suboptimal local minima in actual deep neural
    networks (explored in this aptly titled paper on [truth or backpropaganda](https://arxiv.org/pdf/1910.00359.pdf)).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 许多理论提议，例如局部最小值是全局最小值的见解，通常只适用于有限的架构，[如线性神经网络](http://proceedings.mlr.press/v80/laurent18a/laurent18a.pdf)或简单的非线性网络。通常，从学习理论中得出的结果可能与实际使深度神经网络有效的实践经验相矛盾，例如实际深度神经网络中存在许多次优的局部最小值（在这篇恰如其分标题的论文中探讨了[真相或反向传播](https://arxiv.org/pdf/1910.00359.pdf)）。
- en: But while there are no universally agreed-upon explanations yet, there are many
    fascinating insights from deep learning theory that can advance our understanding
    and intuitions about why generalization in DNNs works.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管尚未有普遍认可的解释，但深度学习理论中有许多引人入胜的见解可以推进我们对为什么DNNs中的泛化有效的理解和直觉。
- en: 'In the rest of this article, I want to explore a couple of explanatory attempts
    at understanding why ***more is frequently more*** for neural network training
    (a quick note: I will focus on in-distribution generalization, and not address
    the growing and equally exciting field looking at distribution shifts, called
    out-of-distribution/OOD generalization).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文的其余部分，我想探索几种解释尝试，以理解为什么***更多往往意味着更多***对于神经网络训练（简要说明：我将专注于分布内泛化，而不讨论分布转移领域，即被称为分布外/OOD
    泛化的领域）。
- en: 'As mentioned in the beginning, there are roughly speaking, two potential culprits:
    **SGD/optimization** and the **Neural Networks** themselves.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如开头所提，大致有两个潜在的罪魁祸首：**SGD/优化**和**神经网络**本身。
- en: The possibility that SGD might be responsible for the inductive bias that leads
    to good generalization has sparked a wealth of research. This research delves
    into the various mechanisms and optimizers under which these generalization properties
    manifest. Interestingly, among the myriad of optimizer variants available today,
    from SGD to Adam to Adagrad to LBFGS, it appears that the choice of optimizer
    **plays a minimal role in influencing generalization**. [Most, if not all, manage
    to achieve comparable generalization outcomes across diverse tasks](https://www.youtube.com/watch?v=kcVWAKf7UAg).
    And while it has been theorized that the inherently stochastic nature of SGD helps
    models escape from sharp, non-generalizing minima, pushing them towards broader,
    more generalizing regions of the loss landscape, and has a regularizing effect
    on parameters, even full batch SGD (performing training on the whole train data
    at once) generalizes well in many cases.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 认为SGD可能是导致良好泛化的归纳偏差的原因，已经引发了大量研究。这些研究深入探讨了在这些泛化特性显现的各种机制和优化器。值得注意的是，在今天的众多优化器变体中，从SGD到Adam再到Adagrad到LBFGS，选择优化器**对泛化的影响很小**。[大多数，甚至所有，都能在不同任务中实现相当的泛化结果](https://www.youtube.com/watch?v=kcVWAKf7UAg)。虽然有人理论认为，SGD固有的随机性有助于模型逃离尖锐的、不具泛化性的最小值，推动其向损失景观中更广泛、更具泛化性的区域移动，并对参数具有正则化效果，即使是全批次SGD（一次性在整个训练数据上进行训练）在许多情况下也表现良好。
- en: 'This suggests that generalization is perhaps more intertwined with the **characteristics
    of the loss landscape** than with the specific optimizer steering the learning
    algorithm’s journey. It raises an interesting question: how does the generalization
    capability of a particular minimum correlate with the surrounding loss landscape’s
    attributes?'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明泛化可能与**损失景观的特征**更加密切相关，而非特定的优化器引导学习算法的过程。这引出了一个有趣的问题：特定最小值的泛化能力如何与周围损失景观的属性相关联？
- en: 'A popular [explanation that’s been around since the 90s](https://www.bioinf.jku.at/publications/older/3304.pdf)
    posits that **sharp minima tend to generalize poorly compared to their flat counterparts**.
    Intuitively, this relates to how close decision boundaries are to data points
    during classification: In scenarios where generalization is suboptimal and overfitting
    is rampant, decision boundaries are nestled closely to data points, so small changes
    in parameters can lead to sharp changes in loss, since they also change the classification
    of several points at once. In turn, flat minima lead to wide decision margins,
    reminiscent of the margins aimed for in Support Vector Machines.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一个自90年代以来流行的[解释](https://www.bioinf.jku.at/publications/older/3304.pdf)认为，**尖锐的最小值相较于平坦的最小值通常推广效果较差**。直观上，这与分类时决策边界距离数据点的远近有关：在泛化效果不佳和过拟合严重的情况下，决策边界紧贴数据点，因此参数的微小变化可能导致损失的剧烈变化，因为它们也会改变多个点的分类。相反，平坦的最小值会导致较宽的决策边际，类似于支持向量机中的目标边际。
- en: '![](../Images/2e6581d694363be95b7f032eaaea348b.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2e6581d694363be95b7f032eaaea348b.png)'
- en: Rugged loss landscapes with spiky, sharp local minima vs. smoother and wider
    local minima. Smoothness of the loss landscape also depends on training techniques
    and regulaization techniques, such as drop-out. [Adopted from our recent ICML
    paper](https://proceedings.mlr.press/v202/hess23a.html).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 崎岖的损失景观中包含尖锐的局部最小值与较平滑且较宽的局部最小值。损失景观的平滑度也取决于训练技术和正则化技术，如丢弃法。[摘自我们最近的ICML论文](https://proceedings.mlr.press/v202/hess23a.html)。
- en: Since gradient descent is more **likely to run into flat minima** during optimization,
    generalizing solutions are naturally preferred, an effect exacerbated by the usually
    very high-dimensional loss landscapes. Sharp minima become the proverbial *needle
    in the high-dimensional haystack*, while flat minima are, well, the hay in the
    haystack, and given the high-dimensional nature of the problem [**it can be several
    thousands of orders** harder to find a sharp, overfitting minimum than a flat
    one](https://www.youtube.com/watch?v=kcVWAKf7UAg).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 由于梯度下降在优化过程中更**容易遇到平坦的最小值**，因此自然更倾向于泛化的解决方案，这种效果因通常非常高维的损失景观而加剧。尖锐的最小值就像是高维稻草堆中的*针*，而平坦的最小值则是稻草堆中的稻草，考虑到问题的高维性质，[**找到尖锐的过拟合最小值比找到平坦的最小值要困难几个数量级**](https://www.youtube.com/watch?v=kcVWAKf7UAg)。
- en: On top, flatter minima are also simply larger than sharp minima volume-wise,
    and again the counter-intuitive scaling of volume in high-dimensional spaces means
    that this can lead to many orders of magnitude differences in their respective
    volumes.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，平坦的最小值在体积上也比尖锐的最小值大，而高维空间中体积的反直观扩展意味着这可能导致它们各自体积上的数量级差异。
- en: 'Another phenomenon frequently observed in practice can further come in handy:
    [local minima are in practice often thought to be connected by valleys of small
    loss](http://proceedings.mlr.press/v80/draxler18a/draxler18a.pdf), known as “loss
    valleys.” SGD, with its iterative nature, can traverse these valleys and is more
    likely to find and settle in these broader, more generalizable regions.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个在实践中经常观察到的现象可以进一步发挥作用：[局部最小值在实践中通常被认为由小损失的山谷连接](http://proceedings.mlr.press/v80/draxler18a/draxler18a.pdf)，即“损失山谷”。SGD以其迭代性质可以穿越这些山谷，更容易找到并停留在这些更广泛、可泛化的区域。
- en: 'Having discussed SGD and its interplay with loss landscapes, it’s time to think
    about neural networks directly: could it be that randomly initialized deep neural
    networks themselves already exhibit some kind of **inductive bias for “simple/generalizing”
    solutions**, whatever simple means?'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论了SGD及其与损失景观的相互作用后，现在该直接思考神经网络了：随机初始化的深度神经网络本身是否已经表现出某种**“简单/泛化”解的归纳偏差**，无论简单指的是什么？
- en: If simple networks lead to simple solutions, which many of the processes modeled
    by DNNs actually are (i.e. much lower-dimensional than fitting e.g. random noise),
    then the propensity of a neural network to represent a simple solution would already
    give us a lot.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果简单的网络导致简单的解决方案，而DNNs所建模的许多过程实际上是简单的（即，比拟合随机噪声的维度低得多），那么神经网络表现出简单解决方案的倾向将给我们带来很多信息。
- en: A nice way to check whether SGD has much to do with the success of neural networks
    is to see how they would solve problems without being explicitly trained via SGD.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 检查SGD是否与神经网络的成功有很大关系的一个好方法是观察它们在没有明确通过SGD训练的情况下如何解决问题。
- en: A recent paper ([Is SGD a Bayesian sampler? Well, almost.](https://arxiv.org/pdf/2006.15191.pdf))
    proposes that the inductive bias, which nudges DNNs towards “simpler” solutions,
    might not primarily be a special property of SGD, but rather an **inherent characteristic
    of the networks**.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一篇近期论文 ([SGD是贝叶斯采样器吗？差不多。](https://arxiv.org/pdf/2006.15191.pdf)) 提出，促使DNN朝向“简单”解决方案的归纳偏差，可能不仅仅是SGD的特殊属性，而是**网络的固有特征**。
- en: 'The paper’s central premise revolves around two probabilities: P_SGD(f∣S)—
    the likelihood that an overparameterized DNN, trained using SGD or its variants,
    will converge to a function f consistent with a training set S; and P_Bayesian(f∣S)
    — the Bayesian posterior probability that the function f is expressed by a DNN
    when its parameters are randomly initialized, given the training set S.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的核心前提围绕两个概率：P_SGD(f∣S)— 使用SGD或其变体训练的过参数化DNN收敛到一个与训练集S一致的函数f的可能性；以及P_Bayesian(f∣S)—
    给定训练集S，函数f由DNN表示的贝叶斯后验概率，当其参数随机初始化时。
- en: Leaving details aside, the study discovered a significant correlation between
    these two probabilities. More importantly, **the Bayesian sampler showcased a
    strong inclination towards functions with low error and minimal complexity**.
    This finding suggests that the DNNs’ inherent structure and its vastness, rather
    than the specificities of SGD, could be the dominant factors behind their ability
    to generalize in overparameterized settings.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 不讨论细节，这项研究发现这两个概率之间存在显著的相关性。更重要的是，**贝叶斯采样器表现出强烈倾向于具有低错误和最小复杂性的函数**。这一发现表明，DNN的固有结构及其庞大规模，而不是SGD的具体细节，可能是其在过参数化设置中具备良好泛化能力的主导因素。
- en: However, the paper also highlights that while these two probabilities are closely
    related, there still exist nuanced differences sensitive to hyperparameters. This
    means that factors like architecture variations, batch size, learning rate, and
    optimizer choice can still matter a lot in DNN performance.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，论文也指出，尽管这两个概率紧密相关，但仍存在对超参数敏感的细微差异。这意味着架构变化、批量大小、学习率和优化器选择等因素在DNN性能中仍然非常重要。
- en: “Luck is what happens when preparation meets opportunity.”
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “幸运是准备与机会相遇时发生的事情。”
- en: '***— Seneca***'
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '***— 塞内卡***'
- en: 'The fact that architecture could prefer simple solutions also connects nicely
    with another new insight on overparameterized networks: the so-called [Lottery
    Ticket Hypothesis](https://arxiv.org/abs/1803.03635).'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 架构偏好简单解决方案的事实也与过参数化网络的另一个新见解相契合，即所谓的[彩票票据假设](https://arxiv.org/abs/1803.03635)。
- en: It proposes that within large, over-parameterized networks, there exist smaller,
    sparser subnetworks that are primed for success. These “winning tickets”, as they
    are called, are not just lucky finds but are already initialized in a way that
    makes them particularly good at learning the specific task at hand. Within the
    overparameterized model, there exist numerous “winning tickets” or optimal subnetworks.
    These substructures are predisposed to learn efficiently and generalize well.
    The overarching network’s redundancy provides a form of ensemble learning, where
    multiple “good” subnetworks collectively converge towards robust solutions. Furthermore,
    the overabundance of parameters might aid in the exploration of the loss landscape,
    enabling the model to discover and leverage these subnetworks effectively.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 它提出，在大型的过参数化网络中，存在着较小、稀疏的子网络，这些子网络具有成功的潜力。这些被称为“赢票”的网络，不仅仅是幸运的发现，而是已经以一种特别适合学习特定任务的方式初始化。在过参数化模型中，存在着众多的“赢票”或最佳子网络。这些子结构倾向于高效学习和良好的泛化能力。整体网络的冗余性提供了一种集成学习的形式，其中多个“优质”子网络共同收敛到稳健的解决方案。此外，参数的过剩可能有助于探索损失景观，使模型能够有效地发现和利用这些子网络。
- en: 'In my previous article, I have delved into the [Free Lunch Theorem](https://medium.com/towards-data-science/why-there-kind-of-is-free-lunch-56f3d3c4279f),
    and how transfer learning of large pre-trained models gives us something akin
    to free lunch. This connects nicely with the topic of generalization: large pre-trained
    models often exhibit remarkable generalization capabilities, even with limited
    domain-specific data. This phenomenon seemingly defies the traditional bias-variance
    trade-off, echoing the broader theme of overparameterized models generalizing
    well. Models like GPT-4 have shown us that having more parameters when complemented
    by diverse enough data, can indeed lead to models that not only fit the training
    data well but also excel in seemingly novel, unseen scenarios.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在我之前的文章中，我深入探讨了[免费午餐定理](https://medium.com/towards-data-science/why-there-kind-of-is-free-lunch-56f3d3c4279f)，以及大型预训练模型的迁移学习如何给我们提供类似于免费午餐的东西。这与泛化的主题紧密相关：大型预训练模型通常表现出显著的泛化能力，即使在特定领域的数据有限的情况下。这种现象似乎违背了传统的偏差-方差权衡，呼应了过参数化模型良好泛化的更广泛主题。像GPT-4这样的模型向我们展示了，拥有更多参数并辅以足够多样化的数据，确实可以导致模型不仅能够很好地拟合训练数据，而且在看似新颖且未见过的场景中也表现出色。
- en: 'But the quality of the data that is trained on is perhaps the most crucial
    ingredient to generalization. Sam Altman frequently emphasizes that data curation
    was instrumental in the effectiveness of GPT-4\. And “overfitting” on the right
    kind of data can be enough to give you what you are looking for. As Letitia Parcalabescu,
    the host of [AI Coffee Break](https://www.youtube.com/c/aicoffeebreak), said in
    a [recent podcast episode](https://open.spotify.com/episode/2sUYtwQKvLleRshLio4t1g?si=2b8af33b7d3b4bd9)
    we recorded together: “If you overfit on the entire world, you are basically done”.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 但用于训练的数据质量可能是泛化的最关键因素。Sam Altman经常强调数据策划在GPT-4的有效性中发挥了重要作用。而在正确类型的数据上“过拟合”可能足以让你获得所需的结果。正如[AI
    Coffee Break](https://www.youtube.com/c/aicoffeebreak)的主持人Letitia Parcalabescu在我们一起录制的[最新播客集](https://open.spotify.com/episode/2sUYtwQKvLleRshLio4t1g?si=2b8af33b7d3b4bd9)中所说：“如果你在整个世界上过拟合，你基本上就完成了。”
- en: It is likely that large neural networks generalize well for a number of reasons.
    It is something most people did not expect, coming from a decade-old, deeply ingrained
    tradition of statistical thinking about the bias-variance tradeoff in the 20th
    century.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 大型神经网络之所以能够良好泛化，可能有多种原因。这是大多数人没有预料到的，尤其是在20世纪对偏差-方差权衡的深刻传统思维影响下。
- en: But it works. And it’s fascinating that despite all the models and algorithms
    having been developed by humans, we still are not sure what‘s going on. Maybe,
    someday in the future, we will find out. And if we don’t, I’m sure AI algorithms,
    in a quest to know themselves, will.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 但它确实有效。尽管所有的模型和算法都是由人类开发的，但我们仍然不确定发生了什么。也许，未来某天，我们会找到答案。如果我们找不到答案，我相信AI算法在自我探索的过程中会找到的。
- en: Thanks for reading!
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读！
- en: If you like my writing, please [subscribe to get my stories via mail](https://manuel-brenner.medium.com/subscribe),
    or [consider becoming a referred member](https://manuel-brenner.medium.com/membership).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢我的写作，请[订阅以通过邮件获取我的故事](https://manuel-brenner.medium.com/subscribe)，或[考虑成为推荐会员](https://manuel-brenner.medium.com/membership)。
