- en: Data Science Better Practices, Part 1 — Test Your Queries
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据科学最佳实践，第1部分——测试你的查询
- en: 原文：[https://towardsdatascience.com/data-science-better-practices-part-1-test-your-queries-629ad5209f28?source=collection_archive---------5-----------------------#2023-09-07](https://towardsdatascience.com/data-science-better-practices-part-1-test-your-queries-629ad5209f28?source=collection_archive---------5-----------------------#2023-09-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/data-science-better-practices-part-1-test-your-queries-629ad5209f28?source=collection_archive---------5-----------------------#2023-09-07](https://towardsdatascience.com/data-science-better-practices-part-1-test-your-queries-629ad5209f28?source=collection_archive---------5-----------------------#2023-09-07)
- en: How to make sure our queries do what we expect them to — and other future boons.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何确保我们的查询按预期工作——以及其他未来的好处。
- en: '[](https://medium.com/@scf1984?source=post_page-----629ad5209f28--------------------------------)[![Shachaf
    Poran](../Images/ac1ac57b8777c3441ad69358af1d649b.png)](https://medium.com/@scf1984?source=post_page-----629ad5209f28--------------------------------)[](https://towardsdatascience.com/?source=post_page-----629ad5209f28--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----629ad5209f28--------------------------------)
    [Shachaf Poran](https://medium.com/@scf1984?source=post_page-----629ad5209f28--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@scf1984?source=post_page-----629ad5209f28--------------------------------)[![Shachaf
    Poran](../Images/ac1ac57b8777c3441ad69358af1d649b.png)](https://medium.com/@scf1984?source=post_page-----629ad5209f28--------------------------------)[](https://towardsdatascience.com/?source=post_page-----629ad5209f28--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----629ad5209f28--------------------------------)
    [Shachaf Poran](https://medium.com/@scf1984?source=post_page-----629ad5209f28--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F33e74b6a3393&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-science-better-practices-part-1-test-your-queries-629ad5209f28&user=Shachaf+Poran&userId=33e74b6a3393&source=post_page-33e74b6a3393----629ad5209f28---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----629ad5209f28--------------------------------)
    ·11 min read·Sep 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F629ad5209f28&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-science-better-practices-part-1-test-your-queries-629ad5209f28&user=Shachaf+Poran&userId=33e74b6a3393&source=-----629ad5209f28---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F33e74b6a3393&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-science-better-practices-part-1-test-your-queries-629ad5209f28&user=Shachaf+Poran&userId=33e74b6a3393&source=post_page-33e74b6a3393----629ad5209f28---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----629ad5209f28--------------------------------)
    · 11 分钟阅读 · 2023年9月7日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F629ad5209f28&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-science-better-practices-part-1-test-your-queries-629ad5209f28&user=Shachaf+Poran&userId=33e74b6a3393&source=-----629ad5209f28---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F629ad5209f28&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-science-better-practices-part-1-test-your-queries-629ad5209f28&source=-----629ad5209f28---------------------bookmark_footer-----------)![](../Images/639f0db4a613a31f29c317358dc28752.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F629ad5209f28&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdata-science-better-practices-part-1-test-your-queries-629ad5209f28&source=-----629ad5209f28---------------------bookmark_footer-----------)![](../Images/639f0db4a613a31f29c317358dc28752.png)'
- en: Generated with Midjourney
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由 Midjourney 生成
- en: The field of Data Science has its roots in Mathematics and Statistics as well
    as Computer Science. While it has developed considerably in the past few decades,
    it is only in the past 10–15 years that it rose to prominence as a well-established
    role in the organization and as a stand-alone field in the tech industry.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学领域根植于数学和统计学以及计算机科学。尽管在过去几十年中它有了显著的发展，但只有在过去的10到15年里，它才成为组织中的一个成熟角色，并且作为科技行业中的一个独立领域而崛起。
- en: Being a relatively young profession, best practices in Data Science haven’t
    had sufficient time to coalesce and are not well-documented. This is in contrast
    to the tangential field of Software Engineering, which is much more mature and
    is rich in know-how guides, structures, and methodologies that have proven beneficial
    over time.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个相对年轻的职业，数据科学中的最佳实践还没有足够的时间来凝聚，也没有得到很好的记录。这与软件工程这一相对成熟的领域形成对比，后者充满了经验丰富的指南、结构和方法论，经过时间的考验被证明是有益的。
- en: It would be logical to expect that Data Scientists would benefit from the overlap
    and close collaboration with Software Engineers, especially as it relates to practice.
    Unfortunately, it is often not the case as many Data Scientists are either unaware
    of these methodologies or are [unwilling to learn them](https://digma.ai/blog/coding-horrors-refactoring-and-feature-creep/),
    claiming that they are either not relevant or do not lie within their field of
    responsibility.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 从逻辑上讲，数据科学家应从与软件工程师的重叠和紧密合作中受益，特别是在实践方面。不幸的是，情况往往并非如此，因为许多数据科学家要么对这些方法论不了解，要么[不愿意学习它们](https://digma.ai/blog/coding-horrors-refactoring-and-feature-creep/)，声称这些方法论不相关或不在他们的职责范围内。
- en: In this blog series, I would like to share tips, tricks, and systematic approaches
    that may be used in the Data Scientist’s work, with the aim of increasing the
    correctness and stability of our code, better managing our models, and improving
    teamwork.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个博客系列中，我想分享一些数据科学家在工作中可以使用的技巧、窍门和系统性方法，旨在提高我们代码的正确性和稳定性，更好地管理我们的模型，并改善团队合作。
- en: The premise
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前提是
- en: 'We start with a scenario anyone working with big data faces at some point,
    and some of us may even face it on a daily basis:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从每个处理大数据的人都在某个时刻面对的场景开始，有些人可能甚至每天都面临这个问题：
- en: You are using `PySpark` and would like to extract some bits of information from
    a big table. You can never hold the enormous amounts of relevant data in memory
    so you’re forced to do all the transformations, aggregations, joins and whatnots
    within the query language.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你正在使用`PySpark`并希望从一个大表中提取一些信息。你无法在内存中保持大量的相关数据，因此你被迫在查询语言中完成所有的转换、聚合、连接等操作。
- en: You start writing the query and you’re happy about it since `PySpark` makes
    it easy to use a Pythonic and elegant api even when the query is too complicated
    to explain to other humans. Even if you decide to use the SQL interface — you’re
    still typing joyfully.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你开始编写查询，并且对它感到满意，因为`PySpark`使得即使查询太复杂而难以解释给其他人，它也能轻松使用具有Python风格的优雅API。即使你决定使用SQL接口——你仍然愉快地输入着。
- en: Then, you realize that you forgot a key column in a `groupBy` call and go back
    to fix it.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你意识到在`groupBy`调用中遗漏了一个关键列，于是回去修复它。
- en: Then, you realize that one of the window functions is missing an `orderBy` clause.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你意识到一个窗口函数缺少`orderBy`子句。
- en: Then, you decide that this magic number you used in the fourth line should be
    1.25 rather than 1.2.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你决定将第四行中使用的这个神奇数字改为1.25，而不是1.2。
- en: You end up going over these 20–50 lines of the query back and forth repeatedly
    for 20–30 minutes, changing it ever so slightly while building towards the final
    query structure.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你最终会反复地查看这些20到50行的查询，来回调整20到30分钟，稍微修改一下，同时构建最终的查询结构。
- en: Then… You run the query and it **fails**.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然后……你运行查询，它**失败**了。
- en: You once again traverse the rows and rows of code you just manifested, struggling
    to find what logical necessities you’ve missed and spot-fix them one-by-one.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你再次遍历你刚刚生成的代码行，努力找出遗漏的逻辑需求，并逐一修复它们。
- en: Eventually, the query runs and returns some results.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，查询运行并返回了一些结果。
- en: But…
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 但是……
- en: Who’s to promise you that these results indeed reflect what you’ve been trying
    to get all this time, and that it matches the process you’re currently holding
    in your head?
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 谁能保证这些结果确实反映了你一直以来想要的内容，并且与当前你头脑中的过程相匹配？
- en: This is where testing comes to our aid.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这时测试会为我们提供帮助。
- en: Testing?
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试？
- en: 'Yes. What we do is:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 是的。我们做的是：
- en: Hand-craft a small dataset.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 手动制作一个小数据集。
- en: Calculate by hand the results we wish to get with the query.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 手动计算我们希望通过查询获得的结果。
- en: Apply the query we wrote on that small dataset.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在那个小数据集上应用我们编写的查询。
- en: Match the query results with our own calculations.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将查询结果与我们自己的计算结果进行匹配。
- en: If we get a mismatch, we have to fix something — either our manual calculations
    were wrong or the query does not perform what we expect it to. On the other hand,
    if the results match — we have a go to continue to the next step.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们遇到不匹配的情况，我们必须修复一些东西——要么我们的手动计算错误，要么查询没有按预期执行。另一方面，如果结果匹配——我们可以继续到下一步。
- en: I will now take you step-by-step into the structure I use when writing these
    tests.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我将逐步向你展示我在编写这些测试时使用的结构。
- en: Setting up the environment
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置环境
- en: Let’s start with creating the environment (a.k.a. a fixture) we need to work
    with `PySpark`. We may run numerous test cases each run, so we set the `PySpark`
    session up at the module level. Otherwise, we may resort to starting and stopping
    a session for each test, and this has a non-negligible overhead.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从创建我们需要与`PySpark`一起工作的环境（即固定装置）开始。我们每次运行可能会测试许多用例，因此我们在模块级别设置`PySpark`会话。否则，我们可能需要为每个测试启动和停止会话，这会产生不可忽略的开销。
- en: I use Python’s built-in `*unittest*`, however if you or other members of your
    team are using `*pytest*` or `*nose*` or any other testing framework I trust you
    will find a way to perform these actions.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用Python内置的`*unittest*`，但如果你或你的团队其他成员使用`*pytest*`、`*nose*`或任何其他测试框架，我相信你会找到执行这些操作的方法。
- en: '`*unittest*` has the two hooks `*setUpModule*` and `*tearDownModule*` that
    run before and after the tests, respectively. We’ll use those to start and to
    stop our `PySpark` session.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`*unittest*`有两个钩子`*setUpModule*`和`*tearDownModule*`，分别在测试之前和之后运行。我们将使用这些钩子来启动和停止我们的`PySpark`会话。'
- en: test_pyspark.py
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: test_pyspark.py
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'I like my session-creating function to be reusable, so here it is (I’ll fill
    the non-local option when it’s due):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢我的会话创建函数是可重用的，所以这里是它（我会在适当的时候填写非本地选项）：
- en: query_pyspark.py
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: query_pyspark.py
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If the project becomes bigger I’d put this function in a `PySpark`-specific
    utils file, but at the moment let’s keep the project flat and small.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果项目变得更大，我会将这个函数放在`PySpark`专用的工具文件中，但目前我们将项目保持扁平和小巧。
- en: Our very first test
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们的第一个测试
- en: 'The very first thing I’m going to test now is that I actually get a session
    when running this test. Here’s the test:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我现在要测试的第一件事是运行这个测试时是否实际获得了会话。以下是测试：
- en: test_pyspark.py
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: test_pyspark.py
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And what do you know, I run the test (PyCharm lets you do that directly from
    the code, you should see a green “play” button next to each test) and I get an
    OK message:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你瞧，我运行了测试（PyCharm允许你直接从代码中运行，你应该会看到每个测试旁边有一个绿色的“播放”按钮），并且收到了OK消息：
- en: '![](../Images/deb15d316a4af5979cf0cdac24eb8429.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/deb15d316a4af5979cf0cdac24eb8429.png)'
- en: Creating *and* testing our data
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建*和*测试我们的数据
- en: At this point we can start talking about data. What you should have at hand
    is a small dataset that covers different cases you might encounter, and you can
    still handle by hand. In terms of actual size I’d usually vote for 20–50 rows
    depending on the domain and complexity of the query. If there’s grouping involved,
    go for 5–10 different groups.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始讨论数据了。你应该手头有一个小的数据集，涵盖你可能遇到的不同情况，并且仍然可以手动处理。就实际大小而言，我通常建议20–50行，具体取决于领域和查询的复杂性。如果涉及分组，请选择5–10个不同的组。
- en: 'For instructional purposes I created a dataset of names and birth dates. I
    will assume for the sake of simplicity that all individuals with the same last
    name are siblings. I also introduced randomness to the order of the rows, to prevent
    order-sensitive queries from getting the right answer without addressing the order
    directly. The data looks like this:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 出于教学目的，我创建了一个包含名字和出生日期的数据集。为了简单起见，我假设所有具有相同姓氏的个体是兄弟姐妹。我还引入了行顺序的随机性，以防止顺序敏感的查询在不直接处理顺序的情况下获得正确的答案。数据如下所示：
- en: '![](../Images/bba6b4af9486e541502ebcee25aeed16.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bba6b4af9486e541502ebcee25aeed16.png)'
- en: Now it’s time to load the data into our `PySpark` session. But first, let’s
    create a sanity test for it. By the way, creating a test and only afterwards writing
    the code that makes the test pass is part of the Test-Driven Development (TDD)
    methodology, however I don’t preach it for Data Scientists, just the testing part.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候将数据加载到我们的`PySpark`会话中了。但首先，让我们为它创建一个基本的测试。顺便说一下，创建测试然后编写使测试通过的代码是测试驱动开发（TDD）方法的一部分，但我不对数据科学家传授这个方法，只关注测试部分。
- en: For a sanity test we may test for column names, we may test for dataset size,
    we may go for both or we may come up with deeper tests. Heck, we may even write
    a test that matches the CSV file with the `DataFrame` row-by-row.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个理智的测试，我们可以测试列名，测试数据集大小，或者两者都测试，或者我们可以提出更深层次的测试。甚至，我们可以编写一个测试，逐行匹配CSV文件和`DataFrame`。
- en: The stricter we are when writing the tests the more sure we’ll be later that
    the code is correct, but it will also make changes more difficult in the future,
    *e.g.* what if we want to add/change a line in the dataset to test for a specific
    edge case?
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在编写测试时越严格，后来对代码正确性的确信就越强，但这也会使未来的更改更加困难，*例如*，如果我们想在数据集中添加/更改一行以测试特定边界情况怎么办？
- en: Balancing these speed and correctness factors is part of the art rather than
    the science in our line of work, and will come more naturally with time and practice.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的工作中，平衡这些速度和正确性因素更像是一门艺术，而不是科学，这随着时间和实践会变得更加自然。
- en: test_pyspark.py
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: test_pyspark.py
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, let’s write the function to load the data:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们编写加载数据的函数：
- en: query_pyspark.py
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: query_pyspark.py
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: And when I run the test… It fails? But how can it be?
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当我运行测试时……失败了？这怎么可能？
- en: '![](../Images/7ab75221a73cfd8a023056f0e562cbc2.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7ab75221a73cfd8a023056f0e562cbc2.png)'
- en: 'After counting the number of rows again and making sure it’s 25 I end up adding
    `*header=True*` to the code, and the tests pass (don’t worry, I’ll spare you the
    fake drama in the next examples):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在重新计算行数并确保为25后，我最终在代码中添加了`*header=True*`，测试通过了（不用担心，我会在接下来的示例中省略虚假的戏剧性）：
- en: query_pyspark.py
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: query_pyspark.py
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Testing our query(ies)
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试我们的查询
- en: 'Now it’s time for the query-specific test. Let’s say I’d like to get the eldest
    child from each family. I go over the dataset by eye (or use a sorted spreadsheet)
    to find the exact set of name I expect to get, and hard-code it into my test:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是查询特定测试的时间。假设我想从每个家庭中获取最年长的孩子。我查看数据集（或使用排序的电子表格）来找出我期望获得的确切名字集合，并将其硬编码到我的测试中：
- en: test_pyspark.py
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: test_pyspark.py
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'And the code that makes the test pass:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 使测试通过的代码如下：
- en: query_pyspark.py
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: query_pyspark.py
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Despite sparing you the drama, I’ll share that I had to fix the query several
    times before getting the test to pass. For example, I grouped by `*first_name*`,
    I aggregated the values of `*last_name*`, and I forgot to make the sort descending.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我省去了戏剧性的描述，我还是要告诉你，我不得不多次修复查询才能使测试通过。例如，我按`*first_name*`分组，聚合了`*last_name*`的值，并且忘记了进行降序排序。
- en: In my jobs, **testing saved me from losing face numerous times**.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的工作中，**测试让我多次避免了丢脸**。
- en: Are we done? Definitely not.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 完成了吗？绝对没有。
- en: We should think of edge cases like what if we have twins? Are there families
    with no kids? What about null values if our data is not unreliable?
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该考虑边界情况，比如如果有双胞胎呢？是否有没有孩子的家庭？如果数据不可靠，那么空值怎么办？
- en: For each of these options we’ll go to our dataset, change it to produce such
    a case, then update our test and our code.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些选项，我们将转到我们的数据集，将其更改为产生这样的案例，然后更新我们的测试和代码。
- en: If we encounter these special cases through bugs that surface later (even if
    we did not come up with them by ourselves), we’ll do the same — change the dataset
    to reflect these cases and continue from there.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们通过后来出现的错误遇到这些特殊情况（即使我们不是自己提出的），我们也会这样做——更改数据集以反映这些情况，然后从那里继续。
- en: We should also write tests for the other queries, and we’ll meet different kinds
    of tests. In the test above we cared for the **set of results**, but what if we
    want to test a simple 1:1 transformation, *i.e.* `f(row) = y`. We need to factor
    in the non-determinism of Spark in terms of row order.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还应该为其他查询编写测试，并且我们会遇到不同类型的测试。在上面的测试中，我们关心的是**结果集**，但如果我们想测试简单的1:1转换，*即* `f(row)
    = y`，我们需要考虑Spark在行顺序上的非确定性。
- en: For example, let’s say we want to get the initials of the names in our dataset.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们想要获取数据集中名字的首字母。
- en: 'One option would be to sort the `*DataFrame*`and trust this order when we assert
    equalities with our hand-made list:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 一种选择是对`*DataFrame*`进行排序，并在使用我们手工制作的列表进行相等断言时相信这个顺序：
- en: query_pyspark.py
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: query_pyspark.py
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: test_pyspark.py
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: test_pyspark.py
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Another option would be to write a **native** function that does the same work
    and test it well. Then we can apply it to the inputs after we load the results
    to memory, and write a test to assert equality on each row. Here’s an example:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是编写一个**本地**函数来完成相同的工作，并进行充分测试。然后，我们可以在将结果加载到内存后应用它，并编写测试以逐行断言相等。以下是一个示例：
- en: query_pyspark.py
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: query_pyspark.py
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: test_pyspark.py
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: test_pyspark.py
- en: '[PRE11]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Of the two options I would definitely prefer the latter, as it’s more **flexible**
    since it does not rely on the data directly, but by the proxy of the support function
    — which is tested without any coupling to the dataset.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两个选项中，我肯定会选择后者，因为它更**灵活**，因为它不直接依赖数据，而是通过支持函数的代理——这些函数在不与数据集耦合的情况下进行测试。
- en: Of course, if the function is not too heavy for your query you may prefer to
    use it as a UDF, keeping the code complexity low.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果函数对你的查询负载不重，你可以选择将其用作UDF，保持代码复杂性低。
- en: Wait, there’s more?
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 等等，还有更多内容吗？
- en: Of course. There are plenty of different cases, such as results of joins and
    window functions, but I trust the examples above suffice in making the point that
    testing is an important tool and a valid choice of methodology when writing queries,
    even for Data Scientists like me.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当然。有许多不同的情况，比如联接和窗口函数的结果，但我相信上述示例足以说明测试是一种重要工具，是编写查询时的有效方法，即使对像我这样的数据科学家也是如此。
- en: Note, I chose to show how this might work while working with `PySpark` since
    it is a common big-data tool, however this pattern is not restricted to just `PySpark`
    nor to just big-data databases. In fact, it should work with any database. You
    can also use this methodology with in-memory database-ish tools, like `*pandas*`.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我选择展示如何在使用`PySpark`时工作，因为它是一个常见的大数据工具，但这种模式不仅限于`PySpark`或大数据数据库。实际上，它应该适用于任何数据库。你也可以使用这种方法与内存数据库类似的工具，如`*pandas*`。
- en: 'As long as you can:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 只要你能够：
- en: Connect to the data source.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连接到数据源。
- en: Load/mock the data in the data source.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载/模拟数据源中的数据。
- en: Execute the query.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行查询。
- en: Retrieve and process the query result.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检索并处理查询结果。
- en: You’re good to go. If the tool you’re using does not let you perform one of
    these steps, you might want to reconsider using this tool.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 一切准备就绪。如果你使用的工具无法执行这些步骤中的任何一个，你可能需要重新考虑使用这个工具。
- en: And what do you know, there is a hidden benefit to testing your code. Let’s
    say you find that one of your functions is underperforming in terms of runtime
    or memory consumption, and decide to try and optimize or refactor it. You can
    now use your existing tests as assurance that your **new code gives the same output**
    as the previous one. Without these tests I would personally be afraid to change
    even a single line of code, terrified from breaking some dependency downstream.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 而且你会发现，测试你的代码还有一个隐藏的好处。假设你发现你的某个函数在运行时间或内存消耗方面表现不佳，并决定尝试优化或重构它。现在你可以使用现有的测试来保证你的**新代码给出的输出与之前的相同**。没有这些测试，我个人会害怕改动哪怕一行代码，担心会破坏某些下游依赖。
- en: Summary
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Testing is a powerful and important way to assert the correctness of code and
    make any refactoring easy to manage.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 测试是一种强大且重要的方法，可以验证代码的正确性，并使任何重构易于管理。
- en: In future posts, I will give other examples of what I find to be good practices
    when it comes to Data Science. We will touch subjects such as how to work together
    on the same model without stepping on each other’s toes, how to manage dataset
    versions, [how to observe our code’s](https://digma.ai/blog/ci-cd-cf-the-devops-toolchains-missing-link-continuous-feedback/)
    performance in production environments, and much more.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来的帖子中，我将提供更多关于数据科学中良好实践的示例。我们将涉及如何在不相互干扰的情况下协同工作在同一个模型上，如何管理数据集版本，[如何观察我们代码在生产环境中的](https://digma.ai/blog/ci-cd-cf-the-devops-toolchains-missing-link-continuous-feedback/)性能，以及更多内容。
- en: Stay tuned.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 敬请关注。
- en: FAQ
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见问题解答
- en: 'Q: Wait, what?'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 问：等等，什么？
- en: 'A: Feel free to start a conversation here or elsewhere about the notions brought
    in this blog series.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 答：欢迎在这里或其他地方就本博客系列中的概念展开讨论。
- en: 'Q: What if in my query I need thousands of rows to test?'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 问：如果我的查询需要成千上万行来测试呢？
- en: 'A: Write a parametric version of the query, e.g. `*def get_n_smalles_children(family_df,
    n): …*`and make the parameter small enough. Another option is to simulate the
    data programmatically, however this also presents new questions and challenges
    in turn.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '答：编写查询的参数化版本，例如`*def get_n_smalles_children(family_df, n): …*`并使参数足够小。另一个选项是通过编程方式模拟数据，但这也会带来新的问题和挑战。'
- en: 'Q: What if I keep changing the query? Does that mean I need to change the tests
    as well?'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 问：如果我不断更改查询，这是否意味着我也需要更改测试？
- en: 'A: Ideally you won’t change the query over time, but I am aware of the exploratory
    nature of our field. So the answer is Yes. This is one of the reasons you may
    feel that your velocity is reduced when writing tests. However, velocity is weighted
    against accuracy/correctness. You may resort to writing tests later in the process,
    when the query structure is more solidified.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 'A: 理想情况下，你不会随时间改变查询，但我知道我们领域的探索性。因此答案是“是的”。这也是为什么你可能会感觉编写测试时速度变慢的原因之一。然而，速度是与准确性/正确性权衡的。你可能会在后期过程中编写测试，当查询结构更加稳定时。'
- en: 'Q: How do I run the tests if I don’t use PyCharm?'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 'Q: 如果我不使用PyCharm，怎么运行测试？'
- en: 'A: Add the magic lines below to the end of your tests file and run it with
    `*python test_pyspark.py*`. Don’t forget to make sure the code root is included
    in `PYTHONPATH` for the imports to work (PyCharm does that automatically).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 'A: 在测试文件的末尾添加以下魔法行，然后使用`*python test_pyspark.py*`运行它。别忘了确保代码根目录被包含在`PYTHONPATH`中，以便导入能够正常工作（PyCharm会自动处理这个问题）。'
- en: '[PRE12]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Q: What if I don’t want to (or otherwise can’t) keep my data in a .csv?'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 'Q: 如果我不想（或者无法）将数据保存在.csv文件中怎么办？'
- en: 'A: Any way of storing and loading the data that works for you is fine, just
    try to keep it tidy. For very small datasets I’ve used dict-to-DataFrame (or if
    you’d like json-to-DataFrame), for larger datasets I used tables permanently stored
    on Hadoop.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 'A: 任何对你有效的数据存储和加载方式都可以，只要尽量保持整洁即可。对于非常小的数据集，我使用了dict-to-DataFrame（或者如果你愿意，可以使用json-to-DataFrame），对于较大的数据集，我使用了永久存储在Hadoop上的表。'
- en: 'Q: Aren’t the example functions you gave above, like, super simple?'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 'Q: 你给出的示例函数不是非常简单吗？'
- en: 'A: Well, yes. That’s how I approach teaching — by giving simple examples and
    making them gradually more complex. Unfortunately the margins of this post are
    too small to contain the latter part.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 'A: 是的。这就是我教授的方式——通过给出简单的例子，并逐渐使其变得更加复杂。不幸的是，这篇文章的边距太小，无法容纳后面的部分。'
- en: 'Q: Do you have the code above in some repository so I can use it as a reference?'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 'Q: 你有没有将上述代码存储在某个仓库中，以便我可以用作参考？'
- en: 'A: [Yes, yes I do.](https://github.com/scf1984/clean-data-science/tree/main/pyspark_tests)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 'A: [是的，我确实这样做了。](https://github.com/scf1984/clean-data-science/tree/main/pyspark_tests)'
