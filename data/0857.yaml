- en: Teaching CLIP Some Fashion
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 教学 CLIP 时尚
- en: 原文：[https://towardsdatascience.com/teaching-clip-some-fashion-3005ac3fdcc3?source=collection_archive---------3-----------------------#2023-03-07](https://towardsdatascience.com/teaching-clip-some-fashion-3005ac3fdcc3?source=collection_archive---------3-----------------------#2023-03-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/teaching-clip-some-fashion-3005ac3fdcc3?source=collection_archive---------3-----------------------#2023-03-07](https://towardsdatascience.com/teaching-clip-some-fashion-3005ac3fdcc3?source=collection_archive---------3-----------------------#2023-03-07)
- en: Training FashionCLIP, a domain-specific CLIP model for Fashion
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练 FashionCLIP，一个专门用于时尚的 CLIP 模型
- en: '[](https://fede-bianchi.medium.com/?source=post_page-----3005ac3fdcc3--------------------------------)[![Federico
    Bianchi](../Images/fa38ff2051af04df7803af7d84c5cd4d.png)](https://fede-bianchi.medium.com/?source=post_page-----3005ac3fdcc3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3005ac3fdcc3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3005ac3fdcc3--------------------------------)
    [Federico Bianchi](https://fede-bianchi.medium.com/?source=post_page-----3005ac3fdcc3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://fede-bianchi.medium.com/?source=post_page-----3005ac3fdcc3--------------------------------)[![Federico
    Bianchi](../Images/fa38ff2051af04df7803af7d84c5cd4d.png)](https://fede-bianchi.medium.com/?source=post_page-----3005ac3fdcc3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3005ac3fdcc3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3005ac3fdcc3--------------------------------)
    [Federico Bianchi](https://fede-bianchi.medium.com/?source=post_page-----3005ac3fdcc3--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2aff872fe60e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fteaching-clip-some-fashion-3005ac3fdcc3&user=Federico+Bianchi&userId=2aff872fe60e&source=post_page-2aff872fe60e----3005ac3fdcc3---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3005ac3fdcc3--------------------------------)
    ·10 min read·Mar 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3005ac3fdcc3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fteaching-clip-some-fashion-3005ac3fdcc3&user=Federico+Bianchi&userId=2aff872fe60e&source=-----3005ac3fdcc3---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F2aff872fe60e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fteaching-clip-some-fashion-3005ac3fdcc3&user=Federico+Bianchi&userId=2aff872fe60e&source=post_page-2aff872fe60e----3005ac3fdcc3---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3005ac3fdcc3--------------------------------)
    ·10分钟阅读·2023年3月7日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3005ac3fdcc3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fteaching-clip-some-fashion-3005ac3fdcc3&user=Federico+Bianchi&userId=2aff872fe60e&source=-----3005ac3fdcc3---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3005ac3fdcc3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fteaching-clip-some-fashion-3005ac3fdcc3&source=-----3005ac3fdcc3---------------------bookmark_footer-----------)![](../Images/34cea25c9ba0b5d71769a94173d2ba36.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3005ac3fdcc3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fteaching-clip-some-fashion-3005ac3fdcc3&source=-----3005ac3fdcc3---------------------bookmark_footer-----------)![](../Images/34cea25c9ba0b5d71769a94173d2ba36.png)'
- en: Photo by [Domenico Loia](https://unsplash.com/@domenicoloia?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/hGV2TfOh0ns).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Domenico Loia](https://unsplash.com/@domenicoloia?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，发布在 [Unsplash](https://unsplash.com/photos/hGV2TfOh0ns) 上。
- en: This is a short blog post describing [FashionCLIP](https://doi.org/10.1038/s41598-022-23052-9).
    If you are a data scientist you probably have to deal with both images and text.
    However, your data will be very specific to your domain, and standard models might not work well.
    This post explains how **domain-specific** vision and language models can be used
    in a **domain-specific** setting and why using them can be a promising way to
    create a search engine or a (zero-shot) classifier.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一篇简短的博客文章，描述了 [FashionCLIP](https://doi.org/10.1038/s41598-022-23052-9)。如果你是数据科学家，你可能需要处理图像和文本。然而，你的数据将非常特定于你的领域，标准模型可能效果不佳。本文解释了如何在**领域特定**的环境中使用**领域特定**的视觉和语言模型，以及为何使用这些模型可能是创建搜索引擎或（零样本）分类器的一个有前景的方式。
- en: 'FashionCLIP is a new vision and language model for the fashion industry and
    supports practitioners in solving two tasks:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: FashionCLIP 是一种用于时尚行业的新视觉和语言模型，支持从业者解决两个任务：
- en: '**Categorization**: zero-shot classification of product images;'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类**：产品图像的零样本分类；'
- en: '**Search**: efficient retrieval of products given a query.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**搜索**：根据查询高效检索产品。'
- en: While FashionCLIP is the result of many people working hard, this blog post
    is mainly my summary and my personal view of the amazing experience I had while
    building this, and does not necessarily represent the view of all other authors
    and their organizations.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 FashionCLIP 是许多人努力工作的结果，这篇博客文章主要是我在构建过程中获得的惊人经验的总结和个人观点，并不一定代表所有其他作者及其组织的观点。
- en: Models
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型
- en: 'We currently release the model in two different formats:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们目前以两种不同的格式发布模型：
- en: '[Our Internal Wrapper](https://github.com/patrickjohncyh/fashion-clip)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[我们的内部封装](https://github.com/patrickjohncyh/fashion-clip)'
- en: '[HuggingFace Weights](https://huggingface.co/patrickjohncyh/fashion-clip)'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[HuggingFace 权重](https://huggingface.co/patrickjohncyh/fashion-clip)'
- en: We also have a [colab tutorial](https://colab.research.google.com/drive/1Z1hAxBnWjF76bEi9KQ6CMBBEmI_FVDrW?usp=sharing)
    that goes over most of the things you can do with FashionCLIP.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有一个 [colab 教程](https://colab.research.google.com/drive/1Z1hAxBnWjF76bEi9KQ6CMBBEmI_FVDrW?usp=sharing)
    介绍了使用 FashionCLIP 可以做的大部分事情。
- en: Introduction
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Fashion is one of those industries that can benefit the most from AI products.
    Indeed, due to the nature of the domain, the existence of different catalogs,
    and client-specific datasets it is often difficult to build solutions that can
    be applied seamlessly to different problems.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 时尚是可以从 AI 产品中受益最多的行业之一。实际上，由于领域的性质、不同的目录和客户特定的数据集，通常很难构建可以无缝应用于不同问题的解决方案。
- en: 'Imagine two data scientists at a major fashion company: Mary, and Luis. The
    two have to deal with an ever-changing system, and its operations require constant
    care:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下在一家大型时尚公司工作的两位数据科学家：Mary 和 Luis。他们必须应对不断变化的系统，其操作需要持续的关注：
- en: 'Mary is building a ***product classifier***to help with categorization at scale:
    her model takes a product and selects one among a list of categories (shoes, dress,
    etc.);'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mary 正在构建一个 ***产品分类器*** 以帮助大规模分类：她的模型接收一个产品并从一系列类别中选择一个（鞋子、连衣裙等）；
- en: 'Luis is working on ***product matching*** to improve the search experience:
    his model takes a query in one of the supported languages (e.g., “a red dress”),
    and gives back a list of products matching the query.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luis 正在研究 ***产品匹配*** 以改善搜索体验：他的模型接受一种支持的语言中的查询（例如，“一件红色连衣裙”），并返回匹配该查询的产品列表。
- en: 'As every practitioner knows, any new model in production brings to life a complex
    life cycle and somehow brittle dependencies:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 正如每个从业者所知道的，任何新的生产模型都会带来复杂的生命周期和某种程度的脆弱依赖：
- en: Mary’s model needs to be constantly re-trained as inventory grows and categories
    shift;
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着库存的增长和类别的变化，Mary 的模型需要不断重新训练；
- en: Luis’ model depends on the quality of product meta-data.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luis 的模型依赖于产品元数据的质量。
- en: '**Same company, different use-cases, different models.**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**同一公司，不同用例，不同模型。**'
- en: '**What if there was another way?**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果有另一种方法呢？**'
- en: Today we try to take a step forward, showing how we can build a general model
    for Fashion data. We describe FashionCLIP a fine-tuned version of the famous [CLIP](https://openai.com/blog/clip/)
    model, tailored to treat Fashion data. Our recent paper on [FashionCLIP](https://doi.org/10.1038/s41598-022-23052-9)
    has been published in Nature Scientific Reports.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 今天我们尝试向前迈出一步，展示如何构建一个用于时尚数据的通用模型。我们描述了 FashionCLIP，它是著名的 [CLIP](https://openai.com/blog/clip/)
    模型的微调版本，专门处理时尚数据。我们最近的关于 [FashionCLIP](https://doi.org/10.1038/s41598-022-23052-9)
    的论文已在《自然科学报告》中发布。
- en: Chia, P.J., Attanasio, G., Bianchi, F. *et al.* **Contrastive language and vision
    learning of general fashion concepts**. *Sci Rep* **12**, 18958 (2022). [https://doi.org/10.1038/s41598-022-23052-9](https://doi.org/10.1038/s41598-022-23052-9)
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Chia, P.J., Attanasio, G., Bianchi, F. *等* **一般时尚概念的对比语言与视觉学习**。*Sci Rep* **12**,
    18958 (2022)。 [https://doi.org/10.1038/s41598-022-23052-9](https://doi.org/10.1038/s41598-022-23052-9)
- en: FashionCLIP came to life through a collaboration with [*Farfetch*](https://www.farfetch.com/),
    a giant (and real) luxury e-commerce traded on the NYSE. FashionCLIP is a joint
    work with people from both industry (Coveo, Farfetch) and academia (Stanford,
    Bocconi, Bicocca). Model weights are available online in [HuggingFace format](https://huggingface.co/patrickjohncyh/fashion-clip).
    An example of usage can be found on [Patrick’s repo](https://github.com/patrickjohncyh/fashion-clip/blob/master/fashion_clip_api_demo.ipynb).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: FashionCLIP的诞生源于与[*Farfetch*](https://www.farfetch.com/)的合作，这是一家在纽约证券交易所上市的巨大（且真实的）奢侈品电商。FashionCLIP是与来自业界（Coveo、Farfetch）和学术界（斯坦福、博科尼、比科卡）的人们共同完成的工作。模型权重可以在线获得，格式为[HuggingFace](https://huggingface.co/patrickjohncyh/fashion-clip)。使用示例可以在[Patrick的repo](https://github.com/patrickjohncyh/fashion-clip/blob/master/fashion_clip_api_demo.ipynb)中找到。
- en: We will first go over the use case and explain some more in-depth details of
    the model. Finally, we will share the code we have been using to train the model
    and how to access the weights.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先介绍用例，并解释一些模型的更深入细节。最后，我们将分享我们用来训练模型的代码以及如何获取权重。
- en: 'FashionCLIP: The Story'
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'FashionCLIP: 故事'
- en: 'FashionCLIP is a general model to embed images of fashion products and their
    description in the same vector space: each image and each product will be represented
    by a single dense vector.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: FashionCLIP是一个通用模型，用于将时尚产品的图像及其描述嵌入到同一个向量空间中：每个图像和每个产品将由一个单独的稠密向量表示。
- en: Why are we putting them in the same vector space? **So that they can be compared.**
    This principle is the key to the success of a model like CLIP.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们要把它们放在同一个向量空间中？ **这样它们才能进行比较。** 这个原则是像CLIP这样的模型成功的关键。
- en: 'FashionCLIP is derived from the original CLIP. The idea is pretty straightforward.
    If you take:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: FashionCLIP源自原始的CLIP。这个想法非常简单。如果你：
- en: A ton of images with captions;
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大量带有标题的图像；
- en: An image encoder (this could be a CNN or ViT);
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个图像编码器（这可以是CNN或ViT）；
- en: A text encoder (this could be a transformers-based language model).
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个文本编码器（这可以是基于transformers的语言模型）。
- en: You can train a model (with a contrastive loss) to put the embedding of an image
    close to its caption embedding and far from irrelevant captions. In the GIF you
    show an example in 2 dimensions. The concept generalizes to N dimensions.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以训练一个模型（使用对比损失）来使图像的嵌入接近其标题嵌入，并远离不相关的标题。在GIF中，你展示了一个二维的例子。这个概念可以推广到N维。
- en: '![](../Images/d12f8e4672fad8eedc508ddc8bf324ab.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d12f8e4672fad8eedc508ddc8bf324ab.png)'
- en: FashionCLIP embeds descriptions and images in the same vector space. This is
    useful for zero-shot classification and image retrieval. *Image by the author
    using Farfetch catalog.*
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: FashionCLIP将描述和图像嵌入到同一个向量空间中。这对于零-shot分类和图像检索非常有用。*图片由作者使用Farfetch目录提供。*
- en: 'The end result is a **multi-modal space**, allowing you to move between visual
    and textual interactions using novel images and novel text descriptions: if you
    have some text, you can retrieve *corresponding images* (as in product search);
    if you have some images, you can *rank* captions based on semantic similarity
    (as in classification).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果是一个**多模态空间**，允许你在视觉和文本交互之间移动，使用新的图像和新的文本描述：如果你有一些文本，你可以检索到*对应的图像*（如产品搜索）；如果你有一些图像，你可以*排序*标题基于语义相似性（如分类）。
- en: To fine-tune CLIP, you need a good dataset. We jointly worked with Farfetch
    to train CLIP with high-quality images and captions. The dataset (soon to be openly
    released) comprises more than 800K samples.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 要微调CLIP，你需要一个好的数据集。我们与Farfetch合作，使用高质量的图像和标题来训练CLIP。这个数据集（即将公开发布）包含了超过80万的样本。
- en: We train the model for a couple of epochs and check the performance on several
    benchmarks encompassing zero-shot classification, probing, and retrieval. Before
    seeing the results, let’s take a deeper look at what we can do now that we have
    a trained FashionCLIP.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练模型几个周期，并检查在多个基准上的表现，包括零-shot分类、探测和检索。在查看结果之前，让我们深入了解一下现在有了训练好的FashionCLIP后我们可以做什么。
- en: 'We will not delve deeper into CLIP itself. If you want to know more about CLIP,
    I have a dedicated blog post here:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会深入探讨CLIP本身。如果你想了解更多关于CLIP的内容，我这里有一篇专门的博客文章：
- en: '[](/how-to-train-your-clip-45a451dcd303?source=post_page-----3005ac3fdcc3--------------------------------)
    [## How to Train your CLIP'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/how-to-train-your-clip-45a451dcd303?source=post_page-----3005ac3fdcc3--------------------------------)
    [## 如何训练你的CLIP]'
- en: Introduction to CLIP and to how we fine-tuned it for the Italian Language during
    the HuggingFace Community Week.
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 介绍CLIP以及我们如何在HuggingFace社区周期间为意大利语言微调它。
- en: towardsdatascience.com](/how-to-train-your-clip-45a451dcd303?source=post_page-----3005ac3fdcc3--------------------------------)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/how-to-train-your-clip-45a451dcd303?source=post_page-----3005ac3fdcc3--------------------------------)'
- en: 'The two key tasks that FashionCLIP can tackle are:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: FashionCLIP可以处理的两个关键任务是：
- en: Image Retrieval
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像检索
- en: Zero-shot Classification
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 零-shot分类
- en: 'Retrieval: From Text to Image'
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检索：从文本到图像
- en: 'We first move from text to image: we encode a search query (“A red dress”)
    with FashionCLIP text encoder and retrieve the closest image vectors through a
    simple dot product. **The greater the value of the dot product, the more similar
    the text and the image are**. In the GIF below, the search is done on 4 product
    images as an example.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先从文本到图像：我们使用FashionCLIP文本编码器对搜索查询（“一件红色连衣裙”）进行编码，并通过简单的点积检索最接近的图像向量。**点积的值越大，文本和图像之间的相似度越高**。在下面的GIF中，搜索以4个产品图像为例进行。
- en: '![](../Images/6c62314d50e09c9bb8aad4a8e7602a44.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6c62314d50e09c9bb8aad4a8e7602a44.png)'
- en: '*For retrieval, we can pre-compute image embeddings on the target catalog.
    At runtime, we encode the query and rank images through a simple dot product.
    Image by the author using Farfetch catalog.*'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*对于检索，我们可以在目标目录上预先计算图像嵌入。在运行时，我们编码查询并通过简单的点积对图像进行排名。图片由作者使用Farfetch目录提供。*'
- en: 'While “red dress” is a simple query for which the [search engine](https://www.farfetch.com/shopping/women/search/items.aspx?q=red+dress&skipsl=1)
    may not need additional input, things get quickly get interesting with slightly
    more ambiguous queries, such as “light red dress” vs “dark red dress”, in which
    “light” and “dark” are modifiers of the same color:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然“红色连衣裙”是一个简单的查询，[搜索引擎](https://www.farfetch.com/shopping/women/search/items.aspx?q=red+dress&skipsl=1)可能不需要额外的输入，但稍微模糊一些的查询，如“浅红色连衣裙”与“深红色连衣裙”则变得有趣，其中“浅”和“深”是同一颜色的修饰词：
- en: '![](../Images/08c90f64ddd3663a05b31ff1a44d1b7b.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08c90f64ddd3663a05b31ff1a44d1b7b.png)'
- en: '*FashionCLIP helps disambiguate geometric features. Image by the author using
    Farfetch catalog.*'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*FashionCLIP有助于消歧义几何特征。图片由作者使用Farfetch目录提供。*'
- en: 'Even more interesting is FashionCLIP''s ability to capture items *represented
    within clothes*. Product descriptions often fail to explicitly mention figurative
    patterns, FashionCLIP instead is able to recognize printed items, even in a cartoonish-like
    shape, like the *cat* hanging on a bag in the t-shirt below:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 更有趣的是FashionCLIP捕捉到*衣物中代表的物品*的能力。产品描述通常未能明确提及具象图案，FashionCLIP能够识别印刷的物品，即使是类似卡通的形状，如下面T恤上挂着的*猫*：
- en: '![](../Images/489ca1c4a29687e99febe59046b17e22.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/489ca1c4a29687e99febe59046b17e22.png)'
- en: '*FashionCLIP recognizes figurative items printed on t-shirts. Image by the
    author using Farfetch catalog.*'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*FashionCLIP识别印刷在T恤上的具象物品。图片由作者使用Farfetch目录提供。*'
- en: While we have not evaluated this capability in detail, we believe this might
    come from the “knowledge” possessed by the original CLIP, which is partially kept
    during fine-tuning.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们尚未详细评估这一能力，但我们相信这可能来自原始CLIP所具备的“知识”，在微调过程中部分保留。
- en: Of course, information is better encoded in descriptions (e.g., brands are often
    mentioned in the description) than in any semantic nuances FashionCLIP may capture.
    However, its capabilities in augmenting standard learn-to-rank signals without
    behavioral data may greatly improve the search experience, especially for cold-start
    scenarios.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，信息在描述中（例如，品牌通常在描述中提及）比FashionCLIP可能捕获的任何语义细微差别编码得更好。然而，它在增强标准学习排名信号而没有行为数据方面的能力可能大大改善搜索体验，特别是在冷启动场景下。
- en: 'Classification: From Image to Text'
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类：从图像到文本
- en: 'We now go from image to text for classification: we encode the image of a fashion
    item we want to classify with FashionCLIP’s image encoder and retrieve the closest
    label vectors through a dot product:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在从图像到文本进行分类：我们使用FashionCLIP的图像编码器对要分类的时尚物品图像进行编码，并通过点积检索最接近的标签向量：
- en: '![](../Images/335ba8d48b19e52d28ffb351596aa5c1.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/335ba8d48b19e52d28ffb351596aa5c1.png)'
- en: '*For zero-shot classification, we compute the image embeddings of the query
    item and the text embedding of the target labels. Image by the author using Farfetch
    catalog.*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*对于零-shot分类，我们计算查询项的图像嵌入和目标标签的文本嵌入。图片由作者使用Farfetch目录提供。*'
- en: The trick of CLIP-like models is treating labels not as categorical variables,
    but as semantically meaningful labels.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP-like模型的技巧在于将标签视为语义上有意义的标签，而不是类别变量。
- en: In other words, when “classifying”, we are asking the question “which of these
    texts is the best caption for this image?”.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，当我们“分类”时，我们在问“这些文本中哪个是这个图像的最佳标题？”。
- en: 'Thanks to CLIP pre-training and the infinite possibilities of natural language,
    we now have a classifier that is not confined to *any specific set of labels,
    categories, or attributes*: while, of course, the first application could be using
    this classifier on new products in the Farfetch catalog, we can re-use the same
    model on other datasets with different labels or purposes, e.g.:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 得益于 CLIP 的预训练和自然语言的无限可能性，我们现在拥有一个不局限于*任何特定标签、类别或属性*的分类器：当然，首要应用可能是在 Farfetch
    目录中的新产品上使用该分类器，我们还可以在具有不同标签或用途的其他数据集上重复使用相同的模型，例如：
- en: if a supplier doesn’t categorize shoes as “high-heel shoes” vs “flat shoes”,
    we can add that attribute;
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果供应商没有将鞋子分类为“高跟鞋”与“平底鞋”，我们可以添加该属性；
- en: If merchandisers are creating new views on the catalog — for example, matching
    items to *styles* — we can classify existing products according to new dimensions
    (“elegant”, “streetwear”, etc.).
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果商品管理员在目录中创建新的视图——例如，将项目匹配到*风格*——我们可以根据新的维度（“优雅”、“街头风”等）对现有产品进行分类。
- en: 'The generalization abilities of CLIP come of course at the expense of *some
    precision*: that is, if we train a new classifier in a supervised fashion to solve
    the use cases above, they all will be a bit better than FashionCLIP. As usual,
    there is no one-size fits all with real-world ML, and the trade-off between one
    model or many can be assessed in different ways depending on the importance of
    the use case, training time, labeling costs, etc.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP 的泛化能力当然是以*某些精度*为代价的：也就是说，如果我们以监督方式训练一个新的分类器来解决上述用例，它们都会比 FashionCLIP 更好。像往常一样，真实世界的机器学习没有一刀切的方案，模型之间的权衡可以根据用例的重要性、训练时间、标注成本等不同方式进行评估。
- en: '**Performance**'
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**性能**'
- en: We compare FashionCLIP to CLIP on two different tasks on various datasets. More
    details about the setup are found in the paper, the scope of this section is just
    to show that there is a boost in performance when using FashionCLIP in place of
    CLIP for fashion-related tasks.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在两个不同任务和多个数据集上将 FashionCLIP 与 CLIP 进行比较。有关设置的更多细节请参阅论文，本节的范围只是为了展示在时尚相关任务中使用
    FashionCLIP 替代 CLIP 时性能的提升。
- en: For Zero-Shot Classification we use three different datasets (KAGL, DEEP, and
    FMNIST) that should serve as out-of-distribution datasets (we know from other
    experiments that we work much better than CLIP on in-domain data, but this is
    expected).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于零样本分类，我们使用了三个不同的数据集（KAGL、DEEP 和 FMNIST），这些数据集应作为分布外数据集（我们知道从其他实验中我们在领域内数据上表现比
    CLIP 好得多，但这是预期中的）。
- en: '![](../Images/0e48f853336913b2ae8c0da04d530ac8.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0e48f853336913b2ae8c0da04d530ac8.png)'
- en: Weighted Macro F1 score on different datasets (out-of-domain data). FashionCLIP
    shows a significant improvement over CLIP on these datasets.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 不同数据集上的加权宏 F1 分数（领域外数据）。FashionCLIP 在这些数据集上显示出相对于 CLIP 的显著提升。
- en: Zero-shot results confirm that our model works as expected!
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Zero-shot 结果确认我们的模型表现如预期！
- en: 'For Image Retrieval, we use a portion of the original dataset that we left
    out during training. Note that this obviously gives us an advantage with respect
    to CLIP as this dataset is going to be in-domain for us. However, it is still
    an interesting experiment. The following results confirm that our model is best:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像检索，我们使用了在训练时遗漏的原始数据集的一部分。需要注意的是，这显然使我们相对于 CLIP 有优势，因为这个数据集对于我们来说是领域内的。然而，这仍然是一个有趣的实验。以下结果确认我们的模型表现最佳：
- en: '![](../Images/4f374e654dfa0c33699501fff4429f67.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f374e654dfa0c33699501fff4429f67.png)'
- en: Precision at 5 and at 10 on our internal test set (in-domain data). FashionCLIP
    has a much better retrieval performance.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们内部测试集上的前 5 和前 10 精度（领域内数据）。FashionCLIP 的检索性能明显更好。
- en: Torch Implementation and HuggingFace weights
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Torch 实现和 HuggingFace 权重
- en: Thanks to Patrick’s work, FashionCLIP is very easy to use. You can simply load
    the model and run zero-shot classification with a simple method, all with python!
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Patrick 的工作，FashionCLIP 使用起来非常简单。你只需加载模型并使用简单的方法进行零样本分类，所有这些都可以用 Python 完成！
- en: '[PRE0]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: And you can also do image retrieval!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以进行图像检索！
- en: '[PRE1]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Farewell
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 告别
- en: The Conclusion of a Long Journey
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 漫长旅程的总结
- en: Building FashionCLIP has been a long and fun adventure with old and new friends
    from some of the coolest places on earth. The results always taste better when
    you get them with your friends. Also, some of us have been working for years together
    and actually never met in real life!
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 构建 FashionCLIP 是一段长时间且有趣的冒险，涉及到来自地球上一些最酷地方的老朋友和新朋友。结果总是更美好，当你和朋友一起获得它们时。此外，我们中的一些人已经合作多年，实际上从未在现实生活中见过面！
- en: 'On a more pragmatic note, we hope that FashionCLIP can open up unprecedented
    opportunities for companies quickly iterating in internal and external fashion
    use cases: for example, while you may end up building a devoted style classifier,
    using FashionCLIP for your proof of concept will go a long way in proving the
    value of the feature *without investing upfront in a new model life-cycle support*.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 从更务实的角度来看，我们希望 FashionCLIP 能为快速迭代内部和外部时尚用例的公司开辟前所未有的机会：例如，虽然你可能会最终构建一个专注的风格分类器，但使用
    FashionCLIP 进行概念验证将大大证明该功能的价值，*而无需在新的模型生命周期支持上进行前期投资*。
- en: 'When we consider the growing number of SaaS players for intelligent APIs in
    retail — Coveo, Algolia, Bloomreach — the importance of vertical models cannot
    be underestimated: since B2B companies grow with accounts, robustness, and re-usability
    matter more than pure precision. We envision a near future in which FashionCLIP
    — *and DIYCLIP, ElectronicsCLIP, etc*. — will be a standard component of B2B Machine
    Learning players, enabling quick iteration, data standardization, and economies
    of scale on a completely different level than what has been possible so far.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们考虑零售领域日益增长的智能 API SaaS 服务提供商——如 Coveo、Algolia、Bloomreach——时，垂直模型的重要性不可低估：由于
    B2B 公司以账户为基础增长，稳健性和可重用性比纯粹的精准度更为重要。我们展望不久的将来，FashionCLIP —— *以及 DIYCLIP、ElectronicsCLIP
    等* —— 将成为 B2B 机器学习参与者的标准组件，使得迭代迅速、数据标准化，并在完全不同于目前的水平上实现规模经济。
- en: 'I also gave a talk last year at Pinecone about FashionCLIP:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我去年也在 Pinecone 上做了一个关于 FashionCLIP 的演讲：
- en: The talk I gave at Pinecone about building models like FashionCLIP.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我在 Pinecone 上关于如何构建像 FashionCLIP 这样的模型的演讲。
- en: An Additional Demo
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另一个演示
- en: 'What’s the power of Open Source? [Pablo](https://www.linkedin.com/in/pablomendes/)
    saw the model and reached out with a UI to help us test the difference between
    the standard HuggingFace CLIP vs the FashionCLIP we just released — I then used
    [Objective Search](https://www.objective.inc) to test the search using FashionCLIP
    with a couple of queries (see it for yourself [here](https://www.objective.inc/demos/fashion-clip)):'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 开源的力量是什么？[Pablo](https://www.linkedin.com/in/pablomendes/) 看到这个模型并联系了我们，提供了一个用户界面来帮助我们测试标准的
    HuggingFace CLIP 与我们刚刚发布的 FashionCLIP 之间的差异——然后我使用了 [Objective Search](https://www.objective.inc)
    来测试使用 FashionCLIP 的几个查询（您可以[在这里](https://www.objective.inc/demos/fashion-clip)亲自查看）：
- en: '![](../Images/14f32da68e1f2c2d972fc31f78f7409b.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/14f32da68e1f2c2d972fc31f78f7409b.png)'
- en: Using FashionCLIP for search. GIF by author, images from the H&M dataset.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 FashionCLIP 进行搜索。GIF 由作者提供，图片来自 H&M 数据集。
- en: Cool, isn’t it?
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 很酷，不是吗？
- en: Limitations, Bias, and Fairness
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局限性、偏见与公平性
- en: 'We acknowledge certain limitations of FashionCLIP and expect that it inherits
    certain limitations and biases present in the original CLIP model. We do not expect
    our fine-tuning to significantly augment these limitations: we acknowledge that
    the fashion data we use makes explicit assumptions about the notion of gender
    as in “blue shoes for a woman” that inevitably associate aspects of clothing with
    specific people.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们承认 FashionCLIP 存在某些限制，并预计它继承了原始 CLIP 模型中的一些局限性和偏见。我们不期望我们的微调会显著增加这些限制：我们承认，我们使用的时尚数据对性别的概念做出了明确假设，例如“女性的蓝色鞋子”，这不可避免地将服装的某些方面与特定的人联系在一起。
- en: Our investigations also suggest that the data used introduces certain limitations
    in FashionCLIP. From the textual modality, given that most captions derived from
    the Farfetch dataset are long, we observe that FashionCLIP may be more performant
    in longer queries than shorter ones.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的调查还表明，所使用的数据在 FashionCLIP 中引入了某些限制。从文本模态来看，鉴于大多数来自 Farfetch 数据集的标题较长，我们观察到
    FashionCLIP 在处理较长查询时可能比短查询表现更好。
- en: From the image modality, FashionCLIP is also biased towards standard product
    images (centered, white background). This means that the model might underperform
    on images that do not have the same structure.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 从图像模态来看，FashionCLIP 对标准产品图像（居中、白色背景）也存在偏见。这意味着模型可能在不具备相同结构的图像上表现不佳。
- en: More Things We Did
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们做的更多事情
- en: FashionCLIP has been a long journey, but there are a couple of things we did
    while we waited for the official release.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: FashionCLIP 的发展经历了漫长的过程，但在等待正式发布期间我们做了一些事情。
- en: GradedRecs
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GradedRecs
- en: We built on top of our work in FashionCLIP to explore recommendations by traversing
    the latent space. Check out our [paper](https://aclanthology.org/2022.ecnlp-1.22/)
    if you’re interested!
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 FashionCLIP 的基础上进行了探索，通过遍历潜在空间来研究推荐。如果你感兴趣，请查看我们的 [论文](https://aclanthology.org/2022.ecnlp-1.22/)！
- en: '![](../Images/da83f43146bfb0a834a9359af421fe9a.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/da83f43146bfb0a834a9359af421fe9a.png)'
- en: GradedRec. Image by the author.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: GradedRec。图片由作者提供。
- en: Fairness in Recommender System Evaluation
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推荐系统评估中的公平性
- en: If you are interested in related industry tasks, such as recommendations, we
    ran a challenge last year on a well-rounded evaluation of recommender systems.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对相关行业任务感兴趣，例如推荐系统，我们去年进行了一项关于推荐系统全面评估的挑战。
- en: The challenge was meant at understanding how we can build evaluations that are
    not focused only on point-wise metrics (e.g., accuracy). You can find some details
    and an introductory blog post here
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这个挑战旨在理解如何构建不仅仅关注点对点度量（例如准确率）的评估。你可以在这里找到一些细节和介绍性的博客文章
- en: '[](https://fede-bianchi.medium.com/a-rounded-evaluation-of-recommender-systems-b9fa101ef79a?source=post_page-----3005ac3fdcc3--------------------------------)
    [## A Rounded Evaluation of Recommender Systems'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://fede-bianchi.medium.com/a-rounded-evaluation-of-recommender-systems-b9fa101ef79a?source=post_page-----3005ac3fdcc3--------------------------------)
    [## 关于推荐系统的全面评估'
- en: 'EvalRS: Evaluating Recommender Systems on Many Tests'
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: EvalRS：在多个测试中评估推荐系统
- en: fede-bianchi.medium.com](https://fede-bianchi.medium.com/a-rounded-evaluation-of-recommender-systems-b9fa101ef79a?source=post_page-----3005ac3fdcc3--------------------------------)
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: fede-bianchi.medium.com](https://fede-bianchi.medium.com/a-rounded-evaluation-of-recommender-systems-b9fa101ef79a?source=post_page-----3005ac3fdcc3--------------------------------)
