- en: Image Classification with Vision Transformer
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Vision Transformer 进行图像分类
- en: 原文：[https://towardsdatascience.com/image-classification-with-vision-transformer-8bfde8e541d4](https://towardsdatascience.com/image-classification-with-vision-transformer-8bfde8e541d4)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/image-classification-with-vision-transformer-8bfde8e541d4](https://towardsdatascience.com/image-classification-with-vision-transformer-8bfde8e541d4)
- en: How to classify images with the help of Transformer-based model
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何借助基于 Transformer 的模型进行图像分类
- en: '[](https://medium.com/@marcellusruben?source=post_page-----8bfde8e541d4--------------------------------)[![Ruben
    Winastwan](../Images/15ad0dd03bf5892510abdf166a1e91e1.png)](https://medium.com/@marcellusruben?source=post_page-----8bfde8e541d4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8bfde8e541d4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8bfde8e541d4--------------------------------)
    [Ruben Winastwan](https://medium.com/@marcellusruben?source=post_page-----8bfde8e541d4--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@marcellusruben?source=post_page-----8bfde8e541d4--------------------------------)[![Ruben
    Winastwan](../Images/15ad0dd03bf5892510abdf166a1e91e1.png)](https://medium.com/@marcellusruben?source=post_page-----8bfde8e541d4--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8bfde8e541d4--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8bfde8e541d4--------------------------------)
    [Ruben Winastwan](https://medium.com/@marcellusruben?source=post_page-----8bfde8e541d4--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8bfde8e541d4--------------------------------)
    ·13 min read·Apr 13, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8bfde8e541d4--------------------------------)
    ·阅读时长 13 分钟·2023 年 4 月 13 日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/a404a96d42476c650eab290a3eb6734f.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a404a96d42476c650eab290a3eb6734f.png)'
- en: Photo by [drmakete lab](https://unsplash.com/@drmakete?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/photos/hsg538WrP0Y?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[drmakete lab](https://unsplash.com/@drmakete?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    在 [Unsplash](https://unsplash.com/photos/hsg538WrP0Y?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    上的照片'
- en: Since its introduction in 2017, Transformer has been widely recognized as a
    powerful encoder-decoder model to solve pretty much any language modeling task.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 自 2017 年推出以来，Transformer 已被广泛认可为一种强大的编码器-解码器模型，可以解决几乎所有语言建模任务。
- en: BERT, RoBERTa, and XLM-RoBERTa are a few examples of state-of-the-art models
    in language processing that use a stack of Transformer encoders as the backbone
    in their architecture. ChatGPT and the GPT family also use the decoder part of
    Transformer to generate texts. It’s safe to say that almost any state-of-the-art
    model in natural language processing incorporate Transformer in its architecture.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: BERT、RoBERTa 和 XLM-RoBERTa 是在语言处理领域使用 Transformer 编码器堆栈作为其架构基础的一些最先进模型的例子。ChatGPT
    和 GPT 系列也使用 Transformer 的解码器部分来生成文本。可以肯定地说，几乎所有最先进的自然语言处理模型都在其架构中融入了 Transformer。
- en: 'Transformer performance is so good that it seems wasteful not to use it for
    tasks beyond natural language processing, like computer vision for example. However,
    the big question is: can we actually use it for computer vision tasks?'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 的表现非常优秀，以至于不把它用于自然语言处理之外的任务（例如计算机视觉）似乎有些浪费。然而，大问题是：我们能否实际将其用于计算机视觉任务？
- en: It turns out that Transformer also has a good potential to be applied to computer
    vision tasks. In 2020, Google Brain team introduced a Transformer-based model
    that can be used to solve an image classification task called Vision Transformer
    (ViT). Its performance is very competitive in comparison with conventional CNNs
    on several image classification benchmarks.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，Transformer 也具有应用于计算机视觉任务的良好潜力。在 2020 年，Google Brain 团队推出了一种基于 Transformer
    的模型，可以用于解决图像分类任务，称为 Vision Transformer（ViT）。与传统 CNN 在多个图像分类基准上的表现相比，它的表现非常有竞争力。
- en: Therefore, in this article, we’re going to talk about this model. Specifically,
    we’re going to talk about how a ViT model works and how we can fine-tune it on
    our own custom dataset with the help of HuggingFace library for an image classification
    task.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本文中，我们将讨论这个模型。具体来说，我们将讨论 ViT 模型如何工作以及如何利用 HuggingFace 库在我们自己的自定义数据集上对其进行微调，以进行图像分类任务。
- en: So, as the first step, let’s get started with the dataset that we’re going to
    use in this article.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，作为第一步，让我们开始使用本文中将要使用的数据集。
- en: About the Dataset
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于数据集
- en: We will use a snack dataset that you can easily access from `dataset` library
    from HuggingFace. This dataset is listed as having a CC-BY 2.0 license, which
    means that you are free to share and use it, as long as you cite the dataset source
    in your work.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个小吃数据集，该数据集可以从HuggingFace的`dataset`库中轻松访问。该数据集标注为CC-BY 2.0许可证，这意味着你可以自由分享和使用它，只要在你的工作中引用数据集来源即可。
- en: 'Let’s take a sneak peek of this dataset:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来瞧一瞧这个数据集：
- en: '![](../Images/23a1eb2ca0482bdf2de55bfbd9de8b62.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/23a1eb2ca0482bdf2de55bfbd9de8b62.png)'
- en: Subset of images in the dataset
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中图像的子集
- en: 'We only need a few lines of code to load the dataset, as you can see below:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需要几行代码就可以加载数据集，如下所示：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The dataset is a dictionary object that consists of 4898 training images, 955
    validation images, and 952 test images.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集是一个字典对象，由4898张训练图像、955张验证图像和952张测试图像组成。
- en: 'Each image comes with a label, which belongs to one of 20 snack classes. We
    can check these 20 different classes with the following code:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 每张图片都有一个标签，属于20个小吃类别之一。我们可以通过以下代码检查这20种不同的类别：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: And let’s create a mapping between each label and its corresponding index.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来创建一个标签与其对应索引之间的映射。
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: One important thing that we need to know before we move on is the fact that
    each image has varying dimension. Therefore, we need to perform some image preprocessing
    steps before feeding the images into the model for fine-tuning purposes.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我们需要了解的一件重要事情是每张图像的尺寸是不同的。因此，我们需要在将图像输入模型进行微调之前执行一些图像预处理步骤。
- en: Now that we know the dataset that we’re working with, let’s take a closer look
    at ViT architecture.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了正在使用的数据集，让我们更详细地了解ViT架构。
- en: How ViT Works
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ViT的工作原理
- en: Before the introduction of ViT, the fact that a Transformer model relies on
    self-attention mechanism raised a big challenge for us to use it for computer
    vision tasks.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在ViT引入之前，Transformer模型依赖自注意力机制，这给我们在计算机视觉任务中使用它带来了很大的挑战。
- en: The self-attention mechanism is the reason why Transformer-based models can
    differentiate the semantic meaning of a word used in different contexts. For example,
    a BERT model can distinguish the meaning of the word *‘park’* in sentences *‘They
    park their car in the basement’* and *‘She walks her dog in a park’* due to self-attention
    mechanism*.*
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力机制是Transformer模型能够区分一个词在不同上下文中语义的原因。例如，BERT模型能够通过自注意力机制区分词语*‘park’*在句子*‘They
    park their car in the basement’*和*‘She walks her dog in a park’*中的含义。*
- en: 'However, there is one problem with self-attention: it’s a computationally expensive
    operation as it requires each token to attend every other token in a sequence.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，自注意力有一个问题：这是一个计算开销大的操作，因为它要求每个标记关注序列中的每个其他标记。
- en: Now if we use self-attention mechanism on image data, then each pixel in an
    image would need to attend and be compared to every other pixel. The problem is,
    if we increase the pixel value by one, then the computational cost would increase
    quadratically. This is simply not feasible if we have an image with a reasonably
    large resolution.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们在图像数据上使用自注意力机制，那么图像中的每个像素都需要关注并与每个其他像素进行比较。问题是，如果我们将像素值增加一个，那么计算成本将会呈二次增长。如果图像分辨率较大，这显然是不可行的。
- en: '![](../Images/f419838075b136290ec794459e282028.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f419838075b136290ec794459e282028.png)'
- en: Image by author
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'In order to overcome this problem, ViT introduces the concept of splitting
    the input image into patches. Each patch has a dimension of 16 x 16 pixels. Let’s
    say that we have an image with the dimension of 48 x 48 pixels, then the patches
    of our image will look something like this:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，ViT引入了将输入图像拆分为图像块的概念。每个图像块的尺寸为16 x 16像素。假设我们有一张48 x 48像素的图像，那么图像块将会像这样：
- en: '![](../Images/8e9020397bf23d2ba96145c42b30f364.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8e9020397bf23d2ba96145c42b30f364.png)'
- en: Image by author
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: 'In its application, there are two options for how ViT splits our image into
    patches:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，ViT有两种选项来将我们的图像拆分成图像块：
- en: Reshape our input image that has a size of `height x width x channel` into a
    sequence of flattened 2D image patches with a size of `no.of patches x (patch_size^2.channel)`
    . Then, we project the flattened patches into a basic linear layer to get the
    embedding of each patch.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将我们的输入图像（大小为`height x width x channel`）重塑为一个展平的2D图像块序列，大小为`no.of patches x (patch_size^2.channel)`。然后，我们将展平的图像块投影到一个基本的线性层中，以获得每个图像块的嵌入表示。
- en: Project our input image into a convolutional layer with the kernel size and
    stride equal to the patch size. Then, we flatten the output from that convolutional
    layer.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将我们的输入图像投影到卷积层中，卷积核的大小和步幅等于补丁大小。然后，我们将该卷积层的输出展平。
- en: After testing the model performance on several datasets, it turns out that the
    second approach leads to the better performance. Therefore, in this article, we’re
    going to use the second approach.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在对多个数据集测试模型性能后，结果表明第二种方法具有更好的性能。因此，在本文中，我们将使用第二种方法。
- en: Let’s use a toy example to demonstrate the splitting process of an input image
    into patches with a convolutional layer.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个简单的例子来演示如何使用卷积层将输入图像拆分成补丁。
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The next thing that the model will do is flatten the patches and put them sequentially
    as you can see in the image below:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 模型接下来会将补丁展平，并按顺序排列，如下图所示：
- en: '![](../Images/62ca9cce1423a569cb951956a041ac5b.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/62ca9cce1423a569cb951956a041ac5b.png)'
- en: Image by author
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: 'We can do the flattening process with the following code:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码进行展平处理：
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: What we have after the flattening process is basically the vector embedding
    of each patch. This is similar to token embeddings in many Transformer-based language
    models.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在展平处理后得到的基本上是每个补丁的向量嵌入。这类似于许多基于Transformer的语言模型中的标记嵌入。
- en: Next, similar to BERT, ViT will add a special vector embedding for the **[CLS]**
    token in the first position of our patches’ sequence.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，类似于BERT，ViT将在我们补丁序列的第一个位置添加一个特殊的**[CLS]**向量嵌入。
- en: '![](../Images/1b8b3805d82eb7dae37b8609b3019f3c.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b8b3805d82eb7dae37b8609b3019f3c.png)'
- en: Image by author
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you can see, by prepending the **[CLS]** token embedding in the beginning
    of our patch embedding, the length of the sequence increases by one. The final
    step after this would be adding the positional embedding into our sequence of
    patches. This step is important so that our ViT model can learn the sequence order
    of our patches.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，通过在补丁嵌入的开头添加**[CLS]**标记嵌入，序列的长度增加了一个。接下来的最后一步是将位置嵌入添加到我们的补丁序列中。这一步很重要，以便我们的ViT模型可以学习补丁的序列顺序。
- en: This position embedding is a learnable parameter that will be updated by the
    model during the training process.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这个位置嵌入是一个可学习的参数，将在训练过程中由模型更新。
- en: '![](../Images/e0957e29cfbbcebe7099774485b63c5d.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e0957e29cfbbcebe7099774485b63c5d.png)'
- en: Image by author
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, the position embedding plus vector embedding of each patch will be the
    input of a stack of Transformer encoders. The number of Transformer encoders depends
    on the type of ViT model that you use. Overall, there are three types of ViT model:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，每个补丁的位置信息加上向量嵌入将作为一组Transformer编码器的输入。Transformer编码器的数量取决于你使用的ViT模型类型。总体上，有三种类型的ViT模型：
- en: '**ViT-base:** it has 12 layers, hidden size of 768, and the total of 86M parameters.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ViT-base：**它具有12层，隐藏大小为768，总参数量为86M。'
- en: '**ViT-large:** it has 24 layers, hidden size of 1024, and the total of 307M
    parameters.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ViT-large：**它具有24层，隐藏大小为1024，总参数量为307M。'
- en: '**ViT-huge:** it has 32 layers, hidden size of 1280, and the total of 632M
    parameters.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ViT-huge：**它具有32层，隐藏大小为1280，总参数量为632M。'
- en: 'In the following code snippet, let’s say that we want to use **Vit-base**.
    This means that we have 12 layers of Transformer encoders:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码片段中，假设我们想使用**Vit-base**。这意味着我们有12层Transformer编码器：
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Finally, the stack of Transformer encoders will output the final vector representation
    of each image patch. The dimensionality of the final vector corresponds to the
    hidden size of the ViT model that we use.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Transformer编码器堆叠将输出每个图像补丁的最终向量表示。最终向量的维度对应于我们使用的ViT模型的隐藏大小。
- en: '![](../Images/89d85ae70d147091e3f4f9f257855730.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/89d85ae70d147091e3f4f9f257855730.png)'
- en: Image by author
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: And that’s basically it.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这些了。
- en: We can certainly build and train our own ViT model from scratch. However, as
    with other Transformer-based models, ViT requires training on a large amount of
    image data (14M-300M of images) in order for them to generalize well on unseen
    data.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当然可以从头开始构建和训练自己的ViT模型。然而，与其他基于Transformer的模型一样，ViT需要在大量图像数据（14M-300M图像）上进行训练，以便在未见过的数据上具有良好的泛化能力。
- en: If we want to use ViT on a custom dataset, the most common approach is to fine-tune
    a pretrained model. The easiest way to do this is by utilizing HuggingFace library.
    All we have to do is call `ViTModel.from_pretrained()` method and put the path
    to our pretrained model as an argument. The `VitModel()`class from HuggingFace
    will also act as a wrapper of all of steps that we’ve discussed above.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想在自定义数据集上使用ViT，最常见的方法是微调预训练模型。最简单的方法是利用HuggingFace库。我们只需调用`ViTModel.from_pretrained()`方法，并将预训练模型的路径作为参数传递即可。HuggingFace的`VitModel()`类还将作为我们上述所有步骤的封装器。
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The output of the complete ViT model is a vector embedding representing each
    image patch plus the **[CLS]** token. It has the dimension of `[batch_size, image_patches+1,
    hidden_size]`.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 完整ViT模型的输出是一个向量嵌入，表示每个图像补丁加上**[CLS]**标记。其维度为`[batch_size, image_patches+1, hidden_size]`。
- en: To perform an image classification task, we follow the same approach as with
    the BERT model. We extract the output vector embedding of the **[CLS]** token
    and pass it through the final linear layer to determine the class of the image.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行图像分类任务，我们遵循与BERT模型相同的方法。我们提取**[CLS]**标记的输出向量嵌入，并通过最终的线性层来确定图像的类别。
- en: '![](../Images/7eab045d0cbbcc51471c5add96bec3b2.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7eab045d0cbbcc51471c5add96bec3b2.png)'
- en: Image by author
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图片
- en: '[PRE9]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Fine-Tuning Implementation
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调实现
- en: In this section, we will fine-tune a **ViT-base** model that was pre-trained
    on the ImageNet-21K dataset, which consists of approximately 14 million images
    and 21,843 classes. Each image in the dataset has a dimension of 224 x 224 pixels.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将微调一个在ImageNet-21K数据集上进行过预训练的**ViT-base**模型，该数据集包含约1400万张图像和21,843个类别。数据集中的每张图像的尺寸为224
    x 224像素。
- en: To begin, we need to define the checkpoint path for the pre-trained model and
    load the necessary libraries.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要定义预训练模型的检查点路径，并加载必要的库。
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Image Dataloader
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像数据加载器
- en: As previously mentioned, the ViT-base model has been pretrained on a dataset
    consisting of images with the dimension of 224 x 224 pixels. The images have also
    been normalized according to a particular mean and standard deviation in each
    of their color channels.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，ViT-base模型已在包含224 x 224像素尺寸图像的数据集上进行过预训练。这些图像还根据其每个颜色通道的特定均值和标准差进行了归一化。
- en: As a result, before we can feed our own dataset into the ViT model for fine-tuning,
    we must first preprocess our images. This involves transforming each image into
    a tensor, resizing it to the appropriate dimensions, and then normalizing it using
    the same mean and standard deviation values as the dataset on which the model
    was pretrained.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在我们将自己的数据集输入ViT模型进行微调之前，我们必须首先对图像进行预处理。这包括将每张图像转换为张量，将其调整到适当的尺寸，然后使用与模型预训练数据集相同的均值和标准差值进行归一化。
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: From the image dataloader above, we will then get a batch of preprocessed images
    with their corresponding label. We can use the ouput of image dataloader above
    as an input for our model during the fine-tuning process.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的图像数据加载器中，我们将获取一批预处理过的图像及其对应的标签。我们可以使用上述图像数据加载器的输出作为微调过程中模型的输入。
- en: Model Definition
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型定义
- en: The architecture of our ViT model is straightforward. Since we’ll be fine-tuning
    a pretrained model, we can use the `VitModel.from_pretrained()` method and provide
    the checkpoint of the model as an argument.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的ViT模型架构非常简单。由于我们将微调一个预训练模型，我们可以使用`VitModel.from_pretrained()`方法，并提供模型的检查点作为参数。
- en: We also need to add a linear layer at the end, which will act as the final classifier.
    The output of this layer should be equal to the number of distinct labels in our
    dataset.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要在最后添加一个线性层，作为最终的分类器。这个层的输出应该等于数据集中不同标签的数量。
- en: '[PRE12]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The above ViT model generates final vector embeddings for each image patch plus
    the **[CLS]** token. To classify images, as you can see above, we extract the
    final vector embedding of the **[CLS]** token and pass it to the final linear
    layer to obtain the final class prediction.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 上述ViT模型为每个图像补丁和**[CLS]**标记生成最终的向量嵌入。为了对图像进行分类，如上所示，我们提取**[CLS]**标记的最终向量嵌入，并将其传递给最终的线性层以获得最终的类别预测。
- en: Model Fine-Tuning
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型微调
- en: 'Now that we have defined the model architecture and prepared the input images
    for batching process, we can start to fine-tune our ViT model. The training script
    is a standard Pytorch training script, as you can see below:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了模型架构并准备了输入图像用于批处理过程，我们可以开始微调我们的ViT模型。训练脚本是一个标准的Pytorch训练脚本，如下所示：
- en: '[PRE13]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Since our snack dataset has 20 distinct classes, then we’re dealing with a multiclass
    classification problem. Therefore, `CrossEntropyLoss()` would be the appropriate
    loss function. In the example above, we train our model for 10 epochs, learning
    rate is set to be 1e-4, with the batch size of 8\. You can play around with these
    hyperparameters to tune the performance of the model.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的零食数据集有20个不同的类别，因此我们面临的是一个多类分类问题。因此，`CrossEntropyLoss()`将是合适的损失函数。在上面的示例中，我们训练了模型10个周期，学习率设置为1e-4，批量大小为8。你可以调整这些超参数以优化模型的性能。
- en: 'After you trained the model, you will get an output that looks similar as the
    one below:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型后，你将得到一个类似下面的输出：
- en: '![](../Images/ab1e13dc6653a70aed12426bb4094162.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ab1e13dc6653a70aed12426bb4094162.png)'
- en: Image by author
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供
- en: Model Prediction
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型预测
- en: Since we have fine-tuned our model, naturally we want to use it for prediction
    on the test data. To do so, first let’s create a function that encapsulate all
    of the necessary image preprocessing steps and the model inference process.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经微调了模型，自然希望将其用于测试数据的预测。为此，首先创建一个函数来封装所有必要的图像预处理步骤和模型推理过程。
- en: '[PRE14]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As you can see above, the image preprocessing step during inference is exactly
    the same as the step that we did on the training data. Then, we use the transformed
    image as the input to our trained model, and finally we map its prediction to
    the corresponding label.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，上面显示的推理过程中的图像预处理步骤与我们在训练数据上进行的步骤完全相同。然后，我们将变换后的图像作为输入传递给训练好的模型，最后将其预测映射到相应的标签。
- en: If we want to predict a specific image on the test data, we can just call the
    function above and we’ll get the prediction afterwards. Let’s try it out.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想对测试数据中的特定图像进行预测，我们只需调用上述函数，之后我们会得到预测结果。让我们试试看。
- en: '[PRE15]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](../Images/056b21551628c53ba34c5f6d32abd3b5.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/056b21551628c53ba34c5f6d32abd3b5.png)'
- en: Example of test data from the dataset
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的测试数据示例
- en: Our model predicted our test image correctly. Let’s try another one.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型正确预测了我们的测试图像。让我们尝试另一张。
- en: '[PRE16]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](../Images/61fc4b4a831b31060af95251c8425697.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/61fc4b4a831b31060af95251c8425697.png)'
- en: Example of test data from the dataset
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的测试数据示例
- en: And our model predicted the test data correctly again. By fine-tuning a ViT
    model, we can get a good performance on a custom dataset. You can also do the
    same process for any custom dataset in an image classification task.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型再次正确地预测了测试数据。通过微调ViT模型，我们可以在自定义数据集上获得良好的性能。你也可以对任何自定义数据集执行相同的过程，用于图像分类任务。
- en: Conclusion
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we have seen how Transformer can be used not only for language
    modeling tasks, but also for computer vision tasks, which in this case is image
    classification.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们已经看到Transformer不仅可以用于语言建模任务，还可以用于计算机视觉任务，在本例中是图像分类。
- en: To do so, first the input image is decomposed into patches with a size of 16
    x 16 pixels. Then, the Vision Transformer model utilizes a stack of Transformer
    encoders to learn the vector representation of each image patch. Finally, we can
    use the final vector representation of the **[CLS]** token prepended at the beginning
    of image patch sequence to predict the label of our input image.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，首先将输入图像分解成大小为16 x 16像素的补丁。然后，Vision Transformer模型利用一系列Transformer编码器来学习每个图像补丁的向量表示。最后，我们可以使用图像补丁序列开头的**[CLS]**标记的最终向量表示来预测输入图像的标签。
- en: I hope this article is useful for you to get started with Vision Transformer
    model. As always, you can find the code implementation presented in this article
    in [**this notebook**](https://github.com/marcellusruben/medium-resources/blob/main/ViT/Vision_Transformer.ipynb).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这篇文章对你开始使用Vision Transformer模型有所帮助。与往常一样，你可以在[**这个笔记本**](https://github.com/marcellusruben/medium-resources/blob/main/ViT/Vision_Transformer.ipynb)中找到本文中展示的代码实现。
- en: Dataset Reference
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集参考
- en: '[https://huggingface.co/datasets/Matthijs/snacks](https://huggingface.co/datasets/Matthijs/snacks)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://huggingface.co/datasets/Matthijs/snacks](https://huggingface.co/datasets/Matthijs/snacks)'
