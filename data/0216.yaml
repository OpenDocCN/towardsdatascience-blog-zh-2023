- en: How to Estimate the Number of Parameters in Transformer models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何估算 Transformer 模型中的参数数量
- en: 原文：[https://towardsdatascience.com/how-to-estimate-the-number-of-parameters-in-transformer-models-ca0f57d8dff0?source=collection_archive---------3-----------------------#2023-01-13](https://towardsdatascience.com/how-to-estimate-the-number-of-parameters-in-transformer-models-ca0f57d8dff0?source=collection_archive---------3-----------------------#2023-01-13)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-estimate-the-number-of-parameters-in-transformer-models-ca0f57d8dff0?source=collection_archive---------3-----------------------#2023-01-13](https://towardsdatascience.com/how-to-estimate-the-number-of-parameters-in-transformer-models-ca0f57d8dff0?source=collection_archive---------3-----------------------#2023-01-13)
- en: An inside look at the Transformer Encoder/Decoder building blocks
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入了解 Transformer 编码器/解码器的构建模块
- en: '[](https://medium.com/@andimid?source=post_page-----ca0f57d8dff0--------------------------------)[![Dmytro
    Nikolaiev (Dimid)](../Images/4121156b9c08ed20e7aa620712a391d9.png)](https://medium.com/@andimid?source=post_page-----ca0f57d8dff0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ca0f57d8dff0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ca0f57d8dff0--------------------------------)
    [Dmytro Nikolaiev (Dimid)](https://medium.com/@andimid?source=post_page-----ca0f57d8dff0--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@andimid?source=post_page-----ca0f57d8dff0--------------------------------)[![Dmytro
    Nikolaiev (Dimid)](../Images/4121156b9c08ed20e7aa620712a391d9.png)](https://medium.com/@andimid?source=post_page-----ca0f57d8dff0--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ca0f57d8dff0--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ca0f57d8dff0--------------------------------)
    [Dmytro Nikolaiev (Dimid)](https://medium.com/@andimid?source=post_page-----ca0f57d8dff0--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F97b5279dad26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-estimate-the-number-of-parameters-in-transformer-models-ca0f57d8dff0&user=Dmytro+Nikolaiev+%28Dimid%29&userId=97b5279dad26&source=post_page-97b5279dad26----ca0f57d8dff0---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ca0f57d8dff0--------------------------------)
    ·10 min read·Jan 13, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fca0f57d8dff0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-estimate-the-number-of-parameters-in-transformer-models-ca0f57d8dff0&user=Dmytro+Nikolaiev+%28Dimid%29&userId=97b5279dad26&source=-----ca0f57d8dff0---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F97b5279dad26&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-estimate-the-number-of-parameters-in-transformer-models-ca0f57d8dff0&user=Dmytro+Nikolaiev+%28Dimid%29&userId=97b5279dad26&source=post_page-97b5279dad26----ca0f57d8dff0---------------------post_header-----------)
    发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----ca0f57d8dff0--------------------------------)
    ·10分钟阅读·2023年1月13日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fca0f57d8dff0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-estimate-the-number-of-parameters-in-transformer-models-ca0f57d8dff0&user=Dmytro+Nikolaiev+%28Dimid%29&userId=97b5279dad26&source=-----ca0f57d8dff0---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fca0f57d8dff0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-estimate-the-number-of-parameters-in-transformer-models-ca0f57d8dff0&source=-----ca0f57d8dff0---------------------bookmark_footer-----------)![](../Images/fb985f7db85197f007d20523d048a8ba.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fca0f57d8dff0&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-estimate-the-number-of-parameters-in-transformer-models-ca0f57d8dff0&source=-----ca0f57d8dff0---------------------bookmark_footer-----------)![](../Images/fb985f7db85197f007d20523d048a8ba.png)'
- en: Preview. Image by Author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 预览。图像由作者提供
- en: Thanks to [Regan Yue](https://medium.com/u/84ea27feb7de?source=post_page-----ca0f57d8dff0--------------------------------),
    you can read the Chinese version of this article at [mp.weixin.qq.com](https://mp.weixin.qq.com/s/-zAqPfR-RVeRYjX5raehtA),
    [juejin.cn](https://juejin.cn/post/7243435843145924667), [segmentfault.com](https://segmentfault.com/a/1190000043888810)
    and [xie.infoq.cn](https://xie.infoq.cn/article/3d1edc1049f34d0f797231c2c)!
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 感谢[Regan Yue](https://medium.com/u/84ea27feb7de?source=post_page-----ca0f57d8dff0--------------------------------)，你可以在[mp.weixin.qq.com](https://mp.weixin.qq.com/s/-zAqPfR-RVeRYjX5raehtA)、[juejin.cn](https://juejin.cn/post/7243435843145924667)、[segmentfault.com](https://segmentfault.com/a/1190000043888810)
    和[xie.infoq.cn](https://xie.infoq.cn/article/3d1edc1049f34d0f797231c2c)阅读这篇文章的中文版本！
- en: The most effective way to understand new machine learning architecture (as well
    as any new technology in general) is to *implement it from scratch*. This is the
    best approach that helps you understand the implementation *down to the smallest
    details*, although it is very complex, time-consuming, and sometimes **just impossible**.
    For example, if you do not have similar computing resources or data you will not
    be able to make sure that there is no hidden bug in your solution.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 理解新的机器学习架构（以及一般的新技术）的最有效方式是*从零开始实现它*。这是一种最佳方法，有助于你理解实现的*每一个细节*，尽管这非常复杂、耗时，并且有时**简直不可能**。例如，如果你没有类似的计算资源或数据，你将无法确保你的解决方案中没有隐藏的错误。
- en: However, there is a much easier way — to count the number of parameters. It’s
    not much harder than just reading the paper, but allows you to dig quite deep
    and check that you fully understand the building blocks of the new architecture
    (Transformer Encoder and Decoder blocks in our case).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，还有一种更简单的方法 —— 计算参数数量。这并不比仅仅阅读论文更困难，但它允许你深入挖掘，检查你是否完全理解新架构的构建模块（在我们的例子中是 Transformer
    编码器和解码器模块）。
- en: You can think about this with the following diagram, which presents three ways
    to understand a new ML architecture —*the size of the circle represents the level
    of understanding*.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下图示来思考这个问题，该图示展示了理解新机器学习架构的三种方法 —— *圆圈的大小表示理解的程度*。
- en: '![](../Images/19e581dc0cc02ea2ec3094601facb28b.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/19e581dc0cc02ea2ec3094601facb28b.png)'
- en: Ways to understand ML architecture. Calculating the number of parameters is
    not much more difficult than simply reading the paper, but it will allow you to
    delve deeper into the topic. Image by Author
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 理解机器学习架构的方法。计算参数数量并不比简单阅读论文更困难，但它会让你更深入地了解这个话题。图片由作者提供
- en: In this article, we will take a look at the famous Transformer architecture
    and consider **how to calculate the number of parameters** in PyTorch [*TransformerEncoderLayer*](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html)
    and [*TransformerDecoderLayer*](https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html)classes.
    Thus, we will make sure that there are no mysteries left for us about what this
    architecture consists of.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将查看著名的 Transformer 架构，并考虑**如何计算** PyTorch [*TransformerEncoderLayer*](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html)和[*TransformerDecoderLayer*](https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html)类中的参数数量。这样，我们可以确保对这个架构的组成没有任何神秘之处。
- en: TL;DR
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TL;DR
- en: All formulas are summarized in the **Conclusions** section. You are welcome
    to take a look at them right now.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 所有公式总结在**结论**部分。欢迎你现在就查看它们。
- en: I present not only exact formulas but also their less accurate *approximate
    versions*, which will allow you to **quickly estimate** the number of parameters
    in any Transformer-based model.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我不仅展示了确切的公式，还展示了它们的不那么准确的*近似版本*，这将帮助你**快速估算**任何基于 Transformer 的模型中的参数数量。
- en: Transformer Architecture
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer 架构
- en: The famous Transformer architecture was presented in the breathtaking [“Attention
    Is All You Need” paper](https://arxiv.org/abs/1706.03762) in 2017 and became the
    de-facto standard in the majority of Natural Language Processing and Computer
    Vision tasks because of its ability to effectively capture long-range dependencies.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 著名的 Transformer 架构在 2017 年的震撼[“Attention Is All You Need” 论文](https://arxiv.org/abs/1706.03762)中首次提出，并因其有效捕捉长期依赖关系的能力而成为大多数自然语言处理和计算机视觉任务的事实标准。
- en: Now, in early 2023, [diffusion](https://techcrunch.com/2022/12/22/a-brief-history-of-diffusion-the-tech-at-the-heart-of-modern-image-generating-ai/)
    is gaining extreme popularity, mainly due to [text-to-image generative models](https://www.washingtonpost.com/technology/interactive/2022/ai-image-generator/).
    Maybe, soon **they** will become the new state-of-the-art in various tasks, as
    it was with Transformers vs LSTMs and CNNs. But let’s take a look at Transformers
    first…
  id: totrans-21
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 现在，在 2023 年初，[扩散](https://techcrunch.com/2022/12/22/a-brief-history-of-diffusion-the-tech-at-the-heart-of-modern-image-generating-ai/)正在获得极大的关注，主要是由于[文本到图像生成模型](https://www.washingtonpost.com/technology/interactive/2022/ai-image-generator/)。也许，很快**它们**将成为各种任务中的新一代最先进技术，正如之前
    Transformers 与 LSTMs 和 CNNs 的对比。但让我们先来看一下 Transformers…
- en: My article is not an attempt to explain Transformer architecture since there
    are enough articles that do it very well. It just might allow you *to look at
    it from a different side* or *clarify some details* if you haven’t fully figured
    it out yet. So if you are seeking more resources to learn about this architecture,
    I’m referring you to some of them; otherwise, you can just keep reading.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我的文章并不是试图解释Transformer架构，因为已经有足够多的文章做得很好。它可能会让你*从不同的角度看待它*，或*澄清一些细节*，如果你还没有完全弄清楚的话。因此，如果你想了解更多关于这个架构的资源，我会推荐一些；否则，你可以继续阅读。
- en: Resources to know more about Transformer
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解更多关于Transformer的资源
- en: 'If you are looking for a more detailed Transformer architecture overview, take
    a look at these materials (note that there are plenty of other recourses on the
    Internet, I just personally like these):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要更详细的Transformer架构概述，可以看看这些资料（请注意，互联网上有很多其他资源，我个人只是喜欢这些）：
- en: First of all, [the official paper](https://arxiv.org/abs/1706.03762). It may
    be not the best way for the first time, but it is not as complex as it seems.
    You can try [Explainpaper to help you read this](https://www.explainpaper.com/papers/attention)
    or [other papers](https://www.explainpaper.com/) (*it is an AI-based tool that
    can explain the text you highlight*).
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，[官方论文](https://arxiv.org/abs/1706.03762)。虽然这可能不是第一次阅读的最佳方式，但它并不像看起来那么复杂。你可以尝试[Explainpaper来帮助你阅读](https://www.explainpaper.com/papers/attention)或者[其他论文](https://www.explainpaper.com/)（*这是一个基于AI的工具，可以解释你高亮的文本*）。
- en: Great [Illustrated Transformer article by Jay Alammar](https://jalammar.github.io/illustrated-transformer/).
    If you do not enjoy the reading, watch the [YouTube video](https://youtu.be/-QH8fRhqFHM)
    by the same author.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 极好的[Jay Alammar的插图Transformer文章](https://jalammar.github.io/illustrated-transformer/)。如果你不喜欢阅读，可以看看同一作者的[YouTube视频](https://youtu.be/-QH8fRhqFHM)。
- en: Awesome [Tensor2Tensor talk by Lukasz Kaiser from Google Brain](https://www.youtube.com/watch?v=rBCqOTEfxvg).
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精彩的[Google Brain的Lukasz Kaiser讲解的Tensor2Tensor讲座](https://www.youtube.com/watch?v=rBCqOTEfxvg)。
- en: If you want to go straight to practice and use various Transformer models to
    build real applications, check the [Hugging Face course](https://huggingface.co/course/chapter1/1).
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你想直接进行实践并使用各种Transformer模型来构建实际应用，可以查看[Hugging Face课程](https://huggingface.co/course/chapter1/1)。
- en: Original Transformer
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 原始Transformer
- en: To begin with, let’s remember Transformer basics.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们回顾一下Transformer的基础。
- en: 'The architecture of the Transformer consists of two components: the encoder
    (on the left) and the decoder (on the right). The encoder takes a sequence of
    input tokens and produces a sequence of hidden states, while the decoder takes
    this sequence of hidden states and produces a sequence of output tokens.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer的架构由两个组件组成：编码器（左侧）和解码器（右侧）。编码器接收一系列输入标记并生成一系列隐藏状态，而解码器则接收这些隐藏状态并生成一系列输出标记。
- en: '![](../Images/e05a6b25c3b4e8beaa9d36c81fa96568.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e05a6b25c3b4e8beaa9d36c81fa96568.png)'
- en: Transformer architecture. Figure 1 from the [public domain paper](https://arxiv.org/pdf/1706.03762.pdf)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer架构。图1来自[公共领域论文](https://arxiv.org/pdf/1706.03762.pdf)
- en: Both the encoder and decoder consist of a stack of identical layers. For the
    encoder, this layer includes ***multi-head attention*** *(1 — here, and later
    numbers refer to the image below)* and a ***feed-forward neural network*** *(2)*
    with some ***layer normalizations*** *(3)* and skip connections.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器和解码器都由一堆相同的层组成。对于编码器，这一层包括***多头注意力*** *(1 — 这里和后续数字指的是下图)* 和一个***前馈神经网络***
    *(2)*，以及一些***层归一化*** *(3)* 和跳跃连接。
- en: The decoder is similar to the encoder, but in addition to the *first* ***multi-head
    attention*** *(4)* (which is *masked* for machine translation task so the decoder
    doesn’t cheat by looking at the future tokens) and a ***feedforward network (5)***,
    it also has a *second* ***multi-head attention*** *mechanism (6)*. Itallows the
    decoder to use the context provided by the encoder when generating output. As
    the encoder, the decoder also has some ***layer normalization*** *(7)* and skip
    connections components.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器类似于编码器，但除了*第一个* ***多头注意力*** *(4)*（在机器翻译任务中进行*掩蔽*，以防止解码器通过查看未来标记来作弊）和一个***前馈网络
    (5)***外，它还具有*第二个* ***多头注意力*** *机制 (6)*。它允许解码器在生成输出时使用编码器提供的上下文。与编码器一样，解码器也有一些***层归一化***
    *(7)* 和跳跃连接组件。
- en: '![](../Images/3698ec4a74fda435a95004c2a20e00d0.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3698ec4a74fda435a95004c2a20e00d0.png)'
- en: Transformer architecture with signed components. Adapted from figure 1 from
    the [public domain paper](https://arxiv.org/pdf/1706.03762.pdf)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 带有符号组件的 Transformer 架构。改编自 [公共领域论文](https://arxiv.org/pdf/1706.03762.pdf) 的图
    1。
- en: I will not consider the input embedding layer with positional encoding and final
    output layer (linear + softmax) as Transformer components, focusing only on Encoder
    and Decoder blocks. I do so since these components are specific to the task and
    embedding approach, while both Encoder and Decoder stacks formed the basis of
    many other architectures later.
  id: totrans-38
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我将不考虑带有位置编码的输入嵌入层和最终输出层（线性 + softmax）作为 Transformer 组件，只关注编码器和解码器模块。我这样做是因为这些组件特定于任务和嵌入方法，而编码器和解码器堆栈后来形成了许多其他架构的基础。
- en: ''
  id: totrans-39
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Examples of such architectures include BERT-based models for the Encoder ([BERT](https://arxiv.org/abs/1810.04805),
    [RoBERTa](https://arxiv.org/abs/1907.11692), [ALBERT](https://arxiv.org/abs/1909.11942),
    [DeBERTa](https://arxiv.org/abs/2006.03654), etc.), GPT-based models for the Decoder
    ([GPT](https://paperswithcode.com/paper/improving-language-understanding-by),
    [GPT-2](https://paperswithcode.com/paper/language-models-are-unsupervised-multitask),
    [GPT-3](https://arxiv.org/abs/2005.14165v4), [ChatGPT](https://openai.com/blog/chatgpt/)),
    and models built on the full Encoder-Decoder framework ([T5](https://arxiv.org/abs/1910.10683v3),
    [BART](https://arxiv.org/abs/1910.13461), and others).
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 这类架构的例子包括用于编码器的 BERT 基础模型（[BERT](https://arxiv.org/abs/1810.04805)，[RoBERTa](https://arxiv.org/abs/1907.11692)，[ALBERT](https://arxiv.org/abs/1909.11942)，[DeBERTa](https://arxiv.org/abs/2006.03654)
    等），用于解码器的 GPT 基础模型（[GPT](https://paperswithcode.com/paper/improving-language-understanding-by)，[GPT-2](https://paperswithcode.com/paper/language-models-are-unsupervised-multitask)，[GPT-3](https://arxiv.org/abs/2005.14165v4)，[ChatGPT](https://openai.com/blog/chatgpt/)），以及基于完整编码器-解码器框架的模型（[T5](https://arxiv.org/abs/1910.10683v3)，[BART](https://arxiv.org/abs/1910.13461)
    等）。
- en: 'Although we counted *seven* components in this architecture, we can see that
    there are only *three* unique ones:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在这个架构中计数了 *七* 个组件，但我们可以看到其中只有 *三个* 是独特的：
- en: Multi-head attention;
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 多头注意力；
- en: Feed-forward network;
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前馈网络；
- en: Layer normalization.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 层归一化。
- en: '![](../Images/b23d6dff8e208ddadec792f1e2115217.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b23d6dff8e208ddadec792f1e2115217.png)'
- en: Transformer building blocks. Adapted from figure 1 from the [public domain paper](https://arxiv.org/pdf/1706.03762.pdf)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 构建模块。改编自 [公共领域论文](https://arxiv.org/pdf/1706.03762.pdf) 的图 1。
- en: Together they form the basis of a Transformer. Let’s look at them in more detail!
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 它们共同构成了 Transformer 的基础。让我们更详细地了解它们！
- en: Transformer Building Blocks
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer 构建模块
- en: Let’s consider the internal structure of each block and how many parameters
    it requires. In this section, we will also start using [PyTorch](https://pytorch.org/)
    to validate our calculations.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑每个模块的内部结构及其所需的参数数量。在本节中，我们还将开始使用 [PyTorch](https://pytorch.org/) 来验证我们的计算。
- en: 'To check the number of parameters of a certain model block, I will use [the
    following one-line function](https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325/9):'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查某个模型模块的参数数量，我将使用 [以下一行函数](https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325/9)：
- en: Before we start, pay attention to the fact that all blocks **are standardized**
    and used with skip connections. This means that ***the shape*** *(more precisely,
    its last number, since the batch size and the number of tokens may vary)* ***of
    all inputs and outputs must be the same***. For the original paper, this number
    (*d_model*) is 512.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，请注意所有模块 **都经过标准化** 并且使用跳跃连接。这意味着 ***所有输入和输出的形状*** *(更准确地说，是最后一个数字，因为批量大小和令牌数量可能会有所变化)*
    ***必须相同***。对于原始论文，这个数字 (*d_model*) 是 512。
- en: Multi-Head Attention
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多头注意力
- en: A famous attention mechanism is a key to Transformer architecture. But putting
    aside all motivations and technical details it is just a few matrix multiplications.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一个著名的注意力机制是 Transformer 架构的关键。但撇开所有动机和技术细节，它实际上只是一些矩阵乘法。
- en: '![](../Images/2bbff8eb952d341cdd39a0b685a7a26d.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2bbff8eb952d341cdd39a0b685a7a26d.png)'
- en: Transformer multi-head attention. Adapted from figure 2 from the [public domain
    paper](https://arxiv.org/pdf/1706.03762.pdf)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 多头注意力。改编自 [公共领域论文](https://arxiv.org/pdf/1706.03762.pdf) 的图 2。
- en: After calculating attention for every *head*, we concatenate all heads together
    and pass it through a linear layer (*W_O matrix*). In turn, each head is *scaled
    dot-product attention* with three separate matrix multiplications for the query,
    key, and value (*W_Q*, *W_K*, *and W_V matrices respectively*). These three matrices
    are *different for each head*, which is why subscript *i* is present.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 计算每个*head*的注意力后，我们将所有heads拼接在一起，并通过一个线性层（*W_O矩阵*）进行处理。每个head是*scaled dot-product
    attention*，包括三个分别用于查询、键和值的矩阵乘法（*W_Q*、*W_K*、*W_V矩阵*）。这三个矩阵*对于每个head都是不同的*，这就是为什么存在下标*i*。
- en: 'The shape of the final linear layer (*W_O*) is *d_model to d_model*. The shape
    of the rest three matrices (*W_Q*, *W_K*, *and W_V)* is the same: *d_model to
    d_qkv*.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 最终线性层（*W_O*）的形状是*d_model到d_model*。其余三个矩阵（*W_Q*、*W_K*、*W_V*）的形状是相同的：*d_model到d_qkv*。
- en: Note that **d_qkv** in the image above is denoted as **d_k** or **d_v** in the
    original paper. I just find this name more intuitive, because although these matrices
    may have different shapes, it is almost always the same.
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 注意，上图中的**d_qkv**在原论文中表示为**d_k**或**d_v**。我发现这个名称更直观，因为尽管这些矩阵可能有不同的形状，但几乎总是相同的。
- en: ''
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Also, note that **d_qkv** = **d_model** / **num_heads** (**h** in the paper).
    That’s why **d_model** must be divisible by **num_heads**:to ensure correct concatenation
    later.
  id: totrans-60
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 还要注意**d_qkv** = **d_model** / **num_heads**（论文中的**h**）。这就是为什么**d_model**必须能够被**num_heads**整除：以确保后续的正确拼接。
- en: You can test yourself by checking the shapes in all the intermediate phases
    in the picture above (the correct ones are indicated at the bottom right).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过检查上图中所有中间阶段的形状（正确的形状在右下角标示）来测试自己。
- en: As a result, we need three smaller matrices for each head and one large final
    matrix. How many parameters do we need (do not forget biases)?
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，我们需要为每个head准备三个较小的矩阵和一个大的最终矩阵。我们需要多少参数（不要忘记偏置）？
- en: '![](../Images/95dfd1dcbc52d9035f58ee25b56edec4.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/95dfd1dcbc52d9035f58ee25b56edec4.png)'
- en: The formula for calculating the number of parameters in the Transformer attention
    module. Image by Author
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 计算Transformer注意力模块中参数数量的公式。图片来源：作者
- en: I hope it’s not too tedious — I tried to make the deduction as clear as possible.
    Don’t worry! The future formulas will be much smaller.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这不会太枯燥——我尽力使推导尽可能清晰。不要担心！未来的公式会小得多。
- en: The approximate number of parameters is such because we can neglect `4*d_model`
    compared to `4*d_model^2`. Let’s test ourselves using PyTorch.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 近似参数数量是这样的，因为我们可以忽略`4*d_model`相对于`4*d_model^2`。让我们使用PyTorch进行测试。
- en: The numbers match, meaning we are good!
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 数字匹配，这意味着我们做得很好！
- en: Feed-forward Network
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前馈网络
- en: The feed-forward network in Transformer consists of two fully connected layers
    with a ReLU activation function in between. The network is built in such a way
    that its internal part is more expressive than input and output (which, as we
    remember, must be the same).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer中的前馈网络由两个全连接层组成，中间夹着一个ReLU激活函数。网络的内部部分比输入和输出更具表现力（正如我们所记得的，输入和输出必须是相同的）。
- en: In general case, it is *MLP(d_model, d_ff) -> ReLU -> MLP(d_ff, d_model)*, and
    for original paper *d_ff* = 2048.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一般情况下，它是*MLP(d_model, d_ff) -> ReLU -> MLP(d_ff, d_model)*，而在原论文中，*d_ff* = 2048。
- en: '![](../Images/ea1ffe4e4242074997ba1b1160766feb.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea1ffe4e4242074997ba1b1160766feb.png)'
- en: Feed-forward neural network description. [Public domain paper](https://arxiv.org/pdf/1706.03762.pdf)
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈神经网络描述。 [公共领域论文](https://arxiv.org/pdf/1706.03762.pdf)
- en: A little visualization never hurts.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一点可视化永远不会有害。
- en: '![](../Images/713ed89b5d5b0d1c210b5a8b20651879.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/713ed89b5d5b0d1c210b5a8b20651879.png)'
- en: Transformer feed-forward network. Image by Author
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer前馈网络。图片来源：作者
- en: The calculation of parameters is quite easy, the main thing, again, is not to
    get entangled by biases.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 参数的计算非常简单，主要的，还是不要被偏置搞混。
- en: '![](../Images/d91d8182ceafce4606125912c86afbc2.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d91d8182ceafce4606125912c86afbc2.png)'
- en: The formula for calculating the number of parameters in the Transformer feed-forward
    net. Image by Author
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 计算Transformer前馈网络中参数数量的公式。图片来源：作者
- en: 'We can describe such a simple network and check the number of its parameters
    using the following code (*note that official PyTorch implementation also uses*
    ***dropout****, which we will see later in Encoder/Decoder code. But as we know,
    the dropout layer does not have trainable parameters, so I omit it here for simplicity)*:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以描述这样一个简单的网络，并使用以下代码检查其参数数量（*请注意，官方PyTorch实现也使用* ***dropout****，稍后我们将在编码器/解码器代码中看到。但如我们所知，dropout层没有可训练的参数，因此为了简洁起见，我在这里省略了它）：
- en: The numbers match again, and only one component remains.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 数字再次匹配，只剩下一个组件。
- en: Layer Normalization
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层归一化
- en: The last building block of the Transformer architecture is [layer normalization](https://arxiv.org/abs/1607.06450).
    Long story short, it is just an intelligent (i.e. *learnable*) way of *normalization
    with scaling*, that improves the stability of the training process.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer架构的最后一个构建块是[层归一化](https://arxiv.org/abs/1607.06450)。简而言之，它只是一个智能（即*可学习的*）*带有缩放的归一化*方法，改善了训练过程的稳定性。
- en: '![](../Images/053d5bd1d841ee3bcee9ef0e91c1265d.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/053d5bd1d841ee3bcee9ef0e91c1265d.png)'
- en: Transformer layer normalization. Image by Author
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer层归一化。图片由作者提供
- en: The trainable parameters here are two vectors *gamma* and *beta*, each of which
    has a *d_model* dimension.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的可训练参数是两个向量 *gamma* 和 *beta*，每个向量都有一个 *d_model* 维度。
- en: '![](../Images/fae096125fcc0f4066499a5df89dcd3e.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fae096125fcc0f4066499a5df89dcd3e.png)'
- en: The formula for calculating the number of parameters in the Transformer layer
    normalization module. Image by Author
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 计算Transformer层归一化模块参数数量的公式。图片由作者提供
- en: Let’s check our assumptions with code.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用代码验证我们的假设。
- en: Good! In approximate calculations, this number can be neglected, since layer
    normalization has dramatically fewer parameters than feed-forward network or multi-head
    attention block (despite the fact that this module occurs several times).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 好的！在近似计算中，这个数字可以忽略，因为层归一化的参数远少于前馈网络或多头注意力块（尽管这个模块出现了几次）。
- en: Derive Complete Formulas
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推导完整公式
- en: Now we have everything to count the parameters for the entire Encoder/Decoder
    block!
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们拥有计算整个编码器/解码器块参数所需的一切！
- en: '**Encoder and Decoder in PyTorch**'
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**PyTorch中的编码器和解码器**'
- en: Let’s remember that the Encoder consists of an attention block, feed-forward
    net, and two layer normalizations.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们记住，编码器由一个注意力块、前馈网络和两个层归一化组成。
- en: '![](../Images/ce41cc3bda830f558c502de939225f9f.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ce41cc3bda830f558c502de939225f9f.png)'
- en: Transformer Encoder. Adapted from figure 1 from the [public domain paper](https://arxiv.org/pdf/1706.03762.pdf)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer Encoder。改编自[公共领域论文](https://arxiv.org/pdf/1706.03762.pdf)中的图1
- en: We can verify that all components are in place by looking inside the PyTorch
    code. Here *multi-head attention* is indicated *in red* (on the left), *the feed-forward
    network* *in blue* and *layer normalizations in green* (screenshot of the Python
    console in PyCharm).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过查看PyTorch代码内部来验证所有组件是否到位。这里的*多头注意力*标记为*红色*（左侧），*前馈网络*标记为*蓝色*，*层归一化*标记为绿色（PyCharm中的Python控制台截图）。
- en: '![](../Images/e2cb88d31d9cd297995b48a6c24676a4.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e2cb88d31d9cd297995b48a6c24676a4.png)'
- en: PyTorch TransformerEncoderLayer. Image by Author
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch TransformerEncoderLayer. 图片由作者提供
- en: As noted above, this implementation includes *dropout* in the feed-forward net.
    Now we can see the dropout layers related to layer normalizations as well.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，这个实现包括了前馈网络中的*dropout*。现在我们也可以看到与层归一化相关的dropout层。
- en: The decoder, in turn, consists of two attention blocks, a feed-forward net,
    and three layer normalizations.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器则由两个注意力块、一个前馈网络和三个层归一化组成。
- en: '![](../Images/55cc840de8bd34c145efcb9fbc32c22b.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/55cc840de8bd34c145efcb9fbc32c22b.png)'
- en: Transformer Decoder. Adapted from figure 1 from the [public domain paper](https://arxiv.org/pdf/1706.03762.pdf)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer Decoder。改编自[公共领域论文](https://arxiv.org/pdf/1706.03762.pdf)中的图1
- en: Let’s look at PyTorch again (the colors are the same).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次查看PyTorch（颜色相同）。
- en: '![](../Images/dd02ab95ca05c2166633f362d48a5ebe.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd02ab95ca05c2166633f362d48a5ebe.png)'
- en: PyTorch TransformerDecoderLayer. Image by Author
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch TransformerDecoderLayer. 图片由作者提供
- en: Final Formula
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最终公式
- en: After making sure, we can write the following function to calculate the number
    of parameters. In fact, these are just three lines of code that can even be combined
    into one. The rest of the function is a docstring for clarification.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 确认后，我们可以编写以下函数来计算参数数量。实际上，这只是三行代码，甚至可以合并为一行。其余部分是文档字符串，用于说明。
- en: Now it’s time to test it.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是测试的时候了。
- en: The exact formulas are correct, meaning we have correctly identified all building
    blocks and deconstructed them into their components. Interestingly, since we ignored
    relatively small values (*thousands compared to millions*) in the approximate
    formulas, **the error is only about 0.2%** compared to the exact results! But
    there is a way to make these formulas even simpler.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 精确公式是正确的，意味着我们已经正确识别了所有构建块并将其分解为组成部分。有趣的是，由于我们在近似公式中忽略了相对较小的值（*与百万相比的几千*），**误差仅约为0.2%**，与精确结果相比！但还有一种方法可以使这些公式更简单。
- en: The approximate number of parameters for the attention block is `4*d_model^2`.
    It sounds pretty simple, considering that *d_model* is an important hyperparameter.
    But for the feed-forward network, we need to know *d_ff*, since the formula is
    `2*d_model*d_ff`.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力块的近似参数数量是`4*d_model^2`。考虑到*d_model*是一个重要的超参数，这听起来相当简单。但对于前馈网络，我们需要知道*d_ff*，因为公式是`2*d_model*d_ff`。
- en: '*d_ff* is a separate hyperparameter that you have to memorize in the formula,
    so let’s think about how to get rid of it. In fact, as we saw above, *d_ff = 2048*
    when *d_model = 512*, so ***d_ff = 4*d_model***.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*d_ff*是一个需要在公式中记住的单独超参数，因此让我们考虑如何摆脱它。实际上，正如我们上面所看到的，当*d_model = 512*时，*d_ff
    = 2048*，所以***d_ff = 4*d_model***。'
- en: For many Transformer models, such an assumption will make sense, *greatly simplifying
    the formula*, and still giving you an **estimate** of the approximate number of
    parameters. After all, no one wants to know the precise amount, it’s just useful
    to understand whether this number is in the *hundreds of thousands* or *tens of
    millions*.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多Transformer模型，这样的假设是有意义的，*大大简化了公式*，同时仍然给出一个**参数数量的估计**。毕竟，没有人想知道确切的数量，只需了解这个数字是在*几十万*还是*几千万*就足够了。
- en: '![](../Images/7d63ab6f245d5b9ce7712f19b7d15e2b.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7d63ab6f245d5b9ce7712f19b7d15e2b.png)'
- en: Approximate Encoder-Decoder formulas. Image by Author
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 近似的编码器-解码器公式。图片由作者提供
- en: To get an understanding of *the order of magnitude* you are dealing with, you
    can also round the multipliers and you will get `10*d_model^2` for every Encoder/Decoder
    layer.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解您处理的*数量级*，您也可以对乘数进行四舍五入，得到每个编码器/解码器层为`10*d_model^2`。
- en: Conclusions
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Here is a summary of all the formulas that we have deduced today.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们今天推导出的所有公式的总结。
- en: '![](../Images/e170b5192082a059743fe34ef2c5a4c4.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e170b5192082a059743fe34ef2c5a4c4.png)'
- en: Formulas recap. Image by Author
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 公式回顾。图片由作者提供
- en: In this article, we’ve calculated the number of parameters in Transformer Encoder/Decoder
    blocks, but of course, I don’t invite you to count the parameters of all new models.
    I have just chosen this method because I was surprised that I didn’t find such
    an article when I started studying Transformers.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们计算了Transformer编码器/解码器块的参数数量，但当然，我并不邀请您去计算所有新模型的参数。我之所以选择这种方法，是因为当我开始学习Transformers时，发现没有找到这样的文章，这让我感到惊讶。
- en: 'While the number of parameters can give us an indication of the complexity
    of the model and the amount of data it will require to train, it is just one way
    to gain a deeper understanding of the architecture. I want to encourage you to
    explore and experiment: take a look at implementation, run the code with different
    hyperparameters, etc. So, keep learning and have fun!'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管参数数量可以给我们提供模型复杂性及其训练所需数据量的指示，但这只是深入了解架构的一种方式。我鼓励您探索和实验：查看实现，运行不同超参数的代码等。所以，继续学习并享受其中的乐趣！
- en: Thank you for reading!
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感谢您的阅读！
- en: I hope these materials were useful to you. [Follow me on Medium](https://medium.com/@andimid)
    to get more articles like this.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 希望这些材料对您有帮助。请[在Medium上关注我](https://medium.com/@andimid)，获取更多类似的文章。
- en: If you have any questions or comments, I will be glad to get any feedback. Ask
    me in the comments, or connect via [LinkedIn](https://www.linkedin.com/in/andimid/)
    or [Twitter](https://twitter.com/dimid_ml).
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您有任何问题或意见，我很高兴收到任何反馈。请在评论中问我，或通过[LinkedIn](https://www.linkedin.com/in/andimid/)或[Twitter](https://twitter.com/dimid_ml)与我联系。
- en: To support me as a writer and to get access to thousands of other Medium articles,
    get Medium membership using [my referral link](https://medium.com/@andimid/membership)
    (no extra charge for you).
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持我作为作者，并获取访问数千篇其他Medium文章的权限，请使用[我的推荐链接](https://medium.com/@andimid/membership)获得Medium会员（对您没有额外收费）。
