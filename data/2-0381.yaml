- en: Beyond NeRFs (Part One)
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超越NeRF（第一部分）
- en: 原文：[https://towardsdatascience.com/beyond-nerfs-part-one-7e84eae816d8](https://towardsdatascience.com/beyond-nerfs-part-one-7e84eae816d8)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/beyond-nerfs-part-one-7e84eae816d8](https://towardsdatascience.com/beyond-nerfs-part-one-7e84eae816d8)
- en: Increasing NeRF training speed by 100x or more…
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提高NeRF训练速度100倍或更多……
- en: '[](https://wolfecameron.medium.com/?source=post_page-----7e84eae816d8--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----7e84eae816d8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7e84eae816d8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7e84eae816d8--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----7e84eae816d8--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://wolfecameron.medium.com/?source=post_page-----7e84eae816d8--------------------------------)[![Cameron
    R. Wolfe, Ph.D.](../Images/52bb88d7cf1105501be2fae5ccbe7a03.png)](https://wolfecameron.medium.com/?source=post_page-----7e84eae816d8--------------------------------)[](https://towardsdatascience.com/?source=post_page-----7e84eae816d8--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----7e84eae816d8--------------------------------)
    [Cameron R. Wolfe, Ph.D.](https://wolfecameron.medium.com/?source=post_page-----7e84eae816d8--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7e84eae816d8--------------------------------)
    ·15 min read·Jun 7, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----7e84eae816d8--------------------------------)
    ·15分钟阅读·2023年6月7日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/3185851e9a8756d697031a415a23b913.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3185851e9a8756d697031a415a23b913.png)'
- en: (Photo by [Mathew Schwartz](https://unsplash.com/@cadop?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    on [Unsplash](https://unsplash.com/s/photos/speed?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: （照片由 [Mathew Schwartz](https://unsplash.com/@cadop?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)
    提供，来源于 [Unsplash](https://unsplash.com/s/photos/speed?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)）
- en: As we have seen in previous overviews, the proposal of [Neural Radiance Fields
    (NeRFs)](https://cameronrwolfe.substack.com/p/understanding-nerfs) [4] was a breakthrough
    in the domain of neural scene representations. Given some images of an underlying
    scene, we can train a NeRF to generate arbitrary viewpoints of this scene at high
    resolution. Put simply, NeRFs leverage deep learning to provide photorealistic
    renderings of 3D scenes.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在之前的概述中所见，[神经辐射场（NeRFs）](https://cameronrwolfe.substack.com/p/understanding-nerfs)
    [4] 的提出在神经场景表示领域是一个突破。给定一些底层场景的图像，我们可以训练一个NeRF以高分辨率生成该场景的任意视角。简而言之，NeRF利用深度学习提供3D场景的摄影级渲染。
- en: 'But, they have a few notable problems. Within this overview, we will focus
    on two limitations of NeRFs in particular:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 但，它们有一些显著的问题。在本概述中，我们将特别关注NeRF的两个局限性：
- en: Training a NeRF that can accurately render new viewpoints requires many images
    of the underlying scene.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个可以准确渲染新视角的NeRF需要大量的场景图像。
- en: Performing training (and rendering) with NeRFs is slow.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用NeRF进行训练（和渲染）是很慢的。
- en: 'As a solution to these problems, we will overview two notable extensions of
    the NeRF methodology: PixelNeRF [1] and InstantNGP [2]. In learning about these
    methods, we will see that most problems faced by NeRF can be solved by crafting
    higher-quality input data and leveraging the ability of deep neural networks to
    generalize learned patterns to new scenarios.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 作为解决这些问题的方案，我们将概述NeRF方法的两个显著扩展：PixelNeRF [1] 和 InstantNGP [2]。在学习这些方法的过程中，我们会看到，NeRF所面临的大部分问题可以通过制作更高质量的输入数据以及利用深度神经网络将已学模式推广到新场景的能力来解决。
- en: '![](../Images/ea4ac87de7567d65a45b1908a380b387.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea4ac87de7567d65a45b1908a380b387.png)'
- en: (from [1, 2])
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1, 2]）
- en: Background
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景
- en: 'We have recently learned about many different methods for modeling 3D shapes
    and scenes with deep learning. These overviews have contained several background
    concepts that will also be useful for understanding the concepts within this overview:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最近了解到许多不同的使用深度学习建模3D形状和场景的方法。这些概述包含了一些背景概念，这些概念也将有助于理解本概述中的概念：
- en: Feed-forward neural networks [[link](https://cameronrwolfe.substack.com/i/94634004/feed-forward-neural-networks)]
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前馈神经网络 [[链接](https://cameronrwolfe.substack.com/i/94634004/feed-forward-neural-networks)]
- en: Positional embeddings [[link](https://cameronrwolfe.substack.com/i/97915766/position-encodings)]
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置信息嵌入 [[链接](https://cameronrwolfe.substack.com/i/97915766/position-encodings)]
- en: Signed distance functions [[link](https://cameronrwolfe.substack.com/i/94634004/signed-distance-functions)]
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 签名距离函数 [[链接](https://cameronrwolfe.substack.com/i/94634004/signed-distance-functions)]
- en: How 3D data is represented [[link](https://cameronrwolfe.substack.com/i/94634004/representing-d-shapes)]
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3D数据的表示方式 [[链接](https://cameronrwolfe.substack.com/i/94634004/representing-d-shapes)]
- en: Beyond these concepts, it will also be very useful within this overview to have
    a working understanding of NeRFs [4]. To build this understanding, I recommend
    reading my overview on NeRFs [here](https://cameronrwolfe.substack.com/p/understanding-nerfs).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些概念外，在这个概述中，理解NeRFs [4]也会非常有用。为了建立这种理解，我建议阅读我对NeRFs的概述[这里](https://cameronrwolfe.substack.com/p/understanding-nerfs)。
- en: Feature Pyramids
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征金字塔
- en: Within this post, we will see several instances in which we use deep neural
    networks to convert an image into a corresponding (pyramid) feature representation.
    But, some of us might be unfamiliar with this concept. As such, we need to quickly
    learn about features representations and overview some different variants of this
    idea we might encounter within deep learning.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将看到多个实例，展示如何使用深度神经网络将图像转换为相应的（金字塔）特征表示。但有些人可能对这个概念不太熟悉。因此，我们需要快速了解特征表示，并概述我们在深度学习中可能遇到的一些不同变体。
- en: '**What are features?** Before learning about feature pyramids, we need to understand
    what is meant by the word “features”. Typically, the output of a neural network
    will be a classification, a set of bounding boxes, a segmentation mask, or something
    else along these lines. In image classification, for example, we take an image
    as input, pass it through our neural network, and the final layer of this network
    is a classification module that converts the hidden state into a vector of class
    probabilities. Simple enough!'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**什么是特征？** 在了解特征金字塔之前，我们需要理解“特征”一词的含义。通常，神经网络的输出会是分类、边界框集合、分割掩码或其他类似的东西。例如，在图像分类中，我们将图像作为输入，通过神经网络传递，网络的最后一层是一个分类模块，将隐藏状态转换为类别概率向量。简单明了！'
- en: '![](../Images/0c77c5951bc7bbada893433c2a3f1e26.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c77c5951bc7bbada893433c2a3f1e26.png)'
- en: Extracting features from a deep neural network (created by author)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从深度神经网络中提取特征（由作者创建）
- en: Sometimes, however, we don’t want to perform this last step. Instead, we can
    just take the final hidden state of the network (before the classification module)
    and use this vector as a representation of our data; see above. This vector, also
    referred to as features (or a feature representation), is a compressed representation
    of the semantic information in our data, and we can use it to perform a variety
    of tasks (e.g., similarity search).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有时我们不希望执行最后一步。相反，我们可以直接取网络的最终隐藏状态（在分类模块之前），并将这个向量作为数据的表示；见上文。这个向量，也称为特征（或特征表示），是数据中语义信息的压缩表示，我们可以用它来执行各种任务（例如，相似性搜索）。
- en: '**What is a feature pyramid?** Multi-scale (or “pyramid”) strategies are an
    important, fundamental concept within computer vision. The basic idea is simple.
    Throughout the layers of a neural network, we occasionally *(i)* downsample the
    spatial resolution of our features and *(ii)* increase the channel dimension.
    See, for example, the schema of a [ResNet-18](https://pytorch.org/vision/0.8/_modules/torchvision/models/resnet.html)
    [6]. This CNN contains four “sections”, each of which has a progressively higher
    channel dimension and lower spatial dimension; see below.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**什么是特征金字塔？** 多尺度（或“金字塔”）策略是计算机视觉中的一个重要基本概念。基本思想很简单。在神经网络的层次中，我们偶尔*(i)* 降采样特征的空间分辨率，并*(ii)*
    增加通道维度。例如，见[ResNet-18](https://pytorch.org/vision/0.8/_modules/torchvision/models/resnet.html)
    [6]的示意图。这个CNN包含四个“部分”，每个部分的通道维度逐渐增高，空间维度逐渐降低；见下文。'
- en: '![](../Images/67ed7ce4de99f80edc9a8d338e8e35f0.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/67ed7ce4de99f80edc9a8d338e8e35f0.png)'
- en: Basic illustration of “sections” in a ResNet architecture (created by author)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet架构中“部分”的基本示意图（由作者创建）
- en: One way to extract features from this network is to just use the final hidden
    representation. But, this representation does not contain much spatial information
    (i.e., the spatial dimension gets progressively lower in each layer!) compared
    to earlier layers in the network. This is a problem for dense prediction tasks
    (e.g., object detection) that are heavily dependent upon spatial info in an image!
    To fix this, we need to construct a *feature pyramid* [3].
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个网络中提取特征的一种方法是仅使用最终的隐藏表示。但是，与网络早期层相比，这种表示不包含太多的空间信息（即，空间维度在每一层中逐渐降低！）。这对于依赖图像中空间信息的密集预测任务（例如目标检测）是一个问题！为了解决这个问题，我们需要构建一个*特征金字塔*[3]。
- en: Put simply, a feature pyramid extracts features from several different layers
    within a network instead of just using the features from the network’s final layer;
    see below.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，特征金字塔从网络中的几个不同层中提取特征，而不是仅使用网络最终层的特征；见下文。
- en: '![](../Images/ba386f48c7c39c7f786be698e3fc52a3.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba386f48c7c39c7f786be698e3fc52a3.png)'
- en: (from [3])
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: （来自[3]）
- en: The resulting set of features contains information with varying amounts of spatial
    and semantic information, as each layer has a varying spatial and channel dimension.
    Thus, feature pyramids tend to produce image features that are useful for a variety
    of different tasks. In this overview, we will see feature pyramids used to provide
    extra input information for a variant of NeRF!
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 得到的特征集包含不同量的空间和语义信息，因为每一层的空间和通道维度都不同。因此，特征金字塔通常生成对各种不同任务有用的图像特征。在本概述中，我们将看到特征金字塔用于为NeRF的变体提供额外的输入信息！
- en: Input Encodings
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输入编码
- en: Sometimes, we have data that we don’t want to input directly into a machine
    learning model, so we pass an encoded version of this data as input instead. This
    is a fundamental concept in machine learning. Think about, for example, [one-hot
    encodings](https://www.educative.io/blog/one-hot-encoding) of categorical variables.
    A more sophisticated example would be [kernel functions](https://www.geeksforgeeks.org/major-kernel-functions-in-support-vector-machine-svm/#:~:text=Kernel%20Function%20is%20a%20method,window%20to%20manipulate%20the%20data.),
    or functions that we pass our data through (i.e., possibly to make it [linearly
    separable](https://en.wikipedia.org/wiki/Linear_separability)) before giving it
    to our model. In each of these cases, we are encoding/transforming our input so
    that it is in a more model-friendly format.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们有些数据不想直接输入到机器学习模型中，因此我们将这些数据的编码版本作为输入。这是机器学习中的一个基本概念。例如，[独热编码](https://www.educative.io/blog/one-hot-encoding)的分类变量。一个更复杂的例子是[核函数](https://www.geeksforgeeks.org/major-kernel-functions-in-support-vector-machine-svm/#:~:text=Kernel%20Function%20is%20a%20method,window%20to%20manipulate%20the%20data.)，或我们将数据通过的函数（即，可能使其[线性可分](https://en.wikipedia.org/wiki/Linear_separability)）然后再提供给模型。在这些情况下，我们都在编码/转换输入，以便它以更适合模型的格式呈现。
- en: '![](../Images/272dae94587286f4b5e18369396a06aa.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/272dae94587286f4b5e18369396a06aa.png)'
- en: Positional encoding in the NeRF architecture (created by author)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: NeRF架构中的位置编码（由作者创建）
- en: '**Positional encodings.** Similarly, when we pass 3D coordinates as input to
    a NeRF’s feed-forward network, we don’t want to directly use these coordinates
    as input. Instead, we convert them into a higher-dimensional vector using a positional
    encoding scheme; see above. This positional encoding scheme is the same exact
    technique used to add positional information to tokenized inputs within transformers
    [6]. In NeRFs, positional encodings have been shown to yield significantly improved
    scene renderings.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**位置编码。** 类似地，当我们将3D坐标作为输入传递给NeRF的前馈网络时，我们不想直接使用这些坐标作为输入。相反，我们使用位置编码方案将它们转换为更高维的向量；见上文。这种位置编码方案是用于在变换器[6]中为标记化输入添加位置数据的完全相同的技术。在NeRF中，位置编码已被证明能显著改善场景渲染效果。'
- en: '**Learnable embedding layers.** There’s one problem with positional encoding
    schemes — *they are fixed*. What if we want to learn these encodings instead?
    One way to do this would be to construct an *embedding matrix*. Given a function
    that maps each spatial location to an index in this matrix, we could retrieve
    the corresponding embedding for each spatial location and use it as input. Then,
    these embeddings could be trained like normal model parameters!'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**可学习的嵌入层。** 位置编码方案有一个问题——*它们是固定的*。如果我们想学习这些编码呢？一种方法是构造一个*嵌入矩阵*。给定一个将每个空间位置映射到矩阵中的索引的函数，我们可以检索每个空间位置对应的嵌入并将其作为输入。然后，这些嵌入可以像普通模型参数一样进行训练！'
- en: Publications
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 出版物
- en: Now, we will overview some publications that extend and improve upon NeRFs.
    In particular, these publications *(i)* produce high-quality scene representation
    with fewer images of the scene and *(ii)* make the training and rendering process
    of NeRF much faster.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将概述一些扩展和改进 NeRF 的出版物。特别是，这些出版物 *(i)* 通过较少的场景图像生成高质量的场景表示，并 *(ii)* 使 NeRF
    的训练和渲染过程更快。
- en: '[PixelNeRF: Neural Radiance Fields from One or Few Images](https://arxiv.org/abs/2012.02190)
    [1]'
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[PixelNeRF：来自一张或几张图片的神经辐射场](https://arxiv.org/abs/2012.02190) [1]'
- en: '![](../Images/9dec6ef785de3edadc878b37506e1e2d.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9dec6ef785de3edadc878b37506e1e2d.png)'
- en: (from [1])
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: （来源于 [1]）
- en: One of the main drawbacks of the original NeRF formulation is that it must be
    trained and used per-scene. Obtaining a representation for each scene via NeRFs
    is computationally expensive and requires many posed images of the scene. PixelNeRF
    [1] aims to mitigate this problem by conditioning a NeRF’s output upon image features
    — created by a pre-trained deep neural network — from the underlying scene. By
    using image features as input, *PixelNeRF can leverage prior information to generate
    high-quality scene renderings given only a few images of a scene*. Thus, it drastically
    improves the quality of scene representations given limited data.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 原始 NeRF 公式的主要缺点之一是它必须针对每个场景进行训练和使用。通过 NeRF 获取每个场景的表示在计算上是昂贵的，并且需要许多具有姿态的场景图像。PixelNeRF
    [1] 旨在通过将 NeRF 的输出条件化为由预训练的深度神经网络创建的图像特征来缓解这一问题。通过使用图像特征作为输入，*PixelNeRF 可以利用先前的信息，在仅有少数场景图像的情况下生成高质量的场景渲染*。因此，它在数据有限的情况下显著提高了场景表示的质量。
- en: '**method.** PixelNeRF is quite similar to the original NeRF formulation. It
    uses a feed-forward neural network to model a radiance field by predicting color
    and opaqueness values given a spatial location and viewing direction (that have
    been converted into [positional embeddings](https://cameronrwolfe.substack.com/i/97915766/position-encodings))
    as input. The volume rendering and training procedures are not changed. The main
    difference between these methods is that pixelNeRF has an additional input component:
    *image features derived from a view of the underlying scene*.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**方法。** PixelNeRF 与原始 NeRF 公式非常相似。它使用前馈神经网络通过预测给定空间位置和视角方向（已转换为 [位置嵌入](https://cameronrwolfe.substack.com/i/97915766/position-encodings)）的颜色和不透明度值来建模辐射场。体积渲染和训练过程没有改变。这些方法之间的主要区别在于
    pixelNeRF 具有一个额外的输入组件：*从底层场景视图中衍生的图像特征*。'
- en: '![](../Images/beb0ac1511966f3b6880d17347bc7f13.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/beb0ac1511966f3b6880d17347bc7f13.png)'
- en: (from [1])
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: （来源于 [1]）
- en: PixelNeRF has the ability to consider one or multiple images of the underlying
    scene as part of its input. Images are first passed through a pre-trained encoder
    — a feature pyramid R esNet variant[6] — to produce a feature pyramid. From here,
    we can extract the region of these features corresponding to a specific spatial
    location (this can be done pretty easily using camera pose information; see Section
    4.1 of [1]). Then, we concatenate these extracted features with the corresponding
    spatial location and viewing direction as input to PixelNeRF’s feed-forward network.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: PixelNeRF 能够将一个或多个场景图像作为输入的一部分。图像首先通过一个预训练的编码器——一个特征金字塔 ResNet 变体[6]——来生成特征金字塔。从这里，我们可以提取这些特征中对应于特定空间位置的区域（这可以通过相机位姿信息比较容易地完成；见
    [1] 的第4.1节）。然后，我们将这些提取的特征与对应的空间位置和视角方向串联作为 PixelNeRF 前馈网络的输入。
- en: 'Let’s think about a single forward pass of the feed-forward neural network
    of PixelNeRF. We consider a single spatial location and viewing direction in this
    forward pass. If we have access to a single image of the scene, we can include
    this information by:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们思考一下 PixelNeRF 的前馈神经网络的单次前向传播。我们在这一前向传播中考虑一个单一的空间位置和观察方向。如果我们可以访问到场景的单幅图像，我们可以通过以下方式包含这些信息：
- en: Passing the image through the encoder to produce a feature grid.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过编码器传递图像以生成特征网格。
- en: Obtaining features by extracting the region of this feature pyramid that corresponds
    to the current spatial location.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过提取与当前空间位置对应的特征金字塔区域来获取特征。
- en: Concatenating the spatial, directional, and feature input.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连接空间、方向和特征输入。
- en: Then, the remaining components of PixelNeRF match the original NeRF formulation;
    see below.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，PixelNeRF 的其余组件与原始的 NeRF 公式相匹配；见下文。
- en: '![](../Images/c3d9cdf82abf657888eb905f4d15289d.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c3d9cdf82abf657888eb905f4d15289d.png)'
- en: (from [1])
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: If multiple images of the underlying scene are available, we just divide PixelNeRF’s
    feed-forward network into two components. The first component separately processes
    each image using the procedure described above. Namely, the network performs a
    separate forward pass by concatenating the features of each image with the same
    spatial and directional input information.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有多个场景图像可用，我们只需将 PixelNeRF 的前馈网络分为两个组件。第一个组件使用上述过程单独处理每张图像。即，网络通过将每张图像的特征与相同的空间和方向输入信息连接起来来执行单独的前向传播。
- en: '![](../Images/133dac84694b854f15956b047e23fbf1.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/133dac84694b854f15956b047e23fbf1.png)'
- en: PixelNeRF architecture with multiple input views (created by author)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: PixelNeRF 具有多个输入视角的架构（作者创建）
- en: Each of these forward passes produces an output vector. We can aggregate these
    vectors by taking their average, then pass this average vector through a few more
    feed-forward layers to produce the final RGB and opaqueness output; see above.
    Despite this modified architecture, the training process for PixelNeRF is similar
    to NeRF and only requires a dataset of scene images.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 每次前向传播都会产生一个输出向量。我们可以通过计算这些向量的平均值来聚合它们，然后将这个平均向量通过更多的前馈层以生成最终的 RGB 和不透明度输出；见上文。尽管有这种修改后的架构，PixelNeRF
    的训练过程与 NeRF 相似，并且只需要场景图像的数据集。
- en: '**results.** PixelNeRF is evaluated on tasks like object and scene view synthesis
    on [ShapeNet](https://shapenet.org/), as well as on its ability to represent real-world
    scenes. First, PixelNeRF is trained to represent objects from a specific ShapeNet
    class (e.g., chair or car) given one or two images as input (i.e., one or two-shot
    scenario). In this case, we see that PixelNeRF is better than baselines at reconstructing
    objects from a few input images; see below.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**结果。** PixelNeRF 在 [ShapeNet](https://shapenet.org/) 上进行物体和场景视图合成等任务的评估，以及其表示真实世界场景的能力。首先，PixelNeRF
    被训练以代表来自特定 ShapeNet 类（例如，椅子或汽车）的对象，给定一张或两张输入图像（即，一张或两张图像场景）。在这种情况下，我们发现 PixelNeRF
    在从少量输入图像重建对象方面优于基线；见下文。'
- en: '![](../Images/64e6be681dea36cdf7a5254befba17ca.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/64e6be681dea36cdf7a5254befba17ca.png)'
- en: (from [1])
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: Plus, PixelNeRF does not perform any test-time optimization, which is not true
    of baselines like [SRNs](https://cameronrwolfe.substack.com/p/scene-representation-networks)
    [5]. Thus, PixelNeRF performs more favorably despite being faster and solving
    a more difficult problem compared to baselines. When we train PixelNeRF in a category-agnostic
    fashion (i.e., over 13 object classes in ShapeNet), we see that its performance
    gains are even more significant! PixelNeRF outperforms baselines across the board
    on representing this wider set of objects; see below.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，PixelNeRF 不进行任何测试时优化，而像 [SRNs](https://cameronrwolfe.substack.com/p/scene-representation-networks)
    [5] 这样的基线则不是这样。因此，尽管 PixelNeRF 更快且解决了比基线更困难的问题，但它的表现更为出色。当我们以类别无关的方式训练 PixelNeRF（即，在
    ShapeNet 的 13 个对象类别上），我们看到其性能提升更为显著！PixelNeRF 在表示这更广泛的对象集方面超越了基线；见下文。
- en: '![](../Images/695f19b191e20c87d07770d7e569cf98.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/695f19b191e20c87d07770d7e569cf98.png)'
- en: (from [1])
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: （来自 [1]）
- en: When PixelNeRF is evaluated in more complex settings (e.g., unseen categories,
    multi-object scenes, real images, etc.), we continue to see improved performance.
    Most notably, PixelNeRF drastically improves upon baselines’ ability to capture
    multi-object scenes and infer unseen objects at test time; see below.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当 PixelNeRF 在更复杂的设置中进行评估时（例如，未见过的类别、多对象场景、真实图像等），我们继续看到性能的提升。最值得注意的是，PixelNeRF
    在捕捉多对象场景和在测试时推断未见对象的能力上显著优于基线；见下文。
- en: '![](../Images/198773df4fc7a96b24930b348358e02d.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/198773df4fc7a96b24930b348358e02d.png)'
- en: (from [1])
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: (来自[1])
- en: Taking this to the extreme, PixelNeRF can reconstruct scenes with pretty high
    fidelity given only three input images of a real-world scene; see below. Such
    results emphasize the ability of PixelNeRF to model scenes given a limited and
    noisy input data. NeRF is unable to accurately reconstruct scenes in this regime.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 将这一点推向极限，PixelNeRF可以仅凭三张真实场景的输入图像就重建出相当高保真的场景；见下文。这些结果强调了PixelNeRF在给定有限且噪声数据下建模场景的能力。在这种情况下，NeRF无法准确重建场景。
- en: '![](../Images/097f0d1408321d400285d687f1e0e513.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/097f0d1408321d400285d687f1e0e513.png)'
- en: (from [1])
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: (来自[1])
- en: '[Instant Neural Graphics Primitives with a Multiresolution Hash Encoding](https://arxiv.org/abs/2201.05989)
    [2]'
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[多分辨率哈希编码的即时神经图形原语](https://arxiv.org/abs/2201.05989) [2]'
- en: '![](../Images/cb5fbc37dc1e89ba445c76e73755bdf6.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb5fbc37dc1e89ba445c76e73755bdf6.png)'
- en: (from [2])
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: (来自[2])
- en: 'PixelNeRF [1] allows us to recover scene representations from few images of
    the underlying scene. But, recall that the training process for NeRFs is also
    slow (i.e., [2 days](https://cameronrwolfe.substack.com/i/97915766/takeaways)
    on a single GPU)! With this in mind, we might ask ourselves: *how much faster
    can we train a NeRF?* The proposal of Instant Neural Graphics Primitives (InstantNGP)
    in [2] shows us that we can train NeRFs *a lot* faster.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: PixelNeRF [1] 允许我们从少量图像中恢复场景表示。但请记住，NeRF的训练过程也很慢（即单个GPU上需要[2天](https://cameronrwolfe.substack.com/i/97915766/takeaways)）！考虑到这一点，我们可能会问自己：*我们能多快训练一个NeRF？*
    在[2]中提出的即时神经图形原语（InstantNGP）展示了我们可以*大大*加快训练NeRF的速度。
- en: '![](../Images/b71c2a3ae37298e253e3a394d8a00fcf.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b71c2a3ae37298e253e3a394d8a00fcf.png)'
- en: Indexing a simple feature embedding matrix with a hash function (created by
    author)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 使用哈希函数对简单的特征嵌入矩阵进行索引（作者创建）
- en: The approach of InstantNGP is similar to NeRF [4] — the only difference lies
    in how we construct the feed-forward network’s input. Instead of using a positional
    encoding scheme, we construct a multi-resolution hash table that maps each input
    coordinate to a trainable feature vector; see above. This approach *(i)* adds
    more learnable parameters to the NeRF and *(ii)* produces rich input representations
    for each input coordinate, allowing the feed-forward network to be made much smaller.
    Overall, this approach allows the NeRF training process to be significantly accelerated.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: InstantNGP的方法类似于NeRF [4]——唯一的区别在于我们如何构建前馈网络的输入。我们没有使用位置编码方案，而是构建了一个多分辨率哈希表，将每个输入坐标映射到一个可训练的特征向量；见上文。该方法*(i)*
    向NeRF添加了更多可学习的参数，并*(ii)* 为每个输入坐标生成丰富的输入表示，从而使前馈网络变得更小。总体而言，这种方法可以显著加快NeRF的训练过程。
- en: '**method.** The actual approach for constructing and querying the hash table
    of input features is (unfortunately) more complicated than the simple figure depicted
    above. Let explore a bit more how exactly input features are handled by InstantNGP
    [2].'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**方法。** 实际上构建和查询输入特征哈希表的方法（不幸的是）比上述简单示意图要复杂得多。让我们更深入地探讨InstantNGP [2]是如何处理输入特征的。'
- en: 'InstantNGP follows a parametric approach to encoding inputs. Unlike NeRFs that
    use positional embedding functions to map coordinates to fixed, higher-dimensional
    inputs, InstantNGP *learns* input features during training. At a high level, this
    is done by:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: InstantNGP采用参数化的方法来编码输入。与使用位置嵌入函数将坐标映射到固定的高维输入的NeRF不同，InstantNGP在训练过程中*学习*输入特征。从高层次看，这是通过以下方式完成的：
- en: Storing input features in an embedding matrix.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在嵌入矩阵中存储输入特征。
- en: Indexing the embedding matrix based on input coordinates.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于输入坐标对嵌入矩阵进行索引。
- en: Updating the features normally via [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent).
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过[随机梯度下降](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)正常更新特征。
- en: Let’s address these components one-by-one. First, we need to create our table
    of learnable input features that we can index. In [2], input features are stored
    in a multi-resolution table that contains `L` levels of features (i.e., just `L`
    different embedding matrices). Each level of the table has `T` features vectors
    of dimension `F` (i.e., a matrix of size `T x F`). Typically, these parameters
    follow the settings shown below.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一处理这些组件。首先，我们需要创建一个可以索引的可学习输入特征表。在[2]中，输入特征存储在一个包含`L`级特征（即`L`个不同的嵌入矩阵）的多分辨率表中。每一层表都有`T`个维度为`F`的特征向量（即一个`T
    x F`大小的矩阵）。通常，这些参数遵循以下所示的设置。
- en: '![](../Images/8616752a4ef99a20f3dbd88a74a4e1d5.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8616752a4ef99a20f3dbd88a74a4e1d5.png)'
- en: (from [2])
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: (来源于 [2])
- en: Each of the levels is meant to represent 3D space at a different resolution,
    from `N-min` (lowest resolution) to `N-max` (highest resolution). We can think
    of this as dividing 3D space into [voxel grids](https://cameronrwolfe.substack.com/i/94634004/representing-d-shapes)
    at different levels of granularity (e.g., `N-min` will use very large/coarse voxels).
    By partitioning 3D space in this way, we can determine the voxel in which an input
    coordinate resides—this will be different for every resolution level. The voxel
    in which a coordinate resides is then used to map this coordinate to an entry
    in each level’s embedding matrix.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 每个级别旨在以不同的分辨率表示3D空间，从`N-min`（最低分辨率）到`N-max`（最高分辨率）。我们可以将其视为将3D空间划分为不同粒度的[体素网格](https://cameronrwolfe.substack.com/i/94634004/representing-d-shapes)（例如，`N-min`将使用非常大/粗糙的体素）。通过这种方式划分3D空间，我们可以确定输入坐标所在的体素——这对于每个分辨率级别都是不同的。坐标所在的体素随后用于将该坐标映射到每个级别的嵌入矩阵中的一个条目。
- en: '![](../Images/dfa2b985f730a98b41b2ac9152d9fddd.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dfa2b985f730a98b41b2ac9152d9fddd.png)'
- en: (from [2])
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: (来源于 [2])
- en: More specifically, authors in [2] use the hash function shown above to map voxel
    locations (i.e., given by the coordinates of a voxel’s edge) to indices for entries
    in the embedding matrix at each resolution level. Notably, levels with coarser
    resolution (i.e., larger voxels) will have fewer hash collisions, meaning that
    it is less likely for input coordinates at completely different locations to be
    mapped to the same feature vector.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，[2]中的作者使用上面显示的哈希函数将体素位置（即，由体素边缘的坐标给出）映射到每个分辨率级别的嵌入矩阵中的条目索引。值得注意的是，分辨率较粗的级别（即，较大的体素）将具有较少的哈希冲突，这意味着完全不同位置的输入坐标被映射到相同特征向量的可能性较小。
- en: '![](../Images/0397cb039d736fdfdeb775bacbf70c12.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0397cb039d736fdfdeb775bacbf70c12.png)'
- en: (from [2])
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: (来源于 [2])
- en: After we retrieve the corresponding feature vector at each level of resolution,
    we have multiple feature vectors corresponding to a single input coordinate. To
    combine these vectors, we linearly interpolate them, where the weights of this
    interpolation are derived using the relative position of the input coordinate
    within each level’s voxel. From here, we concatenate these vectors with other
    input information (e.g., the positionally encoded viewing direction) to form the
    final input! The full multi-resolution approach in InstantNGP is illustrated within
    the figure above.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们检索到每个分辨率级别的相应特征向量后，我们会得到多个特征向量对应于单个输入坐标。为了合并这些向量，我们进行线性插值，其中插值的权重是通过输入坐标在每个级别的体素中的相对位置得出的。从这里开始，我们将这些向量与其他输入信息（例如，位置编码的视角方向）拼接起来，形成最终输入！InstantNGP中的完整多分辨率方法如上图所示。
- en: '![](../Images/10ef757aa97375f97112aa07e141e0ac.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10ef757aa97375f97112aa07e141e0ac.png)'
- en: (from [2])
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: (来源于 [2])
- en: Due to using higher-quality, learnable input features, InstantNGP is able to
    use much smaller feed-forward networks relative to NeRF while achieving similar
    results in terms of quality; see above. When these modifications are combined
    with a more efficient implementation (i.e., fully-fused cuda kernels that minimize
    bandwidth and wasted operations), the training time for NeRFs can be drastically
    reduced. In fact, *we can use InstantNGP to obtain high-quality scene representations
    in a matter of seconds*.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 由于使用了更高质量的可学习输入特征，InstantNGP能够相对于NeRF使用更小的前馈网络，同时在质量上取得类似的结果；见上文。当这些修改与更高效的实现（即，完全融合的cuda内核，最小化带宽和浪费操作）结合时，NeRF的训练时间可以显著缩短。事实上，*我们可以在几秒钟内使用InstantNGP获得高质量的场景表示*。
- en: '**results.** InstantNGP trains a NeRF using a nearly identical setup as proposed
    in [4], aside from the modified input encoding scheme and smaller feed-forward
    neural network. Coordinate inputs are encoded using a multi-resolution hash table,
    while the viewing direction is encoded using normal, positional embeddings.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**结果。** InstantNGP使用与[4]中提出的几乎相同的设置来训练NeRF，除了修改的输入编码方案和更小的前馈神经网络。坐标输入使用多分辨率哈希表进行编码，而视角方向则使用普通的、位置编码的嵌入进行编码。'
- en: '![](../Images/5908e130cb652443b322e0a97dadb114.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5908e130cb652443b322e0a97dadb114.png)'
- en: (from [2])
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: (来源于 [2])
- en: Using the proposed approach and a faster rendering procedure, authors in [2]
    find that InstantNGP can train scene representations in seconds and even render
    scenes at 60 FPS! This is a massive improvement in efficiency relative to NeRF;
    see above. Notably, InstantNGP becomes competitive with NeRF (which takes hours
    to train) *after only 15 seconds of training*!
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 使用提出的方法和更快的渲染程序，[2]中的作者发现 InstantNGP 可以在几秒钟内训练场景表示，甚至可以以 60 FPS 渲染场景！这相对于 NeRF
    是一个巨大的效率提升；详见上文。值得注意的是，InstantNGP 在训练仅 15 秒后就能与 NeRF（需要数小时训练）竞争，*表现突出*！
- en: To determine if this speedup comes from the more efficient cuda implementation
    or the multi-resolution hash table, the authors do some analysis. They find that
    the efficient implementation does provide a large speedup, but using the hash
    table and smaller feed-forward network alone yields a 20X-60X speedup in training
    NeRFs.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定这种加速是否来源于更高效的 cuda 实现或多分辨率哈希表，作者进行了一些分析。他们发现高效的实现确实提供了很大的加速，但仅使用哈希表和较小的前馈网络即可在训练
    NeRFs 时获得 20 倍到 60 倍的加速。
- en: '*“We replace the hash encoding by the frequency encoding and enlarge the MLP
    to approximately match the architecture of [NeRF]… This version of our algorithm
    approaches NeRF’s quality after training for just ∼5 min, yet is outperformed
    by our full method after training for a much shorter duration (5s–15s), amounting
    to a 20–60X improvement caused by the hash encoding and smaller MLP.”* — from
    [2]'
  id: totrans-105
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“我们用频率编码替代了哈希编码，并扩大了 MLP 以大致匹配[NeRF]的架构……我们算法的这个版本在训练约 ∼5 分钟后接近 NeRF 的质量，但在训练更短时间（5
    秒至 15 秒）后被我们的完整方法超越，这得益于哈希编码和较小的 MLP，使得效率提高了 20 倍到 60 倍。”* — 摘自[2]'
- en: In certain cases, we do see that baseline methods outperform InstantNGP in scenes
    that contain complex, view-dependent reflections and [non-Lambertian](https://en.wikipedia.org/wiki/Lambertian_reflectance)
    effects. The authors claim this is due to the use of a smaller feed-forward network
    in [2]; see below.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，我们确实看到基准方法在包含复杂的视角依赖反射和[非朗伯](https://en.wikipedia.org/wiki/Lambertian_reflectance)效应的场景中优于
    InstantNGP。作者声称这是由于[2]中使用了较小的前馈网络；详见下文。
- en: '![](../Images/2f03238dd843e73b9297cf1cb9963deb.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f03238dd843e73b9297cf1cb9963deb.png)'
- en: (from [2])
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: （摘自[2]）
- en: '**what else can we use this for?** Although we are focusing upon improvements
    to NeRF, the approach of InstantNGP is quite generic — it can improve the efficiency
    of a variety of computer graphics primitives (i.e., functions that characterize
    appearance). For example, InstantNGP is shown in [2] to be effective at:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们还可以用它做什么？** 尽管我们专注于改进 NeRF，但 InstantNGP 的方法相当通用——它可以提高各种计算机图形原语（即，描述外观的函数）的效率。例如，InstantNGP
    在[2]中被证明在以下方面有效：'
- en: Generating [super-resolution](http://www.infognition.com/articles/what_is_super_resolution.html)
    images
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成[超分辨率](http://www.infognition.com/articles/what_is_super_resolution.html)图像
- en: Modeling [signed distance functions](https://cameronrwolfe.substack.com/i/94634004/signed-distance-functions)
    (SDFs)
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建模[签名距离函数](https://cameronrwolfe.substack.com/i/94634004/signed-distance-functions)（SDFs）
- en: Performing [neural radiance caching](https://research.nvidia.com/publication/2021-06_real-time-neural-radiance-caching-path-tracing)
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行[神经辐射缓存](https://research.nvidia.com/publication/2021-06_real-time-neural-radiance-caching-path-tracing)
- en: Takeaways
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 收获
- en: Although NeRFs revolutionized the quality of neural scene representations, we
    have seen within this overview that there is a lot of room for improvement! NeRFs
    still take a long time to train and require a lot of training data to work well.
    Some of the basic takeaways of how we can mitigate these problems are outlined
    below.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 NeRFs 革新了神经场景表示的质量，但在本综述中我们看到还有很大的改进空间！NeRFs 仍然需要很长时间来训练，并且需要大量的训练数据才能良好工作。下面概述了一些减轻这些问题的基本要点。
- en: '**Improving sample complexity.** In their original form, NeRFs require many
    input observations of an underlying scene to perform view synthesis. This is mainly
    because NeRFs are trained separately per-scene, preventing any prior information
    from being used in generating novel views. PixelNeRF [1] mitigates this problem
    by adding pre-trained image features as input to NeRF’s feed-forward network.
    Such an approach allows learned, prior information from other training data to
    be leveraged. As a result, this method can produce scene representations given
    only a few images as input!'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**提高样本复杂性。** 在其原始形式中，NeRF 需要大量的输入观察来进行视图合成。这主要是因为 NeRF 是逐场景训练的，无法利用任何先前的信息来生成新的视图。PixelNeRF
    [1] 通过将预训练的图像特征作为输入添加到 NeRF 的前馈网络中来缓解这个问题。这种方法允许利用来自其他训练数据的学习到的先验信息。因此，这种方法可以仅凭几张图像就生成场景表示！'
- en: '**Higher-quality input goes a long way!** As shown by InstantNGP [2], the input
    encoding scheme used by NeRF is incredibly important. Using a richer, learnable
    encoding scheme for our input allows us to reduce the size of the feed-forward
    network, which yields significant gains in training and rendering efficiency.
    In my opinion, such a finding can inspire a lot of future work. *Can we find an
    encoding scheme that’s even better? Are there other types of deep learning models
    to which this concept can be applied?*'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**更高质量的输入非常重要！** 正如 InstantNGP [2] 所示，NeRF 使用的输入编码方案至关重要。使用更丰富、可学习的编码方案可以缩小前馈网络的大小，从而在训练和渲染效率上取得显著提升。在我看来，这种发现可以激发未来大量的工作。*我们能找到更好的编码方案吗？是否有其他类型的深度学习模型可以应用这个概念？*'
- en: '**Limitations.** The approaches we have seen in this overview do a lot to solve
    known limitations of NeRF, but they are not perfect. InstantNGP provides incredible
    speedups in NeRF training time, but the quality of the resulting scene representations
    aren’t always the best. InstantNGP struggles to capture complex effects like reflections
    compared to baselines, revealing that we sacrifice representation quality for
    faster training.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**局限性。** 我们在此概述中看到的方法在解决 NeRF 已知的局限性方面做了很多努力，但它们并不完美。InstantNGP 在 NeRF 训练时间上提供了令人难以置信的加速，但结果场景表示的质量并不总是最佳的。与基线相比，InstantNGP
    在捕捉复杂效果如反射方面表现不佳，这表明我们为更快的训练牺牲了表示质量。'
- en: '*“On one hand, our method performs best on scenes with high geometric detail…
    On the other hand, mip-NeRF and NSVF outperform our method on scenes with complex,
    view-dependent reflections… we attribute this to the much smaller MLP that we
    necessarily employ to obtain our speedup of several orders of magnitude over these
    competing implementations.”* — from [2]'
  id: totrans-118
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“一方面，我们的方法在几何细节丰富的场景中表现最佳… 另一方面，mip-NeRF 和 NSVF 在具有复杂视角依赖反射的场景中超越了我们的方法… 我们将此归因于我们为了获得比这些竞争实现快几个数量级的速度提升而必然使用的更小的MLP。”*
    — 来自 [2]'
- en: Additionally, PixelNeRF [1] — due to processing each input image separately
    in its initial feed-forward component — has a runtime that increases linearly
    with the number of views used as input. Such a linear dependence can cause both
    training and rendering to be quite slow. Thus, we can solve some major problems
    with NeRFs, but it might come at a slight cost!
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于 PixelNeRF [1] 在其初始前馈组件中分别处理每个输入图像，其运行时间随输入视图数量线性增长。这种线性依赖性可能导致训练和渲染速度相当慢。因此，我们可以解决一些
    NeRF 的主要问题，但可能会付出一些代价！
- en: Closing Thoughts
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结语
- en: Thanks so much for reading this article. I am [Cameron R. Wolfe](https://cameronrwolfe.me/),
    Director of AI at [Rebuy](https://www.rebuyengine.com/). I study the empirical
    and theoretical foundations of deep learning. You can also check out my [other
    writings](https://medium.com/@wolfecameron) on medium! If you liked it, please
    follow me on [twitter](https://twitter.com/cwolferesearch) or subscribe to my
    [Deep (Learning) Focus newsletter](https://cameronrwolfe.substack.com/), where
    I help readers build a deeper understanding of topics in deep learning research
    via understandable overviews of popular papers.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢你阅读这篇文章。我是 [Cameron R. Wolfe](https://cameronrwolfe.me/)，[Rebuy](https://www.rebuyengine.com/)
    的 AI 总监。我研究深度学习的经验和理论基础。你也可以查看我在 medium 上的 [其他著作](https://medium.com/@wolfecameron)！如果你喜欢这篇文章，请在
    [twitter](https://twitter.com/cwolferesearch) 上关注我或订阅我的 [Deep (Learning) Focus
    新闻通讯](https://cameronrwolfe.substack.com/)，我通过对流行论文的易懂概述来帮助读者更深入地理解深度学习研究中的主题。
- en: Bibliography
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Yu, Alex, et al. “pixelnerf: Neural radiance fields from one or few images.”
    *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*.
    2021.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Yu, Alex 等人。“pixelnerf: 从一张或几张图片中生成神经辐射场。” *IEEE/CVF计算机视觉与模式识别会议论文集*。2021年。'
- en: '[2] Müller, Thomas, et al. “Instant neural graphics primitives with a multiresolution
    hash encoding.” *ACM Transactions on Graphics (ToG)* 41.4 (2022): 1–15.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Müller, Thomas 等人。“具有多分辨率哈希编码的即时神经图形原语。” *ACM图形学汇刊（ToG）* 41.4（2022年）：1–15。'
- en: '[3] Lin, Tsung-Yi, et al. “Feature pyramid networks for object detection.”
    *Proceedings of the IEEE conference on computer vision and pattern recognition*.
    2017.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Lin, Tsung-Yi 等人。“用于目标检测的特征金字塔网络。” *IEEE计算机视觉与模式识别会议论文集*。2017年。'
- en: '[4] Mildenhall, Ben, et al. “Nerf: Representing scenes as neural radiance fields
    for view synthesis.” *Communications of the ACM* 65.1 (2021): 99–106.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Mildenhall, Ben 等人。“Nerf：将场景表示为神经辐射场以进行视图合成。” *ACM通讯* 65.1（2021年）：99–106。'
- en: '[5] Sitzmann, Vincent, Michael Zollhöfer, and Gordon Wetzstein. “Scene representation
    networks: Continuous 3d-structure-aware neural scene representations.” *Advances
    in Neural Information Processing Systems* 32 (2019).'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Sitzmann, Vincent, Michael Zollhöfer 和 Gordon Wetzstein。“场景表示网络：连续的3D结构感知神经场景表示。”
    *神经信息处理系统进展* 32（2019年）。'
- en: '[6] Vaswani, Ashish, et al. “Attention is all you need.” *Advances in neural
    information processing systems* 30 (2017).'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Vaswani, Ashish 等人。“注意力机制是你所需的一切。” *神经信息处理系统进展* 30（2017年）。'
