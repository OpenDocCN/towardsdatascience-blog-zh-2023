- en: Fine-Tuning Sentence Transformers with MNR Loss
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调句子变换器与 MNR 损失
- en: 原文：[https://towardsdatascience.com/fine-tuning-sentence-transformers-with-mnr-loss-cd6a26685b81?source=collection_archive---------5-----------------------#2023-02-03](https://towardsdatascience.com/fine-tuning-sentence-transformers-with-mnr-loss-cd6a26685b81?source=collection_archive---------5-----------------------#2023-02-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/fine-tuning-sentence-transformers-with-mnr-loss-cd6a26685b81?source=collection_archive---------5-----------------------#2023-02-03](https://towardsdatascience.com/fine-tuning-sentence-transformers-with-mnr-loss-cd6a26685b81?source=collection_archive---------5-----------------------#2023-02-03)
- en: '[NLP For Semantic Search](https://jamescalam.medium.com/list/nlp-for-semantic-search-d3a4b96a52fe)'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[NLP 语义搜索](https://jamescalam.medium.com/list/nlp-for-semantic-search-d3a4b96a52fe)'
- en: Next-Gen Sentence Embeddings with Multiple Negatives Ranking Loss
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下一代句子嵌入与多负样本排序损失
- en: '[](https://jamescalam.medium.com/?source=post_page-----cd6a26685b81--------------------------------)[![James
    Briggs](../Images/cb34b7011748e4d8607b7ff4a8510a93.png)](https://jamescalam.medium.com/?source=post_page-----cd6a26685b81--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cd6a26685b81--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cd6a26685b81--------------------------------)
    [James Briggs](https://jamescalam.medium.com/?source=post_page-----cd6a26685b81--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://jamescalam.medium.com/?source=post_page-----cd6a26685b81--------------------------------)[![James
    Briggs](../Images/cb34b7011748e4d8607b7ff4a8510a93.png)](https://jamescalam.medium.com/?source=post_page-----cd6a26685b81--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cd6a26685b81--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cd6a26685b81--------------------------------)
    [James Briggs](https://jamescalam.medium.com/?source=post_page-----cd6a26685b81--------------------------------)'
- en: ·
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb9d77a4ca1d1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-sentence-transformers-with-mnr-loss-cd6a26685b81&user=James+Briggs&userId=b9d77a4ca1d1&source=post_page-b9d77a4ca1d1----cd6a26685b81---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cd6a26685b81--------------------------------)
    ·8 min read·Feb 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcd6a26685b81&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-sentence-transformers-with-mnr-loss-cd6a26685b81&user=James+Briggs&userId=b9d77a4ca1d1&source=-----cd6a26685b81---------------------clap_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb9d77a4ca1d1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-sentence-transformers-with-mnr-loss-cd6a26685b81&user=James+Briggs&userId=b9d77a4ca1d1&source=post_page-b9d77a4ca1d1----cd6a26685b81---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cd6a26685b81--------------------------------)
    ·8 分钟阅读·2023 年 2 月 3 日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcd6a26685b81&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-sentence-transformers-with-mnr-loss-cd6a26685b81&user=James+Briggs&userId=b9d77a4ca1d1&source=-----cd6a26685b81---------------------clap_footer-----------)'
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcd6a26685b81&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-sentence-transformers-with-mnr-loss-cd6a26685b81&source=-----cd6a26685b81---------------------bookmark_footer-----------)![](../Images/1281f41ecb283e0989e5a4230b2c20f6.png)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcd6a26685b81&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Ffine-tuning-sentence-transformers-with-mnr-loss-cd6a26685b81&source=-----cd6a26685b81---------------------bookmark_footer-----------)![](../Images/1281f41ecb283e0989e5a4230b2c20f6.png)'
- en: Photo by [Lars Kienle](https://unsplash.com/@larskienle?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral). [Article
    originally published on pinecone.io](https://www.pinecone.io/learn/fine-tune-sentence-transformers-mnr/)
    where the author is employed.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Lars Kienle](https://unsplash.com/@larskienle?utm_source=medium&utm_medium=referral)
    提供，发布在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)。
    [文章最初发布于 pinecone.io](https://www.pinecone.io/learn/fine-tune-sentence-transformers-mnr/)
    作者在该网站工作。
- en: Transformer-produced sentence embeddings have come a long way in a very short
    time. Starting with the slow but accurate similarity prediction of BERT cross-encoders,
    the world of [sentence embeddings](https://apps-showcase--optimistic-curran-b817a8.netlify.app/learn/sentence-embeddings/)
    was ignited with the introduction of SBERT in 2019 [1]. Since then, many more
    sentence transformers have been introduced. These models quickly made the original
    SBERT obsolete.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer生成的句子嵌入在短时间内取得了长足的进步。从BERT交叉编码器的缓慢但准确的相似性预测开始，随着2019年SBERT的引入，[句子嵌入](https://apps-showcase--optimistic-curran-b817a8.netlify.app/learn/sentence-embeddings/)的世界被点燃。此后，许多新的句子变换器相继问世。这些模型很快使得最初的SBERT过时。
- en: How did these newer sentence transformers manage to outperform SBERT so quickly?
    The answer is *multiple negatives ranking (MNR) loss*.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这些新型句子变换器如何迅速超越SBERT？答案是*多重负样本排序（MNR）损失*。
- en: This article will cover what MNR loss is, the data it requires, and how to implement
    it to fine-tune our own high-quality sentence transformers.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将涵盖MNR损失是什么、所需数据以及如何实施它来微调我们自己的高质量句子变换器。
- en: Implementation will cover two training approaches. The first is more involved
    and outlines the exact steps to fine-tune the model. The second approach makes
    use of the `sentence-transformers` library’s excellent utilities for fine-tuning.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 实施将涵盖两种训练方法。第一种方法更为详细，概述了微调模型的确切步骤。第二种方法利用了`sentence-transformers`库中出色的微调工具。
