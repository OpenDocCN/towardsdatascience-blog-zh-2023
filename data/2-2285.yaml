- en: Exploding & Vanishing Gradient Problem in Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ·±åº¦å­¦ä¹ ä¸­çš„æ¢¯åº¦çˆ†ç‚¸ä¸æ¶ˆå¤±é—®é¢˜
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/vanishing-exploding-gradient-problem-neural-networks-101-c8f48ec6a80b](https://towardsdatascience.com/vanishing-exploding-gradient-problem-neural-networks-101-c8f48ec6a80b)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/vanishing-exploding-gradient-problem-neural-networks-101-c8f48ec6a80b](https://towardsdatascience.com/vanishing-exploding-gradient-problem-neural-networks-101-c8f48ec6a80b)
- en: How to ensure your neural network doesnâ€™t â€œdieâ€ or â€œblow-upâ€
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¦‚ä½•ç¡®ä¿ä½ çš„ç¥ç»ç½‘ç»œä¸ä¼šâ€œæ­»æ‰â€æˆ–â€œçˆ†ç‚¸â€
- en: '[](https://medium.com/@egorhowell?source=post_page-----c8f48ec6a80b--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----c8f48ec6a80b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c8f48ec6a80b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c8f48ec6a80b--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----c8f48ec6a80b--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@egorhowell?source=post_page-----c8f48ec6a80b--------------------------------)[![Egor
    Howell](../Images/1f796e828f1625440467d01dcc3e40cd.png)](https://medium.com/@egorhowell?source=post_page-----c8f48ec6a80b--------------------------------)[](https://towardsdatascience.com/?source=post_page-----c8f48ec6a80b--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----c8f48ec6a80b--------------------------------)
    [Egor Howell](https://medium.com/@egorhowell?source=post_page-----c8f48ec6a80b--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c8f48ec6a80b--------------------------------)
    Â·9 min readÂ·Dec 8, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘å¸ƒäº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----c8f48ec6a80b--------------------------------)
    Â·é˜…è¯»æ—¶é—´ 9 åˆ†é’ŸÂ·2023å¹´12æœˆ8æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/8ec02d8429b5e34801dd4ccb016d5e74.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8ec02d8429b5e34801dd4ccb016d5e74.png)'
- en: '[https://www.flaticon.com/free-icons/neural-network](https://www.flaticon.com/free-icons/neural-network).
    title=â€neural network icons.â€ Neural network icons created by Paul J. â€” Flaticon.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.flaticon.com/free-icons/neural-network](https://www.flaticon.com/free-icons/neural-network)ã€‚æ ‡é¢˜=â€ç¥ç»ç½‘ç»œå›¾æ ‡ã€‚â€
    ç¥ç»ç½‘ç»œå›¾æ ‡ç”± Paul J. åˆ›å»º â€” Flaticonã€‚'
- en: What are Vanishing & Exploding Gradients?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯æ¢¯åº¦æ¶ˆå¤±ä¸æ¢¯åº¦çˆ†ç‚¸ï¼Ÿ
- en: 'In one of my previous posts, we explained neural networks learn through the
    backpropagation algorithm. The main idea is that we start on the output layer
    and move or â€œpropagateâ€ the error all the way to the input layer updating the
    weights with respect to the loss function as we go. If you are unfamiliar with
    this, then I highly recommend you check that post:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä¹‹å‰çš„ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬è§£é‡Šäº†ç¥ç»ç½‘ç»œå¦‚ä½•é€šè¿‡åå‘ä¼ æ’­ç®—æ³•è¿›è¡Œå­¦ä¹ ã€‚ä¸»è¦æ€è·¯æ˜¯æˆ‘ä»¬ä»è¾“å‡ºå±‚å¼€å§‹ï¼Œå°†è¯¯å·®â€œä¼ æ’­â€åˆ°è¾“å…¥å±‚ï¼Œåœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­æ ¹æ®æŸå¤±å‡½æ•°æ›´æ–°æƒé‡ã€‚å¦‚æœä½ å¯¹æ­¤ä¸ç†Ÿæ‚‰ï¼Œæˆ‘å¼ºçƒˆå»ºè®®ä½ æŸ¥çœ‹é‚£ç¯‡æ–‡ç« ï¼š
- en: '[](/forward-pass-backpropagation-neural-networks-101-3a75996ada3b?source=post_page-----c8f48ec6a80b--------------------------------)
    [## Forward Pass & Backpropagation: Neural Networks 101'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/forward-pass-backpropagation-neural-networks-101-3a75996ada3b?source=post_page-----c8f48ec6a80b--------------------------------)
    [## å‰å‘ä¼ æ’­ä¸åå‘ä¼ æ’­ï¼šç¥ç»ç½‘ç»œ 101'
- en: Explaining how neural networks â€œtrainâ€ and â€œlearnâ€ patterns in data by hand
    and in code using PyTorch
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è§£é‡Šç¥ç»ç½‘ç»œå¦‚ä½•é€šè¿‡æ‰‹åŠ¨å’Œä½¿ç”¨ PyTorch çš„ä»£ç â€œè®­ç»ƒâ€å’Œâ€œå­¦ä¹ â€æ•°æ®ä¸­çš„æ¨¡å¼
- en: towardsdatascience.com](/forward-pass-backpropagation-neural-networks-101-3a75996ada3b?source=post_page-----c8f48ec6a80b--------------------------------)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/forward-pass-backpropagation-neural-networks-101-3a75996ada3b?source=post_page-----c8f48ec6a80b--------------------------------)'
- en: The weights are updated using their partial derivative with respect to the loss
    function. The problem is that these gradients get smaller and smaller as we approach
    the lower layers of the network. This leads to the lower layersâ€™ weights barely
    changing when training the network. This is known as the *vanishing gradient problem.*
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æƒé‡é€šè¿‡å…¶ç›¸å¯¹äºæŸå¤±å‡½æ•°çš„åå¯¼æ•°è¿›è¡Œæ›´æ–°ã€‚é—®é¢˜åœ¨äºï¼Œå½“æˆ‘ä»¬æ¥è¿‘ç½‘ç»œçš„ä¸‹å±‚æ—¶ï¼Œè¿™äº›æ¢¯åº¦å˜å¾—è¶Šæ¥è¶Šå°ã€‚è¿™å¯¼è‡´ä¸‹å±‚çš„æƒé‡åœ¨è®­ç»ƒç½‘ç»œæ—¶å‡ ä¹æ²¡æœ‰å˜åŒ–ã€‚è¿™è¢«ç§°ä¸º*æ¢¯åº¦æ¶ˆå¤±é—®é¢˜*ã€‚
- en: The opposite can be true where gradients continue getting larger through the
    layers. This is the *exploding gradient problem* whichis mainly an issue in [***recurrent
    neural networks***](https://en.wikipedia.org/wiki/Recurrent_neural_network).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸åï¼Œæ¢¯åº¦ä¹Ÿå¯èƒ½åœ¨å„å±‚ä¸­æŒç»­å¢å¤§ã€‚è¿™å°±æ˜¯*æ¢¯åº¦çˆ†ç‚¸é—®é¢˜*ï¼Œå®ƒä¸»è¦æ˜¯[***é€’å½’ç¥ç»ç½‘ç»œ***](https://en.wikipedia.org/wiki/Recurrent_neural_network)ä¸­çš„ä¸€ä¸ªé—®é¢˜ã€‚
- en: However, a [***paper***](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
    published by [***Xavier Glorot***](https://www.linkedin.com/in/xglorot/?originalSubdomain=ca)
    and [***Yoshua Bengio***](https://en.wikipedia.org/wiki/Yoshua_Bengio) in 2010
    diagnosed several reasons why this is happening to the gradients. The main culprits
    were the [***sigmoid activation function***](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701)and
    how weights are initialised (typically from the standard normal distribution).
    This combination leads to the variances changing between layers until they *saturate*
    at the extreme edges of the sigmoid function.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œ2010å¹´ç”±[***Xavier Glorot***](https://www.linkedin.com/in/xglorot/?originalSubdomain=ca)å’Œ[***Yoshua
    Bengio***](https://en.wikipedia.org/wiki/Yoshua_Bengio)å‘å¸ƒçš„[***è®ºæ–‡***](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)è¯Šæ–­äº†å¯¼è‡´æ¢¯åº¦å‡ºç°é—®é¢˜çš„å‡ ä¸ªåŸå› ã€‚ä¸»è¦çš„ç½ªé­ç¥¸é¦–æ˜¯[***sigmoidæ¿€æ´»å‡½æ•°***](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701)å’Œæƒé‡çš„åˆå§‹åŒ–æ–¹å¼ï¼ˆé€šå¸¸æ¥è‡ªæ ‡å‡†æ­£æ€åˆ†å¸ƒï¼‰ã€‚è¿™ç§ç»„åˆå¯¼è‡´äº†å±‚é—´æ–¹å·®çš„å˜åŒ–ï¼Œç›´åˆ°å®ƒä»¬åœ¨sigmoidå‡½æ•°çš„æç«¯è¾¹ç¼˜*é¥±å’Œ*ã€‚
- en: Below is the mathematical equation and plot of the sigmoid function. Notice
    that in its extremes, the gradient becomes zero. Therefore, no â€œlearningâ€ is done
    at these saturation points.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯sigmoidå‡½æ•°çš„æ•°å­¦æ–¹ç¨‹å’Œå›¾ç¤ºã€‚è¯·æ³¨æ„ï¼Œåœ¨å…¶æç«¯æƒ…å†µä¸‹ï¼Œæ¢¯åº¦å˜ä¸ºé›¶ã€‚å› æ­¤ï¼Œåœ¨è¿™äº›é¥±å’Œç‚¹æ²¡æœ‰â€œå­¦ä¹ â€å‘ç”Ÿã€‚
- en: '![](../Images/29fff662bec33adab7258d011d2e9db6.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29fff662bec33adab7258d011d2e9db6.png)'
- en: Sigmoid function. Equation by author in LaTeX.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoidå‡½æ•°ã€‚æ–¹ç¨‹ç”±ä½œè€…ä½¿ç”¨LaTeXç¼–å†™ã€‚
- en: '![](../Images/b71b01559c120a8967c3bf80bc04c16f.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b71b01559c120a8967c3bf80bc04c16f.png)'
- en: Sigmoid function. Plot generated by author in Python.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoidå‡½æ•°ã€‚ç”±ä½œè€…åœ¨Pythonä¸­ç”Ÿæˆçš„å›¾ç¤ºã€‚
- en: We will now go through some techniques that can reduce the chance of our gradients
    vanishing or exploding during training.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å°†ä»‹ç»ä¸€äº›å¯ä»¥å‡å°‘è®­ç»ƒè¿‡ç¨‹ä¸­æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸çš„æŠ€æœ¯ã€‚
- en: 'If you want to learn more about activation functions along with their pros
    and cons, check my previous post on the subject:'
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³äº†è§£æ›´å¤šå…³äºæ¿€æ´»å‡½æ•°åŠå…¶ä¼˜ç¼ºç‚¹çš„ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹æˆ‘ä¹‹å‰çš„å¸–å­ï¼š
- en: '[](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701?source=post_page-----c8f48ec6a80b--------------------------------)
    [## Activation Functions & Non-Linearity: Neural Networks 101'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701?source=post_page-----c8f48ec6a80b--------------------------------)
    [## æ¿€æ´»å‡½æ•°ä¸éçº¿æ€§ï¼šç¥ç»ç½‘ç»œ101'
- en: Explaining why neural networks can learn (nearly) anything and everything
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: è§£é‡Šç¥ç»ç½‘ç»œä¸ºä½•èƒ½å­¦ä¹ ï¼ˆå‡ ä¹ï¼‰ä»»ä½•å’Œä¸€åˆ‡
- en: towardsdatascience.com](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701?source=post_page-----c8f48ec6a80b--------------------------------)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/activation-functions-non-linearity-neural-networks-101-ab0036a2e701?source=post_page-----c8f48ec6a80b--------------------------------)
- en: Glorot/Xavier Initialisation
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Glorot/Xavieråˆå§‹åŒ–
- en: Fortunately, the authors of the paper suggested methods to neutralise the above
    problem. They suggested a new initialisation method for the weights, named Glorot
    initialisation after the author, that ensures the variance between layers remains
    constant.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¸è¿çš„æ˜¯ï¼Œè®ºæ–‡çš„ä½œè€…æå‡ºäº†ä¸­å’Œä¸Šè¿°é—®é¢˜çš„æ–¹æ³•ã€‚ä»–ä»¬å»ºè®®äº†ä¸€ç§æ–°çš„æƒé‡åˆå§‹åŒ–æ–¹æ³•ï¼Œå‘½åä¸ºGlorotåˆå§‹åŒ–ï¼Œä»¥ä½œè€…å‘½åï¼Œç¡®ä¿å±‚é—´çš„æ–¹å·®ä¿æŒä¸å˜ã€‚
- en: 'The paper linked above has the full mathematical details, but their proposed
    initialisation strategy was:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°è®ºæ–‡ä¸­åŒ…å«äº†å®Œæ•´çš„æ•°å­¦ç»†èŠ‚ï¼Œä½†ä»–ä»¬æå‡ºçš„åˆå§‹åŒ–ç­–ç•¥æ˜¯ï¼š
- en: '**Normal Distribution**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­£æ€åˆ†å¸ƒ**'
- en: For a normal distribution*,* ***X ~ N(0, ğœÂ²)****,* the weights will be initialised
    as follows*:*
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ­£æ€åˆ†å¸ƒ***X ~ N(0, ğœÂ²)***ï¼Œæƒé‡å°†å¦‚ä¸‹åˆå§‹åŒ–ï¼š
- en: '![](../Images/5e33362e36072d25bec235c65713679a.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5e33362e36072d25bec235c65713679a.png)'
- en: Mean and variance for Glorot normal distribution initialisation. Equation by
    author in LaTeX.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Glorotæ­£æ€åˆ†å¸ƒåˆå§‹åŒ–çš„å‡å€¼å’Œæ–¹å·®ã€‚æ–¹ç¨‹ç”±ä½œè€…ä½¿ç”¨LaTeXç¼–å†™ã€‚
- en: If ***n_in*** ***=*** ***n_out*** then we have [***LeCun initialisation***](https://wandb.ai/sauravmaheshkar/initialization/reports/A-Gentle-Introduction-To-Weight-Initialization-for-Neural-Networks--Vmlldzo2ODExMTg)named
    after the computer scientist [***Yann LeCun***](https://en.wikipedia.org/wiki/Yann_LeCun).
    This initialisation was proposed in the 1990s, a decade before Glorotâ€™s paper.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœ***n_in*** ***=*** ***n_out***ï¼Œåˆ™æˆ‘ä»¬æœ‰[***LeCunåˆå§‹åŒ–***](https://wandb.ai/sauravmaheshkar/initialization/reports/A-Gentle-Introduction-To-Weight-Initialization-for-Neural-Networks--Vmlldzo2ODExMTg)ä»¥è®¡ç®—æœºç§‘å­¦å®¶[***Yann
    LeCun***](https://en.wikipedia.org/wiki/Yann_LeCun)å‘½åã€‚è¿™ä¸ªåˆå§‹åŒ–æ–¹æ³•æ˜¯åœ¨Glorotçš„è®ºæ–‡å‘è¡¨å‰åå¹´ï¼Œå³1990å¹´ä»£æå‡ºçš„ã€‚
- en: '![](../Images/742f40143e3cdca52d66f0ba0d4d427b.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/742f40143e3cdca52d66f0ba0d4d427b.png)'
- en: LeCun initialisation. Equation by author in LaTeX.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: LeCunåˆå§‹åŒ–ã€‚æ–¹ç¨‹ç”±ä½œè€…ä½¿ç”¨LaTeXç¼–å†™ã€‚
- en: '**Uniform Distribution**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**å‡åŒ€åˆ†å¸ƒ**'
- en: For a uniform distribution ***X ~ U(-a, a)****:*
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå‡åŒ€åˆ†å¸ƒ***X ~ U(-a, a)***ï¼š
- en: '![](../Images/a93c6c22f95680d5b3463ea4a9cbd8d7.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a93c6c22f95680d5b3463ea4a9cbd8d7.png)'
- en: Mean and variance for Glorot uniform distribution initialisation. Equation by
    author in LaTeX.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Glorotå‡åŒ€åˆ†å¸ƒåˆå§‹åŒ–çš„å‡å€¼å’Œæ–¹å·®ã€‚æ–¹ç¨‹ç”±ä½œè€…åœ¨LaTeXä¸­ç¼–å†™ã€‚
- en: Setting up our neural networks using these initialisations leads them to converge
    faster since the weights are not too small or large at the beginning of training.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¿™äº›åˆå§‹åŒ–è®¾ç½®æˆ‘ä»¬çš„ç¥ç»ç½‘ç»œå¯ä»¥æ›´å¿«åœ°æ”¶æ•›ï¼Œå› ä¸ºåœ¨è®­ç»ƒå¼€å§‹æ—¶æƒé‡ä¸ä¼šè¿‡å°æˆ–è¿‡å¤§ã€‚
- en: The above expressions are only valid for the sigmoid and [***tanh***](https://medium.com/towards-data-science/activation-functions-non-linearity-neural-networks-101-ab0036a2e701)
    activation functions. For example, for the [***ReLU***](/breaking-linearity-with-relu-d2cfa7ebf264)(rectified
    linear unit) activation function, the normal distribution variance needs to be
    initialised using ***1/n_in.***
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸Šè¡¨è¾¾å¼ä»…é€‚ç”¨äºsigmoidå’Œ[***tanh***](https://medium.com/towards-data-science/activation-functions-non-linearity-neural-networks-101-ab0036a2e701)æ¿€æ´»å‡½æ•°ã€‚ä¾‹å¦‚ï¼Œå¯¹äº[***ReLU***](/breaking-linearity-with-relu-d2cfa7ebf264)ï¼ˆä¿®æ­£çº¿æ€§å•å…ƒï¼‰æ¿€æ´»å‡½æ•°ï¼Œæ­£å¸¸åˆ†å¸ƒæ–¹å·®éœ€è¦ä½¿ç”¨***1/n_in***è¿›è¡Œåˆå§‹åŒ–ã€‚
- en: A full list of activation functions and their corresponding initialisations
    can be found [***here***](/weight-initialization-and-activation-functions-in-deep-learning-50aac05c3533)for
    the interested reader.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…´è¶£çš„è¯»è€…å¯ä»¥åœ¨[***è¿™é‡Œ***](/weight-initialization-and-activation-functions-in-deep-learning-50aac05c3533)æ‰¾åˆ°å®Œæ•´çš„æ¿€æ´»å‡½æ•°åŠå…¶å¯¹åº”çš„åˆå§‹åŒ–åˆ—è¡¨ã€‚
- en: Better Activation Functions
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ›´å¥½çš„æ¿€æ´»å‡½æ•°
- en: ReLU
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ReLU
- en: 'For most industry standard neural networks, the sigmoid activation function
    has largely been abandoned and replaced with the ReLU as it doesnâ€™t saturate for
    large positive values (itâ€™s also more compute efficient):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå¤§å¤šæ•°è¡Œä¸šæ ‡å‡†çš„ç¥ç»ç½‘ç»œï¼Œsigmoidæ¿€æ´»å‡½æ•°å·²è¢«å¤§é‡å¼ƒç”¨ï¼Œå–è€Œä»£ä¹‹çš„æ˜¯ReLUï¼Œå› ä¸ºå®ƒå¯¹å¤§æ­£å€¼ä¸ä¼šé¥±å’Œï¼ˆå®ƒä¹Ÿæ›´å…·è®¡ç®—æ•ˆç‡ï¼‰ï¼š
- en: '![](../Images/ad1cf21ebe2a1666034bee8102b78d29.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ad1cf21ebe2a1666034bee8102b78d29.png)'
- en: ReLU function. Equation by author in LaTeX.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ReLUå‡½æ•°ã€‚æ–¹ç¨‹ç”±ä½œè€…åœ¨LaTeXä¸­ç¼–å†™ã€‚
- en: '![](../Images/cf6409fc078754bcab71802d9bbe477e.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf6409fc078754bcab71802d9bbe477e.png)'
- en: ReLU activation function. Plot generated by author in Python.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ReLUæ¿€æ´»å‡½æ•°ã€‚å›¾ç”±ä½œè€…åœ¨Pythonä¸­ç”Ÿæˆã€‚
- en: However, ReLU is not perfect and suffers from the [***dying ReLU problem***](https://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks)***.***
    This is where neurons start â€œdyingâ€ as they only output zero because their inputted
    weighted sum is always negative. This leads to a zero gradient, so the network
    stops â€œlearningâ€ anything. This is particularly bad for a large learning rate.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼ŒReLUå¹¶ä¸å®Œç¾ï¼Œä¸”å—åˆ°[***dying ReLUé—®é¢˜***](https://datascience.stackexchange.com/questions/5706/what-is-the-dying-relu-problem-in-neural-networks)çš„å½±å“***ã€‚***
    è¿™æ˜¯æŒ‡ç¥ç»å…ƒå¼€å§‹â€œæ­»äº¡â€ï¼Œå› ä¸ºå®ƒä»¬åªè¾“å‡ºé›¶ï¼Œå› ä¸ºå®ƒä»¬çš„è¾“å…¥åŠ æƒå’Œæ€»æ˜¯è´Ÿçš„ã€‚è¿™å¯¼è‡´é›¶æ¢¯åº¦ï¼Œä»è€Œç½‘ç»œåœæ­¢â€œå­¦ä¹ â€ä»»ä½•ä¸œè¥¿ã€‚è¿™å¯¹è¾ƒå¤§çš„å­¦ä¹ ç‡ç‰¹åˆ«ç³Ÿç³•ã€‚
- en: 'ReLU is easily applied in [***PyTorch***](https://pytorch.org/) as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ReLUå¯ä»¥å¾ˆå®¹æ˜“åœ°åœ¨[***PyTorch***](https://pytorch.org/)ä¸­åº”ç”¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Leaky ReLU
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Leaky ReLU
- en: 'You can get around this by using variants of the classic ReLU function such
    as [***â€˜Leakyâ€™ ReLU***](https://www.educative.io/answers/what-is-leaky-relu#)
    where the negative inputs are not zero, but rather have some shallow slope with
    gradient ***Î±***:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥é€šè¿‡ä½¿ç”¨ç»å…¸ReLUå‡½æ•°çš„å˜ä½“æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä¾‹å¦‚[***â€˜Leakyâ€™ ReLU***](https://www.educative.io/answers/what-is-leaky_relu#)ï¼Œå…¶ä¸­è´Ÿè¾“å…¥ä¸æ˜¯é›¶ï¼Œè€Œæ˜¯å…·æœ‰ä¸€äº›æµ…å¡åº¦çš„æ¢¯åº¦***Î±***ï¼š
- en: '![](../Images/d5dbcbc68a24c8ab953e5b19704fabfa.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d5dbcbc68a24c8ab953e5b19704fabfa.png)'
- en: Leaky ReLU function. Equation by author in LaTeX.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Leaky ReLUå‡½æ•°ã€‚æ–¹ç¨‹ç”±ä½œè€…åœ¨LaTeXä¸­ç¼–å†™ã€‚
- en: '![](../Images/4d44ec1c36473a7f01361080256878af.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d44ec1c36473a7f01361080256878af.png)'
- en: Leaky ReLU, notice the small gradient for negative x values. Plot generated
    by author in Python.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Leaky ReLUï¼Œæ³¨æ„è´Ÿxå€¼çš„å°æ¢¯åº¦ã€‚å›¾ç”±ä½œè€…åœ¨Pythonä¸­ç”Ÿæˆã€‚
- en: The Leaky ReLU often [outperforms](https://ai.stackexchange.com/questions/40576/why-use-relu-over-leaky-relu)
    the classic ReLU as it reduces the chance of this dying neuron problem, hence
    more robust learning. It is often found that a larger â€œleakâ€ is better but within
    reason.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Leaky ReLUé€šå¸¸[è¶…è¶Š](https://ai.stackexchange.com/questions/40576/why-use-relu-over-leaky-relu)ç»å…¸ReLUï¼Œå› ä¸ºå®ƒå‡å°‘äº†è¿™ç§â€œæ­»äº¡ç¥ç»å…ƒâ€é—®é¢˜çš„å¯èƒ½æ€§ï¼Œä»è€Œå®ç°æ›´ç¨³å¥çš„å­¦ä¹ ã€‚é€šå¸¸å‘ç°æ›´å¤§çš„â€œæ³„æ¼â€æ›´å¥½ï¼Œä½†è¦é€‚åº¦ã€‚
- en: 'There are other variants that also frequently improve performance over the
    basic ReLU :'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰å…¶ä»–å˜ä½“ä¹Ÿå¸¸å¸¸æ”¹å–„åŸºæœ¬ReLUçš„æ€§èƒ½ï¼š
- en: '[**Randomised Leaky ReLU (RReLU)**](https://paperswithcode.com/method/rrelu):
    During training the hyperparameter ***Î±*** is randomly initialised.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**éšæœºLeaky ReLU (RReLU)**](https://paperswithcode.com/method/rrelu)ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè¶…å‚æ•°***Î±***è¢«éšæœºåˆå§‹åŒ–ã€‚'
- en: '[**Parametric Leaky ReLU (PReLU):**](https://www.educative.io/answers/what-is-parametric-relu)During
    training the hyperparameter ***Î±*** is learned.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**å‚æ•°åŒ– Leaky ReLU (PReLU):**](https://www.educative.io/answers/what-is-parametric-relu)
    åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¶…å‚æ•° ***Î±*** ä¼šè¢«å­¦ä¹ ã€‚'
- en: 'You can apply Leaky ReLU in PyTorch as:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨ PyTorch ä¸­åº”ç”¨ Leaky ReLUï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE1]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Exponential Linear Unit
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æŒ‡æ•°çº¿æ€§å•å…ƒ
- en: The last activation function we will consider is the [***exponential linear
    unit***](https://paperswithcode.com/method/elu) (ELU).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†è€ƒè™‘çš„æœ€åä¸€ä¸ªæ¿€æ´»å‡½æ•°æ˜¯ [***æŒ‡æ•°çº¿æ€§å•å…ƒ***](https://paperswithcode.com/method/elu)ï¼ˆELUï¼‰ã€‚
- en: '![](../Images/e0667299960e38ea18c78404225f18e0.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e0667299960e38ea18c78404225f18e0.png)'
- en: ELU function. Equation by author in LaTeX.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ELU å‡½æ•°ã€‚ä½œè€…ä½¿ç”¨ LaTeX è¡¨ç¤ºçš„æ–¹ç¨‹ã€‚
- en: '![](../Images/1acad18951fc3a113560ef45efc7a38d.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1acad18951fc3a113560ef45efc7a38d.png)'
- en: ELU, notice the exponential value for negative values. Plot generated by author
    in Python.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ELUï¼Œæ³¨æ„è´Ÿå€¼çš„æŒ‡æ•°å€¼ã€‚å›¾åƒç”±ä½œè€…åœ¨ Python ä¸­ç”Ÿæˆã€‚
- en: 'The key differences between ELU and ReLU are:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ELU å’Œ ReLU ä¹‹é—´çš„ä¸»è¦åŒºåˆ«æ˜¯ï¼š
- en: '*For negative values, the ELU function is not zero, so it alleviates the dying
    neuron problem with the vanilla ReLU.*'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*å¯¹äºè´Ÿå€¼ï¼ŒELU å‡½æ•°ä¸ä¸ºé›¶ï¼Œå› æ­¤ç¼“è§£äº†æ™®é€š ReLU çš„ç¥ç»å…ƒæ­»äº¡é—®é¢˜ã€‚*'
- en: '*Its gradient is non-zero for negative inputs.*'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*å®ƒçš„æ¢¯åº¦å¯¹äºè´Ÿè¾“å…¥æ˜¯éé›¶çš„ã€‚*'
- en: '*ELU has a slower compute speed due to the exponential than ReLU and its variants.*'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ç”±äºæŒ‡æ•°è®¡ç®—ï¼ŒELU çš„è®¡ç®—é€Ÿåº¦æ¯” ReLU åŠå…¶å˜ä½“è¦æ…¢ã€‚*'
- en: '*The ELU generally leads to better performance than the ReLU*'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ELU é€šå¸¸æ¯” ReLU æ€§èƒ½æ›´å¥½*'
- en: 'ELU in PyTorch:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch ä¸­çš„ ELUï¼š
- en: '[PRE2]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Which one to use?
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é€‰æ‹©å“ªä¸ªï¼Ÿ
- en: With so many activation functions, itâ€™s hard to know which one to choose. The
    general rule is ***ELU > Leaky ReLU > ReLU > tanh > sigmoid***. However, compute
    speed may play a factor in your model, so you may have to reconsider which activation
    you go for. Always best to consider your problem at hand and also play around
    with a few to see which one is best.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰è¿™ä¹ˆå¤šæ¿€æ´»å‡½æ•°ï¼Œå¾ˆéš¾çŸ¥é“é€‰æ‹©å“ªä¸ªã€‚ä¸€èˆ¬è§„åˆ™æ˜¯ ***ELU > Leaky ReLU > ReLU > tanh > sigmoid***ã€‚ç„¶è€Œï¼Œè®¡ç®—é€Ÿåº¦å¯èƒ½ä¼šå½±å“ä½ çš„æ¨¡å‹ï¼Œæ‰€ä»¥ä½ å¯èƒ½éœ€è¦é‡æ–°è€ƒè™‘é€‰æ‹©å“ªä¸ªæ¿€æ´»å‡½æ•°ã€‚æœ€å¥½è€ƒè™‘ä¸€ä¸‹ä½ çš„é—®é¢˜å¹¶å°è¯•å‡ ç§ï¼Œçœ‹çœ‹å“ªä¸ªæœ€é€‚åˆã€‚
- en: 'The code used to generate these activation function plots is available at my
    GitHub here:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºç”Ÿæˆè¿™äº›æ¿€æ´»å‡½æ•°å›¾çš„ä»£ç å¯ä»¥åœ¨æˆ‘çš„ GitHub ä¸Šæ‰¾åˆ°ï¼š
- en: '[](https://github.com/egorhowell/Medium-Articles/blob/main/Data%20Science%20Basics/activation_functions.py?source=post_page-----c8f48ec6a80b--------------------------------)
    [## Medium-Articles/Data Science Basics/activation_functions.py at main Â· egorhowell/Medium-Articles'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/egorhowell/Medium-Articles/blob/main/Data%20Science%20Basics/activation_functions.py?source=post_page-----c8f48ec6a80b--------------------------------)
    [## Medium-Articles/Data Science Basics/activation_functions.py at main Â· egorhowell/Medium-Articles'
- en: Code I use in my medium blog/articles. Contribute to egorhowell/Medium-Articles
    development by creating an account onâ€¦
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨æˆ‘çš„ Medium åšå®¢/æ–‡ç« ä¸­ä½¿ç”¨çš„ä»£ç ã€‚é€šè¿‡åˆ›å»ºä¸€ä¸ªè´¦æˆ·ä¸º egorhowell/Medium-Articles å¼€å‘åšè´¡çŒ®â€¦â€¦
- en: github.com](https://github.com/egorhowell/Medium-Articles/blob/main/Data%20Science%20Basics/activation_functions.py?source=post_page-----c8f48ec6a80b--------------------------------)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/egorhowell/Medium-Articles/blob/main/Data%20Science%20Basics/activation_functions.py?source=post_page-----c8f48ec6a80b--------------------------------)
- en: Batch Normalisation
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ‰¹é‡å½’ä¸€åŒ–
- en: Using Glorot initialisation and the ReLU (and variants) activation function
    helps curtail the chance of vanishing/exploding gradients at the beginning of
    the algorithm, but doesnâ€™t help during training.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ Glorot åˆå§‹åŒ–å’Œ ReLUï¼ˆåŠå…¶å˜ä½“ï¼‰æ¿€æ´»å‡½æ•°æœ‰åŠ©äºå‡å°‘ç®—æ³•å¼€å§‹æ—¶æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸çš„å¯èƒ½æ€§ï¼Œä½†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ²¡æœ‰å¸®åŠ©ã€‚
- en: One way to prevent the vanishing/exploding gradients during training is [***Batch
    Normalisation***](https://en.wikipedia.org/wiki/Batch_normalization#:~:text=Batch%20normalization%20%28also%20known%20as,%2Dcentering%20and%20re%2Dscaling.)
    (BN). This process zero centers and re-normalising the outputs or inputs just
    before or after the activation function is applied. Then, it shifts and scales
    this result allowing each layer to have its own â€œlearnedâ€ mean and variance.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: é˜²æ­¢è®­ç»ƒè¿‡ç¨‹ä¸­æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸çš„ä¸€ç§æ–¹æ³•æ˜¯ [***æ‰¹é‡å½’ä¸€åŒ–***](https://en.wikipedia.org/wiki/Batch_normalization#:~:text=Batch%20normalization%20%28also%20known%20as,%2Dcentering%20and%20re%2Dscaling.)ï¼ˆBNï¼‰ã€‚è¿™ä¸ªè¿‡ç¨‹åœ¨æ¿€æ´»å‡½æ•°åº”ç”¨ä¹‹å‰æˆ–ä¹‹åé›¶ä¸­å¿ƒåŒ–å’Œé‡æ–°å½’ä¸€åŒ–è¾“å‡ºæˆ–è¾“å…¥ã€‚ç„¶åï¼Œå®ƒä¼šå¯¹ç»“æœè¿›è¡Œå¹³ç§»å’Œç¼©æ”¾ï¼Œä½¿æ¯ä¸€å±‚éƒ½æœ‰è‡ªå·±â€œå­¦ä¹ â€çš„å‡å€¼å’Œæ–¹å·®ã€‚
- en: This is analogous to why we scale and normalise our features before training.
    It ensures everything is on an even playing field and large valued features wonâ€™t
    drown out the smaller ones. Batch Norm is applying this process after the output
    of each layer as they are inputs into the next!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç±»ä¼¼äºæˆ‘ä»¬åœ¨è®­ç»ƒå‰å¯¹ç‰¹å¾è¿›è¡Œç¼©æ”¾å’Œå½’ä¸€åŒ–çš„åŸå› ã€‚å®ƒç¡®ä¿æ‰€æœ‰ç‰¹å¾å¤„äºåŒä¸€æ°´å¹³ï¼Œå¹¶ä¸”è¾ƒå¤§å€¼çš„ç‰¹å¾ä¸ä¼šæ·¹æ²¡è¾ƒå°å€¼çš„ç‰¹å¾ã€‚æ‰¹é‡å½’ä¸€åŒ–æ˜¯åœ¨æ¯ä¸€å±‚çš„è¾“å‡ºä¹‹ååº”ç”¨è¿™ä¸€è¿‡ç¨‹ï¼Œå› ä¸ºå®ƒä»¬æ˜¯ä¸‹ä¸€å±‚çš„è¾“å…¥ï¼
- en: Great [explanation here](/batch-norm-explained-visually-how-it-works-and-why-neural-networks-need-it-b18919692739)
    about why we need to normalise our features.
  id: totrans-88
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å…³äºä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦å½’ä¸€åŒ–ç‰¹å¾çš„[è¯¦ç»†è§£é‡Šåœ¨è¿™é‡Œ](/batch-norm-explained-visually-how-it-works-and-why-neural-networks-need-it-b18919692739)ã€‚
- en: 'The algorithm looks like as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ç®—æ³•å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](../Images/ba0f4224ca3afadcc5641717efdf783f.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba0f4224ca3afadcc5641717efdf783f.png)'
- en: Batch Normalisation algorithm. Equation generated in LaTeX by author.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰¹é‡å½’ä¸€åŒ–ç®—æ³•ã€‚æ–¹ç¨‹ç”±ä½œè€…åœ¨LaTeXä¸­ç”Ÿæˆã€‚
- en: 'Where:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼š
- en: '***x_i***â€‹ is the input to a batch normalisation layer.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***x_i*** æ˜¯æ‰¹é‡å½’ä¸€åŒ–å±‚çš„è¾“å…¥ã€‚'
- en: '***Î¼_B***â€‹ is the mean of the batch.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Î¼_B*** æ˜¯æ‰¹æ¬¡çš„å‡å€¼ã€‚'
- en: '***ÏƒÂ²_B***â€‹ is the variance of the batch.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***ÏƒÂ²_B*** æ˜¯æ‰¹æ¬¡çš„æ–¹å·®ã€‚'
- en: '***Ïµ*** is a small constant added for numerical stability (to avoid division
    by zero).'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Ïµ*** æ˜¯ä¸ºæ•°å€¼ç¨³å®šæ€§ï¼ˆé¿å…é™¤ä»¥é›¶ï¼‰æ·»åŠ çš„å°å¸¸æ•°ã€‚'
- en: '***Î³*** is the scale parameter that is learned during training'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Î³*** æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ çš„ç¼©æ”¾å‚æ•°ã€‚'
- en: '***Î²*** is the shift parameter that is learned during training'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Î²*** æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ çš„åç§»å‚æ•°ã€‚'
- en: We can walk through it to make it clearer. The first part is just calculating
    the mean and variance of the inputs or outputs of each layer in the network. These
    outputs are then normalised using the mean and variance. The final part is to
    scale, using ***Î³*** hyperparameter, and shift, using ***Î²*** hyperparameter***.***
    These hyperparameters are learned by the network, which is what makes BN so powerful.
    Each layer will have its custom transformation!
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€šè¿‡å®ƒä½¿å…¶æ›´æ¸…æ™°ã€‚ç¬¬ä¸€éƒ¨åˆ†åªæ˜¯è®¡ç®—ç½‘ç»œä¸­æ¯å±‚è¾“å…¥æˆ–è¾“å‡ºçš„å‡å€¼å’Œæ–¹å·®ã€‚è¿™äº›è¾“å‡ºéšåä½¿ç”¨å‡å€¼å’Œæ–¹å·®è¿›è¡Œå½’ä¸€åŒ–ã€‚æœ€åä¸€éƒ¨åˆ†æ˜¯ä½¿ç”¨ ***Î³*** è¶…å‚æ•°è¿›è¡Œç¼©æ”¾ï¼Œä½¿ç”¨
    ***Î²*** è¶…å‚æ•°è¿›è¡Œåç§»ã€‚è¿™äº›è¶…å‚æ•°ç”±ç½‘ç»œå­¦ä¹ ï¼Œè¿™ä½¿å¾—BNå¦‚æ­¤å¼ºå¤§ã€‚æ¯ä¸€å±‚éƒ½ä¼šæœ‰å…¶è‡ªå®šä¹‰çš„å˜æ¢ï¼
- en: One other important thing to note is that whilst training BN keeps track of
    an [***exponential moving average***](https://en.wikipedia.org/wiki/Moving_average#:~:text=An%20exponential%20moving%20average%20%28EMA,decreases%20exponentially%2C%20never%20reaching%20zero.)
    (EMA) for both the mean and variance. This is used when predicting because you
    canâ€™t really apply BN to a single prediction row, which is what happens during
    inference.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä»¶é‡è¦çš„äº‹æƒ…æ˜¯ï¼Œåœ¨è®­ç»ƒæœŸé—´ï¼ŒBN ä¼šè·Ÿè¸ª[***æŒ‡æ•°ç§»åŠ¨å¹³å‡***](https://en.wikipedia.org/wiki/Moving_average#:~:text=An%20exponential%20moving%20average%20%28EMA,decreases%20exponentially%2C%20never%20reaching%20zero.)ï¼ˆEMAï¼‰çš„å‡å€¼å’Œæ–¹å·®ã€‚è¿™åœ¨é¢„æµ‹æ—¶ä½¿ç”¨ï¼Œå› ä¸ºä½ ä¸èƒ½çœŸçš„å°†BNåº”ç”¨äºå•ä¸ªé¢„æµ‹è¡Œï¼Œè¿™åœ¨æ¨ç†è¿‡ç¨‹ä¸­å‘ç”Ÿã€‚
- en: 'You can apply BN in PyTorch as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨PyTorchä¸­æŒ‰å¦‚ä¸‹æ–¹å¼åº”ç”¨BNï¼š
- en: '[PRE3]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you can see, we insert a batch norm layer in between all the hidden layers.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼Œæˆ‘ä»¬åœ¨æ‰€æœ‰éšè—å±‚ä¹‹é—´æ’å…¥ä¸€ä¸ªæ‰¹é‡å½’ä¸€åŒ–å±‚ã€‚
- en: Batch normalisation has been shown to improve the training of deep neural networks
    and reduce the impact of the vanishing gradients problem. However, training each
    epoch is slower as we need to pass hidden layers outputs through batch normalisation
    layers that increase the total number of parameters in the network.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰¹é‡å½’ä¸€åŒ–å·²è¢«è¯æ˜èƒ½æ”¹å–„æ·±åº¦ç¥ç»ç½‘ç»œçš„è®­ç»ƒï¼Œå¹¶å‡å°‘æ¢¯åº¦æ¶ˆå¤±é—®é¢˜çš„å½±å“ã€‚ç„¶è€Œï¼Œæ¯ä¸ªepochçš„è®­ç»ƒé€Ÿåº¦è¾ƒæ…¢ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦å°†éšè—å±‚çš„è¾“å‡ºé€šè¿‡æ‰¹é‡å½’ä¸€åŒ–å±‚ï¼Œè¿™å¢åŠ äº†ç½‘ç»œä¸­çš„æ€»å‚æ•°æ•°é‡ã€‚
- en: Batch norm can also act like regulariser!
  id: totrans-105
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ‰¹é‡å½’ä¸€åŒ–ä¹Ÿå¯ä»¥ä½œä¸ºæ­£åˆ™åŒ–å™¨ï¼
- en: Gradient Clipping
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¢¯åº¦è£å‰ª
- en: The final method to reduce the chance of vanishing/exploding gradient is to
    clip the gradient at some maximum threshold. For example, we can clip the gradient
    at a maximum value of 1\. So, any gradient that is greater than that will be â€œclippedâ€
    down to 1\. This technique is often applied to recurrent neural networks (RNN)
    as it is hard to apply batch norm to RNNs.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: å‡å°‘æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸å‡ ç‡çš„æœ€ç»ˆæ–¹æ³•æ˜¯å°†æ¢¯åº¦è£å‰ªåˆ°æŸä¸ªæœ€å¤§é˜ˆå€¼ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ¢¯åº¦è£å‰ªåˆ°æœ€å¤§å€¼1ã€‚è¿™æ ·ï¼Œä»»ä½•å¤§äº1çš„æ¢¯åº¦éƒ½ä¼šè¢«â€œè£å‰ªâ€åˆ°1ã€‚è¿™ç§æŠ€æœ¯é€šå¸¸åº”ç”¨äºé€’å½’ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰ï¼Œå› ä¸ºå¾ˆéš¾å¯¹RNNåº”ç”¨æ‰¹é‡å½’ä¸€åŒ–ï¼ˆbatch
    normï¼‰ã€‚
- en: 'This is an example of how gradient clipping can be applied in PyTorch:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªå¦‚ä½•åœ¨PyTorchä¸­åº”ç”¨æ¢¯åº¦è£å‰ªçš„ç¤ºä¾‹ï¼š
- en: '[PRE4]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here `max_norm` is the threshold the gradients are clipped at.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œ `max_norm` æ˜¯æ¢¯åº¦è¢«è£å‰ªçš„é˜ˆå€¼ã€‚
- en: Summary & Further Thoughts
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ€»ç»“ä¸è¿›ä¸€æ­¥æ€è€ƒ
- en: 'Vanishing and exploding gradients occur due to the variance change between
    neural network layers and gradients decreasing due to the multiplication effect
    as they are backpropagated. In this post, we discussed three methods to reduce
    the chance of this effect: better activation functions, batch normalisation, and
    gradient clipping. In my opinion, batch normalisation is probably the best option
    combined with a ReLU activation function. Batch normalisation ensures the variance
    across each layer is constant by normalising and scaling the inputs.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: æ¶ˆå¤±å’Œçˆ†ç‚¸æ¢¯åº¦çš„å‘ç”Ÿæ˜¯ç”±äºç¥ç»ç½‘ç»œå±‚ä¹‹é—´çš„æ–¹å·®å˜åŒ–ä»¥åŠç”±äºåå‘ä¼ æ’­æ—¶çš„ä¹˜æ³•æ•ˆåº”å¯¼è‡´æ¢¯åº¦å‡å°‘ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†ä¸‰ç§å‡å°‘è¿™ç§æ•ˆåº”çš„æ–¹æ³•ï¼šæ›´å¥½çš„æ¿€æ´»å‡½æ•°ã€æ‰¹é‡å½’ä¸€åŒ–å’Œæ¢¯åº¦è£å‰ªã€‚åœ¨æˆ‘çœ‹æ¥ï¼Œæ‰¹é‡å½’ä¸€åŒ–ä¸
    ReLU æ¿€æ´»å‡½æ•°ç»“åˆä½¿ç”¨å¯èƒ½æ˜¯æœ€å¥½çš„é€‰æ‹©ã€‚æ‰¹é‡å½’ä¸€åŒ–é€šè¿‡è§„èŒƒåŒ–å’Œç¼©æ”¾è¾“å…¥æ¥ç¡®ä¿æ¯å±‚ä¹‹é—´çš„æ–¹å·®ä¿æŒæ’å®šã€‚
- en: Another Thing!
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªä¿¡æ¯ï¼
- en: I have a free newsletter, [**Dishing the Data**](https://dishingthedata.substack.com/),
    where I share weekly tips for becoming a better Data Scientist. There is no â€œfluffâ€
    or â€œclickbait,â€ just pure actionable insights from a practicing Data Scientist.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æœ‰ä¸€ä¸ªå…è´¹çš„é€šè®¯ï¼Œ[**Dishing the Data**](https://dishingthedata.substack.com/)ï¼Œæ¯å‘¨åˆ†äº«æˆä¸ºæ›´å¥½çš„æ•°æ®ç§‘å­¦å®¶çš„æŠ€å·§ã€‚æ²¡æœ‰â€œç©ºæ´çš„å†…å®¹â€æˆ–â€œç‚¹å‡»è¯±é¥µâ€ï¼Œåªæœ‰æ¥è‡ªå®è·µæ•°æ®ç§‘å­¦å®¶çš„çº¯ç²¹å¯æ“ä½œè§è§£ã€‚
- en: '[](https://newsletter.egorhowell.com/?source=post_page-----c8f48ec6a80b--------------------------------)
    [## Dishing The Data | Egor Howell | Substack'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://newsletter.egorhowell.com/?source=post_page-----c8f48ec6a80b--------------------------------)
    [## Dishing The Data | Egor Howell | Substack'
- en: How To Become A Better Data Scientist. Click to read Dishing The Data, by Egor
    Howell, a Substack publication withâ€¦
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: å¦‚ä½•æˆä¸ºæ›´å¥½çš„æ•°æ®ç§‘å­¦å®¶ã€‚ç‚¹å‡»é˜…è¯»ã€ŠDishing The Dataã€‹ï¼Œç”± Egor Howell å‘è¡¨çš„ Substack å‡ºç‰ˆç‰©â€¦
- en: newsletter.egorhowell.com](https://newsletter.egorhowell.com/?source=post_page-----c8f48ec6a80b--------------------------------)
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[newsletter.egorhowell.com](https://newsletter.egorhowell.com/?source=post_page-----c8f48ec6a80b--------------------------------)'
- en: Connect With Me!
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è”ç³»æˆ‘ï¼
- en: '[**YouTube**](https://www.youtube.com/@egorhowell)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**YouTube**](https://www.youtube.com/@egorhowell)'
- en: '[**LinkedIn**](https://www.linkedin.com/in/egor-howell-092a721b3/)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**LinkedIn**](https://www.linkedin.com/in/egor-howell-092a721b3/)'
- en: '[**Twitter**](https://twitter.com/EgorHowell)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**Twitter**](https://twitter.com/EgorHowell)'
- en: '[**GitHub**](https://github.com/egorhowell)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[**GitHub**](https://github.com/egorhowell)'
- en: References & Further Reading
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒèµ„æ–™ä¸è¿›ä¸€æ­¥é˜…è¯»
- en: '[*Andrej Karpathy Neural Network Course*](https://www.youtube.com/watch?v=i94OvYb6noo)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*Andrej Karpathy ç¥ç»ç½‘ç»œè¯¾ç¨‹*](https://www.youtube.com/watch?v=i94OvYb6noo)'
- en: '[*PyTorch site*](https://pytorch.org/)'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*PyTorch ç½‘ç«™*](https://pytorch.org/)'
- en: '[*Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition.
    AurÃ©lien GÃ©ron. September 2019\. Publisher(s): Oâ€™Reilly Media, Inc. ISBN: 9781492032649*](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)*.*'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*ã€ŠåŠ¨æ‰‹å­¦ä¹ æœºå™¨å­¦ä¹ ï¼šä½¿ç”¨ Scikit-Learnã€Keras å’Œ TensorFlowï¼ˆç¬¬2ç‰ˆï¼‰ã€‹ï¼Œå¥¥é›·åˆ©å®‰Â·çƒ­é¾™ï¼Œ2019å¹´9æœˆã€‚å‡ºç‰ˆå•†ï¼šO''Reilly
    åª’ä½“å…¬å¸ï¼ŒISBN: 9781492032649*](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)*.*'
- en: '*Paper on vanishing gradients study:* [*https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf*](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*å…³äºæ¶ˆå¤±æ¢¯åº¦ç ”ç©¶çš„è®ºæ–‡ï¼š* [*https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf*](https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)'
- en: '[*Great visual explanation of batch norm*](/batch-norm-explained-visually-how-it-works-and-why-neural-networks-need-it-b18919692739)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*æ‰¹é‡å½’ä¸€åŒ–çš„ç²¾å½©è§†è§‰è§£é‡Š*](/batch-norm-explained-visually-how-it-works-and-why-neural-networks-need-it-b18919692739)'
- en: '[*Great video on the vanishing gradient problem*](https://www.youtube.com/watch?v=8z3DFk4VxRo)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*å…³äºæ¶ˆå¤±æ¢¯åº¦é—®é¢˜çš„ç²¾å½©è§†é¢‘*](https://www.youtube.com/watch?v=8z3DFk4VxRo)'
