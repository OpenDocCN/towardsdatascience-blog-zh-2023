- en: Semantic Textual Similarity with BERT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 BERT 计算语义文本相似度
- en: 原文：[https://towardsdatascience.com/semantic-textual-similarity-with-bert-fc800656e7a3?source=collection_archive---------1-----------------------#2023-02-15](https://towardsdatascience.com/semantic-textual-similarity-with-bert-fc800656e7a3?source=collection_archive---------1-----------------------#2023-02-15)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/semantic-textual-similarity-with-bert-fc800656e7a3?source=collection_archive---------1-----------------------#2023-02-15](https://towardsdatascience.com/semantic-textual-similarity-with-bert-fc800656e7a3?source=collection_archive---------1-----------------------#2023-02-15)
- en: How to use BERT to calculate the semantic similarity between two texts
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用 BERT 计算两个文本之间的语义相似度
- en: '[](https://medium.com/@marcellusruben?source=post_page-----fc800656e7a3--------------------------------)[![Ruben
    Winastwan](../Images/15ad0dd03bf5892510abdf166a1e91e1.png)](https://medium.com/@marcellusruben?source=post_page-----fc800656e7a3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fc800656e7a3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fc800656e7a3--------------------------------)
    [Ruben Winastwan](https://medium.com/@marcellusruben?source=post_page-----fc800656e7a3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@marcellusruben?source=post_page-----fc800656e7a3--------------------------------)[![Ruben
    Winastwan](../Images/15ad0dd03bf5892510abdf166a1e91e1.png)](https://medium.com/@marcellusruben?source=post_page-----fc800656e7a3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----fc800656e7a3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----fc800656e7a3--------------------------------)
    [Ruben Winastwan](https://medium.com/@marcellusruben?source=post_page-----fc800656e7a3--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5dae9da73c9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsemantic-textual-similarity-with-bert-fc800656e7a3&user=Ruben+Winastwan&userId=5dae9da73c9b&source=post_page-5dae9da73c9b----fc800656e7a3---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fc800656e7a3--------------------------------)
    ·11 min read·Feb 15, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffc800656e7a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsemantic-textual-similarity-with-bert-fc800656e7a3&user=Ruben+Winastwan&userId=5dae9da73c9b&source=-----fc800656e7a3---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F5dae9da73c9b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsemantic-textual-similarity-with-bert-fc800656e7a3&user=Ruben+Winastwan&userId=5dae9da73c9b&source=post_page-5dae9da73c9b----fc800656e7a3---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----fc800656e7a3--------------------------------)
    ·11 分钟阅读·2023年2月15日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Ffc800656e7a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsemantic-textual-similarity-with-bert-fc800656e7a3&user=Ruben+Winastwan&userId=5dae9da73c9b&source=-----fc800656e7a3---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffc800656e7a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsemantic-textual-similarity-with-bert-fc800656e7a3&source=-----fc800656e7a3---------------------bookmark_footer-----------)![](../Images/618fe6986fde72b9eaae6d126ce78716.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Ffc800656e7a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fsemantic-textual-similarity-with-bert-fc800656e7a3&source=-----fc800656e7a3---------------------bookmark_footer-----------)![](../Images/618fe6986fde72b9eaae6d126ce78716.png)'
- en: 'Photo by Leeloo Thefirst: [https://www.pexels.com/photo/brown-wooden-ruler-and-colored-pencils-on-papers-8970296/](https://www.pexels.com/photo/brown-wooden-ruler-and-colored-pencils-on-papers-8970296/)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '图片来源：Leeloo Thefirst: [https://www.pexels.com/photo/brown-wooden-ruler-and-colored-pencils-on-papers-8970296/](https://www.pexels.com/photo/brown-wooden-ruler-and-colored-pencils-on-papers-8970296/)'
- en: Ever since its inception in 2017 by Google Brain team, Transformers have rapidly
    become the state-of-the-art model for various use cases within the fields of Computer
    Vision and NLP. Its superior performance led to the development of several state-of-the-art
    models such as BERT and its variants like distilBERT and RoBERTa.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 自2017年由Google Brain团队首次提出以来，Transformer模型迅速成为计算机视觉和自然语言处理领域的**最先进**模型。其卓越的性能促使了多个**最先进**模型的开发，如BERT及其变体distilBERT和RoBERTa。
- en: BERT outperformed old recurrent models in various NLP tasks such as text classification,
    Named Entity Recognition (NER), question answering, and even the task that we’re
    going to focus on in this article, which is semantic textual similarity (STS).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 在各种自然语言处理任务中表现优于旧的递归模型，例如文本分类、命名实体识别（NER）、问答系统，甚至是我们在本文中要重点关注的任务，即语义文本相似度（STS）。
- en: Thus, in this article, we’re going to see how we can train a BERT model for
    STS task with the help of Sentence Transformers library. Next, we’re going to
    use the trained model to predict unknown data. But as a starter, we need to know
    first what STS task actually is and the dataset that we will use for this task.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本文中，我们将看到如何利用 Sentence Transformers 库训练一个 BERT 模型来完成 STS 任务。接下来，我们将使用训练好的模型来预测未知数据。但作为开始，我们需要首先了解
    STS 任务到底是什么，以及我们将用于此任务的数据集。
- en: Semantic Textual Similarity and the Dataset
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语义文本相似度及数据集
- en: Semantic textual similarity (STS) refers to a task in which we compare the similarity
    between one text to another.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 语义文本相似度（STS）是指将一个文本与另一个文本进行相似度比较的任务。
