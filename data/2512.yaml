- en: Graph Machine Learning @ ICML 2023
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å›¾å½¢æœºå™¨å­¦ä¹  @ ICML 2023
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/graph-machine-learning-icml-2023-9b5e4306a1cc?source=collection_archive---------0-----------------------#2023-08-06](https://towardsdatascience.com/graph-machine-learning-icml-2023-9b5e4306a1cc?source=collection_archive---------0-----------------------#2023-08-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/graph-machine-learning-icml-2023-9b5e4306a1cc?source=collection_archive---------0-----------------------#2023-08-06](https://towardsdatascience.com/graph-machine-learning-icml-2023-9b5e4306a1cc?source=collection_archive---------0-----------------------#2023-08-06)
- en: Whatâ€™s new in Graph ML?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å›¾å½¢æœºå™¨å­¦ä¹ çš„æ–°åŠ¨æ€ï¼Ÿ
- en: Recent advancements and hot trends, August 2023 edition
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æœ€è¿‘çš„è¿›å±•å’Œçƒ­é—¨è¶‹åŠ¿ï¼Œ2023å¹´8æœˆç‰ˆ
- en: '[](https://mgalkin.medium.com/?source=post_page-----9b5e4306a1cc--------------------------------)[![Michael
    Galkin](../Images/c5eb13334712ca0462d8a5df4a268ad0.png)](https://mgalkin.medium.com/?source=post_page-----9b5e4306a1cc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9b5e4306a1cc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9b5e4306a1cc--------------------------------)
    [Michael Galkin](https://mgalkin.medium.com/?source=post_page-----9b5e4306a1cc--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://mgalkin.medium.com/?source=post_page-----9b5e4306a1cc--------------------------------)[![Michael
    Galkin](../Images/c5eb13334712ca0462d8a5df4a268ad0.png)](https://mgalkin.medium.com/?source=post_page-----9b5e4306a1cc--------------------------------)[](https://towardsdatascience.com/?source=post_page-----9b5e4306a1cc--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----9b5e4306a1cc--------------------------------)
    [Michael Galkin](https://mgalkin.medium.com/?source=post_page-----9b5e4306a1cc--------------------------------)'
- en: Â·
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4d4f8ddd1e68&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-machine-learning-icml-2023-9b5e4306a1cc&user=Michael+Galkin&userId=4d4f8ddd1e68&source=post_page-4d4f8ddd1e68----9b5e4306a1cc---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9b5e4306a1cc--------------------------------)
    Â·16 min readÂ·Aug 6, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9b5e4306a1cc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-machine-learning-icml-2023-9b5e4306a1cc&user=Michael+Galkin&userId=4d4f8ddd1e68&source=-----9b5e4306a1cc---------------------clap_footer-----------)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4d4f8ddd1e68&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-machine-learning-icml-2023-9b5e4306a1cc&user=Michael+Galkin&userId=4d4f8ddd1e68&source=post_page-4d4f8ddd1e68----9b5e4306a1cc---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----9b5e4306a1cc--------------------------------)
    Â·16åˆ†é’Ÿé˜…è¯»Â·2023å¹´8æœˆ6æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F9b5e4306a1cc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-machine-learning-icml-2023-9b5e4306a1cc&user=Michael+Galkin&userId=4d4f8ddd1e68&source=-----9b5e4306a1cc---------------------clap_footer-----------)'
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9b5e4306a1cc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-machine-learning-icml-2023-9b5e4306a1cc&source=-----9b5e4306a1cc---------------------bookmark_footer-----------)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F9b5e4306a1cc&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgraph-machine-learning-icml-2023-9b5e4306a1cc&source=-----9b5e4306a1cc---------------------bookmark_footer-----------)'
- en: Magnificent beaches and tropical Hawaiian landscapes ğŸŒ´did not turn brave scientists
    away from attending the [International Conference on Machine Learning](https://icml.cc/Conferences/2023)
    in Honolulu and presenting their recent work! Letâ€™s see whatâ€™s new in our favorite
    Graph Machine Learning area.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å£®ä¸½çš„æµ·æ»©å’Œçƒ­å¸¦å¤å¨å¤·é£å…‰ğŸŒ´å¹¶æ²¡æœ‰é˜»æ­¢å‹‡æ•¢çš„ç§‘å­¦å®¶ä»¬å‚åŠ åœ¨æª€é¦™å±±ä¸¾åŠçš„[å›½é™…æœºå™¨å­¦ä¹ å¤§ä¼š](https://icml.cc/Conferences/2023)å¹¶å±•ç¤ºä»–ä»¬çš„æœ€æ–°ç ”ç©¶æˆæœï¼è®©æˆ‘ä»¬ä¸€èµ·çœ‹çœ‹æˆ‘ä»¬æœ€å–œæ¬¢çš„å›¾å½¢æœºå™¨å­¦ä¹ é¢†åŸŸçš„æ–°åŠ¨æ€ã€‚
- en: '![](../Images/c984ed95b5365a9c6b59956d31ab3dac.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c984ed95b5365a9c6b59956d31ab3dac.png)'
- en: Image By Author.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ä½œè€…ã€‚
- en: '*Thanks Santiago Miret for proofreading the post.*'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ„Ÿè°¢Santiago Miretæ ¡å¯¹æœ¬æ–‡ã€‚*'
- en: To make the post less boring about papers, I took some photos around Honolulu
    ğŸ“·
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è®©å¸–å­ä¸é‚£ä¹ˆæ¯ç‡¥ï¼Œæˆ‘åœ¨æª€é¦™å±±æ‹äº†ä¸€äº›ç…§ç‰‡ğŸ“·
- en: 'Table of contents (clickable):'
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç›®å½•ï¼ˆå¯ç‚¹å‡»ï¼‰ï¼š
- en: '[Graph Transformers: Sparser, Faster, and Directed](#8d41)'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[å›¾å½¢å˜æ¢å™¨ï¼šæ›´ç¨€ç–ã€æ›´å¿«ä¸”æœ‰æ–¹å‘](#8d41)'
- en: '[Theory: VC dimension of GNNs, deep dive in over-squashing](#0d40)'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ç†è®ºï¼šGNNçš„VCç»´ï¼Œæ·±å…¥æ¢è®¨è¿‡åº¦æŒ¤å‹](#0d40)'
- en: '[New GNN architectures: delays and half-hops](#c5be)'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[æ–°çš„GNNæ¶æ„ï¼šå»¶è¿Ÿå’ŒåŠè·³](#c5be)'
- en: '[Generative Models â€” Stable Diffusion for Molecules, Discrete diffusion](#7e7c)'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ç”Ÿæˆæ¨¡å‹ â€” åˆ†å­ç¨³å®šæ‰©æ•£ï¼Œç¦»æ•£æ‰©æ•£](#7e7c)'
- en: '[Geometric Learning: Geometric WL, Clifford Algebras](#b0d0)'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[å‡ ä½•å­¦ä¹ ï¼šå‡ ä½• WLï¼Œå…‹åˆ©ç¦å¾·ä»£æ•°](#b0d0)'
- en: '[Molecules: 2D-3D pretraining, Uncertainty Estimation in MD](#32a5)'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[åˆ†å­ï¼š2D-3D é¢„è®­ç»ƒï¼ŒMD ä¸­çš„ä¸ç¡®å®šæ€§ä¼°è®¡](#32a5)'
- en: '[Materials & Proteins: CLIP for proteins, Ewald Message Passing, Equivariant
    Augmentations](#1ff6)'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ææ–™ä¸è›‹ç™½è´¨ï¼šç”¨äºè›‹ç™½è´¨çš„ CLIPï¼ŒEwald æ¶ˆæ¯ä¼ é€’ï¼Œå¯¹ç§°å¢å¼º](#1ff6)'
- en: '[Cool Applications: Algorithmic reasoning, Inductive KG completion, GNNs for
    mass spectra](#1891)'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[é…·åº”ç”¨ï¼šç®—æ³•æ¨ç†ã€å½’çº³ KG å®Œæˆã€ç”¨äºè´¨è°±çš„ GNN](#1891)'
- en: '[The Concluding Meme Part](#5eb2)'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[æ€»ç»“çš„ Meme éƒ¨åˆ†](#5eb2)'
- en: '**Graph Transformers: Sparser, Faster, and Directed**'
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**å›¾è½¬æ¢å™¨ï¼šæ›´ç¨€ç–ã€æ›´å¿«ã€ä¸”æœ‰æ–¹å‘**'
- en: We [presented](/graphgps-navigating-graph-transformers-c2cc223a051c) **GraphGPS**
    about a year ago and it is pleasing to see many ICML papers building upon our
    framework and expanding GT capabilities even further.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬[å¤§çº¦ä¸€å¹´å‰](/graphgps-navigating-graph-transformers-c2cc223a051c) æå‡ºäº† **GraphGPS**ï¼Œå¾ˆé«˜å…´çœ‹åˆ°è®¸å¤š
    ICML è®ºæ–‡åŸºäºæˆ‘ä»¬çš„æ¡†æ¶å¹¶è¿›ä¸€æ­¥æ‰©å±• GT èƒ½åŠ›ã€‚
- en: '**â¡ï¸ Exphormer** by [Shirzad, Velingker, Venkatachalam et al](https://openreview.net/forum?id=3Ge74dgjjU)
    adds a missing piece of graph-motivated sparse attention to GTs: instead of BigBird
    or Performer (originally designed for sequences), Exphormerâ€™s attention builds
    upon 1-hop edges, virtual nodes (connected to all nodes in a graph), and a neat
    idea of [expander edges](https://en.wikipedia.org/wiki/Expander_graph). Expander
    graphs have a constant degree and are shown to approximate fully-connected graphs.
    All components combined, attention costs *O(V+E)* instead of *O(VÂ²)*. This allows
    Exphormer to outperform GraphGPS almost everywhere and scale to really large graphs
    of up to 160k nodes. Amazing work and all chances to make Exphormer the standard
    sparse attention mechanism in GTs ğŸ‘.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**â¡ï¸ Exphormer** ç”± [Shirzad, Velingker, Venkatachalam ç­‰äºº](https://openreview.net/forum?id=3Ge74dgjjU)
    æ·»åŠ äº†å›¾åŠ¨æœºçš„ç¨€ç–æ³¨æ„åŠ›çš„ç¼ºå¤±éƒ¨åˆ†ï¼šä¸ BigBird æˆ– Performerï¼ˆæœ€åˆä¸ºåºåˆ—è®¾è®¡ï¼‰ä¸åŒï¼ŒExphormer çš„æ³¨æ„åŠ›åŸºäº 1-hop è¾¹ã€è™šæ‹ŸèŠ‚ç‚¹ï¼ˆä¸å›¾ä¸­çš„æ‰€æœ‰èŠ‚ç‚¹ç›¸è¿ï¼‰ä»¥åŠä¸€ä¸ªç²¾å·§çš„
    [æ‰©å±•è¾¹](https://en.wikipedia.org/wiki/Expander_graph) æƒ³æ³•ã€‚æ‰©å±•å›¾å…·æœ‰å›ºå®šåº¦ï¼Œå¹¶è¢«è¯æ˜å¯ä»¥è¿‘ä¼¼å®Œå…¨è¿æ¥çš„å›¾ã€‚æ‰€æœ‰ç»„ä»¶ç»“åˆèµ·æ¥ï¼Œæ³¨æ„åŠ›æˆæœ¬ä¸º
    *O(V+E)* è€Œä¸æ˜¯ *O(VÂ²)*ã€‚è¿™ä½¿å¾— Exphormer èƒ½å¤Ÿåœ¨å‡ ä¹æ‰€æœ‰åœ°æ–¹è¶…è¶Š GraphGPSï¼Œå¹¶æ‰©å±•åˆ°æœ€å¤š 16 ä¸‡ä¸ªèŠ‚ç‚¹çš„éå¸¸å¤§å›¾ã€‚ä»¤äººæƒŠå¹çš„å·¥ä½œï¼ŒExphormer
    æœ‰å¾ˆå¤§æœºä¼šæˆä¸º GT ä¸­æ ‡å‡†çš„ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ ğŸ‘ã€‚'
- en: '**â¡ï¸** Concurrently with graph transformers, expander graphs can already be
    used to enhance the performance of any MPNN architecture as shown in [Expander
    Graph Propagation](https://arxiv.org/abs/2210.02997) by *Deac, Lackenby, and VeliÄkoviÄ‡*.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**â¡ï¸** ä¸å›¾è½¬æ¢å™¨å¹¶è¡Œï¼Œæ‰©å±•å›¾å·²ç»å¯ä»¥ç”¨æ¥å¢å¼ºä»»ä½• MPNN æ¶æ„çš„æ€§èƒ½ï¼Œå¦‚ *Deac, Lackenby, å’Œ VeliÄkoviÄ‡* åœ¨
    [Expander Graph Propagation](https://arxiv.org/abs/2210.02997) ä¸­æ‰€ç¤ºã€‚'
- en: In a similar vein, [Cai et al](https://openreview.net/forum?id=1EuHYKFPgA) show
    that MPNNs with virtual nodes can approximate linear Performer-like attention
    such that even classic GCN and GatedGCN imbued with virtual nodes show pretty
    much a SOTA performance in long-range graph tasks (we [released](/lrgb-long-range-graph-benchmark-909a6818f02c)
    the [LGRB benchmark](https://github.com/vijaydwivedi75/lrgb) last year exactly
    for measuring long-range capabilities of GNNs and GTs).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼åœ°ï¼Œ[Cai ç­‰äºº](https://openreview.net/forum?id=1EuHYKFPgA) è¯æ˜äº†å…·æœ‰è™šæ‹ŸèŠ‚ç‚¹çš„ MPNN å¯ä»¥è¿‘ä¼¼çº¿æ€§
    Performer-like æ³¨æ„åŠ›ï¼Œå› æ­¤å³ä½¿æ˜¯ç»å…¸çš„ GCN å’Œ GatedGCN åªè¦åŠ å…¥è™šæ‹ŸèŠ‚ç‚¹ï¼Œä¹Ÿèƒ½åœ¨é•¿èŒƒå›´å›¾ä»»åŠ¡ä¸­è¡¨ç°å‡ºç›¸å½“çš„ SOTA æ€§èƒ½ï¼ˆæˆ‘ä»¬[å‘å¸ƒäº†](/lrgb-long-range-graph-benchmark-909a6818f02c)
    [LGRB åŸºå‡†æµ‹è¯•](https://github.com/vijaydwivedi75/lrgb)æ¥æµ‹é‡ GNN å’Œ GT çš„é•¿èŒƒå›´èƒ½åŠ›ï¼‰ã€‚
- en: '![](../Images/70f4870bdd013ec69ff15c45339cc653.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/70f4870bdd013ec69ff15c45339cc653.png)'
- en: 'Source: [Shirzad, Velingker, Venkatachalam et al](https://openreview.net/forum?id=3Ge74dgjjU)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[Shirzad, Velingker, Venkatachalam ç­‰äºº](https://openreview.net/forum?id=3Ge74dgjjU)
- en: '**â¡ï¸** A few **patch-based** subsampling approaches for GTs inspired by vision
    models: [**â€œA Generalization of ViT/MLP-Mixer to Graphsâ€**](https://openreview.net/forum?id=l7yTbEWuOQ)
    by *He et al* split the input into several patches, encode each patch with a GNN
    into a token, and run a transformer over those tokens.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**â¡ï¸** ä¸€äº›å—è§†è§‰æ¨¡å‹å¯å‘çš„ GT çš„ **åŸºäºè¡¥ä¸** çš„å­é‡‡æ ·æ–¹æ³•ï¼š*He ç­‰äºº* çš„ [**â€œViT/MLP-Mixer åœ¨å›¾ä¸Šçš„æ¨å¹¿â€**](https://openreview.net/forum?id=l7yTbEWuOQ)
    å°†è¾“å…¥åˆ†æˆå¤šä¸ªè¡¥ä¸ï¼Œä½¿ç”¨ GNN å°†æ¯ä¸ªè¡¥ä¸ç¼–ç ä¸ºä¸€ä¸ªä»¤ç‰Œï¼Œå¹¶å¯¹è¿™äº›ä»¤ç‰Œè¿è¡Œå˜æ¢å™¨ã€‚'
- en: '![](../Images/e3818ed7d6eef99eb892d922370b497d.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e3818ed7d6eef99eb892d922370b497d.png)'
- en: 'Source: [â€œA Generalization of ViT/MLP-Mixer to Graphsâ€](https://openreview.net/forum?id=l7yTbEWuOQ)
    by He et al'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[â€œViT/MLP-Mixer åœ¨å›¾ä¸Šçš„æ¨å¹¿â€](https://openreview.net/forum?id=l7yTbEWuOQ) ç”± He
    ç­‰äºº
- en: In **GOAT** by [Kong et al](https://openreview.net/forum?id=Le2dVIoQun), node
    features are projected into a codebook of K clusters with K-Means, and a sampled
    3-hop neighborhood of each node attends to the codebook. GOAT is a 1-layer model
    and scales to graphs of millions of nodes.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ **GOAT** ä¸­ï¼Œç”± [Kong ç­‰äºº](https://openreview.net/forum?id=Le2dVIoQun) æå‡ºï¼ŒèŠ‚ç‚¹ç‰¹å¾è¢«æŠ•å½±åˆ°
    K-Means çš„ K ä¸ªç°‡çš„ä»£ç æœ¬ä¸­ï¼Œå¹¶ä¸”æ¯ä¸ªèŠ‚ç‚¹çš„é‡‡æ ·çš„ä¸‰è·³é‚»åŸŸéƒ½å…³æ³¨è¿™ä¸ªä»£ç æœ¬ã€‚GOAT æ˜¯ä¸€ä¸ªå•å±‚æ¨¡å‹ï¼Œå¹¶ä¸”å¯ä»¥æ‰©å±•åˆ°æ•°ç™¾ä¸‡èŠ‚ç‚¹çš„å›¾ä¸­ã€‚
- en: '**â¡ï¸ Directed graphs** got some transformer love as well ğŸ’—. [**â€œTransformers
    Meet Directed Graphsâ€**](https://openreview.net/forum?id=a7PVyayyfp) by *Geisler
    et al* introduces Magnetic Laplacian â€” a generalization of a Laplacian for directed
    graphs with a non-symmetric adjacency matrix. Eigenvectors of the Magnetic Laplacian
    paired with directed random walks are strong input features for the transformer
    that enable setting a new SOTA on the [OGB Code2](https://ogb.stanford.edu/docs/leader_graphprop/#ogbg-code2)
    graph property prediction dataset by a good margin!'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**â¡ï¸ æœ‰å‘å›¾** ä¹Ÿå—åˆ°äº†ä¸€äº› Transformer çš„å–œçˆ± ğŸ’—ã€‚[â€œTransformers Meet Directed Graphsâ€](https://openreview.net/forum?id=a7PVyayyfp)
    ç”± *Geisler ç­‰äºº* å¼•å…¥äº†ç£æ‹‰æ™®æ‹‰æ–¯ â€”â€” éå¯¹ç§°é‚»æ¥çŸ©é˜µçš„æ‹‰æ™®æ‹‰æ–¯çš„æ³›åŒ–ã€‚ç£æ‹‰æ™®æ‹‰æ–¯çš„ç‰¹å¾å‘é‡ä¸æœ‰å‘éšæœºæ¸¸èµ°ç»“åˆï¼Œæˆä¸º Transformer
    çš„å¼ºå¤§è¾“å…¥ç‰¹å¾ï¼Œåœ¨ [OGB Code2](https://ogb.stanford.edu/docs/leader_graphprop/#ogbg-code2)
    å›¾å±æ€§é¢„æµ‹æ•°æ®é›†ä¸Šè®¾ç½®äº†æ–°çš„ SOTAï¼Œè¶…è¿‡äº†ç°æœ‰æ–¹æ³•å¾ˆå¤šï¼'
- en: ğŸ… Last but not least, we have a new SOTA GT on the community standard ZINC dataset
    â€” **GRIT** by [Ma, Lin, et al](https://openreview.net/forum?id=HjMdlNgybR) incorporates
    the full *d*-dimensional random walk matrix, coined as relative random walk probabilities
    (RRWP), as edge features to the attention computation (for comparison, popular
    [RWSE](https://openreview.net/forum?id=wTTjnvGphYj) features are just the diagonal
    elements of this matrix). RRWP are provably more powerful than shortest path distance
    features and set a record-low 0.059 MAE on ZINC (down from 0.070 by GraphGPS).
    GRIT often outperforms GPS in other benchmarks as well ğŸ‘. In a similar vein, [Eliasof
    et al](https://openreview.net/forum?id=1Nx2n1lk5T) propose a neat idea to combine
    random and spectral features as positional encodings that outperform RWSE but
    were not tried with GTs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ… æœ€åä½†å¹¶éæœ€ä¸é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬åœ¨ç¤¾åŒºæ ‡å‡† ZINC æ•°æ®é›†ä¸Šæœ‰äº†ä¸€ä¸ªæ–°çš„ SOTA GT â€” **GRIT**ï¼Œç”± [Ma, Lin, ç­‰äºº](https://openreview.net/forum?id=HjMdlNgybR)
    æå‡ºï¼Œå…¶å…¨ *d*-ç»´éšæœºæ¸¸èµ°çŸ©é˜µè¢«ç§°ä¸ºç›¸å¯¹éšæœºæ¸¸èµ°æ¦‚ç‡ï¼ˆRRWPï¼‰ï¼Œä½œä¸ºè¾¹ç‰¹å¾ç”¨äºæ³¨æ„åŠ›è®¡ç®—ï¼ˆç›¸æ¯”ä¹‹ä¸‹ï¼Œæµè¡Œçš„ [RWSE](https://openreview.net/forum?id=wTTjnvGphYj)
    ç‰¹å¾åªæ˜¯è¿™ä¸ªçŸ©é˜µçš„å¯¹è§’å…ƒç´ ï¼‰ã€‚RRWP æ˜æ˜¾æ¯”æœ€çŸ­è·¯å¾„è·ç¦»ç‰¹å¾æ›´å¼ºå¤§ï¼Œåœ¨ ZINC ä¸Šå–å¾—äº†åˆ›çºªå½•çš„ä½ 0.059 MAEï¼ˆæ¯” GraphGPS çš„ 0.070
    ä½ï¼‰ã€‚GRIT åœ¨å…¶ä»–åŸºå‡†æµ‹è¯•ä¸­é€šå¸¸ä¹Ÿä¼˜äº GPS ğŸ‘ã€‚åŒæ ·åœ°ï¼Œ[Eliasof ç­‰äºº](https://openreview.net/forum?id=1Nx2n1lk5T)
    æå‡ºäº†ä¸€ä¸ªå·§å¦™çš„æ€è·¯ï¼Œå°†éšæœºå’Œè°±ç‰¹å¾ç»“åˆä¸ºä½ç½®ç¼–ç ï¼Œåœ¨è¶…è¶Š RWSE çš„åŒæ—¶å¹¶æœªå°è¯•è¿‡ä¸ GTs ç»“åˆã€‚
- en: '![](../Images/5dbb993c02b7139828ad024f7a1d9328.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5dbb993c02b7139828ad024f7a1d9328.png)'
- en: Image by Author.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: '**Theory: VC dimension of GNNs, deep dive into over-squashing**'
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ç†è®ºï¼šGNNs çš„ VC ç»´ï¼Œæ·±å…¥æ¢è®¨è¿‡åº¦å‹ç¼©**'
- en: '**â¡ï¸** [VC dimension](https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension)
    measures model capacity and expressiveness. It is studied well for classical ML
    algorithms but, surprisingly, has never been applied to study GNNs. In[**â€œWL meet
    VCâ€**](https://openreview.net/forum?id=rZN3mc5m3C) by *Morris et al*, the connection
    between the WL test and VC dimension is finally uncovered â€” turns out it the VC
    dimension can be bounded by the bitlength of GNN weights, i.e., float32 weights
    would imply the VC dimension of 32\. Furthermore, the VC dimension depends logarithmically
    on the number of unique WL colors in the given task and polynomially on the depth
    and number of layers. This is a great theoretical result and Iâ€™d encourage you
    to have a look!'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**â¡ï¸** [VC ç»´](https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_dimension)
    è¡¡é‡äº†æ¨¡å‹çš„å®¹é‡å’Œè¡¨è¾¾èƒ½åŠ›ã€‚å®ƒå·²ç»å¹¿æ³›åº”ç”¨äºç»å…¸æœºå™¨å­¦ä¹ ç®—æ³•çš„ç ”ç©¶ä¸­ï¼Œä½†ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œä»æœªåº”ç”¨äºç ”ç©¶ GNNsã€‚åœ¨ [â€œWL meet VCâ€](https://openreview.net/forum?id=rZN3mc5m3C)
    ä¸­ï¼Œ*Morris ç­‰äºº* æœ€ç»ˆæ­ç¤ºäº† WL æµ‹è¯•å’Œ VC ç»´ä¹‹é—´çš„è”ç³» â€” åŸæ¥ VC ç»´å¯ä»¥ç”± GNN æƒé‡çš„æ¯”ç‰¹é•¿åº¦ç•Œå®šï¼Œå³ float32 æƒé‡æ„å‘³ç€
    VC ç»´ä¸º 32ã€‚æ­¤å¤–ï¼ŒVC ç»´å¯¹ç»™å®šä»»åŠ¡ä¸­å”¯ä¸€ WL é¢œè‰²çš„æ•°é‡ä»¥å¯¹æ•°æ–¹å¼ä¾èµ–ï¼Œå¯¹æ·±åº¦å’Œå±‚æ•°åˆ™å¤šé¡¹å¼ä¾èµ–ã€‚è¿™æ˜¯ä¸€ä¸ªé‡è¦çš„ç†è®ºç»“æœï¼Œæˆ‘é¼“åŠ±ä½ æ·±å…¥äº†è§£ï¼'
- en: '![](../Images/3e0e2bb4b10259d188b9ce9b748796f3.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e0e2bb4b10259d188b9ce9b748796f3.png)'
- en: 'Source: [â€œWL meet VCâ€](https://openreview.net/forum?id=rZN3mc5m3C) by *Morris
    et al*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[â€œWL meet VCâ€](https://openreview.net/forum?id=rZN3mc5m3C) by *Morris ç­‰äºº*
- en: ğŸŠğŸ–ï¸ The over-squashing effect â€” information loss when you try to stuff messages
    from too many neighboring nodes â€” is another common problem of MPNNs, and we donâ€™t
    fully understand how to properly deal with it. This year, there were 3 papers
    dedicated to this topic. Perhaps the most foundational is the work by [**Di Giovanni
    et al**](https://openreview.net/forum?id=t2tTfWwAEl) that explains how MPNNs width,
    depth, and graph topology affect over-squashing.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸŠğŸ–ï¸ è¿‡åº¦å‹ç¼©æ•ˆåº”â€”â€”å°è¯•å°†æ¥è‡ªè¿‡å¤šé‚»å±…èŠ‚ç‚¹çš„ä¿¡æ¯å¡å…¥ä¼šå¯¼è‡´ä¿¡æ¯ä¸¢å¤±â€”â€”æ˜¯MPNNçš„å¦ä¸€ä¸ªå¸¸è§é—®é¢˜ï¼Œæˆ‘ä»¬å°šæœªå®Œå…¨ç†è§£å¦‚ä½•å¦¥å–„å¤„ç†ã€‚ä»Šå¹´ï¼Œæœ‰3ç¯‡è®ºæ–‡ä¸“é—¨è®¨è®ºäº†è¿™ä¸ªè¯é¢˜ã€‚ä¹Ÿè®¸æœ€åŸºç¡€çš„æ˜¯[**Di
    Giovanni ç­‰**](https://openreview.net/forum?id=t2tTfWwAEl)çš„å·¥ä½œï¼Œè§£é‡Šäº†MPNNçš„å®½åº¦ã€æ·±åº¦å’Œå›¾æ‹“æ‰‘å¦‚ä½•å½±å“è¿‡åº¦å‹ç¼©ã€‚
- en: '![](../Images/9b93ed516702c4448b7a0a9772541cdd.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9b93ed516702c4448b7a0a9772541cdd.png)'
- en: 'Source: [**Di Giovanni et al**](https://openreview.net/forum?id=t2tTfWwAEl)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[**Di Giovanni ç­‰**](https://openreview.net/forum?id=t2tTfWwAEl)
- en: Turns out that **width** might help (but with generalization issues), **depth**
    does **not** really help, and **graph topology** (characterized by the commute
    time between nodes) plays the most important role. We can reduce the commute time
    by various *graph rewiring* strategies (adding and removing edges based on spatial
    or spectral properties), and there are many of them (you might have heard about
    the [Ricci flow-based rewiring](https://openreview.net/forum?id=7UmjRGzp-A) that
    took home the Outstanding Paper award at ICLR 2022). In fact, there is a [follow-up
    work](https://arxiv.org/abs/2306.03589) to this study that goes even deeper and
    derives some impossibility statements wrt over-squashing and some MPNN properties
    â€” Iâ€™d highly encourage to read it as well!
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœè¡¨æ˜ï¼Œ**å®½åº¦**å¯èƒ½æœ‰å¸®åŠ©ï¼ˆä½†å­˜åœ¨æ³›åŒ–é—®é¢˜ï¼‰ï¼Œ**æ·±åº¦**å®é™…ä¸Š**æ²¡æœ‰**ä»€ä¹ˆå¸®åŠ©ï¼Œè€Œ**å›¾æ‹“æ‰‘**ï¼ˆç”±èŠ‚ç‚¹é—´çš„é€šå‹¤æ—¶é—´è¡¨å¾ï¼‰èµ·ç€æœ€é‡è¦çš„ä½œç”¨ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡å„ç§*å›¾é‡è¿*ç­–ç•¥ï¼ˆåŸºäºç©ºé—´æˆ–è°±ç‰¹æ€§æ·»åŠ å’Œç§»é™¤è¾¹ï¼‰æ¥å‡å°‘é€šå‹¤æ—¶é—´ï¼Œè¿™äº›ç­–ç•¥æœ‰å¾ˆå¤šï¼ˆä½ å¯èƒ½å¬è¯´è¿‡[åŸºäºRicciæµçš„é‡è¿](https://openreview.net/forum?id=7UmjRGzp-A)ï¼Œè¯¥ç ”ç©¶åœ¨ICLR
    2022ä¸Šè·å¾—äº†æ°å‡ºè®ºæ–‡å¥–ï¼‰ã€‚äº‹å®ä¸Šï¼Œè¿˜æœ‰ä¸€é¡¹[åç»­ç ”ç©¶](https://arxiv.org/abs/2306.03589)å¯¹æ­¤è¿›è¡Œäº†æ›´æ·±å…¥çš„æ¢è®¨ï¼Œæ¨å¯¼äº†ä¸€äº›å…³äºè¿‡åº¦å‹ç¼©å’ŒMPNNå±æ€§çš„ä¸å¯èƒ½æ€§å£°æ˜â€”â€”æˆ‘å¼ºçƒˆæ¨èé˜…è¯»ï¼
- en: '**â¡ï¸** Effective resistance is one example of spatial rewiring strategies,
    and [**Black et al**](https://openreview.net/forum?id=50SO1LwcYU)study it in great
    detail. The Ricci flow-based rewiring works with graph curvature and is studied
    further in the work by [Nguyen et al](https://openreview.net/forum?id=eWAvwKajx2).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**â¡ï¸** æœ‰æ•ˆç”µé˜»æ˜¯ç©ºé—´é‡è¿ç­–ç•¥çš„ä¸€ä¸ªä¾‹å­ï¼Œ[**Black ç­‰**](https://openreview.net/forum?id=50SO1LwcYU)å¯¹æ­¤è¿›è¡Œäº†è¯¦ç»†ç ”ç©¶ã€‚åŸºäºRicciæµçš„é‡è¿æ–¹æ³•ä¸å›¾çš„æ›²ç‡ç›¸å…³ï¼Œè¿›ä¸€æ­¥ç ”ç©¶è§äº[Nguyen
    ç­‰](https://openreview.net/forum?id=eWAvwKajx2)çš„å·¥ä½œä¸­ã€‚'
- en: '**â¡ï¸** Subgraph GNNs continue to be in the spotlight: two works ([**Zhang,
    Feng, Du, et al**](https://openreview.net/forum?id=2Hp7U3k5Ph) and [**Zhou, Wang,
    Zhang**](https://openreview.net/forum?id=K07XAlzh5i)) concurrently derive expressiveness
    hierarchies of the recently proposed subgraph GNNs and their relationship to the
    1- and higher-order WL tests.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**â¡ï¸** å­å›¾GNNç»§ç»­å—åˆ°å…³æ³¨ï¼šä¸¤é¡¹å·¥ä½œï¼ˆ[**Zhang, Feng, Du ç­‰**](https://openreview.net/forum?id=2Hp7U3k5Ph)
    å’Œ [**Zhou, Wang, Zhang**](https://openreview.net/forum?id=K07XAlzh5i)ï¼‰åŒæ—¶æ¨å¯¼äº†æœ€è¿‘æå‡ºçš„å­å›¾GNNçš„è¡¨ç°åŠ›ç­‰çº§åŠå…¶ä¸1é˜¶åŠæ›´é«˜é˜¶WLæµ‹è¯•çš„å…³ç³»ã€‚'
- en: '![](../Images/ba83d721dae2076255c799aaeeb07f6b.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ba83d721dae2076255c799aaeeb07f6b.png)'
- en: Image By Author.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒä½œè€…æä¾›ã€‚
- en: '**New GNN architectures: Delays and Half-hops**'
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**æ–°å‹GNNæ¶æ„ï¼šå»¶è¿Ÿå’ŒåŠè·³**'
- en: 'If you are tired of yet another variation of GCN or GAT, here are some fresh
    ideas that can work with any GNN of your choice:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ åŒå€¦äº†GCNæˆ–GATçš„å„ç§å˜ä½“ï¼Œè¿™é‡Œæœ‰ä¸€äº›æ–°é²œçš„æƒ³æ³•ï¼Œå¯ä»¥ä¸ä»»ä½•ä½ é€‰æ‹©çš„GNNæ­é…ä½¿ç”¨ï¼š
- en: 'â³ As we know from the **Theory** section, rewiring helps combat over-squashing.
    [**Gutteridge et al**](https://openreview.net/forum?id=WEgjbJ6IDN) introduce *â€œDRew:
    Dynamically Rewired Message Passing with Delayâ€* which gradually densifies the
    graph in later GNN layers such that long-distance nodes see the original states
    of previous nodes (the original DRew) or those skip-connections are added based
    on the *delay* â€” depending on a distance between two nodes (the vDRew version).
    For example ( ğŸ–¼ï¸ğŸ‘‡), in vDRew delayed message passing, a starting node from layer
    0 will show its state to 2-hop neighbors on layer 1, and will show its state to
    a 3-hop neighbor on layer 2\. **DRew** significantly improves the ability of vanilla
    GNNs to perform long-range tasks â€” in fact, a DRew-enabled GCN is the current
    [SOTA](https://github.com/vijaydwivedi75/lrgb) on the Peptides-func dataset from
    the [Long Range Graph Benchmark](https://github.com/vijaydwivedi75/lrgb) ğŸ‘€'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 'â³ æ­£å¦‚æˆ‘ä»¬åœ¨**ç†è®º**éƒ¨åˆ†æ‰€çŸ¥ï¼Œé‡æ–°è¿æ¥æœ‰åŠ©äºå¯¹æŠ—è¿‡åº¦æŒ¤å‹ã€‚[**Gutteridge ç­‰äºº**](https://openreview.net/forum?id=WEgjbJ6IDN)
    ä»‹ç»äº†*â€œDRew: åŠ¨æ€é‡è¿çš„å»¶è¿Ÿæ¶ˆæ¯ä¼ é€’â€*ï¼Œå®ƒåœ¨åæœŸ GNN å±‚ä¸­é€æ¸ç¨ å¯†åŒ–å›¾ï¼Œä½¿å¾—é•¿è·ç¦»èŠ‚ç‚¹èƒ½å¤Ÿçœ‹åˆ°å…ˆå‰èŠ‚ç‚¹çš„åŸå§‹çŠ¶æ€ï¼ˆåŸå§‹ DRewï¼‰ï¼Œæˆ–è€…è¿™äº›è·³è·ƒè¿æ¥æ˜¯åŸºäº*å»¶è¿Ÿ*æ¥æ·»åŠ çš„â€”â€”å–å†³äºä¸¤ä¸ªèŠ‚ç‚¹ä¹‹é—´çš„è·ç¦»ï¼ˆvDRew
    ç‰ˆæœ¬ï¼‰ã€‚ä¾‹å¦‚ï¼ˆ ğŸ–¼ï¸ğŸ‘‡ï¼‰ï¼Œåœ¨ vDRew å»¶è¿Ÿæ¶ˆæ¯ä¼ é€’ä¸­ï¼Œæ¥è‡ªå±‚ 0 çš„èµ·å§‹èŠ‚ç‚¹å°†åœ¨å±‚ 1 ä¸Šå‘ 2-hop é‚»å±…å±•ç¤ºå…¶çŠ¶æ€ï¼Œå¹¶å°†åœ¨å±‚ 2 ä¸Šå‘ 3-hop
    é‚»å±…å±•ç¤ºå…¶çŠ¶æ€ã€‚**DRew** æ˜¾è‘—æé«˜äº†æ™®é€š GNN å¤„ç†é•¿è·ç¦»ä»»åŠ¡çš„èƒ½åŠ›â€”â€”å®é™…ä¸Šï¼Œå¯ç”¨ DRew çš„ GCN æ˜¯æ¥è‡ª [é•¿è·ç¦»å›¾åŸºå‡†](https://github.com/vijaydwivedi75/lrgb)
    çš„ Peptides-func æ•°æ®é›†ä¸Šçš„å½“å‰ [SOTA](https://github.com/vijaydwivedi75/lrgb) ğŸ‘€'
- en: '![](../Images/8089fde4f860732baf4f33dc17a967be.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8089fde4f860732baf4f33dc17a967be.png)'
- en: 'Source: [**Gutteridge et al**](https://openreview.net/forum?id=WEgjbJ6IDN)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ¥æº: [**Gutteridge ç­‰äºº**](https://openreview.net/forum?id=WEgjbJ6IDN)'
- en: ğŸ¦˜ Another neat idea by [**Azabou et al**](https://openreview.net/forum?id=lXczFIwQkv)
    is to slow down message passing by inserting new, *slow nodes* at each edge with
    a special connectivity pattern â€” only an incoming connection from the starting
    node and a symmetric edge with the destination node. Slow nodes improve the performance
    of vanilla GNNs on heterophilic benchmarks by a large margin, and it is also possible
    to use slow nodes for self-supervised learning by creating views with different
    locations of slow nodes for the same original graph. **HalfHop** is a no-brainer-to-include
    SSL component that boosts performance and should be in a standard suite of many
    GNN libraries ğŸ‘.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¦˜ å¦ä¸€ä¸ªæœ‰è¶£çš„æƒ³æ³•æ¥è‡ª[**Azabou ç­‰äºº**](https://openreview.net/forum?id=lXczFIwQkv)ï¼Œå³é€šè¿‡åœ¨æ¯æ¡è¾¹ä¸Šæ’å…¥å…·æœ‰ç‰¹æ®Šè¿æ¥æ¨¡å¼çš„æ–°*æ…¢èŠ‚ç‚¹*æ¥å‡æ…¢æ¶ˆæ¯ä¼ é€’â€”â€”ä»…ä»èµ·å§‹èŠ‚ç‚¹æ¥çš„ä¸€ä¸ªè¾“å…¥è¿æ¥å’Œä¸ç›®æ ‡èŠ‚ç‚¹çš„å¯¹ç§°è¾¹ã€‚æ…¢èŠ‚ç‚¹åœ¨å¼‚è´¨æ€§åŸºå‡†æµ‹è¯•ä¸­å¤§å¹…æå‡äº†æ™®é€š
    GNN çš„æ€§èƒ½ï¼Œä¹Ÿå¯ä»¥é€šè¿‡åˆ›å»ºå…·æœ‰ä¸åŒæ…¢èŠ‚ç‚¹ä½ç½®çš„è§†å›¾æ¥å®ç°è‡ªç›‘ç£å­¦ä¹ ï¼Œé’ˆå¯¹ç›¸åŒçš„åŸå§‹å›¾ã€‚**HalfHop** æ˜¯ä¸€ä¸ªæ— éœ€æ·±æ€ç†Ÿè™‘çš„è‡ªç›‘ç£å­¦ä¹ ç»„ä»¶ï¼Œå®ƒæå‡äº†æ€§èƒ½ï¼Œå¹¶ä¸”åº”è¯¥æˆä¸ºè®¸å¤š
    GNN åº“çš„æ ‡å‡†å¥—ä»¶ ğŸ‘ã€‚
- en: '![](../Images/05240ff796d9ab8c9f9984a30a0e5dda.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/05240ff796d9ab8c9f9984a30a0e5dda.png)'
- en: 'Source: [**Azabou et al**](https://openreview.net/forum?id=lXczFIwQkv)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ¥æº: [**Azabou ç­‰äºº**](https://openreview.net/forum?id=lXczFIwQkv)'
- en: '![](../Images/d16ae29ccd4c9d38a6e3081cc4edf72c.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d16ae29ccd4c9d38a6e3081cc4edf72c.png)'
- en: Image By Author.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: '**Generative Models â€” Stable Diffusion for Molecules, Discrete Diffusion**'
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ç”Ÿæˆæ¨¡å‹ â€” åˆ†å­ç¨³å®šæ‰©æ•£ï¼Œç¦»æ•£æ‰©æ•£**'
- en: '**â¡ï¸** Diffusion models might work in the **feature** space (e.g., pixel space
    in image generation like the original DDPM) or in the **latent** space (like Stable
    Diffusion). In the feature space, you have to design the noising process to respect
    symmetries and equivariances of your feature space. In the latent space, you can
    just add Gaussian noise to the features produced by (pre-trained) encoder. Most
    3D molecule generation models work in the feature space (like a pioneering [EDM](https://arxiv.org/abs/2203.17003)),
    and the new **GeoLDM** model by [Xu et al](https://openreview.net/forum?id=sLfHWWrfe2)
    (authors of the prominent [GeoDiff](https://arxiv.org/abs/2203.02923)) is the
    first to define **latent** diffusion for 3D molecule generation. That is, after
    training an EGNN autoencoder, GeoLDM is trained on the denoising objective where
    noise is sampled from a standard Gaussian. GeoLDM brings significant improvements
    over EDM and other non-latent diffusion approaches ğŸ‘.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**â¡ï¸** æ‰©æ•£æ¨¡å‹å¯ä»¥åœ¨**ç‰¹å¾**ç©ºé—´ï¼ˆä¾‹å¦‚ï¼Œå›¾åƒç”Ÿæˆä¸­çš„åƒç´ ç©ºé—´ï¼Œå¦‚åŸå§‹DDPMï¼‰æˆ–**æ½œåœ¨**ç©ºé—´ï¼ˆå¦‚ç¨³å®šæ‰©æ•£ï¼‰ä¸­å·¥ä½œã€‚åœ¨ç‰¹å¾ç©ºé—´ä¸­ï¼Œå¿…é¡»è®¾è®¡å™ªå£°è¿‡ç¨‹ä»¥å°Šé‡ç‰¹å¾ç©ºé—´çš„å¯¹ç§°æ€§å’Œç­‰å˜æ€§ã€‚åœ¨æ½œåœ¨ç©ºé—´ä¸­ï¼Œå¯ä»¥ç®€å•åœ°å‘ï¼ˆé¢„è®­ç»ƒçš„ï¼‰ç¼–ç å™¨ç”Ÿæˆçš„ç‰¹å¾æ·»åŠ é«˜æ–¯å™ªå£°ã€‚å¤§å¤šæ•°3Dåˆ†å­ç”Ÿæˆæ¨¡å‹åœ¨ç‰¹å¾ç©ºé—´ä¸­å·¥ä½œï¼ˆå¦‚å¼€åˆ›æ€§çš„[EDM](https://arxiv.org/abs/2203.17003)ï¼‰ï¼Œè€Œç”±[Xuç­‰äºº](https://openreview.net/forum?id=sLfHWWrfe2)ï¼ˆè‘—åçš„[GeoDiff](https://arxiv.org/abs/2203.02923)çš„ä½œè€…ï¼‰æå‡ºçš„æ–°**GeoLDM**æ¨¡å‹æ˜¯é¦–ä¸ªä¸º3Dåˆ†å­ç”Ÿæˆå®šä¹‰**æ½œåœ¨**æ‰©æ•£çš„æ¨¡å‹ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œåœ¨è®­ç»ƒEGNNè‡ªç¼–ç å™¨ä¹‹åï¼ŒGeoLDMåœ¨å»å™ªç›®æ ‡ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå…¶ä¸­å™ªå£°ä»æ ‡å‡†é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·ã€‚GeoLDMç›¸æ¯”äºEDMå’Œå…¶ä»–éæ½œåœ¨æ‰©æ•£æ–¹æ³•å¸¦æ¥äº†æ˜¾è‘—çš„æ”¹è¿›ğŸ‘ã€‚'
- en: '![](../Images/0ff52f569c070f1ff4ed28af64260f86.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0ff52f569c070f1ff4ed28af64260f86.png)'
- en: 'GeoLDM. Source: [Xu et al](https://openreview.net/forum?id=sLfHWWrfe2)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: GeoLDMã€‚æ¥æºï¼š[Xuç­‰äºº](https://openreview.net/forum?id=sLfHWWrfe2)
- en: '**â¡ï¸** In the realm of non-geometric graphs (just with an adjacency and perhaps
    categorical node features), discrete graph diffusion pioneered by [DiGress](https://openreview.net/forum?id=UaAD-Nu86WX)
    (ICLRâ€™23) seems the most applicable option. [Chen et al](https://openreview.net/forum?id=vn9O1N5ZOw)
    propose **EDGE,** a discrete diffusion model guided by the node degree distribution.
    In contrast to DiGress, the final target graph in EDGE is a disconnected graph
    without edges, a forward noising model removes edges through a Bernoulli distribution,
    and a reverse process adds edges to the most recent *active* nodes (active are
    the nodes whose degrees changed in the previous step). Thanks to the sparsity
    introduced by the degree guidance, EDGE can generate pretty large graphs up to
    4k nodes and 40k edges!'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**â¡ï¸** åœ¨éå‡ ä½•å›¾çš„é¢†åŸŸï¼ˆä»…å…·æœ‰é‚»æ¥å…³ç³»å’Œå¯èƒ½çš„ç±»åˆ«èŠ‚ç‚¹ç‰¹å¾ï¼‰ä¸­ï¼Œç”±[DiGress](https://openreview.net/forum?id=UaAD-Nu86WX)ï¼ˆICLRâ€™23ï¼‰å¼€åˆ›çš„ç¦»æ•£å›¾æ‰©æ•£ä¼¼ä¹æ˜¯æœ€é€‚ç”¨çš„é€‰é¡¹ã€‚[Chenç­‰äºº](https://openreview.net/forum?id=vn9O1N5ZOw)æå‡ºäº†**EDGE**ï¼Œè¿™æ˜¯ä¸€ç§ç”±èŠ‚ç‚¹åº¦åˆ†å¸ƒå¼•å¯¼çš„ç¦»æ•£æ‰©æ•£æ¨¡å‹ã€‚ä¸DiGressä¸åŒï¼ŒEDGEä¸­çš„æœ€ç»ˆç›®æ ‡å›¾æ˜¯æ²¡æœ‰è¾¹çš„æ–­å¼€å›¾ï¼Œå‰å‘å™ªå£°æ¨¡å‹é€šè¿‡ä¼¯åŠªåˆ©åˆ†å¸ƒç§»é™¤è¾¹ï¼Œåå‘è¿‡ç¨‹å°†è¾¹æ·»åŠ åˆ°æœ€è¿‘çš„*æ´»è·ƒ*èŠ‚ç‚¹ï¼ˆæ´»è·ƒèŠ‚ç‚¹æ˜¯æŒ‡åœ¨å‰ä¸€æ­¥ä¸­åº¦æ•°å‘ç”Ÿå˜åŒ–çš„èŠ‚ç‚¹ï¼‰ã€‚ç”±äºåº¦å¼•å¯¼å¼•å…¥çš„ç¨€ç–æ€§ï¼ŒEDGEå¯ä»¥ç”Ÿæˆé«˜è¾¾4kèŠ‚ç‚¹å’Œ40kè¾¹çš„å¤§å‹å›¾ï¼'
- en: '![](../Images/80698f82635bb17812b19baa5e974b13.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/80698f82635bb17812b19baa5e974b13.png)'
- en: Graph Generation with EDGE. Source:[Chen et al](https://openreview.net/forum?id=vn9O1N5ZOw)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç”Ÿæˆä¸EDGEã€‚æ¥æºï¼š[Chenç­‰äºº](https://openreview.net/forum?id=vn9O1N5ZOw)
- en: '**â¡ï¸** Finally, [**â€œGraphically Structured Diffusion Modelsâ€**](https://openreview.net/forum?id=24wzmwrldX)
    by *Weilbach et al* bridges the gap between continuous generative models and probabilistic
    graphical models that induce a certain structure in the problem of interest â€”
    often such problems have a combinatorial nature. The central idea is to encode
    the problemâ€™s structure as an attention mask that respects permutation invariances
    and use this mask in the attention computation in the Transformer encoder (which
    by definition is equivariant to input token permutation unless you use positional
    embeddings). **GSDM** can tackle binary continuous matrix factorization, boolean
    circuits, can generate sudokus, and perform sorting. Particularly enjoyable is
    a pinch of irony the paper is written with ğŸ™ƒ.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**â¡ï¸** æœ€åï¼Œ[**â€œå›¾å½¢ç»“æ„æ‰©æ•£æ¨¡å‹â€**](https://openreview.net/forum?id=24wzmwrldX)ç”±*Weilbachç­‰äºº*æå‡ºï¼Œå¼¥åˆäº†è¿ç»­ç”Ÿæˆæ¨¡å‹ä¸åœ¨æ„Ÿå…´è¶£é—®é¢˜ä¸­å¼•å…¥ç‰¹å®šç»“æ„çš„æ¦‚ç‡å›¾æ¨¡å‹ä¹‹é—´çš„å·®è·â€”â€”é€šå¸¸æ­¤ç±»é—®é¢˜å…·æœ‰ç»„åˆæ€§è´¨ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯å°†é—®é¢˜çš„ç»“æ„ç¼–ç ä¸ºå°Šé‡æ’åˆ—ä¸å˜æ€§çš„æ³¨æ„åŠ›æ©ç ï¼Œå¹¶åœ¨Transformerç¼–ç å™¨çš„æ³¨æ„åŠ›è®¡ç®—ä¸­ä½¿ç”¨è¯¥æ©ç ï¼ˆæ ¹æ®å®šä¹‰ï¼Œé™¤éä½¿ç”¨ä½ç½®åµŒå…¥ï¼Œå¦åˆ™å¯¹è¾“å…¥æ ‡è®°æ’åˆ—æ˜¯ç­‰å˜çš„ï¼‰ã€‚**GSDM**å¯ä»¥å¤„ç†äºŒè¿›åˆ¶è¿ç»­çŸ©é˜µåˆ†è§£ã€å¸ƒå°”ç”µè·¯ï¼Œç”Ÿæˆæ•°ç‹¬ï¼Œå¹¶æ‰§è¡Œæ’åºã€‚ç‰¹åˆ«æœ‰è¶£çš„æ˜¯è®ºæ–‡ä¸­è•´å«çš„ä¸€ä¸è®½åˆºğŸ™ƒã€‚'
- en: '![](../Images/e620b5450f6622c4e491e7a640999da5.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e620b5450f6622c4e491e7a640999da5.png)'
- en: 'GSDM task-to-attention-bias. Source: [**â€œGraphically Structured Diffusion Modelsâ€**](https://openreview.net/forum?id=24wzmwrldX)
    by *Weilbach et al*'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: GSDM ä»»åŠ¡åˆ°æ³¨æ„åŠ›åå·®ã€‚æ¥æºï¼š[**â€œå›¾å½¢ç»“æ„æ‰©æ•£æ¨¡å‹â€**](https://openreview.net/forum?id=24wzmwrldX)
    ç”± *Weilbach ç­‰*
- en: '![](../Images/7385dbac39148c3f404eed5abcd3b871.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7385dbac39148c3f404eed5abcd3b871.png)'
- en: Image By Author
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…æä¾›çš„å›¾ç‰‡
- en: '**Geometric Learning: Geometric WL, Clifford Algebras**'
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**å‡ ä½•å­¦ä¹ ï¼šå‡ ä½• WLï¼ŒClifford ä»£æ•°**'
- en: Geometric Deep Learning thrives! There were so many interesting papers presented
    that would take pretty much the whole post, so Iâ€™d highlight only a few.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å‡ ä½•æ·±åº¦å­¦ä¹ è“¬å‹ƒå‘å±•ï¼æœ‰å¾ˆå¤šæœ‰è¶£çš„è®ºæ–‡å±•ç¤ºï¼Œè¿™å°†å æ®æ•´ä¸ªå¸–å­ï¼Œå› æ­¤æˆ‘åªä¼šé‡ç‚¹ä»‹ç»å‡ ä¸ªã€‚
- en: '**â¡ï¸ Geometric WL** has finally arrived in the work by [Joshi, Bodnar, et al](https://openreview.net/forum?id=6Ed3gchl9L).
    Geometric WL extends the notion of the WL test with geometric features (e.g.,
    coordinates or velocity) and derives the expressiveness hierarchy up to k-order
    GWL. Key takeaways: 1ï¸âƒ£ **equivariant** models are more expressive than **invariant**
    (with a note that in fully connected graphs the difference disappears), 2ï¸âƒ£ **tensor
    order** of features improves expressiveness, 3ï¸âƒ£ **body order** of features improves
    expressiveness (see the image ğŸ‘‡). That is, *spherical > cartesian > scalars*,
    and *many-body interactions > just distances*. The paper also features the amazing
    learning source [Geometric GNN Dojo](https://github.com/chaitjo/geometric-gnn-dojo)
    where you can derive and implement most SOTA models from the first principles!'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**â¡ï¸ å‡ ä½• WL** ç»ˆäºåœ¨ [Joshi, Bodnar ç­‰](https://openreview.net/forum?id=6Ed3gchl9L)
    çš„å·¥ä½œä¸­å‡ºç°ã€‚å‡ ä½• WL æ‰©å±•äº† WL æµ‹è¯•çš„æ¦‚å¿µï¼ŒåŠ å…¥äº†å‡ ä½•ç‰¹å¾ï¼ˆä¾‹å¦‚åæ ‡æˆ–é€Ÿåº¦ï¼‰ï¼Œå¹¶æ¨å¯¼å‡ºè¡¨è¾¾åŠ›å±‚çº§ï¼Œç›´åˆ° k-order GWLã€‚å…³é”®ç‚¹ï¼š1ï¸âƒ£ **ç­‰å˜**
    æ¨¡å‹æ¯” **ä¸å˜** æ¨¡å‹æ›´å…·è¡¨ç°åŠ›ï¼ˆæ³¨æ„åœ¨å®Œå…¨è¿æ¥çš„å›¾ä¸­å·®å¼‚æ¶ˆå¤±ï¼‰ï¼Œ2ï¸âƒ£ **å¼ é‡é˜¶** çš„ç‰¹å¾æå‡äº†è¡¨ç°åŠ›ï¼Œ3ï¸âƒ£ **ä½“åº** çš„ç‰¹å¾æå‡äº†è¡¨ç°åŠ›ï¼ˆè§ä¸‹å›¾
    ğŸ‘‡ï¼‰ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œ*çƒé¢ > ç¬›å¡å°”åæ ‡ > æ ‡é‡*ï¼Œä»¥åŠ *å¤šä½“äº¤äº’ > ä»…è·ç¦»*ã€‚è®ºæ–‡è¿˜å±•ç¤ºäº†ä¸€ä¸ªä»¤äººæƒŠå¹çš„å­¦ä¹ èµ„æº [Geometric GNN Dojo](https://github.com/chaitjo/geometric-gnn-dojo)ï¼Œä½ å¯ä»¥ä»åŸºæœ¬åŸç†æ¨å¯¼å¹¶å®ç°å¤§å¤šæ•°
    SOTA æ¨¡å‹ï¼'
- en: '![](../Images/1a31d31c426441c200f0f409dda8d99b.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1a31d31c426441c200f0f409dda8d99b.png)'
- en: 'Source: [Joshi, Bodnar, et al](https://openreview.net/forum?id=6Ed3gchl9L)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[Joshi, Bodnar ç­‰](https://openreview.net/forum?id=6Ed3gchl9L)
- en: '**â¡ï¸** Going beyond vectors to Clifford algebras, [Ruhe et al](https://openreview.net/forum?id=DNAJdkHPQ5)
    derive **Geometric Clifford Algebra Networks** (GCANs). Clifford algebras naturally
    support higher-order interactions by means of bivectors, trivectors, and (in general)
    multivectors. The key idea is the [Cartan-DieudonnÃ© theorem](https://en.wikipedia.org/wiki/Cartan%E2%80%93Dieudonn%C3%A9_theorem)
    that every orthogonal transformation can be decomposed into *reflections* in hyperplanes,
    and geometric algebras represent data as the elements of the *Pin(p,q,r)* group.
    GCANs introduce a notion of linear layers, normalizations, non-linearities, and
    how they can be parameterized with neural networks. Experiments include modeling
    fluid dynamics and Navier-Stokes equations.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**â¡ï¸** è¶…è¶Šå‘é‡åˆ° Clifford ä»£æ•°ï¼Œ[Ruhe ç­‰](https://openreview.net/forum?id=DNAJdkHPQ5)
    æ¨å¯¼äº† **å‡ ä½• Clifford ä»£æ•°ç½‘ç»œ**ï¼ˆGCANsï¼‰ã€‚Clifford ä»£æ•°é€šè¿‡åŒå‘é‡ã€ä¸‰å‘é‡å’Œï¼ˆä¸€èˆ¬ï¼‰å¤šå‘é‡è‡ªç„¶æ”¯æŒé«˜é˜¶äº¤äº’ã€‚å…³é”®æ€æƒ³æ˜¯ [Cartan-DieudonnÃ©
    å®šç†](https://en.wikipedia.org/wiki/Cartan%E2%80%93Dieudonn%C3%A9_theorem)ï¼Œå³æ¯ä¸ªæ­£äº¤å˜æ¢éƒ½å¯ä»¥åˆ†è§£ä¸ºåœ¨è¶…å¹³é¢ä¸Šçš„
    *åå°„*ï¼Œè€Œå‡ ä½•ä»£æ•°å°†æ•°æ®è¡¨ç¤ºä¸º *Pin(p,q,r)* ç¾¤çš„å…ƒç´ ã€‚GCANs å¼•å…¥äº†çº¿æ€§å±‚ã€å½’ä¸€åŒ–ã€éçº¿æ€§ç­‰æ¦‚å¿µï¼Œä»¥åŠå®ƒä»¬å¦‚ä½•ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œå‚æ•°åŒ–ã€‚å®éªŒåŒ…æ‹¬æµä½“åŠ¨åŠ›å­¦å»ºæ¨¡å’Œ
    Navier-Stokes æ–¹ç¨‹ã€‚'
- en: '![](../Images/27b33c8d8cda800ff13af41870110670.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/27b33c8d8cda800ff13af41870110670.png)'
- en: 'Source: [Ruhe et al](https://openreview.net/forum?id=DNAJdkHPQ5)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[Ruhe ç­‰](https://openreview.net/forum?id=DNAJdkHPQ5)
- en: In fact, there is already a [follow-up work](https://arxiv.org/abs/2305.11141)
    introducing equivariant Clifford NNs â€” you can learn more about Clifford algebras
    foundations and the most recent papers on [CliffordLayers](https://microsoft.github.io/cliffordlayers/)
    supported by Microsoft Research.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œå·²ç»æœ‰ä¸€ç¯‡[åç»­å·¥ä½œ](https://arxiv.org/abs/2305.11141)ä»‹ç»äº†ç­‰å˜ Clifford ç¥ç»ç½‘ç»œâ€”â€”ä½ å¯ä»¥äº†è§£æ›´å¤šå…³äº
    Clifford ä»£æ•°åŸºç¡€ä»¥åŠå¾®è½¯ç ”ç©¶é™¢æ”¯æŒçš„æœ€æ–°è®ºæ–‡ [CliffordLayers](https://microsoft.github.io/cliffordlayers/)ã€‚
- en: ğŸ’Š [Equivariant GNN](http://proceedings.mlr.press/v139/satorras21a/satorras21a.pdf)
    (EGNN) is the Aspirin of Geometric DL that gets applied to almost every task and
    has seen quite a number of improvements. [**Eijkelboom et al**](https://openreview.net/forum?id=hF65aKF8Bf)
    marry EGNN with [Simplicial networks](https://arxiv.org/abs/2103.03212) that operate
    on higher-order structures (namely, simplicial complexes) into **EMPSN**. This
    is one of the first examples that combines geometric and topological features
    and has great improvement potential! Finally, [**Passaro and Zitnick**](https://openreview.net/forum?id=QIejMwU0r9)
    derive a neat trick to reduce SO(3) convolutions to SO(2) bringing the complexity
    down from O(Lâ¶) to O(LÂ³) but with mathematical equivalence guarantees ğŸ‘€. This
    finding allows to scale up geometric models on larger datasets like OpenCatalyst
    and already made it to [Equiformer V2](https://arxiv.org/abs/2306.12059) â€” soon
    in many other libraries for geometric models ğŸ˜‰
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’Š [ç­‰å˜ GNN](http://proceedings.mlr.press/v139/satorras21a/satorras21a.pdf)ï¼ˆEGNNï¼‰æ˜¯å‡ ä½•æ·±åº¦å­¦ä¹ ä¸­çš„é˜¿å¸åŒ¹æ—ï¼Œå‡ ä¹é€‚ç”¨äºæ‰€æœ‰ä»»åŠ¡ï¼Œå¹¶ä¸”å·²ç»çœ‹åˆ°ç›¸å½“å¤šçš„æ”¹è¿›ã€‚[**Eijkelboom
    ç­‰**](https://openreview.net/forum?id=hF65aKF8Bf) å°† EGNN ä¸ [å•çº¯å½¢ç½‘ç»œ](https://arxiv.org/abs/2103.03212)
    ç»“åˆï¼Œè¿™äº›ç½‘ç»œåœ¨é«˜é˜¶ç»“æ„ï¼ˆå³å•çº¯å½¢å¤åˆä½“ï¼‰ä¸Šæ“ä½œï¼Œå½¢æˆ **EMPSN**ã€‚è¿™æ˜¯ç¬¬ä¸€ä¸ªç»“åˆå‡ ä½•å’Œæ‹“æ‰‘ç‰¹å¾çš„ä¾‹å­ä¹‹ä¸€ï¼Œå…·æœ‰å¾ˆå¤§çš„æ”¹è¿›æ½œåŠ›ï¼æœ€åï¼Œ[**Passaro
    å’Œ Zitnick**](https://openreview.net/forum?id=QIejMwU0r9) æ¨å¯¼å‡ºä¸€ç§å·§å¦™çš„æŠ€å·§ï¼Œå°† SO(3) å·ç§¯ç®€åŒ–ä¸º
    SO(2)ï¼Œå°†å¤æ‚åº¦ä» O(Lâ¶) é™åˆ° O(LÂ³)ï¼ŒåŒæ—¶æä¾›äº†æ•°å­¦ç­‰ä»·æ€§ä¿è¯ ğŸ‘€ã€‚è¿™ä¸€å‘ç°ä½¿å¾—å‡ ä½•æ¨¡å‹åœ¨åƒ OpenCatalyst è¿™æ ·çš„å¤§å‹æ•°æ®é›†ä¸Šå¾—ä»¥æ‰©å±•ï¼Œå¹¶å·²è¢«åº”ç”¨äº
    [Equiformer V2](https://arxiv.org/abs/2306.12059) â€”â€” å¾ˆå¿«å°†åœ¨è®¸å¤šå…¶ä»–å‡ ä½•æ¨¡å‹åº“ä¸­å‡ºç° ğŸ˜‰
- en: '![](../Images/211f7d576a50de6991e79cca291a5ac4.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/211f7d576a50de6991e79cca291a5ac4.png)'
- en: Image By Author.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºï¼šä½œè€…ã€‚
- en: '**Molecules: 2D-3D pretraining, Uncertainty Estimation in MD**'
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**åˆ†å­ï¼š2D-3D é¢„è®­ç»ƒï¼ŒMD ä¸­çš„ä¸ç¡®å®šæ€§ä¼°è®¡**'
- en: '**â¡ï¸** [Liu, Du, et al](https://openreview.net/forum?id=mPEVwu50th) propose
    **MoleculeSDE**, a new framework for joint 2D-3D pretraining on molecular data.
    In addition to standard contrastive loss, the authors add two **generative** components:
    reconstructing 2D -> 3D and 3D -> 2D inputs through the score-based diffusion
    generation. Using standard GIN and SchNet as 2D and 3D models, MoleculeSDE is
    pre-trained on PCQM4M v2 and performs well on downstream fine-tuning tasks.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**â¡ï¸** [Liu, Du ç­‰](https://openreview.net/forum?id=mPEVwu50th) æå‡ºäº† **MoleculeSDE**ï¼Œè¿™æ˜¯ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œç”¨äºåœ¨åˆ†å­æ•°æ®ä¸Šè¿›è¡Œ
    2D-3D è”åˆé¢„è®­ç»ƒã€‚é™¤äº†æ ‡å‡†çš„å¯¹æ¯”æŸå¤±å¤–ï¼Œä½œè€…è¿˜æ·»åŠ äº†ä¸¤ä¸ª **ç”Ÿæˆ** ç»„ä»¶ï¼šé€šè¿‡åŸºäºåˆ†æ•°çš„æ‰©æ•£ç”Ÿæˆé‡å»º 2D -> 3D å’Œ 3D -> 2D è¾“å…¥ã€‚ä½¿ç”¨æ ‡å‡†çš„
    GIN å’Œ SchNet ä½œä¸º 2D å’Œ 3D æ¨¡å‹ï¼ŒMoleculeSDE åœ¨ PCQM4M v2 ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒï¼Œå¹¶åœ¨ä¸‹æ¸¸å¾®è°ƒä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ã€‚'
- en: '![](../Images/36ea80c8fb548a5e0c318b4447aa0425.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/36ea80c8fb548a5e0c318b4447aa0425.png)'
- en: 'Source: [MoleculeSDE Github repo](https://github.com/chao1224/MoleculeSDE)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[MoleculeSDE Github ä»“åº“](https://github.com/chao1224/MoleculeSDE)
- en: '**â¡ï¸** [WollschlÃ¤ger et al](https://openreview.net/forum?id=DjwMRloMCO) perform
    a comprehensive study of Uncertainty Estimation in GNNs for molecular dynamics
    and force fields. Identifying key physics-informed and application-focused principles,
    the authors propose a **Localized Neural Kernel**, a Gaussian Process-based extension
    to any geometric GNN that works on invariant and equivariant quantities (tried
    on SchNet, DimeNet, and NequIP). In many cases, LNKâ€™s estimations from one model
    are on par with or better than costly ensembling where youâ€™d need to train several
    models.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**â¡ï¸** [WollschlÃ¤ger ç­‰](https://openreview.net/forum?id=DjwMRloMCO) å¯¹ GNN ä¸­çš„ä¸ç¡®å®šæ€§ä¼°è®¡è¿›è¡Œäº†ä¸€é¡¹å…¨é¢ç ”ç©¶ï¼Œé‡ç‚¹å…³æ³¨åˆ†å­åŠ¨åŠ›å­¦å’ŒåŠ›åœºã€‚è¯†åˆ«å…³é”®çš„ç‰©ç†ä¿¡æ¯å’Œåº”ç”¨å¯¼å‘åŸåˆ™ï¼Œä½œè€…æå‡ºäº†ä¸€ç§
    **å±€éƒ¨ç¥ç»æ ¸**ï¼Œè¿™æ˜¯ä¸€ç§åŸºäºé«˜æ–¯è¿‡ç¨‹çš„æ‰©å±•ï¼Œé€‚ç”¨äºä»»ä½•å‡ ä½• GNNï¼Œå¤„ç†ä¸å˜å’Œç­‰å˜é‡ï¼ˆå·²åœ¨ SchNetã€DimeNet å’Œ NequIP ä¸Šè¿›è¡Œå°è¯•ï¼‰ã€‚åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼ŒLNK
    ä»ä¸€ä¸ªæ¨¡å‹çš„ä¼°è®¡ä¸éœ€è¦è®­ç»ƒå¤šä¸ªæ¨¡å‹çš„æ˜‚è´µé›†æˆæ•ˆæœç›¸å½“æˆ–æ›´å¥½ã€‚'
- en: '![](../Images/c44e143e7fcd2be4bb3a3c5e1a0b13c8.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c44e143e7fcd2be4bb3a3c5e1a0b13c8.png)'
- en: 'Source: [WollschlÃ¤ger et al](https://openreview.net/forum?id=DjwMRloMCO)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[WollschlÃ¤ger ç­‰](https://openreview.net/forum?id=DjwMRloMCO)
- en: '![](../Images/72f1a4c6b25d78630777885379b512c2.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/72f1a4c6b25d78630777885379b512c2.png)'
- en: Image By Author.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡æ¥æºï¼šä½œè€…ã€‚
- en: '**Materials & Proteins: CLIP for proteins, Ewald Message Passing, Equivariant
    Augmentations**'
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**ææ–™ä¸è›‹ç™½è´¨ï¼šCLIP ç”¨äºè›‹ç™½è´¨ï¼ŒEwald æ¶ˆæ¯ä¼ é€’ï¼Œç­‰å˜å¢å¼º**'
- en: CLIP and its descendants have become a standard staple in text-to-image models.
    Can we do the same but for text-to-protein? Yes!
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP åŠå…¶åä»£å·²ç»æˆä¸ºæ–‡æœ¬åˆ°å›¾åƒæ¨¡å‹ä¸­çš„æ ‡å‡†å·¥å…·ã€‚æˆ‘ä»¬èƒ½å¦å¯¹æ–‡æœ¬åˆ°è›‹ç™½è´¨åšåŒæ ·çš„äº‹æƒ…ï¼Ÿå¯ä»¥ï¼
- en: '**â¡ï¸** [Xu, Yuan, et al](https://openreview.net/forum?id=ZOOwHgxfR4) present
    **ProtST**, a framework for learning joint representations of text protein descriptions
    (via PubMedBERT) and protein sequences (via ESM). In addition to a contrastive
    loss, ProtST has a multimodal mask prediction objective, e.g., masking 15% of
    tokens in text and protein sequence, and predicting those jointly based on latent
    representations, and mask prediction losses based on sequences or language alone.
    Additionally, the authors design a novel **ProtDescribe** dataset with 550K aligned
    protein sequence-description pairs. **ProtST** excels across many protein modeling
    tasks in the [**PEER**](https://github.com/DeepGraphLearning/PEER_Benchmark) benchmark,
    including protein function annotation and localization, but also allows for zero-shot
    protein retrieval right from the textual description (see an example below). Looks
    like **ProtST** has a bright future of being a backbone behind many protein generative
    models ğŸ˜‰'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**â¡ï¸** [Xu, Yuan ç­‰](https://openreview.net/forum?id=ZOOwHgxfR4) æå‡ºäº† **ProtST**ï¼Œä¸€ä¸ªç”¨äºå­¦ä¹ æ–‡æœ¬è›‹ç™½è´¨æè¿°ï¼ˆé€šè¿‡
    PubMedBERTï¼‰å’Œè›‹ç™½è´¨åºåˆ—ï¼ˆé€šè¿‡ ESMï¼‰è”åˆè¡¨ç¤ºçš„æ¡†æ¶ã€‚é™¤äº†å¯¹æ¯”æŸå¤±å¤–ï¼ŒProtST è¿˜æœ‰ä¸€ä¸ªå¤šæ¨¡æ€æ©ç é¢„æµ‹ç›®æ ‡ï¼Œä¾‹å¦‚ï¼Œæ©ç›–æ–‡æœ¬å’Œè›‹ç™½è´¨åºåˆ—ä¸­çš„
    15% ä»¤ç‰Œï¼Œå¹¶åŸºäºæ½œåœ¨è¡¨ç¤ºè”åˆé¢„æµ‹è¿™äº›ä»¤ç‰Œï¼Œä»¥åŠåŸºäºåºåˆ—æˆ–è¯­è¨€å•ç‹¬çš„æ©ç é¢„æµ‹æŸå¤±ã€‚æ­¤å¤–ï¼Œä½œè€…è®¾è®¡äº†ä¸€ä¸ªæ–°çš„ **ProtDescribe** æ•°æ®é›†ï¼ŒåŒ…å«
    550K å¯¹é½çš„è›‹ç™½è´¨åºåˆ—-æè¿°å¯¹ã€‚ **ProtST** åœ¨ [**PEER**](https://github.com/DeepGraphLearning/PEER_Benchmark)
    åŸºå‡†æµ‹è¯•ä¸­åœ¨è®¸å¤šè›‹ç™½è´¨å»ºæ¨¡ä»»åŠ¡ä¸Šè¡¨ç°å‡ºè‰²ï¼ŒåŒ…æ‹¬è›‹ç™½è´¨åŠŸèƒ½æ³¨é‡Šå’Œå®šä½ï¼ŒåŒæ—¶è¿˜å…è®¸ä»æ–‡æœ¬æè¿°ä¸­è¿›è¡Œé›¶æ ·æœ¬è›‹ç™½è´¨æ£€ç´¢ï¼ˆè§ä¸‹ä¾‹ï¼‰ã€‚çœ‹èµ·æ¥ **ProtST** å…·æœ‰æˆä¸ºè®¸å¤šè›‹ç™½è´¨ç”Ÿæˆæ¨¡å‹èƒŒåæ ¸å¿ƒçš„å…‰æ˜å‰æ™¯
    ğŸ˜‰'
- en: '![](../Images/724581ab255403ddca3fe2aebda87a1d.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/724581ab255403ddca3fe2aebda87a1d.png)'
- en: 'Source: [Xu, Yuan, et al](https://openreview.net/forum?id=ZOOwHgxfR4)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ¥æº: [Xu, Yuan ç­‰](https://openreview.net/forum?id=ZOOwHgxfR4)'
- en: Actually, ICML features several protein generation works like **GENIE** by [Lin
    and AlQuraishi](https://openreview.net/forum?id=4Kw5hKY8u8) and **FrameDiff**
    by [Yim, Trippe, De Bortoli, Mathieu, et al](https://openreview.net/forum?id=m8OUBymxwv)
    â€” those are not yet conditioned on textual descriptions, so incorporating ProtST
    there looks like a no-brainer performance boost ğŸ“ˆ.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼ŒICML å±•ç¤ºäº†å‡ é¡¹è›‹ç™½è´¨ç”Ÿæˆå·¥ä½œï¼Œå¦‚ [Lin å’Œ AlQuraishi](https://openreview.net/forum?id=4Kw5hKY8u8)
    çš„ **GENIE** å’Œ [Yim, Trippe, De Bortoli, Mathieu ç­‰](https://openreview.net/forum?id=m8OUBymxwv)
    çš„ **FrameDiff** â€” è¿™äº›å·¥ä½œå°šæœªä¾èµ–æ–‡æœ¬æè¿°ï¼Œå› æ­¤å°† ProtST èå…¥å…¶ä¸­ä¼¼ä¹æ˜¯æå‡æ€§èƒ½çš„æ˜æ™ºé€‰æ‹© ğŸ“ˆ
- en: '![](../Images/acfc681ee5d4ad4efc9a0d11266aae01.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/acfc681ee5d4ad4efc9a0d11266aae01.png)'
- en: 'Gif Source: [SE(3) Diffusion Github](https://github.com/jasonkyuyim/se3_diffusion)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 'Gif æ¥æº: [SE(3) Diffusion Github](https://github.com/jasonkyuyim/se3_diffusion)'
- en: âš›ï¸ MPNNs on molecules have a strict locality bias that inhibits modeling long-range
    interactions. [Kosmala et al](https://openreview.net/forum?id=vd5JYAml0A) derive
    **Ewald Message Passing** and apply the idea of [Ewald summation](https://en.wikipedia.org/wiki/Ewald_summation)
    that breaks down the interaction potential into short-range and long-range terms.
    Short-range interaction is modeled by any GNN while long-range interaction is
    new and is modeled with a **3D Fourier transform** and message passing over Fourier
    frequencies. Turns out this long-range term is pretty flexible and can be applied
    to any network modeling periodic and aperiodic systems (like crystals or molecules)
    like SchNet, DimeNet, or GemNet. The model was evaluated on OC20 and OE62 datasets.
    If you are interested in more details, check out the [1-hour talk by Arthur Kosmala](https://www.youtube.com/watch?v=Ip8EGde5SUQ)
    at the LOG2 Reading Group!
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: âš›ï¸ åˆ†å­ä¸Šçš„ MPNNs å…·æœ‰ä¸¥æ ¼çš„å±€éƒ¨æ€§åå·®ï¼Œè¿™ä¼šæŠ‘åˆ¶å¯¹é•¿ç¨‹äº¤äº’çš„å»ºæ¨¡ã€‚ [Kosmala ç­‰](https://openreview.net/forum?id=vd5JYAml0A)
    æ¨å¯¼å‡ºäº† **Ewald æ¶ˆæ¯ä¼ é€’**ï¼Œå¹¶åº”ç”¨äº† [Ewald æ±‚å’Œ](https://en.wikipedia.org/wiki/Ewald_summation)
    çš„æ€æƒ³ï¼Œè¯¥æ€æƒ³å°†ç›¸äº’ä½œç”¨åŠ¿èƒ½åˆ†è§£ä¸ºçŸ­ç¨‹å’Œé•¿ç¨‹é¡¹ã€‚çŸ­ç¨‹äº¤äº’ç”±ä»»ä½• GNN å»ºæ¨¡ï¼Œè€Œé•¿ç¨‹äº¤äº’åˆ™æ˜¯æ–°çš„ï¼Œé€šè¿‡ **3D å‚…é‡Œå¶å˜æ¢** å’Œå‚…é‡Œå¶é¢‘ç‡ä¸Šçš„æ¶ˆæ¯ä¼ é€’æ¥å»ºæ¨¡ã€‚ç»“æœè¡¨æ˜ï¼Œè¿™ä¸€é•¿ç¨‹é¡¹ç›¸å½“çµæ´»ï¼Œå¯ä»¥åº”ç”¨äºä»»ä½•å»ºæ¨¡å‘¨æœŸæ€§å’Œéå‘¨æœŸæ€§ç³»ç»Ÿï¼ˆå¦‚æ™¶ä½“æˆ–åˆ†å­ï¼‰çš„ç½‘ç»œï¼Œå¦‚
    SchNetã€DimeNet æˆ– GemNetã€‚è¯¥æ¨¡å‹åœ¨ OC20 å’Œ OE62 æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚å¦‚æœä½ å¯¹æ›´å¤šç»†èŠ‚æ„Ÿå…´è¶£ï¼Œå¯ä»¥æŸ¥çœ‹ [Arthur Kosmala
    çš„ 1 å°æ—¶è®²åº§](https://www.youtube.com/watch?v=Ip8EGde5SUQ)ï¼
- en: '![](../Images/5d9f656fc6565a98a0b8db30a1aca1ba.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5d9f656fc6565a98a0b8db30a1aca1ba.png)'
- en: 'Source: [Kosmala et al](https://openreview.net/forum?id=vd5JYAml0A)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ¥æº: [Kosmala ç­‰](https://openreview.net/forum?id=vd5JYAml0A)'
- en: A similar idea of using Ewald summation for 3D crystals is used in **PotNet**
    by [Lin et al](https://openreview.net/forum?id=jxI4CulNr1) where the long-range
    connection is modeled with incomplete Bessel functions. PotNet was evaluated on
    the Materials Project dataset and JARVIS â€” so reading those two papers you can
    have a good understanding of the benefits brought by Ewald summation for many
    crystal-related tasks ğŸ˜‰
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨**PotNet**ä¸­ï¼Œ[æ—ç­‰äºº](https://openreview.net/forum?id=jxI4CulNr1)ä½¿ç”¨äº†ä¸Ewaldæ±‚å’Œç±»ä¼¼çš„æ€æƒ³æ¥å¤„ç†3Dæ™¶ä½“ï¼Œå…¶ä¸­é•¿ç¨‹è¿æ¥ä½¿ç”¨ä¸å®Œå…¨è´å¡å°”å‡½æ•°å»ºæ¨¡ã€‚PotNetåœ¨Materials
    Projectæ•°æ®é›†å’ŒJARVISä¸Šè¿›è¡Œäº†è¯„ä¼°ï¼Œå› æ­¤é˜…è¯»è¿™ä¸¤ç¯‡è®ºæ–‡å¯ä»¥å¾ˆå¥½åœ°ç†è§£Ewaldæ±‚å’Œä¸ºè®¸å¤šä¸æ™¶ä½“ç›¸å…³çš„ä»»åŠ¡å¸¦æ¥çš„å¥½å¤„ ğŸ˜‰
- en: '![](../Images/2c37a782852ab0a7c1e90efe7aea17d7.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2c37a782852ab0a7c1e90efe7aea17d7.png)'
- en: 'Source: [Lin et al](https://openreview.net/forum?id=jxI4CulNr1)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[æ—ç­‰äºº](https://openreview.net/forum?id=jxI4CulNr1)
- en: '**â¡ï¸** Another look at imbuing *any* GNNs with equivariance for crystals and
    molecules is given by [Duval, Schmidt, et al](https://openreview.net/forum?id=HRDRZNxQXc)
    in **FAENet**. A standard way is to bake certain symmetries and equivariances
    right into GNN architectures (like in EGNN, GemNet, and Ewald Message Passing)
    â€” this is a safe but computationally expensive way (especially when it comes to
    spherical harmonics and tensor products). Another option often used in vision
    â€” show many augmentations of the same input and the model should eventually learn
    the same invariances in the augmentations. The authors go for the 2nd path and
    design a rigorous way to sample 2D / 3D data invariant or equivariant augmentations
    (e.g., for energy or forces, respectively) all with fancy proofs âœï¸. For that,
    the data augmentation pipeline includes projecting 2D / 3D inputs to a canonical
    representation (based on PCA of the covariance matrix of distances) from which
    we can uniformly sample rotations.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**â¡ï¸** å¦ä¸€ç§æ–¹æ³•æ˜¯é€šè¿‡[æœç“¦å°”ã€æ–½å¯†ç‰¹ç­‰äºº](https://openreview.net/forum?id=HRDRZNxQXc)åœ¨**FAENet**ä¸­èµ‹äºˆ*ä»»ä½•*
    GNNsæ™¶ä½“å’Œåˆ†å­ç­‰æ•ˆæ€§çš„çœ‹æ³•ã€‚ä¸€ç§æ ‡å‡†çš„æ–¹æ³•æ˜¯å°†æŸäº›å¯¹ç§°æ€§å’Œç­‰æ•ˆæ€§ç›´æ¥åµŒå…¥GNNæ¶æ„ä¸­ï¼ˆä¾‹å¦‚åœ¨EGNNã€GemNetå’ŒEwald Message Passingä¸­ï¼‰â€”
    è¿™æ˜¯ä¸€ç§å®‰å…¨ä½†è®¡ç®—æ˜‚è´µçš„æ–¹å¼ï¼ˆç‰¹åˆ«æ˜¯æ¶‰åŠçƒè°å‡½æ•°å’Œå¼ é‡ç§¯æ—¶ï¼‰ã€‚å¦ä¸€ç§é€‰é¡¹é€šå¸¸ç”¨äºè§†è§‰ â€” å±•ç¤ºç›¸åŒè¾“å…¥çš„è®¸å¤šå¢å¼ºï¼Œæ¨¡å‹æœ€ç»ˆåº”è¯¥å­¦ä¹ åˆ°å¢å¼ºä¸­çš„ç›¸åŒä¸å˜æ€§ã€‚ä½œè€…é€‰æ‹©ç¬¬äºŒæ¡è·¯å¾„ï¼Œå¹¶è®¾è®¡äº†ä¸€ç§ä¸¥æ ¼çš„æ–¹æ³•æ¥é‡‡æ ·2D
    / 3Dæ•°æ®çš„ä¸å˜æˆ–ç­‰å˜å¢å¼ºï¼ˆä¾‹å¦‚ç”¨äºèƒ½é‡æˆ–åŠ›çš„å¢å¼ºï¼‰ï¼Œæ‰€æœ‰è¿™äº›å¢å¼ºéƒ½æœ‰èŠ±å“¨çš„è¯æ˜ âœï¸ã€‚ä¸ºæ­¤ï¼Œæ•°æ®å¢å¼ºç®¡é“åŒ…æ‹¬å°†2D / 3Dè¾“å…¥æŠ•å½±åˆ°åŸºäºè·ç¦»åæ–¹å·®çŸ©é˜µçš„PCAçš„è§„èŒƒè¡¨ç¤ºä¸­ï¼Œä»ä¸­æˆ‘ä»¬å¯ä»¥å‡åŒ€é‡‡æ ·æ—‹è½¬ã€‚'
- en: The proposed FAENet is a simple model that uses only distances but shows very
    good performance with the stochastic frame averaging data augmentation while being
    6â€“20 times faster. Works for crystal structures as well!
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: æå‡ºçš„FAENetæ˜¯ä¸€ä¸ªç®€å•çš„æ¨¡å‹ï¼Œåªä½¿ç”¨è·ç¦»ï¼Œä½†åœ¨ä½¿ç”¨éšæœºå¸§å¹³å‡æ•°æ®å¢å¼ºæ—¶è¡¨ç°éå¸¸å¥½ï¼ŒåŒæ—¶é€Ÿåº¦å¿«6åˆ°20å€ã€‚åŒæ ·é€‚ç”¨äºæ™¶ä½“ç»“æ„ï¼
- en: '![](../Images/893a76e8a13595d1073078d444759d13.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/893a76e8a13595d1073078d444759d13.png)'
- en: 'Augmentations and Stochastic Frame Averaging. Source: [Duval, Schmidt, et al](https://openreview.net/forum?id=HRDRZNxQXc)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: å¢å¼ºå’Œéšæœºå¸§å¹³å‡ã€‚æ¥æºï¼š[æœç“¦å°”ã€æ–½å¯†ç‰¹ç­‰](https://openreview.net/forum?id=HRDRZNxQXc)
- en: '![](../Images/45f447e910299d52cd636e7cb49cf723.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/45f447e910299d52cd636e7cb49cf723.png)'
- en: Image By Author.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±ä½œè€…æä¾›ã€‚
- en: '**Cool Applications: Algorithmic Reasoning, Inductive KG Completion, GNNs for
    Mass Spectra**'
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**é…·ç‚«åº”ç”¨ï¼šç®—æ³•æ¨ç†ï¼Œå½’çº³KGå®Œæˆï¼Œè´¨è°±GNN**'
- en: A few papers in this section did not belong to any of the above but are still
    worthy of your attention.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚ä¸­çš„å‡ ç¯‡è®ºæ–‡ä¸å±äºä¸Šè¿°ä»»ä½•ä¸€ç¯‡ï¼Œä½†ä»å€¼å¾—å…³æ³¨ã€‚
- en: '**â¡ï¸** [**â€Neural Algorithmic Reasoning with Causal Regularisationâ€**](https://openreview.net/forum?id=kP2p67F4G7)
    by *Bevilacqua et al* tackles a common issue in graph learning â€” OOD generalization
    to larger inputs at test time. Studying OOD generalization in algorithmic reasoning
    problems, the authors observe that there exist many different inputs that make
    identical computations at a certain step. At the same time, it means that some
    subset of inputs does not (should not) affect the prediction result. This assumption
    allows to design a self-supervised objective (termed **Hint-ReLIC**) that prefers
    a â€œmeaningfulâ€ step to a bunch of steps that do not affect the prediction result.
    The new objective significantly bumps the performance on many CLRS-30 tasks to
    90+% micro-F1\. It is an interesting question whether we could leverage the same
    principle in general message passing and improve OOD transfer in other graph learning
    tasks ğŸ¤”'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**â¡ï¸** [**â€œç¥ç»ç®—æ³•æ¨ç†ä¸å› æœæ­£åˆ™åŒ–â€**](https://openreview.net/forum?id=kP2p67F4G7) *Bevilacqua
    ç­‰äºº* è§£å†³äº†å›¾å­¦ä¹ ä¸­çš„ä¸€ä¸ªå¸¸è§é—®é¢˜â€”â€”åœ¨æµ‹è¯•æ—¶å¯¹æ›´å¤§è¾“å…¥çš„OODæ³›åŒ–ã€‚ç ”ç©¶ç®—æ³•æ¨ç†é—®é¢˜ä¸­çš„OODæ³›åŒ–æ—¶ï¼Œä½œè€…è§‚å¯Ÿåˆ°åœ¨æŸä¸€æ­¥éª¤ä¸­å­˜åœ¨è®¸å¤šä¸åŒçš„è¾“å…¥äº§ç”Ÿç›¸åŒçš„è®¡ç®—ã€‚ä¸æ­¤åŒæ—¶ï¼Œè¿™ä¹Ÿæ„å‘³ç€æŸäº›è¾“å…¥å­é›†ä¸ä¼šï¼ˆæˆ–ä¸åº”è¯¥ï¼‰å½±å“é¢„æµ‹ç»“æœã€‚è¿™ä¸ªå‡è®¾ä½¿å¾—å¯ä»¥è®¾è®¡ä¸€ä¸ªè‡ªç›‘ç£ç›®æ ‡ï¼ˆç§°ä¸º**Hint-ReLIC**ï¼‰ï¼Œè¯¥ç›®æ ‡æ›´å€¾å‘äºä¸€ä¸ªâ€œæœ‰æ„ä¹‰â€çš„æ­¥éª¤è€Œä¸æ˜¯ä¸€å †ä¸å½±å“é¢„æµ‹ç»“æœçš„æ­¥éª¤ã€‚è¿™ä¸ªæ–°ç›®æ ‡æ˜¾è‘—æé«˜äº†è®¸å¤šCLRS-30ä»»åŠ¡çš„è¡¨ç°ï¼Œè¾¾åˆ°90%ä»¥ä¸Šçš„micro-F1ã€‚æ˜¯å¦å¯ä»¥åœ¨ä¸€èˆ¬æ¶ˆæ¯ä¼ é€’ä¸­åˆ©ç”¨ç›¸åŒçš„åŸåˆ™æ¥æ”¹è¿›å…¶ä»–å›¾å­¦ä¹ ä»»åŠ¡ä¸­çš„OODè½¬ç§»æ˜¯ä¸€ä¸ªæœ‰è¶£çš„é—®é¢˜
    ğŸ¤”'
- en: '![](../Images/41ff0b6d703cdfba98fb986c679f46b4.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41ff0b6d703cdfba98fb986c679f46b4.png)'
- en: 'Source: [**â€Neural Algorithmic Reasoning with Causal Regularisationâ€**](https://openreview.net/forum?id=kP2p67F4G7)
    by *Bevilacqua et al*'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[**â€œç¥ç»ç®—æ³•æ¨ç†ä¸å› æœæ­£åˆ™åŒ–â€**](https://openreview.net/forum?id=kP2p67F4G7) *Bevilacqua
    ç­‰äºº*ã€‚
- en: If you are further interested in neural algorithmic reasoning, check out the
    proceedings of the [Knowledge and Logical Reasoning workshop](https://klr-icml2023.github.io/papers.html)
    which has even more works on that topic.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å¯¹ç¥ç»ç®—æ³•æ¨ç†æ›´æ„Ÿå…´è¶£ï¼Œå¯ä»¥æŸ¥çœ‹[çŸ¥è¯†ä¸é€»è¾‘æ¨ç†ç ”è®¨ä¼š](https://klr-icml2023.github.io/papers.html)çš„ä¼šè®®è®°å½•ï¼Œå…¶ä¸­åŒ…å«æ›´å¤šç›¸å…³å·¥ä½œã€‚
- en: '**â¡ï¸** [**â€œInGram: Inductive Knowledge Graph Embedding via Relation Graphsâ€**](https://openreview.net/forum?id=OoOpO0u4Xd)
    by *Lee et al* seems to be one of the very few knowledge graph papers at ICMLâ€™23
    (to the best of my search). **InGram** is one of the first approaches that can
    inductively generalize to both unseen entities and **unseen relations** at test
    time. Previously, inductive KG models needed to learn at least relation embeddings
    in some form to generalize to new nodes, and in this paradigm, new unseen relations
    are non-trivial to model. InGram builds a relation graph on top of the original
    multi-relational graph, that is, a graph of relation types, and learns representations
    of relations based on this graph by running a GAT. Entity representations are
    obtained from the random initialization and a GNN encoder. Having both entity
    and relation representations, a DistMult decoder is applied as a scoring function.
    There are good chances that InGram for unseen relations might be as influential
    as [GraIL (ICML 2020)](http://proceedings.mlr.press/v119/teru20a/teru20a.pdf)
    for unseen entities ğŸ˜‰.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**â¡ï¸** [**â€œInGram: é€šè¿‡å…³ç³»å›¾çš„å½’çº³çŸ¥è¯†å›¾åµŒå…¥â€**](https://openreview.net/forum?id=OoOpO0u4Xd)
    *Lee ç­‰äºº* ä¼¼ä¹æ˜¯ICMLâ€™23ä¸Šä¸ºæ•°ä¸å¤šçš„çŸ¥è¯†å›¾è°±è®ºæ–‡ä¹‹ä¸€ï¼ˆæ ¹æ®æˆ‘çš„æœç´¢ï¼‰ã€‚**InGram** æ˜¯é¦–æ‰¹èƒ½å¤Ÿåœ¨æµ‹è¯•æ—¶å¯¹æœªè§å®ä½“å’Œ**æœªè§å…³ç³»**è¿›è¡Œå½’çº³æ³›åŒ–çš„æ–¹æ³•ä¹‹ä¸€ã€‚ä¹‹å‰çš„å½’çº³KGæ¨¡å‹éœ€è¦ä»¥æŸç§å½¢å¼å­¦ä¹ å…³ç³»åµŒå…¥ä»¥å¯¹æ–°èŠ‚ç‚¹è¿›è¡Œæ³›åŒ–ï¼Œè€Œåœ¨è¿™ç§èŒƒå¼ä¸‹ï¼Œæ–°çš„æœªè§å…³ç³»å¾ˆéš¾å»ºæ¨¡ã€‚InGramåœ¨åŸå§‹çš„å¤šå…³ç³»å›¾ä¸Šæ„å»ºäº†ä¸€ä¸ªå…³ç³»å›¾ï¼Œå³å…³ç³»ç±»å‹çš„å›¾ï¼Œå¹¶é€šè¿‡è¿è¡ŒGATåŸºäºè¿™ä¸ªå›¾å­¦ä¹ å…³ç³»çš„è¡¨ç¤ºã€‚å®ä½“è¡¨ç¤ºæ˜¯ä»éšæœºåˆå§‹åŒ–å’ŒGNNç¼–ç å™¨ä¸­è·å¾—çš„ã€‚æ‹¥æœ‰å®ä½“å’Œå…³ç³»è¡¨ç¤ºåï¼Œåº”ç”¨DistMultè§£ç å™¨ä½œä¸ºè¯„åˆ†å‡½æ•°ã€‚InGramåœ¨æœªè§å…³ç³»æ–¹é¢æœ‰å¾ˆå¤§æœºä¼šä¸[GraIL
    (ICML 2020)](http://proceedings.mlr.press/v119/teru20a/teru20a.pdf)å¯¹æœªè§å®ä½“çš„å½±å“ç›¸å½“
    ğŸ˜‰ã€‚'
- en: '![](../Images/6d1039c15f9e6f9a04a02604d808dda2.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d1039c15f9e6f9a04a02604d808dda2.png)'
- en: 'Source: [**â€œInGram: Inductive Knowledge Graph Embedding via Relation Graphsâ€**](https://openreview.net/forum?id=OoOpO0u4Xd)
    by *Lee et al*'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ¥æºï¼š[**â€œInGram: é€šè¿‡å…³ç³»å›¾çš„å½’çº³çŸ¥è¯†å›¾åµŒå…¥â€**](https://openreview.net/forum?id=OoOpO0u4Xd)
    *Lee ç­‰äºº*ã€‚'
- en: ğŸŒˆ [**â€Efficiently predicting high resolution mass spectra with graph neural
    networksâ€**](https://openreview.net/forum?id=81RIPI742h) by *Murphy et al* is
    a cool application of GNNs to a real physics problem of predicting mass spectra.
    The main finding is that most of the signal in mass spectra is explained by a
    small number of components (product ion and neutral loss *formulas*). And it is
    possible to mine a vocabulary of those *formulas* from training data. The problem
    can thus be framed as graph classification (or graph property prediction) when,
    given a molecular graph, we predict tokens from a vocabulary that correspond to
    certain mass spectra values. The approach, **GRAFF-MS**, builds molecular graph
    representation through GIN with edge features, with Laplacian features (via SignNet),
    and pooled with covariate features. Compared to the baseline CFM-ID, GRAFF-MS
    performs inference in ~19 minutes compared to 126 hours reaching much higher performance
    ğŸ‘€.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸŒˆ [**â€œåˆ©ç”¨å›¾ç¥ç»ç½‘ç»œé«˜æ•ˆé¢„æµ‹é«˜åˆ†è¾¨ç‡è´¨è°±â€**](https://openreview.net/forum?id=81RIPI742h)ç”±*Murphyç­‰äºº*æå‡ºï¼Œæ˜¯ä¸€ä¸ªå°†GNNsåº”ç”¨äºé¢„æµ‹è´¨è°±è¿™ä¸€å®é™…ç‰©ç†é—®é¢˜çš„é…·åº”ç”¨ã€‚ä¸»è¦å‘ç°æ˜¯ï¼Œå¤§éƒ¨åˆ†è´¨è°±ä¿¡å·å¯ä»¥é€šè¿‡å°‘æ•°å‡ ä¸ªæˆåˆ†ï¼ˆäº§ç‰©ç¦»å­å’Œä¸­æ€§ä¸§å¤±*å…¬å¼*ï¼‰æ¥è§£é‡Šã€‚å¹¶ä¸”æœ‰å¯èƒ½ä»è®­ç»ƒæ•°æ®ä¸­æŒ–æ˜å‡ºè¿™äº›*å…¬å¼*çš„è¯æ±‡ã€‚å› æ­¤ï¼Œè¿™ä¸ªé—®é¢˜å¯ä»¥è¢«æ¡†æ¶ä¸ºå›¾åˆ†ç±»ï¼ˆæˆ–å›¾å±æ€§é¢„æµ‹ï¼‰ï¼Œåœ¨ç»™å®šåˆ†å­å›¾çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬é¢„æµ‹ä¸æŸäº›è´¨è°±å€¼å¯¹åº”çš„è¯æ±‡é¡¹ã€‚è¿™ç§æ–¹æ³•ï¼Œ**GRAFF-MS**ï¼Œé€šè¿‡GINæ„å»ºå¸¦æœ‰è¾¹ç‰¹å¾çš„åˆ†å­å›¾è¡¨ç¤ºï¼Œé€šè¿‡SignNetè·å¾—æ‹‰æ™®æ‹‰æ–¯ç‰¹å¾ï¼Œå¹¶ä¸åå˜é‡ç‰¹å¾è¿›è¡Œæ±‡æ€»ã€‚ä¸åŸºçº¿CFM-IDç›¸æ¯”ï¼ŒGRAFF-MSçš„æ¨æ–­æ—¶é—´ä¸ºçº¦19åˆ†é’Ÿï¼Œè€ŒCFM-IDåˆ™ä¸º126å°æ—¶ï¼Œæ€§èƒ½æ˜¾è‘—æå‡ğŸ‘€ã€‚
- en: '![](../Images/594717432f130e81320f52d0aae3cda5.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/594717432f130e81320f52d0aae3cda5.png)'
- en: 'Source: [**â€Efficiently predicting high resolution mass spectra with graph
    neural networksâ€**](https://openreview.net/forum?id=81RIPI742h) by *Murphy et
    al*'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ¥æº: [**â€œåˆ©ç”¨å›¾ç¥ç»ç½‘ç»œé«˜æ•ˆé¢„æµ‹é«˜åˆ†è¾¨ç‡è´¨è°±â€**](https://openreview.net/forum?id=81RIPI742h)ç”±*Murphyç­‰äºº*æå‡º'
- en: The Concluding Meme Part
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®ºæ€§è¡¨æƒ…åŒ…éƒ¨åˆ†
- en: '![](../Images/ea3760ec5d97e92fb80054a68be94867.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea3760ec5d97e92fb80054a68be94867.png)'
- en: Four Michaels (+ epsilon in the background) on the same photo!
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: åŒä¸€å¼ ç…§ç‰‡ä¸Šçš„å››ä½Michaelsï¼ˆèƒŒæ™¯ä¸­è¿˜æœ‰ä¸€ä¸ªÎµï¼‰ï¼
- en: The meme of 2022 has finally converged to [Michael Bronstein](https://michael-bronstein.medium.com/)!
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 2022å¹´çš„è¡¨æƒ…åŒ…ç»ˆäºæ±‡èšåˆ°äº†[Michael Bronstein](https://michael-bronstein.medium.com/)ï¼
