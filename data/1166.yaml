- en: Neural Networks as Decision Trees
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络作为决策树
- en: 原文：[https://towardsdatascience.com/neural-networks-as-decision-trees-89cd9fdcdf6a?source=collection_archive---------5-----------------------#2023-04-03](https://towardsdatascience.com/neural-networks-as-decision-trees-89cd9fdcdf6a?source=collection_archive---------5-----------------------#2023-04-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/neural-networks-as-decision-trees-89cd9fdcdf6a?source=collection_archive---------5-----------------------#2023-04-03](https://towardsdatascience.com/neural-networks-as-decision-trees-89cd9fdcdf6a?source=collection_archive---------5-----------------------#2023-04-03)
- en: '![](../Images/1b720424104554f1a4898b202caf97a0.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1b720424104554f1a4898b202caf97a0.png)'
- en: Photo by [Jens Lelie](https://unsplash.com/@madebyjens?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Jens Lelie](https://unsplash.com/@madebyjens?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Get the power of a Neural Network with the interpretable structure of a Decision
    Tree
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将神经网络的强大功能与决策树的可解释结构结合起来
- en: '[](https://medium.com/@upadhyan?source=post_page-----89cd9fdcdf6a--------------------------------)[![Nakul
    Upadhya](../Images/336cb21272e9b1f098177adbde50e92e.png)](https://medium.com/@upadhyan?source=post_page-----89cd9fdcdf6a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----89cd9fdcdf6a--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----89cd9fdcdf6a--------------------------------)
    [Nakul Upadhya](https://medium.com/@upadhyan?source=post_page-----89cd9fdcdf6a--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@upadhyan?source=post_page-----89cd9fdcdf6a--------------------------------)[![Nakul
    Upadhya](../Images/336cb21272e9b1f098177adbde50e92e.png)](https://medium.com/@upadhyan?source=post_page-----89cd9fdcdf6a--------------------------------)[](https://towardsdatascience.com/?source=post_page-----89cd9fdcdf6a--------------------------------)[![数据科学之路](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----89cd9fdcdf6a--------------------------------)
    [Nakul Upadhya](https://medium.com/@upadhyan?source=post_page-----89cd9fdcdf6a--------------------------------)'
- en: ·
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4d9dddc62a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-as-decision-trees-89cd9fdcdf6a&user=Nakul+Upadhya&userId=4d9dddc62a80&source=post_page-4d9dddc62a80----89cd9fdcdf6a---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----89cd9fdcdf6a--------------------------------)
    ·9 min read·Apr 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F89cd9fdcdf6a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-as-decision-trees-89cd9fdcdf6a&user=Nakul+Upadhya&userId=4d9dddc62a80&source=-----89cd9fdcdf6a---------------------clap_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F4d9dddc62a80&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-as-decision-trees-89cd9fdcdf6a&user=Nakul+Upadhya&userId=4d9dddc62a80&source=post_page-4d9dddc62a80----89cd9fdcdf6a---------------------post_header-----------)
    发布于 [数据科学之路](https://towardsdatascience.com/?source=post_page-----89cd9fdcdf6a--------------------------------)
    · 9 分钟阅读 · 2023年4月3日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F89cd9fdcdf6a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-as-decision-trees-89cd9fdcdf6a&user=Nakul+Upadhya&userId=4d9dddc62a80&source=-----89cd9fdcdf6a---------------------clap_footer-----------)'
- en: --
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F89cd9fdcdf6a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-as-decision-trees-89cd9fdcdf6a&source=-----89cd9fdcdf6a---------------------bookmark_footer-----------)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F89cd9fdcdf6a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fneural-networks-as-decision-trees-89cd9fdcdf6a&source=-----89cd9fdcdf6a---------------------bookmark_footer-----------)'
- en: The recent boom in AI has clearly shown the power of deep neural networks in
    various tasks, especially in the field of classification problems where the data
    is high-dimensional and has complex, non-linear relationships with the target
    variables. However, explaining the decisions of any neural classifier is an incredibly
    hard problem. While many post-hoc methods such as DeepLift [2] and Layer-Wise
    Relevance Propagation [3] can help with explaining individual decisions, explaining
    the global decision mechanisms (or what the model generally looks for) is much
    more difficult.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能的近期繁荣清楚地展示了深度神经网络在各种任务中的强大能力，尤其是在数据维度高且与目标变量之间存在复杂非线性关系的分类问题领域。然而，解释任何神经分类器的决策是一个非常困难的问题。虽然许多后置方法如DeepLift
    [2] 和 Layer-Wise Relevance Propagation [3] 可以帮助解释单个决策，但解释全局决策机制（即模型通常寻找的内容）则更加困难。
- en: Because of this, many practitioners in high-stakes fields instead opt for more
    interpretable models like basic Decision Trees since the decision hierarchy can
    be clearly visualized and understood by stakeholders. However, basic trees by
    themselves often do not provide enough accuracy for the task at hand and often
    ensemble methods like Bagging or Boosting are used to improve the model’s performance.
    This however again sacrifices some interpretability as in order for one to understand
    a single decision, a practitioner would need to look through hundreds of trees.
    However, these are still preferred over deep networks as at least feature importance
    (on both a local and global scale) can be easily retrieved and displayed.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，许多高风险领域的从业者更倾向于选择更具可解释性的模型，如基本的决策树，因为决策层级可以被利益相关者清晰地可视化和理解。然而，基本的决策树往往不能提供足够的准确性，通常会使用集成方法如Bagging或Boosting来提高模型的性能。不过，这又牺牲了一些可解释性，因为要理解一个单一的决策，从业者需要查看数百棵树。然而，这些方法仍然比深度网络更受欢迎，因为至少特征重要性（无论是局部还是全局）可以被轻松提取和展示。
- en: So the problem at hand is that we want the discriminative power of a neural
    network, but with the interpretability of a decision tree. So why don’t we just
    structure our network as a tree? Well, that is the main approach taken by Fross
    and Hinton (2017) in their paper “Distilling a Neural Network into a Soft Decision
    Tree” [1]. In this article, I will break down the key mechanisms behind a Neural
    Decision Tree and explain some of the benefits of their approach as well as some
    factors one may need to consider when implementing this methodology in practice.
    While we will mainly be discussing classification trees, the approaches detailed
    can also be applied to regression trees with a relatively small number of tweaks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，目前的问题是我们想要神经网络的区分能力，但又希望具备决策树的可解释性。那么，为什么不把网络结构化成一棵树呢？这就是Fross和Hinton（2017）在他们的论文“将神经网络提炼成软决策树”[1]中采用的主要方法。在这篇文章中，我将深入探讨神经决策树背后的关键机制，并解释这种方法的一些优点以及在实际应用中可能需要考虑的一些因素。虽然我们主要讨论分类树，但详细的方法也可以应用于回归树，只需进行一些相对较小的调整。
- en: Methodology
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法论
- en: Soft Vs. Hard Decision Trees
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 软决策树与硬决策树
- en: Before diving into how to construct a neural network into a soft decision tree,
    let's first define what a soft decision tree is.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解如何将神经网络构建成软决策树之前，让我们首先定义什么是软决策树。
- en: When people think of decision trees (such as the ones implemented in sklearn),
    they are thinking about hard decision trees where every decision is deterministic.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当人们想到决策树（例如sklearn中实现的决策树）时，他们想到的是每个决策都是确定性的硬决策树。
- en: '![](../Images/2bebf2d9966b06b8088614c10d6b18ad.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2bebf2d9966b06b8088614c10d6b18ad.png)'
- en: Example of a Hard Decision Tree (Image by Author)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 硬决策树的示例（图片由作者提供）
- en: If a condition is met, we go towards the left branch, otherwise, we go right.
    Each leaf node has a class and a prediction is made by simply going through the
    tree and picking the class we end up in. The large we allow the tree to grow,
    the more paths we can take to achieve the final decision.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果满足某个条件，我们将走向左分支，否则我们走向右分支。每个叶子节点都有一个类别，通过简单地遍历树并选择我们最终到达的类别来进行预测。我们允许树生长得越大，可以采取的路径就越多，从而实现最终决策。
- en: Soft decision trees have many similarities, but work slightly differently
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 软决策树有许多相似之处，但工作方式略有不同
- en: '![](../Images/a046b34b34c672f21cb0774203878164.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a046b34b34c672f21cb0774203878164.png)'
- en: Example of a soft decision tree (Image by Author)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 软决策树的示例（图片由作者提供）
- en: While in hard decision trees, each branching is deterministic, soft decision
    trees define the probability of going into a certain branch if the condition is
    met. So while a hard decision tree outputs a single value, **soft decision trees
    instead output a probability distribution for all possible classes where the probability
    of a class is the product of the probabilities that we travel through to reach
    the leaves.** For example, the probability of approval in the tree above is equal
    to *P(b1|X)(1-P(b2|X) + (1-P(b2|X))(P(b3|X)).* Classification decisions are then
    just the class with the highest probability.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在硬决策树中，每个分支是确定性的，而软决策树定义了在满足条件的情况下进入某个分支的概率。因此，虽然硬决策树输出一个单一值，**软决策树则输出所有可能类别的概率分布，其中类别的概率是我们通过到达叶子的概率的乘积。**
    例如，上面树的批准概率等于 *P(b1|X)(1-P(b2|X)) + (1-P(b2|X))(P(b3|X))。* 分类决策就是选择具有最高概率的类别。
- en: This structure has many benefits. For one, having non-deterministic decisions
    provides users with an idea of the uncertainty present in a given classification.
    Additionally, technically hard trees are just special variants of soft trees where
    all branching probabilities are all equal to 1.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这种结构有许多优点。首先，非确定性决策使用户了解给定分类中的不确定性。此外，从技术上讲，硬树只是软树的特殊变体，其中所有分支概率都等于 1。
- en: 'One downside of these trees is the slight reduction in interpretability. From
    a stakeholder standpoint, its easier to understand “we approved a loan because
    the individual made $100k a year and had less than $400k in debt” compared to:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这些树的一个缺点是解释性略有下降。从利益相关者的角度来看，“我们批准了一个贷款，因为个人年收入为 $100k，债务少于 $400k”比起：
- en: If the income is $110k we have a .7 probability to go right and if the debt
    is under 400k we have a .8 probability to approve which results in in a .56 probability
    plus whatever happens in the left branch
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果收入是 $110k，我们有 0.7 的概率向右分支；如果债务低于 400k，我们有 0.8 的概率批准，这样结果就是 0.56 的概率加上左分支中发生的情况。
- en: That doesn’t mean that these are not interpretable (as one can still see exactly
    what the model is looking at) but just require a bit more help from the model
    developer.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不意味着这些树不可解释（因为仍然可以确切看到模型关注的内容），只是需要模型开发者提供更多的帮助。
- en: Oblique Decision Trees
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 倾斜决策树
- en: The second concept that is required before getting into Neural Decision Trees
    is the concept of “Oblique” Decision trees.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解神经决策树之前，第二个需要掌握的概念是“倾斜”决策树的概念。
- en: Traditional decision trees are considered “Orthogonal” trees in that their decisions
    are made orthogonal to a given axis. Simply put, only one variable is used in
    any given decision. Oblique trees on the other hand use multiple variables in
    their decision-making, usually in a linear combination.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的决策树被认为是“正交”树，因为它们的决策是相对于给定的轴进行的。简单来说，每次决策中只使用一个变量。另一方面，倾斜树在决策过程中使用多个变量，通常是线性组合的形式。
- en: '![](../Images/4956ffd1bb2e36c939f14afff5813c17.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4956ffd1bb2e36c939f14afff5813c17.png)'
- en: Example of an oblique decision boundary (Figure from Zhang et. al 2017 [4])
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 倾斜决策边界的示例（图源：Zhang et. al 2017 [4]）
- en: Some examples of the values in decision nodes could be “Income — Debt > 0”.
    This can result in stronger decision boundaries. One downside is that without
    proper regularization, these boundaries can get increasingly complex.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 决策节点中的一些示例值可能是“收入 — 债务 > 0”。这可以导致更强的决策边界。一个缺点是，如果没有适当的正则化，这些边界可能会变得越来越复杂。
- en: Putting it together
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将它们结合起来
- en: Now that we understand Soft and Oblique decision trees, we can put these together
    to understand the neural formulation.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了软决策树和倾斜决策树，我们可以将它们结合起来理解神经公式。
- en: 'The first component is the decision nodes. For each node what we need is some
    probability based on the input value. To achieve this, we can use the bread and
    butter of neural networks: Weights and activations. In each decision node, we
    will first take the linear combination of the input variables and then apply a
    sigmoid function on the sum, resulting in the branching probability.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个组成部分是决策节点。对于每个节点，我们需要基于输入值的一些概率。为了实现这一点，我们可以使用神经网络的基本工具：权重和激活。在每个决策节点中，我们首先对输入变量进行线性组合，然后对总和应用一个
    sigmoid 函数，得到分支概率。
- en: To prevent extremely soft decisions (and make the decision tree more like a
    hard decision tree), [a tempered sigmoid](https://ojs.aaai.org/index.php/AAAI/article/view/17123)
    (or a multiplication of the linear combination before applying the sigmoid) can
    be used instead.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止极软的决策（使决策树更像硬决策树），可以使用[温和的 sigmoid](https://ojs.aaai.org/index.php/AAAI/article/view/17123)（或在应用
    sigmoid 之前对线性组合进行乘法运算）。
- en: Each leaf node contains an N-dimensional tensor where N is the number of classes.
    This tensor represents the probability distribution of samples being in a class.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 每个叶节点包含一个 N 维张量，其中 N 是类别的数量。这个张量表示样本属于某一类别的概率分布。
- en: '![](../Images/d5127a16fca501035478e0ac6091036d.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d5127a16fca501035478e0ac6091036d.png)'
- en: Neural Network as a Decision Tree (Replication of a figure from Frosst & Hinton
    2017 [1])
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络作为决策树（图像复制自 Frosst & Hinton 2017 [1]）
- en: Like in a soft decision tree, the output of this neural tree is a probability
    distribution of the classes. The output distribution is equal to the sum of the
    distributions multiplied by the path probability to reach that distribution.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 与软决策树一样，这棵神经树的输出是类别的概率分布。输出分布等于分布的总和乘以到达该分布的路径概率。
- en: Training the Tree
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练树
- en: 'One benefit of neural trees is that they can be trained through continuous
    optimization algorithms like Gradient Descent instead of the greedy algorithms
    normal decision trees need to be constructed with. All we need to do is define
    the loss function:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 神经树的一个好处是可以通过像梯度下降这样的连续优化算法进行训练，而不是像普通决策树需要构建的贪婪算法。我们需要做的就是定义损失函数：
- en: '![](../Images/f97462919156b6ef811c76aa0aa73ecd.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f97462919156b6ef811c76aa0aa73ecd.png)'
- en: The loss function for the neural tree (Image from Frosst & Hinton 2017 [1])
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 神经树的损失函数（图像来自 Frosst & Hinton 2017 [1]）
- en: The loss function for this tree is similar to cross-entropy loss. In this equation,
    *P^l(***x***)* is the probability of reaching the leaf node *l* given the data
    point **x,** *T_k* is the probability of the target being class *k* (1 or 0),
    and Q_k^l is the element of the tensor (probability distribution) in the leaf
    node *l* that corresponds to class *k*.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这棵树的损失函数类似于交叉熵损失。在这个方程中，*P^l(***x***)* 是在给定数据点 **x** 的情况下到达叶节点 *l* 的概率，*T_k*
    是目标类别 *k* 的概率（1 或 0），而 Q_k^l 是叶节点 *l* 中与类别 *k* 对应的张量（概率分布）元素。
- en: One important note about this structure is that **the tree shape is fixed**.
    Unlike normal decision trees which use a greedy algorithm to split one node at
    a time and grow the tree, with this soft decision tree we first set the size of
    the tree and then use gradient descent to update all parameters simultaneously.
    One benefit of this approach is that it is much easier to constrain the size of
    the tree without losing much discriminative power.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这一结构的一个重要说明是**树形结构是固定的**。与使用贪婪算法逐个拆分节点并生长树的普通决策树不同，使用这种软决策树时，我们首先设置树的大小，然后使用梯度下降同时更新所有参数。这种方法的一个好处是更容易在不损失太多判别能力的情况下约束树的大小。
- en: One potential pitfall we may run into during the training process is that the
    model can heavily favor a single branch and not leverage the power of a tree.
    To avoid getting stuck on poor solutions, it is recommended to introduce a penalty
    to the loss function that encourages the tree to leverage both left and right
    sub-trees.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中可能遇到的一个潜在陷阱是模型可能过度偏向单个分支，而未能利用树的全部力量。为了避免陷入不良解决方案，建议在损失函数中引入惩罚，鼓励树同时利用左右子树。
- en: The penalty is the cross-entropy between the desired averaged distribution (50/50
    for both left and right trees) and the actual average distribution defined as
    alpha.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 惩罚是期望平均分布（左右树各 50/50）与实际平均分布（定义为 alpha）之间的交叉熵。
- en: '![](../Images/be85248d2e8af155179bd3a7b948d4e7.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/be85248d2e8af155179bd3a7b948d4e7.png)'
- en: Definition of alpha for a given node i (Image from Frosst & Hinton 2017 [1])
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 节点 i 的 alpha 定义（图像来自 Frosst & Hinton 2017 [1]）
- en: In this equation, *P^i*(**x**) is the path probability from the root node to
    node *i* for a given data point **x**. We then sum the penalty across all internal
    nodes.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，*P^i*(**x**) 是从根节点到节点 *i* 的路径概率。我们然后对所有内部节点的惩罚进行求和。
- en: '![](../Images/b85663f715637106eb679f8bdbb7adb1.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b85663f715637106eb679f8bdbb7adb1.png)'
- en: Definition of the penalty (Image from Frosst & Hinton 2017 [1])
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 惩罚的定义（图像来自 Frosst & Hinton 2017 [1]）
- en: 'In this equation, lambda is a hyperparameter that determines the strength of
    the penalty. However, this may cause some problems as we descend the tree, the
    data has less of a chance to fall into a 50/50 split, so therefore it is encouraged
    to instead use an adaptive lambda that changes depending on the depth of the tree.
    This would modify the penalty to:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，lambda 是一个超参数，决定了惩罚的强度。然而，这可能会导致一些问题，因为随着树的下降，数据分裂成 50/50 的机会减少，因此建议使用根据树的深度变化的自适应
    lambda。这将修改惩罚为：
- en: '![](../Images/268b1e9a9b438cded55af1710a5828d3.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/268b1e9a9b438cded55af1710a5828d3.png)'
- en: Modified Penalty function (Image by author, taken from text in Frosst & Hinton
    2017 [1])
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 修改后的惩罚函数（图片由作者提供，摘自 Frosst & Hinton 2017 [1]）
- en: As we go deeper into the tree, it is recommended to decay the lambda proportional
    to 2^-d.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们深入树中时，建议根据 2^-d 比例衰减 lambda。
- en: Visualizing Results
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果可视化
- en: While the reformulation of a neural network into a tree is interesting, the
    main reason one would pursue this avenue is to provide more model interpretability.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然将神经网络重新表述为树形结构很有趣，但追求这种方法的主要原因是提供更多的模型可解释性。
- en: 'Let’s first see the interpretation on a classic problem—digit classification
    in MNIST:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 首先看看经典问题的解释——MNIST 中的数字分类：
- en: '![](../Images/49812e36c1d1b088c4b26ca48b531778.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/49812e36c1d1b088c4b26ca48b531778.png)'
- en: Example on MNIST (Image from Frosst & Hinton 2017 [1])
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST 示例（图片来自 Frosst & Hinton 2017 [1]）
- en: In the figure above, the images at the inner nodes are the learned filters,
    and the images at the leaves are visualizations of the learned probability distribution
    over classes. For each leaf and node, the most likely classification is annotated
    in blue.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，内部节点的图像是学习到的过滤器，叶节点的图像是学习到的类别概率分布的可视化。对于每个叶节点和节点，最可能的分类用蓝色标注。
- en: Looking at this tree, we can see some interesting features. For example, if
    we look at the right-most internal node the potential classifications are between
    3 and 8\. In fact, we can actually see the outline of the 3 in the decision node
    visualization. The white areas seem to indicate that the model looks for lines
    that close out the inner loops of a 3 to make it an 8\. We can also see that the
    model looks for the shape of a zero in the 3rd node from the left in the bottom
    set of inner nodes.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 从这棵树来看，我们可以看到一些有趣的特征。例如，如果我们查看最右边的内部节点，潜在的分类是 3 和 8。实际上，我们可以在决策节点可视化中看到 3 的轮廓。白色区域似乎表明模型寻找能够闭合
    3 的内部循环的线条，从而将其转换为 8。我们还可以看到模型在左侧倒数第三个节点中寻找 0 的形状。
- en: Another interesting example is predicting wins in Connect4
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的例子是预测 Connect4 游戏中的胜利
- en: '![](../Images/ea094be911091bf222626ecce49da15c.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea094be911091bf222626ecce49da15c.png)'
- en: Visualization of the first 2 Layers of a neural decision tree predicting who
    would win a Connect4 game (Image from Frosst & Hinton 2017)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化神经决策树前 2 层预测 Connect4 游戏胜者（图片来自 Frosst & Hinton 2017）
- en: 'The learned filters in this example show that the game can be split into two
    distinct types of games: ones where the players have primarily focused on the
    edge of the board versus the ones where players have placed pieces in the center
    of the board.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子中的学习到的过滤器表明，游戏可以分为两种不同类型：一种是玩家主要集中在棋盘边缘的游戏，另一种是玩家在棋盘中心放置棋子的游戏。
- en: Conclusion
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Constructing Neural Networks as a Soft-Decision Tree allows us to leverage the
    power of neural networks while still preserving some interpretability. As shown
    by the results on the MNIST dataset, the learned filters can provide both local
    and global explainability, a trait that is welcome and helpful for higher stake
    tasks as well. Additionally, the training method (optimizing and updating the
    entire tree at a time) allows us to get more discriminative power while keeping
    the size of the tree fixed, something we are not able to achieve with normal decision
    trees.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 将神经网络构建为软决策树使我们能够利用神经网络的强大能力，同时仍保留一些可解释性。正如 MNIST 数据集上的结果所示，学习到的过滤器可以提供局部和全局的解释能力，这对于高风险任务也是一种受欢迎且有帮助的特性。此外，训练方法（一次优化和更新整个树）使我们能够在保持树的大小固定的情况下获得更多的区分能力，这是我们在正常决策树中无法实现的。
- en: Despite this, Neural Trees are not perfect. The soft nature of the tree does
    mean that the data scientist using these needs to “preprocess” the tree before
    presenting it to non-technical stakeholders, while normal decision trees can be
    shown as is (as they are pretty self-explanatory). Additionally, while the oblique
    nature of the tree helps with accuracy, having too many variables in a given node
    can again make it harder to explain. This means that regularization is not only
    recommended but to a certain extent it is necessary. Also, no matter how interpretable
    a model is, there is still [the need for interpretable features that stakeholders
    can understand](/defining-interpretable-features-ebd7ed94897).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，神经树仍然不完美。树的软性特征意味着使用这些树的数据科学家需要在向非技术利益相关者展示之前“预处理”树，而普通决策树可以直接展示（因为它们相对自解释）。此外，虽然树的斜向特性有助于准确性，但在给定节点中变量过多会使解释变得更加困难。这意味着正则化不仅是推荐的，而且在一定程度上是必要的。此外，无论模型多么可解释，仍然存在[对利益相关者可理解的解释性特征的需求](/defining-interpretable-features-ebd7ed94897)。
- en: All of these drawbacks however do not detract from the potential of these models
    to grow the interpretability-performance frontier. I highly encourage everyone
    to try these models in their next data science task. I also recommend that everyone
    [read the original paper](https://arxiv.org/abs/1711.09784#).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些缺点并未削弱这些模型在推动解释性与性能前沿方面的潜力。我强烈建议大家在下一个数据科学任务中尝试这些模型。我也推荐大家[阅读原始论文](https://arxiv.org/abs/1711.09784#)。
- en: Resources and References
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源与参考文献
- en: '[Implementation of the Soft Decision Tree in PyTorch](https://github.com/kimhc6028/soft-decision-tree)'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[在PyTorch中实现软决策树](https://github.com/kimhc6028/soft-decision-tree)'
- en: For more information on XAI and time-series forecasting, follow
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 想了解更多关于XAI和时间序列预测的信息，请关注
- en: References
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] N. Frosst, G. Hinton. [Distilling a Neural Network Into a Soft Decision
    Tree](https://arxiv.org/abs/1711.09784#) (2017). 2017 Artificial Intelligence
    in Action Conference'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] N. Frosst, G. Hinton. [将神经网络蒸馏为软决策树](https://arxiv.org/abs/1711.09784#)
    (2017). 2017人工智能行动会议'
- en: '[2] A. Shrikumar, P. Greenside, A. Jundjae. [Learning Important Features Through
    Propagating Activation Differences](https://arxiv.org/abs/1704.02685) (2017).
    International Conference on Machine Learning PMLR 2017.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] A. Shrikumar, P. Greenside, A. Jundjae. [通过传播激活差异学习重要特征](https://arxiv.org/abs/1704.02685)
    (2017). 国际机器学习会议 PMLR 2017。'
- en: '[3] S.Bach, A. Binder, G. Montavon, F. Klauschen, K-R. Muller, W. Samek. [On
    Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance
    Propagation](https://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0130140)
    (2015). PloS one, 10(7), e0130140'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] S.Bach, A. Binder, G. Montavon, F. Klauschen, K-R. Muller, W. Samek. [基于层级相关传播的非线性分类器像素级解释](https://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0130140)
    (2015). PloS one, 10(7), e0130140'
- en: '[4] L. Zhang, J. Varadarajan, P. N. Suganthan, N. Ahuja, P. Moulin. [Robust
    Visual Tracking Using Oblique Random Forests](https://ieeexplore.ieee.org/abstract/document/8100100)
    (2017). Conference on Computer Vision and Pattern Recognition 2017.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] L. Zhang, J. Varadarajan, P. N. Suganthan, N. Ahuja, P. Moulin. [使用斜向随机森林的鲁棒视觉跟踪](https://ieeexplore.ieee.org/abstract/document/8100100)
    (2017). 2017计算机视觉与模式识别会议。'
