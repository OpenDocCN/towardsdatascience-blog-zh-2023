- en: Geometric Deep Learning on Groups
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 群体上的几何深度学习
- en: 原文：[https://towardsdatascience.com/geometric-deep-learning-on-groups-cec82eb9366?source=collection_archive---------17-----------------------#2023-03-06](https://towardsdatascience.com/geometric-deep-learning-on-groups-cec82eb9366?source=collection_archive---------17-----------------------#2023-03-06)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/geometric-deep-learning-on-groups-cec82eb9366?source=collection_archive---------17-----------------------#2023-03-06](https://towardsdatascience.com/geometric-deep-learning-on-groups-cec82eb9366?source=collection_archive---------17-----------------------#2023-03-06)
- en: Continuous vs discrete approaches on the sphere
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Continuous vs discrete approaches on the sphere
- en: '[](https://jasonmcewen.medium.com/?source=post_page-----cec82eb9366--------------------------------)[![Jason
    McEwen](../Images/794e7e6546ed049860dab5e294535880.png)](https://jasonmcewen.medium.com/?source=post_page-----cec82eb9366--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cec82eb9366--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cec82eb9366--------------------------------)
    [Jason McEwen](https://jasonmcewen.medium.com/?source=post_page-----cec82eb9366--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://jasonmcewen.medium.com/?source=post_page-----cec82eb9366--------------------------------)[![Jason
    McEwen](../Images/794e7e6546ed049860dab5e294535880.png)](https://jasonmcewen.medium.com/?source=post_page-----cec82eb9366--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cec82eb9366--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cec82eb9366--------------------------------)
    [Jason McEwen](https://jasonmcewen.medium.com/?source=post_page-----cec82eb9366--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fea87e920245&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgeometric-deep-learning-on-groups-cec82eb9366&user=Jason+McEwen&userId=ea87e920245&source=post_page-ea87e920245----cec82eb9366---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cec82eb9366--------------------------------)
    ·6 min read·Mar 6, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcec82eb9366&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgeometric-deep-learning-on-groups-cec82eb9366&user=Jason+McEwen&userId=ea87e920245&source=-----cec82eb9366---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fea87e920245&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgeometric-deep-learning-on-groups-cec82eb9366&user=Jason+McEwen&userId=ea87e920245&source=post_page-ea87e920245----cec82eb9366---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cec82eb9366--------------------------------)
    ·6 min read·Mar 6, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcec82eb9366&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgeometric-deep-learning-on-groups-cec82eb9366&user=Jason+McEwen&userId=ea87e920245&source=-----cec82eb9366---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcec82eb9366&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgeometric-deep-learning-on-groups-cec82eb9366&source=-----cec82eb9366---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcec82eb9366&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgeometric-deep-learning-on-groups-cec82eb9366&source=-----cec82eb9366---------------------bookmark_footer-----------)'
- en: '*Ideally geometric deep learning techniques on groups would encode equivariance
    to group transformations, to provide well-behaved representation spaces and excellent
    performance, while also being computationally efficient. However, no single approach
    provides both of these desirable properties. Continuous approaches offer excellent
    equivariance but with a very large computational cost. Discrete approaches are
    typically relatively computationally efficient but sacrifice equivariance. We
    point towards future techniques that achieve the best of both worlds.*'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*理想情况下，群体上的几何深度学习技术应该能够编码对群体变换的等变性，以提供良好的表示空间和出色的性能，同时也要具备计算效率。然而，没有一种方法能够同时提供这两种理想的属性。连续的方法提供了优秀的等变性，但计算成本非常高。离散的方法通常计算效率相对较高，但牺牲了等变性。我们指出了未来能够兼具这两者优点的技术。*'
- en: '![](../Images/b7a2db73eea2c07103c581afc144a2ff.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b7a2db73eea2c07103c581afc144a2ff.png)'
- en: Photo by [Serg Antonov](https://unsplash.com/@antonov?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Serg Antonov](https://unsplash.com/@antonov?utm_source=medium&utm_medium=referral)
    提供，发布在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Deep learning on groups is a rapidly growing area of geometric deep learning
    (see our recent TDS article on [*A Brief Introduction to Geometric Deep Learning*](/a-brief-introduction-to-geometric-deep-learning-dae114923ddb)).
    [Groups](https://en.wikipedia.org/wiki/Group_theory) include homogenous spaces
    with global symmetries, with the archetypical example being the sphere.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 群体上的深度学习是几何深度学习一个快速增长的领域（参见我们最近的 TDS 文章 [*几何深度学习简要介绍*](/a-brief-introduction-to-geometric-deep-learning-dae114923ddb)）。[群体](https://en.wikipedia.org/wiki/Group_theory)包括具有全局对称性的同质空间，其中最典型的例子是球面。
- en: Practical applications on geometric deep learning on groups are prevalent, particularly
    for the sphere. For example, spherical data arise in myrad applications, not only
    when data is acquired directly on the sphere (such as over the Earth or by 360°
    cameras that capture panoramic photos and videos), but also when considering spherical
    symmetries (such as in molecular chemistry or magnetic resonance imaging).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 几何深度学习在群体上的实际应用很普遍，尤其是在球面上。例如，球形数据在许多应用中出现，不仅在数据直接获取自球面时（例如地球上的数据或通过 360° 相机捕捉全景照片和视频时），还包括考虑球面对称性时（例如分子化学或磁共振成像）。
- en: We need deep learning techniques on groups that are both highly effective and
    scalable to huge datasets of high-resolution data. In general this problem remains
    unsolved.
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们需要在群体上既高效又可扩展到大规模高分辨率数据集的深度学习技术。一般来说，这个问题仍然没有解决。
- en: '![](../Images/3875a12177434d39615895de8cbdece3.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3875a12177434d39615895de8cbdece3.png)'
- en: An example of spherical data. [Photo by [NASA](https://unsplash.com/@nasa?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)]
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一个球形数据的例子。[照片由 [NASA](https://unsplash.com/@nasa?utm_source=medium&utm_medium=referral)
    提供，来自 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)]
- en: Goals
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目标
- en: One of the reasons deep learning techniques have been so effective is due to
    the inductive biases encoded in modern architectures.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习技术之所以如此有效，部分原因是现代架构中编码的归纳偏差。
- en: One particularly powerful inductive bias is to encode symmetries that the data
    are known to satisfy (as elaborated in our TDS article [*What Einstein Can Teach
    Us About Machine Learning*](/what-einstein-can-teach-us-about-machine-learning-1661e26bef2c)*).*
    Convolutional neural networks (CNNs), for example, encode translational symmetry
    or, more precisely, translational equivariance, as illustrated in the diagram
    below.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一个特别强大的归纳偏差是编码数据已知满足的对称性（如我们 TDS 文章 [*爱因斯坦可以教我们什么关于机器学习*](/what-einstein-can-teach-us-about-machine-learning-1661e26bef2c)*）所阐述的）。例如，卷积神经网络（CNNs）编码了平移对称性，或更确切地说，平移等变性，如下图所示。
- en: '![](../Images/dfaac1eb43106dd1ca19d77ccbeeb762.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dfaac1eb43106dd1ca19d77ccbeeb762.png)'
- en: llustration of translational equivariance. Given an image (top left), applying
    a convolutional kernel (𝒜) to obtain a feature map (top right) and then translating
    (𝒯) the feature map (bottom right) is equivalent to first translating the image
    (bottom left) and then applying the convolution kernel (bottom right). [Original
    figure created by authors.]
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 平移等变性的说明。给定一张图像（左上），应用卷积核（𝒜）以获得特征图（右上），然后平移（𝒯）特征图（右下），等同于首先平移图像（左下），然后应用卷积核（右下）。[原始图形由作者创建。]
- en: Encoding equivariance in deep learning architectures results in well-behaved
    feature spaces where learning can be performed very effectively.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习架构中编码等变性会产生行为良好的特征空间，使得学习可以非常有效地进行。
- en: For geometric deep learning on groups we would therefore like to encode equivariance
    to various group transformations, which typically results in very good performance.
    However, in the general group setting this becomes highly computational demanding
    — prohibitively so in many cases.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于群体上的几何深度学习，我们希望编码对各种群体变换的等变性，这通常会带来非常好的性能。然而，在一般的群体设置中，这变得非常计算密集——在许多情况下是不可行的。
- en: How to encode equivariance in deep learning architectures on groups in a computationally
    scalable manner is an active area of research.
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如何在深度学习架构中以计算上可扩展的方式编码等变性是一个活跃的研究领域。
- en: Group convolution
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 群体卷积
- en: The notion of convolution, which is responsible for the huge success of CNN
    architectures for planar images, naturally encodes equivariance and can be generalised
    to the group setting.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积的概念，负责 CNN 架构在平面图像上的巨大成功，自然地编码了等变性，并且可以推广到群体设置。
- en: The group convolution of a signal (i.e. data, feature map) *f* defined over
    the group, with a filter *𝝭*, is given by
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 群上信号（即数据、特征图）*f* 与滤波器 *𝝭* 的群卷积表示为
- en: '![](../Images/709f954b8e725411af8150a6d441f80d.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/709f954b8e725411af8150a6d441f80d.png)'
- en: where *g* is an element of the group *G* and d*µ(u)* is the (Haar) measure of
    integration. The above expression is entirely analogous to convolution in the
    more common planar setting. We apply a transformation to the filter (a translation
    for planar CNNs), take the product with the signal of interest, and then sum,
    i.e. integrate.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *g* 是群 *G* 的一个元素，而 d*µ(u)* 是积分的（Haar）测度。上述表达式与更常见的平面设置中的卷积完全类似。我们对滤波器进行变换（对于平面
    CNN 来说是平移），与感兴趣的信号相乘，然后求和，即积分。
- en: On the sphere we consider transformations given by 3D rotations and so the convolution
    of a signal on the sphere reads
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在球面上，我们考虑由 3D 旋转给出的变换，因此球面上的信号卷积表示为
- en: '![](../Images/34b7b3b493003ea983d69ea818088ff6.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/34b7b3b493003ea983d69ea818088ff6.png)'
- en: where *R* denotes a rotation and *ω* spherical coordinates.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *R* 表示旋转，*ω* 为球面坐标。
- en: Once a convolution on the group is defined, we can then construct a CNN on the
    group in a manner analogous to standard planar CNNs. That is, by composing convolutions
    and pointwise non-linear activations (also with pooling and normalisation layers,
    appropriately constructed on the group).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了群上的卷积，我们就可以以类似于标准平面 CNN 的方式在群上构建 CNN。即，通过组合卷积和逐点非线性激活（还包括适当地构建在群上的池化和归一化层）。
- en: 'The question then remains: how do we compute the group convolution in practice?'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 问题仍然是：我们如何在实践中计算群卷积？
- en: On one hand, we’d like the implementation to accurately capture the equivariance
    properties on the convolution. While on the other hand, we’d like the implementation
    to be highly computationally efficient. As we will see, existing approaches typically
    capture one of these requirements but not both simultaneously.
  id: totrans-34
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一方面，我们希望实现能够准确捕捉卷积上的等变性特性。另一方面，我们希望实现具有高计算效率。如我们将看到的，现有方法通常只能捕捉这些要求中的一个，而不能同时满足两个要求。
- en: Discrete spherical CNN approaches
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 离散球面 CNN 方法
- en: Existing approaches can be broadly categorised in discrete and continuous approaches.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的方法可以大致分为离散和连续方法。
- en: Discrete approaches work with a discrete version of the data, typically either
    pixels or a graph representation, which can often be highly computationally efficient.
    However, in general regular discretizations do not exist.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 离散方法处理数据的离散版本，通常是像素或图形表示，这通常具有很高的计算效率。然而，通常不存在规则的离散化。
- en: Taking the sphere as an example, it is well known that a regular discretization
    of the sphere does not exist. Consequently, there is no way to discretise the
    sphere in a manner that is invariant to rotations, as illustrated in the diagram
    below.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以球面为例，众所周知，球面的规则离散化并不存在。因此，没有方法可以以不变于旋转的方式对球面进行离散化，如下图所示。
- en: '![](../Images/c0fcfa6c76dcce8dd5b460409d7b84a4.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c0fcfa6c76dcce8dd5b460409d7b84a4.png)'
- en: Rotating a set of pixels on the sphere results in a set on pixels that cannot
    be overlayed with the existing set. This is true for all samplings of the sphere.
    [Original figure created by authors.]
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在球面上旋转一组像素会得到一组不能与现有集合重叠的像素。这在球面的所有采样中都是正确的。[原始图形由作者创建。]
- en: Capturing strict equivariance with operations defined directly on the discretized
    space is simply not possible.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在离散空间上直接定义的操作无法严格捕捉等变性。
- en: Discrete approaches therefore over favourable computational performance but
    at the cost of equivariance.
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 因此，离散方法在计算性能上具有优势，但代价是等变性。
- en: Continuous spherical CNN approaches
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连续球面 CNN 方法
- en: As an alternative to the discrete approaches discussed above, a continuous representation
    of the underlying signal can also be considered.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 作为上述离散方法的替代方案，也可以考虑信号的连续表示。
- en: Functions on the sphere can be represented by an expansion in terms of spherical
    harmonics (illustrated below). For a bandlimited signal, it is posible to capture
    all of the signal’s information content in a finite set of samples, from which
    spherical harmonic coefficients can be computed exactly [1]. This is the analog
    of the well-known [Nyquist-Shannon sampling theorem](https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem)
    extended to the sphere.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 球面上的函数可以通过球面谐波的展开表示（如下图所示）。对于带限信号，可以在有限的样本集合中捕捉到信号的所有信息内容，从中可以准确地计算球面谐波系数[1]。这类似于著名的[奈奎斯特-香农采样定理](https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem)扩展到球面。
- en: '![](../Images/d363eb6335ceb277a928b5ee435d5ad8.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d363eb6335ceb277a928b5ee435d5ad8.png)'
- en: Spherical harmoinc functions. [Image sourced from [Wikimedia Commons](https://en.wikipedia.org/wiki/Spherical_harmonics#/media/File:Spherical_Harmonics.png).]
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 球面谐波函数。[图像来源于 [Wikimedia Commons](https://en.wikipedia.org/wiki/Spherical_harmonics#/media/File:Spherical_Harmonics.png)。]
- en: Since the sphere is a compact manifold, its harmonic space is discrete. By working
    with a finite spherical harmonic space representation it is therefore possible
    to access the underlying continuous signal.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 由于球面是一个紧致流形，其谐波空间是离散的。因此，通过使用有限的球面谐波空间表示，可以访问底层的连续信号。
- en: Various spherical CNN architecutres have been constructed where convolutions
    are computed through their harmonic representation [2–6]. By accessing the underlying
    continuous signal, these approaches achieve execellent equivariance properties.
    However, they involve repeatedly performing spherical harmonic transforms, which
    is computationally costly.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 各种球面 CNN 架构已经被构建，其中卷积通过其谐波表示来计算[2–6]。通过访问底层的连续信号，这些方法实现了卓越的等变性。然而，它们涉及反复执行球面谐波变换，这在计算上是昂贵的。
- en: Continuous appraoches capture rotational equivariance acurately but are computationally
    demanding.
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 连续方法准确捕捉旋转等变性，但计算要求高。
- en: 'Dichotomy: discrete vs continuous approaches'
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 二分法：离散 vs 连续方法
- en: As we have seen above, a dichotomy exists between discrete and continous approaches,
    as illustrated in the diagram below. Ideally we’d like techniques that are both
    equivariant and computationally scalable.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，离散和连续方法之间存在一种二分法，如下图所示。理想情况下，我们希望技术既具有等变性又具有计算可扩展性。
- en: However, continuous approaches offer equivariance but with a large computational
    cost. Discrete approaches, on the other hand, are typically relatively computationally
    efficient but sacrifice equivariance.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，连续方法提供等变性，但计算成本很高。另一方面，离散方法通常计算效率较高，但牺牲了等变性。
- en: '![](../Images/191b1b697f3daf4718c598e3bb5585cc.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/191b1b697f3daf4718c598e3bb5585cc.png)'
- en: Dichotomy between continuous and discrete geometric deep learning techniques
    on groups. [Original figure created by authors.]
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 连续和离散几何深度学习技术在群体上的二分法。[原始图由作者创建。]
- en: Breaking the dichotomy
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 破除二分法
- en: We desire geometric deep learning techniques on groups that provide equivariance
    (which typically translates to well-behaved representation spaces and excellent
    performance) and are also computationally scalable.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们期望在群体上进行几何深度学习的技术能够提供等变性（这通常转化为良好的表示空间和卓越的性能），同时也具有计算可扩展性。
- en: In our next post we will describe a new hyrbid discrete-continuous (DISCO) approach,
    recently accepted for ICLR, that achieves precisely these goals [7]. By keeping
    some components of the representation continuous we achieve excellent equivariance
    properties, while other components are discretized to provide highly efficient
    scalable computation.
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在我们的下一篇文章中，我们将描述一种新的混合离散-连续（DISCO）方法，最近被 ICLR 接受，这种方法正好实现了这些目标[7]。通过保持表示的一些组件为连续的，我们实现了卓越的等变性，而其他组件则被离散化以提供高效的可扩展计算。
- en: References
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] McEwen & Wiaux, *A novel sampling theorem on the sphere*, IEEE TSP (2012),
    [arXiv:1110.6298](https://arxiv.org/abs/1110.6298)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] McEwen & Wiaux, *A novel sampling theorem on the sphere*, IEEE TSP (2012),
    [arXiv:1110.6298](https://arxiv.org/abs/1110.6298)'
- en: '[2] Cohen, Geiger, Koehler, Welling, *Spherical CNNs*, ICLR (2018), [arxiv:1801.10130](https://arxiv.org/abs/1801.10130).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Cohen, Geiger, Koehler, Welling, *Spherical CNNs*, ICLR (2018), [arxiv:1801.10130](https://arxiv.org/abs/1801.10130)。'
- en: '[3] Esteves, Allen-Blanchette, Makadia, Daniilidis, *Learning SO(3) Equivariant
    Representations with Spherical CNNs*, ECCV (2018), [arXiv:1711.06721](https://arxiv.org/abs/1711.06721).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Esteves, Allen-Blanchette, Makadia, Daniilidis, *学习SO(3)等变表示的球面卷积神经网络*，ECCV（2018），[arXiv:1711.06721](https://arxiv.org/abs/1711.06721)。'
- en: '[4] Kondor, Lin, Trivedi, *Clebsch-Gordan Nets: a Fully Fourier Space Spherical
    Convolutional Neural Network*, NeurIPS (2018), [arXiv:1806.09231](https://arxiv.org/abs/1806.09231)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Kondor, Lin, Trivedi, *Clebsch-Gordan 网络：一种完全傅里叶空间的球面卷积神经网络*，NeurIPS（2018），[arXiv:1806.09231](https://arxiv.org/abs/1806.09231)'
- en: '[5] Cobb, Wallis, Mavor-Parker, Marignier, Price, d’Avezac, McEwen, *Efficient
    Generalised Spherical CNNs*, ICLR (2021), [arXiv:2010.11661](https://arxiv.org/abs/2010.11661#)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Cobb, Wallis, Mavor-Parker, Marignier, Price, d’Avezac, McEwen, *高效的广义球面卷积神经网络*，ICLR（2021），[arXiv:2010.11661](https://arxiv.org/abs/2010.11661#)'
- en: '[6] McEwen, Wallis, Mavor-Parker*, Scattering Networks on the Sphere for Scalable
    and Rotationally Equivariant Spherical CNNs*, ICLR (2022), [arXiv:2102.02828](https://arxiv.org/abs/2102.02828)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] McEwen, Wallis, Mavor-Parker*，*球面上的散射网络：用于可扩展和旋转等变的球面卷积神经网络*，ICLR（2022），[arXiv:2102.02828](https://arxiv.org/abs/2102.02828)'
- en: '[7] Ocampo, Price, McEwen, *Scalable and equivariant spherical CNNs by discrete-continuous
    (DISCO) convolutions*, ICLR (2023), [arXiv:2209.13603](https://arxiv.org/abs/2209.13603)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Ocampo, Price, McEwen, *通过离散-连续（DISCO）卷积实现可扩展和等变的球面卷积神经网络*，ICLR（2023），[arXiv:2209.13603](https://arxiv.org/abs/2209.13603)'
