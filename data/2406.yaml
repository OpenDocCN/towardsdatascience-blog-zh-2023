- en: Generative AI Ethics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成式AI伦理
- en: 原文：[https://towardsdatascience.com/generative-ai-ethics-b2db92ecb909?source=collection_archive---------15-----------------------#2023-07-25](https://towardsdatascience.com/generative-ai-ethics-b2db92ecb909?source=collection_archive---------15-----------------------#2023-07-25)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/generative-ai-ethics-b2db92ecb909?source=collection_archive---------15-----------------------#2023-07-25](https://towardsdatascience.com/generative-ai-ethics-b2db92ecb909?source=collection_archive---------15-----------------------#2023-07-25)
- en: Key Considerations in the Age of Autonomous Content
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自主内容时代的关键考虑因素
- en: '[](https://medium.com/@davidsweenor?source=post_page-----b2db92ecb909--------------------------------)[![David
    Sweenor](../Images/7dbb5c549ab67bc78f906fb707969ff6.png)](https://medium.com/@davidsweenor?source=post_page-----b2db92ecb909--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b2db92ecb909--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b2db92ecb909--------------------------------)
    [David Sweenor](https://medium.com/@davidsweenor?source=post_page-----b2db92ecb909--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@davidsweenor?source=post_page-----b2db92ecb909--------------------------------)[![David
    Sweenor](../Images/7dbb5c549ab67bc78f906fb707969ff6.png)](https://medium.com/@davidsweenor?source=post_page-----b2db92ecb909--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b2db92ecb909--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b2db92ecb909--------------------------------)
    [David Sweenor](https://medium.com/@davidsweenor?source=post_page-----b2db92ecb909--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fec7aed1f3ef1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerative-ai-ethics-b2db92ecb909&user=David+Sweenor&userId=ec7aed1f3ef1&source=post_page-ec7aed1f3ef1----b2db92ecb909---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b2db92ecb909--------------------------------)
    ·11 min read·Jul 25, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fb2db92ecb909&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerative-ai-ethics-b2db92ecb909&user=David+Sweenor&userId=ec7aed1f3ef1&source=-----b2db92ecb909---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fec7aed1f3ef1&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerative-ai-ethics-b2db92ecb909&user=David+Sweenor&userId=ec7aed1f3ef1&source=post_page-ec7aed1f3ef1----b2db92ecb909---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b2db92ecb909--------------------------------)
    ·11分钟阅读·2023年7月25日'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb2db92ecb909&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerative-ai-ethics-b2db92ecb909&source=-----b2db92ecb909---------------------bookmark_footer-----------)![](../Images/50051e39312ac433cb92b7aeb9b7fd60.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fb2db92ecb909&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fgenerative-ai-ethics-b2db92ecb909&source=-----b2db92ecb909---------------------bookmark_footer-----------)![](../Images/50051e39312ac433cb92b7aeb9b7fd60.png)'
- en: Photo Courtesy of Author — David E. Sweenor
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的照片 — 大卫·E·斯维诺
- en: With all the hubbub surrounding generative artificial intelligence (AI), there
    are an increasing number of unanswered questions about how to implement this transformative
    technology responsibly. This blog will review the European Union (EU) AI ethics
    guidelines and discuss key considerations for implementing an AI ethics framework
    when large language models (LLMs) are used.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着围绕生成式人工智能（AI）的喧嚣，关于如何负责任地实施这一变革性技术的问题越来越多。本文将回顾欧盟（EU）的AI伦理指南，并讨论在使用大型语言模型（LLMs）时实施AI伦理框架的关键考虑因素。
- en: Ethics Guidelines for Trustworthy AI
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可信AI伦理指南
- en: 'On 8th April 2019, the European Union put into practice [a framework for the
    ethical and responsible use of artificial intelligence (AI)](https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai).
    The report defines three guiding principles for building trustworthy AI:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 2019年4月8日，欧盟实施了[人工智能（AI）的伦理和负责任使用框架](https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai)。报告定义了构建可信AI的三项指导原则：
- en: '**Lawful**: AI should adhere to the rule of law and location regulations.'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**合法性**：AI应遵守法律法规和当地规定。'
- en: '**Ethical**: The AI system should be ethical and abide by ethical principles
    and values.'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**伦理**：AI系统应具备伦理性，遵循伦理原则和价值观。'
- en: '**Robust**: Because AI can do significant harm to large populations in a short
    amount of time, it needs to be technically and socially robust.'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**稳健**：由于AI可能在短时间内对大规模人群造成显著危害，它需要在技术和社会上都具备稳健性。'
- en: For multinational corporations, this raises an interesting question of how they
    should apply this framework across geopolitical boundaries since what is considered
    lawful and ethical in one region of the world may not be in another. Many companies
    take the most stringent regulations and apply that unilaterally across all geographies.
    However, a “one-size-fits-most” approach may not be appropriate or acceptable.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对于跨国公司而言，这提出了一个有趣的问题：如何在地缘政治边界之间应用这一框架，因为在世界某个地区被视为合法和伦理的做法，可能在另一个地区并不适用。许多公司采取最严格的规定，并在所有地区单方面适用。然而，“一刀切”的方法可能并不适用或可接受。
- en: The EU’s framework can be seen below in Figure 1.1.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 欧盟的框架见下图 1.1。
- en: '**Figure 1.1: AI Ethics Framework from the European Union**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 1.1：欧洲联盟的AI伦理框架**'
- en: '![](../Images/f663277b62d1ff5b11f15253b8480f68.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f663277b62d1ff5b11f15253b8480f68.png)'
- en: Digram Provided by Author — David E. Sweenor, Founder of TinyTechGuides
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图示——David E. Sweenor，TinyTechGuides创始人
- en: 'Based on the three foundational principles, four ethical principles and seven
    key requirements result. The ethical principles include:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这三项基础原则，得出四项伦理原则和七项关键要求。这些伦理原则包括：
- en: '**Respect for Human Autonomy**: This principle emphasizes that humans should
    maintain control and freedom in their interactions with AI. “AI systems should
    not unjustifiably subordinate, coerce, deceive, manipulate, condition or herd
    humans.”[1] Fundamentally, AI should support human participation in democratic
    processes. We’ve seen [some countries implement “social scoring” on its citizens](https://www.technologyreview.com/2022/11/22/1063605/china-announced-a-new-social-credit-law-what-does-it-mean/),
    which should be cause for concern.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**尊重人类自主权**：这一原则强调，人类应在与AI的互动中保持控制和自由。“AI系统不应无理地使人类从属、胁迫、欺骗、操控、条件化或驱使。”[1]
    从根本上讲，AI应支持人类参与民主过程。我们已经看到[一些国家对公民实施“社会评分”](https://www.technologyreview.com/2022/11/22/1063605/china-announced-a-new-social-credit-law-what-does-it-mean/)，这应引起关注。'
- en: '**Prevention of Harm**: AI systems should not cause physical, mental or emotional
    harm. Given the pervasiveness and rapid impact of AI, it’s important that AI outputs
    are closely monitored to prevent the inadvertent manipulation of citizens, employees,
    businesses, consumers and governments “due to asymetries of power or information.”[2]
    We’ve seen autonomous vehicle makers wrestle with this principle in what’s known
    as the [AI Trolley Problem](https://www.turing.ac.uk/blog/ais-trolley-problem-problem).
    Of course, this is not limited to robotic systems; people are relying on ChatGPT
    for medical advice and given its propensity to make things up, we need to be careful.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**防止伤害**：AI系统不应造成身体、心理或情感上的伤害。鉴于AI的普及性和迅速影响，重要的是要密切监控AI的输出，以防止“由于权力或信息的不对称”对公民、员工、企业、消费者和政府的无意操控。[2]
    我们已经看到自动驾驶汽车制造商在所谓的[AI电车难题](https://www.turing.ac.uk/blog/ais-trolley-problem-problem)中与这一原则斗争。当然，这不仅限于机器人系统；人们依赖ChatGPT获取医疗建议，鉴于其生成虚假信息的倾向，我们需要小心。'
- en: '**Fairness**: AI systems should be unbiased and non-discriminatory, aiming
    for “an equal distribution of benefits and costs.”[3] Fairness implies that human
    choices should not be undermined and that “AI practitioners should balance competing
    interests and objectives, respecting the principle of proportionality between
    means and ends.”[4] On the surface, this seems straight forward, but did you know
    there are over twenty mathematical definitions of fairness?[5]'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**公平性**：AI系统应当无偏见且不歧视，旨在“实现利益和成本的平等分配”。[3] 公平性意味着不应削弱人类选择，并且“AI从业者应平衡竞争利益和目标，尊重手段与目的之间的比例原则”。[4]
    从表面上看，这似乎很简单，但你知道吗，公平性有超过二十种数学定义？[5]'
- en: '**Explicability**: AI systems need to be transparent, auditable, reproducible
    and interpretable. If AI is used to decide something that impacts you, you have
    a have the right to an explanation as to how that decision was made by the algorithm.
    For example, if you are denied credit, the operator of that AI system should be
    able to provide you with all of the factors that contributed to the decision.
    This can be problematic when “black-box” models are used — like the neural networks
    and general adversarial networks (GANs) that are the foundation for many LLMs.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**可解释性**：AI系统需要透明、可审计、可复现和可解释。如果AI用于决定影响你的事情，你有权了解算法如何做出该决定。例如，如果你被拒绝了信用，AI系统的操作员应该能够提供所有影响该决定的因素。当使用“黑箱”模型时，例如神经网络和生成对抗网络（GANs），这可能会成为问题。'
- en: 'This leads us to the seven requirements:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了七项要求：
- en: '**Human Agency and Oversight**: Essentially, this requirement states that AI
    systems should respect human rights and they should not operate entirely autonomously.
    AI should augment, not replace, human decisions. There should be a process for
    challenging AI decisions, and a human should be able to override AI decisions
    when necessary. This sounds nice, but when hundreds and thousands of decisions
    are made automatically, how can you effectively track all of them to make sure
    things don’t go awry?'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**人类代理和监督**：本质上，这一要求指出AI系统应该尊重人权，不应完全自主运行。AI应该辅助，而不是替代人类决策。应有一个挑战AI决策的过程，并且在必要时，人类应能够覆盖AI的决策。这听起来不错，但当数以百计或千计的决策自动做出时，你如何有效跟踪所有这些决策以确保不会出错？'
- en: '**Technical Robustness and Safety:** AI systems need to be secure, robust‌
    and resilient against bad actors and cyber attacks. They should provide accurate
    predictions that are reliable and reproducible. Organizations must prioritize
    cybersecurity and have contingency plans for attacks and how to operate if the
    system goes offline. They need to pay special attention to adversarial data poisoning,
    where malicious actors alter training data to cause incorrect predictions.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**技术健壮性和安全性**：AI系统需要安全、稳健，并能够抵御恶意行为者和网络攻击。它们应提供准确的预测，且可靠、可复现。组织必须优先考虑网络安全，并制定攻击的应急计划及系统离线时的操作方式。他们需要特别注意对抗性数据投毒，即恶意行为者修改训练数据以导致错误预测。'
- en: '**Privacy and Governance**: “AI systems must guarantee privacy and data protection
    throughout a system’s entire lifecycle.”[6] Developers of AI systems need to put
    safeguards in place to prevent malicious data or code from being fed into the
    system. The guidelines also emphasize that only authorized users should access
    an individual’s data, which must be fair, unbiased, and follow all privacy regulations
    throughout its lifecycle. One area that organizations need to think about is what
    constitutes an “authorized user”? Did you see the case of Roomba [taking pictures
    of a woman on the commode](https://www.technologyreview.com/2022/12/19/1065306/roomba-irobot-robot-vacuums-artificial-intelligence-training-data-privacy/)
    which ended up on Facebook?'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**隐私和治理**：“AI系统必须在整个系统生命周期内保证隐私和数据保护。”[6] AI系统的开发者需要设置保护措施，以防止恶意数据或代码被输入系统。指南还强调，只有授权用户才能访问个人数据，这些数据必须公平、公正，并遵守整个生命周期中的所有隐私规定。组织需要思考的一个领域是什么构成“授权用户”？你看到过[Roomba拍摄女性上厕所](https://www.technologyreview.com/2022/12/19/1065306/roomba-irobot-robot-vacuums-artificial-intelligence-training-data-privacy/)的案例吗？'
- en: '**Transparency**: Organizations must be able to trace data lineage, understanding
    its source, how it was collected, transformed‌ and used. This process should be
    auditable, and the AI outputs should be explainable. This poses a challenge for
    data scientists because oftentimes, explainable models are often less accurate
    than “black-box” algorithms. This requirement also states that people interacting
    with AI should be aware that they are doing so — in other words, AI shouldn’t
    pretend to be human and it should be clear that we’re interacting with a bot.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**透明性**：组织必须能够追溯数据的来源，了解其来源、如何收集、转化和使用。这一过程应该是可审计的，AI 输出应该是可解释的。这对数据科学家来说是一个挑战，因为可解释的模型通常不如“黑箱”算法准确。此要求还说明，人与AI互动时应该意识到这一点——换句话说，AI不应该假装成真人，应该清楚我们是在与机器人互动。'
- en: '**Diversity, Non-discrimination‌ and Fairness**: AI should treat all groups
    equally — which may be easier said and done. The requirement suggests that designers
    should include people from diverse cultures, experiences‌ and backgrounds to help
    mitigate some of the historic bias that pervades many cultures. AI should be accessible
    to everyone, regardless of disability or other factors. This begs the question,
    what defines a “group”? There’s the obvious protected classes — age, race, color,
    region/creed, national origin, sex, age, physical or mental disability‌ or veteran
    status. Are there other factors that should be considered? If I’m an insurance
    company, can I charge people who have “healthier” habits less than those who are
    considered “unhealthy”?'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**多样性、非歧视和公平性**：AI应平等对待所有群体——这可能说起来容易做起来难。该要求建议设计师应包括来自不同文化、经验和背景的人，以帮助减轻许多文化中存在的历史偏见。AI应对所有人都可及，无论是否有残疾或其他因素。这就引发了一个问题，什么定义了“群体”？显而易见的保护类别包括年龄、种族、肤色、地区/信仰、国籍、性别、年龄、身体或精神残疾或退伍军人身份。是否还有其他因素需要考虑？如果我是保险公司，我可以对拥有“更健康”习惯的人收取比那些被认为“不健康”的人更少的费用吗？'
- en: '**Societal and Environmental Well-being**: AI systems should aim to improve
    society, promote democracy‌ and create environmentally friendly and sustainable
    systems. Just because you can do something, doesn’t mean you *should* do something.
    Business leaders need to critically consider the potential societal impacts of
    AI. What are the costs invovled in training your AI models? Are they at odds with
    your environmental, social‌ and corporate governance (ESG) policies? We’ve already
    seen examples where social media platforms like [TikTok are pushing harmful content
    at kids](https://www.cnn.com/2022/12/15/tech/tiktok-teens-study-trnd/index.html).'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**社会和环境福祉**：AI系统应致力于改善社会、促进民主，并创建环保和可持续的系统。仅仅因为你可以做某件事，并不意味着你*应该*做这件事。商业领袖需要批判性地考虑AI的潜在社会影响。训练AI模型的成本是什么？这些成本是否与您的环境、社会和公司治理（ESG）政策相抵触？我们已经看到社交媒体平台如
    [TikTok在向孩子推送有害内容](https://www.cnn.com/2022/12/15/tech/tiktok-teens-study-trnd/index.html)。'
- en: '**Accountability**: AI system designers should be accountable for their systems,
    which should be auditable and provide a way for those impacted by decisions to
    rectify and correct any unfair decisions. Designers may be held liable for any
    harm done to individuals or groups. This raises an interesting question — who
    is culpable of the system goes haywire? Is it the provider of the foundation model
    or is it the company that is using generative AI?'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**问责制**：AI系统设计师应对其系统负责，这些系统应可审计，并提供一种方式，让受决策影响的人能够纠正任何不公平的决定。设计师可能会对对个人或群体造成的任何伤害负责。这引发了一个有趣的问题——如果系统出现故障，谁应负责？是基础模型的提供者，还是使用生成式AI的公司？'
- en: While these principles seem intuitive on the surface, there is “substantive
    divergence in relation to how these principles are interpreted; why they are deemed
    important; what issue, domain or actors they pertainto; and how they should be
    implemented.”[7]
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些原则表面上似乎直观，但“在这些原则的解释、重要性、涉及的问题、领域或行为者以及如何实施方面存在实质性差异。”[7]
- en: AI Ethics Considerations for LLMs
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM的AI伦理考虑
- en: Now that we understand the EU AI Ethics guidelines, let’s delve into unique
    considerations for LLMs.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了欧盟AI伦理指南，让我们深入探讨LLM的独特考虑因素。
- en: 'In a previous blog titled [GenAIOps: Evolving the MLOps Framework](https://medium.com/towards-data-science/genaiops-evolving-the-mlops-framework-b0012f936379),
    I outlined three key capabilities of generative AI and LLMs which include:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '在上一篇博客中，[GenAIOps: Evolving the MLOps Framework](https://medium.com/towards-data-science/genaiops-evolving-the-mlops-framework-b0012f936379)
    我概述了生成式AI和LLM的三项关键能力，包括：'
- en: '● **Content Generation**: Generative AI can generate human-like quality — including
    text, audio, images/video and even software code. Now, one should note that content
    generated may not be factually accurate — the onus is on the end-user to make
    sure the generated content is true and not misleading. Developers need to make
    sure that the code generated is free of bugs and viruses.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ● **内容生成**：生成式AI可以生成类似人类质量的内容——包括文本、音频、图像/视频甚至软件代码。现在，需要注意的是，生成的内容可能不准确——最终用户有责任确保生成的内容真实且不具误导性。开发人员需要确保生成的代码没有漏洞和病毒。
- en: '● **Content Summarization and Personalization**: The ability to sift through
    large corpora of documents and quickly summarize the content is a strength of
    generative AI. In addition to quickly creating summaries of documents, emails
    and Slack messages, Generative AI can personalize these summaries for specific
    individuals or personas.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ● **内容总结与个性化**：快速浏览大量文档并迅速总结内容是生成性人工智能的一个强项。除了快速创建文档、电子邮件和Slack消息的总结外，生成性人工智能还可以为特定个人或角色个性化这些总结。
- en: '● **Content Discovery and Q&A**: Many organizations have a significant amount
    of content and data scattered across their organization in different data silos.
    Many data and analytics vendors are using LLMs and generative AI to automatically
    discover and connect disparate sources. End-users can then query this data, in
    plain language, to comprehend key points and drill-down for more detail.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ● **内容发现与问答**：许多组织在不同的数据孤岛中分散着大量的内容和数据。许多数据和分析供应商正在利用大型语言模型（LLMs）和生成性人工智能自动发现并连接这些分散的来源。最终用户可以用普通语言查询这些数据，以理解关键点并深入探讨更多细节。
- en: Given these various capabilities, what factors do we need to consider when creating
    an AI Ethics Framework?
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这些不同的能力，我们在创建人工智能伦理框架时需要考虑哪些因素？
- en: Human Agency and Oversight
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工干预与监督
- en: Since generative AI can essentially produce content autonomously, there’s a
    risk that human involvement and oversight may be reduced. If you think about it,
    how much email spam do you receive daily? Marketing teams create these emails,
    load them into a marketing automation system and push the “Go” button. These run
    on auto-pilot and often times, are forgotten and run in perpetuity.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 由于生成性人工智能本质上可以自主生成内容，这可能会降低人工干预和监督的程度。你可以想象一下，你每天收到多少垃圾邮件？营销团队创建这些邮件，将它们加载到营销自动化系统中，然后按下“开始”按钮。这些系统在自动驾驶模式下运行，往往被遗忘并持续运行。
- en: Given that generative AI can produce text, image, audio, video and software
    code at breakneck speeds — what steps can we put in place ‌to make sure that there
    is a human-in-the-loop; especially in critical applications? If we’re automating
    healthcare advice, legal advice and other more “sensitive” types of content, organizations
    need to think critically about how they can keep their agency and oversight over
    these systems. Companies need to put safeguards in place to ensure that the decisions
    being made align with human values and intentions.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于生成性人工智能可以以惊人的速度生成文本、图像、音频、视频和软件代码，我们可以采取哪些步骤来确保有人工干预；特别是在关键应用中？如果我们在自动化医疗建议、法律建议和其他更“敏感”的内容时，组织需要认真思考如何保持对这些系统的控制和监督。公司需要采取保障措施，确保做出的决策符合人类的价值观和意图。
- en: Technical Robustness and Safety
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术稳健性与安全性
- en: It is well known that generative AI models can create content that is unexpected
    or‌ even harmful. Companies need to rigorously test and validate their generative
    AI models to make sure they are reliable and safe. Also, if the generated content
    is erroneous, we need to have a mechanism in place to handle and correct that
    output. The internet is full of horrible and divisive content and some companies
    have hired content moderators to try and review suspicious content, but this seems
    like an impossible task. Just recently, it was reported that some of this content
    can be quite a detriment to ones mental health ([AP News — ​​Facebook content
    moderators in Kenya call the work ‘torture.’ Their lawsuit may ripple worldwide.](https://apnews.com/article/kenya-facebook-content-moderation-lawsuit-8215445b191fce9df4ebe35183d8b322))
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，生成性人工智能模型可能会生成意外或甚至有害的内容。公司需要严格测试和验证其生成性人工智能模型，以确保它们是可靠和安全的。此外，如果生成的内容有误，我们需要有机制来处理和纠正这些输出。互联网充满了糟糕和具有分裂性的内容，一些公司已经雇佣内容审核员来尝试审查可疑内容，但这似乎是一个不可能完成的任务。最近有报道称，这些内容可能对心理健康造成严重损害（[美联社新闻
    — ​​肯尼亚的Facebook内容审核员称工作是‘折磨’。他们的诉讼可能会产生全球影响。](https://apnews.com/article/kenya-facebook-content-moderation-lawsuit-8215445b191fce9df4ebe35183d8b322)）
- en: Privacy and Governance
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 隐私与治理
- en: Generative AI models were trained on data that was gathered from across the
    internet. Many of the LLM makers do not really disclose the fine details of what
    data was used to train the model. Now, the models could have been trained on sensitive
    or private data that should not be publicly available. Just look at Samsung who
    inadvertently leaked proprietary data ([TechCrunch — Samsung bans use of generative
    AI tools like ChatGPT after April internal data leak](https://techcrunch.com/2023/05/02/samsung-bans-use-of-generative-ai-tools-like-chatgpt-after-april-internal-data-leak/)).
    What if generative AI generates outputs that include or resemble real, private
    data? According to Bloomberg Law, [OpenAI was recently serveed a defamation lawsuit](https://news.bloomberglaw.com/tech-and-telecom-law/openai-hit-with-first-defamation-suit-over-chatgpt-hallucination)
    over a ChatGPT hallucination.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 生成性AI模型是在从互联网收集的数据上进行训练的。许多LLM制造商实际上并没有公开详细的训练数据细节。现在，这些模型可能在敏感或私人数据上进行训练，而这些数据本不应公开可用。看看三星，因无意中泄露专有数据而被禁止使用生成性AI工具（[TechCrunch
    — 三星在4月内部数据泄露后禁止使用生成性AI工具如ChatGPT](https://techcrunch.com/2023/05/02/samsung-bans-use-of-generative-ai-tools-like-chatgpt-after-april-internal-data-leak/)）。如果生成性AI产生的输出包含或类似真实的私人数据会怎样？根据《彭博法律》，[OpenAI
    最近被起诉诽谤](https://news.bloomberglaw.com/tech-and-telecom-law/openai-hit-with-first-defamation-suit-over-chatgpt-hallucination)由于ChatGPT的幻觉。
- en: We can certainly say that companies need to have a detailed understanding of
    the sources of data used to train generative AI models. As you fine tune and adapt
    your models using your own data, it is within your power to either remove or anonymize
    that data. However, you could still be at risk if the foundation model provider
    used data that was inappropriate for model training. If this is the case, who
    is liable?
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以肯定地说，公司需要对用于训练生成性AI模型的数据来源有详细的了解。当你使用自己的数据对模型进行微调和适配时，你可以选择删除或匿名化这些数据。然而，如果基础模型提供者使用了不适合模型训练的数据，你仍可能面临风险。如果是这种情况，谁应负责任？
- en: Transparency
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 透明度
- en: By their nature, “black-box” models are hard to interpret. In fact, many of
    these LLMs have billions of parameters so I would suggest that they are not interpretable.
    Companies should strive for transparency and create documentation on how the model
    works, it’s limitations, risks, and the data that was used to train the model.
    Again, this is easier said than done.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 就其本质而言，“黑箱”模型很难解释。实际上，许多这些LLMs具有数十亿个参数，所以我建议它们不可解释。公司应致力于透明度，创建有关模型工作原理、局限性、风险和用于训练模型的数据的文档。同样，这说起来容易做起来难。
- en: Diversity, Non-discrimination‌ and Fairness
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多样性、非歧视和公平
- en: Related to the above, if not properly trained and accounted for, generative
    AI can produce biased or discriminatory output. Companies can do their best to
    ensure that data is diverse and representative, but this is a tall order given
    that many of the LLM providers do not disclose what data was used for training.
    In addition to taking all possible precautions with understanding the training
    data used, its risks and limitations, companies need to put in place a monitoring
    system to detect this harmful content and a mechanism to flag, prevent its distribution
    and correct as necessary.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 与上述相关，如果没有得到妥善训练和管理，生成性AI可能会产生偏见或歧视性的输出。公司可以尽力确保数据的多样性和代表性，但鉴于许多LLM提供者并未公开用于训练的数据，这是一项艰巨的任务。除了尽可能了解训练数据的风险和局限性之外，公司还需要建立监控系统，以检测有害内容，并设立机制以标记、阻止其分发并根据需要进行纠正。
- en: Societal and Environmental Well-being
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 社会和环境福祉
- en: For companies with ESG initiatives, training LLMs consumes significant amounts
    of compute — meaning they use quite a bit of electricity. As you begin to deploy
    generative AI capability, organization’s need to be mindful of the environmental
    footprint and seek for ways to reduce it. There are several researchers who are
    looking at ways to reduce model size and accelerate the training process. As this
    evolves, companies should at least account for the environmental impact in their
    annual reports.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有ESG计划的公司来说，训练LLMs消耗大量计算资源——即消耗相当多的电力。当你开始部署生成性AI功能时，组织需要关注环境足迹，并寻求减少的方法。有几位研究人员正在寻找减少模型大小和加速训练过程的方法。随着这一领域的发展，公司至少应在年报中考虑环境影响。
- en: Accountability
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 责任
- en: This will be an active area of litigation for several years to come. Who is
    accountable if generative AI produces harmful or misleading content? Who is legally
    responsible? Several lawsuits are ‌pending in the U.S. court system which will
    set the stage for other litigation as things move forward. In addition to harmful
    content, what if your LLM produces a derivative work? Was your LLM trained on
    copyrighted or legally protected material? If it produces a data derivative, how
    will the courts address this? As companies implement generative AI capability
    — there should be controls and feedback mechanisms put in place so a course of
    action can be taken to remedy the situation.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这将成为未来几年内一个活跃的诉讼领域。如果生成性 AI 产生有害或误导性内容，谁应对此负责？谁在法律上负有责任？在美国法院系统中已有几个诉讼悬而未决，这些诉讼将为未来的其他诉讼奠定基础。除了有害内容之外，如果你的
    LLM 生成了衍生作品呢？你的 LLM 是否在受版权或法律保护的材料上进行了训练？如果它生成了数据衍生物，法院将如何处理这一问题？随着公司实施生成性 AI
    能力，应当建立控制和反馈机制，以便在出现问题时采取相应的行动。
- en: Summary
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Generative AI holds immense promise in revolutionizing how things get done in
    the world, but its rapid evolution brings forth a myriad of ethical dilemmas.
    As companies venture into the realm of generative AI, it’s paramount to navigate
    its implementation with a deep understanding of established ethical guidelines.
    By doing so, organizations can harness the transformative power of AI while making
    sure they uphold ethical standards, safeguarding against potential pitfalls and
    harms.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 生成性 AI 在革新世界运作方式方面蕴藏着巨大潜力，但其快速演变也带来了众多伦理困境。随着公司进入生成性 AI 的领域，必须深刻理解已建立的伦理指南以导航其实施。通过这样做，组织可以利用
    AI 的变革力量，同时确保遵守伦理标准，防范潜在的陷阱和危害。
- en: 'If you want to learn more about Artificial Intelligence, check out my book
    [Artificial Intelligence: An Executive Guide to Make AI Work for Your Business
    on Amazon](https://www.amazon.com/Artificial-Intelligence-Executive-Guide-Business/dp/B09X4KTYS4)
    or the [AI narrated audio book on Google Play](https://play.google.com/store/audiobooks/details/Artificial_Intelligence_An_Executive_Guide_to_Make?id=AQAAAECisyHzkM&hl=en_US&gl=US).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于人工智能的内容，可以查看我的书籍 [《人工智能：让 AI 为你的业务服务的执行指南》在亚马逊](https://www.amazon.com/Artificial-Intelligence-Executive-Guide-Business/dp/B09X4KTYS4)
    或 [在 Google Play 上的 AI 解说有声书](https://play.google.com/store/audiobooks/details/Artificial_Intelligence_An_Executive_Guide_to_Make?id=AQAAAECisyHzkM&hl=en_US&gl=US)。
- en: '[1] European Commission. 2021\. “Ethics Guidelines for Trustworthy AI | Shaping
    Europe’s Digital Future.” Digital-Strategy.ec.europa.eu. March 8, 2021\. [https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai](https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai).'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 欧洲委员会. 2021\. “可信 AI 的伦理指南 | 塑造欧洲的数字未来。” Digital-Strategy.ec.europa.eu.
    2021年3月8日\. [https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai](https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai).'
- en: '[2] European Commission. 2021\. “Ethics Guidelines for Trustworthy AI | Shaping
    Europe’s Digital Future.” Digital-Strategy.ec.europa.eu. March 8, 2021\. [https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai](https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] 欧洲委员会. 2021\. “可信 AI 的伦理指南 | 塑造欧洲的数字未来。” Digital-Strategy.ec.europa.eu.
    2021年3月8日\. [https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai](https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai).'
- en: '[3] European Commission. 2021\. “Ethics Guidelines for Trustworthy AI | Shaping
    Europe’s Digital Future.” Digital-Strategy.ec.europa.eu. March 8, 2021\. [https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai](https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] 欧洲委员会. 2021\. “可信 AI 的伦理指南 | 塑造欧洲的数字未来。” Digital-Strategy.ec.europa.eu.
    2021年3月8日\. [https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai](https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai).'
- en: '[4] European Commission. 2021\. “Ethics Guidelines for Trustworthy AI | Shaping
    Europe’s Digital Future.” Digital-Strategy.ec.europa.eu. March 8, 2021\. [https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai](https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] 欧洲委员会. 2021\. “可信 AI 的伦理指南 | 塑造欧洲的数字未来。” Digital-Strategy.ec.europa.eu.
    2021年3月8日\. [https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai](https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai).'
- en: '[5] Verma, Sahil, and Julia Rubin. 2018\. “Fairness Definitions Explained.”
    Proceedings of the International Workshop on Software Fairness — FairWare ’18\.
    [https://doi.org/10.1145/3194770.3194776](https://doi.org/10.1145/3194770.3194776).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Verma, Sahil, 和 Julia Rubin. 2018. “公平性定义解释。”《国际软件公平性研讨会论文集 — FairWare
    ’18》。 [https://doi.org/10.1145/3194770.3194776](https://doi.org/10.1145/3194770.3194776)。'
- en: '[6] European Commission. 2021\. “Ethics Guidelines for Trustworthy AI | Shaping
    Europe’s Digital Future.” Digital-Strategy.ec.europa.eu. March 8, 2021\. [https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai](https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] 欧洲委员会. 2021. “值得信赖的人工智能伦理指南 | 塑造欧洲的数字未来。” Digital-Strategy.ec.europa.eu.
    2021年3月8日。 [https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai](https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai)。'
- en: '[7] Jobin, Anna, Marcello Ienca, and Effy Vayena. 2019\. “The Global Landscape
    of AI Ethics Guidelines.” Nature Machine Intelligence 1 (9): 389–99\. [https://doi.org/10.1038/s42256-019-0088-2](https://doi.org/10.1038/s42256-019-0088-2).'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Jobin, Anna, Marcello Ienca, 和 Effy Vayena. 2019. “全球人工智能伦理指南概况。”《自然机器智能》1
    (9): 389–99。 [https://doi.org/10.1038/s42256-019-0088-2](https://doi.org/10.1038/s42256-019-0088-2)。'
