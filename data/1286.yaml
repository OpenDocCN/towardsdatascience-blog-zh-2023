- en: Accelerating Block Sparse Matrix Multiplication with Graphcore IPU and the PopSparse
    library
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Graphcore IPU和PopSparse库加速块稀疏矩阵乘法
- en: 原文：[https://towardsdatascience.com/accelerating-block-sparse-matrix-multiplication-with-graphcore-ipu-and-the-popsparse-library-3791284a2127?source=collection_archive---------10-----------------------#2023-04-12](https://towardsdatascience.com/accelerating-block-sparse-matrix-multiplication-with-graphcore-ipu-and-the-popsparse-library-3791284a2127?source=collection_archive---------10-----------------------#2023-04-12)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/accelerating-block-sparse-matrix-multiplication-with-graphcore-ipu-and-the-popsparse-library-3791284a2127?source=collection_archive---------10-----------------------#2023-04-12](https://towardsdatascience.com/accelerating-block-sparse-matrix-multiplication-with-graphcore-ipu-and-the-popsparse-library-3791284a2127?source=collection_archive---------10-----------------------#2023-04-12)
- en: '**Faster sparse matrix multiplication with the new library PopSparse**'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**使用新库PopSparse加速稀疏矩阵乘法**'
- en: '[](https://medium.com/@dominic.masters.gc?source=post_page-----3791284a2127--------------------------------)[![Dominic
    Masters](../Images/a4bb210cb0a19c81941c50a1e07a11b6.png)](https://medium.com/@dominic.masters.gc?source=post_page-----3791284a2127--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3791284a2127--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3791284a2127--------------------------------)
    [Dominic Masters](https://medium.com/@dominic.masters.gc?source=post_page-----3791284a2127--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@dominic.masters.gc?source=post_page-----3791284a2127--------------------------------)[![多米尼克·马斯特斯](../Images/a4bb210cb0a19c81941c50a1e07a11b6.png)](https://medium.com/@dominic.masters.gc?source=post_page-----3791284a2127--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3791284a2127--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3791284a2127--------------------------------)
    [多米尼克·马斯特斯](https://medium.com/@dominic.masters.gc?source=post_page-----3791284a2127--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb23ad1d05ffe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faccelerating-block-sparse-matrix-multiplication-with-graphcore-ipu-and-the-popsparse-library-3791284a2127&user=Dominic+Masters&userId=b23ad1d05ffe&source=post_page-b23ad1d05ffe----3791284a2127---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3791284a2127--------------------------------)
    ·10 min read·Apr 12, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3791284a2127&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faccelerating-block-sparse-matrix-multiplication-with-graphcore-ipu-and-the-popsparse-library-3791284a2127&user=Dominic+Masters&userId=b23ad1d05ffe&source=-----3791284a2127---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fb23ad1d05ffe&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faccelerating-block-sparse-matrix-multiplication-with-graphcore-ipu-and-the-popsparse-library-3791284a2127&user=Dominic+Masters&userId=b23ad1d05ffe&source=post_page-b23ad1d05ffe----3791284a2127---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3791284a2127--------------------------------)
    ·10分钟阅读·2023年4月12日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3791284a2127&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faccelerating-block-sparse-matrix-multiplication-with-graphcore-ipu-and-the-popsparse-library-3791284a2127&user=Dominic+Masters&userId=b23ad1d05ffe&source=-----3791284a2127---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3791284a2127&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faccelerating-block-sparse-matrix-multiplication-with-graphcore-ipu-and-the-popsparse-library-3791284a2127&source=-----3791284a2127---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3791284a2127&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Faccelerating-block-sparse-matrix-multiplication-with-graphcore-ipu-and-the-popsparse-library-3791284a2127&source=-----3791284a2127---------------------bookmark_footer-----------)'
- en: 'Authors: **Zhiyi Li**, AI Engineer, **Douglas Orr**, Research Scientist, **Valeriu
    Ohan**, Software Engineer, and [**Dominic Masters**](https://medium.com/u/b23ad1d05ffe?source=post_page-----3791284a2127--------------------------------),
    Research Scientist'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作者：**李智怡**，AI工程师，**道格拉斯·奥尔**，研究科学家，**瓦勒留·奥汉**，软件工程师，以及 [**多米尼克·马斯特斯**](https://medium.com/u/b23ad1d05ffe?source=post_page-----3791284a2127--------------------------------)，研究科学家
- en: '![](../Images/5b70b53b06a68360136d4264b945137d.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5b70b53b06a68360136d4264b945137d.png)'
- en: Image by author.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者提供。
- en: In light of the significant growth in deep learning model size as well as the
    increased uptake of artificial intelligence across a broad range of applications,
    the incentives to reduce the computational cost of running these models have never
    been greater.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于深度学习模型规模的显著增长以及人工智能在广泛应用中的普及，减少运行这些模型的计算成本的激励比以往任何时候都更大。
- en: One method that has shown significant promise to reduce these costs is to use
    *sparsity*. Sparsity typically refers to the method of inducing as many zeros
    as possible in the model weights through a process of *pruning*, then skipping
    these values when doing the computation itself.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一种显示出显著前景的减少这些成本的方法是使用*稀疏性*。稀疏性通常指通过*剪枝*过程在模型权重中引入尽可能多的零，然后在实际计算时跳过这些值。
- en: While much success has been achieved in using sparsity to reduce theoretical
    measures of cost like FLOPs and parameter count, achieving actual speed improvements
    while maintaining acceptable task performance has been much more difficult, particularly
    on general-purpose accelerators such as GPUs using low-precision number formats.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在利用稀疏性减少理论成本指标（如FLOPs和参数计数）方面取得了很大成功，但在保持可接受任务性能的同时实现实际速度提升要困难得多，特别是在使用低精度数字格式的通用加速器（如GPU）上。
- en: 'To help realise these theoretical benefits in practice and enable further investigation
    of sparse techniques on Graphcore IPUs, we introduce PopSparse, a library that
    enables fast sparse operations by leveraging both the unique hardware characteristics
    of IPUs as well as any block structure defined in the data. We target two different
    types of sparsity: static, where the sparsity pattern is fixed at compile-time;
    and dynamic, where it can change each time the model is run. We benchmarked sparse
    dense matrix multiplication for both modes on IPU. Results indicate that the PopSparse
    implementations are faster than dense matrix multiplications on IPU at a range
    of sparsity levels with large matrix size and block size. Furthermore, static
    sparsity in general outperforms dynamic sparsity. While previous work on GPUs
    has shown speedups only for very high sparsity (typically 99% and above), we demonstrate
    that Graphcore’s static sparse implementation outperforms equivalent dense calculations
    in FP16 at lower sparsity (around 90%).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在实践中实现这些理论上的好处，并进一步研究在Graphcore IPUs上的稀疏技术，我们推出了PopSparse，这是一种通过利用IPUs的独特硬件特性以及数据中定义的任何块结构来实现快速稀疏操作的库。我们针对两种不同类型的稀疏性：静态稀疏性，即稀疏模式在编译时固定；和动态稀疏性，即每次运行模型时可以改变。我们在IPU上对这两种模式的稀疏稠密矩阵乘法进行了基准测试。结果表明，PopSparse的实现比IPU上的稠密矩阵乘法在各种稀疏水平、大矩阵尺寸和块尺寸下都更快。此外，静态稀疏性通常优于动态稀疏性。虽然以前在GPU上的研究仅在非常高的稀疏性（通常为99%及以上）下显示出加速效果，但我们展示了Graphcore的静态稀疏实现比在较低稀疏性（约90%）下的等效稠密计算表现更好。
- en: These results were published online on [arXiv](https://arxiv.org/abs/2303.16999)
    and accepted to the [Sparse Neural Network workshop at ICLR 2023](https://www.sparseneural.net/?).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果已在线发布在[arXiv](https://arxiv.org/abs/2303.16999)上，并被[2023年ICLR Sparse Neural
    Network工作坊](https://www.sparseneural.net/?)接受。
- en: '**Graphcore IPU for sparse dense matrix multiplication in deep neural network**'
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**Graphcore IPU用于深度神经网络中的稀疏稠密矩阵乘法**'
- en: The notion of sparsity in deep learning most commonly refers to the idea of
    sparsifying the model weights with the aim of reducing the associated storage
    and compute costs. This is typically done through a single pruning step at the
    end of training (Zhu & Gupta, 2017) or potentially through some sparse training
    regime (Evci et al., 2019). These approaches have typically achieved between 90–99%
    reduction in model weights while maintaining and acceptable level of accuracy
    (Hoefler et al., 2021).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习中的稀疏性概念最常指的是通过稀疏化模型权重来减少相关的存储和计算成本。通常，这通过在训练结束时的单次剪枝步骤（Zhu & Gupta, 2017）或通过某些稀疏训练机制（Evci
    et al., 2019）来实现。这些方法通常在保持可接受的准确度水平的同时，实现了模型权重减少90–99%的效果（Hoefler et al., 2021）。
- en: While the benefits to model size can be easily realised, increased computational
    efficiency can often be more difficult to achieve as the resulting unstructured
    sparsity patterns do not align well with the vectorised instruction sets in highly
    parallel deep learning accelerators (Qin et al., 2022). Therefore, one approach
    to improving sparse compute efficiency is to enforce some degree of structure
    on the sparsity pattern. This has motivated a wide range of structured pruning
    techniques such as neuron (Ebrahimi & Klabjan, 2021), channel-wise (He et al.,
    2017), block (Gray et al., 2017), 2:4 (Mishra et al., 2021). However, these methods
    generally incur a task performance penalty compared to equivalent unstructured
    methods.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管模型大小的好处很容易实现，但提高计算效率通常更为困难，因为产生的无结构稀疏模式与高度并行的深度学习加速器中的矢量化指令集不太匹配（Qin 等人，2022）。因此，提高稀疏计算效率的一种方法是对稀疏模式施加一定程度的结构。这促使了广泛的结构化剪枝技术，如神经元（Ebrahimi
    & Klabjan，2021）、通道级（He 等人，2017）、块（Gray 等人，2017）、2:4（Mishra 等人，2021）。然而，这些方法通常会比等效的无结构方法带来性能惩罚。
- en: 'Here we present PopSparse, a library for the effective acceleration of sparse
    dense matrix multiplication (SpMM) on Graphcore IPUs. IPUs have multiple architectural
    features that help accelerate sparse operations:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们介绍 PopSparse，这是一个用于 Graphcore IPU 上稀疏稠密矩阵乘法（SpMM）有效加速的库。IPU 拥有多个架构特性，有助于加速稀疏操作：
- en: Large, high bandwidth on-chip SRAM that improves performance for communication
    heavy, low arithmetic efficiency operations.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型、高带宽的片上 SRAM 可提高通信密集型、低算术效率操作的性能。
- en: Fine-grained MIMD processing model that partitions work over 1472 independent
    compute units (tiles) that enables high degrees of parallelism.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 细粒度 MIMD 处理模型将工作划分到 1472 个独立计算单元（瓦片）上，支持高度的并行性。
- en: Ahead-of-time compilation model that allows further optimisations to be employed
    when sparsity structure is known at compile time.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提前编译模型，当稀疏性结构在编译时已知时，可以采用进一步优化。
- en: '**SpMM in PopSparse library on IPU**'
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**PopSparse 库中的 SpMM 在 IPU 上**'
- en: 'The SpMM can be written as:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: SpMM 可以写作：
- en: '![](../Images/803733b50cd0ffffee604b7f742cf57b.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/803733b50cd0ffffee604b7f742cf57b.png)'
- en: where ⊙ denotes elementwise multiplication, * for inner product, *Y ∈ Rᵐ* ˣ*ⁿ*,
    *X ∈ Rᵏ* ˣ*ⁿ* are dense output and input matrices respectively. The sparse weight
    matrix (*M*⊙*W*) is defined via *M ∈ Bᵐ* ˣ*ᵏ (B = {*0,1*})*, a mask that represents
    the sparsity pattern, itself derived from *M̂* ∈ *B*⁽ᵐᐟᵇ⁾ˣ⁽ᵏᐟᵇ⁾, a block mask
    and *W ∈ Rᵐ* ˣ*ᵏ* defines weight values.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ⊙ 表示元素逐一相乘，* 表示内积，*Y ∈ Rᵐ* ˣ*ⁿ*，*X ∈ Rᵏ* ˣ*ⁿ* 分别是稠密的输出和输入矩阵。稀疏权重矩阵 (*M*⊙*W*)
    通过 *M ∈ Bᵐ* ˣ*ᵏ (B = {*0,1*})* 定义，这是一种表示稀疏性模式的掩码，来自 *M̂* ∈ *B*⁽ᵐᐟᵇ⁾ˣ⁽ᵏᐟᵇ⁾，块掩码和
    *W ∈ Rᵐ* ˣ*ᵏ* 定义权重值。
- en: In this formulation, (*M*⊙*W*) has a block-sparse structure, where contiguous
    square blocks of weights of shape *b* × *b* contain all the non-zero elements,
    given block-size *b*. The dimensions *m* and *k* are referred to as output and
    input feature size and n is referred to as batch size, corresponding to their
    typical role in weight-sparse neural network computation.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种表述中，(*M*⊙*W*) 具有块稀疏结构，其中形状为 *b* × *b* 的连续方形权重块包含所有非零元素，给定块大小 *b*。维度 *m* 和
    *k* 分别称为输出和输入特征大小，而 *n* 被称为批量大小，对应于其在权重稀疏神经网络计算中的典型角色。
- en: 'We define density, *d* = ∑ᵢⱼMᵢⱼ/(*m⋅k*), where the standard term sparsity corresponds
    to *(*1 *— d)*. We use floating point operation (FLOP) count to quantify the amount
    of useful arithmetic work in an SpMM as: 2*⋅m⋅k⋅n⋅d*. Note that this only counts
    operations performed on non-zero values and does not depend on block size *b*.
    The PopSparse library supports unstructured (block size *b* = 1) or structured
    sparsity (*b* = 4, 8, 16).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义密度，*d* = ∑ᵢⱼMᵢⱼ/(*m⋅k*)，其中标准术语稀疏度对应于 *(*1 *— d)*。我们使用浮点运算（FLOP）计数来量化 SpMM
    中有用的算术工作量为：2*⋅m⋅k⋅n⋅d*。注意，这只计算非零值上的操作，并不依赖于块大小 *b*。PopSparse 库支持无结构（块大小 *b* =
    1）或有结构的稀疏性（*b* = 4, 8, 16）。
- en: Static sparsity
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 静态稀疏性
- en: With static sparsity, during compile time, the sparsity pattern and matrix shapes
    are known to the partitioner, which splits the non-zero elements (specific values
    not known yet) of the sparse matrix across *k* dimension into *qₖ* partitions
    and the dense matrix across n dimension into *qₙ* partitions. Figure 1a illustrates
    the distribution, computation and reduction of the sparse matrix in the static
    sparse case. Splits over the k dimension do not have to be evenly sized and are
    chosen to ensure a balanced distribution of the non-zero elements (denoted with
    coloured small blocks in the figure). Once the model is compiled, during execution,
    the specific non-zero values of *W* are provided. As the sparsity pattern is known
    in advance, no extra exchange of data is needed. Local dot product computation
    with the dense matrix is then executed and there is a final reduction across tiles
    to give the dense output matrix.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在静态稀疏性中，编译时，稀疏模式和矩阵形状对分区器是已知的，分区器将稀疏矩阵（具体值尚未确定）的非零元素在 *k* 维度上划分为 *qₖ* 个分区，将密集矩阵在
    *n* 维度上划分为 *qₙ* 个分区。图 1a 说明了静态稀疏情况中稀疏矩阵的分配、计算和减少。*k* 维度上的分区不需要均匀大小，而是选择以确保非零元素（图中用彩色小块表示）的平衡分布。模型编译后，在执行期间提供
    *W* 的具体非零值。由于稀疏模式已知，无需额外的数据交换。然后执行与密集矩阵的局部点积计算，并在块间进行最终的归约，以给出密集输出矩阵。
- en: Dynamic sparsity
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动态稀疏性
- en: With dynamic sparsity, the maximum number of non-zero elements for the sparse
    operand is fixed, while the sparsity pattern is updated during execution. To minimise
    compute cycles, we employ a planner during compile time to optimise the partitioning
    while accommodating for the full range of sparsity patterns that can be defined
    during execution. As the sparsity pattern could change for each run, compared
    to static sparsity, there is an additional step to distribute sparse matrix indices
    and non-zero values by the partitioner. This is demonstrated in Figure 1b. Furthermore,
    as fixed size partitions over m and k dimensions are required, the number of non-zero
    elements in each partition may not be balanced. In Figure 1b the *B* elements
    on the second partition of *k* dimension do not fit on Tile1\. Three of the *B*
    elements overflow to Tile2\. However, the Tile2 does not have all the partition
    information (slices of *m, k, n*) corresponding to B elements to compute the result.
    Consequently, extra steps of propagation are induced to finish the computation.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在动态稀疏性中，稀疏操作数的非零元素的最大数量是固定的，而稀疏模式在执行期间会更新。为了最小化计算周期，我们在编译时使用规划工具来优化分区，同时适应执行过程中可以定义的所有稀疏模式。由于每次运行时稀疏模式可能会改变，与静态稀疏性相比，分区器需要额外的步骤来分配稀疏矩阵索引和非零值。这在图
    1b 中得到了演示。此外，由于在 *m* 和 *k* 维度上需要固定大小的分区，因此每个分区中的非零元素可能不平衡。在图 1b 中，第二个 *k* 维度分区中的
    *B* 元素无法适应 Tile1\。三个 *B* 元素溢出到 Tile2\。然而，Tile2 并没有包含所有与 B 元素对应的分区信息（*m, k, n*
    切片），因此无法计算结果。因此，需要额外的传播步骤来完成计算。
- en: 'Overall, our dynamic sparse implementation has several additional costs compared
    to the static case:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，我们的动态稀疏实现相比于静态稀疏情况有几个额外的成本：
- en: Additional control flow after compilation, which incurs some cost overhead.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编译后的额外控制流会带来一定的成本开销。
- en: We must compile for the largest inter-tile communication volume possible and
    thus are less efficient on average.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们必须针对最大的块间通信量进行编译，因此平均效率较低。
- en: When data is unbalanced, additional propagation and compute phases are required
    to rebalance the data. The number of steps required is dependent on the degree
    of the imbalance.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当数据不平衡时，需要额外的传播和计算阶段来重新平衡数据。所需的步骤数量取决于不平衡的程度。
- en: '![](../Images/9d7f8e5ea83338795d555ffeee22d8ca.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9d7f8e5ea83338795d555ffeee22d8ca.png)'
- en: 'Figure 1: Illustration of static and dynamic sparse partitioning of the input
    feature dimension, *k*. Image by author.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：输入特征维度 *k* 的静态和动态稀疏分区示意图。图像由作者提供。
- en: '**Benchmarks**'
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**基准测试**'
- en: 'Our micro benchmarking experiments explore a single sparse-dense matmul SpMM:
    *Y=(M*⊙*W)***X* on IPU and GPU. The APIs used for IPU experiments and GPU baselines
    are shown in Table 1\. The range of parameters that were benchmarked is detailed
    in Table 2.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的微基准测试实验探索了 IPU 和 GPU 上的单一稀疏-密集矩阵乘法 SpMM: *Y=(M*⊙*W)***X*。IPU 实验和 GPU 基线使用的
    API 如表 1 所示。基准测试的参数范围详见表 2。'
- en: '![](../Images/2b827a018386682c4aacf6b98a025ea8.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b827a018386682c4aacf6b98a025ea8.png)'
- en: 'Table 1: APIs used when benchmarking IPU and GPU.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：基准测试 IPU 和 GPU 时使用的 API。
- en: '**IPU**: The operation is executed a single time on a single Bow IPU in a Bow-2000
    chassis, with a randomly generated sparsity pattern and values. We extract cycle
    count information and convert these cycle counts into TFLOP/s values given a constant
    clock of 1.85 GHz. Host transfers are excluded from measured time.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**IPU**：该操作在 Bow-2000 机箱中的单个 Bow IPU 上执行一次，使用随机生成的稀疏模式和数值。我们提取周期计数信息，并在 1.85
    GHz 的固定时钟下将这些周期计数转换为 TFLOP/s 值。主机传输不计入测量时间。'
- en: '**GPU**: Baseline sparse implementations on GPU are provided by the cuSPARSE
    library v11.6.2 (NVIDIA, 2022). These are divided according to the sparse format
    used, with no distinction between static and dynamic sparsity patterns. We consider
    compressed sparse row (CSR) and block sparse row (BSR) for unstructured and block
    sparsity, respectively. We generate a random sparsity pattern and values, copy
    to device, execute 25 iterations of the operation and copy results to host for
    validation. We start timing after 5 iterations and measure wall clock time using
    *cudaEventRecord*. All experiments were performed on a single A100 GPU running
    in a 4 x A100-SXM4–40G chassis.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPU**：GPU 上的基线稀疏实现由 cuSPARSE 库 v11.6.2（NVIDIA, 2022）提供。这些实现根据使用的稀疏格式进行分类，不区分静态和动态稀疏模式。我们考虑压缩稀疏行（CSR）和块稀疏行（BSR）分别用于非结构化和块稀疏。我们生成随机稀疏模式和数值，复制到设备上，执行
    25 次操作迭代，并将结果复制到主机进行验证。我们在 5 次迭代后开始计时，并使用 *cudaEventRecord* 测量壁钟时间。所有实验都在一个 A100
    GPU 上进行，该 GPU 运行在 4 x A100-SXM4–40G 机箱中。'
- en: '![](../Images/3d9b4f3d6e2d0f3bcbf2bb005fec005e.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3d9b4f3d6e2d0f3bcbf2bb005fec005e.png)'
- en: 'Table 2: Ranges of parameters swept for benchmarks. Image by author.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：基准测试中扫取的参数范围。图像作者提供。
- en: The key results of this work are shown in Figure 2\. These plots show the compute
    FLOP/s (in log scale) vs density with fixed feature size, with each line presenting
    different implementations of the same operation. By construction, the dense implementation
    shows linear scaling with density — since zeros are not counted in the FLOP/s
    calculation. While on the other hand, for a sparse implementation, perfect scaling
    would predict constant FLOP/s as density varies.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究的关键结果如图 2 所示。这些图表显示了计算 FLOP/s（对数尺度）与密度的关系，特征大小固定，每条线代表相同操作的不同实现。由于零在 FLOP/s
    计算中不被计算，密集实现随着密度的增加呈线性扩展。而在稀疏实现中，完美的扩展应预测 FLOP/s 在密度变化时保持不变。
- en: '![](../Images/efcdb9f5f8a9a9edd15a098060cf823b.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/efcdb9f5f8a9a9edd15a098060cf823b.png)'
- en: 'Figure 2: IPU FP16 and GPU block-sparse matmul performance as density varies
    for various block size b, square feature size m = k = 4096, best over batch size
    n. Image by author.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：IPU FP16 和 GPU 块稀疏矩阵乘法性能在不同块大小 b 的密度变化下，平方特征大小 m = k = 4096，以最佳批量大小 n 为准。图像作者提供。
- en: For the dense baselines, while we broadly see chip-for-chip parity between IPU
    and GPU in FP16, it's important to highlight that the IPUs used are approximately
    [half the price in the cloud](https://docs.paperspace.com/gradient/machines/),
    highlighting their cost efficiency for these basic operations. Furthermore, for
    the sparse operations, we see even larger benefits from using IPUs.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对于密集基线，虽然我们大致看到 IPU 和 GPU 在 FP16 上的芯片对芯片相等，但重要的是要强调所使用的 IPU 的价格大约是 [云端价格的一半](https://docs.paperspace.com/gradient/machines/)，突显了其在这些基本操作中的成本效益。此外，对于稀疏操作，我们看到使用
    IPU 的好处更大。
- en: Specifically, for IPUs (Figure 2a), we see that using static sparse operations
    and block sparsity both individually improve performance. For example, the block
    size 16, dynamic case shows benefits over the dense baseline across all densities
    considered and unstructured (b = 1) static sparsity outperforming dense in the
    low-density regime (<5%). In addition, when both static and block sparsity methods
    are used, we see benefits of as much as 5x in throughput over the dense baseline.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，对于 IPU（图 2a），我们看到使用静态稀疏操作和块稀疏都能单独提高性能。例如，块大小为 16 的动态情况在所有考虑的密度下相较于密集基线显示出优势，而在低密度区域（<5%）中，非结构化（b
    = 1）静态稀疏优于密集稀疏。此外，当同时使用静态和块稀疏方法时，我们看到吞吐量相比密集基线提高了多达 5 倍。
- en: For the GPU (Figure 2b), we see much less favourable performance with the sparse
    operations, with no case tested outperforming the FP16 dense baseline. However,
    we did find that the FP32 block sparse (BSR) case scaled very well as density
    decreased. It looks possible that for very low densities, this may outperform
    the dense baseline.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 GPU（图 2b），我们看到稀疏操作的性能远不如预期，没有任何测试用例超过 FP16 密集基线。然而，我们发现 FP32 块稀疏（BSR）情况在密度降低时扩展非常好。看起来对于非常低的密度，这可能会超过密集基线。
- en: Comparing the two platforms, we can therefore see that IPUs achieve as much
    as a 5x throughput benefit over the dense GPU baseline and as much as a 25x throughput
    improvement when only sparse operations are considered. With the additional price
    benefits, we believe this presents a compelling case for investigating sparse
    applications on IPUs.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 比较两个平台，我们可以看到 IPU 相比于密集 GPU 基线在吞吐量上实现了高达 5 倍的收益，而在仅考虑稀疏操作时则提高了高达 25 倍的吞吐量。考虑到额外的价格优势，我们认为这为在
    IPU 上调查稀疏应用提供了有力的理由。
- en: 'It should however be noted that even on IPU, the sparse operations only outperform
    the dense baselines under certain conditions. More extensive result can be found
    in [our paper](https://arxiv.org/abs/2303.16999), but as an approximate guide,
    sparsity typically brings speedup over the dense baseline on IPU (FP16, large
    batch size for both) when:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然而需要注意的是，即使在 IPU 上，稀疏操作也仅在某些条件下超过密集基线。更多详细结果可以在 [我们的论文](https://arxiv.org/abs/2303.16999)
    中找到，但作为大致指南，当以下条件满足时，稀疏性通常能在 IPU（FP16，两个模型的大批量）上带来比密集基线更快的速度：
- en: Static sparsity, block size b = 1, features (m = k) ≥ 4096, density d ≤ 1/32.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 静态稀疏性，块大小 b = 1，特征 (m = k) ≥ 4096，密度 d ≤ 1/32。
- en: 'Static sparsity, block size b ≥ 4: features (m = k) ≥ 4096, density d ≤ 1/8.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 静态稀疏性，块大小 b ≥ 4：特征 (m = k) ≥ 4096，密度 d ≤ 1/8。
- en: Dynamic sparsity, block size b ≥ 8, features (m = k) ≥ 4096, density d ≤ 1/32.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态稀疏性，块大小 b ≥ 8，特征 (m = k) ≥ 4096，密度 d ≤ 1/32。
- en: Importantly, these ranges are somewhat challenging to use today in the context
    of weight sparse training and inference. This is primarily because finding block
    sparse patterns that do not degrade performance is a hard problem and that, on
    theoretical efficiency metrics (per non-zero FLOP), unstructured sparsity will
    always appear as good or better than block sparsity. We hope that this work shows
    that block sparsity is a promising method for achieving practical acceleration
    of sparse models and will motivate further investigation into effective block
    sparse pruning algorithms in the research community.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，这些范围在今天的权重稀疏训练和推理中有些难以使用。这主要是因为找到不会降低性能的块稀疏模式是一个困难的问题，并且在理论效率指标（每个非零 FLOP）上，无结构稀疏总是显得和块稀疏一样好或更好。我们希望这项工作表明，块稀疏是一种有前景的方法，可以实现稀疏模型的实际加速，并将激励研究社区对有效的块稀疏剪枝算法进行进一步的研究。
- en: Come and [try our PopSparse operations](https://console.paperspace.com/github/graphcore-research/notebooks?container=graphcore%2Fpytorch-jupyter%3A3.2.0-ubuntu-20.04&machine=Free-IPU-POD4&file=%2Fsparsity_benchmarks%2FSpMM.ipynb)
    for yourself on IPUs on Paperspace and then why not try integrating them into
    your own models? If you are actively pursuing research in sparsity that could
    benefit from IPU support, please consider signing up for our [Academic program](https://www.graphcore.ai/academics)
    where we support researchers to achieve great things using IPUs.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 来 [尝试我们的 PopSparse 操作](https://console.paperspace.com/github/graphcore-research/notebooks?container=graphcore%2Fpytorch-jupyter%3A3.2.0-ubuntu-20.04&machine=Free-IPU-POD4&file=%2Fsparsity_benchmarks%2FSpMM.ipynb)
    在 Paperspace 的 IPUs 上，然后为什么不试试将它们集成到自己的模型中呢？如果你正在积极从事稀疏性研究，并且希望获得 IPU 支持，请考虑注册我们的
    [学术计划](https://www.graphcore.ai/academics)，我们在这里支持研究人员利用 IPUs 实现卓越的成果。
- en: Try it yourself
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自己尝试一下
- en: To try SpMM on IPU — [run on Paperspace](https://console.paperspace.com/github/graphcore-research/notebooks?container=graphcore%2Fpytorch-jupyter%3A3.2.0-ubuntu-20.04&machine=Free-IPU-POD4&file=%2Fsparsity_benchmarks%2FSpMM.ipynb).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试在 IPU 上使用 SpMM — [在 Paperspace 上运行](https://console.paperspace.com/github/graphcore-research/notebooks?container=graphcore%2Fpytorch-jupyter%3A3.2.0-ubuntu-20.04&machine=Free-IPU-POD4&file=%2Fsparsity_benchmarks%2FSpMM.ipynb)。
- en: '**References**'
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**参考文献**'
- en: '[1] Michael Zhu and Suyog Gupta. To prune, or not to prune: exploring the efficacy
    of pruning for model compression, 2017\. URL [https://arxiv.org/abs/1710.01878.](https://arxiv.org/abs/1710.01878.)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Michael Zhu 和 Suyog Gupta. 剪枝，还是不剪枝：探索剪枝在模型压缩中的有效性，2017\. 网址 [https://arxiv.org/abs/1710.01878.](https://arxiv.org/abs/1710.01878.)'
- en: '[2] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen.
    Rigging the lottery: Making all tickets winners, 2019\. URL [https://arxiv.org/abs/1911.11134.](https://arxiv.org/abs/1911.11134.)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro 和 Erich Elsen.
    改变彩票：让所有票据都成为赢家，2019\. URL [https://arxiv.org/abs/1911.11134.](https://arxiv.org/abs/1911.11134.)'
- en: '[3] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra
    Peste. Sparsity in deep learning: Pruning and growth for efficient inference and
    training in neural networks, 2021\. URL [https://arxiv.org/abs/2102.00554](https://arxiv.org/abs/2102.00554).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden 和 Alexandra Peste.
    深度学习中的稀疏性：神经网络中的剪枝与增长以实现高效推理和训练，2021\. URL [https://arxiv.org/abs/2102.00554](https://arxiv.org/abs/2102.00554).'
- en: '[4] Eric Qin, Raveesh Garg, Abhimanyu Bambhaniya, Michael Pellauer, Angshuman
    Parashar, Sivasankaran Rajamanickam, Cong Hao, and Tushar Krishna. Enabling flexibility
    for sparse tensor acceleration via heterogeneity, 2022\. URL [https://arxiv.org/abs/2201.08916.](https://arxiv.org/abs/2201.08916.)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[4] Eric Qin, Raveesh Garg, Abhimanyu Bambhaniya, Michael Pellauer, Angshuman
    Parashar, Sivasankaran Rajamanickam, Cong Hao 和 Tushar Krishna. 通过异质性实现稀疏张量加速的灵活性，2022\.
    URL [https://arxiv.org/abs/2201.08916.](https://arxiv.org/abs/2201.08916.)'
- en: '[5] Abdolghani Ebrahimi and Diego Klabjan. Neuron-based pruning of deep neural
    networks with better generalization using kronecker factored curvature approximation,
    2021\. URL [https://arxiv.org/abs/2111.08577.](https://arxiv.org/abs/2111.08577.)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[5] Abdolghani Ebrahimi 和 Diego Klabjan. 基于神经元的深度神经网络剪枝，通过克罗内克因子曲率近似实现更好的泛化，2021\.
    URL [https://arxiv.org/abs/2111.08577.](https://arxiv.org/abs/2111.08577.)'
- en: '[6] Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating
    very deep neural networks, 2017\. URL [https://arxiv.org/abs/1707.06168.](https://arxiv.org/abs/1707.06168.)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[6] Yihui He, Xiangyu Zhang 和 Jian Sun. 通过通道剪枝加速非常深的神经网络，2017\. URL [https://arxiv.org/abs/1707.06168.](https://arxiv.org/abs/1707.06168.)'
- en: '[7] Scott Gray, Alec Radford, and Diederik P. Kingma. GPU kernels for block-sparse
    weights. 2017.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[7] Scott Gray, Alec Radford 和 Diederik P. Kingma. 用于块稀疏权重的GPU内核。2017.'
- en: '[8] Asit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic,
    Ganesh Venkatesh, Chong Yu, and Paulius Micikevicius. Accelerating sparse deep
    neural networks, 2021\. URL [https://arxiv.org/abs/2104.08378](https://arxiv.org/abs/2104.08378).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[8] Asit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic,
    Ganesh Venkatesh, Chong Yu 和 Paulius Micikevicius. 加速稀疏深度神经网络，2021\. URL [https://arxiv.org/abs/2104.08378](https://arxiv.org/abs/2104.08378).'
- en: '[9] NVIDIA. API reference guide for cuSPARSE, 2022\. URL [https://docs.nvidia.com/cuda/cusparse/](https://docs.nvidia.com/cuda/cusparse/).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[9] NVIDIA. cuSPARSE的API参考指南，2022\. URL [https://docs.nvidia.com/cuda/cusparse/](https://docs.nvidia.com/cuda/cusparse/).'
