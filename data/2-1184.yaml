- en: How to Fine-Tune Llama2 for Python Coding on Consumer Hardware
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何在消费级硬件上微调 Llama2 以进行 Python 编程
- en: 原文：[https://towardsdatascience.com/how-to-fine-tune-llama2-for-python-coding-on-consumer-hardware-46942fa3cf92](https://towardsdatascience.com/how-to-fine-tune-llama2-for-python-coding-on-consumer-hardware-46942fa3cf92)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/how-to-fine-tune-llama2-for-python-coding-on-consumer-hardware-46942fa3cf92](https://towardsdatascience.com/how-to-fine-tune-llama2-for-python-coding-on-consumer-hardware-46942fa3cf92)
- en: '*Enhancing Llama2’s proficiency in Python through supervised fine-tuning and
    low-rank adaptation techniques*'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*通过监督微调和低秩适配技术提升 Llama2 在 Python 中的熟练度*'
- en: '[](https://medium.com/@luisroque?source=post_page-----46942fa3cf92--------------------------------)[![Luís
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----46942fa3cf92--------------------------------)[](https://towardsdatascience.com/?source=post_page-----46942fa3cf92--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----46942fa3cf92--------------------------------)
    [Luís Roque](https://medium.com/@luisroque?source=post_page-----46942fa3cf92--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@luisroque?source=post_page-----46942fa3cf92--------------------------------)[![Luís
    Roque](../Images/e281d470b403375ba3c6f521b1ccf915.png)](https://medium.com/@luisroque?source=post_page-----46942fa3cf92--------------------------------)[](https://towardsdatascience.com/?source=post_page-----46942fa3cf92--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----46942fa3cf92--------------------------------)
    [Luís Roque](https://medium.com/@luisroque?source=post_page-----46942fa3cf92--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----46942fa3cf92--------------------------------)
    ·18 min read·Aug 17, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----46942fa3cf92--------------------------------)
    ·18 分钟阅读·2023年8月17日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: About me
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于我
- en: Serial entrepreneur and leader in the AI space. I develop AI products for businesses
    and invest in AI-focused startups.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 连续创业者和 AI 领域的领军人物。我为企业开发 AI 产品，并投资于 AI 专注的初创公司。
- en: '[Founder @ ZAAI](http://zaai.ai) | [LinkedIn](https://www.linkedin.com/in/luisbrasroque/)
    | [X/Twitter](https://x.com/luisbrasroque)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[创始人 @ ZAAI](http://zaai.ai) | [LinkedIn](https://www.linkedin.com/in/luisbrasroque/)
    | [X/Twitter](https://x.com/luisbrasroque)'
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Our previous article covered Llama 2 in detail, presenting the family of Large
    Language models (LLMs) that Meta introduced recently and made available for the
    community for research and commercial use. There are variants already designed
    for specific tasks; for example, Llama2-Chat for chat applications. Still, we
    might want to get an LLM even more tailored for our application.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的上一篇文章详细介绍了 Llama 2，介绍了 Meta 最近推出并向社区提供研究和商业使用的大型语言模型（LLMs）家族。已经有专为特定任务设计的变体；例如，Llama2-Chat
    适用于聊天应用。不过，我们可能希望获得一个更贴合我们应用的 LLM。
- en: Following this line of thought, the technique we are referring to is transfer
    learning. This approach involves leveraging the vast knowledge already in models
    like Llama2 and transferring that understanding to a new domain. Fine-tuning is
    a subset or specific form of transfer learning. In fine-tuning, the weights of
    the entire model, including the pre-trained layers, are typically allowed to adjust
    to the new data. It means that the knowledge gained during pre-training is refined
    based on the specifics of the new task.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 按照这一思路，我们所提到的技术是迁移学习。这种方法涉及利用像 Llama2 这样的模型中已有的广泛知识，并将这些理解迁移到新的领域。微调是迁移学习的一个子集或特定形式。在微调过程中，整个模型的权重，包括预训练层，通常会根据新数据进行调整。这意味着在预训练期间获得的知识将根据新任务的具体情况进行细化。
- en: In this article, we outline a systematic approach to enhance Llama2’s proficiency
    in Python coding tasks by fine-tuning it on a custom dataset. First, we curate
    and align a dataset with Llama2’s prompt structure to meet our objectives. We
    then use Supervised Fine-Tuning (SFT) and Quantized Low-Rank Adaptation (QLoRA)
    to optimize the Llama2 base model. After optimization, we combine our model’s
    weights with the foundational Llama2\. Finally, we showcase how to perform inference
    using the fine-tuned model and how does it compare against the baseline model.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们概述了一种系统的方法，通过在自定义数据集上微调 Llama2 来提升其在 Python 编码任务中的熟练度。首先，我们整理和对齐一个与 Llama2
    的提示结构相匹配的数据集，以满足我们的目标。然后，我们使用监督微调（SFT）和量化低秩适配（QLoRA）来优化 Llama2 基础模型。优化后，我们将我们模型的权重与基础的
    Llama2 结合起来。最后，我们展示了如何使用微调后的模型进行推理，并比较其与基线模型的效果。
- en: '![](../Images/cfb419c3a9c7606b26c5f499751a59fe.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cfb419c3a9c7606b26c5f499751a59fe.png)'
- en: 'Figure 1: Llama2, the Python coder ([image source](https://unsplash.com/photos/Z5Yha3jIHnc))'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：Llama2，Python 编码器 ([图片来源](https://unsplash.com/photos/Z5Yha3jIHnc))
- en: One important caveat to recognize is that fine-tuning is sometimes unnecessary.
    Other approaches are easier to implement and, in some cases, better suited for
    our use case. For example, semantic search with vector databases efficiently handles
    informational queries, leveraging existing knowledge without custom training.
    The use cases where fine-tuning is required is when we need tailored interactions,
    like specialized Q&A or context-aware responses that use custom data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的警示是，有时微调是不必要的。其他方法更易于实施，并且在某些情况下更适合我们的使用场景。例如，使用向量数据库进行的语义搜索可以有效处理信息查询，利用现有知识而无需自定义训练。微调所需的使用场景是当我们需要量身定制的交互，例如专业的问答或利用自定义数据的上下文感知响应时。
- en: Supervised Fine-Tuning
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督式微调
- en: Modern machine learning paradigms commonly leverage pre-trained models. These
    pre-trained models have already undergone training on large datasets. The goal
    with SFT is to adapt them to specific tasks using minimal training data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现代机器学习范式通常利用预训练模型。这些预训练模型已经在大数据集上进行过训练。SFT 的目标是使用最少的训练数据将它们适应于特定任务。
- en: The way SFT works is by adjusting an LLM, such as Llama2, based on labeled examples
    that specify the data the model should generate. The dataset for SFT consists
    of prompts and their associated responses. Developers can either manually create
    this dataset or generate it using other LLMs. In fact, the open-source community
    frequently adopts this practice. A review of the top LLMs on the Open LLM Leaderboard
    [1] shows that almost all of them undergo some form of fine-tuning with an Orca-styled
    dataset. An Orca-style dataset contains numerous entries, each with a question
    and a corresponding response from GPT-4 or GPT-3.5\. In essence, SFT sharpens
    the knowledge within Llama2 using a specific set of examples.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: SFT 的工作方式是通过根据标记的示例调整 LLM，例如 Llama2，这些示例指定了模型应生成的数据。SFT 的数据集由提示和相关的响应组成。开发者可以手动创建这个数据集，也可以使用其他
    LLM 生成。事实上，开源社区经常采用这种做法。对 Open LLM Leaderboard [1] 上顶级 LLM 的审查显示，几乎所有的模型都经过了某种形式的微调，使用的是
    Orca 风格的数据集。Orca 风格的数据集包含大量条目，每个条目都有一个问题和来自 GPT-4 或 GPT-3.5 的相应回答。实质上，SFT 通过一组特定的示例来提升
    Llama2 的知识水平。
- en: Researchers now explicitly fine-tune many LLMs for instruction-following capabilities.
    This fine-tuning helps the models understand and act on user instructions better.
    For example, a fine-tuned model can produce a concise summary when a user instructs
    it to create a summary. A non-fine-tuned model might struggle with the task and
    become more verbose. As LLMs evolve, this kind of fine-tuning can produce more
    specialized models that fit the intended use case.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员现在明确地对许多 LLM 进行指令跟随能力的微调。这种微调帮助模型更好地理解和执行用户指令。例如，当用户指示模型创建一个总结时，一个经过微调的模型可以生成一个简洁的总结。一个未经过微调的模型可能会在这个任务上挣扎，并变得更加冗长。随着
    LLM 的发展，这种微调可以产生更专业的模型，更适合预期的使用场景。
- en: 'LoRA and QLoRA: An Efficient Approach to Fine-tuning Large Models'
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LoRA 和 QLoRA：一种高效的微调大型模型的方法
- en: 'To understand LoRA’s operation, we must first know the meaning of a matrix’s
    rank. The rank of a matrix shows the number of its independent rows or columns.
    For instance, an NxN matrix filled with random numbers has a rank of N. Nevertheless,
    if every column of this matrix is just a multiple of the first column, the rank
    becomes 1\. Thus, we can represent a rank 1 matrix as the product of two matrices:
    an Nx1 matrix times a 1xN matrix, creating an NxN matrix with a rank of 1\. In
    the same way, we can express a rank ‘r’ matrix as the product of an (Nxr) and
    an (rxN) matrix.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解 LoRA 的运作，我们必须首先了解矩阵秩的含义。矩阵的秩显示了其独立行或列的数量。例如，一个填充随机数的 NxN 矩阵的秩为 N。然而，如果这个矩阵的每一列只是第一列的倍数，则秩变为
    1。因此，我们可以将秩为 1 的矩阵表示为两个矩阵的乘积：一个 Nx1 矩阵乘以一个 1xN 矩阵，从而得到一个秩为 1 的 NxN 矩阵。同样，我们可以将秩为
    ‘r’ 的矩阵表示为一个 (Nxr) 矩阵和一个 (rxN) 矩阵的乘积。
- en: LoRA uses the concept of matrix rank to fine-tune using less memory. Instead
    of adjusting all weights of an LLM, LoRA fine-tunes low-rank matrices and adds
    them to the existing weights [2]. The existing weights (or the large matrices)
    stay the same, while training adjusts only the low-rank matrices.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA 利用矩阵秩的概念来使用更少的内存进行微调。LoRA 并不是调整 LLM 的所有权重，而是对低秩矩阵进行微调，并将其添加到现有的权重中 [2]。现有的权重（或大矩阵）保持不变，而训练仅调整低秩矩阵。
- en: Why is this efficient? A low-rank matrix has significantly fewer parameters.
    Instead of managing N² parameters, with LoRA, one only needs to handle 2*r*N parameters.
    Intuitively, fine-tuning is like making slight adjustments to the original matrix.
    LoRA determines these adjustments in a computationally cheaper way, trading off
    some accuracy for efficiency.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这效率高？低秩矩阵的参数显著较少。与管理 N² 个参数相比，使用 LoRA 只需处理 2*r*N 个参数。直观地说，微调就像对原始矩阵进行轻微调整。LoRA
    以计算成本较低的方式确定这些调整，以换取一些准确度来提高效率。
- en: Training with LoRA still requires the entire pre-trained model for the forward
    pass, accompanied by additional LoRA computations. Nevertheless, during the backward
    propagation, calculations are focused mainly on the gradients of the LoRA section.
    This approach results in computational savings, especially in GPU memory requirements.
    That is why it is currently one of the most popular methods for adapting models
    to new tasks without the extensive computational overhead of traditional fine-tuning.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 LoRA 进行训练仍然需要整个预训练模型进行前向传递，伴随额外的 LoRA 计算。然而，在反向传播过程中，计算主要集中在 LoRA 部分的梯度上。这种方法在
    GPU 内存需求方面带来了计算节省。这就是为什么它目前是将模型调整到新任务的最流行的方法之一，而无需传统微调的广泛计算开销。
- en: To make it even more memory efficient, we can use QLoRA [3]. It builds on top
    of LoRA and enables the usage of these adapters with quantized pre-trained models.
    In practical terms, this method allows for fine-tuning a 65B parameter model on
    a 48GB GPU while retaining the performance of a full 16-bit fine-tuning task.
    QLoRA also introduced other features to enhance memory efficiency. The 4-bit NormalFloat
    (NF4) data type offers a more compact representation for normally distributed
    weights. It also employed Double Quantization to further minimize memory usage
    by quantizing the quantization constants themselves (think of turtles all the
    way down but with quantization). The last feature was the Paged Optimizers to
    manage memory spikes.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高内存效率，我们可以使用 QLoRA [3]。它建立在 LoRA 的基础上，允许使用这些适配器与量化的预训练模型。实际上，这种方法使得在 48GB
    GPU 上对 65B 参数模型进行微调成为可能，同时保持了全 16 位微调任务的性能。QLoRA 还引入了其他特性来提高内存效率。4 位 NormalFloat
    (NF4) 数据类型提供了对正态分布权重的更紧凑表示。它还采用了双重量化，通过对量化常量本身进行量化（可以把它想象成“一层层的海龟”但加上量化）来进一步减少内存使用。最后一个特性是分页优化器来管理内存峰值。
- en: Curating a Dataset Suited for Python Programming Tasks
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为 Python 编程任务策划适用的数据集
- en: We start by defining a Config class that serves as a centralized repository
    for configuration settings and metadata related to our fine-tuning process. It
    stores various constants, such as the model and dataset names, output directories,
    and several parameters, which we will discuss in the upcoming sections. Let’s
    see the relevant variables for our data pre-processing.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始定义一个 Config 类，作为与微调过程相关的配置设置和元数据的集中存储库。它存储各种常量，如模型和数据集名称、输出目录和几个参数，这些我们将在接下来的部分中讨论。让我们看看与数据预处理相关的变量。
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The dataset selection is quite important when tailoring a model like Llama2
    for Python-centric tasks. We are using the [Python Questions from StackOverflow
    Dataset](https://www.kaggle.com/datasets/stackoverflow/pythonquestions) ([CC-BY-SA
    3.0](https://creativecommons.org/licenses/by-sa/3.0/)) dataset, which comprises
    a vast selection of coding interactions with the Python tag. Since we want to
    fine-tune our model in coding in Python, we refined this dataset, focusing specifically
    on Python-related exchanges.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当为像 Llama2 这样的模型量身定制以处理以 Python 为中心的任务时，数据集的选择非常重要。我们正在使用 [Python Questions
    from StackOverflow Dataset](https://www.kaggle.com/datasets/stackoverflow/pythonquestions)
    ([CC-BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/)) 数据集，该数据集包含大量带有
    Python 标签的编码互动。由于我们希望对模型进行 Python 编程方面的微调，因此我们对该数据集进行了优化，专注于 Python 相关的交流。
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In the next step, we ensure our data aligns with Llama2’s prompt structure:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们确保我们的数据符合 Llama2 的提示结构：
- en: '*<s>[INST] <<SYS>> {{ system_prompt }} <</SYS>> {{ user_message }} [/INST]*'
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*<s>[INST] <<SYS>> {{ system_prompt }} <</SYS>> {{ user_message }} [/INST]*'
- en: The above structure aligns with the training procedure of the model, and thus
    it significantly impacts the fine-tuning quality. Recall that ‘system_prompt’
    represents the instructions or context for the model. The user’s message follows
    the system prompt and seeks a specific response from the model.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 上述结构与模型的训练过程一致，因此对微调质量有显著影响。请记住，‘system_prompt’ 代表模型的指令或上下文。用户的消息紧随系统提示，并寻求模型的特定响应。
- en: We tailor each data entry to carry explicit system instructions, guiding the
    model during training.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为每个数据条目定制了明确的系统指令，以指导模型在训练期间。
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Once we’ve transformed our dataset to align with Llama2’s prompt structure,
    we leverage the Hugging Face platform to store it. We split the dataset, setting
    1,000 entries for validation purposes, which will be helpful later. For enthusiasts
    and researchers, we’ve encapsulated our refined dataset under the name [*luisroque/instruct-python-llama2–20k*](https://huggingface.co/datasets/luisroque/instruct-python-llama2-20k)and
    a bigger one under the name [*luisroque/instruct-python-llama2–500k*](https://huggingface.co/datasets/luisroque/instruct-python-llama2-500k),
    which are publicly available on the Hugging Face Hub.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们将数据集转换为符合Llama2提示结构的格式，我们利用Hugging Face平台进行存储。我们拆分了数据集，为验证目的设置了1000条记录，这在后续会有所帮助。对于爱好者和研究人员，我们将精炼的数据集命名为[*luisroque/instruct-python-llama2–20k*](https://huggingface.co/datasets/luisroque/instruct-python-llama2-20k)，更大的数据集命名为[*luisroque/instruct-python-llama2–500k*](https://huggingface.co/datasets/luisroque/instruct-python-llama2-500k)，这些都在Hugging
    Face Hub上公开提供。
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Fine-tuning Llama2 Using Supervised Fine-Tuning (SFT) and Quantized Low-Rank
    Adaptation (QLoRA)
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用监督微调（SFT）和量化低秩适应（QLoRA）对Llama2进行微调。
- en: When selecting hyperparameters for fine-tuning Llama2, we want to balance efficiency
    and effectiveness. We want to ensure a quick experimentation cycle and, thus,
    we defined just one `epoch` and a modest `batch size` of 2\. After some tests,
    we chose a `learning rate` of 2e-4, since it converges well for our use case.
    The `weight decay` of 0.001 helps in regularizing and preventing overfitting.
    Given the complexity of the LLM, we've opted for a maximum gradient norm of 0.3
    to prevent excessively large updates during training. The scheduler's `cosine`
    nature ensures learning rate annealing for stable convergence, while our optimizer,
    `paged_adamw_32bit`, introduced by the QLoRA paper, offers fewer memory spikes.
    We also employed 4-bit quantization to enhance memory efficiency further, selecting
    the `nf4` type for quantization (another addition of the QLoRA paper). Lastly,
    the LoRA-specific parameters, with an `alpha` of 16, `dropout` of 0.1, and `rank`
    of 64, were also selected based on empirical experimentation.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在为Llama2进行微调选择超参数时，我们希望平衡效率和效果。我们要确保快速的实验周期，因此我们只定义了一个`epoch`和一个适中的`batch size`为2。在一些测试后，我们选择了`learning
    rate`为2e-4，因为它在我们的用例中收敛良好。`weight decay`为0.001有助于正则化并防止过拟合。鉴于LLM的复杂性，我们选择了最大梯度范数为0.3，以防止训练期间的更新过大。调度器的`cosine`特性确保了学习率的退火以稳定收敛，而我们的优化器`paged_adamw_32bit`由QLoRA论文引入，提供了较少的内存波动。我们还采用了4-bit量化进一步提高内存效率，选择了`nf4`类型进行量化（这是QLoRA论文的另一个补充）。最后，LoRA特定的参数，包括`alpha`为16，`dropout`为0.1，以及`rank`为64，也是根据经验实验选择的。
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the `initialize_model_and_tokenizer` function, we set the compute datatype
    using `Config.BNB_4BIT_COMPUTE_DTYPE` to optimize for 4-bit quantization. We then
    configure this quantization using `BitsAndBytesConfig`. We load the base pre-trained
    Llama2 model with `AutoModelForCausalLM` and initialize it with our quantization
    configuration, turning off caching to conserve memory. We map the model to a single
    GPU, but we could easily modify this configuration for a multi-GPU setup. We then
    fetch the tokenizer, which translates the inputs for the model.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在`initialize_model_and_tokenizer`函数中，我们使用`Config.BNB_4BIT_COMPUTE_DTYPE`设置计算数据类型，以优化4-bit量化。然后使用`BitsAndBytesConfig`配置这种量化。我们使用`AutoModelForCausalLM`加载基础预训练Llama2模型，并使用我们的量化配置初始化它，关闭缓存以节省内存。我们将模型映射到单个GPU上，但可以很容易地修改此配置以支持多GPU设置。然后我们获取tokenizer，它将输入翻译给模型。
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We can print our models to make sure that we loaded them correctly. Let’s start
    by loading the pre-trained Llama2 model with 4-bit quantization. Note that the
    all the layers were quantized correctly.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以打印我们的模型，以确保它们正确加载。让我们开始加载经过4-bit量化的预训练Llama2模型。注意，所有层都已正确量化。
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As a side note, we can get an idea of the Llama2 architecture by the printing
    information above. It uses an embedding layer transforming up to 32,000 tokens
    into 4,096-dimensional vectors. The model’s computational engine comprises 32
    sequential `LlamaDecoderLayer` modules. Within each decoder layer, the `LlamaAttention`
    mechanism operates with 4-bit precision linear projections for the query, key,
    value, and output. The attention mechanism uses rotary embeddings, which are used
    to dynamically capture positional information in sequence data. Alongside the
    attention mechanism, it features 4-bit linear projections and leverages the Sigmoid
    Linear Unit (SiLU) activation function for non-linear transformations. To ensure
    consistent activations across layers, the model incorporates `LlamaRMSNorm` for
    layer normalization for post-input and post-attention. The last linear layer transforms
    the high-dimensional representations back to the 32,000-token vocabulary size,
    which is what enables the token prediction.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 作为附注，我们可以通过上述打印信息了解Llama2的架构。它使用一个嵌入层将最多32,000个令牌转换为4,096维的向量。该模型的计算引擎由32个顺序的`LlamaDecoderLayer`模块组成。在每个解码器层中，`LlamaAttention`机制以4位精度的线性投影进行查询、键、值和输出。注意力机制使用旋转嵌入，这些嵌入用于动态捕捉序列数据中的位置信息。除了注意力机制，它还具有4位线性投影，并利用Sigmoid线性单元（SiLU）激活函数进行非线性变换。为了确保跨层一致的激活，模型结合了`LlamaRMSNorm`进行输入后和注意力后层归一化。最后一个线性层将高维表示转换回32,000令牌的词汇大小，这使得令牌预测成为可能。
- en: We are now ready to use QLoRA to fine-tune Llama2 on our dataset. First, we
    configure the model with the previously defined LoRA settings. We then prepare
    the model for 4-bit training and integrate it with the LoRA configurations. We
    set the training parameters and feed them into the SFTTrainer for fine-tuning.
    The `configure_training_args` function defines the training parameters for the
    model, referencing the `Config` class that we already discussed. After training,
    we save the model and tokenizer in a specified directory and test the model’s
    performance using a generation task. Following good practices, we clear the model
    from memory and empty the GPU cache. We also decorated our function to monitor
    both the execution time and the memory consumption.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用QLoRA对Llama2进行微调。首先，我们使用之前定义的LoRA设置配置模型。然后，我们将模型准备好进行4位训练，并将其与LoRA配置集成。我们设置训练参数，并将它们输入到SFTTrainer进行微调。`configure_training_args`函数定义了模型的训练参数，参考了我们之前讨论的`Config`类。训练完成后，我们将模型和分词器保存在指定目录中，并使用生成任务测试模型的性能。按照良好实践，我们清除了模型的内存并清空了GPU缓存。我们还装饰了我们的函数，以监控执行时间和内存消耗。
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Once again, we can print our model to ensure we have correctly set up the LoRA
    parameters and the quantization. Recall that by introducing a set of new, low-rank
    trainable parameters, LoRA creates a bottleneck in the model where representations
    are channeled through these parameters. Note that the LoRA components, notably
    the `lora_A` and `lora_B` linear layers, are integrated into the attention mechanism.
    Only these LoRA parameters are actively trained during fine-tuning, preserving
    the model’s original knowledge while optimizing it for the new task. The default
    configuration for LoRA applies them to the`q_proj` (query projection) and `v_proj`
    (value projection) within the attention mechanism to make the process more efficient.
    The LoRA paper [3] actually applied it to all the layers, so these can also be
    experimented with.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以再次打印模型，以确保我们正确设置了LoRA参数和量化。回顾一下，通过引入一组新的、低秩的可训练参数，LoRA在模型中创建了一个瓶颈，将表示通过这些参数进行通道化。请注意，LoRA组件，特别是`lora_A`和`lora_B`线性层，已经集成到注意力机制中。只有这些LoRA参数在微调期间被积极训练，从而保留模型的原始知识，同时优化它以适应新任务。LoRA的默认配置将它们应用于注意力机制中的`q_proj`（查询投影）和`v_proj`（值投影），以提高效率。LoRA论文[3]实际上将其应用于所有层，因此也可以尝试这些层。
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We can quickly get the number of trainable parameters and check how it compares
    to the overall number of parameters in the pre-trained model. We can see that
    we are training less than 1% of the parameters.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以快速获取可训练参数的数量，并检查它与预训练模型中总参数数量的比较。我们可以看到，我们训练的参数不到1%。
- en: '[PRE9]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The final step is to merge the new weights with the base model. This can be
    accomplished simply by loading both instances and calling the `merge_and_unload()`
    method .
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是将新权重与基础模型合并。这可以通过简单地加载两个实例并调用`merge_and_unload()`方法来完成。
- en: '[PRE10]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: After fine-tuning, the trained model and tokenizer can be easily shared in the
    Hugging Face Hub, promoting collaboration and reusability. You can find our fine-tuned
    model at [*luisroque/Llama-2-7b-minipython-instruct*](https://huggingface.co/luisroque/Llama-2-7b-minipython-instruct)*.*
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 微调后，训练好的模型和分词器可以轻松地分享到 Hugging Face Hub，促进协作和可重用性。你可以在 [*luisroque/Llama-2-7b-minipython-instruct*](https://huggingface.co/luisroque/Llama-2-7b-minipython-instruct)*.*
    找到我们的微调模型。
- en: '[PRE11]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Inference Process Using Llama2 and Fine-Tuned Models**'
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**使用 Llama2 和微调模型的推断过程**'
- en: The final step in our long task of fine-tuning Llama2 is to test it. We have
    implemented an easy way to run inference for the base model and for the fine-tuned
    one to help compare the two.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们漫长的 Llama2 微调任务中，最后一步是进行测试。我们实现了一种简单的方法来运行基础模型和微调模型的推断，以帮助比较这两者。
- en: The function `generate_response` is responsible for the actual inference. It
    employs Hugging Face’s pipeline utility to generate text based on a given prompt,
    model, and tokenizer. If the fine-tine model is already in the Hugging Face Hub
    or stored locally, we don’t need to merge them once again, you can just access
    it directly.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 `generate_response` 负责实际的推断。它利用 Hugging Face 的 pipeline 工具，根据给定的提示、模型和分词器生成文本。如果微调后的模型已经在
    Hugging Face Hub 中或存储在本地，我们不需要再次合并，只需直接访问即可。
- en: '[PRE12]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We defined the script’s entry point to be command-line based. Users can specify
    their model preference through arguments, either “new_model” or “llama2”, enabling
    easy toggling between models and directly comparing their inference outputs.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将脚本的入口点定义为基于命令行的。用户可以通过参数指定他们的模型偏好，"new_model" 或 "llama2"，使得在模型之间轻松切换，并直接比较它们的推断输出。
- en: '[PRE13]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '**Simplifying the Workflow with a Makefile**'
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**简化工作流程的 Makefile**'
- en: Automation plays a big role in streamlining complex processes, especially in
    machine learning tasks with multiple sequential steps. A makefile is a great tool
    to help us provide a clear, structured, and easy-to-execute workflow for users.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化在简化复杂流程中发挥了重要作用，特别是在具有多个顺序步骤的机器学习任务中。makefile 是一个很好的工具，帮助我们为用户提供一个清晰、结构化且易于执行的工作流程。
- en: In the provided makefile, each step of the fine-tuning process, from setting
    up the environment to running inference, is defined as a separate target. This
    abstraction allows users to execute specific tasks with a single, concise command.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在提供的 makefile 中，微调过程的每一步，从设置环境到运行推断，都被定义为一个单独的目标。这种抽象允许用户通过单个、简洁的命令来执行特定任务。
- en: 'Here’s an example of how the user can run the different tasks using the provided
    makefile:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是用户如何使用提供的 makefile 运行不同任务的示例：
- en: '[PRE14]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This command will execute the setup target, creating a new conda environment
    named fine_tune_llama2 with Python 3.10.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令将执行 setup 目标，创建一个名为 fine_tune_llama2 的新 conda 环境，使用 Python 3.10。
- en: '[PRE15]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The install target will install the necessary packages from the requirements.txt
    file. The same applies for the rest of the commands.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: install 目标将从 requirements.txt 文件中安装必要的包。其余命令也是如此。
- en: For running the complete process from setup to inference in one
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 从设置到推断的一整个过程
- en: '[PRE16]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The `all` target, as defined, will sequentially run all the specified targets.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`all` 目标，如定义，将顺序运行所有指定的目标。'
- en: The use of the makefile not only simplifies the execution of tasks but also
    provides a standardized way to run the process, ensuring consistency and reproducibility.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 makefile 不仅简化了任务执行，还提供了一种标准化的方式来运行流程，确保一致性和可重现性。
- en: '[PRE17]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Results
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: Now it is time to compare the results from the baseline model and the new model
    that we just fine-tuned using a Python instructions dataset.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候使用 Python 指令数据集比较基线模型和我们刚刚微调的新模型的结果了。
- en: 'The prompt that we defined is the same for both models:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义的提示对于两种模型都是相同的：
- en: '[INST] <<SYS>>'
  id: totrans-78
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[INST] <<SYS>>'
- en: Given a puzzle-like code question, provide a well-reasoned, step-by-step Python
    solution.
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 给定一个类似谜题的代码问题，提供一个有理有据、逐步的 Python 解决方案。
- en: <</SYS>>
  id: totrans-80
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: <</SYS>>
- en: ''
  id: totrans-81
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Write a function that reverses a linked list. [/INST]
  id: totrans-82
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 编写一个函数，反转链表。 [/INST]
- en: The answer from the base model is below. It did not follow the step-by-step
    approach giving the proper context for the solution provided. Also, there is a
    minor bug with the function itself with what the function returns. Finally, the
    model continues generating random requests similar to the one we provided in the
    prompt. It simulates a new one to “Write a function that counts the number of
    nodes in a linked list.” and answers it. Then, it goes on with the same prompt
    and answer until the token limit that we defined was reached.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型的回答如下。它没有遵循逐步的方法来提供正确的上下文。还有一个小错误在于函数的返回值。最后，模型继续生成类似于我们在提示中提供的随机请求。它模拟了一个“编写一个函数来计算链表中的节点数”的新请求并给出答案。然后，它继续用相同的提示和答案，直到达到我们定义的令牌限制。
- en: '[PRE18]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: <</SYS>>
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: <</SYS>>
- en: Write a function that counts the number of nodes in a linked list. [/INST]
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 编写一个函数来计算链表中的节点数。[/INST]
- en: Write a function that counts the number of nodes in a linked list.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 编写一个函数来计算链表中的节点数。
- en: '"`python'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '"`python'
- en: 'def count_nodes(head):'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 'def count_nodes(head):'
- en: count = 0
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: count = 0
- en: curr = head
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: curr = head
- en: 'while curr:'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'while curr:'
- en: count += 1
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: count += 1
- en: curr = curr.next
  id: totrans-94
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: curr = curr.next
- en: return count
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: return count
- en: '[PRE19]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The new model answered quite differently. Notice that it follows our request
    much closer. While there are also inconsistencies with the code (e.g., the circular
    reference current.next = current), it provides a much more elegant solution. It
    uses classes to build the node and the linked list and defines a method on the
    latter to reverse it. Notice that it also explains each code step as we prompt
    it to do.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 新模型的回答有很大不同。请注意，它更接近我们的请求。尽管代码中仍有一些不一致之处（例如，循环引用current.next = current），但它提供了一个更优雅的解决方案。它使用类来构建节点和链表，并在链表上定义了一个方法来反转它。请注意，它还解释了每一步代码，如我们所提示的那样。
- en: '[PRE20]python'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE20]python'
- en: 'class Node:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 'class Node:'
- en: 'def __init__(self, data):'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'def __init__(self, data):'
- en: self.data = data
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: self.data = data
- en: self.next = None
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: self.next = None
- en: 'class LinkedList:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 'class LinkedList:'
- en: 'def __init__(self):'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'def __init__(self):'
- en: self.head = None
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: self.head = None
- en: 'def reverse(self):'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'def reverse(self):'
- en: current = self.head
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: current = self.head
- en: next = None
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: next = None
- en: 'while current:'
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'while current:'
- en: next = current.next
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: next = current.next
- en: current.next = current
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: current.next = current
- en: current = next
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: current = next
- en: self.head = current
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: self.head = current
- en: '[PRE21]python'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE21]python'
- en: linked_list = LinkedList()
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: linked_list = LinkedList()
- en: linked_list.append(1)
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: linked_list.append(1)
- en: linked_list.append(2)
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: linked_list.append(2)
- en: linked_list.append(3)
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: linked_list.append(3)
- en: linked_list.append(4)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: linked_list.append(4)
- en: linked_list.reverse()
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: linked_list.reverse()
- en: print(linked_list.head)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: print(linked_list.head)
- en: '[PRE22]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '4'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '4'
- en: '[PRE23]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'These results show that we have successfully fine-tuned Llama2 as a better
    Python coding assistant model. There are still inconsistencies, but we need to
    consider the following:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果表明，我们成功地将Llama2精调为更好的Python编码助手模型。虽然仍然存在不一致，但我们需要考虑以下几点：
- en: We are using the smallest Llama2 model (7b);
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用了最小的Llama2模型（7b）；
- en: We have fine-tuned it for only 2,500 steps;
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们仅为2,500步进行了精调；
- en: We have used the maximum quantization possible (4-bit);
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用了最大量化（4-bit）；
- en: We have only retrained a very small percentage of the model weights.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们仅重新训练了模型权重的非常小的一部分。
- en: Feel free to test the model yourself, it is stored at [https://huggingface.co/luisroque/Llama-2-7b-minipython-instruct](https://huggingface.co/luisroque/Llama-2-7b-minipython-instruct).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎你自己测试这个模型，它存储在[https://huggingface.co/luisroque/Llama-2-7b-minipython-instruct](https://huggingface.co/luisroque/Llama-2-7b-minipython-instruct)。
- en: Conclusions
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In our quest to fine-tune Llama2 for coding tasks in Python, we first curated
    a dataset tailored for Python interactions. We then employed SFT since this type
    of fine-tuning allows for more instruction-following capabilities. Instead of
    adjusting all model weights, we used LoRA that offers a more efficient approach
    by fine-tuning low-rank matrices instead. With Quantized LoRA, we achieved further
    memory efficiency, making it possible to fine-tune large models on standard GPU
    configurations.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们为Python编码任务精调Llama2的过程中，我们首先策划了一个针对Python交互的数据集。然后，我们采用了SFT，因为这种精调方式能够更好地跟随指令。我们没有调整所有的模型权重，而是使用了LoRA，它提供了一种更高效的方法，通过精调低秩矩阵来实现。通过量化LoRA，我们进一步提高了内存效率，使得可以在标准GPU配置上对大型模型进行精调。
- en: After optimization, we merged our model’s weights with the foundational Llama2
    and also implemented a makefile to simplify our workflow while ensuring replicability
    and ease of execution for new users.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 优化后，我们将模型权重与基础Llama2进行了合并，并实施了makefile，以简化工作流程，同时确保了新用户的可重复性和执行的便捷性。
- en: After having our fine-tuned Llama2 model, we performed inference using the same
    prompt for both models. Our side-by-side comparison clearly showed the impact
    of the fine-tuning process. The refined model adhered more accurately to instructions,
    produced better-structured code, and offered explanations for each implementation
    step.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用我们微调后的 Llama2 模型后，我们使用相同的提示对两个模型进行了推理。我们的并排比较清楚地显示了微调过程的影响。经过精细调整的模型更准确地遵循了指令，生成了结构更好的代码，并为每个实施步骤提供了解释。
- en: 'Large Language Models Chronicles: Navigating the NLP Frontier'
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**大型语言模型编年史：探索 NLP 前沿**'
- en: 'This article belongs to “Large Language Models Chronicles: Navigating the NLP
    Frontier”, a new weekly series of articles that will explore how to leverage the
    power of large models for various NLP tasks. By diving into these cutting-edge
    technologies, we aim to empower developers, researchers, and enthusiasts to harness
    the potential of NLP and unlock new possibilities.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 本文属于“**大型语言模型编年史：探索 NLP 前沿**”，这是一个新的每周系列文章，旨在探索如何利用大型模型的力量完成各种 NLP 任务。通过深入研究这些前沿技术，我们希望赋能开发者、研究人员和爱好者，挖掘
    NLP 的潜力，开启新的可能性。
- en: 'Articles published so far:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 目前已发布的文章：
- en: '[Summarizing the latest Spotify releases with ChatGPT](https://medium.com/towards-data-science/summarizing-the-latest-spotify-releases-with-chatgpt-553245a6df88)'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[用 ChatGPT 总结最新的 Spotify 发布]([https://medium.com/towards-data-science/summarizing-the-latest-spotify-releases-with-chatgpt-553245a6df88](https://medium.com/towards-data-science/summarizing-the-latest-spotify-releases-with-chatgpt-553245a6df88))'
- en: '[Master Semantic Search at Scale: Index Millions of Documents with Lightning-Fast
    Inference Times using FAISS and Sentence Transformers](https://medium.com/towards-data-science/master-semantic-search-at-scale-index-millions-of-documents-with-lightning-fast-inference-times-fa395e4efd88)'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[掌握大规模语义搜索：使用 FAISS 和 Sentence Transformers 索引数百万个文档，实现闪电般的推理时间]([https://medium.com/towards-data-science/master-semantic-search-at-scale-index-millions-of-documents-with-lightning-fast-inference-times-fa395e4efd88](https://medium.com/towards-data-science/master-semantic-search-at-scale-index-millions-of-documents-with-lightning-fast-inference-times-fa395e4efd88))'
- en: '[Unlock the Power of Audio Data: Advanced Transcription and Diarization with
    Whisper, WhisperX, and PyAnnotate](https://medium.com/towards-data-science/unlock-the-power-of-audio-data-advanced-transcription-and-diarization-with-whisper-whisperx-and-ed9424307281)'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[释放音频数据的力量：使用 Whisper、WhisperX 和 PyAnnotate 进行高级转录和区分]([https://medium.com/towards-data-science/unlock-the-power-of-audio-data-advanced-transcription-and-diarization-with-whisper-whisperx-and-ed9424307281](https://medium.com/towards-data-science/unlock-the-power-of-audio-data-advanced-transcription-and-diarization-with-whisper-whisperx-and-ed9424307281))'
- en: '[Whisper JAX vs PyTorch: Uncovering the Truth about ASR Performance on GPUs](https://medium.com/towards-data-science/whisper-jax-vs-pytorch-uncovering-the-truth-about-asr-performance-on-gpus-8794ba7a42f5)'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Whisper JAX 与 PyTorch：揭示 GPU 上 ASR 性能的真相]([https://medium.com/towards-data-science/whisper-jax-vs-pytorch-uncovering-the-truth-about-asr-performance-on-gpus-8794ba7a42f5](https://medium.com/towards-data-science/whisper-jax-vs-pytorch-uncovering-the-truth-about-asr-performance-on-gpus-8794ba7a42f5))'
- en: '[Vosk for Efficient Enterprise-Grade Speech Recognition: An Evaluation and
    Implementation Guide](https://medium.com/towards-data-science/vosk-for-efficient-enterprise-grade-speech-recognition-an-evaluation-and-implementation-guide-87a599217a6c)'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Vosk 以提高企业级语音识别效率：评估和实施指南]([https://medium.com/towards-data-science/vosk-for-efficient-enterprise-grade-speech-recognition-an-evaluation-and-implementation-guide-87a599217a6c](https://medium.com/towards-data-science/vosk-for-efficient-enterprise-grade-speech-recognition-an-evaluation-and-implementation-guide-87a599217a6c))'
- en: '[Testing the Massively Multilingual Speech (MMS) Model that Supports 1162 Languages](https://medium.com/towards-data-science/testing-the-massively-multilingual-speech-mms-model-that-supports-1162-languages-5db957ee1602)'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[测试支持 1162 种语言的大规模多语言语音 (MMS) 模型]([https://medium.com/towards-data-science/testing-the-massively-multilingual-speech-mms-model-that-supports-1162-languages-5db957ee1602](https://medium.com/towards-data-science/testing-the-massively-multilingual-speech-mms-model-that-supports-1162-languages-5db957ee1602))'
- en: '[Harnessing the Falcon 40B Model, the Most Powerful Open-Source LLM](https://medium.com/towards-data-science/harnessing-the-falcon-40b-model-the-most-powerful-open-source-llm-f70010bc8a10)'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[利用 Falcon 40B 模型，最强大的开源 LLM]([https://medium.com/towards-data-science/harnessing-the-falcon-40b-model-the-most-powerful-open-source-llm-f70010bc8a10](https://medium.com/towards-data-science/harnessing-the-falcon-40b-model-the-most-powerful-open-source-llm-f70010bc8a10))'
- en: '[The Power of OpenAI’s Function Calling in Language Learning Models: A Comprehensive
    Guide](https://medium.com/towards-data-science/the-power-of-openais-function-calling-in-language-learning-models-a-comprehensive-guide-cce8cd84dc3c)'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[OpenAI 功能调用在语言学习模型中的力量：全面指南]([https://medium.com/towards-data-science/the-power-of-openais-function-calling-in-language-learning-models-a-comprehensive-guide-cce8cd84dc3c](https://medium.com/towards-data-science/the-power-of-openais-function-calling-in-language-learning-models-a-comprehensive-guide-cce8cd84dc3c))'
- en: '[Document-Oriented Agents: A Journey with Vector Databases, LLMs, Langchain,
    FastAPI, and Docker](/document-oriented-agents-a-journey-with-vector-databases-llms-langchain-fastapi-and-docker-be0efcd229f4)'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[面向文档的智能体：与向量数据库、LLMs、Langchain、FastAPI 和 Docker 的旅程](/document-oriented-agents-a-journey-with-vector-databases-llms-langchain-fastapi-and-docker-be0efcd229f4)'
- en: '[Leveraging Llama 2 Features in Real-world Applications: Building Scalable
    Chatbots with FastAPI, Celery, Redis, and Docker](https://medium.com/towards-data-science/leveraging-llama-2-features-in-real-world-applications-building-scalable-chatbots-with-fastapi-406f1cbeb935)'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[在实际应用中利用 Llama 2 特性：使用 FastAPI、Celery、Redis 和 Docker 构建可扩展的聊天机器人](https://medium.com/towards-data-science/leveraging-llama-2-features-in-real-world-applications-building-scalable-chatbots-with-fastapi-406f1cbeb935)'
- en: As always, the code is available on my [Github](https://github.com/luisroque/large_laguage_models).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，代码可在我的 [Github](https://github.com/luisroque/large_laguage_models) 上找到。
- en: References
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] — HuggingFace. (n.d.). Open LLM Leaderboard. Retrieved August 14, 2023,
    from [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] — HuggingFace. (无日期). 开放 LLM 排行榜. 访问于 2023年8月14日，网址：[https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)'
- en: '[2] — Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang,
    L., & Chen, W. (2021). LoRA: Low-Rank Adaptation of Large Language Models. *arXiv
    preprint arXiv:2106.09685*.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] — Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang,
    L., & Chen, W. (2021). LoRA: 大型语言模型的低秩适配. *arXiv 预印本 arXiv:2106.09685*'
- en: '[3] — Dettmers, T., Pagnoni, A., Holtzman, A., & Zettlemoyer, L. (2023). QLoRA:
    Efficient Finetuning of Quantized LLMs. *arXiv preprint arXiv:2305.14314*.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] — Dettmers, T., Pagnoni, A., Holtzman, A., & Zettlemoyer, L. (2023). QLoRA:
    高效量化 LLM 微调. *arXiv 预印本 arXiv:2305.14314*.'
- en: 'Keep in touch: [LinkedIn](https://www.linkedin.com/in/luisbrasroque/)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 保持联系：[LinkedIn](https://www.linkedin.com/in/luisbrasroque/)
