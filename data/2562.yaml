- en: 'HashGNN: Deep Dive into Neo4j GDS’s New Node Embedding Algorithm'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'HashGNN: 深入探讨 Neo4j GDS 的新节点嵌入算法'
- en: 原文：[https://towardsdatascience.com/hashgnn-deep-dive-into-neo4j-gdss-new-node-embedding-algorithm-5ce9c3029a5c?source=collection_archive---------7-----------------------#2023-08-10](https://towardsdatascience.com/hashgnn-deep-dive-into-neo4j-gdss-new-node-embedding-algorithm-5ce9c3029a5c?source=collection_archive---------7-----------------------#2023-08-10)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/hashgnn-deep-dive-into-neo4j-gdss-new-node-embedding-algorithm-5ce9c3029a5c?source=collection_archive---------7-----------------------#2023-08-10](https://towardsdatascience.com/hashgnn-deep-dive-into-neo4j-gdss-new-node-embedding-algorithm-5ce9c3029a5c?source=collection_archive---------7-----------------------#2023-08-10)
- en: In this article, we will explore alongside a small example how HashGNN hashes
    graph nodes into an embedding space.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将通过一个小示例来探讨 HashGNN 如何将图节点哈希到嵌入空间。
- en: '[](https://medium.com/@philipp.brunenberg?source=post_page-----5ce9c3029a5c--------------------------------)[![Philipp
    Brunenberg](../Images/b2384bcc51966f669f84c949a33ebfcc.png)](https://medium.com/@philipp.brunenberg?source=post_page-----5ce9c3029a5c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5ce9c3029a5c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5ce9c3029a5c--------------------------------)
    [Philipp Brunenberg](https://medium.com/@philipp.brunenberg?source=post_page-----5ce9c3029a5c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@philipp.brunenberg?source=post_page-----5ce9c3029a5c--------------------------------)[![Philipp
    Brunenberg](../Images/b2384bcc51966f669f84c949a33ebfcc.png)](https://medium.com/@philipp.brunenberg?source=post_page-----5ce9c3029a5c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5ce9c3029a5c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5ce9c3029a5c--------------------------------)
    [Philipp Brunenberg](https://medium.com/@philipp.brunenberg?source=post_page-----5ce9c3029a5c--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6edbaee01d5d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhashgnn-deep-dive-into-neo4j-gdss-new-node-embedding-algorithm-5ce9c3029a5c&user=Philipp+Brunenberg&userId=6edbaee01d5d&source=post_page-6edbaee01d5d----5ce9c3029a5c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5ce9c3029a5c--------------------------------)
    ·9 min read·Aug 10, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5ce9c3029a5c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhashgnn-deep-dive-into-neo4j-gdss-new-node-embedding-algorithm-5ce9c3029a5c&user=Philipp+Brunenberg&userId=6edbaee01d5d&source=-----5ce9c3029a5c---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6edbaee01d5d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhashgnn-deep-dive-into-neo4j-gdss-new-node-embedding-algorithm-5ce9c3029a5c&user=Philipp+Brunenberg&userId=6edbaee01d5d&source=post_page-6edbaee01d5d----5ce9c3029a5c---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5ce9c3029a5c--------------------------------)
    ·9分钟阅读·2023年8月10日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F5ce9c3029a5c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhashgnn-deep-dive-into-neo4j-gdss-new-node-embedding-algorithm-5ce9c3029a5c&user=Philipp+Brunenberg&userId=6edbaee01d5d&source=-----5ce9c3029a5c---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5ce9c3029a5c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhashgnn-deep-dive-into-neo4j-gdss-new-node-embedding-algorithm-5ce9c3029a5c&source=-----5ce9c3029a5c---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F5ce9c3029a5c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fhashgnn-deep-dive-into-neo4j-gdss-new-node-embedding-algorithm-5ce9c3029a5c&source=-----5ce9c3029a5c---------------------bookmark_footer-----------)'
- en: If you prefer watching a video on this, you can do so [here](https://youtu.be/fccFuyjNEcM).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你更喜欢观看视频，你可以[点击这里](https://youtu.be/fccFuyjNEcM)。
- en: HashGG (#GNN) is a node embedding technique, which employs concepts of Message
    Passing Neural Networks (MPNN) to capture high-order proximity and node properties.
    It significantly speeds-up calculation in comparison to traditional Neural Networks
    by utilizing an approximation technique called MinHashing. Therefore, it is a
    hash-based approach and introduces a trade-off between efficiency and accuracy.
    In this article, we will understand what all of that means and will explore along
    a small example how the algorithms works.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: HashGG（#GNN）是一种节点嵌入技术，它采用了消息传递神经网络（MPNN）的概念来捕捉高阶邻近性和节点属性。通过利用一种称为MinHashing的近似技术，它显著加快了计算速度，相比于传统神经网络。因此，它是一种基于哈希的方法，在效率和准确性之间引入了权衡。本文将深入理解这些内容，并通过一个小示例探索算法的工作原理。
- en: 'Node Embeddings: Nodes with similar contexts should be close in the embedding
    space'
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 节点嵌入：具有相似上下文的节点在嵌入空间中应当接近
- en: 'Many graph machine learning use-cases like link prediction and node classification
    require the calculation of similarities of nodes. In a graph context, these similarities
    are most expressive when they capture (i) the neighborhood (i.e. the graph structure)
    and (ii) the properties of the node to be embedded. Node embedding algorithms
    project nodes into a low-dimensional embedding space — i.e. they assign each node
    a numerical vector. These vectors — the embeddings — can be used for further numerical
    predictive analysis (e.g. machine learning algorithms). Embedding algorithms optimize
    for the metric: Nodes with a similar graph context (neighborhood) and/or properties
    should be mapped close in the embedding space. Graph embedding algorithms usually
    employ two fundamental steps: (i) Define a mechanism to sample the context of
    nodes (Random walk in node2vec, k-fold transition matrix in FastRP), and (ii)
    subsequently reduce the dimensionality while preserving pairwise distances (SGD
    in node2vec, random projections in FastRP).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 许多图机器学习用例，如链接预测和节点分类，都需要计算节点的相似度。在图的背景下，当这些相似度捕捉到（i）邻域（即图结构）和（ii）待嵌入节点的属性时，才最具表现力。节点嵌入算法将节点投射到低维嵌入空间中——即它们为每个节点分配一个数值向量。这些向量——即嵌入——可以用于进一步的数值预测分析（例如，机器学习算法）。嵌入算法优化的指标是：具有相似图上下文（邻域）和/或属性的节点应当在嵌入空间中被映射得接近。图嵌入算法通常采用两个基本步骤：（i）定义一种机制来采样节点的上下文（node2vec中的随机游走，FastRP中的k-fold转移矩阵），和（ii）随后在保留成对距离的同时减少维度（node2vec中的SGD，FastRP中的随机投影）。
- en: 'HashGNN: Circumvent training a Neural Network'
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HashGNN：绕过训练神经网络
- en: The clue about HashGNN is that it does not require us to train a Neural Network
    based on a loss function, as we would have to in a traditional Message Passing
    Neural Network. As node embedding algorithms optimize for “similar nodes should
    be close in the embedding space”, the evaluation of the loss involves calculating
    the true similarity of a node pair. This is then used as feedback to the training,
    how accurate the predictions are, and to adjust the weights accordingly. Often,
    the cosine similarity is used as similarity measure.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 关于HashGNN的线索是，它不要求我们基于损失函数训练神经网络，就像我们在传统的消息传递神经网络中需要做的那样。由于节点嵌入算法优化的是“相似的节点应该在嵌入空间中接近”，损失的评估涉及计算节点对的真实相似度。这被用作训练的反馈，来评估预测的准确性，并相应地调整权重。通常，余弦相似度被用作相似度度量。
- en: HashGNN circumvents the model training and actually does not employ a Neural
    Network altogether. Instead of training weight matrices and defining a loss function,
    it uses a randomized hashing scheme, which hashes node vectors to the same signature
    with the probability of their similarity, meaning that we can embed nodes without
    the necessity of comparing nodes directly against each other (i.e. no need to
    calculate a cosine similarity). This hashing technique is known as MinHashing
    and was originally defined to approximate the similarity of two sets without comparing
    them. As sets are encoded as binary vectors, HashGNN requires a binary node representation.
    In order to understand how this can be used to embed nodes of a general graph,
    several techniques are required. Let’s have a look.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: HashGNN规避了模型训练，实际上完全不使用神经网络。它不训练权重矩阵或定义损失函数，而是使用随机哈希方案，将节点向量哈希到具有相似度的相同签名，这意味着我们可以嵌入节点，而不需要直接比较节点（即无需计算余弦相似度）。这种哈希技术称为MinHashing，最初定义为在不比较集合的情况下近似两个集合的相似度。由于集合被编码为二进制向量，HashGNN需要二进制节点表示。为了理解如何将其用于嵌入一般图的节点，需要几种技术。让我们来看一看。
- en: MinHashing
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MinHashing
- en: 'First of all, let’s talk about MinHashing. MinHashing is a locality-sensitive
    hashing technique to approximate the Jaccard similarity of two sets. The Jaccard
    similarity measures the overlap (intersection) of two sets by dividing the intersection
    size by the number of the unique elements present (union) in the two sets. It
    is defined on sets, which are encoded as binary vectors: Each element in the universe
    (the set of all elements) is assigned a unique row index. If a particular set
    contains an element, this is represented as value 1 in the respective row of the
    set’s vector. The MinHashing algorithm hashes each set’s binary vector independently
    and uses `K` hash functions to generate a `K`-dimensional signature for it. The
    intuitive explanation of MinHashing is to randomly select a non-zero element `K`
    times, by choosing the one with the smallest hash value. This will yield the signature
    vector for the input set. The interesting part is, that if we do this for two
    sets without comparing them, they will hash to the same signature with the probability
    of their Jaccard similarity (if `K` is large enough). In other words: The probability
    converges against the Jaccard similarity.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们讨论一下MinHashing。MinHashing是一种局部敏感哈希技术，用于近似两个集合的Jaccard相似度。Jaccard相似度通过将交集大小除以两个集合中存在的唯一元素数量（并集）来度量两个集合的重叠（交集）。它定义在编码为二进制向量的集合上：宇宙中的每个元素（所有元素的集合）都分配一个唯一的行索引。如果特定集合包含一个元素，则在集合向量的相应行中表示为值1。MinHashing算法独立地哈希每个集合的二进制向量，并使用`K`个哈希函数生成`K`维签名。MinHashing的直观解释是随机选择非零元素`K`次，选择哈希值最小的那个。这将产生输入集合的签名向量。有趣的是，如果我们对两个集合进行这种操作而不进行比较，它们将以其Jaccard相似度的概率哈希到相同的签名（如果`K`足够大）。换句话说：概率趋近于Jaccard相似度。
- en: '![](../Images/92b7a735435b2b2a6c645fd5ac4fbc0a.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/92b7a735435b2b2a6c645fd5ac4fbc0a.png)'
- en: The Jaccard similarity measures the similarity of two sets. Generally, sets
    can also be encoded as binary vectors. Image by author.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Jaccard相似度度量两个集合的相似度。通常，集合也可以编码为二进制向量。图像来源：作者。
- en: 'In the illustration: Example sets `s1` and `s2` are represented as binary vectors.
    We can easily compute the Jaccard similarity by comparing the two and counting
    the rows where both vectors have a 1\. These are quite simple operations, but
    the complexity resides in the pair-wise comparison of vectors if we have many
    vectors.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在插图中：示例集合`s1`和`s2`被表示为二进制向量。我们可以通过比较这两个向量并计算两个向量都为1的行数，轻松计算Jaccard相似度。这些操作相当简单，但复杂性在于当我们有多个向量时，向量之间的成对比较。
- en: '![](../Images/ca3602e2959515ed684910fab2596254.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ca3602e2959515ed684910fab2596254.png)'
- en: The MinHashing algorithm generates k permutations of the set features and selects
    the feature with the smallest hashing value to create the minHash signature vector.
    Image by author.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: MinHashing算法生成集合特征的k个排列，并选择具有最小哈希值的特征以创建minHash签名向量。图像来源：作者。
- en: Our universe `U` has size 6 and we choose `K` (the number of hash functions)
    to be 3\. We can easily generate new hash functions by using a simple formula
    and boundaries for `a`, `b` and `c`. Now, what we actually do is to hash the indices
    of our vectors (`1-6`), each relating to a single element in our universe, with
    each of our hash functions. This will give us 3 random permutations of the indices
    and therefore elements in our universe. Subsequently, we can take our set vectors
    `s1` and `s2` and use them as masks on our permuted features. For each permutation
    and set vector, we select the index of the smallest hash value, which is present
    in the set. This will yield two 3-dimensional vectors, one for each set, which
    is called the MinHash signature of the set.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的宇宙 `U` 大小为 6，我们选择 `K`（哈希函数的数量）为 3。我们可以通过使用简单的公式和 `a`、`b` 和 `c` 的边界来轻松生成新的哈希函数。现在，我们实际上做的是使用每个哈希函数对我们向量的索引（`1-6`）进行哈希，每个索引与我们宇宙中的一个单一元素相关联。这将为我们提供
    3 个随机排列的索引，从而得到我们宇宙中的元素。随后，我们可以将我们的集合向量 `s1` 和 `s2` 用作我们排列特征的掩码。对于每个排列和集合向量，我们选择集合中最小哈希值的索引。这将生成两个
    3 维向量，每个集合一个，这就是集合的 MinHash 签名。
- en: MinHashing simply selects random features from the input sets, and we need the
    hash functions only as a means to reproduce this randomness equally on all input
    sets. We have to use the same set of hash functions on all vectors, so that the
    signature values of two input sets collide if both have the selected element present.
    The signature values will collide with the probability of the sets’ Jaccard similarities.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: MinHashing 仅仅从输入集选择随机特征，我们只需使用哈希函数来在所有输入集上均等地再现这种随机性。我们必须在所有向量上使用相同的哈希函数，以便两个输入集的签名值在两个集合都包含所选元素时发生冲突。签名值将以集合的
    Jaccard 相似度的概率发生冲突。
- en: WLKNN (Weisfeiler-Lehman Kernel Neural Network)
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: WLKNN（Weisfeiler-Lehman 核心神经网络）
- en: HashGNN uses a message passing scheme as defined in Weisfeiler-Lehman Kernel
    Neural Networks (WLKNN) to capture high-order graph structure and node properties.
    It defines the context sampling strategy as mentioned earlier for HashGNN. The
    WLK runs in `T` iterations. In each iteration, it generates a new node vector
    for each node by combining the node's current vector with the vectors of all directly
    connected neighbors. Therefore, it is considered to pass messages (node vectors)
    along the edges to neighboring nodes.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: HashGNN 使用如 Weisfeiler-Lehman 核心神经网络（WLKNN）中定义的消息传递方案来捕捉高阶图结构和节点属性。它定义了之前提到的
    HashGNN 上下文采样策略。WLK 在 `T` 次迭代中运行。在每次迭代中，它通过将节点的当前向量与所有直接连接邻居的向量组合，为每个节点生成一个新的节点向量。因此，它被认为是沿边将消息（节点向量）传递给相邻节点。
- en: '![](../Images/6d8c869a768d2965965db28608e63dcd.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6d8c869a768d2965965db28608e63dcd.png)'
- en: The WLK runs in T iterations. In each iteration it passes node information along
    the edges to the neighboring nodes. Image by author.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: WLK 在 T 次迭代中运行。在每次迭代中，它将节点信息沿边传递给相邻节点。图片来源于作者。
- en: After `T` iterations, each node contains information of nodes at `T` hops distance
    (high-order). The calculation of the new node vector in iteration `t` essentially
    aggregates all neighbor messages (from iteration `t-1`) into a single neighbor
    message and then combines it with the node vector of the previous iteration. Additionally,
    the WLKNN employs three neural networks (weight matrices and activation function);
    (i) for the aggregated neighbor vector, (ii) for the node vector and (iii) for
    the combination of the two. It is a notable feature of the WLK that the calculation
    in iteration `t` is only dependent on the result of iteration `t-1`. Therefore,
    it can be considered a Markov chain.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `T` 次迭代之后，每个节点包含 `T` 跳距离（高阶）的节点信息。迭代 `t` 中的新节点向量的计算本质上是将所有邻居消息（来自迭代 `t-1`）聚合为单一邻居消息，然后与前一迭代的节点向量组合。此外，WLKNN
    采用三个神经网络（权重矩阵和激活函数）；（i）用于聚合邻居向量，（ii）用于节点向量，以及（iii）用于两者的组合。WLK 的一个显著特征是迭代 `t` 中的计算仅依赖于迭代
    `t-1` 的结果。因此，它可以被视为一个马尔可夫链。
- en: '![](../Images/fd153c49ff0f6c9c916dadd15064c065.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd153c49ff0f6c9c916dadd15064c065.png)'
- en: 'WLK: In each iteration each node vector gets updated with information from
    neighboring nodes. Therefore, after t iterations, each node contains information
    from nodes with t-hop distance. Image by author.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: WLK：在每次迭代中，每个节点向量都会更新来自相邻节点的信息。因此，在 t 次迭代之后，每个节点包含来自 t 跳距离节点的信息。图片来源于作者。
- en: HashGNN by example
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HashGNN 示例
- en: Let’s explore how HashGNN combines the two approaches to embed graph vectors
    efficiently to embedding vectors. Just like the WLKNN, the HashGNN algorithm runs
    in `T` iterations, calculating a new node vector for every node by aggregating
    the neighbor vectors and its own node vector from the previous iteration. However,
    instead of training three weight matrices, it uses three hashing schemes for locality-sensitive
    hashing. Each of the hashing schemes consists of `K` randomly constructed hashing
    functions to draw `K` random features from a binary vector.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探讨一下 HashGNN 如何结合这两种方法高效地将图向量嵌入到嵌入向量中。与 WLKNN 类似，HashGNN 算法在 `T` 次迭代中运行，通过聚合邻居向量和前一迭代的自身节点向量，为每个节点计算一个新的节点向量。然而，它不是训练三个权重矩阵，而是使用三种哈希方案进行局部敏感哈希。每种哈希方案包含
    `K` 个随机构造的哈希函数，从二进制向量中提取 `K` 个随机特征。
- en: '![](../Images/2518fbe2829144f4daf4ba000e1e1fa8.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2518fbe2829144f4daf4ba000e1e1fa8.png)'
- en: 'The HashGNN Algorithm: We initialize the node vectors with their binary feature
    vectors. Image by author.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: HashGNN 算法：我们用其二进制特征向量初始化节点向量。图片由作者提供。
- en: 'In each iteration we perform the following steps:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中，我们执行以下步骤：
- en: '**Step 1: Calculate the node’s signature vector:** Min-hash (randomly select
    `K` features) the node vector from the previous iteration using hashing scheme
    3\. In the first iteration the nodes are initialized with their binary feature
    vectors (we''ll talk about how to binarize nodes later). The resulting signature
    (or message) vector is the one which is passed along the edges to all neighbors.
    Therefore, we have to do this for all nodes first in every iteration.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 1 步：计算节点的签名向量：** 使用哈希方案 3 对来自前一迭代的节点向量进行最小哈希（随机选择 `K` 个特征）。在第一次迭代中，节点用其二进制特征向量初始化（稍后我们将讨论如何对节点进行二值化）。得到的签名（或消息）向量是沿着边传递给所有邻居的。因此，我们必须在每次迭代中首先对所有节点执行此操作。'
- en: '![](../Images/d6cb39108a5ad1faa4c451660f264125.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6cb39108a5ad1faa4c451660f264125.png)'
- en: 'HashGNN Step 1: in each iteration is to calculate the message vector for each
    node by using hashing scheme 3\. Image by author.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: HashGNN 第 1 步：在每次迭代中，通过使用哈希方案 3 计算每个节点的消息向量。图片由作者提供。
- en: '**Step 2: Construct the neighbor vector:** In each node, we’ll receive the
    signature vectors from all directly connected neighbors and aggregate them into
    a single binary vector. Subsequently, we use hashing scheme 2 to select `K` random
    features from the aggregated neighbor vector and call the result the neighbor
    vector.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 2 步：构建邻居向量：** 在每个节点中，我们将接收来自所有直接连接邻居的签名向量，并将它们聚合成一个二进制向量。随后，我们使用哈希方案 2
    从聚合的邻居向量中选择 `K` 个随机特征，并将结果称为邻居向量。'
- en: '![](../Images/773b590d410d722cdbdb6f9c82944cf1.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/773b590d410d722cdbdb6f9c82944cf1.png)'
- en: 'HashGNN Step 2: We collect the message vectors from all neighbors and aggregate
    them. Image by author.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: HashGNN 第 2 步：我们收集所有邻居的消息向量并将其聚合。图片由作者提供。
- en: '**Step 3: Combine node and neighbor vector into new node vector:** Finally,
    we use hashing scheme 1 to randomly select `K` features from the node vector of
    the previous iteration and combine the result with the neighbor vector. The resulting
    vector is the new node vector which is the starting point for the next iteration.
    Note, that this is not the same as in step 1: There, we apply hashing scheme 3
    on the node vector to construct the message/signature vector.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 3 步：将节点向量和邻居向量合并成新的节点向量：** 最后，我们使用哈希方案 1 从前一迭代的节点向量中随机选择 `K` 个特征，并将结果与邻居向量结合。得到的向量是新的节点向量，它是下一次迭代的起点。注意，这与第
    1 步不同：在第 1 步中，我们对节点向量应用哈希方案 3 来构建消息/签名向量。'
- en: '![](../Images/a3caa1e99ca7dc8ef64304c84234ba7d.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a3caa1e99ca7dc8ef64304c84234ba7d.png)'
- en: 'HashGNN Step 3: We combine the min-hashed node vector with the aggregated neighbor
    vector to arrive at the resulting node vector for this iteration. Image by author.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: HashGNN 第 3 步：我们将最小哈希的节点向量与聚合的邻居向量结合，以得到该迭代的结果节点向量。图片由作者提供。
- en: As we can see, the resulting (new) node vector has influence of its own node
    features (3 & 5), as well as its neighbors’ features (2 & 5). After iteration
    1 the node vector will capture information from neighbors with 1 hop distance.
    However, as we use this as input for iteration 2, it will already be influenced
    by features 2 hops away.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中我们可以看到，得到的（新）节点向量受自身节点特征（3 和 5）以及其邻居特征（2 和 5）的影响。经过第一次迭代后，节点向量将捕捉到来自距离 1
    跳的邻居的信息。然而，当我们将其用作第二次迭代的输入时，它已经受到距离 2 跳特征的影响。
- en: '![](../Images/208f648a73632a7df517c06f669af84a.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/208f648a73632a7df517c06f669af84a.png)'
- en: After the first iteration, the new node vector has influence from its own features
    and the neighboring node’s features. Image by author.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次迭代后，新的节点向量受自身特征和邻近节点特征的影响。图片由作者提供。
- en: Neo4j GDS generalizations
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Neo4j GDS 泛化
- en: HashGNN was implemented by the Neo4j GDS (Graph Data Science Library) and adds
    some useful generalizations to the algorithm.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: HashGNN 是由 Neo4j GDS（图数据科学库）实现的，并对算法进行了有用的泛化。
- en: 'One important auxiliary step in the GDS is feature binarization. MinHashing
    and therefore HashGNN requires binary vectors as input. Neo4j offers an additional
    preparation step to transform real-valued node vectors into binary feature vectors.
    They utilize a technique called hyperplane rounding. The algorithm works as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: GDS 中一个重要的辅助步骤是特征二值化。MinHashing 和因此 HashGNN 需要二进制向量作为输入。Neo4j 提供了一个额外的准备步骤，将实值节点向量转换为二进制特征向量。他们使用了一种称为超平面取整的技术。算法的工作流程如下：
- en: '**Step 1: Define node features:** Define the (real-valued) node properties
    to be used as node features. This is done using the parameter `featureProperties`.
    We will call this the node input vector `f`.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 1：定义节点特征：** 定义用于节点特征的（实值）节点属性。这是通过参数`featureProperties`完成的。我们将其称为节点输入向量`f`。'
- en: '**Step 2: Construct random binary classifiers:** Define one hyperplane for
    each target dimension. The number of resulting dimensions is controlled by the
    parameter `dimensions`. A hyperplane is a high-dimensional plane and, as long
    as it resides in the origin, can be described solely by its normal vector `n`.
    The `n` vector is orthogonal to the plane''s surface and therefore describes its
    orientation. In our case the `n` vector needs to be of the same dimensionality
    as the node input vectors (`dim(f) = dim(n)`). To construct a hyperplane, we simply
    draw `dim(f)` times from a Gaussian distribution.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 2：构建随机二进制分类器：** 为每个目标维度定义一个超平面。结果维度的数量由参数`dimensions`控制。超平面是一个高维平面，只要它位于原点，就可以仅通过其法向量`n`来描述。`n`向量垂直于平面的表面，因此描述了其方向。在我们的情况下，`n`向量需要与节点输入向量具有相同的维度（`dim(f)
    = dim(n)`）。为了构建一个超平面，我们简单地从高斯分布中抽取`dim(f)`次。'
- en: '![](../Images/d2f1bc2713cd84e395263a60dd11230e.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d2f1bc2713cd84e395263a60dd11230e.png)'
- en: 'Binarization of features: We use hyperplane rounding to construct binary features
    from real-valued input vectors. We use one random Gaussian classifier for each
    target dimension. Image by author.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 特征二值化：我们使用超平面取整将实值输入向量构造为二进制特征。我们为每个目标维度使用一个随机高斯分类器。图片由作者提供。
- en: '**Step 3: Classify node vectors:** Calculate the dot-product of the node input
    vector and each hyperplane vector, which yields the angle between the hyperplane
    and the input vector. Using a `threshold` parameter, we can decide whether the
    input vector is above (1) or below (0) the hyperplane and assign the respective
    value to the resulting binary feature vector. This is exactly what also happens
    in a binary classification — with the only difference that we do not iteratively
    optimize our hyperplane, but rather use random Gaussian classifiers.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 3：分类节点向量：** 计算节点输入向量和每个超平面向量的点积，从而得到超平面和输入向量之间的角度。使用`threshold`参数，我们可以决定输入向量是高于（1）还是低于（0）超平面，并将相应的值分配给结果的二进制特征向量。这与二元分类中的过程完全一致——唯一的不同是我们不迭代优化超平面，而是使用随机高斯分类器。'
- en: '![](../Images/60c9c061a86addd96ac456bd37d3013f.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/60c9c061a86addd96ac456bd37d3013f.png)'
- en: Using n hyperplanes leads to n-dimensional binary node signatures. Image by
    author.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 n 个超平面会导致 n 维的二进制节点签名。图片由作者提供。
- en: In essence, we draw one random Gaussian classifier for each target dimension
    and set a threshold parameter. Then, we classify our input vectors for each target
    dimension and obtain a `d`-dimensional binary vector which we use as input for
    HashGNN.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，我们为每个目标维度绘制一个随机高斯分类器，并设置一个阈值参数。然后，我们对每个目标维度的输入向量进行分类，并得到一个`d`维的二进制向量，该向量将作为HashGNN的输入。
- en: Conclusion
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: HashGNN uses locality-sensitive hashing to embed node vectors into an embedding
    space. By using this technique, it circumvents the computationally intensive,
    iterative training of a Neural Network (or other optimization), as well as direct
    node comparisons. The authors of the paper report a 2–4 orders of magnitude faster
    runtime than learning based algorithms like SEAL and P-GNN, while still producing
    highly comparable (in some cases even better) accuracy.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: HashGNN 使用局部敏感哈希将节点向量嵌入到嵌入空间中。通过使用这种技术，它绕过了计算密集型的神经网络（或其他优化）的迭代训练以及直接的节点比较。论文的作者报告称，与基于学习的算法如
    SEAL 和 P-GNN 相比，其运行时间快 2 到 4 个数量级，同时仍能产生高度可比（在某些情况下甚至更好）的准确性。
- en: '![](../Images/57a5e45e97bc200333c9ccaf5309e684.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57a5e45e97bc200333c9ccaf5309e684.png)'
- en: 'HashGNN is 2–4 orders of magnitudes faster than learning-based algorithms,
    while delivering comparable results. Image taken from: [https://arxiv.org/abs/2105.14280.](https://arxiv.org/abs/2105.14280.)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: HashGNN 比基于学习的算法快 2 到 4 个数量级，同时提供了可比的结果。图像来源：[https://arxiv.org/abs/2105.14280.](https://arxiv.org/abs/2105.14280.)
- en: HashGNN is implemented in the Neo4j GDS (Graph Data Science Library) and can
    therefore be used out-of-the-box on your Neo4j graph. In the next post, I will
    go into practical details about how to use it, and what to keep in mind.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: HashGNN 在 Neo4j GDS（图数据科学库）中实现，因此可以直接在你的 Neo4j 图上使用。在下一篇文章中，我将详细讲解如何使用它以及需要注意的事项。
- en: Thanks for stopping by and see you next time. 🚀
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你的光临，下次见。🚀
- en: References
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[Original Paper: Hashing-Accelerated Graph Neural Networks for Link Prediction](https://doi.org/10.48550/arXiv.2105.14280)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[原始论文：哈希加速的图神经网络用于链路预测](https://doi.org/10.48550/arXiv.2105.14280)'
- en: '[Neo4j GDS Documentation: HashGNN](https://neo4j.com/docs/graph-data-science/current/machine-learning/node-embeddings/hashgnn/)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Neo4j GDS 文档：HashGNN](https://neo4j.com/docs/graph-data-science/current/machine-learning/node-embeddings/hashgnn/)'
