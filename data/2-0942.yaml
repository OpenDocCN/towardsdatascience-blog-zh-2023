- en: From Encodings to Embeddings
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»ç¼–ç åˆ°åµŒå…¥
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/from-encodings-to-embeddings-5b59bceef094](https://towardsdatascience.com/from-encodings-to-embeddings-5b59bceef094)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/from-encodings-to-embeddings-5b59bceef094](https://towardsdatascience.com/from-encodings-to-embeddings-5b59bceef094)
- en: '**concepts and fundamentals: from SVD to neural networks**'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**æ¦‚å¿µä¸åŸºç¡€ï¼šä»SVDåˆ°ç¥ç»ç½‘ç»œ**'
- en: '[](https://medium.com/@mina.ghashami?source=post_page-----5b59bceef094--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page-----5b59bceef094--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5b59bceef094--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5b59bceef094--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page-----5b59bceef094--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mina.ghashami?source=post_page-----5b59bceef094--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page-----5b59bceef094--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5b59bceef094--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5b59bceef094--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page-----5b59bceef094--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5b59bceef094--------------------------------)
    Â·16 min readÂ·Sep 7, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5b59bceef094--------------------------------)
    Â·16åˆ†é’Ÿé˜…è¯»Â·2023å¹´9æœˆ7æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ea2a97bef9136382285c2f7b5a61786a.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea2a97bef9136382285c2f7b5a61786a.png)'
- en: 'credit: [https://unsplash.com/](https://unsplash.com/)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç‰ˆæƒ: [https://unsplash.com/](https://unsplash.com/)'
- en: 'In this article, we will talk about two fundamental concepts in the fields
    of data representation and machine learning: **Encoding** and **Embedding**. The
    content of this article is partly taken from one of my lectures in [CS246 Mining
    Massive DataSet (MMDS) course at Stanford University](https://web.stanford.edu/class/cs246/).
    I hope you find it useful.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºæ•°æ®è¡¨ç¤ºå’Œæœºå™¨å­¦ä¹ é¢†åŸŸä¸­çš„ä¸¤ä¸ªåŸºæœ¬æ¦‚å¿µï¼š**ç¼–ç **å’Œ**åµŒå…¥**ã€‚æœ¬æ–‡çš„å†…å®¹éƒ¨åˆ†æ¥æºäºæˆ‘åœ¨[æ–¯å¦ç¦å¤§å­¦çš„CS246çŸ¿å¤§æ•°æ®é›†ï¼ˆMMDSï¼‰è¯¾ç¨‹](https://web.stanford.edu/class/cs246/)ä¸­çš„è®²åº§ã€‚å¸Œæœ›ä½ è§‰å¾—æœ‰ç”¨ã€‚
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¼•è¨€
- en: 'All Machine Learning (ML) methods work with input feature vectors and almost
    all of them require input features to be *numerical*. From a ML perspective, there
    are four types of features:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ–¹æ³•éƒ½å¤„ç†è¾“å…¥ç‰¹å¾å‘é‡ï¼Œå‡ ä¹æ‰€æœ‰æ–¹æ³•éƒ½è¦æ±‚è¾“å…¥ç‰¹å¾æ˜¯*æ•°å€¼å‹*çš„ã€‚ä»MLçš„è§’åº¦æ¥çœ‹ï¼Œæœ‰å››ç§ç±»å‹çš„ç‰¹å¾ï¼š
- en: '*Numerical (continuous or discrete)*: numerical data can be characterized by
    continuous or discrete data. Continuous data can assume any value within a range
    whereas discrete data has distinct values. Example of continues numerical variable
    is *`height`*, and an example of discrete numerical variable is *`age`*.'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*æ•°å€¼å‹ï¼ˆè¿ç»­æˆ–ç¦»æ•£ï¼‰*ï¼šæ•°å€¼æ•°æ®å¯ä»¥åˆ†ä¸ºè¿ç»­æ•°æ®æˆ–ç¦»æ•£æ•°æ®ã€‚è¿ç»­æ•°æ®å¯ä»¥åœ¨ä¸€ä¸ªèŒƒå›´å†…å–ä»»æ„å€¼ï¼Œè€Œç¦»æ•£æ•°æ®æœ‰æ˜æ˜¾çš„å€¼ã€‚è¿ç»­æ•°å€¼å˜é‡çš„ä¾‹å­æ˜¯*`èº«é«˜`*ï¼Œç¦»æ•£æ•°å€¼å˜é‡çš„ä¾‹å­æ˜¯*`å¹´é¾„`*ã€‚'
- en: '*Categorical (ordinal or nominal)*: categorical data represents characteristics
    such as eye color, and hometown. Categorical data can be ordinal or nominal. In
    ordinal variable, the data falls into ordered categories that are ranked in some
    particular way. An example is *`skill level`* that takes values of *[`beginner`,
    `intermediate`, `advanced`]*. Nominal variable has no order among its values.
    An example is *`eye color`* that takes values of *[`black`, `brownâ€™, `blue`, `green`]*.'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*åˆ†ç±»æ•°æ®ï¼ˆæœ‰åºæˆ–æ— åºï¼‰*ï¼šåˆ†ç±»æ•°æ®è¡¨ç¤ºè¯¸å¦‚çœ¼ç›é¢œè‰²å’Œå®¶ä¹¡ç­‰ç‰¹å¾ã€‚åˆ†ç±»æ•°æ®å¯ä»¥æ˜¯æœ‰åºçš„æˆ–æ— åºçš„ã€‚åœ¨æœ‰åºå˜é‡ä¸­ï¼Œæ•°æ®è¢«åˆ†åˆ°æŒ‰ç‰¹å®šæ–¹å¼æ’åºçš„ç±»åˆ«ä¸­ã€‚ä¾‹å¦‚*`æŠ€èƒ½æ°´å¹³`*ï¼Œå…¶å€¼ä¸º*[`åˆçº§`ï¼Œ`ä¸­çº§`ï¼Œ`é«˜çº§`]ã€‚æ— åºå˜é‡åœ¨å…¶å€¼ä¹‹é—´æ²¡æœ‰é¡ºåºã€‚ä¾‹å¦‚*`çœ¼ç›é¢œè‰²`*ï¼Œå…¶å€¼ä¸º*[`é»‘è‰²`ï¼Œ`æ£•è‰²`ï¼Œ`è“è‰²`ï¼Œ`ç»¿è‰²`]ã€‚'
- en: '*Time series*: Time series is a sequence of numbers collected at regular intervals
    over some period of time. This data is ordered in time unlike previous variables.
    An example of this is *`average of home sale price over years in USA`*.'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*æ—¶é—´åºåˆ—*ï¼šæ—¶é—´åºåˆ—æ˜¯ä¸€ä¸ªåœ¨ä¸€å®šæ—¶é—´æ®µå†…ä»¥è§„åˆ™é—´éš”æ”¶é›†çš„æ•°å­—åºåˆ—ã€‚è¿™äº›æ•°æ®æŒ‰æ—¶é—´æ’åºï¼Œä¸ä¹‹å‰çš„å˜é‡ä¸åŒã€‚ä¸€ä¸ªä¾‹å­æ˜¯*`ç¾å›½å¤šå¹´æˆ¿å±‹é”€å”®ä»·æ ¼çš„å¹³å‡å€¼`*ã€‚'
- en: '*Text*: Any document is a text data, that we often represent them as a â€˜bag
    of wordsâ€™.'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*æ–‡æœ¬*ï¼šä»»ä½•æ–‡æ¡£éƒ½æ˜¯æ–‡æœ¬æ•°æ®ï¼Œæˆ‘ä»¬é€šå¸¸å°†å…¶è¡¨ç¤ºä¸ºâ€œè¯è¢‹â€ã€‚'
- en: To feed any variables to an ML model, we have to convert them into numerical.
    Both encoding and embedding techniques do this trick.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å°†ä»»ä½•å˜é‡è¾“å…¥åˆ°MLæ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬å¿…é¡»å°†å…¶è½¬æ¢ä¸ºæ•°å€¼ã€‚ç¼–ç å’ŒåµŒå…¥æŠ€æœ¯éƒ½å¯ä»¥å®ç°è¿™ä¸€ç‚¹ã€‚
- en: Encoding
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¼–ç 
- en: 'Encoding is the process of converting raw data, such as text, images, or audio,
    into a structured numerical format that can be easily processed by computers.
    There are two ways to encode a categorical variable:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼–ç æ˜¯å°†åŸå§‹æ•°æ®ï¼ˆå¦‚æ–‡æœ¬ã€å›¾åƒæˆ–éŸ³é¢‘ï¼‰è½¬æ¢ä¸ºç»“æ„åŒ–çš„æ•°å€¼æ ¼å¼ï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿè½»æ¾å¤„ç†ã€‚ç¼–ç ç±»åˆ«å˜é‡æœ‰ä¸¤ç§æ–¹æ³•ï¼š
- en: 1ï¸âƒ£ Integer encoding
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ æ•´æ•°ç¼–ç 
- en: 2ï¸âƒ£ One-hot encoding
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ ä¸€çƒ­ç¼–ç 
- en: 3ï¸âƒ£ Multi-hot encoding (this is the extension of one-hot encoding)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 3ï¸âƒ£ å¤šçƒ­ç¼–ç ï¼ˆè¿™æ˜¯å¯¹ä¸€çƒ­ç¼–ç çš„æ‰©å±•ï¼‰
- en: 'To explain each method letâ€™s work through the following example:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è§£é‡Šæ¯ç§æ–¹æ³•ï¼Œæˆ‘ä»¬é€šè¿‡ä»¥ä¸‹ç¤ºä¾‹æ¥è¯´æ˜ï¼š
- en: ğŸ¬ Consider a tiny movie dataset containing only 4 movies and 5 features.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¬ è®¾æƒ³ä¸€ä¸ªåªæœ‰ 4 éƒ¨ç”µå½±å’Œ 5 ä¸ªç‰¹å¾çš„å°å‹ç”µå½±æ•°æ®é›†ã€‚
- en: '![](../Images/81d64c7f84e6ea98a7e74640933791c9.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81d64c7f84e6ea98a7e74640933791c9.png)'
- en: 'Figure 1: movie dataset â€” Image by the author'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 1ï¼šç”µå½±æ•°æ®é›† â€” å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: As we see, two features (release year, IMDB rating) are numerical, one feature
    (title) is text, and the remaining two (provider, IMDB genres) are categorical.
    Letâ€™s see how encoding methods apply to these.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬æ‰€è§ï¼Œä¸¤ä¸ªç‰¹å¾ï¼ˆå‘è¡Œå¹´ä»½ã€IMDB è¯„åˆ†ï¼‰æ˜¯æ•°å€¼å‹çš„ï¼Œä¸€ä¸ªç‰¹å¾ï¼ˆæ ‡é¢˜ï¼‰æ˜¯æ–‡æœ¬å‹çš„ï¼Œå‰©ä¸‹çš„ä¸¤ä¸ªï¼ˆæä¾›è€…ã€IMDB ç±»å‹ï¼‰æ˜¯ç±»åˆ«å‹çš„ã€‚è®©æˆ‘ä»¬æ¥çœ‹çœ‹ç¼–ç æ–¹æ³•å¦‚ä½•åº”ç”¨äºè¿™äº›ç‰¹å¾ã€‚
- en: 1ï¸âƒ£ **Integer encoding:**
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 1ï¸âƒ£ **æ•´æ•°ç¼–ç ï¼š**
- en: 'This method assigns an integer to each category value. For example if *provider*
    variable takes four distinct values *`[Netflix, Prime Video, HBO Max, Hulu]`*,
    we assign them integers 1, 2, 3 and 4 respectively:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•ä¸ºæ¯ä¸ªç±»åˆ«å€¼åˆ†é…ä¸€ä¸ªæ•´æ•°ã€‚ä¾‹å¦‚ï¼Œå¦‚æœ *æä¾›è€…* å˜é‡æœ‰å››ä¸ªä¸åŒçš„å€¼ *`[Netflix, Prime Video, HBO Max, Hulu]`*ï¼Œæˆ‘ä»¬åˆ†åˆ«å°†å…¶åˆ†é…æ•´æ•°
    1ã€2ã€3 å’Œ 4ï¼š
- en: '*Netflix -> 1, Prime Video -> 2, HBO Max ->3 , Hulu -> 4*'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*Netflix -> 1, Prime Video -> 2, HBO Max -> 3, Hulu -> 4*'
- en: The pro of this approach is that it provides a dense representation. The con
    is that it implies ordering between different categories, i.e.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•çš„ä¼˜ç‚¹æ˜¯æä¾›äº†ç´§å‡‘çš„è¡¨ç¤ºã€‚ç¼ºç‚¹æ˜¯å®ƒæš—ç¤ºäº†ä¸åŒç±»åˆ«ä¹‹é—´çš„é¡ºåºï¼Œå³
- en: '*Netflix < Prime Video < HBO Max < Hulu.*'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*Netflix < Prime Video < HBO Max < Hulu.*'
- en: So it might makes more sense to use integer encoding for ordinal variables,
    e.g. for *`educationâ€™* taking values of *â€˜[Diploma, Undergrad, Masters, PhD ]`*.
    However it still implies values are equally spaced out ğŸ™Š!! Obviously, this is
    undesirable, so letâ€™s move to the next method.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå¯¹äºåºæ•°å˜é‡ï¼ˆä¾‹å¦‚ *`æ•™è‚²`* å–å€¼ä¸º *â€˜[æ–‡å‡­, æœ¬ç§‘, ç¡•å£«, åšå£«]`*ï¼‰ï¼Œä½¿ç”¨æ•´æ•°ç¼–ç å¯èƒ½æ›´æœ‰æ„ä¹‰ã€‚ç„¶è€Œï¼Œè¿™ä»ç„¶æ„å‘³ç€å€¼æ˜¯å‡åŒ€é—´éš”çš„
    ğŸ™Š!! æ˜¾ç„¶ï¼Œè¿™å¹¶ä¸ç†æƒ³ï¼Œå› æ­¤æˆ‘ä»¬å°†è½¬å‘ä¸‹ä¸€ç§æ–¹æ³•ã€‚
- en: 'ğŸ’» In Python, you can perform integer encoding using various libraries, such
    as scikit-learn or TensorFlow/Keras. Here, we use scikit-learnâ€™s `LabelEncoder`
    for encoding categorical labels into integers:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’» åœ¨ Python ä¸­ï¼Œä½ å¯ä»¥ä½¿ç”¨å„ç§åº“ï¼ˆå¦‚ scikit-learn æˆ– TensorFlow/Kerasï¼‰æ‰§è¡Œæ•´æ•°ç¼–ç ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ scikit-learn
    çš„ `LabelEncoder` å°†ç±»åˆ«æ ‡ç­¾ç¼–ç ä¸ºæ•´æ•°ï¼š
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 2ï¸âƒ£ **One-hot encoding:**
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 2ï¸âƒ£ **ä¸€çƒ­ç¼–ç ï¼š**
- en: 'This method first applies integer encoding, then creates a binary vector that
    represents the numerical values e.g. for *`provider`* variable, we assign integers
    first: *Netflix -> 1, Prime Video -> 2, HBO Max ->3 , Hulu -> 4*. Then we create
    a binary vector of length 4 for each value as following:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•é¦–å…ˆåº”ç”¨æ•´æ•°ç¼–ç ï¼Œç„¶ååˆ›å»ºä¸€ä¸ªäºŒè¿›åˆ¶å‘é‡æ¥è¡¨ç¤ºæ•°å€¼ï¼Œä¾‹å¦‚ï¼Œå¯¹äº *`æä¾›è€…`* å˜é‡ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ†é…æ•´æ•°ï¼š*Netflix -> 1, Prime
    Video -> 2, HBO Max -> 3, Hulu -> 4*ã€‚ç„¶åæˆ‘ä»¬ä¸ºæ¯ä¸ªå€¼åˆ›å»ºä¸€ä¸ªé•¿åº¦ä¸º 4 çš„äºŒè¿›åˆ¶å‘é‡ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](../Images/1da255730afaec50cfc5c1747bde6fad.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1da255730afaec50cfc5c1747bde6fad.png)'
- en: 'Figure 2: one-hot vectors of provider variable â€” Image by the author'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 2ï¼šæä¾›è€…å˜é‡çš„ä¸€çƒ­ç¼–ç å‘é‡ â€” å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: As you see, this method explodes the dimension of the feature vector to number
    of values the categorical feature takes ğŸ™ˆ!! That can quickly get out of hands.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼Œè¿™ç§æ–¹æ³•å°†ç‰¹å¾å‘é‡çš„ç»´åº¦æ‰©å±•åˆ°ç±»åˆ«ç‰¹å¾çš„å–å€¼æ•°é‡ ğŸ™ˆ!! è¿™å¯èƒ½è¿…é€Ÿå˜å¾—éš¾ä»¥æ§åˆ¶ã€‚
- en: 'ğŸ’» In Python, you can perform one-hot encoding using libraries like scikit-learn
    or pandas. Hereâ€™s how to do it using scikit-learnâ€™s `OneHotEncoder`:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’» åœ¨ Python ä¸­ï¼Œä½ å¯ä»¥ä½¿ç”¨åƒ scikit-learn æˆ– pandas è¿™æ ·çš„åº“æ¥æ‰§è¡Œä¸€çƒ­ç¼–ç ã€‚ä»¥ä¸‹æ˜¯ä½¿ç”¨ scikit-learn çš„ `OneHotEncoder`
    è¿›è¡Œä¸€çƒ­ç¼–ç çš„æ–¹æ³•ï¼š
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, letâ€™s look at the extension of this method too.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä¹Ÿæ¥çœ‹çœ‹è¿™ç§æ–¹æ³•çš„æ‰©å±•ã€‚
- en: 3ï¸âƒ£ **Multi-hot encoding:**
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 3ï¸âƒ£ **å¤šçƒ­ç¼–ç ï¼š**
- en: Multi-hot encoding is an extension of one-hot encoding when a categorical variable
    can take multiple values at the same time. For example, there are 28 distinct
    IMDB genres, and a movie can take multiple genres, e.g. the movie `*stranger things`*
    is *drama, fantasy, horror* at the same time.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šçƒ­ç¼–ç æ˜¯å¯¹ä¸€çƒ­ç¼–ç çš„æ‰©å±•ï¼Œå½“ç±»åˆ«å˜é‡å¯ä»¥åŒæ—¶å–å¤šä¸ªå€¼æ—¶ã€‚ä¾‹å¦‚ï¼Œæœ‰ 28 ç§ä¸åŒçš„ IMDB ç±»å‹ï¼Œè€Œä¸€éƒ¨ç”µå½±å¯ä»¥åŒæ—¶å±äºå¤šä¸ªç±»å‹ï¼Œä¾‹å¦‚ç”µå½± `*æ€ªå¥‡ç‰©è¯­*`
    åŒæ—¶å±äº *å‰§æƒ…ã€å¥‡å¹»ã€ææ€–* ç±»å‹ã€‚
- en: '![](../Images/0a1bfdeb0a244d9794a44801dcf62229.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a1bfdeb0a244d9794a44801dcf62229.png)'
- en: 'Figure 3: IMDB genres with their integer encoding â€” Image by the author'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 3ï¼šIMDB ç±»å‹åŠå…¶æ•´æ•°ç¼–ç  â€” å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: 'Applying multi-hot encoding to moviesâ€™ genres give us 28-dimensional encoding
    vectors:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å°†å¤šçƒ­ç¼–ç åº”ç”¨äºç”µå½±ç±»å‹ï¼Œä¼šå¾—åˆ° 28 ç»´çš„ç¼–ç å‘é‡ï¼š
- en: '![](../Images/0926c9622ff5b2f628471caa7a464357.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0926c9622ff5b2f628471caa7a464357.png)'
- en: 'Figure 4: multi-hot encoding of genres for each movie â€” Image by the author'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 4ï¼šæ¯éƒ¨ç”µå½±çš„ç±»å‹çš„å¤šçƒ­ç¼–ç  â€” ä½œè€…æä¾›çš„å›¾ç‰‡
- en: We see that obviously, multi-hot encoding suffers from same caveat as one-hot
    encoding which is dimensionality explosion.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥æ˜æ˜¾çœ‹åˆ°ï¼Œå¤šçƒ­ç¼–ç ä¸ one-hot ç¼–ç æœ‰ç›¸åŒçš„ç¼ºç‚¹ï¼Œå³ç»´åº¦çˆ†ç‚¸ã€‚
- en: 'ğŸ’» We can use scikit-learn or pandas to achieve multi-hot encoding in Python.
    Hereâ€™s how to do it using scikit-learnâ€™s `MultiLabelBinarizer`:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’» æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ scikit-learn æˆ– pandas åœ¨ Python ä¸­å®ç°å¤šçƒ­ç¼–ç ã€‚ä»¥ä¸‹æ˜¯ä½¿ç”¨ scikit-learn çš„ `MultiLabelBinarizer`
    è¿›è¡Œæ“ä½œçš„æ–¹æ³•ï¼š
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, applying all above encodings on the movie dataset results in the following:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œå°†ä»¥ä¸Šæ‰€æœ‰ç¼–ç åº”ç”¨äºç”µå½±æ•°æ®é›†ï¼Œä¼šå¾—åˆ°ä»¥ä¸‹ç»“æœï¼š
- en: '![](../Images/5f32868ac44b63c1710dbd1d0532d864.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5f32868ac44b63c1710dbd1d0532d864.png)'
- en: 'Figure 5: movie dataset with all encodings â€” Image by the author'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 5ï¼šåŒ…å«æ‰€æœ‰ç¼–ç çš„ç”µå½±æ•°æ®é›† â€” ä½œè€…æä¾›çš„å›¾ç‰‡
- en: As we see, after applying all encodings, dimension of data increased from 5
    to 35 ğŸ™Š!! In fact, It will blow up to thousands or a million if we multi-hot encode
    the *`title`* variable too!
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘ä»¬æ‰€è§ï¼Œåœ¨åº”ç”¨æ‰€æœ‰ç¼–ç åï¼Œæ•°æ®ç»´åº¦ä» 5 å¢åŠ åˆ° 35 ğŸ™Š!! å®é™…ä¸Šï¼Œå¦‚æœæˆ‘ä»¬å¯¹ *`title`* å˜é‡ä¹Ÿè¿›è¡Œå¤šçƒ­ç¼–ç ï¼Œå®ƒå°†è†¨èƒ€åˆ°æ•°åƒæˆ–ä¸€ç™¾ä¸‡ï¼
- en: 'ğŸ““The takeaway is that one-hot and multi-hot encodings are not practical for
    features with large value sets. In a corpus of documents with one million distinct
    words, representing each document via multi-hot encoding creates vectors that
    are:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ““è¦ç‚¹æ˜¯ï¼Œå¯¹äºå…·æœ‰å¤§é‡å€¼é›†çš„ç‰¹å¾ï¼Œone-hot å’Œ multi-hot ç¼–ç å¹¶ä¸å®ç”¨ã€‚åœ¨ä¸€ä¸ªåŒ…å«ä¸€ç™¾ä¸‡ä¸ªä¸åŒå•è¯çš„æ–‡æ¡£è¯­æ–™åº“ä¸­ï¼Œé€šè¿‡å¤šçƒ­ç¼–ç è¡¨ç¤ºæ¯ä¸ªæ–‡æ¡£ä¼šç”Ÿæˆå¦‚ä¸‹å‘é‡ï¼š
- en: 'high dimensional: multi-hot encodings will create 1-million dimensional vectors!'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é«˜ç»´ï¼šå¤šçƒ­ç¼–ç å°†ç”Ÿæˆ 100 ä¸‡ç»´çš„å‘é‡ï¼
- en: 'sparse: Since an average document contains 500 words, the multi-hot vectors
    will be 99.95% sparse'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¨€ç–ï¼šç”±äºä¸€ä¸ªå¹³å‡æ–‡æ¡£åŒ…å« 500 ä¸ªè¯ï¼Œå› æ­¤å¤šçƒ­ç¼–ç å‘é‡å°†ä¼šæœ‰ 99.95% çš„ç¨€ç–åº¦
- en: 'short of semantic: encoding of two words â€˜goodâ€™ and â€˜greatâ€™ are as different
    as encoding of â€˜goodâ€™ and â€˜badâ€™!'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¯­ä¹‰çŸ­ç¼ºï¼šâ€˜goodâ€™ å’Œ â€˜greatâ€™ çš„ç¼–ç ä¸ â€˜goodâ€™ å’Œ â€˜badâ€™ çš„ç¼–ç ä¸€æ ·ä¸åŒï¼
- en: âœï¸ In a nutshell, use one-hot/multi-hot encoding when the number of categories
    is small; usually lesser than 15 or so. For text data which has millions of categories
    (every word as one category) we have to use more efficient methods.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: âœï¸ ç®€è€Œè¨€ä¹‹ï¼Œå½“ç±»åˆ«æ•°é‡è¾ƒå°‘æ—¶ï¼Œä½¿ç”¨ one-hot/multi-hot ç¼–ç ï¼›é€šå¸¸å°‘äº 15 ä¸ªã€‚å¯¹äºç±»åˆ«æ•°é‡ä»¥ç™¾ä¸‡è®¡çš„æ–‡æœ¬æ•°æ®ï¼ˆæ¯ä¸ªå•è¯ä½œä¸ºä¸€ä¸ªç±»åˆ«ï¼‰ï¼Œæˆ‘ä»¬å¿…é¡»ä½¿ç”¨æ›´é«˜æ•ˆçš„æ–¹æ³•ã€‚
- en: In the rest of the article, we work with the problem of *`computing word embeddings`*.
    Through this example, we will study few fundamental embedding methods.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡çš„å…¶ä½™éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†å¤„ç† *`è®¡ç®—è¯åµŒå…¥`* çš„é—®é¢˜ã€‚é€šè¿‡è¿™ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬å°†ç ”ç©¶å‡ ç§åŸºæœ¬çš„åµŒå…¥æ–¹æ³•ã€‚
- en: '**Embedding**'
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**åµŒå…¥**'
- en: To address above shortcomings, we move from high dimensional sparse vector to
    short dense vectors; these vectors are called ***embeddings***. An embedding is
    a translation of a high-dimensional vector into a low-dimensional space and captures
    semantic similarity.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œæˆ‘ä»¬ä»é«˜ç»´ç¨€ç–å‘é‡è½¬å‘çŸ­çš„ç¨ å¯†å‘é‡ï¼›è¿™äº›å‘é‡è¢«ç§°ä¸º ***åµŒå…¥***ã€‚åµŒå…¥æ˜¯å°†é«˜ç»´å‘é‡è½¬æ¢ä¸ºä½ç»´ç©ºé—´çš„è¿‡ç¨‹ï¼Œå¹¶æ•æ‰è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚
- en: '*â€œOne of the benefits of using dense and low-dimensional vectors is computational:
    The majority of neural network toolkits do not play well with very high-dimensional,
    sparse vectors.â€ [5]*'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*â€œä½¿ç”¨ç¨ å¯†ä¸”ä½ç»´å‘é‡çš„ä¸€ä¸ªå¥½å¤„æ˜¯è®¡ç®—ä¸Šçš„ï¼šå¤§å¤šæ•°ç¥ç»ç½‘ç»œå·¥å…·åŒ…ä¸éå¸¸é«˜ç»´çš„ç¨€ç–å‘é‡å…¼å®¹æ€§å·®ã€‚â€ [5]*'
- en: Letâ€™s first look at a very simple embedding method called SVD.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆè®©æˆ‘ä»¬æ¥çœ‹ä¸€ç§éå¸¸ç®€å•çš„åµŒå…¥æ–¹æ³•ï¼Œç§°ä¸º SVDã€‚
- en: Singular Value Decomposition (SVD)
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¥‡å¼‚å€¼åˆ†è§£ (SVD)
- en: 'The simplest embedding method is perhaps *Singular Value Decomposition (SVD)*
    that takes an input matrix *A* and decompose it into three matrices as shown below:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç®€å•çš„åµŒå…¥æ–¹æ³•å¯èƒ½æ˜¯ *å¥‡å¼‚å€¼åˆ†è§£ (SVD)*ï¼Œå®ƒå°†è¾“å…¥çŸ©é˜µ *A* åˆ†è§£ä¸ºä»¥ä¸‹ä¸‰ä¸ªçŸ©é˜µï¼š
- en: '![](../Images/9a9ee8fc2dece9e9a50e7c01b3a60009.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9a9ee8fc2dece9e9a50e7c01b3a60009.png)'
- en: 'Figure 6: SVD decomposition â€” Image by the author'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 6ï¼šSVD åˆ†è§£ â€” ä½œè€…æä¾›çš„å›¾ç‰‡
- en: '***U, V*** are left and right singular vectors, respectively. They are column
    orthonormal meaning that each column in them has norm of one, and every two column
    in U (and in V, respectively) are orthogonal. In mathematic, we write this as'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '***U, V*** åˆ†åˆ«æ˜¯å·¦å¥‡å¼‚å‘é‡å’Œå³å¥‡å¼‚å‘é‡ã€‚å®ƒä»¬æ˜¯åˆ—æ­£äº¤çš„ï¼Œè¿™æ„å‘³ç€å®ƒä»¬ä¸­çš„æ¯ä¸€åˆ—çš„èŒƒæ•°ä¸º 1ï¼Œå¹¶ä¸” U ä¸­çš„æ¯ä¸¤åˆ—ï¼ˆä»¥åŠ V ä¸­çš„æ¯ä¸¤åˆ—ï¼‰æ˜¯æ­£äº¤çš„ã€‚ç”¨æ•°å­¦è¯­è¨€è¡¨ç¤ºä¸º'
- en: '![](../Images/0dffcbbba175de107ca99c532e87b0a1.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0dffcbbba175de107ca99c532e87b0a1.png)'
- en: 'Both *U* and *V* define an r-dimensional subspace, hence projecting ***A***
    onto them produces r-dimensional embeddings:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*U* å’Œ *V* å®šä¹‰äº†ä¸€ä¸ª r ç»´å­ç©ºé—´ï¼Œå› æ­¤å°†***A***æŠ•å½±åˆ°å®ƒä»¬ä¸Šé¢äº§ç”Ÿ r ç»´åµŒå…¥ï¼š'
- en: '![](../Images/7d21a53734ef4b03161ed22eda0321be.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7d21a53734ef4b03161ed22eda0321be.png)'
- en: ğŸ—’ **Letâ€™s look at an example together** ğŸ—’**:**
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ—’ **è®©æˆ‘ä»¬ä¸€èµ·çœ‹ä¸€ä¸ªä¾‹å­** ğŸ—’**ï¼š**
- en: 'Given a corpus of documents, we can use SVD to compute document embeddings
    & word embeddings. Here are the steps:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šä¸€ä¸ªæ–‡æ¡£è¯­æ–™åº“ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ SVD æ¥è®¡ç®—æ–‡æ¡£åµŒå…¥å’Œè¯åµŒå…¥ã€‚æ­¥éª¤å¦‚ä¸‹ï¼š
- en: '**Step 1**: Convert it to bag of words (BOW) vectors and get a term-document
    matrix. We can use term frequencies (tf), or normalize using tf-idf technique.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 1**ï¼šå°†å…¶è½¬æ¢ä¸ºè¯è¢‹ï¼ˆBOWï¼‰å‘é‡ï¼Œå¹¶è·å¾—è¯-æ–‡æ¡£çŸ©é˜µã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¯é¢‘ï¼ˆtfï¼‰ï¼Œæˆ–ä½¿ç”¨ tf-idf æŠ€æœ¯è¿›è¡Œå½’ä¸€åŒ–ã€‚'
- en: '![](../Images/4ddecdc930870ddb7d838bb92b70c37c.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ddecdc930870ddb7d838bb92b70c37c.png)'
- en: 'Figure 7: term frequency matrix â€” Image by the author'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 7ï¼šè¯é¢‘çŸ©é˜µ â€” ä½œè€…æä¾›çš„å›¾ç‰‡
- en: '**Step 2**: apply SVD on the term-document matrix and pick a value *r < rank(A)*.
    This will create three matrices, each of rank r.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 2**ï¼šå¯¹è¯-æ–‡æ¡£çŸ©é˜µåº”ç”¨ SVDï¼Œå¹¶é€‰æ‹©ä¸€ä¸ªå€¼*r < rank(A)*ã€‚è¿™å°†åˆ›å»ºä¸‰ä¸ªçŸ©é˜µï¼Œæ¯ä¸ªçŸ©é˜µçš„ç§©ä¸º rã€‚'
- en: '![](../Images/a3eb55940c85f8504f4a232336853145.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a3eb55940c85f8504f4a232336853145.png)'
- en: 'Figure 8: SVD decomposition of matrix A â€” Image by the author'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8ï¼šçŸ©é˜µ A çš„ SVD åˆ†è§£ â€” ä½œè€…æä¾›çš„å›¾ç‰‡
- en: '**Step 3**: compute embedding of documents as'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 3**ï¼šè®¡ç®—æ–‡æ¡£çš„åµŒå…¥ï¼Œå¦‚ä¸‹æ‰€ç¤º'
- en: '*emb = [<doc, v1> , <doc, v2> , <doc, v3>]*'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*emb = [<doc, v1> , <doc, v2> , <doc, v3>]*'
- en: The first two dot-products are shown in the figure below.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹å›¾æ˜¾ç¤ºäº†å‰ä¸¤ä¸ªç‚¹ç§¯ã€‚
- en: '![](../Images/f1f92b7d90b085ee574cde786fb5bfc3.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1f92b7d90b085ee574cde786fb5bfc3.png)'
- en: 'Figure 9: embedding of first document â€” only two dot-product operation are
    shown â€” Image by the author'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 9ï¼šç¬¬ä¸€ä¸ªæ–‡æ¡£çš„åµŒå…¥ â€” ä»…æ˜¾ç¤ºäº†ä¸¤ä¸ªç‚¹ç§¯æ“ä½œ â€” ä½œè€…æä¾›çš„å›¾ç‰‡
- en: 'Once we compute all three vector dot-products, the embedding of the document
    is:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æˆ‘ä»¬è®¡ç®—äº†æ‰€æœ‰ä¸‰ä¸ªå‘é‡çš„ç‚¹ç§¯ï¼Œæ–‡æ¡£çš„åµŒå…¥å°±æ˜¯ï¼š
- en: '![](../Images/2f2b73592e696a5c5c60006e9ccb5ee3.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f2b73592e696a5c5c60006e9ccb5ee3.png)'
- en: 'Figure 10: embedding of first document â€” Image by the author'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 10ï¼šç¬¬ä¸€ä¸ªæ–‡æ¡£çš„åµŒå…¥ â€” ä½œè€…æä¾›çš„å›¾ç‰‡
- en: Similarly we can compute word (term) embeddings as *emb = [<term, u1> , <term,
    u2> , <term, u3>].*
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼åœ°ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—è¯ï¼ˆæœ¯è¯­ï¼‰åµŒå…¥ä¸º*emb = [<term, u1> , <term, u2> , <term, u3>]ã€‚*
- en: '![](../Images/c3809d1aa915f00bde28ec158c6be8a4.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c3809d1aa915f00bde28ec158c6be8a4.png)'
- en: 'Figure 11: term embedding for first term â€” Image by the author'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 11ï¼šç¬¬ä¸€ä¸ªæœ¯è¯­çš„è¯åµŒå…¥ â€” ä½œè€…æä¾›çš„å›¾ç‰‡
- en: We can show that these representations group similar terms and documents together
    in the 3-dimensional space. Terms and documents that are related or have similar
    context tend to have similar representations in this reduced space.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥è¯æ˜ï¼Œè¿™äº›è¡¨ç¤ºå°†ç›¸ä¼¼çš„æœ¯è¯­å’Œæ–‡æ¡£åˆ†ç»„åœ¨ä¸‰ç»´ç©ºé—´ä¸­ã€‚ç›¸å…³æˆ–å…·æœ‰ç›¸ä¼¼ä¸Šä¸‹æ–‡çš„æœ¯è¯­å’Œæ–‡æ¡£åœ¨è¿™ä¸ªé™ç»´ç©ºé—´ä¸­å¾€å¾€å…·æœ‰ç›¸ä¼¼çš„è¡¨ç¤ºã€‚
- en: 'ğŸ’» In python, we use scikit-learn to convert a corpus of document to tf-idf
    matrices and then apply SVD on them. Here is an example:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ’» åœ¨ Python ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ scikit-learn å°†æ–‡æ¡£è¯­æ–™åº“è½¬æ¢ä¸º tf-idf çŸ©é˜µï¼Œç„¶åå¯¹å…¶åº”ç”¨ SVDã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼š
- en: '[PRE3]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Overall, SVD is a simple and powerful technique for maintaining semantic information
    however, it is impractical on real-world datasets. Itâ€™s computationally heavy
    and does not utilize sparsity of the input matrix at all. Letâ€™s see how we can
    avoid these shortcomings in another method.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»çš„æ¥è¯´ï¼ŒSVD æ˜¯ä¸€ç§ç®€å•ä¸”å¼ºå¤§çš„æŠ€æœ¯ï¼Œç”¨äºä¿æŒè¯­ä¹‰ä¿¡æ¯ï¼Œä½†åœ¨å®é™…æ•°æ®é›†ä¸Šå¹¶ä¸å®ç”¨ã€‚å®ƒè®¡ç®—é‡å¤§ï¼Œå¹¶ä¸”å®Œå…¨æ²¡æœ‰åˆ©ç”¨è¾“å…¥çŸ©é˜µçš„ç¨€ç–æ€§ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åœ¨å¦ä¸€ç§æ–¹æ³•ä¸­é¿å…è¿™äº›ç¼ºé™·ã€‚
- en: '**Neural Networks as Embedder**'
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**ç¥ç»ç½‘ç»œä½œä¸ºåµŒå…¥å™¨**'
- en: 'ğŸŒŸState of the art embedders are among Neural Networks (NN). There are many
    NN techniques to compute word embeddings: Word2Vec, Glove, BERT, fastText, etc.
    In this article, we look at *Word2Vec* which was developed by [Tomas Mikolov](https://scholar.google.com/citations?user=oBu8kMMAAAAJ&hl=en)
    and his colleagues at Google in a series of papers published between 2013 and
    2014 ([paper](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸŒŸ ç›®å‰æœ€å…ˆè¿›çš„åµŒå…¥å™¨ä¹‹ä¸€æ˜¯ç¥ç»ç½‘ç»œï¼ˆNNï¼‰ã€‚æœ‰è®¸å¤šç¥ç»ç½‘ç»œæŠ€æœ¯å¯ä»¥è®¡ç®—è¯åµŒå…¥ï¼šWord2Vecã€Gloveã€BERTã€fastText ç­‰ã€‚æœ¬æ–‡è®¨è®ºäº†*Word2Vec*ï¼Œè¯¥æ–¹æ³•ç”±[æ‰˜é©¬æ–¯Â·ç±³ç§‘æ´›å¤«](https://scholar.google.com/citations?user=oBu8kMMAAAAJ&hl=en)åŠå…¶åœ¨è°·æ­Œçš„åŒäº‹ä»¬å¼€å‘ï¼Œå¹¶åœ¨
    2013 å¹´è‡³ 2014 å¹´é—´å‘è¡¨çš„ä¸€ç³»åˆ—è®ºæ–‡ä¸­ä»‹ç»äº†è¯¥æ–¹æ³•ï¼ˆ[è®ºæ–‡](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)ï¼‰ã€‚
- en: '*Word2Vec* is a statistical, self-supervised, and task independent method.
    It comes in two flavors: Continuous bag of words (CBOW), and Skip Gram. The two
    flavors are very similar, both use a shallow neural network with only one hidden
    layer and no activation function to learn the words representations. In this article,
    we study the skip-gram model.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*Word2Vec* æ˜¯ä¸€ç§ç»Ÿè®¡ã€è‡ªç›‘ç£ã€ä¸ä»»åŠ¡æ— å…³çš„æ–¹æ³•ã€‚å®ƒæœ‰ä¸¤ç§å˜ä½“ï¼šè¿ç»­è¯è¢‹æ¨¡å‹ï¼ˆCBOWï¼‰å’Œ Skip Gramã€‚è¿™ä¸¤ç§å˜ä½“éå¸¸ç›¸ä¼¼ï¼Œéƒ½ä½¿ç”¨ä¸€ä¸ªåªæœ‰ä¸€å±‚éšè—å±‚ä¸”æ²¡æœ‰æ¿€æ´»å‡½æ•°çš„æµ…å±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ è¯è¯­è¡¨ç¤ºã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶
    skip-gram æ¨¡å‹ã€‚'
- en: 'ğŸ”‘ The key idea of word2Vec is that words with similar context have similar
    meanings. The more often a word appears in the ***context*** of certain other
    words, the closer they are in meaning. *Context* of a word are few words to its
    left and few words to its right. Formally, given a document, we set a window size
    (e.g. window_size = 2). Then for any given word in the document (call it *`target`*
    word), the *`window_size`* words to its left and *`window_size`* words to its
    right are its *context* . For example:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”‘ Word2Vec çš„å…³é”®æ€æƒ³æ˜¯å…·æœ‰ç›¸ä¼¼ä¸Šä¸‹æ–‡çš„è¯è¯­å…·æœ‰ç›¸ä¼¼çš„å«ä¹‰ã€‚ä¸€ä¸ªè¯åœ¨æŸäº›å…¶ä»–è¯çš„***ä¸Šä¸‹æ–‡***ä¸­å‡ºç°å¾—è¶Šé¢‘ç¹ï¼Œå®ƒä»¬çš„å«ä¹‰å°±è¶Šæ¥è¿‘ã€‚ä¸€ä¸ªè¯çš„*ä¸Šä¸‹æ–‡*æ˜¯å®ƒå·¦è¾¹å’Œå³è¾¹çš„å‡ ä¸ªè¯ã€‚å½¢å¼ä¸Šï¼Œç»™å®šä¸€ä¸ªæ–‡æ¡£ï¼Œæˆ‘ä»¬è®¾ç½®ä¸€ä¸ªçª—å£å¤§å°ï¼ˆä¾‹å¦‚ï¼Œwindow_size
    = 2ï¼‰ã€‚é‚£ä¹ˆå¯¹äºæ–‡æ¡£ä¸­çš„ä»»ä½•ç»™å®šè¯ï¼ˆç§°ä¸º*`ç›®æ ‡`*è¯ï¼‰ï¼Œå®ƒå·¦è¾¹çš„*`window_size`* ä¸ªè¯å’Œå³è¾¹çš„*`window_size`* ä¸ªè¯å°±æ˜¯å®ƒçš„*ä¸Šä¸‹æ–‡*ã€‚ä¾‹å¦‚ï¼š
- en: '![](../Images/2b9f9598081ebed87d9e2cac044fe371.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b9f9598081ebed87d9e2cac044fe371.png)'
- en: 'Figure 12: target and context words (window size = 2 )â€” Image by the author'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾12ï¼šç›®æ ‡è¯å’Œä¸Šä¸‹æ–‡è¯ï¼ˆçª—å£å¤§å° = 2ï¼‰ â€” å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: 'Given a document, we can slide the window from left to right and find all pairs
    of *(target, context)* words. Consider the following document:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šä¸€ä¸ªæ–‡æ¡£ï¼Œæˆ‘ä»¬å¯ä»¥ä»å·¦åˆ°å³æ»‘åŠ¨çª—å£ï¼Œæ‰¾åˆ°æ‰€æœ‰çš„*(ç›®æ ‡è¯, ä¸Šä¸‹æ–‡è¯)* å¯¹ã€‚è€ƒè™‘ä»¥ä¸‹æ–‡æ¡£ï¼š
- en: 'Document: *â€œI read sci-fi books and drink orange juiceâ€*,'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: æ–‡æ¡£ï¼š*â€œæˆ‘è¯»ç§‘å¹»ä¹¦ç±å¹¶å–æ©™æ±â€*ï¼Œ
- en: Let window size = 2\. Then the set of (target, context) words are depicted below.
    The highlighted word in the image is the `target` word, and other words in the
    box are `context` words.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾çª—å£å¤§å° = 2ã€‚é‚£ä¹ˆ (ç›®æ ‡è¯, ä¸Šä¸‹æ–‡è¯) çš„é›†åˆå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚å›¾ç‰‡ä¸­é«˜äº®çš„è¯æ˜¯`ç›®æ ‡`è¯ï¼Œæ¡†ä¸­çš„å…¶ä»–è¯æ˜¯`ä¸Šä¸‹æ–‡`è¯ã€‚
- en: '![](../Images/79792923bb530410d7b8ecd8577fa349.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/79792923bb530410d7b8ecd8577fa349.png)'
- en: 'Figure 13: sliding window on a document to extract (target, context) words
    â€” Image by the author'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾13ï¼šåœ¨æ–‡æ¡£ä¸Šæ»‘åŠ¨çª—å£ä»¥æå–ï¼ˆç›®æ ‡è¯ï¼Œä¸Šä¸‹æ–‡è¯ï¼‰ â€” å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: 'Now, that we know the meaning of target and context words, letâ€™s see how Word2vec
    uses them. Word2vec is a 2-layer neural network that takes a target word as input,
    and predicts all context words in the window. Here is a schematic view of its
    architecture for window size = 2:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬çŸ¥é“äº†ç›®æ ‡è¯å’Œä¸Šä¸‹æ–‡è¯çš„å«ä¹‰ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹ Word2vec å¦‚ä½•ä½¿ç”¨å®ƒä»¬ã€‚Word2vec æ˜¯ä¸€ä¸ª2å±‚ç¥ç»ç½‘ç»œï¼Œå®ƒä»¥ç›®æ ‡è¯ä½œä¸ºè¾“å…¥ï¼Œå¹¶é¢„æµ‹çª—å£ä¸­çš„æ‰€æœ‰ä¸Šä¸‹æ–‡è¯ã€‚ä»¥ä¸‹æ˜¯çª—å£å¤§å°
    = 2 çš„æ¶æ„ç¤ºæ„å›¾ï¼š
- en: '![](../Images/3f133049b4f0df7d8f3ce51032f4b5ad.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f133049b4f0df7d8f3ce51032f4b5ad.png)'
- en: 'Figure 14: skip-gram architecture for window size = 2 â€” Image by the author'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾14ï¼šçª—å£å¤§å° = 2 çš„skip-gramæ¶æ„ â€” å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: Now, we can guess why this architecture is called skip-gramğŸ™‚â€¦ Itâ€™s because it
    predicts all the grams in the context except the target word as it is the input
    to the model, hence the name is skip-gram.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥çŒœæµ‹ä¸ºä»€ä¹ˆè¿™ä¸ªæ¶æ„è¢«ç§°ä¸ºskip-gramğŸ™‚â€¦ è¿™æ˜¯å› ä¸ºå®ƒé¢„æµ‹ä¸Šä¸‹æ–‡ä¸­çš„æ‰€æœ‰è¯ç»„ï¼Œé™¤äº†ç›®æ ‡è¯ï¼Œå› ä¸ºç›®æ ‡è¯æ˜¯æ¨¡å‹çš„è¾“å…¥ï¼Œæ‰€ä»¥è¿™ä¸ªåå­—å«åšskip-gramã€‚
- en: 'Now, letâ€™s focus on predicting only one context word and dive into details.
    Let *V = size of the vocabulary* and *N = embedding dimension*, i.e. the size
    of the only hidden layer. Then the architecture for predicting one context word
    is:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬ä¸“æ³¨äºåªé¢„æµ‹ä¸€ä¸ªä¸Šä¸‹æ–‡è¯ï¼Œå¹¶æ·±å…¥ç»†èŠ‚ã€‚è®¾*V = è¯æ±‡è¡¨çš„å¤§å°*å’Œ*N = åµŒå…¥ç»´åº¦*ï¼Œå³å”¯ä¸€éšè—å±‚çš„å¤§å°ã€‚åˆ™ç”¨äºé¢„æµ‹ä¸€ä¸ªä¸Šä¸‹æ–‡è¯çš„æ¶æ„æ˜¯ï¼š
- en: '![](../Images/04747218c33cee937ef2ad6544131be0.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04747218c33cee937ef2ad6544131be0.png)'
- en: 'Figure 15: skip-gram architecture for predicting one context word â€” Image by
    the author'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾15ï¼šç”¨äºé¢„æµ‹ä¸€ä¸ªä¸Šä¸‹æ–‡è¯çš„skip-gramæ¶æ„ â€” å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: 'ğŸ§¬The input layer is the one-hot encoding of the target word i.e. w(t); since
    we have V word in our vocabulary the input layer is V-dimensional. The hidden
    layer is N-dimensional and produces the embeddings. Often *N << V,* for example
    in a corpus of web documents, we have millions of tokens (V is of order of millions)
    while N is somewhere between 256 to 512\. The point of the hidden layer is to
    map the words into a lower dimensionality while maintaining separation between
    dissimilar words. This layer does not use any activation function. The reason
    is most activation functions involve some â€œsquishingâ€ of space in one region and
    â€œexpandingâ€ of space in the other e.g., sigmoid/tanh will â€œsquish togetherâ€ all
    values < -1, and same with all values >1\. RELU would â€œsquish togetherâ€ all values
    <0, eliminating half of the representative capacity. As you can see, this actually
    *hurts* the separation quality, reducing the amount of â€œword mapping spaceâ€ available.
    The output layer is V dimensional, a softmax function applies on this layer therefore
    each neuron in the output layer is a probability and all neurons sum up to 1\.
    The j-th neuron in output layer i.e. y_j, indicates the probability that the j-th
    word is the context word. As we see, there are two weight matrices involved:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ§¬è¾“å…¥å±‚æ˜¯ç›®æ ‡è¯çš„ one-hot ç¼–ç ï¼Œå³ w(t)ï¼›ç”±äºæˆ‘ä»¬åœ¨è¯æ±‡è¡¨ä¸­æœ‰ V ä¸ªè¯ï¼Œè¾“å…¥å±‚æ˜¯ V ç»´çš„ã€‚éšè—å±‚æ˜¯ N ç»´çš„ï¼Œå¹¶ç”ŸæˆåµŒå…¥ã€‚é€šå¸¸ *N
    << V*ï¼Œä¾‹å¦‚åœ¨ä¸€ä¸ªç½‘é¡µæ–‡æ¡£çš„è¯­æ–™åº“ä¸­ï¼Œæˆ‘ä»¬æœ‰æ•°ç™¾ä¸‡ä¸ªè¯æ±‡ï¼ˆV çº§åˆ«ä¸ºç™¾ä¸‡çº§ï¼‰ï¼Œè€Œ N åœ¨ 256 åˆ° 512 ä¹‹é—´ã€‚éšè—å±‚çš„ç›®çš„æ˜¯å°†è¯æ˜ å°„åˆ°ä¸€ä¸ªæ›´ä½çš„ç»´åº¦ï¼ŒåŒæ—¶ä¿æŒä¸åŒè¯ä¹‹é—´çš„åˆ†ç¦»ã€‚è¿™ä¸ªå±‚ä¸ä½¿ç”¨ä»»ä½•æ¿€æ´»å‡½æ•°ã€‚åŸå› æ˜¯å¤§å¤šæ•°æ¿€æ´»å‡½æ•°æ¶‰åŠåœ¨ä¸€ä¸ªåŒºåŸŸâ€œå‹ç¼©â€ç©ºé—´ï¼Œè€Œåœ¨å¦ä¸€ä¸ªåŒºåŸŸâ€œæ‰©å±•â€ç©ºé—´ï¼Œä¾‹å¦‚
    sigmoid/tanh ä¼šå°†æ‰€æœ‰å€¼ < -1â€œå‹ç¼©åœ¨ä¸€èµ·â€ï¼Œæ‰€æœ‰å€¼ >1 åŒæ ·å¦‚æ­¤ã€‚RELU ä¼šå°†æ‰€æœ‰å€¼ <0â€œå‹ç¼©åœ¨ä¸€èµ·â€ï¼Œä»è€Œæ¶ˆé™¤äº†ä»£è¡¨èƒ½åŠ›çš„ä¸€åŠã€‚å¦‚ä½ æ‰€è§ï¼Œè¿™å®é™…ä¸Šä¼š
    *æŸå®³* åˆ†ç¦»è´¨é‡ï¼Œå‡å°‘å¯ç”¨çš„â€œè¯æ˜ å°„ç©ºé—´â€é‡ã€‚è¾“å‡ºå±‚æ˜¯ V ç»´çš„ï¼Œsoftmax å‡½æ•°ä½œç”¨äºæ­¤å±‚ï¼Œå› æ­¤è¾“å‡ºå±‚ä¸­çš„æ¯ä¸ªç¥ç»å…ƒéƒ½æ˜¯ä¸€ä¸ªæ¦‚ç‡ï¼Œæ‰€æœ‰ç¥ç»å…ƒçš„æ€»å’Œä¸º
    1ã€‚è¾“å‡ºå±‚çš„ç¬¬ j ä¸ªç¥ç»å…ƒï¼Œå³ y_jï¼Œè¡¨ç¤ºç¬¬ j ä¸ªè¯æ˜¯ä¸Šä¸‹æ–‡è¯çš„æ¦‚ç‡ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œè¿™é‡Œæ¶‰åŠä¸¤ä¸ªæƒé‡çŸ©é˜µï¼š
- en: '![](../Images/9e9756e6106ff636cf8b364a37c9067c.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e9756e6106ff636cf8b364a37c9067c.png)'
- en: Given the topology of the network, if ***x*** is the input and ***y*** is the
    output, then
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šç½‘ç»œçš„æ‹“æ‰‘ç»“æ„ï¼Œå¦‚æœ ***x*** æ˜¯è¾“å…¥ï¼Œ***y*** æ˜¯è¾“å‡ºï¼Œåˆ™
- en: '![](../Images/188405c242c36054b4f5597c4e1ef14f.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/188405c242c36054b4f5597c4e1ef14f.png)'
- en: After the network is trained, we obtain the embedding of a word by multiplying
    its one-hot vector *x* with the weight matrix *W*.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ç½‘ç»œè®­ç»ƒå®Œæˆåï¼Œæˆ‘ä»¬é€šè¿‡å°†å…¶ one-hot å‘é‡ *x* ä¸æƒé‡çŸ©é˜µ *W* ç›¸ä¹˜æ¥è·å¾—è¯çš„åµŒå…¥ã€‚
- en: 'ğŸ”**How is the network trained?** Since there are no labels, we will create
    a fake task ğŸ˜ƒ! The fake task is:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”**ç½‘ç»œæ˜¯å¦‚ä½•è®­ç»ƒçš„ï¼Ÿ** ç”±äºæ²¡æœ‰æ ‡ç­¾ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿä»»åŠ¡ ğŸ˜ƒï¼è¿™ä¸ªè™šæ‹Ÿä»»åŠ¡æ˜¯ï¼š
- en: 'ğŸ“‘*The fake task: `given a target word, predict its context words`*'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ“‘*è™šæ‹Ÿä»»åŠ¡ï¼š`ç»™å®šä¸€ä¸ªç›®æ ‡è¯ï¼Œé¢„æµ‹å…¶ä¸Šä¸‹æ–‡è¯`*
- en: 'Now, there are two questions: 1) how to make the training data, 2) what loss
    function to choose to train the network?'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæœ‰ä¸¤ä¸ªé—®é¢˜ï¼š1ï¼‰å¦‚ä½•ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œ2ï¼‰é€‰æ‹©ä»€ä¹ˆæŸå¤±å‡½æ•°æ¥è®­ç»ƒç½‘ç»œï¼Ÿ
- en: 'ğŸ—’**How to make the training data?** We take our corpus of *data = {d1, d2,
    â€¦.}*, we might collect millions of documents, wiki pages, blot posts, etc. Then
    we tokenize all documents and build a vocabulary. There are many tokenization
    methods available e.g. workpiece, BytePairEncoding (BPE), k-gram. The simplest
    method is k-gram. If you take *k=1* and do tokenization by words (instead of characters)
    then 1-gram is equivalent to split sentences by space! After tokenization, we
    have a list of tokenized documents. We then move the sliding window over tokenized
    documents and collect training data as pairs of (target, context). See the example
    below:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ—’**å¦‚ä½•ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Ÿ** æˆ‘ä»¬å–æˆ‘ä»¬çš„è¯­æ–™åº“ *data = {d1, d2, â€¦.}*ï¼Œå¯èƒ½ä¼šæ”¶é›†åˆ°æ•°ç™¾ä¸‡ç¯‡æ–‡æ¡£ã€ç»´åŸºé¡µé¢ã€åšå®¢æ–‡ç« ç­‰ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹æ‰€æœ‰æ–‡æ¡£è¿›è¡Œåˆ†è¯ï¼Œå¹¶å»ºç«‹è¯æ±‡è¡¨ã€‚å¯ç”¨çš„åˆ†è¯æ–¹æ³•æœ‰å¾ˆå¤šï¼Œä¾‹å¦‚
    workpieceã€BytePairEncoding (BPE)ã€k-gramã€‚æœ€ç®€å•çš„æ–¹æ³•æ˜¯ k-gramã€‚å¦‚æœä½ å– *k=1* å¹¶æŒ‰è¯è¿›è¡Œåˆ†è¯ï¼ˆè€Œä¸æ˜¯æŒ‰å­—ç¬¦ï¼‰ï¼Œé‚£ä¹ˆ
    1-gram ç›¸å½“äºæŒ‰ç©ºæ ¼åˆ†å‰²å¥å­ï¼åˆ†è¯åï¼Œæˆ‘ä»¬å¾—åˆ°ä¸€ä¸ªåˆ†è¯æ–‡æ¡£çš„åˆ—è¡¨ã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨åˆ†è¯æ–‡æ¡£ä¸Šæ»‘åŠ¨çª—å£ï¼Œå¹¶å°†è®­ç»ƒæ•°æ®æ”¶é›†ä¸º (target, context)
    çš„å¯¹ã€‚è¯·å‚è§ä¸‹é¢çš„ç¤ºä¾‹ï¼š
- en: '![](../Images/29424af3ffd1c684d0908e195c8a4f3f.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29424af3ffd1c684d0908e195c8a4f3f.png)'
- en: 'Figure 16: collecting training data from a document â€” Image by the author'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 16ï¼šä»æ–‡æ¡£ä¸­æ”¶é›†è®­ç»ƒæ•°æ® â€” ä½œè€…æä¾›çš„å›¾åƒ
- en: 'ğŸ“ˆ**what loss function to choose?** We use cross-entropy loss function. The
    reason is we train against the target-context pairs (w_t, w_c) where w_t is the
    target word and w_c is the context word. The context word w_c represents the ideal
    prediction, given the target word w_t. Note that W_c is represented as one-hot,
    i.e. it has value 1 at some position j and other positions are 0:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ“ˆ**é€‰æ‹©ä»€ä¹ˆæŸå¤±å‡½æ•°ï¼Ÿ** æˆ‘ä»¬ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°ã€‚åŸå› æ˜¯æˆ‘ä»¬é’ˆå¯¹ç›®æ ‡-ä¸Šä¸‹æ–‡å¯¹ (w_t, w_c) è¿›è¡Œè®­ç»ƒï¼Œå…¶ä¸­ w_t æ˜¯ç›®æ ‡è¯ï¼Œw_c æ˜¯ä¸Šä¸‹æ–‡è¯ã€‚ä¸Šä¸‹æ–‡è¯
    w_c è¡¨ç¤ºç»™å®šç›®æ ‡è¯ w_t çš„ç†æƒ³é¢„æµ‹ã€‚æ³¨æ„ W_c ä»¥ç‹¬çƒ­ç¼–ç è¡¨ç¤ºï¼Œå³åœ¨æŸä¸ªä½ç½® j çš„å€¼ä¸º 1ï¼Œå…¶ä»–ä½ç½®ä¸º 0ï¼š
- en: '![](../Images/8c99d46e9872335b145a7d9a2b0863fe.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c99d46e9872335b145a7d9a2b0863fe.png)'
- en: 'Figure 17: one-hot encoding of context word â€” Image by the author'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 17ï¼šä¸Šä¸‹æ–‡è¯çš„ç‹¬çƒ­ç¼–ç  â€” ä½œè€…æä¾›çš„å›¾ç‰‡
- en: The loss function needs to evaluate the output layer at the same position j,
    i.e. y_j. (Remember y is a probability distribution; ideal value of y_j is being
    1). The cross-entropy loss function is defined as
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: æŸå¤±å‡½æ•°éœ€è¦åœ¨ç›¸åŒä½ç½® j è¯„ä¼°è¾“å‡ºå±‚ï¼Œå³ y_jã€‚ï¼ˆè®°ä½ y æ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼›y_j çš„ç†æƒ³å€¼ä¸º 1ï¼‰ã€‚äº¤å‰ç†µæŸå¤±å‡½æ•°å®šä¹‰ä¸º
- en: '![](../Images/03abb4184f315c05058444fd1c8ecdf2.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03abb4184f315c05058444fd1c8ecdf2.png)'
- en: For the example above,
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä¸Šè¿°ç¤ºä¾‹ï¼Œ
- en: '![](../Images/185c44256b650df1b21faa401bcbfbae.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/185c44256b650df1b21faa401bcbfbae.png)'
- en: 'Figure 17: cross entropy loss in an example â€” Image by the author'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 17ï¼šç¤ºä¾‹ä¸­çš„äº¤å‰ç†µæŸå¤± â€” ä½œè€…æä¾›çš„å›¾ç‰‡
- en: 'ğŸ”¥**Training the neural network**: Now that the loss function and training data
    are clear, we will train the network to learn the weights. We want to find the
    values of ***W*** and ***Wâ€²*** that minimize the loss function. We do so by running
    back-propagation and updating parameters based on derivative of the loss. This
    concludes the word2vec algorithm.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ”¥**è®­ç»ƒç¥ç»ç½‘ç»œ**ï¼šæ—¢ç„¶æŸå¤±å‡½æ•°å’Œè®­ç»ƒæ•°æ®å·²ç»æ˜ç¡®ï¼Œæˆ‘ä»¬å°†è®­ç»ƒç½‘ç»œä»¥å­¦ä¹ æƒé‡ã€‚æˆ‘ä»¬å¸Œæœ›æ‰¾åˆ°ä½¿æŸå¤±å‡½æ•°æœ€å°çš„ ***W*** å’Œ ***Wâ€²*** çš„å€¼ã€‚æˆ‘ä»¬é€šè¿‡åå‘ä¼ æ’­è¿è¡Œå¹¶æ ¹æ®æŸå¤±çš„å¯¼æ•°æ›´æ–°å‚æ•°æ¥å®ç°è¿™ä¸€ç‚¹ã€‚è¿™å°±ç»“æŸäº†
    word2vec ç®—æ³•ã€‚
- en: ğŸ“Š**Results on word embeddings:** Below, we see an example of vector representations
    of four words *â€œman,â€ â€œwoman,â€ â€œkingâ€ â€œqueenâ€* that illustrate how Word2Vec captures
    semantic. In the context of Word2Vec, the relationships between words can be represented
    as vector arithmetic. The algorithm captures relationships like *â€œking â€” man +
    woman = queenâ€.* This is possible because the vector representation of â€œkingâ€
    minus the vector representation of â€œmanâ€ plus the vector representation of â€œwomanâ€
    results in a vector that is very close to the vector representation of â€œqueen.â€
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ“Š**è¯åµŒå…¥çš„ç»“æœï¼š** ä¸‹é¢ï¼Œæˆ‘ä»¬çœ‹åˆ°å››ä¸ªè¯ *â€œman,â€ â€œwoman,â€ â€œking,â€ â€œqueenâ€* çš„å‘é‡è¡¨ç¤ºç¤ºä¾‹ï¼Œè¿™äº›ç¤ºä¾‹è¯´æ˜äº† Word2Vec
    å¦‚ä½•æ•æ‰è¯­ä¹‰ã€‚åœ¨ Word2Vec çš„èƒŒæ™¯ä¸‹ï¼Œè¯ä¹‹é—´çš„å…³ç³»å¯ä»¥è¡¨ç¤ºä¸ºå‘é‡è¿ç®—ã€‚ç®—æ³•æ•æ‰åˆ°åƒ *â€œking â€” man + woman = queenâ€* è¿™æ ·çš„å…³ç³»ã€‚è¿™æ˜¯å› ä¸ºâ€œkingâ€çš„å‘é‡è¡¨ç¤ºå‡å»â€œmanâ€çš„å‘é‡è¡¨ç¤ºå†åŠ ä¸Šâ€œwomanâ€çš„å‘é‡è¡¨ç¤ºï¼Œç»“æœæ˜¯ä¸€ä¸ªéå¸¸æ¥è¿‘â€œqueenâ€çš„å‘é‡è¡¨ç¤ºã€‚
- en: '![](../Images/cf41ccda2f851553c0d66f601054506e.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf41ccda2f851553c0d66f601054506e.png)'
- en: 'Figure 18: word embeddings capture semantics â€” Image by the author'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 18ï¼šè¯åµŒå…¥æ•æ‰è¯­ä¹‰ â€” ä½œè€…æä¾›çš„å›¾ç‰‡
- en: Itâ€™s important to note that Word2Vec is trained on large text corpora, and the
    quality of learned embeddings depends on the quantity and quality of the training
    data. Additionally, while Word2Vec can capture many semantic relationships, it
    might not capture all possible nuances of language and meaning. Other word embedding
    algorithms and models have since been developed that further enhance the understanding
    of word relationships.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: é‡è¦çš„æ˜¯è¦æ³¨æ„ï¼ŒWord2Vec æ˜¯åœ¨å¤§å‹æ–‡æœ¬è¯­æ–™åº“ä¸Šè®­ç»ƒçš„ï¼Œå­¦ä¹ åˆ°çš„åµŒå…¥è´¨é‡å–å†³äºè®­ç»ƒæ•°æ®çš„æ•°é‡å’Œè´¨é‡ã€‚æ­¤å¤–ï¼Œè™½ç„¶ Word2Vec å¯ä»¥æ•æ‰è®¸å¤šè¯­ä¹‰å…³ç³»ï¼Œä½†å®ƒå¯èƒ½æ— æ³•æ•æ‰è¯­è¨€å’Œæ„ä¹‰çš„æ‰€æœ‰å¯èƒ½ç»†å¾®å·®åˆ«ã€‚ä¹‹åè¿˜å¼€å‘äº†å…¶ä»–è¯åµŒå…¥ç®—æ³•å’Œæ¨¡å‹ï¼Œä»¥è¿›ä¸€æ­¥å¢å¼ºå¯¹è¯å…³ç³»çš„ç†è§£ã€‚
- en: '**Item-Item Neural Collaborative Filtering**'
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**ç‰©å“-ç‰©å“ç¥ç»ååŒè¿‡æ»¤**'
- en: In this section, we will look into another example of learning embeddings through
    neural network. This network models item-item collaborative filtering algorithm.
    We will study this network through the use case of video recommendation.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨é€šè¿‡ç¥ç»ç½‘ç»œå­¦ä¹ åµŒå…¥çš„å¦ä¸€ä¸ªç¤ºä¾‹ã€‚è¯¥ç½‘ç»œå»ºæ¨¡äº†ç‰©å“-ç‰©å“ååŒè¿‡æ»¤ç®—æ³•ã€‚æˆ‘ä»¬å°†é€šè¿‡è§†é¢‘æ¨èçš„ç”¨ä¾‹æ¥ç ”ç©¶è¿™ä¸ªç½‘ç»œã€‚
- en: 'ğŸ¥**Video recommendation usecase**: Consider the use case of video recommendation
    where we have 1 million videos, and 500,000 users who have watched some of these
    movies, and the task at hand is to recommend videos to users. We want to solve
    this problem using neural networks, so we formulate it as multi-class classification
    where each video is a class. We design a neural network to learn embeddings for
    videos such that similar videos have similar embeddings. Letâ€™s build the training
    data first, and then design the network architecture.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ¥**è§†é¢‘æ¨èç”¨ä¾‹**ï¼šè€ƒè™‘ä¸€ä¸ªè§†é¢‘æ¨èçš„ç”¨ä¾‹ï¼Œæˆ‘ä»¬æœ‰100ä¸‡ä¸ªè§†é¢‘å’Œ50ä¸‡åç”¨æˆ·ï¼Œè¿™äº›ç”¨æˆ·è§‚çœ‹äº†ä¸€äº›è¿™äº›è§†é¢‘ï¼Œæˆ‘ä»¬çš„ä»»åŠ¡æ˜¯å‘ç”¨æˆ·æ¨èè§†é¢‘ã€‚æˆ‘ä»¬æƒ³ç”¨ç¥ç»ç½‘ç»œè§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå› æ­¤æˆ‘ä»¬å°†å…¶è¡¨è¿°ä¸ºå¤šç±»åˆ†ç±»ï¼Œæ¯ä¸ªè§†é¢‘å°±æ˜¯ä¸€ä¸ªç±»åˆ«ã€‚æˆ‘ä»¬è®¾è®¡ä¸€ä¸ªç¥ç»ç½‘ç»œæ¥å­¦ä¹ è§†é¢‘çš„åµŒå…¥ï¼Œä½¿å¾—ç›¸ä¼¼çš„è§†é¢‘å…·æœ‰ç›¸ä¼¼çš„åµŒå…¥ã€‚é¦–å…ˆè®©æˆ‘ä»¬æ„å»ºè®­ç»ƒæ•°æ®ï¼Œç„¶åè®¾è®¡ç½‘ç»œæ¶æ„ã€‚
- en: ğŸ—’**Building Training Data:** Given users logs that contain videos users have
    watched, we sort out watched videos of each user in ascending order in time. Then
    split them in time on a fixed proportion. We take the first split as train data
    and the second split as test data. Note that if our split ratio is 70â€“30, the
    data of each user is split on this ratio. By this time-based split, our task has
    implicitly become *â€˜predicting what users watch nextâ€™*.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ—’**æ„å»ºè®­ç»ƒæ•°æ®ï¼š** ç»™å®šåŒ…å«ç”¨æˆ·è§‚çœ‹è§†é¢‘çš„æ—¥å¿—ï¼Œæˆ‘ä»¬æŒ‰æ—¶é—´å‡åºæ’åˆ—æ¯ä¸ªç”¨æˆ·è§‚çœ‹çš„è§†é¢‘ã€‚ç„¶åæŒ‰å›ºå®šæ¯”ä¾‹å°†å…¶åˆ†å‰²ã€‚æˆ‘ä»¬å°†ç¬¬ä¸€æ¬¡åˆ†å‰²ä½œä¸ºè®­ç»ƒæ•°æ®ï¼Œç¬¬äºŒæ¬¡åˆ†å‰²ä½œä¸ºæµ‹è¯•æ•°æ®ã€‚è¯·æ³¨æ„ï¼Œå¦‚æœæˆ‘ä»¬çš„åˆ†å‰²æ¯”ä¾‹æ˜¯70-30ï¼Œåˆ™æ¯ä¸ªç”¨æˆ·çš„æ•°æ®æŒ‰è¿™ä¸ªæ¯”ä¾‹è¿›è¡Œåˆ†å‰²ã€‚é€šè¿‡è¿™ç§åŸºäºæ—¶é—´çš„åˆ†å‰²ï¼Œæˆ‘ä»¬çš„ä»»åŠ¡éšå¼åœ°å˜æˆäº†*â€˜é¢„æµ‹ç”¨æˆ·æ¥ä¸‹æ¥è§‚çœ‹ä»€ä¹ˆâ€™*ã€‚
- en: Another way of splitting is to avoid sorting and randomly hold out few videos
    of each user as test data, and use the rest to build train data. This approach
    however, runs the risk of leaking information from train to test.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ç§åˆ†å‰²æ–¹å¼æ˜¯é¿å…æ’åºï¼Œéšæœºä¿ç•™æ¯ä¸ªç”¨æˆ·çš„ä¸€äº›è§†é¢‘ä½œä¸ºæµ‹è¯•æ•°æ®ï¼Œä½¿ç”¨å…¶ä½™çš„è§†é¢‘æ„å»ºè®­ç»ƒæ•°æ®ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•æœ‰æ³„éœ²ä»è®­ç»ƒåˆ°æµ‹è¯•çš„ä¿¡æ¯çš„é£é™©ã€‚
- en: '![](../Images/45466c3c9f7f17936df09c768aa2daa0.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/45466c3c9f7f17936df09c768aa2daa0.png)'
- en: 'Figure 19: video recommendation train-test split â€” Image by the author'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾19ï¼šè§†é¢‘æ¨èè®­ç»ƒ-æµ‹è¯•åˆ†å‰² â€” å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: Once data is split, we will build train data as pairs of (movie1, movie2) where
    both movies are watched by the same user.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®åˆ†å‰²åï¼Œæˆ‘ä»¬å°†è®­ç»ƒæ•°æ®æ„å»ºä¸º(movie1, movie2)å¯¹ï¼Œå…¶ä¸­ä¸¤ä¸ªç”µå½±éƒ½ç”±åŒä¸€ä¸ªç”¨æˆ·è§‚çœ‹ã€‚
- en: '![](../Images/d6cd414b95017f6fef54ed2233320aeb.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6cd414b95017f6fef54ed2233320aeb.png)'
- en: 'Figure 20: building training data, and test data â€” Image by the author'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾20ï¼šæ„å»ºè®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ® â€” å›¾ç‰‡ç”±ä½œè€…æä¾›
- en: We then build a neural network that performs item-item collaborative filtering
    while learning 3-dim embeddings!! (Three is too small but for illustrative purposes
    we continue with it.)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬æ„å»ºä¸€ä¸ªæ‰§è¡Œé¡¹ç›®-é¡¹ç›®ååŒè¿‡æ»¤çš„ç¥ç»ç½‘ç»œï¼ŒåŒæ—¶å­¦ä¹ 3ç»´åµŒå…¥ï¼ï¼ï¼ˆä¸‰ç»´è™½ç„¶å¤ªå°ï¼Œä½†ä¸ºäº†è¯´æ˜ç›®çš„æˆ‘ä»¬ç»§ç»­ä½¿ç”¨å®ƒã€‚ï¼‰
- en: '![](../Images/051b00575dd48aa45092f806b2752891.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/051b00575dd48aa45092f806b2752891.png)'
- en: 'Figure 21: item-item neural collaborative filtering â€” Image from [1] modified
    by author'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾21ï¼šé¡¹ç›®-é¡¹ç›®ç¥ç»ååŒè¿‡æ»¤ â€” å›¾ç‰‡æ¥æºäº[1]ï¼Œç”±ä½œè€…ä¿®æ”¹
- en: For every pair (m1, m2) in the training data, we feed one-hot vector of m1 in
    the input layer in blue, and feed m1 feature vectors (containing metadata such
    as genre, cast, director, popularity, etc.) in the yellow input layer. The one-hot
    of m2 will be used to compute the loss function. We want the model to learn the
    association between m1 and m2\. The more often m1 and m2 happen together in the
    training data, the closer their embedding will become.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè®­ç»ƒæ•°æ®ä¸­çš„æ¯ä¸€å¯¹(m1, m2)ï¼Œæˆ‘ä»¬åœ¨è“è‰²çš„è¾“å…¥å±‚ä¸­è¾“å…¥m1çš„ç‹¬çƒ­å‘é‡ï¼Œå¹¶åœ¨é»„è‰²è¾“å…¥å±‚ä¸­è¾“å…¥m1çš„ç‰¹å¾å‘é‡ï¼ˆåŒ…å«å¦‚ç±»å‹ã€æ¼”å‘˜ã€å¯¼æ¼”ã€å—æ¬¢è¿ç¨‹åº¦ç­‰å…ƒæ•°æ®ï¼‰ã€‚m2çš„ç‹¬çƒ­å‘é‡å°†ç”¨äºè®¡ç®—æŸå¤±å‡½æ•°ã€‚æˆ‘ä»¬å¸Œæœ›æ¨¡å‹å­¦ä¹ m1å’Œm2ä¹‹é—´çš„å…³è”ã€‚m1å’Œm2åœ¨è®­ç»ƒæ•°æ®ä¸­å‡ºç°å¾—è¶Šé¢‘ç¹ï¼Œå®ƒä»¬çš„åµŒå…¥å°±ä¼šè¶Šæ¥è¿‘ã€‚
- en: '**How to recommend a movie to a user?** Assume Alice has watched m1, m4, m5\.
    To recommend a movie to her, we can find movies that have similar embeddings to
    m1\. For any movie v we compute the similarity score as *score= <emb(m1), emb(v)>*.
    We find the top 5 movies with highest similarity score, and recommend them to
    Alice. An even better recommendation is to repeat above for m1, m4 and m5, and
    recommend movies in intersection of these sets.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¦‚ä½•å‘ç”¨æˆ·æ¨èç”µå½±ï¼Ÿ** å‡è®¾Aliceè§‚çœ‹äº†m1ã€m4ã€m5ã€‚ä¸ºäº†å‘å¥¹æ¨èç”µå½±ï¼Œæˆ‘ä»¬å¯ä»¥æ‰¾åˆ°ä¸m1å…·æœ‰ç›¸ä¼¼åµŒå…¥çš„ç”µå½±ã€‚å¯¹äºä»»ä½•ç”µå½±vï¼Œæˆ‘ä»¬è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°ä¸º*score=
    <emb(m1), emb(v)>*ã€‚æˆ‘ä»¬æ‰¾åˆ°ç›¸ä¼¼åº¦åˆ†æ•°æœ€é«˜çš„å‰5éƒ¨ç”µå½±ï¼Œå¹¶æ¨èç»™Aliceã€‚æ›´å¥½çš„æ¨èæ˜¯å¯¹m1ã€m4å’Œm5é‡å¤ä¸Šè¿°è¿‡ç¨‹ï¼Œå¹¶æ¨èè¿™äº›é›†åˆçš„äº¤é›†ä¸­çš„ç”µå½±ã€‚'
- en: This concludes our section on item-item collaborative filtering. If you want
    to read more on neural collaborative filtering, take a look at [this paper](https://arxiv.org/abs/1708.05031).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™éƒ¨åˆ†å†…å®¹ç»“æŸäº†å…³äºé¡¹ç›®-é¡¹ç›®ååŒè¿‡æ»¤çš„è®¨è®ºã€‚å¦‚æœä½ æƒ³æ·±å…¥äº†è§£ç¥ç»ååŒè¿‡æ»¤ï¼Œè¯·æŸ¥çœ‹[è¿™ç¯‡è®ºæ–‡](https://arxiv.org/abs/1708.05031)ã€‚
- en: Summary
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¦‚è¿°
- en: In this article, we discussed encoding and embedding methods. We learnt that
    encoding refers to the process of converting raw data, such as text, images, or
    audio, into a structured format that can be easily processed by computers. This
    transformation often involves reducing the dimensionality of the data while retaining
    its essential features. On the other hand, embedding involves mapping data points
    into a lower-dimensional space, where each point is represented by a vector of
    continuous values. Embeddings are designed to capture semantic relationships and
    similarities between data points, enabling algorithms to effectively learn patterns
    and make meaningful predictions. Both encoding and embedding play crucial roles
    in various applications, from natural language processing and computer vision
    to recommendation systems and anomaly detection, enhancing the efficiency and
    effectiveness of data analysis and machine learning tasks.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†ç¼–ç å’ŒåµŒå…¥æ–¹æ³•ã€‚æˆ‘ä»¬äº†è§£åˆ°ï¼Œç¼–ç æŒ‡çš„æ˜¯å°†åŸå§‹æ•°æ®ï¼ˆå¦‚æ–‡æœ¬ã€å›¾åƒæˆ–éŸ³é¢‘ï¼‰è½¬æ¢ä¸ºä¸€ç§ç»“æ„åŒ–æ ¼å¼çš„è¿‡ç¨‹ï¼Œä»¥ä¾¿è®¡ç®—æœºèƒ½å¤Ÿè½»æ¾å¤„ç†ã€‚è¿™ç§è½¬æ¢é€šå¸¸æ¶‰åŠé™ä½æ•°æ®çš„ç»´åº¦ï¼ŒåŒæ—¶ä¿ç•™å…¶æœ¬è´¨ç‰¹å¾ã€‚å¦ä¸€æ–¹é¢ï¼ŒåµŒå…¥åˆ™æ˜¯å°†æ•°æ®ç‚¹æ˜ å°„åˆ°ä¸€ä¸ªä½ç»´ç©ºé—´ï¼Œå…¶ä¸­æ¯ä¸ªç‚¹ç”±ä¸€ä¸ªè¿ç»­å€¼çš„å‘é‡è¡¨ç¤ºã€‚åµŒå…¥æ—¨åœ¨æ•æ‰æ•°æ®ç‚¹ä¹‹é—´çš„è¯­ä¹‰å…³ç³»å’Œç›¸ä¼¼æ€§ï¼Œä½¿ç®—æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°å­¦ä¹ æ¨¡å¼å¹¶åšå‡ºæœ‰æ„ä¹‰çš„é¢„æµ‹ã€‚ç¼–ç å’ŒåµŒå…¥åœ¨å„ç§åº”ç”¨ä¸­å‘æŒ¥ç€å…³é”®ä½œç”¨ï¼Œä»è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰åˆ°æ¨èç³»ç»Ÿå’Œå¼‚å¸¸æ£€æµ‹ï¼Œæå‡äº†æ•°æ®åˆ†æå’Œæœºå™¨å­¦ä¹ ä»»åŠ¡çš„æ•ˆç‡å’Œæœ‰æ•ˆæ€§ã€‚
- en: 'If you have any questions or suggestions, feel free to reach out to me:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·éšæ—¶è”ç³»æˆ‘ï¼š
- en: 'Email: mina.ghashami@gmail.com'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ç”µå­é‚®ä»¶ï¼šmina.ghashami@gmail.com
- en: 'LinkedIn: [https://www.linkedin.com/in/minaghashami/](https://www.linkedin.com/in/minaghashami/)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 'LinkedIn: [https://www.linkedin.com/in/minaghashami/](https://www.linkedin.com/in/minaghashami/)'
- en: References
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[Google blog post on embedding](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture#:~:text=An%20embedding%20is%20a%20relatively,like%20sparse%20vectors%20representing%20words.)'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[å…³äºåµŒå…¥çš„è°·æ­Œåšå®¢æ–‡ç« ](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture#:~:text=An%20embedding%20is%20a%20relatively,like%20sparse%20vectors%20representing%20words.)'
- en: '[Distributed Representations of Words and Phrases and their Compositionality](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[è¯æ±‡å’ŒçŸ­è¯­çš„åˆ†å¸ƒå¼è¡¨ç¤ºåŠå…¶ç»„åˆæ€§](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)'
- en: '[Learning embeddings â€” CS246, Stanford University](https://web.stanford.edu/class/cs246/slides/14-emb.pdf)'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[å­¦ä¹ åµŒå…¥ â€” CS246ï¼Œæ–¯å¦ç¦å¤§å­¦](https://web.stanford.edu/class/cs246/slides/14-emb.pdf)'
- en: '[Neural collaborative filtering](https://arxiv.org/abs/1708.05031)'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[ç¥ç»ååŒè¿‡æ»¤](https://arxiv.org/abs/1708.05031)'
- en: '[Neural Network Methods in Natural Language Processing](http://amzn.to/2wycQKA),
    2017'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ç¥ç»ç½‘ç»œæ–¹æ³•](http://amzn.to/2wycQKA)ï¼Œ2017'
