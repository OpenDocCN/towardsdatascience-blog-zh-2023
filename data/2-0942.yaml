- en: From Encodings to Embeddings
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从编码到嵌入
- en: 原文：[https://towardsdatascience.com/from-encodings-to-embeddings-5b59bceef094](https://towardsdatascience.com/from-encodings-to-embeddings-5b59bceef094)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/from-encodings-to-embeddings-5b59bceef094](https://towardsdatascience.com/from-encodings-to-embeddings-5b59bceef094)
- en: '**concepts and fundamentals: from SVD to neural networks**'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**概念与基础：从SVD到神经网络**'
- en: '[](https://medium.com/@mina.ghashami?source=post_page-----5b59bceef094--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page-----5b59bceef094--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5b59bceef094--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5b59bceef094--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page-----5b59bceef094--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@mina.ghashami?source=post_page-----5b59bceef094--------------------------------)[![Mina
    Ghashami](../Images/745f53b94f5667a485299b49913c7a21.png)](https://medium.com/@mina.ghashami?source=post_page-----5b59bceef094--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5b59bceef094--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5b59bceef094--------------------------------)
    [Mina Ghashami](https://medium.com/@mina.ghashami?source=post_page-----5b59bceef094--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5b59bceef094--------------------------------)
    ·16 min read·Sep 7, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5b59bceef094--------------------------------)
    ·16分钟阅读·2023年9月7日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/ea2a97bef9136382285c2f7b5a61786a.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ea2a97bef9136382285c2f7b5a61786a.png)'
- en: 'credit: [https://unsplash.com/](https://unsplash.com/)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '版权: [https://unsplash.com/](https://unsplash.com/)'
- en: 'In this article, we will talk about two fundamental concepts in the fields
    of data representation and machine learning: **Encoding** and **Embedding**. The
    content of this article is partly taken from one of my lectures in [CS246 Mining
    Massive DataSet (MMDS) course at Stanford University](https://web.stanford.edu/class/cs246/).
    I hope you find it useful.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将讨论数据表示和机器学习领域中的两个基本概念：**编码**和**嵌入**。本文的内容部分来源于我在[斯坦福大学的CS246矿大数据集（MMDS）课程](https://web.stanford.edu/class/cs246/)中的讲座。希望你觉得有用。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: 'All Machine Learning (ML) methods work with input feature vectors and almost
    all of them require input features to be *numerical*. From a ML perspective, there
    are four types of features:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 所有机器学习（ML）方法都处理输入特征向量，几乎所有方法都要求输入特征是*数值型*的。从ML的角度来看，有四种类型的特征：
- en: '*Numerical (continuous or discrete)*: numerical data can be characterized by
    continuous or discrete data. Continuous data can assume any value within a range
    whereas discrete data has distinct values. Example of continues numerical variable
    is *`height`*, and an example of discrete numerical variable is *`age`*.'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*数值型（连续或离散）*：数值数据可以分为连续数据或离散数据。连续数据可以在一个范围内取任意值，而离散数据有明显的值。连续数值变量的例子是*`身高`*，离散数值变量的例子是*`年龄`*。'
- en: '*Categorical (ordinal or nominal)*: categorical data represents characteristics
    such as eye color, and hometown. Categorical data can be ordinal or nominal. In
    ordinal variable, the data falls into ordered categories that are ranked in some
    particular way. An example is *`skill level`* that takes values of *[`beginner`,
    `intermediate`, `advanced`]*. Nominal variable has no order among its values.
    An example is *`eye color`* that takes values of *[`black`, `brown’, `blue`, `green`]*.'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*分类数据（有序或无序）*：分类数据表示诸如眼睛颜色和家乡等特征。分类数据可以是有序的或无序的。在有序变量中，数据被分到按特定方式排序的类别中。例如*`技能水平`*，其值为*[`初级`，`中级`，`高级`]。无序变量在其值之间没有顺序。例如*`眼睛颜色`*，其值为*[`黑色`，`棕色`，`蓝色`，`绿色`]。'
- en: '*Time series*: Time series is a sequence of numbers collected at regular intervals
    over some period of time. This data is ordered in time unlike previous variables.
    An example of this is *`average of home sale price over years in USA`*.'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*时间序列*：时间序列是一个在一定时间段内以规则间隔收集的数字序列。这些数据按时间排序，与之前的变量不同。一个例子是*`美国多年房屋销售价格的平均值`*。'
- en: '*Text*: Any document is a text data, that we often represent them as a ‘bag
    of words’.'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*文本*：任何文档都是文本数据，我们通常将其表示为“词袋”。'
- en: To feed any variables to an ML model, we have to convert them into numerical.
    Both encoding and embedding techniques do this trick.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 要将任何变量输入到ML模型中，我们必须将其转换为数值。编码和嵌入技术都可以实现这一点。
- en: Encoding
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码
- en: 'Encoding is the process of converting raw data, such as text, images, or audio,
    into a structured numerical format that can be easily processed by computers.
    There are two ways to encode a categorical variable:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 编码是将原始数据（如文本、图像或音频）转换为结构化的数值格式，使计算机能够轻松处理。编码类别变量有两种方法：
- en: 1️⃣ Integer encoding
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ 整数编码
- en: 2️⃣ One-hot encoding
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ 一热编码
- en: 3️⃣ Multi-hot encoding (this is the extension of one-hot encoding)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 3️⃣ 多热编码（这是对一热编码的扩展）
- en: 'To explain each method let’s work through the following example:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释每种方法，我们通过以下示例来说明：
- en: 🎬 Consider a tiny movie dataset containing only 4 movies and 5 features.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 🎬 设想一个只有 4 部电影和 5 个特征的小型电影数据集。
- en: '![](../Images/81d64c7f84e6ea98a7e74640933791c9.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81d64c7f84e6ea98a7e74640933791c9.png)'
- en: 'Figure 1: movie dataset — Image by the author'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：电影数据集 — 图片由作者提供
- en: As we see, two features (release year, IMDB rating) are numerical, one feature
    (title) is text, and the remaining two (provider, IMDB genres) are categorical.
    Let’s see how encoding methods apply to these.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，两个特征（发行年份、IMDB 评分）是数值型的，一个特征（标题）是文本型的，剩下的两个（提供者、IMDB 类型）是类别型的。让我们来看看编码方法如何应用于这些特征。
- en: 1️⃣ **Integer encoding:**
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 1️⃣ **整数编码：**
- en: 'This method assigns an integer to each category value. For example if *provider*
    variable takes four distinct values *`[Netflix, Prime Video, HBO Max, Hulu]`*,
    we assign them integers 1, 2, 3 and 4 respectively:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法为每个类别值分配一个整数。例如，如果 *提供者* 变量有四个不同的值 *`[Netflix, Prime Video, HBO Max, Hulu]`*，我们分别将其分配整数
    1、2、3 和 4：
- en: '*Netflix -> 1, Prime Video -> 2, HBO Max ->3 , Hulu -> 4*'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*Netflix -> 1, Prime Video -> 2, HBO Max -> 3, Hulu -> 4*'
- en: The pro of this approach is that it provides a dense representation. The con
    is that it implies ordering between different categories, i.e.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的优点是提供了紧凑的表示。缺点是它暗示了不同类别之间的顺序，即
- en: '*Netflix < Prime Video < HBO Max < Hulu.*'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*Netflix < Prime Video < HBO Max < Hulu.*'
- en: So it might makes more sense to use integer encoding for ordinal variables,
    e.g. for *`education’* taking values of *‘[Diploma, Undergrad, Masters, PhD ]`*.
    However it still implies values are equally spaced out 🙊!! Obviously, this is
    undesirable, so let’s move to the next method.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于序数变量（例如 *`教育`* 取值为 *‘[文凭, 本科, 硕士, 博士]`*），使用整数编码可能更有意义。然而，这仍然意味着值是均匀间隔的
    🙊!! 显然，这并不理想，因此我们将转向下一种方法。
- en: '💻 In Python, you can perform integer encoding using various libraries, such
    as scikit-learn or TensorFlow/Keras. Here, we use scikit-learn’s `LabelEncoder`
    for encoding categorical labels into integers:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 💻 在 Python 中，你可以使用各种库（如 scikit-learn 或 TensorFlow/Keras）执行整数编码。在这里，我们使用 scikit-learn
    的 `LabelEncoder` 将类别标签编码为整数：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 2️⃣ **One-hot encoding:**
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 2️⃣ **一热编码：**
- en: 'This method first applies integer encoding, then creates a binary vector that
    represents the numerical values e.g. for *`provider`* variable, we assign integers
    first: *Netflix -> 1, Prime Video -> 2, HBO Max ->3 , Hulu -> 4*. Then we create
    a binary vector of length 4 for each value as following:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法首先应用整数编码，然后创建一个二进制向量来表示数值，例如，对于 *`提供者`* 变量，我们首先分配整数：*Netflix -> 1, Prime
    Video -> 2, HBO Max -> 3, Hulu -> 4*。然后我们为每个值创建一个长度为 4 的二进制向量，如下所示：
- en: '![](../Images/1da255730afaec50cfc5c1747bde6fad.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1da255730afaec50cfc5c1747bde6fad.png)'
- en: 'Figure 2: one-hot vectors of provider variable — Image by the author'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：提供者变量的一热编码向量 — 图片由作者提供
- en: As you see, this method explodes the dimension of the feature vector to number
    of values the categorical feature takes 🙈!! That can quickly get out of hands.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这种方法将特征向量的维度扩展到类别特征的取值数量 🙈!! 这可能迅速变得难以控制。
- en: '💻 In Python, you can perform one-hot encoding using libraries like scikit-learn
    or pandas. Here’s how to do it using scikit-learn’s `OneHotEncoder`:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 💻 在 Python 中，你可以使用像 scikit-learn 或 pandas 这样的库来执行一热编码。以下是使用 scikit-learn 的 `OneHotEncoder`
    进行一热编码的方法：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, let’s look at the extension of this method too.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们也来看看这种方法的扩展。
- en: 3️⃣ **Multi-hot encoding:**
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 3️⃣ **多热编码：**
- en: Multi-hot encoding is an extension of one-hot encoding when a categorical variable
    can take multiple values at the same time. For example, there are 28 distinct
    IMDB genres, and a movie can take multiple genres, e.g. the movie `*stranger things`*
    is *drama, fantasy, horror* at the same time.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 多热编码是对一热编码的扩展，当类别变量可以同时取多个值时。例如，有 28 种不同的 IMDB 类型，而一部电影可以同时属于多个类型，例如电影 `*怪奇物语*`
    同时属于 *剧情、奇幻、恐怖* 类型。
- en: '![](../Images/0a1bfdeb0a244d9794a44801dcf62229.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0a1bfdeb0a244d9794a44801dcf62229.png)'
- en: 'Figure 3: IMDB genres with their integer encoding — Image by the author'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：IMDB 类型及其整数编码 — 图片由作者提供
- en: 'Applying multi-hot encoding to movies’ genres give us 28-dimensional encoding
    vectors:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 将多热编码应用于电影类型，会得到 28 维的编码向量：
- en: '![](../Images/0926c9622ff5b2f628471caa7a464357.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0926c9622ff5b2f628471caa7a464357.png)'
- en: 'Figure 4: multi-hot encoding of genres for each movie — Image by the author'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：每部电影的类型的多热编码 — 作者提供的图片
- en: We see that obviously, multi-hot encoding suffers from same caveat as one-hot
    encoding which is dimensionality explosion.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以明显看到，多热编码与 one-hot 编码有相同的缺点，即维度爆炸。
- en: '💻 We can use scikit-learn or pandas to achieve multi-hot encoding in Python.
    Here’s how to do it using scikit-learn’s `MultiLabelBinarizer`:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 💻 我们可以使用 scikit-learn 或 pandas 在 Python 中实现多热编码。以下是使用 scikit-learn 的 `MultiLabelBinarizer`
    进行操作的方法：
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, applying all above encodings on the movie dataset results in the following:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，将以上所有编码应用于电影数据集，会得到以下结果：
- en: '![](../Images/5f32868ac44b63c1710dbd1d0532d864.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5f32868ac44b63c1710dbd1d0532d864.png)'
- en: 'Figure 5: movie dataset with all encodings — Image by the author'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：包含所有编码的电影数据集 — 作者提供的图片
- en: As we see, after applying all encodings, dimension of data increased from 5
    to 35 🙊!! In fact, It will blow up to thousands or a million if we multi-hot encode
    the *`title`* variable too!
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，在应用所有编码后，数据维度从 5 增加到 35 🙊!! 实际上，如果我们对 *`title`* 变量也进行多热编码，它将膨胀到数千或一百万！
- en: '📓The takeaway is that one-hot and multi-hot encodings are not practical for
    features with large value sets. In a corpus of documents with one million distinct
    words, representing each document via multi-hot encoding creates vectors that
    are:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 📓要点是，对于具有大量值集的特征，one-hot 和 multi-hot 编码并不实用。在一个包含一百万个不同单词的文档语料库中，通过多热编码表示每个文档会生成如下向量：
- en: 'high dimensional: multi-hot encodings will create 1-million dimensional vectors!'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高维：多热编码将生成 100 万维的向量！
- en: 'sparse: Since an average document contains 500 words, the multi-hot vectors
    will be 99.95% sparse'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稀疏：由于一个平均文档包含 500 个词，因此多热编码向量将会有 99.95% 的稀疏度
- en: 'short of semantic: encoding of two words ‘good’ and ‘great’ are as different
    as encoding of ‘good’ and ‘bad’!'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语义短缺：‘good’ 和 ‘great’ 的编码与 ‘good’ 和 ‘bad’ 的编码一样不同！
- en: ✏️ In a nutshell, use one-hot/multi-hot encoding when the number of categories
    is small; usually lesser than 15 or so. For text data which has millions of categories
    (every word as one category) we have to use more efficient methods.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ✏️ 简而言之，当类别数量较少时，使用 one-hot/multi-hot 编码；通常少于 15 个。对于类别数量以百万计的文本数据（每个单词作为一个类别），我们必须使用更高效的方法。
- en: In the rest of the article, we work with the problem of *`computing word embeddings`*.
    Through this example, we will study few fundamental embedding methods.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文的其余部分，我们将处理 *`计算词嵌入`* 的问题。通过这个例子，我们将研究几种基本的嵌入方法。
- en: '**Embedding**'
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**嵌入**'
- en: To address above shortcomings, we move from high dimensional sparse vector to
    short dense vectors; these vectors are called ***embeddings***. An embedding is
    a translation of a high-dimensional vector into a low-dimensional space and captures
    semantic similarity.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决上述问题，我们从高维稀疏向量转向短的稠密向量；这些向量被称为 ***嵌入***。嵌入是将高维向量转换为低维空间的过程，并捕捉语义相似性。
- en: '*“One of the benefits of using dense and low-dimensional vectors is computational:
    The majority of neural network toolkits do not play well with very high-dimensional,
    sparse vectors.” [5]*'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*“使用稠密且低维向量的一个好处是计算上的：大多数神经网络工具包与非常高维的稀疏向量兼容性差。” [5]*'
- en: Let’s first look at a very simple embedding method called SVD.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们来看一种非常简单的嵌入方法，称为 SVD。
- en: Singular Value Decomposition (SVD)
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 奇异值分解 (SVD)
- en: 'The simplest embedding method is perhaps *Singular Value Decomposition (SVD)*
    that takes an input matrix *A* and decompose it into three matrices as shown below:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的嵌入方法可能是 *奇异值分解 (SVD)*，它将输入矩阵 *A* 分解为以下三个矩阵：
- en: '![](../Images/9a9ee8fc2dece9e9a50e7c01b3a60009.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9a9ee8fc2dece9e9a50e7c01b3a60009.png)'
- en: 'Figure 6: SVD decomposition — Image by the author'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：SVD 分解 — 作者提供的图片
- en: '***U, V*** are left and right singular vectors, respectively. They are column
    orthonormal meaning that each column in them has norm of one, and every two column
    in U (and in V, respectively) are orthogonal. In mathematic, we write this as'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '***U, V*** 分别是左奇异向量和右奇异向量。它们是列正交的，这意味着它们中的每一列的范数为 1，并且 U 中的每两列（以及 V 中的每两列）是正交的。用数学语言表示为'
- en: '![](../Images/0dffcbbba175de107ca99c532e87b0a1.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0dffcbbba175de107ca99c532e87b0a1.png)'
- en: 'Both *U* and *V* define an r-dimensional subspace, hence projecting ***A***
    onto them produces r-dimensional embeddings:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*U* 和 *V* 定义了一个 r 维子空间，因此将***A***投影到它们上面产生 r 维嵌入：'
- en: '![](../Images/7d21a53734ef4b03161ed22eda0321be.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7d21a53734ef4b03161ed22eda0321be.png)'
- en: 🗒 **Let’s look at an example together** 🗒**:**
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 🗒 **让我们一起看一个例子** 🗒**：**
- en: 'Given a corpus of documents, we can use SVD to compute document embeddings
    & word embeddings. Here are the steps:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个文档语料库，我们可以使用 SVD 来计算文档嵌入和词嵌入。步骤如下：
- en: '**Step 1**: Convert it to bag of words (BOW) vectors and get a term-document
    matrix. We can use term frequencies (tf), or normalize using tf-idf technique.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 1**：将其转换为词袋（BOW）向量，并获得词-文档矩阵。我们可以使用词频（tf），或使用 tf-idf 技术进行归一化。'
- en: '![](../Images/4ddecdc930870ddb7d838bb92b70c37c.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4ddecdc930870ddb7d838bb92b70c37c.png)'
- en: 'Figure 7: term frequency matrix — Image by the author'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：词频矩阵 — 作者提供的图片
- en: '**Step 2**: apply SVD on the term-document matrix and pick a value *r < rank(A)*.
    This will create three matrices, each of rank r.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 2**：对词-文档矩阵应用 SVD，并选择一个值*r < rank(A)*。这将创建三个矩阵，每个矩阵的秩为 r。'
- en: '![](../Images/a3eb55940c85f8504f4a232336853145.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a3eb55940c85f8504f4a232336853145.png)'
- en: 'Figure 8: SVD decomposition of matrix A — Image by the author'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：矩阵 A 的 SVD 分解 — 作者提供的图片
- en: '**Step 3**: compute embedding of documents as'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤 3**：计算文档的嵌入，如下所示'
- en: '*emb = [<doc, v1> , <doc, v2> , <doc, v3>]*'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*emb = [<doc, v1> , <doc, v2> , <doc, v3>]*'
- en: The first two dot-products are shown in the figure below.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了前两个点积。
- en: '![](../Images/f1f92b7d90b085ee574cde786fb5bfc3.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f1f92b7d90b085ee574cde786fb5bfc3.png)'
- en: 'Figure 9: embedding of first document — only two dot-product operation are
    shown — Image by the author'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：第一个文档的嵌入 — 仅显示了两个点积操作 — 作者提供的图片
- en: 'Once we compute all three vector dot-products, the embedding of the document
    is:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们计算了所有三个向量的点积，文档的嵌入就是：
- en: '![](../Images/2f2b73592e696a5c5c60006e9ccb5ee3.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f2b73592e696a5c5c60006e9ccb5ee3.png)'
- en: 'Figure 10: embedding of first document — Image by the author'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：第一个文档的嵌入 — 作者提供的图片
- en: Similarly we can compute word (term) embeddings as *emb = [<term, u1> , <term,
    u2> , <term, u3>].*
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以计算词（术语）嵌入为*emb = [<term, u1> , <term, u2> , <term, u3>]。*
- en: '![](../Images/c3809d1aa915f00bde28ec158c6be8a4.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c3809d1aa915f00bde28ec158c6be8a4.png)'
- en: 'Figure 11: term embedding for first term — Image by the author'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：第一个术语的词嵌入 — 作者提供的图片
- en: We can show that these representations group similar terms and documents together
    in the 3-dimensional space. Terms and documents that are related or have similar
    context tend to have similar representations in this reduced space.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以证明，这些表示将相似的术语和文档分组在三维空间中。相关或具有相似上下文的术语和文档在这个降维空间中往往具有相似的表示。
- en: '💻 In python, we use scikit-learn to convert a corpus of document to tf-idf
    matrices and then apply SVD on them. Here is an example:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 💻 在 Python 中，我们使用 scikit-learn 将文档语料库转换为 tf-idf 矩阵，然后对其应用 SVD。以下是一个示例：
- en: '[PRE3]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Overall, SVD is a simple and powerful technique for maintaining semantic information
    however, it is impractical on real-world datasets. It’s computationally heavy
    and does not utilize sparsity of the input matrix at all. Let’s see how we can
    avoid these shortcomings in another method.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，SVD 是一种简单且强大的技术，用于保持语义信息，但在实际数据集上并不实用。它计算量大，并且完全没有利用输入矩阵的稀疏性。让我们看看如何在另一种方法中避免这些缺陷。
- en: '**Neural Networks as Embedder**'
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**神经网络作为嵌入器**'
- en: '🌟State of the art embedders are among Neural Networks (NN). There are many
    NN techniques to compute word embeddings: Word2Vec, Glove, BERT, fastText, etc.
    In this article, we look at *Word2Vec* which was developed by [Tomas Mikolov](https://scholar.google.com/citations?user=oBu8kMMAAAAJ&hl=en)
    and his colleagues at Google in a series of papers published between 2013 and
    2014 ([paper](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 🌟 目前最先进的嵌入器之一是神经网络（NN）。有许多神经网络技术可以计算词嵌入：Word2Vec、Glove、BERT、fastText 等。本文讨论了*Word2Vec*，该方法由[托马斯·米科洛夫](https://scholar.google.com/citations?user=oBu8kMMAAAAJ&hl=en)及其在谷歌的同事们开发，并在
    2013 年至 2014 年间发表的一系列论文中介绍了该方法（[论文](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)）。
- en: '*Word2Vec* is a statistical, self-supervised, and task independent method.
    It comes in two flavors: Continuous bag of words (CBOW), and Skip Gram. The two
    flavors are very similar, both use a shallow neural network with only one hidden
    layer and no activation function to learn the words representations. In this article,
    we study the skip-gram model.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*Word2Vec* 是一种统计、自监督、与任务无关的方法。它有两种变体：连续词袋模型（CBOW）和 Skip Gram。这两种变体非常相似，都使用一个只有一层隐藏层且没有激活函数的浅层神经网络来学习词语表示。在本文中，我们研究
    skip-gram 模型。'
- en: '🔑 The key idea of word2Vec is that words with similar context have similar
    meanings. The more often a word appears in the ***context*** of certain other
    words, the closer they are in meaning. *Context* of a word are few words to its
    left and few words to its right. Formally, given a document, we set a window size
    (e.g. window_size = 2). Then for any given word in the document (call it *`target`*
    word), the *`window_size`* words to its left and *`window_size`* words to its
    right are its *context* . For example:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 🔑 Word2Vec 的关键思想是具有相似上下文的词语具有相似的含义。一个词在某些其他词的***上下文***中出现得越频繁，它们的含义就越接近。一个词的*上下文*是它左边和右边的几个词。形式上，给定一个文档，我们设置一个窗口大小（例如，window_size
    = 2）。那么对于文档中的任何给定词（称为*`目标`*词），它左边的*`window_size`* 个词和右边的*`window_size`* 个词就是它的*上下文*。例如：
- en: '![](../Images/2b9f9598081ebed87d9e2cac044fe371.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2b9f9598081ebed87d9e2cac044fe371.png)'
- en: 'Figure 12: target and context words (window size = 2 )— Image by the author'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：目标词和上下文词（窗口大小 = 2） — 图片由作者提供
- en: 'Given a document, we can slide the window from left to right and find all pairs
    of *(target, context)* words. Consider the following document:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个文档，我们可以从左到右滑动窗口，找到所有的*(目标词, 上下文词)* 对。考虑以下文档：
- en: 'Document: *“I read sci-fi books and drink orange juice”*,'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 文档：*“我读科幻书籍并喝橙汁”*，
- en: Let window size = 2\. Then the set of (target, context) words are depicted below.
    The highlighted word in the image is the `target` word, and other words in the
    box are `context` words.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 设窗口大小 = 2。那么 (目标词, 上下文词) 的集合如下图所示。图片中高亮的词是`目标`词，框中的其他词是`上下文`词。
- en: '![](../Images/79792923bb530410d7b8ecd8577fa349.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/79792923bb530410d7b8ecd8577fa349.png)'
- en: 'Figure 13: sliding window on a document to extract (target, context) words
    — Image by the author'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：在文档上滑动窗口以提取（目标词，上下文词） — 图片由作者提供
- en: 'Now, that we know the meaning of target and context words, let’s see how Word2vec
    uses them. Word2vec is a 2-layer neural network that takes a target word as input,
    and predicts all context words in the window. Here is a schematic view of its
    architecture for window size = 2:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们知道了目标词和上下文词的含义，让我们看看 Word2vec 如何使用它们。Word2vec 是一个2层神经网络，它以目标词作为输入，并预测窗口中的所有上下文词。以下是窗口大小
    = 2 的架构示意图：
- en: '![](../Images/3f133049b4f0df7d8f3ce51032f4b5ad.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3f133049b4f0df7d8f3ce51032f4b5ad.png)'
- en: 'Figure 14: skip-gram architecture for window size = 2 — Image by the author'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：窗口大小 = 2 的skip-gram架构 — 图片由作者提供
- en: Now, we can guess why this architecture is called skip-gram🙂… It’s because it
    predicts all the grams in the context except the target word as it is the input
    to the model, hence the name is skip-gram.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以猜测为什么这个架构被称为skip-gram🙂… 这是因为它预测上下文中的所有词组，除了目标词，因为目标词是模型的输入，所以这个名字叫做skip-gram。
- en: 'Now, let’s focus on predicting only one context word and dive into details.
    Let *V = size of the vocabulary* and *N = embedding dimension*, i.e. the size
    of the only hidden layer. Then the architecture for predicting one context word
    is:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们专注于只预测一个上下文词，并深入细节。设*V = 词汇表的大小*和*N = 嵌入维度*，即唯一隐藏层的大小。则用于预测一个上下文词的架构是：
- en: '![](../Images/04747218c33cee937ef2ad6544131be0.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04747218c33cee937ef2ad6544131be0.png)'
- en: 'Figure 15: skip-gram architecture for predicting one context word — Image by
    the author'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：用于预测一个上下文词的skip-gram架构 — 图片由作者提供
- en: '🧬The input layer is the one-hot encoding of the target word i.e. w(t); since
    we have V word in our vocabulary the input layer is V-dimensional. The hidden
    layer is N-dimensional and produces the embeddings. Often *N << V,* for example
    in a corpus of web documents, we have millions of tokens (V is of order of millions)
    while N is somewhere between 256 to 512\. The point of the hidden layer is to
    map the words into a lower dimensionality while maintaining separation between
    dissimilar words. This layer does not use any activation function. The reason
    is most activation functions involve some “squishing” of space in one region and
    “expanding” of space in the other e.g., sigmoid/tanh will “squish together” all
    values < -1, and same with all values >1\. RELU would “squish together” all values
    <0, eliminating half of the representative capacity. As you can see, this actually
    *hurts* the separation quality, reducing the amount of “word mapping space” available.
    The output layer is V dimensional, a softmax function applies on this layer therefore
    each neuron in the output layer is a probability and all neurons sum up to 1\.
    The j-th neuron in output layer i.e. y_j, indicates the probability that the j-th
    word is the context word. As we see, there are two weight matrices involved:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 🧬输入层是目标词的 one-hot 编码，即 w(t)；由于我们在词汇表中有 V 个词，输入层是 V 维的。隐藏层是 N 维的，并生成嵌入。通常 *N
    << V*，例如在一个网页文档的语料库中，我们有数百万个词汇（V 级别为百万级），而 N 在 256 到 512 之间。隐藏层的目的是将词映射到一个更低的维度，同时保持不同词之间的分离。这个层不使用任何激活函数。原因是大多数激活函数涉及在一个区域“压缩”空间，而在另一个区域“扩展”空间，例如
    sigmoid/tanh 会将所有值 < -1“压缩在一起”，所有值 >1 同样如此。RELU 会将所有值 <0“压缩在一起”，从而消除了代表能力的一半。如你所见，这实际上会
    *损害* 分离质量，减少可用的“词映射空间”量。输出层是 V 维的，softmax 函数作用于此层，因此输出层中的每个神经元都是一个概率，所有神经元的总和为
    1。输出层的第 j 个神经元，即 y_j，表示第 j 个词是上下文词的概率。我们可以看到，这里涉及两个权重矩阵：
- en: '![](../Images/9e9756e6106ff636cf8b364a37c9067c.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9e9756e6106ff636cf8b364a37c9067c.png)'
- en: Given the topology of the network, if ***x*** is the input and ***y*** is the
    output, then
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 给定网络的拓扑结构，如果 ***x*** 是输入，***y*** 是输出，则
- en: '![](../Images/188405c242c36054b4f5597c4e1ef14f.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/188405c242c36054b4f5597c4e1ef14f.png)'
- en: After the network is trained, we obtain the embedding of a word by multiplying
    its one-hot vector *x* with the weight matrix *W*.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 网络训练完成后，我们通过将其 one-hot 向量 *x* 与权重矩阵 *W* 相乘来获得词的嵌入。
- en: '🔁**How is the network trained?** Since there are no labels, we will create
    a fake task 😃! The fake task is:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 🔁**网络是如何训练的？** 由于没有标签，我们将创建一个虚拟任务 😃！这个虚拟任务是：
- en: '📑*The fake task: `given a target word, predict its context words`*'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 📑*虚拟任务：`给定一个目标词，预测其上下文词`*
- en: 'Now, there are two questions: 1) how to make the training data, 2) what loss
    function to choose to train the network?'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有两个问题：1）如何生成训练数据，2）选择什么损失函数来训练网络？
- en: '🗒**How to make the training data?** We take our corpus of *data = {d1, d2,
    ….}*, we might collect millions of documents, wiki pages, blot posts, etc. Then
    we tokenize all documents and build a vocabulary. There are many tokenization
    methods available e.g. workpiece, BytePairEncoding (BPE), k-gram. The simplest
    method is k-gram. If you take *k=1* and do tokenization by words (instead of characters)
    then 1-gram is equivalent to split sentences by space! After tokenization, we
    have a list of tokenized documents. We then move the sliding window over tokenized
    documents and collect training data as pairs of (target, context). See the example
    below:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 🗒**如何生成训练数据？** 我们取我们的语料库 *data = {d1, d2, ….}*，可能会收集到数百万篇文档、维基页面、博客文章等。然后，我们对所有文档进行分词，并建立词汇表。可用的分词方法有很多，例如
    workpiece、BytePairEncoding (BPE)、k-gram。最简单的方法是 k-gram。如果你取 *k=1* 并按词进行分词（而不是按字符），那么
    1-gram 相当于按空格分割句子！分词后，我们得到一个分词文档的列表。然后，我们在分词文档上滑动窗口，并将训练数据收集为 (target, context)
    的对。请参见下面的示例：
- en: '![](../Images/29424af3ffd1c684d0908e195c8a4f3f.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/29424af3ffd1c684d0908e195c8a4f3f.png)'
- en: 'Figure 16: collecting training data from a document — Image by the author'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16：从文档中收集训练数据 — 作者提供的图像
- en: '📈**what loss function to choose?** We use cross-entropy loss function. The
    reason is we train against the target-context pairs (w_t, w_c) where w_t is the
    target word and w_c is the context word. The context word w_c represents the ideal
    prediction, given the target word w_t. Note that W_c is represented as one-hot,
    i.e. it has value 1 at some position j and other positions are 0:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 📈**选择什么损失函数？** 我们使用交叉熵损失函数。原因是我们针对目标-上下文对 (w_t, w_c) 进行训练，其中 w_t 是目标词，w_c 是上下文词。上下文词
    w_c 表示给定目标词 w_t 的理想预测。注意 W_c 以独热编码表示，即在某个位置 j 的值为 1，其他位置为 0：
- en: '![](../Images/8c99d46e9872335b145a7d9a2b0863fe.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c99d46e9872335b145a7d9a2b0863fe.png)'
- en: 'Figure 17: one-hot encoding of context word — Image by the author'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17：上下文词的独热编码 — 作者提供的图片
- en: The loss function needs to evaluate the output layer at the same position j,
    i.e. y_j. (Remember y is a probability distribution; ideal value of y_j is being
    1). The cross-entropy loss function is defined as
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数需要在相同位置 j 评估输出层，即 y_j。（记住 y 是一个概率分布；y_j 的理想值为 1）。交叉熵损失函数定义为
- en: '![](../Images/03abb4184f315c05058444fd1c8ecdf2.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03abb4184f315c05058444fd1c8ecdf2.png)'
- en: For the example above,
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 对于上述示例，
- en: '![](../Images/185c44256b650df1b21faa401bcbfbae.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/185c44256b650df1b21faa401bcbfbae.png)'
- en: 'Figure 17: cross entropy loss in an example — Image by the author'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17：示例中的交叉熵损失 — 作者提供的图片
- en: '🔥**Training the neural network**: Now that the loss function and training data
    are clear, we will train the network to learn the weights. We want to find the
    values of ***W*** and ***W′*** that minimize the loss function. We do so by running
    back-propagation and updating parameters based on derivative of the loss. This
    concludes the word2vec algorithm.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 🔥**训练神经网络**：既然损失函数和训练数据已经明确，我们将训练网络以学习权重。我们希望找到使损失函数最小的 ***W*** 和 ***W′*** 的值。我们通过反向传播运行并根据损失的导数更新参数来实现这一点。这就结束了
    word2vec 算法。
- en: 📊**Results on word embeddings:** Below, we see an example of vector representations
    of four words *“man,” “woman,” “king” “queen”* that illustrate how Word2Vec captures
    semantic. In the context of Word2Vec, the relationships between words can be represented
    as vector arithmetic. The algorithm captures relationships like *“king — man +
    woman = queen”.* This is possible because the vector representation of “king”
    minus the vector representation of “man” plus the vector representation of “woman”
    results in a vector that is very close to the vector representation of “queen.”
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 📊**词嵌入的结果：** 下面，我们看到四个词 *“man,” “woman,” “king,” “queen”* 的向量表示示例，这些示例说明了 Word2Vec
    如何捕捉语义。在 Word2Vec 的背景下，词之间的关系可以表示为向量运算。算法捕捉到像 *“king — man + woman = queen”* 这样的关系。这是因为“king”的向量表示减去“man”的向量表示再加上“woman”的向量表示，结果是一个非常接近“queen”的向量表示。
- en: '![](../Images/cf41ccda2f851553c0d66f601054506e.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cf41ccda2f851553c0d66f601054506e.png)'
- en: 'Figure 18: word embeddings capture semantics — Image by the author'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18：词嵌入捕捉语义 — 作者提供的图片
- en: It’s important to note that Word2Vec is trained on large text corpora, and the
    quality of learned embeddings depends on the quantity and quality of the training
    data. Additionally, while Word2Vec can capture many semantic relationships, it
    might not capture all possible nuances of language and meaning. Other word embedding
    algorithms and models have since been developed that further enhance the understanding
    of word relationships.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，Word2Vec 是在大型文本语料库上训练的，学习到的嵌入质量取决于训练数据的数量和质量。此外，虽然 Word2Vec 可以捕捉许多语义关系，但它可能无法捕捉语言和意义的所有可能细微差别。之后还开发了其他词嵌入算法和模型，以进一步增强对词关系的理解。
- en: '**Item-Item Neural Collaborative Filtering**'
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**物品-物品神经协同过滤**'
- en: In this section, we will look into another example of learning embeddings through
    neural network. This network models item-item collaborative filtering algorithm.
    We will study this network through the use case of video recommendation.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将深入探讨通过神经网络学习嵌入的另一个示例。该网络建模了物品-物品协同过滤算法。我们将通过视频推荐的用例来研究这个网络。
- en: '🎥**Video recommendation usecase**: Consider the use case of video recommendation
    where we have 1 million videos, and 500,000 users who have watched some of these
    movies, and the task at hand is to recommend videos to users. We want to solve
    this problem using neural networks, so we formulate it as multi-class classification
    where each video is a class. We design a neural network to learn embeddings for
    videos such that similar videos have similar embeddings. Let’s build the training
    data first, and then design the network architecture.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 🎥**视频推荐用例**：考虑一个视频推荐的用例，我们有100万个视频和50万名用户，这些用户观看了一些这些视频，我们的任务是向用户推荐视频。我们想用神经网络解决这个问题，因此我们将其表述为多类分类，每个视频就是一个类别。我们设计一个神经网络来学习视频的嵌入，使得相似的视频具有相似的嵌入。首先让我们构建训练数据，然后设计网络架构。
- en: 🗒**Building Training Data:** Given users logs that contain videos users have
    watched, we sort out watched videos of each user in ascending order in time. Then
    split them in time on a fixed proportion. We take the first split as train data
    and the second split as test data. Note that if our split ratio is 70–30, the
    data of each user is split on this ratio. By this time-based split, our task has
    implicitly become *‘predicting what users watch next’*.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 🗒**构建训练数据：** 给定包含用户观看视频的日志，我们按时间升序排列每个用户观看的视频。然后按固定比例将其分割。我们将第一次分割作为训练数据，第二次分割作为测试数据。请注意，如果我们的分割比例是70-30，则每个用户的数据按这个比例进行分割。通过这种基于时间的分割，我们的任务隐式地变成了*‘预测用户接下来观看什么’*。
- en: Another way of splitting is to avoid sorting and randomly hold out few videos
    of each user as test data, and use the rest to build train data. This approach
    however, runs the risk of leaking information from train to test.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种分割方式是避免排序，随机保留每个用户的一些视频作为测试数据，使用其余的视频构建训练数据。然而，这种方法有泄露从训练到测试的信息的风险。
- en: '![](../Images/45466c3c9f7f17936df09c768aa2daa0.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/45466c3c9f7f17936df09c768aa2daa0.png)'
- en: 'Figure 19: video recommendation train-test split — Image by the author'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图19：视频推荐训练-测试分割 — 图片由作者提供
- en: Once data is split, we will build train data as pairs of (movie1, movie2) where
    both movies are watched by the same user.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分割后，我们将训练数据构建为(movie1, movie2)对，其中两个电影都由同一个用户观看。
- en: '![](../Images/d6cd414b95017f6fef54ed2233320aeb.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d6cd414b95017f6fef54ed2233320aeb.png)'
- en: 'Figure 20: building training data, and test data — Image by the author'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图20：构建训练数据和测试数据 — 图片由作者提供
- en: We then build a neural network that performs item-item collaborative filtering
    while learning 3-dim embeddings!! (Three is too small but for illustrative purposes
    we continue with it.)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们构建一个执行项目-项目协同过滤的神经网络，同时学习3维嵌入！！（三维虽然太小，但为了说明目的我们继续使用它。）
- en: '![](../Images/051b00575dd48aa45092f806b2752891.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/051b00575dd48aa45092f806b2752891.png)'
- en: 'Figure 21: item-item neural collaborative filtering — Image from [1] modified
    by author'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图21：项目-项目神经协同过滤 — 图片来源于[1]，由作者修改
- en: For every pair (m1, m2) in the training data, we feed one-hot vector of m1 in
    the input layer in blue, and feed m1 feature vectors (containing metadata such
    as genre, cast, director, popularity, etc.) in the yellow input layer. The one-hot
    of m2 will be used to compute the loss function. We want the model to learn the
    association between m1 and m2\. The more often m1 and m2 happen together in the
    training data, the closer their embedding will become.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练数据中的每一对(m1, m2)，我们在蓝色的输入层中输入m1的独热向量，并在黄色输入层中输入m1的特征向量（包含如类型、演员、导演、受欢迎程度等元数据）。m2的独热向量将用于计算损失函数。我们希望模型学习m1和m2之间的关联。m1和m2在训练数据中出现得越频繁，它们的嵌入就会越接近。
- en: '**How to recommend a movie to a user?** Assume Alice has watched m1, m4, m5\.
    To recommend a movie to her, we can find movies that have similar embeddings to
    m1\. For any movie v we compute the similarity score as *score= <emb(m1), emb(v)>*.
    We find the top 5 movies with highest similarity score, and recommend them to
    Alice. An even better recommendation is to repeat above for m1, m4 and m5, and
    recommend movies in intersection of these sets.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '**如何向用户推荐电影？** 假设Alice观看了m1、m4、m5。为了向她推荐电影，我们可以找到与m1具有相似嵌入的电影。对于任何电影v，我们计算相似度分数为*score=
    <emb(m1), emb(v)>*。我们找到相似度分数最高的前5部电影，并推荐给Alice。更好的推荐是对m1、m4和m5重复上述过程，并推荐这些集合的交集中的电影。'
- en: This concludes our section on item-item collaborative filtering. If you want
    to read more on neural collaborative filtering, take a look at [this paper](https://arxiv.org/abs/1708.05031).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分内容结束了关于项目-项目协同过滤的讨论。如果你想深入了解神经协同过滤，请查看[这篇论文](https://arxiv.org/abs/1708.05031)。
- en: Summary
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this article, we discussed encoding and embedding methods. We learnt that
    encoding refers to the process of converting raw data, such as text, images, or
    audio, into a structured format that can be easily processed by computers. This
    transformation often involves reducing the dimensionality of the data while retaining
    its essential features. On the other hand, embedding involves mapping data points
    into a lower-dimensional space, where each point is represented by a vector of
    continuous values. Embeddings are designed to capture semantic relationships and
    similarities between data points, enabling algorithms to effectively learn patterns
    and make meaningful predictions. Both encoding and embedding play crucial roles
    in various applications, from natural language processing and computer vision
    to recommendation systems and anomaly detection, enhancing the efficiency and
    effectiveness of data analysis and machine learning tasks.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们讨论了编码和嵌入方法。我们了解到，编码指的是将原始数据（如文本、图像或音频）转换为一种结构化格式的过程，以便计算机能够轻松处理。这种转换通常涉及降低数据的维度，同时保留其本质特征。另一方面，嵌入则是将数据点映射到一个低维空间，其中每个点由一个连续值的向量表示。嵌入旨在捕捉数据点之间的语义关系和相似性，使算法能够有效地学习模式并做出有意义的预测。编码和嵌入在各种应用中发挥着关键作用，从自然语言处理和计算机视觉到推荐系统和异常检测，提升了数据分析和机器学习任务的效率和有效性。
- en: 'If you have any questions or suggestions, feel free to reach out to me:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有任何问题或建议，请随时联系我：
- en: 'Email: mina.ghashami@gmail.com'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 电子邮件：mina.ghashami@gmail.com
- en: 'LinkedIn: [https://www.linkedin.com/in/minaghashami/](https://www.linkedin.com/in/minaghashami/)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 'LinkedIn: [https://www.linkedin.com/in/minaghashami/](https://www.linkedin.com/in/minaghashami/)'
- en: References
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[Google blog post on embedding](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture#:~:text=An%20embedding%20is%20a%20relatively,like%20sparse%20vectors%20representing%20words.)'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[关于嵌入的谷歌博客文章](https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture#:~:text=An%20embedding%20is%20a%20relatively,like%20sparse%20vectors%20representing%20words.)'
- en: '[Distributed Representations of Words and Phrases and their Compositionality](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[词汇和短语的分布式表示及其组合性](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)'
- en: '[Learning embeddings — CS246, Stanford University](https://web.stanford.edu/class/cs246/slides/14-emb.pdf)'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[学习嵌入 — CS246，斯坦福大学](https://web.stanford.edu/class/cs246/slides/14-emb.pdf)'
- en: '[Neural collaborative filtering](https://arxiv.org/abs/1708.05031)'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[神经协同过滤](https://arxiv.org/abs/1708.05031)'
- en: '[Neural Network Methods in Natural Language Processing](http://amzn.to/2wycQKA),
    2017'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[自然语言处理中的神经网络方法](http://amzn.to/2wycQKA)，2017'
