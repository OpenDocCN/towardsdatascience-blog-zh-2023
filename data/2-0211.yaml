- en: A New Way to Predict Probability Distributions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测概率分布的新方法
- en: 原文：[https://towardsdatascience.com/a-new-way-to-predict-probability-distributions-e7258349f464](https://towardsdatascience.com/a-new-way-to-predict-probability-distributions-e7258349f464)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/a-new-way-to-predict-probability-distributions-e7258349f464](https://towardsdatascience.com/a-new-way-to-predict-probability-distributions-e7258349f464)
- en: Exploring multi-quantile regression with Catboost
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索 Catboost 的多分位数回归
- en: '[](https://harrisonfhoffman.medium.com/?source=post_page-----e7258349f464--------------------------------)[![Harrison
    Hoffman](../Images/5eaa3e2bd0507297eb6c4a7efcf06324.png)](https://harrisonfhoffman.medium.com/?source=post_page-----e7258349f464--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e7258349f464--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e7258349f464--------------------------------)
    [Harrison Hoffman](https://harrisonfhoffman.medium.com/?source=post_page-----e7258349f464--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://harrisonfhoffman.medium.com/?source=post_page-----e7258349f464--------------------------------)[![Harrison
    Hoffman](../Images/5eaa3e2bd0507297eb6c4a7efcf06324.png)](https://harrisonfhoffman.medium.com/?source=post_page-----e7258349f464--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e7258349f464--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e7258349f464--------------------------------)
    [Harrison Hoffman](https://harrisonfhoffman.medium.com/?source=post_page-----e7258349f464--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e7258349f464--------------------------------)
    ·10 min read·Feb 14, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e7258349f464--------------------------------)
    ·阅读时间 10 分钟 ·2023年2月14日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/908e3e8a3e927aa30c512a2bb15cd827.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/908e3e8a3e927aa30c512a2bb15cd827.png)'
- en: Dice. Image by Author.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 骰子。图片由作者提供。
- en: How confident can we be in a machine learning model’s prediction? This question
    has been a prominent area of research over the last decade, and it has major implications
    in high-stakes machine learning applications such as finance and healthcare. While
    many classification models, particularly [calibrated](/why-calibrators-part-1-of-the-series-on-probability-calibration-9110831c6bde)
    models, come with uncertainty quantification by predicting a probability distribution
    over target classes, quantifying uncertainty in regression tasks is much more
    nuanced.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对机器学习模型的预测可以有多自信？这个问题在过去十年中一直是一个重要的研究领域，并且在高风险的机器学习应用如金融和医疗保健中具有重大意义。尽管许多分类模型，特别是[校准](https://why-calibrators-part-1-of-the-series-on-probability-calibration-9110831c6bde)模型，通过预测目标类别的概率分布提供了不确定性量化，但在回归任务中量化不确定性要复杂得多。
- en: Amongst many proposed methods, quantile regression is one of the most popular
    because no assumptions are made about the target distribution. Until recently,
    the main disadvantage of quantile regression was that one model had to be trained
    per predicted quantile. For instance, in order to predict the 10th, 50th, and
    90th quantiles of a target distribution, three independent models would need to
    be trained. Catboost has since addressed this issue with the [multi-quantile loss
    function](https://catboost.ai/en/docs/concepts/loss-functions-regression#MultiQuantile:~:text=MultiQuantile,-%5Cdisplaystyle%5Cfrac%7B%5Csum)
    — a loss function that enables a single model to predict an arbitrary number of
    quantiles.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多提出的方法中，分位数回归是最受欢迎的之一，因为它对目标分布没有假设。直到最近，分位数回归的主要缺点是每个预测的分位数都需要训练一个模型。例如，为了预测目标分布的第10、第50和第90百分位数，需要训练三个独立的模型。Catboost通过[多分位数损失函数](https://catboost.ai/en/docs/concepts/loss-functions-regression#MultiQuantile:~:text=MultiQuantile,-%5Cdisplaystyle%5Cfrac%7B%5Csum)解决了这个问题——这是一个使单个模型能够预测任意数量分位数的损失函数。
- en: 'This article will explore two examples using the multi-quantile loss function
    on synthetic data. While these examples aren’t necessarily reflective of real-world
    datasets, they will help us understand how well this loss function quantifies
    uncertainty by predicting the quantiles of a target distribution. For a quick
    refresher on noise and uncertainty in machine learning, see this article:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将探讨两个使用多分位数损失函数的合成数据示例。虽然这些示例不一定能反映真实世界的数据集，但它们将帮助我们理解该损失函数如何通过预测目标分布的分位数来量化不确定性。有关机器学习中噪声和不确定性的快速回顾，请参见这篇文章：
- en: '[](/understanding-noisy-data-and-uncertainty-in-machine-learning-4a2995a84198?source=post_page-----e7258349f464--------------------------------)
    [## Understanding Noisy Data and Uncertainty in Machine Learning'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/understanding-noisy-data-and-uncertainty-in-machine-learning-4a2995a84198?source=post_page-----e7258349f464--------------------------------)
    [## 理解机器学习中的噪声数据和不确定性'
- en: The actual reason your machine learning model isn’t working
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你的机器学习模型不起作用的真正原因
- en: towardsdatascience.com](/understanding-noisy-data-and-uncertainty-in-machine-learning-4a2995a84198?source=post_page-----e7258349f464--------------------------------)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/understanding-noisy-data-and-uncertainty-in-machine-learning-4a2995a84198?source=post_page-----e7258349f464--------------------------------)
- en: 'A follow-up to this article is now available:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章的后续内容现已发布：
- en: '[](/another-conformal-way-to-predict-probability-distributions-fcc63e78680d?source=post_page-----e7258349f464--------------------------------)
    [## Another (Conformal) Way to Predict Probability Distributions'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[](/another-conformal-way-to-predict-probability-distributions-fcc63e78680d?source=post_page-----e7258349f464--------------------------------)
    [## 另一种（符合性）预测概率分布的方法'
- en: Conformal multi-quantile regression with Catboost
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 Catboost 的符合性多分位数回归
- en: towardsdatascience.com](/another-conformal-way-to-predict-probability-distributions-fcc63e78680d?source=post_page-----e7258349f464--------------------------------)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: towardsdatascience.com](/another-conformal-way-to-predict-probability-distributions-fcc63e78680d?source=post_page-----e7258349f464--------------------------------)
- en: A Brief Overview of Quantile Regression
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分位数回归简要概述
- en: 'In supervised machine learning, the usual task is to train a model that predicts
    the expected value of a target given a set of input features:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在有监督的机器学习中，通常的任务是训练一个模型，以预测给定一组输入特征时目标的期望值：
- en: '![](../Images/9c0a9c217cb0b9d60d0137967bc922f8.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9c0a9c217cb0b9d60d0137967bc922f8.png)'
- en: Supervised Machine Learning. Image by Author.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 有监督机器学习。图片由作者提供。
- en: Notably, training a model this way produces a single prediction indicating what
    the model believes is the most *likely* value of the target given the features.
    In regression tasks, this is usually the mean of the target distribution conditioned
    on the features.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，这种方式训练的模型产生一个单一预测，指示模型认为在给定特征的情况下目标的最*可能*值。在回归任务中，这通常是条件特征下目标分布的均值。
- en: 'However, as illustrated in a [previous article](https://medium.com/towards-data-science/understanding-noisy-data-and-uncertainty-in-machine-learning-4a2995a84198),
    most machine learning models are trained on noisy datasets, and simply predicting
    the conditional expected value of the target does not sufficiently characterize
    the target distribution. To see this, consider the following noisy regression
    dataset:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如 [上一篇文章](https://medium.com/towards-data-science/understanding-noisy-data-and-uncertainty-in-machine-learning-4a2995a84198)
    中所示，大多数机器学习模型在噪声数据集上训练，而仅仅预测目标的条件期望值不足以充分描述目标分布。要看到这一点，请考虑以下噪声回归数据集：
- en: '![](../Images/57dde15864f912447b122781e67dc0a7.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57dde15864f912447b122781e67dc0a7.png)'
- en: Noisy Linear Regression Data. Image by Author.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声线性回归数据。图片由作者提供。
- en: Even though the model prediction fits the data optimally, it doesn’t quantify
    uncertainty. For instance, when x is around 0.4, the line of best fit predicts
    that y is 3.8\. While it is true that 3.8 is the most likely value of y when x
    is near 0.4, there are plenty of examples in the data where y is much higher or
    lower than 3.8\. Said differently, the conditional distribution of the target
    has high variability beyond its exception, and quantile regression helps us describe
    this.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 即使模型预测与数据最优匹配，它也没有量化不确定性。例如，当 x 接近 0.4 时，最佳拟合线预测 y 为 3.8。虽然在 x 接近 0.4 时 3.8
    是 y 最可能的值，但数据中有许多例子 y 的值远高于或低于 3.8。换句话说，目标的条件分布在其例外之外具有高变异性，而分位数回归帮助我们描述这一点。
- en: In quantile regression, the loss function is modified to encourage a model to
    learn the desired quantile of the conditional target distribution.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在分位数回归中，损失函数被修改以鼓励模型学习条件目标分布的期望分位数。
- en: '![](../Images/688d0d5e47a7fd86dc565693bd59e373.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/688d0d5e47a7fd86dc565693bd59e373.png)'
- en: Quantile Regression Loss Function. Image by Author.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 分位数回归损失函数。图片由作者提供。
- en: 'To gain a better intuition about this loss function, suppose a model is being
    trained to learn the 95th quantile of a target distribution. In the first case,
    consider a single training example where the target was 45 and the model predicted
    60 (i.e. the model overestimated the target by 15). Assume also that each training
    example has a weight of 1\. The loss function evaluated at these values looks
    like this:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这个损失函数，假设一个模型正在训练以学习目标分布的第95百分位数。在第一个例子中，考虑一个训练样本，其中目标值为45，模型预测为60（即模型高估了目标15）。假设每个训练样本的权重为1。损失函数在这些值下的评估如下：
- en: '![](../Images/1c438a0fad9131f3276801ff7f145f53.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c438a0fad9131f3276801ff7f145f53.png)'
- en: 95th Quantile Loss Function Overestimation. Image by Author.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 第95百分位数损失函数高估。作者提供的图像。
- en: 'Now, with the same target of 45, assume that the model predicted 30 (i.e. the
    model underestimated the target by 15). The value of the loss function looks much
    different:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设目标值为45，模型预测为30（即模型低估目标15）。损失函数的值看起来差异很大：
- en: '![](../Images/0c8c1c4a0e44e28e1d5f5b38193bdc24.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0c8c1c4a0e44e28e1d5f5b38193bdc24.png)'
- en: 95th Quantile Loss Function Underestimation. Image by Author.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 第95百分位数损失函数低估。作者提供的图像。
- en: 'Despite the model prediction being off by 15 in both examples, the loss function
    penalized the underestimation much higher than the overestimation. Because the
    95th quantile is being learned, the loss function penalizes any prediction below
    the target by a factor of 0.95, and any prediction above the target by a factor
    of 0.05\. Hence, the model is “forced” to favor overprediction above underprediction
    when learning the 95th quantile. This is the case when learning any quantiles
    above the median — the opposite is true when learning quantiles below the median.
    To see this better, we can visualize the loss function for each predicted quantile:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在两个例子中模型预测都偏差了15，但损失函数对低估的惩罚远高于高估。因为学习的是第95百分位数，损失函数对低于目标的任何预测惩罚系数为0.95，对高于目标的预测惩罚系数为0.05。因此，当学习第95百分位数时，模型“被迫”倾向于高估而非低估。这种情况发生在学习任何中位数以上的百分位数时
    — 学习中位数以下的百分位数时情况正好相反。为了更好地理解这一点，我们可以可视化每个预测百分位数的损失函数：
- en: '![](../Images/21f46d6f89e042cf8a5cf91e2f4590f8.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/21f46d6f89e042cf8a5cf91e2f4590f8.png)'
- en: 95th Quantile Loss Function. Underprediction (i.e. Target — Prediction > 0)
    is Penalized Heavier. Image by Author.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 第95百分位数损失函数。低估（即目标 — 预测 > 0）受到更重的惩罚。作者提供的图像。
- en: '![](../Images/b734176dae97f1e8f2fcf205c840ecc7.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b734176dae97f1e8f2fcf205c840ecc7.png)'
- en: 50th Quantile Loss Function. Overprediction and Underprediction are Penalized
    Equally. Image by Author.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 第50百分位数损失函数。高估和低估受到相同的惩罚。作者提供的图像。
- en: '![](../Images/635f3ac4d1a4fc989377c33fb52c82f7.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/635f3ac4d1a4fc989377c33fb52c82f7.png)'
- en: 10th Quantile Loss Function. Overprediction (i.e. Target — Prediction < 0) is
    Penalized Heavier. Image by Author.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 第10百分位数损失函数。高估（即目标 — 预测 < 0）受到更重的惩罚。作者提供的图像。
- en: 'Catboost now extends this idea by allowing the base decision trees to output
    multiple quantiles per node. This allows a single model to predict multiple quantiles
    by minimizing a new loss function:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Catboost 现在通过允许基础决策树在每个节点输出多个百分位数来扩展这个概念。这使得单个模型可以通过最小化新的损失函数来预测多个百分位数：
- en: '![](../Images/4bdc970cb9f26cf6e75704aee39d7ec4.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4bdc970cb9f26cf6e75704aee39d7ec4.png)'
- en: Catboost Multi-Quantile Loss Function. Image by Author.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Catboost 多百分位数损失函数。作者提供的图像。
- en: Example 1 —Simple Linear Regression
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例 1 — 简单线性回归
- en: 'To understand how the multi-quantile loss function works, let''s start with
    a simple dataset. The following python code generates a synthetic linear dataset
    with gaussian additive noise:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解多百分位数损失函数如何工作，我们从一个简单的数据集开始。以下 Python 代码生成一个带有高斯加性噪声的合成线性数据集：
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The resulting training data should look similar to this:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 结果训练数据应该类似于此：
- en: '![](../Images/a3d6ec859125989e546af3cca02b1eed.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a3d6ec859125989e546af3cca02b1eed.png)'
- en: Noisy Linear Regression Data. Image by Author.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 嘈杂的线性回归数据。作者提供的图像。
- en: Next, the quantiles of the target distribution need to be specified for prediction.
    To illustrate the power of the multi-quantile loss, this model will seek to predict
    99 different quantile values for each observation. This can almost be thought
    of as taking samples of size 99 from each predicted distribution.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，需要为预测指定目标分布的百分位数。为了展示多百分位数损失的强大功能，该模型将寻求为每个观察预测99个不同的百分位数值。这几乎可以认为是从每个预测分布中采样99个样本。
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The resulting predictions DataFrame looks something like this:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 结果预测的数据框如下所示：
- en: '![](../Images/bda08f4e5ee8be3cdad67bdc7ded837c.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bda08f4e5ee8be3cdad67bdc7ded837c.png)'
- en: Quantile Predictions. Image by Author.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 百分位数预测。图片由作者提供。
- en: 'Each row corresponds to a testing example and each column gives a predicted
    quantile. For instance, the 10th quantile prediction for the first test example
    was 3.318624\. Since there is only one input feature, we can visualize a few predicted
    quantiles overlayed on the testing data:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 每一行对应一个测试示例，每一列给出一个预测的百分位数。例如，第一个测试示例的第 10 百分位数预测值为 3.318624\. 由于只有一个输入特征，我们可以将几个预测的百分位数叠加在测试数据上进行可视化：
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/a681046fe00ee611e24fa4392486ff68.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a681046fe00ee611e24fa4392486ff68.png)'
- en: Testing Data overlaid with Predicted Quantiles. Image by Author.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 测试数据与预测的百分位数叠加。图片由作者提供。
- en: 'Visually, the 95th and 5th quantiles appear to do a good job accounting for
    uncertainty in the data. Moreover, the 50th quantile (i.e the median) roughly
    approximates the line of best fit. When working with predicted quantiles, one
    metric we’re often interested in analyzing is coverage. Coverage is the percentage
    of targets that fall between two desired quantiles. As an example, coverage can
    be computed using the 5th and 95th quantiles as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 从视觉上看，第 95 百分位数和第 5 百分位数似乎很好地考虑了数据的不确定性。此外，第 50 百分位数（即中位数）大致近似于最佳拟合线。在处理预测的百分位数时，我们通常感兴趣的一个指标是覆盖率。覆盖率是落在两个期望百分位数之间的目标百分比。例如，覆盖率可以使用第
    5 百分位数和第 95 百分位数计算如下：
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Using the 5th and 95th quantiles, assuming perfect calibration, we would expect
    to have a coverage of 95–5 = 90%. In this example, the predicted quantiles were
    slightly off but still close, giving a coverage value of 91.4%.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用第 5 百分位数和第 95 百分位数，假设完美的校准，我们预计覆盖率为 95–5 = 90%。在这个示例中，预测的百分位数略有偏差，但仍然接近，覆盖率值为
    91.4%。
- en: 'Let’s now analyze the entire predicted distribution that the model outputs.
    Recall, the line of best fit is y = 2x + 3\. Therefore, for any input x, the mean
    of the predicted distribution should be around 2x + 3\. Likewise, because random
    gaussian noise was added to the data with a standard deviation of 0.3, the standard
    deviation of the predicted distribution should be around 0.3\. Let’s test this:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们分析模型输出的整个预测分布。回顾一下，最佳拟合线是 y = 2x + 3\. 因此，对于任何输入 x，预测分布的均值应接近 2x + 3\.
    同样，由于数据中加入了标准差为 0.3 的随机高斯噪声，预测分布的标准差应接近 0.3\. 让我们来测试一下：
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/5deef62a1debe937cc7fd7dcf2fdc47b.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5deef62a1debe937cc7fd7dcf2fdc47b.png)'
- en: Predicted Distribution of y when x = 4\. Image by Author.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当 x = 4 时 y 的预测分布。图片由作者提供。
- en: Amazingly, the predicted distribution seems to align with our expectations.
    Next, let’s try a more difficult example.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，预测分布似乎与我们的预期一致。接下来，让我们尝试一个更困难的例子。
- en: Example 2— Non-Linear Regression with Variable Noise
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例 2—具有可变噪声的非线性回归
- en: 'To see the true power of multi-quantile regression, let’s make the learning
    task more difficult. The following code creates a non-linear dataset with heterogeneous
    noise that depends on specific regions of the domain:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看到多百分位数回归的真正威力，让我们使学习任务更加困难。以下代码创建了一个具有异质噪声的非线性数据集，这些噪声依赖于域的特定区域：
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The resulting training data looks like this:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 结果训练数据如下所示：
- en: '![](../Images/066b53a5d56c4f784779618db2a7d5e2.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/066b53a5d56c4f784779618db2a7d5e2.png)'
- en: Training Data for Example 2\. Image by Author.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 2 的训练数据。图片由作者提供。
- en: 'We’ll fit the Catboost regressor in the same way as example 1 and visualize
    the predictions on a test set:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以与示例 1 相同的方式拟合 Catboost 回归器，并在测试集上可视化预测结果：
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/92768f191a1b5d1ed75aea353a2b60af.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/92768f191a1b5d1ed75aea353a2b60af.png)'
- en: Testing Data overlaid with Predicted Quantiles. Image by Author.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 测试数据与预测的百分位数叠加。图片由作者提供。
- en: 'Upon visual inspection, the model has characterized this non-linear, heteroscedastic
    relationship well. Notice how, near x = 0, the three predicted quantiles converge
    towards a single value. This is because the region near x = 0 has almost no noise
    — any model that correctly predicts the conditional probability distribution in
    this region should predict a small variance. Conversely, when x is between 7.5
    and 10.0, the predicted quantiles are much further apart because of the inherent
    noise in the region. 90% coverage can be computed as before:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 通过视觉检查，模型很好地刻画了这种非线性、异方差的关系。注意在 x = 0 附近，三个预测分位数趋向于一个单一值。这是因为 x = 0 附近的区域几乎没有噪声——任何正确预测该区域条件概率分布的模型都应预测出小方差。相反，当
    x 在 7.5 到 10.0 之间时，由于该区域的固有噪声，预测的分位数则相距更远。90% 覆盖度可以像以前一样计算：
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Using the 5th and 95th quantiles, assuming perfect calibration, the expected
    coverage is 95–5 = 90%. In this example, the predicted quantiles were even better
    than example 1, giving a coverage of 90.506%.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用第5和第95分位数，假设完美校准，期望覆盖度为 95–5 = 90%。在这个例子中，预测的分位数甚至比示例1更好，覆盖度达到了90.506%。
- en: Finally, let’s look at a few inputs and their corresponding predicted distribution.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们查看一些输入及其相应的预测分布。
- en: '![](../Images/98e49ab357176e4c5b5e5bedc4e7ad96.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/98e49ab357176e4c5b5e5bedc4e7ad96.png)'
- en: Predicted Target Distribution for x = -0.06\. Image by Author.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: x = -0.06 的预测目标分布。图像作者提供。
- en: 'The distribution above does a nice job of capturing the target value with a
    relatively small variance. This is to be expected as the target values in the
    training data when x = 0 have little noise. Contrast this with the predicted target
    distribution when x = -8.56:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 上述分布在相对较小的方差下很好地捕捉了目标值。这是可以预期的，因为当 x = 0 时，训练数据中的目标值几乎没有噪声。与此对比的是当 x = -8.56
    时的预测目标分布：
- en: '![](../Images/086cf53bc3a59709447c3af27ee02b69.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/086cf53bc3a59709447c3af27ee02b69.png)'
- en: Predicted Target Distribution for x = -8.56\. Image by Author.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: x = -8.56 的预测目标分布。图像作者提供。
- en: This distribution is right skewed and has a much higher variance. This is expected
    for this region of data because the noise was sampled from an exponential distribution
    with high variance.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 该分布向右偏斜，方差更高。这在该数据区域是预期的，因为噪声是从高方差的指数分布中采样的。
- en: Final Thoughts
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: This article demonstrated the power of multi-quantile regression for learning
    arbitrary conditional target distributions. We only explored two one-dimensional
    examples to visually inspect model performance, but I would encourage anyone interested
    to try the multi-quantiles loss function on higher dimensional data. It’s important
    to note that quantile regression makes no statistical or algorithmic guarantees
    of convergence, and the performance of these models will vary depending on the
    nature of the learning problem. Thanks for reading!
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 本文展示了多分位回归在学习任意条件目标分布方面的强大能力。我们只探索了两个一维示例以视觉检查模型性能，但我鼓励任何感兴趣的人尝试在更高维数据上使用多分位损失函数。重要的是要注意，分位回归不提供收敛的统计或算法保证，这些模型的表现将根据学习问题的性质而有所不同。感谢阅读！
- en: '*Become a Member:* [*https://harrisonfhoffman.medium.com/membership*](https://harrisonfhoffman.medium.com/membership)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '*成为会员：* [*https://harrisonfhoffman.medium.com/membership*](https://harrisonfhoffman.medium.com/membership)'
- en: '*Buy me a coffee:* [*https://www.buymeacoffee.com/HarrisonfhU*](https://www.buymeacoffee.com/HarrisonfhU)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*请我喝杯咖啡：* [*https://www.buymeacoffee.com/HarrisonfhU*](https://www.buymeacoffee.com/HarrisonfhU)'
- en: References
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*Catboost Loss Functions —* [https://catboost.ai/en/docs/concepts/loss-functions-regression#MultiQuantile](https://catboost.ai/en/docs/concepts/loss-functions-regression#MultiQuantile)'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Catboost 损失函数 —* [https://catboost.ai/en/docs/concepts/loss-functions-regression#MultiQuantile](https://catboost.ai/en/docs/concepts/loss-functions-regression#MultiQuantile)'
