- en: Deploying LLMs On Amazon SageMaker With DJL Serving
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Amazon SageMaker 上部署 LLMs 与 DJL Serving
- en: 原文：[https://towardsdatascience.com/deploying-llms-on-amazon-sagemaker-with-djl-serving-8220e3cfad0c?source=collection_archive---------8-----------------------#2023-06-07](https://towardsdatascience.com/deploying-llms-on-amazon-sagemaker-with-djl-serving-8220e3cfad0c?source=collection_archive---------8-----------------------#2023-06-07)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/deploying-llms-on-amazon-sagemaker-with-djl-serving-8220e3cfad0c?source=collection_archive---------8-----------------------#2023-06-07](https://towardsdatascience.com/deploying-llms-on-amazon-sagemaker-with-djl-serving-8220e3cfad0c?source=collection_archive---------8-----------------------#2023-06-07)
- en: Deploy BART on Amazon SageMaker Real-Time Inference
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Amazon SageMaker 上实时推理部署 BART
- en: '[](https://ram-vegiraju.medium.com/?source=post_page-----8220e3cfad0c--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page-----8220e3cfad0c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8220e3cfad0c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8220e3cfad0c--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page-----8220e3cfad0c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://ram-vegiraju.medium.com/?source=post_page-----8220e3cfad0c--------------------------------)[![Ram
    Vegiraju](../Images/07d9334e905f710d9f3c6187cf69a1a5.png)](https://ram-vegiraju.medium.com/?source=post_page-----8220e3cfad0c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----8220e3cfad0c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----8220e3cfad0c--------------------------------)
    [Ram Vegiraju](https://ram-vegiraju.medium.com/?source=post_page-----8220e3cfad0c--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6e49569edd2b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeploying-llms-on-amazon-sagemaker-with-djl-serving-8220e3cfad0c&user=Ram+Vegiraju&userId=6e49569edd2b&source=post_page-6e49569edd2b----8220e3cfad0c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8220e3cfad0c--------------------------------)
    ·8 min read·Jun 7, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8220e3cfad0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeploying-llms-on-amazon-sagemaker-with-djl-serving-8220e3cfad0c&user=Ram+Vegiraju&userId=6e49569edd2b&source=-----8220e3cfad0c---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F6e49569edd2b&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeploying-llms-on-amazon-sagemaker-with-djl-serving-8220e3cfad0c&user=Ram+Vegiraju&userId=6e49569edd2b&source=post_page-6e49569edd2b----8220e3cfad0c---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----8220e3cfad0c--------------------------------)
    ·8 分钟阅读·2023年6月7日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F8220e3cfad0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeploying-llms-on-amazon-sagemaker-with-djl-serving-8220e3cfad0c&user=Ram+Vegiraju&userId=6e49569edd2b&source=-----8220e3cfad0c---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8220e3cfad0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeploying-llms-on-amazon-sagemaker-with-djl-serving-8220e3cfad0c&source=-----8220e3cfad0c---------------------bookmark_footer-----------)![](../Images/27d3d2701b592cc5346c24eb2c67afe6.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F8220e3cfad0c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdeploying-llms-on-amazon-sagemaker-with-djl-serving-8220e3cfad0c&source=-----8220e3cfad0c---------------------bookmark_footer-----------)![](../Images/27d3d2701b592cc5346c24eb2c67afe6.png)'
- en: Image from [Unsplash](https://unsplash.com/photos/CejqWHRRXUQ)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源 [Unsplash](https://unsplash.com/photos/CejqWHRRXUQ)
- en: Large Language Models (LLMs) and Generative AI continue to take over the Machine
    Learning and general tech space in 2023\. With the LLM expansion has come an influx
    of new models that continue to improve at a stunning rate.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）和生成式 AI 在 2023 年继续主导机器学习和科技领域。随着 LLM 的扩展，新模型的涌现不断以惊人的速度提升。
- en: While the accuracy and performance of these models are incredible, they have
    their own set of challenges in terms of hosting these models. Without model hosting,
    it is hard to recognize the value that these LLMs provide in real-world applications.
    What are the specific challenges with LLM hosting and performance tuning?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些模型的准确性和性能令人难以置信，但在托管这些模型时仍面临一系列挑战。如果没有模型托管，就很难认识到这些 LLM 在实际应用中的价值。LLM 托管和性能调优面临的具体挑战是什么？
- en: How can we load these larger models that are scaling up to past 100s of GBs
    in size?
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何加载这些扩展到超过100 GB的大型模型？
- en: How can we properly apply model partitioning techniques to efficiently utilize
    hardware while not compromising on model accuracy?
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何正确地应用模型划分技术以高效利用硬件，同时不妥协模型准确性？
- en: How can we fit these models on a singular GPU or multiple?
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何将这些模型适配到单个GPU或多个GPU上？
- en: 'These are all challenging questions that are addressed and abstracted out through
    a model server known as [DJL Serving](http://djl.ai/serving/). DJL Serving is
    a high performance universal solution that integrates directly with various model
    partitioning frameworks such as the following: [HuggingFace Accelerate](https://huggingface.co/docs/accelerate/index),
    [DeepSpeed](https://github.com/microsoft/DeepSpeed), and [FasterTransformers](https://github.com/NVIDIA/FasterTransformer).
    With DJL Serving you…'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都是通过被称为[DJL Serving](http://djl.ai/serving/)的模型服务器解决并抽象出来的挑战性问题。DJL Serving是一个高性能的通用解决方案，直接集成了各种模型划分框架，如[HuggingFace
    Accelerate](https://huggingface.co/docs/accelerate/index)、[DeepSpeed](https://github.com/microsoft/DeepSpeed)和[FasterTransformers](https://github.com/NVIDIA/FasterTransformer)。通过DJL
    Serving，你可以…
