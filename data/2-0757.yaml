- en: Does Bagging Help to Prevent Overfitting in Decision Trees?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Baggingæ˜¯å¦æœ‰åŠ©äºé˜²æ­¢å†³ç­–æ ‘çš„è¿‡æ‹Ÿåˆï¼Ÿ
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/does-bagging-help-to-prevent-overfitting-in-decision-trees-42262943a81e](https://towardsdatascience.com/does-bagging-help-to-prevent-overfitting-in-decision-trees-42262943a81e)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/does-bagging-help-to-prevent-overfitting-in-decision-trees-42262943a81e](https://towardsdatascience.com/does-bagging-help-to-prevent-overfitting-in-decision-trees-42262943a81e)
- en: Understand why decision trees are highly prone to overfitting and its potential
    remedies
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç†è§£ä¸ºä»€ä¹ˆå†³ç­–æ ‘é«˜åº¦å®¹æ˜“è¿‡æ‹ŸåˆåŠå…¶æ½œåœ¨çš„è¡¥æ•‘æªæ–½
- en: '[](https://medium.com/@gurjinderkaur95?source=post_page-----42262943a81e--------------------------------)[![Gurjinder
    Kaur](../Images/d5c6746466025dad06077b1a89a789d1.png)](https://medium.com/@gurjinderkaur95?source=post_page-----42262943a81e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----42262943a81e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----42262943a81e--------------------------------)
    [Gurjinder Kaur](https://medium.com/@gurjinderkaur95?source=post_page-----42262943a81e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@gurjinderkaur95?source=post_page-----42262943a81e--------------------------------)[![Gurjinder
    Kaur](../Images/d5c6746466025dad06077b1a89a789d1.png)](https://medium.com/@gurjinderkaur95?source=post_page-----42262943a81e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----42262943a81e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----42262943a81e--------------------------------)
    [Gurjinder Kaur](https://medium.com/@gurjinderkaur95?source=post_page-----42262943a81e--------------------------------)'
- en: Â·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----42262943a81e--------------------------------)
    Â·12 min readÂ·Dec 13, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·å‘è¡¨äº[Towards Data Science](https://towardsdatascience.com/?source=post_page-----42262943a81e--------------------------------)
    Â·12åˆ†é’Ÿé˜…è¯»Â·2023å¹´12æœˆ13æ—¥
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/49dc05d1ffe1c84260864fe2ab15c16c.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/49dc05d1ffe1c84260864fe2ab15c16c.png)'
- en: Photo by [Jan Huber](https://unsplash.com/@jan_huber?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”±[Jan Huber](https://unsplash.com/@jan_huber?utm_source=medium&utm_medium=referral)æä¾›ï¼Œæ¥æºäº[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»‹ç»
- en: Decision trees are a class of machine learning algorithms well known for their
    ability to solve both classification and regression problems, and not to forget
    the ease of interpretation they offer. However, they suffer from overfitting and
    can fail to generalize well if not controlled properly.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å†³ç­–æ ‘æ˜¯ä¸€ç±»æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œä»¥å…¶è§£å†³åˆ†ç±»å’Œå›å½’é—®é¢˜çš„èƒ½åŠ›è€Œé—»åï¼ŒåŒæ—¶å®ƒä»¬æä¾›äº†æ˜“äºè§£é‡Šçš„ä¼˜ç‚¹ã€‚ç„¶è€Œï¼Œå¦‚æœæ²¡æœ‰å¾—åˆ°é€‚å½“æ§åˆ¶ï¼Œå®ƒä»¬å®¹æ˜“å‡ºç°è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå¯èƒ½æ— æ³•å¾ˆå¥½åœ°æ³›åŒ–ã€‚
- en: In this article, we will discuss what is overfitting, to what extent a decision
    tree overfits the training data, why it is an issue, and how it can be addressed.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºä»€ä¹ˆæ˜¯è¿‡æ‹Ÿåˆï¼Œå†³ç­–æ ‘åœ¨å¤šå¤§ç¨‹åº¦ä¸Šä¼šå¯¹è®­ç»ƒæ•°æ®è¿‡æ‹Ÿåˆï¼Œä¸ºä»€ä¹ˆè¿™æ˜¯ä¸€ä¸ªé—®é¢˜ï¼Œä»¥åŠå¦‚ä½•è§£å†³å®ƒã€‚
- en: Then, we will get ourselves acquainted with one of the ensemble techniques i.e.,
    *bagging*, and see if it can be used to make decision trees more robust.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å°†äº†è§£ä¸€ç§é›†æˆæŠ€æœ¯ï¼Œå³*bagging*ï¼Œå¹¶çœ‹çœ‹å®ƒæ˜¯å¦å¯ä»¥ç”¨æ¥ä½¿å†³ç­–æ ‘æ›´å¼ºå¥ã€‚
- en: 'We will cover the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æ¶µç›–ä»¥ä¸‹å†…å®¹ï¼š
- en: Create our regression dataset using NumPy.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨NumPyåˆ›å»ºæˆ‘ä»¬çš„å›å½’æ•°æ®é›†ã€‚
- en: Train a decision tree model using scikit-learn.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨scikit-learnè®­ç»ƒä¸€ä¸ªå†³ç­–æ ‘æ¨¡å‹ã€‚
- en: Understand what overfitting means by looking at the performance of the same
    model on the training set and test set.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡æŸ¥çœ‹åŒä¸€æ¨¡å‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šçš„è¡¨ç°ï¼Œç†è§£è¿‡æ‹Ÿåˆçš„å«ä¹‰ã€‚
- en: Discuss why overfitting is more common in non-parametric models such as decision
    trees (and of course learn what is meant by the term non-parametric) and how it
    can be prevented using regularization.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®¨è®ºä¸ºä»€ä¹ˆåœ¨éå‚æ•°æ¨¡å‹ï¼ˆå¦‚å†³ç­–æ ‘ï¼‰ä¸­è¿‡æ‹Ÿåˆæ›´ä¸ºå¸¸è§ï¼ˆå½“ç„¶ä¹Ÿäº†è§£ä¸€ä¸‹éå‚æ•°çš„å«ä¹‰ï¼‰ï¼Œä»¥åŠå¦‚ä½•é€šè¿‡æ­£åˆ™åŒ–æ¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚
- en: Understand what *bootstrap aggregation* (*bagging* in short) is and how it can
    potentially help with overfitting.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: äº†è§£ä»€ä¹ˆæ˜¯*è‡ªåŠ©èšåˆ*ï¼ˆç®€ç§°*bagging*ï¼‰ï¼Œä»¥åŠå®ƒå¦‚ä½•æœ‰å¯èƒ½å¸®åŠ©è§£å†³è¿‡æ‹Ÿåˆé—®é¢˜ã€‚
- en: Finally, we will implement the bagging version of the decision tree and see
    if it helps or not ğŸ¤
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å°†å®ç°å†³ç­–æ ‘çš„baggingç‰ˆæœ¬ï¼Œçœ‹çœ‹å®ƒæ˜¯å¦æœ‰å¸®åŠ©ğŸ¤
- en: '**Still wondering if itâ€™s worth reading?** ğŸ¤” If youâ€™ve ever wondered why Random
    Forests are usually preferred over vanilla Decision Trees, this is the best place
    to start since Random Forests use the idea of *bagging plus something else* to
    improve upon decision trees.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**è¿˜åœ¨çŠ¹è±«æ˜¯å¦å€¼å¾—é˜…è¯»ï¼Ÿ** ğŸ¤” å¦‚æœä½ æ›¾ç»æƒ³çŸ¥é“ä¸ºä»€ä¹ˆéšæœºæ£®æ—é€šå¸¸æ¯”æ™®é€šçš„å†³ç­–æ ‘æ›´å—æ¬¢è¿ï¼Œè¿™æ˜¯æœ€å¥½çš„èµ·ç‚¹ï¼Œå› ä¸ºéšæœºæ£®æ—ä½¿ç”¨äº†*baggingåŠ ä¸Šä¸€äº›å…¶ä»–æ–¹æ³•*æ¥æ”¹è¿›å†³ç­–æ ‘ã€‚'
- en: Letâ€™s get started!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¼€å§‹å§ï¼
- en: We will set up a Python notebook and import the libraries first.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é¦–å…ˆå°†è®¾ç½®ä¸€ä¸ª Python ç¬”è®°æœ¬å¹¶å¯¼å…¥ç›¸å…³åº“ã€‚
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Step 1: Creating the dataset**'
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 1ï¼šåˆ›å»ºæ•°æ®é›†**'
- en: 'We are going to use a dataset resembling a quadratic function with *y* as the
    target variable and *X* as the independent variable. Since *y* is numeric, we
    will be fitting a regression tree on this dataset. Letâ€™s build our dataset as
    follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªç±»ä¼¼äºäºŒæ¬¡å‡½æ•°çš„æ•°æ®é›†ï¼Œå…¶ä¸­ *y* ä¸ºç›®æ ‡å˜é‡ï¼Œ*X* ä¸ºè‡ªå˜é‡ã€‚ç”±äº *y* æ˜¯æ•°å€¼å‹çš„ï¼Œæˆ‘ä»¬å°†å¯¹è¿™ä¸ªæ•°æ®é›†æ‹Ÿåˆä¸€ä¸ªå›å½’æ ‘ã€‚è®©æˆ‘ä»¬æŒ‰å¦‚ä¸‹æ–¹å¼æ„å»ºæ•°æ®é›†ï¼š
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We have created a dataset with 500 samples where both *X* and *y* are continuous
    as shown below. The link to the full notebook along with visualizations can be
    found at the end of this article, so donâ€™t worry about the missing viz code in
    this article.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåŒ…å« 500 ä¸ªæ ·æœ¬çš„æ•°æ®é›†ï¼Œå…¶ä¸­ *X* å’Œ *y* éƒ½æ˜¯è¿ç»­çš„ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚å®Œæ•´çš„ç¬”è®°æœ¬åŠå…¶å¯è§†åŒ–çš„é“¾æ¥å¯ä»¥åœ¨æœ¬æ–‡æœ«å°¾æ‰¾åˆ°ï¼Œæ‰€ä»¥ä¸ç”¨æ‹…å¿ƒæœ¬æ–‡ä¸­ç¼ºå¤±çš„å¯è§†åŒ–ä»£ç ã€‚
- en: '![](../Images/0437751797643f16d797f0f7643e3809.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0437751797643f16d797f0f7643e3809.png)'
- en: 'Source: Image by author'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼šä½œè€…æä¾›çš„å›¾ç‰‡
- en: '**Step 2: Train-test split**'
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 2ï¼šè®­ç»ƒ-æµ‹è¯•æ‹†åˆ†**'
- en: 'We can use scikit-learnâ€™s *train_test_split* to split our dataset into a training
    set and test set as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ scikit-learn çš„ *train_test_split* å°†æ•°æ®é›†æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`Cell Output:`'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`å•å…ƒè¾“å‡ºï¼š`'
- en: '![](../Images/fd12191c5e8907a2adb39bb4d9e74aba.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd12191c5e8907a2adb39bb4d9e74aba.png)'
- en: We will only use the training set for training our model and keep the test set
    aside to use it just for testing the modelâ€™s performance. This will ensure we
    are testing our model against samples it has never seen before and will help us
    evaluate how well it generalizes. How smart, right? ğŸ˜
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä»…ä½¿ç”¨è®­ç»ƒé›†æ¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œå¹¶å°†æµ‹è¯•é›†ä¿ç•™ç”¨äºæµ‹è¯•æ¨¡å‹çš„æ€§èƒ½ã€‚è¿™å°†ç¡®ä¿æˆ‘ä»¬æµ‹è¯•æ¨¡å‹çš„æ ·æœ¬æ˜¯æ¨¡å‹ä»æœªè§è¿‡çš„ï¼Œä»è€Œå¸®åŠ©æˆ‘ä»¬è¯„ä¼°æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚èªæ˜å§ï¼Ÿ
    ğŸ˜
- en: 'Okay, the following is what our training and test sets look like:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œæ¥ä¸‹æ¥æ˜¯æˆ‘ä»¬çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„æ ·å­ï¼š
- en: '![](../Images/f8bd44f240b3660c4fc379aa69a2b617.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8bd44f240b3660c4fc379aa69a2b617.png)'
- en: 'Source: Image by author'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼šä½œè€…æä¾›çš„å›¾ç‰‡
- en: '**Step 3: Fitting the regression tree on the training set**'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 3ï¼šåœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆå›å½’æ ‘**'
- en: Fitting a decision tree regressor using scikit-learn is just two lines of code.
    However, if youâ€™re not sure whatâ€™s happening under the hood and are a curious
    learner, [this article](https://medium.com/p/fbb908cf548b) would be your go-to
    guide for understanding how exactly a decision tree solves a regression problem.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ scikit-learn æ‹Ÿåˆå†³ç­–æ ‘å›å½’å™¨åªéœ€ä¸¤è¡Œä»£ç ã€‚ç„¶è€Œï¼Œå¦‚æœä½ ä¸ç¡®å®šèƒŒåå‘ç”Ÿäº†ä»€ä¹ˆï¼Œå¹¶ä¸”æ˜¯ä¸€ä¸ªå¥½å¥‡çš„å­¦ä¹ è€…ï¼Œ[è¿™ç¯‡æ–‡ç« ](https://medium.com/p/fbb908cf548b)
    å°†æ˜¯ä½ äº†è§£å†³ç­–æ ‘å¦‚ä½•è§£å†³å›å½’é—®é¢˜çš„å¿…å¤‡æŒ‡å—ã€‚
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`Cell Output:`'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`å•å…ƒè¾“å‡ºï¼š`'
- en: '![](../Images/8f2679ca1bf0088971339172164d23c4.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8f2679ca1bf0088971339172164d23c4.png)'
- en: '**Step 4: Evaluating the regression tree on training and test sets**'
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 4ï¼šåœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šè¯„ä¼°å›å½’æ ‘**'
- en: 'Now, since our model has been trained, letâ€™s use it to make predictions on:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæ—¢ç„¶æˆ‘ä»¬çš„æ¨¡å‹å·²ç»è®­ç»ƒå¥½äº†ï¼Œè®©æˆ‘ä»¬ç”¨å®ƒæ¥è¿›è¡Œé¢„æµ‹ï¼š
- en: '**the training set** i.e., the data it already is friends with.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è®­ç»ƒé›†** å³æ¨¡å‹å·²ç»â€œç†Ÿæ‚‰â€çš„æ•°æ®ã€‚'
- en: '**the test set** i.e., the data it has never seen before (***the real test
    lies here***).'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æµ‹è¯•é›†** å³æ¨¡å‹ä»æœªè§è¿‡çš„æ•°æ®ï¼ˆ***çœŸæ­£çš„è€ƒéªŒå°±åœ¨è¿™é‡Œ***ï¼‰ã€‚'
- en: We are going to use mean squared error to evaluate the quality of our predictions.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨å‡æ–¹è¯¯å·®æ¥è¯„ä¼°æˆ‘ä»¬é¢„æµ‹çš„è´¨é‡ã€‚
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`Cell Output:`'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`å•å…ƒè¾“å‡ºï¼š`'
- en: '![](../Images/c2ad8618288b2a377c418f63a9ed893b.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c2ad8618288b2a377c418f63a9ed893b.png)'
- en: The decision tree regressor manages to get a ZERO ERROR on the training set.
    We are almost going to crown this model as the greatest of all time until we see
    the results on the test set, and there we stop ğŸ¥¶
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å†³ç­–æ ‘å›å½’å™¨åœ¨è®­ç»ƒé›†ä¸Šçš„è¯¯å·®ä¸ºé›¶ã€‚æˆ‘ä»¬å‡ ä¹è¦å°†è¿™ä¸ªæ¨¡å‹å† ä»¥â€œå²ä¸Šæœ€ä¼Ÿå¤§â€çš„ç§°å·ï¼Œç›´åˆ°æˆ‘ä»¬çœ‹åˆ°æµ‹è¯•é›†ä¸Šçš„ç»“æœï¼Œé‚£æ—¶æˆ‘ä»¬æ‰åœä¸‹ ğŸ¥¶
- en: The same model gets a whopping 173.336 mean squared error on the test set. Looks
    like it failed miserably on the real test ğŸ˜”
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸åŒçš„æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„å‡æ–¹è¯¯å·®è¾¾åˆ°äº†æƒŠäººçš„ 173.336ã€‚çœ‹æ¥å®ƒåœ¨çœŸå®æµ‹è¯•ä¸­è¡¨ç°ç³Ÿç³• ğŸ˜”
- en: This is because the model overfitted (over-learned, over-relied, over-crammed,
    over-memorized) the training data points so perfectly that it failed to learn
    the underlying patterns within the data; rather it caught up on the noise of the
    training set that was specific just to the training set and had nothing to do
    with the overall behavior of the data. This is called ***Overfitting.***
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å› ä¸ºæ¨¡å‹è¿‡åº¦æ‹Ÿåˆï¼ˆè¿‡åº¦å­¦ä¹ ã€è¿‡åº¦ä¾èµ–ã€è¿‡åº¦å¡«å……ã€è¿‡åº¦è®°å¿†ï¼‰äº†è®­ç»ƒæ•°æ®ç‚¹ï¼Œå¯¼è‡´å®ƒæœªèƒ½å­¦ä¹ æ•°æ®ä¸­çš„æ½œåœ¨æ¨¡å¼ï¼›ç›¸åï¼Œå®ƒæŠ“ä½äº†ä»…å¯¹è®­ç»ƒé›†ç‰¹æœ‰çš„å™ªå£°ï¼Œè€Œä¸æ•°æ®çš„æ•´ä½“è¡Œä¸ºæ— å…³ã€‚è¿™è¢«ç§°ä¸º
    ***è¿‡æ‹Ÿåˆã€‚***
- en: '**Overfitting** is the property of a model such that the model predicts very
    well the labels of the examples used during training but frequently makes errors
    when applied to examples that werenâ€™t seen by the learning algorithm during training
    â€” Andriy Burkov (The Hundred Page Machine Learning Book)'
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**è¿‡æ‹Ÿåˆ**æ˜¯æŒ‡æ¨¡å‹åœ¨è®­ç»ƒæœŸé—´å¯¹ç¤ºä¾‹æ ‡ç­¾çš„é¢„æµ‹éå¸¸å‡†ç¡®ï¼Œä½†åœ¨åº”ç”¨äºæœªè§è¿‡çš„ç¤ºä¾‹æ—¶ç»å¸¸å‡ºç°é”™è¯¯çš„ç‰¹æ€§â€”â€”å®‰å¾·é‡ŒÂ·å¸ƒå°”ç§‘å¤«ï¼ˆã€Šç™¾é¡µæœºå™¨å­¦ä¹ ä¹¦ã€‹ï¼‰'
- en: We can see in the following plots how the predicted values are exactly overlapping
    with the actual values for the training set, but not for the test set.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»¥ä¸‹å›¾è¡¨ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è®­ç»ƒé›†çš„é¢„æµ‹å€¼ä¸å®é™…å€¼å®Œå…¨é‡å ï¼Œä½†æµ‹è¯•é›†å´ä¸æ˜¯ã€‚
- en: '![](../Images/41b4140e94e371aa57a713bdc3d10df4.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41b4140e94e371aa57a713bdc3d10df4.png)'
- en: 'Source: Image by author'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼šä½œè€…æä¾›çš„å›¾ç‰‡
- en: Following is what the predictions will look like for the given training and
    test data. It can be clearly seen that the model is trying to fit the training
    data very closely. `Depth=None` indicates that unless specified, there is no restriction
    on the maximum depth a tree can reach. It is the default value of the *max_depth*
    hyperparameter.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ç»™å®šè®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®çš„é¢„æµ‹ç»“æœã€‚å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°ï¼Œæ¨¡å‹è¯•å›¾éå¸¸ç´§å¯†åœ°æ‹Ÿåˆè®­ç»ƒæ•°æ®ã€‚ `Depth=None` è¡¨ç¤ºé™¤éç‰¹åˆ«æŒ‡å®šï¼Œå¦åˆ™æ ‘å¯ä»¥è¾¾åˆ°çš„æœ€å¤§æ·±åº¦æ²¡æœ‰é™åˆ¶ã€‚è¿™æ˜¯
    *max_depth* è¶…å‚æ•°çš„é»˜è®¤å€¼ã€‚
- en: '![](../Images/53e9d9b07733811a938afe0e59d22531.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/53e9d9b07733811a938afe0e59d22531.png)'
- en: 'Source: Image by author'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼šä½œè€…æä¾›çš„å›¾ç‰‡
- en: '***Just Thinking:*** *The term Tightfitting instead of Overfitting makes a
    much better literal sense because thatâ€™s what our model is doing here* ğŸ¤·ğŸ»â€â™€ï¸ Anyways,
    letâ€™s not break the rules and stick with overfitting.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '***ä»…ä»…æ€è€ƒï¼š*** *â€œè¿‡æ‹Ÿåˆâ€ä¸€è¯æ¯”â€œç´§æ‹Ÿåˆâ€æ›´èƒ½å‡†ç¡®è¡¨è¾¾æ¨¡å‹çš„å®é™…è¡Œä¸º* ğŸ¤·ğŸ»â€â™€ï¸ ä¸è¿‡ï¼Œè¿˜æ˜¯ä¸è¦æ‰“ç ´è§„åˆ™ï¼ŒåšæŒä½¿ç”¨è¿‡æ‹Ÿåˆã€‚'
- en: '**Why does overfitting come naturally to decision trees?**'
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**ä¸ºä»€ä¹ˆè¿‡æ‹Ÿåˆåœ¨å†³ç­–æ ‘ä¸­è‡ªç„¶å‘ç”Ÿï¼Ÿ**'
- en: Decision trees are *non-parametric* i.e., they donâ€™t make any assumptions about
    the training data. If left unconstrained, the tree structure will completely adapt
    itself to the training data, fitting it very closely, most likely overfitting
    it.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: å†³ç­–æ ‘æ˜¯ *éå‚æ•°* çš„ï¼Œå³å®ƒä»¬ä¸å¯¹è®­ç»ƒæ•°æ®åšå‡ºä»»ä½•å‡è®¾ã€‚å¦‚æœæ²¡æœ‰é™åˆ¶ï¼Œæ ‘çš„ç»“æ„å°†å®Œå…¨é€‚åº”è®­ç»ƒæ•°æ®ï¼Œç´§å¯†æ‹Ÿåˆï¼Œå¾ˆå¯èƒ½ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆã€‚
- en: Itâ€™s termed as *non-parametric* not because it doesnâ€™t have any parameters but
    because the number of parameters is not determined before training and hence the
    model structure is free to stick closely to the training data (*as opposed to
    linear regression, where we have a fixed number of coefficients i.e., parameters
    that we want the model to learn, so its degree of freedom is limited*)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å†³ç­–æ ‘è¢«ç§°ä¸º *éå‚æ•°* ä¸æ˜¯å› ä¸ºå®ƒæ²¡æœ‰å‚æ•°ï¼Œè€Œæ˜¯å› ä¸ºå‚æ•°çš„æ•°é‡åœ¨è®­ç»ƒä¹‹å‰æ²¡æœ‰ç¡®å®šï¼Œå› æ­¤æ¨¡å‹ç»“æ„å¯ä»¥è‡ªç”±åœ°ç´§è´´è®­ç»ƒæ•°æ®ï¼ˆ*ä¸çº¿æ€§å›å½’ä¸åŒï¼Œçº¿æ€§å›å½’æœ‰å›ºå®šæ•°é‡çš„ç³»æ•°ï¼Œå³æˆ‘ä»¬å¸Œæœ›æ¨¡å‹å­¦ä¹ çš„å‚æ•°ï¼Œæ‰€ä»¥å…¶è‡ªç”±åº¦æ˜¯æœ‰é™çš„*ï¼‰
- en: '**Why is overfitting an issue?**'
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**ä¸ºä»€ä¹ˆè¿‡æ‹Ÿåˆæ˜¯ä¸€ä¸ªé—®é¢˜ï¼Ÿ**'
- en: Overfitting is undesirable because it doesnâ€™t allow the model to generalize
    well on the new data, and if that happens, it will not be able to perform well
    on the classification or prediction tasks it was originally intended for.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: è¿‡æ‹Ÿåˆæ˜¯ä¸å—æ¬¢è¿çš„ï¼Œå› ä¸ºå®ƒä¸å…è®¸æ¨¡å‹åœ¨æ–°æ•°æ®ä¸Šè¿›è¡Œè‰¯å¥½çš„æ³›åŒ–ï¼Œå¦‚æœå‘ç”Ÿè¿™ç§æƒ…å†µï¼Œæ¨¡å‹å°†æ— æ³•åœ¨å…¶æœ€åˆè®¾è®¡çš„åˆ†ç±»æˆ–é¢„æµ‹ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ã€‚
- en: '**What we couldâ€™ve done differently**'
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**æˆ‘ä»¬æœ¬å¯ä»¥åšå¾—ä¸åŒ**'
- en: In case our model shows signs of overfitting, we can infer that the model is
    overly complex and needs to be ***regularized*.**
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬çš„æ¨¡å‹å‡ºç°è¿‡æ‹Ÿåˆçš„è¿¹è±¡ï¼Œæˆ‘ä»¬å¯ä»¥æ¨æ–­æ¨¡å‹è¿‡äºå¤æ‚ï¼Œéœ€è¦***æ­£åˆ™åŒ–*ã€‚**
- en: Regularization is the process of restricting a modelâ€™s freedom by enforcing
    some constraints on it so that the chance of overfitting is reduced.
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ­£åˆ™åŒ–æ˜¯é€šè¿‡å¯¹æ¨¡å‹æ–½åŠ ä¸€äº›çº¦æŸæ¥é™åˆ¶å…¶è‡ªç”±åº¦çš„è¿‡ç¨‹ï¼Œä»è€Œå‡å°‘è¿‡æ‹Ÿåˆçš„æœºä¼šã€‚
- en: Several hyperparameters such as maximum depth, minimum number of samples in
    a leaf node, etc. can be tuned to regularize the decision tree.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥è°ƒæ•´å‡ ä¸ªè¶…å‚æ•°ï¼Œä¾‹å¦‚æœ€å¤§æ·±åº¦ã€å¶å­èŠ‚ç‚¹ä¸­çš„æœ€å°æ ·æœ¬æ•°ç­‰ï¼Œä»¥å¯¹å†³ç­–æ ‘è¿›è¡Œæ­£åˆ™åŒ–ã€‚
- en: The least we could do to prevent a situation like above is to set the *max_depth*
    to stop the tree from over-growing. The default value of *max_depth* is set to
    *None* which means there is no limit on the growth of the decision tree. Reducing
    *max_depth* will regularize the model and thus reduce the risk of overfitting.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥åšçš„æœ€å°‘æªæ–½æ˜¯å°† *max_depth* è®¾ç½®ä¸ºé™åˆ¶æ ‘çš„è¿‡åº¦ç”Ÿé•¿ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œ*max_depth* çš„å€¼è®¾ç½®ä¸º *None*ï¼Œæ„å‘³ç€å¯¹å†³ç­–æ ‘çš„ç”Ÿé•¿æ²¡æœ‰é™åˆ¶ã€‚å‡å°‘
    *max_depth* ä¼šå¯¹æ¨¡å‹è¿›è¡Œæ­£åˆ™åŒ–ï¼Œä»è€Œé™ä½è¿‡æ‹Ÿåˆçš„é£é™©ã€‚
- en: Following are the predicted vs actual plots for the training and test sets for
    different values of *max_depth.*
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ä¸åŒ*max_depth*å€¼çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„é¢„æµ‹ä¸å®é™…ç»“æœå›¾ã€‚
- en: '![](../Images/307b5d4c500bb509fba6ae041a4709e9.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/307b5d4c500bb509fba6ae041a4709e9.png)'
- en: 'Source: Image by author'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥æºï¼šä½œè€…æ’å›¾
- en: '**Did you notice a trade-off?**'
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**ä½ æ³¨æ„åˆ°æƒè¡¡äº†å—ï¼Ÿ**'
- en: As we increase the *max_depth* the performance of the model keeps getting better
    for the training set but worse for the test set.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: éšç€*max_depth*çš„å¢åŠ ï¼Œæ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šçš„è¡¨ç°è¶Šæ¥è¶Šå¥½ï¼Œä½†åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°å´è¶Šæ¥è¶Šå·®ã€‚
- en: Increasing the *max_depth* makes the model more complex and hence reduces its
    generalization capability. *This is the same as having high* ***variance.***
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¢åŠ *max_depth*ä½¿æ¨¡å‹æ›´å¤æ‚ï¼Œä»è€Œé™ä½å…¶æ³›åŒ–èƒ½åŠ›ã€‚*è¿™ä¸é«˜* ***æ–¹å·®* ç›¸åŒã€‚
- en: Reducing the *max_depth* makes the model more simple and hence it can underfit
    (this happens when the model is too weak to perform well even on the training
    set, forget about the test set). *This is the same as having high* ***bias.***
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‡å°‘*max_depth*ä½¿æ¨¡å‹æ›´ç®€å•ï¼Œå› æ­¤å¯èƒ½ä¼šå‘ç”Ÿæ¬ æ‹Ÿåˆï¼ˆè¿™å‘ç”Ÿåœ¨æ¨¡å‹å³ä½¿åœ¨è®­ç»ƒé›†ä¸Šä¹Ÿè¡¨ç°ä¸ä½³ï¼Œæ›´ä¸ç”¨è¯´æµ‹è¯•é›†äº†ï¼‰ã€‚*è¿™ä¸é«˜* ***åå·®* ç›¸åŒã€‚
- en: In the following plot, we can see the model predictions for different values
    of *max_depth* and it can help us understand that high bias leads to underfitting
    whereas high variance leads to overfitting.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹å›¾ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ä¸åŒ*max_depth*å€¼çš„æ¨¡å‹é¢„æµ‹ï¼Œè¿™æœ‰åŠ©äºæˆ‘ä»¬ç†è§£é«˜åå·®å¯¼è‡´æ¬ æ‹Ÿåˆï¼Œè€Œé«˜æ–¹å·®å¯¼è‡´è¿‡æ‹Ÿåˆã€‚
- en: '![](../Images/efb0f439058fd1b7c4aad2d47c017786.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/efb0f439058fd1b7c4aad2d47c017786.png)'
- en: 'Understanding bias-variance tradeoff (Source: Image by author)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ç†è§£åå·®-æ–¹å·®æƒè¡¡ï¼ˆæ¥æºï¼šä½œè€…æ’å›¾ï¼‰
- en: Attempting to reduce the bias increases the variance, and vice versa. We need
    to find a sweet spot where both the bias and variance are not too high but also
    not too low. **This is called the bias-variance tradeoff.**
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: å°è¯•å‡å°‘åå·®ä¼šå¢åŠ æ–¹å·®ï¼Œåä¹‹äº¦ç„¶ã€‚æˆ‘ä»¬éœ€è¦æ‰¾åˆ°ä¸€ä¸ªæœ€ä½³ç‚¹ï¼Œä½¿åå·®å’Œæ–¹å·®éƒ½ä¸å¤ªé«˜ä½†ä¹Ÿä¸å¤ªä½ã€‚**è¿™å°±æ˜¯åå·®-æ–¹å·®æƒè¡¡ã€‚**
- en: The good thing is that we donâ€™t have to do it manually. We can leverage automated
    hyperparameter tuning and cross-validation to come up with the best values of
    regularization hyperparameters that are not just limited to the *max_depth.*
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½æ¶ˆæ¯æ˜¯æˆ‘ä»¬ä¸éœ€è¦æ‰‹åŠ¨å®Œæˆè¿™ä¸€ä»»åŠ¡ã€‚æˆ‘ä»¬å¯ä»¥åˆ©ç”¨è‡ªåŠ¨è¶…å‚æ•°è°ƒæ•´å’Œäº¤å‰éªŒè¯æ¥æ‰¾åˆ°æœ€ä½³çš„æ­£åˆ™åŒ–è¶…å‚æ•°å€¼ï¼Œè¿™äº›å€¼ä¸ä»…é™äº*max_depth*ã€‚
- en: '**Is Overfitting the Only Problem?**'
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**è¿‡æ‹Ÿåˆæ˜¯å”¯ä¸€çš„é—®é¢˜å—ï¼Ÿ**'
- en: '*Short Answer:* No (but not too helpful, you still have to read the long answer,
    sorry ğŸ˜…)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '*çŸ­ç­”æ¡ˆï¼š* ä¸ï¼ˆä½†è¿™ä¸å¤ªæœ‰å¸®åŠ©ï¼Œä½ ä»ç„¶éœ€è¦é˜…è¯»é•¿ç­”æ¡ˆï¼Œå¯¹ä¸èµ·ğŸ˜…ï¼‰'
- en: '*Long Answer:* You might be wondering if overfitting can be prevented using
    regularization, then whatâ€™s the need of bagging, or other ensemble techniques.
    The thing is that in addition to overfitting, ***decision trees are also prone
    to instability.***'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*é•¿ç­”æ¡ˆï¼š* ä½ å¯èƒ½ä¼šæƒ³ï¼Œå¦‚æœé€šè¿‡æ­£åˆ™åŒ–å¯ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œé‚£ä¹ˆä¸ºä½•è¿˜éœ€è¦è£…è¢‹æˆ–å…¶ä»–é›†æˆæŠ€æœ¯ã€‚é—®é¢˜åœ¨äºï¼Œé™¤äº†è¿‡æ‹Ÿåˆä¹‹å¤–ï¼Œ***å†³ç­–æ ‘è¿˜å®¹æ˜“å‡ºç°ä¸ç¨³å®šæ€§ã€‚***'
- en: Decision trees are highly sensitive to small variations in the dataset. Even
    minor changes in the training data can lead to drastically different decision
    trees. This instability can be limited by training many trees on random subsamples
    of the data and then averaging the predictions of these trees.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: å†³ç­–æ ‘å¯¹æ•°æ®é›†ä¸­çš„å°å˜åŒ–éå¸¸æ•æ„Ÿã€‚å³ä½¿æ˜¯è®­ç»ƒæ•°æ®çš„å¾®å°å˜åŒ–ä¹Ÿä¼šå¯¼è‡´å®Œå…¨ä¸åŒçš„å†³ç­–æ ‘ã€‚è¿™ç§ä¸ç¨³å®šæ€§å¯ä»¥é€šè¿‡åœ¨æ•°æ®çš„éšæœºå­æ ·æœ¬ä¸Šè®­ç»ƒå¤šä¸ªæ ‘ï¼Œç„¶åå¯¹è¿™äº›æ ‘çš„é¢„æµ‹ç»“æœè¿›è¡Œå¹³å‡æ¥é™åˆ¶ã€‚
- en: '**The Idea of Ensemble Learning**'
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**é›†æˆå­¦ä¹ çš„æ€æƒ³**'
- en: An *ensemble* is a group of models and the technique of aggregating the predictions
    of these models is known as ***ensemble learning.***
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*é›†æˆ* æ˜¯ä¸€ç»„æ¨¡å‹ï¼Œè€Œèšåˆè¿™äº›æ¨¡å‹é¢„æµ‹çš„æŠ€æœ¯è¢«ç§°ä¸º***é›†æˆå­¦ä¹ ã€‚***'
- en: 'There are two approaches to ensemble learning:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: é›†æˆå­¦ä¹ æœ‰ä¸¤ç§æ–¹æ³•ï¼š
- en: Use different training algorithms such as decision trees, SVM, etc. for each
    predictor and train them on the given training set.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹æ¯ä¸ªé¢„æµ‹å™¨ä½¿ç”¨ä¸åŒçš„è®­ç»ƒç®—æ³•ï¼Œå¦‚å†³ç­–æ ‘ã€SVM ç­‰ï¼Œå¹¶åœ¨ç»™å®šçš„è®­ç»ƒé›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚
- en: Use the same training algorithm for every predictor and train them on different
    subsets of the training set. *Bagging falls in this category.*
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹æ¯ä¸ªé¢„æµ‹å™¨ä½¿ç”¨ç›¸åŒçš„è®­ç»ƒç®—æ³•ï¼Œå¹¶åœ¨è®­ç»ƒé›†çš„ä¸åŒå­é›†ä¸Šè¿›è¡Œè®­ç»ƒã€‚*è£…è¢‹å±äºè¿™ä¸€ç±»åˆ«ã€‚*
- en: '**Introduction to Bagging**'
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**è£…è¢‹ç®€ä»‹**'
- en: Bagging is a short word for ***bootstrap aggregation****.*
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: è£…è¢‹æ˜¯***bootstrap aggregation***çš„ç®€ç§°ã€‚
- en: Bagging is an ensemble method in which multiple models are trained on different
    random subsamples of the training set, and the sampling is performed with replacement.
  id: totrans-95
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: è£…è¢‹æ˜¯ä¸€ç§é›†æˆæ–¹æ³•ï¼Œå…¶ä¸­å¤šä¸ªæ¨¡å‹åœ¨è®­ç»ƒé›†çš„ä¸åŒéšæœºå­æ ·æœ¬ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¸”æŠ½æ ·æ˜¯æœ‰æ”¾å›çš„ã€‚
- en: Sampling with replacement means that some instances can be sampled several times
    for any given predictor, while others may not be sampled at all. This ensures
    that sensitivity to minor variations in the training data gets accounted for and
    no longer harms the stability of the final ensemble.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: å¸¦æ›¿æ¢é‡‡æ ·æ„å‘³ç€æŸäº›å®ä¾‹å¯èƒ½ä¼šè¢«å¤šä¸ªé¢„æµ‹å™¨å¤šæ¬¡é‡‡æ ·ï¼Œè€Œå…¶ä»–å®ä¾‹å¯èƒ½æ ¹æœ¬ä¸ä¼šè¢«é‡‡æ ·ã€‚è¿™ç¡®ä¿äº†å¯¹è®­ç»ƒæ•°æ®ä¸­å¾®å°å˜åŒ–çš„æ•æ„Ÿæ€§å¾—åˆ°è€ƒè™‘ï¼Œå¹¶ä¸”ä¸å†å½±å“æœ€ç»ˆé›†æˆçš„ç¨³å®šæ€§ã€‚
- en: '![](../Images/a8efccce1215892f87eb106f225e6a04.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8efccce1215892f87eb106f225e6a04.png)'
- en: Illustration of Bagging (Image by author)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: è¢‹è£…ç¤ºæ„å›¾ (ä½œè€…æä¾›çš„å›¾ç‰‡)
- en: '***Note:*** *We can either subsample the training set with replacement or without
    replacement.* ***When the sampling is done with replacement, it is known as bagging.
    When the sampling is done without replacement, it is known as pasting.***'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '***æ³¨æ„:*** *æˆ‘ä»¬å¯ä»¥é€‰æ‹©å¸¦æ›¿æ¢æˆ–ä¸å¸¦æ›¿æ¢åœ°å¯¹è®­ç»ƒé›†è¿›è¡Œå­é‡‡æ ·ã€‚* ***å½“é‡‡æ ·æ˜¯å¸¦æ›¿æ¢çš„ï¼Œè¿™è¢«ç§°ä¸ºè¢‹è£…ã€‚å½“é‡‡æ ·æ˜¯ä¸å¸¦æ›¿æ¢çš„ï¼Œè¿™è¢«ç§°ä¸ºè¿‡å»æ³•ã€‚***'
- en: 'Once all the models are trained on random subsamples of the training data,
    their predictions can be aggregated as:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æ‰€æœ‰æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®çš„éšæœºå­æ ·æœ¬ä¸Šè®­ç»ƒå®Œæˆï¼Œå®ƒä»¬çš„é¢„æµ‹å¯ä»¥è¢«èšåˆä¸ºï¼š
- en: averaging the predictions for regression
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å›å½’é¢„æµ‹çš„å¹³å‡
- en: majority voting for classification
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ†ç±»çš„å¤šæ•°æŠ•ç¥¨
- en: Now that we have an idea of ensemble learning and bagging, letâ€™s implement it
    in scikit-learn. Letâ€™s continue the following steps in our notebook.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯¹é›†æˆå­¦ä¹ å’Œè¢‹è£…æœ‰äº†ä¸€å®šäº†è§£ï¼Œè®©æˆ‘ä»¬åœ¨ scikit-learn ä¸­å®ç°å®ƒã€‚æ¥ä¸‹æ¥è®©æˆ‘ä»¬åœ¨ç¬”è®°æœ¬ä¸­ç»§ç»­æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ã€‚
- en: '**Step 5: Implement bagging in scikit-learn**'
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 5: åœ¨ scikit-learn ä¸­å®ç°è¢‹è£…**'
- en: We can simply pass our decision tree regressor inside a bagging regressor and
    specify the number of models we want to train (*n_estimators),* and thenumber
    of samples to consider for training each model (*max_samples).*
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ç®€å•åœ°å°†å†³ç­–æ ‘å›å½’å™¨ä¼ é€’ç»™è¢‹è£…å›å½’å™¨ï¼Œå¹¶æŒ‡å®šæˆ‘ä»¬æƒ³è¦è®­ç»ƒçš„æ¨¡å‹æ•°é‡ (*n_estimators*)ï¼Œä»¥åŠæ¯ä¸ªæ¨¡å‹è®­ç»ƒæ—¶è€ƒè™‘çš„æ ·æœ¬æ•°é‡ (*max_samples*)ã€‚
- en: Here, `bootstrap=True` means that the data will be sampled with replacement
    and if we want to use pasting instead of bagging, we can set `bootstrap=False`.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œ`bootstrap=True` æ„å‘³ç€æ•°æ®å°†è¿›è¡Œå¸¦æ›¿æ¢çš„é‡‡æ ·ï¼Œå¦‚æœæˆ‘ä»¬æƒ³ä½¿ç”¨è¿‡å»æ³•è€Œä¸æ˜¯è¢‹è£…ï¼Œå¯ä»¥å°† `bootstrap=False` è®¾ç½®ä¸º
    Falseã€‚
- en: '[PRE5]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`Cell Output:`'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`å•å…ƒè¾“å‡º:`'
- en: '![](../Images/750e3796c72cc86d955b7d1b67cac124.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/750e3796c72cc86d955b7d1b67cac124.png)'
- en: It means we have trained 200 decision trees separately such that each decision
    tree has used a random subsample of size 100 as the training set. The end prediction
    will be the average of all predictions.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€æˆ‘ä»¬åˆ†åˆ«è®­ç»ƒäº† 200 æ£µå†³ç­–æ ‘ï¼Œæ¯æ£µå†³ç­–æ ‘éƒ½ä½¿ç”¨äº†å¤§å°ä¸º 100 çš„éšæœºå­æ ·æœ¬ä½œä¸ºè®­ç»ƒé›†ã€‚æœ€ç»ˆé¢„æµ‹å°†æ˜¯æ‰€æœ‰é¢„æµ‹çš„å¹³å‡å€¼ã€‚
- en: '**Step 6: Evaluate the bagged version of the decision tree regressor**'
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**æ­¥éª¤ 6: è¯„ä¼°å†³ç­–æ ‘å›å½’å™¨çš„è¢‹è£…ç‰ˆæœ¬**'
- en: We will use mean squared error again to evaluate how well the model predicts
    the samples in training as well as the test set.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å†æ¬¡ä½¿ç”¨å‡æ–¹è¯¯å·®æ¥è¯„ä¼°æ¨¡å‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šçš„æ ·æœ¬é¢„æµ‹æ•ˆæœã€‚
- en: '[PRE6]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`Cell Output:`'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`å•å…ƒè¾“å‡º:`'
- en: '![](../Images/d59594848411f96d374f5a0a10075b58.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d59594848411f96d374f5a0a10075b58.png)'
- en: After using bagging, the training MSE has gone up from 0 to 69.438 but the test
    MSE has gone down from 173.336 to 101.521 which is indeed an improvement!
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¢‹è£…åï¼Œè®­ç»ƒçš„å‡æ–¹è¯¯å·®ä» 0 ä¸Šå‡åˆ° 69.438ï¼Œä½†æµ‹è¯•çš„å‡æ–¹è¯¯å·®ä» 173.336 ä¸‹é™åˆ° 101.521ï¼Œè¿™ç¡®å®æ˜¯ä¸€ä¸ªæ”¹è¿›ï¼
- en: We can verify from the below plot that the final predictions after the bagged
    ensemble of decision trees have a lot better generalization capability than the
    previous one.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä»ä¸‹é¢çš„å›¾ä¸­éªŒè¯ï¼Œç»è¿‡è¢‹è£…é›†æˆçš„å†³ç­–æ ‘çš„æœ€ç»ˆé¢„æµ‹å…·æœ‰æ¯”ä¹‹å‰æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚
- en: '![](../Images/d64363495f57cadc97ac5144dde52b7c.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d64363495f57cadc97ac5144dde52b7c.png)'
- en: 'Source: Image by author'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ¥æº: ä½œè€…æä¾›çš„å›¾ç‰‡'
- en: 'The following plot shows the bagging regressorâ€™s predictions for the given
    training and test data:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹å›¾æ˜¾ç¤ºäº†è¢‹è£…å›å½’å™¨å¯¹ç»™å®šè®­ç»ƒå’Œæµ‹è¯•æ•°æ®çš„é¢„æµ‹ï¼š
- en: '![](../Images/3e4ed0b338726ca7465bc44100249ec6.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e4ed0b338726ca7465bc44100249ec6.png)'
- en: 'Source: Image by author'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 'æ¥æº: ä½œè€…æä¾›çš„å›¾ç‰‡'
- en: The final predictions from the ensemble are smoother than what a single decision
    tree would have produced and the model shows a similar fit for both the training
    and test sets.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥è‡ªé›†æˆçš„æœ€ç»ˆé¢„æµ‹æ¯”å•æ£µå†³ç­–æ ‘çš„é¢„æµ‹æ›´å¹³æ»‘ï¼Œæ¨¡å‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šçš„æ‹Ÿåˆæƒ…å†µç›¸ä¼¼ã€‚
- en: Link to full notebook
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®Œæ•´ç¬”è®°æœ¬é“¾æ¥
- en: You can find the notebook [here](https://github.com/gurjinderbassi/Machine-Learning/blob/main/Overfitting%20and%20Bagging%20in%20Decision%20Trees.ipynb).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨[è¿™é‡Œ](https://github.com/gurjinderbassi/Machine-Learning/blob/main/Overfitting%20and%20Bagging%20in%20Decision%20Trees.ipynb)æ‰¾åˆ°ç¬”è®°æœ¬ã€‚
- en: 'Bonus: Random Forests'
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'å¥–åŠ±: éšæœºæ£®æ—'
- en: At the beginning of this article, I specified that random forests use the idea
    of bagging plus *something else.*I donâ€™t want you to keep pondering upon what
    this *something else* is, and since youâ€™ve almost got to the end of this article,
    this bonus section is your reward ğŸ˜¸
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡å¼€å¤´ï¼Œæˆ‘æåˆ°éšæœºæ£®æ—ä½¿ç”¨äº†è¢‹è£…åŠ ä¸Š*å…¶ä»–å†…å®¹*çš„ç†å¿µã€‚æˆ‘ä¸æƒ³è®©ä½ ç»§ç»­ç¢ç£¨è¿™ä¸ª*å…¶ä»–å†…å®¹*æ˜¯ä»€ä¹ˆï¼Œæ—¢ç„¶ä½ å·²ç»å¿«åˆ°è¾¾æ–‡ç« çš„ç»“å°¾äº†ï¼Œè¿™ä¸€é¢å¤–éƒ¨åˆ†å°±æ˜¯ä½ çš„å¥–åŠ±
    ğŸ˜¸
- en: A random forest is an ensemble of decision trees that are trained via the bagging
    method.
  id: totrans-128
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: éšæœºæ£®æ—æ˜¯é€šè¿‡è¢‹è£…æ–¹æ³•è®­ç»ƒçš„å†³ç­–æ ‘é›†æˆã€‚
- en: Shedding light on *something else:* The random forest algorithm introduces extra
    randomness while growing the trees. While splitting a node, instead of searching
    the entire feature space, it searches for the best feature among a random subset
    of features. This further enhances the diversity of the models and reduces variance,
    giving rise to an overall better ensemble.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: æ­ç¤º*å…¶ä»–å†…å®¹ï¼š* éšæœºæ£®æ—ç®—æ³•åœ¨ç”Ÿé•¿æ ‘æœ¨æ—¶å¼•å…¥äº†é¢å¤–çš„éšæœºæ€§ã€‚åœ¨åˆ†è£‚ä¸€ä¸ªèŠ‚ç‚¹æ—¶ï¼Œå®ƒä¸æ˜¯æœç´¢æ•´ä¸ªç‰¹å¾ç©ºé—´ï¼Œè€Œæ˜¯åœ¨éšæœºç‰¹å¾å­é›†ä¸­å¯»æ‰¾æœ€ä½³ç‰¹å¾ã€‚è¿™è¿›ä¸€æ­¥å¢å¼ºäº†æ¨¡å‹çš„å¤šæ ·æ€§ï¼Œå¹¶å‡å°‘äº†æ–¹å·®ï¼Œä»è€Œäº§ç”Ÿäº†æ›´å¥½çš„é›†æˆæ¨¡å‹ã€‚
- en: Random forests can also be implemented using scikit-learn for both the regression
    and classification tasks. It has all the hyperparameters of a *DecisionTreeRegressor
    (or DecisionTreeClassifier)* to control how individual trees are grown, plus all
    the hyperparameters of a *BaggingRegressor (or BaggingClassifier),* with some
    exceptions. The other set of hyperparameters is also there to control the sampling
    of features to consider at each node.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: éšæœºæ£®æ—ä¹Ÿå¯ä»¥ä½¿ç”¨ scikit-learn å®ç°ï¼Œé€‚ç”¨äºå›å½’å’Œåˆ†ç±»ä»»åŠ¡ã€‚å®ƒå…·æœ‰æ§åˆ¶å•æ£µæ ‘ç”Ÿé•¿çš„æ‰€æœ‰è¶…å‚æ•°ï¼Œå¦‚*DecisionTreeRegressorï¼ˆæˆ–
    DecisionTreeClassifierï¼‰*ï¼Œä»¥åŠ*BaggingRegressorï¼ˆæˆ– BaggingClassifierï¼‰* çš„æ‰€æœ‰è¶…å‚æ•°ï¼Œä½†æœ‰ä¸€äº›ä¾‹å¤–ã€‚å¦ä¸€ä¸ªè¶…å‚æ•°é›†è¿˜ç”¨äºæ§åˆ¶æ¯ä¸ªèŠ‚ç‚¹è€ƒè™‘çš„ç‰¹å¾é‡‡æ ·ã€‚
- en: Conclusion
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: In this article, we discussed the issues of overfitting and instability in decision
    trees and how we can use ensemble methods such as bagging to overcome them.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†å†³ç­–æ ‘ä¸­çš„è¿‡æ‹Ÿåˆå’Œä¸ç¨³å®šæ€§é—®é¢˜ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨è¯¸å¦‚è¢‹è£…ï¼ˆbaggingï¼‰ç­‰é›†æˆæ–¹æ³•æ¥å…‹æœè¿™äº›é—®é¢˜ã€‚
- en: Decision trees are powerful machine learning algorithms that can solve both
    regression and classification problems, however, they suffer from overfitting
    and instability.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å†³ç­–æ ‘æ˜¯å¼ºå¤§çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œèƒ½å¤Ÿè§£å†³å›å½’å’Œåˆ†ç±»é—®é¢˜ï¼Œä½†å®ƒä»¬å®¹æ˜“è¿‡æ‹Ÿåˆä¸”ä¸ç¨³å®šã€‚
- en: Overfitting occurs when a model fits the training data so perfectly that it
    fails to generalize well and learn the underlying behavior of the data.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿‡æ‹Ÿåˆå‘ç”Ÿåœ¨æ¨¡å‹è¿‡äºå®Œç¾åœ°æ‹Ÿåˆè®­ç»ƒæ•°æ®ï¼Œä»¥è‡³äºæ— æ³•å¾ˆå¥½åœ°æ¦‚æ‹¬å’Œå­¦ä¹ æ•°æ®çš„æ½œåœ¨è¡Œä¸ºã€‚
- en: Regularization can be used to reduce the chance of overfitting by limiting the
    growth of the decision tree.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ­£åˆ™åŒ–å¯ä»¥é€šè¿‡é™åˆ¶å†³ç­–æ ‘çš„å¢é•¿æ¥å‡å°‘è¿‡æ‹Ÿåˆçš„å¯èƒ½æ€§ã€‚
- en: Another problem with decision trees is that they are highly sensitive to small
    variations in the data that make them unstable. This can be overcome by using
    ensemble techniques.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å†³ç­–æ ‘çš„å¦ä¸€ä¸ªé—®é¢˜æ˜¯å¯¹æ•°æ®çš„å°å˜åŒ–éå¸¸æ•æ„Ÿï¼Œä½¿å…¶å˜å¾—ä¸ç¨³å®šã€‚è¿™å¯ä»¥é€šè¿‡ä½¿ç”¨é›†æˆæŠ€æœ¯æ¥å…‹æœã€‚
- en: Ensemble learning consists of training multiple predictors on random subsets
    of the training data and then aggregating their predictions. Bagging is one such
    technique that samples the training data with replacement.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é›†æˆå­¦ä¹ åŒ…æ‹¬åœ¨è®­ç»ƒæ•°æ®çš„éšæœºå­é›†ä¸Šè®­ç»ƒå¤šä¸ªé¢„æµ‹å™¨ï¼Œç„¶åèšåˆå®ƒä»¬çš„é¢„æµ‹ã€‚è¢‹è£…æ˜¯ä¸€ç§ä½¿ç”¨æ›¿æ¢çš„è®­ç»ƒæ•°æ®é‡‡æ ·æŠ€æœ¯ã€‚
- en: Random Forests improve upon the decision trees by incorporating bagging and
    random feature selection at each node to reduce the overall variance.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: éšæœºæ£®æ—é€šè¿‡åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šç»“åˆè¢‹è£…å’Œéšæœºç‰¹å¾é€‰æ‹©æ¥æ”¹è¿›å†³ç­–æ ‘ï¼Œä»è€Œå‡å°‘æ€»ä½“æ–¹å·®ã€‚
- en: Thank you for reading, I hope it was helpful!
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢é˜…è¯»ï¼Œå¸Œæœ›å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼
- en: Open to any feedback or suggestions.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: æ¬¢è¿ä»»ä½•åé¦ˆæˆ–å»ºè®®ã€‚
- en: 'References:'
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®ï¼š
- en: '[1] [https://www.ibm.com/topics/overfitting](https://www.ibm.com/topics/overfitting)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [https://www.ibm.com/topics/overfitting](https://www.ibm.com/topics/overfitting)'
- en: '[2] Hands-on machine learning with Scikit-Learn, Keras and TensorFlow: concepts,
    tools, and techniques to build intelligent systems (2nd ed.). Oâ€™Reilly. AurÃ©lien
    GÃ©ron, 2019.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] ã€ŠåŠ¨æ‰‹æœºå™¨å­¦ä¹ ï¼šä½¿ç”¨ Scikit-Learnã€Keras å’Œ TensorFlow æ„å»ºæ™ºèƒ½ç³»ç»Ÿï¼ˆç¬¬2ç‰ˆï¼‰ã€‹ O''Reillyã€‚å¥¥é›·åˆ©å®‰Â·çƒ­ç½—ï¼Œ2019å¹´ã€‚'
- en: '[3] The Hundred-Page Machine Learning Book, Andriy Burkov, 2019.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] ã€Šç™¾é¡µæœºå™¨å­¦ä¹ ä¹¦ã€‹ï¼Œå®‰å¾·çƒˆÂ·å¸ƒå°”ç§‘å¤«ï¼Œ2019å¹´ã€‚'
