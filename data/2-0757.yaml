- en: Does Bagging Help to Prevent Overfitting in Decision Trees?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Bagging是否有助于防止决策树的过拟合？
- en: 原文：[https://towardsdatascience.com/does-bagging-help-to-prevent-overfitting-in-decision-trees-42262943a81e](https://towardsdatascience.com/does-bagging-help-to-prevent-overfitting-in-decision-trees-42262943a81e)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/does-bagging-help-to-prevent-overfitting-in-decision-trees-42262943a81e](https://towardsdatascience.com/does-bagging-help-to-prevent-overfitting-in-decision-trees-42262943a81e)
- en: Understand why decision trees are highly prone to overfitting and its potential
    remedies
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解为什么决策树高度容易过拟合及其潜在的补救措施
- en: '[](https://medium.com/@gurjinderkaur95?source=post_page-----42262943a81e--------------------------------)[![Gurjinder
    Kaur](../Images/d5c6746466025dad06077b1a89a789d1.png)](https://medium.com/@gurjinderkaur95?source=post_page-----42262943a81e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----42262943a81e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----42262943a81e--------------------------------)
    [Gurjinder Kaur](https://medium.com/@gurjinderkaur95?source=post_page-----42262943a81e--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@gurjinderkaur95?source=post_page-----42262943a81e--------------------------------)[![Gurjinder
    Kaur](../Images/d5c6746466025dad06077b1a89a789d1.png)](https://medium.com/@gurjinderkaur95?source=post_page-----42262943a81e--------------------------------)[](https://towardsdatascience.com/?source=post_page-----42262943a81e--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----42262943a81e--------------------------------)
    [Gurjinder Kaur](https://medium.com/@gurjinderkaur95?source=post_page-----42262943a81e--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----42262943a81e--------------------------------)
    ·12 min read·Dec 13, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于[Towards Data Science](https://towardsdatascience.com/?source=post_page-----42262943a81e--------------------------------)
    ·12分钟阅读·2023年12月13日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/49dc05d1ffe1c84260864fe2ab15c16c.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/49dc05d1ffe1c84260864fe2ab15c16c.png)'
- en: Photo by [Jan Huber](https://unsplash.com/@jan_huber?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由[Jan Huber](https://unsplash.com/@jan_huber?utm_source=medium&utm_medium=referral)提供，来源于[Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Decision trees are a class of machine learning algorithms well known for their
    ability to solve both classification and regression problems, and not to forget
    the ease of interpretation they offer. However, they suffer from overfitting and
    can fail to generalize well if not controlled properly.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是一类机器学习算法，以其解决分类和回归问题的能力而闻名，同时它们提供了易于解释的优点。然而，如果没有得到适当控制，它们容易出现过拟合问题，可能无法很好地泛化。
- en: In this article, we will discuss what is overfitting, to what extent a decision
    tree overfits the training data, why it is an issue, and how it can be addressed.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将讨论什么是过拟合，决策树在多大程度上会对训练数据过拟合，为什么这是一个问题，以及如何解决它。
- en: Then, we will get ourselves acquainted with one of the ensemble techniques i.e.,
    *bagging*, and see if it can be used to make decision trees more robust.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将了解一种集成技术，即*bagging*，并看看它是否可以用来使决策树更强健。
- en: 'We will cover the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下内容：
- en: Create our regression dataset using NumPy.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用NumPy创建我们的回归数据集。
- en: Train a decision tree model using scikit-learn.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用scikit-learn训练一个决策树模型。
- en: Understand what overfitting means by looking at the performance of the same
    model on the training set and test set.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过查看同一模型在训练集和测试集上的表现，理解过拟合的含义。
- en: Discuss why overfitting is more common in non-parametric models such as decision
    trees (and of course learn what is meant by the term non-parametric) and how it
    can be prevented using regularization.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论为什么在非参数模型（如决策树）中过拟合更为常见（当然也了解一下非参数的含义），以及如何通过正则化来防止过拟合。
- en: Understand what *bootstrap aggregation* (*bagging* in short) is and how it can
    potentially help with overfitting.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解什么是*自助聚合*（简称*bagging*），以及它如何有可能帮助解决过拟合问题。
- en: Finally, we will implement the bagging version of the decision tree and see
    if it helps or not 🤞
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们将实现决策树的bagging版本，看看它是否有帮助🤞
- en: '**Still wondering if it’s worth reading?** 🤔 If you’ve ever wondered why Random
    Forests are usually preferred over vanilla Decision Trees, this is the best place
    to start since Random Forests use the idea of *bagging plus something else* to
    improve upon decision trees.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**还在犹豫是否值得阅读？** 🤔 如果你曾经想知道为什么随机森林通常比普通的决策树更受欢迎，这是最好的起点，因为随机森林使用了*bagging加上一些其他方法*来改进决策树。'
- en: Let’s get started!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: We will set up a Python notebook and import the libraries first.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将设置一个 Python 笔记本并导入相关库。
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Step 1: Creating the dataset**'
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**步骤 1：创建数据集**'
- en: 'We are going to use a dataset resembling a quadratic function with *y* as the
    target variable and *X* as the independent variable. Since *y* is numeric, we
    will be fitting a regression tree on this dataset. Let’s build our dataset as
    follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个类似于二次函数的数据集，其中 *y* 为目标变量，*X* 为自变量。由于 *y* 是数值型的，我们将对这个数据集拟合一个回归树。让我们按如下方式构建数据集：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We have created a dataset with 500 samples where both *X* and *y* are continuous
    as shown below. The link to the full notebook along with visualizations can be
    found at the end of this article, so don’t worry about the missing viz code in
    this article.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个包含 500 个样本的数据集，其中 *X* 和 *y* 都是连续的，如下所示。完整的笔记本及其可视化的链接可以在本文末尾找到，所以不用担心本文中缺失的可视化代码。
- en: '![](../Images/0437751797643f16d797f0f7643e3809.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0437751797643f16d797f0f7643e3809.png)'
- en: 'Source: Image by author'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者提供的图片
- en: '**Step 2: Train-test split**'
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**步骤 2：训练-测试拆分**'
- en: 'We can use scikit-learn’s *train_test_split* to split our dataset into a training
    set and test set as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 scikit-learn 的 *train_test_split* 将数据集拆分为训练集和测试集，如下所示：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`Cell Output:`'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`单元输出：`'
- en: '![](../Images/fd12191c5e8907a2adb39bb4d9e74aba.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd12191c5e8907a2adb39bb4d9e74aba.png)'
- en: We will only use the training set for training our model and keep the test set
    aside to use it just for testing the model’s performance. This will ensure we
    are testing our model against samples it has never seen before and will help us
    evaluate how well it generalizes. How smart, right? 😎
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将仅使用训练集来训练我们的模型，并将测试集保留用于测试模型的性能。这将确保我们测试模型的样本是模型从未见过的，从而帮助我们评估模型的泛化能力。聪明吧？
    😎
- en: 'Okay, the following is what our training and test sets look like:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，接下来是我们的训练集和测试集的样子：
- en: '![](../Images/f8bd44f240b3660c4fc379aa69a2b617.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f8bd44f240b3660c4fc379aa69a2b617.png)'
- en: 'Source: Image by author'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者提供的图片
- en: '**Step 3: Fitting the regression tree on the training set**'
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**步骤 3：在训练集上拟合回归树**'
- en: Fitting a decision tree regressor using scikit-learn is just two lines of code.
    However, if you’re not sure what’s happening under the hood and are a curious
    learner, [this article](https://medium.com/p/fbb908cf548b) would be your go-to
    guide for understanding how exactly a decision tree solves a regression problem.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 拟合决策树回归器只需两行代码。然而，如果你不确定背后发生了什么，并且是一个好奇的学习者，[这篇文章](https://medium.com/p/fbb908cf548b)
    将是你了解决策树如何解决回归问题的必备指南。
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`Cell Output:`'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`单元输出：`'
- en: '![](../Images/8f2679ca1bf0088971339172164d23c4.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8f2679ca1bf0088971339172164d23c4.png)'
- en: '**Step 4: Evaluating the regression tree on training and test sets**'
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**步骤 4：在训练集和测试集上评估回归树**'
- en: 'Now, since our model has been trained, let’s use it to make predictions on:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，既然我们的模型已经训练好了，让我们用它来进行预测：
- en: '**the training set** i.e., the data it already is friends with.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练集** 即模型已经“熟悉”的数据。'
- en: '**the test set** i.e., the data it has never seen before (***the real test
    lies here***).'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试集** 即模型从未见过的数据（***真正的考验就在这里***）。'
- en: We are going to use mean squared error to evaluate the quality of our predictions.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用均方误差来评估我们预测的质量。
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`Cell Output:`'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`单元输出：`'
- en: '![](../Images/c2ad8618288b2a377c418f63a9ed893b.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c2ad8618288b2a377c418f63a9ed893b.png)'
- en: The decision tree regressor manages to get a ZERO ERROR on the training set.
    We are almost going to crown this model as the greatest of all time until we see
    the results on the test set, and there we stop 🥶
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树回归器在训练集上的误差为零。我们几乎要将这个模型冠以“史上最伟大”的称号，直到我们看到测试集上的结果，那时我们才停下 🥶
- en: The same model gets a whopping 173.336 mean squared error on the test set. Looks
    like it failed miserably on the real test 😔
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的模型在测试集上的均方误差达到了惊人的 173.336。看来它在真实测试中表现糟糕 😔
- en: This is because the model overfitted (over-learned, over-relied, over-crammed,
    over-memorized) the training data points so perfectly that it failed to learn
    the underlying patterns within the data; rather it caught up on the noise of the
    training set that was specific just to the training set and had nothing to do
    with the overall behavior of the data. This is called ***Overfitting.***
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为模型过度拟合（过度学习、过度依赖、过度填充、过度记忆）了训练数据点，导致它未能学习数据中的潜在模式；相反，它抓住了仅对训练集特有的噪声，而与数据的整体行为无关。这被称为
    ***过拟合。***
- en: '**Overfitting** is the property of a model such that the model predicts very
    well the labels of the examples used during training but frequently makes errors
    when applied to examples that weren’t seen by the learning algorithm during training
    — Andriy Burkov (The Hundred Page Machine Learning Book)'
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**过拟合**是指模型在训练期间对示例标签的预测非常准确，但在应用于未见过的示例时经常出现错误的特性——安德里·布尔科夫（《百页机器学习书》）'
- en: We can see in the following plots how the predicted values are exactly overlapping
    with the actual values for the training set, but not for the test set.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图表中，我们可以看到训练集的预测值与实际值完全重叠，但测试集却不是。
- en: '![](../Images/41b4140e94e371aa57a713bdc3d10df4.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41b4140e94e371aa57a713bdc3d10df4.png)'
- en: 'Source: Image by author'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者提供的图片
- en: Following is what the predictions will look like for the given training and
    test data. It can be clearly seen that the model is trying to fit the training
    data very closely. `Depth=None` indicates that unless specified, there is no restriction
    on the maximum depth a tree can reach. It is the default value of the *max_depth*
    hyperparameter.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是给定训练数据和测试数据的预测结果。可以清楚地看到，模型试图非常紧密地拟合训练数据。 `Depth=None` 表示除非特别指定，否则树可以达到的最大深度没有限制。这是
    *max_depth* 超参数的默认值。
- en: '![](../Images/53e9d9b07733811a938afe0e59d22531.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/53e9d9b07733811a938afe0e59d22531.png)'
- en: 'Source: Image by author'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者提供的图片
- en: '***Just Thinking:*** *The term Tightfitting instead of Overfitting makes a
    much better literal sense because that’s what our model is doing here* 🤷🏻‍♀️ Anyways,
    let’s not break the rules and stick with overfitting.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '***仅仅思考：*** *“过拟合”一词比“紧拟合”更能准确表达模型的实际行为* 🤷🏻‍♀️ 不过，还是不要打破规则，坚持使用过拟合。'
- en: '**Why does overfitting come naturally to decision trees?**'
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**为什么过拟合在决策树中自然发生？**'
- en: Decision trees are *non-parametric* i.e., they don’t make any assumptions about
    the training data. If left unconstrained, the tree structure will completely adapt
    itself to the training data, fitting it very closely, most likely overfitting
    it.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是 *非参数* 的，即它们不对训练数据做出任何假设。如果没有限制，树的结构将完全适应训练数据，紧密拟合，很可能会导致过拟合。
- en: It’s termed as *non-parametric* not because it doesn’t have any parameters but
    because the number of parameters is not determined before training and hence the
    model structure is free to stick closely to the training data (*as opposed to
    linear regression, where we have a fixed number of coefficients i.e., parameters
    that we want the model to learn, so its degree of freedom is limited*)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树被称为 *非参数* 不是因为它没有参数，而是因为参数的数量在训练之前没有确定，因此模型结构可以自由地紧贴训练数据（*与线性回归不同，线性回归有固定数量的系数，即我们希望模型学习的参数，所以其自由度是有限的*）
- en: '**Why is overfitting an issue?**'
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**为什么过拟合是一个问题？**'
- en: Overfitting is undesirable because it doesn’t allow the model to generalize
    well on the new data, and if that happens, it will not be able to perform well
    on the classification or prediction tasks it was originally intended for.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合是不受欢迎的，因为它不允许模型在新数据上进行良好的泛化，如果发生这种情况，模型将无法在其最初设计的分类或预测任务中表现良好。
- en: '**What we could’ve done differently**'
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**我们本可以做得不同**'
- en: In case our model shows signs of overfitting, we can infer that the model is
    overly complex and needs to be ***regularized*.**
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的模型出现过拟合的迹象，我们可以推断模型过于复杂，需要***正则化*。**
- en: Regularization is the process of restricting a model’s freedom by enforcing
    some constraints on it so that the chance of overfitting is reduced.
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 正则化是通过对模型施加一些约束来限制其自由度的过程，从而减少过拟合的机会。
- en: Several hyperparameters such as maximum depth, minimum number of samples in
    a leaf node, etc. can be tuned to regularize the decision tree.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 可以调整几个超参数，例如最大深度、叶子节点中的最小样本数等，以对决策树进行正则化。
- en: The least we could do to prevent a situation like above is to set the *max_depth*
    to stop the tree from over-growing. The default value of *max_depth* is set to
    *None* which means there is no limit on the growth of the decision tree. Reducing
    *max_depth* will regularize the model and thus reduce the risk of overfitting.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以做的最少措施是将 *max_depth* 设置为限制树的过度生长。默认情况下，*max_depth* 的值设置为 *None*，意味着对决策树的生长没有限制。减少
    *max_depth* 会对模型进行正则化，从而降低过拟合的风险。
- en: Following are the predicted vs actual plots for the training and test sets for
    different values of *max_depth.*
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是不同*max_depth*值的训练集和测试集的预测与实际结果图。
- en: '![](../Images/307b5d4c500bb509fba6ae041a4709e9.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/307b5d4c500bb509fba6ae041a4709e9.png)'
- en: 'Source: Image by author'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：作者插图
- en: '**Did you notice a trade-off?**'
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**你注意到权衡了吗？**'
- en: As we increase the *max_depth* the performance of the model keeps getting better
    for the training set but worse for the test set.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着*max_depth*的增加，模型在训练集上的表现越来越好，但在测试集上的表现却越来越差。
- en: Increasing the *max_depth* makes the model more complex and hence reduces its
    generalization capability. *This is the same as having high* ***variance.***
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加*max_depth*使模型更复杂，从而降低其泛化能力。*这与高* ***方差* 相同。
- en: Reducing the *max_depth* makes the model more simple and hence it can underfit
    (this happens when the model is too weak to perform well even on the training
    set, forget about the test set). *This is the same as having high* ***bias.***
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少*max_depth*使模型更简单，因此可能会发生欠拟合（这发生在模型即使在训练集上也表现不佳，更不用说测试集了）。*这与高* ***偏差* 相同。
- en: In the following plot, we can see the model predictions for different values
    of *max_depth* and it can help us understand that high bias leads to underfitting
    whereas high variance leads to overfitting.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们可以看到不同*max_depth*值的模型预测，这有助于我们理解高偏差导致欠拟合，而高方差导致过拟合。
- en: '![](../Images/efb0f439058fd1b7c4aad2d47c017786.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/efb0f439058fd1b7c4aad2d47c017786.png)'
- en: 'Understanding bias-variance tradeoff (Source: Image by author)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 理解偏差-方差权衡（来源：作者插图）
- en: Attempting to reduce the bias increases the variance, and vice versa. We need
    to find a sweet spot where both the bias and variance are not too high but also
    not too low. **This is called the bias-variance tradeoff.**
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试减少偏差会增加方差，反之亦然。我们需要找到一个最佳点，使偏差和方差都不太高但也不太低。**这就是偏差-方差权衡。**
- en: The good thing is that we don’t have to do it manually. We can leverage automated
    hyperparameter tuning and cross-validation to come up with the best values of
    regularization hyperparameters that are not just limited to the *max_depth.*
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是我们不需要手动完成这一任务。我们可以利用自动超参数调整和交叉验证来找到最佳的正则化超参数值，这些值不仅限于*max_depth*。
- en: '**Is Overfitting the Only Problem?**'
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**过拟合是唯一的问题吗？**'
- en: '*Short Answer:* No (but not too helpful, you still have to read the long answer,
    sorry 😅)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '*短答案：* 不（但这不太有帮助，你仍然需要阅读长答案，对不起😅）'
- en: '*Long Answer:* You might be wondering if overfitting can be prevented using
    regularization, then what’s the need of bagging, or other ensemble techniques.
    The thing is that in addition to overfitting, ***decision trees are also prone
    to instability.***'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*长答案：* 你可能会想，如果通过正则化可以防止过拟合，那么为何还需要装袋或其他集成技术。问题在于，除了过拟合之外，***决策树还容易出现不稳定性。***'
- en: Decision trees are highly sensitive to small variations in the dataset. Even
    minor changes in the training data can lead to drastically different decision
    trees. This instability can be limited by training many trees on random subsamples
    of the data and then averaging the predictions of these trees.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树对数据集中的小变化非常敏感。即使是训练数据的微小变化也会导致完全不同的决策树。这种不稳定性可以通过在数据的随机子样本上训练多个树，然后对这些树的预测结果进行平均来限制。
- en: '**The Idea of Ensemble Learning**'
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**集成学习的思想**'
- en: An *ensemble* is a group of models and the technique of aggregating the predictions
    of these models is known as ***ensemble learning.***
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*集成* 是一组模型，而聚合这些模型预测的技术被称为***集成学习。***'
- en: 'There are two approaches to ensemble learning:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习有两种方法：
- en: Use different training algorithms such as decision trees, SVM, etc. for each
    predictor and train them on the given training set.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对每个预测器使用不同的训练算法，如决策树、SVM 等，并在给定的训练集上进行训练。
- en: Use the same training algorithm for every predictor and train them on different
    subsets of the training set. *Bagging falls in this category.*
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对每个预测器使用相同的训练算法，并在训练集的不同子集上进行训练。*装袋属于这一类别。*
- en: '**Introduction to Bagging**'
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**装袋简介**'
- en: Bagging is a short word for ***bootstrap aggregation****.*
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 装袋是***bootstrap aggregation***的简称。
- en: Bagging is an ensemble method in which multiple models are trained on different
    random subsamples of the training set, and the sampling is performed with replacement.
  id: totrans-95
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 装袋是一种集成方法，其中多个模型在训练集的不同随机子样本上进行训练，并且抽样是有放回的。
- en: Sampling with replacement means that some instances can be sampled several times
    for any given predictor, while others may not be sampled at all. This ensures
    that sensitivity to minor variations in the training data gets accounted for and
    no longer harms the stability of the final ensemble.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 带替换采样意味着某些实例可能会被多个预测器多次采样，而其他实例可能根本不会被采样。这确保了对训练数据中微小变化的敏感性得到考虑，并且不再影响最终集成的稳定性。
- en: '![](../Images/a8efccce1215892f87eb106f225e6a04.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a8efccce1215892f87eb106f225e6a04.png)'
- en: Illustration of Bagging (Image by author)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 袋装示意图 (作者提供的图片)
- en: '***Note:*** *We can either subsample the training set with replacement or without
    replacement.* ***When the sampling is done with replacement, it is known as bagging.
    When the sampling is done without replacement, it is known as pasting.***'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '***注意:*** *我们可以选择带替换或不带替换地对训练集进行子采样。* ***当采样是带替换的，这被称为袋装。当采样是不带替换的，这被称为过去法。***'
- en: 'Once all the models are trained on random subsamples of the training data,
    their predictions can be aggregated as:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有模型在训练数据的随机子样本上训练完成，它们的预测可以被聚合为：
- en: averaging the predictions for regression
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归预测的平均
- en: majority voting for classification
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类的多数投票
- en: Now that we have an idea of ensemble learning and bagging, let’s implement it
    in scikit-learn. Let’s continue the following steps in our notebook.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对集成学习和袋装有了一定了解，让我们在 scikit-learn 中实现它。接下来让我们在笔记本中继续执行以下步骤。
- en: '**Step 5: Implement bagging in scikit-learn**'
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**步骤 5: 在 scikit-learn 中实现袋装**'
- en: We can simply pass our decision tree regressor inside a bagging regressor and
    specify the number of models we want to train (*n_estimators),* and thenumber
    of samples to consider for training each model (*max_samples).*
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以简单地将决策树回归器传递给袋装回归器，并指定我们想要训练的模型数量 (*n_estimators*)，以及每个模型训练时考虑的样本数量 (*max_samples*)。
- en: Here, `bootstrap=True` means that the data will be sampled with replacement
    and if we want to use pasting instead of bagging, we can set `bootstrap=False`.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`bootstrap=True` 意味着数据将进行带替换的采样，如果我们想使用过去法而不是袋装，可以将 `bootstrap=False` 设置为
    False。
- en: '[PRE5]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`Cell Output:`'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`单元输出:`'
- en: '![](../Images/750e3796c72cc86d955b7d1b67cac124.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/750e3796c72cc86d955b7d1b67cac124.png)'
- en: It means we have trained 200 decision trees separately such that each decision
    tree has used a random subsample of size 100 as the training set. The end prediction
    will be the average of all predictions.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们分别训练了 200 棵决策树，每棵决策树都使用了大小为 100 的随机子样本作为训练集。最终预测将是所有预测的平均值。
- en: '**Step 6: Evaluate the bagged version of the decision tree regressor**'
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**步骤 6: 评估决策树回归器的袋装版本**'
- en: We will use mean squared error again to evaluate how well the model predicts
    the samples in training as well as the test set.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将再次使用均方误差来评估模型在训练集和测试集上的样本预测效果。
- en: '[PRE6]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`Cell Output:`'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`单元输出:`'
- en: '![](../Images/d59594848411f96d374f5a0a10075b58.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d59594848411f96d374f5a0a10075b58.png)'
- en: After using bagging, the training MSE has gone up from 0 to 69.438 but the test
    MSE has gone down from 173.336 to 101.521 which is indeed an improvement!
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 使用袋装后，训练的均方误差从 0 上升到 69.438，但测试的均方误差从 173.336 下降到 101.521，这确实是一个改进！
- en: We can verify from the below plot that the final predictions after the bagged
    ensemble of decision trees have a lot better generalization capability than the
    previous one.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从下面的图中验证，经过袋装集成的决策树的最终预测具有比之前更好的泛化能力。
- en: '![](../Images/d64363495f57cadc97ac5144dde52b7c.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d64363495f57cadc97ac5144dde52b7c.png)'
- en: 'Source: Image by author'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '来源: 作者提供的图片'
- en: 'The following plot shows the bagging regressor’s predictions for the given
    training and test data:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了袋装回归器对给定训练和测试数据的预测：
- en: '![](../Images/3e4ed0b338726ca7465bc44100249ec6.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3e4ed0b338726ca7465bc44100249ec6.png)'
- en: 'Source: Image by author'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '来源: 作者提供的图片'
- en: The final predictions from the ensemble are smoother than what a single decision
    tree would have produced and the model shows a similar fit for both the training
    and test sets.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 来自集成的最终预测比单棵决策树的预测更平滑，模型在训练集和测试集上的拟合情况相似。
- en: Link to full notebook
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完整笔记本链接
- en: You can find the notebook [here](https://github.com/gurjinderbassi/Machine-Learning/blob/main/Overfitting%20and%20Bagging%20in%20Decision%20Trees.ipynb).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[这里](https://github.com/gurjinderbassi/Machine-Learning/blob/main/Overfitting%20and%20Bagging%20in%20Decision%20Trees.ipynb)找到笔记本。
- en: 'Bonus: Random Forests'
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '奖励: 随机森林'
- en: At the beginning of this article, I specified that random forests use the idea
    of bagging plus *something else.*I don’t want you to keep pondering upon what
    this *something else* is, and since you’ve almost got to the end of this article,
    this bonus section is your reward 😸
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文开头，我提到随机森林使用了袋装加上*其他内容*的理念。我不想让你继续琢磨这个*其他内容*是什么，既然你已经快到达文章的结尾了，这一额外部分就是你的奖励
    😸
- en: A random forest is an ensemble of decision trees that are trained via the bagging
    method.
  id: totrans-128
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 随机森林是通过袋装方法训练的决策树集成。
- en: Shedding light on *something else:* The random forest algorithm introduces extra
    randomness while growing the trees. While splitting a node, instead of searching
    the entire feature space, it searches for the best feature among a random subset
    of features. This further enhances the diversity of the models and reduces variance,
    giving rise to an overall better ensemble.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 揭示*其他内容：* 随机森林算法在生长树木时引入了额外的随机性。在分裂一个节点时，它不是搜索整个特征空间，而是在随机特征子集中寻找最佳特征。这进一步增强了模型的多样性，并减少了方差，从而产生了更好的集成模型。
- en: Random forests can also be implemented using scikit-learn for both the regression
    and classification tasks. It has all the hyperparameters of a *DecisionTreeRegressor
    (or DecisionTreeClassifier)* to control how individual trees are grown, plus all
    the hyperparameters of a *BaggingRegressor (or BaggingClassifier),* with some
    exceptions. The other set of hyperparameters is also there to control the sampling
    of features to consider at each node.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林也可以使用 scikit-learn 实现，适用于回归和分类任务。它具有控制单棵树生长的所有超参数，如*DecisionTreeRegressor（或
    DecisionTreeClassifier）*，以及*BaggingRegressor（或 BaggingClassifier）* 的所有超参数，但有一些例外。另一个超参数集还用于控制每个节点考虑的特征采样。
- en: Conclusion
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we discussed the issues of overfitting and instability in decision
    trees and how we can use ensemble methods such as bagging to overcome them.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们讨论了决策树中的过拟合和不稳定性问题，以及如何使用诸如袋装（bagging）等集成方法来克服这些问题。
- en: Decision trees are powerful machine learning algorithms that can solve both
    regression and classification problems, however, they suffer from overfitting
    and instability.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树是强大的机器学习算法，能够解决回归和分类问题，但它们容易过拟合且不稳定。
- en: Overfitting occurs when a model fits the training data so perfectly that it
    fails to generalize well and learn the underlying behavior of the data.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过拟合发生在模型过于完美地拟合训练数据，以至于无法很好地概括和学习数据的潜在行为。
- en: Regularization can be used to reduce the chance of overfitting by limiting the
    growth of the decision tree.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化可以通过限制决策树的增长来减少过拟合的可能性。
- en: Another problem with decision trees is that they are highly sensitive to small
    variations in the data that make them unstable. This can be overcome by using
    ensemble techniques.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树的另一个问题是对数据的小变化非常敏感，使其变得不稳定。这可以通过使用集成技术来克服。
- en: Ensemble learning consists of training multiple predictors on random subsets
    of the training data and then aggregating their predictions. Bagging is one such
    technique that samples the training data with replacement.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成学习包括在训练数据的随机子集上训练多个预测器，然后聚合它们的预测。袋装是一种使用替换的训练数据采样技术。
- en: Random Forests improve upon the decision trees by incorporating bagging and
    random feature selection at each node to reduce the overall variance.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林通过在每个节点上结合袋装和随机特征选择来改进决策树，从而减少总体方差。
- en: Thank you for reading, I hope it was helpful!
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢阅读，希望对你有所帮助！
- en: Open to any feedback or suggestions.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎任何反馈或建议。
- en: 'References:'
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献：
- en: '[1] [https://www.ibm.com/topics/overfitting](https://www.ibm.com/topics/overfitting)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [https://www.ibm.com/topics/overfitting](https://www.ibm.com/topics/overfitting)'
- en: '[2] Hands-on machine learning with Scikit-Learn, Keras and TensorFlow: concepts,
    tools, and techniques to build intelligent systems (2nd ed.). O’Reilly. Aurélien
    Géron, 2019.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] 《动手机器学习：使用 Scikit-Learn、Keras 和 TensorFlow 构建智能系统（第2版）》 O''Reilly。奥雷利安·热罗，2019年。'
- en: '[3] The Hundred-Page Machine Learning Book, Andriy Burkov, 2019.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] 《百页机器学习书》，安德烈·布尔科夫，2019年。'
