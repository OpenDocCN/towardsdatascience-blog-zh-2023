- en: 91% of ML Models Degrade in Time
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 91% 的机器学习模型随着时间的推移会退化
- en: 原文：[https://towardsdatascience.com/91-of-ml-models-degrade-in-time-cfd467905615?source=collection_archive---------10-----------------------#2023-04-11](https://towardsdatascience.com/91-of-ml-models-degrade-in-time-cfd467905615?source=collection_archive---------10-----------------------#2023-04-11)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/91-of-ml-models-degrade-in-time-cfd467905615?source=collection_archive---------10-----------------------#2023-04-11](https://towardsdatascience.com/91-of-ml-models-degrade-in-time-cfd467905615?source=collection_archive---------10-----------------------#2023-04-11)
- en: The performance of ML models degrades as time passes and data distribution changes.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随着时间的推移和数据分布的变化，机器学习模型的性能会退化。
- en: '[](https://santiviquez.medium.com/?source=post_page-----cfd467905615--------------------------------)[![Santiago
    Víquez](../Images/5526cf0e92f31d2438bf6522afa5b212.png)](https://santiviquez.medium.com/?source=post_page-----cfd467905615--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cfd467905615--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cfd467905615--------------------------------)
    [Santiago Víquez](https://santiviquez.medium.com/?source=post_page-----cfd467905615--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://santiviquez.medium.com/?source=post_page-----cfd467905615--------------------------------)[![Santiago
    Víquez](../Images/5526cf0e92f31d2438bf6522afa5b212.png)](https://santiviquez.medium.com/?source=post_page-----cfd467905615--------------------------------)[](https://towardsdatascience.com/?source=post_page-----cfd467905615--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----cfd467905615--------------------------------)
    [Santiago Víquez](https://santiviquez.medium.com/?source=post_page-----cfd467905615--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F85c82fdad717&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F91-of-ml-models-degrade-in-time-cfd467905615&user=Santiago+V%C3%ADquez&userId=85c82fdad717&source=post_page-85c82fdad717----cfd467905615---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cfd467905615--------------------------------)
    ·9 min read·Apr 11, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcfd467905615&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F91-of-ml-models-degrade-in-time-cfd467905615&user=Santiago+V%C3%ADquez&userId=85c82fdad717&source=-----cfd467905615---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F85c82fdad717&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F91-of-ml-models-degrade-in-time-cfd467905615&user=Santiago+V%C3%ADquez&userId=85c82fdad717&source=post_page-85c82fdad717----cfd467905615---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----cfd467905615--------------------------------)
    ·9 min read·2023年4月11日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fcfd467905615&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F91-of-ml-models-degrade-in-time-cfd467905615&user=Santiago+V%C3%ADquez&userId=85c82fdad717&source=-----cfd467905615---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcfd467905615&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F91-of-ml-models-degrade-in-time-cfd467905615&source=-----cfd467905615---------------------bookmark_footer-----------)![](../Images/a31369d8347657f5810e3b7f2b2559e5.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fcfd467905615&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2F91-of-ml-models-degrade-in-time-cfd467905615&source=-----cfd467905615---------------------bookmark_footer-----------)![](../Images/a31369d8347657f5810e3b7f2b2559e5.png)'
- en: Model aging chart showing the performance of an ML model degrading in time.
    Image retrieved from the original paper, annotated by the author.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 模型老化图表展示了机器学习模型随着时间推移的性能退化情况。图片摘自原始论文，由作者注释。
- en: A recent study from MIT, Harvard, The University of Monterrey, and Cambridge
    showed that [91% of ML models degrade over time](https://www.nature.com/articles/s41598-022-15245-z).
    This study is one of the first of its kind, where researchers focus on studying
    machine learning models’ behavior after deployment and how their performance evolves
    with unseen data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最近来自麻省理工学院、哈佛大学、蒙特雷大学和剑桥大学的研究显示，[91% 的机器学习模型随着时间推移会退化](https://www.nature.com/articles/s41598-022-15245-z)。这是同类研究中的首个之一，研究人员关注于机器学习模型在部署后的行为及其性能如何随未见数据的发展而变化。
- en: “While much research has been done on various types and markers of temporal
    data drifts, there is no comprehensive study of how the models themselves can
    respond to these drifts.”
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “虽然已经对各种类型和时间数据漂移的标记进行了大量研究，但尚未有关于模型自身如何应对这些漂移的全面研究。”
- en: This blog post will review the most critical parts of the research, highlight
    their results, and stress the importance of these results, especially for the
    ML industry.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本博客文章将回顾研究的关键部分，突出其结果，并强调这些结果的重要性，特别是对机器学习行业的意义。
- en: Introduction
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: If you have been previously exposed to concepts like covariate shift or concept
    drift, you may be aware that changes in the distribution of the production data
    may affect the model’s performance. This phenomenon is one of the challenges of
    maintaining an ML model in production.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你以前接触过像协变量偏移或概念漂移这样的概念，你可能知道生产数据分布的变化可能会影响模型的性能。这种现象是维护生产环境下机器学习模型的一大挑战。
- en: By definition, ML models depend on the data it was trained on, meaning that
    if the distribution of the production data starts to change, the model may no
    longer perform as well as before. And as time passes, the model’s performance
    may degrade more and more. The authors refer to this phenomenon as *“AI aging.”*
    I like to call it model performance degradation, and depending on how significant
    the drop in performance is, we may consider it a model failure.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定义，机器学习模型依赖于其训练的数据，这意味着如果生产数据的分布开始变化，模型可能不再像以前那样表现良好。随着时间的推移，模型的性能可能会越来越差。作者称这种现象为*“AI老化。”*
    我喜欢称之为模型性能退化，具体的性能下降幅度可能让我们将其视为模型失败。
- en: To get a better understanding of this phenomenon, the authors developed a framework
    for identifying temporal model degradation. They applied the framework to 32 datasets
    from four industries, using four standard ML models, and investigated how temporal
    model degradation can develop under minimal drifts in the data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这种现象，作者开发了一个识别时间性模型退化的框架。他们将该框架应用于来自四个行业的32个数据集，使用四种标准机器学习模型，并调查了在数据的最小漂移下，时间性模型退化如何发展。
- en: Models and Data
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型和数据
- en: To avoid any model bias, the authors chose four different standard ML methods
    (Linear Regression, Random Forest Regressor, XGBoost, and a Multilayer Perceptron
    Neural Network). Each of these methods represents different mathematical approaches
    to learning from data. By choosing different model types, they were able to investigate
    similarities and differences in the way diverse models can age **on the same data**.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免任何模型偏差，作者选择了四种不同的标准机器学习方法（线性回归、随机森林回归、XGBoost和多层感知器神经网络）。这些方法代表了从数据中学习的不同数学方法。通过选择不同的模型类型，他们能够调查不同模型在**相同数据**上的老化方式的相似性和差异。
- en: Similarly, to avoid domain bias, they chose 32 datasets from four industries
    (Healthcare, Weather, Airport Traffic, and Financial).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，为了避免领域偏差，他们选择了来自四个行业（医疗保健、天气、机场交通和金融）的32个数据集。
- en: Another critical decision is that they only investigated pairs of model-dataset
    with good initial performance. This decision is crucial since it is not worthwhile
    investigating the degradation of a model with a poor initial fit.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个关键决定是，他们只调查了初始性能良好的模型-数据集对。这一决定至关重要，因为研究一个初始适配差的模型的退化是不值得的。
- en: '![](../Images/07c8874d017956cf56a2d2832630e1d7.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07c8874d017956cf56a2d2832630e1d7.png)'
- en: Examples of original data used in temporal degradation experiments. The timeline
    is on the horizontal axis and, each dataset target variable is on the vertical
    axis. When multiple data points were collected per day, they were shown with background
    color and a moving daily average curve. The colors highlighting the titles are
    going to be used along the blog post to easily recognize each dataset industry.
    Image retrieved from the original paper, annotated by the author.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 时间退化实验中使用的原始数据示例。时间轴在横轴上，每个数据集的目标变量在纵轴上。当每天收集了多个数据点时，它们会用背景颜色和移动的日均曲线来显示。突出标题的颜色将在博客文章中用于轻松识别每个数据集的行业。图片来自原始论文，由作者标注。
- en: Proposed Framework
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提议的框架
- en: To identify temporal model performance degradation, the authors designed a framework
    that emulates a typical production ML model. And ran multiple dataset-model experiments
    following this framework.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了识别时间性模型性能退化，作者设计了一个模拟典型生产机器学习模型的框架，并按照该框架进行了多个数据集-模型实验。
- en: 'For each experiment, they did four things:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个实验，他们做了四件事：
- en: Randomly select one year of historical data as training data
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机选择一年的历史数据作为训练数据。
- en: Select an ML model
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个机器学习模型。
- en: Randomly pick a future datetime point where they will test the model
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机选择一个未来的日期时间点来测试模型。
- en: Calculate the model’s performance change
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算模型性能的变化
- en: To better understand the framework, we need a couple of definitions. The most
    recent point in the training data was defined as *t_0*. The number of days between
    *t_0* and the point in the future where they test the model was defined as *dT*,
    which symbolizes the model’s age.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解框架，我们需要几个定义。训练数据中最新的点定义为*t_0*。在*t_0*和测试模型的未来时间点之间的天数定义为*dT*，它象征着模型的年龄。
- en: For example, a weather forecasting model was trained with data from January
    1st to December 31st of 2022\. And on February 1st, 2023, we ask it to make a
    weather forecast.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个天气预报模型使用2022年1月1日至12月31日的数据进行训练。然后在2023年2月1日，我们要求它进行天气预报。
- en: In this case
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下
- en: '*t_0* = December 31st, 2022 since it is the most recent point in the training
    data.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*t_0* = 2022年12月31日，因为这是训练数据中最新的点。'
- en: '*dT* = 32 days (days from December 31st and February 1st). This is the age
    of the model.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*dT* = 32天（从12月31日到2月1日的天数）。这是模型的年龄。'
- en: The diagram below summarizes how they performed every “history-future” simulation.
    We have added annotations to make it easier to follow.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 下图总结了他们如何执行每次“历史-未来”模拟。我们添加了注释以便于理解。
- en: '![](../Images/70128da5eab652b9c09dc391f311a9d0.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/70128da5eab652b9c09dc391f311a9d0.png)'
- en: Diagram of the AI temporal degradation experiment. Image retrieved from the
    original paper, annotated by the author.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: AI时间降解实验的图示。图片取自原始论文，由作者标注。
- en: To quantify the model’s performance change, they measured the mean squared error
    (MSE) at time *t_0* as *MSE(t_0)* and at the time of the model evaluation as *MSE(t_1)*.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了量化模型性能的变化，他们测量了在时间*t_0*时的均方误差（MSE）为*MSE(t_0)*，以及在模型评估时的*MSE(t_1)*。
- en: Since *MSE(t_0)* is supposed to be low (each model was generalizing well at
    dates close to training). One can measure the relative performance error as the
    ratio between *MSE(t_0)* and *MSE(t_1)*.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由于*MSE(t_0)*应该较低（每个模型在接近训练日期时表现良好）。可以将相对性能误差测量为*MSE(t_0)*和*MSE(t_1)*之间的比率。
- en: '*E_rel* = *MSE(t_1)/MSE(t_0)*'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*E_rel* = *MSE(t_1)/MSE(t_0)*'
- en: The researchers ran 20,000 experiments of this type for each dataset-model pair!
    Where *t_0* and *dT* were randomly sampled from a uniform distribution.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员为每个数据集-模型对进行了20,000次这种类型的实验！其中*t_0*和*dT*是从均匀分布中随机抽取的。
- en: After running all of these experiments, they reported an *aging model chart*
    for each dataset-model pair. This chart contains 20,000 purple points, each representing
    the relative performance error *E_rel* obtained at *dT* days after training.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行所有这些实验之后，他们为每个数据集-模型对报告了一个*老化模型图表*。该图表包含20,000个紫色点，每个点代表训练后*dT*天获得的相对性能误差*E_rel*。
- en: '![](../Images/a31369d8347657f5810e3b7f2b2559e5.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a31369d8347657f5810e3b7f2b2559e5.png)'
- en: Model aging chart for the Financial dataset and the Neural Network model. Each
    small dot represents the outcome of a single temporal degradation experiment.
    Image retrieved from the original paper, annotated by the author.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 财务数据集和神经网络模型的模型老化图表。每个小点代表一次单独的时间降解实验结果。图片取自原始论文，由作者标注。
- en: 'The chart summarizes how the model’s performance changes when the model’s age
    increases. Key takeaways:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图表总结了随着模型年龄的增加，模型性能的变化。关键要点：
- en: '**The error increases over time:** the model becomes less and less performant
    as time passes. This may be happening due to a drift present in any of the model’s
    features or due to concept drift.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**误差随时间增加：** 随着时间的推移，模型的性能越来越差。这可能是由于模型的任何特征存在漂移或由于概念漂移造成的。'
- en: '**The error variability increases over time:** The gap between the best and
    worst-case scenarios increases as the model ages. When an ML model has high error
    variability, it means that it sometimes performs well and sometimes badly. The
    model performance is not just degrading, but it has erratic behavior.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**误差变异性随时间增加：** 随着模型的老化，最佳和最差情况之间的差距增加。当一个机器学习模型具有高误差变异性时，意味着它有时表现良好，有时表现不佳。模型性能不仅仅是退化，还存在不稳定行为。'
- en: The reasonably low median model error may still create the illusion of accurate
    model performance while the actual outcomes become less and less certain.
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 合理较低的中位数模型误差可能仍会产生准确模型性能的错觉，而实际结果变得越来越不确定。
- en: Major Degradation Patterns
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主要降解模式
- en: After performing all the experiments for all 4 (models) x 32 (datasets) = 128
    (model, dataset) pairs, temporal model degradation was observed in **91% of the
    cases**. Here we will look at the four most common degradation patterns and their
    impact on ML model implementations.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在对所有4（模型）x 32（数据集）= 128（模型、数据集）对进行实验后，**91%的情况下**观察到了时间上的模型性能下降。接下来，我们将探讨四种最常见的性能下降模式及其对ML模型实现的影响。
- en: Gradual or no degradation
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 渐进或无降级
- en: Although no strong degradation was observed in the two examples below, these
    results still present a challenge. Looking at the original Patient and Weather
    datasets, we can see that the patient data has a lot of outliers in the Delay
    variable. In contrast, the weather data has seasonal shifts in the Temperature
    variable. But even with these two behaviors in the target variables, both models
    seem to perform accurately over time.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在下面的两个示例中没有观察到强烈的性能下降，但这些结果仍然提出了挑战。查看原始的患者和天气数据集，我们可以看到患者数据中的延迟变量有很多异常值。相比之下，天气数据中的温度变量则有季节性变化。但即便在这些目标变量的行为下，两种模型似乎随着时间的推移表现准确。
- en: '![](../Images/a462360b1374531bd605395323ea4bab.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a462360b1374531bd605395323ea4bab.png)'
- en: Gradual ML model degradation patterns, with relative model error increasing
    no faster than linearly over time. Image retrieved from the original paper, annotated
    by the author.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 渐进的ML模型降级模式，相对模型错误随时间的增加不超过线性。图片来源于原始论文，由作者注释。
- en: The authors claim that these and similar results demonstrate that data drifts
    alone cannot be used to explain model failures or trigger model quality checks
    and retraining.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 作者声称，这些及类似结果表明，单凭数据漂移无法解释模型失败或触发模型质量检查和再训练。
- en: We have also observed this in practice. Data drift does not necessarily translates
    into a model performance degradation. That is why in our [ML monitoring workflow](https://www.notion.so/efc658f138e047e9bceb51d6975d0153),
    we focus on performance monitoring and use data drift detection tools only to
    investigate plausible explanations of the degradation issue since data drifts
    alone should not be used to trigger model quality checks.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在实践中也观察到了这一点。数据漂移并不一定意味着模型性能下降。这就是为什么在我们的[ML 监控工作流程](https://www.notion.so/efc658f138e047e9bceb51d6975d0153)中，我们关注于性能监控，仅使用数据漂移检测工具来调查性能下降问题的合理解释，因为仅凭数据漂移不应触发模型质量检查。
- en: Explosive degradation
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 爆炸性降级
- en: Model performance degradation can also escalate very abruptly. Looking at the
    plot below, we can see that both models were performing well in the first year.
    But at some point, they started to degrade at an explosive rate. The authors claim
    that these degradations can’t be explained alone by a particular drift in the
    data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 模型性能下降也可能非常突然。查看下图，我们可以看到两种模型在第一年表现良好。但在某个时刻，它们开始以爆炸性的速度下降。作者声称，这些性能下降不能仅通过数据中的特定漂移来解释。
- en: '![](../Images/cac6af66fbb95e4673baa338b236def1.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cac6af66fbb95e4673baa338b236def1.png)'
- en: Explosive ML model aging patterns. Image retrieved from the original paper,
    annotated by the author.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 爆炸性ML模型老化模式。图片来源于原始论文，由作者注释。
- en: Let’s compare two model aging plots made from the same dataset but with different
    ML models. On the left, we see an explosive degradation pattern, while on the
    right, almost no degradation was seen. Both models were performing well initially,
    but the neural network seemed to degrade in performance faster than the linear
    regression (labeled as RV model).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 比较两个使用相同数据集但不同ML模型的模型老化图。左侧是爆炸性降级模式，而右侧几乎没有降级。两种模型最初表现良好，但神经网络似乎比线性回归（标记为RV模型）更快地降级。
- en: '![](../Images/ff176754b0996ebf7c463567dac6579e.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ff176754b0996ebf7c463567dac6579e.png)'
- en: Explosive and no degradation comparison. Image retrieved from the original paper,
    annotated by the author.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 爆炸性与无降级对比。图片来源于原始论文，由作者注释。
- en: Given this, and similar results, the authors concluded that *Temporal model
    quality depends on the choice of the ML model and its stability on a certain data
    set.*
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于此及类似结果，作者总结认为*时间性模型质量取决于ML模型的选择及其在特定数据集上的稳定性*。
- en: In practice, we can deal with this type of phenomenon by continuously monitoring
    the estimated model performance. This allows us to address the performance issues
    before an explosive degradation is found.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们可以通过持续监控估计的模型性能来应对这种现象。这使我们能够在发现爆炸性降级之前解决性能问题。
- en: Increase in error variability
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 错误变异性增加
- en: While the yellow (25th percentile) and the black (median) lines remain at relatively
    low error levels, the gap between them and the red line (75th percentile) increases
    significantly with time. As mentioned before, this may create the illusion of
    an accurate model performance while the real model outcomes become less and less
    certain.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管黄色（第25百分位数）和黑色（中位数）线保持在相对较低的误差水平，但它们与红色线（第75百分位数）之间的差距随着时间显著增加。如前所述，这可能会造成准确模型性能的错觉，而实际模型结果却变得越来越不确定。
- en: '![](../Images/35ceea74e6221416629ef5054ab955dc.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/35ceea74e6221416629ef5054ab955dc.png)'
- en: Increasing unpredictability AI model aging patterns. Image retrieved from the
    original paper, annotated by the author.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 增加不可预测的人工智能模型老化模式。图像取自原始论文，由作者标注。
- en: '*Neither the data nor the model alone can be used to guarantee consistent predictive
    quality. Instead, the temporal model quality is determined by the stability of
    a specific model applied to the specific data at a particular time.*'
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*单凭数据或模型本身都无法保证一致的预测质量。相反，时间模型的质量取决于在特定时间对特定数据应用的特定模型的稳定性。*'
- en: Potential Solutions
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 潜在解决方案
- en: Once we have found the underlying cause of the model aging problem, we can search
    for the best technique to fix the problem. The appropriate solution is context-dependent,
    so there is no simple fix that fits every problem.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们找到了模型老化问题的根本原因，我们可以寻找最佳的解决技术。适当的解决方案依赖于具体情境，因此没有一种简单的修复方法适用于所有问题。
- en: Every time we see a model performance degradation, we should investigate the
    issue and understand the cause of it. Automatic fixes are almost impossible to
    generalize for every situation since multiple reasons can cause the degradation
    issue.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们看到模型性能下降时，我们应该调查问题并理解其原因。自动修复几乎不可能针对每种情况进行概括，因为多个原因可能导致退化问题。
- en: 'In the paper, the authors proposed a potential solution to the temporal degradation
    problem. It is focused on ML model retraining and assumes that we have access
    to newly labeled data, that there are no data quality issues, and that there is
    no concept drift. To make this solution practically feasible, they mentioned that
    one needs the following:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在论文中，作者提出了一个可能的解决方案来应对时间退化问题。该方案集中于机器学习模型的再训练，并假设我们可以获得新的标注数据，数据质量没有问题，并且不存在概念漂移。为了使这个解决方案在实际中可行，他们提到需要以下条件：
- en: '**1\. Alert when your model must be retrained.**'
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**1\. 当你的模型需要再训练时发出警报。**'
- en: Alerting when the model’s performance has been degrading is not a trivial task.
    One needs access to the latest ground truth or be able to estimate the model’s
    performance. Solutions like [DLE and CBPE](https://nannyml.readthedocs.io/en/stable/how_it_works/performance_estimation.html#confidence-based-performance-estimation-cbpe)
    from [NannyML](https://go.nannyml.com/nanny-github) can help to do that. For example,
    DLE (Direct Looks Estimation) and CBPE (Confidence-based Performance Estimation)
    use probabilistic methods to estimate the model’s performance even when targets
    are absent. They monitor the estimated performance and alert when the model has
    degraded.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型的性能出现退化时发出警报并非易事。需要访问最新的真实数据或能够估计模型的性能。像 [DLE 和 CBPE](https://nannyml.readthedocs.io/en/stable/how_it_works/performance_estimation.html#confidence-based-performance-estimation-cbpe)
    这样的解决方案来自 [NannyML](https://go.nannyml.com/nanny-github) 可以提供帮助。例如，DLE（直接预测估计）和
    CBPE（基于置信度的性能估计）使用概率方法来估计模型的性能，即使目标缺失时也能进行估计。他们监控估计的性能，并在模型退化时发出警报。
- en: '![](../Images/0b79b7d98fe34ad960204bdc781ff820.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0b79b7d98fe34ad960204bdc781ff820.png)'
- en: Plot taken from [NannyML](https://nannyml.com/)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图表取自 [NannyML](https://nannyml.com/)
- en: '**2\. Develop an efficient and robust mechanism for automatic model retraining.**'
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**2\. 开发一个高效且稳健的自动模型再训练机制。**'
- en: If we know that there is no data quality issue or concept drift, frequently
    retraining the ML model with the latest labeled data could help. However, this
    may cause new challenges, such as lack of model convergence, suboptimal changes
    to the training parameters, and [“catastrophic forgetting”](https://en.wikipedia.org/wiki/Catastrophic_interference)
    which is the *tendency of an artificial neural network to abruptly forget previously
    learned information upon learning new information.*
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们知道没有数据质量问题或概念漂移，频繁使用最新标注数据再训练机器学习模型可能有帮助。然而，这可能会带来新的挑战，例如模型收敛不足、训练参数的次优变化以及
    [“灾难性遗忘”](https://en.wikipedia.org/wiki/Catastrophic_interference)，即*人工神经网络在学习新信息时突然忘记以前学习的信息的倾向*。
- en: 3\. Have constant access to the most recent ground truth.
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 持续访问最新的真实数据。
- en: The most recent ground truth will allow us to retrain the ML model and calculate
    the realized performance. The problem is that in practice, ground truth is often
    delayed, or it is expensive and time-consuming to get newly labeled data.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 最新的真实数据将允许我们重新训练机器学习模型并计算实际性能。问题在于，实际上，真实数据往往会延迟，或者获取新标记数据既昂贵又耗时。
- en: When retraining is very expensive, one potential solution would be to have a
    model catalog and then use the estimated performance to select the model with
    the best-expected performance. This could fix the issue of different models aging
    differently on the same dataset.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 当重新训练非常昂贵时，一个潜在的解决方案是拥有一个模型目录，然后利用估计的性能选择预期表现最佳的模型。这可以解决在相同数据集上不同模型老化速度不同的问题。
- en: Other popular solutions used in the industry are reverting your model back to
    a previous checkpoint, fixing the issue downstream, or changing the business process.
    To learn more about when it is best to apply each solution check out our previous
    blog post on [How to address data distribution shift](https://nannyml.com/blog/6-ways-to-address-data-distribution-shift).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 行业内其他流行的解决方案包括将模型恢复到之前的检查点，修复下游问题，或改变业务流程。要了解每种解决方案的最佳应用时机，请查看我们之前关于[如何应对数据分布变化](https://nannyml.com/blog/6-ways-to-address-data-distribution-shift)的博客文章。
- en: Conclusions
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The study by Vela et al. showed that the ML model’s performance doesn’t remain
    static, even when they achieve high accuracy at the time of deployment. And that
    different ML models age at different rates even when trained on the same datasets.
    Another relevant remark is that not all temporal drifts will cause performance
    degradation. Therefore, the choice of the model and its stability also becomes
    one of the most critical factors in dealing with performance temporal degradation.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Vela等人的研究显示，尽管机器学习模型在部署时可能达到高准确率，但其性能不会保持静态。即使在相同数据集上训练，不同的机器学习模型也会以不同的速度老化。另一个相关的观察是，并非所有的时间漂移都会导致性能下降。因此，模型的选择及其稳定性也成为处理性能时间退化的最关键因素之一。
- en: These results give a theoretical backup of why monitoring solutions are important
    for the ML industry. Furthermore, it shows that ML model performance is prone
    to degradation. This is why every production ML model must be monitored. Otherwise,
    the model may fail without alerting the businesses.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果为为什么监控解决方案对机器学习行业至关重要提供了理论支持。此外，它显示了机器学习模型性能容易退化。这就是为什么每个生产中的机器学习模型都必须进行监控，否则模型可能在没有警示的情况下失败。
- en: References
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Vela, D., Sharp, A., Zhang, R., *et al.* Temporal quality degradation in AI
    models. *Sci Rep* 12, 11654 (2022). [https://doi.org/10.1038/s41598-022-15245-z](https://doi.org/10.1038/s41598-022-15245-z)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Vela, D., Sharp, A., Zhang, R., *et al.* 时间质量退化在AI模型中。*Sci Rep* 12, 11654 (2022).
    [https://doi.org/10.1038/s41598-022-15245-z](https://doi.org/10.1038/s41598-022-15245-z)
- en: I’m constantly writing about data science and machine learning on [Twitter](https://twitter.com/santiviquez)
    and [LinkedIn](https://www.linkedin.com/in/santiagoviquez/). Follow me if you
    want to join me and keep learning in public :)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我在[Twitter](https://twitter.com/santiviquez)和[LinkedIn](https://www.linkedin.com/in/santiagoviquez/)上不断撰写关于数据科学和机器学习的内容。如果你想和我一起在公开场合继续学习，请关注我
    :)
