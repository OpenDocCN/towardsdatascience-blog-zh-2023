- en: Replace Manual Normalization with Batch Normalization in Vision AI Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 替换视觉 AI 模型中的手动归一化为批量归一化
- en: 原文：[https://towardsdatascience.com/replace-manual-normalization-with-batch-normalization-in-vision-ai-models-e7782e82193c?source=collection_archive---------9-----------------------#2023-05-18](https://towardsdatascience.com/replace-manual-normalization-with-batch-normalization-in-vision-ai-models-e7782e82193c?source=collection_archive---------9-----------------------#2023-05-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/replace-manual-normalization-with-batch-normalization-in-vision-ai-models-e7782e82193c?source=collection_archive---------9-----------------------#2023-05-18](https://towardsdatascience.com/replace-manual-normalization-with-batch-normalization-in-vision-ai-models-e7782e82193c?source=collection_archive---------9-----------------------#2023-05-18)
- en: A neat trick to avoid expensive manual pixel normalization for Vision (Image/Video)
    AI models is to stick a Batch normalization layer as the first layer of your model
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个巧妙的技巧是，将批量归一化层作为模型的第一层，以避免在视觉（图像/视频）AI 模型中进行昂贵的手动像素归一化
- en: '[](https://medium.com/@dhruvbird?source=post_page-----e7782e82193c--------------------------------)[![Dhruv
    Matani](../Images/d63bf7776c28a29c02b985b1f64abdd3.png)](https://medium.com/@dhruvbird?source=post_page-----e7782e82193c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e7782e82193c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e7782e82193c--------------------------------)
    [Dhruv Matani](https://medium.com/@dhruvbird?source=post_page-----e7782e82193c--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@dhruvbird?source=post_page-----e7782e82193c--------------------------------)[![Dhruv
    Matani](../Images/d63bf7776c28a29c02b985b1f64abdd3.png)](https://medium.com/@dhruvbird?source=post_page-----e7782e82193c--------------------------------)[](https://towardsdatascience.com/?source=post_page-----e7782e82193c--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----e7782e82193c--------------------------------)
    [Dhruv Matani](https://medium.com/@dhruvbird?source=post_page-----e7782e82193c--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F63f5d5495279&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freplace-manual-normalization-with-batch-normalization-in-vision-ai-models-e7782e82193c&user=Dhruv+Matani&userId=63f5d5495279&source=post_page-63f5d5495279----e7782e82193c---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e7782e82193c--------------------------------)
    ·8 min read·May 18, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe7782e82193c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freplace-manual-normalization-with-batch-normalization-in-vision-ai-models-e7782e82193c&user=Dhruv+Matani&userId=63f5d5495279&source=-----e7782e82193c---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F63f5d5495279&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freplace-manual-normalization-with-batch-normalization-in-vision-ai-models-e7782e82193c&user=Dhruv+Matani&userId=63f5d5495279&source=post_page-63f5d5495279----e7782e82193c---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----e7782e82193c--------------------------------)
    ·8 分钟阅读·2023年5月18日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fe7782e82193c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freplace-manual-normalization-with-batch-normalization-in-vision-ai-models-e7782e82193c&user=Dhruv+Matani&userId=63f5d5495279&source=-----e7782e82193c---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe7782e82193c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freplace-manual-normalization-with-batch-normalization-in-vision-ai-models-e7782e82193c&source=-----e7782e82193c---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fe7782e82193c&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Freplace-manual-normalization-with-batch-normalization-in-vision-ai-models-e7782e82193c&source=-----e7782e82193c---------------------bookmark_footer-----------)'
- en: Co-authored with [Naresh](https://medium.com/u/1e659a80cffd?source=post_page-----e7782e82193c--------------------------------).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 与 [Naresh](https://medium.com/u/1e659a80cffd?source=post_page-----e7782e82193c--------------------------------)
    共同撰写
- en: '![](../Images/4c45a0398260aad3d9333c698c15e9e9.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4c45a0398260aad3d9333c698c15e9e9.png)'
- en: Photo by [Kevin Ku](https://unsplash.com/ko/@ikukevk?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Kevin Ku](https://unsplash.com/ko/@ikukevk?utm_source=medium&utm_medium=referral)
    拍摄，发布在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
    上
- en: Channel Normalization for image pre-processing
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像预处理中的通道归一化
- en: Colour images usually have 3 channels (RGB). It’s common for vision AI models
    to pre-process image pixels and normalize them so that the pixels in a given channel
    are normalized to have a mean of 0.0 and a variance of 1.0\. Normalization is
    performed per-channel since each channel can have its own statistics. The use
    of batch normalization is a general best practice used in vision models to avoid
    a phenomenon known as covariate shift.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 彩色图像通常有3个通道（RGB）。视觉 AI 模型通常会对图像像素进行预处理和归一化，以便将给定通道中的像素归一化到均值为0.0和方差为1.0。由于每个通道可以有自己独特的统计数据，所以归一化是按通道进行的。批量归一化是视觉模型中用于避免被称为协变量转移现象的一种通用最佳实践。
- en: What is covariate shift?
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是协变量转移？
- en: '[Covariate shift](https://en.wikipedia.org/wiki/Batch_normalization) is a phenomenon
    that occurs when the distribution of the input features (i.e., covariates) changes
    between the training and testing phases of a machine learning model. This can
    lead to degraded performance of the model, as the assumptions made during training
    may not hold true during testing. Covariate shift can occur due to changes in
    the data collection process, changes in the population being sampled, or changes
    in the environment in which the model is being used. In order to address covariate
    shift, techniques such as [domain adaptation](https://en.wikipedia.org/wiki/Domain_adaptation)
    and importance weighting can be used to adjust the model’s predictions based on
    the changes in the input distribution.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[协变量转移](https://en.wikipedia.org/wiki/Batch_normalization)是一种现象，当输入特征（即协变量）的分布在机器学习模型的训练阶段和测试阶段之间发生变化时，就会发生这种现象。这可能导致模型性能下降，因为在训练期间做出的假设在测试期间可能不再成立。协变量转移可能由于数据收集过程的变化、被采样的人群的变化或模型使用环境的变化而发生。为了解决协变量转移，可以使用[领域适应](https://en.wikipedia.org/wiki/Domain_adaptation)和重要性加权等技术，以根据输入分布的变化调整模型的预测。'
- en: However, these techniques are fairly complex, and require a deeper understanding
    of the input data distribution.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些技术相当复杂，需要对输入数据分布有更深入的理解。
- en: How does batch normalization avoid covariate shift?
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量归一化如何避免协变量转移？
- en: '[Batch normalization](https://en.wikipedia.org/wiki/Batch_normalization) helps
    with covariate shift by normalizing the activations of each layer in a neural
    network. This means that the mean and variance of the activations are maintained
    at a fixed value, regardless of the distribution of the inputs to that layer.
    By doing so, batch normalization reduces the effect of covariate shift between
    the training and testing datasets. Batch normalization can be applied to any domain
    and scales well to various use cases without any modifications.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[批量归一化](https://en.wikipedia.org/wiki/Batch_normalization)通过规范化神经网络中每一层的激活值来帮助解决协变量转移。这意味着每一层的激活值的均值和方差保持在一个固定的值上，而不受该层输入分布的影响。通过这样做，批量归一化减少了训练数据集和测试数据集之间的协变量转移影响。批量归一化可以应用于任何领域，并且能够很好地扩展到各种使用案例而无需修改。'
- en: More specifically, during training, batch normalization centers and scales the
    activations of each layer based on the mean and variance of the activations computed
    over the current batch. This normalizes the activations to have zero mean and
    unit variance, which helps to stabilize and accelerate the training process. A
    running mean and variance is tracked during the training phase.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，在训练过程中，批量归一化根据当前批次计算的激活均值和方差对每一层的激活值进行中心化和缩放。这将激活值归一化为零均值和单位方差，这有助于稳定和加速训练过程。在训练阶段，跟踪一个运行均值和方差。
- en: During testing, the mean and variance computed during training are used to normalize
    the activations. This ensures that the normalization is consistent with the training
    data and reduces the effect of covariate shift.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试过程中，使用训练期间计算的均值和方差来归一化激活值。这确保了归一化与训练数据一致，并减少了协变量转移的影响。
- en: By reducing the effect of covariate shift, batch normalization can improve the
    generalization performance of neural networks and make them more robust to changes
    in the input distribution.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通过减少协变量转移的影响，批量归一化可以提高神经网络的泛化性能，使其对输入分布的变化更加稳健。
- en: Why to pre-process inputs?
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么要预处理输入？
- en: 'This is done to allow a model to converge faster and generalize well to the
    input data. Models perform relatively better when the input data distribution
    is in a consistent and well-specified range. Specifically, it has the following
    benefits:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做是为了让模型更快地收敛，并对输入数据有良好的泛化能力。当输入数据分布在一致且指定良好的范围内时，模型的表现相对更好。具体来说，它有以下好处：
- en: '**To prevent overflow or underflow**: Machine learning models often involve
    mathematical operations such as addition, multiplication, and exponentiation.
    If the input values are too large or too small, the operations can result in overflow
    or underflow, leading to inaccurate or undefined results. For example, adding
    a small floating point (fp32) number to a large one can end up [ignoring the small
    number](https://stackoverflow.com/questions/22186589/why-does-adding-a-small-float-to-a-large-float-just-drop-the-small-one).'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**防止溢出或下溢**：机器学习模型通常涉及诸如加法、乘法和指数运算等数学操作。如果输入值过大或过小，操作可能会导致溢出或下溢，从而产生不准确或未定义的结果。例如，将一个小的浮点数（fp32）加到一个大数上可能会导致[忽略小数](https://stackoverflow.com/questions/22186589/why-does-adding-a-small-float-to-a-large-float-just-drop-the-small-one)。'
- en: '**To ensure efficient learning**: Neural networks often use a backpropagation
    algorithm to update the weights in the network. This algorithm relies on calculating
    gradients, which can become very small or very large if the input values are not
    normalized, making the learning process slower and less efficient.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**确保高效学习**：神经网络通常使用反向传播算法来更新网络中的权重。该算法依赖于计算梯度，如果输入值没有归一化，这些梯度可能会变得非常小或非常大，从而使学习过程变得更慢且效率降低。'
- en: '**To improve generalization**: When a model is trained on a specific range
    of input values, it may not perform well on inputs outside of that range. Normalizing
    the inputs can help the model generalize to new and unseen data.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**提高泛化能力**：当模型在特定范围的输入值上训练时，它可能无法在该范围之外的输入上表现良好。归一化输入可以帮助模型对新数据和未见过的数据进行泛化。'
- en: '**Batch normalization** can help address some of these issues by normalizing
    the inputs to each layer of the network during training. This can improve the
    stability of the model and make it less sensitive to the scale of the inputs.
    However, Batch normalization is not always a drop-in replacement for input normalization
    done manually with hard-coded constants.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**批量归一化**可以通过在训练过程中归一化网络每一层的输入来解决一些这些问题。这可以提高模型的稳定性，并使其对输入的尺度不那么敏感。然而，批量归一化并不总是可以完全替代使用硬编码常数手动完成的输入归一化。'
- en: What is Batch normalization?
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是批量归一化？
- en: The topic of what Batch normalization is and how it helps models has been covered
    extensively in numerous articles, so we shall link to the ones that provide the
    most detailed insights and allow the reader to develop an intuition for the operation.
    We also provide some links that compare batch normalization with other normalization
    techniques.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 关于批量归一化是什么及其如何帮助模型的主题已经在众多文章中进行了广泛的讨论，因此我们将链接到提供最详细见解的文章，并让读者对其操作形成直观理解。我们还提供了一些将批量归一化与其他归一化技术进行比较的链接。
- en: '[Batch normalization in 3 levels of understanding](/batch-normalization-in-3-levels-of-understanding-14c2da90a338)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[三层理解的批量归一化](/batch-normalization-in-3-levels-of-understanding-14c2da90a338)'
- en: '[Batch Normalisation Explained](/batch-normalisation-explained-5f4bd9de5feb)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[批量归一化解释](/batch-normalisation-explained-5f4bd9de5feb)'
- en: '[Dive Into Deep Learning: Batch Normalization](https://d2l.ai/chapter_convolutional-modern/batch-norm.html)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[深入了解深度学习：批量归一化](https://d2l.ai/chapter_convolutional-modern/batch-norm.html)'
- en: '[What are the consequences of layer norm vs batch norm?](https://ai.stackexchange.com/questions/27309/what-are-the-consequences-of-layer-norm-vs-batch-norm)'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[层归一化与批量归一化的后果是什么？](https://ai.stackexchange.com/questions/27309/what-are-the-consequences-of-layer-norm-vs-batch-norm)'
- en: How is input normalization typically done?
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入归一化通常是如何完成的？
- en: Typically, the person training the model is responsible for computing the per-channel
    statistics (mean and variance) for the entire training dataset, and normalizing
    the input before training the vision AI model. This normalization is also expected
    to be performed during inference.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，训练模型的人员负责计算整个训练数据集的每个通道的统计数据（均值和方差），并在训练视觉AI模型之前进行归一化。这种归一化也应该在推理过程中进行。
- en: The code for this pre-processing may look like this when using [torchvision
    transforms](https://pytorch.org/vision/main/transforms.html).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[torchvision transforms](https://pytorch.org/vision/main/transforms.html)时，这种预处理的代码可能如下所示。
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You can read more about the [Normalize transform here](https://pytorch.org/vision/main/generated/torchvision.transforms.Normalize.html#torchvision.transforms.Normalize).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里阅读更多关于[Normalize 变换的信息](https://pytorch.org/vision/main/generated/torchvision.transforms.Normalize.html#torchvision.transforms.Normalize)。
- en: You’ll notice that the **means and standard deviations are pre-computed** and
    **then hard-coded** into the **pre-processing pipeline**. There’s a lot of **discussion
    online** about how to do this both **correctly** and **efficiently**. For example,
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到**均值和标准差是预先计算的**，然后**硬编码**到**预处理管道**中。关于如何**正确**和**高效**地做这件事，网上有很多**讨论**。例如，
- en: '[Computing the mean and std of dataset](https://discuss.pytorch.org/t/computing-the-mean-and-std-of-dataset/34949)'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[计算数据集的均值和标准差](https://discuss.pytorch.org/t/computing-the-mean-and-std-of-dataset/34949)'
- en: '[Kaggle Notebook: Computing Dataset Mean and STD (using PyTorch)](https://www.kaggle.com/code/kozodoi/computing-dataset-mean-and-std/notebook)'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[Kaggle Notebook: 计算数据集均值和标准差（使用 PyTorch）](https://www.kaggle.com/code/kozodoi/computing-dataset-mean-and-std/notebook)'
- en: '[How to calculate mean and standard deviation of images in PyTorch](https://www.binarystudy.com/2021/04/how-to-calculate-mean-standard-deviation-images-pytorch.html)'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[如何在 PyTorch 中计算图像的均值和标准差](https://www.binarystudy.com/2021/04/how-to-calculate-mean-standard-deviation-images-pytorch.html)'
- en: How to make this less painful?
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何减少这项工作的痛苦？
- en: Now that we know how painful this process is, we’ll see a neat trick to make
    this less painful for you!
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道这个过程有多痛苦，我们将看到一个巧妙的技巧来减轻你的痛苦！
- en: Simply stick a [BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)
    layer as the first layer of your vision AI model, and drop the [Normalize](https://pytorch.org/vision/main/generated/torchvision.transforms.Normalize.html#torchvision.transforms.Normalize)
    transform from your pre-processing steps!
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 只需将一个[BatchNorm2d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)层作为视觉
    AI 模型的第一层，并从预处理步骤中移除[Normalize](https://pytorch.org/vision/main/generated/torchvision.transforms.Normalize.html#torchvision.transforms.Normalize)变换！
- en: By using batch normalization as the first layer of the model, the input data
    will be **normalized automatically during the training process**, and you won’t
    need to manually normalize the image pixels. This can save you some coding time
    and reduce the chances of introducing errors in the normalization process.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将批量归一化作为模型的第一层，输入数据将在训练过程中**自动归一化**，你不需要手动归一化图像像素。这可以节省一些编码时间，并减少在归一化过程中引入错误的机会。
- en: The rough equivalence of manual normalization and Batch normalization
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 手动归一化和批量归一化的粗略等价性
- en: Here we’ll see some code that can convince us about the rough equivalence of
    the 2 approaches.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将看到一些代码，可以说服我们这两种方法的粗略等价性。
- en: We’ll take 1000 batches of a randomly generated 1x1 image with 3 channels, and
    see if the manually computed mean and variance are similar to the ones computed
    using PyTorch’s BatchNorm2d layer.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 1000 批随机生成的 1x1 图像（具有 3 个通道），看看手动计算的均值和方差是否与使用 PyTorch 的 BatchNorm2d 层计算的结果相似。
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You can see that output of the code cell below.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以查看下面代码单元的输出。
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We started with a random tensor initialized using torch.randn_like(), so we
    expect that over a sufficiently large (40k) number of samples, the mean and variance
    will tend to 0.0 and 1.0 respectively, since that’s what we expect [torch.randn_like()](https://pytorch.org/docs/stable/generated/torch.randn_like.html)
    to generate.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个使用 `torch.randn_like()` 初始化的随机张量开始，因此我们期望在足够大的样本数量（40k）中，均值和方差将趋近于 0.0
    和 1.0，因为这正是我们期望[torch.randn_like()](https://pytorch.org/docs/stable/generated/torch.randn_like.html)生成的结果。
- en: We see that the difference between the manually computed mean and variance over
    the entire input and the mean and variance computed using BatchNorm2d’s rolling
    average based method is close enough for all practical purposes. We can see that
    the means computed using BatchNorm2d are consistently higher or lower (by up to
    40x) than those computed manually. However, in practical terms, this should not
    matter.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到手动计算的均值和方差与使用 BatchNorm2d 的滚动平均法计算的均值和方差之间的差异在所有实际应用中都足够接近。我们可以看到，使用 BatchNorm2d
    计算的均值始终高于或低于手动计算的均值（最多 40 倍）。然而，从实际的角度来看，这应该没有问题。
- en: Caveats
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意事项
- en: There are definitely pros and cons of using this Batch normalization substitution
    in place of manual normalization, and this article wouldn’t be complete without
    a detailed comparison between the 2 in terms of when each one may or may not be
    applicable.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用批量归一化代替手动归一化确实有利有弊，这篇文章如果不详细比较这两者在何时适用或不适用的话就不完整了。
- en: Transfer learning
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迁移学习
- en: When using transfer learning, it is often recommended to keep the normalization
    method used in the [pre-trained model](https://pytorch.org/vision/main/models.html)
    to avoid introducing unnecessary changes. In this case, it may not be appropriate
    to replace manual normalization with batch normalization.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用迁移学习时，通常建议保持[预训练模型](https://pytorch.org/vision/main/models.html)中使用的归一化方法，以避免引入不必要的变化。在这种情况下，用批量归一化替换手动归一化可能不合适。
- en: For example, here’s what the torchvision page has to say on this subject.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以下是 torchvision 页面对这个主题的描述。
- en: “Before using the pre-trained models, one must preprocess the image (resize
    with right resolution/interpolation, apply inference transforms, rescale the values
    etc). There is no standard way to do this as it depends on how a given model was
    trained. It can vary across model families, variants or even weight versions.
    Using the correct preprocessing method is critical and failing to do so may lead
    to decreased accuracy or incorrect outputs.”
  id: totrans-59
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “在使用预训练模型之前，必须对图像进行预处理（调整正确的分辨率/插值、应用推理变换、重新缩放值等）。没有标准的方法来做到这一点，因为这取决于给定模型的训练方式。它可以在模型系列、变体甚至权重版本之间有所不同。使用正确的预处理方法至关重要，否则可能导致准确性下降或输出错误。”
- en: Training Efficiency
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练效率
- en: When considering training efficiency, it may be beneficial to pre-compute the
    mean and variance of your dataset and hard-code it as a pre-training normalization
    step. This prevents the repeated computation of these statistics during training.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑训练效率时，预先计算数据集的均值和方差并将其硬编码为预训练的归一化步骤可能是有益的。这可以防止在训练过程中重复计算这些统计量。
- en: Note that using either method uses an equal amount of compute during inference
    since you’ll normalize the inputs using the computed mean and variance either
    before feeding the inputs into the model (manually computed) or as the first step
    of the model (using Batch normalization).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，使用任何方法在推理期间的计算量相等，因为你会使用计算出的均值和方差对输入进行归一化，无论是在将输入喂入模型之前（手动计算）还是作为模型的第一步（使用批量归一化）。
- en: Human Efficiency
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人工效率
- en: Based on what we’ve seen above, it’s much easier for a human to stick a Batch
    normalization layer instead of manually computing statistics.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们上面的观察，对于人类来说，插入一个批量归一化层比手动计算统计量要容易得多。
- en: Data Augmentation
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据增强
- en: When augmenting inputs before feeding them into the model, one must be careful
    to apply the normalization **after** all the augmentation and other pre-processing
    steps are done to avoid computing incorrect statistics. For example, if you’re
    using a [ColorJitter transform](https://pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html#torchvision.transforms.ColorJitter),
    then it will change the computed statistics meaningfully.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在将输入送入模型之前进行数据增强时，必须小心在所有增强和其他预处理步骤完成后**再**应用归一化，以避免计算出不正确的统计量。例如，如果你使用了[ColorJitter
    变换](https://pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html#torchvision.transforms.ColorJitter)，它将会显著改变计算出的统计量。
- en: This leads us to another interesting question ***“When should the mean and variance
    of the dataset be computed when using data augmentation?”***
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这引出了另一个有趣的问题***“在使用数据增强时，应该什么时候计算数据集的均值和方差？”***
- en: This is tricky since manually computing the mean and variance accurately across
    augmented inputs would require you to know a priori which images will be augmented
    with which transforms, and then requires you to compute the statistics and apply
    augmentations during model training in a consistent manner. This is somewhat hard
    to do in general since augmentations are applied to input images at random. In
    addition, the same image is augmented differently at every training epoch to prevent
    the model from overfitting on the training dataset. Hence, in this case, using
    Batch normalization would result in better model accuracy as well, since it’s
    computing the mean and variance on the augmented image and not on the original
    un-augmented images.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点棘手，因为准确地跨增强输入计算均值和方差需要你预先知道哪些图像将使用哪些转换进行增强，然后需要你在模型训练期间以一致的方式计算统计信息并应用增强。这在一般情况下有些困难，因为增强是随机应用在输入图像上的。此外，同一图像在每个训练周期会以不同方式进行增强，以防止模型过度拟合训练数据集。因此，在这种情况下，使用批量标准化也会提高模型的准确性，因为它计算的是增强图像的均值和方差，而不是原始未增强的图像。
- en: Mini-batch distribution
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 小批量分布
- en: Since Batch normalization computes the mean and variance of the mini-batch for
    normalizing values during training, it is important to randomize the input data
    to ensure the mean and variance of the mini-batch are somewhat representative
    of the entire training dataset. If your mini-batches are small or biased, then
    you should consider un-biasing them or using manual normalization.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 由于批量标准化在训练期间计算小批量的均值和方差以进行归一化值，因此重要的是随机化输入数据以确保小批量的均值和方差在某种程度上代表整个训练数据集。如果你的小批量数据很小或有偏差，那么你应该考虑消除偏差或使用手动标准化。
- en: This problem doesn’t exist at test or inference time.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题在测试或推理时间不存在。
- en: Conclusion
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Normalization is an important pre-processing step in any vision AI pipeline.
    Computing it correctly and efficiently via manual steps can be tedious and error
    prone. Using Batch normalization as the first layer of your vision AI model is
    a viable substitute for manual normalization in many scenarios.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化是任何视觉 AI 管道中的重要预处理步骤。通过手动步骤正确并高效地计算它可能会很繁琐且容易出错。在许多情况下，将批量标准化作为视觉 AI 模型的第一层是手动标准化的可行替代品。
