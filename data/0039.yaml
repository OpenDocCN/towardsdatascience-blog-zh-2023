- en: The 1958 Perceptron as a tumour classifier
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1958年的感知机作为肿瘤分类器
- en: 原文：[https://towardsdatascience.com/the-1958-perceptron-as-a-breast-cancer-classifier-672556186156?source=collection_archive---------18-----------------------#2023-01-03](https://towardsdatascience.com/the-1958-perceptron-as-a-breast-cancer-classifier-672556186156?source=collection_archive---------18-----------------------#2023-01-03)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/the-1958-perceptron-as-a-breast-cancer-classifier-672556186156?source=collection_archive---------18-----------------------#2023-01-03](https://towardsdatascience.com/the-1958-perceptron-as-a-breast-cancer-classifier-672556186156?source=collection_archive---------18-----------------------#2023-01-03)
- en: A practical example implemented in Mathematica
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Mathematica中实现的实际示例
- en: '[](https://medium.com/@m.emmanuel?source=post_page-----672556186156--------------------------------)[![Mario
    Emmanuel](../Images/e6461a139937d40d09ba6af7078b3b8e.png)](https://medium.com/@m.emmanuel?source=post_page-----672556186156--------------------------------)[](https://towardsdatascience.com/?source=post_page-----672556186156--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----672556186156--------------------------------)
    [Mario Emmanuel](https://medium.com/@m.emmanuel?source=post_page-----672556186156--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@m.emmanuel?source=post_page-----672556186156--------------------------------)[![Mario
    Emmanuel](../Images/e6461a139937d40d09ba6af7078b3b8e.png)](https://medium.com/@m.emmanuel?source=post_page-----672556186156--------------------------------)[](https://towardsdatascience.com/?source=post_page-----672556186156--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----672556186156--------------------------------)
    [Mario Emmanuel](https://medium.com/@m.emmanuel?source=post_page-----672556186156--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7728df51142e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-1958-perceptron-as-a-breast-cancer-classifier-672556186156&user=Mario+Emmanuel&userId=7728df51142e&source=post_page-7728df51142e----672556186156---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----672556186156--------------------------------)
    ·14 min read·Jan 3, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F672556186156&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-1958-perceptron-as-a-breast-cancer-classifier-672556186156&user=Mario+Emmanuel&userId=7728df51142e&source=-----672556186156---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7728df51142e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-1958-perceptron-as-a-breast-cancer-classifier-672556186156&user=Mario+Emmanuel&userId=7728df51142e&source=post_page-7728df51142e----672556186156---------------------post_header-----------)
    发表在[Towards Data Science](https://towardsdatascience.com/?source=post_page-----672556186156--------------------------------)
    ·14分钟阅读·2023年1月3日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F672556186156&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-1958-perceptron-as-a-breast-cancer-classifier-672556186156&user=Mario+Emmanuel&userId=7728df51142e&source=-----672556186156---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F672556186156&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-1958-perceptron-as-a-breast-cancer-classifier-672556186156&source=-----672556186156---------------------bookmark_footer-----------)![](../Images/102748f6bc62bb86ea6dbffa91874926.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F672556186156&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fthe-1958-perceptron-as-a-breast-cancer-classifier-672556186156&source=-----672556186156---------------------bookmark_footer-----------)![](../Images/102748f6bc62bb86ea6dbffa91874926.png)'
- en: The US Navy using the Mark I Perceptron to read letters. National Museum of
    the US Navy, 1960\. Image from Wikimedia Commons (Public Domain).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 美国海军使用Mark I Perceptron读取字母。美国海军国家博物馆，1960年。图片来源于维基共享资源（公共领域）。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: The Rosenblatt Perceptron, developed by Frank Rosenblatt in 1958[1][2], is considered
    the origin of neural networks because it was the first algorithm that demonstrated
    the ability of a machine to learn from data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 1958年由**弗兰克·罗森布拉特**开发的罗森布拉特感知机[1][2]，被认为是神经网络的起源，因为它是第一个展示机器从数据中学习能力的算法。
- en: The Perceptron was a simple model that consisted of a single layer of artificial
    neurons, or units, that could be trained to recognize patterns in input data.
    This marked the beginning of the field of artificial intelligence and paved the
    way for the development of more complex neural networks that have been used in
    a variety of applications.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机是一个简单的模型，由一层人工神经元或单元组成，这些单元可以被训练来识别输入数据中的模式。这标志着人工智能领域的开始，并为更复杂的神经网络的发展铺平了道路，这些神经网络已被用于各种应用。
- en: The first implementation was the Mark I Perceptron developed by the MIT Lincoln
    Laboratory in the late 1950s. It was the first machine that was capable of learning
    from examples, and it was used to perform pattern recognition tasks such as reading
    handwritten letters and recognizing spoken words. The Mark I Perceptron was built
    using vacuum tubes and other electronic components, and it consisted of a series
    of units that were connected together in a layered structure. Each unit was able
    to process input data and adjust its internal weights and biases in order to recognize
    patterns in the data.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个实现是1950年代末由麻省理工学院林肯实验室开发的Mark I感知机。它是第一个能够从示例中学习的机器，用于执行模式识别任务，如读取手写信件和识别口语。Mark
    I感知机使用真空管和其他电子组件构建，由一系列按层结构连接的单元组成。每个单元能够处理输入数据，并调整其内部权重和偏置，以识别数据中的模式。
- en: '![](../Images/d8c9547ec9fa34d7bacbb8857ac7f3f1.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d8c9547ec9fa34d7bacbb8857ac7f3f1.png)'
- en: 'Figure 1\. A detail of the Mark 1\. Source: Wikimedia Commons.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. Mark 1的细节。来源：维基共享资源。
- en: This machine was used by the US Navy in the 1960s to read handwritten letters.
    At the time, the Navy was receiving a large volume of correspondence and needed
    a way to quickly and accurately process the letters. They turned to the Perceptron,
    which had the ability to learn from examples and recognize patterns in input data.
    By training the Perceptron on a large dataset of handwritten letters, the Navy
    was able to develop a system that could accurately read and classify the letters
    with minimal human intervention. This was a significant achievement at the time,
    as it demonstrated the potential of artificial intelligence and machine learning
    to automate tasks that had previously been done by humans. The success of the
    Perceptron in this application helped to establish the technology as a powerful
    tool for pattern recognition and opened the door to its use in a wide range of
    applications.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这台机器在1960年代被美国海军用于读取手写信件。那时，海军接收到大量的信件，需要一种快速而准确的处理信件的方法。他们转向了感知机，这种机器能够从示例中学习并识别输入数据中的模式。通过在大量手写信件数据集上训练感知机，海军能够开发出一个能够准确读取和分类信件的系统，且只需最少的人力干预。这在当时是一个重要的成就，因为它展示了人工智能和机器学习在自动化先前由人类完成的任务方面的潜力。感知机在这一应用中的成功帮助确立了这项技术作为一种强大的模式识别工具，并为其在各种应用中的使用打开了大门。
- en: The single neuron network
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单神经元网络
- en: The Rosenblatt Perceptron is a simple model of an artificial neuron. In this
    model, the single neuron has multiple inputs, which are the values of the features
    in the input data, and a single output, which is a binary value that indicates
    whether the input data belongs to one of two classes. The Perceptron works by
    assigning weights to each input, which represent the importance of that input
    in determining the output. The output is then calculated using a mathematical
    function that combines the weighted inputs with a bias term. The bias term is
    a fixed value that can be adjusted to shift the output of the Perceptron. The
    Perceptron can be trained to recognize patterns in the input data by adjusting
    the weights and bias based on the errors in its predictions. This process of adjusting
    the weights and bias to minimize the errors is known as training the Perceptron.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 罗斯伯拉特感知机是一个简单的人工神经元模型。在这个模型中，单个神经元有多个输入，这些输入是输入数据中特征的值，以及一个输出，这是一个二进制值，指示输入数据是否属于两个类别之一。感知机通过为每个输入分配权重来工作，权重表示该输入在确定输出中的重要性。然后，使用一个数学函数来计算输出，这个函数将加权输入与偏置项结合在一起。偏置项是一个可以调整的固定值，用来移动感知机的输出。通过根据预测中的误差调整权重和偏置，感知机可以被训练以识别输入数据中的模式。这种调整权重和偏置以最小化误差的过程称为训练感知机。
- en: '![](../Images/cb1d39036ae60dac0006ed76d1f6a917.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cb1d39036ae60dac0006ed76d1f6a917.png)'
- en: Figure 2\. The Perceptron corresponds to a one neuron Neural Network. In the
    image no bias is shown, it has to be either added separately, applied as a constant
    to one of the weights or not added at all. Image by [Chrislb from Wikimedia Commons](https://commons.wikimedia.org/wiki/File:ArtificialNeuronModel_english.png).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. 感知器对应于一个神经元神经网络。在图像中未显示偏置，偏置必须单独添加，作为一个常数应用到一个权重中，或完全不添加。图片来自[Chrislb，维基共享资源](https://commons.wikimedia.org/wiki/File:ArtificialNeuronModel_english.png)。
- en: The model consists of a single layer of units, each of which has *d* features
    as inputs and a binary output that can take on the values *-1* and *+1*. The Perceptron
    uses these inputs and output to learn how to classify data into two classes.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型由一个单层单元组成，每个单元具有*d*个特征作为输入，和一个可以取值为*-1*和*+1*的二进制输出。感知器利用这些输入和输出学习如何将数据分类为两个类别。
- en: '![](../Images/bfac8c1d1aa0eaab69f8d4128677c41e.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bfac8c1d1aa0eaab69f8d4128677c41e.png)'
- en: Figure 3\. Training observation Equations.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. 训练观测方程。
- en: In order to train the Perceptron, we need a training set that consists of multiple
    observations, each of which consists of the d features (**X** vector) and the
    actual output (**y**). The Perceptron uses these observations to learn how to
    predict the output based on the features. The output of the Perceptron is calculated
    using the sign of the dot product between the weight vector and the feature vector.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练感知器，我们需要一个训练集，其中包含多个观测值，每个观测值包括d个特征（**X**向量）和实际输出（**y**）。感知器利用这些观测值来学习如何根据特征预测输出。感知器的输出通过权重向量和特征向量的点积的符号计算得出。
- en: Prediction function
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测函数
- en: 'The prediction function of the Rosenblatt Perceptron is used to calculate the
    output of the Perceptron based on the input data and the internal weights and
    bias of the unit. The output of the Perceptron is a binary value that indicates
    whether the input data belongs to one of two classes. The prediction function
    is defined as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 罗斯布拉特感知器的预测函数用于根据输入数据和单位的内部权重及偏置计算感知器的输出。感知器的输出是一个二进制值，表示输入数据是否属于两个类别之一。预测函数定义如下：
- en: '![](../Images/cab0f212948c62ab66919af75f2a2807.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cab0f212948c62ab66919af75f2a2807.png)'
- en: Figure 4\. Prediction function without bias.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. 无偏置的预测函数。
- en: '![](../Images/f07eac91f3fbf969e6c77d542ab76d36.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f07eac91f3fbf969e6c77d542ab76d36.png)'
- en: Figure 5\. Prediction function with bias.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图5\. 带偏置的预测函数。
- en: Where **W** is a vector of weights that corresponds to the input features, **X**
    is a vector of the input values for the features. The model can be implemented
    with and without bias, being **b** the bias term. The sign function returns **-1**
    if the value inside the parentheses is negative and +1 if it is positive. The
    Perceptron adjusts the weights and bias during training in order to minimize the
    errors in its predictions and improve its accuracy.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 其中**W**是与输入特征对应的权重向量，**X**是特征的输入值向量。模型可以有偏置或没有偏置，其中**b**是偏置项。符号函数如果括号中的值为负数，则返回**-1**，如果为正数，则返回+1。感知器在训练过程中调整权重和偏置，以最小化预测中的误差并提高准确性。
- en: The bias
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 偏置
- en: 'The bias term in the Rosenblatt Perceptron represents an additional weight
    that is applied to all of the inputs. It is used to shift the output of the Perceptron
    and can be adjusted during training to improve the accuracy of the model. The
    bias can be implemented in one of two ways: as a constant +1 in one of the features,
    or as an external parameter that is adjusted separately from the other weights.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 罗斯布拉特感知器中的偏置项代表了应用于所有输入的额外权重。它用于移动感知器的输出，并可以在训练过程中进行调整以提高模型的准确性。偏置可以通过两种方式之一实现：作为一个常数+1添加到一个特征中，或作为一个单独调整的外部参数。
- en: If the bias is implemented as a constant +1 feature (*feature trick*), it is
    treated like any other input feature and is given its own weight. This means that
    the bias term is included in the calculation of the output.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果偏置作为常数+1特征（*特征技巧*）实现，它将像任何其他输入特征一样处理，并赋予其自己的权重。这意味着偏置项包含在输出计算中。
- en: Alternatively, the bias can be implemented as an external parameter that is
    adjusted separately from the other weights. The bias is then added to the output
    calculated with the weight vector and the feature vectors.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，偏置也可以作为一个外部参数来实现，与其他权重单独调整。然后将偏置添加到通过权重向量和特征向量计算的输出中。
- en: The bias term in the Rosenblatt Perceptron is useful when the feature variables
    are mean centered, but the mean of the binary class prediction is not 0, because
    it allows the model to adjust the decision boundary of the model in order to better
    fit the data. This can be especially important when the binary class distribution
    is highly imbalanced, as the model may tend to predict the majority class more
    often in order to minimize errors. In this case, the bias can be used to adjust
    the position of the decision boundary and improve the ability of the model to
    correctly classify the minority class.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当特征变量的均值已居中，但二元类预测的均值不为 0 时，Rosenblatt 感知器中的偏置项是有用的，因为它允许模型调整决策边界，以更好地适应数据。当二元类别分布高度不平衡时，这一点尤为重要，因为模型可能倾向于更频繁地预测多数类，以最小化错误。在这种情况下，偏置可以用来调整决策边界的位置，提升模型正确分类少数类的能力。
- en: The loss function
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失函数
- en: 'The Rosenblatt Perceptron model did not include a formal definition of a loss
    function, even when its goal was to minimize the error between the predicted and
    actual values. In order to achieve this, the Perceptron adjusts the weights and
    bias of the model based on the errors in its predictions. One way to define the
    error in the Perceptron model is to use the least-squares method, which involves
    minimizing the sum of the squared differences between the predicted and actual
    values. This loss function can be expressed mathematically as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Rosenblatt 感知器模型没有包含损失函数的正式定义，即使其目标是最小化预测值和实际值之间的误差。为了实现这一点，感知器根据预测中的误差调整模型的权重和偏置。定义感知器模型中的误差的一种方法是使用最小二乘法，即最小化预测值和实际值之间平方差的总和。这个损失函数可以用数学形式表示如下：
- en: '![](../Images/197b6c5a61a83f23cc86d87a846aa239.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/197b6c5a61a83f23cc86d87a846aa239.png)'
- en: Figure 6\. A loss function for the Perceptron.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6\. 感知器的损失函数。
- en: While gradient descent is the usual way to minimize loss functions in Machine
    Learning, it can not be applied to Rosenblatt Perceptron, the reason being that
    the function is not continuous. Instead, the Perceptron used a learning rule called
    the perceptron convergence theorem, which was based on the idea of adjusting the
    weights and bias of the model in order to minimize the errors in its predictions.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管梯度下降是机器学习中最常用的损失函数最小化方法，但它不能应用于 Rosenblatt 感知器，原因在于该函数不连续。相反，感知器使用了一种称为感知器收敛定理的学习规则，该规则基于调整模型的权重和偏置以最小化预测误差的思想。
- en: '![](../Images/dea74c6d89c3cac39287de9a69823209.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dea74c6d89c3cac39287de9a69823209.png)'
- en: Figure 7\. Equivalent to gradient descent of the Loss function.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7\. 相当于损失函数的梯度下降。
- en: '![](../Images/37cd374c01c82be42f8406c75a4ef80e.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/37cd374c01c82be42f8406c75a4ef80e.png)'
- en: Figure 8\. Iterative equations to optimise Weight vector.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. 优化权重向量的迭代方程。
- en: The learning parameter in the Rosenblatt Perceptron model is a hyperparameter
    that determines the step size of the learning algorithm. It is used to control
    the rate at which the weights and bias of the model are updated based on the errors
    in the predictions. A larger learning parameter will result in larger updates
    to the weights and bias, which can lead to faster learning but may also increase
    the risk of overfitting. A smaller learning parameter will result in smaller updates,
    which can lead to slower learning but may also reduce the risk of overfitting.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Rosenblatt 感知器模型中的学习参数是一个超参数，它决定了学习算法的步长。它用于控制模型权重和偏置根据预测误差的更新速度。较大的学习参数会导致权重和偏置的更新幅度较大，这可能导致更快的学习，但也可能增加过拟合的风险。较小的学习参数会导致较小的更新幅度，这可能导致学习速度较慢，但也可能减少过拟合的风险。
- en: The Perceptron model is often described as having a stochastic gradient descent
    (SGD) learning algorithm, despite the fact that it was developed before the concept
    of gradient descent was introduced. This is because the Perceptron learning rule,
    known as the perceptron convergence theorem, is similar in spirit to gradient
    descent, as it involves iteratively adjusting the weights and bias of the model
    in order to minimize the errors in the predictions. Like gradient descent, the
    Perceptron uses a learning parameter to control the step size of the updates,
    and it can be seen as a form of online learning, as it processes the training
    data one sample at a time.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管感知机模型是在引入梯度下降概念之前开发的，但通常描述为具有随机梯度下降（SGD）学习算法。这是因为感知机学习规则，即感知机收敛定理，精神上类似于梯度下降，因为它涉及迭代调整模型的权重和偏置，以最小化预测误差。与梯度下降一样，感知机使用学习参数来控制更新的步长，并且可以被视为一种在线学习形式，因为它一次处理一个训练样本。
- en: Overall, the learning parameter in the Perceptron model plays a crucial role
    in controlling the speed and accuracy of the learning process. By adjusting the
    learning parameter, it is possible to fine-tune the performance of the Perceptron
    model and achieve better results on a variety of classification tasks.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，感知机模型中的学习参数在控制学习过程的速度和准确性方面起着至关重要的作用。通过调整学习参数，可以微调感知机模型的性能，并在各种分类任务中获得更好的结果。
- en: 'The Perceptron in action: a Breast Cancer predictor'
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感知机在行动中：一个乳腺癌预测器
- en: The Wisconsin Breast Cancer dataset[3] is a commonly used dataset for demonstrating
    the capabilities of the Rosenblatt Perceptron model. This dataset consists of
    699 samples of breast cancer biopsy images taken from 1989 until 1992, which have
    been classified as benign or malignant based on the presence of certain features.
    The dataset includes a total of 9 features that were calculated from the images,
    including the clump thickness, uniformity of the cell size, uniformity of the
    cell shape, marginal adhesion, single epithelial cell size, bare nuclei, bland
    chromatin, normal nucleoli and mitoses of the tumors.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 威斯康星州乳腺癌数据集[3]是一个常用的数据集，用于展示罗森布拉特感知机模型的能力。该数据集包含 1989 年至 1992 年期间拍摄的 699 个乳腺癌活检图像样本，这些样本根据某些特征的存在被分类为良性或恶性。数据集中包括从图像中计算出的
    9 个特征，包括肿块厚度、细胞大小的均匀性、细胞形状的均匀性、边缘粘附、单个上皮细胞大小、裸核、平淡的染色质、正常核仁和肿瘤的有丝分裂。
- en: The dataset is a good example to see the Perceptron in action because it is
    a relatively simple dataset with a clear separation between the benign and malignant
    classes. This means that the Perceptron should be able to learn to classify the
    samples correctly with a high degree of accuracy. In addition, the features in
    the dataset are well-defined and easy to understand, which makes it easy to interpret
    the results of the Perceptron model.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集是观察感知机实际应用的一个好例子，因为它是一个相对简单的数据集，良性和恶性类别之间有明显的分隔。这意味着感知机应该能够以较高的准确度正确分类样本。此外，数据集中的特征定义明确且易于理解，这使得解释感知机模型的结果变得容易。
- en: 'The Wisconsin Breast Cancer dataset includes the following features:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 威斯康星州乳腺癌数据集包括以下特征：
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this example the Perceptron will be implemented using Wolfram Mathematica
    Language (it can be adapted to any other language easily).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，感知机将使用 Wolfram Mathematica 语言实现（可以很容易地适应任何其他语言）。
- en: 'The steps to implement will be:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 实现的步骤包括：
- en: Define the features.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义特征。
- en: Load the data (including cleaning).
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据（包括清理）。
- en: Divide dataset into training and test.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集划分为训练集和测试集。
- en: Assign an initial Weight vector.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分配初始权重向量。
- en: Train the model (optimise weight vector).
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型（优化权重向量）。
- en: Use the trained model to compare test data set.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练好的模型比较测试数据集。
- en: Calculate the accuracy.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算准确率。
- en: Calculate the recall.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算召回率。
- en: Calculate the confussion matrix.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算混淆矩阵。
- en: Step 1\. Define the features
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 1 步：定义特征
- en: '[PRE1]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Step 2\. Load the data and clean it
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 2 步：加载数据并清理
- en: '[PRE2]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Step 3\. Divide dataset into training and test
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 3 步：将数据集划分为训练集和测试集
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Step 4\. Assign an initial Weight vector
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 4 步：分配初始权重向量
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Step 5\. Train the model (optimise the weight vector)
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 5 步：训练模型（优化权重向量）
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Step 6\. Use the trained model to compare test data set
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 6 步：使用训练好的模型比较测试数据集
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Step 7\. Calculate accuracy
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 7 步：计算准确率
- en: '[PRE7]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Step 8\. Calculate recall
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 8 步：计算召回率
- en: '[PRE8]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Step 9\. Confussion matrix
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 9 步：混淆矩阵
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/1c94e665e64e9aac879f20cfc3a1abcb.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c94e665e64e9aac879f20cfc3a1abcb.png)'
- en: Figure 9\. Confussion Matrix of our Perceptron
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图9. 我们感知机的混淆矩阵
- en: Measuring performance of our classifier
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量我们分类器的性能
- en: One way to measure how good a Perceptron (or any other classifier) is to evaluate
    its performance on a test dataset. There are several metrics that can be used
    to evaluate the performance of a classifier, such as accuracy, precision, recall,
    and F1 score.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 测量一个感知机（或任何其他分类器）表现的一个方法是评估其在测试数据集上的表现。可以用来评估分类器表现的几个指标有准确率、精确度、召回率和F1分数。
- en: Accuracy is the percentage of correct predictions made by the classifier. It
    is calculated as the number of correct predictions divided by the total number
    of predictions. However, accuracy can be misleading if the classes are imbalanced
    (i.e., one class is much more common than the other).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率是分类器做出正确预测的百分比。它是通过将正确预测的数量除以总预测数量来计算的。然而，当类别不平衡（即一个类别比另一个类别更常见）时，准确率可能会具有误导性。
- en: Recall is the percentage of positive cases that were correctly identified by
    the classifier. It is calculated as the number of true positive predictions divided
    by the total number of positive cases. Recall is particularly important in applications
    where it is important to minimize the number of false negatives (e.g., a cancer
    detector). In our example we get an 86%, which means that our predictor is missing
    14% of the malignant tumours.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率是分类器正确识别的正例的百分比。它是通过将真实正例预测的数量除以总正例数量来计算的。召回率在需要最小化假阴性数量的应用中尤为重要（例如癌症检测器）。在我们的例子中，我们获得了86%的召回率，这意味着我们的预测器遗漏了14%的恶性肿瘤。
- en: A confusion matrix is a table that shows the number of true positive, true negative,
    false positive, and false negative predictions made by a classifier. It is a useful
    tool for understanding the strengths and weaknesses of a classifier, and for comparing
    the performance of different classifiers.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵是一个表格，显示了分类器做出的真实正例、真实负例、假正例和假负例的预测数量。它是理解分类器优缺点以及比较不同分类器性能的有用工具。
- en: In a cancer detector, the confusion matrix is particularly important because
    it can help identify cases where the classifier is making incorrect predictions.
    For example, if the classifier is making a large number of false negatives (i.e.,
    it is missing a lot of cancer cases), it may be necessary to adjust the classifier
    or to gather more training data to improve its performance. On the other hand,
    if the classifier is making a large number of false positives (i.e., it is identifying
    a lot of benign cases as cancerous), it may be necessary to adjust the classifier
    to be more conservative in its predictions.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在癌症检测器中，混淆矩阵特别重要，因为它可以帮助识别分类器做出错误预测的情况。例如，如果分类器产生了大量的假阴性（即漏掉了很多癌症病例），可能需要调整分类器或收集更多的训练数据以提高其性能。另一方面，如果分类器产生了大量的假阳性（即将许多良性病例误判为癌症），可能需要调整分类器以使其在预测时更加保守。
- en: Summary
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This article provided a basic mathematical description of the Perceptron, a
    type of single-layer neural network that was developed in the 1950s. It explained
    the mathematics behind Perceptrons, including how they can be used to classify
    data into different categories.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章提供了对感知机的基本数学描述，感知机是一种在1950年代开发的单层神经网络。它解释了感知机背后的数学原理，包括它们如何用于将数据分类到不同的类别中。
- en: The Perceptron was applied to a well-known dataset in data science, the *Wisconsin
    Breast Cancer dataset* from 1995, and demonstrated how different metrics can be
    used to evaluate the performance of the classifier. The implementation of the
    Perceptron in Mathematica showed how easily these concepts can be represented
    in modern programming languages, and the different steps described in the implementation
    demonstrate how a classifier can be designed from scratch and its performance
    evaluated.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机被应用于一个在数据科学中广为人知的数据集，即1995年的*威斯康星乳腺癌数据集*，并演示了如何使用不同的指标来评估分类器的性能。感知机在Mathematica中的实现展示了如何在现代编程语言中轻松表示这些概念，实施中的不同步骤展示了如何从头设计分类器并评估其性能。
- en: While Perceptrons are no longer used in modern machine learning practice due
    to the development of more advanced neural network architectures, the article
    showed that they are still a valuable tool for understanding the fundamental principles
    of neural networks.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管感知器由于更先进的神经网络架构的出现而不再被现代机器学习实践使用，但文章显示它们仍然是理解神经网络基本原理的*有价值*工具。
- en: References
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] [https://news.cornell.edu/stories/2019/09/professors-perceptron-paved-way-ai-60-years-too-soon](https://news.cornell.edu/stories/2019/09/professors-perceptron-paved-way-ai-60-years-too-soon)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] [https://news.cornell.edu/stories/2019/09/professors-perceptron-paved-way-ai-60-years-too-soon](https://news.cornell.edu/stories/2019/09/professors-perceptron-paved-way-ai-60-years-too-soon)'
- en: '[2] [https://psycnet.apa.org/record/1959-09865-001](https://psycnet.apa.org/record/1959-09865-001)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[2] [https://psycnet.apa.org/record/1959-09865-001](https://psycnet.apa.org/record/1959-09865-001)'
- en: '[3] [https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic))
    | [https://archive-beta.ics.uci.edu/dataset/15/breast+cancer+wisconsin+original](https://archive-beta.ics.uci.edu/dataset/15/breast+cancer+wisconsin+original)
    (CC BY 4.0 license, see acknowledges).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[3] [https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic))
    | [https://archive-beta.ics.uci.edu/dataset/15/breast+cancer+wisconsin+original](https://archive-beta.ics.uci.edu/dataset/15/breast+cancer+wisconsin+original)
    (CC BY 4.0 许可，见致谢)。'
- en: Acknowledges
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 致谢
- en: 'The dataset used was made available by the UCI Machine Learning Repository.
    Dataset created by:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的数据集由UCI机器学习库提供。数据集由以下人员创建：
- en: 1\. Dr. William H. Wolberg, General Surgery Dept.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. Dr. William H. Wolberg，普通外科系。
- en: University of Wisconsin, Clinical Sciences Center
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 威斯康星大学，临床科学中心
- en: Madison, WI 53792
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 麦迪逊，WI 53792
- en: 2\. W. Nick Street, Computer Sciences Dept.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. W. Nick Street，计算机科学系。
- en: University of Wisconsin, 1210 West Dayton St., Madison, WI 53706
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 威斯康星大学，1210 West Dayton St.，麦迪逊，WI 53706
- en: 3\. Olvi L. Mangasarian, Computer Sciences Dept.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. Olvi L. Mangasarian，计算机科学系。
- en: University of Wisconsin, 1210 West Dayton St., Madison, WI 53706
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 威斯康星大学，1210 West Dayton St.，麦迪逊，WI 53706
- en: 'Donor: Nick Street.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 捐赠者：Nick Street。
- en: 'UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA:
    University of California, School of Information and Computer Science. ([https://archive.ics.uci.edu/ml/about.html](https://archive.ics.uci.edu/ml/about.html)
    / [https://archive.ics.uci.edu/ml/citation_policy.html](https://archive.ics.uci.edu/ml/citation_policy.html)).'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: UCI机器学习库 [http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)。加州欧文：加州大学信息与计算机科学学院。
    ([https://archive.ics.uci.edu/ml/about.html](https://archive.ics.uci.edu/ml/about.html)
    / [https://archive.ics.uci.edu/ml/citation_policy.html](https://archive.ics.uci.edu/ml/citation_policy.html))。
