- en: Fine-tune MPT-7B on Amazon SageMaker
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Amazon SageMaker 上微调 MPT-7B
- en: 原文：[https://towardsdatascience.com/fine-tune-mpt-7b-on-amazon-sagemaker-1e68e71051fa](https://towardsdatascience.com/fine-tune-mpt-7b-on-amazon-sagemaker-1e68e71051fa)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/fine-tune-mpt-7b-on-amazon-sagemaker-1e68e71051fa](https://towardsdatascience.com/fine-tune-mpt-7b-on-amazon-sagemaker-1e68e71051fa)
- en: '![](../Images/2f5691f2830e17d1af6c907a144f4567.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2f5691f2830e17d1af6c907a144f4567.png)'
- en: Photo by [Jeffery Ho](https://unsplash.com/@jefferyho?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 照片由 [Jeffery Ho](https://unsplash.com/@jefferyho?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)。
- en: Learn how to prepare a dataset and create a training job to fine-tune MPT-7B
    on Amazon SageMaker
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习如何准备数据集并创建一个训练任务，以便在 Amazon SageMaker 上微调 MPT-7B。
- en: '[](https://medium.com/@joao.pereira.abt?source=post_page-----1e68e71051fa--------------------------------)[![João
    Pereira](../Images/2946b185eb134ddfaa71cf5af5e3cda6.png)](https://medium.com/@joao.pereira.abt?source=post_page-----1e68e71051fa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1e68e71051fa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1e68e71051fa--------------------------------)
    [João Pereira](https://medium.com/@joao.pereira.abt?source=post_page-----1e68e71051fa--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@joao.pereira.abt?source=post_page-----1e68e71051fa--------------------------------)[![João
    Pereira](../Images/2946b185eb134ddfaa71cf5af5e3cda6.png)](https://medium.com/@joao.pereira.abt?source=post_page-----1e68e71051fa--------------------------------)[](https://towardsdatascience.com/?source=post_page-----1e68e71051fa--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----1e68e71051fa--------------------------------)
    [João Pereira](https://medium.com/@joao.pereira.abt?source=post_page-----1e68e71051fa--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1e68e71051fa--------------------------------)
    ·9 min read·Jun 20, 2023
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----1e68e71051fa--------------------------------)
    ·阅读时间 9 分钟·2023 年 6 月 20 日
- en: --
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: New large language models (LLMs) are being announced every week, each trying
    to beat its predecessor and take over the evaluation leaderboards. One of the
    latest models out there is [MPT-7B](https://www.mosaicml.com/blog/mpt-7b) that
    was released by MosaicML. Unlike other models of its kind, this 7-billion-parameter
    model is open-source and licensed for commercial use ([Apache 2.0 license](https://github.com/mosaicml/llm-foundry/blob/main/LICENSE))
    🚀.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 每周都会有新的大型语言模型（LLMs）被宣布，每个模型都试图超越其前任并占据评估排行榜的首位。其中最新的模型之一是 [MPT-7B](https://www.mosaicml.com/blog/mpt-7b)，由
    MosaicML 发布。与同类其他模型不同，这款 70 亿参数的模型是开源的，并且获得了商业使用的许可证 ([Apache 2.0 许可证](https://github.com/mosaicml/llm-foundry/blob/main/LICENSE))
    🚀。
- en: Foundation models like MPT-7B are pre-trained on datasets with trillions of
    tokens (100 tokens ~ 75 words) crawled from the web and, when prompted well, they
    can produce impressive outputs. However, to truly unlock the value of large language
    models in real-world applications, smart prompt-engineering might not be enough
    to make them work for your use case and, therefore, fine-tuning a foundation model
    on a domain-specific dataset is required.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 像 MPT-7B 这样的基础模型是在具有万亿个标记（100 个标记约等于 75 个单词）的数据集上进行预训练的，并且当提示得当时，它们能够生成令人印象深刻的输出。然而，要真正释放大型语言模型在实际应用中的价值，仅仅智能的提示工程可能不足以使其适用于你的用例，因此，需要在领域特定的数据集上对基础模型进行微调。
- en: LLMs have billions of parameters and, consequently, fine-tuning such large models
    is challenging. Good news is that fine-tuning is much cheaper and faster as compared
    to pre-training the foundation model given that 1) the domain-specific datasets
    are "small" and 2) fine-tuning requires only a few passes over the training data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）具有数十亿个参数，因此微调如此庞大的模型是具有挑战性的。好消息是，相比于预训练基础模型，微调的成本更低、速度更快，因为 1) 领域特定的数据集是“较小”的，2)
    微调只需对训练数据进行少量遍历。
- en: '***Here is what we will learn in this article:***'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '***在本文中我们将学习：***'
- en: How to create and structure a dataset for fine-tuning a large language model.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何创建和构建用于微调大型语言模型的数据集。
- en: What is and how to configure a distributed training job with fully sharded data
    parallel.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是完全分片数据并行的分布式训练作业以及如何配置它。
- en: How to define a 😊 HuggingFace estimator.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何定义一个 😊 HuggingFace 估算器。
- en: How to launch a training job in Amazon SageMaker that fine-tunes MPT-7B.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在 Amazon SageMaker 中启动一个微调 MPT-7B 的训练作业。
- en: 1\. Install dependencies and set S3 paths
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 安装依赖项并设置 S3 路径
- en: Let's start by installing the [SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk)
    and a few other packages. This SDK makes it possible to train and deploy machine
    learning models on AWS with a few lines of Python code. The code below is available
    in the `[sagemaker_finetuning.ipynb](https://github.com/jpcpereira/sagemaker-fine-tune-mpt-7b/blob/main/sagemaker_finetuning.ipynb)`notebook
    in Github. Run the notebook in SageMaker Studio, a SageMaker notebook instance,
    or in your laptop after [authenticating to an AWS account](https://docs.aws.amazon.com/signin/latest/userguide/command-line-sign-in.html).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先安装 [SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk)
    和其他一些包。这个 SDK 使得在 AWS 上训练和部署机器学习模型变得可能，只需几行 Python 代码。下面的代码可以在 Github 的 `[sagemaker_finetuning.ipynb](https://github.com/jpcpereira/sagemaker-fine-tune-mpt-7b/blob/main/sagemaker_finetuning.ipynb)`
    notebook 中找到。在 SageMaker Studio、SageMaker notebook 实例或在笔记本电脑上运行 notebook，前提是 [认证到
    AWS 账户](https://docs.aws.amazon.com/signin/latest/userguide/command-line-sign-in.html)。
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next step is to define the paths where the data will be saved in S3 and create
    a SageMaker session.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是定义数据将保存到 S3 的路径，并创建一个 SageMaker 会话。
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 2\. Build a fine-tuning dataset
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 构建微调数据集
- en: We will create a dummy dataset to demonstrate how to fine-tune MPT-7B. Since
    training models of this size on a complete dataset takes long and is costly, it
    is a good idea to first test & debug the training job on a small dataset and second
    scale training to the complete dataset.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个虚拟数据集来演示如何微调 MPT-7B。由于在完整数据集上训练此大小的模型需要较长时间且成本较高，因此首先在小数据集上测试和调试训练任务是一个好主意，其次将训练规模扩大到完整数据集。
- en: '**Format dataset as a list of dictionaries** — The dataset should be formatted
    as a list of dictionaries, where each example has a key-value structure, *e.g.*,'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**将数据集格式化为字典列表** — 数据集应格式化为字典列表，每个示例具有键值结构，*例如*，'
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `prompt` is the input given to the model (*e.g.*, a question). The `response`
    is the output that the model is trained to predict (*e.g.*, the answer to the
    question in the `prompt`). The raw prompt is often preprocessed to fit in a prompt
    template that helps the model to generate better outputs. Note that the model
    is trained for causal language modelling, so you can think of it as a "document
    completer". It is a good idea to design the prompt template in such a way that
    the model thinks that it is completing a document. Andrej Karpathy explains well
    this mechanism in his talk [*State of GPT*](https://youtu.be/bZQun8Y4L2A).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '`prompt` 是给模型的输入（*例如*，一个问题）。`response` 是模型训练预测的输出（*例如*，`prompt` 中问题的答案）。原始提示通常会经过预处理，以适应提示模板，这有助于模型生成更好的输出。请注意，模型是为因果语言建模训练的，因此可以把它看作是一个“文档完成器”。设计提示模板时，最好让模型觉得它正在完成一个文档。Andrej
    Karpathy 在他的演讲 [*State of GPT*](https://youtu.be/bZQun8Y4L2A) 中很好地解释了这一机制。'
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Upload the training and test data to S3** — Once the training and test sets
    are ready and formatted as a list of dictionaries, we upload them to S3 as JSON
    lines using the utility function below:'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**将训练和测试数据上传到 S3** — 一旦训练和测试集准备好并格式化为字典列表，我们将使用下面的工具函数将它们作为 JSON 行上传到 S3：'
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 3\. SageMaker Training job
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. SageMaker 训练任务
- en: 'With the datasets available in S3, we will now create a training job in Amazon
    SageMaker. For that, we have to create an entry point script, modify the configuration
    file specifying the training settings, and define an HuggingFace estimator. We
    will (re-)use the training script from [LLM Foundry](https://github.com/mosaicml/llm-foundry)
    and [Composer](https://github.com/mosaicml/composer) library’s CLI launcher that
    sets up the distributed training environment. Both of these packages are maintained
    by [MosaicML](https://www.mosaicml.com/), the company behind MPT-7B. The working
    folder should be structured like:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 S3 中可用的数据集，我们现在将在 Amazon SageMaker 中创建一个训练任务。为此，我们需要创建一个入口点脚本，修改配置文件以指定训练设置，并定义一个
    HuggingFace 估算器。我们将（重新）使用来自 [LLM Foundry](https://github.com/mosaicml/llm-foundry)
    和 [Composer](https://github.com/mosaicml/composer) 库的 CLI 启动器，这些启动器设置了分布式训练环境。这两个包均由
    [MosaicML](https://www.mosaicml.com/) 维护，该公司是 MPT-7B 的背后公司。工作文件夹应按如下结构组织：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We will now dive deep into each of these files.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将深入了解这些文件中的每一个。
- en: '**Create a configuration file** `[**finetuning_config.yaml**](https://github.com/jpcpereira/sagemaker-fine-tune-mpt-7b/blob/main/finetuning_config.yaml)`—
    The template provided in the [LLM Foundry](https://github.com/mosaicml/llm-foundry)
    repository is a good starting point, specifically the `[mpt-7b-dolly-sft.yaml](https://github.com/mosaicml/llm-foundry/blob/main/scripts/train/yamls/finetune/mpt-7b_dolly_sft.yaml)`
    file. However, depending on your dataset size and training instance, you might
    have to adjust some of these configurations, such as the batch size. I have modified
    the file to fine-tune the model in SageMaker (check `[finetuning_config.yaml](https://github.com/jpcpereira/sagemaker-fine-tune-mpt-7b/blob/main/finetuning_config.yaml)`).
    The parameters that you should pay attention to are the following:'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创建一个配置文件** `[**finetuning_config.yaml**](https://github.com/jpcpereira/sagemaker-fine-tune-mpt-7b/blob/main/finetuning_config.yaml)`—
    [LLM Foundry](https://github.com/mosaicml/llm-foundry) 仓库中提供的模板是一个很好的起点，特别是 `[mpt-7b-dolly-sft.yaml](https://github.com/mosaicml/llm-foundry/blob/main/scripts/train/yamls/finetune/mpt-7b_dolly_sft.yaml)`
    文件。然而，根据数据集大小和训练实例，您可能需要调整一些配置，例如批量大小。我已经修改了该文件以在 SageMaker 中微调模型（请查看 `[finetuning_config.yaml](https://github.com/jpcpereira/sagemaker-fine-tune-mpt-7b/blob/main/finetuning_config.yaml)`）。您需要关注的参数如下：'
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The`max_seq_length` indicates the maximum number of tokens of the input (remember
    that 100 tokens ~ 75 words). The training and test data will be loaded using the
    😊 [Datasets](https://huggingface.co/docs/datasets/index) library from the `/opt/ml/input/data/{train,
    test}` directory inside the container associated with the training job. Check
    out the [SageMaker Training Storage Folders](https://docs.aws.amazon.com/sagemaker/latest/dg/model-train-storage.html)’
    documentation to understand how the container directories are structured. The
    `max_duration` specifies the number of epochs for fine-tuning. Two to three epochs
    is typically a good choice. `eval_interval` indicates how often the model will
    be evaluated on the test set.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_seq_length` 指示输入的最大令牌数（记住 100 个令牌约等于 75 个单词）。训练和测试数据将使用 😊 [Datasets](https://huggingface.co/docs/datasets/index)
    库从容器内与训练任务关联的 `/opt/ml/input/data/{train, test}` 目录加载。查看 [SageMaker Training Storage
    Folders](https://docs.aws.amazon.com/sagemaker/latest/dg/model-train-storage.html)
    文档，以了解容器目录的结构。`max_duration` 指定微调的轮数。两到三轮通常是一个不错的选择。`eval_interval` 指示模型在测试集上的评估频率。'
- en: The distributed training strategy is Fully Sharded Data Parallel (FSDP), which
    enables efficient training of large models like MPT-7B. Unlike the traditional
    data parallel strategy, which keeps a copy of the model in each GPU, FSDP shards
    model parameters, optimizer states, and gradients across data parallel workers.
    If you want to learn more about FSDP, check this insightful [PyTorch intro post](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/).
    FSDP is integrated in [Composer](https://github.com/mosaicml/composer), the distributed
    training library used by [LLM Foundry](https://github.com/mosaicml/llm-foundry).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式训练策略是 Fully Sharded Data Parallel (FSDP)，这使得像 MPT-7B 这样的巨大模型的训练变得高效。与传统的数据并行策略不同，后者在每个
    GPU 中保留模型副本，FSDP 将模型参数、优化器状态和梯度在数据并行工作者之间进行分片。如果您想深入了解 FSDP，可以查看这篇有见地的 [PyTorch
    介绍文章](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)。FSDP
    已集成在 [Composer](https://github.com/mosaicml/composer) 中，这是 [LLM Foundry](https://github.com/mosaicml/llm-foundry)
    使用的分布式训练库。
- en: '`save_folder` determines where the model checkpoint (`.pt` file) is saved.
    We set it to the temporary folder `/tmp/checkpoints`.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`save_folder` 确定模型检查点（`.pt` 文件）保存的位置。我们将其设置为临时文件夹 `/tmp/checkpoints`。'
- en: '**Create the entry point script** `[**launcher.sh**](https://github.com/jpcpereira/sagemaker-fine-tune-mpt-7b/blob/main/launcher.sh)`—
    A bash script is used as entry point. The bash script clones the LLM Foundry repository,
    installs requirements, and, more importantly, runs the training script using Composer
    library''s distributed launcher. Note that, typically, training jobs in SageMaker
    run the training script using a command like `python train.py`. However, it is
    possible to pass a bash script as entry point, which provides more flexibility
    in our scenario. Finally, we convert the model checkpoint saved to `/tmp/checkpoints`
    to the HuggingFace model format and save the final artifacts into `/opt/ml/model/`.
    SageMaker will compress all files in this directory, create a tarball `model.tar.gz`,
    and upload it to S3\. The tarball is useful for inference.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创建入口脚本** `[**launcher.sh**](https://github.com/jpcpereira/sagemaker-fine-tune-mpt-7b/blob/main/launcher.sh)`—
    一个 bash 脚本作为入口点。这个 bash 脚本克隆了 LLM Foundry 仓库，安装了所需的依赖，并且更重要的是，使用 Composer 库的分布式启动器运行训练脚本。请注意，通常在
    SageMaker 中，训练作业是通过像 `python train.py` 这样的命令运行训练脚本。然而，在我们的场景中，可以将 bash 脚本作为入口点，这提供了更多的灵活性。最后，我们将保存到
    `/tmp/checkpoints` 的模型检查点转换为 HuggingFace 模型格式，并将最终的工件保存到 `/opt/ml/model/`。SageMaker
    会压缩此目录中的所有文件，创建一个 tarball `model.tar.gz`，并将其上传到 S3。这个 tarball 对于推理非常有用。'
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Define 😊 HuggingFace Estimator** — The Estimator sets the Docker container
    used to run the training job. We will use an image with PyTorch 2.0.0 and Python
    3.10\. The bash script and the configuration file are automatically uploaded to
    S3 and made available inside the container (handled by the SageMaker Python SDK).
    We set the training instance to`[g5.48xlarge](https://aws.amazon.com/ec2/instance-types/g5/)`
    that has 8x NVIDIA A10G GPUs. The `[p4d.24xlarge](https://aws.amazon.com/ec2/instance-types/p4/)`
    is also a good choice. Even though it is more expensive, it is equipped with 8x
    NVIDIA A100 GPUs. We also indicate the metrics to track on the training and test
    sets (Cross Entropy and Perplexity). The values of these metrics are captured
    via Regex expressions and sent to Amazon CloudWatch.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义 😊 HuggingFace 估算器** — 估算器设置了用于运行训练作业的 Docker 容器。我们将使用一个带有 PyTorch 2.0.0
    和 Python 3.10 的镜像。bash 脚本和配置文件会自动上传到 S3，并在容器内可用（由 SageMaker Python SDK 处理）。我们将训练实例设置为`[g5.48xlarge](https://aws.amazon.com/ec2/instance-types/g5/)`，它配备了
    8x NVIDIA A10G GPU。`[p4d.24xlarge](https://aws.amazon.com/ec2/instance-types/p4/)`
    也是一个不错的选择。虽然它更贵，但配备了 8x NVIDIA A100 GPU。我们还指明了要在训练集和测试集中跟踪的指标（交叉熵和困惑度）。这些指标的值通过正则表达式捕获并发送到
    Amazon CloudWatch。'
- en: '[PRE8]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ⚠️ Make sure to request the respective quotas for SageMaker Training, along
    with [Warm Pools](https://docs.aws.amazon.com/sagemaker/latest/dg/train-warm-pools.html)’
    quota in case you are making use of this cool feature. If you plan to run many
    jobs in SageMaker, take a look at [SageMaker Saving Plans](https://aws.amazon.com/savingsplans/ml-pricing/).
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⚠️ 确保请求 SageMaker 训练的相应配额，并在使用此酷炫功能时请求 [热池](https://docs.aws.amazon.com/sagemaker/latest/dg/train-warm-pools.html)
    的配额。如果你计划在 SageMaker 中运行多个作业，可以查看 [SageMaker 节省计划](https://aws.amazon.com/savingsplans/ml-pricing/)。
- en: '**Launch the training job 🚀** — We have all set to start the training job on
    Amazon SageMaker:'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**启动训练作业 🚀** — 我们已准备好在 Amazon SageMaker 上开始训练作业：'
- en: '[PRE9]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The training time will depend on the size of your dataset. With our dummy dataset,
    training takes roughly **20min** to complete. Once the model is trained and converted
    to 😊 HuggingFace format, SageMaker will upload the model tarball (`model.tar.gz`)
    to the S3 `output_path`. I found that in practice the uploading step takes rather
    long (>1h), which might be due to the size of the model artifacts to compress
    (~25GB).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 训练时间将取决于数据集的大小。使用我们的虚拟数据集，训练大约需要 **20min** 完成。一旦模型训练完成并转换为 😊 HuggingFace 格式，SageMaker
    将把模型 tarball (`model.tar.gz`) 上传到 S3 `output_path`。我发现实际上上传步骤花费的时间较长（>1h），这可能是由于模型工件压缩的大小（~25GB）。
- en: 4\. Summary
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 总结
- en: In this article, I showed how you can prepare a dataset and create a training
    job in SageMaker to fine-tune MPT-7B for your use case. The implementation leverages
    the training script from [LLM Foundry](https://github.com/mosaicml/llm-foundry)
    and uses [Composer](https://github.com/mosaicml/composer) library’s distributed
    training launcher. Once you have fine-tuned your model and want to deploy it,
    I recommend to check out the [blog posts by Philipp Schmid](https://www.philschmid.de/);
    there are plenty of examples on how to deploy LLMs in SageMaker. Have fun with
    your fine-tuned MPT-7B model! 🎉
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我展示了如何准备数据集并在 SageMaker 中创建训练任务，以微调 MPT-7B 以适应你的使用案例。该实现利用了来自 [LLM Foundry](https://github.com/mosaicml/llm-foundry)
    的训练脚本，并使用了 [Composer](https://github.com/mosaicml/composer) 库的分布式训练启动器。一旦你微调了你的模型并想要部署它，我建议查看
    [Philipp Schmid 的博客文章](https://www.philschmid.de/)；那里有很多关于如何在 SageMaker 中部署 LLM
    的示例。祝你玩得愉快，享受你的微调 MPT-7B 模型！🎉
- en: 'All the code used in this article is available in Github:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中使用的所有代码都可以在 Github 上找到：
- en: '[](https://github.com/jpcpereira/sagemaker-fine-tune-mpt-7b?source=post_page-----1e68e71051fa--------------------------------)
    [## GitHub - jpcpereira/sagemaker-fine-tune-mpt-7b'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[## GitHub - jpcpereira/sagemaker-fine-tune-mpt-7b](https://github.com/jpcpereira/sagemaker-fine-tune-mpt-7b?source=post_page-----1e68e71051fa--------------------------------)'
- en: github.com](https://github.com/jpcpereira/sagemaker-fine-tune-mpt-7b?source=post_page-----1e68e71051fa--------------------------------)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[github.com](https://github.com/jpcpereira/sagemaker-fine-tune-mpt-7b?source=post_page-----1e68e71051fa--------------------------------)'
- en: — João Pereira
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: — 乔昂·佩雷拉
- en: '*Thank you for reading. Hope this article helps you getting started with fine-tuning
    large language models like MPT-7B in Amazon SageMaker. If you would like to read
    my future articles, please* [*follow me*](https://medium.com/@joao.pereira.abt/subscribe)*.
    Feedback is highly appreciated! Leave a comment below if you have any questions
    or reach out to me directly* [***by email***](mailto:mail@joao-pereira.pt) *or
    in* [***LinkedIn***](https://www.linkedin.com/in/jpcpereira/)*.*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*感谢阅读。希望本文能帮助你入门在 Amazon SageMaker 中微调大型语言模型如 MPT-7B。如果你想阅读我未来的文章，请* [*关注我*](https://medium.com/@joao.pereira.abt/subscribe)*。非常感谢反馈！如果你有任何问题，请在下面留言，或直接通过*
    [***电子邮件***](mailto:mail@joao-pereira.pt) *或在* [***LinkedIn***](https://www.linkedin.com/in/jpcpereira/)*联系我。*'
