- en: Improving Performance and Explainability of Zero-Shot CLIP
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提高零-shot CLIP的性能和解释性
- en: 原文：[https://towardsdatascience.com/improving-performance-and-explainability-of-zero-shot-clip-33e579d3f4bb?source=collection_archive---------9-----------------------#2023-11-25](https://towardsdatascience.com/improving-performance-and-explainability-of-zero-shot-clip-33e579d3f4bb?source=collection_archive---------9-----------------------#2023-11-25)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/improving-performance-and-explainability-of-zero-shot-clip-33e579d3f4bb?source=collection_archive---------9-----------------------#2023-11-25](https://towardsdatascience.com/improving-performance-and-explainability-of-zero-shot-clip-33e579d3f4bb?source=collection_archive---------9-----------------------#2023-11-25)
- en: Part 2 — Visual classification via description from LLMs
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二部分 — 通过LLMs的描述进行视觉分类
- en: '[](https://medium.com/@alexml0123?source=post_page-----33e579d3f4bb--------------------------------)[![Alexey
    Kravets](../Images/3b31f9b3c73c6c7ca709f845e6f70023.png)](https://medium.com/@alexml0123?source=post_page-----33e579d3f4bb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----33e579d3f4bb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----33e579d3f4bb--------------------------------)
    [Alexey Kravets](https://medium.com/@alexml0123?source=post_page-----33e579d3f4bb--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@alexml0123?source=post_page-----33e579d3f4bb--------------------------------)[![Alexey
    Kravets](../Images/3b31f9b3c73c6c7ca709f845e6f70023.png)](https://medium.com/@alexml0123?source=post_page-----33e579d3f4bb--------------------------------)[](https://towardsdatascience.com/?source=post_page-----33e579d3f4bb--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----33e579d3f4bb--------------------------------)
    [Alexey Kravets](https://medium.com/@alexml0123?source=post_page-----33e579d3f4bb--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcf3e4a05b535&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-performance-and-explainability-of-zero-shot-clip-33e579d3f4bb&user=Alexey+Kravets&userId=cf3e4a05b535&source=post_page-cf3e4a05b535----33e579d3f4bb---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----33e579d3f4bb--------------------------------)
    ·6 min read·Nov 25, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F33e579d3f4bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-performance-and-explainability-of-zero-shot-clip-33e579d3f4bb&user=Alexey+Kravets&userId=cf3e4a05b535&source=-----33e579d3f4bb---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fcf3e4a05b535&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-performance-and-explainability-of-zero-shot-clip-33e579d3f4bb&user=Alexey+Kravets&userId=cf3e4a05b535&source=post_page-cf3e4a05b535----33e579d3f4bb---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----33e579d3f4bb--------------------------------)
    ·6分钟阅读·2023年11月25日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F33e579d3f4bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-performance-and-explainability-of-zero-shot-clip-33e579d3f4bb&user=Alexey+Kravets&userId=cf3e4a05b535&source=-----33e579d3f4bb---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F33e579d3f4bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-performance-and-explainability-of-zero-shot-clip-33e579d3f4bb&source=-----33e579d3f4bb---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F33e579d3f4bb&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fimproving-performance-and-explainability-of-zero-shot-clip-33e579d3f4bb&source=-----33e579d3f4bb---------------------bookmark_footer-----------)'
- en: This is the second part of a series on enhancing Zero-Shot CLIP performance.
    In the first part, I provided a detailed explanation of how the CLIP model operates
    and described a straightforward method to improve its performance. This involved
    extending standard prompts like *“A picture of {class}”* with customized prompts
    generated by a large language model (LLM). If you haven’t already, you can find
    part 1 [here](https://medium.com/towards-data-science/simple-way-of-improving-zero-shot-clip-performance-4eae474cb447).
    In this article we will present a relatively similar method to improve zero-shot
    CLIP performance which is additionally highly explainable.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这是关于提升零-shot CLIP 性能系列文章的第二部分。在第一部分中，我详细解释了 CLIP 模型的操作原理，并描述了一种简单的方法来提高其性能。这涉及到通过大型语言模型（LLM）生成的自定义提示，扩展标准提示，如*“{class}的图片”*。如果你还没有阅读第一部分，可以在[这里](https://medium.com/towards-data-science/simple-way-of-improving-zero-shot-clip-performance-4eae474cb447)找到。在本文中，我们将介绍一种相对类似的方法来提高零-shot
    CLIP 性能，并且这种方法具有高度的可解释性。
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: The CLIP model is an impressive zero-shot predictor, enabling predictions on
    tasks it hasn’t explicitly been trained for. Despite its inherent capabilities,
    there exist several strategies to notably improve its performance. In the first
    article we have seen one of these strategies, however, while achieving enhanced
    performance is valuable, there are instances where we might be willing to make
    trade-offs to prioritize better explainability. In this second article of our
    series we will explore a method that not only enhances the performance of the
    zero-shot CLIP model but also ensures that its predictions are easily understandable
    and interpretable.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP 模型是一种令人印象深刻的零-shot 预测器，能够对其没有明确训练过的任务进行预测。尽管它具有内在的能力，但仍然存在若干策略可以显著提高其性能。在第一篇文章中，我们已经看到其中一种策略。然而，虽然提升性能是有价值的，但在某些情况下，我们可能愿意做出权衡，以优先考虑更好的可解释性。在系列的第二篇文章中，我们将探讨一种方法，这种方法不仅提升了零-shot
    CLIP 模型的性能，还确保其预测结果易于理解和解释。
- en: Explainability in Deep Neural Networks
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度神经网络的可解释性
