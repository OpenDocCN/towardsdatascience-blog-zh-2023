- en: Why Do We Have Huge Language Models and Small Vision Transformers?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么我们拥有庞大的语言模型而视觉变换器却很小？
- en: 原文：[https://towardsdatascience.com/why-do-we-have-huge-language-models-and-small-vision-transformers-5d59ac36c1d6](https://towardsdatascience.com/why-do-we-have-huge-language-models-and-small-vision-transformers-5d59ac36c1d6)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/why-do-we-have-huge-language-models-and-small-vision-transformers-5d59ac36c1d6](https://towardsdatascience.com/why-do-we-have-huge-language-models-and-small-vision-transformers-5d59ac36c1d6)
- en: Artificial intelligence | computer vision | Vision Transformers
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人工智能 | 计算机视觉 | 视觉变换器
- en: Google ViT-22 paves the way for new large transformers and to revolutionize
    computer vision
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Google ViT-22 为新的大型变换器铺平了道路，并彻底改变了计算机视觉领域
- en: '[](https://salvatore-raieli.medium.com/?source=post_page-----5d59ac36c1d6--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----5d59ac36c1d6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5d59ac36c1d6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5d59ac36c1d6--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----5d59ac36c1d6--------------------------------)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://salvatore-raieli.medium.com/?source=post_page-----5d59ac36c1d6--------------------------------)[![Salvatore
    Raieli](../Images/6bb4520e2df40d20283e7283141b5e06.png)](https://salvatore-raieli.medium.com/?source=post_page-----5d59ac36c1d6--------------------------------)[](https://towardsdatascience.com/?source=post_page-----5d59ac36c1d6--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----5d59ac36c1d6--------------------------------)
    [Salvatore Raieli](https://salvatore-raieli.medium.com/?source=post_page-----5d59ac36c1d6--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5d59ac36c1d6--------------------------------)
    ·9 min read·Feb 17, 2023
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ·发表于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----5d59ac36c1d6--------------------------------)
    ·阅读时间 9 分钟·2023年2月17日
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/e70451dc343d2e3cbc3ea2e6b5bbd3ea.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e70451dc343d2e3cbc3ea2e6b5bbd3ea.png)'
- en: image by [Joshua Earle](https://unsplash.com/@joshuaearle) at unsplash.com
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Joshua Earle](https://unsplash.com/@joshuaearle) 提供，来源于 unsplash.com
- en: In recent years we have seen a growth in the number of transformer parameters.
    Looking closely though, these are Language Models (LMs), up to an incredible [540
    B of parameters](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html).
    **Why not for visual models?**
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最近几年，我们看到变换器参数数量的增长。然而仔细观察，这些主要是语言模型（LMs），其参数量高达令人惊叹的 [540B](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html)。**那么视觉模型为什么没有类似增长呢？**
- en: As for text models, an increase in dataset size, scalable architectures, and
    new training methods have enabled this growth in the number of parameters. This
    has not only served to increase performance in particular tasks (classification
    and so on), **but as the number of parameters has grown, we have seen emerging
    capabilities.**
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对于文本模型来说，数据集规模的增加、可扩展的架构以及新的训练方法使得参数数量得以增长。这不仅提高了特定任务（分类等）的性能，**而且随着参数数量的增加，我们看到了新兴的能力。**
- en: '![](../Images/57b77e6e745a2ea296895080a8738106.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/57b77e6e745a2ea296895080a8738106.png)'
- en: '“Trend of sizes of state-of-the-art Natural Language Processing (NLP) models
    with time. The number of floating-point operations to train these models is increasing
    at an exponential rate”. source: [here](https://arxiv.org/abs/2104.04473)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: “最先进的自然语言处理（NLP）模型的规模随时间的变化趋势。这些模型的训练所需的浮点运算数量以指数级速度增长。” 来源：[这里](https://arxiv.org/abs/2104.04473)
- en: In addition, a large model can be used as a basis for [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning)
    and fine-tuning, so there is interest in developing increasingly high-performance
    models. As much as LMs have been used successfully in a number of tasks, there
    are many other tasks where a model capable of image analysis is needed.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，大型模型可以作为 [迁移学习](https://en.wikipedia.org/wiki/Transfer_learning) 和微调的基础，因此对开发性能越来越高的模型充满兴趣。尽管
    LMs 在许多任务中取得了成功，但仍有许多其他任务需要能够进行图像分析的模型。
- en: As of 2016, the [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))
    is the architecture of choice, and the use of [self-attention](https://en.wikipedia.org/wiki/Attention_(machine_learning))
    has shown obvious advantages. Therefore, several groups have trained transformer
    models capable of working with images (vision transformer, ViT). **So far, the
    widest ViT has only 15 B parameters. why?**
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 截至2016年，[transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model))是首选架构，而[自注意力](https://en.wikipedia.org/wiki/Attention_(machine_learning))的使用显示了明显的优势。因此，多个团队训练了能够处理图像的变换器模型（视觉变换器，ViT）。**到目前为止，最宽的ViT只有15
    B参数。为什么？**
- en: In a new study, Google was able to train a 22 B model of parameters and understand
    why there is difficulty in scaling ViTs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在一项新研究中，谷歌成功训练了一个22 B参数的模型，并理解了为何在扩展ViTs时存在困难。
- en: '**In summary:**'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**总结：**'
- en: They explain why the traditional method of training ViTs produces instability
    during scaling.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们解释了为何传统的ViTs训练方法在扩展时会产生不稳定性。
- en: How to modify the architecture for scaling, and how the model reaches the state
    of the art.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何修改架构以进行扩展，以及模型如何达到最新水平。
- en: How improving fairness when scaling the model
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在扩展模型时提高公平性
- en: '**What are vision transformers?**'
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**什么是视觉变换器？**'
- en: '![](../Images/0da782113c539e2c89f97d130055d58d.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0da782113c539e2c89f97d130055d58d.png)'
- en: Image from Wikipedia ([source](https://en.wikipedia.org/wiki/Vision_transformer))
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自维基百科 ([source](https://en.wikipedia.org/wiki/Vision_transformer))
- en: Transformers are of course permutation invariant, however, they cannot process
    grid-structured data (only sequences). So, we have to in order to use a Transformer
    with images we have to find a way to transform them into sequences. **How?**
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器当然是排列不变的，但它们不能处理网格结构的数据（只能处理序列）。因此，为了将变换器与图像一起使用，我们必须找到将它们转换为序列的方法。**怎么做？**
- en: The first step is to transform the images into a sequence of patches (the image
    is divided into a series of fragments called patches). These patches are basically
    our tokens (like words in the classic transformer). These images are then flattened
    and transformed into lower-dimensional embedding (this preserves the information
    but reduces the number of dimensions). Also, as in the original transformer, we
    use positional encoding so the model knows the position of the patch in the image.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是将图像转换为一系列的图块（图像被分成一系列称为图块的碎片）。这些图块基本上是我们的令牌（就像经典变换器中的单词）。这些图像随后被展平并转换为低维嵌入（这保留了信息但减少了维度）。此外，和原始变换器一样，我们使用位置编码，以便模型知道图块在图像中的位置。
- en: '![](../Images/fff9926c269163af2d81f73c5de4a0ba.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fff9926c269163af2d81f73c5de4a0ba.png)'
- en: original article describing ViT ([arXiv prepint](https://arxiv.org/pdf/2010.11929.pdf))
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 描述ViT的原始文章 ([arXiv prepint](https://arxiv.org/pdf/2010.11929.pdf))
- en: The model then is trained in supervised learning (a huge dataset of which we
    have image labels) and then can be used for downstream tasks.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 模型随后在监督学习中进行训练（我们有图像标签的大型数据集），然后可以用于下游任务。
- en: '[](https://pub.towardsai.net/a-visual-journey-in-what-vision-transformers-see-9db9c8ba62d4?source=post_page-----5d59ac36c1d6--------------------------------)
    [## A Visual Journey in What Vision-Transformers See'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://pub.towardsai.net/a-visual-journey-in-what-vision-transformers-see-9db9c8ba62d4?source=post_page-----5d59ac36c1d6--------------------------------)
    [## 视觉变换器的视觉旅程'
- en: How some of the largest models see the world
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一些最大模型如何看待世界
- en: pub.towardsai.net](https://pub.towardsai.net/a-visual-journey-in-what-vision-transformers-see-9db9c8ba62d4?source=post_page-----5d59ac36c1d6--------------------------------)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[pub.towardsai.net](https://pub.towardsai.net/a-visual-journey-in-what-vision-transformers-see-9db9c8ba62d4?source=post_page-----5d59ac36c1d6--------------------------------)'
- en: Why is hard to scale them and how to solve it?
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么难以扩展它们以及如何解决？
- en: Before ViTs, convolutional networks were the standard for computer vision tasks.
    In “[A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545)” the authors note
    that the issue is still open and current.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在ViTs之前，卷积网络是计算机视觉任务的标准。在“[2020年代的ConvNet](https://arxiv.org/abs/2201.03545)”中，作者指出这个问题仍然存在且很重要。
- en: On the other hand, though, we have not been able to scale up ViTs. Since in
    transformers scaling the model also leads to the emergence of behaviors that cannot
    be imagined in advance, this is a serious problem.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我们还未能扩展ViTs。由于在变换器中，模型的扩展也会导致无法预先想象的行为的出现，这是一个严重的问题。
- en: '**The authors noted that over 8 B of parameters**, instability emerged as divergent
    training loss after a few thousand steps. This was caused “by extremely large
    values in attention logits, which lead to (almost one-hot) attention weights with
    near-zero entropy.” **To solve this, the authors added a layer-normalization to
    the Queries and Keys before the dot-product.**'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**作者指出，超过 80 亿参数时**，不稳定性表现为几千步后出现发散的训练损失。这是由于“注意力 logits 中的极大值导致几乎是 one-hot
    的注意力权重，且熵接近零。” **为解决此问题，作者在点积前为 Queries 和 Keys 添加了层归一化。**'
- en: In the figure, they show how this expedient improves training.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图中展示了这种措施如何改善训练。
- en: '![](../Images/46b59c54b596fea99aa96126e0fa2602.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/46b59c54b596fea99aa96126e0fa2602.png)'
- en: '(source: [here](https://arxiv.org/pdf/2302.05442.pdf))'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '(来源: [这里](https://arxiv.org/pdf/2302.05442.pdf))'
- en: '**The second expedient is to modify the architecture.** In the classical transformer
    after the self-attention output is followed by a [multi-layer-perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron)
    (MLP). Instead, here the self-attention blocks are in parallel with the MLP. This
    operation does not degrade performance while speeding up the training by 15 %
    (as shown with PaLM, another large Google model, this operation is basically combining
    matrix multiplications in a single operation).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**第二种措施是修改架构。** 在经典的 transformer 中，自注意力输出后跟随一个[多层感知机](https://en.wikipedia.org/wiki/Multilayer_perceptron)（MLP）。而在这里，自注意力块与
    MLP 并行。这种操作不会降低性能，同时将训练速度提高了 15%（如 PaLM 这类其他大型谷歌模型所示，该操作基本上是将矩阵乘法合并为单个操作）。'
- en: In addition, the bias term is removed for attention projection (this also reduces
    the training time without reducing performance).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，注意力投影中移除了偏置项（这也减少了训练时间而不降低性能）。
- en: 'In the figure, the new block after these expedients:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图中展示了这些措施之后的新块：
- en: '![](../Images/72f7205f412233cfb8f97cd3fb6fb19f.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/72f7205f412233cfb8f97cd3fb6fb19f.png)'
- en: '(source: [here](https://arxiv.org/pdf/2302.05442.pdf))'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '(来源: [这里](https://arxiv.org/pdf/2302.05442.pdf))'
- en: The table compares Google’s model (ViT-22) with the previously reported largest
    ViT models, ViT-G and ViT-e.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 表格比较了谷歌的模型（ViT-22）与之前报告的最大 ViT 模型，ViT-G 和 ViT-e。
- en: '![](../Images/08b0c0b8453ad27fa65953179f7de4be.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08b0c0b8453ad27fa65953179f7de4be.png)'
- en: '(source: [here](https://arxiv.org/pdf/2302.05442.pdf))'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '(来源: [这里](https://arxiv.org/pdf/2302.05442.pdf))'
- en: Training has also been optimized. Google used JAX (Google has been [focusing
    more on JAX](https://www.businessinsider.com/facebook-pytorch-beat-google-tensorflow-jax-meta-ai-2022-6)
    than TensorFlow for some time). They also used a number of tricks (asynchronous
    parallel linear operations, parameter sharding) to ensure the model was optimized
    for [TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 训练也已得到优化。谷歌使用了 JAX（谷歌在某段时间内[更加专注于 JAX](https://www.businessinsider.com/facebook-pytorch-beat-google-tensorflow-jax-meta-ai-2022-6)而非
    TensorFlow）。他们还使用了一些技巧（异步并行线性操作、参数分片）以确保模型优化适用于[TPU](https://en.wikipedia.org/wiki/Tensor_Processing_Unit)。
- en: The authors used a dataset of about 4 B images, which were semi-automatically
    annotated with 30,000 classes. As a gentle reminder, in a ViT the images are divided
    into several sections (called patches) which are then together with position (positional
    encoding) transformed into a sequence. Each image (224 x 224) is divided into
    14 x 14 patches. So an image is eventually represented by 256 tokens.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用了大约 40 亿张图像的数据集，这些图像半自动标注了 30,000 个类别。温馨提示，在 ViT 中，图像被划分为若干部分（称为补丁），然后与位置编码一起转化为序列。每张图像（224
    x 224）被划分为 14 x 14 个补丁。因此，一张图像最终由 256 个标记表示。
- en: Does scaling the ViT worth it?
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展 ViT 是否值得？
- en: Once the model was trained, they tested it on [ImageNet](https://www.image-net.org/)
    (1 M images and 1000 classes) to test its classification ability. The authors
    show, that the frozen model (i.e., without the need for fine-tuning) has comparable
    performance to other models.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练完成后，他们在[ImageNet](https://www.image-net.org/)（100 万图像和 1000 类）上进行了测试，以检验其分类能力。作者表明，冻结模型（即无需微调）的性能与其他模型相当。
- en: '![](../Images/75707b112c852e613c38010d65233623.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/75707b112c852e613c38010d65233623.png)'
- en: '(source: [here](https://arxiv.org/pdf/2302.05442.pdf))'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '(来源: [这里](https://arxiv.org/pdf/2302.05442.pdf))'
- en: Moreover, the model has been tested on another dataset using different image
    resolutions. **ViT-22B leads to significant accuracy improvement especially when
    the input size is small.**
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，该模型已在不同图像分辨率的另一数据集上进行了测试。**ViT-22B 在输入尺寸较小时，显著提高了准确性。**
- en: '![](../Images/03e6d9af221a76bcf308712196d3ca2c.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03e6d9af221a76bcf308712196d3ca2c.png)'
- en: '(source: [here](https://arxiv.org/pdf/2302.05442.pdf))'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：[这里](https://arxiv.org/pdf/2302.05442.pdf)）
- en: 'On the other hand, one of the most frequent uses of large models is transfer
    learning. After all, people often work with small datasets and use a large model
    for fine-tuning and for a different task than the one in which it was trained.
    In the authors’ words:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，大型模型的一个最常见的用途是迁移学习。毕竟，人们通常使用小数据集，并利用大型模型进行微调，用于与训练任务不同的任务。正如作者所说：
- en: 'Transfer learning for dense prediction is critical especially since obtaining
    pixel-level labels can be costly. In this section, we investigate the quality
    of captured geometric and spatial information by the ViT-22B model (trained using
    image-level classification objective) on semantic segmentation and monocular depth
    estimation tasks. (source: [here](https://arxiv.org/pdf/2302.05442.pdf))'
  id: totrans-56
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对于密集预测，迁移学习至关重要，特别是因为获得像素级标签可能很昂贵。在本节中，我们研究了ViT-22B模型（使用图像级分类目标进行训练）在语义分割和单目深度估计任务中捕捉的几何和空间信息的质量。
    （来源：[这里](https://arxiv.org/pdf/2302.05442.pdf)）
- en: The authors tested the model with three benchmark datasets for semantic segmentation
    (ADEK20k, Pascal Context, Pascal VOC). Not only that but they tested the model
    using a limited amount of data for transfer.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用了三个语义分割基准数据集（ADEK20k、Pascal Context、Pascal VOC）对模型进行了测试。不仅如此，他们还使用了有限的数据量进行迁移测试。
- en: '![](../Images/739dbc1d99d1ef8afb2ec668e527b489.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/739dbc1d99d1ef8afb2ec668e527b489.png)'
- en: '“Fewshot semantic segmentation on ADE20k, when only a fraction of the training
    set is used. We report mean IoU for semantic segmentation on the validation set”
    (source: [here](https://arxiv.org/pdf/2302.05442.pdf))'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: “在ADE20k上进行少样本语义分割，仅使用了训练集的一部分。我们报告了验证集上的语义分割平均IoU” （来源：[这里](https://arxiv.org/pdf/2302.05442.pdf)）
- en: ViT-22 has the best performance when there is little data, which is useful because
    often getting images and their segmentation mask is expensive, so fewer examples
    are needed than the other models.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据较少时，ViT-22表现最佳，这非常有用，因为获取图像及其分割掩码通常很昂贵，因此比其他模型需要的样本更少。
- en: In addition, the model showed superior capabilities in monocular depth estimation
    on the Waymo Open dataset.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，该模型在Waymo Open数据集上显示出了卓越的单目深度估计能力。
- en: '![](../Images/27131d454d774d155d89a525d1d54d9d.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/27131d454d774d155d89a525d1d54d9d.png)'
- en: 'Monocular depth estimation from frozen ViT features using different decoders
    on the Waymo Open dataset. (source: [here](https://arxiv.org/pdf/2302.05442.pdf))'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用不同解码器对Waymo Open数据集中的冻结ViT特征进行单目深度估计。 （来源：[这里](https://arxiv.org/pdf/2302.05442.pdf)）
- en: In addition, by repurposing the model (but keeping the pre-trained ViT-22 as
    a component) it was possible to use it for video classification. **This demonstrates
    the plasticity of using the model for various possible tasks.**
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过重新利用模型（但保留预训练的ViT-22作为组件），可以将其用于视频分类。**这展示了模型在各种可能任务中的灵活性。**
- en: 'In addition, the authors showed that fine-tuning is capable of improving performance:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，作者展示了微调能够提升性能：
- en: '![](../Images/1c61c29d70cf7393915f42c9642cf7fa.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1c61c29d70cf7393915f42c9642cf7fa.png)'
- en: 'Video classification results. We evaluate the ViT-22B representations by freezing
    the backbone, and training a small transformer to aggregate frozen, per-frame
    representations. ViT-22B outperforms the largest previous vision backbone, ViT-e
    which contains 4 billion parameters. (source: [here](https://arxiv.org/pdf/2302.05442.pdf))'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 视频分类结果。我们通过冻结骨干网络来评估ViT-22B的表示，并训练一个小型变换器来汇聚冻结的逐帧表示。ViT-22B的表现超越了包含40亿参数的最大前视图骨干网络ViT-e。
    （来源：[这里](https://arxiv.org/pdf/2302.05442.pdf)）
- en: How fair is this model?
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这个模型的公平性如何？
- en: Artificial intelligence models are susceptible to unintended bias. Many of these
    biases are present in the training dataset and the model can amplify, and learn
    spurious correlation and error disparities. **Because pre-trained models are then
    used for subsequent tasks, errors are carried along.**
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能模型容易受到意外偏见的影响。这些偏见许多存在于训练数据集中，模型可以放大这些偏见，学习虚假的关联和错误的差异。**由于预训练模型随后被用于后续任务，错误会被带到新的任务中。**
- en: The authors argue that scaling the model can serve to mitigate these biases
    and decided to test this by using demographic parity (DP) as a measure of fairness.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 作者认为，扩大模型规模可以帮助缓解这些偏见，并决定通过使用人口统计平等（DP）作为公平性的衡量标准来进行测试。
- en: 'The authors explain their approach:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 作者解释了他们的方法：
- en: 'We use CelebA (Liu et al., 2015) with binary gender as a sensitive attribute
    while the target is “attractive” or “smiling”. We emphasize that such experiments
    are carried out only to verify technical claims and shall by no means be interpreted
    as an endorsement of such vision-related tasks. We choose the latter attributes
    because they exhibit gender related bias as shown in Figure 15\. (source: [here](https://arxiv.org/pdf/2302.05442.pdf))'
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们使用CelebA（Liu等，2015）中的二元性别作为敏感属性，而目标是“吸引人”或“微笑”。我们强调，这种实验仅用于验证技术声明，绝不应被解释为对这种视觉相关任务的支持。我们选择后者属性，因为它们如图15所示展现了与性别相关的偏差。（来源：[here](https://arxiv.org/pdf/2302.05442.pdf)）
- en: '![](../Images/fccc11c2f66497bb971f1aeb4a1b8e77.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fccc11c2f66497bb971f1aeb4a1b8e77.png)'
- en: '“DP in the model often reflects DP in the data in the absence of bias mitigation.
    In this figure, binary sex is the sensitive attribute and linear heads are trained
    to predict other attributes in CelebA using pretrained features.” (source: [here](https://arxiv.org/pdf/2302.05442.pdf))'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: “模型中的DP通常反映了数据中的DP，除非采取偏见缓解措施。在此图中，二元性别是敏感属性，线性头被训练用于预测CelebA中的其他属性，使用预训练特征。”（来源：[here](https://arxiv.org/pdf/2302.05442.pdf)）
- en: Scaling the model, as described in the literature, offers a more favorable tradeoff
    (“performance improves with scale subject to any prescribed level of bias constraint”).
    Second, all subgroups benefit from the improvement, and scaling reduces the disparity
    in performance across the subgroups.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如文献所述，扩展模型提供了更有利的权衡（“性能随着规模的增加而改善，前提是任何规定的偏见约束水平”）。其次，所有子组都受益于这种改善，并且扩展减少了子组之间的性能差距。
- en: '![](../Images/25cd25363402ccecf5c8e48343a59e54.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/25cd25363402ccecf5c8e48343a59e54.png)'
- en: '“top: Accuracy (ACC) for ViT variants after debiasing for each DP level. middle:
    Accuracy for each subgroup in CelebA prior to debiasing. bottom: y-axis is absolute
    difference in performance across the two subgroups: females and males. ViT-22B
    provides a more equitable performance, compared to smaller ViT architectures.”
    (source: [here](https://arxiv.org/pdf/2302.05442.pdf))'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: “上图：在每个DP水平上，ViT变体去偏倚后的准确率（ACC）。中图：去偏倚前CelebA中每个子组的准确率。下图：y轴是两个子组（女性和男性）之间的性能绝对差异。与较小的ViT架构相比，ViT-22B提供了更公平的性能。”（来源：[here](https://arxiv.org/pdf/2302.05442.pdf)）
- en: What does the model see?
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型看到什么？
- en: As described earlier, computer vision models focus primarily on texture, while
    humans rely more on shapes.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，计算机视觉模型主要关注纹理，而人类则更多依赖于形状。
- en: 'Humans are at 96% shape / 4% texture bias and ViT-22B-384 achieves a previously
    unseen 87% shape bias / 13% texture bias. (source: [here](https://arxiv.org/pdf/2302.05442.pdf))'
  id: totrans-80
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 人类的形状偏差为96% / 纹理偏差为4%，而ViT-22B-384实现了前所未见的87%形状偏差 / 13%纹理偏差。（来源：[here](https://arxiv.org/pdf/2302.05442.pdf)）
- en: The result is very interesting because most models have 20–30 % shape bias and
    70–80 % texture bias (and this is also true for convolutional networks). This
    bias is also one of the reasons why by varying the texture of the image even if
    the shape is recognizable, a model can be tricked to misinterpret the image and
    mislabel it.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果非常有趣，因为大多数模型的形状偏差为20-30% 和纹理偏差为70-80%（卷积网络也如此）。这种偏差也是为什么通过改变图像的纹理，即使形状可识别，模型也可能被欺骗，错误解读和标记图像的原因之一。
- en: '![](../Images/e4c98e275488190cdbb77ce72acdbb4e.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e4c98e275488190cdbb77ce72acdbb4e.png)'
- en: '“Shape bias: many vision models have a low shape / high texture bias, whereas
    ViT-22B fine-tuned on ImageNet (red, green, blue trained on 4B images as indicated
    by brackets after model names, unless trained on ImageNet only) have the highest
    shape bias recorded in a ML model to date, bringing them closer towards a human-like
    shape bias.” (source: [here](https://arxiv.org/pdf/2302.05442.pdf))'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: “形状偏差：许多视觉模型具有低形状/高纹理偏差，而在ImageNet上微调的ViT-22B（如模型名称后的括号所示，红色、绿色、蓝色训练于40亿张图像，除非仅在ImageNet上训练）拥有迄今为止记录的最高形状偏差，使其更接近于人类的形状偏差。”（来源：[here](https://arxiv.org/pdf/2302.05442.pdf)）
- en: In addition, another way to understand what the model sees is to obtain saliency
    maps (gradient-based feature attribution methods).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，理解模型所见的另一种方法是获取显著性图（基于梯度的特征归因方法）。
- en: '![](../Images/9cf2f5d8beeb5d76b0e1bd51ed43c6b5.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9cf2f5d8beeb5d76b0e1bd51ed43c6b5.png)'
- en: 'Saliency before and after model cooldown (source: [here](https://arxiv.org/pdf/2302.05442.pdf))'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 模型冷却前后的显著性（来源：[here](https://arxiv.org/pdf/2302.05442.pdf)）
- en: Conclusions
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Google unveiled a model that is more than 5 times the previous model of ViTs.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Google推出了一款比之前的ViT模型大5倍以上的模型。
- en: 'We presented ViT-22B, the currently largest vision transformer model at 22
    billion parameters. We show that with small, but critical changes to the original
    architecture, we can achieve both excellent hardware utilization and training
    stability, yielding a model that advances the SOTA on several benchmarks. (source:
    [here](https://arxiv.org/pdf/2302.05442.pdf))'
  id: totrans-89
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们展示了ViT-22B，这是一种当前最大规模的视觉变换器模型，具有220亿个参数。我们展示了通过对原始架构进行小但关键的修改，我们可以实现优秀的硬件利用率和训练稳定性，从而获得一个在多个基准测试中推动SOTA的模型。（来源：[这里](https://arxiv.org/pdf/2302.05442.pdf)）
- en: Beyond the size and results in the benchmarks, this model is a starting point
    for much larger models. In fact, before now succeeding in scaling a ViT model
    was difficult because of instability during training. The authors showed that
    these problems can be solved by modifying the architecture.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 除了模型的规模和基准测试结果之外，这一模型也是更大模型的起点。事实上，在此之前，成功地扩展 ViT 模型很困难，因为训练期间的不稳定性。作者们展示了通过修改架构可以解决这些问题。
- en: Large models can be used as pre-trained scaffolds for different tasks ( computer
    vision models can be used in many real-world tasks). In addition, [unexpected
    behaviors emerge](https://arxiv.org/pdf/2206.07682.pdf) (which are not present
    in small models and cannot be predicted by the scaling law). Moreover, as shown
    these models can then integrate into multi-modal models (and [could influence
    emergent behaviors in them](https://twitter.com/YiTayML/status/1625205983239880704?s=20&t=_W_AqpJHeJgJ_Af32Av5Jw)).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 大模型可以用作不同任务的预训练支架（计算机视觉模型可以用于许多现实世界的任务）。此外，[意外行为出现](https://arxiv.org/pdf/2206.07682.pdf)（这些行为在小模型中不存在，且无法通过缩放法则预测）。而且，正如所示，这些模型随后可以整合到多模态模型中（并且[可能影响其中的突现行为](https://twitter.com/YiTayML/status/1625205983239880704?s=20&t=_W_AqpJHeJgJ_Af32Av5Jw)）。
- en: In addition, ViT-22B shows how scaling has improved in terms of fairness. This
    model is also more robust and more aligned with human vision (less dependent on
    texture and more on the shape).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，ViT-22B 显示了在公平性方面的改进。这一模型也更加稳健，更符合人类视觉（不那么依赖纹理，更依赖形状）。
- en: '**Most likely, we will soon see larger ViTs (alone or as a component of a multi-modal
    model). What do you think about it?**'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们很可能很快会看到更大的 ViTs（单独使用或作为多模态模型的组成部分）。你对此有何看法？**'
- en: 'If you have found this interesting:'
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如果你觉得这很有趣：
- en: You can look for my other articles, you can also [**subscribe**](https://salvatore-raieli.medium.com/subscribe)
    to get notified when I publish articles, and you can also connect or reach me
    on[**LinkedIn**](https://www.linkedin.com/in/salvatore-raieli/)**.**
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以查看我的其他文章，也可以[**订阅**](https://salvatore-raieli.medium.com/subscribe)以便在我发布文章时获得通知，你还可以通过[**LinkedIn**](https://www.linkedin.com/in/salvatore-raieli/)**与我联系**。
- en: Here is the link to my GitHub repository, where I am planning to collect code
    and many resources related to machine learning, artificial intelligence, and more.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我 GitHub 存储库的链接，我计划在这里收集与机器学习、人工智能以及更多相关的代码和资源。
- en: '[](https://github.com/SalvatoreRa/tutorial?source=post_page-----5d59ac36c1d6--------------------------------)
    [## GitHub - SalvatoreRa/tutorial: Tutorials on machine learning, artificial intelligence,
    data science…'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://github.com/SalvatoreRa/tutorial?source=post_page-----5d59ac36c1d6--------------------------------)
    [## GitHub - SalvatoreRa/tutorial: 机器学习、人工智能、数据科学的教程…'
- en: Tutorials on machine learning, artificial intelligence, data science with math
    explanation and reusable code (in python…
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于机器学习、人工智能、数据科学的教程，包含数学解释和可重用的代码（使用 Python…）
- en: github.com](https://github.com/SalvatoreRa/tutorial?source=post_page-----5d59ac36c1d6--------------------------------)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: github.com](https://github.com/SalvatoreRa/tutorial?source=post_page-----5d59ac36c1d6--------------------------------)
- en: 'or you may be interested in one of my recent articles:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 或者你可能对我最近的一篇文章感兴趣：
- en: '[](https://levelup.gitconnected.com/microsoft-biogpt-towards-the-chatgpt-of-life-science-56e251536af6?source=post_page-----5d59ac36c1d6--------------------------------)
    [## Microsoft BioGPT: Towards the ChatGPT of life science?'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://levelup.gitconnected.com/microsoft-biogpt-towards-the-chatgpt-of-life-science-56e251536af6?source=post_page-----5d59ac36c1d6--------------------------------)
    [## 微软 BioGPT：迈向生命科学的 ChatGPT？'
- en: BioGPT achieves the SOTA in different biomedical NLP tasks
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: BioGPT 在不同的生物医学 NLP 任务中达到了 SOTA。
- en: 'levelup.gitconnected.com](https://levelup.gitconnected.com/microsoft-biogpt-towards-the-chatgpt-of-life-science-56e251536af6?source=post_page-----5d59ac36c1d6--------------------------------)
    [](https://levelup.gitconnected.com/sparsegpt-fewer-parameters-is-better-7b47ad60ac00?source=post_page-----5d59ac36c1d6--------------------------------)
    [## SparseGPT: fewer parameters is better?'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[SparseGPT：参数更少更好？](https://levelup.gitconnected.com/sparsegpt-fewer-parameters-is-better-7b47ad60ac00?source=post_page-----5d59ac36c1d6--------------------------------)'
- en: How to get rid of 100 billion parameters and happily infer on one GPU
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何摆脱 1000 亿参数，并在一个 GPU 上愉快地进行推理
- en: levelup.gitconnected.com](https://levelup.gitconnected.com/sparsegpt-fewer-parameters-is-better-7b47ad60ac00?source=post_page-----5d59ac36c1d6--------------------------------)
    [](https://medium.com/data-driven-fiction/everything-but-everything-you-need-to-know-about-chatgpt-546af7153ee2?source=post_page-----5d59ac36c1d6--------------------------------)
    [## Everything but everything you need to know about ChatGPT
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[关于 ChatGPT 你需要知道的一切](https://medium.com/data-driven-fiction/everything-but-everything-you-need-to-know-about-chatgpt-546af7153ee2?source=post_page-----5d59ac36c1d6--------------------------------)'
- en: what is known, the latest news, what it is impacting, and what is changing.
    all in one article
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 已知的内容、最新消息、影响和变化，都在一篇文章中
- en: 'medium.com](https://medium.com/data-driven-fiction/everything-but-everything-you-need-to-know-about-chatgpt-546af7153ee2?source=post_page-----5d59ac36c1d6--------------------------------)
    [](https://medium.datadriveninvestor.com/razzaie-awards-2022-what-are-the-worst-ai-of-the-year-a2596c566218?source=post_page-----5d59ac36c1d6--------------------------------)
    [## RazzAIe awards 2022: what are the worst AI of the year?'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[RazzAIe 奖 2022：年度最差 AI 是哪些？](https://medium.datadriveninvestor.com/razzaie-awards-2022-what-are-the-worst-ai-of-the-year-a2596c566218?source=post_page-----5d59ac36c1d6--------------------------------)'
- en: What are the worst models of the year? What went wrong?
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 年度最差模型是什么？出了什么问题？
- en: medium.datadriveninvestor.com](https://medium.datadriveninvestor.com/razzaie-awards-2022-what-are-the-worst-ai-of-the-year-a2596c566218?source=post_page-----5d59ac36c1d6--------------------------------)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[年度最差模型有哪些？出了什么问题？](https://medium.datadriveninvestor.com/razzaie-awards-2022-what-are-the-worst-ai-of-the-year-a2596c566218?source=post_page-----5d59ac36c1d6--------------------------------)'
