- en: Self-Supervised Learning Using Projection Heads
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用投影头的自监督学习
- en: 原文：[https://towardsdatascience.com/self-supervised-learning-using-projection-heads-b77af3911d33](https://towardsdatascience.com/self-supervised-learning-using-projection-heads-b77af3911d33)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/self-supervised-learning-using-projection-heads-b77af3911d33](https://towardsdatascience.com/self-supervised-learning-using-projection-heads-b77af3911d33)
- en: Boost performance with unlabeled data
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用无标记数据提升性能
- en: '[](https://medium.com/@danielwarfield1?source=post_page-----b77af3911d33--------------------------------)[![Daniel
    Warfield](../Images/c1c8b4dd514f6813e08e401401324bca.png)](https://medium.com/@danielwarfield1?source=post_page-----b77af3911d33--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b77af3911d33--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b77af3911d33--------------------------------)
    [Daniel Warfield](https://medium.com/@danielwarfield1?source=post_page-----b77af3911d33--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@danielwarfield1?source=post_page-----b77af3911d33--------------------------------)[![丹尼尔·沃菲尔德](../Images/c1c8b4dd514f6813e08e401401324bca.png)](https://medium.com/@danielwarfield1?source=post_page-----b77af3911d33--------------------------------)[](https://towardsdatascience.com/?source=post_page-----b77af3911d33--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----b77af3911d33--------------------------------)
    [丹尼尔·沃菲尔德](https://medium.com/@danielwarfield1?source=post_page-----b77af3911d33--------------------------------)'
- en: ·Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b77af3911d33--------------------------------)
    ·13 min read·Jun 29, 2023
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----b77af3911d33--------------------------------)
    ·阅读时间13分钟·2023年6月29日
- en: --
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '![](../Images/289279afe6fc5596e60b7f246b366c9d.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/289279afe6fc5596e60b7f246b366c9d.png)'
- en: “Self-supervised” by Daniel Warfield using p5.js
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: “自监督”由丹尼尔·沃菲尔德使用 p5.js 实现
- en: In this post you’ll learn about self-supervised learning, how it can be used
    to boost model performance, and the role projection heads play in the self-supervised
    learning process. We will cover the intuition, some literature, and a computer
    vision example in PyTorch.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，你将学习自监督学习、如何利用它提升模型性能，以及投影头在自监督学习过程中的作用。我们将涵盖直觉、一些文献以及 PyTorch 中的计算机视觉示例。
- en: '**Who is this useful for?** Anyone who has unlabeled and augmentable data.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**适合谁？** 任何拥有无标记且可增补数据的人。'
- en: '**How advanced is this post?** The beginning of this post is conceptually accessible
    to beginners, but the example is more focused on intermediate and advanced data
    scientists.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**这篇文章的难度如何？** 本文的开头对初学者概念上是可及的，但例子更多地集中在中级和高级数据科学家上。'
- en: '**Pre-requisites:** A high level understanding of convolutional and dense networks.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**前提条件：** 对卷积网络和密集网络有较高水平的理解。'
- en: '**Code:** Full code can be found [here](https://github.com/DanielWarfield1/MLWritingAndResearch/blob/main/sslDemo.ipynb).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码：** 完整代码请见[这里](https://github.com/DanielWarfield1/MLWritingAndResearch/blob/main/sslDemo.ipynb)。'
- en: Self-Supervision vs Other Approaches
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自监督与其他方法
- en: 'Generally, when one thinks of models, they consider two camps: supervised and
    unsupervised models.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当人们想到模型时，他们会考虑两类：监督模型和无监督模型。
- en: '**Supervised Learning** is the process of training a model based on **labeled**
    information. When training a model to predict if images contain cats or dogs,
    for instance, one curates a set of images which are labeled as having a cat or
    a dog, then trains the model (using [gradient descent](https://medium.com/@danielwarfield1/what-are-gradients-and-why-do-they-explode-add23264d24b))
    to understand the difference between images with cats and dogs.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监督学习** 是基于**标记**信息训练模型的过程。例如，在训练一个预测图像是否包含猫或狗的模型时，首先收集一组标记为猫或狗的图像，然后训练模型（使用
    [梯度下降](https://medium.com/@danielwarfield1/what-are-gradients-and-why-do-they-explode-add23264d24b)）来理解包含猫和狗图像之间的差异。'
- en: '**Unsupervised Learning** is the process of giving some sort of model **unlabeled**
    information, and extracting useful inferences through some sort of transformation
    of the data. A classic example of unsupervised learning is clustering; where groups
    of information are extracted from un-grouped data based on local position.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无监督学习** 是向模型提供某种**无标记**信息，并通过对数据进行某种转化来提取有用的推断。无监督学习的经典例子是聚类；在此过程中，从未分组的数据中基于局部位置提取信息组。'
- en: Self-supervised learning is somewhere in between. **Self-supervision uses**
    **labels** **that are generated programmatically, not by humans.** In some ways
    it’s supervised because the model learns from labeled data, but in other ways
    it’s unsupervised because no labels are provided to the training algorithm. Hence
    self-supervised.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习介于两者之间。**自监督使用** **程序生成的标签** **而非人工标签。** 在某些方面，它是监督式学习，因为模型从标记的数据中学习，但在其他方面，它是无监督的，因为训练算法没有提供标签。因此称为自监督。
- en: Self-supervised learning (SSL) aims to produce useful feature representations
    without access to any human-labeled data annotations. — K Gupta Et al.
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 自监督学习（SSL）的目标是在没有任何人工标记数据注释的情况下生成有用的特征表示。— K Gupta 等
- en: Self-Supervision in a Nutshell
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自监督概述
- en: Self supervision uses transformations to the data, along with a clever loss
    function, to teach the model to **understand similar data**. We might not know
    what an image contains (it’s unlabeled by a human), but we do know a **slightly
    modified image of a something is still an image of a that thing**. As a result,
    you can label an image, and a flipped picture of an image, as containing the same
    thing.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督使用对数据的变换，加上巧妙的损失函数，来教模型**理解相似的数据**。我们可能不知道图像包含什么（它是无人标记的），但我们知道**稍微修改的图像仍然是那个东西的图像**。因此，你可以将一张图像及其翻转图像标记为包含相同的东西。
- en: '![](../Images/900de1dc92b14b2d2dc06002fbe435dd.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/900de1dc92b14b2d2dc06002fbe435dd.png)'
- en: Even if we don’t know this image contains a cat, we know the image contains
    the same thing regardless of how we manipulate the image.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们不知道这张图像包含一只猫，我们也知道图像包含的内容不论如何操控图像都是相同的。
- en: The idea is, by training a model to learn if the data contains similar things,
    you are teaching the model to understand data regardless of how it is presented.
    In other words, **You are training the model to understand the images, generally,
    regardless of class**. Once self-supervision is done, the model can be refined
    on a small amount of labeled data to understand the final task (is an image of
    a dog or a cat).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是，通过训练模型学习数据是否包含相似的东西，你是在教模型理解数据，而不论数据是如何呈现的。换句话说，**你是在训练模型理解图像，通常情况下，不论类别**。一旦自监督完成，模型可以在少量标记数据上进行精炼，以理解最终任务（是狗的图像还是猫的图像）。
- en: '![](../Images/b74711381dc53fcb6a28afb689461154.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b74711381dc53fcb6a28afb689461154.png)'
- en: The general idea of how self supervised learning fits into the general workflow
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习如何融入一般工作流程的基本概念
- en: I’m using images in this example, but self-supervision can be applied to any
    data that has augmentations that alter the data without modifying their essence
    from the perspective of the final modeling problem. For example, augmentation
    of audio data can be done using wave tables, which I describe in [this article](https://medium.com/roundtableml/use-frequency-more-frequently-14715714de38).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这个例子中使用的是图像，但自监督可以应用于任何数据，这些数据通过增强方法来改变数据，而不改变其从最终建模问题的角度来看本质。例如，音频数据的增强可以使用波表进行，具体说明见[这篇文章](https://medium.com/roundtableml/use-frequency-more-frequently-14715714de38)。
- en: p.s. Another common way to conceptualize this is **style invariance**. In other
    words, you’re training a model to be good at ignoring stylistic differences in
    images.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 附言：另一种常见的概念化方式是**风格不变性**。换句话说，你是在训练一个模型来忽略图像中的风格差异。
- en: Projection Heads
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 投影头
- en: As machine learning has progressed as a discipline, certain architectural choices
    have proven to be generally useful. In convolutional networks, for instance, some
    networks have backbones, some have necks, and some have heads. **The head, generally,
    is a dense network at the end of a larger network which turns features into a
    final output**.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习作为一个学科的发展，一些架构选择已经证明是普遍有用的。例如，在卷积网络中，一些网络有主干，一些有颈部，还有一些有头部。**头部，通常来说，是一个位于较大网络末端的密集网络，将特征转化为最终输出**。
- en: '![](../Images/cbfa3ad2d3f752baaba9d828f7d575e1.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/cbfa3ad2d3f752baaba9d828f7d575e1.png)'
- en: 'The First YOLO paper is a classic convolutional architecture. It can be thought
    of as 2 sections: A series of convolutions which convert raw images to key features
    (the backbone), and a dense network which turns those features into a final result
    (the head). [Source](https://arxiv.org/pdf/1506.02640.pdf)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 第一篇YOLO论文是经典的卷积架构。它可以被认为有两个部分：一系列卷积将原始图像转换为关键特征（主干），以及一个密集网络将这些特征转化为最终结果（头部）。[来源](https://arxiv.org/pdf/1506.02640.pdf)
- en: the function of this head is often described as a **projection**. Throughout
    math and many other disciplines, a projection is the idea of mapping something
    in one space to another space, like how a light from a lamp can map your 3d form
    into a 2d shadow on the wall. **A projection head is a dense network at the end
    of a larger network tasked with transforming some information to other information.**
    In our toy example of cats vs dogs, the projection head would project the general
    understanding of images as features into a prediction of cat vs dog.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这个头部的功能通常被描述为**投影**。在数学和许多其他学科中，投影是将某种信息从一个空间映射到另一个空间的概念，就像灯光如何将你的三维形态映射到墙上的二维阴影中一样。**投影头是一个位于较大网络末端的密集网络，负责将一些信息转换为其他信息**。在我们的玩具示例中，猫与狗，投影头将图像的通用理解作为特征映射到猫与狗的预测中。
- en: Why Projection Heads are So Important in Self-Supervision.
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么投影头在自监督学习中如此重要。
- en: 'Imagine you’re playing monopoly. There’s a lot to learn; investing in real-estate
    can pay dividends, it’s important to consider the future before making investments,
    pass go and collect $200, there’s no fundamental difference between a shoe and
    a thimble, etc. **Within the game of monopoly there are two types of information:
    generally applicable and task specific information**. You should not get excited
    every time you see the word “go” in your daily life: that’s task specific. You
    should, however, consider your investments carefully: that’s generally useful.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下你正在玩大富翁游戏。有很多需要学习的东西；投资房地产可以带来收益，做出投资前需要考虑未来，经过“起点”并获得200美元，对鞋子和顶针之间没有根本区别等等。**在大富翁游戏中，有两种类型的信息：一般适用的信息和任务特定的信息**。你不应该在日常生活中每次看到“起点”这个词就兴奋：那是任务特定的。你应该仔细考虑你的投资：那是普遍有用的。
- en: '**We can think of self supervision as a “game”, where the model learns to recognize
    similar or dissimilar images**. Through playing this game, it learns to generally
    understand images, as well as specific rules in realizing if two images are the
    same image.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**我们可以把自监督看作是一场“游戏”，在这场游戏中，模型学习识别相似或不相似的图像**。通过玩这个游戏，模型学习到一般的图像理解，以及实现两张图像是否相同的具体规则。'
- en: '![](../Images/32b15cf6a0d786859d4b8b2ad54cacf7.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/32b15cf6a0d786859d4b8b2ad54cacf7.png)'
- en: In a classic convolutional network with a neck and a head, a common intuition
    is that the convolution extracts features, styles, textures, and other general
    pieces of information necessary for general image understanding. The dense head,
    on the other hand, projects those found features into a task specific output (for
    instance, recognizing that two images are of the same thing, like in self supervised
    learning).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个经典的卷积网络中，颈部和头部是常见的直觉，其中卷积提取特征、风格、纹理以及其他用于一般图像理解的必要信息。另一方面，密集头部则将这些发现的特征映射到任务特定的输出中（例如，识别两张图片是否相同，类似于自监督学习）。
- en: Once we have trained a self supervised model on similar data, and we now want
    to refine this model based on labeled data, we don’t care about the task specific
    logic to identify if two images are the same. We want to keep the general image
    understanding, but replace the task specific knowledge with classification knowledge.
    To do this, **we throw out the projection head, and replace it with a new one**.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们在相似数据上训练了一个自监督模型，并且现在想根据标记数据来细化这个模型，我们不再关心识别两张图像是否相同的任务特定逻辑。我们希望保留一般的图像理解，但用分类知识替换任务特定知识。为此，**我们丢弃投影头，并用一个新的投影头替换它**。
- en: '![](../Images/b9d2960fdbfcac9f7aa4b12d965b4158.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b9d2960fdbfcac9f7aa4b12d965b4158.png)'
- en: The parts of a model which are discarded for self supervised learning (top)
    to supervised learning (bottom). The convolutional backbone is preserved, while
    the projection head, which is responsible task specific logic, is discarded.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在自监督学习（顶部）和监督学习（底部）中被丢弃的部分。卷积骨干网络被保留，而负责任务特定逻辑的投影头则被丢弃。
- en: 'The use of projection heads during the self-supervised learning process is
    a current point of research ([this](https://arxiv.org/pdf/2212.11491.pdf) is a
    good paper on the subject), but the intuition is this: in self supervised learning
    **You have to have the necessary logic to get good at the self supervised task
    so that you can learn generally applicable feature representations**. Once you
    learn those features, the projection head, which contains the logic specific to
    optimizing self supervision, can be discarded.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在自监督学习过程中使用投影头是当前的研究热点（[这篇](https://arxiv.org/pdf/2212.11491.pdf)文章对这一主题进行了很好的阐述），但直观上是这样的：在自监督学习中**你必须具备必要的逻辑才能在自监督任务中表现良好，以便你能学习到一般适用的特征表示**。一旦你学会了这些特征，包含优化自监督逻辑的投影头可以被丢弃。
- en: Creating and using a projection head is a bit different than traditional modeling.
    The objective of the projection head isn’t *necessarily*to make a model which
    is good at the self-supervised task, but entice the creation of feature representations
    which are more useful in later, downstream tasks.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 创建和使用投影头与传统建模有所不同。投影头的目标并不是*必然*要创建一个擅长自监督任务的模型，而是引导生成在后续任务中更有用的特征表示。
- en: Self-Supervision in PyTorch
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch中的自监督
- en: In this example we will be using a modification of the MNIST dataset, which
    is a classic dataset consisting of images of written numbers, paired with labels
    denoting which number the image represents.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用MNIST数据集的一个修改版，MNIST是一个经典的数据集，包含手写数字的图像，并配有标记以表示图像所代表的数字。
- en: 'MNIST consists of 60,000 labeled training images, and 10,000 labeled test images.
    In this example, however, **We will discard all but 200 of the training labels**.
    That means we will have a set of 200 labeled images to train from, and 59,800
    unlabeled images to train from. **This modification reflects the types of applications
    in which self supervision is most useful: Datasets with a lot of data, but which
    are expensive to label.**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST包含60,000张有标签的训练图像和10,000张有标签的测试图像。然而，在这个例子中，**我们将丢弃除了200个训练标签之外的所有标签**。这意味着我们将有200张有标签的图像用于训练，以及59,800张无标签的图像用于训练。**这个修改反映了自监督最有用的应用类型：数据量大但标记成本高的数据集。**
- en: Full code can be found [here](https://github.com/DanielWarfield1/MLWritingAndResearch/blob/main/sslDemo.ipynb).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码可以在[这里](https://github.com/DanielWarfield1/MLWritingAndResearch/blob/main/sslDemo.ipynb)找到。
- en: The MNIST dataset is licensed under [GNU General Public License v3.0](https://github.com/sharmaroshan/MNIST-Dataset/blob/master/LICENSE),
    and the torchvision module used to load it is licensed under [BSD 3-Clause “New”
    or “Revised” License](https://github.com/UiPath/torchvision/blob/master/LICENSE),
    both permitting commercial use.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST数据集在[GNU通用公共许可证v3.0](https://github.com/sharmaroshan/MNIST-Dataset/blob/master/LICENSE)下授权使用，而用于加载它的torchvision模块在[BSD
    3-Clause “New” or “Revised” License](https://github.com/UiPath/torchvision/blob/master/LICENSE)下授权使用，两者都允许商业使用。
- en: 1) Load the Data
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1) 加载数据
- en: Loading the dataset
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 加载数据集
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/32c05516ae399c125462b7e42f173a7a.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/32c05516ae399c125462b7e42f173a7a.png)'
- en: Downloaded dataset, with a few samples
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 下载的数据集，包含一些样本
- en: 2) Separate into labeled and unlabeled data
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2) 将数据分为有标签和无标签数据
- en: In this example we will artificially ignore most of the labels in the training
    set to mimic a use case where it is easy to collect large amounts of data, but
    difficult or resource intensive to label all of the data. This code block also
    does some of the necessary data manipulation necessary to leverage PyTorch.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将人工忽略大部分训练集中的标签，以模拟一种容易收集大量数据但难以或资源密集型地标记所有数据的用例。这个代码块还执行了一些必要的数据操作，以便利用PyTorch。
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](../Images/e15fb72bff5c4ad8d3490b2aac4db739.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e15fb72bff5c4ad8d3490b2aac4db739.png)'
- en: Printout from reformatting process
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 从格式化过程中的打印输出
- en: '**3) Defining Model**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**3) 定义模型**'
- en: 'To speed up training, this problem uses a super simple conv net and minimal
    hyperparameter exploration. This model has two general parts: the convolutional
    backbone and the densely connected head.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加快训练，这个问题使用了一个超简单的卷积网络和最少的超参数探索。这个模型有两个主要部分：卷积主干和密集连接的头部。
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](../Images/11f197e64b0ce894f8ab9efc46bc6f9e.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11f197e64b0ce894f8ab9efc46bc6f9e.png)'
- en: Output dimension and Model Architecture.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 输出维度和模型架构。
- en: 4) Train and test using only supervised learning as a baseline
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4) 使用仅有监督学习作为基准进行训练和测试
- en: To get an idea of how much self supervision improves performance, we’ll train
    our baseline model on only the 200 labeled samples.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解自监督如何提高性能，我们将在仅有的200个标记样本上训练我们的基线模型。
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/10f42e98ac9c0667a3b6553482fbece4.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10f42e98ac9c0667a3b6553482fbece4.png)'
- en: Test accuracy throughout training of the supervised-only model. I’m surprised
    performance is this good, considering randomly guessing would result in a 10%
    accuracy, and this model was only exposed to 200 labeled samples. Still, we can
    do much better by incorporating self-supervised learning.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 监督模型在训练过程中的测试准确率。考虑到随机猜测的准确率为10%，而这个模型仅接触了200个标记样本，我对其表现如此之好感到惊讶。尽管如此，通过结合自监督学习，我们仍然可以做得更好。
- en: 5) Defining Augmentations
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5) 定义数据增强
- en: Self supervised learning requires augmentations. This function augments a batch
    of images twice, resulting in a pair of stochastically augmented images to be
    used in contrastive learning.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习需要数据增强。这个函数将一批图像增强两次，结果是得到一对随机增强的图像，用于对比学习。
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/de2661ac8357443888f41fcc7afe9508.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/de2661ac8357443888f41fcc7afe9508.png)'
- en: Two sample positive pairs within the same batch
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 同一批次内的两个正样本对
- en: 6) Defining Contrastive Loss
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6) 定义对比损失
- en: Contrastive loss is the loss function used to entice positive pairs to be positioned
    closely in an embedding space, and negative pairs to be positioned further apart.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对比损失是用于将正样本对在嵌入空间中紧密放置，而将负样本对放置得更远的损失函数。
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/4489cc808a2ab516b9c2b741ee589bd0.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4489cc808a2ab516b9c2b741ee589bd0.png)'
- en: Output of loss function. Critically, a `grad_fn` exists, meaning the function
    is differentiable and thus can update model parameters.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数的输出。关键是存在`grad_fn`，意味着该函数是可微的，因此可以更新模型参数。
- en: 7) Self Supervised Training
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7) 自监督训练
- en: Training the model to understand image similarity and difference via self supervision
    and contrastive loss. Because this is an intermediary step, it’s difficult to
    create clear and intuitive performance indicators. As a result, I opted to spend
    some extra compute to intimately understand loss, which was useful in tuning parameters
    to get consistent model improvement.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型通过自监督和对比损失理解图像的相似性和差异。由于这是一个中间步骤，很难创建清晰和直观的性能指标。因此，我选择花费一些额外的计算来深入理解损失，这对于调整参数以获得一致的模型改进非常有用。
- en: '[PRE6]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/63f0f6030c2fc5f2681c95d24070ded8.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/63f0f6030c2fc5f2681c95d24070ded8.png)'
- en: First few epochs of training output, with several loss based performance indicators,
    which are useful in tweaking parameters.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 训练的前几个时期输出，包含若干基于损失的性能指标，这些指标对于调整参数非常有用。
- en: 8) Self Supervised Training Progress
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8) 自监督训练进展
- en: This is the loss improvement as a result of self supervised learning. You can
    see the relationship between the exponentially decreasing learning rate and loss
    value.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这是自监督学习带来的损失改进。你可以看到指数递减的学习率与损失值之间的关系。
- en: '[PRE7]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/934c43d85e1a53725324e94672931522.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/934c43d85e1a53725324e94672931522.png)'
- en: Learning rate is plotted per sample, while loss is plotted per epoch, but you
    get the idea. Loss goes down and then converges, then learning stops when loss
    reduction becomes negligible.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率按样本绘制，而损失按时期绘制，但你可以理解。损失下降然后收敛，学习在损失减少变得微不足道时停止。
- en: 9) Fine Tuning Self Supervised Model with Supervised Learning
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9) 用监督学习微调自监督模型
- en: Using the supervised function from before to train the self supervised model
    on supervised data. This is done twice; once with the original self supervised
    learning head, and one with a new randomly-initialized head.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 使用之前的监督函数在监督数据上训练自监督模型。这做了两次：一次使用原始自监督学习头部，一次使用新随机初始化的头部。
- en: '[PRE8]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/309f10baa8128644b78045e2e07f313b.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/309f10baa8128644b78045e2e07f313b.png)'
- en: Training results with the original head (left) and randomly initialized head
    (right)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 使用原始头部（左）和随机初始化头部（右）的训练结果
- en: 10) Discussion
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10) 讨论
- en: As can be seen, pure supervised learning performed the worst, self supervised
    learning with supervised learning performed second best, and self supervised learning
    with supervised learning on a new head performed best.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，纯监督学习表现最差，自监督学习与监督学习结合表现第二好，而自监督学习与新头部结合表现最佳。
- en: These results are purely demonstrative; there was no significant hyperparameter
    optimization which would be necessary in production. However, this notebook does
    support the theoretical utility of self supervision, and the importance of careful
    usage of the projection head.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果仅用于演示；没有进行显著的超参数优化，这在实际应用中是必要的。然而，本笔记本确实支持自监督的理论效用，并且强调了投影头谨慎使用的重要性。
- en: 'Only supervised learning: 52.5% accuracy'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅监督学习：52.5%准确率
- en: 'SSL and supervised on SSL head: 59.7% accuracy'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SSL 和在SSL头上的监督：59.7%准确率
- en: 'SSL and supervised on a new head: 63.6%'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SSL 和在新头上的监督：63.6%
- en: '**63.6% accuracy when only considering 200 labeled images is pretty impressive!**'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**仅考虑200张标记图像的情况下，63.6%的准确率非常令人印象深刻！**'
- en: Follow For More!
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关注以获取更多信息！
- en: In future posts, I’ll also describing several landmark papers in the ML space,
    with an emphasis on practical and intuitive explanations.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来的帖子中，我还会描述ML领域的几篇重要论文，重点是实用和直观的解释。
- en: '**Attribution:** All of the images in this document were created by Daniel
    Warfield, unless a source is otherwise provided. You can use any images in this
    post for your own non-commercial purposes, so long as you reference this article,
    [https://danielwarfield.dev](https://danielwarfield.dev/), or both.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**归属：** 本文档中的所有图像均由Daniel Warfield创建，除非另有来源说明。你可以在非商业目的下使用本文中的任何图像，只要你引用本文，[https://danielwarfield.dev](https://danielwarfield.dev/)，或两者皆可。'
