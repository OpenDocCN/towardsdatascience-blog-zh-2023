- en: Enhancing CSV File Query Performance in ChatGPT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升 ChatGPT 中 CSV 文件查询性能
- en: 原文：[https://towardsdatascience.com/enhancing-csv-file-query-performance-in-chatgpt-3e2b67a5f867?source=collection_archive---------1-----------------------#2023-07-31](https://towardsdatascience.com/enhancing-csv-file-query-performance-in-chatgpt-3e2b67a5f867?source=collection_archive---------1-----------------------#2023-07-31)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/enhancing-csv-file-query-performance-in-chatgpt-3e2b67a5f867?source=collection_archive---------1-----------------------#2023-07-31](https://towardsdatascience.com/enhancing-csv-file-query-performance-in-chatgpt-3e2b67a5f867?source=collection_archive---------1-----------------------#2023-07-31)
- en: with LangChain’s Self-Querying based on a customized CSV Loader
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于自定义 CSV 加载器的 LangChain 自查询
- en: '[](https://medium.com/@vanillaxiangshuyang?source=post_page-----3e2b67a5f867--------------------------------)[![Shuyang
    Xiang](../Images/36a5fd18fd9b7b88cb41094f09b83882.png)](https://medium.com/@vanillaxiangshuyang?source=post_page-----3e2b67a5f867--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3e2b67a5f867--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3e2b67a5f867--------------------------------)
    [Shuyang Xiang](https://medium.com/@vanillaxiangshuyang?source=post_page-----3e2b67a5f867--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@vanillaxiangshuyang?source=post_page-----3e2b67a5f867--------------------------------)[![Shuyang
    Xiang](../Images/36a5fd18fd9b7b88cb41094f09b83882.png)](https://medium.com/@vanillaxiangshuyang?source=post_page-----3e2b67a5f867--------------------------------)[](https://towardsdatascience.com/?source=post_page-----3e2b67a5f867--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----3e2b67a5f867--------------------------------)
    [Shuyang Xiang](https://medium.com/@vanillaxiangshuyang?source=post_page-----3e2b67a5f867--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9b74bc8c860d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fenhancing-csv-file-query-performance-in-chatgpt-3e2b67a5f867&user=Shuyang+Xiang&userId=9b74bc8c860d&source=post_page-9b74bc8c860d----3e2b67a5f867---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3e2b67a5f867--------------------------------)
    ·9 min read·Jul 31, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3e2b67a5f867&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fenhancing-csv-file-query-performance-in-chatgpt-3e2b67a5f867&user=Shuyang+Xiang&userId=9b74bc8c860d&source=-----3e2b67a5f867---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9b74bc8c860d&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fenhancing-csv-file-query-performance-in-chatgpt-3e2b67a5f867&user=Shuyang+Xiang&userId=9b74bc8c860d&source=post_page-9b74bc8c860d----3e2b67a5f867---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----3e2b67a5f867--------------------------------)
    · 9分钟阅读 · 2023年7月31日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F3e2b67a5f867&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fenhancing-csv-file-query-performance-in-chatgpt-3e2b67a5f867&user=Shuyang+Xiang&userId=9b74bc8c860d&source=-----3e2b67a5f867---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3e2b67a5f867&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fenhancing-csv-file-query-performance-in-chatgpt-3e2b67a5f867&source=-----3e2b67a5f867---------------------bookmark_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F3e2b67a5f867&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fenhancing-csv-file-query-performance-in-chatgpt-3e2b67a5f867&source=-----3e2b67a5f867---------------------bookmark_footer-----------)'
- en: The advent of sophisticated language models, like ChatGPT, has brought a novel
    and promising approach to querying tabular data. However, due to token limitations,
    directly executing a query becomes challenging without the assistance of APIs
    like retriever. Consequently, the accuracy of queries heavily relies on the query’s
    quality, and it is not uncommon for standard retrievers to fall short in returning
    the exact information required.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 高级语言模型的出现，如 ChatGPT，为查询表格数据带来了新颖而有前途的方法。然而，由于令牌限制，直接执行查询变得具有挑战性，尤其是在没有像检索器这样的
    API 协助的情况下。因此，查询的准确性严重依赖于查询的质量，标准检索器往往无法返回所需的确切信息。
- en: In this article, I will delve into the reasons behind the failure of conventional
    retriever methods in certain use cases. Furthermore, we propose a revolutionary
    solution in the form of a customized CSV data loader that incorporates metadata
    information. By leveraging [LangChain](https://python.langchain.com/docs/get_started/introduction.html)’s
    Self-Querying API alongside the new CSV data loader, we can extract information
    with significantly improved performance and precision.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我将*深入探讨*传统检索方法在某些使用案例中失败的原因。此外，我们提出了一种革命性解决方案，即一种自定义的CSV数据加载器，它整合了元数据。通过利用[LangChain](https://python.langchain.com/docs/get_started/introduction.html)的自查询API及新CSV数据加载器，我们可以以显著提高的性能和精度提取信息。
- en: For detailed code used in this article, please take a look of the notebook [here](https://github.com/ShuyangenFrance/ChatGPTCSV/blob/main/self-querying.ipynb).
    I would like to highlight the fact that this notebook illustrates the possibility
    that **querying big tabular data with an LLM can achieve a remarkable accuracy.**
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 有关本文中使用的详细代码，请查看[这里](https://github.com/ShuyangenFrance/ChatGPTCSV/blob/main/self-querying.ipynb)的笔记本。我想强调的是，这个笔记本展示了**使用LLM查询大型表格数据可以实现显著准确性**的可能性。
- en: Retrieval on Disease Population Dataset
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在疾病人口数据集上的检索
- en: 'We would like to query the following synthetic SIR dataset created by the author:
    we simulate the three different groups of the population during 90 days of disease
    in 10 cities based on a simple [SIR model](https://en.wikipedia.org/wiki/Compartmental_models_in_epidemiology).
    For the case of simplicity, we suppose that the population of each city ranges
    from 5e3 to 2e4 and there is no population movement between cities. Moreover,
    we generate ten random integers between 500 to 2000 as the original infectious
    number of people.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望查询作者创建的以下合成SIR数据集：我们基于一个简单的[SIR模型](https://en.wikipedia.org/wiki/Compartmental_models_in_epidemiology)模拟10个城市在90天内的三组人口。为了简单起见，我们假设每个城市的人口范围从5e3到2e4，并且城市之间没有人口流动。此外，我们生成了十个介于500到2000之间的随机整数作为原始感染人数。
- en: '![](../Images/07fc21dc197c3e04df872fb527f72224.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07fc21dc197c3e04df872fb527f72224.png)'
- en: 'Image by author: Disease population in 10 cities'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图像：10个城市的疾病人口
- en: 'The tabular has the following form with five columns: “time” indicating the
    time when the population was measured, “city” the city where the data was measured
    and “susceptible”, “infectious”, and “removed” the three groups of the population.
    For simplicity, the data has been saved locally as a CSV file.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 表格的形式如下，有五列：“time”表示测量人口的时间，“city”表示测量数据的城市，“susceptible”、“infectious”和“removed”表示人口的三组。为了简单起见，数据已保存为本地CSV文件。
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We would like to ask ChatGPT questions relevant to the dataset. To let ChatGPT
    interact with such tabular data, we follow the following standard steps using
    LangChain:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望向ChatGPT提出与数据集相关的问题。为了让ChatGPT与这种表格数据交互，我们按照以下标准步骤使用LangChain：
- en: Using the CSVLoader to load the data,
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用CSVLoader加载数据，
- en: Create a vectorstore (we use Chroma here) to store the embed data with OpenAI
    embeddings,
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个vectorstore（我们这里使用Chroma）来存储带有OpenAI嵌入的嵌入数据，
- en: Use retrievers to return documents relevant to a given unstructured query.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用检索器返回与给定非结构化查询相关的文档。
- en: You can use the following code to realize the steps above.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下代码实现上述步骤。
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We can now define a *ConversationalRetriverChain* to query the SIR dataset.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以定义一个*ConversationalRetriverChain*来查询SIR数据集。
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the code above, I have defined a conversation chain that will search the
    relevant information of the query in the SIR dataset using the defined retriever
    in the previous step and give an answer based on the retrieved information. To
    give clearer instructions to ChatGPT, I have also given a prompt clarifying the
    definition of all the columns in the dataset.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我定义了一个对话链，它将使用在上一步中定义的检索器在SIR数据集中搜索查询的相关信息，并根据检索到的信息给出答案。为了给ChatGPT更清晰的指示，我还给出了一个提示，说明了数据集中所有列的定义。
- en: 'Let us now ask one simple question: “Which city has the most infectious people
    on 2018–02–03?”'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来问一个简单的问题：“2018年2月3日哪个城市的感染人数最多？”
- en: 'Surprisingly, our chatbot said: “The dataset provided does not include data
    for the date 2018–02–03.”'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 出乎意料的是，我们的聊天机器人说：“提供的数据集中不包括2018年2月3日的数据。”
- en: How could it be possible?
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这怎么可能？
- en: Why retrieval failed?
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么检索失败了？
- en: 'To investigate why the chatbot failed to answer a question whose answer is
    nowhere but in the provided dataset, I took a look at the relevant document it
    retrieved with the question “Which city has the most infectious people on 2018–02–03?”.
    I got the following lines:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了调查聊天机器人为何未能回答一个其答案仅存在于提供的数据集中的问题，我查看了它用问题“2018年2月3日哪个城市的传染病人数最多？”检索到的相关文档。我得到了以下几行：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Surprisingly, even though I specified that I wanted to know what happened on
    the date 2018–02–03, no line of that date was returned. Since no information on
    that date was ever sent to ChatGPT, there is of course no doubt that it cannot
    answer such a question.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，尽管我明确要求了解2018年2月3日发生了什么，但没有返回该日期的任何行。由于该日期的信息从未发送给ChatGPT，因此当然毫无疑问它无法回答这样的问题。
- en: Diving into the [source code](https://github.com/langchain-ai/langchain/blob/df40cd233f0690c1fc82d6fc0a1d25afdd7fdd42/langchain/vectorstores/base.py#L393-L398)
    of the retriever, we can see that the *get_relevant_dcouments* calls *similarity_search*
    by default. The method returns the top n chunks (4 by default but I set the number
    as 20 in my code) based on the computed distance metric ([cosine distance](https://en.wikipedia.org/wiki/Cosine_similarity)
    by default) ranging from 0 to 1 which measures the ‘similarity’ between the vector
    of the query and the vector of the document chunks.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 深入研究检索器的[源代码](https://github.com/langchain-ai/langchain/blob/df40cd233f0690c1fc82d6fc0a1d25afdd7fdd42/langchain/vectorstores/base.py#L393-L398)，我们可以看到*get_relevant_dcouments*默认调用*similarity_search*。该方法根据计算的距离度量（默认使用[余弦距离](https://en.wikipedia.org/wiki/Cosine_similarity)）返回前n个片段（默认4个，但我在代码中将数量设置为20），这些片段的距离范围从0到1，用于测量查询向量与文档片段向量之间的“相似性”。
- en: 'Back to the SIR dataset, we notice that **each line tells almost the same story**:
    on which date, in which city, and how many people are marked as which group. There
    is no surprise the vectors representing these lines are similar to each other.
    A quick check of the similarity score gives us the fact that lots of lines end
    up with a score around 0.29.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 回到SIR数据集，我们注意到**每行几乎讲述了相同的故事**：哪个日期、哪个城市、以及多少人被标记为哪个组。毫不奇怪，代表这些行的向量彼此相似。快速检查相似度得分发现很多行的得分接近0.29。
- en: 'Therefore, a similarity score is not strong enough to distinguish lines of
    how they are relevant to the query: This is always the case in tabular data where
    lines do not have a significant differences between each other. **We need stronger
    filters that can work on the metadata.**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，相似度得分不足以区分行与查询的相关性：这在表格数据中总是如此，因为行之间没有显著差异。**我们需要更强的过滤器来处理元数据。**
- en: CSVLoader with Customized MetaData
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带有自定义元数据的CSVLoader
- en: It seems evident that the improvement of the chatbot’s performance depends largely
    on the efficiency of the retriever. To do so, we start by defining a customized
    CSVLoader with can communicate the metadata information with the retriever.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，聊天机器人的性能提升在很大程度上依赖于检索器的效率。为此，我们首先定义了一个自定义的CSVLoader，它可以将元数据与检索器进行通信。
- en: 'We write the following code:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们编写了以下代码：
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To save space, I have omitted the code that is the same as the original API
    and only included the additional few lines which are mainly used to add certain
    columns that require special attention to the metadata. Indeed, in the printed
    data above, you can notice two parts: page contents and metadata. The standard
    CSVLoader writes all the columns of the table to page contents and only data resources
    and line numbers into the metadata. The defined “MetaDataCSVLoader” allows us
    to write other columns into the metadata.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为节省空间，我省略了与原始API相同的代码，仅包含了主要用于将需要特别注意的某些列写入元数据的附加几行代码。确实，在上面的打印数据中，你可以注意到两个部分：页面内容和元数据。标准CSVLoader将表格的所有列写入页面内容，只将数据资源和行号写入元数据。定义的“MetaDataCSVLoader”允许我们将其他列写入元数据。
- en: We re-create the vector store now, with the same steps as the section above,
    except for the data loader in which we add two metadata_columns “time” and “city”.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们重新创建向量存储，步骤与上述章节相同，只是数据加载器中添加了两个元数据列“time”和“city”。
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: SelfQuerying on MetaData Informed Vectorstore
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自查询元数据通知的向量存储
- en: 'Now we are ready to use the [SelfQuerying API](https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/self_query/)
    of LangChain:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备使用LangChain的[SelfQuerying API](https://js.langchain.com/docs/modules/data_connection/retrievers/how_to/self_query/)：
- en: 'According to LangChain’s documentation: A self-querying retriever is one that,
    as the name suggests, can query itself. … This allows the retriever to not only
    use the user-input query for semantic similarity comparison with the contents
    of stored documents, but to also extract filters from the user query on the metadata
    of stored documents and to execute those filters.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 LangChain 的文档：自查询检索器顾名思义，就是可以对自身进行查询的检索器。… 这使得检索器不仅可以使用用户输入的查询与存储文档的内容进行语义相似性比较，还可以从用户查询中提取关于存储文档的元数据的过滤器，并执行这些过滤器。
- en: '![](../Images/c80238ccb42251d10b5d1c3e53533d3c.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c80238ccb42251d10b5d1c3e53533d3c.png)'
- en: 'Image by LangChain: Self-Querying illustration'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源于 LangChain：自查询示意图
- en: 'You can understand now why I emphasize the metadata in the last chapter: it
    is based on which the ChatGPT or other LLMs can build on the filter to get the
    most relevant information from the dataset. We use the following code to build
    such a self-querying retriever by describing metadata and the document content
    description based on which a well-performed filter can be built to extract the
    accurate information. In particular, we give a *metadata_field_info* to the retriever,
    specifying the type and description of the two columns that we want the retriever
    to pay more attention to.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以理解为什么我在上一章强调元数据：这是基于元数据，ChatGPT 或其他 LLM 能够在其上构建过滤器，以从数据集中获取最相关的信息。我们使用以下代码构建这样的自查询检索器，通过描述元数据和文档内容描述来构建一个性能良好的过滤器，以提取准确的信息。特别地，我们给检索器提供一个*metadata_field_info*，指定我们希望检索器更多关注的两列的类型和描述。
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can now define a similar *ConversationalRetriverChain* to query the SIR
    dataset, but this time, with the *SelfQueryRetriever.* Let us see what will happen
    now when we ask the same question: “Which city has the most infectious people
    on2018–02–03?”'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以定义一个类似的*ConversationalRetriverChain*来查询SIR数据集，但这次使用*SelfQueryRetriever*。让我们看看当我们问同样的问题时会发生什么：“2018年2月3日哪个城市传染人数最多？”
- en: The chatbot said::” The city with the maximum infectious people on 2018–02–03
    is the city3 with 1413 infectious people.”
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天机器人说：“2018年2月3日传染人数最多的城市是 city3，传染人数为1413。”
- en: Ladies and gentlemen, it is correct! The chatbot is doing its job with a better
    retriever!
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 各位女士们先生们，结果是正确的！聊天机器人在使用更好的检索器时表现更好！
- en: 'There is no harm to see which relevant documents the retiever returns this
    time and it gives:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这次查看检索器返回了哪些相关文档是没有害处的，结果是：
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You might notice at once that there are “time” and “city” in “metadata” in the
    retrieved documents now.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会立刻注意到在检索到的文档中现在有“时间”和“城市”这些“元数据”。
- en: Conclusion
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'In this blog post, I have explored the limitations of ChatGPT when querying
    CSV format datasets, using the SIR dataset from 10 cities over a 90-day period
    as an example. To address these limitations, I have proposed a novel approach:
    **a metadata-aware CSV data loader that enables us to leverage the self-querying
    API, significantly improving the accuracy and performance of the Chatbot.**'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇博客文章中，我探讨了 ChatGPT 在查询 CSV 格式数据集时的局限性，以10个城市的SIR数据集在90天内为例。为了应对这些局限性，我提出了一种新方法：**一种感知元数据的CSV数据加载器，使我们能够利用自查询API，显著提高聊天机器人的准确性和性能。**
