- en: 'The Guide To LLM Evals: How To Build and Benchmark Your Evals'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM 评估指南：如何构建和基准测试你的评估
- en: 原文：[https://towardsdatascience.com/llm-evals-setup-and-the-metrics-that-matter-2cc27e8e35f3?source=collection_archive---------0-----------------------#2023-10-13](https://towardsdatascience.com/llm-evals-setup-and-the-metrics-that-matter-2cc27e8e35f3?source=collection_archive---------0-----------------------#2023-10-13)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/llm-evals-setup-and-the-metrics-that-matter-2cc27e8e35f3?source=collection_archive---------0-----------------------#2023-10-13](https://towardsdatascience.com/llm-evals-setup-and-the-metrics-that-matter-2cc27e8e35f3?source=collection_archive---------0-----------------------#2023-10-13)
- en: '![](../Images/0141ad936d35fc1e8736f72b7ec61ce8.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/0141ad936d35fc1e8736f72b7ec61ce8.png)'
- en: Image created by author using Dalle-3 via Bing Chat
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由作者使用 Dalle-3 通过 Bing Chat 创建
- en: How to build and run LLM evals — and why you should use precision and recall
    when benchmarking your LLM prompt template
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何构建和运行 LLM 评估——以及为何在基准测试你的 LLM 提示模板时使用精确度和召回率
- en: '[](https://aparnadhinak.medium.com/?source=post_page-----2cc27e8e35f3--------------------------------)[![Aparna
    Dhinakaran](../Images/e431ee69563ecb27c86f3428ba53574c.png)](https://aparnadhinak.medium.com/?source=post_page-----2cc27e8e35f3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2cc27e8e35f3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2cc27e8e35f3--------------------------------)
    [Aparna Dhinakaran](https://aparnadhinak.medium.com/?source=post_page-----2cc27e8e35f3--------------------------------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://aparnadhinak.medium.com/?source=post_page-----2cc27e8e35f3--------------------------------)[![Aparna
    Dhinakaran](../Images/e431ee69563ecb27c86f3428ba53574c.png)](https://aparnadhinak.medium.com/?source=post_page-----2cc27e8e35f3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----2cc27e8e35f3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----2cc27e8e35f3--------------------------------)
    [Aparna Dhinakaran](https://aparnadhinak.medium.com/?source=post_page-----2cc27e8e35f3--------------------------------)'
- en: ·
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff32f85889f3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllm-evals-setup-and-the-metrics-that-matter-2cc27e8e35f3&user=Aparna+Dhinakaran&userId=f32f85889f3a&source=post_page-f32f85889f3a----2cc27e8e35f3---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2cc27e8e35f3--------------------------------)
    ·12 min read·Oct 13, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2cc27e8e35f3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllm-evals-setup-and-the-metrics-that-matter-2cc27e8e35f3&user=Aparna+Dhinakaran&userId=f32f85889f3a&source=-----2cc27e8e35f3---------------------clap_footer-----------)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ff32f85889f3a&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllm-evals-setup-and-the-metrics-that-matter-2cc27e8e35f3&user=Aparna+Dhinakaran&userId=f32f85889f3a&source=post_page-f32f85889f3a----2cc27e8e35f3---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----2cc27e8e35f3--------------------------------)
    ·12 分钟阅读·2023年10月13日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F2cc27e8e35f3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllm-evals-setup-and-the-metrics-that-matter-2cc27e8e35f3&user=Aparna+Dhinakaran&userId=f32f85889f3a&source=-----2cc27e8e35f3---------------------clap_footer-----------)'
- en: --
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2cc27e8e35f3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllm-evals-setup-and-the-metrics-that-matter-2cc27e8e35f3&source=-----2cc27e8e35f3---------------------bookmark_footer-----------)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F2cc27e8e35f3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fllm-evals-setup-and-the-metrics-that-matter-2cc27e8e35f3&source=-----2cc27e8e35f3---------------------bookmark_footer-----------)'
- en: '*This piece is co-authored by* [*Ilya Reznik*](https://ibreznik.com/)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*这篇文章由* [*Ilya Reznik*](https://ibreznik.com/) *共同创作*'
- en: Large language models (LLMs) are an incredible tool for developers and business
    leaders to create new value for consumers. They make personal recommendations,
    translate between unstructured and structured data, summarize large amounts of
    information, and do so much more.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）是开发者和商业领袖为消费者创造新价值的不可思议的工具。它们可以提供个人化推荐、在非结构化数据和结构化数据之间进行翻译、总结大量信息，等等。
- en: 'As the applications multiply, so does the importance of measuring the performance
    of LLM-based applications. This is a nontrivial problem for several reasons: user
    feedback or any other **“source of truth” is extremely limited and often nonexistent**;
    even when possible, **human labeling is still expensive;** and it is easy to make
    these applications **complex**.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 随着应用程序的增加，衡量基于LLM的应用程序的性能变得越来越重要。这是一个非平凡的问题，原因有几个：用户反馈或任何其他**“真实来源”极其有限且往往不存在**；即使可能，**人工标记仍然很昂贵**；而且这些应用程序很容易变得**复杂**。
- en: This complexity is often hidden by the abstraction layers of code and only becomes
    apparent when things go wrong. One line of code can initiate a cascade of calls
    (spans). **Different evaluations are required for each span**, thus multiplying
    your problems. For example, the simple code snippet below triggers multiple sub-LLM
    calls.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这种复杂性通常被代码的抽象层隐藏，只有当事情出错时才会显现出来。一行代码可以触发一连串的调用（跨度）。**每个跨度需要不同的评估**，从而使问题倍增。例如，下面的简单代码片段会触发多个子LLM调用。
- en: '![](../Images/5300072b0e27ee545a3c0ded1c702754.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5300072b0e27ee545a3c0ded1c702754.png)'
- en: Diagram by author
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 作者绘制的图表
- en: Fortunately, we can use the power of LLMs to automate the evaluation. In this
    article, we will delve into how to set this up and make sure it is reliable.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们可以利用LLM的力量来自动化评估。在这篇文章中，我们将深入探讨如何设置这一过程并确保其可靠性。
- en: '**The core of LLM evals is AI evaluating AI.**'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**LLM评估的核心是AI评估AI。**'
- en: While this may sound circular, we have always had human intelligence evaluate
    human intelligence (for example, at a job interview or your college finals). Now
    AI systems can finally do the same for other AI systems.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这听起来有些循环，但我们一直以来都是由人类智慧来评估人类智慧（例如，在面试或大学期末考试中）。现在，AI系统终于可以对其他AI系统进行同样的评估。
- en: 'The process here is for LLMs to generate synthetic ground truth that can be
    used to evaluate another system. Which begs a question: why not use human feedback
    directly? Put simply, because you will never have enough of it.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的过程是让LLM生成可以用于评估另一个系统的合成基准。这引出了一个问题：为什么不直接使用人工反馈？简单来说，因为你永远不会拥有足够的人工反馈。
- en: Getting human feedback on even one percent of your input/output pairs is a gigantic
    feat. Most teams don’t even get that. But in order for this process to be truly
    useful, it is important to have evals on every LLM sub-call, of which we have
    already seen there can be many.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于你输入/输出对中的哪怕只有百分之一获得人工反馈，这也是一项巨大的成就。大多数团队甚至无法做到这一点。但是，为了使这一过程真正有用，重要的是对每个LLM子调用进行评估，我们已经看到，LLM子调用可能会有很多。
- en: Let’s explore how to do this.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来探讨一下如何做到这一点。
- en: LLM Model Evaluation vs. LLM System Evaluation
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM模型评估与LLM系统评估
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: LLM Model Evals
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM模型评估
- en: You might have heard of LLM evals. This term gets used in many different ways
    that all sound very similar but actually are very different. One of the more common
    ways it gets used is in what we will call **LLM model evals**. LLM model evals
    are focused on the overall performance of the foundational models. The companies
    launching the original customer-facing LLMs needed a way to quantify their effectiveness
    across an array of different tasks.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能听说过LLM评估。这个术语在许多不同的情况下使用，听起来很相似，但实际上非常不同。一个更常见的用法是我们所称的**LLM模型评估**。LLM模型评估关注于基础模型的整体性能。推出原始面向客户的LLM的公司需要一种量化其在各种任务中的有效性的方法。
- en: '![](../Images/ecfd3204dba3144c65b2e87736695bc1.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ecfd3204dba3144c65b2e87736695bc1.png)'
- en: Diagram by author | In this case, we are evaluating two different open source
    foundation models. We are testing the same dataset across the twomodels and seeing
    how their metrics, like hellaswag or mmlu, stack up.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 作者绘制的图表 | 在这种情况下，我们正在评估两个不同的开源基础模型。我们在这两个模型上测试相同的数据集，看看它们的指标，例如hellaswag或mmlu，如何表现。
- en: One popular library that has LLM model evals is the [OpenAI Eval library](/how-to-best-leverage-openais-evals-framework-c38bcef0ec47),
    which was originally focused on the model evaluation use case. There are many
    metrics out there, like [HellaSwag](https://arxiv.org/abs/1905.07830) (which evaluates
    how well an LLM can complete a sentence), [TruthfulQA](https://arxiv.org/abs/2109.07958)
    (measuring truthfulness of model responses), and [MMLU](https://arxiv.org/abs/2009.03300)
    (which measures how well the LLM can multitask). There are even [LLM leaderboards](https://arize.com/blog-course/llm-leaderboards-benchmarks/)
    that looks at how well the open-source LLMs stack up against each other.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一个流行的库是[OpenAI Eval库](/how-to-best-leverage-openais-evals-framework-c38bcef0ec47)，最初专注于模型评估使用案例。现在有许多指标，如[HellaSwag](https://arxiv.org/abs/1905.07830)（评估LLM完成句子的能力）、[TruthfulQA](https://arxiv.org/abs/2109.07958)（衡量模型回应的真实性）和[MMLU](https://arxiv.org/abs/2009.03300)（评估LLM的多任务能力）。甚至还有[LLM排行榜](https://arize.com/blog-course/llm-leaderboards-benchmarks/)查看开源LLM的表现如何。
- en: LLM System Evals
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM系统评估
- en: Up to this point, we have discussed LLM model evaluation. In contrast, **LLM
    system evaluation** is the complete evaluation of components that you have control
    of in your system. The most important of these components are the prompt (or [prompt
    template](https://arize.com/blog/prompt-templates-functions-and-prompt-window-management/))
    and context. LLM system evals assess how well your inputs can determine your outputs.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了LLM模型评估。相比之下，**LLM系统评估**是对你在系统中可控组件的完整评估。这些组件中最重要的是提示（或[提示模板](https://arize.com/blog/prompt-templates-functions-and-prompt-window-management/)）和上下文。LLM系统评估评估你的输入如何决定你的输出。
- en: LLM system evals may, for example, hold the LLM constant and change the prompt
    template. Since prompts are more dynamic parts of your system, this evaluation
    makes a lot of sense throughout the lifetime of the project. For example, an LLM
    can evaluate your chatbot responses for usefulness or politeness, and the same
    eval can give you information about performance changes over time in production.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，LLM系统评估可以保持LLM不变并更改提示模板。由于提示是系统中更动态的部分，这种评估在项目生命周期中非常有意义。例如，LLM可以评估你的聊天机器人响应的有用性或礼貌性，并且相同的评估可以提供关于生产环境中性能变化的信息。
- en: '![](../Images/f45a254033d1cb9a594b71670cd7f58d.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f45a254033d1cb9a594b71670cd7f58d.png)'
- en: Diagram by author | *In this case, we are evaluating two different prompt templates
    on a single foundation model. We are testing the same dataset across the two templates
    and seeing how their metrics like precision and recall stack up.*
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 作者图示 | *在这种情况下，我们正在评估在单一基础模型上使用的两种不同提示模板。我们在这两个模板上测试相同的数据集，并查看它们的精确度和召回率等指标的表现。*
- en: Which To Use? It Depends On Your Role
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用哪种？这取决于你的角色
- en: There are distinct personas who make use of LLM evals. One is the model developer
    or an engineer tasked with fine-tuning the core LLM, and the other is the practitioner
    assembling the user-facing system.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LLM评估的角色有区别。一种是负责调整核心LLM的模型开发者或工程师，另一种是组装用户系统的实践者。
- en: There are very few LLM model developers, and they tend to work for places like
    OpenAI, Anthropic, Google, Meta, and elsewhere. **Model developers care about
    LLM model evals,** as their job is to deliver a model that caters to a wide variety
    of use cases.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: LLM模型开发者非常少，他们通常在OpenAI、Anthropic、Google、Meta等公司工作。**模型开发者关心LLM模型评估，** 因为他们的工作是提供一个适用于各种使用案例的模型。
- en: For ML practitioners, the task also starts with model evaluation. One of the
    first steps in developing an LLM system is picking a model (i.e. GPT 3.5 vs 4
    vs Palm, etc.). The LLM model eval for this group, however, is often a one-time
    step. Once the question of which model performs best in your use case is settled,
    the majority of the rest of the application’s lifecycle will be defined by LLM
    system evals. Thus, **ML practitioners care about both LLM model evals and LLM
    system evals but likely spend much more time on the latter**.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于ML实践者，任务同样从模型评估开始。开发LLM系统的第一步之一是选择一个模型（即GPT 3.5 vs 4 vs Palm等）。然而，对于这一群体，LLM模型评估通常是一个一次性的步骤。一旦确定了哪个模型在你的使用案例中表现最好，应用生命周期的大部分将由LLM系统评估决定。因此，**ML实践者关心LLM模型评估和LLM系统评估，但可能在后者上花费更多时间**。
- en: LLM System Eval Metrics Vary By Use Case
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM系统评估指标因使用案例而异
- en: 'Having worked with other ML systems, your first question is likely this: “What
    should the outcome metric be?” The answer depends on what you are trying to evaluate.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在与其他机器学习系统合作后，你首先会问这个问题：“结果指标应该是什么？”答案取决于你要评估的内容。
- en: '**Extracting structured information**: You can look at how well the LLM extracts
    information. For example, you can look at completeness (is there information in
    the input that is not in the output?).'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提取结构化信息**：你可以查看LLM提取信息的效果。例如，你可以查看完整性（输入中是否有输出中没有的信息？）。'
- en: '**Question answering**: How well does the system answer the user’s question?
    You can look at the accuracy, politeness, or brevity of the answer — or all of
    the above.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问答**：系统回答用户问题的效果如何？你可以查看答案的准确性、礼貌性或简洁性——或者以上所有方面。'
- en: '**Retrieval Augmented Generation (RAG)**: Are the retrieved documents and final
    answer relevant?'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检索增强生成（RAG）**：检索到的文档和最终答案是否相关？'
- en: As a system designer, you are ultimately responsible for system performance,
    and so it is up to you to understand which aspects of the system need to be evaluated.
    For example, If you have an LLM interacting with children, like a tutoring app,
    you would want to make sure that the responses are age-appropriate and are not
    toxic.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 作为系统设计师，你最终负责系统性能，因此你需要了解系统中哪些方面需要评估。例如，如果你有一个与儿童互动的LLM，比如一个辅导应用，你会希望确保回答是适龄的，并且不含有毒内容。
- en: 'Some common evaluations being employed today are relevance, hallucinations,
    question-answering accuracy, and toxicity. Each one of these evals will have different
    templates based on what you are trying to evaluate. Here is an example with relevance:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 目前一些常用的评估包括相关性、幻觉、问答准确性和毒性。每一个评估工具都会有不同的模板，具体取决于你要评估的内容。以下是一个关于相关性的示例：
- en: 'This example uses the open-source [Phoenix tool](https://github.com/Arize-ai/phoenix)
    for simplicity (full disclosure: I am on the team that developed Phoenix). Within
    the Phoenix tool, there exist default templates for most common use cases. Here
    is the one we will use for this example:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例为了简单起见使用开源的[Phoenix工具](https://github.com/Arize-ai/phoenix)（完全披露：我在开发Phoenix的团队中）。在Phoenix工具中，存在适用于大多数常见用例的默认模板。以下是我们将用于本示例的模板：
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We will also use OpenAI’s GPT-4 model and scikitlearn’s precision/recall metrics.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将使用OpenAI的GPT-4模型和scikit-learn的精确度/召回率指标。
- en: 'First, we will import all necessary dependencies:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将导入所有必要的依赖项：
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, let’s bring in the dataset:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们引入数据集：
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now let’s conduct our evaluation:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们进行评估：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Evaluating LLM-Based Systems with LLMs
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用LLM对LLM基础系统进行评估
- en: There are two distinct steps to the process of evaluating your LLM-based system
    with an LLM. First, establish a benchmark for your LLM evaluation metric. To do
    this, you put together a dedicated LLM-based eval whose only task is to label
    data as effectively as a human labeled your “golden dataset.” You then benchmark
    your metric against that eval. Then, run this LLM evaluation metric against results
    of your LLM application (more on this below).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LLM对LLM基础系统进行评估的过程包括两个不同的步骤。首先，为你的LLM评估指标建立基准。为此，你需要组建一个专门的LLM评估工具，其唯一任务是将数据标注得尽可能像人工标注你的“黄金数据集”一样有效。然后，你将你的指标与该评估工具进行基准测试。接着，将这个LLM评估指标应用于你的LLM应用结果（下面会详细说明）。
- en: How To Build An LLM Eval
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何构建LLM评估工具
- en: The first step, as we covered above, is to build a benchmark for your evaluations.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，如上所述，构建评估的基准是第一步。
- en: To do that, you must begin with a **metric best suited for your use case**.
    Then, you need the **golden dataset**. This should be representative of the type
    of data you expect the LLM eval to see. The golden dataset should have the “ground
    truth” label so that we can measure performance of the LLM eval template. Often
    such labels come from human feedback. Building such a dataset is laborious, but
    you can often find a standardized one for the most common use cases (as we did
    in the code above).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，你必须从**最适合你用例的指标**开始。然后，你需要**黄金数据集**。这应该能够代表你期望LLM评估工具看到的数据类型。黄金数据集应该有“真实标签”，以便我们可以衡量LLM评估模板的性能。这些标签通常来自人工反馈。构建这样的数据集是费力的，但你通常可以找到针对最常见用例的标准化数据集（正如我们在上面的代码中所做的）。
- en: '![](../Images/e8d4068cf295bc602a23ab7d4d565b6c.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e8d4068cf295bc602a23ab7d4d565b6c.png)'
- en: Diagram by author
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 作者绘制的图表
- en: Then you need to decide **which LLM** you want to use for evaluation. This could
    be a different LLM from the one you are using for your application. For example,
    you may be using Llama for your application and GPT-4 for your eval. Often this
    choice is influenced by questions of cost and accuracy.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你需要决定**使用哪个 LLM** 进行评估。这可能是与用于应用的 LLM 不同的另一个 LLM。例如，你可能在应用中使用 Llama，而在评估中使用
    GPT-4。这个选择通常受到成本和准确性问题的影响。
- en: '![](../Images/747ecc27cf13c132829bcb912ceabb24.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/747ecc27cf13c132829bcb912ceabb24.png)'
- en: Diagram by author
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图示
- en: 'Now comes the core component that we are trying to benchmark and improve: the
    **eval template**. If you’re using an existing library like OpenAI or Phoenix,
    you should start with an existing template and see how that prompt performs.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在来到我们试图基准化和改进的核心组件：**评估模板**。如果你使用的是像 OpenAI 或 Phoenix 这样的现有库，你应该从现有的模板开始，看看那个提示的表现如何。
- en: If there’s a specific nuance you want to incorporate, adjust the template accordingly
    or build your own from scratch.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有特定的细节你想要加入，可以相应地调整模板或从头开始构建自己的模板。
- en: 'Keep in mind that the template should have a clear structure, like the one
    we used in prior section. Be explicit about the following:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，模板应该有一个清晰的结构，就像我们在前面部分使用的那样。对以下内容要明确：
- en: '**What is the input?** In our example, it is the documents/context that was
    retrieved and the query from the user.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入是什么？** 在我们的例子中，它是检索到的文档/上下文和用户的查询。'
- en: '**What are we asking?** In our example, we’re asking the LLM to tell us if
    the document was relevant to the query'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**我们在问什么？** 在我们的例子中，我们要求 LLM 告诉我们文档是否与查询相关。'
- en: '**What are the possible output formats?** In our example, it is binary relevant/irrelevant,
    but it can also be multi-class (e.g., fully relevant, partially relevant, not
    relevant).'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可能的输出格式是什么？** 在我们的例子中，它是二元相关/不相关，但也可以是多类（例如，完全相关、部分相关、不相关）。'
- en: '![](../Images/bebbe4a5ea7985283fb654af5381d1a1.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/bebbe4a5ea7985283fb654af5381d1a1.png)'
- en: Diagram by author
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图示
- en: You now need to run the eval across your golden dataset. Then you can **generate
    metrics** (overall accuracy, precision, recall, F1, etc.) to determine the benchmark.
    It is important to look at more than just overall accuracy. We’ll discuss that
    below in more detail.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在需要在你的黄金数据集上运行评估。然后你可以**生成指标**（总体准确性、精确度、召回率、F1 等）来确定基准。重要的是要查看的不仅仅是总体准确性。我们将在下面详细讨论。
- en: If you are not satisfied with the performance of your LLM evaluation template,
    you need to change the prompt to make it perform better. This is an iterative
    process informed by hard metrics. As is always the case, it is important to avoid
    overfitting the template to the golden dataset. Make sure to have a representative
    holdout set or run a k-fold cross-validation.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对 LLM 评估模板的性能不满意，你需要更改提示以提高其表现。这是一个由硬指标驱动的迭代过程。像往常一样，重要的是避免将模板过度拟合到黄金数据集。确保有一个代表性的保留集或进行
    k 折交叉验证。
- en: '![](../Images/e160e9a9dee0af2ad439b7df0361e42c.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e160e9a9dee0af2ad439b7df0361e42c.png)'
- en: Diagram by author
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 作者提供的图示
- en: Finally, you arrive at your **benchmark**. The optimized performance on the
    golden dataset represents how confident you can be on your LLM eval. It will not
    be as accurate as your ground truth, but it will be accurate enough, and it will
    cost much less than having a human labeler in the loop on every example.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，你到达了你的**基准**。在黄金数据集上的优化性能代表了你对 LLM 评估的信心。它可能不会像你的真实数据那样准确，但足够准确，而且成本远低于每个示例都需要人工标注的情况。
- en: Preparing and customizing your prompt templates allows you to set up test cases.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 准备和自定义你的提示模板可以让你设置测试用例。
- en: Why You Should Use Precision and Recall When Benchmarking Your LLM Prompt Template
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么在基准测试你的 LLM 提示模板时应该使用精确度和召回率
- en: The industry has not fully standardized best practices on LLM evals. Teams commonly
    do not know how to establish the right benchmark metrics.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 行业尚未完全标准化 LLM 评估的最佳实践。团队通常不知道如何建立正确的基准指标。
- en: Overall accuracy is used often, but it is not enough.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 总体准确性经常被使用，但这还不够。
- en: 'This is one of the most common problems in data science in action: very significant
    class imbalance makes accuracy an impractical metric.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这是数据科学中最常见的问题之一：非常显著的类别不平衡使得准确性成为一个不切实际的指标。
- en: 'Thinking about it in terms of the relevance metric is helpful. Say you go through
    all the trouble and expense of putting together the most relevant chatbot you
    can. You pick an LLM and a template that are right for the use case. This should
    mean that significantly more of your examples should be evaluated as “relevant.”
    Let’s pick an extreme number to illustrate the point: 99.99% of all queries return
    relevant results. Hooray!'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 从相关性指标的角度考虑是有帮助的。假设你经历了所有麻烦和费用，组建了你能组建的最相关的聊天机器人。你选择了一个适合用例的 LLM 和模板。这应该意味着你的示例中会有显著更多的被评估为“相关”的。让我们选择一个极端的数字来说明这个观点：99.99%
    的所有查询返回相关结果。太棒了！
- en: Now look at it from the point of view of the LLM eval template. If the output
    was “relevant” in all cases, without even looking at the data, it would be right
    99.99% of the time. But it would simultaneously miss all of the (arguably most)
    important cases — ones where the model returns irrelevant results, which are the
    very ones we must catch.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在从 LLM 评估模板的角度来看。如果输出在所有情况下都是“相关”，即使不查看数据，它的正确率也会是99.99%。但它同时会错过所有（可以说是最重要的）案例——即模型返回无关结果的那些，我们必须捕捉这些结果。
- en: In this example, accuracy would be high, but [precision and recall](https://arize.com/blog-course/precision-vs-recall/)
    (or a combination of the two, like the [F1 score](https://arize.com/blog-course/f1-score/))
    would be very low. Precision and recall are a better measure of your model’s performance
    here.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，准确率会很高，但[精确率和召回率](https://arize.com/blog-course/precision-vs-recall/)（或者它们的组合，比如[F1
    分数](https://arize.com/blog-course/f1-score/)）会非常低。在这里，精确率和召回率更能衡量你模型的表现。
- en: The other useful visualization is the confusion matrix, which basically lets
    you see correctly and incorrectly predicted percentages of relevant and irrelevant
    examples.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有用的可视化是混淆矩阵，它基本上让你看到相关和无关示例的正确和错误预测百分比。
- en: '![](../Images/e0713aae1a956a4766d5a56b2b5e8d2c.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e0713aae1a956a4766d5a56b2b5e8d2c.png)'
- en: 'Diagram by author | *In this example, we see that the highest percentage of
    predictions are correct: a relevant example in the golden dataset has an 88% chance
    of being labeled as such by our eval. However, we see that the eval performs significantly
    worse on “irrelevant” examples, mislabeling them more than 27% of the time.*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 作者图表 | *在这个例子中，我们看到最高比例的预测是正确的：在黄金数据集中，一个相关的例子被我们的评估标记为相关的机会是88%。然而，我们看到评估在“无关”示例上的表现明显较差，错误标记的概率超过27%*。
- en: How To Run LLM Evals On Your Application
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何在你的应用程序上运行 LLM 评估
- en: At this point you should have both your model and your tested LLM eval. You
    have proven to yourself that the eval works and have a quantifiable understanding
    of its performance against the ground truth. Time to build more trust!
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 此时你应该拥有模型和经过测试的 LLM 评估。你已经证明了评估有效，并对其相对于真实情况的表现有了可量化的理解。是时候建立更多信任了！
- en: Now we can actually use our eval on our application. This will help us measure
    how well our LLM application is doing and figure out how to improve it.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以实际使用我们的评估来评估应用程序。这将帮助我们衡量我们的 LLM 应用程序的表现，并找出如何改进。
- en: '![](../Images/4f86ea814b4142eb391e3b536b290bfb.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4f86ea814b4142eb391e3b536b290bfb.png)'
- en: Diagram by author
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 作者图表
- en: 'The LLM system eval runs your entire system with one extra step. For example:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 系统评估在你的整个系统中运行一个额外的步骤。例如：
- en: You retrieve your input docs and add them to your prompt template, together
    with sample user input.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你检索输入文档并将它们添加到你的提示模板中，以及样本用户输入。
- en: You provide that prompt to the LLM and receive the answer.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将这个提示提供给 LLM 并接收答案。
- en: You provide the prompt and the answer to your eval, asking it if the answer
    is relevant to the prompt.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将提示和答案提供给你的评估，询问答案是否与提示相关。
- en: It is a best practice not to do LLM evals with one-off code but rather a library
    that has built-in prompt templates. This increases reproducibility and allows
    for more flexible evaluation where you can swap out different pieces.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳实践是不使用一次性代码来进行 LLM 评估，而是使用具有内置提示模板的库。这增加了可重复性，并允许更灵活的评估，可以更换不同的部分。
- en: 'These evals need to work in three different environments:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这些评估需要在三种不同的环境中有效：
- en: Pre-production
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预生产
- en: When you’re doing the benchmarking.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 当你进行基准测试时。
- en: Pre-production
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预生产
- en: When you’re testing your application. This is somewhat similar to the offline
    evaluation concept in traditional ML. The idea is to understand the performance
    of your system before you ship it to customers.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在测试你的应用程序时。这在某种程度上类似于传统机器学习中的离线评估概念。其目的是在你将系统交付给客户之前了解系统的性能。
- en: Production
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生产
- en: When it’s deployed. Life is messy. Data drifts, users drift, models drift, all
    in unpredictable ways. Just because your system worked well once doesn’t mean
    it will do so on Tuesday at 7 p.m. Evals help you continuously understand your
    system’s performance after deployment.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 当系统投入使用时，现实生活是混乱的。数据漂移、用户漂移、模型漂移，所有这些都是不可预测的。仅仅因为你的系统曾经运行良好，并不意味着它在周二下午7点也会如此。评估有助于你在部署后持续了解系统的性能。
- en: '![](../Images/98b24eb1c65e16c0d7245e4643e68083.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/98b24eb1c65e16c0d7245e4643e68083.png)'
- en: Diagram by author
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 作者图示
- en: Questions To Consider
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 需要考虑的问题
- en: '**How many rows should you sample?**'
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**你应该抽取多少行？**'
- en: The LLM-evaluating-LLM paradigm is not magic. You cannot evaluate every example
    you have ever run across — that would be prohibitively expensive. However, you
    already have to sample data during human labeling, and having more automation
    only makes this easier and cheaper. So you can sample more rows than you would
    with human labeling.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: LLM评估LLM的范式并不是魔法。你不能评估你曾经遇到的每一个示例——那样会代价高昂。然而，你在人工标记期间已经需要抽样数据，更多的自动化只会使这变得更容易和便宜。因此，你可以抽取比人工标记更多的行。
- en: '**What evals should you use?**'
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**你应该使用什么评估？**'
- en: This depends largely on your use case. For search and retrieval, relevancy-type
    evals work best. Toxicity and hallucinations have specific eval patterns (more
    on that above).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这在很大程度上取决于你的使用案例。对于搜索和检索，相关性类型的评估效果最好。毒性和幻觉有特定的评估模式（上述已有提及）。
- en: Some of these evals are important in the troubleshooting flow. Question-answering
    accuracy might be a good overall metric, but if you dig into why this metric is
    underperforming in your system, you may discover it is because of bad retrieval,
    for example. There are often many possible reasons, and you might need multiple
    metrics to get to the bottom of it.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些评估在故障排查流程中很重要。问答准确性可能是一个很好的总体指标，但如果你深入挖掘为什么这个指标在你的系统中表现不佳，你可能会发现这可能是由于检索不良等原因。通常有很多可能的原因，你可能需要多个指标来找到根本原因。
- en: '**What model should you use?**'
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**你应该使用什么模型？**'
- en: It is impossible to say that one model works best for all cases. Instead, you
    should run model evaluations to understand which model is right for your application.
    You may also need to consider tradeoffs of recall vs. precision, depending on
    what makes sense for your application. In other words, do some data science to
    understand this for your particular case.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 不可能说哪种模型适用于所有情况。相反，你应该进行模型评估，以了解哪个模型适合你的应用程序。你还可能需要考虑召回率与精度的权衡，具体取决于对你的应用程序来说哪种更有意义。换句话说，做一些数据科学来了解你的特定情况。
- en: '![](../Images/3fcd84c78cd24147d2effdb88b6893e2.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3fcd84c78cd24147d2effdb88b6893e2.png)'
- en: Diagram by author
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 作者图示
- en: Conclusion
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Being able to evaluate the performance of your application is very important
    when it comes to production code. In the era of LLMs, the problems have gotten
    harder, but luckily we can use the very technology of LLMs to help us in running
    evaluations. [LLM evaluation](https://arize.com/blog-course/llm-evaluation-the-definitive-guide/)
    should test the whole system and not just the underlying LLM model — think about
    how much a prompt template matters to user experience. Best practices, standardized
    tooling, and curated datasets simplify the job of developing LLM systems.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 能够评估应用程序的性能在生产代码中是非常重要的。在LLM时代，问题变得更加复杂，但幸运的是，我们可以使用LLM技术来帮助我们进行评估。[LLM评估](https://arize.com/blog-course/llm-evaluation-the-definitive-guide/)应该测试整个系统，而不仅仅是基础LLM模型——考虑一下提示模板对用户体验的重要性。最佳实践、标准化工具和精心策划的数据集简化了LLM系统的开发工作。
