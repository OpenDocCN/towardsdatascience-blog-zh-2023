- en: 'Logistic Regression: Faceoff and Conceptual Understanding'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é€»è¾‘å›å½’ï¼šå¯¹å†³ä¸æ¦‚å¿µç†è§£
- en: åŸæ–‡ï¼š[https://towardsdatascience.com/logistic-regression-faceoff-67560de4f492?source=collection_archive---------5-----------------------#2023-05-18](https://towardsdatascience.com/logistic-regression-faceoff-67560de4f492?source=collection_archive---------5-----------------------#2023-05-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: åŸæ–‡ï¼š[https://towardsdatascience.com/logistic-regression-faceoff-67560de4f492?source=collection_archive---------5-----------------------#2023-05-18](https://towardsdatascience.com/logistic-regression-faceoff-67560de4f492?source=collection_archive---------5-----------------------#2023-05-18)
- en: What do log-losses and perfectly separated data have to do with hockey sticks?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é€»è¾‘æŸå¤±å’Œå®Œç¾åˆ†ç¦»çš„æ•°æ®ä¸å†°çƒæ£’æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ
- en: '[](https://medium.com/@igor-s?source=post_page-----67560de4f492--------------------------------)[![Igor
    Å egota](../Images/17c592b71fef9526a0679d47937837f6.png)](https://medium.com/@igor-s?source=post_page-----67560de4f492--------------------------------)[](https://towardsdatascience.com/?source=post_page-----67560de4f492--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----67560de4f492--------------------------------)
    [Igor Å egota](https://medium.com/@igor-s?source=post_page-----67560de4f492--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@igor-s?source=post_page-----67560de4f492--------------------------------)[![Igor
    Å egota](../Images/17c592b71fef9526a0679d47937837f6.png)](https://medium.com/@igor-s?source=post_page-----67560de4f492--------------------------------)[](https://towardsdatascience.com/?source=post_page-----67560de4f492--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----67560de4f492--------------------------------)
    [Igor Å egota](https://medium.com/@igor-s?source=post_page-----67560de4f492--------------------------------)'
- en: Â·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Â·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe5f8ebca4ad8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-faceoff-67560de4f492&user=Igor+%C5%A0egota&userId=e5f8ebca4ad8&source=post_page-e5f8ebca4ad8----67560de4f492---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----67560de4f492--------------------------------)
    Â·7 min readÂ·May 18, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F67560de4f492&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-faceoff-67560de4f492&user=Igor+%C5%A0egota&userId=e5f8ebca4ad8&source=-----67560de4f492---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[å…³æ³¨](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe5f8ebca4ad8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-faceoff-67560de4f492&user=Igor+%C5%A0egota&userId=e5f8ebca4ad8&source=post_page-e5f8ebca4ad8----67560de4f492---------------------post_header-----------)
    å‘è¡¨åœ¨ [Towards Data Science](https://towardsdatascience.com/?source=post_page-----67560de4f492--------------------------------)
    Â·7åˆ†é’Ÿé˜…è¯»Â·2023å¹´5æœˆ18æ—¥[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F67560de4f492&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-faceoff-67560de4f492&user=Igor+%C5%A0egota&userId=e5f8ebca4ad8&source=-----67560de4f492---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F67560de4f492&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-faceoff-67560de4f492&source=-----67560de4f492---------------------bookmark_footer-----------)![](../Images/91a7b0a99da3d1740a217fe617880118.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F67560de4f492&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-faceoff-67560de4f492&source=-----67560de4f492---------------------bookmark_footer-----------)![](../Images/91a7b0a99da3d1740a217fe617880118.png)'
- en: Photo by [Jerry Yu](https://unsplash.com/@jerryyu?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ç‰‡ç”± [Jerry Yu](https://unsplash.com/@jerryyu?utm_source=medium&utm_medium=referral)
    æä¾›ï¼Œæ¥æºäº [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Who ordered this?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è°ä¸‹çš„è¿™ä¸ªè®¢å•ï¼Ÿ
- en: As of this writing, Google search for â€œlogistic regression tutorialâ€ shows about
    11.2M results. Why add another thing to this pile?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æˆªè‡³æ’°å†™æœ¬æ–‡æ—¶ï¼Œè°·æ­Œæœç´¢â€œé€»è¾‘å›å½’æ•™ç¨‹â€æ˜¾ç¤ºå¤§çº¦æœ‰1120ä¸‡æ¡ç»“æœã€‚ä¸ºä»€ä¹ˆè¿˜è¦åœ¨è¿™å †ä¿¡æ¯ä¸­å†æ·»åŠ ä¸€ä»½ï¼Ÿ
- en: After reading a good number of articles, books and guides, I realized that most
    lack clear and intuitive explanations of how logistic regression works. Instead,
    they usually strive to be either practical, by showing how to run models, or as
    mathematically complete as possible, and as a consequence, basic concepts get
    buried underneath a forest of matrix algebra.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: é˜…è¯»äº†å¤§é‡æ–‡ç« ã€ä¹¦ç±å’ŒæŒ‡å—åï¼Œæˆ‘æ„è¯†åˆ°å¤§å¤šæ•°ç¼ºä¹å¯¹é€»è¾‘å›å½’å·¥ä½œåŸç†çš„æ¸…æ™°ç›´è§‚è§£é‡Šã€‚ç›¸åï¼Œå®ƒä»¬é€šå¸¸è¦ä¹ˆè‡´åŠ›äºå±•ç¤ºå¦‚ä½•è¿è¡Œæ¨¡å‹çš„å®ç”¨æ€§ï¼Œè¦ä¹ˆå°½å¯èƒ½åœ°æ•°å­¦å…¨é¢ï¼Œå› æ­¤åŸºæœ¬æ¦‚å¿µè¢«åŸ‹åœ¨çŸ©é˜µä»£æ•°çš„æ£®æ—ä¸­ã€‚
- en: 'We will start by clearing up what seem to be common misconceptions. Logistic
    regression is **not**:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä»æ¾„æ¸…çœ‹ä¼¼å¸¸è§çš„è¯¯è§£å¼€å§‹ã€‚é€»è¾‘å›å½’**ä¸æ˜¯**ï¼š
- en: linear regression but with sigmoid curve instead of a straight line
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: çº¿æ€§å›å½’ï¼Œä½†ç”¨ sigmoid æ›²çº¿ä»£æ›¿ç›´çº¿
- en: classification algorithm (but can be used for this)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ†ç±»ç®—æ³•ï¼ˆä½†å¯ä»¥ç”¨äºæ­¤ï¼‰
- en: sigmoid curve â€œfitâ€ of a decision boundary separating two classes of points
    in the x-y plane
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: sigmoid æ›²çº¿â€œæ‹Ÿåˆâ€äº†x-yå¹³é¢ä¸­åˆ†éš”ä¸¤ç±»ç‚¹çš„å†³ç­–è¾¹ç•Œ
- en: What *is* a logistic regression?
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»€ä¹ˆæ˜¯é€»è¾‘å›å½’ï¼Ÿ
- en: 'Logistic regression is a regression model that returns a probability of a binary
    outcome (0 or 1), assuming that log of the odds is a linear combination of one
    or more inputs. Odds is a ratio between probability of outcome happening (*p*)
    and the probability of the outcome not happening (*1-p*). When we have one input
    or predictor, this starting assumption is mathematically expressed as:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: é€»è¾‘å›å½’æ˜¯ä¸€ç§å›å½’æ¨¡å‹ï¼Œè¿”å›äºŒå…ƒç»“æœï¼ˆ0æˆ–1ï¼‰çš„æ¦‚ç‡ï¼Œå‡è®¾èµ”ç‡çš„å¯¹æ•°æ˜¯ä¸€ä¸ªæˆ–å¤šä¸ªè¾“å…¥çš„çº¿æ€§ç»„åˆã€‚èµ”ç‡æ˜¯ç»“æœå‘ç”Ÿçš„æ¦‚ç‡ï¼ˆ* p*ï¼‰ä¸ç»“æœä¸å‘ç”Ÿçš„æ¦‚ç‡ï¼ˆ*1-p*ï¼‰ä¹‹é—´çš„æ¯”ç‡ã€‚å½“æˆ‘ä»¬åªæœ‰ä¸€ä¸ªè¾“å…¥æˆ–é¢„æµ‹å˜é‡æ—¶ï¼Œè¿™ä¸€åˆå§‹å‡è®¾æ•°å­¦ä¸Šè¡¨ç¤ºä¸ºï¼š
- en: '![](../Images/685c506d6b3786c9ed389a77a810331b.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/685c506d6b3786c9ed389a77a810331b.png)'
- en: 'The goal behind logistic regression is to model cases when inputs are shifting
    the outcome probability progressively from 0 to 1\. The probability of the outcome
    being 1, *p*, can be derived from the previous equation and expressed as a function
    of inputs:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: é€»è¾‘å›å½’çš„ç›®æ ‡æ˜¯å»ºæ¨¡å½“è¾“å…¥å°†ç»“æœæ¦‚ç‡ä»0é€æ¸è½¬ç§»åˆ°1çš„æƒ…å†µã€‚ç»“æœä¸º1çš„æ¦‚ç‡ * p* å¯ä»¥ä»å…ˆå‰çš„æ–¹ç¨‹ä¸­æ¨å¯¼å‡ºæ¥ï¼Œå¹¶è¡¨ç¤ºä¸ºè¾“å…¥çš„å‡½æ•°ï¼š
- en: '![](../Images/eca13451561745c8a572f7c35592f11c.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eca13451561745c8a572f7c35592f11c.png)'
- en: In the last part we swapped from parameters *Î²â‚* and *Î²â‚€* to *k* and *xâ‚€*. Using
    *k* and *xâ‚€* will give us a clearer picture of the model as we go along. We will
    also stick to a single predictor variable *x*, as opposed to marching in with
    an army of matrices, so we can easily visualize logistic fits.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ€åä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬ä»å‚æ•°* Î²â‚* å’Œ* Î²â‚€* è½¬æ¢ä¸º * k* å’Œ * xâ‚€*ã€‚ä½¿ç”¨ * k* å’Œ * xâ‚€* å°†ä½¿æˆ‘ä»¬åœ¨ç»§ç»­è¿‡ç¨‹ä¸­å¯¹æ¨¡å‹æœ‰æ›´æ¸…æ™°çš„äº†è§£ã€‚æˆ‘ä»¬è¿˜å°†åšæŒä½¿ç”¨å•ä¸€é¢„æµ‹å˜é‡*x*ï¼Œè€Œä¸æ˜¯å¼•å…¥ä¸€å¤§å †çŸ©é˜µï¼Œè¿™æ ·æˆ‘ä»¬å¯ä»¥æ›´å®¹æ˜“åœ°å¯è§†åŒ–é€»è¾‘æ‹Ÿåˆã€‚
- en: Logistic curve
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é€»è¾‘æ›²çº¿
- en: 'We will begin by plotting the logistic curve, with parameters *xâ‚€ = 2.5* and
    *k = 3*, on an interval *x* between 0 and 5:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å¼€å§‹ç»˜åˆ¶é€»è¾‘æ›²çº¿ï¼Œå‚æ•°ä¸º * xâ‚€ = 2.5* å’Œ * k = 3*ï¼Œåœ¨åŒºé—´ * x* ä»0åˆ°5ï¼š
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/d11aca277e68ed608888dac0fb31053d.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d11aca277e68ed608888dac0fb31053d.png)'
- en: Points (red for y=0 and teal for y = 1) and p(x) (black) for logistic regression.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ç‚¹ï¼ˆçº¢è‰²è¡¨ç¤º y=0ï¼Œé’ç»¿è‰²è¡¨ç¤º y=1ï¼‰å’Œ p(x)ï¼ˆé»‘è‰²ï¼‰ç”¨äºé€»è¾‘å›å½’ã€‚
- en: 'This logistic curve *p(x)* is described by two parameters:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªé€»è¾‘æ›²çº¿*p(x)*ç”±ä¸¤ä¸ªå‚æ•°æè¿°ï¼š
- en: '*xâ‚€* is the value of a predictor *x* for which the probability is 0.5 (mid-point):
    *p(x = xâ‚€) = 0.5*, so tells us about the location of the mid-point.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '* xâ‚€*æ˜¯é¢„æµ‹å˜é‡*x*çš„å€¼ï¼Œæ­¤æ—¶æ¦‚ç‡ä¸º0.5ï¼ˆä¸­ç‚¹ï¼‰ï¼š* p(x = xâ‚€) = 0.5*ï¼Œæ‰€ä»¥å‘Šè¯‰æˆ‘ä»¬ä¸­ç‚¹çš„ä½ç½®ã€‚'
- en: '*k* is related to the slope of the probability at mid-point: *(dp/dx){x = xâ‚€}
    = k/4*, so tells us about the steepness of the curve at that mid-point. The larger
    the *k*, the steeper the curve in the middle.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '* k* ä¸ä¸­ç‚¹çš„æ¦‚ç‡æ–œç‡æœ‰å…³ï¼š*(dp/dx){x = xâ‚€} = k/4*ï¼Œå› æ­¤å‘Šè¯‰æˆ‘ä»¬è¯¥ä¸­ç‚¹å¤„æ›²çº¿çš„é™¡å³­ç¨‹åº¦ã€‚* k* è¶Šå¤§ï¼Œä¸­é—´çš„æ›²çº¿è¶Šé™¡ã€‚'
- en: If we naively employed ordinary least squares to fit the curve *p(x)* to these
    points, we would find that all residuals would be less than 1 and most points
    on the â€œwrong sideâ€ of the mid-point would have residuals ~ 1\. It would make
    more sense to assign a much larger cost to points that are large outliers.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬å¤©çœŸåœ°ä½¿ç”¨æ™®é€šæœ€å°äºŒä¹˜æ³•æ¥æ‹Ÿåˆæ›²çº¿*p(x)*åˆ°è¿™äº›ç‚¹ä¸Šï¼Œæˆ‘ä»¬ä¼šå‘ç°æ‰€æœ‰æ®‹å·®éƒ½å°äº1ï¼Œå¹¶ä¸”å¤§å¤šæ•°åœ¨ä¸­ç‚¹â€œé”™è¯¯ä¸€ä¾§â€çš„ç‚¹çš„æ®‹å·®æ¥è¿‘1ã€‚å°†æ›´å¤§çš„æˆæœ¬åˆ†é…ç»™é‚£äº›å¤§çš„ç¦»ç¾¤ç‚¹ä¼šæ›´æœ‰æ„ä¹‰ã€‚
- en: Log-loss fit
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¯¹æ•°æŸå¤±æ‹Ÿåˆ
- en: 'Instead of trying to make ordinary least squares work to fit *p(x)* to the
    points, logistic regression proceeds differently:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å…¶å°è¯•è®©æ™®é€šæœ€å°äºŒä¹˜æ³•æ‹Ÿåˆ*p(x)*åˆ°ç‚¹ä¸Šï¼Œé€»è¾‘å›å½’çš„å¤„ç†æ–¹å¼ä¸åŒï¼š
- en: For teal points at *y = 1*, we will fit *-log p(x)* instead of *p(x)*. Negative
    logarithm makes *-log p(x)* progressively larger; as *p(x)* approaches zero.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äº* y = 1*çš„é’ç»¿è‰²ç‚¹ï¼Œæˆ‘ä»¬å°†æ‹Ÿåˆ * -log p(x)* è€Œä¸æ˜¯ * p(x)*ã€‚è´Ÿå¯¹æ•°ä½¿ * -log p(x)* éšç€ * p(x)*
    æ¥è¿‘é›¶è€Œé€æ¸å¢å¤§ã€‚
- en: For the red points at *y = 0* we can do the same by using the probability that
    the outcome is zero, *-log[1-p(x)]*.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äº* y = 0*çš„çº¢è‰²ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ç»“æœä¸ºé›¶çš„æ¦‚ç‡æ¥è¿›è¡Œç›¸åŒçš„å¤„ç†ï¼Œ* -log[1-p(x)]*ã€‚
- en: 'We call these â€œlog-lossesâ€. If we collapse all the points to *y = 0*, then
    for each point these two log-losses represent a *cost* (loss) of that point, for
    being some amount away from the log-loss curves. In order to utilize `numpy` vectorization,
    we will code these two together as a single log-loss function (this combo log-loss
    also goes by the name â€œcross entropyâ€):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç§°è¿™äº›ä¸ºâ€œlog-lossâ€ã€‚å¦‚æœæˆ‘ä»¬å°†æ‰€æœ‰ç‚¹éƒ½æŠ˜å åˆ°*y = 0*ï¼Œé‚£ä¹ˆå¯¹äºæ¯ä¸ªç‚¹ï¼Œè¿™ä¸¤ä¸ªlog-lossä»£è¡¨è¯¥ç‚¹çš„æˆæœ¬ï¼Œå› ä¸ºå®ƒä»¬ä¸log-lossæ›²çº¿æœ‰ä¸€å®šçš„å·®è·ã€‚ä¸ºäº†åˆ©ç”¨`numpy`çš„å‘é‡åŒ–ï¼Œæˆ‘ä»¬å°†è¿™ä¸¤ä¸ªä¸€èµ·ç¼–ç ä¸ºå•ä¸ªlog-losså‡½æ•°ï¼ˆè¿™ç§ç»„åˆlog-lossä¹Ÿè¢«ç§°ä¸ºâ€œäº¤å‰ç†µâ€ï¼‰ã€‚
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'One way to think about logistic regression is a method that simultaneously
    fits: *-log p(x)* for *y = 1* and -*log[1-p(x)]* for *y = 0*.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: é€»è¾‘å›å½’çš„ä¸€ç§ç†è§£æ–¹å¼æ˜¯åŒæ—¶é€‚åˆäº*y = 1*çš„*-log p(x)*å’Œ*y = 0*çš„*-log[1-p(x)]*ã€‚
- en: How do these two log-loss curves look?
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸¤ä¸ªlog-lossæ›²çº¿çœ‹èµ·æ¥å¦‚ä½•ï¼Ÿ
- en: 'To visualize them, we will plot the same data in the previous plot, but now
    with log-losses instead of probability:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å¯è§†åŒ–å®ƒä»¬ï¼Œæˆ‘ä»¬å°†åœ¨å‰é¢çš„å›¾ä¸­ç»˜åˆ¶ç›¸åŒçš„æ•°æ®ï¼Œä½†ç°åœ¨ä½¿ç”¨log-lossæ›¿ä»£æ¦‚ç‡ã€‚
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/c076cb0e9f2619ade776975a758f6cb9.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c076cb0e9f2619ade776975a758f6cb9.png)'
- en: 'Here we collapsed all points to *y = 0*, but use the colors as *y* labels,
    since the values of log-losses on their own represent the cost. Red points (*y
    = 0*) are fit to the red hockey stick curve: *-log[1-p(x)]*. Teal points (*y =
    1*) are fit to the teal hockey stick curve: *-log p(x)*. Sum of the vertical dashed
    lines represents the total log-loss that needs to be minimized for various *k*
    and *xâ‚€.*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æ‰€æœ‰ç‚¹éƒ½æŠ˜å åˆ°*y = 0*ï¼Œä½†ä½¿ç”¨é¢œè‰²ä½œä¸º*y*æ ‡ç­¾ï¼Œå› ä¸ºlog-losså€¼æœ¬èº«ä»£è¡¨æˆæœ¬ã€‚çº¢è‰²ç‚¹ï¼ˆ*y = 0*ï¼‰é€‚åˆäºçº¢è‰²æ›²çº¿ï¼š*-log[1-p(x)]*ã€‚è“ç»¿è‰²ç‚¹ï¼ˆ*y
    = 1*ï¼‰é€‚åˆäºè“ç»¿è‰²æ›²çº¿ï¼š*-log p(x)*ã€‚å‚ç›´è™šçº¿çš„æ€»å’Œè¡¨ç¤ºéœ€è¦æœ€å°åŒ–çš„æ€»log-lossï¼Œé€‚ç”¨äºå„ç§*k*å’Œ*xâ‚€*ã€‚
- en: Unlike probability, log-loss curves have the property of penalizing big outliers
    proportionally more and they do not have residuals that cap out at 1.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸æ¦‚ç‡ä¸åŒï¼Œlog-lossæ›²çº¿å…·æœ‰æŒ‰æ¯”ä¾‹æƒ©ç½šå¤§ç¦»ç¾¤å€¼çš„ç‰¹æ€§ï¼Œå¹¶ä¸”å®ƒä»¬æ²¡æœ‰åœ¨1å¤„æˆªæ–­çš„æ®‹å·®ã€‚
- en: Finding the minimal log-loss
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¯»æ‰¾æœ€å°çš„log-loss
- en: How does changing *k* and *xâ‚€* affect this fit? To answer this, we can run fits
    with various combinations of *k* and *xâ‚€.*
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´æ”¹*k*å’Œ*xâ‚€*å¦‚ä½•å½±å“è¿™ç§æ‹Ÿåˆï¼Ÿè¦å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥è¿è¡Œå…·æœ‰å„ç§*k*å’Œ*xâ‚€*ç»„åˆçš„æ‹Ÿåˆã€‚
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Changing *xâ‚€* moves the intersection point sideways:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´æ”¹*xâ‚€*ä¼šä½¿äº¤ç‚¹å‘ä¾§é¢ç§»åŠ¨ï¼š
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/679f7ce48de4018c7a378a456b070d2f.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/679f7ce48de4018c7a378a456b070d2f.png)'
- en: If *xâ‚€* is chosen away from the optimal point, the log-loss increases because
    increasing number of points gets fitted to the rising parts of the hockey sticks.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœ*xâ‚€*é€‰æ‹©ç¦»æœ€ä¼˜ç‚¹è¾ƒè¿œï¼Œåˆ™log-lossä¼šå¢åŠ ï¼Œå› ä¸ºè¶Šæ¥è¶Šå¤šçš„ç‚¹é€‚åˆäºhockey stickæ›²çº¿çš„ä¸Šå‡éƒ¨åˆ†ã€‚
- en: 'Changing *k* affects the steepness of the log-loss curves (note the different
    y axes):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´æ”¹*k*ä¼šå½±å“log-lossæ›²çº¿çš„é™¡åº¦ï¼ˆæ³¨æ„ä¸åŒçš„yè½´ï¼‰ï¼š
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/fbd25b1c48f6adda8fd7b9973317a959.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fbd25b1c48f6adda8fd7b9973317a959.png)'
- en: If *k* is too low (0.5), most points add small but significant amounts to the
    total log-loss. If *k* is too high (7.0), only the points on the â€œwrong sideâ€
    contribute a significant amount to the total log-loss. In this case, it is the
    two teal points on the left of mid-point at *xâ‚€ = 2.5*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœ*k*å¤ªä½ï¼ˆ0.5ï¼‰ï¼Œå¤§å¤šæ•°ç‚¹ä¼šå¯¹æ€»log-losså¢åŠ ä¸€äº›å°ä½†æ˜¾è‘—çš„è´¡çŒ®ã€‚å¦‚æœ*k*å¤ªé«˜ï¼ˆ7.0ï¼‰ï¼Œåªæœ‰â€œé”™è¯¯æ–¹å‘â€çš„ç‚¹ä¼šå¯¹æ€»log-lossæœ‰æ˜¾è‘—è´¡çŒ®ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ˜¯åœ¨*xâ‚€
    = 2.5*ä¸­ç‚¹å·¦ä¾§çš„ä¸¤ä¸ªè“ç»¿è‰²ç‚¹ã€‚
- en: 'This brings up a question: what if there are no points on the â€œwrong sideâ€
    of the mid-point, such as when the data is perfectly separated?'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¸¦æ¥ä¸€ä¸ªé—®é¢˜ï¼šå¦‚æœä¸­ç‚¹æ²¡æœ‰â€œé”™è¯¯æ–¹å‘â€çš„ç‚¹ï¼Œä¾‹å¦‚æ•°æ®å®Œå…¨åˆ†ç¦»æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ
- en: Perfectly separated data
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®Œå…¨åˆ†ç¦»çš„æ•°æ®
- en: 'It turns out, the logistic model cannot fit data that is perfectly separated!
    ğŸ˜® We can apply what we learned earlier about fitting log-losses to understand
    why. We start by creating perfectly separated data (with *k = 3* and *xâ‚€ = 2.5*):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœè¡¨æ˜ï¼Œé€»è¾‘æ¨¡å‹æ— æ³•æ‹Ÿåˆå®Œå…¨åˆ†ç¦»çš„æ•°æ®ï¼ğŸ˜® æˆ‘ä»¬å¯ä»¥åº”ç”¨æˆ‘ä»¬æ—©å‰å­¦åˆ°çš„æœ‰å…³æ‹Ÿåˆlog-lossçš„çŸ¥è¯†æ¥ç†è§£ä¸ºä»€ä¹ˆã€‚æˆ‘ä»¬é¦–å…ˆåˆ›å»ºå®Œå…¨åˆ†ç¦»çš„æ•°æ®ï¼ˆä½¿ç”¨*k
    = 3*å’Œ*xâ‚€ = 2.5*ï¼‰ï¼š
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/7f62036d309316e52dcff76ef7a35e73.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7f62036d309316e52dcff76ef7a35e73.png)'
- en: Remember how changing *k* affects these hockey stick log-loss curves? When we
    increased *k*, the main contribution to the total log-loss was from the points
    in the â€œwrong sideâ€ of the mid-point. Now, all points are on the â€œcorrect sideâ€
    of the mid-point.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: è®°å¾—æ”¹å˜*k*å¦‚ä½•å½±å“è¿™äº›hockey stick log-lossæ›²çº¿å—ï¼Ÿå½“æˆ‘ä»¬å¢åŠ *k*æ—¶ï¼Œæ€»log-lossçš„ä¸»è¦è´¡çŒ®æ¥è‡ªäºä¸­ç‚¹â€œé”™è¯¯æ–¹å‘â€çš„ç‚¹ã€‚ç°åœ¨ï¼Œæ‰€æœ‰ç‚¹éƒ½åœ¨ä¸­ç‚¹çš„â€œæ­£ç¡®æ–¹å‘â€ã€‚
- en: 'That means that we can create arbitrarily good fits by continuously increasing
    *k*. Here is how the fits look when we set *k = 13*:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥é€šè¿‡æŒç»­å¢åŠ *k*æ¥åˆ›å»ºä»»æ„å¥½çš„é€‚åˆåº¦ã€‚è¿™é‡Œæ˜¯æˆ‘ä»¬è®¾ç½®*k = 13*æ—¶é€‚åˆåº¦çš„æ ·å­ï¼š
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/84fb7ceac71bc3223536de55616f4355.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84fb7ceac71bc3223536de55616f4355.png)'
- en: Fitting perfectly separated data would require log-losses to have a 90-degree
    angle and fitted probability to have an infinite slope at the mid-point. Therefore,
    there is no parameter *k* for which total log-loss has a minimum. In practice,
    numerical algorithms stop after some number of steps and may either return the
    value of *k* in the last step or an error.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œç¾åˆ†ç¦»çš„æ•°æ®éœ€è¦å¯¹æ•°æŸå¤±ï¼ˆlog-lossesï¼‰å…·æœ‰90åº¦è§’ï¼Œå¹¶ä¸”æ‹Ÿåˆæ¦‚ç‡åœ¨ä¸­ç‚¹å¤„å…·æœ‰æ— é™æ–œç‡ã€‚å› æ­¤ï¼Œæ²¡æœ‰å‚æ•°*k*ä½¿æ€»å¯¹æ•°æŸå¤±è¾¾åˆ°æœ€å°å€¼ã€‚åœ¨å®é™…ä¸­ï¼Œæ•°å€¼ç®—æ³•åœ¨è‹¥å¹²æ­¥éª¤ååœæ­¢ï¼Œå¯èƒ½ä¼šè¿”å›æœ€åä¸€æ­¥çš„*k*å€¼æˆ–ä¸€ä¸ªé”™è¯¯ã€‚
- en: To be continued
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¾…ç»­
- en: 'This post covers the main part of what logistic regression conceptually does.
    In the next part, we will cover the somewhat unintuitive meaning of parameter
    k as a log-odds-ratio and show how to run, and break, logistic models in Python
    libraries `statsmodels` and `scikit-learn`:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç¯‡æ–‡ç« æ¶µç›–äº†é€»è¾‘å›å½’æ¦‚å¿µä¸Šçš„ä¸»è¦éƒ¨åˆ†ã€‚åœ¨ä¸‹ä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨å‚æ•°*k*ä½œä¸ºå¯¹æ•°èµ”ç‡æ¯”çš„æœ‰äº›ä¸ç›´è§‚çš„å«ä¹‰ï¼Œå¹¶å±•ç¤ºå¦‚ä½•åœ¨Pythonåº“`statsmodels`å’Œ`scikit-learn`ä¸­è¿è¡Œå’Œç ´è§£é€»è¾‘æ¨¡å‹ã€‚
- en: '[](/logistic-regression-deceptively-flawed-2c3e7f77eac9?source=post_page-----67560de4f492--------------------------------)
    [## Logistic Regression: Deceptively Flawed'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[é€»è¾‘å›å½’ï¼šè¡¨é¢æ— ç¼ºé™·çš„é™·é˜±](/logistic-regression-deceptively-flawed-2c3e7f77eac9?source=post_page-----67560de4f492--------------------------------)'
- en: When can large odds ratios and perfectly separated data bite you?
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ä½•æ—¶å¤§èµ”ç‡æ¯”å’Œå®Œç¾åˆ†ç¦»çš„æ•°æ®ä¼šç»™ä½ å¸¦æ¥éº»çƒ¦ï¼Ÿ
- en: towardsdatascience.com](/logistic-regression-deceptively-flawed-2c3e7f77eac9?source=post_page-----67560de4f492--------------------------------)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/logistic-regression-deceptively-flawed-2c3e7f77eac9?source=post_page-----67560de4f492--------------------------------)'
