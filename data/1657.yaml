- en: 'Logistic Regression: Faceoff and Conceptual Understanding'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归：对决与概念理解
- en: 原文：[https://towardsdatascience.com/logistic-regression-faceoff-67560de4f492?source=collection_archive---------5-----------------------#2023-05-18](https://towardsdatascience.com/logistic-regression-faceoff-67560de4f492?source=collection_archive---------5-----------------------#2023-05-18)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/logistic-regression-faceoff-67560de4f492?source=collection_archive---------5-----------------------#2023-05-18](https://towardsdatascience.com/logistic-regression-faceoff-67560de4f492?source=collection_archive---------5-----------------------#2023-05-18)
- en: What do log-losses and perfectly separated data have to do with hockey sticks?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑损失和完美分离的数据与冰球棒有什么关系？
- en: '[](https://medium.com/@igor-s?source=post_page-----67560de4f492--------------------------------)[![Igor
    Šegota](../Images/17c592b71fef9526a0679d47937837f6.png)](https://medium.com/@igor-s?source=post_page-----67560de4f492--------------------------------)[](https://towardsdatascience.com/?source=post_page-----67560de4f492--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----67560de4f492--------------------------------)
    [Igor Šegota](https://medium.com/@igor-s?source=post_page-----67560de4f492--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@igor-s?source=post_page-----67560de4f492--------------------------------)[![Igor
    Šegota](../Images/17c592b71fef9526a0679d47937837f6.png)](https://medium.com/@igor-s?source=post_page-----67560de4f492--------------------------------)[](https://towardsdatascience.com/?source=post_page-----67560de4f492--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----67560de4f492--------------------------------)
    [Igor Šegota](https://medium.com/@igor-s?source=post_page-----67560de4f492--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe5f8ebca4ad8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-faceoff-67560de4f492&user=Igor+%C5%A0egota&userId=e5f8ebca4ad8&source=post_page-e5f8ebca4ad8----67560de4f492---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----67560de4f492--------------------------------)
    ·7 min read·May 18, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F67560de4f492&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-faceoff-67560de4f492&user=Igor+%C5%A0egota&userId=e5f8ebca4ad8&source=-----67560de4f492---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Fe5f8ebca4ad8&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-faceoff-67560de4f492&user=Igor+%C5%A0egota&userId=e5f8ebca4ad8&source=post_page-e5f8ebca4ad8----67560de4f492---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----67560de4f492--------------------------------)
    ·7分钟阅读·2023年5月18日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F67560de4f492&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-faceoff-67560de4f492&user=Igor+%C5%A0egota&userId=e5f8ebca4ad8&source=-----67560de4f492---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F67560de4f492&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-faceoff-67560de4f492&source=-----67560de4f492---------------------bookmark_footer-----------)![](../Images/91a7b0a99da3d1740a217fe617880118.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F67560de4f492&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Flogistic-regression-faceoff-67560de4f492&source=-----67560de4f492---------------------bookmark_footer-----------)![](../Images/91a7b0a99da3d1740a217fe617880118.png)'
- en: Photo by [Jerry Yu](https://unsplash.com/@jerryyu?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 [Jerry Yu](https://unsplash.com/@jerryyu?utm_source=medium&utm_medium=referral)
    提供，来源于 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: Who ordered this?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 谁下的这个订单？
- en: As of this writing, Google search for “logistic regression tutorial” shows about
    11.2M results. Why add another thing to this pile?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 截至撰写本文时，谷歌搜索“逻辑回归教程”显示大约有1120万条结果。为什么还要在这堆信息中再添加一份？
- en: After reading a good number of articles, books and guides, I realized that most
    lack clear and intuitive explanations of how logistic regression works. Instead,
    they usually strive to be either practical, by showing how to run models, or as
    mathematically complete as possible, and as a consequence, basic concepts get
    buried underneath a forest of matrix algebra.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读了大量文章、书籍和指南后，我意识到大多数缺乏对逻辑回归工作原理的清晰直观解释。相反，它们通常要么致力于展示如何运行模型的实用性，要么尽可能地数学全面，因此基本概念被埋在矩阵代数的森林中。
- en: 'We will start by clearing up what seem to be common misconceptions. Logistic
    regression is **not**:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从澄清看似常见的误解开始。逻辑回归**不是**：
- en: linear regression but with sigmoid curve instead of a straight line
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归，但用 sigmoid 曲线代替直线
- en: classification algorithm (but can be used for this)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类算法（但可以用于此）
- en: sigmoid curve “fit” of a decision boundary separating two classes of points
    in the x-y plane
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: sigmoid 曲线“拟合”了x-y平面中分隔两类点的决策边界
- en: What *is* a logistic regression?
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是逻辑回归？
- en: 'Logistic regression is a regression model that returns a probability of a binary
    outcome (0 or 1), assuming that log of the odds is a linear combination of one
    or more inputs. Odds is a ratio between probability of outcome happening (*p*)
    and the probability of the outcome not happening (*1-p*). When we have one input
    or predictor, this starting assumption is mathematically expressed as:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是一种回归模型，返回二元结果（0或1）的概率，假设赔率的对数是一个或多个输入的线性组合。赔率是结果发生的概率（* p*）与结果不发生的概率（*1-p*）之间的比率。当我们只有一个输入或预测变量时，这一初始假设数学上表示为：
- en: '![](../Images/685c506d6b3786c9ed389a77a810331b.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/685c506d6b3786c9ed389a77a810331b.png)'
- en: 'The goal behind logistic regression is to model cases when inputs are shifting
    the outcome probability progressively from 0 to 1\. The probability of the outcome
    being 1, *p*, can be derived from the previous equation and expressed as a function
    of inputs:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归的目标是建模当输入将结果概率从0逐渐转移到1的情况。结果为1的概率 * p* 可以从先前的方程中推导出来，并表示为输入的函数：
- en: '![](../Images/eca13451561745c8a572f7c35592f11c.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/eca13451561745c8a572f7c35592f11c.png)'
- en: In the last part we swapped from parameters *β₁* and *β₀* to *k* and *x₀*. Using
    *k* and *x₀* will give us a clearer picture of the model as we go along. We will
    also stick to a single predictor variable *x*, as opposed to marching in with
    an army of matrices, so we can easily visualize logistic fits.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一部分，我们从参数* β₁* 和* β₀* 转换为 * k* 和 * x₀*。使用 * k* 和 * x₀* 将使我们在继续过程中对模型有更清晰的了解。我们还将坚持使用单一预测变量*x*，而不是引入一大堆矩阵，这样我们可以更容易地可视化逻辑拟合。
- en: Logistic curve
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑曲线
- en: 'We will begin by plotting the logistic curve, with parameters *x₀ = 2.5* and
    *k = 3*, on an interval *x* between 0 and 5:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开始绘制逻辑曲线，参数为 * x₀ = 2.5* 和 * k = 3*，在区间 * x* 从0到5：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/d11aca277e68ed608888dac0fb31053d.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d11aca277e68ed608888dac0fb31053d.png)'
- en: Points (red for y=0 and teal for y = 1) and p(x) (black) for logistic regression.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 点（红色表示 y=0，青绿色表示 y=1）和 p(x)（黑色）用于逻辑回归。
- en: 'This logistic curve *p(x)* is described by two parameters:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这个逻辑曲线*p(x)*由两个参数描述：
- en: '*x₀* is the value of a predictor *x* for which the probability is 0.5 (mid-point):
    *p(x = x₀) = 0.5*, so tells us about the location of the mid-point.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '* x₀*是预测变量*x*的值，此时概率为0.5（中点）：* p(x = x₀) = 0.5*，所以告诉我们中点的位置。'
- en: '*k* is related to the slope of the probability at mid-point: *(dp/dx){x = x₀}
    = k/4*, so tells us about the steepness of the curve at that mid-point. The larger
    the *k*, the steeper the curve in the middle.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '* k* 与中点的概率斜率有关：*(dp/dx){x = x₀} = k/4*，因此告诉我们该中点处曲线的陡峭程度。* k* 越大，中间的曲线越陡。'
- en: If we naively employed ordinary least squares to fit the curve *p(x)* to these
    points, we would find that all residuals would be less than 1 and most points
    on the “wrong side” of the mid-point would have residuals ~ 1\. It would make
    more sense to assign a much larger cost to points that are large outliers.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们天真地使用普通最小二乘法来拟合曲线*p(x)*到这些点上，我们会发现所有残差都小于1，并且大多数在中点“错误一侧”的点的残差接近1。将更大的成本分配给那些大的离群点会更有意义。
- en: Log-loss fit
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对数损失拟合
- en: 'Instead of trying to make ordinary least squares work to fit *p(x)* to the
    points, logistic regression proceeds differently:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 与其尝试让普通最小二乘法拟合*p(x)*到点上，逻辑回归的处理方式不同：
- en: For teal points at *y = 1*, we will fit *-log p(x)* instead of *p(x)*. Negative
    logarithm makes *-log p(x)* progressively larger; as *p(x)* approaches zero.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于* y = 1*的青绿色点，我们将拟合 * -log p(x)* 而不是 * p(x)*。负对数使 * -log p(x)* 随着 * p(x)*
    接近零而逐渐增大。
- en: For the red points at *y = 0* we can do the same by using the probability that
    the outcome is zero, *-log[1-p(x)]*.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于* y = 0*的红色点，我们可以通过使用结果为零的概率来进行相同的处理，* -log[1-p(x)]*。
- en: 'We call these “log-losses”. If we collapse all the points to *y = 0*, then
    for each point these two log-losses represent a *cost* (loss) of that point, for
    being some amount away from the log-loss curves. In order to utilize `numpy` vectorization,
    we will code these two together as a single log-loss function (this combo log-loss
    also goes by the name “cross entropy”):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称这些为“log-loss”。如果我们将所有点都折叠到*y = 0*，那么对于每个点，这两个log-loss代表该点的成本，因为它们与log-loss曲线有一定的差距。为了利用`numpy`的向量化，我们将这两个一起编码为单个log-loss函数（这种组合log-loss也被称为“交叉熵”）。
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'One way to think about logistic regression is a method that simultaneously
    fits: *-log p(x)* for *y = 1* and -*log[1-p(x)]* for *y = 0*.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归的一种理解方式是同时适合于*y = 1*的*-log p(x)*和*y = 0*的*-log[1-p(x)]*。
- en: How do these two log-loss curves look?
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个log-loss曲线看起来如何？
- en: 'To visualize them, we will plot the same data in the previous plot, but now
    with log-losses instead of probability:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化它们，我们将在前面的图中绘制相同的数据，但现在使用log-loss替代概率。
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../Images/c076cb0e9f2619ade776975a758f6cb9.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c076cb0e9f2619ade776975a758f6cb9.png)'
- en: 'Here we collapsed all points to *y = 0*, but use the colors as *y* labels,
    since the values of log-losses on their own represent the cost. Red points (*y
    = 0*) are fit to the red hockey stick curve: *-log[1-p(x)]*. Teal points (*y =
    1*) are fit to the teal hockey stick curve: *-log p(x)*. Sum of the vertical dashed
    lines represents the total log-loss that needs to be minimized for various *k*
    and *x₀.*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将所有点都折叠到*y = 0*，但使用颜色作为*y*标签，因为log-loss值本身代表成本。红色点（*y = 0*）适合于红色曲线：*-log[1-p(x)]*。蓝绿色点（*y
    = 1*）适合于蓝绿色曲线：*-log p(x)*。垂直虚线的总和表示需要最小化的总log-loss，适用于各种*k*和*x₀*。
- en: Unlike probability, log-loss curves have the property of penalizing big outliers
    proportionally more and they do not have residuals that cap out at 1.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 与概率不同，log-loss曲线具有按比例惩罚大离群值的特性，并且它们没有在1处截断的残差。
- en: Finding the minimal log-loss
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 寻找最小的log-loss
- en: How does changing *k* and *x₀* affect this fit? To answer this, we can run fits
    with various combinations of *k* and *x₀.*
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 更改*k*和*x₀*如何影响这种拟合？要回答这个问题，我们可以运行具有各种*k*和*x₀*组合的拟合。
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Changing *x₀* moves the intersection point sideways:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 更改*x₀*会使交点向侧面移动：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](../Images/679f7ce48de4018c7a378a456b070d2f.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/679f7ce48de4018c7a378a456b070d2f.png)'
- en: If *x₀* is chosen away from the optimal point, the log-loss increases because
    increasing number of points gets fitted to the rising parts of the hockey sticks.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*x₀*选择离最优点较远，则log-loss会增加，因为越来越多的点适合于hockey stick曲线的上升部分。
- en: 'Changing *k* affects the steepness of the log-loss curves (note the different
    y axes):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 更改*k*会影响log-loss曲线的陡度（注意不同的y轴）：
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/fbd25b1c48f6adda8fd7b9973317a959.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fbd25b1c48f6adda8fd7b9973317a959.png)'
- en: If *k* is too low (0.5), most points add small but significant amounts to the
    total log-loss. If *k* is too high (7.0), only the points on the “wrong side”
    contribute a significant amount to the total log-loss. In this case, it is the
    two teal points on the left of mid-point at *x₀ = 2.5*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果*k*太低（0.5），大多数点会对总log-loss增加一些小但显著的贡献。如果*k*太高（7.0），只有“错误方向”的点会对总log-loss有显著贡献。在这种情况下，是在*x₀
    = 2.5*中点左侧的两个蓝绿色点。
- en: 'This brings up a question: what if there are no points on the “wrong side”
    of the mid-point, such as when the data is perfectly separated?'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这带来一个问题：如果中点没有“错误方向”的点，例如数据完全分离时会发生什么？
- en: Perfectly separated data
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 完全分离的数据
- en: 'It turns out, the logistic model cannot fit data that is perfectly separated!
    😮 We can apply what we learned earlier about fitting log-losses to understand
    why. We start by creating perfectly separated data (with *k = 3* and *x₀ = 2.5*):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，逻辑模型无法拟合完全分离的数据！😮 我们可以应用我们早前学到的有关拟合log-loss的知识来理解为什么。我们首先创建完全分离的数据（使用*k
    = 3*和*x₀ = 2.5*）：
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/7f62036d309316e52dcff76ef7a35e73.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7f62036d309316e52dcff76ef7a35e73.png)'
- en: Remember how changing *k* affects these hockey stick log-loss curves? When we
    increased *k*, the main contribution to the total log-loss was from the points
    in the “wrong side” of the mid-point. Now, all points are on the “correct side”
    of the mid-point.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 记得改变*k*如何影响这些hockey stick log-loss曲线吗？当我们增加*k*时，总log-loss的主要贡献来自于中点“错误方向”的点。现在，所有点都在中点的“正确方向”。
- en: 'That means that we can create arbitrarily good fits by continuously increasing
    *k*. Here is how the fits look when we set *k = 13*:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们可以通过持续增加*k*来创建任意好的适合度。这里是我们设置*k = 13*时适合度的样子：
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/84fb7ceac71bc3223536de55616f4355.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/84fb7ceac71bc3223536de55616f4355.png)'
- en: Fitting perfectly separated data would require log-losses to have a 90-degree
    angle and fitted probability to have an infinite slope at the mid-point. Therefore,
    there is no parameter *k* for which total log-loss has a minimum. In practice,
    numerical algorithms stop after some number of steps and may either return the
    value of *k* in the last step or an error.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 完美分离的数据需要对数损失（log-losses）具有90度角，并且拟合概率在中点处具有无限斜率。因此，没有参数*k*使总对数损失达到最小值。在实际中，数值算法在若干步骤后停止，可能会返回最后一步的*k*值或一个错误。
- en: To be continued
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 待续
- en: 'This post covers the main part of what logistic regression conceptually does.
    In the next part, we will cover the somewhat unintuitive meaning of parameter
    k as a log-odds-ratio and show how to run, and break, logistic models in Python
    libraries `statsmodels` and `scikit-learn`:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文章涵盖了逻辑回归概念上的主要部分。在下一部分中，我们将探讨参数*k*作为对数赔率比的有些不直观的含义，并展示如何在Python库`statsmodels`和`scikit-learn`中运行和破解逻辑模型。
- en: '[](/logistic-regression-deceptively-flawed-2c3e7f77eac9?source=post_page-----67560de4f492--------------------------------)
    [## Logistic Regression: Deceptively Flawed'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[逻辑回归：表面无缺陷的陷阱](/logistic-regression-deceptively-flawed-2c3e7f77eac9?source=post_page-----67560de4f492--------------------------------)'
- en: When can large odds ratios and perfectly separated data bite you?
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 何时大赔率比和完美分离的数据会给你带来麻烦？
- en: towardsdatascience.com](/logistic-regression-deceptively-flawed-2c3e7f77eac9?source=post_page-----67560de4f492--------------------------------)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[towardsdatascience.com](/logistic-regression-deceptively-flawed-2c3e7f77eac9?source=post_page-----67560de4f492--------------------------------)'
