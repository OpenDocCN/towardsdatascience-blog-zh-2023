- en: Dynamic MIG Partitioning in Kubernetes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes中的动态MIG分区
- en: 原文：[https://towardsdatascience.com/dynamic-mig-partitioning-in-kubernetes-89db6cdde7a3?source=collection_archive---------4-----------------------#2023-01-26](https://towardsdatascience.com/dynamic-mig-partitioning-in-kubernetes-89db6cdde7a3?source=collection_archive---------4-----------------------#2023-01-26)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/dynamic-mig-partitioning-in-kubernetes-89db6cdde7a3?source=collection_archive---------4-----------------------#2023-01-26](https://towardsdatascience.com/dynamic-mig-partitioning-in-kubernetes-89db6cdde7a3?source=collection_archive---------4-----------------------#2023-01-26)
- en: Maximize GPU utilization and reduce infrastructure costs.
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最大化GPU利用率并降低基础设施成本。
- en: '[](https://medium.com/@telemaco019?source=post_page-----89db6cdde7a3--------------------------------)[![Michele
    Zanotti](../Images/6350ad98e5f057991b2e6f1a86a5c350.png)](https://medium.com/@telemaco019?source=post_page-----89db6cdde7a3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----89db6cdde7a3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----89db6cdde7a3--------------------------------)
    [Michele Zanotti](https://medium.com/@telemaco019?source=post_page-----89db6cdde7a3--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/@telemaco019?source=post_page-----89db6cdde7a3--------------------------------)[![Michele
    Zanotti](../Images/6350ad98e5f057991b2e6f1a86a5c350.png)](https://medium.com/@telemaco019?source=post_page-----89db6cdde7a3--------------------------------)[](https://towardsdatascience.com/?source=post_page-----89db6cdde7a3--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----89db6cdde7a3--------------------------------)
    [Michele Zanotti](https://medium.com/@telemaco019?source=post_page-----89db6cdde7a3--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9b7af839d7e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdynamic-mig-partitioning-in-kubernetes-89db6cdde7a3&user=Michele+Zanotti&userId=9b7af839d7e&source=post_page-9b7af839d7e----89db6cdde7a3---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----89db6cdde7a3--------------------------------)
    ·9 min read·Jan 26, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F89db6cdde7a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdynamic-mig-partitioning-in-kubernetes-89db6cdde7a3&user=Michele+Zanotti&userId=9b7af839d7e&source=-----89db6cdde7a3---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F9b7af839d7e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdynamic-mig-partitioning-in-kubernetes-89db6cdde7a3&user=Michele+Zanotti&userId=9b7af839d7e&source=post_page-9b7af839d7e----89db6cdde7a3---------------------post_header-----------)
    发表在 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----89db6cdde7a3--------------------------------)
    ·9分钟阅读·2023年1月26日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2F89db6cdde7a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdynamic-mig-partitioning-in-kubernetes-89db6cdde7a3&user=Michele+Zanotti&userId=9b7af839d7e&source=-----89db6cdde7a3---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F89db6cdde7a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdynamic-mig-partitioning-in-kubernetes-89db6cdde7a3&source=-----89db6cdde7a3---------------------bookmark_footer-----------)![](../Images/faced2bd5b9da0f06d26dcaf220db464.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F89db6cdde7a3&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fdynamic-mig-partitioning-in-kubernetes-89db6cdde7a3&source=-----89db6cdde7a3---------------------bookmark_footer-----------)![](../Images/faced2bd5b9da0f06d26dcaf220db464.png)'
- en: Photo by [Growtika](https://unsplash.com/@growtika?utm_source=medium&utm_medium=referral)
    on [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[Growtika](https://unsplash.com/@growtika?utm_source=medium&utm_medium=referral)
    在 [Unsplash](https://unsplash.com/?utm_source=medium&utm_medium=referral)
- en: To minimize infrastructure expenses, it’s crucial to use GPU accelerators in
    the most efficient way. One method to achieve this is by dividing the GPU into
    smaller partitions, called slices, so that containers can request only the strictly
    necessary resources. Some workloads may only require a minimal amount of the GPU’s
    compute and memory, so having the ability in Kubernetes to divide a single GPU
    into multiple slices, which can be requested by individual containers, is essential.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少基础设施开支，使用GPU加速器的高效方式至关重要。一种实现方法是将GPU划分为更小的分区，即切片，以便容器仅请求必要的资源。一些工作负载可能只需要GPU计算和内存的一小部分，因此在Kubernetes中能够将单个GPU分成多个切片，并允许个别容器请求这些切片是非常重要的。
- en: This is particularly relevant for large Kubernetes clusters used for running
    Artificial Intelligence (AI) and High-Performance Computing (HPC) workloads, where
    inefficiencies in GPU utilization can have a significant impact on infrastructure
    expenses. These inefficiencies are generally due to lightweight tasks that do
    not fully utilize a GPU, such as inference servers and [Jupyter Notebooks](https://jupyter.org/)
    used for preliminary data and model exploration.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这对用于运行人工智能 (AI) 和高性能计算 (HPC) 工作负载的大型Kubernetes集群特别相关，因为GPU利用效率低下可能对基础设施费用产生重大影响。这些低效通常是由于没有充分利用GPU的轻量级任务，如推理服务器和用于初步数据和模型探索的[Jupyter
    Notebooks](https://jupyter.org/)。
- en: For example, researchers from the [European Organization for Nuclear Research
    (CERN)](https://home.cern/) published a [blog post](https://kubernetes.web.cern.ch/blog/2023/01/09/efficient-access-to-shared-gpu-resources-part-1/?utm_id=FAUN_Kaptain356_Link_title)
    about how they used MIG GPU partitioning for addressing low GPU utilization problems
    caused by spiky workloads running High Energy Physics (HEP) simulations and code
    inefficiencies locking whole GPUs.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，[欧洲核子研究组织 (CERN)](https://home.cern/)的研究人员发布了一篇[博客文章](https://kubernetes.web.cern.ch/blog/2023/01/09/efficient-access-to-shared-gpu-resources-part-1/?utm_id=FAUN_Kaptain356_Link_title)，介绍了他们如何使用MIG
    GPU分区来解决由波动性工作负载运行高能物理 (HEP) 模拟和代码效率低下导致的低GPU利用率问题。
- en: 'The NVIDIA GPU Operator enables using MIG in Kubernetes, but it alone is insufficient
    to ensure efficient GPU partitioning. In this article, we examine the reasons
    for this and offer a more effective solution for using MIG in Kubernetes: **Dynamic
    MIG Partitioning**.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA GPU Operator支持在Kubernetes中使用MIG，但仅凭它不足以确保高效的GPU分区。在本文中，我们将探讨原因，并提供在Kubernetes中使用MIG的更有效解决方案：**动态MIG分区**。
- en: MIG support in Kubernetes
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes中的MIG支持
- en: MIG support in Kubernetes is provided by the [NVIDIA device plugin](https://github.com/NVIDIA/k8s-device-plugin),
    which allows to expose MIG devices, i.e. isolated GPU partitions, either as generic
    `nvidia.com/gpu` resources or as specific resource kinds such as, for instance,
    `nvidia.com/mig-1g.10gb`.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes中的MIG支持由[NVIDIA设备插件](https://github.com/NVIDIA/k8s-device-plugin)提供，该插件允许将MIG设备（即隔离的GPU分区）暴露为通用的`nvidia.com/gpu`资源或特定资源类型，例如`nvidia.com/mig-1g.10gb`。
- en: 'Manually managing MIG devices through `nvidia-smi` is impractical, so NVIDIA
    provides a tool called [nvidia-mig-parted](https://github.com/NVIDIA/mig-parted).
    This allows cluster admins to declaratively define the set of MIG devices they
    need on all GPUs on a node. The tool automatically manages the GPU partitioning
    state to match the desired configuration. For instance, below is an example of
    configurations taken from the nvidia-mig-parted GitHub repository:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`nvidia-smi`手动管理MIG设备是不切实际的，因此NVIDIA提供了一种名为[nvidia-mig-parted](https://github.com/NVIDIA/mig-parted)的工具。该工具允许集群管理员声明性地定义节点上所有GPU所需的MIG设备集。该工具自动管理GPU分区状态，以匹配所需的配置。例如，以下是从nvidia-mig-parted
    GitHub存储库中提取的配置示例：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In Kubernetes, cluster admins generally would not use nvidia-mig-parted directly,
    but they would use it through the [NVIDIA GPU Operator](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/gpu-operator-mig.html).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes中，集群管理员通常不会直接使用nvidia-mig-parted，而是通过[NVIDIA GPU Operator](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/gpu-operator-mig.html)来使用它。
- en: This operator further simplifies the application of MIG configurations. After
    creating a ConfigMap defining a set of allowed MIG configurations, the NVIDIA
    GPU Operator only requires you to label the nodes with `nvidia.com/mig.config`
    and specify as value the name of the specific configuration you want to apply
    on that node.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这个操作程序进一步简化了MIG配置的应用。创建定义了一组允许的MIG配置的ConfigMap之后，NVIDIA GPU Operator只需要你用`nvidia.com/mig.config`标记节点，并指定作为值你想在该节点上应用的具体配置的名称。
- en: 'For instance, referring to the configuration defined above, we could apply
    the config `all-3g.20gb` to the node `node-1` as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，参考上述定义的配置，我们可以将配置`all-3g.20gb`应用于节点`node-1`，如下所示：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Static MIG configurations cause poor usability
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 静态MIG配置会导致较差的可用性
- en: 'The NVIDIA GPU Operator has a significant limitation: MIG devices are created
    through static configurations.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA GPU Operator有一个显著的限制：MIG设备是通过静态配置创建的。
- en: This means that the cluster admin has to first define all the possible MIG configurations
    they think might be required in the cluster, and then manually apply them to each
    node according to their needs.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着集群管理员必须首先定义他们认为可能需要的所有 MIG 配置，然后根据需要手动将其应用于每个节点。
- en: This way of managing MIG devices can easily lead to inefficiencies in GPU utilization
    and time spent by the cluster admin to change MIG configurations. In fact, GPU
    memory and compute requirements vary from Pod to Pod and change over time. To
    achieve optimal GPU utilization as new Pods with different MIG resources requests
    are created, the cluster admin would have to spend his/her time constantly finding
    and applying the most proper configuration for each node of the cluster, which
    is very impractical.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这种管理 MIG 设备的方式很容易导致 GPU 利用率低下以及集群管理员花费大量时间更改 MIG 配置。实际上，GPU 内存和计算需求因 Pod 而异，并且会随着时间变化。为了在创建具有不同
    MIG 资源请求的新 Pod 时实现最佳 GPU 利用率，集群管理员必须不断花时间寻找和应用最合适的配置，这对每个节点来说非常不切实际。
- en: As a trivial example, consider that we need to schedule multiple Pods that require
    20gb of GPU memory. We would therefore create and apply a configuration that provides
    only `nvidia.com/mig-3.20gb` profiles on all the GPUs in our cluster, since it
    allows to perfectly use all GPU resources. In a second moment however, the server
    receives a request to create some Pods that require less resources, say 10GB of
    GPU memory, corresponding to the MIG profile `nvidia.com/mig-2g.10gb`. These Pods
    will not be scheduled until the cluster admin changes the label of at least one
    node applying a MIG configuration that provides the requested profiles.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个简单的例子，假设我们需要调度多个需要 20GB GPU 内存的 Pods。因此，我们将创建并应用一个配置，该配置在集群中的所有 GPU 上提供
    `nvidia.com/mig-3.20gb` 配置，因为它可以完美地利用所有 GPU 资源。然而，稍后，服务器收到创建一些需要较少资源的 Pods 的请求，例如
    10GB GPU 内存，对应于 MIG 配置 `nvidia.com/mig-2g.10gb`。这些 Pods 直到集群管理员更改至少一个节点的标签并应用提供所请求配置的
    MIG 配置之前都不会被调度。
- en: Complications do not end here. While a certain configuration might provide the
    required MIG resources, at the same it might also remove some of the devices that
    are currently in use by some containers. In such cases it is up to the cluster
    admins to find or create the most proper configuration, and to ensure it does
    not delete any of the used devices, introducing significant operational costs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 问题并没有在这里结束。虽然某个配置可能提供所需的 MIG 资源，但它同时可能会移除一些当前由容器使用的设备。在这种情况下，集群管理员需要寻找或创建最合适的配置，并确保不会删除任何正在使用的设备，从而引入显著的操作成本。
- en: '*This approach simply does not scale. With the NVIDIA GPU Operator alone, it
    is impractical to constantly adjust the MIG configurations based on workloads
    demand, resulting in both unused MIG devices and pending Pods.*'
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*这种方法根本无法扩展。仅靠 NVIDIA GPU 操作员，不可能根据工作负载需求不断调整 MIG 配置，从而导致未使用的 MIG 设备和待处理的 Pods。*'
- en: Let’s see how we can solve this issue with Dynamic MIG Partitioning.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何通过动态 MIG 分区解决这个问题。
- en: Dynamic MIG Partitioning
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态 MIG 分区
- en: Dynamic MIG Partitioning automates the creation and deletion of MIG profiles
    based on real-time requirements of the workloads in the cluster, ensuring that
    the optimal MIG configuration is always applied to the available GPUs.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 动态 MIG 分区根据集群中工作负载的实时需求自动创建和删除 MIG 配置，确保始终将最佳的 MIG 配置应用于可用的 GPU。
- en: To apply dynamic partitioning, we need to use `[nos](https://github.com/nebuly-ai/nos)`,
    an open-source module that runs alongside the NVIDIA GPU Operator and makes MIG
    partitioning dynamic.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要应用动态分区，我们需要使用 `[nos](https://github.com/nebuly-ai/nos)`，这是一个开源模块，它与 NVIDIA
    GPU 操作员一起运行，使 MIG 分区变得动态。
- en: 'You can think of `nos` as a [Cluster Autoscaler](https://github.com/kubernetes/autoscaler)
    for GPUs: instead of scaling up the number of nodes and GPUs, it dynamically partitions
    them to maximize their utilization, leading to spare GPU capacity. Then, you can
    schedule more Pods or reduce the number of GPU nodes needed, reducing infrastructure
    costs.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以把 `nos` 看作是 GPUs 的 [集群自动扩展器](https://github.com/kubernetes/autoscaler)：它不是增加节点和
    GPU 的数量，而是动态地分区它们以最大化其利用率，从而导致闲置的 GPU 容量。然后，你可以调度更多的 Pods 或减少所需的 GPU 节点数量，从而降低基础设施成本。
- en: '*With* `*nos*`*, there is no need to manually create and manage MIG configurations.
    Simply submit your Pods to the cluster and the requested MIG devices are automatically
    provisioned.*'
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*有了* `*nos*` *，无需手动创建和管理 MIG 配置。只需将你的 Pods 提交到集群，请求的 MIG 设备将自动配置。*'
- en: Let’s explore how `nos` and Dynamic MIG Partitioning work in practice.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解 `nos` 和动态 MIG 分区在实际中的工作方式。
- en: Prerequisites
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 先决条件
- en: 'As already mentioned, `nos` does not replace the NVIDIA GPU Operator, but it
    works alongside it. Hence, you need to install it first by meeting two requirements:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，`nos` 并不会替代 NVIDIA GPU Operator，而是与其一起工作。因此，你需要首先安装它，并满足两个要求：
- en: '`mig.strategy` must be set to `mixed`, so that every different MIG profile
    is exposed to Kubernetes as a specific resource type'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`mig.strategy` 必须设置为 `mixed`，这样每个不同的 MIG 配置文件就会作为特定的资源类型暴露给 Kubernetes。'
- en: '`migManager` must be disabled'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`migManager` 必须被禁用'
- en: 'If not already done, you can install the NVIDIA GPU Operator with Helm as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果尚未完成，你可以通过 Helm 安装 NVIDIA GPU Operator，如下所示：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'By default, MIG mode is not activated on NVIDIA GPUs. So, first you need to
    enable MIG on all the GPUs you want to be partitioned. You can do this by SSHing
    into the node and running the following command for each GPU, where `<index>`
    corresponds to their respective index:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，MIG 模式在 NVIDIA GPU 上未激活。因此，首先你需要在所有你希望进行分区的 GPU 上启用 MIG。你可以通过 SSH 进入节点并为每个
    GPU 运行以下命令，其中 `<index>` 对应于各自的索引：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Depending on the type of machine you are using, it may be necessary to reboot
    the node after this operation. For more information and troubleshooting, you can
    refer to the [NVIDIA MIG User Guide](https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#enable-mig-mode).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你使用的机器类型，可能需要在此操作后重启节点。有关更多信息和故障排除，请参阅 [NVIDIA MIG 用户指南](https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#enable-mig-mode)。
- en: Installation
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装
- en: 'Once you have installed the NVIDIA GPU Operator and enabled MIG mode on your
    GPUs, you can simply install `nos` as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你安装了 NVIDIA GPU Operator 并启用了你的 GPU 上的 MIG 模式，你可以简单地按如下方式安装 `nos`：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: That’s it! Now you are ready to activate Dynamic MIG Partitioning on your nodes.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！现在你可以在你的节点上激活动态 MIG 分区。
- en: Dynamic partitioning in action
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动态分区操作中
- en: 'First, you need to specify to `nos` for which nodes it should manage GPU partitioning.
    Label those nodes as follow:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要向 `nos` 指定它应该管理哪些节点的 GPU 分区。将这些节点标记如下：
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This label marks a node as a “MIG node”, delegating the management of MIG devices
    of all the node’s GPUs to `nos`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个标签将节点标记为“MIG 节点”，将所有节点 GPU 的 MIG 设备管理委托给 `nos`。
- en: After that, you can submit workloads requesting MIG resources. `nos` will automatically
    find and apply the best MIG configuration on the GPUs of the nodes you previously
    marked as “MIG nodes”, creating the missing MIG devices requested by Pods and
    deleting the unnecessary unused ones.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，你可以提交请求 MIG 资源的工作负载。`nos` 将自动找到并应用你之前标记为“MIG 节点”的节点上的最佳 MIG 配置，创建 Pods 请求的缺失
    MIG 设备，并删除不必要的未使用设备。
- en: Let’s take a look at a simple example of `nos` in action.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个 `nos` 实际操作的简单示例。
- en: 'Assume we are operating a simple cluster with two nodes, one of which has a
    single NVIDIA A100 80GB. We have already enabled MIG mode on that GPU, so we can
    enable automatic partitioning for that node:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们操作一个简单的集群，其中一个节点有一个 NVIDIA A100 80GB。我们已经在该 GPU 上启用了 MIG 模式，因此我们可以为该节点启用自动分区：
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output of `kubectl describe node aks-gpua100–24975740-vmss000000` shows
    that the node does not have any available MIG resources, since no MIG device has
    been requested or created yet:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl describe node aks-gpua100–24975740-vmss000000` 的输出显示节点没有任何可用的 MIG
    资源，因为尚未请求或创建任何 MIG 设备：'
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Let’s create some Pods requesting MIG resources. In this case, we create a deployment
    with 5 replicas of a Pod with a container requesting a GPU slice of 10 GB of memory.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一些请求 MIG 资源的 Pods。在这种情况下，我们创建一个部署，包含 5 个 Pod 副本，每个 Pod 的容器请求 10 GB 内存的
    GPU 切片。
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'There are now 5 pending Pods in the namespace `demo`, requesting a total of
    five `nvidia.com/mig-1g.10gb` resources which are not yet available in the cluster:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，命名空间 `demo` 中有 5 个待处理的 Pods，请求总共五个 `nvidia.com/mig-1g.10gb` 资源，而这些资源在集群中尚不可用：
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In a few seconds, `nos` will detect these pending Pods. It will try to create
    the requested resources selecting the most suitable MIG configuration. In this
    example, `nos` applies a configuration that provides five 1g.10gb and one 2g.20gb
    devices:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 几秒钟内，`nos` 将检测到这些待处理 Pods。它将尝试创建所请求的资源，选择最合适的 MIG 配置。在这个例子中，`nos` 应用了一种提供五个
    1g.10gb 和一个 2g.20gb 设备的配置：
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'If we check once again the state of the Pods, we can see that this time they
    are now in Running state:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们再次检查 Pods 的状态，可以看到这次它们现在处于运行状态：
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Note that, besides the 1g.10gb devices, `nos` also created an additional 2g.20gb
    device. This is because each MIG GPU model only supports a [specific set of configurations](https://docs.nvidia.com/datacenter/tesla/mig-user-guide/#supported-profiles),
    and, in this scenario, the best configuration that met the required devices also
    included the 2g.20gb device. Keep in mind that:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，除了 1g.10gb 设备外，`nos` 还创建了额外的 2g.20gb 设备。这是因为每种 MIG GPU 模型仅支持 [特定的配置集合](https://docs.nvidia.com/datacenter/tesla/mig-user-guide/#supported-profiles)，在这种情况下，满足所需设备的最佳配置也包括
    2g.20gb 设备。请记住：
- en: '`nos` selects the configuration that allows to schedule the highest number
    of pending pods, which is computed leveraging a scheduling simulation done by
    the `nos` the internal scheduler'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nos` 选择允许调度最多待处理 Pods 的配置，这一计算是通过 `nos` 内部调度器进行的调度模拟完成的。'
- en: MIG devices that are already in use are never deleted. Any MIG configuration
    that would require the deletion of these devices is rejected.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已经在使用中的 MIG 设备不会被删除。任何需要删除这些设备的 MIG 配置都会被拒绝。
- en: Conclusions
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: The possibility of requesting GPU slices is crucial for improving GPU utilization
    and cutting down infrastructure costs.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 请求 GPU 切片的可能性对提高 GPU 利用率和降低基础设施成本至关重要。
- en: NVIDIA MIG allows to create fully-isolated GPU instances with dedicated memory
    and compute resources, but the support in Kubernetes provided by the NVIDIA GPU
    Operator is not enough if we want to achieve operational excellence. Static configurations
    do not automatically adjust to the changing demands of workloads and thus are
    inadequate to provide each Pod with the GPU slices it requires, especially in
    scenarios with workloads demanding a variety of slices in terms of memory and
    computing that change over time.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA MIG 允许创建完全隔离的 GPU 实例，配备专用的内存和计算资源，但如果我们希望实现卓越的操作，NVIDIA GPU 操作员提供的 Kubernetes
    支持是不够的。静态配置不能自动调整以适应工作负载的变化需求，因此无法为每个 Pod 提供所需的 GPU 切片，特别是在需要不同内存和计算切片的工作负载场景中，这些需求随着时间的推移而变化。
- en: '`*nos*` *overcomes NVIDIA GPU Operator static configurations limitations through
    Dynamic GPU Partitioning, which increases GPU utilization and reduces the operation
    burden of manually defining and applying MIG configurations on the cluster nodes.*'
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '`*nos*` *通过动态 GPU 分区克服了 NVIDIA GPU 操作员静态配置的限制，这种方法提高了 GPU 利用率，减少了在集群节点上手动定义和应用
    MIG 配置的操作负担。*'
- en: It is worth noting that NVIDIA MIG has its limitations and is not the only partitioning
    technology, nor the only way to increase the utilization in a Kubernetes cluster.
    Specifically, MIG is only supported on newer architectures (Ampere and Hopper)
    and does not offer fine-grained GPU partitioning, meaning it is not possible to
    create GPU slices with arbitrary memory and compute resources.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，NVIDIA MIG 有其局限性，并不是唯一的分区技术，也不是提高 Kubernetes 集群利用率的唯一方法。具体而言，MIG 仅支持较新的架构（Ampere
    和 Hopper），且不提供细粒度的 GPU 分区，这意味着无法创建具有任意内存和计算资源的 GPU 切片。
- en: To overcome these limitations, `nos` also offers [Dynamic GPU Partitioning through
    NVIDIA Multi-Process Service (MPS)](https://docs.nebuly.com/nos/overview/), a
    partitioning technology that is supported by almost all NVIDIA GPUs and allows
    to create slices of any desired amount of memory. You can find more information
    on Dynamic MPS partitioning [here](https://docs.nebuly.com/nos/dynamic-gpu-partitioning/getting-started-mps/).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这些限制，`nos` 还提供了 [通过 NVIDIA 多进程服务 (MPS) 的动态 GPU 分区](https://docs.nebuly.com/nos/overview/)，这是一种支持几乎所有
    NVIDIA GPU 的分区技术，允许创建任意数量的内存切片。你可以在 [这里](https://docs.nebuly.com/nos/dynamic-gpu-partitioning/getting-started-mps/)
    找到有关动态 MPS 分区的更多信息。
- en: Resources
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源
- en: '[Dynamic GPU Partitioning documentation](https://docs.nebuly.com/nos/dynamic-gpu-partitioning/overview/)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[动态 GPU 分区文档](https://docs.nebuly.com/nos/dynamic-gpu-partitioning/overview/)'
- en: '[Nos source code](https://github.com/nebuly-ai/nos)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Nos 源代码](https://github.com/nebuly-ai/nos)'
- en: '[NVIDIA GPU Operator documentation](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/getting-started.html)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[NVIDIA GPU 操作员文档](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/getting-started.html)'
- en: '[NVIDIA MIG User guide](https://docs.nvidia.com/datacenter/tesla/mig-user-guide/)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[NVIDIA MIG 用户指南](https://docs.nvidia.com/datacenter/tesla/mig-user-guide/)'
- en: Credits
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 版权
- en: Special thanks to [Emile Courthoud](https://medium.com/u/2c53981c8a83?source=post_page-----89db6cdde7a3--------------------------------)
    for their review and contributions to this article.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 特别感谢[Emile Courthoud](https://medium.com/u/2c53981c8a83?source=post_page-----89db6cdde7a3--------------------------------)对本文的审阅和贡献。
