- en: Can Weight Decay Work Without Residual Connections?
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 权重衰减在没有残差连接的情况下能有效吗？
- en: 原文：[https://towardsdatascience.com/weight-decay-is-useless-without-residual-connections-ef65197e5944?source=collection_archive---------13-----------------------#2023-02-21](https://towardsdatascience.com/weight-decay-is-useless-without-residual-connections-ef65197e5944?source=collection_archive---------13-----------------------#2023-02-21)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://towardsdatascience.com/weight-decay-is-useless-without-residual-connections-ef65197e5944?source=collection_archive---------13-----------------------#2023-02-21](https://towardsdatascience.com/weight-decay-is-useless-without-residual-connections-ef65197e5944?source=collection_archive---------13-----------------------#2023-02-21)
- en: How do residual connections secretly fight overfitting?
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 残差连接如何秘密地对抗过拟合？
- en: '[](https://guydar.medium.com/?source=post_page-----ef65197e5944--------------------------------)[![Guy
    Dar](../Images/0a3b1ddd33d595e97e7a0dad551b2708.png)](https://guydar.medium.com/?source=post_page-----ef65197e5944--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ef65197e5944--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ef65197e5944--------------------------------)
    [Guy Dar](https://guydar.medium.com/?source=post_page-----ef65197e5944--------------------------------)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://guydar.medium.com/?source=post_page-----ef65197e5944--------------------------------)[![Guy
    Dar](../Images/0a3b1ddd33d595e97e7a0dad551b2708.png)](https://guydar.medium.com/?source=post_page-----ef65197e5944--------------------------------)[](https://towardsdatascience.com/?source=post_page-----ef65197e5944--------------------------------)[![Towards
    Data Science](../Images/a6ff2676ffcc0c7aad8aaf1d79379785.png)](https://towardsdatascience.com/?source=post_page-----ef65197e5944--------------------------------)
    [Guy Dar](https://guydar.medium.com/?source=post_page-----ef65197e5944--------------------------------)'
- en: ·
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ·
- en: '[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffab216dbde3e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweight-decay-is-useless-without-residual-connections-ef65197e5944&user=Guy+Dar&userId=fab216dbde3e&source=post_page-fab216dbde3e----ef65197e5944---------------------post_header-----------)
    Published in [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ef65197e5944--------------------------------)
    ·9 min read·Feb 21, 2023[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fef65197e5944&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweight-decay-is-useless-without-residual-connections-ef65197e5944&user=Guy+Dar&userId=fab216dbde3e&source=-----ef65197e5944---------------------clap_footer-----------)'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[关注](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2Ffab216dbde3e&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweight-decay-is-useless-without-residual-connections-ef65197e5944&user=Guy+Dar&userId=fab216dbde3e&source=post_page-fab216dbde3e----ef65197e5944---------------------post_header-----------)
    发布于 [Towards Data Science](https://towardsdatascience.com/?source=post_page-----ef65197e5944--------------------------------)
    ·9 min read·2023年2月21日[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Ftowards-data-science%2Fef65197e5944&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweight-decay-is-useless-without-residual-connections-ef65197e5944&user=Guy+Dar&userId=fab216dbde3e&source=-----ef65197e5944---------------------clap_footer-----------)'
- en: --
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: --
- en: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fef65197e5944&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweight-decay-is-useless-without-residual-connections-ef65197e5944&source=-----ef65197e5944---------------------bookmark_footer-----------)![](../Images/f8e4f8de8c3ec7f53556112e57392c22.png)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2Fef65197e5944&operation=register&redirect=https%3A%2F%2Ftowardsdatascience.com%2Fweight-decay-is-useless-without-residual-connections-ef65197e5944&source=-----ef65197e5944---------------------bookmark_footer-----------)![](../Images/f8e4f8de8c3ec7f53556112e57392c22.png)'
- en: Photo by ThisisEngineering RAEng on Unsplash
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图片由 ThisisEngineering RAEng 提供，来源于 Unsplash
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: In this post, we will show mathematically (using *simple* math, don’t worry!)
    that LayerNorm could have caused detrimental overfitting in neural networks if
    it weren’t for residual connections (* a few technical caveats will be discussed
    soon). Overfitting is a severe problem in machine learning that scientists could
    really do without. Roughly, it means that the network memorizes the training set
    instead of learning the task and generalizing to new unseen data. Weight decay
    is a regularization technique that is supposed to fight against overfitting. However,
    we will show that in rather standard feedforward networks, they need residual
    connections to be effective (in a sense I will clarify below). Residual connections
    are known for their role in stabilizing training during backpropagation. Normalization
    layers, such as LayerNorm, on the other hand, stabilize the input as it passes
    through the network’s layers. The idea behind normalization layers is that we
    want the norms of internal representations to stay in check. In LayerNorm, we
    divide each internal representation vector by its norm (I’m omitting some details
    at this point). Some previous work has suggested adding LayerNorm to residual
    networks [*Liu et al, 2020*] helps them perform better. Our investigation is the
    other way around — why you shouldn’t probably use LayerNorm without residual connections,
    at least from a theoretical point of view. In fact, even post-norm residual connections
    (whatever that means..) suffer from the same problem.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇文章中，我们将用数学方法（使用*简单*的数学，别担心！）展示如果没有残差连接，LayerNorm 可能会导致神经网络的过拟合问题。过拟合是机器学习中的一个严重问题，科学家们真的希望避免这种情况。粗略来说，过拟合意味着网络记住了训练集，而不是学习任务并推广到新的未见数据上。权重衰减是一种用于对抗过拟合的正则化技术。然而，我们将展示在相当标准的前馈网络中，它们需要残差连接才能有效（在下面的解释中我会进一步澄清）。残差连接以其在反向传播过程中稳定训练的作用而闻名。另一方面，Normalization
    层（如 LayerNorm）则在输入通过网络层时稳定输入。Normalization 层的想法是我们希望内部表示的范数保持在一定范围内。在 LayerNorm
    中，我们通过其范数来除以每个内部表示向量（此时我省略了一些细节）。一些先前的工作建议将 LayerNorm 添加到残差网络中[*Liu et al, 2020*]，这有助于提高它们的性能。我们的研究则是另一个方向——为什么从理论上讲，你可能不应该在没有残差连接的情况下使用
    LayerNorm。实际上，即使是后规范残差连接（无论那是什么意思……）也会遭遇同样的问题。
- en: 'The idea in broad strokes is fairly simple: we can render weight decay practically
    useless by making it arbitrarily small (except for the last layer, which is a
    bane but still renders most of the weight decay useless). Just a quick recap of
    what weight decay is: weight decay is a regularization technique that is used
    to prevent neural networks from converging to solutions that do not generalize
    to unseen data (overfitting). If we train the neural network to only minimize
    the loss on the training data we might find a solution specifically tailored to
    this particular data and its idiosyncrasies. To avoid that, we add a term that
    corresponds to the norm of the weight matrices of the network. This is supposed
    to encourage the optimization process to converge to solutions that might not
    be optimal for the training data, but have smaller weight matrices in terms of
    norm. The thinking is that models that have high norm weights are less natural
    and might be trying to fit specific data points in order to lower the loss a bit
    more. In a way, this is a way to integrate Occam’s razor (the philosophical idea
    that simpler solutions are probably the right ones) into the loss — where simplicity
    is captured by the norm of the weights. We will not discuss deeper justifications
    for weight decay here.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 从广义上讲，思想相当简单：通过将权重衰减设得非常小（除了最后一层，它虽然令人头疼，但仍使大部分权重衰减变得无用），我们可以使权重衰减几乎没有实际用处。简单回顾一下什么是权重衰减：权重衰减是一种正则化技术，用于防止神经网络收敛到不能推广到未见数据的解决方案（即过拟合）。如果我们训练神经网络只去最小化训练数据上的损失，我们可能会找到一个专门为这特定数据及其特性量身定制的解决方案。为避免这种情况，我们添加一个与网络权重矩阵的范数相关的项。这旨在鼓励优化过程收敛到可能对训练数据不是最优的解决方案，但在范数上具有较小权重矩阵的解决方案。其思路是，具有高范数权重的模型不够自然，可能试图拟合特定的数据点以进一步降低损失。在某种程度上，这是一种将奥卡姆剃刀（哲学观点认为简单的解决方案可能是正确的）集成到损失中的方式——其中简单性由权重的范数来体现。我们在这里不会讨论权重衰减的更深层次的理论依据。
- en: '**TL;DR**:in this article,we show that in ReLU feedforward networks with LayerNorm
    that don’t have residual connections, the optimal loss value is not changed by
    weight decay regularization (* except for the pesky last layer but we will discuss
    later why we think it’s not enough).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**TL;DR**：在这篇文章中，我们展示了在没有残差连接的 ReLU 前馈网络中，添加 LayerNorm 后，权重衰减正则化不会改变最优损失值（*除了讨厌的最后一层，我们稍后会讨论为什么我们认为这还不够）。 '
- en: Theory Playtime
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理论游戏时间
- en: '**Positive Scaling of ReLU Networks**'
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**ReLU 网络的正尺度性**'
- en: 'Both linear and ReLU networks share the following scaling feature: Let **a**
    > 0 be a positive scalar. Then **ReLU(ax) = a ReLU(x)**. Eventually, in any network
    that is composed of a stack of matrix multiplication followed by a ReLU activation,
    this property will still hold. This is the most vanilla kind of neural network
    — no normalization layers and no residual connections. Yet, it’s rather surprising
    that such feedforward (FF) networks that have been ubiquitous not so long ago
    demonstrate such a structured behavior: multiply your input by a positive scalar
    and, lo and behold, the output isscaled **exactly** by the same factor. This is
    what we call *(positive)* ***scale equivariance*** (meaning that scaling in the
    input translates to scaling in the output, unlike *invariance* where the output
    is not affected at all by scaling in the input)*.* But there’s more: if we do
    this with any of the weight matrices along the way (and the corresponding bias
    terms) — the same effect occurs: the output will be multiplied by the same factor.
    Nice? For sure. But can we use it? Let’s see.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 线性网络和 ReLU 网络具有以下尺度特性：设 **a** > 0 为正标量。那么 **ReLU(ax) = a ReLU(x)**。最终，在任何由矩阵乘法堆叠后跟
    ReLU 激活组成的网络中，这个属性仍然有效。这是最普通的神经网络——没有归一化层，也没有残差连接。然而，这种之前曾广泛存在的前馈 (FF) 网络展示了这样一种结构化行为确实令人惊讶：将输入乘以正标量，结果输出**正好**按相同的因子缩放。这就是我们所称的
    *(正向)* ***尺度等变性***（这意味着输入的尺度变换会传递到输出的尺度变换，而 *不变性* 则表示输入的尺度变化不会对输出产生影响）。但还有更多：如果我们在过程中对任何权重矩阵（以及相应的偏置项）进行同样的操作——效果也是一样的：输出将按相同因子放大。不错吧？当然。但是我们能利用它吗？让我们看看。
- en: 'Let’s see what happens when we add LayerNorm. First, what is LayerNorm? Quick
    recap:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看添加 LayerNorm 后会发生什么。首先，什么是 LayerNorm？简单回顾一下：
- en: '![](../Images/5124a0125668a4824ed9dcf3ede31074.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5124a0125668a4824ed9dcf3ede31074.png)'
- en: where **µ** is the average of the entries of **x**, *◦*stands for elementwise
    multiplication, and **β, γ** are vectors.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 **µ** 是 **x** 条目的平均值，*◦* 表示逐元素乘法，**β, γ** 是向量。
- en: 'So, what happens to the scaling property when we add LayerNorm? Of course,
    nothing changes before the point we added the LayerNorm, so if we scaled a weight
    matrix by **a**>0 before this point, the input to the LayerNorm is scaled by **a**,and
    then what happens is:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，当我们添加 LayerNorm 时，尺度特性会发生什么变化？当然，在添加 LayerNorm 之前，情况不会改变，因此如果我们在此之前将权重矩阵缩放了
    **a** > 0，那么 LayerNorm 的输入会被 **a** 缩放，然后发生的情况是：
- en: '![](../Images/b1e9da85a8554edb34aabbfe7c5ca942.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b1e9da85a8554edb34aabbfe7c5ca942.png)'
- en: So, we get a new property, this time scaling just leaves the output *unchanged
    —* **positive scale *invariance***. And it’s about to regret that...
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们得到一个新的属性，这次尺度缩放只是使输出 *保持不变——* **正尺度 *不变性***。而且，这即将变得令人遗憾……
- en: '***Note:*** while we discuss LayerNorm, other forms of normalization, such
    as BatchNorm, satisfy the positive scale invariance property and so they are as
    susceptible as LayerNorm to the discussed problems.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '***注意：*** 当我们讨论 LayerNorm 时，其他形式的归一化，如 BatchNorm，满足正尺度不变性属性，因此它们也像 LayerNorm
    一样容易受到讨论中提到的问题的影响。'
- en: How to Disappear Completely
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何完全消失
- en: 'Let’s remind ourselves what we’re trying to minimize:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们提醒自己我们正在尝试最小化什么：
- en: '![](../Images/f19404ac9585bb2ca33a7afd4e576dde.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f19404ac9585bb2ca33a7afd4e576dde.png)'
- en: 'where the training set is represented as a set of pairs of {***(x_i, y_i)***},and
    the parameters (weights) of the neural network ***f*** are designated by **Θ**.
    The expression is made of two parts: the empirical loss to minimize — the loss
    of the neural network on the training set, and the regularization term designed
    to make the model reach “simpler” solutions. In this case, simplicity is quantified
    as the weights of the network having low norms.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 其中训练集表示为一组 {***(x_i, y_i)***} 的对，神经网络的参数（权重）***f*** 由 **Θ** 指定。这个表达式由两部分组成：需要最小化的经验损失——神经网络在训练集上的损失，以及旨在使模型达到“更简单”解决方案的正则化项。在这种情况下，简单性被量化为网络权重具有低范数。
- en: But here’s the problem, we found a way to bypass restrictions on the weight
    scale. We can scale every weight matrix by an arbitrarily small factor and still
    get the same output. Said otherwise — the **function *f***that both networks,
    the original one and the scaled one, implement is exactly the same! The internals
    might differ, but the output is the same. It holds for every network with this
    architecture, regardless of the actual values of the parameters. The shrewd reader
    might notice that **β, γ** are also parameters and so they also should be accounted
    for, but due to the scale invariance of the next LayerNorm, they can also be scaled
    (* except again for the pesky last layer).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 但问题在于，我们发现了一种绕过权重规模限制的方法。我们可以将每个权重矩阵按任意小的因子进行缩放，仍然能得到相同的输出。换句话说——**函数 *f***，即原始网络和缩放网络实现的函数是完全相同的！内部可能不同，但输出是相同的。这适用于每一个具有这种架构的网络，不管参数的实际值是什么。聪明的读者可能会注意到**β,
    γ**也是参数，因此也应考虑，但由于下一个LayerNorm的尺度不变性，它们也可以被缩放（*除了那个恼人的最后一层）。
- en: Recall that generalization to unseen data is our goal. If the regularization
    term goes to zero, the network is free to overfit the training data, and the regularization
    term becomes useless. As we have seen, for every network with such architecture,
    we can design an equivalent network (i.e. computing exactly the same function)
    with arbitrarily small weight matrix norms, meaning the regularization term can
    go to zero without affecting the empirical loss term. In other words, we can remove
    the weight decay term and it’s not going to matter.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们的目标是对未见数据进行泛化。如果正则化项趋近于零，网络就可以自由地过拟合训练数据，正则化项也就变得无用。正如我们所见，对于每一个具有这种架构的网络，我们可以设计一个等效的网络（即计算完全相同的函数），其权重矩阵范数可以任意小，这意味着正则化项可以趋近于零而不影响经验损失项。换句话说，我们可以去掉权重衰减项，它不会有太大影响。
- en: 'The last layer technicality is still a pebble in our shoe, we cannot take its
    weight decay term out, because the last layer is usually not followed by a LayerNorm,
    moreover: the last LayerNorm cannot be rescaled because it is not followed by
    any LayerNorm, so this leaves us also its **β, γ**. So we are left with the norm
    of the last layer and the parameters of the LayerNorm. Why is it not that bad?
    Overfitting can take place in the entire network and so we are free to overfit
    almost all the parameters. Even more importantly, the inner layers of the model
    are the feature extractors that learn what to pay attention to, while the last
    layer is a classifier over the learned features. It seems (intuitively at least)
    that its role in overfitting might be minor in comparison to feature extraction.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一层的技术细节仍然是我们面临的一个小问题，我们不能去掉它的权重衰减项，因为最后一层通常没有跟随LayerNorm，而且：最后的LayerNorm无法重新缩放，因为它后面没有任何LayerNorm，因此我们还要考虑它的**β,
    γ**。所以我们剩下的只有最后一层的范数和LayerNorm的参数。为什么这并不是那么糟糕？过拟合可能发生在整个网络中，因此我们可以几乎所有参数进行过拟合。更重要的是，模型的内部层是学习关注点的特征提取器，而最后一层是对学习到的特征进行分类的分类器。至少从直观上看，它在过拟合中的作用可能相比于特征提取要小得多。
- en: 'Another word of caution is due: while theoretically, the model should find
    a solution that overfits the training data, it has been observed that optimization
    might converge to generalizing solutions even without explicit regularization.
    This has to do with the optimization algorithm. We use local optimization algorithms
    such as gradient descent, SGD, Adam, AdaGrad, etc. They are not guaranteed to
    converge to the most ***globally*** optimal solution. This sometimes happens to
    be a blessing. An interesting line of work (e.g., [*Neyshabur, 2017*]) suggests
    that these algorithms are a form of **implicit regularization**, even when explicit
    regularization is missing! It’s not bulletproof, but sometimes the model converges
    to a generalizing solution — even without regularization terms!'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一句警告：尽管理论上，模型应该找到一个过拟合训练数据的解决方案，但观察到优化可能会收敛到泛化解决方案，即使没有显式正则化。这与优化算法有关。我们使用局部优化算法，例如梯度下降、SGD、Adam、AdaGrad等。它们不能保证收敛到最***全局***最优的解决方案。这有时反而是一种福气。一项有趣的研究（例如，[*Neyshabur,
    2017*]）表明，即使没有显式正则化，这些算法也属于**隐式正则化**的一种形式！虽然并非万无一失，但有时模型会收敛到一个泛化解决方案——即使没有正则化项！
- en: How Do Residual Connections Resolve This?
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 残差连接如何解决这个问题？
- en: Let me remind you what residual connections are. A residual connection adds
    the input of the layer to the output. If the original function that the layer
    is computing is ***f(x) = ReLU(Wx)*** then the new function is***x + f(x)***.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我提醒你什么是残差连接。残差连接将层的输入加到输出上。如果该层正在计算的原始函数是 ***f(x) = ReLU(Wx)***，那么新函数是 ***x
    + f(x)***。
- en: 'Now, the scaling property on weights breaks for this new layer. This is because
    there is no coefficient learned in front of the residual part of the expression.
    So the ***f(x)*** part is being scaled by a constant because of the weight scaling,
    but the ***x*** part remains unchanged. Now, when we apply LayerNorm to this,
    the scaling factor can no longer cancel out: ***LayerNorm(x + a f(x)) ≠ LayerNorm(x
    + f(x))***.Importantly, it is the case only when the residual connection is applied
    ***before*** LayerNorm. If we apply LayerNorm and only then the residual connection,
    it turns out that we still get the scaling invariance of LayerNorm: ***x + LayerNorm(a
    f(x)) = x + LayerNorm(f(x))***.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这一新层的权重缩放特性被打破了。这是因为在表达式的残差部分前没有学习到系数。因此，***f(x)*** 部分由于权重缩放而被一个常数缩放，但***x***
    部分保持不变。现在，当我们对其应用 LayerNorm 时，缩放因子无法再被抵消：***LayerNorm(x + a f(x)) ≠ LayerNorm(x
    + f(x))***。重要的是，只有在残差连接应用在 LayerNorm ***之前*** 时才会出现这种情况。如果我们先应用 LayerNorm，然后再进行残差连接，则会发现我们仍然获得
    LayerNorm 的缩放不变性：***x + LayerNorm(a f(x)) = x + LayerNorm(f(x))***。
- en: The first variant is often referred to as the ***pre-norm variant***(more precisely,
    it is actually ***x + f(LayerNorm(x))*** that is called this way, but we can attribute
    the LayerNorm to the previous layer, and take the next layer’s LayerNormyielding
    the above expression, apart for the edge cases of the first and last layers).
    The second variant is called the ***post-norm variant***. These terms are often
    used in transformer architectures, which are out of the scope of this article.
    However, it might be interesting to mention that a few works such as [*Xioang
    et al, 2020*] found that pre-norm is easier to optimize (they discuss different
    reasons for the problem). Note however this may not be related to the scaling
    invariance discussed here. Transformer pre-training datasets often contain vast
    amounts of data, and overfitting becomes less of a problem. Also, we haven’t discussed
    transformer architectures per se. It is nonetheless still something to think about.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个变体通常被称为 ***pre-norm 变体***（更准确地说，实际上是 ***x + f(LayerNorm(x))*** 被称作这种方式，但我们可以将
    LayerNorm 归因于前一层，并将下一层的 LayerNorm 视为上面的表达式，除了第一层和最后一层的边缘情况）。第二个变体被称为 ***post-norm
    变体***。这些术语常用于 transformer 架构中，而这些超出了本文的范围。然而，值得一提的是，一些研究如 [*Xioang et al, 2020*]
    发现 pre-norm 更易于优化（他们讨论了不同的原因）。然而，请注意这可能与这里讨论的缩放不变性无关。Transformer 预训练数据集通常包含大量数据，过拟合问题变得不那么严重。此外，我们还没有讨论
    transformer 架构本身。但这仍然值得考虑。
- en: Final Words
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this article, we saw some interesting properties of feedforward neural networks
    without pre-norm residual connections. Specifically, we noticed that if they don't
    contain LayerNorm, they propagate input scaling and weight scaling to the output.
    If they do contain LayerNorm, they are scale-invariant (except for the last layer),
    and weight/input scaling does not affect the output at all. We used this property
    to show that the **optimal** solutions to such networks can avoid weight norm
    penalty in feature extraction (leaving only classifier penalty), and so the network
    can converge to more or less the same function it would have converged to without
    them. While this is a statement about optimality, there is still the question
    of whether these solutions are actually found using gradient descent. We might
    tackle this in a future post. We also discussed how (pre-norm) residual connections
    break the scale invariance and thus seem to resolve the above theoretical problem.
    It is still possible that there will be similar properties that residual connections
    could not avoid that I failed to consider. As always, I want to thank you for
    reading and I’ll see you in the next post!
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们观察到了一些没有预归一化残差连接的前馈神经网络的有趣特性。具体来说，我们注意到，如果它们不包含 LayerNorm，它们会将输入缩放和权重缩放传播到输出。如果它们包含
    LayerNorm，它们是尺度不变的（除了最后一层），而权重/输入缩放完全不会影响输出。我们利用这一特性展示了这样的网络的**最佳**解决方案可以避免特征提取中的权重范数惩罚（仅保留分类器惩罚），因此网络可以收敛到一个或多或少相同的函数，这个函数是它在没有这些技术的情况下会收敛的。虽然这是一个关于最优性的声明，但是否这些解决方案实际上是通过梯度下降找到的仍然是一个问题。我们可能会在未来的文章中探讨这一点。我们还讨论了（预归一化）残差连接如何破坏尺度不变性，从而似乎解决了上述理论问题。仍然可能存在一些残差连接无法避免的类似特性，我可能未曾考虑。像往常一样，感谢你的阅读，我们下篇文章见！
- en: References
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: F. Liu, X. Ren, Z. Zhang, X. Sun, and Y. Zou. *Rethinking residual connection
    with layer normalization,* 2020.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: F. Liu, X. Ren, Z. Zhang, X. Sun, 和 Y. Zou. *重新思考带有层归一化的残差连接*，2020.
- en: B. Neyshabur. *Implicit regularization in deep learning*, 2017\. URL [https://arxiv.org/abs/1709.01953](https://arxiv.org/abs/1709.01953).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: B. Neyshabur. *深度学习中的隐式正则化*，2017\. 网址 [https://arxiv.org/abs/1709.01953](https://arxiv.org/abs/1709.01953).
- en: R. Xiong, Y. Yang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zhang, Y. Lan, L.
    Wang, and T.-Y. Liu. *On layer normalization in the transformer architecture*,
    2020\. URL [https://arxiv.org/abs/2002.04745](https://arxiv.org/abs/2002.04745).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: R. Xiong, Y. Yang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zhang, Y. Lan, L.
    Wang, 和 T.-Y. Liu. *关于 Transformer 架构中的层归一化*，2020\. 网址 [https://arxiv.org/abs/2002.04745](https://arxiv.org/abs/2002.04745).
